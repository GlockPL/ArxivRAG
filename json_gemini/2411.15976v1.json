{"title": "DRIVE: DUAL-ROBUSTNESS VIA INFORMATION VARIABILITY AND ENTROPIC CONSISTENCY IN SOURCE-FREE UNSUPERVISED DOMAIN ADAPTATION", "authors": ["Ruiqiang Xiao", "Songning Lai", "Yijun Yang", "Jiemin Wu", "Yutao Yue", "Lei Zhu"], "abstract": "Adapting machine learning models to new domains without labeled data, especially when source data is inaccessible, is a critical challenge in applications like medical imaging, autonomous driving, and remote sensing. This task, known as Source-Free Unsupervised Domain Adaptation (SFUDA), involves adapting a pre-trained model to a target domain using only unlabeled target data, which can lead to issues such as overfitting, underfitting, and poor generalization due to domain discrepancies and noise. Existing SFUDA methods often rely on single-model architectures, struggling with uncertainty and variability in the target domain. To address these challenges, we propose DRIVE (Dual-Robustness through Information Variability and Entropy), a novel SFUDA framework leveraging a dual-model architecture. The two models, initialized with identical weights, work in parallel to capture diverse target domain characteristics. One model is exposed to perturbations via projection gradient descent (PGD) guided by mutual information, focusing on high-uncertainty regions. We also introduce an entropy-aware pseudo-labeling strategy that adjusts label weights based on prediction uncertainty, ensuring the model focuses on reliable data while avoiding noisy regions. The adaptation process has two stages: the first aligns the models on stable features using a mutual information consistency loss, and the second dynamically adjusts the perturbation level based on the loss from the first stage, encouraging the model to explore a broader range of the target domain while preserving existing performance. This enhances generalization capabilities and robustness against interference. Evaluations on standard SFUDA benchmarks show that DRIVE consistently outperforms previous methods, delivering improved adaptation accuracy and stability across complex target domains.", "sections": [{"title": "Introduction", "content": "Adapting machine learning models to new domains where labeled data is unavailable-especially when the original source data cannot be reused-represents a complex yet critical challenge in practical applications across fields"}, {"title": "Under Review", "content": "like medical imaging [1], autonomous driving [2, 3], and remote sensing [4]. This task, known as Source-Free Unsupervised Domain Adaptation (SFUDA), aims to adapt a pretrained model to a target domain using only unlabeled target data. Unlike traditional domain adaptation methods, which typically require access to both source and target data for knowledge transfer [5, 6], SFUDA operates under stricter conditions due to privacy, storage, and regulatory constraints that prohibit the reuse of source data. Thus, SFUDA has become a crucial area of research for real-world applications where source data is inaccessible.\nA major challenge in SFUDA is the high risk of model overfitting or underfitting in the target domain [7, 8], especially when domain discrepancies are large, or when the target data exhibits significant variability and noise [9, 10]. Without source data to guide the model, SFUDA methods must rely solely on unlabeled target data, which provides limited and often indirect information about the distributional differences between domains. To address this, most SFUDA techniques rely on self-supervised learning, consistency regularization, or confidence-based approaches to iteratively refine the model's adaptation to the target domain [11, 12, 13]. However, these approaches often use single-model architectures that struggle to generalize effectively when the target domain is noisy or uncertain, leading to brittle performance and suboptimal results [14, 15, 16].\nExisting methods like DIFO (Distilling multimodal Foundation models) [17], which leverages CLIP [18] and a two- stage distillation process for source-free adaptation, have shown promise. However, DIFO still faces challenges in handling substantial uncertainty or variability in the target domain. Its reliance on a fixed distillation strategy, which combines predictions from CLIP and the target model, often results in suboptimal pseudo-labels, especially when the target data is noisy or uncertain. This introduces a significant challenge for reliable domain transfer, as the model may overfit to noisy data or fail to capture the comprehensive underlying distribution of the target domain.\nThis limitation of single-model approaches motivates us to explore more robust and adaptive frameworks. While DIFO utilizes a single pretrained model for adaptation, it does not fully capitalize on the potential benefits of using multiple models to handle the inherent variability and uncertainty in the target domain. We hypothesize that incorporating a dual-model\u00b2 architecture can address these challenges more effectively, by leveraging complementary strengths of two models working in parallel to promote more stable and transferable adaptations.\nTo this end, we propose DRIVE (Dual-Robustness through Information Variability and Entropy), a novel SFUDA framework based on DIFO, designed to overcome these limitations. Our approach builds upon the insights from DIFO but introduces several key modifications to enhance robustness and generalizability. Rather than relying on a fixed, weighted combination of CLIP and the target model outputs, we introduce an entropy-based strategy to determine the weight of pseudo-labels. This entropy-aware method accounts for the uncertainty in the model's predictions, ensuring that the model prioritizes reliable regions of the target domain while avoiding over-reliance on noisy or uncertain labels.\nAt the core of DRIVE is a dual-model architecture, where two models with identical initial weights operate in parallel throughout the adaptation process. One model is exposed to perturbations generated through projection gradient descent (PGD) [19], guided by mutual information. These perturbations target high-uncertainty regions of the target domain, allowing the model to handle the domain's variability more effectively. This dual-model setup takes advantage of both consistency-encouraging convergence toward stable, transferable features-and divergence-fostering the exploration of diverse characteristics of the target domain. This ensures that the models are not only robust but also capable of capturing the full spectrum of target domain variability.\nThe adaptation process in DRIVE occurs in two stages. In the first stage, we apply a mutual information consistency loss to align the two models on stable, transferable features despite the presence of perturbations. This phase minimizes the impact of noise, ensuring that the adaptation process is based on reliable features that generalize well across domains. Crucially, the loss function in the first phase also influences the initialization of PGD perturbations in the second phase. By grounding the perturbations in robust, consistent features, the second phase can more effectively explore the target domain's variability, leading to more effective adaptation. This interdependence between the stages ensures that the exploration of the target domain in the second phase is built upon a stable foundation, leading to more effective adaptation and preventing overfitting to noisy data.\nIn the second stage, we introduce a mutual information divergence loss, which encourages the models to explore complementary aspects of the target data. This phase mitigates the risk of overfitting and enables the models to learn complementary information, enhancing the overall robustness and adaptability of the system. The interplay between the two phases ensures that the first stage's stable learning process guides the more exploratory second phase, leading to a seamless and effective adaptation trajectory.\nOur approach offers several key contributions to the SFUDA landscape:"}, {"title": "Related Work", "content": "Source-Free Unsupervised Domain Adaptation. SFUDA seeks to adapt models to a new, unlabeled target domain without access to the original source data, often restricted due to privacy or storage constraints. Recent SFUDA methods have emphasized self-supervised learning, leveraging pseudo-labeling and entropy minimization to infer target domain structures [20, 21, 22]. Consistency regularization techniques promote model stability across augmented target data, effectively reducing uncertainties in the target domain [23, 24, 25, 16]. Additionally, approaches such as CPGA [26] and BAIT [27] employ contrastive learning frameworks to align samples with category-wise prototypes. NRC [28]"}, {"title": "Under Review", "content": "and LSC-SDA [29] propagate categorical semantics through neighborhood or cluster structures in the feature space. Xia et al. [30] focus on the disagreements between target data and the source model. Litrico et al. [22] leverage a loss reweighting strategy that brings robustness against the noise that inevitably affects the pseudo-labels. DIFO [17] explores the integration of heterogeneous knowledge sources from vision-language models.\nDespite these advancements, most SFUDA methods rely on single-model frameworks that are susceptible to noise and uncertainty in target data, especially when domain gaps are substantial. Our proposed method addresses this limitation by introducing a dual-model structure, incorporating both consistency and divergence regularizations. This approach leverages mutual information to enhance robustness across diverse target conditions.\nMutual Information. Mutual information (MI) has emerged as a valuable tool in domain adaptation, enabling models to capture and retain essential information across domains without requiring direct alignment data. MI-based alignment techniques aim to maximize shared information between model representations and target domain features, thereby enhancing adaptation robustness and the quality of extracted features. Approaches such as those by Peng et al. [31] and Singha et al. [32] focus on MI-based metrics to strengthen domain-invariant feature extraction. In vision- language models (VLMs), MI-based techniques facilitate the alignment of cross-modal information by maximizing dependency between visual and language representations, ensuring robust cross-modal grounding. For instance, Ma et al. [33] introduce mutual information contrastive learning to align vision-language representations, grounding language specifications and rewarding learning from action-free videos with text annotations. Kim et al. [34] propose negative Gaussian cross-mutual information, coined as Mutual Information Divergence, using CLIP features as a unified metric. DIFO [17] customize VLMs by maximizing the mutual information with the target model in a prompt learning manner.\nBeyond facilitating domain adaptation and cross-modal semantic alignment, MI also enables model-specific perturba- tions that enhance adaptability and responsiveness to target-specific uncertainties. Our approach leverages MI both as a consistency mechanism in the initial phase and as a divergence mechanism in the later stage, enabling the framework to harness both alignment and diversity during model learning. This dual-phase application of MI ensures robust feature alignment while promoting model flexibility across diverse target conditions."}, {"title": "Method", "content": ""}, {"title": "3.1 Problem Statement and Overview", "content": "Problem Statement. This work addresses the Source-Free Unsupervised Domain Adaptation (SFUDA) problem, where the objective is to adapt a pre-trained source model \\( \\theta_s \\) to an unlabeled target domain \\( X_t \\) without direct access to the labeled source data \\( X_s \\), often due to privacy or storage constraints. Formally, we consider two domains that share C common classes: a labeled source domain \\( X_s \\), and an unlabeled target domain \\( X_t \\) comprising n target samples \\( {x_i}_{i=1}^n \\), with unknown true labels \\( V_t = {y_i}_{i=1}^n \\) in the target domain. The goal is to adapt \\( \\theta_s \\) to a target model \\( \\theta_t : X_t \\rightarrow Y_t \\) using only \\( X_t \\) and \\( \\theta_s \\).\nOverview of DRIVE Framework. The DRIVE framework employs a dual-model, two-stage approach with perturbation-induced robustness. It leverages Projected Gradient Descent (PGD) noise perturbations, where the loss computed in Stage 1 influences the initialization of the perturbation in Stage 2. Both stages utilize an entropy-based pseudo-labeling strategy. Details of the two stages are outlined below."}, {"title": "3.2 Stage 1: Task-Specific ViL Model Customization with Dual-Model Consistency", "content": "In the first stage, DRIVE employs prompt learning to customize the visual-language (ViL) model \\( \\theta \\) by aligning its predictions with those of the frozen target model \\( \\theta_t \\) initialized with \\( \\theta_s \\). This alignment is achieved through a dual-model approach, where Model 1 (\\( \\theta_{v,1}, \\theta_{t,1} \\)) processes clean target samples, while Model 2 (\\( \\theta_{v,2}, \\theta_{t,2} \\)) is perturbed using Projected Gradient Descent (PGD).\nA PGD perturbation is defined as an adversarial adjustment to \u00e6, designed to maximize a loss function \\( L(\\theta_t(x), y) \\) with respect to a while keeping the perturbed sample within an e-ball around the original input \u00e6. Formally, a PGD perturbation is given by:\n\\[ x^{PGD} = Proj_{\\mathcal{B}(x,\\epsilon)} (x + \\alpha \\nabla_{x}L(\\theta_t(x), y)), \\]\nwhere \\( Proj_{\\mathcal{B}(x,\\epsilon)} \\) denotes projection onto the e-ball around \u00e6, and a is a step size.\nThe perturbations in Model 2, guided by mutual information, explore meaningful variations in the target domain, while the weights of Models 1 and 2 are shared. Importantly, \\( \\theta_{t,1} \\) and \\( \\theta_{t,2} \\) remain frozen throughout Stage 1."}, {"title": "Under Review", "content": "Entropy-Aware Predictor. At this stage, pseudo-labels for the task are generated by combining the categorical distribution outputs of \\( \\theta_{t,1}(x_i) \\) and \\( \\theta_{v,1}(x_i, v) \\) with the learnable prompt context v, weighted by their respective entropies. The pseudo-label for Stage 1 is defined as follows:\n\\[ P_{s,1}^{(1)}(x_i) = \\frac{S_{v,1}(x_i)}{S_{v,1}(x_i) + S_{t,1}(x_i) + \\lambda} \\cdot \\theta_{t,1}(x_i) + \\frac{S_{t,1}(x_i) + \\lambda}{S_{v,1}(x_i) + S_{t,1}(x_i) + \\lambda} \\cdot \\theta_{v, 1}(x_i, v), \\]\nHere, \\( S_{v,1}(x_i) \\) and \\( S_{t,1}(x_i) \\) represent the entropy of the categorical distribution output from \\( \\theta_{v,1}(x_i, v) \\) and \\( \\theta_{t,1}(x_i) \\), respectively. \\( \\lambda \\) is a prior bias used to adjust the model's confidence in the output of the ViL model.\nTask-Specific ViL Loss. To align the predictions of \\( \\theta_{v,1}(x_i) \\) with \\( P_{s,1}^{(1)}(x_i) \\), we apply a mutual information-based consistency loss. The loss is formulated as:\n\\[ L_{TSV}^{(1)} = \\min_v [ - \\mathbb{E}_{x_i \\in X_t} I(\\theta_{v,1}(x_i, v), P_{s,1}^{(1)}(x_i))], \\]\nwhere \\( I(\\cdot, \\cdot) \\) denotes the mutual information between the two predictions.\nMutual Information Consistency Loss for Stage 1. In this step, we apply a mutual information-based consistency loss to align predictions from both the clean and perturbed models. Let the predictions from Model 2 be denoted by \\( \\theta_{v,2}(x_i + \\delta^{(1)}, v) \\) for a target sample \\( x_i \\). The mutual information-based consistency loss is given by:\n\\[ L_{CMIC}^{(1)} = \\min_v - [\\mathbb{E}_{x_i \\in X_t} I(\\theta_{t,1}(x_i), \\theta_{t,2}(x_i + \\delta^{(1)}))], \\]\nThis loss encourages Model 1 to generalize through the learnable text prompt context v in a way that remains consistent with the perturbed Model 2, thus aligning both models' outputs under varying target domain conditions.\nThe optimization process follows the PGD methodology [19], where iterative updates to the first stage perturbation \\( \\delta^{(1)} \\) are performed. At the p-th iteration, the current perturbation \\( \\delta_p^{(1)} \\) is updated as follows:\n\\[ \\delta_p^{(1)} = \\delta_{p-1}^{(1)} + \\frac{\\gamma_p}{\\left|A_{p-1}\\right|} \\sum_{x \\in A_{p-1}} \\nabla_{x} [- \\mathbb{E}_{x_i \\in X_t} I(\\theta_{v,1}(x_i, v), P_{s,1}^{(1)}(x_i))], \\]\nwhere \\( \\delta^{*(1)} = arg \\min_{\\left|\\delta^{(1)}\\right| < R} \\left|\\delta^{(1)} - \\delta_0^{(1)}\\right| \\) and \\( A_{p-1} \\) denotes a batch of samples, \\( \\gamma_p \\) is the step size parameter for PGD, and R is the norm bound for the perturbation. Specifically, the initialization of \\( \\delta_0^{(1)} - \\delta^{(1)} = Random(x_j) \\) is a random input from this batch of samples \\( A_{p-1} \\). Once \\( \\delta_p^{(1)} \\) is obtained after P iterations, we update v to \\( v^* \\) using batched gradients."}, {"title": "3.3 Stage 2: Knowledge Adaptation with Perturbed Model Encouragement", "content": "In this stage, we extend the dual-model approach from Stage 1 to construct PGD perturbations, driving the target model \\( \\theta_{t,1} \\) to perform more extensive exploration of the target domain based on its overall consistency loss from Stage 1.\nDynamic Perturbation Adjustment for Enhanced Exploration. To ensure that the model performs extensive exploration in the target domain while maintaining prediction consistency for familiar samples, DRIVE introduces a dynamic perturbation adjustment mechanism in Stage 2. This mechanism dynamically adjusts the initialization noise magnitude of the PGD perturbations based on the consistency losses from Stage 1. The perturbation noise magnitude \\( \\eta \\) for Stage 2 is defined as:\n\\[ \\eta \\propto L_{TSV}^{(1)} + \\beta L_{MIC}^{(1)} \\]\nThe optimization process for these perturbations follows the PGD methodology [19]. At the p-th iteration, the current perturbation \\( \\delta^{*(2)} \\) is updated similarly to the process described earlier, but with specific adjustments for the iteration and initialization. Specifically, we compute \\( P_{s,1}^{(2)}(x_i) \\) as the pseudo-label for Stage 2 computed based on Eq. 2 and the target-domain-customized context prompt embedding \\( v^* \\). In addition, the initialization of \\( \\delta_0^{(2)} - \\delta^{(2)} = \\eta \\cdot Random(x_j) \\) is a random input from this batch of samples \\( A_{p-1} \\), scaled by \\( \\eta \\)."}, {"title": "Under Review", "content": "when the target model achieves high consistency for a target domain sample \\( x_i \\) in Stage 1, it indicates that the model has learned the label correspondence between this sample and the source domain samples. Choosing a smaller initialization noise helps retain this learned stable mapping in subsequent training. Conversely, if the target model's predictions for the same target domain sample \\( x_i \\) are inconsistent with the existing ViL prior knowledge and the predictions after PGD perturbations in Stage 1, it suggests that the distribution space represented by this sample remains highly uncertain for the current target model. Increasing the magnitude of the initialization noise aids the model in performing more extensive exploration within the neighborhood of this sample.\nMutual Information Consistency Loss for Stage 2. As in Stage 1, to ensure that the perturbed and unperturbed models align under a wide range of perturbations, we use mutual information-based consistency losses. The mutual information consistency loss for Stage 2 is formulated as:\n\\[ L_{MIC}^{(2)} = \\min_{\\theta_{t,1}} - [\\mathbb{E}_{x \\in X_t} I(\\theta_{t,2}(x_i), \\theta_{t,2}(x_i + \\delta^{(2)}))], \\]\nThis loss encourages Model 1 to generalize in a manner that remains consistent with the perturbed Model 2, aligning both models' outputs under varying target domain conditions.\nPredictive Consistency and Category Attention Calibration. To ensure knowledge adaptation and improve model performance, we incorporate two key components following the DIFO method: predictive consistency loss and category attention calibration.\nFirst, the predictive consistency loss ensures that the target model's predictions remain consistent with those of the ViL model. This loss is defined as:\n\\[ L_{PC} = \\min_{\\theta_{t,1}} [- \\mathbb{E}_{x_i \\in X_t} I(\\theta_{t,1}(x_i), \\theta_{v,1}(x_i, v^*)) + \\alpha L_B] \\]\nwhere the category balance term \\( L_B = KL(\\theta_{v,1}(x_i)||\\mathbb{I}) \\) ensures the predicted label distribution matches the uniform distribution \\( \\frac{1}{C} \\).\nSecond, we employ category attention calibration to regularize the model's predictions using pseudo-labels. Specifically, we identify the top-N most probable categories using \\( P_{s,1}(x_i) \\). The indices of these categories are denoted by \\( M_i = {m_k}_{k=1}^N \\). The regularization loss is defined as:\n\\[ L_{MCE} = \\min_{\\theta_{\\tau}} \\mathbb{E}_{x \\in X_t} log ( \\frac{exp (a_i/\\tau)}{\\sum_{j \\in M} exp (b_{i,j}/\\tau)} ) \\]\n\\[ a_i = l_{i,m_k}, b_i = l_{i,m_k} \\]\nwhere \\( l_{i,j} \\) denotes the j-th element of the logit vector l and \\( \\tau \\) is the temperature parameter.\nTogether, these mechanisms ensure that the target model not only adapts to the target domain but also explores new and challenging examples, thereby improving its generalization and robustness."}, {"title": "3.4 Training Procedure", "content": "The training process for DRIVE iterates between Stage 1 (ViL model customization) and Stage 2 (knowledge adaptation), progressively adapting \\( \\theta_{t,1} \\) and \\( \\theta_{t,2} \\) by leveraging both clean and perturbed inputs. In each epoch, Stage 1 optimizes prompt v with \\( L_{total}^{(1)} = L_{TSV}^{(1)} + \\beta L_{MIC}^{(1)} \\), while Stage 2 adapts \\( \\theta_{t,1} \\) using:\n\\[ L_{total}^{(2)} = L_{MCE} + \\xi_1 L_{PC} + \\xi_2 L_{MIC}^{(2)} \\]\nHere, \\( \\xi_1 \\) and \\( \\xi_2 \\) are hyperparameters that control the weights of the respective loss components, \\( L_{pc} \\) and \\( L_{MIC}^{(2)} \\). The details of this procedure are summarized in Algorithm 1."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets: We evaluate our proposed framework, DRIVE, on four benchmark datasets for domain adaptation: Office-31 [35], Office-Home [36], and DomainNet-126 [31]. These datasets offer varying levels of complexity, from small-scale"}, {"title": "Under Review", "content": "(Office-31) to large-scale (Ofiice-Home, DomainNet-126), ensuring a comprehensive assessment across different domain shifts and challenges. Dataset details are provided in the Supplementary Materials.\nBaselines: We compare DRIVE with leading SFUDA methods across three categories:(i) Source model: Baselines include Source (the source-only model). (ii) Multimodal UDA Methods: We include DAPL [23], PADCLIP [25], ADCLIP [32], and DIFO [17], which leverage multimodal models for domain adaptation. (iii) SFUDA Methods: Key SFUDA methods include SHOT [20], NRC [21], and TPDS [37], which address domain shifts without requiring source data during adaptation.\nImplementation Details: WWe present DRIVE-C-B32, which utilizes the ViT-B/32 backbone, and is designed to be broadly similar to DIFO-C-B2 (also using ViT-B/32). Experiments follow the same settings as previous baselines for fair comparison, with further details in the Supplementary Materials."}, {"title": "4.2 Results on Closed-set SFUDA", "content": "On the Office-31 dataset(Table 1), DRIVE achieves a mean accuracy of 92.7%, significantly outperforming state-of-the- art methods. Specifically, in the challenging tasks such as A \u2192 D (97.4%) and A \u2192 W (95.7%), DRIVE demonstrates robustness and adaptability. The entropy-aware pseudo-labeling strategy ensures that the model focuses on reliable regions, reducing overfitting to noisy data. This is particularly important in tasks where the target domain has high variability and noise. For the Office-Home dataset(Table 2), DRIVE achieves a mean accuracy of 83.6%. Notable improvements are observed in tasks such as Ar \u2192 Cl (72.4%) and Cl \u2192 Pr (90.7%), which are known for their high uncertainty. The dynamic perturbation adjustment mechanism enhances the exploration of high-uncertainty regions, leading to better feature alignment and generalization. This is crucial for tasks where the target domain has significant variations, and the model needs to adapt effectively to these changes.\nOn the DomainNet-126 dataset(Table 5), DRIVE achieves a mean accuracy of 80.6%, outperforming other methods. In tasks such as C \u2192 R (88.1%) and R \u2192 C (81.0%), DRIVE shows strong performance, indicating its ability to handle large domain shifts. The dual-model architecture and mutual information-driven consistency loss ensure stable feature alignment, even in complex target domains. This robust performance across multiple datasets and tasks highlights the effectiveness of DRIVE in addressing the challenges of closed-set SFUDA."}, {"title": "Under Review", "content": "These results substantiate DRIVE's capability to enhance cross-domain performance in closed-set SFUDA settings, benefiting from its dual-model architecture and mutual information-driven consistency mechanism."}, {"title": "4.3 Ablation Study", "content": "To thoroughly evaluate the contributions of each component in DRIVE, we conducted an ablation study focusing on the following aspects: Entropy-Aware Predictor, Perturbed Model Encouragement, and Dynamic Perturbation Adjustment.\nEntropy-Aware Predictor. The Entropy-Aware Predictor is designed to adjust label weights based on prediction uncertainty, ensuring the model focuses on reliable data while avoiding noisy regions. As shown in Table 4, enabling the Entropy-Aware Predictor alone (second row) improves performance on the A \u2192 D and D \u2192 W tasks, indicating its effectiveness in enhancing the model's robustness.\nPerturbed Model Encouragement. The Perturbed Model Encouragement mechanism exposes one of the models to perturbations via projection gradient descent (PGD) guided by mutual information, focusing on high-uncertainty regions. When combined with the Entropy-Aware Predictor (third row), there is a further improvement in performance, particularly on the A \u2192 D and D \u2192 W tasks, demonstrating the importance of exploring the feature space more comprehensively.\nDynamic Perturbation Adjustment. The Dynamic Perturbation Adjustment mechanism dynamically adjusts the perturbation level based on the loss from the first stage, encouraging the model to explore a broader range of the target domain while preserving existing performance. Adding this component (fourth row) maintains the performance gains observed in the third row, indicating that dynamic adjustment is beneficial for maintaining robustness and generalization."}, {"title": "4.4 Grad-CAM Visualization of SFUDA", "content": "To gain deeper insights into how DRIVE and competing models attend to domain-relevant features, we conducted Grad-CAM analysis on selected samples from the target domain, which shown on 2. By comparing the attention maps generated by DRIVE against those produced by competitive baseline(DIFO), we can assess the efficacy of our method in focusing on meaningful and domain-invariant features. The Grad-CAM visualizations thus provide empirical evidence supporting our claim that DRIVE not only achieves quantitative performance gains but also qualitatively attends to more meaningful and transferable features within the target domain. This characteristic is critical for achieving robust and reliable domain adaptation, especially in real-world applications where domain shifts are common and unpredictable."}, {"title": "5 Conclusion", "content": "Source-Free Unsupervised Domain Adaptation (SFUDA), involves adapting a pre-trained model to a target domain using only unlabeled target data, leading to issues such as overfitting, underfitting, and poor generalization due to domain discrepancies and noise. Existing SFUDA methods often struggle with these challenges due to their reliance on single-model architectures. To address these difficulties, we introduced DRIVE (Dual-Robustness through Information Variability and Entropy), a novel SFUDA framework leveraging a dual-model architecture. DRIVE captures diverse characteristics of the target domain by using two models initialized with identical weights, one of which is exposed to perturbations via projection gradient descent (PGD) guided by mutual information, focusing on high-uncertainty regions. Additionally, we introduced an entropy-aware pseudo-labeling strategy that adjusts label weights based on prediction uncertainty, ensuring the model focuses on reliable data while avoiding noisy regions. The adaptation process consists"}, {"title": "Under Review", "content": "of two stages: the first aligns the models on stable features using a mutual information consistency loss, and the second dynamically adjusts the perturbation level based on the loss from the first stage, encouraging the model to explore a broader range of the target domain while preserving existing performance. This enhances generalization capabilities and robustness against interference. Evaluations on standard SFUDA benchmarks show that DRIVE consistently outperforms previous methods, delivering improved adaptation accuracy and stability across complex target domains."}, {"title": "A Heoretical Analysis and Proofs", "content": "In this appendix, we provide detailed proofs and derivations that support the methodological choices in the DRIVE framework, specifically regarding the choice of mutual information (MI) as an alignment measure over Kullback-Leibler (KL) divergence and the use of projected gradient descent (PGD) perturbations for robustness. The following sections formalize the lower bound properties of MI in contrast to KL divergence, as well as the theoretical grounding for PGD-based perturbations in model robustness."}, {"title": "A.1 Mutual Information as a Lower Bound", "content": "In the DRIVE framework, we employ mutual information (MI) to align the predictions of the target model Ot with those of the vision-language (ViL) model \u03b8\u03c5. Mutual information is advantageous due to its symmetric and lower-bounding properties, as it captures the shared information between two distributions without biasing toward one. Here, we rigorously establish the relationship between MI and KL divergence, demonstrating MI's role as a lower bound."}, {"title": "A.1.1 Proposition: Mutual Information as a Lower Bound on KL Divergence", "content": "Given two random variables X and Y with a joint distribution p(X, Y) and marginal distributions p(X) and p(Y), the mutual information I(X; Y) between X and Y is defined by:\n\\[ I(X; Y) = \\mathbb{E}_{(X,Y)~p(X,Y)} [\\frac{p(X,Y)}{p(X)p(Y)}] \\]\nEquivalently, MI can be expressed in terms of KL divergence as:\n\\[ I(X; Y) = D_{KL}(p(X, Y) || p(X)p(Y)). \\]\nSince KL divergence is non-negative, we have:\n\\[ I(X; Y) = D_{KL}(p(X, Y) || p(X)p(Y)). \\]\nThus, mutual information satisfies the inequality:\n\\[ -I(X; Y) \\leq 0. \\]"}, {"title": "Under Review", "content": "This inequality shows that mutual information, by virtue of its non-negativity, serves as a lower bound, unlike KL divergence, which biases toward a single reference distribution."}, {"title": "A.1.2 Corollary: Implications for DRIVE", "content": "In the DRIVE framework, where neither \u03b8t nor \u03b8\u2082 has a clear superiority in the unlabeled target domain, MI provides a balanced alignment metric that preserves information about both distributions without bias. This feature is critical in ensuring stable, symmetric adaptation in SFUDA settings, where neither distribution is privileged. Empirical evidence supporting this choice can be found in Section 4, which shows improved performance under MI alignment compared to KL divergence."}, {"title": "A.2 Projected Gradient Descent (PGD) Perturbations and Robustness", "content": "The DRIVE framework leverages PGD-based perturbations to enhance robustness by encouraging the model to focus on high-uncertainty regions in the target domain. This section formalizes the theoretical basis for PGD perturbations as a mechanism for improving robustness, particularly in the presence of domain shifts."}, {"title": "A.2.1 Definition: Projected Gradient Descent (PGD) Perturbation", "content": "Let x \u2208 Xt be an input sample in the target domain. A PGD perturbation is defined as an adversarial adjustment to x, designed to maximize a loss function L(0t(x), y) with respect to a while keeping the perturbed sample within an e-ball around the original input \u00e6. Formally, a PGD perturbation is given by:\n\\[ x^{PGD} = Proj_{\\mathcal{B}(x,\\epsilon)} (x + \\alpha \\nabla_{x}L(\\theta_t(x), y)), \\]\nwhere Proj\u00df(2,\u20ac) denotes projection onto the e-ball around \u00e6, and a is a step size."}, {"title": "Under Review", "content": ""}, {"title": "A.2.2 Theorem: Robustness Improvement via PGD Perturbations", "content": "Under a PGD perturbation, the model Ot is encouraged to learn representations that are invariant to small adversarial changes in x. This invariance can be formalized as robustness to adversarial noise, with the following bound.\nLet ot be a model with parameters w optimized to minimize the expected loss L(0\u2081(x), y) over a distribution p(x). Then, for a small perturbation d\u00e6 within an e-ball, the change in loss is bounded by:\n\\[ |L(\\theta_+(x + dx), y) \u2013 L(\\theta_t(x), y)| \\leq \\epsilon ||\\nabla_xL(\\theta_t(x), y) ||. \\]\nThis result implies that, by minimizing the gradient magnitude ||\u2207xL(0t(x), y)|| through PGD, the model becomes more robust to small perturbations, which is critical for handling noisy or variable target data in SFUDA."}, {"title": "A.2.3 Corollary: Application to DRIVE", "content": "In DRIVE, we apply PGD perturbations to one of the dual models, exposing it to high-uncertainty regions in the target domain. This perturbation-based training encourages the model to learn stable, generalizable representations that are resilient to domain variability, improving adaptation performance on unseen target data."}, {"title": "B Evaluation Datasets", "content": "We evaluate our methods on four standard benchmarks:\n\u2022 Office-31: This is a small-scale dataset comprising three domains: Amazon (A), Webcam (W), and Dslr (D). These domains capture real-world objects in various office settings, totaling 4,652 images across 31 categories. Images in (A) are sourced from online e-commerce platforms, while (W) and (D) feature low-resolution and"}]}