{"title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers", "authors": ["SALVATORE GRECO", "KE ZHOU", "LICIA CAPRA", "TANIA CERQUITELLI", "DANIELE QUERCIA"], "abstract": "Al regulations are expected to prohibit machine learning models from using sensitive attributes during training. However, the latest Natural Language Processing (NLP) classifiers, which rely on deep learning, operate as black-box systems, complicating the detection and remediation of such misuse. Traditional bias mitigation methods in NLP aim for comparable performance across different groups based on attributes like gender or race but fail to address the underlying issue of reliance on protected attributes. To partly fix that, we introduce NLPGUARD, a framework for mitigating the reliance on protected attributes in NLP classifiers. NLPGuard takes an unlabeled dataset, an existing NLP classifier, and its training data as in-put, producing a modified training dataset that significantly reduces dependence on protected attributes without compromising accuracy. NLPGUARD is applied to three classification tasks: identifying toxic lan-guage, sentiment analysis, and occupation classification. Our evaluation shows that current NLP classifiers heavily depend on protected attributes, with up to 23% of the most predictive words associated with these at-tributes. However, NLPGUARD effectively reduces this reliance by up to 79%, while slightly improving accuracy.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, the adoption of deep learning-based NLP models has exponentially increased. Transformer-based models, such as BERT [18], T5 [57], and GPT [8], have achieved unthinkable levels of performance on several natural language tasks. However, despite being increasingly accurate, these models remain black-boxes [28]. For an NLP classification task, models predict a class label from an input text without providing any information on the complex internal decision-making mechanism, making it challenging to identify and mitigate potential bias and/or unfair behavior in such models.\nUpcoming privacy laws regulating the use of AI will soon demand that learning shall not be done on protected attributes such as race, gender, or sexual orientation, as already identified by the General Data Protection Regulation (GDPR), the UK Government, and the anti-discrimination legislation in the United States [10, 21, 26]. Ensuring AI models avoid using protected attributes in decision-making is termed \u2018fairness through unawareness' [45], and it is crucial in many real-world scenarios. For instance, NLP-based systems often assess job applicants' resumes. Following the Civil Rights Act in the US, discrimination based on race, sex, nationality, or other protected attributes is forbidden. Hence, these NLP systems must omit words linked to protected attributes to prevent discriminatory practices against candidates, such as the \u201csexist\u201d Amazon Recruitment tool,\u00b9 a system that learned to downgrade resumes containing the word 'women'. Content moderation is another example, where all users should be treated equitably, without having their contributions censored or suppressed because of, for example, their demographic characteristics.\nHowever, as we will demonstrate in our analysis, state-of-the-art models often base their pre-dictions on protected attributes, and accurate ones are frequently black boxes, posing challenges in identifying such misuse. Consider, for example, the task of determining whether a sentence contains toxic language or not in a dataset we will analyze. In Figure 1, we report four example sentences, together with the outcome of a toxicity classifier P(T); in Figure 2, we highlight in red the important words used by the classifier to make these predictions. As shown, the presence of words such as \u2018black', 'gay', or 'homosexual' is used to distinguish between toxic or non-toxic texts. Yet, these words are protected attributes and should not be used in such classifications at all.\nAs discussed in \u00a72, prior studies on bias in NLP primarily focused on two challenges: ensuring fair performance across different groups and rectifying unfairness in word representations. However, these solutions only target specific biases and fail to eliminate the reliance of models on protected attributes for predictions. Therefore, we propose methods to reduce this bias in black-box NLP clas-sifiers, removing most protected attributes from their decision-making process while maintaining accuracy, and making these approaches applicable across various datasets and tasks.\nIn so doing, we make four main contributions:\n(1) We introduce NLPGUARD (\u00a73), a framework with three components: (1) an Explainer that finds the most important words for predictions; (2) an Identifier that checks if these words are about protected attributes; and (3) a Moderator that adjusts the training data to re-train the NLP model to reduce learning from such protected attributes.\n(2) We evaluate each part of our framework and use it to mitigate toxicity detection in Wikipedia comments with BERT (\u00a74). BERT depends on protected attributes for toxicity predictions (23% of the most predictive words), but our approach cuts this down by 60% and even increases prediction accuracy by 0.8%."}, {"title": "RELATED WORK", "content": "The growth of AI systems has raised privacy and discrimination concerns, leading to the introduction of numerous regulations and laws governing their use. In the European Union (EU), in May 2018, the GDPR [70] was introduced, which demands organizations ensure that personal data is processed lawfully, fairly, and transparently. It prohibits processing sensitive personal attributes such as race, ethnicity, religion, and political opinions, unless legitimately justified. The EU proposed the AI Act [42, 71], which defines rules and obligations depending on the level of risk of AI systems (e.g., transparency, documentation, human oversight) [51]. In the United Kingdom (UK), the UK Equality Act 2010 [35] established that it is unlawful to discriminate based on nine protected characteristics: age, disability, gender reassignment, marriage and civil partnership, pregnancy and maternity, race, religion or belief, sex, and sexual orientation. Compliance with the act is enforced by the Equality and Human Rights Commission (EHRC). In the United States (US), the Anti-discrimination Act [12] safeguards individuals from unfair treatment based on protected attributes. In late 2022, a blueprint of the AI Bill of Rights was passed [32], declaring that algorithms that discriminate or perform unjustified different treatment based on protected attributes violate legal protections. AI regulations will continue to evolve in the coming years [34, 49], driven by a common goal: to minimize discriminatory outputs based on protected characteristics [10, 21, 26].\nBias in NLP decision-making has manifested itself in several ways, including dialogue genera-tion [19], text classification [20], and machine translation [66]. It usually arises from training data [22, 67]. For instance, pre-trained models and word embedding can inherit biases and stereotypes present in the large training corpora [7, 9, 25, 58]. When quantifying bias, existing works generally highlight disparities between demographic groups, with differences in performance or selection bias on protected attributes such as race, gender, religion, and sexual orientation [22, 29, 37, 67].\nTo address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline [38, 75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics). Most existing works focus on the first two stages, exploiting data augmentation and modified model training techniques [4, 6, 20, 52, 58, 64, 77]. Furthermore, most of those studies focus on one protected category at a time. For example, Badjatiya et al. [6] proposed the identification of protected attributes, such as gender, by creating a manual list of words, measuring the skewed occurrence of words across classes or predicted class probability distribution of words. Park et al. [52] introduced gender swapping to equalize the number of male and female entities in the training data. Dixon et al. [20] proposed dataset augmentation strategies that generate new sentences using templates or replace protected attributes with generic tags, such as part-of-speech or named-entity tags. Zhang et al. [77] proposed mitigating biases in the training data by assuming"}, {"title": "OUR MITIGATION FRAMEWORK", "content": "Our Mitigation Framework, namely NLPGUARD, has been designed to be generally applicable to any supervised machine learning-based NLP classification model applied on an unlabelled corpus. As illustrated in Figure 3, NLPGUARD takes in input an unlabeled corpus and a pre-trained NLP classifier (together with its training dataset) to produce a mitigated training dataset in output. Ground truth class labels for the unlabelled corpus are not required; rather, the classifier is used to generate them, both for in-distribution (i.e., data that comes from the same distribution as the original training dataset) and for out-of-distribution data where labels are unavailable. Because of the black-box nature of NLP classifiers based on deep learning models, labels might be predicted using protected attributes. To mitigate that, our framework comprises the following three components:\nA. Explainer. This component uses Explainable Artificial Intelligence (XAI) techniques to identify the most important words used by the model for its predictions. The XAI field has made great strides in making black-box models more transparent, and several techniques exist to explain NLP classifiers [14]. The best one for our purpose should have two qualities: (1) quantify the importance of each feature word (feature-based), and (2) be applicable to explain the model's predictions after training (post-hoc). Many techniques meet these requirements, and most of them measure the importance of each word for the prediction within an individual sentence (local-explanations) [2, 44, 59, 63, 69, 73]. Our Explainer component first identifies the words important for prediction within all individual sentences, exploiting any of those techniques. Each word is, as such, associated with multiple scores, one for each occurrence in each sentence. Second, it determines the most important predictive words for the model as a whole (global-explanations) following the idea of some of these techniques, which aggregate the words' importance over many sentences to compute the overall importance [72, 73]. Specifically, for each word, it sums all their individual scores and divides them by their frequency to compute the word's classification score. The normalization step is required to also identify rare but important words. The output of the Explainer component is the ordered list of the most important words for the model's predictions.\nB. Identifier. This component aims to annotate which of the previously detected important words refer to protected attributes. We consider the nine protected categories defined by the Equality Act: age, disability, gender reassignment, marriage and civil partnership, pregnancy and maternity, race, religion or belief, sex, and sexual orientation. Our framework allows for annotation using human-in-the-loop (B1) and machine-in-the-loop (B2) approaches.\nB1. Human-in-the-loop Annotation. Crowdsourcing platforms, such as Amazon Mechanical Turk (MTurk) and Prolific [13], have been extensively used by the research community to recruit crowdworkers for data labeling purposes. Crowdworkers [60, 68] are anonymous people usually paid for completing simple tasks. This component leverages crowdsourcing to perform the protected attribute annotation of the most important words. In our work, we exploited MTurk, where for each important word {WORD}, participants were asked to answer the following question:\nB2. Machine-in-the-loop Annotation. As a cost-effective and scalable alternative to human-in-the-loop annotations via crowdsourcing, we also implemented a protected attributes annotation process that uses Large Language Models (LLMs). We did so inspired by a recent study [23] that found LLMs, including ChatGPT, to outperform crowdworkers in text-based annotation tasks. It also has been shown that LLMs are effective in solving many NLP tasks [56]. Specifically, we implemented an annotation process that interacts with ChatGPT as follows: two prompts provide the protected categories, their definitions, and links with additional information. Then, for each word, the LLM is asked to: (1) classify the word into one of the protected categories or none of them;\nC. Moderator. This component produces a new mitigated training dataset that can be used to train a new classifier that uses fewer protected attributes previously identified. It takes the original pre-trained classifier, the original training dataset, and the list of the most important words enriched with the protected attribute label as input. It produces a new mitigated training dataset by adjusting the training dataset based on the identified protected attributes that can then be used to train a new mitigated classifier. We designed and tested five mitigation strategies (MS).\n(MS1) Sentence-level removal. Previous works have shown that subsampling can be an easy but effective technique for data balancing [33, 61]. This mitigation strategy eliminates all sentences containing protected attributes from the training set. As a result, it reduces the overall number of training examples. For example, if a word W\u012f is identified as a protected attribute, all sentences in the training set that include that word are removed. The idea behind this strategy is that the imbalance in the number of training examples containing protected attributes for a particular class may have led the model to learn that these protected attributes are crucial for classifying that class.\n(MS2) Word-level removal. This strategy removes only the protected attribute words from the sentences in the training set while preserving the number of examples. The process involves removing the identified protected attribute words from all sentences in the training set, thereby removing their influence on the model's learning process. The idea is that the model should be able to classify sentences without relying solely on the protected words, and rather use other words in the text too0.\n(MS3) Word-level replacement with a random synonym. This strategy replaces every instance of a protected attribute in the training set with one of its synonyms. It first uses embedding similarity techniques to identify the k-nearest neighbors for each protected attribute. Then, it randomly selects one of the k-most similar words to replace each instance of the protected attribute in the training set. This has been shown to mitigate bias in [6], and it is believed that it may also help mitigate the use of protected attributes in classification, as the model may learn to rely on other words for classifying the classes rather than solely relying on protected attributes. This approach maintains the same number of examples in the training set but increases the diversity of words.\n(MS4) Word-level replacement with K random synonyms. This strategy expands the training set by generating new sentences using synonyms of protected attributes. Instead of replacing the protected attribute in-place with one similar word as in MS3, it creates k new sentences by replacing the protected attribute with each of its k-nearest neighbors. For example, given a sentence containing a protected attribute W\u012f, k new sentences are created by replacing W; with each of its k most similar words. This increases the size of the training set and diversifies the words used in the sentences.\n(MS5) Word-level replacement with hypernym. This strategy replaces instances of protected attributes in the training set with higher-level words, called hypernyms, which provide a more general representation of the category to which the protected attribute belongs. For example, the hypernym of 'dog' could be\u2018animal'. By using hypernyms instead of the specific protected attributes, the model may not discriminate based on these attributes in its classifications. This technique has been shown to be effective in mitigating accuracy imbalance between subgroups [6]."}, {"title": "FRAMEWORK EVALUATION: EFFECTIVENESS AND SENSITIVITY", "content": "We evaluate the effectiveness of our framework and the sensitivity of the Explainer (\u00a74.2), Identifier (\u00a74.3), and Moderator (\u00a74.4) components in mitigating a toxicity classifier applied to in-distribution data (i.e., the test set)."}, {"title": "Evaluation task", "content": "We choose toxicity prediction as the main evaluation task in line with previous research. Toxicity classifiers are used in different contexts [39, 76], such as Reddit, Twitter, and 4chan, with competitive performance. However, they suffer from different types of biases [15, 16, 31, 62]. In Wikipedia, for example, any comment containing words associated with insults, offense, or profanity, regardless of the tone, the intent, and the context, would be classified as toxic; toxic language was however more likely predicted from minority communities, as found in [15], thus suggesting the use of protected attributes by such models.\nIn our experiments, we used the \u201coriginal model\u201d in the widely used detoxify [30] library\u00b3 as a pre-trained toxicity classifier. This is a BERT-base and uncased model [18] trained on a dataset of publicly available Wikipedia comments. It was fine-tuned for predicting 6 labels related to toxicity: toxicity, severe toxicity, obscene, threat, insult, and identitiy attack, achieving an average Area Under the ROC Curve (AUC) score of 98.6%. For the toxicity label only, the classifier achieved 0.82 macro and 0.93 weighted F1 scores (the dataset is imbalanced). This classifier is applied to the original test set comprising 153,164 texts, and predicted the toxicity label for 36,148 texts (23.6% of the test set)."}, {"title": "Component evaluation: Explainer", "content": "The Explainer aims to identify the most crucial words utilized by the classifier for predictions (as described in \u00a73-A). The employed XAI technique can influence the words recognized as significant, thereby impacting the identified protected attributes on which the model relies to make predictions."}, {"title": "Evaluation metrics.", "content": "To evaluate the effectiveness of the Explainer component, we first measure the impact on the model's predictive performance (F1 score) by removing the most important words identified by each XAI technique. An effective and precise explainer should result in a noticeable decrease in the predictive performance when these words are removed. Secondly, to assess the sensitivity of the Explainer, we measure the overlap of the most important words identified by different XAI techniques. A substantial overlap indicates consistent outputs across XAI techniques. Lastly, we measure the computation time for generating explanations to assess the efficiency of the Explainer based on the XAI technique, which is a crucial aspect when dealing with large datasets."}, {"title": "Explainer setup.", "content": "There are two main categories of XAI techniques to compute explanations within a sentence: permutation-based and gradient-based [14]. For this comparison, we instantiated the Explainer component with SHapley Additive exPlanations (SHAP) [44] as a representative of the permutation-based and with Integrated Gradients (IG) [69] of the gradient-based techniques. Both techniques have demonstrated competitive performance in prior studies [3]. Specifically, for SHAP, we used the text permutation explainer with 3,000 as the maximum evaluation step parameter. For Integrated Gradients, we exploited the implementation provided by the Ferret [5] library."}, {"title": "Results.", "content": "We produced the explanations within each sentence over the toxic texts in the test set using both techniques (SHAP and IG); we then aggregated individual scores and extracted an ordered list of the most toxic words as previously described in \u00a73-A. Figure 4 shows the decrease in the F1 score by removing the most important words in the range from 50 to 700 with a step of 50. As expected, removing the most important words from the test set causes a marked decrease in predictive performance, especially for the top 250 words. IG exhibits higher precision, leading to a more substantial decrease initially. However, the decrement tends to converge on the top 400 words for both techniques. This shows that both techniques effectively extract the words used by the classifier for its predictions. We selected the top 400 most toxic words (approximately 10%) - removing additional words caused a lower decrease in predictive performance.\nWe then measured the overlap between the 400 most toxic words identified with IG and SHAP. We found that 307 out of the 400 words were identical (77%), indicating a substantial agreement between the two techniques. The 23% of disagreement may be attributable to the varying precision levels inherent in the two methods.\nFinally, we compared the execution times required to generate explanations using both techniques. We observed that IG significantly outperformed SHAP, completing the explanation process more than two orders of magnitude faster. On average, the execution time in seconds to obtain an explanation is approximately 0.2 for IG and 30 for SHAP, using a single Nvidia RTX A6000 GPU.\nIn summary, Integrated Gradients proves to be the most effective technique for the Explainer component, as it exhibits higher precision in identifying the most crucial words and executes significantly faster. As a result, we adopt Integrated Gradients in the Explainer for all subsequent experiments. Nevertheless, the framework allows the use of alternative XAI techniques."}, {"title": "Component evaluation: Identifier", "content": "The Identifier aims to determine which of the most important words are actually protected attributes (\u00a73-B). To evaluate its effectiveness, we compare the protected attributes identified by the component instantiated with human-in-the-loop and machine-in-the-loop approaches against the annotations provided by two expert annotators, who possess a greater depth of knowledge of the definitions of protected categories by AI regulators than participants engaged in the human study. We also add for comparison a pre-defined dictionary of 51 protected attributes from previous works [6, 20]."}, {"title": "Evaluation metrics.", "content": "We measure the Cohen's kappa inter-annotator agreement [11] to evalu-ate the accuracy and reliability of the protected attributes identified by the different approaches. It ranges between [0, 1]; the higher the score is, the higher the agreement."}, {"title": "Protected attributes identifier setup.", "content": "We selected the 400 most toxic words extracted with the Explainer component instantiated with Integrated Gradients as the candidate set to identify protected attributes. Then, we configured the Identifier as follows."}, {"title": "Results.", "content": "The two expert annotators (A1, A2) identified 72/400 (18%) and 66/400 (17%) protected attributes, respectively. The human-in-the-loop (MTurk) approach 108/400 (27%) ones. Instead, the machine-in-the-loop (ChatGPT) approach labeled 93/400 (23%) words as protected attributes. These findings indicate that the original classifier heavily relies on protected attributes for toxicity predictions. Interestingly, the ChatGPT Identifier is also able to annotate proxy words for the"}, {"title": "Component evaluation: Moderator", "content": "The Moderator aims to create a mitigated training corpus to train a new classifier with reduced reliance on protected attributes and similar predictive performance. To evaluate the effectiveness of each mitigation strategy, we trained and evaluated a distinct mitigated model for each strategy."}, {"title": "Evaluation metrics.", "content": "For each mitigation strategy, we examine two key aspects of the mit-igated classifiers: fairness and predictive performance. Our fairness is defined as fairness through unawareness, whereby \u201can algorithm is fair as long as any protected attributes are not explicitly used in the decision-making process\u201d [45]. This is quantified by measuring the number of pro-tected attributes each mitigated model relies on in making predictions based on the Explainer and Identifier components. A lower number indicates a reduced dependence on protected attributes, which signifies progress towards a more fair and unbiased classifier. To evaluate the predictive performance, we measure the F1 score specifically for the toxicity label, providing insight into the model's accuracy in identifying toxic texts. Additionally, we evaluate the Area Under the Curve (AUC) score for all toxicity-related labels. This metric provides an overall measure of the model's performance in identifying various aspects of toxicity. By considering both fairness and predictive performance, we can ascertain the effectiveness of the mitigated models in achieving a balance between reducing reliance on protected attributes and maintaining similar predictive capabilities."}, {"title": "Moderator setup.", "content": "For the mitigation strategies outlined in \u00a73-C, we used the following setup: for the removal-based strategies (MS1, MS2), we removed the sentences or the words if, after the tokenization, the protected attributes are in the list of tokens. In the case of the mitigation strategies based on the k-neighbours (MS3, MS4), we set the value of k to 5, meaning that for each protected attribute, the five closest words were identified. To identify these nearest neighbors, we computed the cosine similarity between each word in the vocabulary and the protected attribute using the 300-dimensional GloVe [55] word embedding, as suggested in [6]. For the hypernyms-based strategy (MS5), we utilized the WordNet lexical database [46] provided in NLTK, as suggested in [6]. We replaced each protected attribute with its first-level hypernym extracted from its synset of synonyms."}, {"title": "Results.", "content": "The last two columns in Table 1 show the percentage and number of the most toxic words labeled as protected attributes for all the mitigated models (M-M). The number of those already present among the protected attributes of the original model (Mo) is indicated in curly brackets. All the mitigation strategies reduced the number of protected attributes the model relied upon. However, the mitigated models trained with removal-based strategies (MS1, MS2) achieved much better results. Only 9% and 10% of their most toxic words were labeled as protected attributes (37 and 40 words out of 400), representing a decrease of 61% from the original model (93 words out of 400). One possible reason for the lower performance of replacement-based mitigation strategies (MS3-MS5) is that they can introduce new protected attributes when replacing words. For a qualitative evaluation of fairness improvement, please refer to Figure 11 and Figure 12 in Appendix C.\nIn Table 1, columns 4 and 5 report the macro and weighted F1 scores on the toxicity label for the original and mitigated models. Column 6 also presents the mean AUC scores across all the toxicity-related labels. The results show that all the mitigated models present similar F1 scores compared to the original model, except for the one trained with MS4, which exhibits a greater decrease. Interestingly, the mitigated models trained on the removal-based mitigation strategies (MS1, MS2) achieve better F1 scores than the original model. The word-removal (MS2) increases the macro and weighted F1 scores by 1.3% and 1.2%. Indeed, we observed that the removal-based mitigation strategies reduced the number of false positives in the toxicity predictions"}, {"title": "FRAMEWORK EVALUATION: GENERALIZABILITY", "content": "We finally evaluate the generalizability of our framework first to toxicity prediction on out-of-distribution data (\u00a75.2), and second on different tasks, i.e., sentiment analysis (\u00a75.3) and occupation classification (\u00a75.4)."}, {"title": "Framework and evaluation settings", "content": "For this evaluation, we instantiated the Explainer with Integrated Gradients, the Identifier with ChatGPT, and the Moderator with the removal-based mitigation strategies. As shown in \u00a74, this turned out to be the optimal framework configuration. We perform a similar evaluation of the mitigated models by measuring their fairness and predictive performance. Fairness is evaluated by quantifying the number of protected attributes each mitigated model relies on (fairness through unawareness). Predictive performance is evaluated using quantitative metrics on the test set. For the toxicity classifier, we measure the F1 score for the toxicity label, which allows us to gauge the model's accuracy in detecting instances of toxicity, and the Area Under the Curve (AUC) score for all toxicity-related labels, providing an overall assessment of the model's performance in identifying different aspects of toxicity. For the sentiment and the occupation classifiers, we solely measure the F1 score, as it provides a comprehensive assessment of the model's accuracy in these tasks."}, {"title": "Mitigating toxicity prediction on out-of-distribution data", "content": "This experiment aims to assess the applicability of our framework in mitigating the toxicity model when applied to out-of-distribution data, specifically company reviews. This is crucial as classifiers are normally applied to datasets from other domains with different word distributions from training."}, {"title": "Company reviews data.", "content": "We collected data from a popular online platform where current and former employees write reviews about companies. Reviewers comment on various aspects such as personal experience with the company or managers, salary information, workplace culture, and typical job interviews. The platform fosters a constructive approach among its users by manually and automatically moderating the content of reviews. However, reviews are published anonymously. On the one hand, this promotes user privacy. On the other hand, it can also cause some users to write public insults and offenses toward companies or people. Specifically, we collected a dataset of 439,163 reviews from U.S.-based companies across all 51 U.S. states written from 2008 to 2020.10 Each review contains a pros part (positive comments in the review) and a cons part (negative comments). We applied the same toxicity classifier introduced in \u00a74.1 to identify toxic company reviews."}, {"title": "Toxicity in company reviews.", "content": "The initial expectation was not to have many toxic reviews in the dataset due to the highly curated nature of the platform. However, if we consider a post to be toxic when at least one of the cons or pros fields contains inappropriate content, we found 1.6% of"}, {"title": "Identify protected attributes in toxicity predictions on company reviews.", "content": "All pros and cons reviews predicted as toxic were analyzed by the Explainer component to extract the most important words used by the model in predicting toxic reviews. Then, we selected the 400 most toxic words extracted, and we annotated those words with GPT-3.5-Turbo. Among the 400 most important words used by the model in predicting toxic reviews, 76 are protected attributes (19%), as shown in the last two columns of the first row in Table 2 (original model M\uff61). We can conclude that the original classifier exhibits a significant reliance on protected attributes for toxicity predictions, even when applied to different out-of-distribution data."}, {"title": "Training the mitigated models.", "content": "We applied the removal-based mitigation strategies (MS1, MS2) to the original Wikipedia comments training dataset based on the protected attributes identified in the toxic company reviews. Table 2 shows the differences in the number of training examples after each strategy in the third column. The original training dataset contained 159,571 examples. MS1 resulted in a decrease of 6k examples, while MS2 did not change it. All mitigated models were fine-tuned for 3 epochs, with a batch size of 16, and Adam as optimizer. To evaluate the mitigated models, all pros and cons reviews were classified by each mitigated model. Then, we applied the Explainer component to extract the most important 400 predictive words used by each mitigated model for the toxicity predictions on company reviews. Finally, we exploited the Identifier to determine if the new important words of the mitigated models were protected attributes."}, {"title": "Results.", "content": "The last two columns in Table 2 show the percentage and number of the most toxic words labeled as protected attributes. The number of those already present among the protected attributes used by the original model (M) is also indicated in curly brackets. The results confirm that removal-based mitigation strategies reduce the number of protected attributes the model relied upon. MS1 and MS2 reduce the percentage of protected attributes from 19% to 4% and 5% (16 and 19 out of 400 words), respectively. This corresponds to a decrease of 79% and 75%. They also reduce the protected attributes the original classifier relies on from 76 to 8 and 11, respectively."}, {"title": "Predictive performance.", "content": "Columns 4 and 5 in Table 2 show the macro and weighted F1 scores achieved by the original and mitigated models on the test set. The mitigated models exhibit higher predictive performance in terms of F1 scores than the original model. The increment is around 1% for both scores and mitigated models. Finally, column 6 shows the AUC for all the toxicity-related labels. The mitigated model produced by MS1 achieves 0.979 on the AUC score, with a decrease of 0.007 from the original model. Instead, with MS2, the decrease in performance is only 0.003."}, {"title": "Summary.", "content": "The experimental results obtained from the out-of-distribution data demonstrate the capability of our framework to effectively mitigate a model's reliance on protected attributes when applied to non-training data, where ground truth labels are unavailable. It showcases its adaptability and robustness in real-world scenarios where labeled data may not be readily accessible."}, {"title": "Mitigating sentiment analysis", "content": "This evaluation aims to assess the versatility and effectiveness of our framework across different classification tasks. For this experiment, we chose sentiment classification to test our framework in mitigating the use of protected attributes for tasks where their reliance might be lower."}, {"title": "Training the original sentiment classifier.", "content": "We selected a dataset of 163K tweets and 37K Reddit comments in English, expressing people's opinions towards the general elections held in India in 2019.11 The task consists of a multi-class sentiment classification problem with 3 classes: negative, neutral, and positive. We split the dataset with 80% for training (160k) and 20% for testing (40k). We fine-tuned the BERT model for 3 epochs, achieving a 0.96 F1 score on the test set."}, {"title": "Identifying protected attributes in sentiment predictions.", "content": "We used the fine-tuned model to predict the sentiment label over the entire test set. Then, we analyzed, separately, all the negative and positive texts with the Explainer component instantiated with Integrated Gradients. The neutral texts do not contain specific patterns that the model should learn and are not of interest for mitigation. Then, we annotated with GPT-3.5-Turbo the 5% of the most important words for the negative and 5% for the positive texts separately, resulting in the top 200 negative and 200 positive words. We found that 16 (8%) of 200 negative words and 11 (6%) of 200 positive words were labeled as protected attributes by the Identifier, suggesting a moderate reliance on protected attributes."}, {"title": "Training the mitigated models.", "content": "We applied the two removal-based mitigation strategies (MS1, MS2). In this case, the mitigation is performed separately per class label (e.g., the protected attributes in the most negative words are mitigated only on the negative training examples). MS1 decreases the training set by 5k negative and 8k positive training examples, as shown in the third and fourth columns in Table 3. Also in this case, the models were fine-tuned for 3 epochs.\nWe used the mitigated models to predict the sentiment label over the test set. Then, we extracted the most important 200 words from the negative and positive texts separately with the Explainer, and we annotated those words with the Identifier."}, {"title": "Results.", "content": "The last four columns in Table 3 show the percentage and the number of protected attributes of the original (M) and mitigated (M\u2081 and M\u2082) sentiment classifiers for the negative and positive classes, separately. MS1 produces a mitigated model that relies on half of the protected attributes of the original classifier (4% for the negative and 3% for the positive classes). Interestingly, the number of protected attributes the original classifier relied on is almost completely mitigated, except for 2 for the negative and 1 for the positive classes. MS2 has a similar behavior in this. However, many new protected attributes emerge as new important words. In the end, the total number of protected attributes remains the same, even though the protected attributes of the original model have almost all been mitigated. Therefore, MS1 is the most effective in this case."}, {}]}