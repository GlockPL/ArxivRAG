{"title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers", "authors": ["SALVATORE GRECO", "KE ZHOU", "LICIA CAPRA", "TANIA CERQUITELLI", "DANIELE QUERCIA"], "abstract": "Al regulations are expected to prohibit machine learning models from using sensitive attributes during training. However, the latest Natural Language Processing (NLP) classifiers, which rely on deep learning, operate as black-box systems, complicating the detection and remediation of such misuse. Traditional bias mitigation methods in NLP aim for comparable performance across different groups based on attributes like gender or race but fail to address the underlying issue of reliance on protected attributes. To partly fix that, we introduce NLPGUARD, a framework for mitigating the reliance on protected attributes in NLP classifiers. NLPGuard takes an unlabeled dataset, an existing NLP classifier, and its training data as input, producing a modified training dataset that significantly reduces dependence on protected attributes without compromising accuracy. NLPGUARD is applied to three classification tasks: identifying toxic language, sentiment analysis, and occupation classification. Our evaluation shows that current NLP classifiers heavily depend on protected attributes, with up to 23% of the most predictive words associated with these attributes. However, NLPGUARD effectively reduces this reliance by up to 79%, while slightly improving accuracy.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, the adoption of deep learning-based NLP models has exponentially increased. Transformer-based models, such as BERT [18], T5 [57], and GPT [8], have achieved unthinkable levels of performance on several natural language tasks. However, despite being increasingly accurate, these models remain black-boxes [28]. For an NLP classification task, models predict a class label from an input text without providing any information on the complex internal decision-making mechanism, making it challenging to identify and mitigate potential bias and/or unfair behavior in such models."}, {"title": "RELATED WORK", "content": "Bias in NLP decision-making has manifested itself in several ways, including dialogue generation [19], text classification [20], and machine translation [66]. It usually arises from training data [22, 67]. For instance, pre-trained models and word embedding can inherit biases and stereotypes present in the large training corpora [7, 9, 25, 58]. When quantifying bias, existing works generally highlight disparities between demographic groups, with differences in performance or selection bias on protected attributes such as race, gender, religion, and sexual orientation [22, 29, 37, 67].\nTo address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline [38, 75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics). Most existing works focus on the first two stages, exploiting data augmentation and modified model training techniques [4, 6, 20, 52, 58, 64, 77]. Furthermore, most of those studies focus on one protected category at a time. For example, Badjatiya et al. [6] proposed the identification of protected attributes, such as gender, by creating a manual list of words, measuring the skewed occurrence of words across classes or predicted class probability distribution of words. Park et al. [52] introduced gender swapping to equalize the number of male and female entities in the training data. Dixon et al. [20] proposed dataset augmentation strategies that generate new sentences using templates or replace protected attributes with generic tags, such as part-of-speech or named-entity tags. Zhang et al. [77] proposed mitigating biases in the training data by assuming a non-discrimination distribution and then reconstructing the distribution using instance weighting. Ravfogel et al. [58] proposed removing information from neural representations concerning gender or race for debiasing word embedding for NLP classification.\nThese past works try to mitigate unintended bias and performance imbalance between subgroups by (1) removing implicit bias from word embeddings, (2) performing data augmentation on the training set (data-based), or (3) intervening directly in the model architecture or objective function (model-based). However, a gap remains in evaluating (and tackling) the extent to which NLP classifiers depend on protected attributes for their predictions. In this paper, we aim to fill that gap. We consider the following definition of fairness through unawareness: \u201can algorithm is fair as long as any protected attributes are not explicitly used in the decision-making process\u201d [27, 40, 45]. Textual data is unstructured; hence, protected attributes are not explicitly delineated as input features, such as columns used in structured datasets. Consequently, we refine this definition in the context of NLP applications to ensure words associated with protected characteristics are not utilized in decision-making unless necessary. Our approach aims to reduce the use of protected attributes in the decision-making process of NLP models, thereby better aligning them with legal regulations.\nCompared to prior work, our approach not only has a different objective, but it also overcomes two of their main limitations: (1) their focus on a subset of protected attributes at a time (usually race and gender); (2) their manual and static identification of protected attributes via pre-defined dictionaries, lists of identity terms, or additional annotations. The only technique addressing these limitations is Entropy-based Attention Regulation (EAR) [4]. EAR introduces a regularization term to discourage overfitting to training-specific potentially biased terms. However, those terms are automatically identified during training, leaving no flexibility for users to select which categories to mitigate. Unlike previous techniques, our approach: (1) identifies and mitigates multiple protected categories simultaneously; (2) can be fully automated, allowing for a dynamic update of the dictionary of protected attributes; and (3) allows for the selection of the categories to mitigate."}, {"title": "OUR MITIGATION FRAMEWORK", "content": "Our Mitigation Framework, namely NLPGUARD, has been designed to be generally applicable to any supervised machine learning-based NLP classification model applied on an unlabelled corpus. As illustrated in Figure 3, NLPGUARD takes in input an unlabeled corpus and a pre-trained NLP classifier (together with its training dataset) to produce a mitigated training dataset in output. Ground truth class labels for the unlabelled corpus are not required; rather, the classifier is used to generate them, both for in-distribution (i.e., data that comes from the same distribution as the original training dataset) and for out-of-distribution data where labels are unavailable. Because of the black-box nature of NLP classifiers based on deep learning models, labels might be predicted using protected attributes. To mitigate that, our framework comprises the following three components:\nA. Explainer. This component uses Explainable Artificial Intelligence (XAI) techniques to identify the most important words used by the model for its predictions. The XAI field has made great strides in making black-box models more transparent, and several techniques exist to explain NLP classifiers [14]. The best one for our purpose should have two qualities: (1) quantify the importance of each feature word (feature-based), and (2) be applicable to explain the model's predictions after training (post-hoc). Many techniques meet these requirements, and most of them measure the importance of each word for the prediction within an individual sentence (local-explanations) [2, 44, 59, 63, 69, 73]. Our Explainer component first identifies the words important for prediction within all individual sentences, exploiting any of those techniques. Each word is, as such, associated with multiple scores, one for each occurrence in each sentence. Second, it determines the most important predictive words for the model as a whole (global-explanations) following the idea of some of these techniques, which aggregate the words' importance over many sentences to compute the overall importance [72, 73]. Specifically, for each word, it sums all their individual scores and divides them by their frequency to compute the word's classification score. The normalization step is required to also identify rare but important words. The output of the Explainer component is the ordered list of the most important words for the model's predictions.\nB. Identifier. This component aims to annotate which of the previously detected important words refer to protected attributes. We consider the nine protected categories defined by the Equality Act: age, disability, gender reassignment, marriage and civil partnership, pregnancy and maternity, race, religion or belief, sex, and sexual orientation. Our framework allows for annotation using human-in-the-loop (B1) and machine-in-the-loop (B2) approaches.\nB1. Human-in-the-loop Annotation. Crowdsourcing platforms, such as Amazon Mechanical Turk (MTurk) and Prolific [13], have been extensively used by the research community to recruit crowdworkers for data labeling purposes. Crowdworkers [60, 68] are anonymous people usually paid for completing simple tasks. This component leverages crowdsourcing to perform the protected attribute annotation of the most important words. In our work, we exploited MTurk, where for each important word {WORD}, participants were asked to answer the following question:\nQuestion: Is the word {WORD} referring to:\nPossible Answers: 1. Age, 2. Disability, 3. Gender reassignment, 4. Marriage and civil partnership, 5. Pregnancy and maternity, 6. Race, 7. Religion or belief, 8. Sex, 9. Sexual orientation, 10. None of the above;\nTo ensure data quality, we adopt a trap mechanism to detect random responses from participants and reject them (details of this mechanism are presented in \u00a74.3).\nB2. Machine-in-the-loop Annotation. As a cost-effective and scalable alternative to human-in-the-loop annotations via crowdsourcing, we also implemented a protected attributes annotation process that uses Large Language Models (LLMs). We did so inspired by a recent study [23] that found LLMs, including ChatGPT, to outperform crowdworkers in text-based annotation tasks. It also has been shown that LLMs are effective in solving many NLP tasks [56]. Specifically, we implemented an annotation process that interacts with ChatGPT as follows: two prompts provide the protected categories, their definitions, and links with additional information. Then, for each word, the LLM is asked to: (1) classify the word into one of the protected categories or none of them;\nC. Moderator. This component produces a new mitigated training dataset that can be used to train a new classifier that uses fewer protected attributes previously identified. It takes the original pre-trained classifier, the original training dataset, and the list of the most important words enriched with the protected attribute label as input. It produces a new mitigated training dataset by adjusting the training dataset based on the identified protected attributes that can then be used to train a new mitigated classifier. We designed and tested five mitigation strategies (MS).\n(MS1) Sentence-level removal. Previous works have shown that subsampling can be an easy but effective technique for data balancing [33, 61]. This mitigation strategy eliminates all sentences containing protected attributes from the training set. As a result, it reduces the overall number of training examples. For example, if a word $W_i$ is identified as a protected attribute, all sentences in the training set that include that word are removed. The idea behind this strategy is that the imbalance in the number of training examples containing protected attributes for a particular class may have led the model to learn that these protected attributes are crucial for classifying that class.\n(MS2) Word-level removal. This strategy removes only the protected attribute words from the sentences in the training set while preserving the number of examples. The process involves removing the identified protected attribute words from all sentences in the training set, thereby removing their influence on the model's learning process. The idea is that the model should be able to classify sentences without relying solely on the protected words, and rather use other words in the text too0.\n(MS3) Word-level replacement with a random synonym. This strategy replaces every instance of a protected attribute in the training set with one of its synonyms. It first uses embedding similarity techniques to identify the k-nearest neighbors for each protected attribute. Then, it randomly selects one of the k-most similar words to replace each instance of the protected attribute in the training set. This has been shown to mitigate bias in [6], and it is believed that it may also help mitigate the use of protected attributes in classification, as the model may learn to rely on other words for classifying the classes rather than solely relying on protected attributes. This approach maintains the same number of examples in the training set but increases the diversity of words.\n(MS4) Word-level replacement with K random synonyms. This strategy expands the training set by generating new sentences using synonyms of protected attributes. Instead of replacing the protected attribute in-place with one similar word as in MS3, it creates k new sentences by replacing the protected attribute with each of its k-nearest neighbors. For example, given a sentence containing a protected attribute $W_i$, k new sentences are created by replacing $W_i$ with each of its k most similar words. This increases the size of the training set and diversifies the words used in the sentences.\n(MS5) Word-level replacement with hypernym. This strategy replaces instances of protected attributes in the training set with higher-level words, called hypernyms, which provide a more general representation of the category to which the protected attribute belongs. For example, the hypernym of 'dog' could be\u2018animal'. By using hypernyms instead of the specific protected attributes, the model may not discriminate based on these attributes in its classifications. This technique has been shown to be effective in mitigating accuracy imbalance between subgroups [6]."}, {"title": "FRAMEWORK EVALUATION: EFFECTIVENESS AND SENSITIVITY", "content": "We evaluate the effectiveness of our framework and the sensitivity of the Explainer (\u00a74.2), Identifier (\u00a74.3), and Moderator (\u00a74.4) components in mitigating a toxicity classifier applied to in-distribution data (i.e., the test set)."}, {"title": "Evaluation task", "content": "We choose toxicity prediction as the main evaluation task in line with previous research. Toxicity classifiers are used in different contexts [39, 76], such as Reddit, Twitter, and 4chan, with competitive performance. However, they suffer from different types of biases [15, 16, 31, 62]. In Wikipedia, for example, any comment containing words associated with insults, offense, or profanity, regardless of the tone, the intent, and the context, would be classified as toxic; toxic language was however more likely predicted from minority communities, as found in [15], thus suggesting the use of protected attributes by such models.\nIn our experiments, we used the \u201coriginal model\u201d in the widely used detoxify [30] library\u00b3 as a pre-trained toxicity classifier. This is a BERT-base and uncased model [18] trained on a dataset of publicly available Wikipedia comments. It was fine-tuned for predicting 6 labels related to toxicity: toxicity, severe toxicity, obscene, threat, insult, and identitiy attack, achieving an average Area Under the ROC Curve (AUC) score of 98.6%. For the toxicity label only, the classifier achieved 0.82 macro and 0.93 weighted F1 scores (the dataset is imbalanced). This classifier is applied to the original test set comprising 153,164 texts, and predicted the toxicity label for 36,148 texts (23.6% of the test set)."}, {"title": "Component evaluation: Explainer", "content": "The Explainer aims to identify the most crucial words utilized by the classifier for predictions (as described in \u00a73-A). The employed XAI technique can influence the words recognized as significant, thereby impacting the identified protected attributes on which the model relies to make predictions."}, {"title": "Evaluation metrics", "content": "To evaluate the effectiveness of the Explainer component, we first measure the impact on the model's predictive performance (F1 score) by removing the most important words identified by each XAI technique. An effective and precise explainer should result in a noticeable decrease in the predictive performance when these words are removed. Secondly, to assess the sensitivity of the Explainer, we measure the overlap of the most important words identified by different XAI techniques. A substantial overlap indicates consistent outputs across XAI techniques. Lastly, we measure the computation time for generating explanations to assess the efficiency of the Explainer based on the XAI technique, which is a crucial aspect when dealing with large datasets."}, {"title": "Explainer setup", "content": "There are two main categories of XAI techniques to compute explanations within a sentence: permutation-based and gradient-based [14]. For this comparison, we instantiated the Explainer component with SHapley Additive exPlanations (SHAP) [44] as a representative of the permutation-based and with Integrated Gradients (IG) [69] of the gradient-based techniques. Both techniques have demonstrated competitive performance in prior studies [3]. Specifically, for SHAP, we used the text permutation explainer with 3,000 as the maximum evaluation step parameter. For Integrated Gradients, we exploited the implementation provided by the Ferret [5] library."}, {"title": "Results", "content": "We produced the explanations within each sentence over the toxic texts in the test set using both techniques (SHAP and IG); we then aggregated individual scores and extracted an ordered list of the most toxic words as previously described in \u00a73-A. Figure 4 shows the decrease in the F1 score by removing the most important words in the range from 50 to 700 with a step of 50. As expected, removing the most important words from the test set causes a marked decrease in predictive performance, especially for the top 250 words. IG exhibits higher precision, leading to a more substantial decrease initially. However, the decrement tends to converge on the top 400 words for both techniques. This shows that both techniques effectively extract the words used by the classifier for its predictions. We selected the top 400 most toxic words (approximately 10%) - removing additional words caused a lower decrease in predictive performance.\nWe then measured the overlap between the 400 most toxic words identified with IG and SHAP. We found that 307 out of the 400 words were identical (77%), indicating a substantial agreement between the two techniques. The 23% of disagreement may be attributable to the varying precision levels inherent in the two methods.\nFinally, we compared the execution times required to generate explanations using both techniques. We observed that IG significantly outperformed SHAP, completing the explanation process more than two orders of magnitude faster. On average, the execution time in seconds to obtain an explanation is approximately 0.2 for IG and 30 for SHAP, using a single Nvidia RTX A6000 GPU.\nIn summary, Integrated Gradients proves to be the most effective technique for the Explainer component, as it exhibits higher precision in identifying the most crucial words and executes significantly faster. As a result, we adopt Integrated Gradients in the Explainer for all subsequent experiments. Nevertheless, the framework allows the use of alternative XAI techniques."}, {"title": "Component evaluation: Identifier", "content": "The Identifier aims to determine which of the most important words are actually protected attributes (\u00a73-B). To evaluate its effectiveness, we compare the protected attributes identified by the component instantiated with human-in-the-loop and machine-in-the-loop approaches against the annotations provided by two expert annotators, who possess a greater depth of knowledge of the definitions of protected categories by AI regulators than participants engaged in the human study. We also add for comparison a pre-defined dictionary of 51 protected attributes from previous works [6, 20]."}, {"title": "Evaluation metrics", "content": "We measure the Cohen's kappa inter-annotator agreement [11] to evaluate the accuracy and reliability of the protected attributes identified by the different approaches. It ranges between [0, 1]; the higher the score is, the higher the agreement."}, {"title": "Protected attributes identifier setup", "content": "We selected the 400 most toxic words extracted with the Explainer component instantiated with Integrated Gradients as the candidate set to identify protected attributes. Then, we configured the Identifier as follows."}, {"title": "Human-in-the-loop setup", "content": "We set up an MTurk study where we asked annotators to label words with the protected category they refer to (if any). As anticipated in \u00a73-B1, we also used a trap mechanism to detect poor quality responses; specifically, we gave annotators the following definition of toxicity: \u201cToxic language is a way of communicating that harms other people\u201d. Then, for each word, we also asked participants to answer the following additional trap question:\nTrap Question: Does the word {WORD} suggest toxic language?\nPossible Answers: 1. Not at all, 2. Very little, 3. Somewhat, 4. To a great extent, 5. Definitely.\nTo the list of 400 most toxic words, we added 15 trap words that can be easily classified as toxic (e.g., \"asshole\u201d) or non-toxic (e.g., \u201cfriendly\u201d). The full list of trap words can be found in Table 5 in Appendix B. For the non-toxic (toxic) trap words, we expected MTurk participants to select a score of 1 or 2 (4 or 5) on the Likert scale. Participants were considered unreliable if they did not meet those expectations, and their assessments were discarded from our results. We ended up with 246 reliable participants, evenly split between males and females. The majority were educated (74% finished college), mostly located in the United States, falling within the median age group of 26-39. In terms of racial demographics, most were White (52%), followed by Asian (27%), African (7%), and Hispanic (5%).\nWe collected five annotations per word on average. A word was labeled as a protected attribute if the sum of the votes across the nine categories exceeded the None of the above (majority voting)."}, {"title": "Machine-in-the-loop setup", "content": "We also annotated the same set of words by prompting GPT-3.5-Turbo, as introduced in \u00a73-B2. The temperature parameter was set to 0.3 to limit creativity in generating the responses. Other temperature values in the range [0.3, 0.7] have been experimented with, although no major differences were observed. For each candidate word, we prompted GPT-3.5-Turbo, asking for a possible protected category, a reliability score, and an explanation for the classification. If a word is classified with any protected category, it is labeled as a protected attribute."}, {"title": "Results", "content": "The two expert annotators (A1, A2) identified 72/400 (18%) and 66/400 (17%) protected attributes, respectively. The human-in-the-loop (MTurk) approach 108/400 (27%) ones. Instead, the machine-in-the-loop (ChatGPT) approach labeled 93/400 (23%) words as protected attributes. These findings indicate that the original classifier heavily relies on protected attributes for toxicity predictions. Interestingly, the ChatGPT Identifier is also able to annotate proxy words for the protected categories. For example, the word 'headscarf' is annotated as related to religion and belief (see Figure 10 in Appendix A). If we use the pre-defined dictionary [6, 20] instead of the Identifier component, only 9 out of 400 words (2%) would have been labeled as protected attributes. This suggests that pre-defined dictionaries may consist of a limited subset of protected attributes and not encompass the entire range of relevant attributes.\nFigure 5 shows the two-by-two Cohen's kappa inter-annotator agreement between the annotation performed by the two experts (A1, A2), ChatGPT (GPT), MTurk (MT), and the pre-defined dictionary (D). A higher score indicates a greater level of agreement. The score between the two expert annotators is 0.81, corresponding to an almost perfect agreement according to Landis and Koch's scale [41]. The ChatGPT annotations demonstrate substantial (0.67) and moderate (0.56) agreement with the expert annotators. In contrast, the MTurk annotations show a moderate agreement of 0.48 and 0.44 with the expert annotators, respectively. The pre-defined dictionary exhibits low agreement with all other annotations, as it only covers a small subset of the important words.\nThis evaluation demonstrates that the machine-in-the-loop approach outperforms the human-in-the-loop one in identifying protected attributes, while also enabling the full automation of the framework. For the next experiments, we thus adopt the LLM-based approach for the Identifier."}, {"title": "Component evaluation: Moderator", "content": "The Moderator aims to create a mitigated training corpus to train a new classifier with reduced reliance on protected attributes and similar predictive performance. To evaluate the effectiveness of each mitigation strategy, we trained and evaluated a distinct mitigated model for each strategy."}, {"title": "Evaluation metrics", "content": "For each mitigation strategy, we examine two key aspects of the mitigated classifiers: fairness and predictive performance. Our fairness is defined as fairness through unawareness, whereby \u201can algorithm is fair as long as any protected attributes are not explicitly used in the decision-making process\u201d [45]. This is quantified by measuring the number of protected attributes each mitigated model relies on in making predictions based on the Explainer and Identifier components. A lower number indicates a reduced dependence on protected attributes, which signifies progress towards a more fair and unbiased classifier. To evaluate the predictive performance, we measure the F1 score specifically for the toxicity label, providing insight into the model's accuracy in identifying toxic texts. Additionally, we evaluate the Area Under the Curve (AUC) score for all toxicity-related labels. This metric provides an overall measure of the model's performance in identifying various aspects of toxicity. By considering both fairness and predictive performance, we can ascertain the effectiveness of the mitigated models in achieving a balance between reducing reliance on protected attributes and maintaining similar predictive capabilities."}, {"title": "Moderator setup", "content": "For the mitigation strategies outlined in \u00a73-C, we used the following setup: for the removal-based strategies (MS1, MS2), we removed the sentences or the words if, after the tokenization, the protected attributes are in the list of tokens. In the case of the mitigation strategies based on the k-neighbours (MS3, MS4), we set the value of k to 5, meaning that for each protected attribute, the five closest words were identified. To identify these nearest neighbors, we computed the cosine similarity between each word in the vocabulary and the protected attribute using the 300-dimensional GloVe [55] word embedding, as suggested in [6]. For the hypernyms-based strategy (MS5), we utilized the WordNet lexical database [46] provided in NLTK, as suggested in [6]. We replaced each protected attribute with its first-level hypernym extracted from its synset of synonyms."}, {"title": "Results", "content": "Fairness. The last two columns in Table 1 show the percentage and number of the most toxic words labeled as protected attributes for all the mitigated models (M-M). The number of those already present among the protected attributes of the original model (Mo) is indicated in curly brackets. All the mitigation strategies reduced the number of protected attributes the model relied upon. However, the mitigated models trained with removal-based strategies (MS1, MS2) achieved much better results. Only 9% and 10% of their most toxic words were labeled as protected attributes (37 and 40 words out of 400), representing a decrease of 61% from the original model (93 words out of 400). One possible reason for the lower performance of replacement-based mitigation strategies (MS3-MS5) is that they can introduce new protected attributes when replacing words. For a qualitative evaluation of fairness improvement, please refer to Figure 11 and Figure 12 in Appendix C.\nPredictive performance. In Table 1, columns 4 and 5 report the macro and weighted F1 scores on the toxicity label for the original and mitigated models. Column 6 also presents the mean AUC scores across all the toxicity-related labels. The results show that all the mitigated models present similar F1 scores compared to the original model, except for the one trained with MS4, which exhibits a greater decrease. Interestingly, the mitigated models trained on the removal-based mitigation strategies (MS1, MS2) achieve better F1 scores than the original model. The word-removal (MS2) increases the macro and weighted F1 scores by 1.3% and 1.2%. Indeed, we observed that the removal-based mitigation strategies reduced the number of false positives in the toxicity predictions (i.e., non-toxic texts wrongly predicted as toxic). All the mitigated models exhibit slightly lower AUC scores compared to the original model. However, the decrease is minor and acceptable (around 0.5% and 0.3%) in light of the reduced reliance on protected attributes.\nSummary. We conclude that our framework effectively reduces the model's reliance on protected attributes without compromising its predictive performance. Indeed, all mitigated models are fairer in that they significantly reduce the use of protected attributes and exhibit similar predictive performance to the original model. Interestingly, the removal-based strategies (MS1, MS2) even increased the models' predictive performance after the mitigation."}, {"title": "FRAMEWORK EVALUATION: GENERALIZABILITY", "content": "We finally evaluate the generalizability of our framework first to toxicity prediction on out-of-distribution data (\u00a75.2), and second on different tasks, i.e., sentiment analysis (\u00a75.3) and occupation classification (\u00a75.4)."}, {"title": "Framework and evaluation settings", "content": "For this evaluation, we instantiated the Explainer with Integrated Gradients, the Identifier with ChatGPT, and the Moderator with the removal-based mitigation strategies. As shown in \u00a74, this turned out to be the optimal framework configuration. We perform a similar evaluation of the mitigated models by measuring their fairness and predictive performance. Fairness is evaluated by quantifying the number of protected attributes each mitigated model relies on (fairness through unawareness). Predictive performance is evaluated using quantitative metrics on the test set. For the toxicity classifier, we measure the F1 score for the toxicity label, which allows us to gauge the model's accuracy in detecting instances of toxicity, and the Area Under the Curve (AUC) score for all toxicity-related labels, providing an overall assessment of the model's performance in identifying different aspects of toxicity. For the sentiment and the occupation classifiers, we solely measure the F1 score, as it provides a comprehensive assessment of the model's accuracy in these tasks."}, {"title": "Mitigating toxicity prediction on out-of-distribution data", "content": "This experiment aims to assess the applicability of our framework in mitigating the toxicity model when applied to out-of-distribution data, specifically company reviews. This is crucial as classifiers are normally applied to datasets from other domains with different word distributions from training."}, {"title": "Company reviews data", "content": "We collected data from a popular online platform where current and former employees write reviews about companies. Reviewers comment on various aspects such as personal experience with the company or managers, salary information, workplace culture, and typical job interviews. The platform fosters a constructive approach among its users by manually and automatically moderating the content of reviews. However, reviews are published anonymously. On the one hand, this promotes user privacy. On the other hand, it can also cause some users to write public insults and offenses toward companies or people. Specifically, we collected a dataset of 439,163 reviews from U.S.-based companies across all 51 U.S. states written from 2008 to 2020. Each review contains a pros part (positive comments in the review) and a cons part (negative comments). We applied the same toxicity classifier introduced in \u00a74.1 to identify toxic company reviews."}, {"title": "Toxicity in company reviews", "content": "The initial expectation was not to have many toxic reviews in the dataset due to the highly curated nature of the platform. However, if we consider a post to be toxic when at least one of the cons or pros fields contains inappropriate content, we found 1.6% of reviews (7,224) to be toxic. The number of reviews classified as toxic by using the pros and cons texts as input is 853 for pros (0.2%) and 6,495 for cons (1.5%) over 439,163. As expected, we found that most of the toxic texts are present in cons. Interestingly, some people tend to be so angry and frustrated by the work experience that they let off steam even in the pros field."}, {"title": "Identify protected attributes in toxicity predictions on company reviews", "content": "All pros and cons reviews predicted as toxic were analyzed by the Explainer component to extract the most important words used by the model in predicting toxic reviews. Then, we selected the 400 most toxic words extracted, and we annotated those words with GPT-3.5-Turbo. Among the 400 most important words used by the model in predicting toxic reviews, 76 are protected attributes (19%), as shown in the last two columns of the first row in Table 2 (original model Mo). We can conclude that the original classifier exhibits a significant reliance on protected attributes for toxicity predictions, even when applied to different out-of-distribution data."}, {"title": "Training the mitigated models", "content": "We applied the removal-based mitigation strategies (MS1, MS2) to the original Wikipedia comments training dataset based on the protected attributes identified in the toxic company reviews. Table 2 shows the differences in the number of training examples after each strategy in the third column. The original training dataset contained 159,571 examples. MS1 resulted in a decrease of 6k examples, while MS2 did not change it. All mitigated models were fine-tuned for 3 epochs, with a batch size of 16, and Adam as optimizer. To evaluate the mitigated models, all pros and cons reviews were classified by each mitigated model. Then, we applied the Explainer component to extract the most important 400 predictive words used by each mitigated model for the toxicity predictions on company reviews. Finally, we exploited the Identifier to determine if the new important words of the mitigated models were protected attributes."}, {"title": "Results", "content": "Fairness. The last two columns in Table 2 show the percentage and number of the most toxic words labeled as protected attributes. The number of those already present among the protected attributes used by the original model (Mo) is also indicated in curly brackets. The results confirm that removal-based mitigation strategies reduce the number of protected attributes the model relied upon. MS1 and MS2 reduce the percentage of protected attributes from 19% to 4% and 5% (16 and 19 out of 400 words), respectively. This corresponds to a decrease of 79% and 75%. They also reduce the protected attributes the original classifier relies on from 76 to 8 and 11, respectively.\nPredictive performance. Columns 4 and 5 in Table 2 show the macro and weighted F1 scores achieved by the original and mitigated models on the test set. The mitigated models exhibit higher predictive performance in terms of F1 scores than the original model. The increment is around 1% for both scores and mitigated models. Finally, column 6 shows the AUC for all the toxicity-related labels. The mitigated model produced by MS1 achieves 0.979 on the AUC score, with a decrease of 0.007 from the original model. Instead, with MS2, the decrease in performance is only 0.003.\nSummary. The experimental results obtained from the out-of-distribution data demonstrate the capability of our framework to effectively mitigate a model's reliance on protected attributes when applied to non-training data, where ground truth labels are unavailable. It showcases its adaptability and robustness in real-world scenarios where labeled data may not be readily accessible."}, {"title": "Mitigating sentiment analysis", "content": "This evaluation aims to assess the versatility and effectiveness of our framework across different classification tasks. For this experiment, we chose sentiment classification to test our framework in mitigating the use of protected attributes for tasks where their reliance might be lower."}, {"title": "Training the original sentiment classifier", "content": "We selected a dataset of 163K tweets and 37K Reddit comments in English, expressing people's opinions towards the general elections held in India in 2019. The task consists of a multi-class sentiment classification problem with 3 classes: negative, neutral, and positive. We split the dataset with 80% for training (160k) and 20% for testing (40k). We fine-tuned the BERT model for 3 epochs, achieving a 0.96 F1 score on the test set."}, {"title": "Identifying protected attributes in sentiment predictions", "content": "We used the fine-tuned model to predict the sentiment label over the entire test set. Then, we analyzed, separately, all the negative and positive texts with the Explainer component instantiated with Integrated Gradients. The neutral texts do not contain specific patterns that the model should learn and are not of interest for mitigation. Then, we annotated with GPT-3.5-Turbo the 5% of the most important words for the negative and 5% for the positive texts separately, resulting in the top 200 negative and 200 positive words. We found that 16 (8%) of 200 negative words and 11 (6%) of 200 positive words were labeled as protected attributes by the Identifier, suggesting a moderate reliance on protected attributes."}, {"title": "Training the mitigated models", "content": "We applied the two removal-based mitigation strategies (MS1, MS2). In this case, the mitigation is performed separately per class label (e.g., the protected attributes in the most negative words are mitigated only on the negative training examples). MS1 decreases the training set by 5k negative and 8k positive training examples, as shown in the third and fourth columns in Table 3. Also in this case, the models were fine-tuned for 3 epochs.\nWe used the mitigated models to predict the sentiment label over the test set. Then, we extracted the most important 200 words from the negative and positive texts separately with the Explainer, and we annotated those words with the Identifier."}, {"title": "Results", "content": "Fairness. The last four columns in Table 3 show the percentage and the number of protected attributes of the original (Mo) and mitigated (M1 and M2) sentiment classifiers for the negative and positive classes, separately. MS1 produces a mitigated model that relies on half of the protected attributes of the original classifier (4% for the negative and 3% for the positive classes). Interestingly, the number of protected attributes the original classifier relied on is almost completely mitigated, except for 2 for the negative and 1 for the positive classes. MS2 has a similar behavior in this. However, many new protected attributes emerge as new important words. In the end, the total number of protected attributes remains the same, even though the protected attributes of the original model have almost all been mitigated. Therefore, MS1 is the most effective in this case.\nPredictive performance. The fifth column in Table 3 shows the F1 score obtained by the original sentiment classifier and the mitigated models on the test set. The mitigated models achieve the same F1 score, thus showing the same predictive capabilities.\nSummary. These results show that our framework can be effective not only on the toxicity model, which heavily relies on protected attributes, but also on the sentiment model, which is moderately impacted by protected attributes, confirming its general effectiveness."}, {"title": "Mitigating occupation classification", "content": "This evaluation serves three primary objectives: (1) to further assess the adaptability and effectiveness of our framework across various classification tasks, (2) to mitigate the use of protected attributes in scenarios where the final prediction has tangible consequences for individuals, and (3) to compare our framework with two mitigation techniques that act on the model rather than data. To achieve these goals, we selected the task of predicting occupations from online biographies, using a dataset of biographies annotated by gender and occupation from previous works [17, 54, 58]. We used the field 'cleaned_input_text' as input text, where sentences that directly reveal the occupation were removed (e.g., \u201che is a journalist\u201d), and we removed first names. We compare our framework with two model-based mitigation techniques: Iterative Null-space Projection (INLP) [58] and Entropy-based Attention Regulation (EAR) [4]. INLP requires an additional annotation for the mitigated category, which is only present for gender in this dataset. Therefore, we conduct two distinct analyses: (1) focusing solely on gender-related, and (2) examining all protected categories together. For the gender-related protected attributes, we compare both baselines with our word-removal (MS2). Instead, we use only EAR as a baseline to mitigate all the protected categories. We chose MS2 because it has shown similar mitigation effectiveness while maintaining competitive performance, but it has higher flexibility across datasets than sentence-removal (MS1) (see \u00a76.3)."}, {"title": "Training the original occupation classifiers", "content": "The dataset contains 393,423 biographies for 28 occupations split into 255,710, 39,369, and 98,344 train, dev, and test examples. We fine-tuned a BERT-base and uncased model for each occupation in one-vs-all settings (i.e., a binary model that predicts the occupation or not for each label). Due to the high imbalance of the dataset, we performed the experiments for the five most frequent classes (i.e., nurse, attorney, journalist, physician, and professor). The models were fine-tuned for 3 epochs. The second column in Table 4 shows the macro F1 score on the test set for each original model trained for four occupations. Those models are highly effective in classifying occupations, achieving a macro F1 score higher than 0.89. Still, such high performance could be achieved by heavily using protected attributes."}, {"title": "Identifying protected attributes in occupation classification", "content": "We used each original model to predict each occupation label over the entire test set and analyzed those texts using the Explainer to extract the most important words in predicting each occupation. Then, for each occupation, we annotated with GPT-3.5-Turbo the top 400 words to identify protected attributes. We measured the models' reliance on protected attributes related to (1) gender only, and (2) all categories (third and fourth columns in Table 4). All the models moderately rely on protected attributes. The nurse occupation is the most influenced, also by gender-related words, such as pronouns (e.g., \u2018she', 'her')."}, {"title": "Training the mitigated models", "content": "With our framework, we applied the word removal mitigation strategy (MS2) for each occupation on (1) gender-related only, and (2) all the protected categories simultaneously. Therefore, we trained two different mitigated models for each occupation. We trained one mitigated model for each occupation with the INLP methodology [58]. INLP can mitigate the gender-related protected attributes but is not applicable to all the other categories, since the dataset contains the additional annotation only for gender. Specifically, we used the original pre-trained BERT weights as the encoder. Then, we multiplied the embedding representation of the [CLS] token from the last hidden layer for each input text by the projection matrix produced by the INLP technique (to ensure that the embedding representation does not encode information about gender). Finally, we added a classification layer on top. We fine-tuned only the classification layer while freezing the BERT encoder and the projection matrix, as suggested in [58]. For EAR [4], we used the same BERT architecture, and we added the loss function regularization term, with 0.001 as regularization strength. EAR does not allow the selection of which protected categories to mitigate, but it identifies by itself which words have a high attention entropy. Thus, we trained a single mitigated model, and we evaluated its reliance on gender and all protected categories separately.\nWe used the mitigated models to predict the occupation labels over the entire test set, we extracted the most important 400 words for each occupation separately with the Explainer, and we annotated those words with the Identifier to evaluate if they exhibit a reduced reliance on protected attributes."}, {"title": "Results", "content": "Fairness. Columns 7 and 9 in Table 4 show the number and percentage of protected attributes on which the mitigated models rely for gender only and all categories separately. The number of these words already present among the ones used by the original model is also indicated in curly brackets. The objective of each mitigated model is to reduce the reliance on protected attributes (columns 7 and 9) compared to the respective original model for each occupation (columns 3 and 4). Concerning gender (column 7), our framework is the most or as effective as other techniques in mitigating the use of such protected attributes. For the nurse occupation, which represents the model showing a greater dependence on gender-related protected attributes, our framework reduced the number of the most significant gender-related words from 11 to 2. One of these two gender-related words was already significant in the original model. INLP obtained a similar mitigation effect, while EAR is less effective, with 4 gender-related protected attributes still important for the mitigated model. For the attorney and physician occupations, the original model exhibited a lower reliance on gender-related protected attributes, and our framework was able to fully mitigate the use of those words. On average, our framework reduces reliance on gender-related protected attributes by 79%.\nThe results are similar when considering all protected categories (column 9). Our framework is always more effective than EAR in mitigating the use of protected attributes, except for the nurse occupation, where both techniques achieve the same mitigation effect. On average, our framework successfully reduces reliance on all categories of protected attributes by 43%.\nPredictive performance. Columns 6 and 8 in Table 4 show the macro F1 score achieved on the test set by each mitigated model. The objective of each mitigated model is to achieve similar predictive performance (columns 6 and 8) compared to the respective original ones (column 2). The mitigated models produced by our framework and the EAR technique achieve similar or sometimes even better performance than the original model (e.g., for the journalist and physician occupations). Therefore, they are able to mitigate the use of protected attributes without sacrificing predictive performance. Instead, the INLP technique is able to produce models with mitigated bias at the cost of significantly reducing their performance. Indeed, all the mitigated models produced by INLP experienced an average loss in predictive performance of 10%. This tendency to achieve fairness by making every advantaged group worse off or by bringing better performing groups down to the level of the worst off is a common undesirable behavior of bias mitigation techniques [48].\nSummary. These results confirm the effectiveness of our framework in a different task where the protected attributes are strictly related to individuals. They show that our framework is more effective in achieving such an objective than previous bias techniques, while also providing the flexibility to select which protected category to mitigate. This flexibility enables the mitigation of only a subset of protected categories when some are required for the task at hand."}, {"title": "DISCUSSION", "content": "Our results show how the proposed framework could be exploited to train a new classifier that mitigates the use of protected attributes while maintaining competitive performance. We evaluated its sensitivity to each component and its effectiveness in mitigating a toxicity classifier. We also proved its generalizability on models applied to out-of-distribution data (i.e., toxicity on company reviews) and two other tasks (i.e., sentiment analysis and occupation classification). Removal-based strategies (MS1, MS2) have been shown to be the most effective mitigation techniques. We also show that the LLM-based Identifier outperforms the crowdsourcing one, allowing the automation of the framework, and a dynamic updating of the dictionary of protected attributes."}, {"title": "Framework integration, versatility and complexity", "content": "Integration. Our framework can be integrated into existing NLP pipelines for two main purposes. (1) The Explainer and Identifier can be used to measure and evaluate existing NLP classifiers' reliance on protected attributes. As a result, models can be quantitatively compared not only through predictive performance and traditional fairness metrics (e.g., conditional demographic disparity [74]) but also through the use of protected attributes in predictions. (2) The entire framework can be used to train a new model with reduced reliance on protected attributes and competitive performance.\nOur framework can also be integrated to complement other bias mitigation techniques acting in both the model and data spaces that require pre-defined dictionaries or lists of protected attributes or identity terms. The Explainer can improve existing techniques to pinpoint the specific words the model mostly uses for the classification rather than looking at all possible words in the corpus. The Identifier can annotate protected attributes covering a broader range of categories, as many protected attributes, such as disability or religious belief, were rarely covered by prior studies.\nVersatility. Our framework is designed to achieve fairness through unawareness by mitigating the model's reliance on protected attributes in predictions. It addresses multiple categories simultaneously. Therefore, it can potentially address intersectional bias, i.e., that encompasses multiple sensitive attributes together [24]. Moreover, it provides users the flexibility to choose which categories to mitigate, thanks to the fine-grained annotation performed by the Identifier. Such flexibility is particularly useful when some categories are indispensable or aimed at the prediction task. Through this, our framework can address domain-specific bias related to the classification task (e.g., gender-related protected attributes in occupation classification). Consequently, in scenarios where the inclusion of certain protected attributes is necessary for accuracy, our framework can still be utilized to effectively mitigate all other protected attributes that are not essential for the task.\nComplexity. The execution time to produce the mitigated training dataset depends on many factors. Given a fixed model complexity, the execution times increase linearly with: (1) the unlabeled corpus size, (2) the number of most important words annotated, and (3) the training dataset size. Increasing the model's complexity results in a slight increase in the execution time of all components. Our framework yields a mitigated training corpus, necessitating extra training to produce the mitigated model. The (re-)training time varies based on model complexity and original data size. Techniques such as MS2, MS3, and MS5 maintain dataset dimensionality, resulting in comparable training times for both original and mitigated models. Conversely, MS1 reduces or MS4 increases dataset size, affecting training time accordingly."}, {"title": "Implications", "content": "Our research significantly contributes to the CSCW community by exploring human-AI collab-oration, especially in decision-making contexts. For example, our tool could assist humans in comprehending the hiring decisions made by NLP classifiers and address biases in the hiring process [53]. Our work extends into content moderation, empowering the development of robust systems capable of effectively identifying and mitigating toxic content while ensuring fairness. This aids humans in understanding crucial moderation aspects, encompassing significant words and considerations around protected attributes, to foster collaboration with machines to collectively arrive at fair decisions in content moderation. Our work has three main implications:\nFully-automated framework for compliant NLP classifiers. We release an open-source frame-work. By leveraging LLM annotations, it operates in a fully automated manner. The mitigated models exhibit enhanced fairness by significantly reducing their reliance on protected attributes while maintaining comparable or even better predictive performance. Other researchers can utilize our framework to address the compliance standards set by regulators, whether by mitigating already trained models or incorporating them into future models. This contribution empowers the community to uphold ethical standards and ensure fairness in NLP applications. For example, the mitigated toxicity classifiers can be used for online moderation in compliance with AI regulations.\nProtected attributes annotation. We advanced the literature in the protected attributes iden-tification in NLP, traditionally done with static and manually pre-defined dictionaries covering only a subset of categories. However, they are difficult to keep up-to-date, especially with the emergence of ever-evolving language trends and slang. In our framework, we demonstrated a novel approach to dynamically identify protected attributes through straightforward prompts to an LLM. This enables the creation of a comprehensive and up-to-date dictionary covering all the protected categories simultaneously, which can be updated periodically, ensuring its relevance in real-time linguistic landscapes.\nIn our research, we annotated 15,000 words, 540 labeled as protected attributes. We release our dictionary in the GitHub repo. It is more comprehensive, covers a broader range of protected categories than existing dictionaries, and can be continuously updated by exploiting our identifier. Researchers can use and enhance this resource to advance bias mitigation in NLP.\nHumans vs. LLM annotations. Building upon a recent finding [23], our study demonstrates that LLM annotation can outperform human-in-the-loop crowdsourcing annotations. Within our framework, we establish that LLM-based annotation of protected attributes proved to be more cost-effective and scalable, and aligns closely with expert annotations. This allows us to design a fully automated framework without human intervention. This finding opens new avenues for exploring the potential of LLMs as an effective tool for obtaining high-quality annotations."}, {"title": "Limitations and Future Directions", "content": "Our current approach has six main potential limitations or areas of concern.\nContext unawareness. Our Identifier and Moderator label and mitigate words related to pro-tected attributes without considering context, simplifying annotation but risking inaccuracies. For instance, \u2018black' may be a protected attribute in one context (\u201cIf you are a guy (black) or lesbian you get hired fast\u201d) but not in another (\u201cI bought a new black desk\u201d). Addressing context could enhance mitigation effectiveness yet poses challenges in the identification and mitigation phases. Human-in-the-loop context annotation is costly, requiring thousands of context-aware annota-tions, potentially increasing noise. Machine-in-the-loop is more scalable but complicates prompt engineering, potentially leading to misunderstandings and noisy responses [43]. We conducted a preliminary experiment assessing context-aware annotation's impact on identifying protected attributes with LLM. Repeating annotations with up to 10 context sentences, we found a 75% overlap between word-level and context-level annotations, with some contradictions, especially for long sentences. However, future research should explore this further across datasets.\nPotential bias introduced by the Identifier. The annotation of protected attributes is a subjective task. Therefore, the Identifier can potentially introduce further sources of bias. In human-in-the-loop settings, crowdworkers should come from various backgrounds to have a broader contextual understanding during the annotation process. The distribution of the demographic backgrounds of crowdworkers can have an impact on the annotated protected attributes. It is important to ensure an equitable distribution of crowdworkers across all protected categories. However, this can often be challenging in practice. Instead, in the machine-in-the-loop settings, the Identifier can introduce potential bias inherent in the LLM system adopted. The LLM can associate certain words with protected attributes based on stereotypes prevalent in the training data. However, addressing bias inherent in LLM is an active area of research expected to resolve numerous current limitations, significantly enhancing the effectiveness of our framework. Finally, introducing specific definitions of protected attributes, such as the nine categories defined by the UK Equality Act 2010, might also inadvertently introduce biases or overlook certain nuances in both human annotators and LLMs.\nReliance on XAI techniques. Our framework relies on XAI techniques to identify the most important words for the model. Nevertheless, it is important to acknowledge that XAI methods have inherent limitations [47, 65], including challenges like effective aggregation and normalization methods, and the contextual variability of words across different explanations. These limitations may hinder the accurate extraction of the most important words used by the model, affecting the Identifier in the identification of protected attributes that the model relies on to make predictions. This issue can extend to the Moderator, impacting the mitigated protected attributes. In the future, improvements in the XAI field could make our framework even more effective. This is because our framework is flexible and can use any feature importance explainability method that can be applied to a pre-trained classifier (e.g., Integrated Gradients or SHAP), as explained in \u00a73-A.\nDefined protected categories. Our framework annotates protected attributes based on the nine categories outlined in the Equality Act 2010. These categories represent a significant step toward addressing discrimination and promoting equality. However, they might not cover all aspects of human diversity or potential discrimination. Since they were formalized in 2010, some characteristics remain unaddressed by the Act. More than a decade later, initiatives are underway to broaden these categories for aspects like socio-economic status, health status, genetic heritage, and physical appearance [50]. Future extensions will further enhance the comprehensiveness of our framework in encompassing a broader range of protected categories. Notably, for the LLM-based annotation, incorporating new categories is a straightforward process that involves modifying the prompt.\nMitigation with small training datasets or common protected attributes. In scenarios with small or imbalanced training data, or when protected attributes are common in most input texts (e.g., 'he' and 'she' in biographies), sentence removal (MS1) may be less effective due to potential consequences of removing sentences containing protected attributes from already limited datasets. Frequent presence of common protected attributes in inputs may exclude most sentences, reducing available training data significantly. This reduction can cause a significant and unacceptable decrease in model accuracy. Hence, alternative strategies, like word-removal (MS2), should be considered. MS2 has similar effectiveness in mitigating protected attributes while maintaining predictive performance, offering flexibility across datasets without suffering from these issues.\nFairness-privacy tradeoff. Our approach neither protects the privacy of individuals nor considers words or sentences to be private. Instead, it focuses on constraining the classification so that does not rely on protected attributes. In the way that loans cannot be given by an automatic system that relies on racial backgrounds, natural language classification should not rely on protected attributes. The tradeoff between fairness and privacy becomes more pronounced when considering human- and machine-in-the-loop identifiers. The protected attributes annotation requires exposing textual sensitive information to individuals who are not necessarily trusted or to LLMs. This creates a risk of privacy violations and potential harm to the individuals whose sensitive information is being used [1]. The proposed methodology requires careful consideration of the privacy-fairness tradeoff.\nIn future work, we plan to develop a context-aware framework that would allow us to identify and mitigate protected attributes based on their context by extracting the words and context information from the dataset, identifying protected attributes within each context, and applying mitigation strategies only to those sentences that contain protected attributes in similar contexts."}, {"title": "APPENDIX", "content": "The Appendix sections are organized as follows. Appendix A shows the LLM prompts used by the Identifier to annotate words related to protected attributes. Appendix B shows the list of trap words used in the MTurk study. Appendix C qualitatively shows the fairness improvement of one mitigated model. Appendix D discusses the framework's running time.\nA. LLM prompts\nThe LLM prompts used by the Identifier component to annotate words related to protected attributes (as described in \u00a73-B2). A first prompt (Figure 6) provides the protected categories and their definitions. A second prompt (Figure 7) suggests some links that provide more information about the protected categories. Then, for each word, the LLM is asked to: (1) classify the word into one of the protected categories or none of them; (2) provide a reliability score in the range [0, 100]; and (3) provide an explanation. Figure 9 shows the response provided by GPT-3.5-Turbo for the annotation of the word 'homosexual', classified with the category sexual orientation and a score of 100/100.\nThe LLM-based Identifier is also able to annotate proxy words that, although not directly and strictly related to a protected attribute, can be used by the model to infer the categories. An example is shown in Figure 10, where the word \u2018headscarf is annotated as related to religion and belief.\nUSER: Consider these 9 protected categories defined by the Equality Act law to avoid discrimination of automatic decision-making algorithms:\n\"Age\": A person belonging to a particular age or range of ages (for example, teenagers).\n\"Disability\": A person has a disability if she or he has a physical or mental impairment which has a substantial and long-term adverse effect on that person's ability to carry out normal day-to-day activities.\n\"Gender reassignment\": The process of transitioning from one sex to another.\n\"Marriage and civil partnership\": Marriage is a union between a man and a woman or between a same-sex couple. Same-sex couples can also have their relationships legally recognised as 'civil partnerships'. Civil partners must not be treated less favourably than married couples.\n\"Pregnancy and maternity\": Pregnancy is the condition of being pregnant or expecting a baby. Maternity refers to the period after the birth, and is linked to maternity leave in the employment context. In the non-work context, protection against maternity discrimination is for 26 weeks after giving birth, and this includes treating a woman unfavourably because she is breastfeeding.\n\"Race\": Refers to the protected characteristic of race. It refers to a group of people defined by their race, colour, and nationality (including citizenship) ethnic or national origins.\n\"Religion and belief\": Religion refers to any religion, including a lack of religion. Belief refers to any religious or philosophical belief and includes a lack of belief. Generally, a belief should affect your life choices or the way you live for it to be included in the definition.\n\"Sex\": A man or a woman.\n\"Sexual orientation\": Whether a person's sexual attraction is towards their own sex, the opposite sex, or both sexes."}, {"title": "Trap words", "content": "Table 5 shows the list of trap words used in the MTurk for the Identifier in the machine-in-the-loop setup (\u00a74.3). They were chosen for their ability to be easily classified as toxic or non-toxic, and were used to detect random responses by MTurk participants. For the non-toxic (toxic) trap words, we expected MTurk participants to select a score of 1 or 2 (4 or 5) on the Likert scale. Participants were considered unreliable if they did not meet those expectations, and their assessments were discarded from our results."}, {"title": "Fairness improvement of a mitigated model: a qualitative analysis", "content": "Figure 11 and Figure 12 show the fairness improvement of a mitigated classifier for toxicity predictions in the in-distribution experiment (discussed in \u00a74.4). They show the prediction on the same texts discussed before (see Figure 1 and Figure 2 in \u00a71) made by one mitigated model (M). The first three sentences (misclassified by the original model) are not predicted as toxic anymore, as the model is not extensively using the words \u2018black', 'homosexual', and 'gay' for toxicity predictions anymore. The fourth sentence is still correctly predicted as toxic. However, the prediction is influenced by words such as \u2018hate', 'fucking', and \u2018shitty' and not by \u2018black' anymore. These results show that the removal-based mitigation strategies (MS1, MS2) are highly effective in reducing the usage of protected attributes in classification in just one mitigation round."}, {"title": "Framework's running time analysis", "content": "The execution time to produce the mitigated training dataset depends on many factors. Given a fixed model complexity, the execution times increase linearly with: (1) the unlabeled corpus size, (2) the number of most important words annotated, and (3) the training dataset size. Increasing the model's complexity results in a slight increase in the execution time of all components.\nWe report the execution time for the mitigation of the BERT model for the nurse occupation classification (discussed in \u00a75.4) with Integrated Gradients as the Explainer and ChatGPT-turbo-3.5 as the Identifier, using a single Nvidia RTX A6000 GPU. We used the test set as the unlabeled corpus, containing approximately 98.3K sentences. Firstly, the Explainer uses the original classifier on each input text of the unlabeled corpus. With a batch size of 512, it takes 846 seconds (0.01 seconds per text). Then, the Explainer generates the explanations within each sentence for all the texts predicted with the nurse occupation, in this case 4,071, and aggregates the scores to compile the overall list of the most important words. This process is completed in 725 seconds (0.18 seconds per text). Next, the Identifier annotates the most important 400 words, running in 534 seconds (1.3 seconds per word). Finally, producing the mitigated training dataset with the word removal mitigation strategy (MS2) of the Moderator on the 255.7k examples in the training set requires 29 seconds. The total execution time is 2,134 seconds (35 minutes).\nOur framework produces a mitigated training corpus, requiring an additional training phase to generate the mitigated model. The (re-)training time depends on the model's complexity and the original training data size. Some mitigation techniques (MS2, MS3, MS5) maintain the training dataset's dimensionality, resulting in equivalent training times for the mitigated and original models (1.5 hours in the previous example). In contrast, techniques like MS1 decrease or MS4 increase the dataset size, leading to corresponding changes in training time."}]}