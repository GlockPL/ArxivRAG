{"title": "Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering", "authors": ["Nick Ferguson", "Liane Guillou", "Alan Bundy", "Kwabena Nuamah"], "abstract": "Large Language Models (LLMs) excel in natural language\ntasks but still face challenges in Question Answering (QA)\ntasks requiring complex, multi-step reasoning. We outline the\ntypes of reasoning required in some of these tasks, and re-\nframe them in terms of meta-level reasoning (akin to high-\nlevel strategic reasoning or planning) and object-level rea-\nsoning (embodied in lower-level tasks such as mathemati-\ncal reasoning). FRANKLIN, a novel dataset with requirements\nof meta- and object-level reasoning, is introduced and used\nalong with three other datasets to evaluate four LLMs at\nquestion answering tasks requiring multiple steps of reason-\ning. Results from human annotation studies suggest LLMS\ndemonstrate meta-level reasoning with high frequency, but\nstruggle with object-level reasoning tasks in some of the\ndatasets used. Additionally, evidence suggests that LLMs find\nthe object-level reasoning required for the questions in the\nFRANKLIN dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning require-\nments.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have emerged as general-\npurpose natural-language-based task solvers. Earlier models\nsuch as BERT (Devlin et al. 2019) and GPT-2 (Radford et al.\n2019) demonstrated capability at tasks previously performed\nby task-specific models, such as sentiment analysis, while\ntoday's vastly increased model sizes and context windows\nmeans LLMs now find application in tasks which handle\nlarge amounts of data such as question answering (QA) over\nlarge volumes of data (Guu et al. 2020), and reading compre-\nhension (Ko\u010disk\u00fd et al. 2018). Alongside advances in model\nsize, turn-based conversation has become a key mode of in-\nteraction, with commercial products like ChatGPT bringing\nconsiderable non-expert attention to the field. QA is a pri-\nmary function of these LLM-based assistants, with research\nefforts shifting away from simpler factoid questions and to-\nwards more complex QA varieties which require reason-\ning in a human-like manner (e.g., StrategyQA (Geva et al.\n2021), CommonsenseQA (Talmor et al. 2019).) However,\nfor a model to perform well at these QA tasks, it is not nec-\nessarily enough to simply ask a given question of a model\nsupplementary techniques such as Chain-of-Thought (Wei\net al. 2022) are required to elicit more complex reasoning.\nIn this paper, we discuss the reasoning tasks expected of\nLLMs (section 2.1), and some of the methods for improv-\ning LLM performance on reasoning tasks in section 2.2. We\nintroduce our re-framing of the LLM reasoning discourse in\nsection 3.1 in terms of the ability of LLMs to demonstrate\nmeta- and object-level reasoning, and introduce our novel\nFRANKLIN dataset in section 3.2. We then introduce and\ndescrybe two annotation studies in section 4.1, which were\nconducted to evaluate the reasoning capabilities of a range\nof state-of-the-art LLMs described in section 4.3. These\nmodels were used to generate responses for a range of QA\ndatasets, whose requirements we re-frame in terms of meta-\nand object-level reasoning in 4.2. Our research questions,\nshown below, are addressed using results from our annota-\ntion studies in section 5.\nRQ1 Do LLMs demonstrate object-level reasoning?\nRQ2 Do LLMs demonstrate meta-level reasoning?\nRQ3 Does our novel FRANKLIN dataset present a challenge\nfor LLMs?\nOur contributions are:\n\u2022 A re-framing of existing LLM reasoning discourse in\nterms of meta- and object-level reasoning (section 3.1),\nenabling better classification of their strengths and weak-\nnesses.\n\u2022 The introduction of the novel FRANKLIN dataset, which\ncontains meta- and object-level reasoning requirements.\n\u2022 Evaluation of a range of state-of-the-art LLMs on\ndatasets requiring multi-step reasoning, and discussion of\nthe strengths and limitations of LLMs (section 4).\n\u2022 A claim that LLMs generally do not possess sufficient\nobject-level reasoning to widely succeed at the datasets\nevaluated.\n\u2022 An additional claim that LLMs are able to demonstrate\nmeta-level reasoning consistently across the variety of\ndatasets selected.\n\u2022 Finally, a claim that our FRANKLIN dataset presents a\nchallenge to LLMs through discussion of the low rates\nwith which answers are provided, and the types of errors\nthat LLMs make."}, {"title": "2 Background", "content": "We overview a selected range of reasoning tasks which\nLLMs are evaluated against, and techniques by which in-\ncreased performance is extracted from LLMs."}, {"title": "2.1 Reasoning Tasks", "content": "The term reasoning is a broad cognitive concept with many\nforms. Reasoning encompasses the drawing of a conclu-\nsion, using logical laws, from a set of statements (formally\n(Bundy 1983) or informally (Wason and Johnson-Laird\n1972)); incorporates elements of attitude revision (McHugh\nand Way 2018); and may be intuitive or explicit (Sloman\n1996). Our working definition of reasoning, aiming to take\ninto account the broad range of tasks to which the term is\napplied, is a task which requires some operation to infer\nconclusions from a set of premises. The range of tasks on\nwhich LLMs are evaluated represents the breadth of appli-\ncation of the term, with key tasks including common sense\nreasoning, mathematical reasoning, and symbolic reasoning.\nWe are particularly interested in multi-step reasoning, which\nrequires multiple intermediate steps of inference to draw a\nfinal conclusion.\nCommon sense reasoning concerns knowledge about ev-\neryday concepts which is generally accepted by a majority\nof people (Bhargava and Ng 2022). While the notion of a\nformal common sense logic does exist (Booth, Meyer, and\nVarzinczak 2012), we will discuss informal common sense\nreasoning grounded in natural language. LLMs have been\nshown to reflect human beliefs about generic concepts across\na range of domains (Weir, Poliak, and Durme 2020), and rea-\nson about physical properties of everyday objects and situa-\ntions (Bisk et al. 2020; Goel, Feng, and Boyd-Graber 2019).\nSimilarly, LLMs encode relational data, allowing recalling\nof facts in a similar manner to symbolic knowledge bases\n(KBs) (Petroni et al. 2019). These instances reflect simple\ntasks where the knowledge, implicit in the parameters of\nan LLM, can be recalled. However, common sense reason-\ning can also be a requirement for some multi-step reasoning\ntasks, such as the creation of a strategy for answering a ques-\ntion which requires multiple inferences (Geva et al. 2021).\nAs we will see, natural language-based common sense rea-\nsoning is a requirement for a variety of tasks which require\nmultiple steps of inference to achieve a wider goal.\nMathematical reasoning concerns a model's ability to per-\nform mathematical operations to solve problems (Ahn et al.\n2024). Specific tasks include arithmetic reasoning, such as\naddition and division, which can be expressed simply in\nsymbolic form (Yuan et al. 2023), or in longer-form, text-\nbased problems (Cobbe et al. 2021; Hendrycks et al. 2021).\nGeometry problems, which represent a conceptually harder\nchallenge, are another example of problems requiring math-\nematical reasoning (Chen et al. 2021).\nSymbolic reasoning tasks involve performing an action\naccording to formal rules, albeit imitated using the prompt-\ning and output of an LLM. This is a broader task than the\nmathematical reasoning task, which encompasses arithmetic\nand polynomial evaluation. Wei et al. (2022) describe two\ntasks which illustrate the challenge, although it is noted that\nthese toy tasks are within current abilities of LLMs. In last\nletter concatenation, a model concatenates the last letters of\na full name (e.g., Barack Obama \u2192 ka), and in coin flipping,\nmodels are prompted to output the state of the coin after\ngiven an initial state and a number of flips. Other examples\ninclude the emulation of formal deductive reasoning in nat-\nural language (Han et al. 2024; Clark, Tafjord, and Richard-\nson 2020). Some symbolic reasoning tasks are presented to\nthe model not expressed in natural language, but symboli-\ncally. For example, finding checkmate in a chess game \u2013 one\nof the many symbolic reasoning tasks in BIG-bench (BIG-\nbench 2023).\nThere is debate over whether LLMs are actually reasoning\nrather than emulating or imitating it, which itself is part of\na wider debate of whether LLMs truly understand language\nand meaning. Even when LLMs appear to perform reason-\ning tasks, it is not clear that they are reliant upon reasoning\n(Wei et al. 2022), or if they simply using heuristics to make\npredictions (Patel, Bhattamishra, and Goyal 2021).\nMany of these reasoning tasks are exemplified in QA\ndatasets, against which LLMs are evaluated. They also do\nnot exist in isolation, and often require multiple steps of in-\nference. For example, multi-step common sense reasoning\nis employed to infer the steps required for solving the nat-\nural language problems in the GSM8k (Cobbe et al. 2021)\nand MATH (Hendrycks et al. 2021) datasets. Similarly, basic\nmathematical reasoning about, for example whether a given\ndate is before or after another date, is required for many\nquestions in the StrategyQA dataset (Geva et al. 2021)."}, {"title": "2.2 Improving LLM Performance at Reasoning\nTasks", "content": "Techniques for achieving improved performance with LLMs\non reasoning tasks can broadly be placed into two cate-\ngories: fine-tuning and prompt-based approaches.\nFine-tuning is a paradigm which involves taking a pre-\ntrained model and updating the model's parameters by fur-\nther training on a specific dataset (Devlin et al. 2019). Fine-\ntuning is performative in increasing the performance of\nLLMs in a variety of reasoning techniques, such as arith-\nmetic reasoning Cobbe et al. (2021); Hendrycks et al. (2021)\nand common sense QA tasks (Talmor et al. 2019).\nPrompting-based techniques, sometimes referred to as in-\ncontext learning\u00b9 involve taking a model's frozen weights\nand manipulating the content of the prompt to \u2018externalise'\nthe model's reasoning in the form of natural language.\nChain-of-Thought (CoT, Wei et al. (2022)) involves insert-\ning intermediate natural language reasoning steps into the\nprocess of solving common sense, mathematical, and sym-\nbolic reasoning tasks. A variety of closely-related techniques\nhave since appeared and shown to increase the performance\nof models on reasoning tasks. Such techniques generally\ninvolve forcing a model to generate the intermediate rea-\nsoning steps required to compute the solution to a multi-\nstep reasoning problem. Examples include appending \"Let's\nthink step-by-step\" to the end of a question (Kojima et al.\n2022); instructing models to decompose the problem into"}, {"title": "3 Reframing the Reasoning Task", "content": "We now describe meta- and object-level reasoning and their\nrelevance to the task at hand, namely, multi-step QA with\nLLMs. Then, building on these distinct reasoning types, we\nintroduce a novel dataset, FRANKLIN, which is inspired by\nthe FRANK system, a QA system which employs meta- and\nobject-level reasoning to infer answers to queries."}, {"title": "3.1 Meta- and Object-Level Reasoning", "content": "Meta- and object-level reasoning are terms associated with\nsymbolic AI, particularly the automated reasoning and proof\nplanning domains. We will first describe a range of defi-\nnitions of these two concepts to build up a picture of their\nmeaning.\nFormally, in automated reasoning, meta-level reasoning\nrefers to the reasoning about the representation of a the-\nory, while the theory itself is at the object-level (Bundy\n1983). Bundy, Byrd, and Luger (1979) uses meta-level in-\nference to control the search of a solution to mechanics\nproblems phrased in natural language, while object-level\ninference is used to compute the steps of the solution it-\nself. Christodoulou and Keravnou (1998) describes the role\nof meta-level reasoning as planning problem-solving strate-\ngies, controlling the use of different problem solvers (which\ncan be thought of as object-level reasoning components),\nand notes the use of meta-level reasoning in adapting a strat-\negy to new knowledge which may arise during computa-\ntion. Aiello, Nardi, and Schaerf (1991) describe meta-level\nreasoning as reasoning about reasoning, and also note its\nfunctionality in driving search strategies and the modifica-\ntion of a system's own behaviour. They also, in the context\nof an agent-based system, distinguish the meta- and object-\nlevels by stating that agents' world knowledge is on the\nobject-level, while meta-level knowledge governs the links\nbetween different agents. Genesereth (1983) distinguishes\nthe actions of an AI system as base-level (or, object-level)\nand meta-level. Object-level actions achieve the program's\ngoals, while meta-level actions decide which object-level ac-\ntions to perform. Nuamah and Bundy (2023) introduce a for-\nmalism for representing knowledge in a QA system consist-\ning of attribute-value pairs. This formalism introduces ad-\nditional attributes to the standard (subject,predicate,object)\ntriple, which may be meta- or object-level attributes. Object-\nlevel attributes are those which encode the meaning of a fac-\ntual statement, such as subject and predicate, while meta-\nlevel attributes capture meta-information, such as the data\nsource for a given fact.\nTo summarise the above examples, meta-level reason-\ning approximately corresponds to the high-level planning\nof a solution to a problem, the decomposition of a prob-\nlem into intermediate steps, and the decisions on which sub-\ncomponents of a system to employ to achieve a specific task.\nReasoning on the object-level concerns the application of the\nsub-components. This includes lower-level inferences, such\nas mathematical operations or natural language deductions,\nwhich are required to execute the intermediate steps.\nWe find that this delineation of reasoning tasks provides\nmeaningful detail and structure to the discourse and classi-\nfication of the reasoning tasks embodied in multi-step QA\ndatasets on which LLMs are evaluated. Taking GSM8k as\nan example, it is described as embodying a single reason-\ning task, namely mathematical reasoning. However, cur-\nsory analysis of the problems contained within the dataset\nshow that both meta- and object-level reasoning are required\nto correctly compute answers to the questions. In section\n4.2 we describe further examples of datasets which require\nmeta- and object-level reasoning, showing that our categori-\nsation of reasoning types as meta- or object-level generalises\nto a range of QA tasks, in addition to adding more fine-\ngrained meaning. Questions in these QA datasets are the ba-\nsis for our annotation studies and evaluation of the meta- and\nobject-level reasoning of the range of LLMs selected in 4.3.\nHowever, in conducting our studies on the ability of LLMs\nto demonstrate meta- and object-level reasoning, we do not\nclaim here that LLMs have any formal meta- or object-level\nreasoning component, and stress here that when applying\nthese terms to the evaluation of LLMs, we are not evalu-\nating a formal meta- or object-level reasoning component.\nRather, when we refer to LLMs as demonstrating meta- or\nobject-level reasoning, we refer to their ability to emulate,\nor imitate such processes via their text generation paradigm.\nOur interpretation of the terms meta- and object-level rea-\nsoning is summarised below.\nMeta-level reasoning High-level planning. With LLMs,\nthis is demonstrated and embodied in an informal, nat-\nural language-based decomposition of a problem in to\nsub-problems or intermediate steps.\nObject-level reasoning Low-level execution. With LLMs,\nthis is demonstrated in the execution of intermediate\nsteps created by the meta-level reasoning process. Exe-\ncution of these steps may require a specific task, for ex-\nample, mathematical reasoning.\nIt is from this characterisation which two of our research\nquestions, introduced in section 1, are drawn. We revisit\nthem here and give further detail using our above definitions.\nRQ1 Do LLMs demonstrate object-level reasoning?\nObject-level reasoning involves low-level inferences,\nsuch as the execution of mathematical operations or"}, {"title": "3.2 Introducing FRANKLIN", "content": "The FRANK System To give further example of meta- and\nobject-level reasoning processes in a QA setting, we will\nrefer to the FRANK system (Nuamah and Bundy 2020) as\nan example. FRANK is a QA system in the form of a sym-\nbolic reasoning framework which employs meta- and object-\nlevel reasoning in the form of a set of rules (Bundy and\nNuamah 2022). These rules break queries down into sub-\nproblems; collect data from online knowledge sources such\nas Wikidata, and apply mathematical operations over that\ndata. In FRANK, meta-level reasoning governs the high-level\napproach to answering a question, including the deduction\nof which intermediate inferences and operations are neces-\nsary. Object-level reasoning manifests in both the queries\nto knowledge bases, and in the mathematical operations ap-\nplied to the data returned from knowledge bases (with the\ndecision to use such operations taking place at the meta-\nlevel.) Multiple lines of reasoning may be explored by the\nsystem before a final solution is assembled \u2013 reasoning is dy-\nnamic at inference time, and not pre-determined for a given\nquestion type.\nFRANK's functionality illustrated using questions con-\ncerning the values of geopolitical indicators belonging to\ndifferent countries and regions at various points in time. As\nan example, consider the question: \u201cWhich country in Africa\nhad the lowest population in 2012?", "What is\nthe capital of Ghana?": "which is simply a single fact that\ncan be looked up. One solution FRANK may explore is to\nsplit Africa into constituent countries, search for their popu-\nlations in 2012, and compare values to find an answer.\nThe symbolic nature of this system does lead to limita-\ntions, generally as a result of the levels of hand-engineering\nrequired. Although solutions are not hard-coded, rules which\ndecompose queries, perform information retrieval, and ag-\ngregate data do require hand-engineering. This lends part of\nthe overall motivation to the project: exploring the capability\nof LLMs at functionality that can be performed by explicit\nsymbolic components.\nThe FRANKLIN Dataset Given this task of meta- and\nobject-level reasoning, we introduce a novel dataset inspired\nby the FRANK system and its exemplar domain of geopo-\nlitical indicators. This dataset, which we call FRANKLIN"}, {"title": "4 Experiment Design", "content": "In this section, we describe the annotation studies which\nwere conducted. We describe the content of the studies\nthemselves; list the datasets and models which were used\nto generate materials for the study; and finally our evalua-\ntion metrics which we use to evaluate our research questions\nin section 5."}, {"title": "4.1 Annotation Studies", "content": "We firstly describe the design of the studies themselves, in-\ncluding the questions which were presented to participants,\nand the size of the studies in terms of the number of exam-\nples annotated.\nStudy Design Two online human annotation studies in\nwhich we evaluated the ability of a range of LLMs to demon-\nstrate meta- and object-level reasoning. Four datasets, de-\nscribed in 4.2 were selected. Responses to a random sample\nof the questions in these datasets were generated using four\nLLMs, introduced in section 4.3. For study 1, we prompted\nmodels to generate answers to a given question, with the in-\ntention of observing object-level ability. For study 2, we first\nprompted models to generate plans for finding an answer\nto a given question, and followed up with an instruction to\nexecute the plan step-by-step. This study was designed to\nobserve the models' meta-level reasoning ability, and also\nthe influence of the breaking down of a problem on their\nability to produce answers. Full details of the prompts used\nare given in appendix A. In both annotation studies (built\nwith Qualtrics\u00b3), we asked human participants sourced from\nProlific to answer questions about models' responses on a\n5-point Likert scale: strongly disagree, somewhat disagree,\nneither agree nor disagree, somewhat agree, and strongly\nagree. In study 1, for each question, participants are required\nto respond to the statements below.\n1. The response contains an answer to the question.\n2. The response contains a clear step-by-step plan.\n3. I would be satisfied with the response if I had asked the\nquestion.\nSimilarly, the below list shows the statements presented\nfor each example in study 2.\n1. The response takes a rational approach to answering the\nquestion.\n2. The response contains an answer to the question."}, {"title": "4.2 Dataset selection", "content": "We selected datasets embodying tasks which require both\nmeta- and object-level reasoning to perform.\nGSM8k (Cobbe et al. 2021) contains grade school math-\nematics problems formulated in natural language, requiring\nmeta-level reasoning to plan the step-by-step approach, and\nobject-level reasoning to perform the arithmetic itself. Strat-\negyQA (Geva et al. 2021) contains questions in which the in-\nference requirements are said to be implicit in the question.\nMeta-level reasoning is required to decompose the prob-\nlem and decide which intermediate inferences are neces-\nsary, while object-level reasoning is required to make de-\nductions about relevant facts. HotpotQA (Yang et al. 2018)\nrequires meta-level reasoning to plan the intermediate steps\nin answering the question, and object-level reasoning to\nsynthesise facts into an answer. We also include the novel\nFRANKLIN dataset, introduced above, which requires meta-\nlevel reasoning to decompose problems into sub-problems,\nand object-level reasoning to retrieve information and per-\nform mathematical reasoning. An example of the questions\nin each dataset is given in figure 3.\nTo the best of our knowledge, according to the models'\nwhite papers (Abdin et al. 2024; Dubey et al. 2024; Team\net al. 2024), none of these datasets were part of a given\nmodel's pre-training or fine-tuning data, and therefore, not\nused to train the models themselves."}, {"title": "4.3 Model selection", "content": "Four off-the-shelf, pre-trained models were used without\nfine-tuning. Meta's Llama 3.1 8B (Dubey et al. 2024), Mi-\ncrosoft's Phi 3.5 Mini (Abdin et al. 2024), and Google's\nGemma 2 9B (Team et al. 2024) were selected as examples\nof popular, performative, open-source models targeted to-\nwards QA and reasoning. OpenAI's GPT-40-mini, a closed-"}, {"title": "4.4 Metrics", "content": "In assessing our claims, we refer to the metrics outlined here.\nAnswer Failure Rate (AFR) In studies 1 and 2, we asked\nparticipants to indicate whether a given response contained\nan answer to the question at hand (shown in questions 1\nand 2 for studies 1 and 2 respectively). The AFR is derived\nfrom these results. It shows, for a given model/dataset com-\nbination, the proportion of questions which contain no at-\ntempted answer to the question at hand. We focus on AFR\nrather than a standard accuracy metric because we want to\nobserve an upper bound for a model's object-level reason-\ning ability we are less interested in absolute performance\non a dataset, more so in making a comment about whether\nsufficient object-level reasoning is demonstrated.\nTo arrive at the AFR for the responses for each model/-\ndataset combination, we took the following approach. For\neach set of four annotations for a given response, we mapped\nthe ratings on the 5-point Likert scale to a 3-point scale rep-\nresenting strongly/somewhat disagree, neither agree nor dis-\nagree, and strongly/somewhat agree. We then took a major-\nity vote of these 3-point ratings to achieve a single verdict for\ngiven response. The proportion of non-strongly/somewhat\nagree verdicts gives the AFR.\nRational Approach Rate (RAR) In study 2, the focus is\non the ability of LLMs to generate plans and, if possible,"}, {"title": "5 Results and Discussion", "content": "We now bring together our findings from our annotation\nstudies, addressing our research questions using the metrics\ndefined in section 4.4."}, {"title": "5.1 Do LLMs demonstrate object-level\nreasoning?", "content": "Table 3 shows AFR for studies 1 and 2. The left number\ndenotes AFR from the study 1, where LLMs provided only\nan answer to the question. The right figure (shaded) denotes\nAFR from study 2, where LLMs were instructed to create a\nplan before executing that plan to answer the question.\nThe figures show that answers were frequently not present\nfor a variety of model/dataset combinations. In the study 1\nsetting, models overall found FRANKLIN the harder of the\ndatasets, with GPT 40-mini performing best with an AFR\nof 53%. Gemma 2's AFR of 88% on the FRANKLIN dataset\nwas the worst of any model/dataset combination. GSM8k,\nwith its simple arithmetic and verbose question formats, was\ncomparatively easy compared to other datasets, with mod-\nels (except in the case of Phi 3.5) failing to provide an-\nswers for less than 10% of responses. HotpotQA and Strat-\negyQA, with their text-based common sense knowledge re-\nquirements, occupied a middle-ground in terms of difficulty.\nAFR is consistently lower in the setting of study 2, in-\ndicating that the generation of a plan enabled models at-\ntempt answers more frequently. This result generally aligns\nwith the results of Chain-of-Thought-adjacent work, in\nwhich models are found to achieve better performance when\nprompted to decompose problems. The exception to this de-\ncreased AFR was GPT 40-mini, which did create plans for\nthe questions, when asked, but specifically declined to exe-\ncute that plan. We hypothesise that this is the result of safety\n'guardrails' being put in place by OpenAI.\nWe conclude by claiming that there is preliminary evi-\ndence that, while instructing the model to perform meta-\nlevel reasoning before answering the question results in\nlower AFR, models did not sufficiently, or consistently,\ndemonstrate high levels of object-level reasoning across\nthe range of multi-step question answering datasets."}, {"title": "5.2 Do LLMs demonstrate meta-level reasoning?", "content": "To answer this question, we make use of our RAR and\nPCR metrics described in 4.4. Table 4 shows RAR and PCR\nacross model/dataset combinations.\nResults at this meta-level reasoning task show both\nstronger, and more consistent levels of performance at\nthe meta-level reasoning task, frequently over 95% for\nmany model/dataset combinations, even for the FRANKLIN\ndataset. As described in section 4.4 reported RAR to pro-\nvide an indication that, even if the model does not produce\na step-by-step plan, it still approaches the problem in a ra-\ntional manner according to our annotators. This way, we\nhave evidence that models possess sufficient meta-level rea-\nsoning to approach the problem in an interpretable, human-\nunderstandable manner and we can explore ways of im-\nposing greater structure on this in future work.\nIn contrast to the results for AFR in table 3, results for\nRAR and PCR appear to suggest that models are very com-\npetent in generating solutions to problems which take an ap-\nproach which humans rate as rational, and they are similarly\ncapable of structuring this approach in a step by step manner.\nIn many cases, models were able to do this for all examples\nin a dataset, such as in the case of Phi 3.5 on the GSM8k\ndataset. Although above we suggested that the object-level\nreasoning in GSM8k was easier for models due its simple\narithmetic and verbose questions, and that FRANKLIN was a\nharder task, we see similar levels of very high competence\nat the meta-level reasoning task across the range of datasets.\nAgain, we point out that there is speculation about\nwhether LLMs are actually reasoning, rather than simply\nimitating it (Wei et al. 2022). We share this scepticism of\nmodels' abilities to formally reason at the meta-level and do\nnot claim that models possess any kind of implicit, under-\nlying, symbolic representation which this process is being\ncompleted by. However, we believe that our results sug-\ngest that models are able to imitate meta-level reasoning\nin their text generation paradigm."}, {"title": "5.3 Does the FRANKLIN dataset present a\nchallenge for LLMs?", "content": "Table 3 shows that LLMs clearly struggled with questions\nfrom FRANKLIN without first being prompted for a plan,"}, {"title": "6 Conclusion", "content": "In this paper, we have outlined reasoning tasks on which\nLLMs are evaluated, which are grounded in the overall set-\nting of multi-step question answering. These reasoning tasks\nhave been re-framed in terms of meta- and object-level rea-\nsoning to allow us to better characterise the strengths and\nlimitations of LLMs at these tasks. We also introduced the\nnovel FRANKLIN dataset, which requires meta- and object-\nlevel reasoning, and which we release to the community in\na proof of concept size. Through two annotation studies, us-\ning FRANKLIN and three other QA datasets requiring meta-\nand object-level reasoning, we show that LLMs lack suffi-\ncient object-level reasoning to frequently provide answers\nto questions requiring object-level reasoning. However, we\nclaim that LLMs are able to sufficiently emulate meta-level\nreasoning in order to produce plans for answering such ques-\ntions, even for the FRANKLIN dataset. However, the object-\nlevel reasoning requirements of the FRANKLIN dataset were\na challenge for LLMs, as demonstrated through a range of\nerror modes. Based on these findings, we plan continued\ndevelopment of FRANKLIN, and evaluation of FRANKLIN\non LLMs both off-the-shelf and fine-tuned, as well as at\nlarger parameter counts. These developments will consist\nof a broader range of question types, along with exam-\nple meta- and object-level reasoning reasoning steps against\nwhich LLMs can be evaluated. This work will enable us to\nmake stronger claims about the ability of LLMs at meta- and\nobject-level reasoning. The highlighting of the weaknesses\nof LLMs in this and future studies will allow more targeted"}]}