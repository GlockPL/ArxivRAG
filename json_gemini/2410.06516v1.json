{"title": "QuadBEV: An Efficient Quadruple-Task Perception Framework via Birds'-Eye-View Representation", "authors": ["Yuxin Li", "Yiheng Li", "Xulei Yang", "Mengying Yu", "Zihang Huang", "Xiaojun Wu", "Chaikiat Yeo"], "abstract": "Birds'-Eye-View (BEV) perception has become a vital component of autonomous driving systems due to its ability to integrate multiple sensor inputs into a unified representation, enhancing performance in various downstream tasks. However, the computational demands of BEV models pose challenges for real-world deployment in vehicles with limited resources. To address these limitations, we propose QuadBEV, an efficient multitask perception framework that leverages the shared spatial and contextual information across four key tasks: 3D object detection, lane detection, map segmentation, and occupancy prediction. QuadBEV not only streamlines the integration of these tasks using a shared backbone and task-specific heads but also addresses common multitask learning challenges such as learning rate sensitivity and conflicting task objectives. Our framework reduces redundant computations, thereby enhancing system efficiency, making it particularly suited for embedded systems. We present comprehensive experiments that validate the effectiveness and robustness of QuadBEV, demonstrating its suitability for real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Bird's-eye-view (BEV) perception is increasingly recog-nized as an essential technology within autonomous driving systems. By fusing data from multiple sensors, BEV tech-niques [1], [2], [3] provide a comprehensive, top-down view representation, which enhanced environmental perception across various tasks [4], [5], [6], [7]. However, the com-putational intensity of traditional BEV methods often limits their deployment in vehicles with restricted computational resources, underscoring the need for efficient BEV-based perception frameworks that can handle multiple tasks without compromising performance.\nRecent research has underscored the potential of multitask learning in autonomous driving. While a few methods [8], [9] attempted to integrate multiple tasks into a unified frame-work, these efforts often prioritize model complexity over efficiency and have been limited to combining only two tasks. Tasks such as 3D object detection, lane detection, occupancy prediction, and map segmentation share substantial spatial and contextual information, suggesting significant untapped potential within multitask learning. A unified framework could lead to several benefits:\nMutual Information Exchange: Features learned for one task can potentially enhance the performance of others. For instance, accurately detected lanes can improve the precision of object localization, while object detection can aid in identifying lane boundaries or occupancy regions.\nComputational Efficiency: By employing shared feature representations, a multitask framework can decrease redun-dant computations, thereby enhancing system efficiency and suitability for embedded systems.\nDespite these potential benefits, progress in multitask perception frameworks remains limited. The integration of multiple perception tasks within a single framework intro-duces significant challenges, notably:\nLearning Rate Sensitivity: Different tasks may respond variably to the same learning rates, where an optimal rate for one could impede another's performance.\nConflicting Task Objectives: Each task may require em-phasis on different feature aspects, potentially leading to conflicts during training. For example, precise localization needed for object detection may conflict with the broader spatial understanding required for map segmentation or oc-cupancy prediction.\nIn response to these challenges, we propose QuadBEV, an efficient, quadruple-task perception framework utilizing BEV representation. QuadBEV innovatively integrates four critical tasks: 3D object detection, lane detection, map segmentation, and occupancy prediction. This framework advocates for a streamlined structure that combines task-specific heads with a shared backbone, effectively utilizing BEV capabilities while simplifying the integration process through strategic multi-source dataset alignment and a tailored training regi-men designed to manage task sequence and loss conflicts.\nOur key contributions can be summarized as follows:\n1) Multitask Architecture: We introduce a pioneering framework that comprehensively addresses four fun-damental tasks in autonomous driving within a BEV framework: 3D object detection, lane detection, map segmentation, and occupancy prediction.\n2) Progressive Training Strategy: We employ a care-fully crafted training strategy with staged learning rate adjustments and a gradient-based loss balancing technique to facilitate balanced learning across diverse tasks.\n3) Experimental Validation: Extensive testing validates the efficacy and robustness of QuadBEV, confirming its potential applicability in real-world autonomous driving scenarios."}, {"title": "II. RELATED WORK", "content": "This section discusses key advancements and challenges in BEV perception and multi-task learning, essential for progress in autonomous driving technologies.\nBird's-Eye-View Perception The BEV approach has gained considerable traction in sensor fusion applications, offering a novel methodology by integrating multiple sensor inputs within a pseudo-3D BEV framework. This innovative fusion technique transforms the traditional sensor align-ment challenge into a manageable data-centric learning task, significantly enhancing the efficacy of various perception processes. Herein, we describe seminal contributions across distinct perception tasks within this domain.\n3D Detection in BEV 3D object detection, crucial for autonomous navigation, has evolved significantly with the in-troduction of pioneer BEV models [1], [10], which integrate temporal dynamics and multi-view data to improve detection accuracy. Successive models such as [11], [12] have focused on leveraging depth cues and anticipatory frame analysis to refine detection capabilities. Later developments like [13], [14] have aimed to improve object interaction understanding in sparse environments, bringing vision-based systems on par with LiDAR-based alternatives.\nSegmentation in BEV BEV map segmentation has also advanced with approaches like HDMapNet [15], introducing vectorized element prediction, and BEVSegFormer [16], utilizing transformer architecture for distortion-resistant real-time segmentation. Further innovations by VectorMapNet [17] and MapTR [5] have enhanced element alignment and inference speed, pushing the boundaries toward effective mapless navigation.\nLane Detection in BEV Lane detection has transitioned from 2D to BEV-based methods, with developments such as [18], [19] enhancing lane line prediction in 3D coordinates to address surface irregularities, signifying a shift towards a three-dimensional understanding of road structures.\nOccupancy Prediction in BEV Occupancy prediction is increasingly critical for autonomous driving, particularly for identifying atypical long-tail objects. Recent approaches like [20], [21], [22], [6] have adopted voxel-based 3D space dis-cretization for robust occupancy prediction, complementing traditional perception methods and offering reliable alterna-tives in complex scenarios.\nMulti-Task Learning Multi-task learning, a longstanding topic of interest, strives to mitigate the complexities of de-signing network architectures capable of handling concurrent tasks. Within autonomous perception, multi-task learning is strategically utilized to reduce computational demands and minimize task interference, adhering to the strict re-source constraints of such systems. The spectrum of methods range from hard and soft parameter [23], [24] sharing to cross-stitch networks [25], task grouping [26], [27], routing methodologies, and unified end-to-end training techniques [28]. In the autonomous perception landscape, there is a call for more sophisticated integration. Preliminary efforts have attempted to unify tasks like map segmentation and object detection, yet a comprehensive integration within the BEV paradigm remains a promising research frontier."}, {"title": "III. METHODOLOGY", "content": "Our methodology centres around a multi-task learning framework tailored for the diverse perception tasks in the BEV space. The architecture, training method, loss design, and evaluation metrics are carefully crafted to address the challenges and needs of autonomous driving."}, {"title": "A. Model Architecture", "content": "As illustrated in Figure 1, our architecture comprises six distinct modules, which are categorized into two primary groups: Shared Feature Extractors and Task-Specific Heads.\nShared Feature Extractors These extractors are pivotal in synthesizing dense BEV semantic feature maps from multiview camera inputs. Initially, a convolutional-only back-bone model [29] is applied to process the inputs iteratively. This is followed by the application of a view converter combining [2], [7] to transition from 2D to a pseudo-3D domain. Parallel to that, a fully-convolutional depth estimator is applied to refine the depth value in 3D space. Subsequent temporal information fusion is achieved via a concatenation module [10], which is then further processed by another convolutional encoder [8] to refine and finalize the BEV feature map.\nTask-Specific Heads Overall Architecture: The archi-tecture of our model comprises four distinct task-specific heads, each connected to a shared BEV feature map. As depicted in Fig. 2, these heads are arranged in a round-robin configuration without specialized routing architectures for different tasks.\nConvolutional Encoders Each of the four task-specific heads is preceded by an identical yet independent convo-lutional encoder, tailored to refine the BEV features into more nuanced, task-sensitive features. These encoders are intentionally lightweight and consist of three convolutional layers, with slight variations in the number of input and output channels to meet the specific resolution requirements of each downstream task.\n3D Object Detection: This head aims to determine the center, scale, and orientation of objects within the scene. The design follows the configuration of CenterPoint [30], using a stack of convolutional modules to predict object parameters such as position (x, y, z), dimensions (w, h, l), and yaw angle.\nMap Segmentation: The map segmentation head is tasked with creating masks for various road elements, framing the problem as a semantic segmentation task. This head outputs multiple binary masks, one for each class, using three convolutional layers, similarly structured to the segmentation branch of BEVFusion [8].\nLane Detection: Focused on identifying lane boundaries, this head utilizes four layers, both deconvolutional and con-volutional, to process information related to the confidence, offset, embedding, and class of lane markers, mirroring the architecture used in BEVLaneDet [18].\nOccupancy Prediction: Addressing scenarios where 3D object detection might fail, such as with out-of-vocabulary or irregularly shaped objects, this head predicts the occupancy status of voxels in 3D space. It comprises a final convolu-tional layer followed by two MLP layers, configured as per the specifications in FlashOcc [6]."}, {"title": "B. Training Method", "content": "Our training methodology is structured into three distinct phases, each designed to progressively refine the model's capabilities across multiple tasks:\nFeature Extractor Pretraining: Initially, we focus on developing a robust feature extractor by training an end-to-end model solely with the map segmentation task head using the NuImages dataset [31] for M epochs. This phase is designed to cultivate a robust feature extractor that benefits all subsequent tasks by establishing a strong foundational representation.\nSequential Training with Multitask Warm-Up: Subse-quently, we stabilize the feature extraction layers, specif-ically, the backbone, depth estimation, and BEV encoder modules, by freezing their parameters. Training then pro-ceeds with all four task-specific heads attached, although they are categorized into primary and auxiliary roles. The primary task head is trained at a standard learning rate,"}, {"title": "C. Loss Design", "content": "Given the diversity and complexity of the tasks within our framework, an integrated and multifaceted loss function is crucial. The combined loss function, $L_{combined}$, encapsulates the contribution of each task-specific loss component, weighted appropriately to reflect their relative importance during training:\n$L_{combined} = \\alpha L_{3D} + \\beta L_{map} + \\gamma L_{lane} + \\delta L_{occ} + \\epsilon L_{depth}$\nDetails of Loss Components\n3D Object Detection Loss ($L_{3D}$): This component is crucial for the accurate localization and classification of 3D objects. It is defined as a combination of classification, regression, and IoU losses:\n$L_{3D} = \\lambda_1 L_{cls} + \\lambda_2 L_{reg} + \\lambda_3 L_{iou}$\nwhere $L_{cls}$ employs binary cross-entropy (BCE) loss for object presence, $L_{reg}$ utilizes L1 loss for bounding box regression, and $L_{iou}$ incorporates IoU loss to enhance the overlap accuracy between predicted and actual object bound-aries, as described in [44].\nMap Segmentation Loss ($L_{map}$): For the map segmentation task, focal loss [45] is used to refine the prediction of various map elements, focusing on reducing the imbalance between the more and less frequent classes.\nLane Prediction Loss ($L_{lane}$): This loss facilitates the detection of lane markers and is articulated as:\n$L_{lane} = \\lambda_1 L_{conf} + \\lambda_2 L_{offset} + \\lambda_3 L_{emb} + \\lambda_4 L_{cls}$\nHere, $L_{conf}$ uses BCE loss for marker confidence, $L_{offset}$ applies mean squared error (MSE) loss for spatial devia-tions, $L_{emb}$ employs a PushPull loss to distinguish between different lane markers, and $L_{cls}$ leverages cross-entropy loss for classification tasks within lane detection [18].\nOccupancy Prediction Loss ($L_{occ}$): Cross-entropy loss is utilized to supervise the prediction of occupancy within a voxel grid, aiming to detect the presence of objects.\nDepth Prediction Loss ($L_{depth}$): This loss uses binary cross-entropy loss to guide the depth estimation process, which is vital for accurate 3D understanding from 2D inputs.\nDynamic Weighting: The coefficients $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, and $\\epsilon$ balance the contributions of each loss component. Initially set to equal values, these weights are dynamically adjusted during training using the GradNorm algorithm [24], which helps maintain task balance and ensure uniform learning progress across all tasks."}, {"title": "IV. EXPERIMENTS", "content": "To demonstrate the effectiveness of our multitask archi-tecture, we have conducted comprehensive experiments on public datasets and compared the results with other state-of-the-art (SOTA) methods.\nTo evaluate our methodology, we employed three publicly available datasets. NuScenes [31], containing data from a complete autonomous vehicle sensor suite (6 cameras, 5 radars, 1 lidar), was used for 3D object detection and map segmentation. NuScenes consists of 1000 scenes (20 seconds each), fully annotated with 3D bounding boxes (23 classes, 8 attributes). We utilized Occ3D [41] to benchmark occupancy prediction tasks. This large-scale dataset provides multi-view images and dense 3D voxel-based labels, annotating both occupancy status and semantic category. Occ3D builds upon popular autonomous driving datasets such as Waymo [46] and NuScenes. OpenLaneV2 [47], designed for perception and reasoning in complex road scenes, was used for lane detection. In addition to lane information, OpenLaneV2 pro-vides centerline and topology annotations for road elements. This dataset is also built from Waymo and NuScenes."}, {"title": "B. Implementation Details", "content": "For our research, we employed the MMDet3D framework [48] for all code implementations. We began by utilizing the camera branch from BEVFusion [8], replacing its backbone with ElanNet [29] to enhance feature extraction. Then we developed the depth estimator following BEVDepth [11] by replacing all modules in BEVDepth with convolutional neural networks. Additionally, we incorporated multi-scale BEV features using LSS-FPN as proposed by [7] and imple-mented a temporal fusion module based on BEVDet4D [10]. The BEV Encoder was adapted from the Resnet branch of BEVFusion.\nFor task-specific implementations, we developed the 3D object detection head using CenterPoint [30], which pre-dicts 3D bounding boxes, and integrated CBGS [49] to enhance performance. The map segmentation task utilized the BEVSegHead from BEVFusion, while lane detection was carried out using BEVLaneDet [18], with modifications to its height prediction branch to classify each cell. The occupancy prediction head was implemented using FlashOcc [6].\nThe datasets employed, NuScenes [31], Occ3D-NuScenes [41], and OpenlaneV2-Subset-B (NuScenes) [47], share the common asset of the NuScenes dataset. We synchronized these datasets by timestamp matching to create a unified dataset supporting four tasks, and developed a wrapper to manage dataset fusion and training sample iteration. We initialized training with a depth prediction model using BEVDepth, generating and storing depth mask ground truths to optimize subsequent training phases. The input resolution was rescaled to 704\u00d7256, with data augmentation techniques including flipping, scaling, cropping, and rotation. Temporal fusion utilized BEV features from the previous two seconds, aggregating four frames at two frames per second.\nEvaluation metrics included mAP and NDS (NuScenes Detection Score) for 3D detection, along with error metrics (mATE, mASE, MAOE, MAVE, mAAE) for various model performance aspects. Segmentation used the IoU metric, while lane detection and occupancy prediction employed the F1 score and mIoU, respectively.\nBefore initiating multi-task training, the backbone was first pre-trained on the NuImages dataset [50]. We employed the AdamW optimizer throughout our training process. During the feature extractor training stage, we set a learning rate of 1e-4 and a weight decay of le-2, continuing for 20 epochs. Subsequently, for the multi-task warmup stage, the base learning rate was increased to 2e-4 with an auxiliary task-specific warmup learning rate of 2e-5; this stage lasted for 10 epochs per task. In the final end-to-end training stage, the learning rate for the base model was reverted to le-4 and the backbone fine-tuning rate was adjusted to le-5, with training extending over another 10 epochs. Training was conducted using a batch size of 8 on four A100 GPU cards. At each training stage, the best model weights were retained for fine-tuning in the subsequent phase."}, {"title": "C. Task Specific Results", "content": "Following a detailed description of our experimental se-tups, we present the outcomes in this section. It is pertinent to note that comparisons with state-of-the-art methods were conducted at a resolution of 704 \u00d7 256, limited by the avail-ability of larger model configurations and weights. Results were replicated using open-source codes.\nIn our experiments, the baseline model was configured to train on a single task utilizing our proposed feature extractor, while the quad mode involved training four task heads simultaneously in a multitask setting.\n3D Detection Our models exhibited exemplary perfor-mance in 3D object detection on the NuScenes Val Dataset,"}, {"title": "D. Latency Results", "content": "Following the task-specific results, this section highlights the efficiency of our Quad model. Table V presents a concise comparison of model efficiency. Remarkably, the Quad model reduces computational demands to just 281.3 GFlops while achieving a significantly reduced latency of 79.2 ms, substantially lower than both the Fast-Sota and Baseline models, which exhibit latencies of 158.5 ms and 169.7 ms, respectively. Despite these reductions, the Quad model sustains competitive performance across all tasks, underscoring its capability to deliver high-performance met-rics efficiently, which is crucial for real-time processing in autonomous driving applications.\nIn conclusion, compared to conventional methods that train tasks individually before running them concurrently on a shared machine, our Quad model markedly enhances com-putational efficiency and processing speed while maintaining competitive benchmark performance against state of the art methods."}, {"title": "V. ABALATION STUDY", "content": "A. Performance Variations against Pretraining Task\nThis section examines the impact of varying the pretrain-ing task within our multitask framework. We conducted an"}, {"title": "B. Comparison of Loss Profile against Progressive Training Strategy", "content": "This section examines the impact of varying learning rate and weight schedules on the loss profiles of our models, thereby validating the efficacy of our progressive training strategy. Initially, the backbone model was trained uniformly across all tasks from scratch without any pretraining or warm-up phases.\nIn the baseline scenario, as depicted in Figure 3a, we ob-served a highly unstable loss descent profile with significant variance in loss values. To mitigate this, we implemented backbone pretraining, which effectively reduced the loss variance as illustrated in Figure 3b. Subsequently, during the second training stage, we initiated a warm-up period for the multitask head, which further diminished the loss variance, as shown in Figure 3c. In our fourth experiment, fine-tuning of the backbone was conducted with a significantly lowered learning rate during the multitask training stage. Results, presented in Figure 3d, showed a considerable reduction in loss variance and a record low in the minimum loss value compared to the baseline model.\nAdditionally, the fifth and sixth experiments explored the effects of manually assigning higher weights for lane detection and implementing the gradient-based weighting algorithm, GradNorm. The outcomes, displayed in Figures 3e and 3f, revealed that the gradient-based weighting approach not only further reduced loss variance but also achieved the lowest minimum loss value. These experiments collectively demonstrate the robustness of our progressive training strat-egy and highlight the complexities involved in effectively training a multitask framework."}, {"title": "VI. CONCLUSION", "content": "In this work, we introduced QuadBEV, an innovative and efficient multitask perception framework grounded in BEV representation. It unifies four fundamental autonomous driving tasks: 3D object detection, lane detection, map seg-mentation, and occupancy prediction. We demonstrate that a streamlined architecture and progressive training strategy are sufficient to manage the complexities of multitask learning. QuadBEV's ability to share representations and optimize training across tasks establishes it as a compelling solution for resource-constrained scenarios. Extensive evaluations un-derscore QuadBEV's robustness and potential for deploy-ment in real-world autonomous driving systems. Future re-search directions could explore the integration of additional perception tasks and further refinement of training strategies for increasingly complex multitask scenarios."}]}