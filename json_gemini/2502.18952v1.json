{"title": "DualSpec: Text-to-spatial-audio Generation via Dual-Spectrogram Guided Diffusion Model", "authors": ["Lei Zhao", "Sizhou Chen", "Linfeng Feng", "Xiao-Lei Zhang", "Xuelong Li"], "abstract": "Text-to-audio (TTA), which generates audio signals from textual descriptions, has received huge attention in recent years. However, recent works focused on text to monaural audio only. As we know, spatial audio provides more immersive auditory experience than monaural audio, e.g. in virtual reality. To address this issue, we propose a text-to-spatial-audio (TTSA) generation framework named DualSpec.Specifically, it first trains variational autoencoders (VAEs) for extracting the latent acoustic representations from sound event audio. Then, given text that describes sound events and event directions, the proposed method uses the encoder of a pretrained large language model to transform the text into text features. Finally, it trains a diffusion model from the latent acoustic representations and text features for the spatial audio generation. In the inference stage, only the text description is needed to generate spatial audio. Particularly, to improve the synthesis quality and azimuth accuracy of the spatial sound events simultaneously, we propose to use two kinds of acoustic features. One is the Mel spectrograms which is good for improving the synthesis quality, and the other is the short-time Fourier transform spectrograms which is good at improving the azimuth accuracy. We provide a pipeline of constructing spatial audio dataset with text prompts, for the training of the VAEs and diffusion model. We also introduce new spatial-aware evaluation metrics to quantify the azimuth errors of the generated spatial audio recordings. Experimental results demonstrate that the proposed method can generate spatial audio with high directional and event consistency.", "sections": [{"title": "I. INTRODUCTION", "content": "Spatial audio generation is essential for immersive extended reality (XR) environments, interactive entertainment systems, and dynamic media production. With the fast development of Artificial Intelligence Generated Content, there is a fancy question whether we could generate spatial audio from simply text that describes the sound events and spatial directions, known as text-to-spatial-audio (TTSA) generation. TTSA is a brand new research direction. It is rooted in the hot research area of text-to-audio (TTA) generation, which is a task of creating monaural audio from text descriptions. Its core challenge is how to guarantee the synthesis quality and azimuth accuracy of the spatial sound events simultaneously. To address this issue, TTA and conventional spatial audio generation techniques are involved. We summarize the two kinds of techniques as follows.\nOn the TTA side, the challenging issue of TTA is how to generate any kinds of sound events, covering from natural environments to human speech, flexibly with guaranteed high quality. Some recent development is as follows. AudioLM [1] utilizes the w2v-BERT model [2] to extract semantic tokens from audio inputs. AudioLDM [3] generates text-conditioned audio using a latent diffusion model (LDM) [4], where the diffusion process is guided by CLAP embeddings [5] and utilizes a variational autoencoder (VAE) [6] to generate latent embeddings of the Mel spectrograms of the audio. Tango [7] is built on AudioLDM [1]. It replaces CLAP with a fine- tuned large language model (LLM) FLAN-T5 [8]. Tango2 [9] further improved the performance of Tango by employing direct preference optimization [10] and alignment training.\nOn the side of the spatial audio generation, it can be categorized into conventional approaches and deep learning based approaches. Conventional digital signal processing (DSP) techniques create spatial sound by integrating two key acoustic models: room impulse responses (RIR) [11] and head-related transfer functions (HRTF) [12\u201314]. RIR characterize how sound waves propagate in physical environments by capturing room-specific reflections and reverberation. HRTF represent directional sound cues through frequency-dependent filtering effects caused by the interactions between sound waves and human anatomical structures (e.g., ears, head, torso).\nIn recent years, deep-learning-based spatial audio generation methods have shown remarkable potential for spatial audio generation. The work in [15] demonstrated that neural networks can implicitly learn HRTF characteristics from data. Meanwhile, the work in [16] developed a convolutional architecture with neural time warping for addressing temporal misalignments between monaural and spatial audio. Building on these foundations, recent deep learning approaches [16, 17] have achieved significant advances in monaural-to-spatial conversion. Notably, these methods share core principles with conventional DSP techniques. That is, both of them map monaural inputs to spatial targets using directional parameters such as the direction-of-arrival (DOA) information or quaternion coordinates. Their key distinction lies in replacing handcrafted acoustic models with data-driven representations learned by DNNs."}, {"title": "II. PRELIMINARIES", "content": "Diffusion models are a class of generative models that learn data distributions through iterative noise injection and denoising processes. These models learn to reverse a gradual noising process through an iterative denoising procedure. Among various diffusion model variants, the denoising diffusion probabilistic model (DDPM) [19, 20] has become particularly influential.\nA DDPM operates over T steps, with two key stages: forward diffusion and reverse generation, both modeled as a T-step Markov chain. In the forward diffusion, noise is added to the initial sample xo over T steps, resulting in the noisy sample xT. The reverse process aims to reconstruct x0 from xT. Due to the Markov property, each step t depends on the previous step t\u22121, as expressed by:\n$q(x_{1},...,x_{T} | x_{0}) = \\prod_{t=1}^{T} q(x_{t} | x_{t-1}),$ (1)\nwhere $q(x_{t} | x_{t-1}) = N(x_{t}; \\sqrt{1 \u2013 \\beta_{t}}x_{t-1}, \\beta_{t}I)$. Here, $\\beta_{t}$ typically increases with each step $(\\beta_{1} < \\beta_{2} < \u22ef < \\beta_{T})$.\nBy reparameterization, the state at step t can be expressed as:\n$x_{t} = \\sqrt{\\bar{\\alpha}_{t}}x_{0} + \\sqrt{1 \u2013 \\bar{\\alpha}_{t}}\\epsilon,$ (2)\nwhere $\\epsilon \\sim N(0, I)$, $\\alpha_{t} = 1 \u2212 \\beta_{t}$, and $\\bar{\\alpha}_{t} = \\prod_{i=1}^{t} \\alpha_{i}$. The distribution for $x_{t}$ given $x_{0}$ is:\n$q(x_{t} | x_{0}) = N(x_{t}; \\sqrt{\\bar{\\alpha}_{t}}x_{0}, \\sqrt{1 \u2013 \\bar{\\alpha}_{t}}I).$ (3)\nFor the reverse process, to reconstruct x0 from xT, the conditional distributions are parameterized as:\n$p_{\\theta}(x_{0},...,x_{T-1}|x_{T}) = \\prod_{t=1}^{T} p_{\\theta}(x_{t-1} | x_{t}),$ (4)\nwith the reverse transition given by: $p_{\\theta}(x_{t-1} | x_{t}) = N(x_{t-1}; \\mu_{\\theta}(x_{t}, t), \\sigma_{\\theta}(x_{t}, t)^{2}I)$, where\n$\\mu_{\\theta}(x_{t},t) = \\frac{1}{\\sqrt{\\alpha_{t}}}(x_{t} - \\frac{\\beta_{t}}{\\sqrt{1 - \\bar{\\alpha}_{t}}} \\epsilon_{\\theta}(x_{t},t)),$ (5)\nand\n$\\sigma_{\\theta}(x_{t},t)^{2} = \\beta_{T}.$ (6)\nFor t > 1, $\\beta_{t} = \\frac{1-\\alpha_{t-1}}{\\beta_{t}}\\beta_{t}$, and for t = 1, $\\beta_{1} = \\beta_{1}$. $\\epsilon_{\\theta}(x, t)$ is a neural network estimating the noise $\\epsilon$ at time t.\nThe model is trained by minimizing the objective:\n$\\mathbb{E}_{x_{0},t,\\epsilon} ||\\epsilon - \\epsilon_{\\theta} (\\sqrt{\\bar{\\alpha}_{t}}x_{0} + \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon, t) ||^{2},$ (7)\nwhere t is chosen randomly from {1,...,T}. The network $\\epsilon_{\\theta}$ is optimized to estimate $\\epsilon$, aiding in the recovery of the original input. During inference, the reverse process starts with a random sample xT \u223c N(0, I), and iteratively refines it back to x0 using the learned model.\nHowever, applying diffusion models directly to high- dimensional data like images poses computational challenges and requires significant memory resources. To address these limitations, LDM transfers the diffusion process from a high- dimensional data space to a lower-dimensional latent space. This approach was first systematically applied to image generation in the work Stable Diffusion [4]. In this method, a pre-trained VAE [6] compresses images into low-dimensional"}, {"title": "III. DUALSPEC", "content": "In this section, we will introduce the proposed DualSpec framework, starting with its workflow pipeline, followed by a detailed explanation of the components it comprises, including the text encoder, diffusion model, VAE, and vocoder.\nA. Overview of DualSpec\nThe pipeline of the proposed model, DualSpec, is illustrated in Fig. 1. During training, DualSpec first adds multi-level Gaussian noise to the input latent representations, then the diffusion model learns to gradually recover the original features from the noise. Two VAE encoders are used to compress images into latent spaces to reduce computational costs, which separately extract latent representations from Mel spectrograms and STFT spectrograms. These representations are then concatenated and fed into the diffusion model for training. The text encoder maps text prompts into semantic vectors to guide the generation direction. The model is optimized by minimizing the difference between predicted noise and ground truth noise, and a conditional mechanism is employed to achieve precise alignment between text and spatial audio.\nDuring inference, the input text is first converted into semantic features by the text encoder. Starting from random Gaussian noise and utilizing a diffusion model, the process iteratively refines and generates latent representations that align with the textual description. These latent representations are then decoded into Mel and STFT spectrograms through two dedicated VAE decoders. The Mel spectrogram is subsequently inverse-transformed into the amplitude spectrogram1 and merged with the STFT phase spectrogram to create new STFT features. Finally, spatial audio is produced via the inverse STFT (ISTFT) transformation.\nB. Text encoder\nFLAN-T5-Small (80M) [8] is used as the text encoder (Etext), which generates text encodings $\\tau \\in \\mathbb{R}^{L \\times d_{text}}$, where L is the token length and dtext is the embedding dimension. Through pretraining on chain-of-thought and instructional datasets [29], FLAN-T5 gains the ability to effectively utilize in-context cues and mimic gradient-based optimization through its attention mechanisms. By treating each input as a separate task and leveraging its advanced pretraining, the model excels at extracting task-relevant information while reducing noise, ultimately enabling more accurate conversion from text to acoustic representations. We freeze the text encoder, which not only saves computational resources but also effectively prevents overfitting.\nC. Diffusion model for text-guided generation\nLDM [4] produces the latent representation prior z0 under the influence of a text-derived representation \u03c4. This involves approximating the distribution q(z0|\u03c4) using a trainable model p\u03b8(z0|\u03c4). In our method, the model receives two different features: Mel and STFT spectrogram. Specifically, Mel spectrogram is encoded by Mel-VAE, while STFT feature is encoded by STFT-VAE, which produce corresponding latent representations. See Section III-E for the details of the VAEs. The two latent representations are then concatenated to form a combined latent input, which is processed by a diffusion model. After processing, the combined input is split into two parts, each used for the loss calculation.\nLDM accomplishes this task via both forward and reverse diffusion processes. We denote superscripts m and s, representing latent diffusion processes for Mel spectrogram and STFT features. The forward process incrementally adds noise to $z_{0}^{m}$ and $z_{0}^{s}$ using a sequence of Gaussian transitions, regulated by noise levels 0 < \u03b21 < \u03b22 < \u22ef < \u03b2N < 1:\n$q(z_{k}^{n} | z_{k}^{n-1}) = N(\\sqrt{1-\\beta_{n}} z_{k}^{n-1}, \\beta_{n}I),$ (8)\n$q(z_{0}^{k} | z_{0}^{n}) = N (\\sqrt{\\bar{\\alpha}_{n}} z_{0}^{k}, (1 \u2013 \\bar{\\alpha}_{n})I),$ (9)\nk \u2208 {m, s},\nwhere N refers to the total number of diffusion steps, $\\alpha_{n} = 1-\\beta_{n}$, and $\\bar{\\alpha}_{n} = \\prod_{i=1}^{n} \\alpha_{i}$. A reparameterization method [30] simplifies sampling any intermediate states $z_{n}^{m}$ and $z_{n}^{s}$ from $z_{0}^{m}$ and $z_{0}^{s}$ through the formula:\n$z_{n}^{k} = \\sqrt{\\bar{\\alpha}_{n}} z_{0}^{k} + \\sqrt{1 \u2013 \\bar{\\alpha}_{n}} \\epsilon^{k}, k \\in \\{m,s\\},$ (10)\nwhere $\\epsilon^{m}, \\epsilon^{s} \\sim N(0,I)$ introduce independent noise. At the final step of forward diffusion, both $z_{N}^{m}$ and $z_{N}^{s}$ resemble standard Gaussians.\nIn the reverse process, noise is removed to recover $z_{0}^{m}$ and $z_{0}^{s}$. The reverse procedure employs a loss function to predict noise for both latents using the text-conditioned model $\\epsilon_{\\theta}$:\n$L_{DM} = \\sum_{k\\in\\{m,s\\}} \\sum_{n=1}^{N} \\gamma_{n} \\mathbb{E}_{z_{k}^{0},\\epsilon^{k} \\sim N (0,1), \\tau} || \\epsilon^{k} - \\epsilon_{\\theta} (z_{n}^{k},\\tau) ||^{2},$ (11)\n$\\epsilon_{\\theta}$ uses a U-Net structure [31] with cross-attention to incorporate text features.\nHere, \u03b3n adjusts the weight of each reverse step according to its signal-to-noise ratio. Sampling for $z_{0}^{m}$ and $z_{0}^{s}$ follows the previously described formulas, and \u03c4 represents the text encoding for guidance (see Section 2.1). Noise predictions guide the reconstruction of both latents, modeled as:\n$p_{\\theta}(z_{0:N} | \\tau) = p(z_{N}^{k}) \\prod_{n=1}^{N} p_{\\theta} (z_{n-1}^{k} | z_{n}^{k}, \\tau),$ (12)\n$p_{\\theta}(z_{n-1}^{k} | z_{n}^{k}, \\tau) = N [z_{n-1}^{k}; \\mu_{\\theta}^{(n)} (z_{n}^{k},\\tau), \\beta_{n}^{(k)} I],$ (13)\n$\\mu_{\\theta}^{(n)} (z_{n}^{k}, \\tau) = \\frac{1}{\\sqrt{\\alpha_{n}}}(z_{n}^{k} - \\frac{\\beta_{n}}{\\sqrt{1 - \\bar{\\alpha}_{n}}} \\epsilon_{\\theta}^{(n)} (z_{n}^{k}, \\tau)),$ (14)\n$\\bar{\\beta_{n}^{(k)}} = \\frac{1-\\bar{\\alpha}_{n-1}}{1-\\bar{\\alpha}_{n}} \\beta_{n},$ (15)\nk \u2208 {m, s}.\nD. Classifier-free guidance\nFor the reverse diffusion process that reconstructs the priors $z_{0}^{m}$ and $z_{0}^{s}$, we integrate a classifier-free guidance strategy [32] conditioned on text input \u03c4. This approach uses a guidance scale w during inference to balance text-conditioned and un-conditional noise predictions. When text guidance is disabled, we pass an empty input \u00d8, and the guided estimations for Mel and STFT latents are given by:\n$\\epsilon_{\\theta}^{(n)} (z_{n}^{k}, \\tau) = w\\epsilon_{\\theta}^{(n)} (z_{n}^{k}) + (1 - w)\\epsilon_{\\theta}^{(n)} (z_{n}^{k}, \u00d8), k \\in \\{m, s\\}.$ (16)\nE. Audio VAE and vocoder\nThe VAE compresses audio features, $m \\in \\mathbb{R}^{T \\times F}$, into latent representations $z^{m} \\in \\mathbb{R}^{C \\times T/r \\times F/r}$ and $z^{s} \\in \\mathbb{R}^{C \\times T/r \\times F/r}$, where C, T, F and r denote channel count, time slot count, frequency slot count, and compression ratio, respectively.\nThe latent diffusion model then uses text guidance \u03c4 to reconstruct the audio priors $z_{0}^{m}$ and $z_{0}^{s}$. Both the encoder and decoder are built upon stacked convolutional modules [3] and jointly optimized by maximizing the evidence lower bound (ELBO) [6] while minimizing adversarial loss [33]. We trained two VAE models: Mel-VAE and STFT-VAE, which compress the Mel spectrogram and STFT spectrogram, respectively.\nAdditionally, in order to explore the diverse combinations of the two acoustic features, we also trained the Dual-VAE model, which takes the concatenation of Mel spectrograms and STFT spectrograms as input and outputs their joint reconstructions, as illustrated in Fig. 2.\nWe use HiFi-GAN [34] as the vocoder to convert the Mel spectrogram into an audible waveform."}, {"title": "IV. OBJECTIVE EVALUATION OF SPATIAL FIDELITY", "content": "To assess the spatial fidelity of synthesized binaural audio, we employ a DNN-based sound source localization framework [18] as an objective evaluation metric. When the localization model achieves extremely low errors on ground-truth binaural audio, the predicted DOA for the synthesized binaural audio can be considered as an approximation of its actual DOA. This allows us to compare the predicted DOA with the ground-truth DOA and compute the spatial error.\nAs shown in Fig. 3, we employ a classification-based localization model. When using conventional compact microphone arrays for localization, the process primarily relies on phase spectrograms [35], as the amplitude differences between microphones are minimal. However, due to the head shadow effect, the amplitude differences between the two ears can be significant. Leveraging this characteristic, we develop an end-to-end architecture that directly processes phase and magnitude spectrograms as dual-branch inputs.\nBuilding on the phase-based framework from [35], we integrate a parallel magnitude spectrogram branch alongside the original phase input, creating a dual-stream architecture that comprehensively utilizes frequency-domain auditory information. By using convolutional layers, our design bypasses manual feature extraction and autonomously learns spatial auditory cues. The first convolutional layer explicitly aligns with binaural physiology through its two-channel structure, enabling more complex nonlinear combinations than interaural time differences and interaural level differences."}, {"title": "V. PIPELINE OF SYNTHESIZING SPATIAL AUDIO DATASETS", "content": "In this section, we introduce the monaural sound event datasets, the HRIRs, and the production process of the spatial audio dataset, as well as explain the division of datasets for model training and testing.\nA. Monaural sound event datasets\n\u2022 ARCA23K [36] is a sound event dataset designed to study real-world label noise, comprising 23,727 audio clips sourced from Freesound. These clips are categorized into 70 classes based on the AudioSet [36] classification framework. The dataset was generated through a fully automated pipeline without human review, resulting in a significant proportion of potentially mislabeled audio samples due to the absence of manual quality control.\n\u2022 The UrbanSound8K dataset [37] is an open-source audio corpus comprising 8,732 labeled short audio clips (< 4 seconds) categorized into 10 urban sound classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren, and street_music. These classes are derived from the urban sound taxonomy to systematically represent typical urban acoustic scenes. The audio clips, sourced primarily from Freesound, undergo manual verification to ensure labeling accuracy. The dataset serves as a benchmark for environmental sound classification and sound event detection (SED).\n\u2022 The ESC-50 dataset [38] is a collection of 2,000 labeled environmental audio recordings, ideal for benchmark- ing methods of environmental sound classification. It encompasses 50 distinct categories of sounds sourced from Freesound, including natural, human, and domestic noises. Each category consists of 40 individual 5-second- long audio clips.\n\u2022 The FSD50K dataset [39] is also a publicly accessible collection of human-annotated sound occurrences, featuring a total of 51,197 audio clips from Freesound that are distributed across 200 categories derived from the AudioSet Ontology [36]. It predominantly encompasses sound events generated by physical sources and production mechanisms, including but not limited to human vocalizations, sounds produced by objects, animal vocalizations, natural soundscapes, and musical instrument performances.\nB. HRIRS\nIn this study, we utilized the HUTUBS HRTF dataset [14], specifically focusing on the HRIR measurements from one individual identified as 'pp96'. The dataset contains HRIRs captured at 440 distinct locations on a spherical grid. For our specific requirements, which involve only horizontal plane analysis, ignoring vertical variations, we extracted data from 12 positions located precisely at 0\u00b0 elevation.\nC. Data processing pipeline\nFirst, we resampled each mono audio clip in Section V-A to a sample rate of 16kHz. and then we removed all audio clips that were shorter than 1s. In this way, we obtained a total of more than 90,000 mono audio clips. Next, we used HRIRS to convert each mono audio clip into corresponding dual-channel stereo audio files. We divided the horizontal plane into 12 directions, each separated by 30 degrees. Consequently, each mono audio clip was transformed into 12 stereo audio files, resulting in a total of approximately 1.2 million spatial stereo audio files.\nFor VAE training, we construct the training batch by randomly selecting one of the twelve spatial audio files associated with each audio event, cropping the audio length to 5 seconds. If the original audio length is less than 5 seconds, it is zero-padded to ensure the required length is met."}, {"title": "VI. EXPERIMENTS", "content": "In this section, we present the implementation details of our method, the evaluation metrics, and the evaluation of VAE and diffusion models.\nA. Implementation details of VAEs\nFor the training of the reconstruction of the STFT spectrogram, we configured the VAE with a compression level of 8. We set the number of FFT points and the window length to 512, while the hop length was set to 256. For the reconstruction of the Mel spectrogram, we set the compression level to 4 and extracted 64 bands Mel spectrogram. When training the combination of two features, we followed the aforementioned settings and set the compression level to 8. We utilized the Adam optimizer [40] with a learning rate of 4.5 \u00d7 10\u22126 to train all VAE models. The training dataset, as described in Section V-C, consisted of 1.2 million samples. During each epoch, for every individual sound event, one out of the 12 available spatial audio was randomly selected for training. All models were trained for 800 epochs using four NVIDIA A100 40GB GPUs. The batch size was set to 64 for Mel and STFT features, and to 2 for the two-feature combination. To stabilize the training process, adversarial loss was not applied during the first 40 epochs.\n2) Spatial Perception Evaluation: For spatial performance assessment, we employ standard sound source localization metrics from [35]: DOA Mean Absolute Error (MAE) and classification accuracy (ACC). While ACC represents a straightforward classification success rate, the circular nature of angular measurements on the azimuth plane necessitates special consideration for MAE calculations. Given a ground-truth angle \u03b8 and its corresponding prediction \u03b8\u02c6 for a sample, the MAE computation follows this angular distance formulation:\n$MAE(\\theta) = min(|\\hat{\\theta} \u2013 \\theta|, 360 \u2013 |\\theta \u2013 \\hat{\\theta}|)$ (17)\nD. Evaluation of VAEs using different features\nTable I presents the evaluation metrics for three VAEs, including Mel-VAE, STFT-VAE, and Dual-VAE, in terms of their generation performance.\nFirst, the performance of the three types of VAEs on audio quality generation is presented. The STFT achieves the best performance in the generation quality, attaining the highest scores of 31.93 in PSNR, 0.929 in SSIM, 10.37 in FD, 0.461 in KL divergence and 12.94 in IS. Although Mel-VAE shows competitive results with the SSIM, FD, KL, and IS scores, it exhibits a lower PSNR score of 25.42 than STFT- VAE's 31.93. This indicates that, in terms of audio quality generation based on VAE, the STFT spectrogram outperforms the Mel spectrogram. This conclusion stands in sharp contrast to the generation performance of diffusion models, as will be further confirmed by subsequent experiments. The overall performance of Dual-VAE is not as good as that of STFT- VAE and Mel-VAE due to the loss of information during the inverse transformation from the Mel spectrogram to the STFT magnitude spectrogram. However, it has shown moderate performance in terms of PSNR and SSIM metrics.\nNext, we present the performance of different VAE models in terms of the direction accuracy of the generated audio. Compared to the other two methods, the STFT spectrogram can also provide more precise spatial position generation, possessing the best MAE and ACC scores. The superiority stems from the fact that the phase spectrogram of the STFT is rich in audio location information, which is lacking in the Mel spectrogram. Therefore, the Mel-VAE have the lowest MAE and ACC scores. Although DualSpec does not match the first two in terms of audio quality generation, it nearly reaches the level of STFT in spatial position generation and significantly outperforms Mel-VAE.\nE. Comparison of different generation methods\nTable II presents a comparison of the proposed different diffusion models against baseline models across multiple performance metrics. Given the current absence of comparable spatial audio generation works with single-channel reference 2, our comparison is limited to recent monaural audio generation approaches, specifically AudioLDM [3], AudioLDM2 [44], TANGO [7], and TANGO2 [9]."}, {"title": "VII. CONCLUSIONS", "content": "In conclusion, the proposed DualSpec demonstrates significant potential in TTSA generation, offering a novel approach that eliminates the need for monaural audio inputs. Under the guidance of text with azimuthal information, DualSpec achieves high directional consistency in generated spatial audio. Furthermore, the pipeline for the spatial audio dataset production also lays the foundation for TTSA tasks. Experimental results demonstrate the effectiveness of the proposed method in generating high-quality spatial audio with accurate spatial positioning."}]}