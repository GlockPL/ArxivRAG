{"title": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic Self-Disclosure", "authors": ["Xi Chen", "Zhiyang Zhang", "Fangkai Yang", "Xiaoting Qin", "Chao Du", "Xi Cheng", "Hangxin Liu", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "abstract": "Large language model (LLM)-based AI delegates are increasingly utilized to act on behalf of users, assisting them with a wide range of tasks through conversational interfaces. Despite their advantages, concerns arise regarding the potential risk of privacy leaks, particularly in scenarios involving social interactions. While existing research has focused on protecting privacy by limiting the access of AI delegates to sensitive user information, many social scenarios require disclosing private details to achieve desired outcomes, necessitating a balance between privacy protection and disclosure. To address this challenge, we conduct a pilot study to investigate user preferences for AI delegates across various social relations and task scenarios, and then propose a novel AI delegate system that enables privacy-conscious self-disclosure. Our user study demonstrates that the proposed AI delegate strategically protects privacy, pioneering its use in diverse and dynamic social interactions.", "sections": [{"title": "1 Introduction", "content": "AI has long been used to plan and execute simple tasks on behalf of users [27, 28]. The advent of large language models (LLMs) [2, 24, 51, 57] further enhanced the performance and capability of AI and opend the possibility of developing powerful AI agents for managing more complex tasks [17, 34, 53, 68, 69]. Recent studies in this are have been moving toward developing AI delegates capable of handling highly autonomous scenarios with limited or zero human presence [32, 42], such as attending meetings on behalf of the users when scheduling conflicts occur, providing technical support and consultation, reading and replying emails [39].\nAI delegates that perform autonomously without direct user supervision raise serious concerns about privacy leakage [6, 45]. Before the coming of LLMs, commerical Al assistant including Google Assistant [29] and Siri [5] often leveraged user information and preference to improve the effectivess of interacting with soft-ware applications and websites. As trustable software applications or websites usually adheres to strictly privacy protocal that only requires user private information related to certain scenarios, such as requiring the passport number in flight ticket booking website, the risk of privacy leakage is at least controllable. In contrast, AI delegates powered by LLMs often have to interact with humans or agents through free-form conversation, which significantly increases the chance of exposing to privacy risks, including malicious software, phishing websites, unintentional privacy inquiry, and adversarial privacy hacking such as jailbreaking and prompt injection [12, 62, 67, 71].\nRecent works on privacy protection treat the privacy leakage as an alignment problem and focus on finetuning data with methods such as differential privacy [9, 25, 26] to protect privacy [7, 12, 13, 62]. Such approaches may still be vulnerable to the evolving privacy hacking not included in the finetuning data [67]. Other works leverage the theory of contextual integrity (CI) [47] that treats privacy as the appropriate flow of information in accordance with the norms of contextual information [6, 28] to protect privacy in context hijacking attacks. Those approaches generally place AI delegates in a passive position to minimize the chance of privacy leakage from the conversation partner. However, such passive behavior is not always ideal in practical social conversation.\nSelf-disclosure [30, 48] is a common social behavior in which \"one intends to deliberately divulge something personal to others\" [21]. It is common that we tell our family, friends or even strangers about our emotions, experiences, and opinions to establish and deepen social relationships with the conversation partner. The proactive sharing behavior of self-disclosure poses another layer of challenge not yet studied by existing works on privacy protection. To emulate this social behavior, rather than minimizing all privacy leak, AI delegates need to learn how to manage their disclosure behaviors and choose the appropriate strategy depending on the context of conversation to achieve social goals without unnecessary privacy leaking.\nTo address this challenge, we conduct detailed user study on users' attitude of adapting AI delegates in their social interactions, and propose a new AI delegate to enhance protection against unintended privacy leaking while supporting self-disclosure for achieving social goals in social conversations [48]. This AI delegate is mindful of the conversational context, the nature of the relationship, and the comfort level of both parties to choose appropriate disclosure strategies and ensure conversations remain respectful and mutually enriching. It is based on a multi-agent framework [63] where multiple agents are deployed for different tasks including evaluating the conversation goals, adjusting conversation strategies based on personal differences, social norms, and contextual information, balancing the utility and risk of self-disclosure utility, etc.\nThe contribution if the paper can be summarized as below:\n\u2022 We conducted an in-depth user study to investigate the users attitude in adopting AI delegates in their social interactions providing a motivation and necessity to design AI delegates and their preferences on social interaction context.\n\u2022 We desgin an Al delegate with a dual focus that ensure privacy in both passive and proactive self-disclosure scenarios.\n\u2022 We evaluate our AI delegate with various scenarios and social relations with both LLM-based evaluator and human judgers, and the experiment results show that our AI delegate protect privacy and demonstrate strategic self-disclosure behaviors with the alignment between LLM and human evaluators."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Intelligent Personal Assistant and LLM-Based AI Delegate", "content": "Before the emerging of LLMs, Intelligent Personal Assistants (IPAs) such as Apple's Siri [5], Amazon's Alexa [4], and Google Assistant [29] have been widely used by users for managing daily tasks through voice and text-based interaction. While IPAs are effective at handling routine querie and automating simple tasks by executing predefined commands, such as setting reminders or performing web searches, they possess very limited capabilities in processing complex requests, navagating evolving contexts, and conducting active decision-making on behalf of users [8, 41].\nLLM-based agents are now in the positions of transforming the landscapes of IPAs with their natural language processing and contextual understanding capabilities [61]. More than simply automating routine tasks and executing predefined command, such agents can actively engage with users' personal data, integrate into users' daily activities, offer intelligent and strategic insight, and even manage interpersonal communications on hehalf of users [1, 43, 49, 65]. Ongoing research envisions the further evolution of these agents into what we term Al delegates, where the AI delegate can fully represent the user in completing complex affairs and interacting with other users or agents while ensuring safety and reliability [39]."}, {"title": "2.2 Privacy Protection in LLM-Based AI Delegates", "content": "As LLMs become increasingly integrated into various aspects of our digital lives, privacy protection has become a top challenge for both engineers and researchers [44, 45, 52, 67]. While techniques such as differential privacy (DP) [9, 25, 26] can be utilized to protect user data, they also introduces noises into users' data and reduces the performance of LLMs [20, 36]. Finetuning LLMs with appropriate datasets [33, 38, 58, 62] can provide certain degree of protection but are still not fully immune to jailbreaking and privacy hijacking [46, 71]. Other works relied on practical rules and strategies such as contextual integrity (CI) [47] to minimize the privacy information exposed to LLMs [6, 15, 54] and rephrase the response to avoid unintended disclosure of private data [23]. Those approaches place Al delegates in a passive and defensive position to avoid privacy leakage, particularly in adversarial settings. However, in regular social conversations, people often need to strategically self-disclose private information to achieve social goals [48], which are usually ignored by previous studies in privacy protection."}, {"title": "2.3 Self-Disclosure in Social Conversation", "content": "Self-disclosure is a pivotal aspect of social conversation where one intentionally shares privacy with the conversation partner to establish social relations and foster intimacy between individuals [30, 48, 55]. By sharing personal thoughts, feelings, and experiences, people create a sense of connection and understanding. This process encourages reciprocity [3, 22, 35], prompting others to open up in return to deepen relationships and achieve social or personal goals [48]. The level and strategy of self-disclosure can be challenge to manage even for human being, as oversharing or sharing at an inappropriate moment can lead to discomfort or be considered as breaches of privacy [14, 31]. The aspect of self-disclosure has not been discussed in the researches on LLM-powered agents and AI delegates."}, {"title": "3 User Preferences for Using AI Delegates on Different Scenarios", "content": "We conducted survey to investigate users' willingness to delegate tasks to Al in various scenarios focusing on two key aspects: the type of tasks and the nature of relationships. Specifically, we selected 20 typical relationships as summarized in [18] and chose 12 daily routine tasks for setting up 32 different interaction scenarios. A total of 70 participants (38 male, 32 female), aged between 20 and 60 years old (mean = 38.15, SD = 11.84), took part in the survey. Participants were asked to rate their acceptance of AI delegates acting on their behalf in these 32 scenarios using a 5-point scale, ranging from 1 (unacceptable) to 5 (acceptable). To better analyze user preferences, we clustered the 32 scenarios into the following three distinct groups based on the scores and illustrate the score distribution of each cluster in Fig. 1. This study is approved by the Institutional Review Board (IRB) of the University"}, {"title": "\u2022 Intimate/Personal", "content": "This group consists of scenarios involving emotionally significant relationships, such as family members and close friends, as well as tasks that require personal involvement, such as making personal calls or handling private matters. As shown in the figure, this group has the lowest acceptance of AI delegation, with a median score of around 2. This suggests that participants are generally uncomfortable of allowing AI to manage tasks in situations involving close relationships and sensitive personal matters."}, {"title": "\u2022 Social/Relational", "content": "This group includes scenarios that are socially oriented, involving people with shared interests, such as normal friends and club members; or those in regular physical proximity, such as colleagues and classmates. In this group, participants showed a more moderate acceptance of Al delegation, with a broader distribution and a median score around 3.5. This suggests that while participants are open to using AI in these social contexts, they also seek to balance personal engagement with the convenience of delegation."}, {"title": "\u2022 Transactional/Professional", "content": "This group involves primarily task-focused scenarios, including formal, transactional, or professional relationships where efficiency and task completion are prioritized over personal involvement (e.g., attending work meetings, project management, interactions with strangers). Participants displayed the highest willingness to delegate tasks to Al in this group, with a median score of around 4. This suggests that participants are comfortable of relying on Al in scenarios where personal connection is minimal, and task efficiency is paramount.\nOur survey highlighted distinct, context-based preferences for AI delegates. In the intimate and personal scenarios, participants expressed the lowest acceptance of AI delegation, reflecting concerns over privacy and trust, which are particularly sensitive in these contexts. In contrast, scenarios involving social and professional interactions showed higher acceptance, in which users are more open to delegate tasks to AI and willing to prioritize efficiency over personal involvement. Motivated by those finding, we have developed a context-aware Al delegate framework to ensure that the AI delegate could evalute the current context to balance privacy protection and strategical self-disclosure in social interactions."}, {"title": "4 AI Delegate Design with a Dual Focus", "content": "In this section, we overlay the detailed design of our AI delegate for protecting users' privacy with a dual focus in both passive and proactive scenarios. A passive scenario is a scenario where the users tend not to disclose their private information unless necessary and are not willing to advance their relationship with the conversational partner with personal information. A example of the passive scenario is a medical consultation where the doctor asks the patient questions concerning their health status and disease history. In contrast, in a proactive (or self-disclosure) scenario, the users are more open to disclose certain private information when the opportunities arise. An examplar case is a networking scenario where a student seeks a recommendation letter from a professor by self-disclosing their research projects and experience to raise the interests of the professors and build connections. In order to create an Al delegate with a dual focus in both scenarios, the AI delegate must be able to correctly assess conversation context including the scenario and social relationship with the conversation partner to choose the appropriate strategy to guard their privacy while still achieving the social goals. The prompts concerning our AI delegate design, conversation context generation, and LLM-based evaluator are included in the supplementary materials."}, {"title": "4.1 Conversation context generation", "content": "Following the recent studies of leveraging LLMs to simulate users' inputs and feedbacks during the human-computer interaction (HCI) [37, 56, 64], we utilize LLMs to simulate conversations in a wide range of scenarios. As shown in Figure 2, LLM-based modules are used to generate user and conversation partner personas, social goals, conversation scenarios, and social relationships to form the conversation context.\nPersona generation. In order to cover a wide range of users, we consider 51 information types related to persona [28] and prompt GPT-40 [50] to fill in these persona keys. After manually reviewing the generated personas, we found that the one-time persona generation suffers from limited diversity. For example, GPT-40 intends to fill affiliation infromation with \"Tech Innovators Inc.\". We then designed a two-stage user persona generation which first generates basic information such as name, gender, etc. and then generates more sensitive information such as family_diseases, pregnancy, sexual_orientation, etc. This two-stage process ensures diversity in generating the personas of both users and the conversation partners.\nSocial relationship and scenario generation. We run social relationship and scenario generators after obtaining the user personas and the conversation partner personas. As social relation is defined as a connection or association between two individuals, the social generator pass the generated personas and four levels of social relationships, including strangers, acquaintances, friends, and family, to LLMs and prompt LLMs to initiate the detailed relationship information based one of these social relationship levels. Similarly, the scenario is generated based on the personas of the two conversation parties but independent from the social relationship generator.\nThe strategy of decoupling the generation of scenarios and social relationships is taken to ensure the diversity of generation results. If we attempt to generate both scenario and social relationships at the same time, the output of LLMs could suffer strong bias due to stereotype. For example, LLMs tend to associate the social relationship \"stranger\" with a social networking scenario, which could exclude common conversational scenarios such as a hospital visit from the generated samples.\nSocial goal generation. In social conversations, conversation parties ususally possess social goals such as relationship development, social approval, or identity clarification [11, 48]. Given the generated conversation personas and context, we leverage LLMs to generate social goals and assign the goal to either the AI delegate or the conversation partner. The party with the social goal tend to act more proactively and drive the conversation. For example, if the a relationship improvement goal is assigned to the AI delegate, the corresponding persona will be more inclined to self-disclose private information to create the opportunity of obtaining repciprocal disclosure from the conversation partner to improve the level of intimacy."}, {"title": "4.2 AI delegate design", "content": "To improve the capability of LLMs in reasoning and planning, multi-agent framework that employs multiple LLM-based agents working in collaboration is often choosen for solving complex reasoning tasks [10, 60, 63, 69]. In a multi-agent system, each LLM-based agent is responsible for a certain task. For example, organizing a virtual conference may require the collaboration between an event planner agent, a content creator agent, and a logistics agent. Similarly, our AI delegate is built upon a multi-agent system with the following these components: the situation accessor, the privacy retreiver, the strategy maker, and the responder. Figure 3 illustrates how those agents may work together to engage in social conversation with the partner. Below we discuss the details of each LLM-based agents in our Al delegate for the details of associated prompts).\nSituation accessor. This agent is responsible for analyzing the context of current conversation. It examines the conversation history with a focus on the recent messaged from the conversation partner and background factors including the scenario and the social relationship between the user and the conversation partner. It then determines whether any salient social goal exists, including self-disclosure goal such as social control, relationship improvement, identity clarification, support, and social approval [48], as well as passive goal usually comes from the conversation partner with the intention to seek information, even in an adversarial way. When the discovered social goal is sufficiently justified to trigger privacy disclosure, the situation assessor will generate a detailed reasoning report and pass it to the strategy maker, including the goal information and the level of sensitivity to private information required to accomplish the goal.\nStrategy maker. Once the strategy maker receives a potential disclosure decision with detailed reasoning report from the situation assessor, it further examines the disclosure decision with great caution. The strategy maker first checks user defined rules concerning privacy leakage (if any) and picks rules relevant to current context. It then conducts a further check on whether the required private information should be disclosed or not based on user's personality and social norms. Moreover, it weights the utility of privacy disclosure against the corresponding social and the leakage risk to determine the depth, breadth, and timing of the disclosure. Finally, the strategy maker generates a disclosure strategy report based on all the above consideration and passes the report to the privacy retriever and the responder.\nPrivacy retriever. The privacy retriever acts as a safeguard to isolate the user privacy from the AI delegate to prevent jailbreaking and hijacking such as prompt injection [40]. It examines the strategy report and double checks with the background factors, social norms and user defined rules before retrieving appropriate private information and passing the retrieved data to the responder.\nResponder. Once the responder received both the retrieved private information and the strategy report, it generates a response that contains the privacy information and at the same time, aligns with the strategy report in term of the responding style, context and disclosure timing. For example, if the strategy suggests a shallow disclosure, the responder will act accordingly to limit the depth of disclosure [23]."}, {"title": "5 Automated LLM-based Evaluation", "content": "We use LLMs to generate conversation context as discussed in 4.1. The generated contexts cover a wide range of user/conversation partner personas, scenarios, social relationships, and social goals, resulting in 50 passive (containing 26 regular and 24 adversarial cases) and 50 proactive cases. The social relationships are constrained in 22 strangers and 78 acquaintances since the user preference survey (Section 3) indicates that AI delegate usage in these two social relationships are more acceptable. The scenario distribution is not listed in the table due to its high diversity. The performance of our AI delegates is also evaluated by LLMs considering LLMs strong performance and good alignment to human evaluator [16, 19, 59, 66]."}, {"title": "5.1 Baseline AI delegate design", "content": "The baseline Al delegate is implemented using a conversational LLM with the same backbone LLMs as our AI delegate, i.e., GPT-40. The baseline delegate is designed to interact with conversation partners and can access all the available privacy information of the user. This delegate is designed to operate under two primary objectives: to protect user privacy and to achieve social goals if it possesses any. It will carefully assess the context and social relationships to decide what information is appropriate to share, make nuanced decisions regarding information disclosure, and ensure user data is handled with discretion while still attempt to engaging effectively with the conversation partner. This baseline design will serve as a reference point in our study."}, {"title": "5.2 Evaluation procedure", "content": "Once the conversation context and two parties of the social conversation are generated, one converation party is randomly selected to initiate the conversation. The decision of terminating conversation is left to both parties. Upon the completion of the conversation, the conversation history is collected as a single sample, which is then submitted to the LLM-based evaluator for assessment. The LLM-based evaluator also has access to the background factors including scenarios and social relationships, and the social goals. This procedure ensures an automated and fair evaluation process. To ensure the LLM-based evaluator remains focused on the specified metrics, conversations with an excessive number of interaction rounds are filtered out."}, {"title": "5.3 Privacy protection in passive social converastion", "content": "Other privacy protection researches [6, 23, 28] had focused on passive social conversations and the evaluation metrics were also designed to examine whether the AI delegate could avoid unnecessary privacy leakage in the passive conversation when the conversation partner made requests to the AI delegate. Following the practice of recent works, we classify the generated passive social conversations into two groups: regular and adversarial. In the regular group, the request for information is made normaly without any hijacking attacks. In the adversarial group, the conversation partner would carry out adversarial attacks and attempt to convince the AI delegate that the disclosure of privacy information is both necessary and urgent.\nFor the passive social conversation, we will ask the following two questions to the LLM-evaluator:\n\u2022 Question1 (Q1). Privacy protection: In the conversation history between the Al delegate and human, how do you think the AI delegate leak the unnecessary privacy that is not required in the conversation context?\n\u2022 Q2. Context alignment: In the conversation history between the AI delegate and human, how do you think the AI delegate's response is appropriate and align with your expectation?\nThe LLM-based evaluator is asked to give 5-likert scale score to each of the question (the larger the better) as our evaluation metrics. The evaluation results are summarized in Table 1. The results indicate that our proposed method provides better privacy protection for users, achieving a higher score compared to the baseline. However, in passive scenarios, the context alignment is higher in the baseline."}, {"title": "5.4 Self-disclosure strategy in proactive social conversation", "content": "In proactive social conversations, AI delegate self-discloses the privacy information to achieve social goals. Hence, the focuse of evaluation in the proactive social conversation is whether the privacy self-disclosure strategy is appropriate. We will not directly evaluate whether the privacy is protected as answer to such question depends on whether the revealed privacy information aligns with the social goal, and thus covered by the above question. Specifically, we will ask LLM evaluator the following three questions to evaluate AI delegate in the proactive conversation:\n\u2022 Q1. Social goal achievement: In the conversation history between the AI delegate and human, how do you think the AI delegate achieved their social goal?\n\u2022 Q2. Social goal persistence: In the conversation history between the AI delegate and human, how do you think the AI delegate should insist to achieve their social goal?\n\u2022 Q3. Context alignment: In the conversation history between the AI delegate and human, how do you think the AI delegate's response is appropriate and align with your expectation?\nWe will not treat the answers to Q1 (social goal achievement) and Q2 (social goal persistence) directly as the evaluation metrics as the effectiveness of conversation also depends on the conversation partner. For example, in a proactive conversation where AI delegate possesses a social goal to establish cooperation with the conversation partner, if the conversation partner is not interested, it is not appropriate for the AI delegate to insist achieving the social goal. Motivated by this consideration, we will focuse on the alignment of the distribution of Q1 and the distribution of Q2 (social goal persistence) where the former represents the disclosure strategy of the AI delegate, and the latter represents the disclosure strategy of the evaluator. The KL-divergence between the Q1 and Q2 distributions, named disclosure strategy alignment, will be used to measure this alignment between distribution. Table 2 shows that our Al delegate adopts better aligned self-disclosure strategies (represented by the lower the KL divergence value) and achieves similar results in context alignment metric compared to the baseline."}, {"title": "6 User Study", "content": "We conducted another round of user study to evaluate our AI delegate in both passive and proactive scenarios and leverage humans as the judge to explore whether the LLM-based evaluation in a wide range (Section 5) is aligned with human evaluation."}, {"title": "6.1 User study settings", "content": "We sampled five cases to assess whether our AI delegate meets user expectations. Three out of the five cases were passive, which require Al delegate to protect privacy when unnecessary information was requested during the conversation. The other two cases were proactive, which require AI delegate to share appropriate information to establish or deepen social connections with the conversation partner. In one of the proactive cases, the conversation partner had a negative attitude toward the desired goal of the conversation. In the other case, the conversation partner had a positive attitude about reaching the goal. The baseline and the evaluation metrics stays the same as the ones in the LLM-based evaluation experiments."}, {"title": "6.2 Participants", "content": "A total of 50 participants (15 male, 35 female), aged between 18 and 40 years old (mean = 36.37, SD = 6.27), took part in the study. For each case, participants were first provided with background information, including the names and relationship of the two parties, the setting, and the goal of the conversation. They were then shown the full conversation and asked to complete the assigned questions."}, {"title": "6.3 Results", "content": "The results from both the passive and proactive cases are presented in Figure 4. For the passive cases, we compared user-rated scores on privacy protection and context alignment. For the proactive cases, we evaluated the alignment of the AI delegate's disclosure strategy with user preferences, as well as the context alignment.\nStrong Performance in Privacy Protection. As illustrated in Figure 4, our AI delegate significantly outperformed the baseline in terms of privacy protection. The violin plot shows a higher mean score and a more concentrated distribution in the upper range for our method, indicating that human evaluator perceived our AI delegate as highly effective at safeguarding privacy during conversations. In contrast, a substantial portion of users rated the baseline system with a score of 2, signifying clear privacy leaks in its responses. This suggests that the baseline system struggled to effectively manage privacy constraints in more complex conversational scenarios.\nBetter Alignment of the Disclosure Strategy with Users. We evaluated the AI delegate's ability to achieve social goals by comparing the percentage of users whose disclosure strategies aligned with those of the AI delegate. The percentage reflects how closely the Al's behavior in proactively disclosing information matched the users' expectations of what should be shared in the given social context. As shown in Figure 4, our method outperformed the baseline by 18%, indicating that our Al delegate was more successful in adopting appropriate self-disclosure strategies that resonated with the preferences of the users.\nImproved Alignment to the conversational context. In the passive cases, our AI delegate achieved a mean score of 4.11, significantly outperforming the baseline scored at 3.35. The distribution of scores of our Al delegate was skewed towards the upper range, indicating that users consistently found its responses to be contextually appropriate and well-suited to the conversation. In contrast, the baseline exhibited a more varied distribution, with a notable presence of lower scores, reflecting its struggle to generate contextually aligned responses, particularly in more nuanced or complex social situations.In the proactive cases, the performance of both methods was more comparable, with our AI delegate scoring a mean of 3.90 and the baseline scoring 3.92. However, our AI delegate had fewer samples with low scores (1 and 2), indicating a more consistent ability to generate appropriate responses even in dynamic social interactions. This founding is contrary to the context alingment evaluated with LLM-based evaluators where LLM-based evaluators score similar or higher context alignment in baseline AI delegate. This might be caused by the fact that LLM-based evaluator favors answers generated by the same model [70] and marks the necessary of human evalutions in social conversations. Overally speaking, our results highlight the effectiveness of our dual-focus AI delegate in protecting privacy in passive cases while also performing reliably in proactive, self-disclosure situations."}, {"title": "7 Discussion", "content": null}, {"title": "7.1 Design Implications for LLM-based AI Delegate", "content": "Our study highlights key design implications for LLM-based AI delegates in social interactions. While privacy protection is crucial, Al delegates must also balance privacy protection and other social behaviors in realistic social interaction scenarios. This dual focus calls a change of research direction from a sole focus on privacy measure to a balanced focus on decision-making and goals in social conversations. Our findings further show that self-disclosure behavior can be achieved effectively with LLMs. As such, designing AI delegates that can strategically self-disclose without compromising privacy and take human-like strategies to build trust and achieve social objectives like deepening relationships or facilitating collaboration is a new direction worthy further exploration."}, {"title": "7.2 Limitation and Future work", "content": "The user study in our work is conducted by asking human to evaluate the AI delegates given conversation history and conversation contexts. This approach may introduce bias due to the gap between the third-person evaluator and the in-person conversation partners. In the future work, we will explore recuiting participants in pairs across different relationships, and conduct the conversation between the Al delegate and the human. Another direction of future work would focus on the multi-agent conversation scenario where the Al delegate must deal with multiple partners in a more complex conversation context which demands more intelligent disclosure strategy."}, {"title": "8 Conclusion", "content": "In this paper, we propose an Al delegate with a dual focus to protect privacy and make strategic self-disclosure in social conversations. To demonstrate the need of adopting AI delegates, we coduct the first survey on user preference of using AI delegates in real-life social interactions. We also adopt a multi-agent design framework to ensure that our AI delegate can deal with complex conversation contexts. Both the LLM-based evaluation and the user study on sampled cases demonstrate that our AI delegate performs much better in privacy protection and response appriopriateness in the passive conversations, even with adversarial attacks. And our AI delegate can also effectively fulfill the social goals with appropriated aligned self-disclosure strategies in the proactive conversations where self-disclosure is needed to achieve the goals."}, {"title": "A User Preference Survey", "content": null}, {"title": "A.1 12 types of daily routine tasks", "content": "The survey question regarding the acceptance of using AI delegates for 12 daily routine tasks is as follows:\nQuestion: \"Assuming an AI delegate can completely replicate your decision-making and behavioral characteristics, to what extent would you accept allowing the AI delegate to act and make decisions on your behalf in the following scenarios when it is inconvenient for you to be present?\"\nAnswer options: The participants were asked to rate their acceptance using a 5-point scale, which includes: \"Completely unacceptable, Somewhat unacceptable, Neutral, Somewhat acceptable, Completely acceptable\"\nTask descriptions: The descriptions of the 12 daily routine tasks are as follows:\n(1) Maintaining good family relationships: communicating with family via phone or video, handling family affairs, etc.\n(2) Maintaining close social needs: daily chatting with friends, sharing updates, social media interactions (like Moments, Facebook, Twitter, etc.)\n(3) Maintaining general social needs: keeping up with classmates and colleagues, company social events, posting on social media\n(4) Interacting with strangers online: such as posting on forums, leaving comments on videos\n(5) Handling private matters: making personal phone calls, replying to personal emails, etc.\n(6) Attending work meetings and discussions: joining video calls, participating in online discussions and communications, work reporting, etc.\n(7) Serving customers, maintaining customer relationships: solving customer issues via chat, email or phone, providing technical support and consultation, maintaining good relationships with customers\n(8) Professional consultation: consulting with doctors for medical advice and health inquiries\n(9) Online project management: coordinating and supervising project progress using project management tools, assigning tasks, tracking project status\n(10) Scheduling, purchasing, receiving and sending packages: scheduling doctor appointments, booking restaurants, placing orders for goods, etc.\n(11) Business negotiations, online bargaining\n(12) Online rights protection, appeals"}, {"title": "A.2 20 types of social relationships", "content": "Question: The survey question regarding the acceptance of using Al delegates to interact with people in 20 types of relationships is as follows:\n\"Assuming an Al delegate can completely replicate your decision-making and behavioral characteristics, to what extent would you accept allowing the Al delegate to interact with the people around you on your behalf? Consider the following relations when it is inconvenient for you to be present.\"\n5 options: The participants were asked to rate their acceptance using a 5-point scale, which includes: \"Completely unacceptable, Somewhat unacceptable, Neutral, Somewhat acceptable, Completely acceptable\"\nRelationship types: The 20 types of relationships are listed as follows:\n(1) Your husband, wife, lover\n(2) Your children\n(3) Your parents\n(4) Other family relatives (like siblings, grandparents, uncles, aunts, cousins, etc.)\n(5) Your close friends\n(6) Your normal friends, acquaintances\n(7) Your company colleagues, classmates\n(8) Your students, interns, subordinates\n(9) Your teachers, supervisors"}, {"title": "A.3 Clustering results", "content": "The clustered task/relationship to the three groups are provided in the following:\n\u2022 Intimate/Personal: Maintaining good family relationships (task-1); Maintaining close social needs (task-2); Handling private matters (task-5); Professional consultation (task-8); Husband/wife/lover (relationship-1); Children (relationship-2); Parents (relationship-3); Other family relatives (relationship-4); Close friends (relationship-5);\n\u2022 Social/Relational: Maintaining general social needs (task-3); Normal friends (relationship-6); Company colleagues/classmates (relationship-7); Students/interns/subordinates (relationship-8); Teachers/supervisors (relationship-9); Club/group members (relationship-12); Business partners (relationship-14); Service providers (relationship-15); Service recipients (relationship-16);\n\u2022 Transactional/Professiona: Interacting with strangers online (task-4); Attending work meetings and discussions (task-6); Serving customers, maintaining customer relationships (task-7); Online project management (task-9); Scheduling, purchasing, receiving and sending packages (task-10); Business negotiations (task-11); Online rights protection, appeals (task-12); Community members (relationship-10); Public service personnel (relationship-11); Strangers (relationship-13); People you dislike or have conflicts with (relationship-17); Competitors (relationship-18); Online relations (relationship-19); Volunteers or members of charity organizations (relationship-20);"}, {"title": "B Conversation Context Generation", "content": null}, {"title": "B.1 User persona description", "content": "The following list includes descriptions of high-level roles, partially drawing on [28"}]}