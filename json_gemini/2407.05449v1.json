{"title": "SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers for Text Detoxification", "authors": ["Elisei Rykov", "Konstantin Zaytsev", "Ivan Anisimov", "Alexandr Voronin"], "abstract": "This paper presents a solution for the Multilingual Text Detoxification task in the PAN-2024 competition of the SmurfCat team. Using data augmentation through machine translation and a special filtering procedure, we collected an additional multilingual parallel dataset for text detoxification. Using the obtained data, we fine-tuned several multilingual sequence-to-sequence models, such as mT0 and Aya, on a text detoxification task. We applied the ORPO alignment technique to the final model. Our final model has only 3.7 billion parameters and achieves state-of-the-art results for the Ukrainian language and near state-of-the-art results for other languages. In the competition, our team achieved first place in the automated evaluation with a score of 0.52 and second place in the final human evaluation with a score of 0.74.", "sections": [{"title": "1. Introduction", "content": "Multilingual text detoxification is a challenging subtask within text style transfer. The most difficult part is the adaptation of such a system to low-resource languages. The concept of PAN-2024 Multilingual Text Detoxification Task [1, 2] is to develop a multilingual text detoxification system for 9 languages: Amharic, Arabic, German, Spanish, Hindi, Chinese, Russian, Ukrainian and English.\nThis paper describes the solution of the SmurfCat team, which achieved first place with an average score of 0.52 in the automatic evaluation and second place with a score of 0.74 in the manual human evaluation. Our solution is based on the mT0 model family [3], which has powerful multilingual capabilities. We fine-tuned all our selected models to each language of the competition, and applied various data augmentation techniques. To improve detoxification, we performed hypothesis filtering using the diverse beam search algorithm [4]. Finally, we applied ORPO [5] alignment to enforce model predictions. Our 3.7-billion-parameter language model demonstrates state-of-the-art results for Ukrainian and near state-of-the-art results for other languages. We published the final best-performing model on the HuggingFace Hub'. You can also find the training scripts and the extended data on GitHub.\nThe rest of the paper is organized as follows: Section 2 discusses data augmentation strategies, Section 3 describes our final solution, and Section 4 presents the results and discussion."}, {"title": "2. Data", "content": "Initially, there were not many parallel datasets for the multilingual detoxification task. More precisely, primarily only the Russian and English ParaDetox datasets were available, with 11 100 and 19 700 samples respectively. During the competition, the organizers published a small human-annotated Multilingual ParaDetox for all languages, containing only 400 samples per language.\nNevertheless, we decided to augment the provided data by automatic translation from English to other languages. To translate the original English data, we used a GoogleTranslator model from deep_translator Python package. We chose API over some of the more advanced machine translation models because of its speed and simplicity. Also, there are not as many translators for low-resource languages like Amharic. As a result, we obtained an additional 19 700 samples for each language.\nSince translation is often imperfect, we decided to perform a specific post-processing procedure. In general, we checked the preservation of meaning after translation and the toxicity of the translated data. First, we used the LaBSE [6] model to evaluate the similarity between translated pairs. Second, we applied XLM-R toxicity classifier to check whether toxic sentences were still toxic after translation and vice versa.\nA distribution for both of two measures is shown on Figures 2, 3. For most samples, similarity between original and translated samples was high enough that many samples preserved their meaning. Regarding toxicity, many neutral sentences became toxic after translation, and many toxic sentences became neutral. For toxicity, we set a threshold parameter to 0.9 for toxic sentences and 0.1 for neutral sentences. The similarity threshold was set to 0.8 for all sentences.\nAfter all filtering steps, 40 500 pairs of neutral and toxic sentences were obtained. A more precise statistic of how many samples remain after filtering is given in Table 1. According to the statistics, Amharic lost the most samples during filtering.\nOur final dataset mixture is shown in Table 2. In total, 74 900 samples were used in the training process."}, {"title": "3. Method", "content": "In this section, we describe our prior method, related to fine-tuning and optimization of Language Models on the text detoxification task."}, {"title": "3.1. Supervised fine-tuning", "content": "As a main approach, we choose fine-tuning of various multilingual LMs. As we suggest, the most promising models for the further fine-tuning were LMs from mT0 family. It is a family of sequence- to-sequence Transformer models initialized from mT5 [7]. We considered that sequence-to-sequence modeling would be more preferable in case of the text detoxification task. The mT0 family was chosen because of its strong multilingual capabilities, so these models were adapted to each language of the competition. We also experimented with the novel Aya-101 model [8]: a fine-tuned mT5-xl model on a multilingual instructions.\nAll models were tuned in an almost similar way. The learning rate was set to 1e-5, the global batch size to 8, and the weight decay to 0.01. The cosine scheduler was used for training. In total, 4 all models were trained during 4 epochs. All other training parameters were default according to HuggingFace Seq2SeqTrainer. The only difference is that for the mT0-XL we updated the weights of the whole model because our computing resources allowed it. In case of a larger model like Aya-101 or mT0-XXL, only the LoRA adapter was trained. The setup of the LoRA adapter was as follows: r and lora alpha were set to 32, lora dropout parameter to 0.1, other parameters were default. The best model was selected according to the validation loss.\nTo enforce the in-context abilities of the models, we added a specific prefix to each toxic sentence depending on the language. As a result, we passed toxic sentences with special prefix prompt into the model during training."}, {"title": "3.2. The Best Candidate Choice", "content": "During inference, we generated 10 hypotheses and selected 5 most likely ones using diverse beam search. The number of beams was set to 10 with 5 beam groups, the diversity penalty was 2.5, the repetition penalty was 1.2. To select the best choice, we calculated a relevance metric using a product of similarity and toxicity scores. Similarity was calculated using LaBSE embeddings, and toxicity was measured using the xlm-roberta-large toxicity classifier. As relevance scores were calculated, we selected then the best candidate according to the highest score."}, {"title": "3.3. ORPO", "content": "Once the models were fine-tuned, we decided to further tune the model for best performance using Odds Ratio Preference Optimization (ORPO) approach. This optimization does not need a reference model like it is in DPO [9]. Alignment was employed on the unseen test dataset.\nAs a preference dataset, we generated hypotheses using diverse beam search on the samples from the test set and annotated them using the relevance score described above. Only candidates with the highest relevance scores were selected as the chosen ones, and all others were selected as the rejected samples.\nThe final ORPO data set for alignment contained the prompt (toxic sentence), the rejected sample (negative candidate), and the selected sample (best candidate). Table 3 shows a small sample of the dataset. Since the dataset was collected, we trained the model on the dataset using the same parameters used to train the other models. Since ORPO uses the beta parameter, it was set to 0.1. For the final submission, we used an aligned model with the algorithm described above to select the best candidate."}, {"title": "4. Results", "content": "The final results of the automatic evaluation are shown in the Table 4. The mT0-XL with ORPO alignment showed the best performance among all approaches from the leaderboard for all languages. Compared to mT0-XL, a model before ORPO alignment, ORPO slightly improved the performance of the model, increasing the average results by 0.01 points. Surprisingly, the larger models are not the best. For example, the mT0-XXL model with 13B parameters performed even worse than the mT0-XL model with only 3.7B parameters. Aya-101, an mT5-XXL model additionally tuned to instructional data for different languages, performed worse than other models. Since Aya-101 and mT0-XXL performed even worse on mt0-XL, we did not perform an ORPO alignment step for these models. Considering other teams on the automatic evaluation, our checkpoints, mainly mT0-XL-ORPO and mT0-XL, are the two best performing approaches for all languages except the Chinese language."}, {"title": "5. Conclusion", "content": "In conclusion, our system demonstrated a strong pipeline for augmenting training data for low-resource languages and further fine-tuning a relatively small 3.7 billion parameter language model for the text detoxification task. Our future research may consider how to adapt text detoxification capabilities from high-resource languages to low-resource languages without translation, as machine translation for low- resource languages often shows low quality. A further direction for investigation is the interpretability of models, specifically the understanding of which tokens have been replaced by the model through the text detoxification process and the rationale behind this."}]}