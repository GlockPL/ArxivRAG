{"title": "Private Language Models via Truncated Laplacian Mechanism", "authors": ["Tianhao Huang", "Tao Yang", "Ivan Habernal", "Lijie Hu", "Di Wang"], "abstract": "Deep learning models for NLP tasks are prone to variants of privacy attacks. To prevent privacy leakage, researchers have investigated word-level perturbations, relying on the formal guarantees of differential privacy (DP) in the embedding space. However, many existing approaches either achieve unsatisfactory performance in the high privacy regime when using the Laplacian or Gaussian mechanism, or resort to weaker relaxations of DP that are inferior to the canonical DP in terms of privacy strength. This raises the question of whether a new method for private word embedding can be designed to overcome these limitations.\nIn this paper, we propose a novel private embedding method called the high dimensional truncated Laplacian mechanism. Specifically, we introduce a non-trivial extension of the truncated Laplacian mechanism, which was previously only investigated in one-dimensional space cases. Theoretically, we show that our method has a lower variance compared to the previous private word embedding methods. To further validate its effectiveness, we conduct comprehensive experiments on private embedding and downstream tasks using three datasets. Remarkably, even in the high privacy regime, our approach only incurs a slight decrease in utility compared to the non-private scenario.", "sections": [{"title": "1 Introduction", "content": "The recent developments of deep learning have led to significant success in various tasks in Natural Language Processing (NLP), from next word prediction in mobile keyboards (Ramaswamy et al., 2019), to critical tasks like predicting patient health conditions from clinical records (Yao et al., 2019). However, such applications may always involve user-generated textual data as the training dataset, which contains sensitive information. To address privacy concerns, text anonymization (Anandan et al., 2012; Pil\u00e1n et al., 2022) has been commonly used, which involves identifying sensitive attributes and replacing them with alternative values. Nevertheless, such heuristic approaches become ineffective as deep neural networks often tend to memorize training data, making them susceptible to information leakage about the training data (Shokri et al., 2017; Carlini et al., 2021, 2019). One way that takes into account the limitations of existing approaches is designing Differentially Private (DP) algorithms. Differential privacy (Dwork et al., 2006) is resilient to arbitrary side information that might be available to attackers and has become a de facto method for private data analysis (Xiang et al., 2024; Wang et al., 2020b; Xiang et al., 2023; Wang et al., 2020a; Su et al., 2022; Wang et al., 2019b; Hu et al., 2023; Wang et al., 2023b, 2017, 2019a, 2018; Xue et al., 2021; Huai et al., 2019; Wang et al., 2023a; Wang and Xu, 2020; Huai et al., 2020; Hu et al., 2022).\nRecently, there has been significant research focusing on differentially private (DP) versions of word embedding from various perspectives (Yue et al., 2021; Feyisetan et al., 2019; Krishna et al., 2021; Feyisetan et al., 2020; Xu et al., 2021a,b; Carvalho et al., 2021b,a; Habernal, 2021, 2022). However, there are still some shortcomings in these approaches. On the one hand, several works consider adding Laplacian or Gaussian noise to the embedding space to ensure DP (Habernal, 2021; Krishna et al., 2021; Habernal, 2022). However, these mechanisms suffer from high noise levels, resulting in low utility, especially in the high privacy regime when the privacy parameter (e) is small. Moreover, these mechanisms can even alter the semantics of sentences (see Fig.1). On the other hand, there is a growing body of work that focuses on a relaxation of the canonical definition of DP, known as metric DP, which can achieve better performance. However, as a relaxed notion of DP, Metric DP cannot provide the same level of strong privacy guarantees as the canonical DP (Mattern et al., 2022). This raises the question of whether we can develop improved private word embedding mechanisms within the framework of canonical DP that can have comparable performance with existing metric DP-based methods.\nIn this paper, we provide an affirmative answer to the previous question by proposing a novel private mechanism for word embedding. Our approach is inspired by the superior performance of the truncated Laplacian mechanism in one-dimensional space (Geng et al., 2020). However, it remains unclear whether this superiority can extend to high dimensional cases, as directly extending the one-dimensional truncated Laplacian mechanism is challenging. To bridge this gap, we develop a high dimensional truncated Laplacian mechanism (TrLaplace), which is a non-trivial extension of the one-dimensional case. Theoretically, we show that compared with Laplacian and Gaussian mechanisms for private word embedding, TrLaplace-based private embedding has a lower variance. Moreover, we also conduct intensive experiments on both private embedding and downstream tasks to show our approach significantly outperforms the previous DP-based methods in the high privacy regime, and it will not drop much accuracy and utility compared with the non-private case. Moreover, compared to the existing metric DP-based method, our mechanism has even better performance for privacy tests while also keeping comparable performance for downstream tasks."}, {"title": "2 Related Work", "content": "Recent years have seen substantial advancements in language models within differential privacy (DP) frameworks. Due to the space limit, here we only mention the existing literature on private word embedding. We refer the readers to the survey (Hu et al., 2024) for more details.\nCurrent research on private word embeddings can be broadly categorized into two approaches: original DP-based methods and metric DP-based methods. The seminal work in the original DP category by Lyu et al. (2020b) introduces a framework utilizing the Unary Encoding mechanism. This approach was subsequently refined by Plant et al. (2021). Further improvements were made by Lyu et al. (2020a), who proposed a dropout technique for perturbed embeddings to enhance downstream task fairness. However, Qu et al. (2021) identify a critical privacy issue in Lyu et al. (2020a), noting that it requires access to users' raw data for fine-tuning during the training phase. Other notable contributions include works by Krishna et al. (2021), Habernal (2021), and Alnasser et al. (2021), who explore privatizing word embeddings. Krishna et al. (2021) and Alnasser et al. (2021) propose ADEPT, an auto-encoder-based DP algorithm. Unfortunately, Habernal (2021) points out that ADEPT is not differentially private by thorough theoretical proof. Igamberdiev et al. (2022) address reproducibility by providing source code for DP Auto-Encoder methods. In this paper, we aim to improve the performance of the mechanisms in Igamberdiev et al. (2022).\nIn the realm of metric DP, Feyisetan et al. (2020) first study this problem and provide a general perturbation-and-projection framework. Xu et al. (2020) reconsider this problem setting, replacing the Euclidean distance with the Mahalanobis distance to improve the utility. Subsequently, Xu et al. (2021b) introduce the Vickrey mechanism to further refine the utility in the projection step. To address the limitations of the multivariate Laplace mechanism, Xu et al. (2021a) and Carvalho et al. (2021b) propose a Truncated Gumbel Noise method. Feyisetan and Kasiviswanathan (2021) tackle high-dimensionality issues using random projection. Additionally, Feyisetan et al. (2019) define hyperbolic embeddings and utilize the Metropolis-Hastings algorithm for sampling"}, {"title": "3 Preliminaries", "content": "Differential Privacy is a data post-processing technique designed to ensure data privacy by adding confusion to potential attackers. Specifically, suppose there is one dataset noted as D, and we change or delete one data record in this dataset which we call D'. If the output distributions of D and D' are close enough, then we cannot distinguish these two distributions, i.e., we cannot infer whether the deleted or replaced data sample is really in this dataset. The formal details are given by (Dwork et al., 2006). Note that in the definition of DP, adjacency is a key notion. One of the commonly used adjacency definitions is that two datasets S and S' are adjacent (denoted as $S \\sim S'$) if $S'$ can be obtained by modifying one record in S.\nDefinition 1 Given a domain of dataset $X$. A randomized algorithm $A : X \\rightarrow R$ is $(\\epsilon, \\delta)$-differentially private (DP) if for all adjacent datasets $S, S'$ with each sample is in $X$ and for all $T \\subseteq R$, the following holds\n$\\text{Pr}(A(S) \\in T) \\leq \\exp(\\epsilon) \\text{Pr}(A(S') \\in T) + \\delta$.\nWhen $\\delta = 0$, we call the algorithm A is $\\epsilon$-DP.\nIn this work, we adopt a similar setting to previous research on private word embedding (Feyisetan et al., 2020; Xu et al., 2021a; Krishna et al., 2021). We consider a scenario where a user inputs a word w from a discrete fixed vocabulary W. Our goal is to preserve the user's privacy with respect to her/his word. To achieve this goal, we aim to design an algorithm that accepts w as input and whose distribution of output is close to the case where $w' \\in W$ is the input, with $w' \\neq w$ is any other word. From the attacker's perspective, based on the output, he cannot distinguish whether the user's input word is w or w' as their output distributions are almost the same. Formally, we have the following definition.\nDefinition 2 Given a discrete vocabulary $W$, a randomized algorithm $A : W \\rightarrow R$ is word-level $(\\epsilon, \\delta)$-differentially private (DP) if for all pair of words $w, w' \\in W$ and for all $T \\subseteq R$ we have $P(A(w) \\in T) \\leq e^{\\epsilon}P(A(w') \\in T) + \\delta$. When $\\delta = 0$, we call the algorithm A is $\\epsilon$-DP.\nIn this paper, we assume the user holds a sentence $S = w_1 w_2 \\dots w_n$ with n words. And we aim to design an $(\\epsilon, \\delta)$-DP algorithm, which is private w.r.t. each word $w_i$."}, {"title": "4 Private Embedding via Truncated Laplacian Mechanism", "content": "In this section, we will provide details of our method. Generally speaking, for each token $w_i$, to achieve DP, our approach consists of three steps. First, each token $w_i$ is mapped to an d-dimensional pre-trained word embedding $\\phi(w_i)$. And we perform a clipping step to get a clipped embedding:\n$\\text{CLIPEmb}(w_i) = \\phi(w_i) \\min\\{1, \\frac{C}{\\|\\phi(w_i)\\|_2}\\},$ (1)\nwhere the threshold $C > 0$ is a hyper-parameter. In the second step, we add some random noise to the clipped embedding vector to make it satisfies DP. Finally, we will perform the projection step by finding the nearest word $w_\\perp$ to the perturbed and clipped embedding vector within the embedding space:\n$w_\\perp = \\text{arg}\\min_{w \\in W} \\|\\phi(w) - \\text{CLIPEmb}(w_i) - \\eta\\|_2,$ (2)\nwhere $\\eta$ is the randomized noise we add in the second step. See Algorithm 1 for details.\nIt is notable that the goal of clipping is to make the $l_2$-norm of embedding vector be bounded so that we can adding noise to ensure DP, such as the Laplacian mechanism or Gaussian mechanism (Dwork and Roth, 2014).\nTheorem 1 (Laplacian Mechanism) Suppose $\\text{CLIPEmb}(w) \\in \\mathbb{R}^d$ denote the clipped embedding vector with threshold C. Then the"}, {"title": "5 Theoretical Sensitivity Analysis", "content": "In the last section, we introduce our truncated laplacian mechanism, we will analyze its sensitivity and proof our claim in this section.\nTheorem 3 Suppose $\\text{CLIPEmb}(w) \\in \\mathbb{R}^d$ is the clipped embedding vector with threshold C. Define $\\Delta_\\infty = 2C$ and $\\Delta_1 = 2\\sqrt{d}C$. For $\\epsilon < \\frac{2\\delta}{\\sqrt{d}}$, if\n$\\alpha = \\frac{\\Delta_1}{\\epsilon}$, $A = \\log(1 - \\frac{\\delta}{\\epsilon} \\frac{1}{\\sqrt{d}})$, \n$B = \\frac{\\epsilon}{2(1 - e^{-\\alpha A}e^{-\\alpha A}) \\alpha}$,\nthen the mechanism $\\mathcal{A}(w) = \\text{CLIPEmb}(w) + \\eta$ is $(\\epsilon, \\delta)$-DP, where $\\eta = (\\eta_1, \\dots, \\eta_1)$ and each $\\eta_i$ has the density function as in (3) with the above parameters."}, {"title": "7 Conclusions", "content": "We introduce a novel method called the high dimensional truncated Laplacian mechanism for private embedding, which extends the one-dimensional case to the high-dimensional case. Theoretical analysis demonstrates that our method exhibits lower variance compared to existing private word embedding techniques. Experiments show that even in the high privacy regime, our approach incurs only a minimal loss in utility compared to the non-private case, which maintains privacy while preserving the quality of embeddings for promising performance."}, {"title": "8 Limitations", "content": "First, the word level DP has the disadvantages of length constraints and linear growth of privacy budget (Mattern et al., 2022). However, such limitations are rooted in the definition of DP instead of our mechanism. Secondly, to ensure DP guarantees, in this paper, our mechanism involves clipping embedding vectors and adding calibrated noises, which inevitably introduce errors to the outputs of the task at hand. And these errors may affect different groups of individuals differently and may cause unfairness issues. However, we still need to mention that such unfairness issues are mainly due to the definition of DP rather than our method, as DP machine learning algorithms will always have a disparate impact on model accuracy (Bagdasaryan et al., 2019). Despite some limitations, word-level DP still offers unique advantages and potential applications (Hu et al., 2024), and brings value to the DP-NLP community."}, {"title": "9 Ethics Review", "content": "This paper presents work whose goal is to advance the field of NLP. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}