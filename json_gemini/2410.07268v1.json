{"title": "Learning Content-Aware Multi-Modal Joint Input Pruning via Birds'-Eye-View Representation", "authors": ["Yuxin Li", "Yiheng Li", "Xulei Yang", "Mengying Yu", "Zihang Huang", "Xiaojun Wu", "Chai Kiat Yeo"], "abstract": "In the landscape of autonomous driving, Bird's-Eye-View (BEV) representation has recently garnered substantial attention, serving as a transformative framework for the fusion of multi-modal sensor inputs. The BEV paradigm effectively shifts the sensor fusion challenge from a rule-based methodology to a data-centric approach, thereby facilitating more nuanced feature extraction from an array of heterogeneous sensors. Notwithstanding its evident merits, the computational overhead associated with BEV-based techniques often mandates high-capacity hardware infrastructure, thus posing challenges for practical, real-world implementations. To mitigate this limitation, we introduce a novel content-aware multi-modal joint input pruning technique. Our method leverages BEV as a shared anchor to algorithmically identify and eliminate non-essential sensor regions prior to their introduction into the perception model's backbone. We validate the efficacy of our approach through extensive experiments on the NuScenes dataset, demonstrating substantial computational efficiency without sacrificing perception accuracy. To the best of our knowledge, this work represents the first attempt to alleviate the computational burden from the input pruning point.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Bird's-Eye-View (BEV) representation has garnered considerable interest as a potent mechanism for sensor fusion at the feature layer within the domain of autonomous driving. By constructing a unified top-down representational space derived from heterogeneous sensors such as cameras and LiDARs, BEV-based methodologies [1], [2], [3], [4], [5] have exhibited a performance edge over conventional early and late fusion techniques, a superiority substantiated by their consistent high rankings on numerous public evaluation leaderboards. The intrinsic spatial coherence provided by BEV frameworks allows for a more nuanced and effective integration of diverse sensor modalities. Moreover, the BEV paradigm affords enhanced efficiency in the extraction of feature representations from voluminous datasets, thereby elevating the performance ceiling for an array of downstream tasks. As such, the implementation of multi-modal sensor fusion approaches within the BEV framework presents itself as a highly promising trajectory for advancing the perceptual efficacy of autonomous vehicular systems.\nNonetheless, the broader deployment of these advanced methodologies is considerably hampered by their extensive computational demands, frequently exceeding the processing capabilities of conventional on-board computing systems in autonomous vehicles. These state-of-the-art (SOTA) methods typically employ a diverse range of sensor inputs, including multi-view cameras and multiple LiDAR systems capable of scanning millions of points. Initially, these inputs are subjected to complex feature extraction via specialized backbone models, followed by a projection and fusion process in the BEV space, which subsequently serves as the common representational layer for a multitude of downstream tasks. It is noteworthy that the computational bottleneck is primarily attributable to the backbone models employed in multi-modal BEV-oriented frameworks. While the incorporation of such diverse sensor inputs undoubtedly enriches the feature extraction stage, it is crucial to recognize that not all regions within the multi-modal sensor space contribute equally to the performance or accuracy of downstream applications. Consequently, a pressing research challenge lies in the development of efficient algorithms that can judiciously harness the multi-faceted spatial and temporal information available from diverse sensors while simultaneously minimizing the computational burden intrinsic to such methodologies.\nInspired by the selective attention mechanisms inherent to human visual perception, we introduce a novel data-driven, content-aware, multi-modal joint input pruning approach to optimize the computational efficiency of BEV-based sensor fusion methods. Conventional Region-of-Interest (ROI) pruning techniques often resort to heuristic and geometry-based selectors that can result in imprecise object boundaries and are sensitive to sensor type disparities, thereby adversely affecting the perception task's performance. In contrast, as"}, {"title": "II. RELATED WORK", "content": "A. Perception in BEV Space\nPerception in the BEV space remains crucial in the realm of autonomous driving, offering a comprehensive understanding of the surrounding environment. Multiple strategies harness BEV representation to fuse LiDAR and camera inputs to foster reliable perception results for autonomous vehicles.\n3D Detection in BEV 3D object detection serves as a foundational task in autonomous driving, facilitating navigation and decision-making. A host of techniques now optimize 3D detection using BEV space features. Contemporary vision-based methods [1], [6], [4], [5], integrating multi-view camera data and depth supervision, have improved by 30 points over non-BEV techniques. Additionally, multi-modal BEV methods [7], [8], [9], [10] have achieved notable performance boost, with a 10-point improvement on the NuScenes leaderboard in recent years.\nSegmentation in BEV In map segmentation, a multitude of approaches [11], [12], [13], [14] have leveraged the BEV space for refined scene understanding. HDMapNet [11] predicts vectorized BEV elements via semantic and instance-level learning, while BEVSegFormer [12] employs a transformer-based setup for real-time segmentation, demonstrating resilience to camera noise.\nB. Model Pruning\nWithin the broader field of model compression, model pruning specifically addresses the challenge of streamlining neural network architectures for deployment in resource-limited environments. This approach seeks to optimize the efficiency of deep learning models by eliminating redundant or less significant nodes and branches."}, {"title": "III. METHODOLOGY", "content": "A. Problem Formulation\nThe primary goal is to reduce computational overhead in processing sensor inputs by removing redundant regions. This is achieved by designing a predictor that identifies unimportant regions in a BEV representation formed by fusing multi-modal sensor inputs (Camera and LiDAR).\nFormally, given a set of raw sensor inputs from both camera $C$ and LiDAR $L$, our goal is to find a function $f$ that maps these inputs to a reduced set of inputs by pruning irrelevant or redundant regions, such that:\n$f (C, L) \\rightarrow C', L'$\nwhere $C'$ and $L'$ are the pruned inputs, with the dimensions reduced for faster inference without significant loss of pertinent information.\nBird's-Eye-View Representation The BEV representation serves as an intermediary layer that combines features extracted from both camera and LiDAR inputs. For each sensor, we employ a backbone model to extract high-dimensional features. These features are then mapped into a common BEV space of dimensions $W \\times H \\times Z$, where each cell in this space corresponds to a physical occupancy in the 3D world.\n$BEV(C, L) = BEV(\\mathcal{B}(C), \\mathcal{B}(L))$\nwhere $\\mathcal{B}$ is the backbone model for feature extraction.\nPredictor Representation We let the predictor $P$ be trained jointly with a multi-modal perception model to identify and prune unimportant regions in the BEV representation. The predictor assigns an importance score $s_i$ to each cell in the BEV representation.\n$s_i = P(\\text{cell}_i)$\nwhere:\n$s_i \\in [0, 1]$"}, {"title": "B. Model Architecture", "content": "Our architectural design, inspired by the BEVFusion framework [7], has been meticulously adapted to cater to the specialized demands of joint pruning. As illustrated in Fig. 2, the architecture seamlessly integrates five pivotal components. Firstly, a sparse encoding backbone processes multi-view camera data, complemented by a counterpart dedicated to LiDAR point cloud inputs. The BEV encoder then synthesizes a cohesive feature map, merging the attributes from both the camera and LiDAR modalities. A perception task head is then incorporated, equipped for either 3D object detection or map segmentation. Parallel to the backbone models, a pruning index predictor is attached, which produces a binary mask facilitating the targeted removal of non-essential raw data inputs.\nLiDAR Backbone For the LiDAR backbone, we adapted the original VoxelNet [22] with a skip mechanism to bypass unnecessary cells in the voxelized 3D space. This skip operation is governed by the binary decision mask rendered by the pruning index predictor, enabling the efficient elimination of superfluous regions from the raw input.\nCamera Backbone Similarly for the camera backbone, we adapted the original DeiT [23] with a drop mechanism to categorize multi-view image into a voxel grid of dimensions $W \\times H \\times Z$ through input patchification, and subsequently glean preliminary features. These features then undergo semantic augmentation in the visual attention modules and are finally reshaped by a view transformer implemented by LSS [24] to constitute the BEV feature.\nIndex Predictor and Index Multiplier As illustrated in Fig. 2, the index predictor plays a crucial role in producing a pruning index of dimensions $W \\times H \\times Z$, which maps to discrete occupancy positions within a three-dimensional space. The index values are binary, with 0 indicating the exclusion of the corresponding region from the original sensor data, and 1 signifying its inclusion. Architecturally, the predictor comprises several residual blocks, processing front-view images to generate a binary mask as output.\nIn the inference phase, as depicted in Fig. 3, an index multiplier operation is utilized to apply the pruning index onto the voxelized inputs, selectively removing voxels indicated by a 0. This operation involves a straightforward linear algebra procedure that inverts the camera-to-BEV transformation process utilized in LSS [24], leveraging both the intrinsic and extrinsic parameters of the camera and LiDAR.\nBEV Encoder Following the original design of BEVFusion, we utilize Second [25] as the encoder to transcribe the BEV features. However, in this context, the BEV Encoder adopts an additional responsibility, which is to act as a comprehensive feature map, thereby facilitating the supervision of the pruning index predictor's training process.\nTask Heads Our model was subjected to rigorous evaluations on two distinct perception tasks: 3D detection and map segmentation, using the pruned BEV feature map. The detection head leverages the BEV feature as its foundation. Drawing from the CenterPoint [26] method, the prediction targets encompass the position, scale, orientation, and velocity of the entities in the autonomous driving scenarios. For the map segmentation task, the design parallels that of BEVFusion [7], with the prediction target being the segmentation masks representing the various road structures."}, {"title": "C. Multi-Modal Joint Input Pruning", "content": "Loss Function The joint training of the predictor with the perception model ensures the pruned inputs still contain significant information for the downstream task. The combined loss function $L_{total}$ is defined as:\n$L_{total} = L_{task} + \\alpha L_{cons} + \\beta L_{sparse} + \\gamma L_{penalty}$\nwhere $\\alpha, \\beta$, and $\\gamma$ are weights to balance the components. Each item in the total loss is explained as follows:\n1) Task Loss: Standard perception task loss (e.g., segmentation loss, detection loss). Denote as $L_{task}$.\n2) Consistency Loss: Measures the similarity between the perception model's outputs and the pruned model's outputs, denoted as $L_{cons}$. We adopted mean squared error (MSE) loss to measure the difference between the original and pruned BEV representation.\n$L_{cons} = ||BEV_{original} - BEV_{pruned}||_2$\n3) Sparsity Loss: Ensures the predictor mask is sparse enough based on the drop ratio, denoted as $L_{sparse}$. Given the binary mask $M$ where each element $M_i \\in \\{0, 1\\}$, the desired ratio $r$, and total number of elements $N$, the actual ratio of \"0\" values in $M$ is defined as:\n$r_{actual} = \\frac{\\sum_{i} (1 - M_i)}{N}$\nThe sparsity loss is:\n$L_{sparse} = (\\frac{\\sum_{i} (1 - M_i)}{N} - r)^2$\n4) Task Penalty Loss: This loss penalizes the predictor if the mask leads to a significant drop in perception performance, denoted as $L_{penalty}$. Let's consider $P_{masked}$ as the perception model's performance with the pruned input and $P_{original}$ as its performance without any pruning. Then:\n$L_{penalty} = \\lambda \\cdot max(0, P_{original} - P_{masked})$\nThis formulation ensures that $L_{mask}$ is zero if $P_{masked}$ surpasses or equals $P_{original}$.\nTraining Method In the training phase, our objective is to cultivate an efficient predictor capable of discerning the intricacies of the present scene and making judicious decisions regarding regions to omit. To realize this objective, we employ a comparative approach: the predictor is trained"}, {"title": "IV. EXPERIMENTS", "content": "A. Implementation Details\nOur implementation closely mirrors the configurations presented in BEVFusion [7], ensuring consistency in processing protocols. The dataset employed for our experiments is NuScenes. For 3D detection, our evaluation criteria encompass mAP and NDS (NuScenes Detection Score), while segmentation performance is gauged using the mIoU (Intersection over Union) metric. The original dimensions of the NuScenes dataset are 1600 \u00d7 900, which we rescale to an input resolution of 1408 \u00d7 512. The data augmentation pipeline comprises random transformations such as flipping, scaling, cropping, and rotation. To address the inherent class imbalance, we integrate a copy-paste mechanism, complemented by the Class-Balanced-Grouping-and-Sampling (CBGS) [36] technique during the training phase, akin to the approach employed by CenterPoint [26]. In the testing phase, scaling adjustments are made to align the image resolution with the model's input requirements.\nFor the camera modality, we deploy DeiT [23] as the primary backbone, employing LSS [24] for projecting 2D imagery into the BEV space. The LiDAR modality leverages the VoxelNet [22] backbone. Consistent with BEVFusion's methodology, we stack BEV features derived from both camera and LiDAR, subsequently integrating them via a convolutional network. The defined LiDAR input span and voxel range are (-51.2, -51.2, -8) to (51.2, 51.2, 8) and (0.1, 0.1, 0.2), respectively. The predictor is implemented with Resnet34 [37] with an output dimension of 128x128x16. The binarization threshold for the decision mask is set at 0.5. Each unit within this output correlates to the native sensor space, facilitated by intrinsic and extrinsic sensor parameters. LiDAR is designated as the primary sensor for pruning superfluous zones. Indices in the multi-view image domain that are misaligned are rounded to synchronize with the LiDAR domain.\nThe training regimen employs the AdamW optimizer, set with a learning rate of 1e-4 and a weight-decay of le-2. As proposed in [7], the learning rate is modulated to linearly ascend to le-3 during the initial 40% of the training regimen and subsequently diminish to zero. During the perception task training stage, we maintain a batch size of 4 across eight A100 GPU cards, training across 24 epochs for each experiment iteration and archiving the optimal model configuration for subsequent evaluations. During the subsequent three independent finetuning stages, we train for 24 epochs each and keep the best for next stage."}, {"title": "V. ABLATION STUDY", "content": "A. Effect of Different Pruning Methods\nIn this investigative study, we aim to evaluate the efficacy of our novel Multimodal-Joint-Pruning (MJP) approach against four recognized pruning methodologies: magnitude-based weight pruning [15], filter pruning [16], structural pruning [17], and a method akin to ours, learning-based pruning [19], with a particular focus on the 3D detection and map segmentation tasks. To enhance the study's efficiency and articulation, we reduce the standard training epoch duration to 12, subsequently reporting the average performance across the specified tasks."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a novel domain-specific pruning method for efficient perception tasks in BEV space. Our method leverages the BEV feature map as a shared anchor to jointly remove redundant sensor inputs before passing them into the backbone model and thus effectively reduce the model complexity. Our experiments show that our method can significantly reduce the model complexity with marginal performance tradeoff, leading to faster and more resource-efficient inference. For future work, we would like to explore a more efficient end-to-end training pipeline and other alternative methods for addressing the complexity of BEV perception methods."}]}