{"title": "A Many-Objective Problem Where Crossover is Provably Indispensable", "authors": ["Andre Opris"], "abstract": "This paper addresses theory in evolutionary multiobjective optimisation (EMO) and focuses on\nthe role of crossover operators in many-objective optimisation. The advantages of using crossover\nare hardly understood and rigorous runtime analyses with crossover are lagging far behind its use in\npractice, specifically in the case of more than two objectives. We present a many-objective problem\nclass together with a theoretical runtime analysis of the widely used NSGA-III to demonstrate\nthat crossover can yield an exponential speedup on the runtime. In particular, this algorithm can\nfind the Pareto set in expected polynomial time when using crossover while without crossover it\nrequires exponential time to even find a single Pareto-optimal point. To our knowledge, this is\nthe first rigorous runtime analysis in many-objective optimisation demonstrating an exponential\nperformance gap when using crossover for more than two objectives.", "sections": [{"title": "Introduction", "content": "Evolutionary multi-objective algorithms (EMOAs) mimic principles from natural evolution as muta-\ntion, crossover (recombination) and selection to evolve a population of solutions dealing with multiple\nconflicting objectives to explore a Pareto optimal set. Those have been frequently applied to a variety\nof multi-objective optimisation problems and also have several applications in practice [16, 7] such\nas scheduling problems [29], vehicle design [50] or practical combinatorial optimisation problems [35].\nThey are also widely used in machine learning, artificial intelligence, and various fields of engineer-\ning [41, 36, 43]. Particularly, in real world scenarios, there exist many problems with four or more\nobjectives [6, 8]. Thus, it is not unexpected that the study of EMOAs became a very important\narea of research in the last decades, especially for many objectives. However, when the number of\nobjectives increases, the size of the Pareto front and the number of incomparable solutions can grow\nexponentially and therefore, covering a high dimensional front, is a difficult task. There are already\nstrong differences between two and more objectives. NSGA-II [18], the most used EMOA, optimises\nbi-objective problems efficiently (see [32] for empirical results or [54, 13, 21, 12, 14] for rigorous run-\ntime analyses) while it perform less when dealing with three or more objectives (see [5] for empirical\nresults or [52] for rigorous negative results). The reason is that the so-called crowding distance, the\ntie breaker in NSGA-II, induces a sorting only for two objectives and therefore, Pareto-optimal search\npoints can be lost between generations. Hence, Deb and Jain [17] proposed NSGA-III, a refinement\nof the very popular NSGA-II, designed to handle more than two objectives, and instead of the crowd-\ning distance, uses reference points (previously set by the user) to guarantee that the solution set is\nwell-distributed across the objective space. In particular, Deb and Jain [17] empirically showed that\nNSGA-III can solve problems between 3 and 15 objectives efficiently. Due to its versatileness, it gained\nsignificant traction (~5500 citations) and now has sereval applications [47, 2, 28]. However, theoretical\nbreakthroughs on its success have only occurred recently. The first rigorous runtime analyses of the"}, {"title": "Preliminaries", "content": "Let $\\ln$ be the logarithm to base $e$ and $[m] := \\{1, ...,m\\}$ for $m\\in \\mathbb{N}$. For a finite set $A$ we denote by\n$|A|$ its cardinality. For two random variables $X$ and $Y$ on $\\mathbb{N}_0$ we say that $Y$ stochastically dominates\n$X$ if $\\mathbb{P}(Y < c) \\le \\mathbb{P}(X \\le c)$ for every $c > 0$. The number of ones in a bit string $x$ is denoted by $|x|_1$.\nThe number of leading zeros in $x$, denoted by $LZ(x)$, is the length of the longest prefix of $x$ which\ncontains only zeros, and the number of trailing zeros in $x$, denoted by $TZ(x)$, the length of the longest\nsuffix of $x$ containing only zeros respectively. For example, if $x = 00110110110000$, then $LZ(x) = 2$\nand $TZ(x) = 4$.\nThis paper is about many-objective optimisation, particularly the maximisation of a discrete $m$-\nobjective function $f(x) := (f_1(x),..., f_m(x))$ where $f_i: \\{0,1\\}^n \\rightarrow \\mathbb{N}_0$ for each $i \\in [m]$. When\n$m = 2$, the function is also called bi-objective. Let $f_{\\max}$ be the maximum possible value of $f$ in one\nobjective, i.e. $f_{\\max} := \\max\\{f_j(x) \\mid x \\in \\{0,1\\}^n, j\\in [m]\\}$. Denote by $\\vec{1} := (1, ...,1) \\in \\mathbb{N}^m$ the unit\nvector. For $N\\subseteq \\{0,1\\}^n$ let $f(N) := \\{f(x) \\mid x \\in N\\}$.\nDefinition 2.1. Consider an $m$-objective function $f$.\n1) Given two search points $x, y \\in \\{0,1\\}^n$, $x$ weakly dominates $y$, denoted by $x \\succeq y$, if $f_i(x) \\ge f_i(y)$\nfor all $i \\in [m]$ and $x$ (strictly) dominates $y$, denoted by $x > y$, if one inequality is strict; if\nneither $x \\succeq y$ nor $y \\succeq x$ then $x$ and $y$ are incomparable.\n2) A set $S \\subseteq \\{0,1\\}^n$ is a set of mutually incomparable solutions with respect to $f$ if all search points\nin $S$ are incomparable.\n3) Each solution not dominated by any other in $\\{0,1\\}^n$ is called Pareto-optimal. A mutually in-\ncomparable set of these solutions that covers all possible non-dominated fitness values is called a\nPareto(-optimal) set of $f$.\nThe NSGA-III algorithm [17] is shown in Algorithm 1 (compare also with [49] or [37]). At first,\na population of size $\\mu$ is generated by initialising $\\mu$ individuals uniformly at random. Then in each\ngeneration, a population $Q_t$ of $\\mu$ new offspring is created by conducting the following operations $\\mu/2$\ntimes. At first two parents $p_1$ and $p_2$ are chosen uniformly at random. Then 1-point crossover will be\napplied on $(p_1, p_2)$ with some probability $p_c \\in [0, 1)$ to produce two solutions $c_1, c_2$. If 1-point crossover\nis not executed (with probability $1 - p_c$), $c_1, c_2$ are exact copies of $p_1, p_2$. Finally, two offspring $s_1$ and\n$s_2$ are created with standard bit mutation on $c_1$ and $c_2$, i.e. by flipping each bit independently with\nprobability $1/n$.\nDuring the survival selection, the parent and offspring populations $P_t$ and $Q_t$ are merged into $R_t$,\nand then partitioned into layers $F_{t+1}^1, F_{t+1}^2,...$ using the non-dominated sorting algorithm [18]. The\nlayer $F_{t+1}^1$ consists of all non-dominated points, and $F_{t+1}^i$ for $i > 1$ consists of points that are only\ndominated by those from $F_{t+1}^1,..., F_{t+1}^{i-1}$. Then the critical and unique index $i^*$ with $\\sum_{j=1}^{i^*-1}|F_{t+1}^j|< \\mu$\nand $\\sum_{j=1}^{i^*}|F_{t+1}^j| \\ge \\mu$ is determined (i.e. there are fewer than $\\mu$ search points in $R_t$ with a lower rank\nthan $i^*$, but at least $\\mu$ search points with rank at most $i^*$). All individuals with a smaller rank than\n$i^*$ are taken into $P_{t+1}$ and the remaining points are chosen from $F_{t+1}^{i^*}$ with Algorithm 2."}, {"title": "The Many-Objective Royal-Road Function", "content": "In this section, we define the many-objective REALROYALROAD function which we denote by m-RRMO.\nFix $m \\in \\mathbb{N}$ divisible by 2 and let $n$ be divisible by $5m/2$. For a bit string $x$ let $x^i := (x_{(i-1)\\cdot 2n/m+1},..., x_{i\\cdot 2n/m})$ where\nall $x^i$ are of equal length $2n/m$. Let $B := \\{y \\in \\{0,1\\}^{2n/m} \\mid |y|_1 = 6n/(5m), LZ(y)+TZ(y) = 4n/(5m)\\}$\nand $A := \\{y \\in \\{0,1\\}^{2n/m} \\mid |y|_1 = 8n/(5m), LZ(y) + TZ(y) = 2n/(5m)\\}$ refer to the substring $x^i$. The\nfollowing sets refer to the whole bit string, and are needed to partition the search space accordingly.\n*   $L := \\{x \\mid 0 < |x^i|_1 \\le 6n/(5m) \\text{ for all } j \\in [m/2], |x^i|_1 < 6n/(5m) \\text{ for an } i \\in [m/2]\\}$,\n*   $M := \\{x \\mid |x^i|_1 = 6n/(5m) \\text{ for all } j \\in [m/2] \\text{ and } x^i \\notin B \\text{ for an } i \\in [m/2]\\}$,\n*   $N := \\{x \\mid x^i \\in A \\cup B \\text{ for all } j\\in [m/2]\\}$.\nDefinition 3.1. The function class m-RRMO : $\\{0,1\\}^n \\rightarrow \\mathbb{N}^m$, is defined as\nm-RRMO(x) = ($f_1(x), f_2(x), ..., f_m(x)$)"}, {"title": "Crossover Guarantees Polynomial Time", "content": "Now we show that for NSGA-III can find the whole Pareto set of RRMO in expected polynomial time.\nTheorem 4.1. Let $m \\in \\mathbb{N}$ be any constant divisible by 2. Then the algorithm NSGA-III (Algorithm 1)\nwith $p_c \\in (0,1)$, $\\epsilon_{\\text{nad}} \\ge 2n/5 + 2n/m$, a set $R_p$ of reference points as defined above for $p \\in \\mathbb{N}$ with\n$p\\ge 2m^{3/2}(2n/5+2n/m)$, and a population size $\\mu > c(4n/(5m)+1)^{m-1}$ for a constant $c \\in \\mathbb{N}$ becoming\n1 if $m = 2$, $\\mu \\in 2^{O(n)}$, finds the Pareto set of $f := m$-RRMO in expected $O(n^3/(1-p_c)+p_c)$ generations\nand $O(\\mu n^3/(1 - p_c) + \\mu p_c)$ fitness evaluations.\nProof. Note that $f_{\\max} = 2n/5+2n/m$ by noticing that $|K_1(x)| \\le m/2$ and $|x^i|_1 + LZ(x^i), |x^i|_1 +\nTZ(x^i) \\le 2n/m$. So during the whole optimisation procedure we may apply Lemma 2.2. Further, we\nuse the method of typical runs [48, Section 11] and divide a run into several phases. For every phase\nwe compute the expected waiting time to reach one of the next phases. A phase can be skipped if the\ngoal of a later phase is achieved.\nPhase 1: Create x with $f(x) \\neq 0$.\nLet x be initialised uniformly at random. By a classical Chernoff bound the probability that 0 <\n$|x^i|_1 \\le 6n/(5m)$ for every $j \\in \\{1,...,m/2\\}$ is $1 - e^{-\\Omega(n)}$ since the expected value of ones in one\nblock of a search point is $1/2\\cdot 2n/m = n/m$ after initialisation. Hence, the probability that every\nindividual has fitness zero after initialisation is $e^{-\\Omega(\\mu n)}$. If this event occurs, the probability is at\nleast $n^{-n}$ to create any individual with mutation (no matter if crossover is executed) and hence, one\nwith fitness distinct from 0. So the expected number of generations to finish this phase is at most\n$(1 - e^{-\\Omega(\\mu n)}) + e^{-\\Omega(\\mu n)}n^n = 1 + o(1)$.\nPhase 2: Create x with $|x^i|_1 = 6n/(5m)$ for all $j \\in [m/2]$."}, {"title": "Difficulty of NSGA-III Without Crossover", "content": "Finally, we point out that NSGA-III without crossover (i.e. when $p_c = 0$) becomes extremely slow.\nThis even holds for finding the first Pareto optimal point.\nTheorem 5.1. Suppose that m is a constant divisible by 2. NSGA-III (Algorithm 1) on m-RRMO\nwith $p_c = 0$, any choice of $R_p$, and $\\mu$ polynomial in n needs at least $n^{\\Omega(n)}$ generations in expectation\nto create any Pareto-optimal search point of m-RRMO.\nProof. We see with probability of $2^{-\\Omega(n)}$ that an individual $x$ with $0 < |x^i|_1 \\le 6n/(5m)$ for every\n$i \\in [m/2]$ initialises with probability $1 - 2^{-\\Omega(n)}$. Hence, by a union bound, with probability $1 -\n\\mu 2^{-\\Omega(n)} = 1 - o(1)$ every individual $a$ initialises with $0 < |x^i|_1 \\le 6n/(5m)$ for every $i \\in [m/2]$ since\n$\\mu\\in \\text{poly}(n)$. Suppose that this happens. Then the algorithm will always reject search points with\nfitness zero. Therefore, it is required to flip $2n/(5m)$ many zeros at once even to create a search point\ny with $y \\in A$ for any $i \\in [m/2]$. This happens with probability $n^{-\\Omega(n)}$. So the expected number of\nneeded generations in total is at least $(1 - o(1))(n^{\\Omega(n)} /\\mu) = n^{\\Omega(n)}$.\nAs in [13],a similar result can be also formulated for a general class of $(\\mu + \\lambda)$ elisist blackbox\nalgorithms, i.e. unary blackbox algorithms, which use so-called unary unbiased variation operators [27]\nwhich generalise standard bit mutation. The proof of that result is very similar to the proof of the\ncorresponding theorem in [13]."}, {"title": "Conclusions", "content": "We defined m-RRMO, a variant of the bi-objective RRMO-function proposed by [13], for the many\nobjective setting on which the EMO algorithm NSGA-III using crossover for a constant $0 < p_c < 1$\nand a constant number of objectives m can find the whole Pareto set in expected $O(n^3)$ generations\nand $O(\\mu n^3)$ fitness evaluations. As for other many-objective function classes like LOTZ, OMM and\nCOCZ, the upper bound on the expected number of generations behaves asymptotically independently\nof $\\mu$ and m. On the other hand, if crossover is disabled, NSGA-III requires exponential time to even\nfind a single Pareto-optimal point. This is the first proof for an exponential performance disparity\nfor the use of crossover in the many-objective setting, particularly for NSGA-III. However, we are\nconfident that for the m-ONEJUMPZEROJUMPk benchmark proposed by Zheng and Doerr [53], the\nmany-objective version of the bi-objective ONEJUMPZEROJUMP [21], crossover provably guarantees a\nsubexponential speedup of order O(n). We hope that our work may serve as a stepping stone towards\na better understanding of the advantages of crossover on more complex problem classes, as it has been\ndone in single-objective optimisation."}]}