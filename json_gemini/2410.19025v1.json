{"title": "Large Language Models for Financial Aid in Financial Time-series Forecasting", "authors": ["Md Khairul Islam", "Ayush Karmacharya", "Timothy Sue", "Judy Fox"], "abstract": "Considering the difficulty of financial time series forecasting in financial aid, much of the current research focuses on leveraging big data analytics in financial services. One modern approach is to utilize \"predictive analysis\", analogous to forecasting financial trends. However, many of these time series data in Financial Aid (FA) pose unique challenges due to limited historical datasets and high dimensional financial information, which hinder the development of effective predictive models that balance accuracy with efficient runtime and memory usage. Pre-trained foundation models are employed to address these challenging tasks. We use state-of-the-art time series models including pre-trained LLMs (GPT-2 as the backbone), transformers, and linear models to demonstrate their ability to outperform traditional approaches, even with minimal (\"few-shot\") or no fine-tuning (\"zero-shot\"). Our benchmark study, which includes financial aid with seven other time series tasks, shows the potential of using LLMs for scarce financial datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "The advancement of AI has taken over many domains including the field of financial market [1] and big data [2]. In particular, financial time series forecasting has improved significantly from using statistical models to machine learning [3] and then deep learning [4]. These financial forecasting areas include currency exchange rate [5], [6], stock market [2] [5] [4], commodity prices [7] [8] and more. Pre-trained foundation models, such as large language models (LLMs) have driven the progress in Natural Language Processing (NLP) and Computer Vision (CV). Foundation models like GPT [9], and Vision Transformer [10] can perform well on a diverse range of tasks in few-shot (little training) or zero-shot (no training) learning. This enables applications where historical data is limited or mostly missing.\nFinancial time series forecasting (FTSF) is an important domain that needs more attention among the multivariate time series forecasting tasks. Previous works on FTSF rely heavily on machine learning [3] or traditional deep learning [4] methods. With some recent works on multi-modal FTSF [11] [12]. Financial aid (FA) is crucial to many students' educational journeys, providing the resources needed to pursue academic dreams while fostering educational equity. However, the process, including tasks like processing the free application for Federal Student Aid (FAFSA), is often manual, time-consuming, and susceptible to errors. Access to historical datasets is limited to yearly intervals and is subject to changes in policy.\nWe describe and evaluate LLM-based foundation models in the FTSF domain using 8 deep-learning models and compare especially the financial aid with 7 other financial datasets. Our research questions are,\n\u2022 Q1: Which models are better as few-shot learners?\n\u2022 Q2: Can pre-trained LLMs perform zero-shot learning?\nAnswering these questions will help us better understand the current advancement of LLMs for financial time series forecasting. In summary, our contributions are,\n\u2022 Collect eight datasets from four financial domains (Stock, Commodity, Currency, Institution) for over 10 years.\n\u2022 Benchmark five state-of-the-art time series deep learning models, and three LLM-based foundation models on these datasets.\n\u2022 Open source code and datasets at GitHub to facilitate full reproducibility and further research in this domain."}, {"title": "II. METHODOLOGY", "content": "Given the input dataset, $X \\in R^{F \\times T}$, $T$ denotes the total timesteps in days and $F$ input features (including past targets and other features). With a lookback window of $L$ past days, the input at time $t$ is $X_t = X_{t-(L-1):t}$ which contains inputs of the last $L$ days. Given this input $X_t$, the model $f$ predicts the targets $O$ (e.g. stock prices) for the next $T_{max}$ days. The target output $\\hat{y}_t$ at time $t$ can be expressed as,\n$\\hat{y}_t = f(X_t)$, where,\n$X_t = X_{t-(L-1):t} = [X_{t-(L-1)}, X_{t-(L-2)},\\ldots,x_t]$  (1)\n$= {xf,l,t}, f \\in {1,\\ldots, F}, l\\in {1,\\ldots,L}$\nFor the financial aid data, funds allocated to each state are a time series with $T$ years (2004 to 2020). A lookback $L$ of 10 years is used to predict funds ($O$) for the next year ($T_{max}$ = 1). For all other datasets, we have daily inputs for 10 years. With a lookback window of the past 96 days ($L$ = 96), we predict the targets for the next 24 days ($T_{max}$ = 24)."}, {"title": "C. Models", "content": "We use the following time series models in our work. The models are chosen based on their popularity and recently published work. We focus on point forecasting in our work. A high-level overview of how pre-trained LLMs are fine-tuned for custom datasets is illustrated in Figure 2.\nLLM Foundation Models: The LLM-based foundation models are selected based on their versatility in time series forecasting. We use the configurations from [15]. The pre-trained foundation models are frozen except for the last layer when fine-tuning. These models use a pre-trained GPT-2 [9] as the LLM backbone. We select the following recent models: (1) TimeLLM [16] (2) CALF [17] (3) GPT4TS (One Fits All, [18]).\nTraditional Models: We choose the following recent non-pre-trained models: (1) DLinear [19] (2) iTransformer [20] (3) TimesNet [6] (4) PatchTST [21] (5) TimeMixer [22]. Most of these models are Transformer-based and have shown great performance in capturing temporal patterns."}, {"title": "D. Implementation Details", "content": "We use the PyTorch framework and follow [6] [15] to implement our experiments. Each experiment runs three times with different random seeds (648, 506, 608), and the average results are presented. Following [18] [17] we use the pre-trained GPT2 as the backbone for the LLMs and only fine-tune the output layer during training shown in Fig. 2. The"}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "In this section, we investigate the research questions, the setup, and the results."}, {"title": "A. Q1. Which models are better as few-shot learners?", "content": "Pre-trained models are preferred largely due to their generalizability and good performance in few-shot or zero-shot learning settings [24]. Since these LLMs are already trained on many datasets, they often outperform the other models when few training data are available [17] or without training [14].\nFollowing [17], we select the last 10% training data to train the models in a few-shot learning setting.\nResults. The few-shot learning results are shown in Table III. All model performance drops significantly after reducing the train data size. This is due to the models' inability to learn enough temporal patterns from the limited input. TimeLLM performs the best overall (three best and four 2nd best cases). While PatchTST performs the 2nd best with a close margin (three best and two 2nd best cases). DLinears performance drops significantly as it is a simple linear model. However, overall the LLMs performed better in the few-shot learning."}, {"title": "B. Q2. Can LLMs perform zero-shot learning in FTSF?", "content": "Real-world scenarios can often have no available past observations (i.e. new company stock in the market, newly launched product). Having zero-shot learning ability is crucial in forecasting those cases since traditional deep learning time series models are unable to train and forecast those cases. We investigate whether LLMs can effectively assist in those cases. We load the pre-trained LLMs and evaluate them on the test set without fine-tuning. Since no training is done in this part, we exclude the traditional time series models from this analysis.\nResults. Table IV shows the zero-shot results of the LLMs. Compared to Q1, the results achieved here are significantly worse. Since each financial data may have distinct temporal patterns, without fine-tuning the LLMs fail to forecast them effectively. We conclude, LLMs are yet not quite effective for zero-shot learning for financial time series."}, {"title": "IV. RELATED WORKS", "content": "Deep learning for time series has significantly outperformed machine learning approaches [22] [20], also in finance [4]. [5] used RNN models to forecast stock market prices, and currency exchange rates. Many recent deep learning models have been used to forecast the stock market [2] [5] [4], commodity prices [7] [8]. Foundation models in time series have recently gained significant attention [24]. Pre-trained LLMs and vision models have been enhanced for time series. [14] [25] showed the ability of LLMs to perform in zero-shot and few-shot settings in time series tasks. GPT4TS [18] leverages pre-trained language models without altering important layers. TimeLLM [16] reprograms LLM's ability to reason with time series data by proposing a prompt-as-prefix technique. CALF [17] proposed a novel fine-tuning framework to reduce the distribution discrepancy between textual and temporal data. Chronos [26] performed significantly in probabilistic forecasting."}, {"title": "V. CONCLUSION AND FUTURE WORKS", "content": "In this paper, we benchmark financial datasets from multiple domains using state-of-the-art time series models and LLM-based foundation models. Our results show that LLMs are more effective for few-shot and zero-shot learning. Especially, the few-shot and zero-shot capabilities of LLMS can be effective for financial Aid practitioners who are currently unable to apply deep learning methods due to limited data availability. We focus on point forecasting with a single modality in this work. Incorporating data from different financial modalities into time series models will be future work, and probabilistic forecasting can help the financial domain by outputting a probabilistic distribution. Our research highlights the potential of foundation LLMs in financial aid and the overall finance time series forecasting domain."}]}