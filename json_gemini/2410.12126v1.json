{"title": "Parametric Graph Representations in the Era of Foundation Models: A Survey and Position", "authors": ["Dongqi Fu", "Liri Fang", "Zihao Li", "Hanghang Tong", "Vetle I. Torvik", "Jingrui He"], "abstract": "Graphs have been widely used in the past decades of big data and AI to model comprehensive relational data. When analyzing a graph's statistical properties, graph laws serve as essential tools for parameterizing its structure. Identifying meaningful graph laws can significantly enhance the effectiveness of various applications, such as graph generation and link prediction. Facing the large-scale foundation model developments nowadays, the study of graph laws reveals new research potential, e.g., providing multi-modal information for graph neural representation learning and breaking the domain inconsistency of different graph data. In this survey, we first review the previous study of graph laws from multiple perspectives, i.e., macroscope and microscope of graphs, low-order and high-order graphs, static and dynamic graphs, different observation spaces, and newly proposed graph parameters. After we review various real-world applications benefiting from the guidance of graph laws, we conclude the paper with current challenges and future research directions.", "sections": [{"title": "1 Introduction", "content": "In the era of big data and AI, graphs are popular data structures for modeling the complex relationships between entities. Also, graph-based research (e.g., graph mining and graph representation learning) provides the foundation for many real-world applications, such as recommender system [22, 46, 49, 51], social network analysis [23, 35], information retrieval [32, 36, 45], anomaly detection [17, 63, 64], natural language processing [7, 50, 65], computer vision [8, 27], AI4Science [15, 18, 59], etc.\nTo model those real-world tasks within graphs, graph representations are indispensable middleware that provides the basis for specific and complex task-oriented computations. To be specific, graph rep-resentations can be decomposed into three aspects, (1) graph embedding (i.e., vector representation), (2) graph law (i.e., parametric representation), and (3) graph visualization (i.e., visual representation). First of all, graph representations can be in the form of embedding matrices, i.e., the graph topological information and attributes are encoded into matrices, which has been widely discussed and studied in the research community and usually can be referred to as graph representation learning. [20]. Then, graphs can be also represented by plotting directly for a better human-understandable illustration. For example, one interesting research topic is how to plot the graph topological structures into the 2D space with less structure distortion. More interesting works can be referred to [16]. Last but not least, graphs can also be represented by a few key parameters such as Erd\u0151s-R\u00e9nyi random graph G(n, p) [12], where n stands for the number of nodes in the graph, and p stands for the independent edge connection probability in the graph. As shown in Figure 2, these three representations can have overlapping to mutually contribute to each other [16]."}, {"title": "1.1 Motivation of this Paper", "content": "In the modern graph deep learning community, graph embedding (also referred to as graph repre-sentation learning) has attracted unprecedented research attention varying from unsupervised (e.g., Node2Vec [19]) to semi-supervised (e.g., GCN [28]), and also the novel neural architecture from transformer (e.g., GraphTransformer [13]). However, the graph law (also referred to as graph parametric representation) has great research potential, yet the corresponding research stays in a not fully exploited stage.\nAt the beginning, we would point out \"What is Graph Law or Graph Parametric Representation?\". In general, it is referred to as finding some key parameters and their relations to describe the given graph, which description is expected to preserve the entire or part of the property of the given graph. According to the previous research [33?, 34], the rigorous graph law (or graph parametric representation) relies two fundamental steps. The first step is to decide which parameters we use to describe the graph, e.g., the node degree. The second step is to observe the graph in a statistical manner and then determine the value of parameters, the distribution of parameters, the relation among parameters, etc. For example, the relation between the possibility of a newly-arrived node connecting to an old node (parameter 1) and the degree of that old node (parameter 2) is studied by maximum likelihood estimation (MLE) based on the observed real-world graph data [34].\nThen, we discuss why studying the graph parametric representation is important in the graph research community nowadays with the following two concrete examples."}, {"title": "1.2 Organization of this Paper", "content": "Graph law is the study of investigating the statistical properties of graphs. In this survey, we introduce the graph laws studies in the macroscopic view and microscopic view, plus multiple angles like low-order and high-order connections, static and dynamic graphs, as shown in Table 1 and Figure 3.\n\u2022 Macroscopic and Microscopic Views. The macroscopic graph laws describe the graph prop-erties in a global view, like how the total degree (or eigenvalues) distribution of the entire graph looks like [33]; while the microscopic laws try to focus on the individual behavior and investigate their behaviors as part in the entire graph [34].\n\u2022 Low-Order and High-Order Connections. Most graph laws are based on the node-level connections (i.e., low-order connections), while some graph law investigations are based on the group activities (high-order connections), i.e., motif in [40, 57], hyperedge in [10, 29], and simplex in [6, 9].\n\u2022 Static and Dynamic Graphs. Compared with static graphs, dynamic graphs allow the graph components like topology structure and node attributes to evolve over time. Correspondingly, some graph laws study how the graph parameters change over time and their temporal relations. Note that, in some graph research, the dynamics are created by the algorithms, like adding virtual"}, {"title": "2 Macroscopic Graph Laws", "content": "In this section, we introduce the graph laws from the macroscope and microscope. In detail, we will introduce what is the intuition of researchers proposing or using graph statistical properties as parameters and how they fit the value of parameters against real-world observations.\nSeveral classical theories model the growth of graphs, for example, Barabasi-Albert model [4, 5] assumes that the graphs follow the uniform growth pattern in terms of the number of nodes, and the Bass model [38] and the Susceptible-Infected model [2] follow the Sigmoid growth (more random graph models can be founded in [12]). However, these pre-defined graph growths have been tested that they could not handle the complex real-world network growth patterns very well [30, 56]. To this end, researchers begin to fit the graph growth on real-world networks directly, to discover graph laws."}, {"title": "2.1 Low-Order Macroscopic Laws", "content": "Based on fitting nine real-world temporal graphs from four different domains, the authors in [33] found two temporal graph laws, called (1) Densification Laws and (2) Shrinking Diameters, respectively. First, the densification law states as follows.\n$e(t) \\propto n(t)^{\\alpha}$ (1)\nwhere e(t) denotes the number of edges at time t, n(t) denotes the number of nodes at time t, \u03b1 \u2208 [1, 2] is an exponent representing the density degree. The second law, shrinking diameters, states that the effective diameter is decreasing as the network grows, in most cases. Here, the diameter means the node-pair shortest distance, and the effective diameter of the graph means the minimum distance d such that approximately 90% of all connected pairs are reachable by a path of length at most d. Later, in [56], the densification law gets in-depth confirmed on four different real social networks, the research shows that the number of nodes and number of edges both grown exponentially with time, i.e., following the power-law distribution."}, {"title": "2.2 High-Order Macroscopic Laws", "content": "Above discoveries are based on the node-level connections (i.e., low-order connections), then several researchers start the investigation based on the group activities, for example, motifs [40], simplices [6] and hyperedges [10, 29]. Motif is defined as a subgraph induced by a sequence of selected temporal edges in [40], where the authors discovered that different domain networks have significantly different numbers of similar motifs, and different motifs usually occur at different time. Similar laws are also discovered in [6] that the authors study 19 graph data sets from domains like biology, medicine, social networks, and the web, to characterize how high-order structure emerges and differs in different domains. They discovered that the higher-order Egonet features can discriminate the domain of the graph, and the probability of simplicial closure events typically increases with additional edges or tie strength.\nIn hypergraphs, each hyperedge could connect an arbitrary number of nodes, rather than two [10], where the authors found that real-world static hypergraphs obey the following properties: (1) Giant Connected Components, that there is a connected component comprising a large proportion of nodes, and this proportion is significantly larger than that of the second-largest connected component. (2) Heavy-Tailed Degree Distributions, that high-degree nodes are more likely to form new links. (3) Small Effective Diameters, that most connected pairs can be reached by a small distance (4) High Clustering Coefficients, that the global average of local clustering coefficient is high. (5) Skewed Singularvalue Distributions, that the singular-value distribution is usually heavy-tailed. Later, the evolution of real-word hypergraphs is investigated in [29], and the following laws are discovered.\n\u2022 Diminishing Overlaps: The overall overlaps of hyperedges decrease over time.\n\u2022 Densification: The average degrees increase over time.\n\u2022 Shrinking Diameter: The effective diameters decrease over time.\nTo be specific, given a hypergraph G(t) = (V(t), E(t)), the density of interactions is stated as follows.\n$DoI(G(t)) = \\frac{|\\{\\{e_i, e_j\\} | e_i \\cap e_j \\neq \\emptyset \\text{ for } e_i, e_j \\in E(t)\\} |}{\\{\\{e_i, e_j\\}|e_i, e_j \\in E(t)\\} }$ (2)\nand the densification is stated as follows.\n$|E(t)| \\propto |V(t)|^s$ (3)\nwhere s > 1 stands for the density term.\nIn heterogeneous information networks (where nodes and edges can have multiple types), the power law distribution is also discovered [47]. For example, for the triplet \"author-paper-venue\" (i.e., A-P-V), the number of authors is power law distributed w.r.t the number of A-P-V instances composed by an author."}, {"title": "3 Microscopic Graph Laws", "content": "In contrast to representing the whole distribution of the entire graph, many researchers try to model individual behavior and investigate how they interact with each other to see the evolution pattern microscopically."}, {"title": "3.1 Low-Order Microscopic Laws", "content": "In [34], the authors view temporal graphs in a three-fold process, i.e., node arrival (determining how many nodes will be added), edge initiation (how many edges will be added), and edge destination (where are each edge will be added). They ignore the deletion of nodes and edges, and they assign variables (models) to parameterize this process.\n\u2022 Edge Attachment with Locality (an inserted edge closing an open triangle): It is responsible for the edge destination.\n\u2022 Node Lifetime and Time Gap between Emitting Edges: It is responsible for edge initiation.\n\u2022 Node Arrival Rate: It is responsible for the node arrival.\nTo model the individual behaviors, there are many candidate models for selection. For example, in edge attachment, the probability of a newcomer u to connect the node v can be proportional to v's current degree or v's current age or the combination. Based on fitting each model to the real-world observation under the supervision of MLE principle, the authors empirically choose the random-random model for edge attachment with locality, i.e., first, let node u choose a neighbor v uniformly and let v uniform randomly choose u's neighbor w to close a triangle. And node lifetime and time gap between emitting edges are defined as follows.\n$a(u) = t_{d_t(u)}(u) \u2013 t_1(u)$ (4)\nwhere a(u) stand for the age of node u, $t_k(u)$ is the time when node u links its kth edge, $d_t(u)$ denote the degree of node u at time t, and $d(u) = d_T(u)$. T is the final timestamp of the data.\n$\\delta_u(d) = t_{d+1}(u) \u2013 t_d(u)$ (5)\nwhere $\\delta_u(d)$ records the time gap between the current time and the time when that node emits its last edge. Finding the node arrival is a regression process in [34], for example, in Flickr graph N(t) = exp(0.25t), and N(t) = 3900$t^2$ + 76000t \u2013 130000 in LinkedIn graph.\nIn [41, 52], the selection of edge attachment gets flourished where the authors propose several variants of edge attachment models for preserving the graph properties. With respect to the triangle closure phenomenon, several in-depth researches follow up. For example, in [24], researchers found that (1) the stronger the third tie (the interaction frequency of the closed edge) is, the less likely the first two ties are weakened; (2) when the stronger the first two ties are, the more likely they are weakened."}, {"title": "3.2 High-Order Microscopic Laws", "content": "Hypergraph ego-network [9] is a structure defined to model the high-order interactions involving an individual node. The star ego-network T(u) is defined as follows.\n$T(u) = \\{s : (u \\in s)\\},\\forall s \\in S$ (6)\nwhere S is the set of all hyperedges (or simplices). Also, in [9], there are other hypergraph ego-networks, like radial ego-network R(u) and contracted ego-network C(u). The relationship between them is as follows.\n$T(u) \\subseteq R(u) \\subseteq C(u)$ (7)\nIn [9], authors observe that contiguous hyperedges (simplices) in an ego-network tend to have relatively large interactions with each other, which suggests that temporally adjacent high-order interactions have high similarity, i.e., the same nodes tend to appear in neighboring simplices.\nIn [57], authors try to model the temporal graph growth in terms of motif evolution activities. In brief, this paper investigates how many motifs change and what are the exact motif types in each time interval and fits the arrival rate parameter of each type of motif against the whole observed temporal graph."}, {"title": "4 Some New Observation Space and Newly Discovered Graph Parameters", "content": "4.1 New Different Spaces\nIn [14], the power law is revisited based on the eigendecomposition and singular value decomposition to provide guidance on the presence of power laws in terms of the degree distribution, singular"}, {"title": "4.2 New Parameters", "content": "Currently, if not all, most graph law research focuses on the traditional graph properties, like the number of nodes, number of edges, degrees, diameters, eigenvalues, and singular values. Here, we provide some recently proposed graph properties, although they have not yet been tested on the scale for fitting the graph law on real-world networks.\nThe local closure coefficient [54] is defined as the fraction of length-2 paths (wedges) emanating from the head node (of the wedge) that induce a triangle, i.e., starting from a seed node of a wedge, how many wedges are closed. According to [54], features extracted within the constraints of the local closure coefficient can improve the link prediction accuracy. The local closure efficient of node u is defined as follows.\n$H(u) = \\frac{2T(u)}{W^h(u)}$\nwhere $W^{(h)}(u)$ is the number of wedges where u stands for the head of the wedge, and T(u) denotes the number of triangles that contain node u.\nThe density of states (or spectral density) [11] is defined as follows.\n$\\mu(\\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} \\delta(\\lambda - \\lambda_i), \\quad f(x) \\mu(x) = \\text{trace}(f(H))$ (8)\nwhere H denotes any symmetric graph matrix, $ \\lambda_1,..., \\lambda_N$ denote the eigenvalues of H in the ascending order, 8 stands for the Dirac delta function and f is any analytic test function."}, {"title": "5 Law-Guided Research Tasks", "content": "The discovered graph laws describe the graph property, which provides guidance to many down-streaming tasks. Some examples are discussed below."}, {"title": "5.1 Graph Generation", "content": "If not all, in most of graph law studies [10, 29, 33, 34, 41, 56, 57], after the law (i.e., evolution pattern) is discovered, a follow-up action is to propose the corresponding graph generative model to test whether there is a realizable graph generator could generate graphs while preserving the discovered law in terms of graph properties. Also, graph generation tasks have impactful application scenarios like drug design and protein discovery [59].\nFor example, in [33], the Forest Fire model is proposed to preserve the macroscopic graph law while larges preserve the discovered evolution pattern.\n\u2022 First, node v first chooses an ambassador (i.e., node w) uniformly random, and establish a link to w;\n\u2022 Second, node v generates a random value x, and selects x links of node w, where selecting in-links r times less than out-links;\n\u2022 Third, node v forms links to w's neighbors; this step executes recursively (neighbors of neigh-bors) until v dies out.\nThis proposed Forest Fire model holds the following graph properties most of time.\n\u2022 Heavy-tailed In-degrees: The highly linked nodes can easily get reached, i.e., \"rich get richer\".\n\u2022 Communities: A newcomer copies neighbors of its ambassador.\n\u2022 Heavy-tailed Out-degrees: The recursive nature produces large out-degree."}, {"title": "5.2 Link Prediction", "content": "To learn node representation vectors for predicting links between node pairs and contributing latent applications like recommender systems, CAW-N [48] is proposed by inserting causal anonymous walks (CAWs) into the representation learning process. The CAW is a sequence of time -aware adjacent nodes, the authors claim that the extracted CAW sequence obeys the triadic closure law. To be specific, the temporal opening and closed triangles can be preserved in the extracted CAW sequence W. Further, to realize the inductive link prediction, CAW-N replaces the identification of each node in W with the relative position information, such that the CAW sequence W is transferred into anonymous W. Then, the entire \u0174 is inserted into an RNN-like model and gets the embedding vector of each node, the loss function states as follows.\n$\\text{enc}(\\hat{W}) = RNN(\\{f_1(I_{caw}(w_i)) \\oplus f_2(t_{i-1} \u2013 t_i)\\}_{i=0,1,...,|\\hat{W}|})$ (9)\nwhere $I_{caw}(w_i)$ is the anonymous identification of node i in \u0174, $f_1$ is the node embedding function realized by a multi-layer perceptron, $f_2$ is the time kernel function for representing a discrete time by a vector, and \u2295 denotes the concatenation operation. The training loss comes from predicting negative (disconnected) node pairs and positive (connected) node pairs.\nAlso, there are some related link prediction models based on the guidance of static graph laws during the representation learning process, for example, SEAL [58] and HHNE [47].\nIn the SEAL framework [58], for each target link, SEAL extracts a local enclosing subgraph around it, and uses a GNN to learn general graph structure features for link prediction. The corresponding graph parameters include but are not limited to\n\u2022 Common Neighbors: Number of common neighbors of two nodes.\n\u2022 Jaccard: Jaccard similarity on the set of neighbors of two nodes.\n\u2022 Preferential Attachment: The product of the cardinal of the sets of neighbors of two nodes.\n\u2022 Katz Index: The summarization over the collection of paths of two nodes."}, {"title": "5.3 Natural Language Processing", "content": "To obtain the semantic representation vector of each word in the corpus, GloVe [42] is proposed, which has been considered as one of the most popular word embedding models. GloVe utilizes the"}, {"title": "6 Future Directions", "content": "In this section, we would like to list several interesting research directions of graph parametric representation in modern graph research."}, {"title": "6.1 Graph Laws on Temporal Graphs", "content": "Discovering accurate temporal graph laws from real-world networks heavily relies on the number of networks and the size of networks (e.g., number of nodes, number of edges, and time duration). However, some of the temporal graph law studies mentioned above usually consider the number of graphs ranging from 10 to 20, when they discover the evolution pattern. The existence of time-dependent structure and feature information increases the difficulty of collecting real-world temporal graph data. To obtain robust and accurate (temporal) graph laws, we may need a considerably large amount of (temporal) network data available. Luckily, we have seen some pioneering work like TGB [26] and TUDataset [39]."}, {"title": "6.2 Graph Laws on Heterogeneous Networks", "content": "Though many graph laws have been proposed and verified on homogeneous graphs, real-world networks are usually heterogeneous [43] and contain a large number of interacting, multi-typed com-ponents. While the existing work [47] only studied 2 datasets to propose and verify the heterogeneous graph power law, the potential exists for a transition in graph laws from homogeneous networks to heterogeneous networks, suggesting the presence of additional parameters contributing to the comprehensive information within heterogeneous networks. For example, in an academic network, the paper citation subgraph and the author collaboration subgraph may have their own subgraph laws which affect other subgraphs' laws. Furthermore, Knowledge graphs, as a special group of heterogeneous networks, have not yet attracted much attention from the research community to study their laws."}, {"title": "6.3 Transferability of Graph Laws", "content": "As we can see in the front part of the paper, many nascent graph laws are described verbally without the exact mathematical expression, which hinders the transfer from the graph law to the numerical constraints for the representation learning process. One latent reason for this phenomenon is that selecting appropriate models and parameters and fitting the exact values of parameters on large evolving graphs are very computationally demanding."}, {"title": "6.4 Taxonomy of Graph Laws", "content": "After we discovered many graph laws, is there any taxonomy or hierarchy of those? For example, graph law A stands in the superclass of graph law B, and when we preserve graph law A during the representation, we actually have already preserved graph law B. For example, there is a hierarchy of different computer vision tasks, recently discovered [55]. And corresponding research on graph law development seems like a promising direction."}, {"title": "6.5 Domain-Specific Graph Laws", "content": "Since graphs serve as general data representations with extreme diversity, it is challenging to find universal graph laws that fit all graph domains because each domain may be internally different from another [60]. In fact, in many cases, we have prior knowledge about the domain of a graph, which can be a social network, a protein network, or a transportation network. Thus, it is possible to study the domain-specific graph laws that work well on only a portion of graphs and then apply the graph laws only on those graphs."}, {"title": "6.6 Graph Laws with LLMS", "content": "In the background of large language models (LLMs) developments, an interesting question attracts much research interest nowadays, i.e., can LLMs replace GNNs as the backbone model for graphs? To answer this question, many recent works show the great efforts [21, 25, 53], where the key point is how to represent the structural information as the input for LLMs.\nFor example, Instruct-GLM [53] follows the manner of instruction tuning and makes the template T of a 2-hop connection for a central node v as follows.\n$\\Tau(v, A) = \\{v\\} \\text{ is connected with } \\{\\|v_2|v_2\\in A\\} \\text{ within two hops.}$ (12)\nwhere A represents the list of node v's k-hop neighbors.\nAs discussed above, the topological information (e.g., 1-hop or 2-hop connections) can serve as external modality information to contribute to (e.g., through prompting) the reasoning ability of large language models (LLMs) [25] and achieve state-of-the-art on low-order tasks like node classification and link prediction.\nTherefore, a natural question can be asked, i.e., instead of inputting local topological information to LLMs, how can we bring global topological information for LLMs to understand and make inferences for high-order tasks like graph classification, graph matching, and graph alignment? To the best of our knowledge, corresponding research still remains nascent but has great potential. Finding a proper graph parametric representation in a macroscopic way may be a viable solution for LLMs to comprehend graph-level information."}, {"title": "7 Conclusion", "content": "Within the survey, we first review the concepts and developing progress of graph parametric repre-sentations (i.e., graph laws) from different perspectives like microscope and microscope, low-order and high-order connections, and static and temporal graphs. We then discuss various real-world application tasks that can benefit the study of graph parametric representations. Finally, we envision the latent challenges and opportunities of graph parametric representations in modern graph research with several interesting and possible future directions."}]}