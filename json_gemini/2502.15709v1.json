{"title": "TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation", "authors": ["Zhaoxing LI", "Vahid Yazdanpanah", "Jindi Wang", "Wen Gu", "Lei Shi", "Alexandra I. Cristea", "Sarah Kiden", "Sebastian Stein"], "abstract": "The integration of AI in education offers significant potential to enhance learning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and Llama, allow students to query a wide range of topics, providing unprecedented flexibility. However, LLMs face challenges, such as handling varying content relevance and lack of personalization. To address these challenges, we propose TutorLLM, a personalized learning recommender LLM system based on Knowledge Tracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM lies in its unique combination of KT and RAG techniques with LLMs, which enables dynamic retrieval of context-specific knowledge and provides personalized learning recommendations based on the student's personal learning state. Specifically, this integration allows TutorLLM to tailor responses based on individual learning states predicted by the Multi-Features with Latent Relations BERT-based KT (MLFBK) model and to enhance response accuracy with a Scraper model. The evaluation includes user assessment questionnaires and performance metrics, demonstrating a 10% improvement in user satisfaction and a 5% increase in quiz scores compared to using general LLMs alone.", "sections": [{"title": "1 INTRODUCTION", "content": "Al techniques are increasingly affecting various aspects of daily life, notably in educational environments. Al offers significant op-portunities to enhance both the learning process and efficiency for students. Prominent among these AI applications are Large Lan-guage Models (LLMs), such as ChatGPT\u00b9, Gemini 2, and Llama 3, which allow students to query a wide array of topics, thus offer-ing unprecedented flexibility in learning. Unlike traditional search engines, LLMs enable students to ask nuanced and complex ques-tions, engage in conversational interactions, and seek clarifications through follow-up questions, enhancing the depth and effective-ness of the learning experience. Despite these advantages, current LLMs face several challenges. These include generating inaccurate information (commonly referred to as \u201challucinations\u201d), lack of per-sonalization, and varying content relevance [1, 10, 25]. Specifically, these models often struggle with problems requiring high-level logical and mathematical problems, such as solving complex equa-tions or providing step-by-step logical reasoning, and fail to tailor responses to individual learning levels, sometimes providing overly generalized answers or requiring extensive prompting to yield use-ful information [25]."}, {"title": "2 RELATED WORK", "content": "Large Language Models (LLMs), which have advanced natural language processing (NLP) and understanding (NLU), have signif-icantly impacted various fields, including education [12]. These models, trained on vast text data, can generate human-like text, comprehend complex queries, and provide detailed explanations [5]. Examples like OpenAI's GPT-3 and GPT-4 are used in educational tools for tutoring, answering questions, and generating study ma-terials [4, 8]. However, LLMs can produce incorrect or misleading information (\"hallucinations\u201d), often lack personalized responses tailored to individual users' learning levels, and sometimes require significant prompting to be useful [23, 25].\nEducational Recommender Systems provide personalized learn-ing experiences by tailoring educational content based on students' needs, preferences, and progress [20]. Traditional systems use col-laborative filtering, content-based filtering, or hybrid approaches to suggest relevant resources. Combining these systems with AI technologies like LLMs and KT models can significantly enhance personalization and performance in learning outcomes [6, 26].\nKnowledge Tracing monitors and predicts students' knowledge states over time by tracking interactions with learning materials and assessments [9, 19]. It enables personalized recommendations and targeted interventions to address the knowledge gaps of the stu-dents. Traditional methods like Bayesian Knowledge Tracing (BKT) [18] and Deep Knowledge Tracing (DKT) [19] have been widely used in Educational Recommender Systems, leveraging historical data to predict future performance and learning needs [16, 17]. Re-cent models, such as Multi-Features with Latent Relations BERT Knowledge Tracing (MLFBK), enhance the accuracy and depth of predictions [7, 13, 15, 22]."}, {"title": "3 IMPLEMENTATION OF TUTORLLM", "content": "TutorLLM consists of three main components: the Scraper Model for collecting educational content, the KT for predicting students' learning states, and the RAG based LLM for dynamically retrieving information and tailoring personalized responses.\nOverall Methodology. The motivation of our approach is to enable LLMs to comprehend the student's learning state (or knowl-edge master state) and the specific content of the ongoing course, thereby furnishing contextualized responses and tailored learning content recommendations. Thus, our method is structured into three distinct components. Firstly, the KT component utilizes an algorithm to trace students' learning state. This encompasses trac-ing skill mastery, ability profiles, problem difficulty, and predicting the next most probable action for the student. We employ MLFBK as our KT method within this component. Secondly, the Scraper function is designed to gather and organize the text information from the online course platform, including captions and subtitles of videos embedded in the web pages, into a background knowledge base. After receiving student action sequence data from the KT component, the TutorLLM first synthesizes the student's current learning status, focusing on identifying weak knowledge areas and predicting the student's potential next actions. Finally, the model gives personalized answers and recommendations based on the background knowledge base and students' learning state. Addi-tionally, the model recommends additional learning materials to"}, {"title": "4 USER STUDY", "content": "Participants & Study Setup. To evaluate the effectiveness of the TutorLLM educational tool on student learning outcomes, 30 first-year undergraduate students from XXX University (anonymous for review) enrolled in a linear algebra module were randomly allocated to one of three groups in a controlled experiment. The control group exclusively used general LLMs (the general web version of chatGPT4) for the duration of the study. Experimental Group 1 was exposed to general LLMs during the first week, followed by TutorLLM in the second week, whereas Experimental Group 2 utilized TutorLLM throughout both weeks. The key objective was to determine if integrating TutorLLM could enhance learning performance compared to the general LLMs.\nStudent Performance. Over a two-week period, students ex-plore a new segment of linear algebra each day. They utilize our TutorLLM to address their queries daily during their studies. At the course's conclusion, TutorLLM suggests pertinent educational resources and exercises for further learning. Students have the autonomy to decide whether to engage with these additional ma-terials, unlike those in the control group who use a general LLM model and do not receive such recommendations. Each day culmi-nates with a quiz testing the knowledge acquired, leading up to a comprehensive exam covering all topics at the end of the two weeks. The assessment framework includes 15 tests overall, with each daily test comprising 10 questions and the potential to score up to 100 points. These daily quizzes and the final comprehensive test together determine the students' overall performance.\nUser Study. A questionnaire was administered to students in Group 1 and Group 2 to evaluate the effectiveness of the TutorLLM and its impact on student satisfaction. The survey was designed to gather quantitative and qualitative data on students' experiences with the TutorLLM. It incorporated a System Usability Scale (SUS) [2] and a User Experience (UX) [24] questionnaire to provide a comprehensive assessment. SUS, a validated tool, measures user perceptions of usability, focusing on user-friendliness and com-prehensibility, with higher SUS scores indicating a more intuitive interface and enhanced user control. UX was measured from three perspectives, which are User Satisfaction (US), Comfort Level (CL) and Continue Willingness (CW). US was evaluated using a 5-point Likert scale to assess the overall satisfaction with the tool, where a positive US score reflects the assistant's ability to meet or exceed user expectations regarding usefulness and responsiveness. CL was rated on a 5-point scale to gauge user comfort during initial tex-tual interactions on online platforms, providing critical insights for optimizing chat interfaces and enhancing interaction experiences [3]. CW, measured on a 5-point scale, captures users' willingness to continue interactions after the initial conversation [11]."}, {"title": "5 RESULTS", "content": "Test Results. We evaluated the student performance of students across three control groups through 15 tests, including a final ex-amination. The groups comprised students utilizing general LLMs, a hybrid LLM approach, and TutorLLM throughout the study. Our initial analysis revealed that the TutorLLM group achieved the highest overall mean score of 74.48, followed by the hybrid LLM group at 72.81 and the general LLM group at 71.97. To determine the statistical significance of the differences observed in the final exam scores among the different groups, we conducted an analy-sis of variance (ANOVA). This analysis produced an F-statistic of 0.795 and a p-value of 0.462, indicating no significant variance in performance across the groups. Figure 3 displays the daily mean scores for each study group. Although the differences were not statistically significant, there was a discernible trend toward im-proved performance in the TutorLLM group. These results suggest that additional research, potentially involving a larger sample size, different study designs, or a longer test duration, is necessary to definitively ascertain the effectiveness of integrating TutorLLM in enhancing academic performance.\nUsability and User Experience. From the usability perspective, the SUS results indicated a generally positive user experience, with an average SUS score of 76.35 and a median score of 79, reflect-ing high usability. The scores showed moderate variability, with a standard deviation of 14.46, and the majority of scores ranged between 61.89 and 90.81, demonstrating consistency in positive evaluations. The Shapiro-Wilk test confirmed the normality of the score distribution (p = 0.721), supporting the validity of the usabil-ity assessment. A bootstrap analysis estimated the median's 95% confidence interval to be between 68.5 and 83, further validating the positive user feedback on the TutorLLM's usability.\nFrom the user experience perspective, the analysis of CL, CW, and US showed average scores of 3.50 for CL, 3.40 for CW, and 3.61 for US, with respective standard deviations of 1.00, 0.99, and 0.82. Correlation analysis revealed moderate positive correlations among the metrics CL and CW at 0.53, CL and US at 0.45, and CW and US at 0.40, suggesting that users felt more comfortable and satisfied with the app and were more willing to continue using it."}, {"title": "6 DISCUSSION", "content": "While our study did not show statistically significant improvements in academic performance with TutorLLM, the increased engage-ment and higher average scores suggest its potential benefits. We monitored the duration of LLM usage among students. Users of Tu-torLLM spent 36% more time engaging with the system compared to those using general LLMs. This increased engagement suggests that students were more satisfied with TutorLLM, making them more willing to invest additional time in the system. Additionally, the learning materials recommended by our TutorLLM appear to have a positive impact on students' learning performance to a cer-tain extent. These findings underscore the importance of further refining Al-driven educational tools to better adapt to individual learning needs and contexts. It also could redefine personalized learning, making educational interactions more effective and en-gaging through tailored content and intelligent response systems."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduced TutorLLM, a novel framework that integrates Large LLMs with KT and RAG to enhance personalized learning. Our main contributions include being the first to com-bine LLMs with KT to improve personalization. We demonstrated the practical application of TutorLLM through a Chrome browser plugin and validated its effectiveness in a two-week study with undergraduate students. Future research should focus on refining TutorLLM's personalization features, testing its effectiveness across various disciplines, and addressing challenges such as integrating existing technologies, ensuring data privacy, and providing educator training."}]}