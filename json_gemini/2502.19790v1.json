{"title": "Mixtera: A Data Plane for Foundation Model Training", "authors": ["Maximilian B\u00f6ther", "Xiaozhe Yao", "Tolga Kerimoglu", "Ana Klimovic"], "abstract": "State-of-the-art large language and vision models are trained over trillions of tokens that are aggregated from a large variety of sources. As training data collections grow, manually managing the samples becomes time-consuming, tedious, and prone to errors. Yet recent research shows that the data mixture and the order in which samples are visited during training can significantly influence model accuracy. We build and present MIXTERA, a data plane for foundation model training that enables users to declaratively express which data samples should be used in which proportion and in which order during training. MIXTERA is a centralized, read-only layer that is deployed on top of existing training data collections and can be declaratively queried. It operates independently of the filesystem structure and supports mixtures across arbitrary properties (e.g., language, source dataset) as well as dynamic adjustment of the mixture based on model feedback. We experimentally evaluate MIXTERA and show that our implementation does not bottleneck training and scales to 256 GH200 superchips. We demonstrate how MIXTERA supports recent advancements in mixing strategies by implementing the proposed Adaptive Data Optimization (ADO) algorithm in the system and evaluating its performance impact. We also explore the role of mixtures for vision-language models.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language and vision models (often called foundation models) have become omnipresent in our daily life. They show enormous capabilities in a diverse set of tasks [8, 9, 38, 46, 52], such as assistance with writing and coding, video understanding, and even agentic interaction with the world. The training of such language and vision models (LLMs/VLMs) presents new challenges for managing training data due to the ever-growing sizes of models and datasets. To achieve high accuracy, state-of-the-art models train over trillions of tokens from aggregated data collections such as RedPajama [68], Dolma [60], or FineWeb [48]. For example, Meta's Llama 3 70B model is trained on a corpus of 15 trillion tokens [38, 39]. These collections are built based on data from various sources, such as Wikipedia or Common Crawl dumps.\nTraining data is typically stored on distributed filesystems in GPU clusters or data lakes in the cloud, and managed manually and ad hoc by ML engineers (Figure 2a). Selecting the right proportions out of this data with particular characteristics (e.g., language, topic, source) for training is critical for model performance [12, 69, 72]. Individual users write ad hoc scripts to process the training data, filter relevant subsets with the properties of interest, often pre-tokenize it, and then mix it for their use case, e.g., they might want to train on 50% data from Wikipedia and 50% from movie subtitles. This reflects our experience working with ML researchers as part of the Swiss AI initiative which aims to develop open-source LLMs [16]. We confirmed through discussions with industry teams that such setups are common. This can get complex as training data may need to be mixed based on a variety of characteristics. For example, in addition to satisfying source data proportions (Wikipedia vs. movie subtitles in the previous example), we may also want the training data to be 80% in English and 20% in German. Recent works show that the data mixture should also be adjusted during training. The SMOLLM2 model is trained with four stages of data mixtures [4]. Algorithms such as Adaptive Data Optimization (ADO) [26], AIOLI [11], PIKE [31], and SKILL-IT [12] even propose to adjust the mixture dynamically based on the model behavior (e.g., loss per domain) during training. Figure 1 shows that using ADO can increase the performance of LLMs over a static mixture across different scales on the downstream HellaSwag benchmark [75].\nToday, there is no open-source system that automatically manages vast amounts of training data based on fine-grained characteristics. Implementing data filtering and subsequent mixing requires users to manually keep track of metadata. At least in the open-source world, this is typically done as part of the directory structure of the filesystem, e.g., there is one subdirectory per source dataset, and then we sample from each directory (Figure 2a). This approach is limited because each data sample has multiple properties that can be used to determine whether it should be used for training. Filesystems fundamentally do not offer the right interface for managing training data and mixing, as they do not provide declarative query interfaces or a native way to track which model was trained on what data. Running a data processing job to fully materialize the mixed training set for each training run has lots of overhead and leads to data duplication. A streaming-oriented approach that selects, tokenizes, and mixes data on the fly provides much more flexibility and eases experimentation. Utilizing a full-fledged DBMS for tracking data properties would burden ML engineers with complexities such as database administration, schema design, and performance tuning. We need a lightweight data plane that enables users to declaratively query and mix data based on arbitrary properties, independent of the filesystem structure, and to adjust this mixture dynamically.\nWe present MIXTERA, a data plane that can be deployed on top of existing LLM/VLM training data collections. It is a centralized, read-only layer and can be declaratively queried from training clients. As shown in Figure 2b, it reduces the amount of materialization and data scripting necessary for training jobs. While existing data loaders as outlined in Table 1 are limited by filesystem constraints, MIXTERA supports arbitrary properties independent of the filesystem, and makes it easy for model engineers to experiment with different filter criteria, fixed learning curricula, and fully dynamic mixing algorithms that learn the mixture during training. MIXTERA follows a client-server model. For a training job, the server, which indexes and manages the samples, first statically filters out the relevant samples, and then continuously distributes chunks to the clients. Chunks are fixed-size collections of pointers to samples in files, and adhere to the current mixture. The clients then fetch the relevant samples and return the relevant data to the training loop. We design and implement MIXTERA tailored to the needs for foundation model training, and make the following key contributions:\n(1) MIXTERA enables users to declaratively specify mixtures across properties independent of the filesystem structure and training frameworks by indexing all samples and their properties.\n(2) MIXTERA enables dynamically changing the data mixture and the properties determining the mixture during training. It achieves this by generating and streaming chunks, i.e., fixed-size lists of pointers to samples following the current mixture.\n(3) We show MIXTERA does not bottleneck training and is versatile, enabling model accuracy improvements for text-only and text-vision models. As an example, we demonstrate how to implement the ADO dynamic data mixing algorithm in MIXTERA and its positive impact on model accuracy."}, {"title": "2 BACKGROUND", "content": "Foundation models are large-scale deep learning models suitable for a variety of tasks [8, 14]. We focus on text-generation models, i.e., autoregressive large language models (LLMs) and multimodal vision-language models (VLMs). As of 2025, most such models are based on the Transformer architecture [66]. They are trained on huge corpora of training data in a self-supervised manner to maximize the likelihood of predicting the tokens of a training sequence.\nTraining phases. Training is structured into pre-training and post-training phases. In pre-training, we train a randomly initialized model on a general purpose data corpus (Section 2.1) to derive a base model. In post-training, common steps include supervised finetuning (SFT) and alignment. The data used in SFT is human-selected for a specific task. In this paper, we focus on the pre-training use case.\nDistributed training. Training foundation models requires distributing computation across multiple GPUs. Training frameworks typically employ 3D parallelism [6, 21], consisting of pipeline parallelism (PP), i.e., partioning the model layers between devices [22, 44, 45], tensor parallelism (TP), i.e., splitting individual tensor operations within layers across devices [55, 57], and data parallelism (DP), i.e., replication of the model across device groups. PP and TP together are referred to as model parallelism. Nodes within the same DP group process identical inputs, while nodes across DP groups receive different data. As an extension to DP, fully-sharded data parallelism (FSDP) shards model parameters, gradients, and optimizer states across data-parallel workers [78]."}, {"title": "2.1 Training Data and Data Mixing", "content": "The data used for pre-training stems from data collections that include data from various sources, such as Wikipedia, Common Crawl dumps, or arXiv papers. Public examples of such collections include RedPajama [68], Dolma [60], and FineWeb [48]. Besides the aggregation of data from different sources, data engineers typically clean the data, which typically involves deduplicating, filtering (e.g., the removal of personal identifiable information - PII), and applying classifiers to the data samples (e.g., to obtain a toxicity score for each sample). Longpre et al. [36] and Penedo et al. [48] provide a great overview of the processing steps and their impact.\nData properties and mixtures. Each sample in the resulting collection has properties, such as its source (e.g., Wikipedia) or its language (e.g., English). Beyond the filtering operations, ML engineers need to define a data mixture. A mixture describes how the data is mixed based on their characteristics, i.e., we might train on 50% data from Common Crawl and 50% from movie subtitles. The data can be combined based on multiple characteristics simultaneously. For instance, besides Common Crawl and movie subtitles, we might also use 80% French and 20% Italian data. The granularity on which the mixture is ensured depends on each training setup and there is no common agreement. For example, a mixture could be ensured within a batch, or across a window of several batches.\nMixing algorithms. Selecting the best mixture is critical for model performance [12, 56, 69, 72]. We differentiate static mixtures, i.e., mixtures that remain constant over the entire training job, and dynamic mixtures, i.e., mixtures that change during the training job. In the last years, several algorithms for finding the best static mixture or how to adjust the mixture dynamically during training have been proposed. Algorithms such as DOREMI [69] or the data mixing laws [72] find the a static mixture via small proxy models. Curriculum learning is an example of a pre-defined dynamic mixture. Xu et al. [70] order samples from easy to hard to improve alignment. The SMOLLM2 model was trained on 4 stages of mixtures [4]. Multilingual models are often first trained on English data, and then data from other languages is included [53, 71].\nBeyond such pre-defined schedules, there is also work on adapting the data mixture to the model training dynamics, e.g., by increasing the weight of data domains which have high loss. Albalak et al. [3] use a multi-armed bandit strategy to adjust the mixture. SKILL-IT orders \"skills\" based on model feedback [12]. AIOLI builds upon SKILL-IT and provides a unified framework for estimating the best mixture during training [11]. PIKE relies on gradient interactions [31]. In this paper, we use Adaptive Data Optimization (ADO) [26] as an example of a dynamic mixing algorithms."}, {"title": "2.1.1 Adaptive Data Optimization", "content": "Adaptive Data Optimization (ADO) is a dynamic mixing algorithm that adjusts the data mixture during training based on the model's learning progress on each domain [26]. The key idea is to prioritize domains where the model shows rapid improvement while considering how much each domain contributes to its own progress. ADO uses neural scaling laws to model how the loss $L_k$ of each domain $k$ decreases with the number of training samples $n$. To this end, it fits a power law $L_k(n) = \\epsilon_k + \\beta_k n^{-\\alpha_k}$ for each domain. Here, $\\epsilon_k$ represents the irreducible loss of the domain, $\\beta_k$ is a scaling factor, and $\\alpha_k$ determines how quickly the loss decreases. The parameters are re-fitted during training. The algorithm combines two components to determine the mixture weights. First, it estimates the learning speed for each domain using the derivative of the scaling law. Second, ADO maintains a credit assignment score $A_k(t)$ that indicates how much each domain contributes to its own progress, based on its recent sampling frequency. These components are combined with a prior (initial) distribution $\\mu_k$ to compute an intermediate preference distribution $p_k(t)$. To ensure stability, the final distribution $\\pi_k(t)$ is then computed as a weighted average between $p_k(t)$ and $\\pi_k(t)$'s temporal average. Additionally, ADO enforces a minimum sampling probability for each domain. For more details, we refer to Jiang et al. [26]."}, {"title": "3 CURRENT CHALLENGES", "content": "We identify three challenges in the status quo of training data management with current open-source infrastructure.\nChallenge 1: High engineering effort for data preparation. The current approach to preparing training data sets involves many manual offline steps with general-purpose data processing and scripting frameworks (Figure 2a). For the offline cleaning step (Section 2.1), ML engineers typically leverage data processing frameworks like Spark [74], Beam [1], Data-Juicer [10], or datatrove [49].\nThe subsequent data mixing can happen offline or online. If data is mixed offline, engineers write ad hoc scripts which create a new mixed copy of the cleaned dataset for each training run. This makes it difficult to get a quick sense of how a data mixture will impact model training when exploring different mixing policies. Some existing data loaders support online data mixing. However, they only implement it across directories, i.e., they assign a weight to each directory and sample data according to the weights. Hence, the directories need to reflect the property we want to mix on, e.g., one directory per language or data source. This inflexible approach neither supports switching the property we mix on nor supports specifying hierarchical mixtures across arbitrary properties, e.g., specify a proportion between Wikipedia and movie subtitles, and between English and German (c.f. Table 1).\nChallenge 2: Inefficient data management on filesystems. Training data is typically stored and managed as files without a proper data management system, which can lead to storage overhead, performance bottlenecks, and consistency issues. Both the source data and mixed training set are typically stored in formats such as jsonl or parquet files in a shared, distributed filesystem. Existing data loaders just wrap around the filesystem and are therefore limited by its constraints (Table 1). Filesystems are commonly used because current database management systems are not natively built for foundation model training data [67]. Using a proper DBMS for such data would require ML developers to define table schemas and to orchestrate dataflow from the DBMS to the model training, in addition to administration overhead to operate the DBMS itself. This situation is problematic, as (i) both the metadata and actual payloads are commonly duplicated across training jobs, (ii) filesystems do not provide an efficient interfaces to query data, (iii) there is no native way of tracking which model was trained on which data if it is accessed via general-purpose filesystem calls. Some frameworks like Megatron [45, 57] even require pre-tokenized data, which leads to duplication of the even bigger tokenized data files.\nChallenge 3: Rapidly evolving research. How to train the best model on a given dataset is an active area of research, with many new techniques such as dynamic mixing emerging. Offline preparation of the mixture or online mixing based on fixed directory weights does not support dynamic mixture at all. Even for researchers who are familiar with the latest mixing techniques, implementing modern mixing algorithms in a training pipeline is a painful, tedious, and error-prone task. The codebases for mixing algorithms are often tailored directly to the training framework, as well as the data collection and properties used in the respective papers. This hinders the adoption of new methods and makes researching, reproducing, and comparing different strategies difficult."}, {"title": "4 MIXTERA'S DESIGN", "content": "We propose to address these challenges for training data management by building MIXTERA, a foundation model training data plane. We derive the following design goals for such a system.\nGoal G1: The system should implement a centralized data layer that users can conveniently and declaratively query to mix data across arbitrary properties, independent of the filesystem structure.\nGoal G2: The system needs to be lightweight, i.e., not require many components to set up and be easily integratable into existing training setups.\nGoal G3: While being user-friendly and flexible, the system needs to ensure high-throughput and determinism (reproducibility).\nGoal G4: The system must support adjusting the mixture dynamically during the training."}, {"title": "4.1 System Overview and Design", "content": "Inspired by the Lakehouse architecture [5], we design MIXTERA as a read-only layer that can be deployed on top of existing training data collections, which are typically stored in a distributed filesystem. MIXTERA manages a centralized database of metadata (i.e., properties such as language or source dataset) of all training data samples (G1). A sample can be a piece of text (for LLM training) or a text-image pair (for VLM training). MIXTERA assigns every sample a unique ID and properties instead of treating the training data as a blob of homogeneous, contiguous data. It allows users to declaratively query the relevant samples for a training. To be lightweight (G2), MIXTERA does not reorganize or modify the data files on disk. For model training, it provides a standard iterator that can be used, e.g., in conjunction with a torch. DataLoader. MIXTERA is agnostic to the model training framework, supports training interruptions using checkpoints, and ensures determinism (G3) through careful shuffling, i.e., for identical queries, MIXTERA always provides data in an identical order, which is important for reproducibility as well as for debugging issues like loss spikes [15, 27, 50, 64, 79]. It supports adjusting the mixture during training (G4) by transferring chunks (lists of pointers to samples) whose mixture can change over time.\nQuery interface. In Figure 4, we show an example of a query that statically selects only Creative Commons data, and then mixes HTML and JavaScript data in a 70:30 ratio during training. MIXTERA'S implementation takes care of executing the query and obtaining the samples without needing to worry about correctness, even in distributed training. The user only needs to provide the ID of the node and its data parallel group, which is obtained from the training framework. MIXTERA currently allows to express static filter operations on properties, and static as well as dynamic mixtures across all properties (G4). For more details on supported mixture types, we refer to Section 5.2.2.\nDataflow. MIXTERA follows a server-client model. As shown in Figure 3, the server runs on a node and each training node runs client instances. Users first register samples at the server to populate the metadata database. They then can submit queries. A query is executed at the server in two phases. First, MIXTERA applies static filters from the query (e.g., English-only) to obtain all samples eligible for training. Second, during training, the server distributes chunks of that query result to the client(s), which specify which samples to train on. The server ensures that the chunks are distributed correctly, i.e., tensor- and pipeline parallel stages receive the same input data. The server generates chunks according to the current mixture, i.e., it iteratively takes samples from the query result generated in the first phase of query execution, such that data in the chunk follow the current mixture. As an iterable data loader, MIXTERA faces the challenges of determinism and checkpointing. We address this by shuffling based on the query and support to load/store the query state.\nChunks. MIXTERA does not store the sample payloads, but rather only the metadata of each sample. During training, the nodes receive chunks. A chunk is a fixed-size collection of pointers to samples in files. They tell the client which samples which file to load and train on (e.g., sample 10 in file wikipedia.jsonl.zst). The files can be local, in a distributed filesystem, or cloud storage.\nStoring and distributing pointers to samples instead has several advantages. First, users can store data in their locations of choice (e.g., an object store, or an distributed filesystem). Chunks are independent of the filesystem structure (G1). Second, it allows MIXTERA to support dynamic mixtures (G4), as the data composition of chunks can change over time. Third, the pointer-model avoids creating a data fetching bottleneck at the MIXTERA server. The server only creates chunks and each client fetches the data they need. Fourth, we avoid a lock-in effect on MIXTERA and allow for easy adoption on existing data collections (G2). Last, we natively support other modalities like images, which would not be straightforward to ingest at scale into a database.\nMixtures. Users can use any subset of sample properties to define a mixture. Users can even change the properties that they mix on during training with dynamic mixture schedules. To handle this, MIXTERA implements a MixtureKey abstraction that allows users to describe which samples they want to use. The keys also implement flexible property matching to define which samples to use during chunk generation (Section 5.2.2).\nHigh-throughput data fetching. The challenge of using chunks is that we need to handle suboptimal data layout. Files may have arbitrary property distributions, or might follow a partial structure (e.g., a file only contains Wikipedia data, but the languages are distributed randomly). We cannot re-organize data within the existing files. Formats like jsonl were not built with random access in mind, but chunks force clients to load individual samples from files. To avoid data stalls (G3), we implement chunk generation to use subsequent sample ranges in files if the samples have the same properties and engineer MIXTERA's implementation to fetch those ranges as efficiently as possible (Section 5.3).\nOpen source. MIXTERA comes as a Python package that provides the entrypoint for the server and abstractions for the client. Its codebase, consisting of approximately 11 k lines of Python and C++ (excluding tests), is open-source\u00b9. It is rigorously tested with a full set of unit and integration tests. We are continuously working on enhancing the system and welcome contributions."}, {"title": "5 IMPLEMENTATION", "content": "We explain how sample metadata is managed (Section 5.1) at the server, how it executes queries and creates chunks (Section 5.2), how those chunks are parsed at the client (Section 5.3), and give details on MIXTERA's integration into training frameworks (Section 5.4)."}, {"title": "5.1 Sample Management", "content": "MIXTERA manages samples in a MixteraDataCollection (MDC). It keeps track of all samples and their properties. The MDC leverages DuckDB [51] as the robust and flexible metadata management system underlying it. Every registered sample has properties according to a schema defined by a MetadataParser, e.g., language. Representing the structure of training data, MIXTERA initializes the database with three tables for source datasets, files, and samples. The source dataset can also be ignored and stored as a schema property instead, to support collections such as The Pile [19] where data from different datasets is contained within the same files.\nMetadataParsers. A MetadataParser is a class that defines a schema for a data collection and, given a sample, extracts and returns the properties according to the schema. A schema is a list of properties which have a type (e.g., string or enum), a nullable field, and a multiple field, describing whether a single sample can take multiple values for this properties, e.g., multiple languages. MIXTERA maps this Python schema to a proper database schema, comes with a set of pre-defined parsers for common datasets, and enables users to define custom parsers. For example, there is a parser for The Pile [19]. Every sample has the property pile_set_name describing the source dataset. This is a non-nullable, non-multiple, enum property, since we know the limited set of possible values.\nData types. Based on the parser, MIXTERA adjusts the DuckDB sample table. Properties marked as multiple get mapped to DuckDB's list type, e.g., a multiple string property gets mapped to VARCHAR[]. Enum properties are appropriately inserted into DuckDB. Using enum instead of strings for properties where all values are known beforehand speeds up query execution in DuckDB.\nInsertion. When inserting (registering) data, the MDC accumulates all relevant files, and prepares the DuckDB table schema. Then, all files are inserted into the files table, and a pool of workers in parallel processes all files using the MetadataParser to extract the sample metadata. We then aggregrate the worker results and insert it into the database, as DuckDB does not support insertion from multiple Python process workers. We found that converting the worker results to columnar pyarrow in-memory tables and then inserting into the samples table has the highest throughput."}, {"title": "5.2 Server-Side Query Execution", "content": "After receiving a query from the client, the server executes it in two phases. The first phase is performed via the MDC DuckDB-abstraction and applies static filters to identify all potentially relevant samples, e.g., all non-English samples are filtered out, and groups consecutive samples into intervals. The second phase constructs a specialized data structure called Chunker Index that enables efficient, mixture-aware chunk generation."}, {"title": "5.2.1 SQL generation and interval detection", "content": "After receiving an object representation of the query (c.f. Figure 4), similar to ORM frameworks like sqlalchemy, the MIXTERA server generates a base SQL query from this object. This query returns a table in which each row represents a sample that the user is interested in. MIXTERA ensures that the generated SQL matches the MDC's table schema, e.g., whether a property can have multiple values or not.\nA key challenge for MIXTERA is efficient random access to samples within files. File formats like jsonl or parquet are optimized for sequential reading rather than random access. To address this, MIXTERA implements an interval-based approach: the server wraps the base filtering query in an outer query that identifies continuous ranges of samples sharing identical properties within the same file. Consider the following example result of a base filtering query:\nInstead of treating these as six individual samples, MIXTERA identifies three intervals:\n\u2022 Interval 1: Samples 1-3 (File 1, JavaScript, MIT)\n\u2022 Interval 2: Samples 4-5 (File 1, Python, Apache)\n\u2022 Interval 3: Sample 1 (File 2, Python, Apache)\nEven though samples 4-5 (file 1) and 1 (file 2) share the same properties (Python, Apache), they are in different files and thus form separate intervals. The primary key is formed by the sample and file ID together. MIXTERA constructs a SQL query that processes the data in multiple stages:\n(1) First, MIXTERA establishes a Common Table Expression (CTE) named base_data that contains the filtered samples:\n(2) Next, MIXTERA identifies breaks in the sample sequence using window functions. The grouped_samples CTE calculates the difference between consecutive sample IDs within groups sharing the same properties:\nHere, a diff value of 1 indicates consecutive samples, while any other value indicates a break in the sequence.\n(3) The intervals CTE then groups the sequences into intervals:\nThe group_id is incremented when there is a break in the sequence, creating unique identifiers for each interval.\n(4) Finally, MIXTERA aggregates the results to get the final intervals:\nWhile we initially implemented the interval aggregation within Python, letting DuckDB optimize and parallelize the calculation of the interval is more efficient. This optimization can only improve I/O if samples within files are clustered by properties and not randomly distributed. For example, if a user has a file structure based on the source dataset and mixes based on that, MIXTERA can heavily leverage the range optimization and just read, e.g., all Wikipedia samples from the sample file. If the user decides to now mix on language as well, there might only be local clusters within the files. Since MIXTERA is read-only by design, it cannot re-shuffle data."}, {"title": "5.2.2 Chunk generation", "content": "The chunk generation algorithm is based on the Chunker Index data structure", "matching": "MixtureKey.\nMixtureKey abstraction. A MixtureKey represents a set of properties and their values", "language": "English to select English text regardless of license"}, {"language": "English", "license": "CC and language: English; license: MIT). The flexible matching enables finding all samples that satisfy certain properties while ignoring irrelevant attributes they might have. It also allows us to define mixtures on multiple properties with multiple values"}, {"language": "English defines the domain for English samples.\nThe ChunkerIndex. The ChunkerIndex is a nested mapping from MixtureKeys to sample locations. We refer to keys in the Chunker Index as component keys. For each unique combination of all available properties and values", "like": "nIn this example"}, {"language": "JavaScript would match the first key despite it having the additional license property and two assigned languages. This demonstrates how the MixtureKey matching allow to work with the full property/value cross-product in the index while supporting mixtures on subsets of properties.\nBuilding the ChunkerIndex. The index is built in parallel in a C++ extension", "79": ".", "algorithm": "nStatic Mixture: Users explicitly specify fixed proportions for different property combinations (Figure 4). This supports arbitrary properties and is not limited by", "Mixture": "Automatically derives mixture proportions from the data distribution in the query result: This is useful when users want to maintain the natural distribution of properties.\nHierarchical Mixture: An advanced static mixture that allows to specify nested property relationships. For example", "Schedule": "A \"meta mixture\" that allows for temporal changes in mixture composition by defining a sequence of mixtures that activate at specific training steps. This enables curriculum learning with predefined schedules.\nDynamic Mixture: Allows adaptation of mixture proportions during training based on feedback (e.g., loss) from the model. If an algorithm is already supported by MIXTERA (e.g., ADO), it can be used directly.\nChunk"}]}