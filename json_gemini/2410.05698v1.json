{"title": "A Two-Step Approach for Data-Efficient French Pronunciation Learning", "authors": ["Hoyeon Lee", "Hyeeun Jang", "Jong-Hwan Kim", "Jae-Min Kim"], "abstract": "Recent studies have addressed intricate phonological phenomena in French, relying on either extensive linguistic knowledge or a significant amount of sentence-level pronunciation data. However, creating such resources is expensive and non-trivial. To this end, we propose a novel two-step approach that encompasses two pronunciation tasks: grapheme-to-phoneme and post-lexical processing. We then investigate the efficacy of the proposed approach with a notably limited amount of sentence-level pronunciation data. Our findings demonstrate that the proposed two-step approach effectively mitigates the lack of extensive labeled data, and serves as a feasible solution for addressing French phonological phenomena even under resource-constrained environments.", "sections": [{"title": "1 Introduction", "content": "Phonetic information plays a crucial role in text-to-speech systems, improving the clarity and naturalness of synthetic speech. Grapheme-to-phoneme (G2P) relationships are typically modeled using a sizeable set of phonetic transcriptions to predict the pronunciation of out-of-vocabulary words. However, pronunciation learning in French remains challenging due to its intricate phonetic structure and phonological phenomena such as Linking (Encha\u00eenement) and Liaison.\nThese phenomena mediate between words by modifying phonemes and their placement (Adda-Decker et al., 1999; Bybee, 2001). Linking is the articulation of a consonant-final word and its re-syllabification with the following vowel-initial word (Gaskell et al., 2002; Fougeron et al., 2003). For example, when a consonant-final word une [yn] precedes a vowel-initial word amie - [a.mi], the phoneme \u201cn\u201d is resyllabified and positioned adjacent to the primary phoneme of the following word, as illustrated in the example below:\nune [yn] amie [a.mi] \u2192 une amie [y.na.mi]\nLiaison refers to the pronunciation of a silent consonant-final word to its phoneme or another when it is followed by a vowel-initial word (Bybee, 2001; Gaskell et al., 2002). For instance, when the determiner mes is an initial word, its pronunciation is altered depending on the following word. If the following word, such as fr\u00e8res, begins with a consonant, the \"s\" grapheme remains silent in the corresponding pronunciation.\nmes [me] fr\u00e8res [f\u03ba\u03b5\u03c2] \u2192 mes fr\u00e8res [me.fr\u03b5\u03c2]\nHowever, when it is followed by a vowel-initial word like amis, the \u201cs\u201d is pronounced as [z].\nmes [me] amis [ami] \u2192 mes amis [me.za.mi]\nThese phonetic modifications are influenced by various factors and contexts, leading to numerous exceptions. Such variability further adds to the complexity of addressing phonological phenomena\u00b9.\nRecent studies have focused on modeling these phonological phenomena through two main approaches: post-lexical rules (PLR) (Tzoukermann, 1998) and data-driven methods (Pontes and Furui, 2010; Taylor et al., 2021; Comini et al., 2023). While both PLR and data-driven methods achieve decent performance, they demand deeper phonological/linguistic knowledge and a substantial amount of human-annotated sentence-level data, respectively. Unfortunately, existing approaches without such expensive resource commitments have not been satisfactory.\nIn this paper, we propose a novel two-step approach to address the challenge of learning French pronunciation with limited resources. Specifically, we explicitly decompose the intricate and comprehensive pronunciation task into two sub-tasks: G2P conversion and post-lexical processing. First, we leverage a large amount of easily accessible word-level pronunciation data to train the autoregressive transformer (ART)-based G2P model (Co-"}, {"title": "2 Related Work", "content": "PLR is one of the essential modules used to address phonological phenomena in the French text-to-speech front-end, yet it requires extensive phonological and linguistic knowledge to construct a comprehensive set of hand-crafted post-lexical rules. Although only a few studies\u00b2 (Tzoukermann, 1998) provide initial guidelines for manually constructing post-lexical rules, the implementation of their intricate interactions still necessitates the deep knowledge and substantial efforts of linguistic experts.\nTo alleviate this burden, data-driven approaches have also been proposed (Pontes and Furui, 2010; Taylor et al., 2021; Comini et al., 2023). Unlike PLR, these approaches demonstrate performance improvements by leveraging large-scale sentence-level pronunciation datasets, even without linguistic knowledge. Comini et al. (2023) reported using"}, {"title": "3 Proposed Method", "content": "In this section, we describe our two-step approach to French pronunciation learning. Our approach addresses the extensive and complex pronunciation task by explicitly decomposing it into two key sub-tasks: G2P conversion and post-lexical processing. The overall architecture is illustrated in Figure 1."}, {"title": "3.1 Grapheme-to-Phoneme", "content": "To generate pronunciation for a given word, we employ a vanilla ART architecture (Vaswani et al., 2017). Following the autoregressive model-based sequence-to-sequence paradigm applied in G2P (Milde et al., 2017; Peters et al., 2017; Yolchuyeva et al., 2019; Yu et al., 2020; Zhu et al., 2022; Comini et al., 2023), the encoder transforms the grapheme sequence x = {X1,X2, ..., Xt} into contextual information, and the decoder generates the corresponding phoneme sequence y = {Y1, Y2, ..., y\u0131} based on the encoder's output. We train the ART G2P model on word-level pronunciation data, which includes <word, pronunciation> pairs like <enfant, afa>. During training, we use cross-entropy (CE) loss between the generated phonemes \u0177 and ground truth phonemes y."}, {"title": "3.2 Post-lexical Phonetization", "content": "The key intuition underlying our approach is to adopt a separate post-lexical processing module rather than directly predicting phoneme sequences covering the phonological phenomena. The hypothesis is that learning French pronunciation, including the post-lexical phenomena, is particularly challenging when relying on a limited sentence-level dataset. This challenge is compounded by the fact that each example contains merely a few words affected by the phenomena. This dedicated module targets post-lexical phonetization, effectively leveraging a modest number of sentence-level examples.\nThe post-lexical phonetization model follows a similar architecture as the G2P model of the first sub-module. The key distinction here is using a fairly shallow, non-autoregressive architecture. The encoder compresses the concatenated grapheme sequences of the partial grapheme sequence of the word xi and the following word xi+1, along with the part-of-speech (POS) tags extracted from a pre-trained POS model. Given that post-lexical phenomena are related to the graphemes between the immediately surrounding words, we use the final n graphemes of the word xi and the initial m graphemes of the following word Xi+1. A [SEP] token is inserted between each partial grapheme sequence. When the number of graphemes in each word is fewer than the pre-defined values of n and m, a [PAD] token is added to the beginning or end of each sequence, respectively. This approach allows the encoder to effectively capture contextual information essential for processing post-lexical phenomena, leveraging both grapheme sequences and POS tags of adjacent words.\nConditioned on the contextual information extracted from the encoder, the decoder then predicts whether a phonological phenomenon occurs Yphen and the resulting phoneme Yph. We utilize word-level phoneme sequences predicted by the pre-trained G2P model as input for the decoder. Similar to the encoder input, the decoder's input comprises the concatenation of partial phoneme sequences, encompassing the final n phonemes of the word xi and the initial m phonemes of the subsequent word Xi+1. [SEP] and [PAD] tokens are appended in the same manner as the encoder.\nWe then introduce two loss terms to train the phonetization model. The first loss term ($\\mathcal{L}_{phen}$) is the weighted binary cross-entropy (WBCE) loss used to identify the presence of the phonological phenomenon in class-imbalanced settings:\n$\\mathcal{L}_{phen} = WBCE (\\hat{y}_{phen}, y_{phen})$\nwhere $y_{phen}$ is the ground truth of phonological phenomenon occurrence, and $\\hat{y}_{phen}$ is the predicted probability of the phonological phenomenon. The second loss term ($\\mathcal{L}_{ph}$) is the CE loss, used for predicting pronunciation variation as follows:\n$\\mathcal{L}_{ph} = y_{phen} \\cdot CE (\\hat{y}_{ph}, y_{ph})$\nwhere $y_{ph}$ is the ground truth of the phoneme at locations where phonological phenomenon may occur. The key characteristic is adding a simple selector that is selectively activated based on the occurrence of a phenomenon, as follows:\n$y_{phen,i} = \\begin{cases}\n1, & \\text{if post-lexical phenomenon} \\\\\n0, & \\text{otherwise}\n\\end{cases}$\nOverall, the loss of the phonetization model is the sum of $\\mathcal{L}_{ph}$ and $\\mathcal{L}_{phen}$, as follows:\n$\\mathcal{L}_{plp} = \\mathcal{L}_{ph} + \\mathcal{L}_{phen}$"}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Datasets", "content": "First, we collect pronunciation data at both the word and sentence levels. Following Comini et al. (2023), we gather word-level pronunciation data using an internal pronunciation dictionary, which contains 106,857 unique entries, each with phonetic transcriptions denoted using X-SAMPA notation. Furthermore, we collect 2,645 sentence-level data from various domains, including news, social media posts, and the Multilingual LibriSpeech (MLS) (Pratap et al., 2020) dataset. Each example is manually annotated by a French linguistic expert using the same X-SAMPA phonetic transcription as the word-level data, including phonological phenomena like Liaison and Linking\u00b3. Each sentence consists of an average of 12.27 (SD = 2.96) words, and the average frequency of phonological phenomena occurrences per sentence is 1.43 (SD ="}, {"title": "4.2 Experimental Setup", "content": "We split the word-level dataset into training (85%), validation (5%), and test (10%) sets for the G2P model training. For the post-lexical phonetization model, the number of training instances k varies from 2,045 to 512, decreasing in 25% intervals, with 300 examples for both the validation and test sets. To evaluate the G2P and post-lexical phonetization models, we use the following metrics: phoneme error rate (PER), word error rate (WER), and Accplp (Appendix B.3).\nWithin our proposed method, the ART G2P model is mainly implemented following the setup described in (Zhu et al., 2022). The transformer encoder and decoder consist of 8 layers each, with 8 self-attention heads, 512-dimensional embeddings, and 2048 feed-forward dimensions, resulting in 58.9M parameters. For the post-lexical phonetization model, we employ a shallow NART architecture. This architecture consists of just 2 transformer layers, each with 8 self-attention heads, and 512-dimensional embeddings, resulting in 14.8M parameters only. The total number of parameters for the proposed architecture is around 73.7M. Additional details of other configurations are provided in Appendix B.4.\nBuilding on this foundation, we compare our proposed method to the ART-based G2P model, which serves as our baseline due to its renowned superior performance in existing approaches. This model is trained in three distinct settings: using word-level data (Yolchuyeva et al., 2019), sentence-level data, and a combination of both (Comini et al., 2023). To ensure a fair comparison with the proposed method, we employ a 10-layer ART G2P model and follow the same procedure to identify optimal hyperparameters in all settings."}, {"title": "4.3 Results and Analysis", "content": "We evaluate the proposed approach alongside several baseline models, by breaking down the analysis into four distinct cases: Whole sentence, Phonological phenomena, Liaison, and Linking.\nCan the baseline models address phonological phenomena? We train the naive ART G2P model on three distinct sets of pronunciation data, with the experimental results detailed in Table 2. As expected, the model trained solely on word-level phonetic transcription data was completely unable to address any phonological phenomena, resulting in an Accplp of 0% in all phonological phenomena cases. Despite this, when leveraging around 100k entries of large-scale word-level data for training, the model showed adequate performance in PER and WER, aligning with the findings of previous studies. For the model trained on sentence-level pronunciation data, we can observe a slight improvement in addressing phonological phenomena with an Accplp of 13.23%. However, due to the limited number of training examples, the overall performance substantially declined. In contrast, compared to using sentence-level data alone, combining a large amount of word-level data with a small amount of sentence-level data led to a considerable improvement. PER and WER significantly decreased in all cases, reaching 29.91% and 34.39%, respectively. Accplp also improved relative to other baselines, reaching 55.38% in the Phonological phenomena case; however, it remains markedly inferior compared to the Whole sentence.\nHow effective is the two-step approach in addressing phonological phenomena? Initially, we evaluate the G2P model of the proposed approach to generate correct word-level pronunciations that serve as inputs for the subsequent post-lexical phonetization model. Interestingly, the G2P model demonstrates performance on par with the"}, {"title": "5 Conclusion", "content": "In this paper, we present an effective two-step approach for French pronunciation learning. Our approach alleviates the burden of extensive resources by decomposing the intricate and comprehensive pronunciation task into two sub-tasks, thereby facilitating greater leverage of a modest number of sentence-level examples. The empirical analysis demonstrates the efficacy of our proposed approach in addressing phonological phenomena even in resource-constrained environments."}, {"title": "6 Limitations", "content": "This study has a few important limitations. We employed closed-source datasets. Although some open-source word-level pronunciation datasets exist, such as the one provided by Zhu et al. (2022), which contains approximately 250k French word-pronunciation pairs and is significantly larger than ours, we chose not to use it due to the presence of noise. Moreover, finding a publicly available dataset designed for post-lexical processing has been significantly challenging, as even sentence-level phonetic transcriptions reflecting phonological phenomena are not available.\nOur manually constructed sentence-level dataset contains a smaller amount of annotated data compared to previous research. This limited size may not completely capture the generalizability of the entire spectrum of French phonological phenomena described in Appendix A. While this smaller dataset may result in some missing contexts, it represents our main contribution. Despite being a data-driven method, we achieved significant results using only less than 2k sentence-level pronunciation data."}, {"title": "A Complexities of French Liaison", "content": "Historically rooted in the pronunciation of final consonants, French liaison has evolved into a complex linguistic feature influenced by socio-cultural factors (Adda-Decker et al., 1999). Consequently, it is categorized as obligatory, optional, or forbidden liaison based on the morpho-syntactic context (Durand and Lyche, 2008). For instance, while it is obligatory for the initial word quand with any subsequent vowel-initial word, the forbidden configuration in which et serves as the initial word indicates that regardless of the subsequent vowel-initial word, liaison will not occur. Additionally, the context of optional liaison may vary on subjective choices made by speakers, which depend on stylistic, socio-linguistic, and situational factors (Durand and Lyche, 2008; De Mare\u00fcil et al., 2003).\nExceptions arising from fixed expressions further compound the complexity of liaison (Laks, 2005; Bybee, 2005). For instance, in the fixed expression accent aigu, the final grapheme \u201ct\u201d of accent forms a liaison with the initial vowel of aigu:\naccent [ak.s\u00e3] aigu [e.gy] \u2192\naccent aigu [ak.s\u00e3.te.gy]"}, {"title": "B Experimental Details", "content": ""}, {"title": "B.1 Phonetic Transcription Example", "content": "The following is an example of a sentence-level phonetic transcription we collected. This example includes word-level phoneme sequences reflecting the phonological phenomena in the sentence.\n\u2022 Sentence: Un enfant innocent a oubli\u00e9 sa petite envelope.\n\u2022 Phonetic transcription: 9~^nA~fA~ / inOsA~ /a/ublije / sa/p@ti^tA~vlOp\nIf no phonological phenomenon occurs between words, a slash (/) is inserted between the word-level phoneme sequences. However, if a phonological phenomenon is present, caret (^) is added."}, {"title": "B.2 Data Preprocessing", "content": "To ensure our data retain their original meaning and naturalness, we apply only essential preprocessing steps: (i) character case folding, (ii) removing of special characters (e.g., HTML tags, links, emojis), and (iii) replacing punctuation marks with \u2018#'. Crucially, apostrophes (') and hyphens (-) between words are preserved to avoid distorting the original meaning. Furthermore, our pronunciation datasets are composed of entries that meet the following criteria: (i) a complete sentence structure, (ii) a minimum of four words per sentence, (iii) words represented by no more than 32 graphemes or phonemes, and (iv) fewer than 192 total characters in the sentence."}, {"title": "B.3 Metrics", "content": "PER is the Levenshtein distance between the predicted and reference phoneme sequences, divided by the reference's length, and WER is the percentage of words with predicted phoneme sequences mismatched with the reference. For a focused evaluation of the post-lexical phonetization model's ability to address phonological phenomena, the Accplp metric is employed, representing the accuracy of phonemes at locations where phonological phenomena may occur."}, {"title": "B.4 Configuration Details", "content": "Hyperparameters Following Lee et al. (2023), we conducted a grid search across several hyperparameters, exploring a variety of combinations to ensure optimal performance. Thus, based on the validation set performance, we selected the following settings: the AdamW optimizer with a learning rate of 1e-4, a dropout rate of 0.1, 100 epochs, and early stopping after 5 epochs.\nPost-lexical Phonetization We set n=5 and m=5 for constructing input sequences of encoder/decoder, as preliminary experiments showed these values to be optimal for effective training and computational efficiency.\nPart-of-Speech Tagging Drawing on insights from the previous studies (De Mare\u00fcil et al., 2003; Taylor et al., 2021), we adopt the open-source pre-trained French Part-of-Speech (POS) tagging model4. The model is fine-tuned on the Free French Treebank dataset consisting of 29 POS tags (Crabb\u00e9 and Candito, 2008). The French POS model extracts POS tags from the input text, which are then utilized as an auxiliary input for the post-lexical phonetization model. Its parameters are frozen during the training."}]}