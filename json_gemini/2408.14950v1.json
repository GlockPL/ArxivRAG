{"title": "NeuralOOD: Improving Out-of-Distribution Generalization Performance with Brain-machine Fusion Learning Framework", "authors": ["Shuangchen Zhao", "Changde Du", "Hui Li", "Huiguang He"], "abstract": "Deep Neural Networks (DNNs) have demonstrated exceptional recognition capabilities in traditional computer vision (CV) tasks. However, existing CV models often suffer a significant decrease in accuracy when confronted with out-of-distribution (OOD) data. In contrast to these DNN models, human can maintain a consistently low error rate when facing OOD scenes, partly attributed to the rich prior cognitive knowledge stored in the human brain. Previous OOD generalization researches only focus on the single modal, overlooking the advantages of multimodal learning method. In this paper, we utilize the multimodal learning method to improve the OOD generalization and propose a novel Brain-machine Fusion Learning (BMFL) framework. We adopt the cross-attention mechanism to fuse the visual knowledge from CV model and prior cognitive knowledge from the human brain. Specially, we employ a pre-trained visual neural encoding model to predict the functional Magnetic Resonance Imaging (fMRI) from visual features which eliminates the need for the fMRI data collection and pre-processing, effectively reduces the workload associated with conventional BMFL methods. Furthermore, we construct a brain transformer to facilitate the extraction of knowledge inside the fMRI data. Moreover, we introduce the Pearson correlation coefficient maximization regularization method into the training process, which improves the fusion capability with better constrains. Our model outperforms the DINOv2 and baseline models on the ImageNet-1k validation dataset as well as six curated OOD datasets, showcasing its superior performance in diverse scenarios.", "sections": [{"title": "1. Introduction", "content": "Deep learning (LeCun et al., 2015) has achieved remarkable success in various fields such as Computer Vision (CV) and Natural Language Processing (NLP), primarily due to the availability of high-quality, large-scale, well-annotated datasets (e.g., ImageNet-1k dataset (Deng et al., 2009)) and the development of well-designed model architectures. In recent years, numerous CV models have continually set new benchmarks in the ImageNet classification task, and the paradigm has also transitioned from Convolutional Neural Networks (CNNs) (Rawat & Wang, 2017) to Vision Transformers (ViTs) (Dosovitskiy et al., 2020). These Deep Neural Networks (DNNs) based on the Empirical Risk Minimization (ERM) optimization method (e.g., gradient descent) have demonstrated impressive image recognition capabilities, surpassing human performance under the assumption of independent and identically distribution (i.i.d.) (Krizhevsky et al., 2012). However, DNNs' effectiveness tends to diminish when confronted with extreme environments that deviate from the i.i.d. assumption, such as out-of-distribution (OOD) samples. In the real-world applications, the decrease in accuracy caused by the ubiquitous distribution shift phenomenon poses potential risks in applications such as autonomous driving, medical diagnosis, and security systems, leading to a sharp increase in model unreliability. This limitation underscores the need to develop more robust models that can maintain high performance across a broader range of conditions and data distributions, thereby reducing the catastrophic failures caused by domain shift.\nIn order to address the aforementioned shortcomings, domain generalization (DG), also known as OOD generalization, was spawned as a critical area of the transfer learning research. OOD generalization aims to train a model not only excel on i.i.d. test dataset but also generalized to unknown distributions (Arjovsky, 2021; Wang et al., 2023). This issue remains a central focus in both academic and industrial research due to its significant implications (Wang et al., 2023)."}, {"title": "2. Related Work", "content": "The proposed BMFL framework is a brain-guide multimodal model which efficiently improves the OOD generalization capability of its visual backbone. In this section, we review the development of the OOD generalization task first. Then we discuss the differences between brain-inspired models and brain-machine fusion models. In the end, we sum up the multimodal learning methods based on the cross-attention mechanism."}, {"title": "2.1. Out-of-Distribution Generalization", "content": "OOD generalization aims to train a model utilizing one or several in-domain (ID) source data that can generalize to the target unseen (i.e., OOD) domain(s) (Wang et al., 2023). The main paradigm of OOD generalization can be broadly categorized into three types: data manipulation, representation learning, and learning strategy. Among these, representation learning, which improves the OOD generalization ability by learning the domain-invariant features, stands out as the most popular method. Some researchers propose an Invariant Risk Minimization (IRM) (Arjovsky et al., 2019) method, which learns the domain-invariant features across different domains by minimizing the domain invariant penalty, while other researchers conduct a knowledge distillation method to learn the domain-invariant features representations (Niu et al., 2023).\nAlthough IRM has demonstrated high OOD generalization ability, its shortcomings are also evident. On the one hand, IRM-based methods perform well on shallow networks, such as linear models, but they are prone to overfitting in deeper networks. On the other hand, IRM is sensitive to the environment partitioning, the effectiveness of IRM may be significantly reduced if the partitioning is not sufficiently reasonable. To address the overfitting issue in the overparameterized networks, the SparseIRM method (Zhou et al., 2022) is proposed to introduce a global sparsity constraint into the IRM process. Futhermore, (Lin et al., 2022) utilizes additional auxiliary information as the prior knowledge to learn the domain-invariant features, thereby reducing the dependency on environment partitioning if IRM. Several works improve OOD generalization performance under the traditional ERM framework, (Bai et al., 2021) tackles the two aforementioned issues by introducing the Neural Architecture Search method into the research of OOD generalization problems, (Liu et al., 2023b) uses conditional variational autoencoder (VAE) and gradient reversal layer to achieve high accuracy.\nThe above methods refine the OOD generalization ability by introducing auxiliary information or altering the training process, but they mainly focus only on the single image modality. Although (Lin et al., 2022) uses the auxiliary information, data from other modalities can contain richer information compared to the auxiliary information. Therefore, we propose a BMFL framework, utilizing multimodal data as well as multimodal learning method to improve the OOD generalization of DNNs. Comparing with the conventional IRM methods, the proposed BMFL framework can obtain higher OOD generalization accuracy with ERM optimization method that eliminates the dependency on environment partitioning."}, {"title": "2.2. Brain-based Models", "content": "Brain-based models can be summarized into two main categories: brain-inspired models and brain-machine fusion models. Spiking Neural Networks (SNNs) is a typical brain-inspired model, which is designed to imitate the biological neuron system. During the information transmission process, neurons emit spikes only when the membrane potential reaches the threshold value. Due to its elaborate design, SNNs can improve the computational efficiency while reducing the power consumption. Based on the above advantages and the strong bio-neurological basis, there are several works conduct their research based on the SNNs architecture (Kasabov et al., 2023; Ji et al., 2023; Hu et al., 2022; Du et al., 2022). Recently, some researchers proposed a brain-score (Schrimpf et al., 2018) to measure the similarity between brain visual streams and DNNs. DenseNet-169 (Huang et al., 2017), CORnet-S (Kubilius et al., 2019), and ResNet-101 (He et al., 2016) are considered as the most brain-like models. Different from other models, CORnet-S uses shallow networks with only four layers which corresponds to the four brain visual areas and obtains high image classification accuracy than the deeper models. Both SNNS and brain-like models belong to the structural improvement of DNNs.\nComparing with those brain-inspired models, the brain-machine fusion models can usually achieve higher performance in different tasks. Previous multimodal learning methods such as CLIP (Radford et al., 2021), obtains a high classification accuracy and even demonstrates the impressive zero-shot classification capability. Comparing with previous multimodal models, the brain-machine fusion models introduce a more robust prior cognitive knowledge into the learning system, which will further improve the performance on variant tasks. The BraVL model (Du et al., 2023) utilizes a multimodal VAE (i.e., Mixture of Product of Experts) to fuse the brain, visual, language modalities. Restricted by evidence lower bound and mutual information maximization, the BraVL model demonstrates proficient performance in the zero-shot visual neural decoding task. Some researchers designed a VLD model (Huang et al., 2024) that introduces Bidirectional Gated Recurrent Unit into the brain visual neural decoding task and use multitask encoder to joint modeling the fMRI-task information. In the end, encoded features will be decoded from the unique task models. Palazzo et al. proposed a siamese network to learn the multimodal joint representation of EEG and the image stimuli (i.e. Cortical-Visual Representations). It performs well in the EEG classification, image classification and saliency detection tasks (Palazzo et al., 2021). Liu et al. proposed a brain-machine coupled learning method for facial emotion recognition (FER) task (Liu et al., 2023a). They utilized three channels (i.e. visual channel, cognitive channel and common channel) to extract visual knowledge, cognitive knowledge and brain-guide-visual knowledge. Their brain-machine coupled learning method obtains the SOTA performance of the FER classification task. Meanwhile, Quan et al. utilized multimodal contrastive learning algorithm for brain-machine fusion learning (QUA, 2024). Specifically, they align the EEG signal and its stimulus images via constractive learning method during the training process. The aligned frozen-weighted image encoder will be used as the classification task to train the SVM classifier. Some researchers successfully increased classification accuracy (Fong et al., 2018) and model robustness (Spampinato et al., 2017) by introducing brain signals to DNNs. Fu et al. used the representation similarity analysis to combine brain knowledge with CNN features, thereby improving the video emotion recognition accuracy (Fu et al., 2023). Several works (Schwartz et al., 2019) also introduce cognitive knowledge into the NLP research, using cognitive knowledge to improve the performance of NLP models in relevant NLP downstream tasks."}, {"title": "2.3. Multimodal Learning Using Cross-attention Mechanism", "content": "Since the remarkable success of the transformer architecture (Vaswani et al., 2017) in NLP (Devlin et al., 2018) and CV (Dosovitskiy et al., 2020) domains, the advent of a unified framework has rendered multimodal learning architecturally viable. As an attention-based method, the cross-attention mechanism can fuse the information from different sequences or modalities. Thus, it performs well on the machine translation, multi-scale input and multimodal learning tasks. In the single-modal learning domain, some researchers introduces the cross-attention mechanism into the image classification task (Chen et al., 2021). By integrating multi-scale inputs from ViTs of different sizes, it efficiently improves the classification performance of the conventional ViT models. In the multimodal learning domain, various Vision-Language Pre-training models (Li et al., 2021; 2023) demonstrate their ability to comprehend the semantic information of the input images. They can answer questions posed by people based on the content of the images. Cross-attention mechanism is the key of its powerful cross-modal content comprehension ability. Through the joint representation modeling of images and text by cross-attention mechanism, the domain distribution of different modalities are aligned. Aside from its cross-modal comprehension capability, the cross-attention mechanism also demonstrates high cross-modal generation ability. Concurrently, some text-to-image generative models (Rombach et al., 2022; Ramesh et al., 2022) show the capability to generate high-quality images through few sentences via cross-attention mechanism. This cross-modalities approach has facilitated a deeper comprehension and synthesis of information across different modalities. Recent study (Ito et al., 2023) proves that different model architectures exhibit varying performance on OOD generalization tasks, models with multiple attention layers or using cross-attention mechanism have a better OOD generalization performance."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Problem Definition and Framework Overview", "content": "There are at least two distinct domains in the OOD generalization task (e.g., training domain $D_{trn}$ and testing OOD domain $D_{ood}$, etc.), our goal is to build a model $F$ that has good performance on both i.i.d test dataset from the training set and OOD testing dataset. During the training process, we optimize the model $F$ by minimizing the loss function $L$ with training set labels $Y$ and the predicted labels $Y_{pred}$. The optimization process can be defined as:\n$F_{o} = \\arg \\min _{F} E_{X,Y \\sim D_{trn}}[L(F(X), Y)]$"}, {"title": "3.2. Image Encoder", "content": "In the proposed BMFL framework, DINOv2 (ViT-b) (Oquab et al., 2024) is used for the image encoder $E_i$. Fig. 2(B) shows the architecture of the image encoder. DINOv2 adopts self-distillation method to learn the semantic features from the input images. Specifically, researchers conduct a unsupervised pre-training method by minimizing the outputs distance between the large parameter teacher model and fewer parameter student model. The experimental results shows that using self-distillation method can obtain higher performance than training a ViT model from scratch. The sophisticated design and high-quality large-scale pre-training dataset fortify the robustness of the image encoder. Without extra fine-tuning, DINOv2 model can obtains the equivalent performance to the supervised models in the CV downstream tasks. The image encoding process is calculated as:\n$[I_c, I_p] = E_i(I)$"}, {"title": "3.3. Brain Encoder", "content": "The brain visual fMRI plays a crucial role in the proposed BMFL framework. Conventionally, the brain visual fMRI is acquired by presenting visual stimuli to the subjects and recording the blood-oxygenation-level-dependent (BOLD) responses. However, the fMRI data collection is expensive and time-consuming, so it's impossible for us to collect and process millions of brain visual fMRI. In order to handle this problem, we employ a pre-trained DETR (Carion et al., 2020) brain visual fMRI prediction model (Adeli et al., 2023) as the brain encoder $E_b$, which is pre-trained on the NSD Dataset (Allen et al., 2022), the best quality open-source brain dataset to date. The 7-Tesla scanner provides high-quality and low-noise brain visual fMRI data, which enhances the reliability of the brain encoder.\nIn the pre-training process, Andeli et al. utilize the DINOv2 as its image encoder. After the image encoding process, the patch image tokens $I_p$ are served as the key and the value of the transformer decoder (Vaswani et al., 2017). The input queries of the decoder correspond to different brain regions of interest (ROIs) from different hemispheres (Adeli et al., 2023). Fig. 2(C) shows the brain encoder architecture and its pre-training process. The decoding output tokens represent eight stream level ROIs: early, mid-ventral, mid-lateral, mid-parietal, ventral, lateral, parietal and unknown (all the unsigned brain vertices) for each left and right hemisphere. The decoding output tokens are then aligned with the NSD fMRI data through a linear regression model, updating the parameter by minimizing the MSE loss.\nGiven the patch image tokens $I_p$, the predicting process of fMRI follows:\n$fMRI = E_b(I_p)$"}, {"title": "3.4. Brain Transformer", "content": "After obtaining the fMRI, we should then consider how to extract knowledge from them. Previous works (Du et al., 2023; 2019) usually use a simple linear model to extract knowledge from the fMRI. Although these methods perform well on visual neural decoding tasks, there are still several limitations. Firstly, the representation capability of the simple linear model is inferior than the transformer architecture. Besides, the main issue is the dimension discrepancy between fMRI features and transformer tokens. As the unified architecture can make it easier to build a multimodal learning model. We propose a brain transformer to learn the hierarchical knowledge from brain visual fMRI with low feature dimension that aligned to the transformer architecture.\nIn order to mitigate the adverse effects of similar response values between neighboring voxels to the model, we conduct a sparse code modeling method. In the proposed brain transformer, the predicted fMRI is divided into patches by 1D convolution operator via the sparse coding method (Chen et al., 2023). Specifically, the kernel size and the stride of the 1D convolution operator are 192 which is equivalent to the embedded dimension of ViT tiny. The number of output channels is 768 that equivalent to the embedded dimension of ViT base. The 192-dimensional fMRI is projected into 768 dimensions by sparse coding, which reduces the influence of adjacent voxel responses on the model and maintains a considerable sequence length. Then, we add the position embedding and the $[CLS]$ token respectively. The $[CLS]$ token $F_c$ will gradually learn the global features of the fMRI follows:\n$[F_c, F_p] = E_f(fMRI)$"}, {"title": "3.5. Brain-visual Fusion Module", "content": "Following the image encoding, fMRI predicting and the fMRI feature extraction stages, we propose a Brain-visual Fusion (BVF) module to learn the joint representation of the fMRI features and visual features $X_j$ via the cross-attention mechanism (Chen et al., 2021). In the proposed BVF module, the extracted fMRI's $[CLS]$ token $F_c$, and patch tokens $F_p$, in conjunction with the visual features $I_c$ and $I_p$ are served as the key and value, or alternatively, the query of cross-attention. Eq. 7 describes the fusion process in the proposed BVF module. Through the BVF module, DNNs are capable of effectively fusing the prior cognitive knowledge from the human brain with visual features. Letting prior cognitive knowledge to mitigate the domain shift phenomenon of DNNs. Fig. 4(B) shows the architecture of the BVF method.\n$CrossAttention(Q, K,V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$\nMore specifically,\n$X_{vb} = CrossAttention(I_c, F_p, F_p)$\n$X_{bv} = CrossAttention(F_c, I_p, I_p)$\n$X_j = Concatenate(X_{vb}, X_{bv})$"}, {"title": "3.6. Pearson Correlation Coefficient Maximization", "content": "In the multimodal learning research, reducing the distance between different modalities' representations is one of the key priorities. Some researchers utilize constructive loss (Radford et al., 2021; Li et al., 2022; 2023), while others use distance metrics to measure and thus reduce the distance between different modalities' representations (Liu et al., 2023a; Du et al., 2023). In the proposed BMFL framework, we optimize the fusion process by maximizing the PCC $R$ value. Minimizing the discrepancy between two modalities enhances the fusion capability, and thus improving the OOD generalization ability.\nThe $R$ value, as shown in Eq. 5, is one of the most important indicators of the degree of the linear correlation between two variables. Specifically, $R=1$ indicates the perfect positive linear correlation between the two variables, $R=0$ indicates the two variables are linearly independent, while $R=-1$ indicates the perfect negative linear correlation of the two variables. In Eq. 5, x denotes the fusion feature $X_{vb}$, $\\bar{x}$ is the average value of x, y denotes the another fusion feature $X_{bv}$, $\\bar{y}$ is the average value of y."}, {"title": "3.7. Overall loss function and training process", "content": "In the proposed BMFL framework, $L_{BMFL}$ is used to regulate the model.\n$L_{BMFL} = L_{cls} + \\alpha L_{fusion}$\nwhere $L_{cls}$ is a standard cross-entropy loss, while $L_{fusion}$ is a non-iterative PCC calculation method following Eq. 13. The hyper-parameter $\\alpha$ is the regularization weight that dictates the degree of contribution of the fusion regularization $L_{fusion}$ to $L_{BMFL}$.\n$L_{cls} (y, \\bar{y}) = -\\sum_{i=1}^n y(i) log \\bar{y}(i)$\n$L_{fusion} (x, y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2 \\sum_{i=1}^n(y_i - \\bar{y})^2}}$\nwhere y is the true probability distribution, $\\bar{y}$ is the predicted probability distribution. We optimize the proposed model by minimizing $L_{BMFL}$, while the component $L_{fusion}$ must be maximized to yield effective results. Thus the regularization weight $\\alpha \\in (-\\infty, 0]$, where $\\alpha = 0$ means the $L_{BMFL}$ is $L_{cls}$.\nAlgorithm 1 shows the PyTorch pseudocode for the implementation of the proposed BMFL framework."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "Training dataset. The ImageNet-1k dataset is one of the most famous image classification dataset with 1000 different categories. In the following experiments, we adopt the ImageNet-1k training dataset during the training process.\nTesting datasets. There are a total of 7 datasets that are used to test the performance of our BMFL framework. Within these 7 datasets, the ImageNet-1k validation dataset is used to test the ID classification performance and the remaining 6 datasets are used for the OOD generalization testing. To make the BMFL framework more theoretical and applied, we use 4 mainstream open-source OOD datasets and create 2 OOD datasets to supply the real-world extreme OOD scenarios. Furthermore, the two self-created datasets and the Stylized ImageNet dataset are created based on the images of the ImageNet validation dataset, while the images of the other 3 open-source datasets are from different sources within the 1000 ImageNet labels.\nStylized ImageNet dataset (Geirhos et al., 2019) is a cross-domain \"16-class-ImageNet\u201d dataset, which contains 1280 images with 80 samples in each category. Researchers create the dataset by randomly selecting the ImageNet images first. The raw Stylized ImageNet dataset is not provided, but the researchers open-source the code for generating the images and we create the Stylized ImageNet dataset following 3.\nImageNet-Renditions (ImageNet-R) dataset (Hendrycks et al., 2021a) contains 30k image renditions with 200 ImageNet classes. Researchers first collect the images from Flickr website, then the collected images will be automatically filtered by the Amazon MTurk tools. In the end, the remaining images will be selected manually to ensure the quality of the dataset.\nImageNet-Sketch (ImageNet-S) dataset (Wang et al., 2019) contains 50k sketch-like greyscale images with 1000 ImageNet classes (50 images per class). Researchers use Google Images website to collect these sketch-like greyscale images. Specially, for those classes that contain less than 50 images, researchers conduct a augmentation operation until the number of images reached to 50.\nImageNet-Adversarial (ImageNet-A) dataset (Hendrycks et al., 2021b) contains 7500 images with 200 ImageNet classes. The images are collected from 3 different sources: iNaturalist, Flickr and DuckDuckGo. After the adversarial filtering process, researchers chose the remaining images with low-confidence as the elements of the ImageNet-A dataset.\nMasked dataset is an ImageNet-based dataset. We create it in order to introduce one of the most common OOD samples in life (i.e., extreme scenarios) into the OOD generalization task. The self-supervised DINO model (Caron et al., 2021) is used for generating masked images through subject occlusion or using random occlusion to partially mask the foreground of the image.\nLow light dataset is another ImageNet-based dataset. Our motivation for producing this dataset is obvious: low light scenario is the most common OOD sample in the real-world applications, as it appears everyday. The low light dataset was created through the steps of unprocessing procedure, low light corruption, and image signal processing following (Cui et al., 2021)."}, {"title": "4.2. Implementation Detail", "content": "In the baseline experiment, we use Timm 4 library to conduct the pre-training baseline models and compete the image classification performance on both ID and OOD datasets.\nDuring the training process, we use 512 brain-image data to optimize our BVL module in one iteration. The learning rate peaks at 5e-5 after 1.5 epochs with the linear warm-up and then decreases with a cosine learning rate schedule for the remaining epochs. We adopt -0.4 as the value of the regularization weight $\\alpha$. Moreover, the AdamW optimizer (Shazeer & Stern, 2018) is employed to update the model parameters during the backpropagation process, with the hyper-parameters $\u03b21 = 0.9$ and $\u03b22 = 0.999$, and $\\epsilon$ = 0.02."}, {"title": "4.3. Results", "content": "We compare the OOD generalization ability of different model architectures, including CNNs such as VGG16 (Simonyan & Zisserman, 2014), Inception-v4 (Szegedy et al., 2017), ResNet-50 (He et al., 2016) and EfficientNet-B0 (Tan & Le, 2019). ViT-based models like DeiT III (Touvron et al., 2022), the image encoder backbone DINOv2 (Oquab et al., 2024) and the brain-inspired model CORnet-S (Kubilius et al., 2019), our BMFL framework demonstrates superior OOD generalization performance among these baseline models. As illustrates in Table 2, ViT architecture exhibits stronger OOD generalization capability than CNN models. This experimental phenomenon can be attributed to two main factors. The first one is the designing of the transformer architecture (i.e., ViT-based models) can better focus on global features through the self-attention mechanism, whereas CNN models predominately rely on the local features extracted by convolutional layers. Secondly, the scale of the pre-training dataset plays a crucial role in the OOD generalization capabilities. In general, ViT-based models that have large parameters can fit large-scale datasets easily, while CNN models are struggle with it. Obviously, high-quality large-scale datasets can provide more information with few noise that can help DNNs to focus on those meaningful information, thus improving the knowledge transfer ability and the OOD generalization capability. In our experiment, we compare the ID and OOD generalization performance of the same model with pre-training datasets. We utilize the DeiT III (ViT-b) model as a representative example. Compared to the ImageNet-1k pre-training dataset, the model pre-trained on the large-scale ImageNet-21k dataset and fine-tuned on the ImageNet-1k datasets exhibits superior OOD generalization capability in most OOD datasets, with the exception of the Stylized ImageNet dataset. The image encoder backbone DINOv2 is another convincing evidence, the large-scale high-quality pre-training dataset LVD-142M is another key factor in achieving such results besides the self-supervised design. Furthermore, the brain-inspired CORnet-S model achieves and even surpasses some DNNs with its shallow model layers. This experimental result shows that conducting a brain-inspired model is a effective way to obtain high OOD generalization capability with few parameters.\nDifferent learning methods can also influence the OOD generalization performance. Compared to the label-driven supervised methods, self-supervised models can learn more semantic information to mitigate the domain shift phenomenon. As a common representation learning method, the dominant approach in OOD generalization task, conducting a self-supervised model is another way to obtain the high OOD generalization performance. Additionally, we observed that if the domain distance is too large, the large-scale pre-training dataset brings a negative impact to the label-driven supervised models.\nThe proposed BMFL framework obtains the highest OOD generalization performance, the key of its success can be summarized into two reasons: 1. The effect of the prior cognitive knowledge. 2. The advantage of the cross-attention mechanism and the PCC maximization regulation method. We conduct the following ablation studies to explore how the aforementioned factors influence the OOD generalization capability."}, {"title": "4.4. Ablation Study", "content": "We conduct the first ablation study to investigate how different fusion methods affect the OOD generalization ability, the results are shown in Table 3. We eliminate the cross-attention module and the 12 loss $L_{fusion}$, using a simple concatenation fusion method to combine the image $[CLS]$ token $I_c$ and the fMRI $[CLS]$ token $F_c$ directly as the classification features. Our completed BMFL framework demonstrates the best OOD generalization performance on the Low light and Masked OOD datasets. Comparing with the visual backbone DINOv2, introducing fMRI directly fails to bring the improvement and even reduces the OOD generalization capability. Comparing with the concatenation fusion method, the cross-attention mechanism and PCC maximization method demonstrate the advantages on the brain-machine fusion task respectively, successfully improving the OOD generalization capability of the CV backbone.\nIn particular, it is important to note that the Stylized ImageNet dataset is the most distinctive one within the three datasets, the more complex the multimodal fusion method is, the worse OOD generalization ability the model has. In conjunction with the aforementioned brain encoder method, the fMRI is predicted with the input features of the visual backbone DINOv2. Due to the low classification accuracy of the Stylized ImageNet dataset, the quality of the corresponding predicted fMRI are thereby influenced. Considering the high noise contained in the predicted fMRI, the lightweight fusion method will not damage the image features as well as the fMRI features during the fusion process and obtains a higher classification performance.\nIn the second ablation experiment, we examine how different brain ROIs affect the OOD generalization performance. According to the previous works, the brain visual cortex can be simply divided into two portions, the low-level visual cortex (LVC) and the high-level visual cortex (HVC). In our experiment, we follow the division of (Gifford et al., 2023), where LVC contains four ROIs: V1, V2, V3 and hV4. As for HVC, the ventral, lateral and parietal cortex can be categorized into the HVC area. In general, the LVC in the occipital region contains contour and color information, while the HVC contains more semantic information (DiCarlo et al., 2012)."}, {"title": "5. Conclusion", "content": "In this paper, we present a novel brain-machine fusion learning framework as well as two self-created OOD classification datasets based on ImageNet-1k validation dataset. Through the above experiments, we have confirmed that introducing prior cognitive knowledge from human brain into DNNs can effectively mitigate the domain shift phenomenon. Letting prior cognitive knowledge guide the perception process of CV model can improve the OOD generalization ability. Through the ablation study, we compared how different fusion methods and different brain regions affect the OOD generalization performance of the proposed BMFL framework. The experimental results are consistent with the basic assumptions of both CV as well as cognitive neuroscience.\nIn our future work, we are willing to develop a robust visual neural encoding model with the lightweight architecture, reducing the computational requirements while improving the quality of predicted fMRI."}]}