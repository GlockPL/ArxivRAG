{"title": "Dual Alignment Maximin Optimization for Offline Model-based RL", "authors": ["Chi Zhou", "Wang Luo", "Haoran Li", "Congying Han", "Tiande Guo", "Zicheng Zhang"], "abstract": "Offline reinforcement learning agents face significant deployment challenges due to the synthetic-to-real distribution mismatch. While most prior research has focused on improving the fidelity of synthetic sampling and incorporating off-policy mechanisms, the directly integrated paradigm often fails to ensure consistent policy behavior in biased models and underlying environmental dynamics, which inherently arise from discrepancies between behavior and learning policies. In this paper, we first shift the focus from model reliability to policy discrepancies while optimizing for expected returns, and then self-consistently incorporate synthetic data, deriving a novel actor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a unified framework to ensure both model-environment policy consistency and synthetic and offline data compatibility. The inner minimization performs dual conservative value estimation, aligning policies and trajectories to avoid out-of-distribution states and actions, while the outer maximization ensures that policy improvements remain consistent with inner value estimates. Empirical evaluations demonstrate that DAMO effectively ensures model and policy alignments, achieving competitive performance across diverse benchmark tasks.", "sections": [{"title": "1. Introduction", "content": "Offline reinforcement learning (RL) (Lange et al., 2012; Levine et al., 2020) aims to learn policies from a pre-collected dataset generated by a behavioral policy within the real environment. This paradigm helps avoid the safety risks and high costs associated with direct interactions, making RL applicable in real-life scenarios, such as healthcare decision-making support and autonomous driving (Emerson et al., 2023; Sinha et al., 2022; Mnih et al., 2015). Offline model-based RL (Yu et al., 2020; Kidambi et al., 2020; Sun et al., 2023) improves upon this by training a dynamics model and using it to generate synthetic data for policy training or planning. The introduction of dynamics models enhances the sample efficiency and allows the agent to answer counterfactual queries (Levine et al., 2020). However, despite strong performance in learned models, policies often degrade significantly when deployed in real environments due to mismatches between synthetic and real distributions.\nMost prior works have focused on enhancing the reliability of synthetic samplings from learned models, and directly incorporating off-policy optimization methods, such as SAC (Haarnoja et al., 2018), to address this mismatch. The representative approach is MOPO (Yu et al., 2020), which quantifies model uncertainty by measuring the predicted variance of learned models and subsequently penalizes model-generated trajectories with high uncertainty. This methodology has inspired subsequent research like MOREL (Kidambi et al., 2020) and MOBILE (Sun et al., 2023), which adopt similar uncertainty-aware frameworks. These approaches help generate synthetic data that remains compatible with the offline dataset and construct conservative value functions to mitigate out-of-distribution (OOD) actions. However, creating a perfect learned model from limited offline datasets is impossible, and as illustrated in Fig. 1, simply combining these methods with off-policy mechanisms still fails to resolve the inherent discrepancies in policy behaviors between the learned model and the underlying environment, leading to inconsistent policy performance and leaving the distribution shift problem unsolved.\nIn this paper, we trace the inherent root of distribution shift challenges in offline RL to the discrepancies between behavior and learning policies in the underlying environmental dynamics. While optimizing for expected returns, we introduce a regularized policy optimization objective for offline RL, that constrains the visitation distribution discrepancies between the behavioral and the learning policies in real environments. We further self-consistently incorporate synthetic data into this objective, deriving a novel maximin optimization objective for offline model-based RL. Building on these objectives, we propose Dual Alignment Maximin Optimization (DAMO), a consistent actor-critic framework"}, {"title": "2. Related Work", "content": "Offline Model-based RL trains a dynamics model using offline data to approximate environmental dynamics and performs conservative policy optimization (Lu et al., 2021) with model-generate data to mitigate OOD issues. MOPO (Yu et al., 2020) penalizes state-action pairs with high model uncertainty, effectively discouraging the agent from selecting OOD actions. MOREL (Kidambi et al., 2020) constructs a pessimistic Markov decision process to prevent the agent from entering OOD regions, ensuring safer and more reliable policy learning. COMBO (Yu et al., 2021) extends Conservative Q-learning (Kumar et al., 2020) to the model-based setting by regularizing the value function on OOD samples. RAMBO (Rigter et al., 2022) employs an adversarial training framework, optimizing both the policy and model jointly to ensure accurate transition predictions while maintaining robustness. MOBILE (Sun et al., 2023) incorporates penalties into value learning by utilizing uncertainty in Bellman function estimates derived from ensemble models. SAMBO (Luo et al., 2024a) imposes reward penalties and fosters exploration by incorporating model and policy shifts inferred through a probabilistic inference framework.\nDICE-based Methods. Distribution Correction Estimation (DICE) is a technique that estimates stationary distribution ratios using duality theory. DICE has shown superior performance in evaluating discrepancies between distributions and has been widely applied in various RL domains, including off-policy evaluation (OPE) (Nachum et al., 2019a), offline imitation learning (IL) (Ma et al., 2022), and offline RL (Nachum et al., 2019b). In offline RL, DICE-based methods correct distribution shifts between offline data and environmental dynamics, formulating a tractable maximin optimization objective. For instance, AlgaeDICE (Nachum et al., 2019b) pioneers the application of DICE-based methods in offline RL, employing regularized dual objectives and Lagrangian techniques to address distribution shift challenges. OptiDICE (Lee et al., 2021) incorporates the Bellman flow constraint and directly estimates stationary distribution corrections for the optimal policy, eliminating the need for policy gradients. ODICE (Mao et al., 2024a) combines orthogonal-gradient updates with DICE to enforce state-action-level constraints. Unlike these model-free approaches, DAMO is, to the best of our knowledge, the first method to promote DICE insights in the model-based setting. While model-free methods typically handle discrepancies between two distributions, DAMO skillfully extends this concept by managing the differences across three distributions, using a divergence upper bound to ensure alignment."}, {"title": "3. Preliminaries", "content": "MDP. We consider the Markov decision process (MDP) defined by the six-element tuple M = (S, A, T, r, \\mu_0, \\gamma), where S denotes the state space, A denotes the action space, T(s'|s, a) is the environment transition dynamics, r(s,a,s') > 0 is the reward function, \\mu_o is the initial state distribution, and \\gamma \\in (0,1) is the discounted factor. Given an MDP, the objective of RL is to find a policy"}, {"title": "4. Demystify OOD in Offline Model-based RL", "content": "In this section, as illustrated in Fig. 2, we explore the out-of-distribution (OOD) issues in offline model-based RL. We focus on the importance of aligning both the model and offline data, as well as the model and environment policy behavior, which motivates the direction of this research."}, {"title": "4.1. OOD Issues in Offline RL", "content": "In offline RL, OOD problems (Mao et al., 2024b) arise when agents encounter states or select actions that fall outside the distribution of the offline data during training or testing.\nOOD actions refer to actions that the behavior policy does not choose in specific states, rather than actions that are simply absent from the offline dataset. This implies that OOD actions are inherently state-dependent. Thus some studies use the term OOD state-action pairs to describe the state-action pair (s, a) that does not appear in the offline dataset. Taking OOD actions during training can lead to severely inaccurate value estimation and significantly degraded policy performance in testing. Consequently, avoiding OOD actions has been a focus in previous offline RL research.\nOOD states primarily arise in three main scenarios: 1) The learned policy executes unreliable OOD actions, leading to transitions into OOD states. 2) The initial state of the real environment lies outside the offline dataset. Additionally, stochastic dynamics could unexpectedly transition the agent into OOD states, even if it takes in-distribution (ID) actions in ID states. 3) Nonstationary dynamics, such as disturbances in real-world environments, can introduce unexpected changes in state distributions. While this factor is outside the scope of our offline model-based setting, it is an important consideration in real-world applications."}, {"title": "4.2. OOD Issues under Dynamics Model Integration", "content": "OOD issues in offline model-based RL are intrinsically linked to the mismatch between synthetic and real data distributions, compounded in model-based settings, where model inaccuracies introduce additional sources of error.\nOOD Actions. The model, M, is used to generate synthetic data for training, but it is essential to avoid taking OOD actions when rolling out policy in the model. This can trigger negative model exploitation (Levine et al., 2020), resulting in poor policy performance. Aligning the distributions of synthetic and offline data can effectively mitigate these issues as it ensures accurate value estimation.\nOOD States. The performance of the learning policy during deployment is critically dependent on the alignment of transition pairs between the learned model and the real environment. Significant discrepancies between these transitions can cause performance degradation and lead to OOD states, highlighting the necessity of correcting transition biases during training. Even if the learned policy performs well in the model, it is important, during real-environment deployment, to avoid OOD states, for which we cannot accurately estimate the value function in the offline setting.\nIn summary, while optimizing the policy \\pi to maximize returns, we should constrain it in two directions: 1) aligning the synthetic data (s, a, s') with the offline data (s, a, s'), and 2) ensuring that \\pi exhibits consistent behavior in both dynamics models and real environments."}, {"title": "5. Dual Alignment Maximin Optimization", "content": "Inspired by the analysis above, we propose Dual Alignment Maximin Optimization (DAMO), a unified approach to mitigate both OOD actions and states. We further explore the distinct roles of the inner minimization and the outer maximization in addressing the distribution shift challenge."}, {"title": "5.1. Maximin Objective for Unified Shift Mitigation", "content": "We start from the root cause of distribution shifts, i.e., the discrepancy between learned and behavior policy behaviors in the underlying environment. To tackle this, we introduce a regularized objective that constrains the difference between these behaviors while optimizing for expected rewards:\n$\\max_\\pi E_{p^{\\pi}}[r(s, a, s')] - \\alpha D_{KL}(p^{\\pi}||p^{\\pi_{\\beta}}),$\nwhere \\alpha is a regularization hyperparameter. Next, we self-consistently incorporate synthetic data through a divergence upper bound, deriving the following surrogate objective:\n$\\max_\\pi E_{p^{\\pi}}[r(s, a, s') - \\alpha \\log \\frac{p^M}{p^{\\pi_{\\beta}}}] - D_f(p^{\\pi}||p^M).$ (1)"}, {"title": "Theorem 5.1.", "content": "(1) is equivalent to the following problem:\n$\\max_\\pi \\min_{Q(s,a)} (1 - \\gamma)E_{s \\sim \\mu_0, a \\sim \\pi}[Q(s, a)] + \\alpha E_{p^{\\pi_{\\beta}}}[f^* (\\Phi(s, a, s')/\\alpha)],$ (2)\nwhere $f_*$ is the conjugate function of the convex function $f$, and $\\Phi(s, a, s') = \\log r(s, a, s') - \\alpha \\log \\frac{p^M}{p^{\\pi_{\\beta}}} + \\tau^\\pi Q(s, a) - Q(s, a)$, with $\\tau^\\pi Q(s, a) = \\gamma \\sum_{a'}Q(s', a')\\pi(a'|s')$. Here, $\\mu_0$ denotes the distribution of the initial state, and $\\alpha$ is a hyperparameter to control the degree of conservatism."}, {"title": "5.2. Practical Implementation of DAMO", "content": "To implement the maximin optimization (2) practically, we employ a classifier to approximate the data alignment term $\\log(\\frac{p^M}{p^{\\pi_{\\beta}}})$ . Specifically, we train a classifier $h(s, a, s')$ using the following loss function to distinguish transitions sampled from offline data and those generated by the model:\n$\\min_h \\frac{1}{|D_R|}\\sum_{(s, a, s') \\in D_R} \\log h(s, a, s') + \\frac{1}{|D_M|}\\sum_{(s, a, s') \\in D_M} [\\log (1 - h(s, a, s'))].$ (3)\nHere, $D_R$ and $D_M$ represent the offline and the model-generated data, respectively. Based on the learned classifier $h^*(s, a, s')$, the data alignment modification $\\log(\\frac{p^M}{p^{\\pi_{\\beta}}})$ can be computed using the following analytical expression:\n$\\log \\frac{p^M}{p^{\\pi_{\\beta}}} = \\log \\frac{h^*(s, a, s')}{1 - h^*(s, a, s')}$ (4)\nNote that (2) can be divided into two phases: an inner value estimation phase and an outer policy improvement phase, resembling the structure of the actor-critic framework. This structure is directly adapted to implement DAMO. The complete DAMO framework is outlined in Algorithm 1."}, {"title": "5.3. Theoretical Insights for DAMO", "content": "The core structure of DAMO involves mapping the inner minimization in (2) to the estimation of a dual conservative value Q(s, a), and the outer maximization to policy improvement based on this value. In this subsection, we present deeper theoretical insights into the workings of DAMO.\nDual Conservative Value Estimation. We first demonstrate the existence of both explicit and implicit reward penalties in the optimization objective (2), which contribute to the construction of the dual conservative value estimation."}, {"title": "Theorem 5.2.", "content": "The optimal solution $Q_*$ for the inner minimization optimization in (2) satisfies the following equation:\n$Q = r - \\alpha \\log \\frac{p^M}{p^{\\pi_{\\beta}}} - \\alpha f'(\\frac{p^{\\pi}}{p^M}) + \\tau^{\\pi}Q$.\nThis indicates that DAMO inherently introduces an explicit data alignment modification, $\\log(\\frac{p^M}{p^{\\pi_{\\beta}}})$, and an implicit behavior alignment adjustment, $f'(\\frac{p^{\\pi}}{p^M})$. Minimizing the inner objective effectively reshapes the reward function, r(s, a, s'), into a refined reward, r'(s, a, s'), which represents the dual conservative value estimation:\n$r' = r - \\alpha \\log \\frac{p^M}{p^{\\pi_{\\beta}}} - \\alpha f'(\\frac{p^{\\pi}}{p^M})$.\nThe data alignment modification $\\log(\\frac{p^M}{p^{\\pi_{\\beta}}})$ penalizes transition (s, a, s') that exhibit discrepancies between synthetic and offline data, promoting alignment with in-distribution transitions. This helps ensure accurate value estimation for these transitions. Meanwhile, the behavior alignment adjustment $f'(\\frac{p^{\\pi}}{p^M})$ focuses on that different dynamics impact the distribution of (s, a, s'). By penalizing these discrepancies, we encourage the agent to select policies that exhibit similar performance in both the underlying environment and the dynamics model, thus mitigating the impact of OOD states and ensuring consistent policy behavior between dynamics models and real environments."}, {"title": "Consistent Policy Improvement.", "content": "Unlike conventional offline model-based approaches that optimize policies purely by maximizing estimated values, DAMO ensures the consistency between value estimation and policy improvement. Specially, DAMO employs Q(s, a) to minimize the objective (2) and utilizes \\pi to maximize it. Conventional approaches typically follow a bilevel optimization framework, while DAMO employs a maximin paradigm, ensuring consistency and preserving dual conservatism. As demonstrated in Sec. 6.1, this consistent structure maintains the conservatism of value estimation during policy improvement.\nOnce the inner minimization in (2) is solved, the outer maximization problem is equivalent to solving the surrogate objective (1), which provides a lower bound for the standard RL objective $J(\\pi) = E_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t, s_{t+1})]$:"}, {"title": "Theorem 5.3.", "content": "(1) is a lower bound of J(\\pi) for all \\pi:\n$J(\\pi) \\geq E_{p^{\\pi}}[r(s, a, s') - \\alpha \\log \\frac{p^M}{p^{\\pi_{\\beta}}}] - \\alpha D_f(\\frac{p^{\\pi}}{p^M}).$\nThis demonstrates that the equivalence between objective (2) and the standard reinforcement learning objective ensures that the learned policy performs well in the environment."}, {"title": "6. Experiments", "content": "In this section, we focus on the following key aspects: 1) The capability of DAMO to handle synthetic-to-real distribution mismatch, as well as the contributions of its components. 2) The comparative performance of DAMO against existing methods on standard offline RL benchmarks. 3) The significant impact of hyperparameter settings and implementation approaches on the overall performance of DAMO.\nWe delve into these aspects using the D4RL benchmark (Fu et al., 2020) on the MuJoCo simulator (Todorov et al., 2012). Our implementation is based on the OfflineRL-Kit library\u00b9, a comprehensive and high-performance library for implementing offline reinforcement learning algorithms. The basic parameters of DAMO are consistent with the settings of this library. The details of each experiment and the specific hyperparameters configuration can be found in Appendix D."}, {"title": "6.1. Effectiveness of DAMO Paradigm", "content": "In this section, we present empirical evidence demonstrating the dual capabilities of DAMO: 1) effectively aligning model-generated data with offline data distributions, and 2) consistently maintaining policy stability between dynamics models and real environments. Our detailed ablation study thoroughly investigates two key components of DAMO: inner value estimation and outer policy improvement."}, {"title": "6.1.1. DUAL CONSERVATIVE VALUE ESTIMATION", "content": "In this part, we present an ablation study examining the individual contributions of the data alignment and behavior alignment terms. We systematically evaluate their impact by removing each term from DAMO and training policies on the hopper-medium-expert-v2 benchmark. To provide intuitive insights, we first visualize the state-action distributions of the final trained policies across three distinct dataset categories. These categories are defined as follows:\nOffline Data: The transition pairs (s, a, s') are collected with behavior policy $\\pi_\\beta$ in dataset Hopper-medium-expert.\nSynthetic Data: The transition pairs are generated by conducting training policy $\\pi$ within the dynamics model $M$.\nReal Data: The transition pairs are generated by conducting training policy $\\pi$ within the real environment $T$.\nEffectiveness of Data Alignment. To evaluate synthetic and offline data compatibility during training, we analyze the state-action pair distributions for both synthetic data and offline data under three distinct conditions: the complete DAMO framework, the configuration without the behavior alignment term (w/o ir), and the configuration without the data alignment term (w/o er). The visualization results are presented in the left column of Fig. 3. Building on these initial observations, we then examine the training dynamics of DAMO under three specific conditions to further investigate their significant impacts. The results are presented in Fig. 4. A comparative analysis between Fig. 3(a) and Fig. 3(c) reveals that the w/o er configuration fails to establish proper alignment between synthetic data and offline data, thereby inadequately mitigating OOD state-action pairs. The results in Fig. 3(b) further highlight that the data alignment term enables w/o ir configuration to align synthetic data with offline data. To clarify the detrimental effects of incompatibility between these two data sources, Fig. 4(b) illustrates that w/o er overestimates the value of OOD actions, leading to suboptimal training performance. This is corroborated by the poor evaluation scores of w/o er depicted in Fig. 4(a).\nEffectiveness of Behavior Alignment. To assess the model-environment consistency during the testing phase, we follow a similar evaluation procedure. We visualize the state distributions of real environment and model-generated data under the same conditions. The visualization results are presented in the right column of Fig. 3. A comparative analysis between Fig. 3(b) and Fig. 3(c) reveals that while w/o ir successfully achieves synthetic-offline data compatibility and mitigates OOD actions, the absence of the behavior alignment term leads to incomplete mitigation of OOD states, manifesting policy inconsistency between dynamics model and real environment. Although the behavior alignment term imposes constraints, the results in Fig. 3(a) demonstrate that w/o er fails to effectively mitigate OOD states. The observed discrepancies in state distributions"}, {"title": "6.1.2. TOWARDS CONSISTENT POLICY IMPROVEMENT", "content": "This section presents a statistical analysis of the value estimation performance of DAMO across multiple datasets, with comparisons against the true value function. Our results validate that the compatible policy improvement framework effectively maintains the conservatism of value estimation.\nTo investigate the significance of objective alignment in DAMO, we conduct a controlled comparison experiment by directly integrating DAMO with standard off-policy mechanisms (Inconsistent Version) while evaluating the critic's estimation of state-action pair values and comparing it with the original DAMO (Consistent Version). We perform this analysis on the medium-expert dataset across three distinct environments: HalfCheetah, Hopper, and Walker2d. The obtained quantitative results are summarized in Table 1. The experimental findings reveal that the Consistent Version achieves conservative value estimations across all environments. Although the Inconsistent version maintains conservatism in the HalfCheetah and Hopper environments, it exhibits significant overestimation in the Walker2d environment. These results collectively demonstrate the critical role of objective alignment in consistently ensuring conservative value estimation and preventing overestimation errors."}, {"title": "6.2. Comparison Results", "content": "Dataset. We evaluate DAMO's performance using the D4RL benchmark (Fu et al., 2020) within the MuJoCo simulation environment. Our comprehensive assessment covers 12 distinct datasets across three environments (HalfCheetah, Hopper, and Walker2d) and four dataset categories: random, medium, medium-replay, and medium-expert. Following standard evaluation protocols, we exclusively utilize the \"v2\u201d versions of all datasets for consistent comparison.\nBaseline. We compare DAMO with two types of offline RL baselines: DICE-based algorithms which follow a similar form of training objective with DAMO and Model-based algorithms which achieve SOTA performance. For DICE-based approaches, Algae-DICE (Nachum et al., 2019b) transforms the intractable state-action-level constraint into a unified objective for policy training. Opti-DICE (Lee et al., 2021) directly estimates the stationary distribution corrections of the optimal policy to attain a high-rewarding policy. O-DICE (Mao et al., 2024a) using the orthogonal-gradient update to diminish the confliction of different gradients during policy training. For model-based approaches, MOPO (Yu et al., 2020) penalizes rewards via the predicted variance of ensembled dynamics models. COMBO (Yu et al., 2021) implements CQL within a model-based framework. TT (Janner et al., 2019) uses a transformer to model offline trajectories and employs beam search for planning. RAMBO (Rigter et al., 2022) adversarially trains the policy and the dynamics model within a robust framework. MOBILE (Sun et al., 2023) penalizes the value learning of synthetic data based on the estimated uncertainty of the Bellman Q-function, derived from an ensemble of models.\nComparison Results. Table 2 reports the scores in D4RL benchmark. Overall, DAMO demonstrates superior performance, achieving the highest average score across all baseline methods. Notably, for high-quality datasets such as medium-expert and medium-replay, DAMO consistently attains state-of-the-art (SOTA) or near-SOTA performance. These results suggest that effective OOD constraints, combined with a high-quality behavior policy, enable the agent to learn high-reward policies within in-distribution (ID) regions. Furthermore, DAMO maintains strong performance on medium-quality datasets. This demonstrates the robustness of DAMO in handling less-than-ideal data conditions and highlights the importance of ensuring both model-environment policy consistency and synthetic-offline data compatibility. However, the performance of DAMO on random datasets reveals limitations, indicating the need for more aggressive exploration strategies in low-quality data scenarios to identify regions beyond those covered by the offline dataset. This phenomenon is particularly pronounced in the Walker2d-random dataset, where DAMO struggles to learn an effective policy. One possible explanation is that the optimization landscape of the environment may contain numerous suboptimal solutions corresponding to saddle points in the maximin framework. If this is the case, such structural characteristics could potentially make DAMO more susceptible to becoming trapped in local optima, thereby limiting its ability to discover better solutions. However, further investigation is needed to confirm this hypothesis."}, {"title": "6.3. Effects of Coefficient \u03b1", "content": "DAMO is regulated by a key hyperparameter: the hyperparameter \u03b1 which controls the extent of policy discrepancies. In practical implementation, we fix \u03b1 in the actor training. While we consider \u03b1 as a hyperparameter controlling the degree of conservatism, the empirical results in Fig. 5(a) demonstrate that DAMO maintains robust performance across a wide range of \u03b1 values, with its primary impact observed in the final stage performance. Notably, DAMO achieves superior performance with larger \u03b1 values (e.g., \u03b1 = 3.0, 5.0), whereas insufficiently small values (e.g., \u03b1 = 1.0) tend to promote excessive risk-taking during exploration, ultimately failing to converge to an optimal policy. To enhance training stability, we initially fix \u03b1 to 1.0 in the actor training. However, experimental results demonstrate that DAMO achieves competitive performance even without this specific implementation. To further validate this observation, we conduct comparative experiments on the HalfCheetah environment, evaluating both the standard DAMO implementation and its variant without fixed \u03b1 under their respective optimal hyperparameter configurations. As shown in Fig. 5(b), both settings achieve a competitive performance, confirming that DAMO maintains robust performance regardless of the implementation of fixing \u03b1."}, {"title": "7. Conclusion and Discussion", "content": "This paper critically examines limitations in existing offline model-based RL approaches, highlighting the crucial need to address behavior inconsistency caused by policy discrepancies between biased models and real environments. Building upon our analysis, we introduce a unified framework that concurrently ensures model-environment policy consistency and synthetic-offline data compatibility, with comprehensive experimental validation demonstrating its effectiveness. Significantly, the maximin optimization framework of DAMO demonstrates the benefits of aligning the actor and critic training objectives within the actor-critic architecture, ensuring essential conservatism during policy updates. The limitations of our work are focused on twofold: the sampling of the initial state and the computing of the data alignment term. Although we assume the distribution of the initial state is known, the offline dataset does not explicitly label which states are initial states. Therefore, we randomly sample a state from the offline dataset to serve as the initial state, which creates a gap between theory and practice. When calculating the data alignment term, we trained a classifier to assist in the process. However, the classifier's accuracy is critical: imprecise classifiers fail to"}, {"title": "Impact Statement", "content": "This work presents an effective solution for addressing the distribution shift in offline model-based RL through dual policy-environment alignment, enabling robust simulation-to-reality transfer in safety-critical domains such as medical robotics. While advancing certifiable policy deployment via implicit behavior constraints and adaptive value estimation, challenges remain in reward specification sensitivity and residual model bias mitigation. The framework highlights the critical need for continuous dynamic alignment calibration to ensure reliable real-world RL system performance."}, {"title": "B. OOD Issues under Dynamics Model Integration", "content": "OOD issues in offline model-based RL are intrinsically linked to the mismatch between synthetic and real data distributions, compounded in model-based settings, where model inaccuracies introduce additional sources of error."}, {"title": "B.1. OOD Actions", "content": "The dynamics model generates data that can significantly deviate from the distribution of the offline dataset, leading to OOD actions. This triggers negative model exploitation (Levine et al., 2020), resulting in poor policy performance. Aligning the distributions of synthetic and offline data can effectively mitigate these issues. The model, M, is used to generate synthetic data for training, but it is essential to avoid taking OOD actions when rolling out policy in the model, to ensure accurate value estimation.\nSpecifically, in an offline setting, where interaction with the real environment is not possible, overestimating the value of OOD actions leads to model exploitation. Consequently, the agent may learn policies that perform poorly in the underlying environment compared to the model, leading to algorithmic failure during deployment. To avoid model exploitation, it is crucial to align synthetic with offline data. Given offline data collected under the behavior policy \u03c0\u03b2, We aim to find a policy \u03c0 that behaves in model M like \u03c0\u03b2 does in environment T. Specifically, we aim to match the distribution of (s, a) with (s, a), ensuring accurate value estimation of (s, a)."}, {"title": "B.2. OOD states", "content": "The performance of the learning policy during deployment is critically dependent on the alignment of transition pairs between the learned model and the real environment. Significant discrepancies between these transitions can cause performance degradation and lead to OOD states, highlighting the necessity of correcting transition biases during training. Even if the learned policy performs well in the model, it is important, during real-environment deployment, to avoid OOD states, for which we cannot accurately estimate the value function in the offline setting.\nFor example, if the learned model predicts the transition (s, a, s\u2081), but the real environment produces a transition (s, a, s\u2082), where s1 is a high-value state and s2 is a low-value state, the policy will perform poorly in the real environment. The"}, {"title": "Dual Alignment Maximin Optimization for Offline Model-based RL", "content": "situation worsens if s\u2082 is an OOD state that has never been encountered in model M before. In this scenario, the agent cannot estimate the value of the state-action pair (s\u2082, a'), leading to ineffective decision-making and model-environment policy inconsistency. In an offline setting, since we cannot adjust the policy by interacting with the environment, we must approximate and correct the discrepancies between the model M and the real environment T during training. This ensures that the distribution of transition (s, a, s') matches the distribution of (s, a, s') in the underlying environment.\nIn summary, while optimizing the policy \u03c0 to maximize returns, we should constrain it in two directions: 1) aligning the synthetic data (s, a, s') with the offline data (s, a, s'), and 2) ensuring that \u03c0 exhibits consistent behavior in both dynamics models and real environments."}, {"title": "C. Implementation Details", "content": "In this section, we present the detailed implementation of DAMO."}, {"title": "C.1. Selection of f-divergence", "content": "As established in Theorem 5.3, the f-divergence must consistently exceed the KL-divergence to guarantee that the objective in Equation (2) serves as a lower bound for the standard RL objective J(\u03c0). Based on this, we specifically choose f(x) = (x - 1)\u00b3, and its conjugate function is f*(x) = (x \u2013 1)\u00b2 / 2 . In principle, any choice of f-divergence is permissible, and we adhere to the selection made in the prior work (Luo et al., 2024b). Due to the tight schedule, we haven't tried other choices of f-divergence. However, we suppose that the choice of f-divergence won't enormously affect the effectiveness of DAMO, since f-divergence primarily serves as a measure of discrepancy."}, {"title": "C.2. Modification of actor training objective", "content": "When optimizing the actor network using objective (2), we observed that the substantial residual of the Bellman error often disrupts the optimization of the actor and leads to training instability. To stabilize the training process, we followed the implementation of previous DICE work (Mao et al., 2024a; Sikchi et al., 2023) and fixed the hyperparameter \u03b1 to 1.0 during the optimization of the actor network. The specific form of the actor training objective is as follows:\n$\\max_\\pi (1 - \\gamma)E_{s \\sim \\mu_0, a \\sim \\pi}[Q(s, a)] + E_{p^{\\pi_{\\beta}}}[f^* (\\Phi(s, a, s'))]$\nIt should be noted that the effectiveness of DAMO does not rely on this trick. We demonstrated this in the experimental section.6.3."}, {"title": "C.3. Model, classifier and Policy Optimization", "content": "For dynamics model M, it was depicted as a probabilistic neural network that generates a Gaussian distribution for the subsequent state and reward, contingent upon the current state and action:\n$m_{\\Theta}(s_{t+1}, r_t | s_t, a_t) = N(\\mu_{\\Theta}(s_t, a_t), \\Sigma_{\\Theta}(s_t, a_t)).$\nOur model training approach is consistent with the methodology used in prior works (Yu et al., 2020; Sun et al., 2023). We train an ensemble of seven dynamics models and select the best five based on their validation prediction error from a"}, {"title": "D. Experiment Setting", "content": "We evaluate our approach using the D4RL benchmark (Fu et al., 2020), focusing on the \u201cv2\u201d version of Gym tasks. For DICE-based methods (Algae-DICE, Opti-DICE, and O-DICE), we directly adopt the scores reported in their respective original papers. However, since the results for O-DICE on the random dataset are not provided in the original paper, we exclude them from our comparison. For offline model-based methods, we retrained MOPO (Yu et al., 2020) and MOBILE (Sun et al., 2023) on the \"v2\" Gym datasets. For the remaining methods, we directly report the scores provided in their original papers. All results are summarized in Table 2."}, {"title": "D.2. Hyperparameters", "content": "We provide a list of the hyperparameters that were tuned during our experiments. The detailed configurations are summarized in Table 4.\nCoefficient \u03b1. The coefficient \u03b1 acts as the sole hyperparameter in our objective function, regulating"}]}