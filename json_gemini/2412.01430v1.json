{"title": "MVImgNet2.0: A Larger-scale Dataset of Multi-view Images", "authors": ["XIAOGUANG HAN", "YUSHUANG WU", "LUYUE SHI", "HAOLIN LIU", "HONGJIE LIAO", "LINGTENG QIU", "WEIHAO YUAN", "XIAODONG GU", "ZILONG DONG", "SHUGUANG CUI"], "abstract": "MVImgNet is a large-scale dataset that contains multi-view images of ~220k real-world objects in 238 classes. As a counterpart of ImageNet, it introduces 3D visual signals via multi-view shooting, making a soft bridge between 2D and 3D vision. This paper constructs the MVImgNet2.0 dataset that expands MVImgNet into a total of ~520k objects and 515 categories, which derives a 3D dataset with a larger scale that is more comparable to ones in the 2D domain. In addition to the expanded dataset scale and category range, MVImgNet2.0 is of a higher quality than MVImgNet owing to four new features: (i) most shoots capture 360\u00b0 views of the objects, which can support the learning of object reconstruction with completeness; (ii) the segmentation manner is advanced to produce foreground object masks of higher accuracy; (iii) a more powerful structure-from-motion method is adopted to derive the camera pose for each frame of a lower estimation error; (iv) higher-quality dense point clouds are reconstructed via advanced methods for objects captured in 360\u00b0 views, which can serve for downstream applications. Extensive experiments confirm the value of the proposed MVImgNet2.0 in boosting the performance of large 3D reconstruction models. MVImgNet2.0 will be public at luyues.github.io/mvimgnet2, including multi-view images of all 520k", "sections": [{"title": "1 INTRODUCTION", "content": "The field of deep learning has witnessed remarkable advancements, fueled primarily by learning from vast amounts of data [Deng et al. 2009; Krishna et al. 2017; Lin et al. 2014; Miech et al. 2019]. Learning from large-scale data has proven to be a key driver in scaling up deep learning models to tackle complex understanding or generative tasks, especially for the development of large models in the fields including natural language processing [Achiam et al. 2023; Thoppilan et al. 2022; Touvron et al. 2023], computer vision [Kirillov et al. 2023; Liu et al. 2023b, 2024b; Ren et al. 2024], and multimodal learning [Li et al. 2023a; Lin et al. 2023; Liu et al. 2024a].\nThis learning regime also attracts great attention in the field of 3D vision. In spite of the greater difficulty in collecting and labeling 3D data compared with textual or 2D visual data, there are still some efforts contributed to constructing large-scale or high-quality 3D generic datasets [Chang et al. 2015; Deitke et al. 2023, 2022; Downs et al. 2022; Reizenstein et al. 2021; Wu et al. 2023b; Yu et al. 2023]. Among them, one line of work constructs datasets like ShapeNet [Chang et al. 2015] and Objaverse [Deitke et al. 2022] composed of synthetic data, which limits the application in real scenarios. Differently, another line of work collects 3D data of real-life objects via scanning or multi-view photogrammetry. However, such datasets like CO3D [Reizenstein et al. 2021] and GSO [Downs et al. 2022] are limited in scale and category range until Yu et al. make the first step in constructing a large-scale one, MVImgNet [Yu et al. 2023], consisting of ~220k multi-view images of 238 classes of common objects. The massive multi-view data do not only prove valuable in 2D visual understanding [Yu et al. 2023] via learning cross-view consistency, but also support the learning of generic shape priors to benefit 3D reconstruction [Hong et al. 2023; Wang et al. 2023; Wu et al. 2023a; Xu et al. 2023]. Considering the larger scale of datasets in the 2D domain, e.g. ImageNet [Deng et al. 2009], containing over 1 million images of 1k categories, MVImgNet is still inferior in scale that may limit its potential to support scaling up 3D learning. Therefore, we propose MVImgNet2.0 that expands MVImgNet to twice its original scale and category range. With a total of 520k objects and 515 categories that is half the scale of ImageNet, MVImgNet2.0 makes a further step towards a larger real-world 3D dataset with a smaller gap to ones in the 2D domain. In addition to the expanded data scale and category range, MVImg-Net2.0 has some other new features in data acquisition and annotation to improve the dataset quality. The biggest difference in data acquisition is that MVImgNet videos usually cover 180\u00b0 views of objects, while most of the videos (230k/300k) collected in MVImgNet2.0"}, {"title": "2 RELATED WORK", "content": "Large-scale datasets. Expanding the size and breadth of training datasets has proven to be a highly effective strategy for enhancing the performance and robustness of deep learning models. In computer vision, the introduction of large-scale datasets such as ImageNet [Deng et al. 2009] and MS-COCO [Lin et al. 2014] has driven significant advancements across various tasks, including image classification, object detection, and captioning. This trend has persisted, with the diversity and scale of available datasets growing exponentially. Notable examples include image datasets"}, {"title": "3 DATASET", "content": "MVImgNet2.0 is a large-scale dataset of multi-view images, which is efficiently collected via shooting 360\u00b0-view videos with phone cameras surrounding objects in the wild. In this section, we introduce the data acquisition and annotation pipeline in constructing"}, {"title": "3.1 Raw Data Acquisition", "content": "Similar to MVImgNet, the raw video data is gained through crowdsourcing. We first specify the diverse data categories to collect and the maximum amount for each category. The categories are chosen following the WordNet [Miller 1994] taxonomy and also from the common objects encountered or utilized in human daily life, and the maximum amount is determined by their generality and the complexity involved in capturing them. In addition to quantitatively expanding some of MVImgNet's categories with a small number of videos (70 categories), we have also collected 277 new categories to expand MVImgNet's collection of categories. Then, we draw up the requirements for the captured videos: (i) The length of each video must be around 10 seconds; (ii) The frames in the video must not"}, {"title": "3.2 Data Annotation", "content": "For each qualified video submission, we exploit a similar data processing procedure as in reconstructing the MVImgNet dataset to conduct semi-automated annotation, as shown in Fig. 3. At first, around 30 frames are extracted from each video for sparse reconstruction, which derives the estimated camera poses of each view. Then, we generate object masks via segmentation methods for each extracted frame. Finally, given the camera poses and the masks in each view, we conduct dense reconstruction to produce the object point clouds. The main differences in MVImgNet2.0 lie in the advanced approaches to achieve higher-quality annotations, including 1) camera poses, 2) foreground object masks, and 3) point clouds."}, {"title": "3.3 Dataset Statistics", "content": "Tab. 1 shows the statistics of MVImgNet2.0 and other alternatives and Fig. 2 and Fig. 11 shows some samples of MVImgNet2.0. In summary, MVImgNet2.0 includes 300k videos with 9 million frames and 347 object classes, of which 277 are new categories not covered by MVImgNet, and the annotations comprehensively cover object masks, camera pose parameters, and point clouds. The categories are organized in a taxonomic manner in SupMat's Fig. R.4, and we recommend our project page for a more detailed display. In addition, we also give more detailed per-category statistics in SupMat's Fig. R.3. With the construction of MVImgNet2.0, the total statistics"}, {"title": "4 EXPERIMENTS", "content": "This section aims to validate the value of the proposed MVImgNet2.0 in the application of 3D reconstruction. We first introduce the experiment setup, then conduct per-scene 3D reconstruction to validate the value of camera pose annotations with higher accuracy, and finally we pay the main focus on justifying the value of new features in MVImgNet2.0 in improving the performance of large reconstruction models, including the larger data scale, the expanded category range, the 360\u00b0-view videos, and the higher-quality annotations."}, {"title": "4.1 Experiment Setup", "content": "Datasets. We adopt three datasets in experiments, including the synthetic dataset Objaverse [Deitke et al. 2022], the original data in MVImgNet [Yu et al. 2023] (MV1-Data), and the newly added data in MVImgNet2.0 (MV2-Data). The training set includes multiple views captured by videos with estimated camera poses or obtained via rendering the synthetic models from random camera poses. Each object has over 30 views to support training. The test set consists of 1k data sampled from 20 held-out categories in MVImgNet2.0 that are unseen in training. Each test sample contains one or more views with estimated camera poses as input and 8 posed novel-view images with the resolution of 512\u00d7512 as the ground truths. Besides, we also provide high-quality dense point cloud reconstructions with manual cleaning by annotators for each sample in the test set as their shape ground truths.\nEvaluation metrics. As MVImgNet2.0 can provide 2D multi-view ground truths, we mainly employ PSNR/SSIM (higher is better) and LPIPS [Zhang et al. 2018] (lower is better) as the evaluation metrics to measure the reconstruction quality in projected views. In the evaluation of category-agnostic reconstruction, all backgrounds of test views are masked out as in the training set to focus on the reconstruction accuracy of foreground objects, but preserved in per-scene reconstruction experiments. As TriplaneGaussian also outputs the reconstructed point cloud shape, we also employ Chamfer Distance (CD) as the measurement of the reconstructed shape quality.\nBaselines. Our baselines attempt to cover a wide range of reconstruction models. For per-scene 3D reconstruction, we utilize two baselines, Instant-NGP (INGP) [M\u00fcller et al. 2022a] and 3D"}, {"title": "4.2 Per-scene 3D Reconstruction", "content": "We begin our investigation with per-scene 3D reconstruction of object-centric scenarios, utilizing two baseline methods: INGP and 3DGS. We randomly choose 50 objects from 25 categories (2 scenes for each category) to conduct experiments. Among a total of 30 views for each object, we randomly select 20 of them for optimizing the parameters in INGP [M\u00fcller et al. 2022a] and 3DGS [Kerbl et al. 2023], and use the remaining 10 views for evaluation. We design two sets of controlled experiments, each differentiated by a single variable: the camera poses utilized during training. For the first group, the camera poses are estimated via the annotation approach used in MVImgNet (MV1-Anno), while the second group employs the advanced approach in MVImgNet2.0 (MV2-Anno).\nResults. We compute the results averaged across the selected 50 scenes. The quantitative results are shown in Tab. 2, where both INGP and 3DGS can get a more accurate reconstruction using the camera poses estimated via the advanced approach in MVImgNet2.0. By using the camera poses estimated by the advanced approach in MVImgNet2.0, INGP can achieve a higher average PSNR by ~1.1dB, and 3DGS can achieve a significant improvement of ~5.8dB in PSNR. It validates the higher quality of camera poses estimated in MVImgNet2.0. These results also indicate that MVImgNet2.0 can better support the learning-based reconstruction methods in the task of per-scene reconstruction or novel view synthesis."}, {"title": "4.3 Category-agnostic 3D Reconstruction", "content": "On the task of category-agnostic 3D reconstruction, we perform experiments to evaluate the performance of large reconstruction models trained on different kinds of data to validate the value of MVImgNet2.0. We apply an LGM [Tang et al. 2024] for the task of multi-view reconstruction, and an LRM [Hong et al. 2023] and a TriplaneGaussian [Zou et al. 2023] to address the single-view reconstruction. By using different kinds of multi-view data in training, we investigate their effect on the learning of large reconstruction models. Besides, we also use different kinds of point cloud supervision in training TriplaneGaussian to further validate the value of the point cloud annotations in MVImgNet2.0. Finally, we deeply investigate the factor of data scale, category range, and view range in training data by evaluating their effects on the performance of a tiny LGM (LGM-tiny).\nExperiments on LGM and LRM. We train LGM and LRM on three kinds of data: (i) synthetic data from Objaverse [Deitke et al. 2022]; (ii) real data from MVImgNet [Yu et al. 2023] (MV1-Data); and (iii) added data in MVImgNet2.0 (MV2-Data). In training, we use 4 randomly selected views as input, and 20 views as supervision to train an LGM-base model, and use 1 randomly selected input view to train"}, {"title": "5 CONCLUSION", "content": "In this paper, we propose MVImgNet2.0, a larger-scale dataset with multi-view images for generic real-world objects. It expands the"}, {"title": "A MORE DETAILS ABOUT MVIMGNET2.0 DATA", "content": "Per-category data distributions. We count the number of object videos in each category in the proposed MVImgNet2.0 dataset. Note that among all 347 classes, 70 are old classes from MVImgNet [Yu et al. 2023], and these classes are not required to collect more than 1000 videos in the data acquisition process of MVImgNet2.0. Excluding them, ~60% of classes (164/277) cover 1000 or more objects. We provide a histogram for the number of objects in each MVImgNet2.0 new class in Fig. R.1. As shown, categories can be divided into three groups: the first group is \"hard classes\u201d, where less than 500 videos can be collected, while another two groups of categories can get around 1000 and even 2000 videos collected, respectively. A more detailed statistics of the number of objects in each category is shown in Fig. R.3.\nCategory taxonomy. The category taxonomy is shown in Fig. R.4 to better exhibit the categories and their hierarchical relationships in MVImgNet2.0."}, {"title": "B MORE EXPERIMENTS", "content": "Mask annotation quality. In the annotation process, we adopt a detection-segmentation-tracking pipeline to generate the foreground object mask in each view. To better demonstrate the superiority of the used segmentation manner over the original one in MVImgNet, we further evaluate the performance of this pipeline on a subset of MVImgNet data where 500 frames (randomly selected from different categories) are manually annotated with object segmentation masks, and also on other object-centric datasets with ground-truth object masks, i.e., ECSSD [Shi et al. 2015] and DAVIS [Pont-Tuset et al. 2017]. The segmentation results of CarveKit [Selin 2024]"}]}