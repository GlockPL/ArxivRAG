{"title": "Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction", "authors": ["Yooseop Lee", "Suin Kim", "Yohan Jo"], "abstract": "In designing multiple-choice questions (MCQs) in education, creating plausible distractors is crucial for identifying students' misconceptions and gaps in knowledge and accurately assessing their understanding. However, prior studies on distractor generation have not paid sufficient attention to enhancing the difficulty of distractors, resulting in reduced effectiveness of MCQs. This study presents a pipeline for training a model to generate distractors that are more likely to be selected by students. First, we train a pairwise ranker to reason about students' misconceptions and assess the relative plausibility of two distractors. Using this model, we create a dataset of pairwise distractor ranks and then train a distractor generator via Direct Preference Optimization (DPO) to generate more plausible distractors. Experiments on computer science subjects (Python, DB, MLDL) demonstrate that our pairwise ranker effectively identifies students' potential misunderstandings and achieves ranking accuracy comparable to human experts. Furthermore, our distractor generator outperforms several baselines in generating plausible distractors and produces questions with a higher item discrimination index (DI).", "sections": [{"title": "1 Introduction", "content": "Multiple-Choice Questions (MCQs) hold significant educational value as they provide a useful tool for assessing students' knowledge. Among the most critical elements in MCQs are distractors\u2014the incorrect answer options. While the growing demand for education has amplified the need for numerous MCQs, manually creating distractors is time-consuming and costly, even for experts (Luo et al., 2024). Consequently, the automation of distractor generation has emerged as a promising solution (Doughty et al., 2024).\nHowever, prior research has focused primarily on generating distractors similar to human-authored ones (Fernandez et al., 2024; Wang et al., 2023), with insufficient emphasis on enhancing their plausibility. Plausible distractors are crucial as they encourage students to deliberate longer over their answers, and high-quality MCQs must possess an appropriate level of difficulty to differentiate among levels of achievement (Baek, 2019). By contrast, overly simplistic distractors are easily dismissed, failing to adequately assess student"}, {"title": "Related Works", "content": "Previous studies on distractor generation can be categorized based on the item format and domain."}, {"title": "2.1 Distractor Generation", "content": ""}, {"title": "Passage-Based", "content": "Many studies focus on generating distractors based on passage (e.g., reading material), often using datasets like RACE (Lai et al., 2017), DREAM (Sun et al., 2019), SciQ (Welbl et al., 2017), and Wikipedia (Le Berre et al., 2022). Qiu et al. (2020) proposed the EDGE framework, which reformulates passages and questions through attention mechanisms to generate distractors. Qu et al. (2024) introduced a dual-task training approach where separate training was conducted using passages and questions as input to generate both answers and distractors.\nHowever, as our study focuses on MCQs in the CS domain without relying on passages, these prior works are not directly comparable to ours."}, {"title": "Cloze-Style", "content": "Cloze-style formats are commonly used in literacy tests that ask for appropriate words to fill in blanks (Chiang et al., 2022) or in quizzes assessing science knowledge (Ren and Q. Zhu, 2021). Wang et al. (2023) proposed a pseudo Kullback-Leibler Divergence method to regulate distractor generation by considering item discrimination factors. Yu et al. (2024) used a knowledge graph to generate distractors by retrieving relevant triplets and selecting those most aligned with the QA context.\nOur framework is not limited to cloze-style questions, which are relatively infrequent in our dataset, and supports a broader range of question types."}, {"title": "Math", "content": "Scarlatos et al. (2024) improved the process of generating distractors for math problems by dividing it into two main steps: overgenerate and rank. In the overgenerate phase, they used a large language model (LLM) to generate n distractors, and in the rank phase, a ranker was employed to filter out the top-k distractors most likely to be chosen by students. Feng et al. (2024) explored a kNN-based approach to retrieve in-context examples similar to the target question and used them to generate distractors. Fernandez et al. (2024) proposed the DiVERT, which generates distractors based on learned error representations in math MCQs.\nThe methods by Scarlatos et al. (2024) and Feng et al. (2024) are used as baselines for comparison with our model. We cannot compare with Fernandez et al. (2024) since their method requires error explanations for each distractor."}, {"title": "Other Domains", "content": "Luo et al. (2024) proposed Chain-of-Exemplar Reasoning, a method to sequentially generate distractors for multimodal questions"}, {"title": "3 Methods", "content": "In this study, we propose a training pipeline to build a model capable of automatically generating more plausible distractors (as shown in Figure 2). Below, we first describe the base MCQ dataset used for training (\u00a73.1), then introduce the modeling methods for the pairwise ranker (\u00a73.2), student choice dataset (\u00a73.3), and distractor generator (\u00a73.4)."}, {"title": "3.1 Base MCQ Dataset", "content": "To train both the pairwise ranker and the distractor generator, we use an MCQ dataset created by educators on a nationwide online learning platform in South Korea. The MCQs in this dataset have been provided to K12 institutions, large corporations, and government agencies, and contain a variety of CS-related questions and student answers. We retained only those related to Python, DB (SQL), and Machine Learning & Deep Learning (MLDL). We target two categories of MCQs-coding and statement (see Figure 1). The statistics of this dataset are described in Table 1.\nA key feature of this dataset is that it includes information on how many students answered each question and the selection rate for each distractor. This allows us to determine which distractors were more confusing and plausible to students. Since each question was solved by hundreds of students from diverse sectors, the selection rate information is considered reliable. This information will play a key role in training the pairwise ranker and distractor generator, as discussed later. We will release a subset of this dataset\u201452 questions with no licensing issues\u2014to the community."}, {"title": "3.2 Pairwise Ranker", "content": "The pairwise ranker (MRank) is designed to take a question (Q), its correct answer (A), and two distractors (DA, DB) as input (Figure 2, Step 1), and determine which distractor is more likely to be selected by students.\n$M_{Rank} (Q, A, D_A, D_B) \\rightarrow \\{R, C_{A or B}\\}$   (1)\nThe model outputs two main components:\n(1) Reasoning (R) To enhance the interpretability and accuracy of ranking results, we utilize the reasoning abilities of LLMs through a structured prompt. Specifically, we instruct the model to generate reasoning about (1) the knowledge being tested (e.g., \"When students approach this problem, they first need to understand ...\") based on the question and the correct answer, and (2) why each of the two given distractors might appear plausible to students (e.g., \u201cDistractor A might confuse students who misunderstand the syntax ...\").\n(2) Choice (CA or B) The model outputs the result of the reasoning process as a single token (either A or B), indicating which distractor is more likely to be selected by students.\nTo train a relatively small LM to perform as a ranker, we prepare some training data of reasoning for supervised fine-tuning (SFT). Specifically, for each question in the training set of the base MCQ dataset, we prompt GPT-4o with a distractor pair and the indicator of which one was more frequently selected by students, and instruct it to generate reasoning about the two distractors that concludes in favor of the more frequently chosen one. This reasoning (R) and the more plausible distractor (CA or B) form the training data for small LMs.\nHowever, the SFT model exhibited suboptimal accuracy and became more erroneous as the reasoning grew longer. To address this, we use DPO to further train the model's reasoning process. After"}, {"title": "3.3 Student Choice Dataset", "content": "The student choice dataset is created to build training data for the distractor generator (Figure 2, Step 2). For each question in the base MCQ dataset, GPT-40 is used to generate three new distractors distinct from the human-authored ones (Appendix D). These new distractors, along with the original ones, are scored using the pairwise ranker. At this stage, the relative rankings of the original distractors are preserved, while rankings between the original and new distractors, as well as among the new distractors, are determined by our pairwise ranker. Each question ultimately has approximately six distractors ranked in plausible order. This dataset serves for training the distractor generator for both SFT and DPO (\u00a73.4)."}, {"title": "3.4 Distractor Generator", "content": "The distractor generator (MGen) takes as input a question (Q), its correct answer (A), and a hyperparameter n, which specifies the number of distractors to generate (Figure 2, Step 3). The model first determines the type (T) of distractor (e.g., Correct/Incorrect knowledge) it will generate, and then outputs n distractors (Di).\n$M_{Gen} (Q, A, n) \\rightarrow \\{T, D_1 ... D_n\\}$   (2)\nWe ensure that the model produces distractors that are both valid and plausible as follows."}, {"title": "4 Experiment Settings", "content": "In this section, we describe the model training setup (\u00a74.1) and introduce the metrics used to evaluate each model (\u00a74.2 and \u00a74.3)."}, {"title": "4.1 Model Training", "content": "For all experiments, both the pairwise ranker and the distractor generator are fine-tuned by applying LORA (Hu et al., 2021) to the Mistral-7B-Instruct-v0.2. The numbers of training and test data are described in Table 1 and Table 2. The detailed settings for SFT and DPO are provided in Appendix A.2 and B.2."}, {"title": "4.2 Pairwise Ranker", "content": "To assess the performance of the proposed pairwise ranker, we compare it against the following baseline models (the prompts for each baseline are included in Appendix A.1):\nWe instruct these GPT models to predict the ranking between two distractros in a zero-shot manner. To examine the impact of different prompt formats, we experiment with four approaches: (1) Reasoning: the reasoning-based prompt format described in \u00a73.2, (2) Rubric: scoring based on evaluation criteria for assessing plausibility, (3) G-Eval: adapting the prompt proposed by Liu et al. (2023) for our specific task, and (4) Discussion: simulating a collaborative learning scenario where two teacher agents discuss while observing students' problem-solving processes.\nWe follow the pairwise ranker prompt and training/inference method proposed in this paper, replacing their data with ours.\nWe use two distinct settings for training data (Table 1):\nModels trained separately with data for each subject-Python, DB, and MLDL.\nA model trained with data from all subjects combined.\nOne known limitation of LLM-based pairwise ranking is positional bias, where the output may vary depending on whether two choices, A and B, are presented in the input prompt as AB or BA (Yoon et al., 2024). To address this, we set the temperature to 0.5 and repeat the reasoning process with both AB and BA input sequences until consistent outputs are achieved, or randomly select a result after 10 attempts.\nThe evaluation metrics for the pairwise ranker are as follows:\nmeasures how often the ranker correctly identifies the distractor with the higher student selection rate in the test set.\n aims to compare the model's performance with human experts. First, two professors in data science perform the pairwise ranking task on 60 test samples (20 per subject), and their results are compared with our model's rank accuracy. Second, three Master's"}, {"title": "4.3 Distractor Generator", "content": "The performance of our distractor generator is evaluated using the following metrics:\nWe compare the plausibility of distractors generated by our model, GPT models, the KNN approach proposed by Feng et al. (2024), and human experts (from the base MCQ dataset) as measured by our pairwise ranker (\u2018DPO, Comb.' in Table 3). Win/tie/lose counts are calculated per question/distractor in two settings:\nFor each test question, three distractors are generated by each model (n = 3), and only valid ones are retained. These are then compared pairwise between two models, with one point awarded to the winner. Identical distractors are excluded from comparisons.\nTo account for cases where models generate fewer than three valid distractors, each model's temperature is increased to generate up to five valid distractors per model. After excluding identical distractors between the models, the top-3 are selected for pairwise comparison.\nWe conduct a human evaluation where actual students assess the difficulty of distractors generated by our method. The test comprises 40 MCQs (Python: 20, DB: 10, MLDL: 10). Each question was sampled from the test set of the base MCQ dataset and paired with four distractors, one from each model (SFT, DPO, GPT-3.5-turbo, and GPT-40), along with a 'None of the above' option. The test is taken by 15 college students enrolled in AI courses at our university. Based on the selection counts for each distractor, we calculate the plausibility and discrimination index for each model. The discrimination"}, {"title": "5 Experiment Results", "content": "In this section, we present the experimental results for the pairwise ranker (\u00a75.1) and the distractor generator (\u00a75.2)."}, {"title": "5.1 Pairwise Ranker", "content": "As shown in Table 3, in terms of accuracy, our DPO model achieved an accuracy of 67.5% (row 9), outperforming GPT-3.5-turbo (58.7%, row 1) and GPT-40 (64.0%, row 2) on average. This result is somewhat surprising because our model was trained on reasoning generated by GPT-40. Moreover, the DPO model significantly outperformed the SFT models (58.7%-65.7%, rows 7\u20138), particularly in Python, showing the effectiveness of DPO in enhancing the reasoning capability of the model. While Scarlatos et al. (2024)'s method achieved strong performance on math questions in their original work, it exhibited lower accuracy on the CS subjects (48.8%, row 6).\nHuman experts (two professors) tasked with choosing the more plausible distractor for 60 questions achieved an accuracy of 71.7%, compared to 70% achieved by our DPO"}, {"title": "5.2 Distractor Generator", "content": "Table 4 summarizes the win/lose counts of our distractor generators against GPT models, Feng et al. (2024), and human-authored distractors, as evaluated by our pairwise ranker (DPO-based). Our DPO model generated more plausible distractors than baseline models in most\nTable 5 compares the frequency of distractors selected by students, showing that our DPO model generated more plausible distractors than GPT-40 across all subjects and outperformed GPT-3.5-turbo in all but one subject. To evaluate whether the distractors have differing impacts based on students' proficiency levels, we divided the students into two groups-Top 50% and Low 50%-based on their average scores. The distractors generated by the DPO model were most frequently chosen by both groups. These findings suggest that our model may effectively identify ar-"}, {"title": "6 Conclusion", "content": "In this study, we proposed a pipeline for training a model to generate more plausible distractors for MCQs and demonstrated its effectiveness across computer science subjects. We trained the pairwise ranker to evaluate the relative plausibility of distractors, and used this to create the student choice dataset where distractors for each question are ranked by plausibility. From this dataset, we created chosen-rejected pairs of distractors to train the distractor generator using DPO. Our models outperformed GPT and other baseline models and performed comparably to humans in various metrics, including pairwise rank accuracy and distractor plausibility. We believe that our work can advance automated educational tools, contributing to"}, {"title": "Limitations", "content": "The models presented in this study have the following limitations. First, the pairwise ranker's method of comparing distractors pairwise significantly increases the number of combinations and requires substantial computing resources due to the need for generating reasoning. A listwise approach using an encoder-decoder structure could be explored as a solution (Yoon et al., 2024).\nSecond, the distractor generator occasionally produces invalid distractors, necessitating review by human experts or high-performing LLMs (e.g., GPT-40) to accurately evaluate students' knowledge. To address this limitation, future work could include an additional supervision phase, such as integrating feedback loops with other models or applying constraints like Counterfactual Contrastive Decoding (Qu et al., 2024).\nFinally, our method focuses on generating difficult distractors, but there are instances where adjusting the difficulty level of MCQs to suit the needs of the target students is necessary. While our pairwise ranker can be utilized to select distractors with varying degrees of plausibility, future work could explore more direct approaches, such as incorporating student knowledge tracing or adaptive decoding, to address this challenge (Cui and Sachan, 2023)."}, {"title": "A Pairwise Ranker", "content": "The instruction prompts of pariwise ranker are in Table 13 (Reasoning), 14 (Rubric), 15 (G-Eval), 16 (Discussion) and 17 (Scarlatos et al., 2024). We used the same prompt (Reasoning) with GPT models and ours (SFT, DPO)."}, {"title": "A.1 Prompt of Pairwise Ranker", "content": ""}, {"title": "A.2 Pairwise Ranker SFT and DPO Settings", "content": "The pairwise ranker model was trained using Mistral-7B-Instruct-v0.2 with 4-bit quantization and fine-tuned using LoRA. For SFT, the learning rate was set to 2e-4 and the model was trained for 5 epochs. For DPO, the learning rate was set to 1e-6, also trained for 5 epochs. These hyperparameters were selected as they allowed stable training without overfitting while preserving the quality of the DPO output. SFT took approximately 2 hours, and DPO took about 1 hour on an NVIDIA A6000 GPU. Scarlatos et al. (2024) model was reproduced for baseline comparison using the same model and DPO settings as above."}, {"title": "A.3 GPT Prompt for Making Pairwise Ranker Training Data", "content": "The instruction prompt for making pairwise ranker training data is in Table 18. To enhance the diversity of expressions and reasoning used in the samples, two reasoning examples are generated for each pair\u2014one with temperature set to 0 and the other to 1."}, {"title": "A.4 Experiment - Consistency in Rank Prediction", "content": "Table 6 demonstrates that our pairwise ranker exhibits relatively robust to positional bias. In comparison to GPT-3.5-turbo, which required an average of more than two attempts to produce consistent results when the input order was altered, our model (DPO) was able to achieve consistent results with significantly fewer attempts. Additionally, our model (DPO) slightly outperformed GPT-40 by requiring fewer average generation attempts."}, {"title": "A.5 Plausibility Factors", "content": "We used GPT-40 to summarize and categorize reasoning samples where our pairwise ranker accu-"}, {"title": "A.6 Human Evaluation", "content": "We conducted a survey with three Master's degree students who voluntarily expressed their willingness to participate in this experiment. The survey was designed to begin only after they agreed to provide their results for research purposes and acknowledged the precautions via an online form. The experiment lasted approximately 90 minutes, and participants were compensated above the standard hourly wage for the time they participated. The entire process of human evaluation was conducted following procedures approved by the IRB committee of our university.\nThe reasoning quality of our pairwise ranker was evaluated on a 5-point Likert scale based on the following criteria:\nWhether the reasoning process is logical.\nWhether the reasoning effectively understands students' misconceptions or problem-solving processes.\nWhether the reasoning is based on accurate and error-free knowledge.\nWhether the evaluator agrees with the model's final choice."}, {"title": "A.7 Ablation Study", "content": "The instruction prompt used for the ablation study (w/o Reasoning) is in Table 22, and the training"}, {"title": "A.8 Error Analysis", "content": "Our pairwise ranker exhibited the following three types of errors:\nFirst, our model tended to incorrectly judge implausible mistakes as plausible errors that real students would not typically make. For example, in the process of calculating the output of Python code, the model incorrectly deemed \u2018unrealistic reasoning' or 'mistakes in obvious calculations' as plausible, even though such errors would be unlikely for actual students to make based on common sense.\nSecond, our model struggled with reasoning when encountering unfamiliar questions that were insufficiently represented in the training data. This issue was particularly evident in subjects like DB and MLDL, where the training set was relatively small and shared few similar concepts or questions with the test set.\nLastly, in questions requiring the selection of an incorrect option, there were cases where our model's final ranking was correct, but its reasoning was flawed. Instead of identifying why each option seemed more incorrect to the students, the model mistakenly focused on determining which option was more correct.\nTo improve the pairwise ranker, future work should focus on enabling the model to learn common student misconceptions for better reasoning and prediction and enhancing the inference process to clearly recognize question requirements."}, {"title": "B Distractor Generator", "content": "The instruction prompt of our distractor generator is in Table 23. We used the same prompt with GPT models and ours. But we instructed to generate in json format for GPT models for stability issue.\nThe instruction prompt for the kNN approach proposed by Feng et al. (2024) is presented in Table 24. Following the method outlined in the paper, the target question and answer were encoded using the SBERT encoder (Reimers and Gurevych, 2019), MPNet, and the top-3 most similar items based on cosine similarity were extracted from the"}, {"title": "B.1 Prompt for Distractor Generator", "content": ""}, {"title": "B.2 Distractor Generator SFT and DPO Settings", "content": "The distractor generator model was trained using Mistral-7B-Instruct-v0.2 with 4-bit quantization and fine-tuned using LoRA. For SFT, the learning rate was set to 2e-4 and the model was trained for 2 epochs. For DPO, the learning rate was set to 1e-5, trained for 3 epochs. These hyperparameters were determined as a result of finding a setup that avoids overfitting while ensuring no issues with the quality of the DPO output. SFT and DPO took approximately 3 hours on an NVIDIA A6000 GPU.\nAs briefly mentioned in \u00a73.4, in addition to the chosen-rejected sample pairing method described in the main text, another setting employs a method similar to a sliding window for pairing. In this setting, all distractor candidates are sorted in descending order and grouped into non-overlapping windows of size n. For example, if there are six candidates and n is 2, a total of three windows are created. Pairwise combinations between these windows are then used to create chosen-rejected samples. A model trained with DPO using these samples showed no significant performance difference compared to the model described in the main text. The plausibility evaluation results for this model are provided in Table 7."}, {"title": "B.3 Experiment - Plausibility", "content": "The results of the plausibility evaluation analyzed from a per-question perspective are presented in Table 8 (compare with Table 4).\nThe analysis of plausibility results based on question types (Code/Statement) is provided in Table 9. A summary of the case study"}, {"title": "B.4 Human Evaluation", "content": "We conducted the evaluation with 15 college students who voluntarily agreed to participate. The test was conducted online, and participants were allowed to begin the test only after agreeing to the instruction stating that their results would be provided for research purposes. The experiment took approximately 60 minutes, and participants were compensated with a reward above"}, {"title": "B.5 Additional Evaluation - Text Similarity", "content": "Table 10 presents the text similarity evaluation results for the distractor generator. sBLEU and BERTScore were used as the text similarity metrics. For sBLEU, the 'smooth_method' was set to 'exp', and the default parameters were used for BERTScore. In terms of sBLEU, our model (DPO) generates distractors that are most similar to human-authored ones across the majority of subjects."}, {"title": "B.6 Additional Evaluation - Validity", "content": "Validity refers to whether the distractors are indeed incorrect options for the question. Table 11 shows the proportion of valid distractors generated by each model according to the type of question. Our models demonstrate stable validity across various question types (e.g., Correct/Incorrect, Code/Statement), significantly outperforming GPT-3.5-turbo and pre-trained Mistral. This highlights the importance of the proposed methodology-first generating the type (T) such as \u2018Correct/Incorrect knowledge'-in enhancing validity."}, {"title": "B.7 Ablation Study", "content": "The training settings used for the ablation study are identical to those of our distractor generator training setup (Appendix B.2), except that the base MCQ dataset was used as the training data instead of the student choice dataset. Table 12 presents the results of the ablation study (compare with Table 4 and 8)."}, {"title": "B.8 Error Analysis", "content": "Analyzing the low-quality samples generated by our distractor generator revealed the following types of errors:\nFirst, the model sometimes failed to produce the specified number of distractors based on the input parameter n, or it created duplicate distractors among the outputs.\nNext, for code type questions, the generated distractors lacked diversity in output formats and often made minimal changes, such as altering only one or two variables, resulting in repetitive and insufficiently varied distractors.\nMeanwhile, for statement type questions, the model overly mimicked the correct answer, creating distractors based on only one or two concepts, while failing to effectively incorporate other related concepts.\nFuture work to improve the distractor generator could involve explicitly providing the model with information on similar concepts or common errors that students are likely to confuse."}, {"title": "B.9 GPT Prompt for Distractor Validity Check", "content": "The instruction prompt for checking validity of distractors is in Table 25. If the output is 'invalid' (as it is an incorrect option for the question), it is considered a distractor."}, {"title": "C Base MCQ Dataset", "content": "We were provided with an MCQ dataset by an online learning platform for educational research purposes and processed it for use within the scope of the provided purpose. The questions and options, originally in Korean, were translated into English for experimental purposes. The provided MCQ data does not contain any personally identifiable information about the individuals who answered the questions, and we manually checked to confirm that the text does not include any offensive content."}, {"title": "D GPT Prompt for Augmenting Distractors in Base MCQ Dataset", "content": "The instruction prompt for augmenting distractors in base MCQ dataset is in Table 26. Through this prompt, the student choice dataset was constructed only when at least one newly generated distractor by GPT-40 was valid and did not overlap with the original."}, {"title": "E Potential Issues", "content": "MCQs serve as a tool for assessing students' knowledge, so the options must be based on accurate information (i.e., both the correct answer and distractors must be valid). As mentioned earlier in the limitations, distractors generated by the model may not be actual incorrect options to the question. To proactively address the potential issue, we explored methodologies to ensure the validity of the distractors generated by the model. As part of these efforts, we implemented instruction prompts and output formats for the model to classify the type (T) of distractors, thereby mitigating this issue.\nWe used selection rate data from questions answered by hundreds of students to ensure the reliability of common misconception information for training the pairwise ranker. However, since misconceptions can vary by learning level or educational environment, the model's reasoning may not generalize to other populations. To make accurate predictions for a target population, selection rates specific to that group should be used."}]}