{"title": "Milmer: a Framework for Multiple Instance Learning based Multimodal Emotion Recognition", "authors": ["Zaitian Wang", "Jian He", "Yu Liang", "Xiyuan Hu", "Tianhao Peng", "Kaixin Wang", "Jiakai Wang", "Chenlong Zhang", "Weili Zhang", "Shuang Niu", "Xiaoyang Xie"], "abstract": "Emotions play a crucial role in human behavior and decision-making, making emotion recognition a key area of interest in human-computer interaction (HCI). This study addresses the challenges of emotion recognition by integrating facial expression analysis with electroencephalogram (EEG) signals, introducing a novel multimodal framework-Milmer. The proposed framework employs a transformer-based fusion approach to effectively integrate visual and physiological modalities. It consists of an EEG preprocessing module, a facial feature extraction and balancing module, and a cross-modal fusion module. To enhance visual feature extraction, we fine-tune a pre-trained Swin Transformer on emotion-related datasets. Additionally, a cross-attention mechanism is introduced to balance token representation across modalities, ensuring effective feature integration. A key innovation of this work is the adoption of a multiple instance learning (MIL) approach, which extracts meaningful information from multiple facial expression images over time, capturing critical temporal dynamics often overlooked in previous studies. Extensive experiments conducted on the DEAP dataset demonstrate the superiority of the proposed framework, achieving a classification accuracy of 96.72% in the four-class emotion recognition task. Ablation studies further validate the contributions of each module, highlighting the significance of advanced feature extraction and fusion strategies", "sections": [{"title": "1. Introduction", "content": "Emotions, as an integral part of human daily life, significantly influence individual behavior, decision making, social interactions, and mental health. In recent years, with the rapid advancement of human-computer interaction (HCI) technologies, emotion recognition has emerged as a highly prominent research area. Extensive studies have explored this topic through various data modalities. The development of deep learning methods, coupled with the utilization of large-scale multimodal datasets such as DEAP [1] have revolutionized the field of emotion recognition. These advancements have paved the way for detecting and interpreting human emotions with unprecedented accuracy.\nEmotion recognition relies broadly on two primary modalities of human data: visual data such as images or videos, and physiological signals such as electroencephalogram (EEG). Research in this field has first focused on vision-based approaches, using facial images and videos, as evident in prior studies [2, 3, 4, 5]. Facial expressions, as direct and easily captured indicators of human emotions, have been extensively studied. However, facial expressions in datasets commonly used in facial emotion recognition often exhibit exaggerated or highly recognizable features. These expressions, while easily identifiable, are not representative of real-life emotional expressions, where emotions are not always expressed in an exaggerated or obvious way. In fact, people often display little to no facial expression in response to their emotions. This discrepancy poses a significant challenge in accurately recognizing emotions in real-world scenarios and highlights a major weakness in the field of facial emotion recognition.\nIn contrast, physiological signals such as EEG offer distinctive insights into human emotions by reflecting intrinsic responses that occur instinctively and are not subject to conscious control, providing a more accurate representation of genuine emotions [6, 7, 8]. Despite their potential, the adoption of deep learning strategies for physiological signals remains limited, hindered"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. EEG-Based Emotion Recognition", "content": "Electroencephalogram (EEG)-based emotion recognition utilizes brain activity signals to classify emotional states, offering an intrinsic and unconscious reflection of emotional responses. Deep learning models such as convolutional neural networks (CNNs) [15] and recurrent neural networks (RNNs) [16] have been extensively applied for feature extraction and classification. More recently, transformer-based approaches [17, 6] have demonstrated strong potential in modeling temporal dynamics and long-range dependencies in EEG signals. Advanced frameworks such as graph neural networks (GNNs)[18] have also been proposed to capture the spatial interrelations among EEG channels effectively.\nHowever, EEG recording is associated with several challenges. Key obstacles include the presence of internal and external artifacts, such as eye movements and muscle activity, which can interfere with signal accuracy. Furthermore, recording EEG data for extended periods can introduce noise and variability, which complicates the extraction of consistent emotional features [19]. These factors limit the effectiveness of EEG as a standalone modality for emotion recognition, highlighting the need for complementary approaches."}, {"title": "2.2. Facial Emotion Recognition", "content": "Facial emotion recognition (FER) involves analyzing human facial expressions from images or videos to classify emotions. Deep learning methods, particularly convolutional neural networks (CNNs), have significantly"}, {"title": "2.3. Multimodal Emotion Recognition", "content": "Multimodal emotion recognition, which integrates facial expressions and EEG signals, has gained significant attention due to its ability to provide more accurate emotion classification by combining complementary information. However, research in this area still lacks sufficient exploration of effective fusion techniques, which limits the potential of multimodal models.\nSeveral approaches have been proposed to fuse EEG and facial expression data. Early methods often relied on simple fusion strategies such as decision trees or voting mechanisms to combine the outputs of each modality [26, 10]. While these approaches are straightforward, they fail to effectively utilize the rich, modality-specific features extracted from each source, often resulting in suboptimal performance. Methods like Deep Canonical Correlation Analysis (DeepCCA), have attempted to improve feature correlation between modalities, yet they still fall short of fully exploiting the complex, individual features of each modality [27]. Low-rank fusion techniques have also been explored, offering a more efficient way to combine modality-specific features [28]. While these methods are computationally efficient, they are not advanced enough to capture the complex interactions between the two modalities.\nConcatenation-based methods, which combine the features of both modalities into a single vector, are also widely used but can lead to information loss"}, {"title": "2.4. Multiple Instance Learning", "content": "Multiple Instance Learning (MIL) is a variation of supervised learning in which a class label is assigned to a bag of instances rather than individual instances. Unlike traditional supervised learning, where each input (such as an image) is labeled with a specific category, MIL is applied in scenarios where the labeling is more ambiguous. In MIL, a \"bag\u201d contains multiple instances, and only the overall class of the bag is provided, not individual labels for each instance. This approach is particularly useful when data annotations are weak or incomplete, a common occurrence in real-world tasks.\nTraditional MIL typically relies on pooling methods like max pooling or average pooling to combine the individual instances in a bag and make a prediction. However, these pooling techniques often fail to capture the importance of specific instances, as they treat all instances equally. In contrast, attention-based MIL (AMIL) [30]introduces an attention mechanism that assigns different weights to instances, allowing the model to focus more on the most relevant instances while ignoring less informative ones. This approach significantly improves MIL performance by enabling more precise feature extraction and decision-making.\nIn the field of multimodal emotion recognition using EEG and facial expressions, the typical approach involves associating a short segment of EEG data (e.g., 3 seconds) with a single facial expression image, which is then used to represent the entire 3-second window. However, this approach is inherently flawed as it may overlook valuable temporal and subtle details present within the sequence of images. A 3-second video, for instance, should be understood as a \"bag\" of instances, with each individual frame or image representing a potential instance. Relying on just one image to represent the entire bag could result in the loss of critical information, especially in capturing emotional variations that occur over time.\nBy adopting MIL, we can leverage all instances within the video to create a more balanced and accurately represent the entire segment. MIL allows for"}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Overview", "content": "As discussed earlier, many existing methods overlook the importance of effective modality fusion, relying on simple concatenation or decision-tree-based approaches that fail to fully exploit the complementary nature of different modalities. Transformer[33], an emerging neural network architecture initially developed for machine translation tasks, has recently achieved remarkable success in the field of natural language processing (NLP). However, its potential application to emotion recognition tasks involving EEG and facial modalities remains underexplored.\nIn this work, we propose a transformer-based framework to achieve effective cross-modal fusion. As illustrated in Fig. 1, the proposed architecture consists of four main modules: (1) EEG preprocessing module, (2) facial feature extraction and balancing module, and (3) modality fusion module.\nIn the first module, noise and artifacts in the EEG signals are filtered out to ensure clean input data. In the second module, Swin Transformer[14] is used to extract features from facial images. A MIL approach is then introduced to better represent the facial expressions by selecting the most representative frames. To further reduce the feature dimension and highlight key information, a cross-attention mechanism[33, 34] is applied to the Swin Transformer output. Then, both the EEG and facial features are embedded and fed into a transformer for cross-modal fusion. Finally, the fused features, which encapsulate frequency, temporal, and spatial information, are passed through a fully connected classifier to produce the final emotion classification results. The details of these three modules are explained in the subsequent sections."}, {"title": "3.2. EEG Preprocessing Module", "content": "The majority of EEG signals are concentrated within the frequency range of 1 Hz-50 Hz. Therefore, a bandpass filter with a passband of 1 Hz-50 Hz is applied in the proposed model. This filtering step serves two primary purposes: (1) removing low-frequency baseline drift, electrocardiographic (ECG) interference, and other high-frequency noise, and (2) effectively mitigating the most prominent power line interference, which typically occurs at 50 Hz in China.\nHowever, EEG signals often overlap with electrooculogram (EOG) and electromyogram (EMG) signals within the same frequency band, making a"}, {"title": "3.3. Facial Feature Extraction and Balancing Module", "content": "This study significantly advances the visual feature extraction process compared to prior research in the field. First, in contrast to traditional CNN-based methods prevalent in the field, we utilize the advanced Swin Transformer fine-tuned on emotion classification dataset as the backbone for feature extraction.\nPrevious studies typically align a single facial image with a segment of EEG data (commonly 3 seconds), assuming that a single frame can represent the entire video segment. However, it is difficult to determine which frame best encapsulates the information from a 3-second video. To address this limitation, we introduce MIL into this domain. By extracting multiple frames from the video segment and leveraging MIL, the most representative K frames are selected. This approach ensures a more comprehensive and nuanced representation of the facial modality, capturing richer emotional information.\nLastly, this study addresses the critical challenge of balancing the contributions of the modality during the fusion stage. Selecting multiple frames through MIL inevitably introduces far more visual tokens compared to the EEG modality, which could lead to the model disproportionately focusing on facial features while neglecting EEG information. To mitigate this imbalance, we employ a cross-attention-based mechanism, enabling the K frames to interact and learn from each other while reducing their dimensionality"}, {"title": "3.3.1. Swin Feature Extraction", "content": "Compared to traditional CNN-based methods commonly used in this field, the Swin Transformer demonstrates superior capabilities in visual feature extraction. By employing a hierarchical architecture with shifted window attention mechanisms, the Swin Transformer captures both local and global dependencies efficiently. This design allows it to retain the fine-grained details of local features while incorporating contextual information across larger regions, surpassing CNNs that rely on fixed local receptive fields and hierarchical feature aggregation. This enables the Swin Transformer to extract richer and more robust features, providing a solid foundation for subsequent multimodal fusion tasks.\nIn this module, we use a 6-layer Swin Transformer as the image encoder, initialized with weights fine-tuned on emotion recognition dataset. The Swin Transformer serves as the backbone for extracting visual features, generating M feature tokens, each with dimension D, from the input facial images. These feature tokens retain hierarchical and contextual information, making them highly suitable for subsequent processing in the fusion stage."}, {"title": "3.3.2. Multiple Instance Learning", "content": "In the classical supervised 2-class classification problem, the objective is to develop a model that predicts a target variable $y \\in 0, 1$ for a given instance, $x \\in R^D$. However, in the context of the MIL problem, the model is presented with a bag of instances $X = \\{X_1,X_2, ..., X_q\\}$, where the individual instances are independent of each other. The bag is associated with a single label Y belonging to one of the two classes, i.e., $y \\in 0,1$. Furthermore, although each instance within the bag is assumed to have its own label $Y_1, Y_2, . . ., Y_q$, these labels are partially indicative of the bag's overall label Y but cannot be equated with it.\nIn MIL research, there is often a focus on understanding the relationships between instances within a bag and selecting a method that represents the bag in a more balanced or universal manner. The most common approach for this is pooling, such as average pooling or max pooling. With the advent of attention mechanisms, attention-based learnable pooling methods have emerged, one of the most notable being AMIL[30]. AMIL presents a simple yet effective approach that significantly enhances accuracy. It proposes an"}, {"title": "3.3.3. Cross Attention", "content": "In multi-modal fusion, the number of tokens provided by each modality plays a critical role in determining the quality of the fusion process. If the facial modality provides an excessively large number of tokens compared to the EEG modality, the transformer used for fusion may focus disproportionately on the visual features while neglecting those from the EEG signals. This imbalance can hinder the overall classification performance.\nTo address this, the cross-attention mechanism is employed to adjust the dimensionality of the feature vectors output by Swin. Specifically, the cross-attention mechanism compresses the M visual tokens into N learnable query tokens, where N \u00ab M. This adjustment enables the visual tokens to match the token count of the EEG modality, achieving a more balanced and"}, {"title": "3.4. Fusion Module", "content": "In the proposed framework, the Fusion Module is designed to effectively combine information from EEG signals and facial features while preserving modality-specific characteristics. To achieve this, we use modal type embedding, position embedding, and a classification token (CLS token).\nModal Type Embedding: To differentiate between the two modalities, we introduce modal type embeddings. These are learnable vectors unique to each modality, appended to their respective input features. Modal-type embeddings provide the model with explicit information about the origin of each input token, enabling the Transformer to process and fuse modality-specific features more effectively. For instance, all EEG tokens are associated with a dedicated embedding vector, while facial tokens are assigned a distinct embedding, ensuring clear separation in the shared representation space.\nPosition Embedding: Position embeddings are applied to retain the order and structure of the input tokens for both EEG and facial features. EEG position embeddings encode temporal dependencies by representing the time-step information of each token, while facial position embeddings capture the spatial relationships among visual tokens extracted by Swin, preserving global contextual information essential for emotion recognition.\nCLS Token for Classification: A CLS token is appended to the input sequence, serving as a global representation of the fused features across modalities. During the Transformer encoding process, the CLS token aggregates information from both EEG and facial feature tokens through attention mechanisms. In the output stage, the CLS token is passed to a fully connected layer for emotion classification. This approach allows the model to condense multimodal information into a compact and informative representation, optimizing the subsequent classification task."}, {"title": "4. Experiments and Result", "content": "In this section, we present the experimental results. First, we provide an overview of the DEAP dataset and describe the preprocessing methods applied to the data. Subsequently, we conduct extensive experiments on the DEAP dataset to evaluate and compare the classification performance of various multimodal fusion strategies. Furthermore, we investigate the impact of the token quantities from the two modalities on classification performance during multimodal fusion. Finally, we perform ablation studies to demonstrate the significance of each component in our proposed network architecture.\nThanks to the fast convergence of Milmer, the experimental trends and results become apparent within just 100 epochs. Loshchilov et al.[36] introduced the cosine learning rate decay strategy, which has been widely validated for its effectiveness. All experiments in this section are conducted using the cosine learning rate decay strategy, with results reported over 100 epochs."}, {"title": "4.1. Dataset and Settings", "content": "We employ the widely recognized DEAP dataset, which is commonly used in the field of multimodal affective computing, to evaluate the effectiveness of our proposed architecture. In the DEAP dataset, participants' emotions are elicited by watching music videos in a controlled laboratory environment, while their facial videos and bio-sensing signals are recorded synchronously. Following the viewing session, participants rate their emotional responses based on four dimensions: valence, arousal, dominance, and liking. Most studies in this area primarily focus on valence and arousal for emotion evaluation, and our study adheres to this convention."}, {"title": "4.2. Comparison Experiments", "content": "The classification results of our proposed method, along with other studies that adopt the four-class approach, are presented in Table 1. The results of studies employing the three-class approach are summarized in Table 2, while the binary classification results for valence and arousal are shown in Table 3."}, {"title": "4.3. Visual Feature Integration Analysis", "content": "Table 4 presents a comparison of three strategies for integrating visual features extracted from multiple facial images before fusing them with EEG"}, {"title": "4.4. Ablation Study", "content": "To validate the contributions of the three critical components in our framework, the EEG module, the facial feature extraction and balancing module, and the modality fusion module, we conducted ablation studies, as shown in Table 5. We assessed the classification performance of each modality independently and compared our transformer-based fusion approach with two commonly used fusion methods in the field.\nThe results presented in Table 5 indicate that when only the EEG modality is used, the framework achieves an accuracy of 62.33% and an F1 score of 60.93%, while the facial modality alone yields slightly better performance, with an accuracy of 83.59% and an F1 score of 83.67%. The relatively lower performance of the EEG-only model can be attributed to the simplified EEG processing adopted in our study, as our primary focus lies in cross-modal fusion rather than sophisticated EEG signal processing.\nWhen both modalities are incorporated, performance improves significantly, with accuracy exceeding 88% across different fusion methods. Specifically, the DeepCCA and Concatenation methods achieve accuracies of 88.13% and 89.88%, respectively, which are notably lower compared to the 96.72%"}, {"title": "5. Conclusion and Future work", "content": "This paper proposes a comprehensive method for assessing the credibility of Internet of Things devices. We evaluate multiple dimensions of device network behaviors and utilize a Transformer to capture the temporal variability of network features and to predict the features for the next time step. The predicted behavior information is then compared with the actual collected behavior information to calculate similarities and differences. These metrics are used to ascertain the credibility of the device. Finally, our method's reliability is validated through experimental testing, demonstrating its effectiveness in accurately assessing the trustworthiness of IoT devices.This study proposes a multimodal learning framework based on EEG and facial expressions, integrating feature extraction and deep learning for emotion recognition. For the EEG modality, widely adopted methods in the field were employed, while more advanced feature extraction techniques were introduced for facial expression processing. Additionally, a more sophisticated fusion module was utilized to explore effective strategies for integrating features from the two modalities. Extensive experiments conducted on the DEAP dataset demonstrate that the proposed framework surpasses state-of-the-art methods in terms of accuracy. Furthermore, ablation studies on individual modules validate the effectiveness of our approach.\nThis study addresses the challenges of emotion recognition by integrating facial expression analysis with EEG signals, introducing a novel frame-"}], "equations": ["z = {xq | q \u2208 T},", "T = {q | aq is among the top K largest values of a1, a2,..., aq}.", "aq =\nexp (w tanh(Vx))\n\u03a3=1 exp (w tanh(Vx))',", "Q = WQXq, K = WKX, V=WvX\u03c5", "A = softmax (\nQKT\n\u221adk\n)V", "Loss =\n1\nbatchsize\nbatchsize\nn\n\u03a3 \u03a3Yjilog(Yji),\nj=1\ni=1", "Emotion = (\n(HAHV, a\u2265 5, v \u2265 5\nHAlV, a \u2265 5, \u03c5 < 5\nLAHV, a < 5, v \u2265 5\nLALV, a < 5, v < 5"]}