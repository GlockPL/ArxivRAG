{"title": "Neural Probabilistic Circuits: Enabling Compositional and Interpretable Predictions through Logical Reasoning", "authors": ["Weixin Chen", "Simon Yu", "Huajie Shao", "Lui Sha", "Han Zhao"], "abstract": "End-to-end deep neural networks have achieved remarkable success across various domains but are often criticized for their lack of interpretability. While post hoc explanation methods attempt to address this issue, they often fail to accurately represent these black-box models, resulting in misleading or incomplete explanations. To overcome these challenges, we propose an inherently transparent model architecture called Neural Probabilistic Circuits (NPCs), which enable compositional and interpretable predictions through logical reasoning. In particular, an NPC consists of two modules: an attribute recognition model, which predicts probabilities for various attributes, and a task predictor built on a probabilistic circuit, which enables logical reasoning over recognized attributes to make class predictions. To train NPCs, we introduce a three-stage training algorithm comprising attribute recognition, circuit construction, and joint optimization. Moreover, we theoretically demonstrate that an NPC's error is upper-bounded by a linear combination of the errors from its modules. To further demonstrate the interpretability of NPC, we provide both the most probable explanations and the counterfactual explanations. Empirical results on four benchmark datasets show that NPCs strike a balance between interpretability and performance, achieving results competitive even with those of end-to-end black-box models while providing enhanced interpretability.", "sections": [{"title": "1 Introduction", "content": "End-to-end deep neural networks (DNNs) [Krizhevsky et al., 2012, He et al., 2016, Vaswani et al., 2017, Devlin et al., 2019] have demonstrated remarkable success across various domains [Hinton et al., 2012, Sutskever et al., 2014, Long et al., 2015]. However, many of them are black-box models containing complex operators, making it hard to interpret and understand how a decision was made. Although many efforts [Ribeiro et al., 2016, Lundberg and Lee, 2017, Selvaraju et al., 2017] have been made to explain a model's decision in a post hoc manner, Alvarez-Melis and Jaakkola [2018], Laugel et al. [2019], Slack et al. [2020], Rudin [2019] show that these explanations are oftentimes not reliable as the explanation model might loosely approximate the underlying model. For example, the explanation model exhibits similar performance to the black-box model but yet relies on entirely different features. Such discrepancy between the explanation model and the black-box model could lead to misleading explanations, e.g., attributing the decision to irrelevant features or missing out important features. Misleading explanations are particularly concerning in high-stake applications such as medical analysis [Hou et al., 2024, Liu et al., 2023] and legal justice [Richmond et al., 2024, Deeks, 2019]. Rather than introducing post hoc explanations to explain a black-box model, Rudin [2019] argues that one should create an interpretable model in the first place where each component is designed with a distinct purpose, facilitating an interpretable prediction.\nConcept bottleneck models (CBMs) [Koh et al., 2020] aim to enhance interpretability by introducing high-level, human-understandable concepts, such as \"red color\" and \"round shape\", as an intermediate bottleneck, which decomposes a model into two modules: a concept recognition model and a task predictor. The neural-network-based concept recognition model maps the input image to probabilities associated with various concepts. Using these probabilities, the task predictor, typically a linear predictor, produces the probabilities for the various classes. Since the final prediction (i.e., the class with the highest probability) can be interpreted in terms of these concepts, the model's decision-making process becomes more intuitive for humans to understand. To improve performance on downstream tasks, methods like CEM [Zarlenga et al., 2022], ProbCBM [Kim et al., 2023], and others [Yeh et al., 2020, Kazhdan et al., 2020] change the outputs of the concept recognition model from concept probabilities to concept embeddings. While boosting task performance, such approaches significantly compromise interpretability since the dimensions within concept embeddings lack semantic meanings. On the other hand, to further improve interpretability, some approaches [Barbiero et al., 2023, Rodr\u00edguez et al., 2024, Ciravegna et al., 2023] propose architectures for the task predictor that incorporate logical rules, allowing task predictions to be explicitly explained through these rules. However, these logical rules are usually learned from data rather than being predefined by humans, limiting our ability to integrate prior domain knowledge into the model. Additionally, there is currently no theoretical guarantee regarding the performance of the overall model, obscuring the relationship between overall performance and that of the concept recognition model or the task predictor.\nTo address these challenges, we propose a novel model architecture called Neural Probabilistic Circuits (NPCs), which enable compositional and interpretable predictions through logical reasoning. An NPC comprises two modules: an attribute recognition model and a task predictor. Unlike existing approaches that primarily focus on numerous binary concepts (e.g., \"red color\", \"yellow color\u201d), we introduce a higher-level categorical characteristic called attributes, which describe the types of concepts (e.g., \"color\"). This approach reduces the need for additional concept selection or pruning to improve model efficiency [Ciravegna et al., 2023, Barbiero et al., 2022, Zarlenga et al., 2023], while also achieving better performance in concept recognition. Given an input image, the neural-network-based attribute recognition model produces probability vectors for various attributes, with each vector representing the likelihood of various values for the corresponding attribute. These probability vectors serve as inputs to the task predictor, which is implemented using a probabilistic circuit. Probabilistic circuits [Poon and Domingos, 2011, Zhao et al., 2016b, 2015b, Choi et al., 2020], a type of probabilistic graphical models [Koller and Friedman, 2009], aim to learn the joint distribution over input variables, in our case, attribute variables and the class variable. During learning, probabilistic circuits embed within their structures and parameters either implicit logical rules learned from data or explicit logical rules predefined by humans. The circuits enable tractable probabilistic reasoning tasks such as joint, marginal, and conditional inferences, thereby revealing relations among the attributes and classes. By leveraging these relations, NPCs can reason over outputs from the attribute recognition model to infer the most probable class. Specifically, the prediction score for a given class is the sum of the likelihood of each combination of attribute values weighted by their relevance to the class. As usual, the final prediction corresponds to the class with the highest score.\nGiven the compositional nature of NPCs, we propose a three-stage training algorithm. Specifically, the whole procedure involves the following stages: 1) Attribute recognition: We begin by training the attribute recognition model within a multi-task learning framework [Caruana, 1997, Ruder, 2017]. 2) Circuit construction: Next, we construct the circuit using two distinct approaches: i) Data-driven approach learns the circuit's structure and optimizes its parameters based on data, allowing the underlying logical rules to be embedded within the circuit. ii)"}, {"title": "2 Preliminaries", "content": "Probabilistic circuits are a class of probabilistic graphical models that is used to express a joint distribution over a set of random variables $Z_{1:N}$. A probabilistic circuit $f_S$ (henceforth simply referred to as a circuit) consists of a rooted directed acyclic graph where leaf nodes are univariate indicators of categorical variables\u00b9 (i.e., $I(Z_i = z_i)$, $z_i \\in Z_i$, $i \\in [N]$) and internal nodes consist of sum nodes and product nodes. Each sum node computes a weighted sum of its children, and each product node computes a product of its children. In an unnormalized circuit, the root node outputs the unnormalized joint probability over variables. Any unnormalized circuit can be transformed into an equivalent, normalized circuit via weight updating Peharz et al. [2015], Zhao et al. [2015a]. Hence, without loss of generality, we always assume that $f_S$ is normalized; thus, $f_S(z_{1:N}) = Pr(Z_{1:N} = z_{1:N})$. Readers are referred to S\u00e1nchez-Cauce et al. [2021] for more details on circuits."}, {"title": "3 Neural Probabilistic Circuits", "content": "In this section, we introduce Neural Probabilistic Circuits (NPCs). We begin by describing the model architecture and the inference process, illustrating how NPCs enable compositional and interpretable predictions through logical reasoning (Section 3.1). Next, we elaborate on the three-stage training algorithm for training NPCs. In particular, we propose two distinct approaches for building circuits: a data-driven approach and a knowledge-injected approach (Section 3.2). Finally, we provide a theoretical analysis establishing the relationship between the error of the overall model and those of its individual modules (Section 3.3)."}, {"title": "3.1 Model Architecture and Inference", "content": "Figure 1 presents an overview of an NPC, which consists of an attribute recognition model and a task predictor. The attribute recognition model is a neural network that processes an input image to identify its high-level visual attributes, such as color and shape. The task predictor is a (normalized) probabilistic circuit that models the joint distribution over attributes and classes, embedding either implicit or explicit logical rules within its structure and parameters during learning. The circuit enables efficient probabilistic reasoning, including joint, marginal, and conditional inferences. Specifically, given a particular assignment of attributes, the circuit can infer the probability of a specific class. By leveraging these conditional dependencies alongside the probability distributions of the various attributes (i.e., outputs from the attribute recognition model), NPC produces the probabilities of the image belonging to various classes. The class with the highest probability is recognized as the predicted class.\nFormally, let $X \\in X$, $A_k \\in A_k$, $Y \\in Y$ denote the input variable, the k-th attribute variable, and the class variable. The variables' instantiations are represented by $x, a_k, y$, respectively. In particular, we consider $K$ attributes, i.e., $A_1,..., A_K$ (or $A_{1:K}$ in short). Each attribute $A_k$ has $q_k$ possible values, i.e., $|A_k| = q_k$. The attribute recognition model $f(X;\\theta)$ is parameterized by $\\theta$. Given an input instance $x$, the model outputs K probability vectors. The k-th probability vector, denoted as $f_k(x;\\theta) \\in \\mathbb{R}^{q_k}$, shows the probabilities of $x$'s k-th-attribute taking different values $a_k$, i.e., $[f_k(x;\\theta)]_{a_k} = Pr_\\theta(A_k = a_k | X = x)$. The task predictor $f_S(Y, A_{1:K}; w)$ is a probabilistic circuit with structure $S$ and parameters $w$, which models the joint distribution over $Y, A_{1:K}$. Specifically, when taking an instance of attributes $a_{1:K}$ and a class label $y$ as input, the circuit outputs the joint probability $Pr_w(Y = y, A_{1:K} = a_{1:K})$. By the virtue of its efficient inferences, the circuit also supports efficient conditional queries, e.g., $Pr_w (Y = y | A_{1:K} = a_{1:K}) = f_S(y,a_{1:k};w)/f_S(\\emptyset, a_{1:K};w)$.\nPrior to describing how an NPC predicts a class, we make the following mild assumptions on the selected attributes.\nAssumption 1 (Sufficient Attributes). Given the attributes, the class label is conditionally independent of the input, i.e., $Y \\perp X | A_1,..., A_K$.\nAssumption 2 (Complete Information). Given any input, all attributes are conditionally mutually independent, i.e., $A_1 \\perp A_2 \\perp ... \\perp A_K | X$.\nRemarks. The first assumption essentially assumes that the attributes are sufficient to infer the class label of interest. The second assumption assumes the input contains complete information regarding the attributes such that they are conditionally mutually independent. These assumptions are mild and often hold in practice. For instance, in the context of traffic signs, if the attributes include the shape (e.g., circle), color (e.g., red), and symbol (e.g., slash) of a sign, they collectively provide enough information to infer the class label (e.g., no entry) without requiring additional details from the raw image. On the other hand, the raw image fully encodes the attributes (e.g., shape, color, and symbol). Once the input is observed, these attributes can be independently determined.\nUnder Assumption 1 and 2, an NPC outputs the probability of an input x being a class y as follows,\n$Pr_{\\theta,w} (Y = y | X = x) = \\sum_{a_{1:K}} Pr_w (Y = y | A_{1:K} = a_{1:K}, X = x) \\cdot Pr_\\theta (A_{1:K} = a_{1:K} | X = x) \\\\ = \\sum_{a_{1:K}} Pr_w (Y = y | A_{1:K} = a_{1:K}) \\cdot \\prod_{k=1}^{K} Pr_\\theta (A_k = a_k | X = x).$"}, {"title": "3.2 Three-Stage Training Algorithm", "content": "In this section, we will propose a three-stage training algorithm for NPCs comprising the following stages: 1) attribute recognition through multi-task learning (Section 3.2.1), 2) circuit"}, {"title": "3.2.1 Attribute Recognition", "content": "We aim to train the attribute recognition model $f(X;\\theta)$ such that each attribute is recognized well. To this end, we adopt a multi-task learning framework [Zhang and Yang, 2021], where each task is to recognize a particular attribute. Specifically, we use the cross-entropy loss for each task and assign weights to the task losses based on the size of the corresponding attribute space. These weights normalize the task losses, preventing certain tasks from dominating the training process [Kendall et al., 2018, Gr\u00e9goire et al., 2024, Wang and Chen, 2020]. The overall training loss for attribute recognition is defined as follows,\n$\\mathcal{L}_{Attribute} (\\theta; D) = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{1}{\\left| A_k \\right|} \\sum_{x \\in D} g_k(x) f_k(x; \\theta).$ \n\nThe term inside the parentheses represents the mean cross-entropy loss over the training dataset D, where $f_k(x;\\theta) \\in \\mathbb{R}^{q_k}$ and $g_k(x) \\in \\mathbb{R}^{q_k}$ are, respectively, the output vector and the label vector corresponding to the k-th attribute. Specifically, $g_k(x)$ is a probability vector that sums to one, with each entry representing the ground-truth likelihood of x having a particular attribute value. For instance, for the color attribute, an image of a polar bear would have a one-hot label vector with 1 representing \"white\". In contrast, an image of a zebra might have a probabilistic label vector, with 0.5 representing \"black\" and 0.5 representing \"white\"."}, {"title": "3.2.2 Circuit Construction", "content": "We aim to construct a probabilistic circuit $f_S(Y, A_{1:K}; w)$ that models the joint distribution over $Y, A_{1:K}$. To achieve this, we propose two distinct approaches for building the circuit's structure and parameters: a data-driven approach and a knowledge-injected approach.\nData-Driven Approach This approach learns a circuit's structure and optimizes its parameters using data in the form of $\\{(y,a_{1:K})\\}$. As described above, the training dataset is defined as $D = \\{(x, g_{1:K}(x), y)\\}$, where each attribute label is represented as a probability vector rather than a single value. For each data, we sample an $a_k$ from the categorical distribution defined by $g_k(x)$, i.e., $a_k \\sim Categorical(g_k(x))$, $k \\in [K]$, which results in a processed dataset $D = \\{(x, y, a_{1:K})\\}$. 1) Structure Learning: LearnSPN Gens and Domingos [2013] is a mainstream algorithm for learning a circuit's structure from data. LearnSPN recursively identifies independent groups to create product nodes, clusters data to form sum nodes, and assigns single variables as leaf nodes. In our approach, we apply LearnSPN on D to derive a structure tailored to the observed data. 2) Parameter Learning: Given the learned structure, optimizing the circuit's weights (i.e., the weights of edges emanating from sum nodes) is framed as a maximum likelihood estimation (MLE) problem, with the following loss function:\n$\\mathcal{L}_{MLE}(W; D) = - \\sum_{(y,a_{1:K}) \\in D} log f_S (y, a_{1:K}; w).$\nWe adopt the widely used CCCP algorthim Zhao et al. [2016b] which iteratively applies multiplicative weight updates on w to minimize $\\mathcal{L}_{MLE}(W; D)$. CCCP is guaranteed to converge monotonously. Overall, with the learned structure and optimized parameters, the circuit captures the underlying logical rules present in the observed data, thus effectively modeling the joint distribution over attributes and classes.\nKnowledge-Injected Approach Incorporating domain knowledge into a model helps ensure that its behavior aligns with the human's understanding of the domain. In practice, domain"}, {"title": "3.2.3 Joint Optimization", "content": "Thanks to the differentiability of circuits, NPCs can be fine-tuned in an end-to-end manner to further improve the performance of the overall model on downstream tasks. Specifically, the loss function is defined as follows,\n$\\mathcal{L}_{Joint} (\\theta, w; (x, y)) = - \\sum_{(x,y) \\in D} log Pr_{\\theta,w} (Y = y | X = x).$\nTo optimize this loss, we simply employ the stochastic gradient descent algorithm for updating $\\theta$, while the projected gradient descent algorithm is used to update w to ensure the positivity of the circuit weights. The detailed optimization process is provided in Appendix A."}, {"title": "3.3 Theoretical Analysis", "content": "In this section, we present an error analysis for NPCs to understand how the performance of individual modules affects that of the overall model. Given the overall model and the attribute recognition model are discriminative models, while the probabilistic circuit is a generative model, we define the following errors: 1) Error of the overall model: $\\epsilon_{\\theta,w} := E_X [d_{TV} (Pr_{\\theta,w} (Y | X), Pr(Y | X))]$, which represents the expected total variance distance between the learned and true conditional distributions of Y given X. 2) Error of the attribute recognition model: $\\epsilon_{\\theta} := E_X [d_{TV} (Pr_{\\theta}(A_{1:K} | X), Pr(A_{1:K} | X))]$, which quantifies the expected total variation distance between the learned and true conditional distributions of the attributes $A_{1:k}$ given X. Additionally, we define $\\epsilon_k := E_X [d_{TV}(Pr_{\\theta}(A_k | X), Pr(A_k | X))]$ as the error for each individual attribute $A_k$. 3) Error of the probabilistic circuit: $\\epsilon_w := d_{TV} (Pr_w (Y, A_{1:K}), Pr(Y, A_{1:K}))$, which measures the total variation distance between the learned and true joint distributions of Y and $A_{1:K}$. The above errors capture how closely the learned models approximate the underlying true distributions.\nTheorem 2 (Compositional Error). Under Assumptions 1 and 2, the error of an NPC is bounded by a linear combination of the errors of the attribute recognition model and the circuit-based task predictor. In particular, the error of the attribute recognition model across all attributes is bounded by the sum of the errors for each attribute, i.e.,\n$\\epsilon_{\\theta,w} < \\epsilon_{\\theta} + 2\\epsilon_w < \\sum_{k=1}^{K} \\epsilon_k +2\\epsilon_w.$\nProof Sketch. By leveraging Equation (1), the upper bound of $\\epsilon_{\\theta,w}$ is decomposed into two terms. The first term depends only on $\\theta$ and represents the error of the attribute recognition model, while the second term depends only on $w$ and captures the error of the circuit. In particular, the overall error of the attribute recognition model is further expanded into the errors across individual attributes.\nThe complete proof is deferred to Appendix B. Theorem 2 demonstrates that reducing the error for any single attribute helps reduce the overall error of the attribute recognition model. More importantly, the error bound of an NPC is decomposable into contributions from its individual modules, which accredits to the compositional nature of NPCs and the incorporation of probabilistic circuits. Consequently, reducing the error of any individual module helps improve the performance of the NPC."}, {"title": "4 Model Explanations", "content": "As discussed in Section 3.1, model predictions can be interpreted using the attribute recognition results and the conditional dependencies between classes and attributes. To further enhance the human's understanding of the model predictions, we provide various explanations addressing"}, {"title": "4.1 Most Probable Explanations", "content": "To address the first question, we define Most Probable Explanations (MPEs) for NPCs for identifying the highest contributing attribute assignments.\nDefinition 1 (Most Probable Explanations). Given an input x with prediction \u0177, the most probable explanation is defined as the attribute assignment that contributes the most in $Pr_{\\theta,w}(Y = \\hat{y} | X = x)$. Formally, $a_{1:K} := arg \\max_{a_{1:K}} Pr_w (Y = \\hat{y} | A_{1:K} = a_{1:K})\\cdot \\prod_{k=1}^{K} Pr_\\theta (A_k = a_k | X = x)$.\nMPE inference is generally challenging for probabilistic circuits. Such inference is tractable for selective circuits [S\u00e1nchez-Cauce et al., 2021], but this type of circuit is relatively restrictive in expressiveness. As the number of attributes is small in our experimental settings, we simply use the brute-force algorithm to infer MPEs. Developing more efficient heuristics for MPE inference remains an open problem and is not the primary focus of this paper. Therefore, we leave it for future work.\nMPEs provide a concrete explanation as to how the model arrives at a specific class prediction. Specifically, the predicted class is primarily due to the input image's attributes being recognized as $a_{1:K}$. These explanations offer attribute-level insights into the model's predictions, thereby enhancing interpretability and the human's understanding of the predictions.\nTo gain deeper insights into how these explanations represent the model's behavior, we define a property for MPEs, called alignment, and introduce a corresponding metric to characterize the behavior of the model.\nDefinition 2 (Alignment Rate). An MPE $a_{1:K}$ is considered aligned with a sample x if $a_k \\in \\{j \\in [q_k] | g(x) > 0\\}$, $k\\in [K]$. The alignment rate is defined as the proportion of aligned MPEs among all correctly predicted samples.\nA high alignment rate reflects strong model reliability, as it suggests that the ground-truth attribute assignments contribute the most during prediction. In other words, the model closely adheres to the human's understanding when making predictions."}, {"title": "4.2 Counterfactual Explanations", "content": "To address the second question, we define Counterfactual Explanations (CEs) [Wachter et al., 2017] for NPCs to explore admissible changes in attribute recognition results that can correct any incorrectly predicted classes.\nDefinition 3 (Counterfactual Explanations). Given an input x which has an incorrect model prediction. Denote $b := \\{b_k\\}_{k\\in[K]}$, $b_k := (..., b_{k,a_k}, ...)_{a_k \\in A_k}$, and $Pr_b(Y = y | X = x) := \\sum_{a_{1:K}} Pr_w (Y = y | A_{1:K} = a_{1:K}) \\cdot \\prod_{k=1}^{K} b_{k,a_k}$. The counterfactual explanation for the ground-truth label y is a set of probability vectors b that maximizes $Pr_b(Y = y | X = x)$, i.e., the solution to the following optimization problem,\n$max_b Pr_b(Y = y | X = x), s.t. \\sum_{a_k \\in A_k} b_{k,a_k} = 1 (0 \\leq b_{k,a_k} \\leq 1), \\forall k \\in [K].$\nWe adopt the projected gradient ascent algorithm to generate the CEs, which is detailed in Algorithm 1."}, {"title": "5 Experiments", "content": "Datasets We evaluate the model performance on a variety of benchmark datasets. 1) MNIST-Addition: We derive this dataset from the original MNIST dataset [LeCun et al., 1998] by following the general preprocessing steps and procedures detailed in [Manhaeve et al., 2018]. Each MNIST-Addition sample consists of two images randomly selected from the original MNIST. The digits in these images, ranging from 0 to 9, represent two attributes, with their sum serving as the class label. A total of 35,000 samples are created for MNIST-Addition. 2) GTSRB: GTSRB [Stallkamp et al., 2012] is a dataset comprising 39,209 images of German traffic signs, with class labels indicating the type of signs. Additionally, we annotate each sample with four attributes: \"color\", \"shape\", \"symbol\", and \"text\". The values for these attributes are detailed in Appendix D. 3) CelebA: CelebA [Liu et al., 2015] consists of 202,599 celebrity face images annotated with 40 binary concepts. Here, we select the 8 most balanced binary concepts\u00b2 and group them into 5 attributes: \u201cmouth\u201d, \u201cface\u201d, \u201ccosmetic\u201d, \u201chair\u201d, and \u201cappearance\u201d. Following Zarlenga et al. [2022], each unique combination of concept values is treated as a group. To balance the dataset and increase its complexity, we rank these groups by the number of images they contain and pair them strategically: the group with the most images is merged with the one with the fewest, the second most with the second fewest, and so on. The above strategy results in 127 total classes. 4) AwA2: AwA2 [Xian et al., 2018] contains 37,322 images of 50 types of animals, each annotated with 85 binary concepts. Certain concepts,"}, {"title": "5.2 NPCs vs. Baselines", "content": "We compare NPCs against baseline models across the four benchmark datasets, with the results summarized in Table 2. Specifically, we refer to the NPC whose circuit was learned using the data-driven approach as \"NPC(Data)\" and the NPC whose circuit was constructed using the knowledge-injected approach as \"NPC(Knowledge)\".\nThe results in Table 2 demonstrate that NPCs outperform all other concept-based baseline models. Specifically, NPC(Knowledge) achieves the best performance on the MNIST-Addition and GTSRB datasets, while NPC(Data) leads on the CelebA and AwA2 datasets. Notably, NPCs achieve superior performance even compared to CEM, an uninterpretable model that relies on high-dimensional concept embeddings, highlighting NPC's effectiveness in leveraging interpretable concept probabilities for downstream classification tasks.\nRemarkably, NPCs achieve superior performance even compared to that of the end-to-end DNN, surpassing its classification accuracy on the MNIST-Addition and GTSRB datasets. Nevertheless, small gaps remain between the end-to-end DNN and NPCon more complex datasets like CelebA and AwA2. The above findings demonstrate that, while there is still small room for improvement compared to black-box models, NPCs strike a compelling balance between interpretability and task performance. Overall, the results underscore the remarkable potential of interpretable models, demonstrating their ability to achieve competitive performance in downstream tasks compared to baselines and even black-box end-to-end DNN models."}, {"title": "5.3 Ablation Studies", "content": "In this section, we delve into NPCs from additional perspectives. Specifically, we will analyze the integration of attributes, the influence of attribute selections, the effects of various approaches to constructing the task predictor, and, lastly, the impact of joint optimization."}, {"title": "5.3.1 Attributes vs. Concepts", "content": "Unlike existing concept-based models that utilize individual binary concepts (e.g., \"red color\", \"yellow color\"), NPCs focus on concept groups, i.e., attributes (e.g., \"color\"). Here, we aim to explore the benefits of using attributes compared to individual concepts. To this end, we replace the concept recognition model in CBM [Koh et al., 2020] with an attribute recognition model, resulting in a new model called the Attribute Bottleneck Model (ABM). ABM comprises an attribute recognition model and a linear layer serving as the task predictor. We employ the training loss from CBM to train ABM, replacing the concept loss with the attribute loss as defined in Equation (2). The performance comparison between CBM and ABM is presented in Table 3.\nThe results in Table 3 show that, in terms of the mean TV distance, ABM outperforms CBM on the MNIST-Addition and GTSRB datasets, albeit exhibiting slightly worse performance on the CelebA and AwA2 datasets. On the other hand, ABM consistently surpasses CBM in"}, {"title": "5.3.2 Impact of Attribute Selection", "content": "During inference, an NPC utilizes sufficient attributes to produce final predictions. Here, we aim to investigate the following questions: How does the exclusion of one particular attribute during inference impact the performance of NPC on downstream tasks? How does the exclusion of different attributes vary the impact?\nAssuming the excluded attribute is $A_1$, the predicted score for the class y would become $\\sum_{a_{2:K}} Pr_w (Y = y | A_{2:K} = a_{2:K}) \\cdot \\prod_{k=2}^{K} Pr_\\theta(A_k = a_k | X)$. We apply this inference process on the GTSRB dataset and the resulting task performance is presented in Figure 3 (Left)."}, {"title": "5.3.3 Impact of Task Predictor Construction Approaches", "content": "In Section 3.2.2, we introduce two distinct approaches for constructing probabilistic circuits: the data-driven approach and the knowledge-injected approach. Here, our objective is to investigate the impact of these construction methods. Specifically, we aim to address the following questions: Which approach produces a circuit that better captures the data distribution? Which approach produces a circuit that performs more effectively as a task predictor? We start by comparing the mean likelihood of the two circuits. Then, we examine the classification accuracy of the overall models consisting of a well-trained attribute recognition model together with either circuit (data driven or knowledge injected). The results are summarized in Table 4."}, {"title": "5.3.4 Impact of Joint Optimization", "content": "When training the NPCs, we adopt a three-stage training algorithm, where we first independently train the attribute recognition model and the task predictor, followed by a joint optimization for the overall model. Here, we aim to investigate how the third stage, i.e., joint optimization, affects the performance of NPCs. To this end, we compare the performance of NPCs before and after applying the joint optimization. The comparison is illustrated in Figure 4."}, {"title": "5.4 Model Explanations", "content": "In this section, we explore two types of explanations and provide examples to illustrate how the explanations facilitate the human's understanding of NPC's inner workings and interpret the model's behavior."}, {"title": "5.4.1 Most Probable Explanations", "content": "Figure 5 presents some examples for NPC(Data) from the four benchmark datasets. Specifically, each example comprises an image, the ground-truth labels for the class and attributes, the class predicted by NPC(Data), and, lastly, the corresponding MPE which accounts for the attribute assignment that contributes most significantly to the prediction. In these instances, NPC(Data) provides correct class predictions, and the MPEs are aligned with the ground-truth attribute labels. For instance, the MPE for the example from GTSRB is {Color: Red; Shape: Circle; Symbol: Text; Text: 30}, which precisely matches the ground-truth attribute labels. Such alignment between MPEs and attribute labels indicates that the model employs human-like reasoning and makes reliable decisions. Examples for NPC(Knowledge) are deferred to Appendix F."}, {"title": "5.4.2 Counterfactual Explanations", "content": "Figure 7 illustrates some examples for NPC(Data) from the four benchmark datasets. Each sample consists of an image, the attribute and class predictions, the generated CE, and, lastly,"}, {"title": "6 Limitations and Discussions", "content": "In this section, we discuss the limitations of NPCs from multiple perspectives, highlighting potential future directions for improvement.\nModel Architecture Compared to end-to-end DNNs, NPCs offer superior interpretability by decomposing a model into semantically meaningful modules, enabling humans to combine module outputs to understand the final decisions. Nevertheless, the attribute recognition model itself remains a black box, and its opaque inner workings make it difficult to ensure that its outputs truly represent the probabilities for the various attributes. For instance, the model might learn spurious correlations and incorrectly map background features, instead of actual attributes, to outputs. Future work may focus on increasing the transparency within the attribute recognition model, thereby enhancing its interpretability.\nStructure of Probabilistic Circuits In NPCs, the task predictor, implemented using a probabilistic circuit, is either learned using LearnSPN [Gens and Domingos, 2013", "2017": "may be explored to create more compact circuits for added inference efficiency. On the other hand, manually constructed circuits employ simpler structures with only two layers. While it may improve efficiency, such simplicity may limit the circuit's expressiveness, potentially degrading its performance on complex datasets like AwA2. Future work may focus on improved balancing between circuit expressiveness and structural complexity.\nError Analysis in More Challenging Scenarios We base our error analysis for NPCs on Assumptions 1"}]}