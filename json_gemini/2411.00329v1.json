{"title": "Personalized Federated Learning via Feature\nDistribution Adaptation", "authors": ["Connor J. McLaughlin", "Lili Su"], "abstract": "Federated learning (FL) is a distributed learning framework that leverages com-\nmonalities between distributed client datasets to train a global model. Under\nheterogeneous clients, however, FL can fail to produce stable training results. Per-\nsonalized federated learning (PFL) seeks to address this by learning individual\nmodels tailored to each client. One approach is to decompose model training\ninto shared representation learning and personalized classifier training. Nonethe-\nless, previous works struggle to navigate the bias-variance trade-off in classifier\nlearning, relying solely on limited local datasets or introducing costly techniques\nto improve generalization. In this work, we frame representation learning as a\ngenerative modeling task, where representations are trained with a classifier based\non the global feature distribution. We then propose an algorithm, pFedFDA, that\nefficiently generates personalized models by adapting global generative classifiers\nto their local feature distributions. Through extensive computer vision benchmarks,\nwe demonstrate that our method can adjust to complex distribution shifts with\nsignificant improvements over current state-of-the-art in data-scarce settings. Our\nsource code is available on GitHub\u00b9.", "sections": [{"title": "Introduction", "content": "The success of deep learning models relies heavily on access to large, diverse, and comprehensive\ntraining data. However, communication constraints, user privacy concerns, and government regula-\ntions on centralized data collection often pose significant challenges to this requirement [31, 37, 18].\nTo address these issues, Federated Learning (FL) [34] has gained considerable attention as a dis-\ntributed learning framework, especially for its privacy-preserving properties and efficiency in training\ndeep networks.\nThe FedAvg algorithm, introduced in the seminal work [34], remains one of the most widely\nadopted algorithms in FL applications [32, 45, 38, 49, 40, 7]. It utilizes a parameter server to\nmaintain a global model, trained through iterative rounds of distributed client local updates and server\naggregation of client models. While effective under independent and identically distributed (i.i.d.)\nclient data, its performance deteriorates as client datasets become more heterogeneous (non-i.i.d.).\nData heterogeneity leads to the well-documented phenomenon of client drift [19], where distinct\nlocal objectives cause the model to diverge from the global optimum, resulting in slow convergence\n[20, 28] and suboptimal local client performance [42]. Despite extensive efforts [27, 19, 46, 6] to\nenhance FedAvg for non-i.i.d. clients, the use of a single global model remains too restrictive for\nmany FL applications.\nPersonalized federated learning (PFL) has emerged as an alternative framework that produces\nseparate models tailored to each client. The success of personalization techniques depends on"}, {"title": "Related Work", "content": "Federated Learning with Non-i.i.d. Data. Various studies have worked to understand and improve\nthe ability of FL to serve heterogeneous clients. In non-i.i.d. scenarios, the traditional FedAvg method\n[34] is susceptible to client drift [19], resulting in slow convergence and poor local client accuracy\n[28, 27]. To tackle this challenge, [27, 1, 21] proposed the use of regularized local objectives to\nreduce the bias on the global model after local training. Another approach focuses on rectifying the\nbias of local updates [19, 10] through techniques such as control variates. Other strategies include\nloss-balancing [15, 47, 3], knowledge distillation [30, 54], prototype learning [43], and contrastive\nlearning [25]. Despite promising results on non-i.i.d. data, their reliance on a single global model\nposes limitations for highly heterogeneous clients [17].\nPersonalized Federated Learning. In response to the limitations of a single global model, PFL\nseeks to overcome heterogeneity by learning models tailored to each client. In this framework,\nmethods attempt to strike a balance between being flexible enough to fit the local distribution and\nrelying on global knowledge to prevent over-fitting on small local datasets. Popular strategies include\nmeta-learning an initialization for client adaptation [16, 9], multi-task learning with local model\nregularization [41, 26], local and global model interpolation [6], personalized model aggregation\n[51, 50], client clustering [39, 8], and decoupled representation and classifier learning [5, 29, 35, 48, 3].\nOur work focuses on this latter approach, in which the neural network is typically decomposed into\nthe first $L-1$ layers used for feature extraction, and the final classification layer.\nExisting works in this category share feature extraction parameters between clients and rely on client\nclassifiers for personalization. These approaches differ primarily in the acquisition of client classifiers"}, {"title": "Problem Formulation", "content": "FL System and Objective. We consider an FL system where a parameter server coordinates with $M$\nclients to train personalized models $\\theta_i$, $i = 1,2,\\ldots, M$. Each client $i$ has a local training dataset\n$\\mathcal{D}_i = \\{(x_j, y_j)\\}_{j=1}^{n_i}$, where $x \\in \\mathbb{R}^m$ and $y \\in \\{1, \\ldots, C\\}$. The model training objective in PFL is:\n$\\min_{\\theta_1,\\ldots,\\theta_M \\in \\Omega} f(\\theta_1,\\ldots, \\theta_M) := \\frac{1}{M} \\sum_{i=1}^M F_i(\\theta_i)$                                                                               (1)"}, {"title": "Methodology", "content": "In this section, we introduce pFedFDA,\na personalized federated learning method\nthat utilizes a generative modeling ap-\nproach to guide global representation learn-\ning and adapt to local client distributions.\nWe present our method using a class-\nconditional Gaussian model of the feature\nspace, with additional discussion of the se-\nlected probability density in Section 4.1.\nAlgorithm 1 describes the workflow of\npFedFDA.\nOur algorithm begins with a careful initial-\nization of parameters for the feature extrac-\ntor $\\phi$, Gaussian means $\\mu = \\{\\mu_c\\}_{c=1}^C$, and\ncovariance $\\Sigma$ (Lines 1-2). We initialize $\\phi$\nwith established techniques (e.g., [12]) such\nthat the output features follow a Gaussian\ndistribution with controlled variance. We\nsimilarly use a spherical Gaussian to ensure\na stable initialization of the corresponding\ngenerative classifier (see Section 4.1).\nAt the start of each FL round $r$, the server broadcasts the current $\\phi^g$, $\\mu^g$, $\\Sigma^g$ to each participating\nclient. The local training of each client consists of two key components: (1) global representation\nlearning, in which clients train $\\phi$ to maximize the likelihood of local features under the global\nfeature distribution $\\mu^g$, $\\Sigma^g$ (Line 7); (2) local distribution adaptation, in which clients obtain robust\nestimates of their local feature distribution $\\mu_i$, $\\Sigma_i$, using techniques for efficient low-sample Gaussian\nestimation (Line 8) and local-global parameter interpolation (Lines 9-11). After local training, clients\nsend their $\\phi_i$, $\\mu_i$, $\\Sigma_i$ to the parameter server for aggregation (Line 14)."}, {"title": "Generative Model of Feature Distributions", "content": "Motivation for Generative Classifiers. A central theme in FL is exploiting inter-client knowledge\nto train more generalizable models than any client could attain using only their local dataset. This\npresents an important bias-variance trade-off, as incorporating global knowledge naively can introduce\nsignificant bias. Fortunately, under a generative modeling approach, this bias can be naturally handled,\nenabling efficient inter-client collaboration.\nFirst note that local class priors $p_i(y)$ can be approximated with local counts: $p_i(y = c) \\approx \\frac{n_i^c}{\\sum_{c' \\in C} n_i^{c'}} := \\pi_i^c$, where $n_i^c$ is the number of local samples whose labels are $c$. This leaves the\nprimary source of bias to the mismatch between local and global feature distributions $p_g(z|y)$ and\n$p_i(z|y)$. Crucially, it turns out that this bias is controllable due to the dependence of $z$ on global repre-\nsentation parameters $\\phi$. Consequentially, we propose to minimize this bias through our classification\nobjective, which we discuss further in Section 4.2.\nClass-Conditional Gaussian Model. In this work we approximate the distribution of latent repre-\nsentations using a class-conditional Gaussian with tied covariance, i.e., $p_i(z|y = c) = \\mathcal{N}(z|\\mu_c, \\Sigma_i)$.\nWe show the resulting generative classifier under this model in Eq. 4. Note that it has a closed form\nand results in a decision boundary that is linear in $z$. I.e., if we know the underlying local feature\ndistribution mean and covariance, we can efficiently compute the optimal header parameters $h_i$ for\nthe inner objective in Eq. 2.\nIn addition to the convenient form of the Bayes classifier, we select this distribution as the Gaussianity\nof latent representations is likely to hold in practice. Notably, by adopting the common technique of\nGaussian weight initialization (e.g., [12]), the resulting feature space is highly Gaussian at the start\nof training. It has also been observed that the standard supervised training of neural networks with\ncross-entropy objectives results in a feature space that is well approximated by a class-conditional\nGaussian distribution [24], i.e., the corresponding generative classifier Eq. 4 has equal accuracy to\nthe learned discriminative classifier. We provide a further discussion of this modeling assumption in\nAppendix A.\n$p(y = c|z) = \\frac{\\mathcal{N}(z|\\mu_c, \\Sigma)\\pi(y = c)}{\\sum_{c' \\in C} \\mathcal{N}(z|\\mu_{c'}, \\Sigma)\\pi(y = c')}$,\n$\\log p(y = c|z) \\propto z^T \\Sigma^{-1}\\mu_c - \\frac{1}{2}(\\mu_c)^T \\Sigma^{-1}\\mu_c + \\log \\pi(y = c)$."}, {"title": "Global Representation Learning", "content": "Next, we describe our process for training the shared feature extractor $\\phi$. Similar to existing works\n[5, 48], our local training consists of training $\\phi$ via gradient descent to minimize the cross-entropy loss\nof predictions from fixed client classifiers. We obtain our client classifiers through Eq. 4, using global\nestimates of $\\mu_g$, $\\Sigma_g$ and local estimated priors $\\pi_i$. For computational efficiency, we avoid inverting\nthe covariance matrix by estimating $\\Sigma^{-1}\\mu_g$ with the least-squares solution $w = \\min_{w'} ||\\Sigma w' - \\mu_g||$.\nThe loss of client $i$ for an individual training sample $(x, y)$ is provided in Eq. 5.\n$\\mathcal{L}(x, y; \\phi, \\mu, \\Sigma, \\pi) = \\sum_{c=1}^C \\mathbb{I}_{y=c} \\log p(y^c|\\phi(x), \\mu_c, \\Sigma, \\pi)$.\nNote that for a spherical Gaussian $\\Sigma = I$ and uniform prior $\\pi$, we recover a nearest-mean classifier\nunder Euclidean distance. This resembles the established global prototype regularization [43], which\nminimizes the Euclidean distance of features from their corresponding global class prototypes.\nNotably, FedPAC [48] uses this prototype loss to align client features. However, this implicitly\nassumes that all feature dimensions have equal variance, and additionally requires a hyperparameter"}, {"title": "Local Distribution Adaptation", "content": "Local Estimation. A key component of pFedFDA is the estimation of local feature distribution param-\neters, used both for model personalization and for updating the global distribution for representation\nlearning.\nGiven a set of $n$ extracted features $Z$ with $n_c$ examples per class $c$, a maximum likelihood esti-\nmate of the class means and an unbiased estimator of the covariance, respectively, are given by:\n$\\mu_c = \\frac{1}{n_c} \\sum_{j=1}^n \\mathbb{I}\\{y_j = c\\} z_j$\n$\\Sigma = \\frac{1}{n-1} Z^T Z$,\nwhere, with slight abuse of notation, $\\tilde{Z} \\in \\mathbb{R}^{n \\times d}$ denotes the matrix of centered features with rows\ncorresponding to each original feature $z_j$ centered by their respective means, i.e.,\n$\\tilde{z}_j = z_j - \\sum_{c \\in C} \\mathbb{I}\\{y_j=c\\} \\mu_c$.\nEstimators Eq. 6 and Eq. 7 may be noisy on clients with limited local data. To illustrate this, consider\nthe common practical scenario where $n_i < d$. The feature covariance matrix $\\Sigma_i$ at client $i$ will be\ndegenerate; in fact, it will have a multitude of zero eigenvalues. In these cases, we can add a small\ndiagonal $\\epsilon I$ to $\\Sigma$, and replace the non-positive-definite matrices with the nearest positive definite\nmatrix with identical variance. This can be efficiently computed by clipping eigenvalues in the\ncorresponding correlation matrix and followed by converting it back to a covariance matrix with\nnormalization to maintain the initial variance. We refer readers to [11] for a review of low-sample\ncovariance estimation.\nLocal-Global Interpolation. We introduce this fusion because even with the aforementioned correc-\ntion to ill-defined covariances, the variance of the local estimates remains highly noisy, indicating the\nnecessity of leveraging global knowledge. It is essential to consider that in the presence of data het-\nerogeneity, clients with differing local data distributions and dataset sizes have varying requirements\nfor global knowledge.\nFor our Gaussian parameters $\\mu$, $\\Sigma$, we consider the introduction of global knowledge through a\npersonalized interpolation between local and global estimates, which can be viewed as a form of\nprior. We provide an analysis of the high-probability bound on estimation error for an interpolated\nmean estimate in simple settings in Theorem 1. The full derivation is deferred to Appendix E.\nTheorem 1 (Bias-Variance Trade-Off). Let $C = 1$. Define $\\mu_i$ as the sample mean of client $i$'s local\nfeatures $\\mu_i := \\frac{1}{n_i} \\sum z_{ij}$, and $\\mu_g$ as the global sample mean using all $N$ samples across $M$ clients:\n$\\mu_g := \\sum_i \\sum_j z_{ij}$. Assume client features are independent and distributed as $z_i \\sim \\mathcal{N}(\\theta_i, \\Sigma_i)$,\nwith true global feature distribution $\\mathcal{N}(\\theta_g, \\Sigma_g)$. We consider the use of global knowledge at client $i$\nthrough an interpolated estimate: $\\hat{\\mu}_i := \\beta \\mu_i + (1 - \\beta) \\mu_g$, where $\\beta \\in [0,1]$. For any $\\delta \\in (0,1)$, with\nprobability at least $1 - \\delta$, it holds that\n$\\|\\hat{\\mu}_i - \\theta_i\\|^2 \\leq (1 - \\beta)^2 \\|\\theta_g - \\theta_i\\|^2 + \\left[1 + 4\\left(\\frac{\\log 1/\\delta}{c} + \\frac{\\log 1/\\delta}{c}\\right)\\right]\\left(\\frac{\\beta^2}{N_i} Tr(\\Sigma_i) + \\frac{(1 - \\beta)^2}{N^2} Tr(\\Sigma_g)\\right)$,\nwhere $c > 0$ is an absolute constant.\nIntuitively, the estimation error and optimal $\\beta$ depend on the bias introduced by using global knowl-\nedge $\\|\\theta_g - \\theta_i\\|^2$, the variance of local and global features, and the respective data volumes.\nWe formulate this as an optimization problem, in which clients estimate interpolation coefficients $\\beta_i$\nto combine local and global estimates of $(\\mu, \\Sigma)$ with minimal $k$-fold validation loss:\n$\\beta_i \\in \\min_{\\beta} \\frac{1}{k} \\sum_{k} \\sum_{(x,y) \\in \\mathcal{D}_k} \\mathcal{L}(x, y, \\phi, \\beta' \\hat{\\mu}_k + (1 - \\beta')\\mu_g, \\beta' \\hat{\\Sigma}_k + (1 - \\beta')\\Sigma_g, \\pi_i)$,"}, {"title": "Experiments", "content": "Experimental Setup\nDatasets, Tasks, and Models: We consider image classification tasks and evaluate our method on\nfour popular datasets. The EMNIST [4] dataset is for 62-class handwriting image classification. The\nCIFAR10/CIFAR100 [22] datasets are for 10 and 10-class color image classification. The TinyIm-\nageNet [23] dataset is for 200-class natural image classification. For EMNIST and CIFAR10/100\ndatasets, we adopt the 4-layer and 5-layer CNNs used in [48]. On the larger TinyImageNet dataset,\nwe use the ResNet18 [13] architecture. Notably, the feature dimension $d$ for EMNIST/CIFAR CNNs\nis 128, and 512 for ResNet. We provide additional details in Appendix C.1.\nClients and Dataset Partitions: The EMNIST dataset has inherent covariate shifts due to the\nindividual styles of each writer. We partition the dataset by writer following [6], and train with\n$M = 1000$ total clients (writers), participating with rate $q = 0.03$. On CIFAR and TinyImageNet\ndatasets, we simulate prior probability shift and quantity skew by partitioning the dataset according\nto a Dirichlet distribution with parameters $\\alpha \\in (0.1, 0.5)$, where lower $\\alpha$ indicates higher levels of\nheterogeneity. On these datasets, we use $M = 100$ clients with participation rate $q = 0.3$. Additional\ndetails of the partitioning strategy are provided in Appendix C.1.2.\nWe split each client's data partition 80-20% between training and testing.\nCovariate Shift and Data Scarcity: We introduce two modifications to client partitions to simulate\nthe challenges of real-world cross-device FL. We first consider common sources of input noise\nfor natural images, which may result from the qualities of the measuring devices (e.g., camera\ncalibration, lens blur) or environmental factors (e.g., weather, lighting). To simulate this, we select\nten image corruptions at five levels of severity defined in [14], and corrupt the training and testing\nsamples of the first 50 clients in CIFAR10/100 with unique corruption-severity pairs. We leave the\nremaining 50 client datasets unchanged. We refer to these datasets with natural covariate shifts as\nCIFAR10-S/CIFAR100-S and detail the specific corruptions in Appendix C.1.1.\nSecond, we perform uniform subsampling of client training sets, leaving them with (75%, 50%, or\n25%) of their original samples. These low-sample settings are more realistic for cross-device FL,\nwhere clients rely more on knowledge sharing.\nBaselines and Metrics: We compare pFedFDA to the following baselines: Local, in which each\nclient trains its model in isolation; FedAvg [34] and FedAvg with fine-tuning (FedAvgFT); APFL [6];\nDitto [26]; pFedMe [41]; FedRoD [3]; FedBABU [35]; FedPAC [48]; FedRep [5]; and LG-FedAvg\n[29]. We report the average and standard deviation of client test accuracies.\nModel Training: We train all algorithms with mini-batch SGD for $E = 5$ local epochs and $R = 200$\nglobal rounds. We apply no data augmentation besides normalization into the range [-1,1]. For\npFedFDA, we use $k = 2$ cross-validation folds to estimate a single $\\beta_i$ term for each client. Additional\ntraining details and hyperparameters for each baseline method are provided in Appendix C.2."}, {"title": "Numerical Results", "content": "Performance under covariate shift and data scarcity. We first present our evaluation under natural\nclient covariate shift with varying data scarcity in Table 1. In all experiments, pFedFDA outperforms"}, {"title": "Ablation of Method Components", "content": "We conduct two studies to verify the efficacy of our local-global interpolation method. In Table 5, we\nsee that our interpolated estimates always perform better than using only local data, indicating the\nbenefits of harnessing global knowledge. Learning separate $\\beta$ terms for the means and covariance\nmay be beneficial in low-sample or covariate-shift settings when the local distribution estimate may\nfluctuate further from the global estimate. However, using a single scalar $\\beta$ appears sufficient and\ncomes with the lowest computational cost (associated with the time to solve Eq. 9).\nWe additionally visualize the spread of learned $\\beta$ across clients as a function of their dataset corruption\nin Fig. 2. As expected, clients with clean datasets rely more on global knowledge (smaller $\\beta$ values)\nthan corrupted clients. Moreover, corruptions with higher $\\beta$ values (e.g., contrast) often align with\nthe more difficult corruptions encountered in Table 4."}, {"title": "Communication and Computation", "content": "The parameter count and relative communication load of our generative classifiers compared to a\nsimple linear classifier varies depending on class count $C$ and feature dimension $d$. In our experimental\nconfigurations (datasets, architectures), the overhead in total parameter count ranges from 1.1% to\n6.8%. See Appendix D.3 for additional details.\nIn Table 6, we compare the local training time (client-side computation) and total runtime of pFedFDA\nto baseline methods on CIFAR10. We observe a slight increase in training time relative to FedAvg,\nwhich can be attributed primarily to cost of learning our parameter interpolation coefficient $\\beta$.\nHowever, this increase is comparable to the existing methods and is lower than representation-"}, {"title": "Conclusion", "content": "Balancing local model flexibility and generalization remains a central challenge in personalized\nfederated learning (PFL). This paper introduces pFedFDA, a novel approach that addresses the bias-\nvariance trade-off in client personalization through representation learning with generative classifiers.\nOur extensive evaluation on computer vision tasks demonstrates that pFedFDA significantly outper-\nforms current state-of-the-art methods in challenging settings characterized by covariate shift and\ndata scarcity. Furthermore, our approach remains competitive in more general settings, showcasing its\nrobustness and adaptability. The promising results underline the potential of our method to improve\npersonalized model performance in real-world federated learning applications. Future work will\nfocus on exploring the scalability of pFedFDA and its application to other domains."}, {"title": "Limitations", "content": "\u2022 The selected class-conditional Gaussian distribution may not work well for all neural network\narchitectures. For example, if the output features are the result of an activation such as ReLU, a\ntruncated Gaussian distribution may be a better model. Future work can look to exploit knowledge\nof the neural network architecture to improve the accuracy of the feature distribution estimate.\n\u2022 In this work, we leverage the insights from a fusion of global and local feature space. As in many\napplications there is often an underlying cluster structure between clients datasets, future works\nmay explore the identification and efficient estimation of feature distributions of client clusters, in\norder to reduce the degree of bias introduced in client collaboration."}, {"title": "Broader Impacts", "content": "Federated learning has become the main trend for distributed learning in recent years and has deployed\nin many popular consumer devices such as Apple's Siri, Google's GBoard, and Amazon's Alexa.\nOur paper addresses the practical limitations of personalization methods in adapting to clients with\ncovariate shifts and/or limited local data, which is a central issue in cross-device FL applications. We\nare unaware of any potential negative social impacts of our work."}, {"title": "Details of Experimental Setup", "content": "All experiments are implemented in PyTorch 2.1 [36] and were each trained with a single NVIDIA\nA100 GPU. Compute time per experiment ranges from approximately 2 hours for CIFAR10/100\nand 20 hours for TinyImageNet. Code for re-implementing our method is provided at the following\nGitHub URL: https://github.com/cj-mclaughlin/pFedFDA."}, {"title": "Dataset Description", "content": "The EMNIST [4] dataset contains over 730,000 28\u00d728 grayscale images of 62 classes of handwritten\ncharacters. The CIFAR10/CIFAR100 [22] datasets contain 60,000 32\u00d732 color images in 10 and\n100 different classes of natural images, respectively. TinyImageNet [23] contains 120,000 64\u00d764\ncolor images of natural images."}, {"title": "CIFAR-S Generation.", "content": "We implement the following 10 common image corruptions at 5 levels of severity as described in\n[14]: Gaussian noise, shot (Poisson) noise, impulse noise, defocus blur, motion blur, fog, brightness,\ncontrast, frost, JPEG compression. We apply a unique corruption-severity pair to all samples of the\nfirst 50 clients."}, {"title": "Non-i.i.d. Partitioning.", "content": "On CIFAR and TinyImageNet datasets, we distribute the proportion of samples of class C across M\nclients according to a Dirichlet distribution: $q_{c, m} \\sim Dir_M(\\alpha)$, where we consider $\\alpha \\in (0.1, 0.5)$ as\nin [30].\nWe provide a visualization of Dirichlet partitioning strategies on CIFAR10 below. The size of each\npoint represents the number of allocated samples. Notably, as $\\alpha$ increases, Dir($\\alpha$) becomes less\nheterogeneous."}, {"title": "Training Settings", "content": "All methods are trained using mini-batch SGD for 200 global rounds with 5 local epochs of training.\nWe use a fixed learning rate of 0.01, momentum of 0.5, and weight decay of 5e-4. The batch size is\nset to 50 for all experiments, except for EMNIST, where we use a batch size of 16. We sample the\nset of active clients uniformly with probability q=0.3 for CIFAR and TinyImageNet and q=0.03 for\nEMNIST. The last global round of training employs full client participation. We split the data of each\nclient 80-20% between training and testing."}, {"title": "Additional Results", "content": "Multi-Domain FL\nIn Table 7, we present results on the DIGIT-5 domain generalization benchmark [53]. This presents\nan alternate form of covariate shift, as the data from each client is drawn from one of 5 datasets\n(SVHN, USPS, SynthDigits, MNIST-M, and MNIST). In particular, we use 20 clients trained with\nfull participation, and assign 4 clients to each domain. Within each domain, we use the Dirichlet(0.5)\npartitioning strategy to assign data to each client. We observe that pFedFDA is effective in all settings,\nbut has the most significant benefits over prior work in the low-data regime."}, {"title": "Effect of Local Epochs", "content": "In many FL settings, we would like clients to perform more local training between rounds to reduce\ncommunication costs. However, too much local training can cause the model to diverge. In Fig. 4,\nwe compare the effect of the local amount of epochs for CIFAR100 and CIFAR100-S-25% sample\ndatasets. We observe that (1) pFedFDA outperforms FedAvgFT at all equivalent budgets of E, (2)"}, {"title": "Communication Load Examples", "content": "In Table 8, we compare the number of distinct parameters in our Gaussian estimates to that of a\ntypical linear classifier for the models and datasets used in this paper, along with some additional\nexamples. We display the resulting overhead relative to the base parameter count of the shared\nrepresentation backbone."}, {"title": "Runtime of Method Components", "content": "In Table 9, we evaluate the proportion of each local iteration of pFedFDA associated with each line of\nour algorithm. Network Passes refers to the time taken to train the base network parameters $\\phi$ (Line 7\nof Alg. 1). Mean/Covariance Est. refers to the time taken to estimate the local mean and covariance\nfrom features extracted during model training (Line 8 of Alg. 1). Interpolation Optimization refers\nto the time taken to optimize the local coefficient $\\beta$ (Line 9 of Alg. 1). Overall, we find that the\nmajority of the overhead of our method comes from estimating the interpolation parameter $\\beta$."}]}