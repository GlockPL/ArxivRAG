{"title": "Estimating Neural Network Robustness via Lipschitz\nConstant and Architecture Sensitivity", "authors": ["Abulikemu Abuduweili", "Changliu Liu"], "abstract": "Ensuring neural network robustness is essential for the safe and reli-\nable operation of robotic learning systems, especially in perception and decision-\nmaking tasks within real-world environments. This paper investigates the robust-\nness of neural networks in perception systems, specifically examining their sen-\nsitivity to targeted, small-scale perturbations. We identify the Lipschitz constant\nas a key metric for quantifying and enhancing network robustness. We derive an\nanalytical expression to compute the Lipschitz constant based on neural network\narchitecture, providing a theoretical basis for estimating and improving robust-\nness. Several experiments reveal the relationship between network design, the\nLipschitz constant, and robustness, offering practical insights for developing safer,\nmore robust robot learning systems.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have been successfully applied to many tasks in robotics [1], including percep-\ntion [2], prediction [3], planning, and control [4]. However, their sensitivity to input perturbations\nposes significant challenges [5], particularly in safety-critical applications like autonomous driving\nand human-robot collaboration [6]. For instance, a small but well-designed modification to an input\nimage can cause a neural network used in perception systems to misclassify the image [7], under-\nmining its reliability in applications like autonomous driving or robotic navigation. Such issues\nraise concerns about the deployment of these models in real-world robotic systems, where safety\nand robustness are critical. In autonomous driving, adversarial examples could trigger unintended\nactions, such as incorrect path planning or object misdetection, potentially compromising the safety\nof autonomous systems.\nIn general, there are two different approaches one can evaluate the robustness of a neural network\n[8]: adversarial attack-based methods that demonstrate an upper bound, or verification-based meth-\nods that prove a lower bound. Adversarial attack approaches are easy to conduct, but the upper\nbound may not be useful [9]. Verification-based approaches, while sound, are substantially more\ndifficult to implement in practice, and all attempts have required approximations [10]. Besides the\nadversarial attack and verification-based methods for evaluating the robustness, some works study\nthe neural network's intrinsic robustness based on the Lipschitz continuity of neural network [11].\nIn deep neural networks, tight bounds on its Lipschitz constant can be extremely useful in a variety\nof applications: (1) The adversarial robustness of a neural network is closely related to its Lipschitz\ncontinuity [12]. Constraining local Lipschitz constants in neural networks is helpful in avoiding\nadversarial attacks [11]. (2) Generalization bounds critically rely on the Lipschitz constant of the\nneural networks in deep learning theory[13]. In these applications and many others, it is essential to\nestimate the Lipschitz constant both accurately and efficiently.\nVarious approaches have been proposed to measure and control the Lipschitz constant of neural\nnetworks [14]. Early work by Szegedy et al. [5] introduced an upper bound based on spectral norms"}, {"title": "2 Related Works", "content": "Robust Learning in Robotics. Robust robot learning focuses on the development of autonomous\nsystems that can operate reliably and safely within dynamic and uncertain environments [16, 17].\nVarious approaches have been implemented to enhance robustness, including safe reinforcement\nlearning, which aims to optimize policies while ensuring safety constraints [18, 19], and techniques\nlike domain adaptation [20] and online adaptation [21, 22], which facilitate system adaptability\nacross diverse environments. Furthermore, methods such as adversarial training and imposing con-\nstraints on the Lipschitz constant have shown promise in enhancing the robustness of robot learning\nsystems by mitigating vulnerability to environmental variations and adversarial inputs [23, 11].\nAdversarial Training and Verification. Adversarial training is a commonly used technique for\nimproving the robustness of a neural network [7]. However, while such an adversarially trained\nnetwork is made robust to some attacks in training, it can still be vulnerable to unseen attacks.\nThen neural Network Verification can be used to prove a lower bound on the robustness of neural\nnetworks [10]. Certified robust training under verification, focusing on training neural networks\nwith certified and provable robustness \u2013 the network is considered robust on an example if and only\nif the prediction is provably correct for any perturbation in a predefined set [24, 25].\nLipschitz constant and Robustness. Different from certified robust training, some researchers\nbound the sensitivity of the function to input perturbations by bounding the Lipschitz constant to\ncertify or improve the robustness of neural networks [11]. Weng et al. [26] convert the robustness\nanalysis problem into a local Lipschitz constant estimation problem. Designing and training neural\nnetworks with bounded Lipschitz constant is a promising way to obtain certifiably robust classifiers\n[27]. Many works handle the Lipschitz bound by leveraging specific mathematical properties such\nas the spectral norm [28, 29]. Unlike previous studies, this work focuses on deriving an analytical\nexpression for the Lipschitz constant based on the architecture of a neural network. We investigate\nthe relationship between neural network architecture and robustness. While prior research typically\ncalculates the Lipschitz constant using exact network parameters and numerical methods, our ap-\nproach emphasizes the role of architecture rather than specific parameters in estimating the Lipschitz\nconstant."}, {"title": "3 Lipschitz Continuity of Neural Networks", "content": "Notation. We use boldface letters to denote vectors (e.g., x) or vector functions (e.g., f), and use\nxi or fi to denote its i-th element. We use capital letters to denote matrices (e.g., W), and use\nWij to denote the element of i-th row and j-th column. For a unary function \u03c3, \u03c3(\u03b1) applies\n\u03c3(\u00b7) element-wise on vector \u00e6. The lp-norm (p \u2265 1) and l\u221e-norm of a vector \u00e6 are defined as\n$||x||_p = (\\sum_i|x_i|^P)^{\\frac{1}{p}}$ and $||x||_\\infty = \\max_i |x_i|$, respectively. In the following, we consider a real-\nvalued function $f : R^n \\rightarrow R^m$. We mainly consider perturbation in lp norm, e.g. ||8||p.\n3.1 Neural Networks\nIn this work, we consider standard neural networks composed of affine layers (such as multilayer\nperceptrons or convolutional layers) combined with element-wise activation functions.\n$x^{(l)} = \\sigma^{(l)} (x^{(l)}), x^{(l)} = W^{(l)}x^{(l)-1} + b^{(l)}$\nHere M is the number of layers and usually $o^{(M)}(x) = x$ is the identity function. The network\ntakes \u00e6(0) := x as the input and outputs x(M) := y. We use f to denote the whole neural network\nfunctions:\n$f(x) = (1) (W(1) (... \u03c3(1) (W(1)x + b(1))) + b(t))$\nIn this work, we mainly consider the classification tasks. Let x \u2208 Rn be an input vector of a\nm-class classification function $f : R^n \\rightarrow R^m$. The predicted logits (with softmax activation) is\nY = [Y1, Y2,\u2026, \u0177m] = f(x), and the predicted class is given as $c(x) = arg \\max_{1<i<m} Y_i$.\n3.2 Lipschitz Continuous\nDefinition 1 (Lipschitz continuous.) A function $f : R^n \\rightarrow R^m$ is called L-Lipschitz continuous\nw.r.t. norm || . || if there exists a constant L for any pair of inputs x, y \u2208 Rm, such that\n$||f(x) - f(y)|| \\leq L||x - y||$.\nThe smallest L for which the previous inequality is true is called the Lipschitz constant of f. For\nlocal Lipschitz functions (i.e. functions whose restriction to some neighborhood around any point is\nLipschitz), the Lipschitz constant may be computed using its differential operator.\nTheorem 1 (Rademacher [30], Theorem 3.1.6) If $f : R^n \\rightarrow R^m$ is is a Lipschitz continuous\nfunction, and f is differentiable almost everywhere, then\n$L = sup || \\nabla f(x)||$,\nwhere J := $\\frac{\\partial f}{\\partial x}(x)$ is the Jacobian matrix, and $||J|| = sup_{u,||u||=1} ||Ju||$ is the operator norm\nof the Jacobian matrix. According to the Min-max principle for singular values [31], the largest\nsingular value smax (M) of a Matrix M is equal to the operator norm: $S_{max}(M) = ||M||$. Then we\nhave the following proposition.\nProposition 1. (Estimating the Lipschitz constant.) Lipschitz constant L of a function f can be\nestimated by the largest singular value of its Jacobian $J = \\nabla f(x)$:\n$L = ||J|| = S_{max} (J)$.\n3.3 Lipschitz Constant and Robustness\nIf the neural network f has a small Lipschitz constant L, then L-Lipschitz continuity implies that\nthe change of network output can be strictly controlled under input perturbations.\nDefinition 2 (Perturbed example and adversarial example.) Let x \u2208 R be an input vector of a clas-\nsification function $f : R^n \\rightarrow R^m$, and the predicted class is given as $c(x) = arg \\max_{1<i<m} Y_i, Y_i =$"}, {"title": "3.4 Maximum Singular Value of Random Matrices", "content": "$f_i(x)$. Given x, we say $x_a = x + \\delta$ is a perturbed example of \u00e6 with noise $\\delta \\in R^n$ under lp per-\nturbation $||\\delta||_p = \\epsilon$. An adversarial example is a perturbed example $x_a$ that changes the predicted\nclass c(x).\nDefinition 3 (Margin of a prediction.) The margin of a prediction denotes the difference between\nthe largest and second-largest output logits:\n$margin(f(x)) = max(0, Y_t - \\max_{i \\neq t} y_i)$,\nwhere y = [Y1, Y2,\u2026] = f(x) is the predicted logits from the model f on data point \u00e6. yt is\nthe correct logit (x belongs to t-th class). We assume that the original prediction is correct: i.e.\n$Y_t = arg \\max_i Y_i$. According to the results from Li et al. [32], we have the sufficient condition for\na data point to be provably robust to perturbation-based adversarial examples:\nTheorem 2 (Li [32]) If $\\frac{2}{\\sqrt{2}} \u00b7 L \u00b7 \\epsilon < margin(f(x))$, where f is a L-Lipschitz continuous function\nunder lp norm, then \u00e6 is robust to any input perturbation \u03b4 with $||\\delta||_p \\leq \\epsilon$.\nSpecifically, we have a proposition for 12 or lo perturbation by simply letting p = 2 or p \u221ein\nTheorem 2.\nProposition 2. (Certified Robustness of Lipschitz networks.) For a neural network classifier\n$f: R^n \\rightarrow R^m$ with Lipschitz constant L. Then the neural network classifier is provably ro-\nbust under 12 perturbation $||\\delta||_2 < \\frac{margin(f(x))}{\\frac{L}{\\sqrt{2}}}$ or provably robust under lo perturbation\n$||\\delta||_{\\infty} \\leq \\frac{margin(f(x))}{2L}$.\nAs demonstrated in proposition 2, the Lipschitz constant determines the certified robustness of a\nneural network classifier unde lp norm perturbation. Our goal is to derive an analytical expression\nfor the Lipschitz constant based on a given neural network architecture and explore the relationship\nbetween the architecture and robustness. According to 3.2, the Lipschitz constant corresponds to the\nlargest singular value of the neural network's Jacobian matrix. Thus, the challenge is to estimate this\nlargest singular value. In this study, we propose an approximation method using Random Matrix\nTheory (RMT) [33] to estimate the Lipschitz constant. Although this method does not yield the\nexact value of the Lipschitz constant, the approximation remains valuable for exploring its analytical\nrelationship with neural network architectures.\nTheorem 3 (Rudelson [34]). Let A be an N \u00d7 n random matrix whose entries are independent\ncopies of some random variable with zero mean, unit variance, and finite fourth moment. Suppose\nthat the dimensions N and n grow to infinity while the aspect ratio n/N converges to some number\ny\u2208 (0,1], n/N \u2192 y. Then the maximum singular value smax(A) converges to:\n$\\frac{1}{\\sqrt{N}} S_{max}(A) \\rightarrow 1+ \\sqrt{\\frac{n}{N}}$ almost surely.\nThus, asymptotically, the expectation of the maximum singular value is $E_{S_{max}(A)} \\sim \\sqrt{N}+ \\sqrt{n}$.\nFor the maximum singular value of the product of matrix A and a scaler \u03b1, we have:\n$S_{max}(\\alpha A) = ||\\alpha A|| = \\alpha ||A|| = \\alpha S_{max}(A)$.\nEquation (7) provides the maximum singular value for matrices with unit variance. If the variance\nis a\u00b2, it can be approximated by scaling the maximum singular value of the unit variance matrix by\na. Thus, we propose the following expression for the maximum singular value of random matrices\nwith variance a\u00b2.\nProposition 3. (Maximum singular value of a random matrix.) Let A be an N \u00d7 n random matrix\nwhose entries are independent copies of some random variable with zero means, a\u00b2 variance, and\nf finite fourth moment. The expectation of the maximum singular value can be approximated by\n$E_{S_{max}(A)} \\approx \\alpha (\\sqrt{N}+ \\sqrt{n})$."}, {"title": "3.5 Variance of the Jacobian", "content": "Proposition 3 provides an approximation method for estimating the singular value of a random\nmatrix. The Jacobian matrix of a neural network is random at initialization due to the randomly\ninitialized network parameters. Therefore, at least during initialization, we can estimate the Lips-\nchitz constant of a neural network using Propositions 3 and 1. Furthermore, even during training,\nthe network parameters remain nearly random for wide neural networks, based on neural tangent\nkernel analysis [35]. Thus, in this subsection, we estimate certain statistical properties (e.g., mean\nand variance) of the Jacobian matrix, which can be used to approximate the Lipschitz constant.\nAs an illustration, we first consider a multilayer perceptron (MLP) with one hidden layer, as shown in\nfig. 1. The analysis can be extended to MLPs with additional hidden layers due to the application of\nthe chain rule for derivatives. In fig. 1, n represents the input dimension, m is the output dimension,\nand d is the hidden dimension. The network parameters include weights W(1), W(2), and biases\nb(1), b(2). The activation function is denoted as o(\u00b7). We initialize these parameters using Xavier\ninitialization [36]:\nW ~ N(0, ), 6(1) = 0,\nW2) ~ N(0, ), 6(2) = 0,\nWe can compute the expectation and variance of the Jacobian as follows. Please refer to appendix A\nfor further details.\nE[Ji,j] = \u2211E[W]E[\u03c3\u2032(\u00b9)]E[W]1 = 0\nVAR[Ji,j] = VAR[W]VAR[\u03c3\u2032((1))]VAR[W] =a4q2\nwhere q\u00b2 = VAR[\u03c3\u2032(x)] denotes the variance of random variables \u03c3\u2032(x), when x ~ N(0, 1).For\nexample, with the ReLU activation function, q\u00b2 = VAR[\u03c3\u2032(x)] = 1 when x follows a standard\nnormal distribution. By applying the chain rule of derivatives, this approach can be extended to\ncompute the variance of the Jacobian matrix for any M-layer network, leading to the following\nproposition.\nProposition 4. (Expectation and Variance of a Jacobian matrix.) Let f denote the M-layer neural\nnetwork with input dimension n, output dimension m, and hidden dimension d. The Jacobian matrix\nis denoted as J = $\\frac{\\partial f}{\\partial x}(x)$ Assume the neural network is initialized using Xavier initialization. Then,\nthe expectation and variance of the Jacobian can be estimated as:\nE[Ji,j] = 0, VAR[Ji,j] =(d+n)(d+m) a2Mq2M-2,\nwhere q\u00b2 = VAR[\u03c3'(x)] denotes the variance of o'(x) for x ~ N(0, 1). For ReLU, q.\n3.6 Lipschitz Constant and Robustness of Neural Networks\nBy combining Proposition 4, Proposition 3, and Proposition 1, we derive an analytical expression\nfor the Lipschitz constant of neural networks"}, {"title": "4 Experiments", "content": "One of the contributions of this work is the proposal of an analytical expression for the Lipschitz\nconstant of neural networks, as given in eq. (13). In this section, we evaluate the accuracy of this\nanalytical expression through toy experiments. We design various neural network architectures with\ndifferent numbers of layers M, hidden dimensions d, and weight variances a\u00b2. At initialization, we\nestimate the Lipschitz constant and compare the analytical estimation from eq. (13) with numerical\nmeasurements obtained using bound propagation [15]. The numerical method provides an accurate\napproximation of the Lipschitz constant. To minimize the impact of randomness, we conduct 10\ntrials for each neural network configuration and report the average results. Additionally, we apply\nmoving average smoothing to filter the curves for better clarity."}, {"title": "5 Conclusions", "content": "Ensuring the robustness of neural networks is essential for the safe and reliable operation of robot\nlearning systems in real-world environments. In this work, we investigated the relationship between\nneural network architecture and robustness. We proposed an analytical expression for estimating the\nLipschitz constant of neural networks. This expression allows us to evaluate the Lipschitz constant\nand investigate its connection to network architecture and robustness. Our analytical expression and\nexperimental results suggest that shallower, wider networks tend to exhibit greater robustness.\nLimitations and Future work. 1) This study focuses solely on multilayer perceptrons (MLPs). In\nfuture work, we plan to extend our analysis to other network architectures, such as convolutional\nneural networks (CNNs) and networks with residual connections. 2) The experiments were con-\nducted on the MNIST image classification task. In future research, we aim to test our methods on\nmore complex perception tasks and other robot-learning applications 3) Our current analysis primar-\nily centers on the Lipschitz constant as a key metric for evaluating robustness. However, as noted in\nProposition 2, robustness is also affected by the margin of predictions (i.e., the confidence level of\nthe model's outputs). In future studies, we intend to incorporate the margin of predictions into our\nrobustness analysis and expand our experiments to more complex datasets."}]}