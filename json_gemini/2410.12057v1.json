{"title": "Large-scale cloze evaluation reveals that token prediction tasks are neither lexically nor semantically aligned", "authors": ["Cassandra L. Jacobs", "Lo\u00efc Grobol", "Alvin Tsang"], "abstract": "In this work we compare the generative behavior at the next token prediction level in several language models by comparing them to human productions in the cloze task. We find that while large models trained for longer are typically better estimators of human productions, but they reliably under-estimate the probabilities of human responses, over-rank rare responses, under-rank top responses, and produce highly distinct semantic spaces. Altogether, this work demonstrates in a tractable, interpretable domain that LM generations can not be used as replacements of or models of the cloze task.", "sections": [{"title": "Introduction", "content": "Many language models are trained to a cloze-like objective (Taylor, 1953), in which they perform next-word prediction (NWP) or masked language modeling (MLM), producing a distribution of probabilities of these tokens given the surrounding context. Effective language models (LMs) assign the greatest probability to words using the surrounding context. Humans similarly can perform this cloze task, such as when we try to guess what others are about to say. By aggregating across many human responses to produce a proportion, one can also obtain a probability distribution over words that fit to a given linguistic context or situation. Despite major advances in word prediction in context, however, it is generally under-stood that token prediction tasks cannot fully capture all of the factors that affect human responses, such as real-world knowledge, or syntactic relationships, even when fine tuned (Bender and Koller, 2020). Cloze productions, while instrumental in understanding lan-guage processing, are nevertheless poorly understood as a means of evaluating LM outputs.\nGiven the continued gap in understanding what makes generated \u201clanguage\u201d natural and human-like, this work aims to better characterize the linguistic-distributional and semantic properties that distinguish LMs from humans in cloze-like tasks. By better understanding the properties of human productions from a probabilistic perspective, we believe it is possible to bridge the gap between LMs and humans.\nHere\u00b9 we compare machine-generated predictions for sentence-final completions against a large-scale human cloze response dataset (Peelle et al., 2020). Such a cloze dataset allows us to specifically target single-token generations, which allows us to sidestep the high degree of uncertainty in multiword utterance production, and the many algorithms used to solve these problems (Giulianelli et al., 2023). That is, not only is multiword utterance production a computa-tionally intractable problem, but different models use different parameters and decoding strategies for the production of longer texts \u2013 effectively dynamical sys-tems \u2013 which makes them difficult to compare and an-alyze. Furthermore, selecting high-quality multiword utterances depends on accurately estimating the proba-bilities of words given the prior context, leading to the explosion of error (Valmari, 1998). Systems that can-not perform single word production cannot be trusted to generate longer, more complex language more ac-curately. We therefore present analyses of single-word and single-token response data to provide an estimate of the best case scenario for language modeling."}, {"title": "Human cloze and language modeling", "content": "The present work examines single-word production in cloze tasks to compare human and LM productions on an identical task. Note that the term \"cloze\" is used in psycholinguistics and natural language processing with close but distinct meanings. It always consists of a task setting where an agent (either a human subject or a computer model) is given a context, typically a sentence where a word has been removed, and is asked to complete it with their best guess as to what the original was. A probability distribution over\n1 All the code used to produce this analysis is available at https://github.com/University-at-Buffalo-CaLiCo-L ab/clamp/."}, {"title": "Data", "content": "We experiment on Peelle et al.'s (2020) completion norms dataset. It consists of 3085 English sentences for which human participants were asked to provide a final word, with each sentence receiving at least 100 manually validated responses in order produce reliable cloze probability estimates. The stimuli varied between eight and ten words in length, and varied in the degree of final-word predictability. The sentences are importantly long enough that LMs should be able to reliably guess appropriate words for the context.\nWe compare the cloze probabilities reported by Peelle et al. (2020) to probabilities extracted from several neural language models: GPT-2 (Radford et al., 2019), RoBERTa (Liu et al., 2019) in its \"base\" configuration, and the Pythia suite of language models (Biderman et al., 2023). All of these models use a subword vocabulary size of approximately 50000 and thus have the same capacity for long tail"}, {"title": "Experiment 1", "content": "This experiment compares GPT-2 (Radford et al., 2019), ROBERTa (Liu et al., 2019), and Pythia (Biderman et al., 2023) to understand the probabilities of next-word generations by comparing human- and LM-derived estimates. Qualitatively, the generations of ROBERTa, GPT-2, and Pythia-160M vary widely in the content of their predictions. We present one example in Table 1.\nIn order to be used as models of human productions, the top LM predictions and/or their estimated proba-bilities should at the very least be consistent with ob-served human responses to the same cloze challenge. However, Figure 1 illustrates that LMs most often as-sign too little probability to human generations (Holtz-man et al., 2021), with most models under-estimating cloze probabilities by several orders of magnitude.\nThese results also reveal that the item-level correspondence between LM and human cloze probabilities is poor, with considerable variance around the trendline and evidence of non-normality (Franke et al., 2024). Taken together, these findings suggest that LMs are generally \"right for the wrong reasons\u201d with respect to questions of predictive power (Giulianelli et al., 2024; Shain et al., 2024).\nWe note that the probabilities and correlations"}, {"title": "Experiment 2", "content": "Experiment 1 identified significant problems in the use of LM-based probabilities, but the selection of next words is largely one of ranking different candi-dates. One could reason that relying only on proba-bilities is unfair to language models, with generally reasonable rankings but poor estimation of the proba-bility of next words due to data or training objective differents. For simplicity, we examined whether the Pythia-160M model (most similar in size to the state-of-the-art GPT-2; Biderman et al. 2023; Shain et al. 2024) was also miscalibrated in its rankings of human"}, {"title": "Experiment 3", "content": "This experiment assesses the contribution of learning in LMs to fit to human data. To assess the influence of model size and training progress on the correlation between NLM and cloze rank distributions, we experiment with the Pythia suite (Biderman et al., 2023), a series of Transformer-based NLM spanning a wide range of model sizes for a single architecture, trained on the same data and with publicly available intermediate checkpoints. For comparisons based on model size, we use all the models up to and including Pythia-2.8B, in standard and \u201cdeduped\u201d versions both. For comparisons based on training progress, we use checkpoints of Pythia-160M at five training"}, {"title": "Experiment 4", "content": "The results of Experiments 1 and 2 showed modest correlations between human- and machine-generated responses in a cloze task for approximating the proba-bilities and ranks of human responses, replicating pre-vious work showing deficiencies in next-word predic-tion (Holtzman et al., 2020; Vaidya et al., 2023; Botch and Finn, 2024; Klein et al., 2024). Moreover, the improvements in fit to cloze probabilities from greater"}, {"title": "Limitations", "content": "In this work, we evaluate only a single dataset to understand the correspondence between cloze probabilities and language model estimates, which may limit the ability to draw conclusions about the nature of the relationship for all cloze tasks and datasets. Some of the cloze datasets that we did not examine include word-by-word cloze completions that allow researchers to assess the probabilities of words at each stage in sentence processing. Additionally, different subword tokenization strategies can have an impact on estimates, and our approach to select only word-initial subwords may inflate estimates of next-word probabilities (Oh and Schuler, 2024). Finally, it is common in the literature to report A log likelihood instead of correlation coefficients; we do not report this here because we do not do model comparison, as we are concerned entirely with unbiased generations. However, future work should compare and contrast these approaches directly."}, {"title": "Ethical Considerations", "content": "The Peelle et al. (2020) data were gathered using crowdsourcing on Mechanical Turk. The original researchers obtained IRB approval for their research study. Some of the human responses, and many of the LM predictions, contain profanity, sexually explicit content, or other offensive content. Furthermore, LMs in general are likely to produce sexist, ableist, and racist responses even when guardrails are implemented. Researchers seeking to evaluate LM outputs using human judgments should be aware of the potential for harmful material in these outputs."}]}