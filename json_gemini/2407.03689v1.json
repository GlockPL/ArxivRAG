{"title": "Text2TimeSeries: Enhancing Financial Forecasting through Time Series Prediction Updates with Event-Driven Insights from Large Language Models", "authors": ["Litton Jose Kurisinkel", "Pruthwik Mishra", "Yue Zhang"], "abstract": "Time series models, typically trained on numerical data, are designed to forecast future values. These models often rely on weighted averaging techniques over time intervals. However, real-world time series data is seldom isolated and is frequently influenced by non-numeric factors. For instance, stock price fluctuations are impacted by daily random events in the broader world, with each event exerting a unique influence on price signals. Previously, forecasts in financial markets have been approached in two main ways: either as time-series problems over price sequence or sentiment analysis tasks. The sentiment analysis tasks aim to determine whether news events will have a positive or negative impact on stock prices, often categorizing them into discrete labels. Recognizing the need for a more comprehensive approach to accurately model time series prediction, we propose a collaborative modeling framework that incorporates textual information about relevant events for predictions. Specifically, we leverage the intuition of large language models about future changes to update real number time series predictions. We evaluated the effectiveness of our approach on financial market data.", "sections": [{"title": "Introduction", "content": "In the rapidly evolving field of global finance, Artificial Intelligence (AI) plays a pivotal role. In an interconnected world characterized by cross-border trade and expanding economies, marked by intricate relationships and interdependencies, AI is essential for navigating these complexities Cao [2022]. Predicting stock price movements has been a long-standing focus for the AI community, as the stock market is highly sensitive to macroeconomic events, making accurate forecasting a significant challenge. Historically, research has primarily concentrated on forecasting financial markets using univariate time series prediction methods Wah and Qian [2002]. Some studies have addressed this issue by employing multivariate time series prediction or by considering the interdependence of price series from different companies to forecast price movements Wu et al. [2013], Xiang et al. [2022]. While time series models are effective at predicting cyclical trends and overall market growth Zhou et al. [2022], Woo et al. [2022], they often fail to capture the impact of sequential financial events. Predictions that do not consider such events tend to be less precise. The current work explores time series prediction of stock prices in a multi-modal setting that incorporates both text and time series data, where the textual description of an event is considered for short-term price prediction.\nEvent-driven stock sentiment prediction primarily focuses on anticipating how an event will affect stock prices, typically classifying the impact into discrete labels such as increase, decrease, or no"}, {"title": "Related Work", "content": "Methods for Time Series Analysis. Recent advancements in deep learning architectures, such as Long Short-Term Memory (LSTM) networks Hochreiter and Schmidhuber [1997], Gated Recurrent Units (GRU) Chung et al. [2014], and transformers Vaswani et al. [2017], have demonstrated significant capabilities in capturing complex temporal relationships within time series data. Various transformer models have been proposed Li et al. [2019], Zhou et al. [2021a], Wu et al. [2021], Zhou et al. [2022], Liu et al. [2021] for forecasting time series, often designing novel attention mechanisms to handle longer sequences and using point-wise attention, which can overlook the importance of patches. Although Triformer Cirstea et al. [2022] introduces patch attention, it does not use patch inputs. Patch Time Series Transformer (Patch TST) Nie et al. [2022] was the first transformer model to use patches as inputs, capturing the semantic coherence among neighboring patches. However, these techniques cannot be directly adapted to a multimodal setting involving textual information. Our current work investigates time series prediction in a multimodal setting, comprising both time series and textual information.\nTime Series Analysis for Stock Prediction.\nSeveral time series analysis methods and machine learning techniques can be applied for stock prediction. These include ARIMA models, Exponential Smoothing State Space models (ETS) Brown [1956], and machine learning techniques such as linear regression, decision trees, random forest, SVM, gradient boosting, Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models Tse and Tsui [2002], Engle [2002], and ensemble methods involving multiple models. Hu et al. [2018] developed a hybrid attention mechanism to predict stock market movements using news articles, while BERT representations have been used to encode texts for the FEARS index Da et al. [2011] in predicting movements in the S&P 500 index Yang et al. [2019]. However, these techniques are typically adapted to handle information derived from a sequence of financial events, which can result in inaccurate predictions during unforeseen events that impact financial decisions. Our approach models time series prediction in a multimodal setting, where predictions are evaluated in the context of specific events.\nNLP for Finance. Financial services have always been tightly regulated by governments due to their pervasive impact on the masses. However, following liberalization and the easing of regulations, financial technology (FinTech) has emerged as one of the top business avenues in the last decade. Chen et al. [2020] highlights the application areas of NLP in the finance domain. Financial institutions use end-to-end transformer models to scan and extract financial events from various news articles and financial announcements Zheng et al. [2019], evaluating the debt-paying ability of corporate customers. Online forums, blogs, and social media posts are monitored to extract sentiment, which is then used to predict company sales using model-agnostic meta-learning methods Lin et al. [2019], Finn et al. [2017]. Similarly, insurance companies track daily posts from customers to detect and initiate early treatment of diseases Losada et al. [2019], Burdisso et al. [2019], mitigating the chances of hazards. Social media posts also serve as indicators for stock recommendations Tsai et al. [2019]. Most of these works are formulated as simple sentiment label predictions, which may not fully capture the complexity of financial events. Therefore, instead of assigning a limited number of sentiment polarities to an event, we model the effects of the event in terms of change directions with associated real values. Our current work investigates methods to convert market excitement related to events into real-valued stock prices over the subsequent n days."}, {"title": "TimeS: Overall Method", "content": "Our objective is to forecast the impact of an event on the price signal of a stock for the next n time units and adjust the prediction of our time series model accordingly. Let's break down the task into three steps.\n1. $P_s[t : t + n] \\leftarrow T_s(P_s[t : t - h]; \\theta_1)$\n2. $\\Delta P_s[t: t + n] \\leftarrow F(E, s; \\theta_2)$\n3. $P'[t: t + n] \\leftarrow U(\\Delta P_s[t : t + n], P_s[t : t + n]; \\theta_3)$\nWhere, $T_s$ represents the time series function which takes the historic price of a specific stock $s$ for the previous $h$ time points as an argument and forecasts its future values for $n$ time units. $F$ denotes a function predicting the impact of an event $E$ on the price of stock $s$ for the subsequent $n$ time units from the point of occurrence of the event. Finally, $U$ signifies an update function that takes outputs from $T_s$ and $F$, adjusting the time signal for the upcoming $n$ time- steps by amplifying or attenuating it. we commence by training a dedicated time-series model, denoted as $T_s$, for each individual stock $s$. This model is designed to project the trajectory and expansion of the stock over the subsequent $n$ days, leveraging prices derived from the preceding $h$ days as its input. Central to our approach is the utilization of function $F$ within the problem formulation, tasked with assessing the influence of specific event, represented as $E$, on the market sentiment surrounding stock $s$. We conceptualize this process as a state transition problem, aimed at depicting the stock's behavior over the ensuing $n$ days following the occurrence of an event. Within this framework, we quantify the extent of amplification or attenuation in the stock price for each future day, predicated on its corresponding stock state. The state transition and prediction are guided by the intuition of an LLM regarding the patterns of future price changes of the stock within the context of the event. Following this assessment, we implement an update mechanism denoted as $U$ to refine the predictions generated by the time-series model, integrating insights into amplification or attenuation derived from the preceding analysis. Notably, while each stock is assigned its own $T_s$, the other components remain consistent across all stocks. The rationale behind this strategic design choice will be explained in subsequent discussions."}, {"title": "T:Time Series Model", "content": "Time series models are trained to predict the values for next n time points by taking previous h time point values. Our time series model can be represented as follows.\n$P_s[t: t + n] = T_s(H_s[t: t -h]) \t\t\t\t\t\t(1)$\nWhere $P_s[t: t + n]$ is the price of the stock $s$ for next $n$ time points from the current time $t$. $H_s[t: t-h]$ is a multivariate sequence of historic data of previous $h$ time points. The multivariate sequence contains a parallel sequence such as stock prices of the stock, different index values, or exchange rates which can play a role in modeling general market tendencies and its effect on price of $s$."}, {"title": "F:Stock state computation using Indicators Predicted by large Language Models", "content": "LLMs trained on text data could intuitively grasp stock price movements across various future time spans, albeit without predicting exact values. For the purpose we fine-tune large language models to predict stock predict stock price trend as discrete labels containing the intuition of large models regarding price change of stock for next n days as follows,\n$I_{s,1}, I_{s,2},..., I_{s,n} = LLM_{stock}(E, S) \t\t\t\t\t(2)$\nThe process of fine-tuning to produce these price change labels is explained in the Appendix D. We calculate the stock state transition using a Gated Recurrent Unit initialized with the embedding $Emb(s)$ of the stock $s$, which takes the corresponding LLM-predicted label $I_{s,t}$ at each time-step $t$ to produce temporal state $S_t$ of the stock $s$.\nAmplification Prediction using Temporal stock State $S_t$ The time series can be viewed as a random walk in the 2D grid as shown in Figure 3. At any point of time, it takes any of the three directions namely increase, decrease, or stay steady which could be represented by direction indicator values 1, -1, 0 respectively. We use the stock states compute the probability for time series to take each of the directions, increase, steady, or decrease. The expected value direction indicator is computed using these probabilities represent the amplification/attenuation value which can be subsequently used to update the time series. With this view in mind, we compute the price amplification/attenuation from stock state $S_t$ at time step $t$ as follows.\n$ProbD_t = W_a. S_t\t\t\t\t\t(3)$\n$A_{st} = (1) * ProbD_t[0] + (-1) * ProbD_t[2] + (0) * ProbD_t[1] \t(4)$\nWhere $W_a$ is a parameter matrix and $ProbD_t$ belongs to $R^3$ which contains the probablity for increase, decrease, and neutral. $A_{st}$ is the amplification or attenuation value. We concatenate the $A_{s1}$ to $A_{sn}$ to form the amplification vector $A_s[1 : n] \\in R^n$."}, {"title": "U: Updating time Series Price Predictions", "content": "Once we compute $A_s[1 : n]$, we use it to update the values predicted by time series model $T_s$. We take a simple linear transformation of the concatenated vector $[A_s[1 : n], P_s[t : t + n]]$ to predict the update price of stock $S$ in the context of the event $E$.\n$P'[t: t + n] = W_a \\cdot [a * A_s[1 : n], P_s[t: t + n]]\t\t\t\t\t\t(5)$\n$P_s[t: t + n]$ is the price predictions by the time series model as represented by the Equation 1 and $a$ is a hyper-parameter.\nLoss: We opt for Mean Squared Error (MSE) loss to quantify the disparity between the prediction and the actual values. The loss is computed as the MSE loss between updated price $P'[t]$ and expected price $P^{as}[t]$."}, {"title": "Experiments", "content": "Our primary objective is to enhance time series predictions in response to events using a large language model (LLM). As illustrated in Figure 2, our method integrates several key components: a time series model, an LLM trained to predict stock price changes over various future time spans as discrete labels, and mechanisms for updating the time series based on the LLM's predictions. This section details the data, settings, and results for the following tasks: 1) Sub Task1: Training the time series models, 2) Sub Task2: Fine-tuning the LLM for price change prediction, and 3) Main Task1: Overall approach for updating the time series using the LLM's predicted labels, as depicted in Figure 2."}, {"title": "Datasets", "content": "ExtEDT: Extended EDT Dataset with News Events and Time Series Data Our experimentation utilized the EDT Dataset, serving as the foundational resource Zhou et al. [2021b]. This dataset comprises stock tickers, with each entry corresponding to a specific company's stock, accompanied by a textual description of a company-related news event and the event's date of occurrence. To enable a detailed evaluation, we partitioned the dataset into small-cap, mid-cap, and large-cap stocks. In order to tailor the dataset to our task, we retrieved the closing price of each stock for the subsequent n days following the event using the Yahoo Finance API 1. Additionally, we automatically annotated the price change labels for future n days, for each event within every record, adhering to the methodology outlined in Appendix D. The EDT dataset is divided into training, validation, and test sets, containing 46397, 5210, and 5263 samples. To create these partitions, we allocate ticker-wise samples in an 80:10:10 ratio.\nDataset: Training Time Series Models The focus of the present paper is on updating Time series models trained on long-term stock price sequences. As previously stated, we chose to train separate time series models for each stock available in the EDT dataset. To achieve this, we gathered time series data of closing prices for each stock over the past 30 years, along with the corresponding values for the dollar exchange index and NASDAQ exchange index using yahoo Finance API2. For every stock, we amalgamated these sequences to form a multivariate time series. This multivariate sequence is then divided into different source and target sequences with fixed source length, target length, and stride values. The input comprises the NASDAQ index, dollar exchange rate, and stock price sequence, while the output is a univariate sequence of stock prices. More details of training individual time series models can be found in Appendix A.1"}, {"title": "Fine tuning LLM for Price Change Label Prediction", "content": "This task is modeled as a sequence-to-sequence prediction task where the input is a news event about a stock prepended with the ticker's name and the output is a sequence of price change labels. Each price change label is discrete in nature where we capture the type of the change with its actual value. The type of change can belong to any of two categories: increase (INC) and decrease (DEC). The actual change value is represented in terms of integers instead of real values. For cases where there is no change in the values, we consider that as an increment (INC) with a zero change value. One example from our dataset is shown in Table 7."}, {"title": "Settings:", "content": "We leverage three variants of T5 (Text-To-Text-Transfer-Transformer) Raffel et al. [2020] models for the price change predictions. T5's unified framework excels at transferring knowledge from various tasks via pre-training on a massive dataset. We restrict ourselves from using newer LLMs Touvron et al. [2023a,b], Jiang et al. [2023, 2024], Le Scao et al. [2023], Li et al. [2023], Zhang et al. [2022] to avoid the potential effects of data contamination as these newer models might report overestimated performance in the test sets. We fine tune 3 variants of T5: T5-Base, T5-Large, and T5-3B. For the T5-Base model, we fine tune all its parameters whereas for larger models we fine tune on reduced sets of parameters. We freeze all the encoders layers of the T5-Large model whereas 8-bit low rank adaptation Hu et al. [2021] is applied to the T5-3B model."}, {"title": "Evaluation and Results", "content": "We evaluate the predictions at two levels. The first one deals with the performance of predicting the change type accurately whereas the second level evaluates the prediction of values. Instead of exactly matching the values, we employ a mechanism of window of values matching for this. We label a prediction correct if the value lies with in a window around the exact value. We use a windows of length 5 for the evaluation of values. For a value v, the window of length 5 is represented as the range v-5..v+5. The change type is evaluated using micro F1 score and the details of the performance of different T5 variants are presented in Table 1."}, {"title": "Main Task: Updating Time Series Prediction with Insights from LLM", "content": "We compared our approach with several state-of-the-art time series models, including variants of Patch-TST and D-Linear, to assess their effectiveness in updating time series predictions. Specifically, we adapted the Patch-TST+W and D-Linear+W variants for multi-channel input to single-channel output prediction (see Appendix A.1 for more details). Additionally, we explored a class of models based on lightweight natural language processing techniques used for stock sentiment predictions. To facilitate a fair comparison, we modified these models to create a time series-specific version that predicts future time-step values instead of sentiment labels. For more information on these settings, please refer to Appendix B."}, {"title": "Model Variants", "content": "We combined our approach TimeS depicted in Figure 2, for stock state computation and amplification prediction with different finetuned variants of T5 model mentioned in Section 4.2. For TimeS, we set the learning rate to $10^{-4}$, using the Adam optimization algorithm Kingma and Ba [2014]. During the"}, {"title": "Results", "content": "We assessed the primary task of updating time series using Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) as metrics. RMSE measures the square root of the average squared differences between predicted and actual values, while MAE represents the average of the absolute differences between predicted and actual values. The results of the updated price prediction, in the context of an event, are presented in the Table 3. Clearly, updates based on LLM-predicted indicators have improved the accuracy of the time-series predictions. In contrast, SentiEvent performed poorly compared to the LLM-based models. This disparity is likely due to the sophisticated background understanding and enhanced text comprehension capabilities of LLMs in the financial domain. The TimeS settings outperformed the TimeL settings. TimeS computes amplification in a probabilistic space, whereas TimeL approximates actual values of amplification from LLM-predicted labels. This approximation limits TimeL's ability to detect errors in LLM predictions and make the necessary adjustments in amplification computation."}, {"title": "Ablation Study", "content": ""}, {"title": "Ablation Study: Performance T5 During Increment and Decrement", "content": "From Table 4, it is evident that all the models perform better in predicting the $INC$ label while $DEC$ label prediction task is challenging for them. Table 5 depicts a picture of the performance in terms of different magnitude ranges of change values for change type predictions. We denote change values in the range of 0..15 as Low, 16..31 as Medium, and rest as Large. We can observe that the performance of all the models to predict the $DEC$ tag increase as we move from the Low to Large range of change"}, {"title": "Ablation Study : Performance During Different Range of Price Variations", "content": "Table 6 represents the prediction accuracies for change values belonging to different categories as mentioned above. It is challenging for all the models to accurately predict the change values when change values are large while smaller change values are predicted with high precision. However, T5 models appear to struggle with anticipating price fluctuations during extreme shifts. For case studies on the prediction of price changes and subsequent updates to time series data, please see Appendix E."}, {"title": "Limitations", "content": "To avoid data contamination, we restrict ourselves from using newer LLMs. This results in sub- optimal predictions for change types and actual change values. The test data and the validation data contains news articles focusing on trading events from PRNewswire and Businesswire websites in the financial year of 2020-21. As T5 models were released before this duration, we could safely assume that training data of T5 did not overlap with the data considered in this research work. However, capabilities have improved tremendously in the recent past."}, {"title": "Conclusion", "content": "The paper introduces a multi-modal framework for modeling stock price time-series within the context of financial events. This framework integrates insights from large language models (LLMs), using predicted price changes as discrete labels to update the time series. This approach improves the accuracy of stock price forecasts during financial events. The paper also presents various experimental results demonstrating the ability of LLMs to anticipate price changes."}, {"title": "Appendix", "content": ""}, {"title": "Time Series Model", "content": "In this section, we describe our adaptations of the PatchTST Nie et al. [2022] and D-LinearZeng et al. [2023] time series models for handling multi-channel input to single-channel output."}, {"title": "PatchTST+W", "content": "The proposed Transformer-based model for multivariate time series forecasting and self-supervised representation learning utilizes two main methodological components: firstly, the segmentation of time series into subseries-level patches, serving as input tokens for the Transformer model. Secondly, the model adopts a channel-independent approach, where each channel represents a single univariate time series, sharing embedding and Transformer weights across all series. This methodological framework offers advantages such as retaining local semantic information in the embedding, reducing computation and memory usage quadratically, and enabling the model to attend to longer historical contexts. Outputs layers of individual channels are flattened and concatenated to project using a transformation matrix W. We utilized a patch window of 5 and set the learning rate to $10^{-4}$, employing the Adam optimization algorithm Kingma and Ba [2014]."}, {"title": "DLinear+W", "content": "In this study, the authors challenge the effectiveness of Transformer-based solutions for long-term time series forecasting (LTSF), arguing that while Transformers excel in capturing semantic correlations, their permutation-invariant self-attention mechanism leads to temporal information loss in time series modeling. They propose a simple one-layer linear model, LTSF-Linear, which surprisingly outperforms existing Transformer-based LTSF models across nine real-life datasets, highlighting the importance of preserving temporal relations. The findings suggest a need to reconsider the suitability of Transformer-based approaches for LTSF and other time series analysis tasks, potentially opening up new research directions in the field. Outputs layers of individual channels are flattened and concatenated to project using a transformation matrix W. We set the learning rate to $10^{-4}$, employing the Adam optimization algorithm Kingma and Ba [2014].\nIndividual Time series models are trained on look back window 30 and prediction length 20."}, {"title": "Why we use Different time series models for different stocks?", "content": "Different stocks exhibit unique behaviors and patterns over time, requiring the use of different time series models. This diversity arises from several factors. Firstly, volatility levels vary, with some stocks experiencing frequent and significant price fluctuations, while others remain stable. Secondly, stocks may follow distinct trends, whether upward, downward, or sideways. Additionally, seasonal patterns or cyclical trends, influenced by factors such as weather, holidays, or economic cycles, contribute to the diversity of stock behavior. Moreover, the degree of randomness or noise in stock prices varies among stocks. Furthermore, the liquidity of stocks plays a crucial role, with different levels impacting market behavior. Therefore, selecting appropriate time series models tailored to these factors is essential for effective stock analysis and forecasting."}, {"title": "SentiEvent: Base Model Settings", "content": "In the current section we explain our method $F_1$ serves to calculate the event-induced price amplification levels for stock S over the subsequent n time steps using a BERT approach. The entire method is depicted in the Figure 2"}, {"title": "F\u2081:Price Amplification Computation Using Temporal Event Embeddings and Stock States", "content": "The impact of an event on a stock's price tends to fade gradually. This fading effect differs across various stocks and event categories. Hence, in our approach denoted as $F_1$, we calculate the changes in stock states by considering the temporal representation of the event over the subsequent n time units. Rest of the methods explain $F_1$ in detail."}, {"title": "ES:Computing Stock Specific event representation", "content": "Each events impacts different stocks differently and the event details relevant for a different stocks are different. For this reason our method computes stock specific event representation encompassing the relevant information. We encode the event details using Bert model.\n$E_{bert} = bert(E)$\nTo compute the stock specific representation of the event, we use muti- head attention of stock in event bert encodings follows.\n$E_s = MultiHead(E_{bert}, Emb(S)) \t\t\t\t\t\t\t(6)$\nWhere $Emb(S)$ is the embedding of stock ticker of stock S from a look up table.\nUpdating Event Representation for Temporal Information The effect of an event on a stock changes over time. For this reason, we have to incorporate temporal changes of an event. We compute the temporal representations for $E_s$ for next n time units as $[E_{s,1}, E_{s,2}, E_{s,3}, ......, E_{s,n}]$ by adding positional embedding of the corresponding time unit to $E_s$.\nStock state transition computation and Price fluctuation Predition We compute the stock state transition using a Gated Recurrent Unit initialized with $Emb(S)$ and takes corresponding temporal event representation $E_{s,t}$ at each time- step t. Each state is used for price amplification computation and updated prices using Equations 4 and 5. We set the learning rate to $10^{-3}$, employing the Adam optimization algorithm Kingma and Ba [2014]."}, {"title": "CTimeL: A Simpler Approach without Stock States", "content": "There are time series models which yielded state of art results with embarrassingly simple one-layer linear models. Inspired by this idea we also include an simple model with temporal stock states computation for computing updated price based on the price change indicator labels predicted by $LLM_{stock}$. For this purpose, we use reverse computation of Equations 7 and 8 using the LLM predicted labels $[l_{s,1}, l_{s,2},...,l_{s,n}]$ to approximate the fractional change $\\frac{(P_{s,t}-P_{s,t-1})}{P_{s,t}}$  in the Equation 7. Such values for the entire label sequence is combined for forming the price amplification sequence. We set the learning rate to $10^{-4}$, employing the Adam optimization algorithm Kingma and Ba [2014]."}, {"title": "How we train LLM?Converting Price Change Values to Discrete Labels", "content": "For each stock-event pairs in our training set we compute discrete labels of their price change using the available price time series data for the stock, for n time steps after the event. At any time step t label $l_{s,t}$ is computed as follows,\n$l_{s,t} = \\lfloor( \\frac{(P_{s,t}-P_{s,1})}{P_{s,1}} \\times 100)/ I \\rfloor \t\t\t(7)$\n$l_{s,t} =\n\\begin{cases}\nINC_+  l_{s,t} & \\text{if } C_{s,t} > 0 \\\\\nNeutral & \\text{if } C_{s,t} = 0 \\\\\nDEC_+  l_{s,t} & \\text{if } C_{s,t} < 0\n\\end{cases} \t\t\t(8)$\nIn Equation 7, $P_{s,t}$ is the price of the stock at time-step t. The Equation 7 computes the percentage of change in price of the stock s between time steps t and 1 divided by a fractional value I and $C_{s,t}$ is computed as the floor of the subsequent value. $C_{s,t}$ can take negative values as absolute values of price change is not considered during computation. Equation 8 is used assign price change label $l_{s,t}$ for the time step t. clearly, each percentage of price change in between a fraction value of I is project to a single discrete label. For our experiments we set I=0.3. 'INC' and 'DEC' prefixes indicates whether percentage of change is in increasing or decreasing direction. Using the auto-computed price change labels for all time- steps, an LLM is trained to predict the price change labels for n time-steps for stock S after the event E. To improve predictability, we divide the n time steps into three windows, and the maximum change value within each window is taken as $P_{s,t}$ for any time-step within the window. For this reason, every time step within a given window receives the same label. Table 7 provides an example of the records used to train the LLM."}, {"title": "CASE STUDIES", "content": "Case Study 1, depicted in Figure 5, illustrates a scenario of moderate upward price movement. The accompanying news highlights the company's victory in a competition, which carries clear positive sentiments. Moreover, the time series updates are nearly accurate. In Case Study 2, also in Figure 6, a pharmaceutical company's success in a clinical trial is showcased. The market's high level of excitement can be easily inferred by a Language and Logic Model (LLM). The time series updates in this case closely approximate the trajectory of upward movement. Both Case Studies 3 and (Figures 7)represent instances of partially accurate market predictions. These involve highly volatile stocks, for which the LLM lacks information on volatility during training or inference. Towards the end of the predicted sequence, the updated time series T5+TimeS tends to be biased towards DLinear+W. Moving on to Case Study 5 in Figure 9, the stock under consideration is a low-valued, highly volatile one. The challenge for the LLM lies in accurately identifying the magnitude of price movement due to its ignorance of the stock's volatility. In Case Study 6, the event concerns operational changes within the company, signaling a potentially risky situation. Consequently, the LLM may predict a negative momentum, and the computed updated time series is nearly accurate. In Case Study 7 (Figure 11), the event revolves around a lawsuit against the company. With enough instances in the training set, the LLM can readily anticipate the magnitude of the negative trend. Finally, in Case Study 8 (Figure 12), the news relates to the quarterly results of a company. Initially appearing positive, the LLM predicts positive labels. However, the company's performance falls short in comparison to previous quarters. The LLM's limitations become apparent here, as it lacks the necessary context and capability for such numerical comparisons."}]}