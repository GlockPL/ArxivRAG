{"title": "From Text to Pose to Image: Improving Diffusion Model Control and Quality", "authors": ["Cl\u00e9ment Bonnet", "Ariel N. Lee", "Franck Wertel", "Antoine Tamano", "Tanguy Cizain", "Pablo Ducru"], "abstract": "In the last two years, text-to-image diffusion models have become extremely popular. As their quality and usage increase, a major concern has been the need for better output control. In addition to prompt engineering, one effective method to improve the controllability of diffusion models has been to condition them on additional modalities such as image style, depth map, or keypoints. This forms the basis of ControlNets or Adapters. When attempting to apply these methods to control human poses in outputs of text-to-image diffusion models, two main challenges have arisen. The first challenge is generating poses following a wide range of semantic text descriptions, for which previous methods involved searching for a pose within a dataset of (caption, pose) pairs. The second challenge is conditioning image generation on a specified pose while keeping both high aesthetic and high pose fidelity. In this article, we fix these two main issues by introducing a text-to-pose (T2P) generative model alongside a new sampling algorithm, and a new pose adapter that incorporates more pose keypoints for higher pose fidelity. Together, these two new state-of-the-art models enable, for the first time, a generative text-to-pose-to-image framework for higher pose control in diffusion models. We release all models and the code used for the experiments at https://github.com/clement-bonnet/text-to-pose.", "sections": [{"title": "1 Introduction", "content": "Text-to-image diffusion models have recently shown impressive results in the space of image genera-tion [13; 4; 14; 5; 12]. As the use and quality of these models have increased, so have the requirements from users to strengthen the controllability of the outputs. Various methods have emerged to yield better output control, from fine-tuning [20], to sophisticated prompt-engineering [2; 8], to adding new modalities as conditions, e.g. ControlNets [23] and Adapters [9; 22].\nIn the context of enhancing human pose fidelity in text-to-image diffusion models, two key problems have emerged:\n\u2022 (1) Obtaining poses to cover the wide variety of situations that can be described semantically through the text description. Previous methods required selecting poses from a dataset, either by giving a picture, extracting its pose using pose-estimation models such as DWPose [21], and transferring it to the new image with systems such as GANs [7].\n\u2022 (2) Pose-conditioned image generation such as the previous SOTA SDXL-Tencent adap-tor [9] does not include faces or hands, hindering pose fidelity, suffers from an aesthetic quality drastically lower than that of the original SDXL model.\nIn this article, we overcome these two main challenges of image generation conditioned on text and pose by first training a text-to-pose generative model, which is to the best of our knowledge the first of its kind, and then by training a new SOTA pose adapter for diffusion models, which incorporates both facial and hand gestures. Combining these two new SOTA models enables a new text-to-pose-to-image generative framework for higher pose control in diffusion models (see figure 4 for a visual in the appendix)."}, {"title": "2 Text-to-Pose Generative Model", "content": "We describe a human pose with a series of points locating key positions of body parts: 18 points for the body, 42 points for the hands, and 68 points for the face (see Figure 5 in the appendix for examples of poses). To train a text-to-pose model, we first design a metric inspired by CLIP [10] to assess the quality of generated poses during training. We then design a transformer architecture for prompt-conditioned pose generation, annotate a dataset of high-quality images, and train the model on it."}, {"title": "2.1 CLaPP: A Contrastive Text-Pose Metric", "content": "Before training a generative text-to-pose model, we first create a metric that will guide training regarding matching text to poses. To do so, we train a contrastive model \u2013 akin to CLIP [10; 11] \u2013 by embedding both text and pose image into a joint latent space and optimizing the projection such as to minimize the angular distance between poses that corresponds to the same text description while maximizing the distance between those that do not. Inspired by CLIP, we call our model Contrastive Language-Pose Pretraining (CLaPP).\nWe train our CLaPP metric on a dataset of 500k (image, prompt) pairs from JourneyDB [15] which we annotate to extract poses (body, face, and hands) using DWPose [21]. Both prompts and poses (images of poses on a black background) are first encoded using CLIP. The CLaPP layers then map each CLIP representation to a new text-pose embedding from which to compute the CLaPP score. The resulting CLaPP scores between different poses and texts are reported in Figure 5 in the appendix. As expected, the diagonal of the score matrix has high CLaPP scores since it corresponds to the exact captions of each image."}, {"title": "2.2 T2P: Text-to-Pose Transformer", "content": "A pose is defined as an ordered sequence of key points, i.e. (x, y) coordinates of points in the image that correspond to e.g. the right thumb, the left shoulder, the nose, etc. Given this, it makes sense to design a sequence model that can embed the whole pose conditioned on text features. For this, we use a decoder-only transformer architecture [16] (see figure 1) to auto-regressively predict the next point in a pose with cross-attention on the text features produced by CLIP. We use positional embeddings of length 128 (18 body points, 68 face points, and 2*21 for the hands). Contrary to language models that have tokens, the pose input space is continuous, therefore, we model it with a Gaussian Mixture Model (GMM) along with a binary classifier. The former allows us to learn rather complex non-normal probability distributions over the next pose keypoint, while the latter predicts if the next keypoint exists. Indeed, poses are not always complete, e.g. if a pose describes a portrait, it is likely that only the 68 points corresponding to the face will be present. We found the use of mixtures necessary since a single Gaussian cannot represent well the multi-modal properties of the conditional probability distribution of the next keypoint, e.g. if the pose represents two people on either side, a GMM can capture this bi-modality of where the next point can be (left or right)."}, {"title": "2.3 Training", "content": "We train the model in a self-supervised fashion by predicting the next pose point in the sequence. The GMM output is trained with maximum likelihood (the GMM likelihood can be computed analytically), and the binary classifier with binary cross-entropy on the existence of the next point or not. We train the model on the 4M (pose, prompt) pairs obtained from JourneyDB by annotating each image with DWPose. We found that a mixture of 6 different Gaussian mixtures and a transformer with 4 layers worked best for our dataset."}, {"title": "2.4 Inference: Tempered Distribution Sampling", "content": "After training, the auto-regressive Gaussian mixture model suffers from too high of an entropy which makes sampling from the model quickly diverge out-of-distribution at inference time. Traditionally, language models have their logits divided by a temperature at inference time to decrease the model's entropy [3; 18; 17]. In our case, the model outputs GMM parameters and although the tempered likelihood function can be analytically computed, to our knowledge there is no known algorithm to sample from it. Therefore, we generalize the approach of tempered distribution sampling to any distribution in theorem 1. We then applied tempered sampling to our GMM and found generated poses to be much more precise with lower temperatures, at the expense of being less diverse (see figure 7 in the appendix)."}, {"title": "2.5 Analysis & Performance", "content": "To analyze the performance of our text-to-pose (T2P) model, we compiled a \u201cCOCO-Pose\" benchmark dataset, where we ex-tracted the poses of 100 image-caption pairs from the standard COCO 2017 bench-mark [6]. For each of the text labels of the COCO-Pose dataset, we then compared the CLaPP score of either: (a) selecting the closest pose from the training dataset (KNN search [1] from CLIP embeddings); or (b) generating a pose using T2P. The re-sults are compiled in figure 2, where we can see that T2P outperforms a KNN in the training dataset 78% of the time. This somewhat proves the local generalization capabilities of T2P and its alignment with respect to prompts describing poses."}, {"title": "3 Pose Adapter for Image Generation", "content": "Several pose-conditioning systems have been built for diffusion models, notably ControlNets [23], and Adapters [9; 22]. The previous SOTA model for pose-conditioning is the SDXL-Tencent adapter [9] which suffers from two key drawbacks. First, it does not include keypoints related to faces and hands. Second, it suffered from lower image aesthetic due to training on low-quality images or optimization issues."}, {"title": "3.1 Adapter Training", "content": "We start from the same architecture as the previous SOTA adapter [9], which consists of a ResNet-like model, and we train it on the same dataset used to train T2P. We use the same hyper-parameters as the original model for a total of 7600 training steps with a batch size of 256."}, {"title": "3.2 Analysis & Performance", "content": "To measure the performance of our adapter compared to the previous SOTA, we conducted a series of benchmarking tests. For these tests, we generate pose-conditioned images using 30 steps of diffusion with both adapters and then 10 steps of the SDXL refiner model to correct small distortions. The fact that our adapter now has faces and hands to condition on yields better pose accuracy, as shown in figure 8 in the appendix. However, one can see the poses are never perfectly matched and the image quality is still below that of the base SDXL model.\nWe also measure the aesthetic score 2, and Human Preference Score (HPS) v2 [19], of both the SDXL-Tencent adapter and ours on the COCO-Pose dataset. As shown in figure 3, our adapter outperforms the Tencent one 70% of the time for the aesthetic score, and 76% of the time for the HPS score on the COCO-Pose benchmark."}, {"title": "4 Discussion", "content": "In this article, we tackle the problem of conditioning on human poses the image generation process of diffusion models. We first solve the issue of finding the right pose given a prompt by training a text-to-pose auto-regressive model, which outperforms searching the training database (using KNN) 78% of the time. We then tackle the task of conditioning on poses in a way that guarantees both high fidelity to the pose and high image aesthetics by training a pose adapter on high-quality images with more pose keypoints than previous models. Our adapter has a win-rate ratio of 70% on the aesthetic score and 76% on the COCO-Pose benchmark compared to the previous SOTA.\nThis work encourages new paradigms for improved user experiences. By creating this intermediate modifiable image semantics (human poses), one can imagine slightly altering the pose while keeping the seed constant so as to modify the human pose directly on the generated image, turning the pixel image somewhat vectorized.\nImproved image fidelity and human pose control bring our attention to the broader ethical impacts that such technologies can have. While our system does not guarantee photorealism, it is important to keep in mind that such AI-generated images should not be misused and mixed up with real photos of humans."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Text-to-pose-to-image framework", "content": "We propose factorizing text-to-image generation into semantics generation (human poses) and then semantics-conditioned image generation (see figure 4). This allows for better control over the semantics and higher overall quality."}, {"title": "A.2 CLaPP metric", "content": "Our contrastive model, called CLaPP, can predict compatibility scores between a prompt and a human pose. Figure 5 demonstrates a few CLaPP scores of samples from the COCO dataset. The closer to 1, the more similar a pose and a prompt are, the worst score being -1."}, {"title": "A.3 Tempered distribution sampling", "content": "We prove some properties of our generalization of tempered sampling to any distribution in theorem 1. We also describe the sampling algorithm used to do inference with the T2P model.\nTheorem 1 (Tempered distribution sampling) Let X be a random variable, with a probability distribution density p(X), i.e. X ~ p(X). Let T \u2208 R+ be a real positive \u201ctemperature\u201d parameter. We define the \"tempered distribution transform\" as X\u0442 ~ pr(X), where:\n$p_T(x) = \\frac{p(x)^{\\frac{1}{T}}}{\\int_X p(x)^{\\frac{1}{T}} dx} = \\frac{e^{\\frac{ln p(x)}{T}}}{\\int_X e^{\\frac{ln p(x)}{T}} dx}$       (1)"}, {"title": "Properties", "content": "Properties \u2013 the tempered distribution pr is related to the original one p by the following properties:\n\u2022 Conservation of modes (and inflection points): if $d_x p = 0$, then $d_x p_T = 0$\n\u2022 Temperature one invariance: $p_{T=1}(x) = p(x)$\n\u2022 Mode selection with smaller temperatures: as T goes to zero, the tempered distributions pr tends towards a dirac distribution selecting the mode of the p distribution, i.e.\n$p_T(x) \\xrightarrow[T\\rightarrow 0]{} \\delta_{max\\_x p(x)}(x)$  (2)\n\u2022 Uniform thermalization with high temperatures: as T goes to infinity, the tempered distribu-tion pr tends towards a uniform distribution over the support of p, i.e.\n$p_T(x) \\xrightarrow[T\\rightarrow \\infty]{} Uniform_X(x)$  (3)\n\u2022 Temperature score scaling: the score of the tempered distribution \u2207lnpy(x) is proportional to the score of the original distribution \u2207xlnp(x), scaled by the temperature, i.e.\n$\\nabla_x ln p_T(x) = \\frac{\\nabla_x ln p(x)}{T}$ (4)"}, {"title": "Sampling scheme", "content": "Sampling scheme \u2013 to sample from the tempered distribution, one can (c.f. Figure 6):\n\u2022 Sample N points xi from p(X) to cover its support\n\u2022 Sample from the following softmax distribution:\n$Softmax \\{ [\\frac{1}{T} ln p(x_i)] \\} = \\frac{e^{[\\frac{1}{T} ln p(x_i)]}}{\\sum_{j=1}^N e^{[\\frac{1}{T} ln p(x_j)]}}$       (5)\nProof 1 The proofs of the mode-conserving and score-scaling properties are established by taking the derivatives from the tempered distribution definitions. The mode-selecting property is established by taking the temperature to zero. The proof of the sampling scheme is derived by importance sampling: $\\int_X p(x) \\frac{e^{\\frac{ln p(x)}{T}}}{p(x)} dx = \\frac{\\int_X e^{\\frac{ln p(x)}{T}}p(x) dx}{\\int_X p(x) dx}$, which is then approximated at large N by Monte Carlo estimate $\\frac{1}{N} \\sum_{j=1}^N e^{\\frac{ln p(x)}{T}}$.\nWe can see in figure 6 the influence of the temperature on a tempered GMM in a toy case with two Gaussian mixtures. The closer to 0 the temperature, the closer to the mode the tempered distribution peaks.\nTempered sampling for a GMM is crucial for our T2P model because the model with a temperature of 1 has too high entropy and ends up generating out-of-distribution high-noise pose sequences. The effect of the temperature on the T2P inference can be observed in figure 7 with the poses failing to shake hands at high temperature but eventually succeeding if the temperature is lowered."}, {"title": "A.4 Pose adapter", "content": "We train our adapter on high-quality images annotated with full poses (including bodies, hands, and faces). This leads to a pose-conditioned image generation of higher quality and fidelity as shown in figure 8."}, {"title": "A.5 Limits", "content": "CLaPP metric The metric we designed to automatically assess the quality of generated poses has some shortcomings. First, it was trained on a not-so-large dataset which means it may consider out-of-distribution captions or poses that are quite different from the dataset it was trained on, biasing a text-pose matching score. Then, we used CLIP as a backbone, whose text and image encoders may have some pose-agnostic representations that hinder the quality of embeddings as much as human poses are concerned. Finally, using images as representations of poses is highly inefficient, one could design a better model by working with the sequence of (x, y) points directly (much lower dimension), for instance using the backbone of the T2P model."}]}