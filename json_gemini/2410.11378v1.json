{"title": "WPFed: Web-based Personalized Federation for Decentralized Systems", "authors": ["Guanhua Ye", "Jifeng He", "Weiqing Wang", "Zhe Xue", "Feifei Kou", "Yawen Li"], "abstract": "Decentralized learning has become crucial for collaborative model training in environments where data privacy and trust are paramount. In web-based applications, clients are liberated from traditional fixed network topologies, enabling the establishment of arbitrary peer-to-peer (P2P) connections. While this flexibility is highly promising, it introduces a fundamental challenge: the optimal selection of neighbors to ensure effective collaboration. To address this, we introduce WPFed, a fully decentralized, web-based learning framework designed to enable globally optimal neighbor selection. WPFed employs a dynamic communication graph and a weighted neighbor selection mechanism. By assessing inter-client similarity through Locality-Sensitive Hashing (LSH) and evaluating model quality based on peer rankings, WPFed enables clients to identify personalized optimal neighbors on a global scale while preserving data privacy. To enhance security and deter malicious behavior, WPFed integrates verification mechanisms for both LSH codes and performance rankings, leveraging blockchain-driven announcements to ensure transparency and verifiability. Through extensive experiments on multiple real-world datasets, we demonstrate that WPFed significantly improves learning outcomes and system robustness compared to traditional federated learning methods. Our findings highlight WPFed's potential to facilitate effective and secure decentralized collaborative learning across diverse and interconnected web environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Federated Learning (FL) has emerged as a pivotal solution for safeguarding data privacy and enabling distributed model training [4, 6]. By allowing clients to train models locally and upload only model updates, FL circumvents the need for centralized sensitive data storage, thereby protecting user privacy [37, 50]. Despite its advantages, traditional FL relies on a central coordinator to aggregate model updates, which introduces trust issues and creates a single point of failure [18]. To mitigate these limitations, Decentralized Federated Learning (DFL) has been introduced [5, 55], enabling clients to collaborate through peer-to-peer (P2P) communication without a centralized entity [11]. However, DFL still faces significant challenges; specifically, in a fully decentralized setup, communication is constrained by fixed network topologies, limiting the flexibility and efficiency of collaborative learning.\nIn the contemporary web environment, most edge devices can connect to the internet [30], enabling arbitrary P2P connections [2]. This shift from fixed communication graphs offers scalable and resilient collaborative frameworks [5], theoretically allowing the identification of globally optimal neighbors for more personalized and effective collaborative learning. However, in large-scale networks, establishing P2P connections with all clients is impractical due to prohibitive communication overhead [52], limiting the full potential of decentralized learning in expansive web-based settings [47].\nRecent research has explored semi-decentralized learning paradigms [20, 46] within web contexts [49], where each client connects to a communication coordinator. This coordinator facilitates global connection establishment, allowing clients to engage in P2P communication with optimally selected peers on a wider horizon and thereby reducing communication overhead [5, 27, 55]. Nonetheless, as the coordinator is unregulated and does not actively participate in the collaborative learning process, this approach relies on two critical assumptions: the coordinator must be fully reliable to prevent data misuse and defend against third-party attacks; and all clients within the network must be honest, ensuring consistency and integrity when interacting with peers and the coordinator [36]. These prerequisites undermine the practicality and robustness of semi-decentralized frameworks, particularly in environments where trust cannot be guaranteed.\nTo this end, we introduce the WPFed framework-a fully decentralized, web-based learning framework designed to achieve globally optimal neighbor selection without relying on a central coordinator (illustrated in Fig. 1). WPFed addresses two primary challenges:\nFirstly, we enable personalized neighbor selection by introducing a mechanism that allows each client to identify beneficial neighbors on a global scale, considering both inter-client similarity and model quality. Inter-client similarity ensures collaboration with peers possessing similar data distributions, leading to more effective and coherent model updates. Assessing model quality guarantees that only high-performing models contribute to the learning process, thereby enhancing overall system performance. Existing works often address these factors separately or rely on centralized coordinators. In contrast, WPFed seamlessly integrates similarity assessment and performance evaluation within a fully decentralized framework, offering a more robust and scalable solution for personalized collaborative learning. Specifically, each client maintains a local model and generates a Locality-Sensitive Hashing (LSH) code from its model parameters [10, 25, 29]. This LSH code serves as a compact and privacy-preserving representation of the model's"}, {"title": "2 PRELIMINARIES", "content": "This section introduces the fundamental concepts and components of the WPFed framework, including the definitions of user clients, P2P communication mechanisms, announcements, the dynamic communication graph, and the overall objective of the system."}, {"title": "2.1 User Client", "content": "In WPFed, each user i is associated with a client $u_i$ that maintains:\n\u2022 A personal local dataset ${X^{loc}, Y^{loc}}$.\n\u2022 A personal reference dataset ${X^{ref}, Y^{ref}}$.\n\u2022 A local model parameterized by $\u03b8_i$.\nThe inference function of a model $\u03b8_j$ is denoted as $f(\u03b8_j, \u00b7)$. The personal reference dataset is designed to facilitate communication and collaboration among clients; it can be preloaded or generated via specific algorithms. To ensure diversity and robustness, each user's reference dataset is unique. Additionally, each client maintains a dynamic set of neighbors, denoted as $N_i = {n_1, n_2, ..., n_i}$, which can change over time based on network conditions and performance metrics. User i will conduct P2P communication with $N_i$ for knowledge delivery."}, {"title": "2.2 Announcements", "content": "Each client disseminates an announcement in the blockchain, comprising two components: the LSH Code $lsh_i^{(t)}$, a Locality-Sensitive Hashing representation of its model at iteration t, and the Performance Ranking $R_i^{(t)}$, which ranks its neighbors based on evaluations using the personal reference dataset (see Section 3.3). Formally, the announcement from client $u_i$ at iteration t is:\n$a_i^{(t)} = {lsh_i^{(t)}, R_i^{(t)}}$.\nUsing the announcements ${a_j^{(t)} | j \u2260 i}$, each client $u_i$ independently computes:\n\u2022 Similarity Scores {$d_{ij}$}: Quantitative measures of model similarity derived from the LSH codes (see Section 3.2).\n\u2022 Ranking Scores {$s_j$}: Performance metrics of peer models based on their evaluations with respective reference datasets (see Section 3.3).\nThis information allows each client to autonomously balance model similarity and peer performance when selecting neighbors, thereby eliminating the need for a central coordinator."}, {"title": "2.3 Dynamic Communication Graph", "content": "The network of user clients is modeled as a dynamic graph $G = (U, E, W)$, where $U = {u_i | i \u2208 [1, M]}$ represents the clients, with M denoting the total number of clients. The edge set $E = {e_{ij} \u2208 {0,1} | i, j \u2208 [1, M]}$ defines communication capabilities, where $e_{ij} = 1$ signifies that clients $u_i$ and $u_j$ can communicate directly. The weight set $W = {w_{ij} \u2208 R^+ | i, j \u2208 [1, M]}$ quantifies the potential benefits of communication between clients."}, {"title": "2.4 WPFed Objective", "content": "WPFed aims to enable each client $u_i$ to determine an optimal personalized neighbor set $N_i$ using the weight set W, thereby enhancing the framework's overall performance. The global loss function to be minimized is defined as:\n$L_{global} = \\frac{1}{M} \\sum_{i=1}^{M} [L_i^{loc} + (1 - \\alpha) L_i^{ref}],$\nwhere $L_i^{loc} = f (f(\u03b8_i, X^{loc}), Y^{loc})$ represents the local loss of client $u_i$ on its own dataset, $L_i^{ref}$ quantifies the consistency between client $u_i$ and its neighbors $N_i$ based on the personal reference dataset (see Section 3.1), and $\u03b1 \u2208 [0, 1]$ is a hyperparameter that balances the trade-off between local loss and collaborative learning.\nBy minimizing $L_{global}$, the framework seeks to enhance each client's model performance while fostering effective collaboration within the network."}, {"title": "3 METHODOLOGY", "content": "In this section, we detail the methodology of the WPFed framework, focusing on how clients perform P2P communication, evaluate peer similarity, securely share model characteristics, compute ranking scores, and select personalized neighbors to enhance collaborative learning without a central coordinator."}, {"title": "3.1 P2P Communication via Distillation", "content": "Clients in WPFeds engage in P2P communication to exchange knowledge and improve their local models through a process akin to knowledge distillation.\nAs shown in Fig. 2, the communication protocol between a client $u_i$ and its neighbor $u_j$ involves the following steps:\n(1) Reference Data Sharing: Client $u_i$ sends the features of its personal reference dataset $X^{ref}$ to client $u_j$. This dataset is designed to be non-sensitive and unique to each client.\n(2) Inference by Neighbor: Client $u_j$ uses its local model $\u03b8_j$ to compute the outputs $\\hat{Y}_j^{web} = f(\u03b8_j, X^{ref})$.\n(3) Response Transmission: Client $u_j$ sends the inference results back to client $u_i$.\n(4) Performance Evaluation: Client $u_i$, possessing the true labels $Y^{ref}$, computes the performance score of $u_j$:\n$l_{ij} = l(Y^{ref}, \\hat{Y}_j^{web}) = l(\\hat{Y}_j^{web}, Y^{ref}),$\nwhere $l(\u00b7, \u00b7)$ is a suitable loss function (e.g., cross-entropy loss). Note that this step is not included in Fig. 2 for a clearer visualization.\n(5) Model Update: Client $u_i$ updates its local model $\u03b8_i$ by minimizing a loss function that encourages consistency between $f (\u03b8_i, X^{ref})$ and $f (\u03b8_j, X^{ref})$."}, {"title": "3.2 Peer Similarity Evaluation", "content": "To facilitate personalized collaboration, each client needs to assess the similarity between its local model and those of other clients. We employ Locality-Sensitive Hashing (LSH) codes to efficiently approximate model similarities without exposing the actual model parameters.\nAfter each iteration, client $u_i$ generates an LSH code $lsh_i$ from its local model parameters $\u03b8_i$:\n$lsh_i = LSH(\u03b8_i, b),$\nwhere b denotes the length (number of bits) of the code. The LSH function is designed such that similar inputs (in this case, model parameters) are mapped to similar hash codes with high probability. The similarity between two clients $u_i$ and $u_j$ is quantified by the Hamming distance between their LSH codes:\n$d_{ij} = Hamming Dist(lsh_i, lsh_j).$\nA smaller $d_{ij}$ indicates greater similarity between the models $\u03b8_i$ and $\u03b8_j$. This distance metric allows clients to estimate peer similarities efficiently and securely."}, {"title": "3.3 Ranking Score Computation", "content": "To assess the reliability and expertise of peers, each client computes and publishes a performance ranking of its neighbors based on the loss scores obtained during P2P communication. Specifically, client $u_i$ generates an ordered list $R_i$, ranking neighbors in ascending order of their loss $l_{ij}$ (i.e., better-performing neighbors rank higher). These rankings are made public on the blockchain. Using the collective rankings ${R_k}^M_{k=1}$, clients compute a ranking score $s_j$ for each peer $u_j$:\n$s_j = \\frac{|{R_k | u_j \\text{ ranks in the top } K \\text{ of } R_k, k \u2208 [1, M]}|}{|{R_k | u_j \u2208 R_k, k \u2208 [1, M]}|}$"}, {"title": "3.4 Personalized Neighbor Selection", "content": "Clients use both the similarity metrics and ranking scores to select a personalized set of neighbors that maximize the potential benefit of collaboration.\nFor each potential neighbor $u_j$, client $u_i$ computes a weight $w_{ij}$ combining the ranking score $s_j$ and the similarity distance $d_{ij}$:\n$w_{ij} = s_j \\cdot exp (-\u03b3 \\cdot d_{ij}),$\nwhere y is a hyperparameter controlling the influence of the similarity distance. This formulation ensures that clients who are both highly ranked and have similar models are given higher weights.\nClient $u_i$ selects the top N peers with the highest weights $w_{ij}$ to form its neighbor set $N_i$. This personalized neighbor selection allows each client to tailor its collaborative partners based on its unique preferences and observations."}, {"title": "3.5 Verification of LSH Code", "content": "Publishing LSH codes enables clients to assess model similarities but introduces a security risk: malicious clients may forge LSH codes to appear more similar, thereby gaining undue trust or access. To mitigate this, we propose a verification method that leverages outputs from peer-to-peer (P2P) interactions to authenticate LSH codes. Specifically, clients use model outputs on reference datasets as implicit proofs of similarity, which can be cross-validated by other clients if necessary.\nWhen client $u_i$ interacts with neighbor $u_j$, it receives the outputs $f(\u03b8_j, X^{ref})$. Client $u_i$ compares these with its own outputs $f(\u03b8_i, X^{ref})$ to compute a similarity metric, such as the Kullback-Leibler (KL) divergence. We implement a filter mechanism to deter LSH deception: if the similarity between $f(\u03b8_i, X^{ref})$ and $f (\u03b8_j, X^{ref})$ ranks in the lower half of all neighbors, then $u_j$ is excluded from the knowledge distillation process."}, {"title": "3.6 Verification of Peer Rankings", "content": "The effectiveness of the ranking score computation depends on clients honestly reporting performance rankings $R_i^{(t)}$ of their neighbors based on personal evaluations. To prevent malicious manipulation, a cryptographic mechanism is introduced to ensure the integrity of reported rankings without compromising privacy.\nA simple commit-and-reveal scheme using cryptographic hash functions is employed:\n(1) Commit phase (iteration t):\nEach client $u_i$ computes a commitment by hashing its ranking:\n$C_i^{(t)} = Hash (R_i^{(t)}),$\nand publishes $C_i^{(t)}$ to the blockchain.\n(2) Reveal phase (iteration t + 1):"}, {"title": "3.7 Framework Summary", "content": "The WPFed framework operates iteratively, enabling decentralized and personalized federated learning through a series of steps that each client performs independently. Algorithm 1 presents the pseudocode of the WPFed framework from the perspective of a single client $u_i$:\n(1) Neighbor Selection: Each client selects optimal neighbors by computing communication weights based on similarity scores and ranking scores derived from retrieved announcements. Note that ranking scores are verified against commitments using hash functions.\n(2) Communication: Clients exchange reference data and model outputs with selected neighbors, performing performance evaluations and verifying LSH codes to ensure authenticity.\n(3) Model Update: Clients update their local models by minimizing a loss function that integrates both local objectives and aggregated knowledge from neighbors.\n(4) Announcement Publication: Clients publish new LSH codes and securely commit to performance rankings using a hash-based commit-and-reveal scheme, sharing announcements for the next iteration."}, {"title": "4 EXPERIMENTS", "content": "We evaluate the effectiveness and robustness of the WPFed framework in decentralized federated learning scenarios, focusing on key aspects of its performance."}, {"title": "4.1 Datasets", "content": "We conduct experiments on three datasets: MNIST [31], PhysioNet Apnea-ECG (A-ECG) [22], and Sleep Cassette (S-EEG)[38].\nMNIST: A benchmark dataset of 70,000 handwritten digit images. To simulate non-IID data distribution, we partition the dataset as described in Section 4.3.\nA-ECG: Contains 35 overnight ECG recordings for sleep apnea detection. We preprocess the ECG signals into 60-dimensional RR interval vectors [9], targeting apnea event classification.\nS-EEG: Includes 153 polysomnography recordings; we use 40 clear EEG records for sleep stage classification into awake, NREM, and REM sleep [42]."}, {"title": "4.2 Baselines", "content": "We compare WPFed with four methods:"}, {"title": "4.3 Implementation Details", "content": "The implementation details are listed for higher reproducibility:\nData Partition: Since WPFed is designed for personalized analysis, it is natural to consider each individual's data as a separate client in datasets that include multiple subjects. For the A-ECG and S-EEG datasets, each patient's recordings are treated as local data for one client, resulting in 35 and 40 clients, respectively. For MNIST, which lacks inherent individual divisions, we introduce data heterogeneity to simulate non-IID conditions. We randomly partition the training samples into 20 shards and assign two shards to each of 10 clients. To create non-IID data distribution, we remove samples of a specific digit class from each shard. This method ensures that each client has data biased towards certain classes, similar to real-world federated learning scenarios. To augment the limited data in A-ECG and S-EEG, we apply a sliding window technique [9], generating more samples from each patient's recordings. Finally, we randomly split each client's local dataset into training and testing sets with a ratio of 7: 3.\nReference Data: For the reference dataset, in A-ECG and S-EEG, we randomly select 20% of the data (before train-test partition) from all patients to form a shared reference dataset repository, while the remaining data is used as clients' local datasets. In MNIST, the original test set of 10, 000 images is used as the reference dataset repository. When creating client objects, each client uniformly samples a non-overlapping subset from the reference dataset repository as its local reference dataset.\nModel Architectures: For MNIST, we employ MobileNetV2 [44] as the base model due to its efficiency and effectiveness in handling image classification tasks on resource-constrained devices. For A-ECG and S-EEG, we utilize the Temporal Convolutional Network (TCN) [23] as the base model, which is well-suited for time-series classification tasks due to its ability to capture temporal dependencies.\nTraining Details: We implement all methods using PyTorch and simulate the federated environment on a server with four NVIDIA V100 GPUs. The loss function l is specified as cross-entropy loss for all devices. We evaluate the performance using accuracy (Acc) on all three datasets. Hyperparameters are tuned via grid search, and the best values along with search ranges are listed in Table 1."}, {"title": "4.4 Performance Comparison", "content": "Table 2 presents the performance of WPFed and the baseline methods on the three datasets. The results are averaged over five runs with different random seeds, and the standard deviation is reported. From the results, we observe that WPFed consistently outperforms the baseline methods across all datasets. On MNIST, WPFed achieves the highest accuracy of 94.03%, surpassing FedMD by approximately 0.28%. This demonstrates the effectiveness of WPFed in handling image classification tasks under non-IID data distributions. For the A-ECG dataset, WPFed achieves an accuracy of 91.61%, which is higher than the baselines. The improvement indicates that WPFed can effectively leverage the personalized data"}, {"title": "4.5 Influence of Hyperparameters", "content": "We investigate the effects of two key hyperparameters: a, controlling the trade-off between local and collaborative learning, and y, adjusting the balance between LSH-based similarity and rank score in neighbor selection. Results are shown in Fig. 3."}, {"title": "4.6 Ablation Study", "content": "We assess the contributions of LSH-based similarity estimation and rank-based neighbor selection through an ablation study, comparing WPFed with three variants:\n\u2022 w/o LSH: Removes LSH-based similarity; neighbors are selected based solely on peer rank score.\n\u2022 w/o Rank: Excludes rank-based selection; neighbors are selected based solely on LSH similarity.\n\u2022 w/o LSH & Rank: Removes both components; neighbor selection is entirely random.\nTable 3 presents the performance differences of these variants compared to the full WPFed framework. The results are averaged over five runs with different random seeds, and the values indicate the decrease in accuracy relative to WPFed. The results demonstrate that removing either the LSH-based similarity estimation or the rank-based neighbor selection leads to a noticeable decline in performance across all datasets. The most significant performance degradation occurs when both components are removed, confirming that each plays a vital role in the effectiveness of WPFed."}, {"title": "4.7 LSH Cheating Resilience", "content": "We evaluate WPFed's robustness against attackers manipulating LSH codes to falsely appear similar to a target client $u_i$, aiming to be selected as a neighbor and degrade $u_i$'s model updates without significantly impacting the overall network. Suppose attackers control half of $u_i$'s potential neighbors and adjust their LSH codes to closely match that of $u_i$, increasing their selection likelihood. Once selected, they share malicious updates to impair $u_i$'s performance."}, {"title": "4.8 Poison Attack Resilience", "content": "To demonstrate WPFed's robustness against poisoning attacks, we conducted experiments where a subset of clients behaved maliciously by injecting corrupted updates into the network. After warm-up steps, all clients collaborate honestly for 50 iterations to allow partial model convergence. Subsequently, selected clients reinitialize their model parameters every three iterations, effectively injecting noise intended to disrupt the training of honest clients. We evaluated scenarios where 20%, 40%, and 60% of the clients are malicious. Fig. 5 presents the average performance and standard deviation of honest clients under both WPFed and ProxyFL frameworks (FedMD and KD-PDFL perform similarly to ProxyFL and are omitted for clarity).\nThe results indicate that under traditional decentralized knowledge distillation frameworks like ProxyFL, the presence of malicious clients significantly degrades honest clients' performance, with the negative impact intensifying as the proportion of malicious clients increases. In contrast, WPFed demonstrates strong resilience to poisoning attacks. When 20% or 40% of the clients are malicious, the performance of honest clients remains stable and virtually unaffected. Even when 60% of the clients are compromised, the impact on honest clients is limited; their performance continues to converge upwards, albeit with some fluctuations (similar observations were made with two other datasets but are omitted here due to space constraints).\nThis robustness arises from WPFed's global neighbor selection mechanism based on both LSH-based similarity and rank-based performance evaluation. Each client selects high-performing neighbors from the entire network, making it unlikely for malicious clients-with low performance metrics due to periodic model resets-to be chosen by honest clients. Moreover, WPFed is robust against disruptions from newly joined or unstable clients, as these clients primarily acquire knowledge from others without adversely affecting honest clients, given their lower likelihood of being selected as influential neighbors."}, {"title": "5 RELATED WORK", "content": "This section reviews recent literature on related areas, including Decentralized Federated Learning Frameworks, Personalized Neighbor Selection Mechanisms, and Web-based Collaborative Learning."}, {"title": "6 CONCLUSION", "content": "Theoretical Implications: In this study, we addressed the challenge of identifying beneficial neighbors in decentralized collaborative environments by designing an innovative neighbor selection mechanism that integrates inter-client similarity and model quality assessments. Our weighted neighbor selection strategy enhances the limited perspectives of individual clients, enabling the determination of optimal personalized neighbor sets on a global scale. Additionally, we developed LSH verification and Ranking Score verification mechanisms tailored for knowledge distillation within WPFed. This marks the first fully decentralized framework to integrate peer similarity and quality assessments with collaboration security, thereby advancing the theoretical foundations of secure and efficient federated learning.\nPractical Implications: Our experiments on multiple real-world datasets demonstrate that WPFed significantly enhances learning performance and system robustness compared to traditional federated learning approaches. Furthermore, WPFed effectively safeguards data privacy and mitigates the risks posed by malicious clients through advanced privacy-preserving knowledge distillation and LSH verification mechanisms. The proposed framework holds significant potential for domains such as e-health and decentralized finance, where secure and personalized federated learning is paramount, facilitating reliable and efficient collaborative models in sensitive and distributed applications.\nLimitations: Despite its advantages, WPFed has limitations. The application of LSH does not fully support heterogeneous models, restricting the framework's applicability to clients with varying model architectures or feature spaces. Moreover, the blockchain component currently lacks comprehensive incentive and punitive mechanisms, such as rewards for high-performing clients and penalties for malicious actors. Future work addressing these aspects will enhance WPFed's scalability and security, broadening its applicability to diverse decentralized applications."}]}