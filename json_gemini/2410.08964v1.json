{"title": "LANGUAGE IMBALANCE DRIVEN REWARDING FOR MULTILINGUAL SELF-IMPROVING", "authors": ["Wen Yang", "Junhong Wu", "Chen Wang", "Chengqing Zong", "Jiajun Zhang"], "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance across numerous tasks. However, these advancements have predominantly benefited \"first-class\" languages such as English and Chinese, leaving many other languages underrepresented. This imbalance, while limiting broader applications, generates a natural preference ranking between languages, offering an opportunity to bootstrap the multilingual capabilities of LLM in a self-improving manner. Thus, we propose Language Imbalance Driven Rewarding, where the inherent imbalance between dominant and non-dominant languages within LLMs is leveraged as a reward signal. Iterative DPO training demonstrates that this approach not only enhances LLM performance in non-dominant languages but also improves the dominant language's capacity, thereby yielding an iterative reward signal. Fine-tuning Meta-Llama-3-8B-Instruct over two iterations of this approach results in continuous improvements in multilingual performance across instruction-following and arithmetic reasoning tasks, evidenced by an average improvement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9% accuracy on the MGSM benchmark. This work serves as an initial exploration, paving the way for multilingual self-improvement of LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) with superior performance across numerous tasks. However, existing studies show that due to the imbalance of pre-training and fine-tuning data across languages, existing LLMs have predominately benefited a few \u201cfirst-class\u201d languages, particularly English and Chinese, thereby overlooking a wide range of other languages (Qin et al., 2024). Given that LLMs are used worldwide, such language imbalance presents significant risks for users who operate in less dominant languages (Deshpande et al., 2023). To this end, enhancing the multilingual performance of LLMs has gained increasing attention.\nPrevious research predominantly frames this imbalance as an issue to be resolved, often addressing it through multilingual training and cross-lingual alignment. The first approach aims to improve multilingual performance by incorporating additional multilingual data (Wei et al., 2023; Dang et al., 2024). However, high-quality multilingual instruction tuning and preference data, particularly for low-resource languages, remain scarce and expensive (Boubdir et al., 2023; Chaudhari et al., 2024). The second approach seeks to bridge the performance gap between languages by aligning non-dominant and dominant ones (Li et al., 2023a; Chen et al., 2023b; Chai et al., 2024; Zhang et al., 2024), which are often bottlenecked by the performance of the dominant language.\nThis work takes a different perspective, positing that language imbalance, while still an issue, creates a natural preference ranking between dominant and non-dominant languages, which can be"}, {"title": "2 LANGUAGE IMBALANCE DRIVEN REWARDING", "content": "Our approach first assumes access to an instruction-following language model, and a set of multilingual training prompts. Starting from any instruction model that possesses basic multilingual generation capabilities, each iteration consists of two steps, (i) Self multilingual preference pair generation and (ii) Multilingual preference optimization, as shown in Figure 1. For the tth iteration, we use the current model Mt, with the seed model denoted as Mo. Step (i) generates multilingual"}, {"title": "3 DISCUSSION", "content": "The insight behind our proposed method is to leverage the inherent differences in the multilingual capabilities of LLMs to provide rewards for DPO training. Therefore, two key questions remain to be addressed:"}, {"title": "3.1 Do LLMS EXHIBIT SIGNIFICANT DIFFERENCES IN MULTILINGUAL CAPABILITIES?", "content": "While the differences in multilingual capabilities have been evidenced by many prior works (Ranaldi & Pucci, 2023; Yuan et al., 2023; Zhao et al., 2024), we further validate the disparity in multilingual capabilities in Llama-3.\nSpecifically, we randomly selected 100 multilingual Alpagasus (Chen et al., 2023a) instructions and evaluated the response quality across different languages using GPT-4 score. Based on the technical report for LLaMA 3 (Meta, 2024), English is selected as the dominant language, while the other languages are considered non-dominant languages. As shown in Table 1, a significant difference in response quality remains between the dominant language (en) and non-dominant languages, demonstrating an inherent imbalance exists in the multilingual capabilities within the model."}, {"title": "3.2 DOES TRANSLATION PRESERVE THE RANKING OF RESPONSE PREFERENCES?", "content": "As shown in Discussion 3.1, Given an English promptx and a non-dominant language prompt xn, model M consistently produces a better response for the English prompt: M(ydlxen) > M(ynl xnl). However, self-translation is employed by our method to convert the English response yen into non-dominant language nl, and vice versa. Therefore, a key question arises: Do the translated responses preserve the preference ranking in Equation 2 and 3?\nAs translation will largely preserve the semantics and the structure of the sentence, it is reasonable to believe that the preference ranking stemming from the quality difference of the response is largely preserved. To verify our assumption, the GPT-4 score is utilized to assess the quality of self-translated responses and compare it with original responses. As shown in Table 2, the GPT-4 score of the translated response ydl\u2192nl is lower than the original response ydl. However, a substantial gap remains between the translated response and the original response in non-dominant languages, which is consistent with the original preference ranking. This conclusion also holds for the dominant language (English), where the original response is superior to the response translated from non-dominant languages. Overall, the self-translation process does preserve the ranking of response preference.\nTo observe the final preference ranking, multilingual preference pairs are constructed between the original and translated responses, sampling 100 pairs from each language. Reward accuracy (Win"}, {"title": "4 GENERAL INSTRUCTION FOLLOWING", "content": "4.1 EXPERIMENTAL SETUP\nBase Models In our experiments, we use a widely adopted instruction-following model as our base model Mo, namely Llama-3-8B-Instruct (Meta, 2024). Llama-3-8B-Instruct, as an English-centric LLM, often encounters off-target issues when handling non-English requests. In Appendix A, our method is scaled to Qwen2-7B-Instruct (Yang et al., 2024) to verify its generalization capability.\nLanguages English is chosen as the common dominant language, and non-dominant languages include high-reward languages (German, Russian) and low-reward languages (Spanish, French). Additionally, Chinese is selected as an unseen language to observe the generalization of our approach. Note that unseen language means that not included in the training data.\nDatasets The Alpagasus dataset (Chen et al., 2023a) includes 9,000 high-quality instruction-following examples filtered from the original 52,000 in the Alpaca dataset (Taori et al., 2023). We sample 1,000 prompts from the Alpagasus dataset and translate them into other languages using the Google Translate API to obtain multilingual prompts.\nImplementation Details Models are trained for one epoch in each iteration across all experiments. During training, the global batch size is set to 16, and the learning rate is 5e-7. More details are described in Appendix B.4.\nEvaluation and Metrics\n\u2022 Head-to-head performance: Head-to-head performance is evaluated between the base model and the iterative models using GPT-4 (Achiam et al., 2023) as an evaluator (Liu et al., 2023) over 805 test prompts in X-AlpacaEval (Zhang et al., 2023). The detailed evaluation setup can be found in Appendix B.1.\n\u2022 X-AlpacaEval leaderboard: We extend the existing AlpacaEval 2.0 toolkit (Li et al., 2023d) from an English-only framework to a multilingual one and compare both proprietary and open-source multilingual models on their multilingual instruction-following abilities."}, {"title": "4.2 HEAD-TO-HEAD PERFORMANCE", "content": "The head-to-head win rates of our models on the X-AlpacaEval (Zhang et al., 2023) dataset are shown in Figure 2.\nFindings 1: Language Imbalance Driven Rewarding is effective. The head-to-head performance shows that M\u2081 achieves notable win rates against Mo across all five training languages and one unseen language. For these five training languages, M\u2081 demonstrates a significant improvement, with AW-L values for each language ranging from 15.3% (en) to 61.5% (ru) compared to the base model. Upon comparing different languages, high-reward languages (ru, de) gain larger improvements than low-reward languages (es, fr) in Iteration 1 (M\u2081 vs Mo), but in Iteration 2 (M2 VS M1), the gains across all languages diminish and converge. We hypothesize that the reward signals across different languages gradually weaken and align over iterations.\nFindings 2: The dominant language also benefits from Language Imbalance Driven Rewarding. Since the responses for non-dominant languages are translated from English, it is natural for these languages to see improvements. However, English also achieves better performance compared to the reference model. As shown in Figure 2, English shows a 15.3% increase in AW-L for M\u2081 vs Mo, and a 14.6% increase in AW-L for M2 vs M1. These results suggest that the dominant language also benefits from rejected responses translated from non-dominant languages, a phenomenon that has rarely been observed in previous cross-lingual alignment research.\nFindings 3: Iterative training is possible and effective. Findings 2 reveals that English benefits from language imbalance driven rewarding, which lays the foundation for iterative optimization. Specifically, the enhancement in English makes it possible to generate higher-quality responses in the next iteration of training, enabling continual self-improving. In Figure 2, a consistent gain is observed in Iteration 1 (M\u2081 vs Mo) and Iteration 2 (M2 vs M\u2081) in all languages. Compared to Iteration 1, the gains for all training languages (except for English) in Iteration 2 become more consistent and convergent. This demonstrates that our approach are capable of iteratively aligning all languages until reaching saturation."}, {"title": "Findings 4: Multilingual optimization can generalize to unseen languages.", "content": "For the unseen language, the gains (AW-L) in Chinese are 32.2% for Iteration 1 (M\u2081 vs Mo) and 22.4% for Iteration 2 (M2 vs M1), respectively. These results indicate that multilingual preference optimization facilitates cross-lingual transfer, which is consistent with observations in Dang et al. (2024)."}, {"title": "4.3 X-ALPACAEVAL LEADERBOARD", "content": "The X-AlpacaEval leaderboard, as shown in Table 4, demonstrates a high degree of consistency with head-to-head evaluations. After two rounds of iteration, Llama-3-8B-Instruct achieves average improvements of 7.46% in win rates over GPT-4 Turbo across five languages. Additionally, we evaluate the performance of state-of-the-art multilingual models on X-AlpacaEval, including OpenAI's GPT-40, GPT-4 (Achiam et al., 2023), along with Qwen series (Bai et al., 2023; Yang et al., 2024), Llama series (Touvron et al., 2023b; Meta, 2024), InternLM2 (Cai et al., 2024), Aya-23 (\u00dcst\u00fcn et al., 2024), Mistral (Jiang et al., 2023) and PolyLM (Wei et al., 2023). Our method based on Llama-3-8B-Instruct outperforms both 7B and 14B-level models, achieving comparable performance to the 70B-level models.\nMoreover, a comparative experiment on multilingual alignment is conducted, which performed supervised fine-tuning by self-translating model responses from the dominant language to non-dominant languages under the same experimental conditions. Multilingual alignment utilizes the performance of the dominant language as an anchor to align the capabilities between dominant and non-dominant languages. While there is an improvement in performance on non-dominant languages, a significant gap remains compared to our method, with 18.32% vs 22.18% in Llama3. Notably, multilingual alignment places excessive focus on non-dominant languages during the SFT process, resulting in a degradation of performance in English (-3.02%). In contrast, our approach improves English performance (+9.19%). This improvement is crucial for enabling iteration in our method, whereas the performance decline in English seen with multilingual alignment hinders further iteration."}, {"title": "4.4 PERFORMANCE ON MULTILINGUAL MT-BENCH", "content": "Table 5 reports the multilingual MT-Bench results on a scale of score 10. A significant performance improvement on MT-Bench in Llama3 is observed across the training iterations, increasing from 6.80 in Mo to 7.51 in M2. This is because Llama3 initially exhibits a strong reward signal in Mo; however, this signal weakens as iterations progress. A detailed analysis is provided in Section 4.6."}, {"title": "4.5 ALIGNMENT TAX ON MULTILINGUAL NLP BENCHMARKS", "content": "Previous studies have shown that instruction tuning and RLHF can lead to forgetting, also known as the alignment tax (Ouyang et al., 2022). The changes in world knowledge and commonsense reasoning abilities are examined throughout the iterative process by evaluating its performance on multilingual NLP benchmarks.\nTable 6 presents the average results across five training languages (English, Spanish, Russian, German, French) and one unseen language (Chinese) on four benchmarks, with more detailed results provided in the Appendix C.1. Overall, during the iteration process, the performance on the benchmarks not only exhibits no significant degradation compared to the base models but also shows a slight improvement. These results indicate that the multilingual preference optimization process did not introduce any alignment tax."}, {"title": "4.6 THE REWARD SIGNAL CHANGES OVER ITERATIONS, GETTING STRONGER OR WEAKER?", "content": "The reward signal strength on pairwise data was analyzed at the beginning of each iteration, as outlined in Section 3.2. Figure 3 shows the change in reward accuracy on training languages (en, es, ru, de, fr) and unseen languages (it, ja) across iterations. For the training languages, high-reward languages, except English, gradually shift to lower-reward status after Iteration 1. As English capabilities improve through iterations, low-reward languages remain in the lower-reward range with some fluctuations, enabling the self-improving process to continue iteratively.\nFor unseen languages, reward accuracy also steadily increases (it) as English capabilities improve continuously. However, during the DPO training, certain preferences, such as controlling off-target responses, are transferred to unseen languages (ja). This is evident from the sharp drop in Japanese reward accuracy after Iteration 1, which corresponds to a reduction in off-target responses."}, {"title": "5 ARITHMETIC REASONING", "content": "Although the proposed method is particularly suitable for instruction-following tasks, the language-based nature of Language Imbalance Driven Rewarding actually makes it applicable to more difficult arithmetic reasoning task. Arithmetic reasoning is a task where language models often struggle (Ahn et al., 2024), and while they are considered language-agnostic (Brannon, 2005), existing LLMs demonstrate inconsistent reasoning capabilities across different languages (She et al., 2024)."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Base Model and Datasets The arithmetic reasoning task is conducted on Llama-3-8B-Instruct, starting with multilingual GSM8K (Cobbe et al., 2021) prompts. The implementation details are described in Appendix B.2.\nEvaluation and Metrics Multilingual Grade School Math (MGSM) benchmark (Shi et al., 2022), is constructed by manually translating 250 math problems from the GSM8K dataset (Cobbe et al., 2021) into ten diverse languages. The reasoning accuracy (Acc) on it across 5 training languages and 5 unseen languages is reported. Additionally, the off-target rate (Off-tag) of the reasoning responses is assessed using the LangDetect library\u00b9."}, {"title": "5.2 RESULTS", "content": "The results are presented in Table 7. A comparative experiment on multilingual alignment is conducted, where English responses were self-translated into other languages for SFT training. In training languages, our approach outperforms multilingual alignment, with M2 improving by 11.6% on average, compared to 7.8% for multilingual alignment. While multilingual alignment struggles to balance English and non-dominant languages, leading to a decline in English performance (-2.0%).\nFrom the unseen languages, M\u2081 achieves an average 16.2% improvement, surpassing the 11.6% improvement on training languages. This demonstrates that our method leverages language imbalance as a reward to learn language-agnostic reasoning alignment, which results in better generalization on unseen languages. In contrast, traditional multilingual alignment tends to overemphasize training languages to align English capabilities, resulting in much poorer generalization on unseen languages compared to our method (average 4.9% vs 16.2%)."}, {"title": "6 RELATED WORK", "content": "LLM Self-Improving The goal of LLM self-improvement is to enhance its capability by leveraging the knowledge embedded within the model itself. Self-improvement can be broadly divided into two categories: self-synthetic and self-critical. Self-synthetic involves generating synthetic training data using the model itself. For example, Self-Instruct (Bai et al., 2022; Wang et al., 2022) is a technique for generating prompts and responses independently, which can be utilized to enhance a base language model. Instruction backtranslation (Li et al., 2023c) similarly augments and curates training data by augmenting it through back-translation from web documents to generate instructions. Self-critical (Dubois et al., 2024; Li et al., 2023e; Fernandes et al., 2023; Bai et al., 2024; Saha et al., 2023) refers to using LLM-as-a-Judge to assess the quality of the data. Self-rewarding (Yuan et al., 2024) involves using the model itself, via LLM-as-a-Judge prompting, to provide its own reward mechanism. However, existing self-improvement methods focus solely on enhancing the overall capabilities of language models and do not explore self-improving across different languages within the model. This is precisely the insight of our work.\nMultilingual LLMs Contemporary LLMs (Touvron et al., 2023a;b; Team et al., 2024; Bai et al., 2023; Achiam et al., 2023) are predominantly trained on multilingual corpora. However, the language distribution in the data primarily focuses on English and Chinese. The imbalanced data distribution above has led to significant limitations in the capabilities of LLMs across most languages. To enhance the multilingual capabilities, one straightforward approach is multilingual training, using multilingual data during the pre-training (Conneau & Lample, 2019; Le Scao et al., 2023), instruction-following (Li et al., 2023b; Muennighoff et al., 2022) and post training (Dang et al., 2024). However, high-quality multilingual data, particularly for low-resource languages, remains scarce and expensive. The second approach is cross-lingual alignment, which seeks to bridge the performance gap by aligning non-dominant and dominant languages. This approach utilizes techniques such as cross-lingual transfer (Etxaniz et al., 2023; Huang et al., 2023; Ranaldi & Pucci, 2023; Qin et al., 2023), cross-lingual instruction tuning (Schuster et al., 2019; Wen-Yi & Mimno, 2023) and self-distillation (Zhang et al., 2024). Despite focusing on improving non-dominant languages through alignment with dominant ones, these methods remain constrained by the initial performance of dominant language. Our work eliminates the reliance on human-authored data and surpasses the performance ceiling for dominant languages."}, {"title": "7 CONCLUSION", "content": "This paper introduces Language Imbalance Driven Rewarding, which leverages the inherent imbalance between dominant and non-dominant languages in LLMs as a reward signal to bootstrap LLMs' multilingual capabilities in a self-improving manner. Starting from any instruction-following model with basic multilingual capabilities, this approach generates and self-translates the responses between dominant and non-dominant languages within LLMs, constructing preference ranking and adopting an Iterative DPO for training. This approach not only enhances LLM performance in non-dominant languages but also improves the dominant language's capacity. Experiments on Llama-3-8B-Instruct demonstrate significant improvements in instruction-following and arithmetic reasoning tasks. While much remains to be explored, this work paves the way for developing models capable of enhancing their multilingual abilities autonomously across all languages."}, {"title": "8 LIMITATIONS AND FUTURE WORK", "content": "Our work has certain limitations. The reward signal is derived from the inherent language imbalance within LLMs, which provides a coarse-grained signal. Developing more refined and accurate reward signals for multilingual self-improvement is an area we plan to explore in future work. Additionally, our approach relies on the LLM to self-translate the multilingual responses. Although LLMs outperform traditional machine translation systems, the translated responses may still exhibit artifacts, which hinders the response quality."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Base Models In our experiments, we use a widely adopted instruction-following model as our base model Mo, namely Llama-3-8B-Instruct (Meta, 2024). Llama-3-8B-Instruct, as an English-centric LLM, often encounters off-target issues when handling non-English requests. In Appendix A, our method is scaled to Qwen2-7B-Instruct (Yang et al., 2024) to verify its generalization capability.\nLanguages English is chosen as the common dominant language, and non-dominant languages include high-reward languages (German, Russian) and low-reward languages (Spanish, French). Additionally, Chinese is selected as an unseen language to observe the generalization of our approach. Note that unseen language means that not included in the training data.\nDatasets The Alpagasus dataset (Chen et al., 2023a) includes 9,000 high-quality instruction-following examples filtered from the original 52,000 in the Alpaca dataset (Taori et al., 2023). We sample 1,000 prompts from the Alpagasus dataset and translate them into other languages using the Google Translate API to obtain multilingual prompts.\nImplementation Details Models are trained for one epoch in each iteration across all experiments. During training, the global batch size is set to 16, and the learning rate is 5e-7. More details are described in Appendix B.4.\nEvaluation and Metrics\n\u2022 Head-to-head performance: Head-to-head performance is evaluated between the base model and the iterative models using GPT-4 (Achiam et al., 2023) as an evaluator (Liu et al., 2023) over 805 test prompts in X-AlpacaEval (Zhang et al., 2023). The detailed evaluation setup can be found in Appendix B.1.\n\u2022 X-AlpacaEval leaderboard: We extend the existing AlpacaEval 2.0 toolkit (Li et al., 2023d) from an English-only framework to a multilingual one and compare both proprietary and open-source multilingual models on their multilingual instruction-following abilities."}, {"title": "A SCALING TO QWEN2", "content": "Qwen2-7B-Instruct (Yang et al., 2024) exhibits stronger multilingual capabilities and seldom produces off-target responses. We believe scaling our experiments to a multilingual LLM enhances the comprehensiveness of the evaluation. Following the experimental setup outlined in Section 4, Qwen2-7B-Instruct was chosen as the base model to validate the generalizability of Language Imbalance Driven Rewarding."}, {"title": "A.1 HEAD-TO-HEAD PERFORMANCE", "content": "Figure 4 illustrates Qwen2's head-to-head performance, which is highly consistent with the results from the Llama3 performance.\nFor the training languages, M\u2081 demonstrates a significant improvement, with AW-L ranging from 21.0% to 31.2% compared to the base model. It demonstrates that Language Imbalance Driven Rewarding is effective. For the dominant language, English in M\u2081 gains 23.5% AW-L compared with Mo, while English in M2 gains 7.3% AW-L compared with M\u2081. These results indicate that the dominant language also benefits from the rejected responses translated from non-dominant languages."}, {"title": "A.2 X-ALPACAEVAL LEADERBOARD", "content": "The X-AlpacaEval leaderboard on Qwen2 as the base model is shown in 8. After two rounds of iterations, Qwen2-7B-Instrct achieved average improvements of 5.84% in win rates over GPT-4 Turbo across five languages, demonstrating performance comparable to 70B-level models."}, {"title": "A.3 MULTILINGUAL MT-BENCH", "content": "In Table 9, Qwen2-7B-Instruct initially achieved a high score of 8.05, reflecting its robust multilingual capabilities. Despite the strong performance reducing the effectiveness of the language imbalance-driven reward signal, Qwen2 improved its average score to 8.20 after two training iterations. This shows that even with high initial scores, our approach continues to improve performance through iterative refinement."}, {"title": "A.4 MULTILINGUAL NLP BENCHMARKS", "content": "Table 10 shows average performance across five training languages and Chinese on four benchmarks based on Qwen2, detailed in Appendix C.1. Slight performance improvements are observed in multilingual optimization iterations compared to the base models, indicating that the multilingual alignment process does not incur any alignment tax."}, {"title": "B IMPLEMENTATION DETAILS", "content": "B.1 EXPERIMENTAL DETAILS ON GENERAL INSTRUCTION-FOLLOWING\nHead-to-head Performance Considering the excellent multilingual understanding ability of GPT-4, we use GPT-42 as a judge to conduct the automatic evaluation. GPT-4 as an evaluator has a higher correlation with human judgements (Liu et al., 2023; Li et al., 2023e).\nSpecifically, we use pairwise evaluation, asking GPT-4 to determine the better response between (r1,r2) from different models, given instruction xi. During the evaluation, GPT-4 assigns a score from 0 to 10 based on the prompt in Appendix D.2.\nGPT-4 as an evaluator, exhibits a significant positional bias, showing a preference for responses that appear earlier (Zheng et al., 2024a). To mitigate this bias, we first request GPT-4 to evaluate"}, {"title": "B.2 EXPERIMENTAL DETAILS ON ARITHMETIC REASONING", "content": "Datasets We start from the GSM8K (Cobbe et al., 2021) dataset, which consists of 8.5K high-quality grade school math problems created by human problem writers in English. We utilize the instructions from the 7,473 training examples and translate them into multiple languages using the Google Translate API to construct the multilingual GSM8K instructions.\nWe input the multilingual GSM8K instructions into the model and explicitly constrain the model's response language in the prompt, as detailed in Appendix D.5, for both training and inference. We believe that by providing instructions in an explicit language and requiring the model to respond in that language, we can fully capture the model's reasoning abilities in that language. After obtaining the multilingual reasoning responses, we filter the responses with correct reasoning in English, followed by applying Language Imbalance Driven Rewarding."}, {"title": "B.3 EXPERIMENTS ENVIRONMENTS", "content": "All experiments were conducted on Ubuntu 22.04 equipped with 8 NVIDIA A100 GPUs. Our code mainly depends on Python 3.10 and PyTorch 2.3.0. we fine-tune all models using LLaMA-Factory (Zheng et al., 2024b) framework, and inference models with vLLM (Kwon et al., 2023) framework. Training for all models was launched with the accelerate (Gugger et al., 2022) in DeepSpeed ZERO Stage2 (Rajbhandari et al., 2021) and Flash-Attention 2 (Dao, 2023) mechanism."}, {"title": "B.4 HYPERPARAMETERS", "content": "All models are optimized using AdamW (Kingma & Ba, 2014), with a cosine learning rate scheduler that includes a warm-up phase constituting 3% of the total training duration. DPO+NLL runs are trained with KL-penalty \u03b2 = 0.1. The coefficient a is set to 1 for all experiments in the paper. The details of hyperparameters are shown in Table 11."}, {"title": "C DETAILED RESULTS AND ANALYSIS", "content": "C.1 MULTILINGUAL NLP BENCHMARKS\nWe list the detailed information of the benchmarks as follows:\n\u2022 MMLU (Massive Multitask Language Understanding) (Hendrycks et al., 2020) is a benchmark designed to evaluate the knowledge acquired during pre-training, focusing on zero-shot and few-shot settings. This makes it more challenging and closer to how humans are evaluated. The benchmark spans 57 subjects, including STEM, the humanities, and the social sciences. We test it in a 5-shot setting.\n\u2022 HellaSwag (Zellers et al., 2019) is a challenging dataset for evaluating commonsense NLI, which is particularly difficult for state-of-the-art models but trivial for humans. We test it in a zero-shot setting.\n\u2022 The AI2 Reasoning Challenge (ARC) dataset (Clark et al., 2018) is a multiple-choice question-answering dataset based on science exams for grades 3 to 9. It is divided into two partitions: Easy and Challenge, with the latter containing more difficult questions requiring reasoning. We test the ARC Challenge in a zero-shot setting.\n\u2022 TruthfulQA (Lin et al., 2021) is a benchmark designed to evaluate whether a language model generates truthful answers. It consists of 817 questions across 38 categories, including health, law, finance, and politics. Since evaluating generation tasks for truthfulness is challenging, the benchmark provides two multiple-choice formats: MC1 (Single-true) and MC2 (Multi-true), testing the ability to identify true statements. We test it in a zero-shot setting."}, {"title": "D PROMPTS TEMPLATE", "content": "D.1 GPT-4 SCORE PROMPT\nPrompt in GPT-4 Score\nYou are a helpful assistant tasked with scoring answers for a given instruction in [LANG].\nPlease evaluate the following answer based on the provided instruction in [LANG]. A good answer\nshould adhere to these criteria:\n1. It should be in [LANG], unless the instruction explicitly requests a different language.\n2. It should address the request made in the instruction.\n3. It should be factually and semantically coherent.\n4. It should be grammatically correct and fluent.\n5. It should be helpful, relevant, detailed, and accurate.\n\n[INSTRUCTION]\n\n[OUTPUT1]\n\nFIRST, provide a one-sentence explanation of your evaluation, detailing the reasoning behind your\nscore.\nSECOND, on a new line, state only the score on a scale from 0 to 10, where a higher score indicates\nbetter overall performance. Your response should follow this format:\nExplanation: \nScore:"}, {"title": "D.2 HEAD-TO-HEAD COMPARISON PROMPT", "content": "Prompt in Head-to-head Comparison\nGiven the question in [LANG] language. You are a helpful and precise assistant for checking the\nquality of the answer.\n\n[INSTRUCTION]\n\n[OUTPUT1]\n\n[OUTPUT2]\n\nA good answer should follow these rules:\n1. It should be in [LANG], unless the instruction explicitly requests a different language.\n2. It should be helpful, relevant, detailed and accurate.\n3. It should answer the request in the instruction\nPlease evaluate both answers with your justification, and only provide a score ranging from 0 to 10\nafter your justifications, the score must be an integer. The score for answer 1 should be wrapped by\n and , and the score for answer 2 should be wrapped by  and\n."}, {"title": "D.3 X-ALPACAEVAL PROMPT", "content": "Prompt modified with weighted_alpaca_eval_gpt4_turbo in AlpacaEval 2.\n<|im_start | >system\nYou are a highly efficient assistant, who evaluates and selects the best large language model (LLMs)\nbased on the quality of their responses to a given instruction. This process will be used to create a\nleaderboard reflecting the most accurate and human-preferred answers.\n<|im_end >\n<|im_start | >user\nI require a leaderboard for various large language models. I'll provide you with prompts given to\nthese models and their corresponding outputs. Your task is to assess these responses, and select the\nmodel that produces the best output from a human perspective.\n## Instruction\n{\n\"instruction\": \"\"\"{instruction}\"\"\"\n}\n## Model Outputs\nHere are the unordered outputs from the models. Each output is associated with a specific\nmodel, identified by a unique model identifier.\n{\n{\n\"model_identifier\": \"m\",\n\"output\": \"\"\"{output_1}\"\"\"\n},\n{\n\"model_identifier\": \"M\",\n\"output\": \"\"\"{output_2}\"\"\"\n}\n}\n## Task\nA good output should be in the same language as the instruction, except when the instruction\nexplicitly requests the output in a different language. Evaluate the models based on the quality and\nrelevance of their outputs, and select the model that generated the best output. Answer by providing\nthe model identifier of the best model. We will use your output as the name of the best model, so make\nsure your output only contains one of the following model identifiers and nothing else (no quotes, no\nspaces, no new lines, ...): m or M.\n## Best Model Identifier\n<|im_end | >"}, {}]}