{"title": "\u039a\u0391\u039d 2.0: Kolmogorov-Arnold Networks Meet Science", "authors": ["Ziming Liu", "Pingchuan Ma", "Yixuan Wang", "Wojciech Matusik", "Max Tegmark"], "abstract": "A major challenge of AI + Science lies in their inherent incompatibility: today's Al is primarily based on connectionism, while science depends on symbolism. To bridge the two worlds, we propose a framework to seamlessly synergize Kolmogorov-Arnold Networks (KANs) and science. The framework highlights KANs' usage for three aspects of scientific discovery: identifying relevant features, revealing modular structures, and discovering symbolic formulas. The synergy is bidirectional: science to KAN (incorporating scientific knowledge into KANs), and KAN to science (extracting scientific insights from KANs). We highlight major new functionalities in pykan: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN compiler that compiles symbolic formulas into KANS. (3) tree converter: convert KANs (or any neural networks) to tree graphs. Based on these tools, we demonstrate KANs' capability to discover various types of physical laws, including conserved quantities, Lagrangians, symmetries, and constitutive laws.", "sections": [{"title": "Introduction", "content": "In recent years, AI + Science has emerged as a promising new field, leading to significant scientific advancements including protein folding prediction [37], automated theorem proving [95, 83], weather forecast [41], among others. A common thread among these tasks is that they can all be well formulated into problems with clear objectives, optimizable by black-box AI systems. While this paradigm works exceptionally well for application-driven science, a different kind of science exists: curiosity-driven science. In curiosity-driven research, the procedure is more exploratory, often lacking clear goals beyond \"gaining more understanding\". To clarify, curiosity-driven science is far from useless; quite the opposite. The scientific knowledge and understanding gained through curiosity often lay a solid foundation for tomorrow's technology and foster a wide range of applications.\nAlthough both application-driven and curiosity-driven science are invaluable and irreplaceable, they ask different questions. When astronomers observe the motion of celestial bodies, application-driven researchers focus on predicting their future states, while curiosity-driven researchers explore the physics behind the motion. Another example is AlphaFold, which, despite its tremendous success in predicting protein structures, remains in the realm of application-driven science because it does not provide new knowledge at a more fundamental level (e.g., atomic forces). Hypothetically, Al-phaFold must have uncovered important unknown physics to achieve its highly accurate predictions. However, this information remains hidden from us, leaving AlphaFold largely a black box. Therefore, we advocate for new AI paradigms to support curiosity-driven science. This new paradigm of AI + Science demands a higher degree of interpretability and interactivity in AI tools so that they can be seamlessly integrated into scientific research.\nRecently, a new type of neural network called Kolmogorov-Arnold Network (KAN) [57], has shown promise for science-related tasks. Unlike multi-layer perceptrons (MLPs), which have fixed activa-tion functions on nodes, KANs feature learnable activation functions on edges. Because KANS can decompose high-dimensional functions into one-dimensional functions, interpretability can be gained by symbolically regressing these 1D functions. However, their definition of interpretability is somewhat narrow, equating it almost exclusively with the ability to extract symbolic formulas. This limited definition restricts their scope, as symbolic formulas are not always necessary or feasible in science. For example, while symbolic equations are powerful and prevalent and physics, systems in chemistry and biology the systems are often too complex to be represented by such equations. In these fields, modular structures and key features may be sufficient to characterize interesting aspects of these systems. Another overlooked aspect is the reverse task of embedding knowledge into KANs: How can we incorporate prior knowledge into KANs, in the spirit of physics-informed learning?\nWe enhance and extend KANs to make them easily used for curiosity-driven science. The goal of this paper can be summarized as follows:\nGoal: Synergize Kolmogorov-Arnold Networks \u2192 Science.\n: Build in scientific knowledge to KANs (Section 3).\n\u21d2: Extract out scientific knowledge from KANS (Section 4).\nTo be more concrete, scientific explanations may have different levels, ranging from the coars-est/easiest/correlational to the finest/hardest/causal:\n\u2022 Important features: For example, \"y is fully determined by 11 and 12, while other factors do no matter.\" In other words, there exists a function f such that y = f(x1,x2).\n\u2022 Modular structures: For instance, \"x1 and 22 contributes to y independently in an additive way.\" This means there exists functions g and h such that y = g(x1) + h(x2).\n\u2022 Symbolic formulas: For example, \"y depends on x1 as a sine function and on 22 as an exponential function\". In other words, y = sin(x1) + exp(x2).\nThe paper reports on how to incorporate and extract these properties from KANs. The structure of the paper is as follows (illustrated in Figure 1): In Section 2, we augment the original KAN with multiplication nodes, introducing a new model called MultKAN. In Section 3, we explore ways to embed scientific inductive biases into KANs, focusing on important features (Section 3.1), modular"}, {"title": "MultKAN: Augmenting KANs with multiplications", "content": "The Kolmogorov-Arnold representation theorem (KART) states that any continuous high-dimensional function can be decomposed into a finite composition of univariate continuous functions and additions:\n$$f(x) = f(x_1,\\dots,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q \\left( \\sum_{p=1}^{n} \\phi_{q,p}(x_p) \\right).$$ (1)\nThis implies that addition is the only true multivariate operation, while other multivariate operations (including multiplication) can be expressed as additions combined with univariate functions. For example, to multiply two positive numbers x and y, we can express this as $$xy = \\exp(\\log x + \\log y)$$\\nwhose right-hand side only consists of addition and univariate functions (log and exp).\nIf x and y can be negative, one may choose a large c > 0 and express $$xy = \\exp(\\log(x + c) + \\log(y + c)) - c(x + y) \u2013 c^2$$. Other constructions include quadratic functions, such as $$xy = ((x + y)^2 - (x - y)^2)/4$$\nor $$xy = ((x + y)^2 \u2013 x^2 \u2013 y^2)/2$$.\nHowever, given the prevalence of multiplications in both science and everyday life, it is desirable to explicitly include multiplications in KANs, which could potentially enhance both interpretability and capacity.\nKolmogorov-Arnold Network (KAN) While the KART Eq. (1) corresponds to a two-layer net-work, Liu et al. [57] managed to extend it to arbitrary depths by recognizing that seemingly different outer functions Pq and inner functions $q,p can be unified through their proposed KAN layers. A depth-L KAN can be constructed simply by stacking L KAN layers. The shape of a depth-L KAN is represented by an integer array [no, n\u2081,\u2026\u2026, n\u2081] where ni denotes the number of neurons in the [th neuron layers. The lth KAN layer, with ni input dimensions and ni+1 output dimensions, transforms an input vector xi \u2208 Rni to X1+1 \u2208 Rni+1\n$$X_{l+1} = \\begin{pmatrix} \\Phi_{l,1,1}(\\cdot) & \\Phi_{l,1,2}(\\cdot) & \\dots & \\Phi_{l,1,n_{l+1}}(\\cdot) \\\\  \\Phi_{l,2,1}(\\cdot) & \\Phi_{l,2,2}(\\cdot) & \\dots & \\Phi_{l,2,n_{l+1}}(\\cdot) \\\\  \\vdots & \\vdots & \\ddots & \\vdots \\\\  \\Phi_{l,n_l,1}(\\cdot) & \\Phi_{l,n_l,2}(\\cdot) & \\dots & \\Phi_{l,n_l,n_{l+1}}(\\cdot) \\end{pmatrix} X_l,$$ (2)\nand the whole network is a composition of L KAN layers, i.e.,\n$$\\text{KAN}(x) = (\\Phi_{L-1} \\circ \\dots \\circ \\Phi_1 \\circ \\Phi_0)x.$$ (3)\nIn diagrams, KANs can be intuitively visualized as a network consisting of nodes (summation) and edges (learnable activations), as shown in Figure 2 top left. When trained on the dataset generated from f(x,y) = xy, the KAN (Figure 2 bottom left) uses two addition nodes, making it unclear what the network is doing. However, after some consideration, we realize it leverages the equality $$xy = ((x + y)^2 \u2013 (x - y)^2)/4$$\nbut this is far from obvious.\nMultiplicative Kolmogorov-Arnold Networks (MultKAN) To explicitly introduce multiplication operations, we propose the MultKAN, which can reveal multiplicative structures in data more clearly. A MultKAN (shown in Figure 2 top right) is similar to a KAN, with both having stan-dard KAN layers. We refer to the input nodes of a KAN layer as nodes, and the output nodes of a KAN layer subnodes. The difference between KAN and MultKAN lies in the transformations from the current layer's subnodes to the next layer's nodes. In KANs, nodes are directly copied from the previous layer's subnodes. In MultKANs, some nodes (addition nodes) are copied from correspond-ing subnodes, while other nodes (multiplication nodes) perform multiplication on k subnodes from the previous layer. For simplicity, we set k = 2 below 3.\nBased on the MultKAN diagram (Figure 2 top right), it can be intuitively understood that a MultKAN is a normal KAN with optional multiplications inserted in. To be mathematically pre-cise, we define the following notations: The number of addition (multiplication) operations in layer l are denoted as $$n_a$$ ($$n_m$$), respectively. These are collected into arrays: addition width $$n^a = [n_0, n_1,\\dots,n_q]$$ and multiplication width $$n^m = [n_0^m, n_1^m,\\dots,n_l^m]$$. When $$n_l^a = n_l^m = n_L = 0$$, the MultKAN reduces to a KAN. For example, Figure 2 (top right) shows a MultKAN with $$n^a = [2, 2, 1]$$ and $$n^m = [0, 2,0]$$.\nA MultKAN layer consists of a standard KANLayer \u03a6\u03b9 and a multiplication layer \u039c\u03b9. \u03a6\u03b9 takes in an input vector x\u012e \u2208 Rni+n\u2122 and outputs z\u2081 = \u03a6\u2081(x) \u2208 Rni+1+2n+1. The multiplication layer consists of two parts: the multiplication part performs multiplications on subnode pairs, while the other part performs identity transformation. Written in Python, M\u2081 transforms z\u012b as follows:\n$$M_l(z_l) = \\text{concatenate}(z_l[: n_{l+1}^a], z_l[n_{l+1}^a :: 2] \\odot z_l[n_{l+1}^a + 1 :: 2]) \\in [R^{n_{l+1}^a + n_{l+1}^m},$$ (4)\nwhere $$ \\odot $$ is element-wise multiplication. The MultKANLayer can be succinctly represented as \u03a8\u2081 = \u039c\u03b9 \u03bf \u03a6\u03b9. The whole MultKAN is thus:\n$$\\text{MultKAN}(x) = (\\Psi_L \\circ \\Psi_{L-1} \\circ \\dots \\circ \\Psi_1 \\circ \\Psi_0)x.$$ (5)\nSince there are no trainable parameters in multiplication layers, all sparse regularization techniques (e.g., l\u2081 and entropy regularization) for KANs [57] can be directly applied to MultKANs. For"}, {"title": "Science to KANS", "content": "In science, domain knowledge is crucial, allowing us to work effectively even with small or zero data. Therefore, it is beneficial to adopt a physics-informed approach for KANs: we should in-corporate available inductive biases into KANs while preserving their flexibility to discover new physics from data.\nWe explore three types of inductive biases that can be integrated into KANs. From the coars-est/easiest/correlational to the finest/hardest/causal, they are important features (Section 3.1), mod-ular structures (Section 3.2) and symbolic formulas (Section 3.3)."}, {"title": "Adding important features to KANS", "content": "In a regression problem, the goal is to find a function f such that y = f(x1,x2,\u00b7\u00b7\u00b7., xn). Suppose we want to introduce an auxiliary input variable a = a(x1, x2,...,xn), transforming the function to y = f(x1,\u2026\u2026,xn,a). Although the auxiliary variable a does not add new information, it can increase the expressive power of the neural network. This is because the network does not need to expend resources to calculate the auxiliary variable. Additionally, the computations may become simpler, leading to improved interpretability. Users can add auxiliary features to inputs using the augment_input method:\nmodel.augment_input(original_variables, auxiliary_variables, dataset) (6)\nAs an example, consider the formula for relativistic mass $$m(m_0, v, c) = m_0/ \\sqrt{1 \u2013 (v/c)^2}$$ where mo is the rest mass, v is the velocity of the point mass, and e is the speed of light. Since physicists often work with dimensionless numbers $$\u03b2 = v/c$$ and $$\u03b3 = 1/\\sqrt{1\u2013 \u03b2^2} = 1/ \\sqrt{1 \u2013 (v/c)^2}$$, they might introduce \u03b2 and y alongside v and c as inputs. Figure 3, shows KANs with and without these auxiliary variables: (a) illustrates the KAN compiled from the symbolic formula (see Section 3.3 for the KAN compiler), which requires 5 edges; (b)(c) shows KANs with auxiliary variables, requiring only 2 or 3 edges and achieving loses of 10-6 and 10\u20134, respectively. Note that (b) and (c) differ only in random seeds. Seed 1 represents a sub-optimal solution because it also identifies $$\u03b2 = v/c$$\nas a key feature. This is not surprising, as in the classical limit v \u226a c, $$\u03b3 = 1/\\sqrt{1 \u2212 (v/c)^2} \u2248 1 + (v/c)^2/2 = 1 + \u03b2^2/2$$. The variation due to different seeds can be seen either as a feature or a bug: As a feature, this diversity can help find sub-optimal solutions which may nevertheless offer"}, {"title": "Building modular structures to KANS", "content": "Modularity is prevalent in nature: for example, the human cerebral cortex is divided into several functionally distinct modules, each of these modules responsible for specific tasks such as percep-tion or decision making. This modularity simplifies the understanding of neural networks, as it allows us to interpret clusters of neurons collectively rather than analyzing each neuron individually. Structural modularity is characterized by clusters of connections where intra-cluster connections are much stronger than inter-cluster ones. To enforce modularity, we introduce the module method, which preserves intra-cluster connections while removing inter-cluster connections. The modules are specified by users. The syntax is\nmodel.module(start_layer_id, \u2018[nodes_id]->[subnodes_id]->[nodes_id]...') (7)\nFor example, if a user wants to assign specific nodes/subnodes to a module say, the Oth node in layer 1, the 1st and 3rd subnode in layer 1, the 1st and 3rd node in layer 2 \u2013 they might use module(1,\u2018[0]->[1,3]->[1,3]'). To be concrete, there are two types of modularity: separabil-ity and symmetry.\nSeparability We say a function is considered separable if it can be expressed as a sum or product of functions of non-overlapping variable groups. For example, a four-variable function f(X1,X2,X3,14) is maximally multiplicatively separable if it has the form f1(x1) f2(x2) f3(x3) f4(x4), creating four distinct groups (1), (2), (3), (4). Users can create these modules by calling the module method four times: module(0,\u2018[i]->[i]'), i = 0,1,2,3, shown in Figure 4 (a). The final call may be skipped since the first three are sufficient to de-fine the groups. Weaker forms of multiplicative separability might be f1(x1,x2) f2(x3,x4) (calling module(0,\u2018[0,1]->[0,1]')) or f1(x1) f2(x2, X3, X4) (calling module(0,\u2018[0] -> [0]\u2019)).\nGeneralized Symmetry We say a function is symmetric in variables (x1, x2) if $$f (x_1, x_2, x_3, \\dots) = g(h(x_1,x_2), X_3,\\dots)$$. This property is termed symmetry because the value of f remains un-changed as long as h(x1,x2) is constant, even if x1 and 12 vary. For example, a function f is rotational invariant in 2D if $$f(x_1,x_2) = g(r)$$, where $$r = \\sqrt{x_1^2 + x_2^2}$$. When symmetry in-volves only a subset of variables, it can be considered hierarchical since 21 and 22 interact first through h (2-Layer KAN), and then h interacts with other variables via g (2-Layer KAN). Sup-pose a four-variable function has a hierarchical form $$f(x_1,x_2,x_3,x_4) = h(f(x_1,x_2), g(x_3, x_4))$$,"}, {"title": "Compiling symbolic formulas to KANS", "content": "Scientists often find satisfaction in representing complex phenomena through symbolic equations. However, while these equations are concise, they may lack the expressive power needed to capture"}, {"title": "KANs to Science", "content": "Today's black box deep neural networks are powerful, but interpreting these models remains chal-lenging. Scientists seek not only high-performing models but also the ability to extract meaningful knowledge from the models. In this section, we focus on enhancing the interpretability of KANS scientific purposes. We will explore three levels of knowledge extraction from KANs, from the most basic to the most complex: important features (Section 4.1), modular structures (Section 4.2), and symbolic formulas (Section 4.3)."}, {"title": "Identifying important features from KANS", "content": "Identifying important variables is crucial for many tasks. Given a regression model f where y \u2248 f(x1,x2,...,xn), we aim to assign scores to the input variables to gauge their importance. Liu et al. [57], used the function L1 norm to indicate the importance of edges, but this metric could be problematic as it only considers local information.\nTo address this, we introduce a more effective attribution score which better reflects the importance of variables than the L1 norm. For simplicity, let us assume there are multiplication nodes, so we do not need to differentiate between nodes and subnodes 4. Suppose we have an L-layer KAN with width [no, n1,\u2026\u2026,nL]. We define El,i,j as the standard deviation of the activations on the (l, i, j) edge, and Ni,i as the standard deviation of the activations on the (l, i) node. We then define the node (attribution) score Ali and the edge (attribution) score Bl,i,j. In [57], we simply defined Bl,i,j = El,i,j and Al,i = N\u0131,i. However, this definition fails to account for the later parts of the network; even if a node or an edge has a large norm itself, it may not contribute to the output if the rest of the network is effectively a zero function. Therefore, we now compute node and edge scores"}, {"title": "Identifying modular structures from KANS", "content": "Although the attribution score provides valuable insights into which edges or nodes are important, it does not reveal modular structures, i.e., how the important edges and nodes are connected. In this part, we aim to uncover modular structures from trained KANs and MLPs by examining two types of modularity: anatomical modularity and functional modularity."}, {"title": "Anatomical modularity", "content": "Anatomical modularity refers to the tendency for neurons placed close to each other spatially to have stronger connections than those further apart. Although artificial neural networks lack physical spatial coordinates, introducing the concept of physical space has been shown to enhance inter-pretability [51, 52]. We adopt the neuron swapping method from [51, 52], which shortens connec-tions while preserving the network's functionality. We call the method auto_swap. The anatomi-cal modular structure revealed through neuron swapping facilitates easy identification of modules, even visually, for two tasks shown Figure 7: (1) multitask sparse parity; and (2) hierarchical ma-jority voting. For multitask sparse parity, we have 10 input bits xi \u2208 {0,1},i = 1,2,\u2026\u2026,10, and output $$Y_j = x_{2j-1} \\oplus x_{2j}, j = 1,\\dots,5$$, where denotes modulo 2 addition. The task exhibits modularity because each output depends only on a subset of inputs. auto_swap suc-cessfully identifies modules for both KANs and MLPs, with the KAN discovering simpler mod-ules. For hierarchical majority voting, with 9 input bits xi \u2208 {0,1}, i = 1,\u2026\u2026,9, and the output y = maj(maj(x1, x2, x3), maj(X4, X5, X6), maj (17,18,19)), where maj stands for majority voting (output 1 if two or three inputs are 1, otherwise 0). The KAN reveals the modular structure even before auto_swap, and the diagram becomes more organized after auto_swap. The MLP shows some modular structure from the pattern of the first layer weights, indicating interactions among variables, but the global modular structure remains unclear regardless of auto_swap."}, {"title": "Functional modularity", "content": "Functional modularity pertains to the overall function represented by the neural network. Given an Oracle network where internal details such as weights and hidden layer activations are inaccessible (too complicated to analyze), we can still gather information about functional modularity through forward and backward passes at the inputs and outputs. We define three types of functional modu-larity (see Figure 8 (a)), based largely on [84].\nSeparability: A function f is additively separable if\n$$f(x_1, x_2,\\dots,x_n) = g(x_1,...,x_k) + h(x_{k+1},...,x_n).$$ (10)\nNote that $$\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = 0$$ when 1 \u2264 i \u2264 k, k + 1 \u2264 j \u2264 n. To detect the separability, we can compute the Hessian matrix $$H = \\nabla \\nabla^T f$$ $$(H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j})$$ and check for block structure. If $$H_{ij} = 0$$ for"}, {"title": "Compiling symbolic formulas to KANS", "content": "Scientists often find satisfaction in representing complex phenomena through symbolic equations. However, while these equations are concise, they may lack the expressive power needed to capture"}, {"title": "Discovering conserved quantities", "content": "Conserved quantities are physical quantities that remain constant over time. For example, a free-falling ball converts its gravitational potential energy into kinetic energy, while the total energy (the sum of both forms of energy) remains constant (assuming negligible air resistance). Conserved quantities are crucial because they often correspond to symmetries in physical systems and can sim-plify calculations by reducing the dimensionality of the system. Traditionally, deriving conserved quantities with paper and pencil can be time-consuming and demands extensive domain knowl-edge. Recently, machine learning techniques have been explored to discover conserved quanti-ties [55, 53, 54, 58, 32, 89]."}, {"title": "Discovering Lagrangians", "content": "In physics, Lagrangian mechanics is a formulation of classical mechanics based on the principle of stationary action. It describes a mechanical system using phase space and a smooth function L known as the Lagrangian. For many systems, L = T \u2013 V, where T and V represent the kinetic and potential energy of the system, respectively. The phase space is typically described by (q, q), where q and q denotes coordinates and velocities, respectively. The equation of motion can be derived from the Lagrangian via the Euler-Lagrange equation: $$\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{q}} \\right) = \\frac{\\partial L}{\\partial q}$$, or equivalently\n$$\\ddot{q} = (\\nabla_{\\dot{q}} \\nabla_{\\dot{q}} L)^{-1} [\\nabla_q L - (\\nabla_{\\dot{q}} \\nabla_{q} L) \\dot{q}].$$ (18)\nGiven the fundamental role of the Lagrangian, an interesting question is whether we can infer the La-grangian from data. Following [19], we train a Lagrangian neural network to predict q from (q, q). An LNN uses an MLP to parameterize L(q, q), and computes the Eq. (18) to predict instant accel-erations q. However, LNNs face two main challenges: (1) The training of LNNs can be unstable"}, {"title": "Discovering hidden symmetry", "content": "Philip Anderson famously argued that \u201cit is only slightly overstating that case to say that physics is the study of symmetry\", emphasizing how the discovery of symmetries has been invaluable for both deepening our understanding and solving problems more efficiently.\nHowever, symmetries are sometimes not manifest but hidden, only revealed by applying some co-ordinate transformation. For example, after Schwarzschild discovered his eponymous black hole metric, it took 17 years for Painlev\u00e9, Gullstrand and Lema\u00eetre to uncover its hidden translational symmetry. They demonstrated that the spatial sections could be made translationally invariant with a clever coordinate transformation, thereby deepening our understanding of black holes [65]. Liu & Tegmark [56] showed that the Gullstrand-Painlev\u00e9 transformation can be discovered by training an MLP in minutes. However, they did not get extremely high precision (i.e., machine precision) for the solution. We attempt to revisit this problem using KANS."}, {"title": "Learning constitutive laws", "content": "A constitutive law defines the behavior and properties of a material by modeling how it responds to external forces or deformations. One of the simplest forms of constitutive law is Hooke's Law [34], which relates the strain and stress of elastic materials linearly. Constitutive laws encompass a wide range of materials, including elastic materials [80, 68], plastic materials [64], and fluids [8]. Tra-ditionally, these laws were derived from first principles based on theoretical and experimental stud-ies [79, 81, 6, 29]. Recent advancements, however, have introduced data-driven approaches that leverage machine learning to discover and refine these laws from dedicated datasets [73, 91, 59, 60]."}, {"title": "KAN interpolates between software 1.0 and 2.0", "content": "KAN interpolates between software 1.0 and 2.0 The key difference between Kolmogorov-Arnold Networks (KANs) and other neural networks (software 2.0, a term coined by Andrej Karpathy) lies in their greater interpretability, which allows for manipulation by users, similar to traditional soft-ware (software 1.0). However, KANs are not entirely traditional software, as they (1) learnability (good), enabling them to learn new things from data, and (2) reduced interpretability (bad) as they"}, {"title": "Efficiency improvement", "content": "Efficiency improvement The original pykan package [57] was poor in efficiency. We have incor-porated a few techniques to improve its efficiency.\n1. Efficient splines evaluations. Inspired by Efficient KAN [9], we have optimized spline eval-uations by avoiding unnecessary input expansions. For a KAN with L layers, N neurons per layer, and grid size G, memory usage has been reduced from O(LN2G) to O(LNG).\n2. Enabling the symbolic branch only when needed. A KAN layer contains both a spline branch and a symbolic branch. The symbolic branch is much more time-consuming than the spline branch since it cannot be parallelized (disastrous double loops are needed). However, in many applications, the symbolic branch is unnecessary, so we can skip it when possible, significantly reducing runtime, especially when the network is large.\n3. Saving intermediate activations only when needed. To plot KAN diagrams, intermediate activations must be saved. Initially, activations were saved by default, leading to slower runtime and excessive memory usage. We now save intermediate activations only when needed (e.g., for plotting or applying regularizations in training). Users can enable these efficiency improvements with a single line: model.speed().\n4. GPU acceleration. Initially, all models were run on CPUs due to the small-scale nature of the problems. We have now made the model GPU-compatible 6. For example, training a [4,100,100,100,1] with Adam for 100 steps used to take an entire day on a CPU (before implementing 1, 2, 3), but now takes 20 seconds on a CPU and less than one second on a GPU. However, KANs still lag behind MLPs in efficiency, especially at large scales. The community has been working towards benchmarking and improving KAN's efficiency and the efficiency gap has been significantly reduced [36]."}, {"title": "Interpretability", "content": "Interpretability Although the learnable univariate functions in KANs are more interpretable than weight matrices in MLPs, scalability remains a challenge. As KAN models scale up, even if all spline functions are interpretable individually, it becomes increasingly difficult to manage the com-bined output of these 1D functions. Consequently, a KAN may only remain interpretable when the network scale is relatively small (Figure 14 (b), thick red line). It is important to note that interpretability depends on both intrinsic factors (related to the model itself) and extrinsic factors (related to interpretability methods). Advanced interpretability methods should be able to handle interpretability at various levels. For example, by interpreting KANs with symbolic regression, modularity discovery and feature attribution (Figure 14 (b), thin red lines), the Pareto Frontier of interpretability versus scale extends beyond what a KAN alone can achieve. A promising direction for future research is to develop more advanced interpretability methods that can further push the current Pareto Frontiers."}, {"title": "Future work", "content": "Future work This paper introduces a framework that integrates KANs with scientific knowledge, focusing primarily on small-scale, physics-related examples. Moving forward, two promising direc-tions include applying this framework to larger-scale problems and extending it to other scientific disciplines beyond physics."}]}