{"title": "\u039a\u0391\u039d 2.0: Kolmogorov-Arnold Networks Meet Science", "authors": ["Ziming Liu", "Pingchuan Ma", "Yixuan Wang", "Wojciech Matusik", "Max Tegmark"], "abstract": "A major challenge of AI + Science lies in their inherent incompatibility: today's Al is primarily based on connectionism, while science depends on symbolism. To bridge the two worlds, we propose a framework to seamlessly synergize Kolmogorov-Arnold Networks (KANs) and science. The framework highlights KANs' usage for three aspects of scientific discovery: identifying relevant features, revealing modular structures, and discovering symbolic formulas. The synergy is bidirectional: science to KAN (incorporating scientific knowledge into KANs), and KAN to science (extracting scientific insights from KANs). We highlight major new functionalities in pykan: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN compiler that compiles symbolic formulas into KANS. (3) tree converter: convert KANs (or any neural networks) to tree graphs. Based on these tools, we demonstrate KANs' capability to discover various types of physical laws, including conserved quantities, Lagrangians, symmetries, and constitutive laws.", "sections": [{"title": "Introduction", "content": "In recent years, AI + Science has emerged as a promising new field, leading to significant scientific advancements including protein folding prediction [37], automated theorem proving [95, 83], weather forecast [41], among others. A common thread among these tasks is that they can all be well formulated into problems with clear objectives, optimizable by black-box AI systems. While this paradigm works exceptionally well for application-driven science, a different kind of science exists: curiosity-driven science. In curiosity-driven research, the procedure is more exploratory, often lacking clear goals beyond \"gaining more understanding\". To clarify, curiosity-driven science is far from useless; quite the opposite. The scientific knowledge and understanding gained through curiosity often lay a solid foundation for tomorrow's technology and foster a wide range of applications.\nAlthough both application-driven and curiosity-driven science are invaluable and irreplaceable, they ask different questions. When astronomers observe the motion of celestial bodies, application-driven researchers focus on predicting their future states, while curiosity-driven researchers explore the physics behind the motion. Another example is AlphaFold, which, despite its tremendous success in predicting protein structures, remains in the realm of application-driven science because it does not provide new knowledge at a more fundamental level (e.g., atomic forces). Hypothetically, AlphaFold must have uncovered important unknown physics to achieve its highly accurate predictions. However, this information remains hidden from us, leaving AlphaFold largely a black box. Therefore, we advocate for new AI paradigms to support curiosity-driven science. This new paradigm of AI + Science demands a higher degree of interpretability and interactivity in AI tools so that they can be seamlessly integrated into scientific research.\nRecently, a new type of neural network called Kolmogorov-Arnold Network (KAN) [57], has shown promise for science-related tasks. Unlike multi-layer perceptrons (MLPs), which have fixed activa- tion functions on nodes, KANs feature learnable activation functions on edges. Because KANS can decompose high-dimensional functions into one-dimensional functions, interpretability can be gained by symbolically regressing these 1D functions. However, their definition of interpretability is somewhat narrow, equating it almost exclusively with the ability to extract symbolic formulas. This limited definition restricts their scope, as symbolic formulas are not always necessary or feasible in science. For example, while symbolic equations are powerful and prevalent and physics, systems in chemistry and biology the systems are often too complex to be represented by such equations. In these fields, modular structures and key features may be sufficient to characterize interesting aspects of these systems. Another overlooked aspect is the reverse task of embedding knowledge into KANs: How can we incorporate prior knowledge into KANs, in the spirit of physics-informed learning?\nWe enhance and extend KANs to make them easily used for curiosity-driven science. The goal of this paper can be summarized as follows:"}, {"title": "MultKAN: Augmenting KANs with multiplications", "content": "The Kolmogorov-Arnold representation theorem (KART) states that any continuous high- dimensional function can be decomposed into a finite composition of univariate continuous func- tions and additions:\n\\(f(x) = f(x_1,\\dots,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q\\left(\\sum_{p=1}^{n} \\phi_{q,p}(x_p)\\right).\\)\nThis implies that addition is the only true multivariate operation, while other multivariate operations (including multiplication) can be expressed as additions combined with univariate functions. For example, to multiply two positive numbers x and y, we can express this as \\(xy = exp(logx +logy)\\) whose right-hand side only consists of addition and univariate functions (log and exp).\nMultiplicative Kolmogorov-Arnold Networks (MultKAN) To explicitly introduce multiplication operations, we propose the MultKAN, which can reveal multiplicative structures in data more clearly. A MultKAN (shown in Figure 2 top right) is similar to a KAN, with both having stan- dard KAN layers. We refer to the input nodes of a KAN layer as nodes, and the output nodes of a KAN layer subnodes. The difference between KAN and MultKAN lies in the transformations from the current layer's subnodes to the next layer's nodes. In KANs, nodes are directly copied from the previous layer's subnodes. In MultKANs, some nodes (addition nodes) are copied from correspond- ing subnodes, while other nodes (multiplication nodes) perform multiplication on k subnodes from the previous layer. For simplicity, we set k = 2 below.\nBased on the MultKAN diagram (Figure 2 top right), it can be intuitively understood that a MultKAN is a normal KAN with optional multiplications inserted in. To be mathematically pre- cise, we define the following notations: The number of addition (multiplication) operations in layer l are denoted as \\(n_a (n_m)\\), respectively. These are collected into arrays: addition width \\(n^a = [n_0, n_1,\\dots,n_L]\\) and multiplication width \\(n^m = [n_0, n_1,\\dots,n_L]\\). When \\(n^a = n^m = n_L = 0\\), the MultKAN reduces to a KAN. For example, Figure 2 (top right) shows a MultKAN with \\(n^a = [2, 2, 1]\\) and \\(n^m = [0, 2,0]\\).\nA MultKAN layer consists of a standard KANLayer \\(\\Phi_l\\) and a multiplication layer \\(M_l\\). \\(\\Phi_l\\) takes in an input vector \\(x_l \\in \\mathbb{R}^{n_i+n_m}\\) and outputs \\(z_l = \\Phi_l(x_l) \\in \\mathbb{R}^{n_{i+1}+2n^m+1}\\). The multiplication layer consists of two parts: the multiplication part performs multiplications on subnode pairs, while the other part performs identity transformation. Written in Python, \\(M_l\\) transforms \\(z_l\\) as follows:\n\\(M_l(z_l) = concatenate(z_l[: n_{i+1}], z_l[n_{i+1} :: 2] \\odot z_l[n_{i+1} + 1 :: 2]) \\in [\\mathbb{R}^{n_{i+1}+n^m+1},\\)\nwhere \\(\\odot\\) is element-wise multiplication. The MultKANLayer can be succinctly represented as \\(\\Psi_l = M_l \\circ \\Phi_l\\). The whole MultKAN is thus:\n\\(MultKAN(x) = (\\Psi_L \\circ \\Psi_{L-1} \\circ \\cdots \\circ \\Psi_1 \\circ \\Psi_0)x.\\)\nSince there are no trainable parameters in multiplication layers, all sparse regularization techniques (e.g., \\(l_1\\) and entropy regularization) for KANs [57] can be directly applied to MultKANs. For"}, {"title": "Science to KANS", "content": "In science, domain knowledge is crucial, allowing us to work effectively even with small or zero data. Therefore, it is beneficial to adopt a physics-informed approach for KANs: we should in- corporate available inductive biases into KANs while preserving their flexibility to discover new physics from data.\nWe explore three types of inductive biases that can be integrated into KANs. From the coars- est/easiest/correlational to the finest/hardest/causal, they are important features (Section 3.1), mod- ular structures (Section 3.2) and symbolic formulas (Section 3.3)."}, {"title": "Adding important features to KANS", "content": "In a regression problem, the goal is to find a function f such that \\(y = f(x_1,x_2,\\cdots, x_n)\\). Suppose we want to introduce an auxiliary input variable \\(a = a(x_1, x_2,\\dots,x_n)\\), transforming the function to \\(y = f(x_1,\\dots,x_n,a)\\). Although the auxiliary variable a does not add new information, it can increase the expressive power of the neural network. This is because the network does not need to expend resources to calculate the auxiliary variable. Additionally, the computations may become simpler, leading to improved interpretability. Users can add auxiliary features to inputs using the augment_input method:\nmodel.augment_input(original_variables, auxiliary_variables, dataset)\nAs an example, consider the formula for relativistic mass \\(m(m_0, v, c) = m_0/ \\sqrt{1 - (v/c)^2}\\) where \\(m_0\\) is the rest mass, v is the velocity of the point mass, and e is the speed of light. Since physicists often work with dimensionless numbers \\(\\beta = v/c\\) and \\(\\gamma = 1/\\sqrt{1- \\beta^2} = 1/\\sqrt{1 - (v/c)^2}\\), they might introduce \\(\\beta\\) and \\(\\gamma\\) alongside v and c as inputs. Figure 3, shows KANs with and without these auxiliary variables: (a) illustrates the KAN compiled from the symbolic formula (see Section 3.3 for the KAN compiler), which requires 5 edges; (b)(c) shows KANs with auxiliary variables, requiring only 2 or 3 edges and achieving loses of \\(10^{-6}\\) and \\(10^{-4}\\), respectively. Note that (b) and (c) differ only in random seeds. Seed 1 represents a sub-optimal solution because it also identifies \\(\\beta = v/c\\) as a key feature. This is not surprising, as in the classical limit \\(v \\ll c\\), \\(\\gamma = 1/\\sqrt{1 - (v/c)^2} \\approx 1 + (v/c)^2/2 = 1 + \\beta^2/2\\). The variation due to different seeds can be seen either as a feature or a bug: As a feature, this diversity can help find sub-optimal solutions which may nevertheless offer"}, {"title": "Building modular structures to KANS", "content": "Modularity is prevalent in nature: for example, the human cerebral cortex is divided into several functionally distinct modules, each of these modules responsible for specific tasks such as percep- tion or decision making. This modularity simplifies the understanding of neural networks, as it allows us to interpret clusters of neurons collectively rather than analyzing each neuron individually. Structural modularity is characterized by clusters of connections where intra-cluster connections are much stronger than inter-cluster ones. To enforce modularity, we introduce the module method, which preserves intra-cluster connections while removing inter-cluster connections. The modules are specified by users. The syntax is\nmodel.module(start_layer_id, '[nodes_id]->[subnodes_id]->[nodes_id]...')\nFor example, if a user wants to assign specific nodes/subnodes to a module \u2013 say, the 0th node in layer 1, the 1st and 3rd subnode in layer 1, the 1st and 3rd node in layer 2 \u2013 they might use module(1,'[0]->[1,3]->[1,3]'). To be concrete, there are two types of modularity: separabil- ity and symmetry.\nSeparability We say a function is considered separable if it can be expressed as a sum or product of functions of non-overlapping variable groups. For example, a four- variable function \\(f(x_1,x_2,x_3,x_4)\\) is maximally multiplicatively separable if it has the form \\(f_1(x_1) f_2(x_2) f_3(x_3) f_4(x_4)\\), creating four distinct groups (1), (2), (3), (4). Users can create these modules by calling the module method four times: module(0,'[i]->[i]'), i = 0,1,2,3, shown in Figure 4 (a). The final call may be skipped since the first three are sufficient to de- fine the groups. Weaker forms of multiplicative separability might be \\(f_1(x_1,x_2) f_2(x_3,x_4)\\) (calling module(0,'[0,1]->[0,1]')) or \\(f_1(x_1) f_2(x_2, x_3, x_4)\\) (calling module(0,'[0] -> [0]')).\nGeneralized Symmetry We say a function is symmetric in variables \\((x_1, x_2)\\) if \\(f (x_1, x_2, x_3, \\dots) = g(h(x_1,x_2), x_3,\\dots)\\). This property is termed symmetry because the value of f remains un- changed as long as \\(h(x_1,x_2)\\) is constant, even if \\(x_1\\) and \\(x_2\\) vary. For example, a function f is rotational invariant in 2D if \\(f(x_1,x_2) = g(r)\\), where \\(r = \\sqrt{x_1^2 + x_2^2}\\). When symmetry in- volves only a subset of variables, it can be considered hierarchical since \\(x_1\\) and \\(x_2\\) interact first through h (2-Layer KAN), and then h interacts with other variables via g (2-Layer KAN). Suppose a four-variable function has a hierarchical form \\(f(x_1,x_2,x_3,x_4) = h(f(x_1,x_2), g(x_3, x_4))\\),"}, {"title": "Compiling symbolic formulas to KANS", "content": "Scientists often find satisfaction in representing complex phenomena through symbolic equations. However, while these equations are concise, they may lack the expressive power needed to capture"}, {"title": "KANs to Science", "content": "Today's black box deep neural networks are powerful, but interpreting these models remains chal- lenging. Scientists seek not only high-performing models but also the ability to extract meaningful knowledge from the models. In this section, we focus on enhancing the interpretability of KANS scientific purposes. We will explore three levels of knowledge extraction from KANs, from the most basic to the most complex: important features (Section 4.1), modular structures (Section 4.2), and symbolic formulas (Section 4.3)."}, {"title": "Identifying important features from KANS", "content": "Identifying important variables is crucial for many tasks. Given a regression model f where \\(y \\approx f(x_1,x_2,...,x_n)\\), we aim to assign scores to the input variables to gauge their importance. Liu et al. [57], used the function L1 norm to indicate the importance of edges, but this metric could be problematic as it only considers local information.\nTo address this, we introduce a more effective attribution score which better reflects the importance of variables than the L1 norm. For simplicity, let us assume there are multiplication nodes, so we do not need to differentiate between nodes and subnodes 4. Suppose we have an L-layer KAN with width \\([n_0, n_1,\\dots,n_L]\\). We define \\(E_{l,i,j}\\) as the standard deviation of the activations on the (l, i, j) edge, and \\(N_{l,i}\\) as the standard deviation of the activations on the (l, i) node. We then define the node (attribution) score \\(A_{l,i}\\) and the edge (attribution) score \\(B_{l,i,j}\\). In [57], we simply defined \\(B_{l,i,j} = E_{l,i,j}\\) and \\(A_{l,i} = N_{l,i}\\). However, this definition fails to account for the later parts of the network; even if a node or an edge has a large norm itself, it may not contribute to the output if the rest of the network is effectively a zero function. Therefore, we now compute node and edge scores"}, {"title": "Identifying modular structures from KANS", "content": "Although the attribution score provides valuable insights into which edges or nodes are important, it does not reveal modular structures, i.e., how the important edges and nodes are connected. In this part, we aim to uncover modular structures from trained KANs and MLPs by examining two types of modularity: anatomical modularity and functional modularity."}, {"title": "Anatomical modularity", "content": "Anatomical modularity refers to the tendency for neurons placed close to each other spatially to have stronger connections than those further apart. Although artificial neural networks lack physical spatial coordinates, introducing the concept of physical space has been shown to enhance inter- pretability [51, 52]. We adopt the neuron swapping method from [51, 52], which shortens connec- tions while preserving the network's functionality. We call the method auto_swap. The anatomi- cal modular structure revealed through neuron swapping facilitates easy identification of modules, even visually, for two tasks shown Figure 7: (1) multitask sparse parity; and (2) hierarchical ma- jority voting. For multitask sparse parity, we have 10 input bits \\(x_i \\in \\{0,1\\},i = 1,2,\\dots,10\\), and output \\(Y_j = x_{2j-1} \\oplus x_{2j}, j = 1,\\dots,5\\), where \\(\\oplus\\) denotes modulo 2 addition. The task exhibits modularity because each output depends only on a subset of inputs. auto_swap suc- cessfully identifies modules for both KANs and MLPs, with the KAN discovering simpler mod- ules. For hierarchical majority voting, with 9 input bits \\(x_i \\in \\{0,1\\}, i = 1,\\dots,9\\), and the output \\(y = maj(maj(x_1, x_2, x_3), maj(x_4, x_5, x_6), maj (x_7,x_8,x_9))\\), where maj stands for majority voting (output 1 if two or three inputs are 1, otherwise 0). The KAN reveals the modular structure even before auto_swap, and the diagram becomes more organized after auto_swap. The MLP shows some modular structure from the pattern of the first layer weights, indicating interactions among variables, but the global modular structure remains unclear regardless of auto_swap."}, {"title": "Functional modularity", "content": "Functional modularity pertains to the overall function represented by the neural network. Given an Oracle network where internal details such as weights and hidden layer activations are inaccessible (too complicated to analyze), we can still gather information about functional modularity through forward and backward passes at the inputs and outputs. We define three types of functional modu- larity (see Figure 8 (a)), based largely on [84].\nSeparability: A function f is additively separable if\n\\(f(x_1, x_2,\\dots,x_n) = g(x_1,\\dots,x_k) + h(x_{k+1},\\dots,x_n).\\)\nNote that \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = 0\\) when \\(1 \\leq i \\leq k, k + 1 \\leq j \\leq n\\). To detect the separability, we can compute the Hessian matrix \\(H = \\nabla^2 f\\) (\\(H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\)) and check for block structure. If \\(H_{ij} = 0\\) for"}, {"title": "Compiling symbolic formulas from KANS", "content": "Symbolic formulas are the most informative, as they clearly reveal both important features and modular structures once they are known. In Liu et al. [57], the authors showed a bunch of examples from which they can extract symbolic formulas, with some prior knowledge when needed. With the new tools proposed above (feature importance, modular structures, and symbolic formulas), users can leverage these new tools to easily interact and collaborate with KANs, making symbolic regression easier. We present three tricks below, illustrated in Figure 9."}, {"title": "Discover and leverage modular structures", "content": "We can first train a general network and probe its modularity. Once the modular structure is identified, we initialize a new model with this modular structure as inductive biases. For instance, consider the function \\(f(q, v, B,m) = qvB/m\\)."}, {"title": "Applications", "content": "The previous sections primarily focused on regression problems for pedagogical purposes. In this section, we apply KANs to discover physical concepts, such as conserved quantities, Lagrangians, hidden symmetries, and constitutive laws. These examples illustrate how the tools proposed in this paper can be effectively integrated into real-life scientific research to tackle these complex tasks."}, {"title": "Discovering conserved quantities", "content": "Conserved quantities are physical quantities that remain constant over time. For example, a free- falling ball converts its gravitational potential energy into kinetic energy, while the total energy (the sum of both forms of energy) remains constant (assuming negligible air resistance). Conserved quantities are crucial because they often correspond to symmetries in physical systems and can sim- plify calculations by reducing the dimensionality of the system. Traditionally, deriving conserved quantities with paper and pencil can be time-consuming and demands extensive domain knowl- edge. Recently, machine learning techniques have been explored to discover conserved quanti- ties [55, 53, 54, 58, 32, 89]."}, {"title": "Discovering Lagrangians", "content": "In physics, Lagrangian mechanics is a formulation of classical mechanics based on the principle of stationary action. It describes a mechanical system using phase space and a smooth function L known as the Lagrangian. For many systems, L = T \u2013 V, where T and V represent the kinetic and potential energy of the system, respectively. The phase space is typically described by \\((q, \\dot{q})\\), where q and \\(\\dot{q}\\) denotes coordinates and velocities, respectively. The equation of motion can be derived from the Lagrangian via the Euler-Lagrange equation: \\(\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{q}} \\right) = \\frac{\\partial L}{\\partial q}\\), or equivalently\n\\ddot{q} = \\frac{\\partial}{\\partial \\dot{q}} \\left( \\dot{q} \\frac{\\partial L}{\\partial \\dot{q}} \\right) - \\left[ \\frac{\\partial L}{\\partial q} - \\left( \\frac{\\partial^2 L}{\\partial q \\partial \\dot{q}} \\right) \\right]\nGiven the fundamental role of the Lagrangian, an interesting question is whether we can infer the La- grangian from data. Following [19], we train a Lagrangian neural network to predict \\(\\ddot{q}\\) from \\((q, \\dot{q})\\). An LNN uses an MLP to parameterize \\(L(q, \\dot{q})\\), and computes the Eq. (18) to predict instant accel- erations \\(\\ddot{q}\\). However, LNNs face two main challenges: (1) The training of LNNs can be unstable"}, {"title": "Discovering hidden symmetry", "content": "Philip Anderson famously argued that \u201cit is only slightly overstating that case to say that physics is the study of symmetry\u201d, emphasizing how the discovery of symmetries has been invaluable for both deepening our understanding and solving problems more efficiently.\nHowever, symmetries are sometimes not manifest but hidden, only revealed by applying some co- ordinate transformation. For example, after Schwarzschild discovered his eponymous black hole metric, it took 17 years for Painlev\u00e9, Gullstrand and Lema\u00eetre to uncover its hidden translational symmetry. They demonstrated that the spatial sections could be made translationally invariant with a clever coordinate transformation, thereby deepening our understanding of black holes [65]. Liu & Tegmark [56] showed that the Gullstrand-Painlev\u00e9 transformation can be discovered by training an MLP in minutes. However, they did not get extremely high precision (i.e., machine precision) for the solution. We attempt to revisit this problem using KANS."}, {"title": "Learning constitutive laws", "content": "A constitutive law defines the behavior and properties of a material by modeling how it responds to external forces or deformations. One of the simplest forms of constitutive law is Hooke's Law [34], which relates the strain and stress of elastic materials linearly. Constitutive laws encompass a wide range of materials, including elastic materials [80, 68], plastic materials [64], and fluids [8]. Tra- ditionally, these laws were derived from first principles based on theoretical and experimental stud- ies [79, 81, 6, 29]. Recent advancements, however, have introduced data-driven approaches that leverage machine learning to discover and refine these laws from dedicated datasets [73, 91, 59, 60]."}, {"title": "Related works", "content": "Kolmogorov-Arnold Networks (KANs), inspired by the Kolmogorov-Arnold representation the- orem (KART), were recently proposed by Liu et al. [57]. Although the connection between KART and networks has long been deemed irrelevant [30], Liu et al. generalized the origi- nal two-layer network to arbitrary depths and demonstrated their promise for science-oriented tasks given their accuracy and interpretability. Subsequent research has explored the application of KANs across various domains, including graphs [12, 22, 38, 99], partial differential equa- tions [87, 78] and operator learning [1, 78, 67], tabular data [70], time series [85, 28, 93, 27], human activity recognition [49, 50],neuroscience [96, 33], quantum science [40, 46, 4], computer vision [17, 7, 44, 16, 76, 10], kernel learning [101], nuclear physics [48], electrical engineering [69], biology [71]. Liu et al. used B-splines to parameterize 1D functions, and other research have ex- plored various activation functions, including wavelet [11, 76], radial basis function [47], Fourier series [92]), finite basis [35, 82], Jacobi basis functions [2], polynomial basis functions [75], ratio- nal functions [3]. Other techniques for KANs have also been proposed including regularization [5], Kansformer (combining transformer and KAN) [15], adaptive grid update [72], federated learn- ing [98], Convolutional KANs [10]. There have been ongoing debates regarding whether KANS really outperform other neural networks (especially MLPs) on various domains [7, 16, 42, 77, 97], which suggests that while KANs show promise for machine learning tasks, further development is needed to surpass state-of-the-art models.\nMachine Learning for Physical Laws A major goal for KANs is to aid in the discovery of new physical laws from data. Previous research has shown that machine learning can be used to learn various types of physical laws, including equations of motion [90, 13, 43, 20], conservation laws [55, 53, 54, 58, 32, 89], symmetries [39, 56, 94], phase transitions [88, 14], Lagrangian and Hamiltonian [19, 31], and symbolic regression [18, 61, 23, 74], etc. However, making neural net- works interpretable often requires domain-specific knowledge, limiting their generality. We hope that KANs will evolve into universal foundation models for physical discoveries.\nMechanistic Interpretability seeks to understand how neural networks operate in a fundamental level [21, 62, 86, 25, 66, 100, 51, 24, 45, 26]. Some research in this area focuses on designing models that are inherently interpretable [24] or proposing training methods that explicitly promote interpretability [51]. KANs fall into this category since the Kolmogorov-Arnold theorem decom- poses a high-dimensional function into a collection of 1D functions, which are significantly easier to interpret than high-dimensional functions."}, {"title": "Discussion", "content": "KAN interpolates between software 1.0 and 2.0 The key difference between Kolmogorov-Arnold Networks (KANs) and other neural networks (software 2.0, a term coined by Andrej Karpathy) lies in their greater interpretability, which allows for manipulation by users, similar to traditional soft- ware (software 1.0). However, KANs are not entirely traditional software, as they (1) learnability (good), enabling them to learn new things from data, and (2) reduced interpretability (bad) as they"}, {"title": "Efficiency improvement", "content": "The original pykan package [57] was poor in efficiency. We have incor- porated a few techniques to improve its efficiency.\n1. Efficient splines evaluations. Inspired by Efficient KAN [9], we have optimized spline eval- uations by avoiding unnecessary input expansions. For a KAN with L layers, N neurons per layer, and grid size G, memory usage has been reduced from \\(O(LN^2G)\\) to \\(O(LNG)\\).\n2. Enabling the symbolic branch only when needed. A KAN layer contains both a spline branch and a symbolic branch. The symbolic branch is much more time-consuming than the spline branch since it cannot be parallelized (disastrous double loops are needed). However, in many applications, the symbolic branch is unnecessary, so we can skip it when possible, significantly reducing runtime, especially when the network is large.\n3. Saving intermediate activations only when needed. To plot KAN diagrams, intermediate activations must be saved. Initially, activations were saved by default, leading to slower runtime and excessive memory usage. We now save intermediate activations only when needed (e.g., for plotting or applying regularizations in training). Users can enable these efficiency improvements with a single line: model.speed().\n4. GPU acceleration. Initially, all models were run on CPUs due to the small-scale nature of the problems. We have now made the model GPU-compatible 6. For example, training a [4,100,100,100,1] with Adam for 100 steps used to take an entire day on a CPU (before implementing 1, 2, 3), but now takes 20 seconds on a CPU and less than one second on a GPU. However, KANs still lag behind MLPs in efficiency, especially at large scales. The community has been working towards benchmarking and improving KAN's efficiency and the efficiency gap has been significantly reduced [36]."}, {"title": "Interpretability", "content": "Although the learnable univariate functions in KANs are more interpretable than weight matrices in MLPs, scalability remains a challenge. As KAN models scale up, even if all spline functions are interpretable individually, it becomes increasingly difficult to manage the com- bined output of these 1D functions. Consequently, a KAN may only remain interpretable when the network scale is relatively small (Figure 14 (b), thick red line). It is important to note that interpretability depends on both intrinsic factors (related to the model itself) and extrinsic factors (related to interpretability methods). Advanced interpretability methods should be able to handle interpretability at various levels. For example, by interpreting KANs with symbolic regression, modularity discovery and feature attribution (Figure 14 (b), thin red lines), the Pareto Frontier of interpretability versus scale extends beyond what a KAN alone can achieve. A promising direction for future research is to develop more advanced interpretability methods that can further push the current Pareto Frontiers."}, {"title": "Future work", "content": "This paper introduces a framework that integrates KANs with scientific knowledge, focusing primarily on small-scale, physics-related examples. Moving forward, two promising direc- tions include applying this framework to larger-scale problems and extending it to other scientific disciplines beyond physics."}]}