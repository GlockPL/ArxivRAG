{"title": "Perceptual Noise-Masking with Music through Deep Spectral Envelope Shaping", "authors": ["Cl\u00e9mentine Berger", "Roland Badeau", "Slim Essid"], "abstract": "People often listen to music in noisy environments, seeking to isolate themselves from ambient sounds. Indeed, a music signal can mask some of the noise's frequency components due to the effect of simultaneous masking. In this article, we propose a neural network based on a psychoacoustic masking model, designed to enhance the music's ability to mask ambient noise by reshaping its spectral envelope with predicted filter frequency responses. The model is trained with a perceptual loss function that balances two constraints: effectively masking the noise while preserving the original music mix and the user's chosen listening level. We evaluate our approach on simulated data replicating a user's experience of listening to music with headphones in a noisy environment. The results, based on defined objective metrics, demonstrate that our system improves the state of the art.", "sections": [{"title": "I. INTRODUCTION", "content": "Listening to music in noisy environments may negatively impact the listening experience as the surrounding noise inter-feres with the perception of the music, partially masking some spectral components of the audio content [1], [2]. A part of the music may even be completely concealed by the noise due to simultaneous masking. For a given audio signal, this effect is quantified using masking thresholds, which indicate the level below which another signal becomes inaudible within specific frequency bands due to the presence of the first signal [3], [4]. Thus, music rendering systems have been developed over the years to enhance listening comfort, initially in automotive environments [5]\u2013[7] and later for more general contexts and personal devices such as headphones or earphones [8]\u2013[10]. One approach involves volume adjustments and compression to increase the loudness of the music when it is masked or partially masked by noise [5], [6], [9], [11]. Similarly, adaptive perceptual equalizers have been proposed to restore the original loudness of the music signal when it is affected by ambient noise [7], [8]. However, the masking effect works both ways: music may also be used to mask ambient noise. Some works on Active Noise Cancelling (ANC) systems have utilized psychoacoustic information to focus on the frequency bands where noise exceeds the music's masking thresholds, ensuring that the residual noise after reduction is completely masked by the music [12]\u2013[14]. Another strategy is to filter the music so that its masking thresholds are raised above the noise level. The main challenge is then how to generate filtering parameters that achieve these perceptual goals while maintaining the original music's identity and the user's preferred listening level. Estreder et al. [15] proposed a system that computes the gain parameters of a graphic equalizer whose goal is to boost the frequency bands where the noise is not masked. However, they simplified the problem by not fully leveraging the psychoacoustic model in the gain computation and not incorporating constraints to minimize music alteration, aside from limiting the gain in each band. In this article, we propose an original deep neural network approach to produce filters that enhance the played-back music's capacity to mask ambient noise. Like Estreder et al. [15], we focus on simultaneous masking, excluding the partial masking effect. We demonstrate that using a well-chosen neu-ral network with a well-designed loss function allows a better exploitation of the psychoacoustic model for gain prediction and the integration of other criteria to balance masking effec-tiveness and musical fidelity. Specifically, our contributions are: i) a U-net architecture optimized with a perceptual loss function trading-off masking and minimal power variations leading to improved results over the method proposed by Estreder [15]; ii) a perceptually motivated procedure to deduce equalization frequency responses from predicted gains in each frequency band; iii) an experimental design studying variants of the proposed system based on different levels of power constraint, using data simulated with headphones responses to reproduce realistic auditory scenes, and evaluation metrics jointly assessing masking and power-preservation. This study focuses on the context of music listened to with headphones; however, the proposed method is more general and can be applied to other use cases."}, {"title": "II. NEURAL MODEL", "content": "In this work, we tackle the task of raising the listened music's masking thresholds to better mask the listener's sur-rounding noise. The proposed model Deep Perceptual Noise Masking with Music (DPNMM) is based on a deep neural network that predicts the frequency responses of filters to apply to the music to achieve the desired masking effect. A schematic overview of the system is shown in Fig. 1."}, {"title": "A. Architecture", "content": "Our model's architecture is illustrated in Fig. 2. We design a U-Net encoder-decoder model, inspired by the DeepFilterNet architecture [19], especially its Equivalent Rectangular Band-width (ERB) gains prediction branch. The encoder consists of 4 convolutional blocks (separable convolution + BatchNorm + ReLU) followed by a linear layer, designed to process frequency information between the Bark features. This fre-quency information is then passed through a Gated Recurrent Unit (GRU) layer to handle the temporal dynamics. The decoder mirrors the encoder and incorporates skip connections, outputting the predicted gains in dB, g(n,v), per frame, and Bark band (up to the 24th band ~ 16500 Hz). Additionally, a gain smoothing filter is applied to prevent too rapid variations in the filters' frequency responses over time, and the gains are finally clamped as in [15] with the following threshold values:\n\\[g(n, v) \\leftarrow max(min(g(n, v); \u22125 dB); 10 dB).\\]"}, {"title": "B. Filters frequency responses", "content": "The obtained gains are converted into filter frequency re-sponses that are generated using a pattern \\(W_{B}^{v}(f)\\) centered on the corresponding Bark band v (see Fig. 3). In each band, the pattern is constant and equal to 1. To simulate the spreading effect of masking across the Bark bands, the pattern transitions smoothly with a cosine shape across the two adjacent bands below (from 0 to 1) and above (from 1 to 0). For the lowest frequency band, the pattern level is set to 2 instead of 1 at the center. This approach accounts for the fact that adjacent bands also contribute to raising the masking threshold of a given band, thereby distributing the required gain across multiple bands and minimizing the boost needed on the central band. For each audio frame the overall frequency response \\(W_{dB}^{g}(n,k)\\) is obtained as:\n\\[W_{dB}^{g}(n,k) = \\sum_{\\nu=1}^{B} g(n,\\nu) \\cdot W_{B}^{\\nu}(k).\\]\nThe input music is then filtered, in the frequency domain, using those frequency responses frame by frame to produce a processed music whose masking properties are enhanced."}, {"title": "C. Loss functions", "content": "Our model DPNMM is trained with a primary loss function designed to raise the music's masking threshold above the noise level in the critical bands where needed. These Bark bands are constrained to exceed specific thresholds, while the energy in other bands is free to change as long as their masking threshold remains above the noise level, thereby supporting masking in adjacent bands:\n\\[L_{0}(t) = \\frac{1}{NB}\\sum_{n}\\sum_{\\nu} ReLU(P_{music}^{proc}(n, \\nu) - T_{dB}^{dB}(n,\\nu)).\\]\nwhere \\(T_{dB}^{dB}(n,\\nu)\\) is the masking threshold computed with the processed music, N the number of time frames and B the number of Bark bands. By using a ReLU function to express the constraint, we only set a minimum threshold level for the network to reach, allowing it greater flexibility to find solutions for masking noise across all frequency bands. However, while this choice aims to provide the network with more freedom, it also brings the challenge that the system is not required to output zero gains when no amplification of the music is needed. To address this, we use the knowledge of the masking spreading effect to compute a mask that identifies, for each band, whether it is close enough to another band (including itself) where the threshold needs to be raised to have an impact on it. If it is not, the gain for that band at the network's output is set to zero. Even with this gain masking, the system still has an infinite number of potential solutions. To guide the learning process in a desired direction, we add a secondary constraint aimed at preserving the naturalness of the original music by limiting the average power variation:\n\\[L_{power}(t) = \\frac{1}{N}\\sum_{n} |P_{music}^{init}(n) - P_{music}^{proc}(n)|.\\]\nwhere the initial music mean power P of frame n is evaluated in dBA [20], as well as the processed music mean power P. To include this constraint in the training process we use a strategy inspired by the method of multipliers [21], [22]. The goal is to ensure that during training the power variation does not exceed a given value APmax using a dynamic weight At to scale the loss. The overall loss function for this constrained optimization problem is:\n\\[L(\\theta_{t}, A_{t}) = L_{0} - A_{t} \\cdot (\\Delta P_{max} - L_{power}(t)).\\]\nAt the start of the training, At = 0. While gradient de-scent is applied to the overall loss L(At), gradient as-cent is simultaneously performed on At using the gradient \nAL(0t,t) = Lpower(0t) APmax with a specific learning rate of 10-3, keeping At always positive. When Lpower(0t) exceeds the threshold AP\u0442\u0430\u0445, At increases, giving more weight to the power constraints in the total loss. Conversely, when Lpower (Ot) drops below \u2206Pmax, At decreases. This approach allows us to guide the training in a direction that satisfies both constraints without requiring a tedious search for an optimal fixed weight. The degree of compromise between the two constraints is directly controlled by the choice of parameter \u2206Pmax. In the rest of the article, we explore several values for this parameter. The code for the implementation of DPNMM is available on github.\u00b9"}, {"title": "III. DATA", "content": "To train and evaluate the proposed model, we generated training, validation, and test datasets that replicate realistic acoustic scenes. These scenes simulate a user listening to music through headphones or earphones while being in a noisy environment. We defined several environments to represent a variety of realistic acoustic scenes with different ambient noise levels: urban, indoor office, construction site, beach, transportation (train/plane/boat), and restaurant/bar. We chose the noise recordings from the DNS Challenge dataset [23]. Each en-vironment is created using the labels from the noise dataset and a realistic noise level distribution in dBA. Noise samples are evenly selected per environment and normalized to levels sampled from the corresponding noise distribution. A pre-processing step is applied to drop the audio signals composed mainly of silence (which are therefore isolated, impulsive noises). After that, all those samples are filtered with one of three headphone frequency responses to reproduce their passive attenuation. Each noise sample is then paired with a music track chosen from the FMA dataset [24] which covers a large diversity of music genres (Pop, Rock, Classical, Jazz, Hip-Hop, etc.). Each music is normalized to a dBA level derived from a Signal-to-Noise Ratio (SNR) value, itself sampled from a defined SNR distribution. The resulting music dBA level is constrained within the range [45,100] dBA, reflecting the typical range offered by standard headphones. We thus generate 50h of training data, 20h of validation data, and 10h of test data, composed of pairs of 10s mono music and noise excerpts sampled at 44100 Hz."}, {"title": "IV. RESULTS", "content": "We use as a baseline the perceptual equalizer used by Estreder et al. [15] that is based on a Bark band graphic equalizer employing second-order peak filters [25], [26]. The gains per Bark band are computed in order to raise the music level in the bands where the noise PSD is above its masking threshold,\n\\[g(n, \\nu) = max\\left(\\frac{P_{noise}(n, \\nu)}{T_{dB}(n,\\nu)}; 0\\right)_{dB}.\\]\nThis computation approach does not consider the additivity property of simultaneous masking. The level in the v-th Bark band is raised to bring the masking threshold at the noise level by adjusting only this band while raising the levels in adjacent bands would also contribute to increasing the masking threshold in the target band via the effect of the spreading function."}, {"title": "B. Discussion", "content": "We evaluate our approach by testing different configurations of the power constraint: no power constraint, \u2206Pmax = 2, 1, and 0.5 dB. In all configurations, the model is trained for 50 epochs with a learning rate of 10-3 and a batch size of 64. The statistical difference between each of DPNMM configurations and the baseline model is evaluated by calculating the mean p-value with the Wilcoxon test over 100 batches of 50 samples from the test set, with a Bonferroni correction applied. The results are presented in Fig. 4. In terms of NMR, all three versions of the neural model outperform Estreder's model on the broadband metric sta-tistically significantly, except DPNMM with \u2206Pmax = 0.5 dBA. The version of the neural model without any power constraint performs the best compared to the baseline (p-value = 7.10-8). Applying a power constraint results in a decrease in performance all the more important the stricter the constraint (low \u2206Pmax), particularly in the low-frequency range to the point of becoming less performant than Estreder's PEQ. This outcome is expected, given the relatively low weight of high frequencies in the power measurement. When the power constraint is strict, the low and mid frequencies are more significantly affected. This trend is confirmed when examining the GLD. Without a power constraint, the neural model achieves excellent NMR performance by significantly amplifying the musical signal compared to Estreder's model. Adding the power constraint has then a clear beneficial effect on the GLD measure, thus achieving significantly better results compared to the baseline model, except at high frequencies where the model is less affected by the constraint. In particular, both neural models with constraints \u2206Pmax = 2, 1 dBA achieve a better NMR than Estreder's model (p-value of 10-6 and 0.01) and a better broadband GLD (p-value of 1.5. 10-4 and 3.3 \u00b7 10-9). Audio examples and further results are available on this article's companion website.\u00b2"}, {"title": "V. CONCLUSIONS", "content": "We proposed a new model, DPNMM, for estimating filters to shape music and raise its masking thresholds according to the spectral characteristics of ambient noise, in the context of users wearing headphones. This approach is perceptu-ally motivated and allows balancing the effectiveness of the masking effect with a power-preservation constraint to ensure fidelity to the original recording. This model shows superior performance compared to the baseline PEQ by Estreder et al. [15] outperforming it both in terms of noise masking and power-preservation. Future works will explore incorporating user preferences to adapt the model's response to the user's preferred sound rendering."}]}