{"title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models", "authors": ["Yujin Wang", "Quanfeng Liu", "Jiaqi Fan", "Jinlong Hong", "Hongqing Chu", "Mengjian Tian", "Bingzhao Gao", "Hong Chen"], "abstract": "Understanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-Language Models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant challenges, such as hallucination and insufficient real-world grounding, which compromise their performance in critical driving scenarios. In this work, we propose RAC3, a novel framework designed to improve VLMs' ability to handle corner cases effectively. The framework integrates Retrieval-Augmented Generation (RAG) to mitigate hallucination by dynamically incorporating context-specific external knowledge. A cornerstone of RAC3 is its cross-modal alignment fine-tuning, which utilizes contrastive learning to embed image-text pairs into a unified semantic space, enabling robust retrieval of similar scenarios. We evaluate RAC3 through extensive experiments using a curated dataset of corner case scenarios, demonstrating its ability to enhance semantic alignment, improve hallucination mitigation, and achieve superior performance metrics, such as Cosine Similarity and ROUGE-L scores. For example, for the LLaVA-v1.6-34B VLM, the cosine similarity between the generated text and the reference text has increased by 5.22%. The F1-score in ROUGE-L has increased by 39.91%, the Precision has increased by 55.80%, and the Recall has increased by 13.74%. This work underscores the potential of retrieval-augmented VLMs to advance the robustness and safety of autonomous driving in complex environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Despite the significant development achieved in the field of autonomous driving, autonomous driving systems still lack the ability to comprehend and generalize when facing corner cases, and thus possible human takeover from the backstage is required. The traditional rule-based approach to the development of autonomous driving cannot solve this problem, and small end-to-end neural networks also have significant deficiencies in scenario comprehension [1]. In recent years, advancements in large-scale machine learning models have propelled the field of embodied intelligence, enabling new paradigms for interaction between artificial systems and their environments. Large Models (LMs), ranging from uni-modal architectures focused on textual tasks to sophisticated multi-modal systems, have demonstrated exceptional capabilities across a variety of applications. Among these, Multi-modal Large Language Models (MLLMs), especially Vision-Language Models (VLMs), have emerged as a significant development, leveraging the complementary strengths of textual and visual data to achieve nuanced understanding and reasoning. Such models have been effectively applied to critical areas, including autonomous driving, where precise scenario comprehension, especially corner case comprehension, is paramount for ensuring safety and functionality in complex environments [2], [3], [4], [5], [6], [7].\nThe integration of LMs into autonomous systems, particularly autonomous driving vehicles, has introduced new challenges and opportunities. These models exhibit robust capabilities in tasks such as object detection, semantic segmentation and trajectory prediction, which are critical for navigating complex urban environments [8], [9], [10], [11], [12], [13], [14]. Moreover, VLMs promise enhanced decision-making through their ability to integrate information from diverse sensory inputs, such as visual streams and textual instructions, creating a more holistic comprehension of the scene [15], [16], [17], [18].\nDespite these advancements, VLMs face persistent challenges in corner case comprehension, particularly due to the phenomenon of hallucination. Hallucination refers to instances where models generate outputs that are inconsistent with the real-world content they aim to represent. In the context of autonomous driving, hallucinations can manifest as erroneous object detection, inaccurate attribute descriptions, or implausible relational interpretations within the environment [19]. Such issues not only undermine the reliability of these systems but also pose significant safety risks. For example, a VLM trained on imperfect visual-textual data or visual question answering (VQA) data may erroneously infer the presence of a pedestrian or vehicle that does not exist, leading to potentially dangerous decision [20], [21], [22], [23]. Addressing hallucination is thus critical for ensuring the robustness and applicability of multi-modal systems in real-world scenarios [24], [25], [26], [27], [28].\nTo mitigate hallucination and enhance the fidelity of VLMs, Retrieval-Augmented Generation (RAG) has emerged as a promising framework. RAG combines the generative capabilities of LMs with external retrieval mechanisms to ground model outputs in factual and contextually relevant data [29], [30]. By incorporating real-time retrieval from structured knowledge bases or unstructured datasets, RAG ensures that model predictions are informed by the most relevant evidence, thereby reducing the likelihood of hallucinations. This capability is particularly valuable in domains such as autonomous driving, where real-time and accurate decision-making is imperative.\nThe application of RAG to scenario comprehension in autonomous driving is especially significant. RAG not only provides a mechanism to validate and refine model predictions but also facilitates the incorporation of domain-specific knowledge, such as traffic regulations and environmental conditions, into the decision-making process [13], [17], [31]. For instance, during navigation in an urban setting, RAG-enabled systems can retrieve contextually relevant data about nearby landmarks, traffic density, or weather conditions to augment the scene comprehension of the model. Furthermore, RAG enhances the interpretability of multi-modal models by enabling the traceability of predictions to specific data sources, thereby fostering trust and reliability in autonomous systems. RAG's utility extends beyond hallucination mitigation to support counter-factual reasoning and scenario-based testing in autonomous driving.\nIt is quite vital that with the enhancement of RAG, the former corner cases that require human takeover could be embedded and added into the existing vector database. The next time when another similar corner case appears, the VLM could get prior knowledge from the database, therefore the human takeover will not be required. This is of great significance for reducing the takeover rate and achieving high-level autonomous driving in the true sense."}, {"title": "B. Contribution", "content": "In this work, we propose a framework illustrated in Figure 1 for adopting RAG to hallucination mitigation, in order to enhance the capabilities of corner case comprehension of VLMs. Our approach achieves promising results, with the key advantage of requiring fewer computational resources, making it more suitable for deployment on vehicles.\nTo summarize, the main contributions of this paper are as follows:\n\u2022 The proposal of RAC3 framework, which aims to enhance the hallucination mitigation capabilities of VLMs when processing corner cases using prior information.\n\u2022 The cross-modal alignment fine-tuning algorithm for embedding models, which utilizes contrastive learning and negative samples mining.\n\u2022 The paradigm of inputting the new corner case image concatenated with the retrieved image into VLMs and the corresponding prompt engineering method. This could achieve better comprehension of VLMs dealing with two images at the same time."}, {"title": "II. RELATED WORKS", "content": "The core idea of RAG technology is to introduce an external retrieval module that dynamically retrieves relevant information during the generation process, thereby enhancing the performance of generative models. In visual-linguistic tasks, RAG effectively compensates for the limitations of scarce knowledge by combining external knowledge bases. The model is not only able to extract information from images but also retrieves supplementary knowledge via the retrieval mechanism, thereby improving the quality and accuracy of the generated output. Jiang et al. [32] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, by incorporating an external knowledge base, allows the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities.\nBuilding upon this, Shao et al. [33] further explore the application of RAG in VQA tasks. They propose that by combining the retrieval mechanism with pre-trained VLMs, model performance in complex reasoning tasks could be significantly enhanced. Furthermore, Ram et al. [34] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance. Meanwhile, Zheng et al. [35] point out that RAG technology not only enhances the model's generative capabilities but also improves its flexibility and adaptability in handling complex multimodal tasks, especially when dealing with tasks lacking sufficient annotations or background knowledge.\nAs RAG technology continues to deepen its application across various tasks, the key challenge, especially in open-domain VQA tasks, lies in how to dynamically retrieve relevant background knowledge through the retrieval mechanism"}, {"title": "B. Hallucination Mitigation of VLMs", "content": "Recent efforts to mitigate hallucinations in VLMs have led to the development of various strategies that target different stages of the model's workflow, including data expansion, model training, and inference correction.\nHalluciDoctor [40] introduces a novel cross-checking paradigm to detect semantic hallucinations and generate counterfactual instruction data, enhancing the model's robustness. Similarly, Recaption [41] refines datasets by rewriting captions with the ChatGPT model and fine-tuning the VLMs on these updated datasets, reducing the occurrence of fine-grained hallucinations.\nMoreover, several model-training techniques have been explored to reduce hallucinations by improving the model's capabilities in perception and generation. For example, He et al. [42] enhance the VLM by incorporating multiple visual expert models, including object detectors and OCR, to enrich the model's knowledge base. Jain et al [43] further improve the model's object perception by providing additional visual inputs such as segmentation and depth maps. Chen et al. [44] introduces a model that injects spatially aware and semantically rich visual evidence into the VLM, enhancing its multimodal understanding. Moreover, Jiang et al. [45] apply contrastive learning, using hallucinated texts as hard negative samples to better align visual and textual representations.\nIn addition to these data and training-based methods, post-hoc corrections during the inference stage also play a critical role in alleviating hallucinations [46]. For example, VCD [47] employs a visual contrastive strategy during decoding, comparing output distributions from both original and distorted visual inputs to ensure consistency between the generated content and the visual data. LogicCheckGPT [48] creates a logical closed-loop method using object-to-attribute and attribute-to-object inquiring to verify consistency, while Volcano [49] takes"}, {"title": "III. METHOD: RAC3", "content": "In this work, we mainly adopt the CODA dataset [50], which is a dataset consisting of over 10,000 corner case images for object detection. All 10,825 images are captioned automatically by GPT-40 [6], and the captions serve as ground truth in the following pipeline.\nTo be detailed, we select 1,768 image-text pairs for the cross-modal alignment fine-tuning for embedding models, and regard another 8,000 as the prior knowledge to be embedded in the database. A smaller-sized set with 1,057 images is used as the evaluation set."}, {"title": "B. Cross-modal Alignment Fine-tuning for Embedding Models", "content": "Cross-modal alignment is crucial in the vector embedding stage. Poor alignment may lead to a decline in model performance and weakened generalization ability [51]. In order to achieve better alignment across different modals during the embedding procedure, we adopt a contrastive learning approach for the embedding model BLIP. The fine-tuning process implements a multimodal contrastive learning algorithm involving image and text embeddings. The goal of this algorithm is to learn a joint representation space where image and text pairs with similar semantics are close, and dissimilar pairs are far apart. This is achieved through a contrastive loss function that maximizes the similarity between matching pairs while minimizing it for non-matching pairs.\nThis pseudocode captures the key steps:\n1) Data embedding: Images and texts are both embedded using the BLIP model.\n2) Negative mining: Depending on whether hard or semi-hard negative mining is used, the algorithm retrieves challenging samples for loss computation.\n3) Loss computation: The contrastive loss is computed on the basis of the similarity matrix [52].\n4) Optimization: The model is fine-tuned using gradient descent.\nThe core of this algorithm is the contrastive loss, which works on the similarity matrix between image and text embeddings. The similarity matrix is denoted as S, where each element Sij is the cosine similarity between the i-th image embedding and j-th text embedding.\nFormula for Cosine Similarity: For image embedding $v_i$ and text embedding $t_j$, cosine similarity is defined as:\n$S_{i,j} = \\frac{v_i \\cdot t_j}{||v_i|| ||t_j||}$  (1)\nwhere $||v_i||$ and $||t_j||$ represent the Euclidean norms of the vectors."}, {"title": "Contrastive Loss:", "content": "The contrastive loss encourages matching pairs (positive samples) to have high similarity and non-matching pairs (negative samples) to have low similarity. The loss function for images $L_{img}$ and for texts $L_{txt}$ is given by:\n$L_{img} = \\frac{1}{N} \\sum_{i=1}^N \\text{CrossEntropy}(S_i, i)$ (2)\ni.e.\n$L_{img} = \\frac{1}{N} \\sum_{i=1}^N - \\log \\frac{\\exp(S_{i,i}/\\tau)}{\\sum_{j=1}^N \\exp(S_{i,j}/\\tau)}$ (3)\n$L_{txt} = \\frac{1}{N} \\sum_{i=1}^N \\text{CrossEntropy}(S_i^T, i)$ (4)\ni.e.\n$L_{txt} = \\frac{1}{N} \\sum_{i=1}^N - \\log \\frac{\\exp(S_{i,i}/\\tau)}{\\sum_{j=1}^N \\exp(S_{j,i}/\\tau)}$ (5)\nwhere N is the number of samples, $S_i$ is the similarity vector for the i-th image, $\\tau$ is a temperature parameter to scale the logits, and i denotes the correct index for positive pairs. The overall loss is the average of these two:\n$L(S) = \\frac{L_{img} + L_{txt}}{2}$ (6)\nThis fine-tuning process and algorithm aim to fine-tune the BLIP model by training new weights through a cross-modal contrastive learning approach. The objective is to align"}, {"title": "C. Cross-modal Embedding, Retrieving and Generating", "content": "Step 1: Embedding We propose a pipeline for generating and storing cross-modal embeddings for a dataset consisting of image-text pairs. The goal of this pipeline is to encode both images and their corresponding textual descriptions (ground truth) into high-dimensional vectors (embeddings) that could be used for further RAG querying. The approach leverages two types of models, namely a multimodal encoding model (BLIP, fine-tuned as formerly introduced) for generating image-text representations, and a text encoding model (bge-base-en-v1.5) for encoding textual descriptions.\nThe core principle of the pipeline is the generation of cross-modal embeddings that jointly represent both images and texts in a shared embedding space, which is illustrated in Figure 2. For each image-text pair, the image is passed through the BLIP processor, which prepares it for input to the model, alongside the corresponding text. The model then generates a set of hidden states from which a final embedding vector is derived by averaging the last layer's hidden states across the sequence dimension. The image and the corresponding text are encoded separately and the vectors are concatenated. This embedding represents both the content of the image and its corresponding textual description in a unified vector space. While the textual descriptions are encoded independently using the bge-base-en-v1.5 model, which is designed for text encoding and retrieval.\nThe embeddings generated for both the image-text pairs (cross-modal embeddings) and the textual descriptions (text-only embeddings) are then stored separately in two databases, which share the same index and allow for easy retrieval and comparison of the embeddings based on their content. This ensures that the embeddings are preserved and can be reloaded for further downstream RAG tasks. In this work, the size of the embedded cross-modal database with 8,000 image-text pairs is only 159.9MB, and the size of the embedded text database is only 1.7MB.\nStep 2: Querying and Retrieving We therefore propose a pipeline for querying a cross-modal database using a new image, namely another corner case, where the goal is to retrieve the most similar case in the database and its corresponding text description based on the similarity between image and text embeddings.\nThe pipeline processes the new image and extracts its embedding using the fine-tuned BLIP processor. The core of the retrieval process involves comparing embedding of the new image with the embeddings in the cross-modal database, which is illustrated in Figure 3. We adopt the cosine similarity to measure the similarity between two vectors, which is defined in (1). The similarity between the new image's embedding and each cross-modal embedding, which contains both image and text embeddings is computed. The similarity is weighted using a parameter \u03b1, which determines the importance of the text versus the image similarity. The retrieved similarity is a weighted sum of the image and text similarities:\nsimilarity = (1 \u2212 \u03b1) \u00b7 img_similarity + \u03b1 \u00b7 text_similarity (8)\nThe embedding with the highest similarity is selected as the most relevant match, and the corresponding text and image index are recorded. After obtaining the retrieved image and text index, the corresponding image and text could be retrieved. This pipeline demonstrates how VLMs can be utilized for cross-modal retrieval tasks, combining image and text understanding in a unified embedding space. Retrieving an image and its corresponding text takes approximately 2 seconds in average.\nStep 3: Generating By using retrieved images and text as prior information, it is quite promising to guide any VLM in generating descriptions of new images. When a new corner case image triggers a query, the most similar image and its corresponding textual description are retrieved from the database following the two-step process outlined above. The retrieved images, concatenated with the new image as illustrated in Figure 4, are used as input to a VLM, while the retrieved textual description forms part of the prompt, which is combined with another pre-designed segment of the prompt, guiding the VLM in generating tasks. The complementary prompt could be described as follows:\nThe given image has left and right parts separated by a distinct red line. The corresponding textual description has been given for the scenario on the right. Please give the textual description of the driving scenario on the left accordingly.\nThe above is the complete process of cross-modal embedding, retrieving and generating pipeline. Based on the pipeline, we conduct multiple experiments which demonstrate the effectiveness of our RAC3 method and are introduced in Section IV in details."}, {"title": "IV. EXPERIMENTS", "content": "The VLMs used in the experiments are all summarized in Table I. We mainly use two series of VLMs, namely LLaVA [5] and InternVL [53]. For each VLM, the prompt \"Please give the textual description of the driving scenario.\" and the corner case image are used together as inputs to generate descriptions of the current scenario. In this work, all the descriptions are generated with four NVIDIA A800 GPUs.\nCosine Similarity: Intuitively, a greater degree of proximity between the generated text and the reference text implies"}, {"title": "Sd", "content": "Next, compute the standard deviation of differences:\n$s_d = \\sqrt{\\frac{\\sum_{i=1}^n (d_i - d)^2}{n-1}}$ (14)\nwhere sd is the standard deviation of differences."}, {"title": "V. CONCLUSION", "content": "In this work, we propose RAC3, a novel retrieval-augmented framework designed to enhance the corner case comprehension capabilities of VLMs for autonomous driving. Through the integration of RAG and cross-modal alignment fine-tuning, RAC3 effectively mitigates hallucinations, ensuring more accurate and reliable scenario comprehension. Extensive experiments demonstrate significant improvements across key metrics, such as Cosine Similarity and ROUGE-L, showcasing the enhanced semantic alignment and reduced hallucinations achieved with our method.\nMoving forward, we aim to extend RAC3 in two key directions. First, we will incorporate more diverse and fine-grained multimodal datasets, encompassing additional corner cases and real-world scenarios, to further enhance model robustness. Second, we plan to generalize the framework to other safety-critical domains, such as decision-making and even controlling of autonomous vehicles, by adapting the retrieval mechanisms and embedding techniques to different operational contexts. These advancements will contribute to the development of safer and more reliable autonomous driving systems, reinforcing the importance of retrieval-augmented strategies in addressing complex real-world challenges."}]}