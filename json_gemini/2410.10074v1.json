{"title": "DIVIDE, REWEIGHT, AND CONQUER: A LOGIT\nARITHMETIC APPROACH FOR IN-CONTEXT LEARNING", "authors": ["Chengsong Huang", "Langlin Huang", "Jiaxin Huang"], "abstract": "In-Context Learning (ICL) emerges as a key feature for Large Language Models\n(LLMs), allowing them to adapt to new tasks by leveraging task-specific examples\nwithout updating model parameters. However, ICL faces challenges with increas-\ning numbers of examples due to performance degradation and quadratic compu-\ntational costs. In this paper, we propose Logit Arithmetic Reweighting Approach\n(LARA), a novel framework that enhances ICL by using logit-based ensembling\nof multiple demonstrations. Our approach divides long input demonstrations into\nparallelizable shorter inputs to significantly reduce memory requirements, and\nthen effectively aggregate the information by reweighting logits of each group via\na non-gradient optimization approach. We further introduce Binary LARA (B-\nLARA), a variant that constrains weights to binary values to simplify the search\nspace and reduces memory usage by filtering out less informative demonstration\ngroups. Experiments on BBH and MMLU demonstrate that LARA and B-LARA\noutperform all baseline methods in both accuracy and memory efficiency. We also\nconduct extensive analysis to show that LARA generalizes well to scenarios of\nvarying numbers of examples from limited to many-shot demonstrations.", "sections": [{"title": "1 INTRODUCTION", "content": "In-Context Learning (ICL) (Brown et al., 2020) is one of the emergent abilities of Large Language\nModels (LLMs) as they are scaled to billions of parameters (Wei et al., 2022). ICL enables LLMs to\nadapt to new tasks by utilizing task-specific examples within the input context (Dong et al., 2023),\nand does not require any updates to or access to model parameters. While ICL has achieved impres-\nsive performance across various domains, it encounters significant challenges when dealing with an\nincreasing number of examples. Longer context window size often leads to performance degrada-\ntion (Xiong et al., 2023). This is due to the low density of useful information within longer prompts,\nand the reduced sensitivity to positional information, both of which diminish the capability of the\nmodel to effectively capture and utilize key content. Additionally, the quadratic growth of computa-\ntional cost with the input length makes it particularly expensive for large-scale models.\nPrevious works primarily focus on two directions to address these challenges. The first direction is\ninput compression, which aims to shorten the input length (Jiang et al., 2023b; Pan et al., 2024; Xu\net al., 2023a; Wingate et al., 2022) or selectively retrieve relevant portions of demonstrations to be\nincluded in the prompt (an Luo et al., 2024). However, these methods risk losing critical information,\nwhich may negatively impact model performance. The second direction involves aggregating hidden\nstates within LLMs to simulate the effect of in-context demonstrations (Hao et al., 2022; Li et al.,\n2023; Hendel et al., 2023). These methods, however, are not applicable to closed-source models\nlike GPT-4, as they require direct access to the model internal weights. Additionally, they contradict\nthe core advantage of in-context learning, which is the ability to operate without modifications to\nhidden states or model parameters.\nIn this study, we propose a novel framework, Logit Arithmetic Reweighting Approach (LARA),\nwhich aims to combine the strengths of both input compression and hidden state approaches. Our\nmethod first divides demonstrations into subgroups to allow LLMs to focus on shorter inputs and re-\nduce computational requirements. We then design a weighted sum aggregation approach to combine"}, {"title": "2 PRELIMINARIES", "content": "In-Context Learning. Traditional In-Context Learning leverages N labeled examples in the input\nprompt, represented as $D_{train} = \\{(x_i, y_i)\\}_{i=1}^{N}$ to provide hints for language model generation. Each\npair (xi, Yi) is converted into a semantically meaningful demonstration $d_i = \\Tau(x_i, y_i)$ using a\npredefined template T. These demonstrations are then concatenated to form a comprehensive context\n$C = d_1 \\oplus d_2 \\oplus ... \\oplus d_N$, with appropriate separators (e.g., newlines or special tokens) between"}, {"title": "3 METHODOLOGY", "content": "In this section, we provide an overview of LARA. Figure 2 illustrates the overall framework of our\napproach. Unlike directly concatenating Dtrain into a single sequence, we first divide the N examples\ninto subgroups, which are used as inputs to the LLM. The output logits from these subgroups are\nthen aggregated, and we assign weights to each subgroup using a non-gradient search algorithm.\nDuring inference, the precomputed weights are used to combine the logits from each group."}, {"title": "3.1 PARTITION STRATEGY", "content": "Given N-shot in-context examples, we first split Dtrain into k disjoint subsets each containing L in-\ncontext examples, such that $D_{train} = S_1 \\cup S_2 \\cup ... \\cup S_k$ with $|S_i| = L$ for all $i \\in \\{1, ..., k\\}$. When\ninputting a subgroup $S_i$ to an LLM, we concatenate all of its elements to get $C_i = d_{(i-1)L+1} \\\nd_{(i-1)L+2} \\oplus d_{iL}$, and the complete input for the i-th subgroup to LLM is $C_i \\oplus X_{test}$. We assume\nthat N is divisible by k in our experiments, so that $L = N/k$. In practice, in cases where N is not\ndivisible by k, we could truncate the last subset and only retain $L(k-1)$ examples."}, {"title": "3.2 LOGIT-ARITHMETIC DECODING", "content": "Previous studies (Li et al., 2022; Liu et al., 2024; Dekoninck et al., 2023) have utilized logit offsets to\ncontrol the outputs of large language models for better generation quality or instruction following.\nInspired by these work, we propose a novel method that combines information from multiple in-\ncontext demonstrations through logit-arithmetic decoding. Specifically, our approach focuses on\naggregating the logits produced by the language model outputs for various contextual inputs. With\nthe input query $x_{test}$ and the example subset being $S_i$, we can compute the logit outputs of the\nlanguage model, denoted as $f_\\theta(S_i, x_{test}) = \\log p(y \\mid S_i, x_{test})$. We then combine these logits using\na weighted sum to get the generation probability over the output token:\n$p(y \\mid x_{test}, w) = \\text{softmax} \\Big(\\sum_{i=1}^{k} w_i f_\\theta(S_i, x_{test})\\Big)$       (2)\nwhere k is the number of example subsets, and wi are weights that indicate the importance of the\ncontribution of each subset, with $\\sum_{i=1}^{k} w_i = 1$. As a baseline approach, we could set uniform\nweighting, where $w_i = 1/k$. However, this may not be optimal for all tasks, as the quality and\nrelevance of different subgroups may vary. In the following section, we introduce a reweighting\nstrategy to optimize these weights to enhance model performance."}, {"title": "3.3 REWEIGHTING LOGITS BY NON-GRADIENT OPTIMIZATION", "content": "To further enhance the model performance, we employ non-gradient optimization methods to opti-\nmize the weights wi based on the loss calculated from $p(y \\mid x_{val})$. Given the combined probability\n$p(y \\mid x_{val})$, our objective is to minimize a cross-entropy loss function $\\mathcal{L}(w)$ over the predicted prob-\nabilities and the ground truth. Specifically, we utilize the following cross-entropy loss function for\nthe generation model:\n$\\mathcal{L}(w) = - \\sum_{(x_{val}, y_{val}) \\in D} \\sum_{t=1}^{T} \\log p(y_t \\mid x_{val}, w)$       \nwhere D represents the validation dataset, T is the length of the sequence, yt is the true word at time\nstep t, xval is the input sequence, w denotes the weight vector, and $p(y_t \\mid x_{val}, w)$ represents the\npredicted probability of the true word yt at time step t, given the input sequence \u00e6val and the weight\nvector w.\nTo avoid introducing additional labeled data, we employ a cross-validation strategy. We partition the\ndemonstration set S into two subsets: $S_A = S_1 \\cup S_2 \\cup... \\cup S_{\\lfloor k/2 \\rfloor}$ and $S_B = S_{\\lfloor k/2 \\rfloor + 1} \\cup S_{\\lfloor k/2 \\rfloor + 2} \\cup\n... \\cup S_k$. When optimizing weights for $S_i \\in S_A$, we use SB as the validation set, and vice versa."}, {"title": "3.4 BINARY CONSTRAINTS FOR LARA", "content": "We further propose a variant of LARA, named as B-LARA, by imposing a hard constraint on the\nweight vector w to binary values {0,1}. This binary constraint offers two key advantages: first, it\nsimplifies the search space and potentially leads to faster convergence; second, it allows for direct\nelimination of demonstration groups with zero weight, thereby improving inference efficiency. In-\ntuitively, the binary optimization of w can be seen as a form of subset selection to identify the most\nrelevant demonstrations in Dtrain benefitting model performance on specific tasks.\nTo solve this binary optimization problem, we employ the simplest evolution strategy (1+1)-\nES (Rechenberg, 1973). It involves a simple cycle: a single parent produces one offspring per\ngeneration through mutation-adding a small, random change. If this offspring performs as well\nor better than the parent based on a predefined fitness criterion, it becomes the new parent for the\nnext generation. Otherwise, the original parent remains. The overall sampling procedure is shown\nin Algorithm 1."}, {"title": "3.5 COMPUTATIONAL COMPLEXITY", "content": "We analyze the computational complexity of LARA and B-LARA compared to standard ICL. Dur-\ning inference, the self-attention mechanism in Transformer models is the primary bottleneck for\nGPU memory requirement, with the memory complexity being O(n\u00b2), where n is the input se-\nquence length. This quadratic scaling is due to the pairwise interactions between tokens in the\nattention matrix.\nBy splitting the input sequence into k groups, each of length around, LARA and B-LARA\ncan leverage parallel computing resources more effectively. The complexity for LARA becomes\nO(2*k)= O(). B-LARA further reduces computational complexity by selecting only a subset\nof groups. If m out of k subgroups are assigned non-zero weights, then the complexity of B-LARA\nbecomes O(mm).\n(m\u00b2). We show the empirical GPU memory usage in Sec. 5.2."}, {"title": "4 EXPERIMENTS", "content": "In this section, we provide details of our main experiments. We first give an overview of the exper-\nimental setup and implementation details in Sec. 4.1, and then present our findings along with the\nresults in Sec. 4.2."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Datasets and Evaluation. We evaluate our methods using two well-established benchmarks:\nBig-Bench Hard (BBH) (Srivastava et al., 2022) and Massive Multitask Language Understanding\n(MMLU) (Hendrycks et al., 2021). BBH tests models on challenging reasoning tasks across do-\nmains including arithmetic reasoning, commonsense reasoning, and linguistics. MMLU measures\ngeneralization across 57 diverse subjects, covering both humanities and STEM fields, offering a\ncomprehensive evaluation of knowledge and problem-solving abilities of LLMs. For both bench-\nmarks, we use exact match (EM) as our evaluation criterion, which requires model predictions to\nperfectly match the correct answers. We report the accuracy scores in our experiment results. The\ndetails about dataset analysis and prompts can be found in Appendix A.\nModels. Our proposed LARA for in-context learning is applicable to any LLM. To demonstrate its\ngenerality, we evaluate it on three open-source, decoder-only models: Llama3.1-8B (Dubey et al.,\n2024), Mistral-7B (Jiang et al., 2023a), and Gemma-7B (Mesnard et al., 2024). Llama-3.1-8B is\nknown for strong performance across various NLP tasks, Mistral-7B is optimized for efficiency and\nis balanced between computational cost and accuracy. Gemma-7B focuses on advanced reasoning\nand language comprehension. These models represent diverse architectures and training strategies,\nallowing us to test the adaptability of our methods. By using open-source models in evaluation,\nwe ensure the reproducibility of our proposed method and validate its broad applicability across\nstate-of-the-art model architectures.\nHyperparameter Setting. In our main experiment, we use Dtrain consisting of N = 32 in-context\nexamples for our methods. For each task, Dtrain is split into subsets of size $L \\in \\{2,4,8\\}$, and for\neach L we perform up to J = 20 iterations for weight optimization. We compare the minimum\nvalidation loss across different settings of L to determine the optimal configuration for the final\ninference phase. The baseline methods also use the same Dtrain as input. For our method and all\nbaselines, we set the temperature to 0 to enforce greedy decoding. Our experiments are conducted\non a single A100 80GB GPU.\nCompared Methods. We introduce several primary baseline methods: Direct In-Context Learn-\ning (ICL), KNN-Augmented In-ConText Example Selection Liu et al. (2022) (KATE), Rationale-\nAugmented Ensembles (RAE) (Wang et al., 2022) and In-context Vector (ICV) (Liu et al., 2023) as\nthe representative of parameter access methods. We use the same 32 in-context examples as inputs"}, {"title": "4.2 MAIN RESULTS", "content": "Results from Table 1 demonstrate the effectiveness of our proposed methods, LARA, and B-LARA,\nacross BBH and MMLU benchmarks. B-LARA consistently outperforms most of baseline methods\nacross three model architectures. Notably, B-LARA achieves the highest accuracy and improves\nover direct ICL by 2.05, 5.67, and 2.12 points on BBH dataset across three models respectively.\nMoreover, our methods and can consistently outperform retrieval or simple ensemble baselines like\nKATE and IRE, indicating that our method is more effective in combining information from multiple\ndemonstration subgroups. Compared to the ICV baseline, which has the advantage of access to\nmodel parameters, our methods still achieve better performance without access to the hidden state,\nwhich further demonstrates the efficacy of our methods in aggregating information without direct\naccess to model internal parameters.\nAn interesting finding is that B-LARA performs better than LARA despite a more constrained search\nspace for the weight vector. We believe this is because we only use 20 iterations for weight opti-\nmization, and the binary constraint brings more benefits by introducing a simplified optimization\nlandscape and providing a regularization effect to prevent overfitting."}, {"title": "5 ANALYSIS", "content": "In this section, we present a comprehensive analysis of our proposed method LARA under various\nconditions."}, {"title": "5.1 HOW DOES THE REWEIGHTING STEP AFFECT MODEL PERFORMANCE?", "content": ""}, {"title": "5.2 How DOES LARA ENHANCE MEMORY EFFICIENCY?", "content": "We empirically evaluate the computational\nefficiency of LARA by measuring GPU\nmemory usage with different input se-\nquence lengths and subgroup configura-\ntions. We set the number of groups k with\n1,2,4,8. Specifically, when k is set as 1,\nLARA will degrade to ICL.\nResults in Figure 3 demonstrate that\nLARA is more memory-efficient com-\npared to standard ICL, especially when\nhandling long sequences. Standard ICL\nresults in Out-of-Memory (OOM) errors\nwhen the input length exceeds 10k tokens\non a Mistral-7B model with a batch size of\n4 on an A100 80GB GPU. In contrast, our\nmethod handles input lengths over 25k to-\nkens with 4 and 8 subgroups, demonstrat-\ning that LARA efficiently utilizes larger\namounts of training data."}, {"title": "5.3 CAN LARA PERFORM WELL WITH MORE EXAMPLES?", "content": "We investigate the performance of LARA with an increased number of demonstrations, leveraging\nthe LongICLBench (Li et al., 2024), a benchmark tailored for addressing challenges in long in-\ncontext learning. For our experiments, we select two datasets: GoEmotion and TacRED. Following\nthe LongICLBench setup, we employ multiple rounds of examples, where each round includes sev-\neral examples, each labeled with a distinct class. To align with the input limit constraints of ICL,\nwe sampled 8 rounds (224 examples) of examples for GoEmotions and 4 rounds (164 examples)\nfor TacRED. For LARA and B-LARA, we choose 4, 8, and 16 as the potential candidate number of\ngroups. We report the accuracy of different methods on these datasets in Table 3.\nThe experimental results clearly highlight the advantages of LARA, which demonstrates consistent\nimprovements over baseline methods across both GoEmotion and TacRED datasets, showcasing\nits effectiveness in diverse tasks. Notably, the B-LARA variant further amplifies this performance,\noutperforming all competing approaches on both datasets and across various models. This suggests\nthat B-LARA can work well in many shot settings."}, {"title": "5.4 CAN LARA PERFORM WELL WITH LIMITED IN-CONTEXT EXAMPLES?", "content": "In previous experiments, we primarily explore the many-shot in-context learning (ICL) setting. In\nthis subsection, we focus on a more constrained scenario, where only a limited number of in-context\nexamples are available. This analysis aims to understand the relationship between the number of\ndemonstrations and the performance of LARA compared to baseline methods with limited examples.\nWe set the number of examples N within\n{2, 4, 8, 16} and compare our proposed method\nwith ICL on the BBH dataset with Mistral-\n7B. Figure 4 demonstrates that both LARA\nand B-LARA consistently outperform the base-\nline ICL, and the performance gap increases\nwith the number of examples used. Note that\nwe do not plot the performance of LARA and\nB-LARA under N= 2. This is because\nLARA and B-LARA are simplified to our non-\nreweighting ablation when the size of each sub-\ngroup becomes 1 and no reweighting is re-\nquired. We also show the performance of per-\nformance without reweighing here. We set the\nnumber of group k as 2 in this experiment.\nWhile there is a significant gap between the\nnon-reweight version and B-LARA, the non-\nreweight version still demonstrates effective-\nness compared to ICL.\nSince B-LARA has a weight constraint of {0, 1}, subgroups with zero-weights are pruned during\ninference for efficiency. As shown in Figure 4, the real number of examples used by B-LARA in\ninference is substantially lower than other methods. In the 32-shot setting, only 45% of subgroups of\nB-LARA are assigned non-zero weights, reducing more than half of the computational load without\ncompromising performance. Additionally, as the total number of examples increases, the propor-\ntion of examples used in inference decreases, indicating that B-LARA is particularly suitable for\nresource-constrained environments."}, {"title": "5.5 Is LARA APPLICABLE TO BLACK-Box LLMS?", "content": "One advantage of our method is that it could\nalso be applied to LLM APIs, since it only uses\noutput logits for example reweighting or selec-\ntion. In these scenarios, techniques such as in-\ncontext vector or task vector, which often rely\non internal state visibility, cannot be applied."}, {"title": "6 RELATED WORK", "content": ""}, {"title": "6.1 LONG IN-CONTEXT LEARNING", "content": "Recent studies on long-context learning problems in LLMs can be categorized into two main strate-\ngies: enhancing the impact of in-context examples and compressing input sequences. Structured\nprompting leverages rescaled attention mechanisms to effectively integrate grouped examples (Hao\net al., 2022). Methods such as task vectors (Hendel et al., 2023) and function vectors (Todd et al.,\n2023) further refine this strategy by generating vectors that assess the contribution of each example\nbased on the offset of hidden state, which improves model adaptability. Liu et al. (2023) generate\ntask-specific vectors that steer model behavior in latent space based on the in-context examples.\nRegarding input compression, methods like prompt pruning (Jiang et al., 2023b; Pan et al., 2024)\nand additional summarization models (Xu et al., 2023a; Gilbert et al., 2023) directly shorten in-\nputs while maintaining essential content. Soft prompt-based compression (Wingate et al., 2022; Mu\net al., 2023) intends to generate a soft-prompt that includes most of the information."}, {"title": "6.2 LOGIT ARITHMETIC", "content": "Several works have employed logit arithmetic across various domains and downstream tasks. Con-\ntrastive decoding (Li et al., 2022) improves performance by utilizing the difference in logits from\nmodels of different sizes. Proxy tuning (Liu et al., 2024) enhances a larger model's capabilities by\nadding the logit differences of a smaller model, recorded before and after training, to simulate train-\ning effects. In model arithmetic (Dekoninck et al., 2023), logits adjusted with various prompts steer\nthe generation processes of large language models. Huang et al. (2024) propose using logit subtrac-\ntion to facilitate the selective forgetting of knowledge in LLMs. Additionally, logit arithmetic has\nbeen leveraged to enhance the safety of generated outputs (Xu et al., 2024)."}, {"title": "6.3 NON-GRADIENT OPTIMIZATION OF LLMS", "content": "Due to the high memory requirements associated with gradient-based optimization methods, recent\nresearch has shifted towards non-gradient techniques for neural network optimization. Zhang et al.\n(2024); Malladi et al. (2023) propose training large language models (LLMs) using non-gradient\nmethods to mitigate these memory constraints. These approaches have also been applied in federated\nlearning, exploring their effectiveness in distributed settings (Xu et al., 2023b). Additionally, a\ngradient-free method has been used to optimize manifold neural networks (Zhang et al., 2022).\nSimilarly, LoraHub (Huang et al., 2023) utilizes non-gradient techniques to dynamically reweight\ndifferent LORA modules, enhancing adaptation to new downstream tasks. Guo et al. (2023) also\nintroduces non-gradient methods to prompt engineering to search for better prompts."}, {"title": "7 CONCLUSION", "content": "We propose LARA, a novel framework that enhances in-context learning by ensembling logits from\nmultiple demonstrations, improving performance without requiring parameter updates. Our method\nreduces computational complexity while achieving better accuracy. Additionally, Binary LARA fur-\nther optimizes efficiency by selectively removing less informative demonstrations. Experiments on\nBBH and MMLU benchmarks show that both LARA and B-LARA outperform traditional ICL meth-\nods in terms of efficiency and performance. Future research directions include extending our study\nto combine logits from different sources beyond just in-context learning (ICL) examples-such\nas different models or varying instructions-and building a distributed inference system based on\nLARA."}]}