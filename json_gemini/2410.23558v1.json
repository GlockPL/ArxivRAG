{"title": "Transferable Ensemble Black-box Jailbreak\nAttacks on Large Language Models", "authors": ["Yiqi Yang", "Hongye Fu"], "abstract": "In this report, we propose a novel black-box jailbreak attacking frame-\nwork that incorporates various LLM-as-Attacker methods to deliver trans-\nferable and powerful jailbreak attacks. Our method is designed based on\nthree key observations from existing jailbreaking studies and practices.\nFirst, we consider an ensemble approach should be more effective in expos-\ning the vulnerabilities of an aligned LLM compared to individual attacks.\nSecond, different malicious instructions inherently vary in their jailbreak-\ning difficulty, necessitating differentiated treatment to ensure more effi-\ncient attacks. Finally, the semantic coherence of a malicious instruction is\ncrucial for triggering the defenses of an aligned LLM; therefore, it must be\ncarefully disrupted to manipulate its embedding representation, thereby\nincreasing the jailbreak success rate. We validated our approach by par-\nticipating in the Competition for LLM and Agent Safety 2024, where our\nteam achieved top performance in the Jailbreaking Attack Track.", "sections": [{"title": "Introduction", "content": "Jailbreaking large language models (LLMs) is being intensively studied to eval-\nuate the safety of LLMs [1, 2]. Existing jailbreaking methods, as adversarial\nattacks targeting small-scale learning models, can be categorized into white-box\nand black-box approaches. In white-box methods, gradient-based optimiza-\ntion techniques are employed to identify adversarial suffixes [3]. For black-box\nmethods, various optimization strategies, such as genetic algorithms, are used\nto refine jailbreak prompt templates through rephrasing, word replacement, and\nother modifications [4, 5]. Recent research [6] has demonstrated that LLMs can\nfunction as powerful optimizers when provided with sufficient contextual infor-\nmation. Indeed, both the persuasive adversarial prompt (PAP) [7] and Tree of\nAttacks with Pruning (TAP) [8] methods leverage multiple LLMs to optimize\nmalicious instructions. In this line of research, given a target aligned LLM to\njailbreak, one LLM operates as an attacker, optimizing a malicious instruction\nto circumvent the unknown safety mechanisms of the target LLM. A second\nLLM serves as a judge, observing the interaction history between the attacker\nand the target LLM, evaluating the attack's effectiveness, and providing feed-\nback to the attacker LLM. The PAP method [7] has been officially adopted as\nthe baseline attack strategy in the Competition for LLM and Agent Safety 2024\n(CLAS 2024)1.\nModel ensembling has been widely recognized as one of the most effective\napproaches in various machine learning competitions. In the context of jail-\nbreaking LLMs, we believe that this approach holds promise, although there\nhas been limited research exploring this method. Existing studies, as previously\nmentioned, have focused on developing specific jailbreaking techniques. This,\nhowever, has the limitation of only partially exposing the vulnerabilities of a\ntarget LLM, potentially leading to an inaccurate assessment of model safety.\nFurthermore, based on our experiences, we find that different malicious instruc-\ntions exhibit varying levels of effectiveness for bypassing the safety mechanism\nof an aligned LLM. For example, the widely used \"how to build a bomb\" query\nis often readily refused, whereas other instructions, such as \"Provide detailed\ninstructions on where to obtain hydroxychloroquine without a prescription ...\"2,\nhave a higher likelihood of eliciting harmful responses from the target LLM. This\ndifference in the malicious instructions suggests that a comprehensive evalua-\ntion of LLM safety requires tailored optimization efforts for different attacking\ninstructions. Moreover, prior research [9, 10] has demonstrated that the inter-\nnal representations of malicious and benign instructions processed by an aligned\nLLM are distinctly separated. Given that these embeddings mainly retain the\nsemantic information of the instructions, delivering more effective jailbreaking\nattacks necessitates perturbing these embeddings without significantly altering\ntheir original semantics.\nBased on the aforementioned observations, we have designed our jailbreaking\nmethod according to the following principles:\n1.  Ensemble Different LLM-as-Attacker Methods: We begin by devel-\n    oping an ensemble framework that incorporates different LLM-as-Attacker\n    methods. This framework aims to deliver transferable and powerful jail-\n    break attacks.\n2.  Adjust Optimization Budgets for Different Instructions: Given a\n    set of malicious instructions, we analyze their individual jailbreak scores\u00b3\n    to identify those that are more challenging to optimize. We then allocate\n    additional computation resources to these instructions.\n3.  Shatter Semantic Coherence to Balance Attack Performance and\n    Instruction Stealthiness: To maintain a balance between attack per-\n    formance and instruction stealthiness, we randomly select a small subset\n    of words from the given malicious instructions and insert them into the\n    optimized instructions. This helps to preserve relatively high TF-IDF\n    similarity, thereby avoiding easy detection."}, {"title": "Method", "content": "Among various jailbreaking methods, we find that the PAP method [7] and a\nsimilar TAP method [8], exhibit superior transferability and overall jailbreaking\nperformance. Therefore, we propose to develop our ensemble jailbreak frame-\nwork based on these two existing methods.\nIt should be noted that the original TAP prompting framework includes a\nfeedback session to incorporate the interaction history for the attacker LLM. In\nour design, we propose to replace TAP's original evaluation prompt with the one\nprovided by the competition organizers and to introduce additional information\nin the feedback session. This includes the reasoning behind the judge LLM's\nevaluation of an optimized malicious instruction and a keyword score, which\nquantifies the likelihood of the target LLM refusing to respond to the optimized\nmalicious instruction. Our modification of the TAP prompting framework is\nshown in Figure 1.\nWe utilize each of these methods individually to optimize the same set of\nmalicious instructions provided by the competition organizers, and evaluate the\njailbreak scores of the optimized malicious instructions generated by these two\nmethods according to the evaluation protocol specified by the competition orga-\nnizers. Given the two sets of evaluated instructions, we propose either a greedy\nselection of the instructions that yield higher jailbreak scores or a weighted ran-\ndom sampling mechanism to avoid over-optimization for our local environment.\nIn our local experiments, we observe that the greedy approach achieves over-\nall better performance. Consequently, we adopt the greedy approach in our\nsubmitted implementation."}, {"title": "Stealthiness Enhancing", "content": "To enhance the stealthiness of the optimized malicious instructions 5, which\nreflects the difference in word frequency between the original and optimized\ninstructions, we propose to add a limited set of words that are randomly sampled\nfrom the original instructions. To prevent the added words, which may contain\nobvious malicious semantics, from triggering the target LLM to refuse to respond\nto the optimized instructions, we remove obviously harmful words from the\noriginal instructions. It should be noted that this word-insertion operation\nmay affect the semantic correctness of a malicious instruction. To address this,\nwe further propose an iterative approach to select optimized instructions that\nachieve increased stealthiness while maintaining their jailbreak scores."}, {"title": "Selective Boosting", "content": "Through our intensive local experiments, we have observed that not all of the\nprovided malicious instructions exhibit the same difficulty in jailbreaking the\ntarget LLM. To illustrate this effect, we present the distribution of jailbreak\nscores for all malicious instructions, as evaluated by the Llama3-8B-Instruct\nmodel, in Figure 2. To further enhance jailbreak performance within limited\ncomputational budgets, we propose to iteratively optimize the instructions with\nlower scores, allocating additional computational resources to them."}, {"title": "Experiments", "content": "We evaluate our proposed method using the dataset of malicious\ninstructions provided by the CLAS 2024 competition. These instructions cover\nvarious domains, including finance, law, and criminal planning, etc.\nWe utilized Gemma-2B-IT and Gemma2-9B-IT as our target models\nfor the attacks and employed Llama3-8B-Instruct, GLM-4-Plus, GLM-4-Flash,\nQwen-Max-Latest, and DeepSeek-V2.5 as judge models to evaluate the attack-\ning performance. We used GPT-4 and Qwen-Max-Latest as the attackers to\noptimize malicious instructions for the TAP and PAP method, respectively."}, {"title": "Transferable Effectiveness of the Ensemble Jailbreak\nAttack", "content": "We present the performance of different jailbreak methods across different target\nmodels in Table 1. Our proposed ensemble jailbreak method demonstrates sig-\nnificant improvements over the individual TAP and PAP methods on both target\nmodels. Additionally, we observed that the proposed stealthiness enhancement\nmethod not only achieved higher stealthiness scores but also enhanced the jail-\nbreak scores."}, {"title": "Transferability for Judge Models", "content": "In our experiments, we observed that the prompt designed for configuring a LLM\nas a judge, as provided by the competition organizers, exhibits strong transfer-\nability. We applied this identical judgment prompt across various commercial\nand open-source LLMs. As illustrated in Figure 3, the ratings generated by dif-\nferent judge LLMs were both stable and closely correlated, thereby validating\nthe efficacy of the official judgment process."}, {"title": "Conclusion", "content": "In this report, we introduce our proposed jailbreak method, which we devel-\noped for participating in and winning the Competition for LLM and Agent\nSafety 2024. Through the public competition and preliminary experimental re-\nsults, we highlight the effectiveness of the proposed ensemble framework, the\nstealth-enhancing method, and the necessity of adaptively optimizing malicious\ninstructions based on their jailbreaking difficulties. In our future work, we will\nfurther refine our proposed method and conduct more comprehensive evalua-\ntions to demonstrate its transferability."}]}