{"title": "Mask-RadarNet: Enhancing Transformer With Spatial-Temporal Semantic Context for Radar Object Detection in Autonomous Driving", "authors": ["Yuzhi Wu", "Jun Liu", "Guangfeng Jiang", "Weijian Liu", "Danilo Orlando"], "abstract": "Abstract\u2014As a cost-effective and robust technology, automotive radar has seen steady improvement during the last years, making it an appealing complement to commonly used sensors like camera and LiDAR in autonomous driving. Radio frequency data with rich semantic information are attracting more and more attention. Most current radar-based models take radio frequency image sequences as the input. However, these models heavily rely on convolutional neural networks and leave out the spatial-temporal semantic context during the encoding stage. To solve these problems, we propose a model called Mask- RadarNet to fully utilize the hierarchical semantic features from the input radar data. Mask-RadarNet exploits the combination of interleaved convolution and attention operations to replace the traditional architecture in transformer-based models. In addition, patch shift is introduced to the Mask-RadarNet for efficient spatial-temporal feature learning. By shifting part of patches with a specific mosaic pattern in the temporal dimension, Mask- RadarNet achieves competitive performance while reducing the computational burden of the spatial-temporal modeling. In order to capture the spatial-temporal semantic contextual information, we design the class masking attention module (CMAM) in our encoder. Moreover, a lightweight auxiliary decoder is added to our model to aggregate prior maps generated from the CMAM. Experiments on the CRUW dataset demonstrate the superiority of the proposed method to some state-of-the-art radar-based object detection algorithms. With relatively lower computational complexity and fewer parameters, the proposed Mask-RadarNet achieves higher recognition accuracy for object detection in autonomous driving.", "sections": [{"title": "I. INTRODUCTION", "content": "AST decades have witnessed growing interests in au- tonomous driving [20], [24]\u2013[26], [37], [51], and ob- ject detection is one of the most fundamental task for practical deployment. To better capture the surrounding ob- jects, sensors equipped in autonomous cars are receiving increasing attention. Among the commonly used sensors,\nmillimeter-wave (MMW) frequency-modulated continuous- wave (FMCW) radar has the following unique advantages: 1) the narrow MMW band allows radar signal to penetrate through fog and smoke, which is crucial in extreme weather conditions; 2) FMCW radar has better acquisition capabilities for detecting longer ranges; 3) FMCW radar is robust to light- ing while being cheap. However, because of the difficulties in deciphering significant clues for semantic understanding, radar is frequently regarded as a complement sensor for RGB cameras and LiDARs. Comparatively, the RGB images and point cloud data from cameras and LiDARs are relatively easy for human to understand since the semantic information they convey is obvious [53]. For example, Fig. 1 shows some RGB images and their corresponding radio frequency (RF) images which represent the same scene. In recent work [13], [35], [38], FMCW radar is merely processed to provide location and speed information for the detected objects without fully exploiting the semantic information. In other words, the development of object detection with FMCW radar is still in its early stages, making it worthwhile to explore further.\nRadar data are usually represented in two different formats, i.e., RF images and radar point clouds. Considering that the current 3D radar point clouds are too sparse to detect objects accurately [43], [44], [52], many researchers start to take advantage of RF images [12], [34]. In the field of traditional signal processing [36], peak detection algorithms, such as those with a constant false alarm rate, are utilized in RF images to determine the object's location. Subsequently, a classifier is employed to identify the object's category [1]. With the"}, {"title": null, "content": "emergence of deep learning, the focus of research is naturally shifted to extract RF image features via neural networks. It has to be pointed out that many labeled data are required for training neural networks. However, it is more difficult to annotate RF images than RGB images due to the abstract semantic information, especially for object detection task. Zhang et al. [61] proposed an instance-wise auto-annotation method to build a new radar dataset called RADDet. But the size of this dataset is small. Ouaknine et al. [40] presented a semi-automatic annotation approach and proposed a dataset of synchronized camera and radar recordings with annota- tions. Recently, Wang et al. [52] developed a cross-modal supervision framework to annotate object labels on RF images automatically with a camera-radar fusion (CRF) strategy, and built a new benchmark for radar object detection task. During the training stage, annotations of object are processed into confidence maps as the ground truth. To test the models, the output is post-processed like [39] to generate final results.\nWith the open access of radar RF image datasets, much work [19], [45], [52] uses 3D convolutional neural network (CNN) to extract semantic and velocity information from multi-frame RF images. While these models perform well in certain tasks, the extensive 3D convolutions come with a high computational cost, which may be inappropriate for real-time applications. Besides, 3D CNN shows poor performance in extracting global features and it cannot acquire the dependen- cies between multi-frame radar RF images well. The work in [22] was the first one to introduce the transformer-based model into radar object detection. The model is a U-shaped one containing convolution and attention operations. Although the architecture facilitates the extraction of multiscale features, it overlooks the significance of the spatial-temporal semantic context of the attention maps, leading to some misclassified results.\nTo solve the issues mentioned above, we propose a novel model called Mask-RadarNet, a 3D transformer for radar object detection. The Mask-RadarNet exploits the combination of interleaved convolution and self-attention operations. The hybrid architecture enables the encoder of Mask-RadarNet to extract local and global features effectively. We utilize a simple but effective method called patch shift [54] for efficient spatial-temporal modeling in the 3D transformer. This attempt enhances spatial-temporal feature learning efficiently for our model. Moreover, we design a class masking atten- tion module (CMAM) in our encoder to capture the spatial- temporal contextual information. Although RF images are non-intuitive compared with RGB images, and much more difficult for human eyes to understand, we still hold the belief that the spatial-temporal semantic context contained in RF image sequences is crucial for radar object detection. With the supplement of the spatial-temporal semantic context, the CMAM enhances the global feature acquired by attention operations. It also generates prior maps for our task and guides the model to update during the training stage. Besides, we add a semantic auxiliary decoder to aggregate prior maps from different stages. As shown in Fig. 2, our model achieves the state-of-the-art (SOTA) performance on the CRUW dataset. In brief, our work has made the following contributions:"}, {"title": null, "content": "1) We propose Mask-RadarNet that is a 3D Transformer combining interleaved convolution and self-attention opera- tions for radar object detection. The proposed Mask-RadarNet achieves significant improvements in object detection perfor- mance over previous models on the CRUW dataset.\n2) We introduce patch shift in the Mask-RadarNet for efficient spatial-temporal feature learning. Our model achieves competitive performance with other methods while reducing the computational burden of spatial-temporal modeling.\n3) We design a specific module called CMAM to capture the spatial-temporal contextual information and enhance the global feature with spatial-temporal semantic context. Besides, we add an auxiliary decoder to generate prior maps during the training stage.\nThe rest of this article is structured as follows. In Section II, we introduce some related work. Section III describes the design of Mask-RadarNet in detail. The experimental results and ablation studies are included in Section IV. Section V concludes with conclusions."}, {"title": "II. RELATED WORK", "content": "In Sections II-A, we discuss the current radar-based per- ception methods for autonomous driving. Then we review some transformer-based methods for various computer vision tasks in Sections II-B. Besides, we introduce some work about semantic context for various computer vision tasks in Sections II-C."}, {"title": "A. Radar-Based Perception Methods for Autonomous Driving", "content": "There are mainly two representations of radar data: one is the dense raw RF images, and the other is the sparse radar point clouds. In Sections II-A, we review related work on perception for autonomous driving from the perspective of two different formats.\n1) RF image: With the advancement in deep learning, a series of research explores neural networks to extract features from raw RF images. Capobianco et al. [5] employed CNN to recognize vehicle categories from range-doppler images."}, {"title": null, "content": "Angelov et al. [1] considered a modular pipelined framework on raw radar data, and explored three distinct kinds of neural networks including convolution-based ones to classify radar objects. Gao et al. [17] used a modified complex-valued convolutional neural network to enhance radar imaging. Zhang et al. [61] utilized a residual network as the backbone and proposed a dual detection head for more accurate predictions. In order to extract semantic and velocity information from multi-frame radar images, some work was proposed to use 3D convolution. Hazara et al. [18] employed a model based on 3D CNN to acquire embedding features from radar data with a distance-based triplet-loss similarity metric. Wang et al. [52] proposed a stacked-hourglass model on multiframe RF images to generate predictions. Hsu et al. [19] further adopted dilated convolution in the backbone network to achieve a larger receptive field. The work in [45] used the squeeze-and- excitation network to predict the location and category of the object. RaLiBEV [59] proposed a novel fusion paradigm with radar range-azimuth heatmaps and LiDAR point clouds, and designed an anchor-free detector based on the fused features. Inspired by DETR [6], LQCANet [67] also employed the learnable-query for feature fusion at various scales. T-RODNet [22] introduced a transformer-based model in radar object detection that contains convolutions and attention operations, with the intention of utilizing the ability of both to acquire local and global features simultaneously. SS-RODNet [68] further proposed a lightweight model by pretraining radar spatial-temporal information."}, {"title": "2) Radar Point Cloud", "content": "In the current real-world automotive application, radar suppliers commonly provide radar point clouds for autonomous driving. As a lightweight data repre- sentation, point clouds provide an intuitive spatial structure of the surroundings. Liu et al. [30] believed that radar points with diverse semantic information rarely belong to the same object, and designed a clustering method based on semantic segmentation. Xiong et al. [57] proposed a contrastive learning method to address the problem of insufficient annotation of radar points, and designed a model that performs well with limited labeled radar points. Kernel density estimation branch is added to the pillar-based backbone for feature encoding in SMURF [31], alleviating the impact of sparsity in radar point clouds. Some work attempts to integrate radar point clouds and corresponding RGB images. RCFusion [65] utilized orthographic feature transform for transforming the image perspective view (PV) features into the bird's-eye-view (BEV) domain, and then fused image BEV features and radar BEV features using interactive attention module. LXL [56] generated radar occupancy grids and predicted image depth distribution maps separately, which both assist in converting image PV features to BEV features, so that the image features can be aligned with radar BEV features. Although radar point clouds have advantages in being a lightweight data representation, they suffer from the inevitable loss of potential information in raw radar tensors during signal processing [34], which may cause the failure in detecting small objects."}, {"title": "B. Transformer-based Methods for Various Computer Vision Tasks", "content": "After being developed in the field of natural language pro- cessing (NLP) [47], transformer-based methods have gained popularity for various computer vision tasks following vision transformer (ViT) [14]. The transformer-based methods have produced outstanding results on semantic segmentation [16], [48], object detection [6], [66], image generation [7], [15], video segmentation [3], [33] and other computer vision tasks [42], [50], [55]. Liu et al. [32] adopted the shifted window- based approach in ViT architectures, which greatly enhances performance. Other work followed this approach. Cao et al. [4] built a U-shape transformer-based model that employs the hierarchical transformer architecture with shifted windows as the backbone for feature extraction. To further improve the semantic segmentation quality of medical images, Lin et al. [27] attempted to simultaneously integrate the advantages of the hierarchical transformer architecture with shifted windows into both encoder and decoder of the U-shaped model.\nHowever, in comparison to CNNs, vision transformers still experience the drawback of image-specific inductive bias, leading to inefficiency in extracting local information. Some researchers try to investigate ways to improve the local feature modeling capabilities of ViTs. To model the relationships between tokens at different scales, Xu et al. [58] adopted a hybrid architecture that contains depthwise convolutions and cross-attention operations. Chu et al. [10] built a model upon pyramid vision transformer [49] by combining depthwise separable convolutions and relative position embedding. Tu et al. [46] proposed a model which involves MBConv-based convolution followed by block-wise attention operations and grid-wise attention operations."}, {"title": "C. Semantic Context for Various Computer Vision Tasks", "content": "Computer vision tasks require semantic context information to get high-quality results. Chen et al. [8] proposed a module which employs atrous convolution to efficiently broaden the field of view of filters in order to include more context information. They further augmented this module with global average pooling in [9]. Zhao et al. [64] proposed a pyramid pooling module which can aggregate context from different region to leverage global context information. Yu et al. [60] added the global pooling on the top of the U-shape model with the purpose of encoding the global context. Zhang et al. [63] designed a new context encoding module that, by introducing prior information, improves the model's performance. Jain et al. [21] incorporated the semantic context of RGB images into the backbone of a hierarchical transformer model. Zhang et al. [62] explored the impact of global contextual information in semantic segmentation. Jin et al. [23] advocated enhanc- ing pixel representations by combining the image-level and semantic-level contextual information."}, {"title": "III. METHODOLOGY", "content": "The overall architecture of our Mask-RadarNet is presented in Fig. 3, which maintains one encoder and two differ-"}, {"title": "B. Encoder", "content": "1) PatchShift 3D SwinTransformer Module: Considering that the inter-frame details within RF image sequences play an indispensable role in assisting the network to accurately recognize targets, we propose PatchShift 3D SwinTransformer Module for spatial-temporal feature extraction. Transformer model with patch shift operation was first proposed by [54] for action recognition. It was originally designed for RGB image sequences. Our model for the first time introduces it in RF image sequences for efficient spatial-temporal fusion. Generally, patch shift is an effective way for temporal mod- eling, which shifts the patches of input features along the temporal dimension following specific patterns. Fig. 4 shows an example of patch shift for three neighboring frames, where the symbols H, W, and T denote the height, width, and"}, {"title": null, "content": "temporal dimension, respectively, and the blue, red and yellow colors represent the frames t 1, t, and t + 1, respectively. Part of patches in red frame are replaced by patches from blue and yellow frames. This means the current frame t aggregates information from other frames t 1 and t + 1 with a specific pattern. Patch shift can be carried out with different patterns. Fig. 5 depicts some typical shift patterns. The numbers denote the frame indices from which the patches are taken. The symbols \"0\", \"-\", \"+\" represent the current, previous, and next frames, respectively. To cover all patches, we continually apply shift pattern in a sliding window way [32]. After patch shift operation, the spatial features mingle with the temporal feature in a zero-computation way. Hence we can directly exploit volume-based 3D transformer module.\nPatch shift operation learns temporal information by moving part of patches from other frames, thus keeping the full channel information of each patch. This means that patch shift is sparse in the spatial domain but dense in the channel domain. Channel shift is also a method for temporal modeling, which is just the opposite to patch shift. It replaces a constant proportion of channels in the current frame with other frames along the temporal dimension. As shown in Fig. 6, part of channels are shifted forward by one frame, while another part of channels are shifted backward by one frame, with the rest remaining unchanged. Fig. 7 illustrates the differences between patch shift and channel shift. We describe a tensor with flattened spatial dimension $HW$, temporal dimension T and channel dimension C. The green, blue, red, yellow colors represent four successive frames. The features at different channel stamps in one frame are denoted as different shades"}, {"title": null, "content": "of the same color. We can see from Fig. 7 that the two shift methods perform shifting operations in orthogonal directions. Previous work [54] has revealed that patch shift and channel shift have a certain amount of complementary to each other, and the ability of temporal modeling can be enhanced by alternating them. We follow this idea and add channel shift to our model as a supplement for patch shift."}, {"title": null, "content": "Fig. 8 depicts two consecutive PatchShift 3D SwinTrans- former blocks. Specifically, given the input feature $X^{l-1} \\in \\mathbb{R}^{T\\times H \\times W \\times C}$ from the previous (l\u22121)th block, we first replace part of the channels of the current frame with neighboring frames following [28], which can be formulated as:\n$X_{cs}^l = \\text{ChannelShift} (\\text{LN} (X^{l-1}))$ (1)\nwhere $X_{cs}^l$ represents the output spatial-temporal mixed feature after channel shift, and LN denotes layer normalization. The shift operation enables the integration of spatial information with temporal information in a zero-computation way, which reduces the computational complexity.\nThen we set the size of each 3D window to $P_T \\times P_H \\times P_w$ and arrange the windows in a non-overlapping way to evenly split feature $X_{cs}^l$ for efficient computation cost. Thus, feature $X_{cs}^l$ is reshaped as $X_{ls}^l \\in \\mathbb{R}^{N \\times P \\times C'}$, where $N = \\frac{T}{P_T} \\frac{H}{P_H} \\frac{W}{P_w}$ denotes the number of windows, and $P = P_T P_H P_W$ rep- resents the flattened window size. Afterwards we apply the window-based multi-head self attention (WMSA) module [32]. Limiting attention computation in non-overlapping windows can bring the locality of convolution operations whiling saving computational resources. Finally we get the output $X^l$ of the lth block through a feed forward network (FFN) and a shortcut connection like a standard transformer architecture. The process can be formulated as:\n$\\hat{X}^l = \\text{WMSA} (X_{ls}^l) + X^{l-1}$ (2)\n$X^l = \\text{FFN} (\\text{LN} (\\hat{X}^l)) + \\hat{X}^l$ (3)\nwhere $\\hat{X}^l$ represents the output feature after WMSA. It should be noted that the lth block also outputs key feature $K_{Prev}^l$ and value feature $V_{Prev}^l$ from the WMSA module preparing for feature fusion in decoders.\nAfter obtaining $X^l$ from the lth block, we first shift the patches of each frame along the temporal dimension with the specific pattern as:\n$\\hat{X}^{l+1}_{ps} = \\text{PatchShift} (\\text{LN} (X^l))$ (4)\nwhere $\\hat{X}^{l+1}_{ps}$ denotes the output feature after patch shift. Previous work [54] has revealed that patch shift and channel shift are complementary to each other, so we adopt patch shift in the (l + 1)th block and channel shift in the lth block in order to enhance the ability of temporal modeling.\nThen we exploit the shifted window multi-head self- attention (SWMSA) module for cross-window connections. After the SWMSA, we follow [54] and shift patches from different frames back to their original locations to keep the frame structure complete. Finally the output feature map $X^{l+1}$ of the (l + 1)th block is generated by a FFN and a shortcut connection. The process can be formulated as:\n$X^{l+1} = \\text{SWMSA} (\\hat{X}^{l+1}_{ps})$ (5)\n$\\hat{X}^{l+1} = \\text{ShiftBack} (\\tilde{X}^{l+1}_{ps}) + X^l$ (6)\n$X^{l+1} = \\text{FFN} (\\text{LN} (\\hat{X}^{l+1})) + \\hat{X}^{l+1}$ (7)\nwhere $\\tilde{X}^{l+1}_{ps}$ and $\\hat{X}^{l+1}$ denote the output features after the SWMSA and shift back, respectively. Note that the (l + 1)th block also outputs the key feature $K_{Prev}^{l+1}$ and value feature $V_{Prev}^{l+1}$ preparing for feature fusion in decoders. By using shift operations, we extract the spatial-temporal feature in RF image sequences without high computation cost."}, {"title": "2) Class Masking Attention Module (CMAM)", "content": "As illus- trated in Fig. 9, the CMAM is proposed to capture the spatial- temporal contextual information from the perspective of the entire RF image sequence and generate enhanced feature maps with class-dependent semantic context information. It is designed to follow the PatchShift 3D SwinTransformer module at each stage of our model.\n$Q = \\text{ClassEmbedding} (X)$ (8)\n$K = \\text{ClassEmbedding} (X)$ (9)\nwhere Q and K are both tensors of size $T \\times H \\times W \\times class$. This strategy is inspired by some pioneering work [21] [62]. Our intuition is: the output Q after class embedding layer contains class-dependent RF image semantic information to some extent, which can serve as the prior representation of current stage. Then Q is sent to auxiliary decoder for further ground truth supervision.\nWe also adopt a normal embedding layer to get value feature V. Normal embedding layer is a linear layer which can convert the channel C to the embedding dimension $C'$, playing the same role as that in standard transformer architecture.\n$V = \\text{NormalEmbedding} (X)$ (10)"}, {"title": null, "content": "where V is a tensor of size $T \\times H \\times W \\times C'$.\nNext we perform reshape operations and calculate the similarities between Q and K:\n$\\hat{Q} = \\text{Reshape1} (Q)$ (11)\n$\\hat{K} = \\text{Reshape1} (K)$ (12)\n$S = \\text{Softmax} (\\hat{Q} \\hat{K}^T)$ (13)\nwhere Reshapel is used to transform tensors Q and K to matrices $\\hat{Q}$ and $\\hat{K}$ of size $THW \\times class$, respectively,$\\odot$ stands for matrix multiplication, and S is the similarity score between $\\hat{Q}$ and $\\hat{K}$. Considering that Q serves as the prior representation of current stage and is indirectly supervised by ground truth in auxiliary decoder, the score values in S contain semantic context information, which guide V to update as:\n$\\hat{V} = \\text{Reshape2} (V)$ (14)\n$R = \\text{Reshape3} (S \\odot \\hat{V})$ (15)\nwhere Reshape2 is used to make $\\hat{V}$ a matrix of size $THW \\times C$ and Reshape3 is used to make R a tensor of size $T \\times H \\times W \\times C$.\nTo mitigate the vanishing gradient problem, we add a shortcut connection and multiply R with a learnable scalar constant $\u03b2$ additionally for smooth finetuning. Finally we follow the traditional transformer architecture and get the final feature maps $X_{Class Masking}$ through a FFN and a shortcut connection. The process can be formulated as:\n$X_{res} = \u03b2R + X$ (16)\n$X_{ClassMasking} = \\text{FFN} (\\text{LN} (X_{res})) + X_{res}$ (17)\nwhere $X_{res}$ is the output after attention operation.\nThe output $X_{classMasking}$ contains more spatial-temporal contextual information compared with the input X. As shown in Fig.10, the output feature map after the CMAM module focuses more on object areas. The results of the subsequent ablation experiments reveal that the CMAM enhances the performance of our model, particularly on average precision."}, {"title": "C. Decoder", "content": "In order to integrate the features and prior maps obtained from different encoder stages respectively, we utilize two different decoders."}, {"title": "1) Main Decoder", "content": "We employ the T-SwinTransformer blocks from [22] as the main decoder in our work. Two consecutive T-SwinTransformer blocks are shown in Fig. 11. Inspired by the original transformer model [47] in NLP, we believe that better prediction results can be obtained by integrating the feature from the encoder into the intrinsic feature of the decoder via cross attention operations.\nSpecifically, given the features $K_{Prev}^l$, $V_{Prev}^l$ from the corresponding PatchShift 3D SwinTransformer module and the inherent feature $X^{l-1}$ from the previous (l \u2212 1)th decoder block, we first process them with the WMSA and the WMCA for cost-efficient computation:\n$\\text{SA}_l = \\text{WMSA} (\\text{LN} (X^{l-1})) + X^{l-1}$ (18)\n$\\text{CA}_l = \\text{WMCA} (\\text{LN} (X^{l-1}), K_{Prev}^l, V_{Prev}^l) + X^{l-1}$ (19)\nwhere WMCA represents window based multi-head cross- attention using regular window partitioning configurations, $\\text{SA}_l$ denotes the inherent feature from the main decoder, and $\\text{CA}_l$ represents the feature obtained by interacting with the encoder. Then $\\text{SA}_l$ and $\\text{CA}_l$ are fused as:\n$\\tilde{X}^l = \u03b3\\text{CA}_l + (1 \u2212 \u03b3)\\text{SA}_l$ (20)\nwhere \u03b3 stands for a scaling factor that can be learned to compare the significance of the two outputs. After a FFN and a residual structure, the output feature map $X^l$ of the lth block is obtained as:\n$X^l = \\text{FFN} (\\text{LN} (\\tilde{X}^l)) + \\tilde{X}^l$ (21)\nNext, we repeat the whole procedures above and shift windows during attention operation for cross-window connections:\n$\\text{SA}_{l+1} = \\text{SWMSA} (\\text{LN} (X^l)) + X^l$ (22)\n$\\text{CA}_{l+1} = \\text{SWMCA} (\\text{LN} (X^l), K_{rev}^l, V_{prev}^l) + X^l$ (23)\n$\\tilde{X}^{l+1} = \u03b3\\text{CA}_{l+1} + (1 \u2212 \u03b3) \\text{SA}_{l+1}$ (24)\n$X^{l+1} = \\text{FFN} (\\text{LN} (\\tilde{X}^{l+1})) + \\tilde{X}^{l+1}$ (25)"}, {"title": "2) Auxiliary Decoder", "content": "During the training stage, a lightweight FPN-like semantic decoder is used to provide ground truth supervision to the prior maps generated from the CMAM at each stage of the encoder. Considering that all prior maps from different stage have the same channel dimension of class, we aggregate them only with some upsampling and summation operations. More details are displayed in Fig. 3."}, {"title": "D. Loss Function", "content": "In this paper, we utilize the auxiliary loss function to supervise the training of our proposed method. After pass- ing through the network, confidence maps (ConfMaps) $\\hat{C} \\in$ [$\\mathbb{R}^{T\\times H \\times W \\times class}$ and prior maps $P \\in \\mathbb{R}^{T\\times H \\times W \\times class}$ are pre- dicted from main decoder and auxiliary decoder, respectively. We use binary cross entropy loss to supervise the output of the main decoder, and the main loss function is defined as:\n$l_{main} = \\sum_i \\sum_j \\{GT_{ij}^{class} \\log \\hat{C}_{ij}^{class} + (1-GT_{ij}^{class}) \\log (1 - \\hat{C}_{ij}^{class})\\}$ (26)\nwhere $GT_{ij}^{class}$ indicates the ground truth generated by CRF at coordinate (i, j) for category label class, and $\\hat{C}_{ij}^{class}$ indicates the predictions generated by the main decoder at coordinate (i, j) for category label class.\nThen we add a specific auxiliary loss function to supervise the output of auxiliary decoder. The auxiliary loss function is consistent with the main loss function, which can be defined as:\n$l_{aux} = \\sum_i \\sum_j \\{GT_{ij}^{class} \\log P_{ij}^{class} + (1-GT_{ij}^{class}) \\log (1 - P_{ij}^{class})\\}$ (27)\nwhere $P_{ij}^{class}$ indicates the prior maps generated by the aux- iliary decoder at coordinate (i, j) for category label class.\nFurthermore, we use the parameter a to balance the weight of the main loss and auxiliary loss, i.e.,\n$1=l_{main} + \u03b1l_{aux}$ (28)\nIt should be noted that we only use the auxiliary loss in the training phase."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we present the experimental evaluation of our model. Firstly we describe the dataset and the evaluation metric that we utilize. Then we give details concerning the experiments. Next we compare our model with the SOTA and analyse the results quantitatively and qualitatively. Finally we perform some ablation studies of Mask-RadarNet."}, {"title": "A. Dataset", "content": "We train our Mask-RadarNet with the training data of the CRUW dataset [52]. The CRUW dataset contains 3.5 hours with 30 FPS of camera-radar data in different driving scenarios, which are collected with an RGB camera and 77 GHz FMCW MMW radar antenna arrays. The high frame rate makes the CRUW dataset being appropriate for evaluating the temporal models. Data in this dataset are processed and presented as Range-Azimuth (RA) heatmaps, depicting a bird- eye-view of the scene seen from the ego-vehicle. RA heatmaps can be described as images with a resolution of 128\u00d7128 and the intensity depicts the magnitude of the RF signal. The cross-modal supervision framework in [52] labels the collected objects with camera-radar locators, which makes full use of FMCW radar and offers an appropriate capability for range estimation free from any systematic bias. Generally, there are around 2.6 \u00d7 105 objects in the CRUW dataset, 92 percent of which are utilized for training and 8 percent for testing. Be- sides, this dataset contains some vision-fail scenarios, allowing the model to be tested in extreme environments."}, {"title": "B. Evaluation Metrics", "content": "To evaluate model on the CRUW dataset, a new metric called object location similarity (OLS) [52] is defined. This method depicts the correlation between two point-based de- tections while taking into account their distance, classes, and scale characteristics. Specifically, the OLS can be written as:\n$OLS = exp\\{-\\frac{d^2}{2S^2}\\}$ (29)\nwhere d is the distance between the two points in an RF image; S is the object distance from sensors, indicating object scale information; and $K_{cls}$ is a per-class constant representing the error tolerance for class cls, which can be determined by the object average size of the corresponding class.\nThe evaluation approach employed in our work is consistent with that of [22]. Here is a synopsis of the procedure:\n1) Get all the 8-neighbor peaks in all class channels in ConfMaps within the 3 \u00d7 3 window as a peak set P.\n2) Pick the peak p* \u2208 P with the highest confidence score, and remove it from the set P to the final peak set P*.\n3) Calculate the OLS with each of the rest peaks pi \u2208 P. If the OLS between p* and pi is greater than a threshold, we remove pi from the set P.\n4) Repeat steps 2 and 3 until the set P becomes empty. The whole procedure is similar to previous work for pose estimation [29]. Finally, average precision (AP) and average recall (AR) are calculated through the variation of OLS threshold between 0.5 to 0.9 with steps of 0.05. The AP and AR are the main evaluation metrics for object detection."}, {"title": "C. Implementation Details", "content": "We run all experiments on Python 3.8, PyTorch 1.10.1 and Ubuntu 18.04. All training procedures have been performed on RTX 3080 GPU. With the purpose of comparing the performance of every model, we split the 40 sequences in the"}, {"title": "D. Comparisons with SOTA", "content": "We perform some experimental analysis of several previous algorithms on the CRUW dataset", "22": ".", "Results": "To facilitate a qualitative compari- son, the numerical results on the CRUW dataset are presented in Table I, revealing that the Mask-RadarNet outperforms other models in general. The AP and AR of Mask-RadarNet in all categories are 1.02% and 0.38%, respectively, higher than the T-RODNet (SOTA). Notably, Mask-RadarNet significantly improves detection performance on small objects such as pedestrians and cyclists. In addition, its GFLOPs and param- eters are less than those of the T-RODNet. It means that the Mask-RadarNet achieves stronger detection results with lower computational complexity. Compared to previous convolution- based temporal modeling methods utilized in radar object detection, such as temporal deformable convolution network in [52"}, {"22": "the shift operation we employ can significantly"}]}