{"title": "Enhancing literature review with LLM and NLP methods. Algorithmic trading case.", "authors": ["Stanis\u0142aw \u0141aniewski", "Robert \u015alepaczuk"], "abstract": "This study utilizes machine learning algorithms to analyze and organize knowledge in the field of algorithmic trading. By filtering a dataset of 136 million research papers, we identified 14,342 relevant articles published between 1956 and Q1 2020. We compare traditional practices such as keyword-based algorithms and embedding techniques with state-of-the-art topic modeling methods that employ dimensionality reduction and clustering. This comparison allows us to assess the popularity and evolution of different approaches and themes within algorithmic trading. We demonstrate the usefulness of Natural Language Processing (NLP) in the automatic extraction of knowledge, highlighting the new possibilities created by the latest iterations of Large Language Models (LLMs) like ChatGPT. The rationale for focusing on this topic stems from our analysis, which reveals that research articles on algorithmic trading are increasing at a faster rate than the overall number of publications. While stocks and main indices comprise more than half of all assets considered, certain asset classes, such as cryptocurrencies, exhibit a much stronger growth trend. Machine learning models have become the most popular methods in recent years. The study demonstrates the efficacy of LLMs in refining datasets and addressing intricate questions about the analyzed articles, such as comparing the efficiency of different models. Our research shows that by decomposing tasks into smaller components and incorporating reasoning steps, we can effectively tackle complex questions supported by case analyses. This approach contributes to a deeper understanding of algorithmic trading methodologies and underscores the potential of advanced NLP techniques in literature reviews.", "sections": [{"title": "1 Introduction", "content": "The motivation for this work is to explore the extent to which automatic methods can be utilized for reviewing scientific journals, starting from the largest possible dataset, refining it with rules and machine learning to identify topics of interest, and addressing complex questions.\nWith the exponentially growing number of scientific journals and papers, it is difficult to keep track of how current methods evolve and change in popularity Fire and Guestrin (2019). To organize knowledge in the field of algorithmic trading we did a thorough analysis enhanced by machine learning algorithms based on 136 million research papers from the S2ORC database, which consists of repositories such as SSRN and arXiv, Microsoft Academic Graph or international journals Lo et al. (2020).\nArchives and public repositories enable the sharing of research studies at various stages of advancement, including initial preprints. As science becomes increasingly complex, interdisciplinary research involving multiple experts is on the rise. This has resulted in hundreds or thousands of papers being published each year in a specific field, making it challenging to keep track of the latest developments.\nFortunately, advances in technology give us tools that we leverage in this study. We demonstrate how exploratory and machine-learning-enhanced analysis can be performed on a set of papers obtained by filtering one of the largest databases of research publications. LLM-assisted literature reviews offer a more dynamic and scalable solution, capturing a broader range of knowledge that manual reviews may overlook.\nOur work presents a methodology that can be replicated in other fields, but we focus specifically on the case of algorithmic investment strategies. Based on automatic analysis methods such as keyword extraction and topic modeling we study a huge amount of research papers, focusing on the dynamics of selected features and how scientists apply different models. These techniques enable us to uncover insights and patterns that might have otherwise gone unnoticed, and ultimately improve our understanding of the field of algorithmic trading by providing a comprehensive list of the most successful models in these works.\nRQ1 - Is algorithmic trading becoming a more popular topic in scientific research? How do scientific themes evolve in articles about algorithmic trading?\nIn addition, we identify the most popular methods and assets in this field and examine their dynamics over time. In particular, we analyze how the popularity of asset classes as a scientific topic evolves, showing that significant events are visible in aggregated statistics (e.g. rise of the popularity of cryptocurrencies). We expect that as computational power continues to increase and access to data becomes easier,"}, {"title": "2 Literature review", "content": "Examples of literature reviews synthesizing available studies and technologies applied in algorithmic trading, performed using traditional manual approaches, can be seen in Ferreira et al. (2021) and Joiner et al. (2022). In the former article, researchers start by filtering a database to create a smaller sample and then present important quantitative statistics about the works and models, providing deeper insights into the scientific methodology by manually evaluating the papers. Meanwhile, in Joiner et al. (2022), the authors follow a traditional narrative approach, explaining the data and models used in each work.\nBoth studies explore the evolution of algorithmic models, their applications, and key details, which we replicate in this study. However, this conventional approach is limited by the researcher's capacity to manually sift through an ever-expanding body of literature a process that becomes especially challenging as fields like machine learning continue to evolve rapidly.\nRecent developments in the use of LLMs for financial text mining, as discussed by Suzuki et al. (2023), where authors apply domain-specific language models in the finance domain. The main focus is comparison of performance of selected methods namely BERT and ELECTRA - and various pre-training methods. By leveraging domain-specific language models, researchers can enhance the accuracy of NLP-based tools to extract key insights from financial reports. However, they \"demonstrate no significant difference between pre-training with the financial corpus and continuous pre-training from the general language model with the financial corpus\". Supporting results can be found in Hong et al. (2023) where ScholarBERT, a general-scientific BERT, outperforms even domain-specific embeddings, which reinforces our approach using general LLM.\nWe draw upon insights from Ofori-Boateng et al. (2024), who highlight the significant challenges and opportunities in automating systematic literature reviews (SLR)"}, {"title": "3 Methodology", "content": "The methodology used in the paper can be summarised as follows.\nTo refine our dataset, we employed a comprehensive search strategy and various filtering techniques based on keywords, topics, expert knowledge, and journals. Then we evaluated multiple embedding methods, including word2vec and universal-sentence-encoder, before selecting sentence BERT as the optimal approach for our problem. To perform topic modeling, we utilized state-of-the-art algorithms, such as BERTopic Grootendorst (2022), which involved dimension reduction using Uniform Manifold Approximation and Projection (UMAP) McInnes et al. (2018) and clustering with Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) McInnes et al. (2017).\nWe curated the outcomes of topic modeling algorithms to find the major themes and analyze which areas of research are growing most rapidly.\nWe calculated embeddings for our research questions and identified topics with the closest cosine distance. We subsequently validated our results by scrutinizing the papers using both expert knowledge and a GPT-based model (ChatGPT). Finally, we demonstrate an automated procedure involving LLMs, which reproduces the researchers' process during literature reviews when searching for properties of the papers (e.g. the results of model comparisons).\nTo illustrate our findings, we present an analysis that includes a statistical overview and supporting visualizations that highlight distinctions in papers, including the algorithms employed, the markets, and the main subjects of study. We also examine these differences across various dimensions such as time and popularity."}, {"title": "3.1 Data selection", "content": "To ensure we review a broad range of relevant research, we began by identifying the most suitable database. After careful consideration, we selected the S2ORC database Lo et al. (2020), which is a huge corpus of over 136M scientific papers enriched with citation data derived from Semantic Scholar, a research tool developed at the Allen Institute for AI. They span over 70 years with the last entries from April 2020.\nWe designed a schema to extract a relevant database (corpus) of documents. Our approach could be replicated for any research topic, but we focused on algorithmic trading, which required a smart filtering process. We considered that essential research, models, or findings could be outside the scope of regular economic journals or be interdisciplinary."}, {"title": "4 Exploratory analysis", "content": "To address our research question about the popularity of algorithmic trading strategies and methods over time, we conducted analyses of the dataset, including publication dates, citation data, and keyword-based and topic-modeling methods.\nFurthermore, we preprocessed the collected documents by removing English stop-words, lemmatizing the words, and tokenizing the texts. We also calculated descriptive statistics to provide a detailed overview of the documents we collected.\nTo understand the corpus more thoroughly, we generated word clouds and found N-grams and noun chunks. The generated world cloud confirms we have captured relevant articles from the targeted domain. Analysis of N-grams revealed the most popular concepts such as efficient market hypotheses, limit order book signals, time series momentum, and models such as the Fama French factor model and capital asset pricing model (CAPM). Additionally, we used Named Entity Recognition algorithms to identify the most commonly studied markets and countries in the scientific literature related to our selected topics."}, {"title": "4.1 Time horizon and top asset classes", "content": "We want to highlight three findings: first, the increasing popularity of algorithmic investment strategies by showing how it is becoming a greater part of the total database in Figure 1.\nSecond, the majority of publications deal with daily and monthly data. In Figure 2 we plot the frequency of each time horizon, which we defined by analyzing keywords such as \"daily\", or \"5-minute\", or \"monthly\" in the context of data or training periods. The evaluation based on sampling from results and checking manually gave an 80% positive rate."}, {"title": "4.2 Top methods used for modelling", "content": "To answer the question of the increasing popularity of machine learning-based methods in recent years, we aggregated them and compared them to linear models and time series. Although linear models account for more than half of all methods considered in the database, we examine the trends of different model families over time. We do this by using regular expressions to search for specific model names and grouping them into three categories: linear, time series, and machine learning (Figure 4). We then plot the number of papers per year that mention each category of models (full regex in Appendix A.1).\nOur analysis shows that machine learning methods are rapidly gaining popularity in algorithmic trading research, especially since 2015. In 2019 machine learning methods surpassed linear models in popularity for the first time in history. This supports trends found by previous researchers, e.g. Ferreira et al. (2021). The neural network is the most researched system from a machine learning environment. On the other hand, the use of time series modeling appears to be losing traction, with a decreasing ratio of papers incorporating them in the algorithmic trading scope."}, {"title": "5 Topics", "content": "To delve deeper into the underlying topics of the research papers, we recognise the need to augment our analysis with more advanced techniques. The statistical-based methods employed thus far have provided valuable insights, but to gain a more nuanced understanding, we require the ability to comprehend language, identify similarities between"}, {"title": "5.1 Procedure", "content": "words and sentences, and extract meaningful summaries from the texts. By doing so, we can uncover the topics that are considered crucial by the scientific community.\nTo do so we apply three various embeddings word2vec, BERT, and Universal Sentence Encoder - to better understand the language, find similarities between words and sentences, and summarise the texts. While word2vec has been successfully used in previous research, we found that its lack of sentence interpretation could be a potential flaw in our analysis. After testing various sentence transformer-based methods, we ultimately chose the 384-dimensional all-MiniLM-L6-v2 model for its superior performance while maintaining a small size, based on Wang et al. (2020).\nTo reduce the dimensionality of our embedded documents, we use UMAP (McInnes et al. (2018)). It is a non-linear dimension reduction algorithm that combines aspects of principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE). By using UMAP, we aimed to preserve the essential global structures of the documents, making it easier to identify similar topics.\nAfter applying UMAP, we further grouped the documents using the HDBSCAN algorithm McInnes et al. (2017), as it was capable of detecting clusters of different densities and able to hand outliers. Therefore documents that had similar embeddings, such as those of the same topic or containing significant word overlap, end up grouped together based on their lower-dimensional representations.\nFollowing the implementation of BERTopic Grootendorst (2022) we used the class version of term frequency-inverse document frequency (TF-IDF). The documents which fall into the same cluster create a topic.\nFinally, we curated outcomes of topic modeling algorithms to identify common themes and determine which areas of research are growing most rapidly. We used a technique of merging smaller topics with the closest thematically larger ones based"}, {"title": "5.2 Analysis", "content": "We identified three distinct clusters: one major group and two smaller (Figure 5). The middle cluster is centered on strategic investments, such as those in transportation, the military, and electricity, while the top-left cluster focuses on education, agriculture, foreign investment, and development. The main group in the bottom-right is about investment strategies; we notice some sub-clusters about the pension system (upper part of the group), the main part consisting of various strategies, and in the bottom right optimal investments for longer periods (from portfolio manager and insurer perspective).\nWe can also notice that topic 16 (Neural Network Trading) is interestingly first matched with topic 4 (High-Frequency Trading, HFT) and 13 (Volatility) rather than general group 0, which matches with 19 (Figure 6). This suggests that topic 16 has stronger connections with the specialized areas covered by topics 4 and 13.\nTopics that are close to each other based on our model will be grouped first. Comparing to 5, we notice that indeed most often topics that were clustered together are grouped first, e.g. 7: Foreign Direct Investment and 11: Social Welfare Policies, or two pairs 2 & 5 and 6 & 8. (2: Renewable Energy Planning, 5: Real Options Analysis, 6: Innovation Technologies Investment, 8: Transportation Planning Strategies).\nIn Figure 7 we notice that trends change over time, for example, topic 4 (HFT) experienced bursts of popularity in response to events such as the first flash crash. On the other hand, topic 16 (Neural Network Trading) has become one of the fastest-growing areas of research in this field in recent years."}, {"title": "5.3 Neural Network Trading", "content": "To analyze the topic that compares different models used in algorithmic trading, we started by identifying relevant keywords and queries, such as \"model outperforms\", \"hyperparameter optimization\", \"learning rate\", \"comparing models\", and specific method-related terms like \"recurrent neural network\", \"long short-term memory/LSTM\", and \"reinforcement learning\". For each query we created an embedding, compared them with the topic embeddings, and identified the most similar topics by calculating the distance based on cosine similarity values (Table 2, the higher Simil. the better).\nFor each of our queries, topic 16 (Neural Network Trading) gets the highest cosine similarity. The 10 top words of this topic include trading, prediction/forecasting, neural network, stock, and machine learning.\nIn the limited domain of 176 research papers, we conducted a detailed analysis to answer our research questions: what assets and venues are most frequently used, how are they tested (models), and which techniques perform better (Table 3 and Tables"}, {"title": "6 Analysis with LLM", "content": "To answer RQ3 & RQ4 and to evaluate the efficiency of LLM, we test two Chat-GPT models, specifically ChatGPT 3.5 (23.03.23) and ChatGPT-40 (01.06.24), on the selected subset of papers, namely the ones labeled in topic modeling as topic 16 - Neural Network Trading."}, {"title": "6.1 Comparing GPT versions on abstracts", "content": "We employed both ChatGPT versions 3.5 and 40 to answer RQ3 regarding comparing models and HPO. We designed a prompt that asks if each abstract contains two"}, {"title": "6.2 Full text analysis", "content": "From the 176 articles on the topic of Neural Network Trading, we accessed 153 full papers and removed 7 biggest files (books), ending up with 146 full texts for analysis.\nFurthermore, since we analyze full papers now, we asked more elaborate questions. There are 3 new questions added to model comparison and HPO, namely frequency of data used, loss function used, and what was chosen as the best model. We expect two answers for each question - one with yes/no, the other with the explanation provided for each question.\nIf there is a comparison of different models or methods used.\nIf there is hyperparameter optimization.\nThe frequency of data used.\nThe loss function used.\nThe best model (chosen in comparison)."}, {"title": "6.2.1 Comparing to Abstracts", "content": "Despite having 30 fewer full texts than abstracts, the LLM was able to find snippets where researchers compared models or performed HPO, leading to a significant"}, {"title": "6.2.2 Time intervals", "content": "The full-text analysis provides us with more accurate information about the frequency of data. We defined the bins by taking a list of unique answers (A.6) and grouping them manually."}, {"title": "6.2.3 Loss functions", "content": "The majority of the loss functions fall into the \"Other/Unspecified\" category, indicating a variety of less commonly named or unique loss functions. The most commonly specified loss function group is MSE-related, followed by Cross-entropy-related.\nTo present the results, we grouped the loss functions based on expert knowledge A.7."}, {"title": "6.2.4 Best models", "content": "Here is the summary based on NLP and lazy ChatGPT (that is, the one that uses Python to analyze data instead of reading manually, as it's longer than its context).\n11 is the detailed categorization of models that reflects the thorough analysis performed by prompting LLM with each answer and its corresponding elaboration. This method of batching ensures that the LLM meticulously 'reads' the elaboration, leveraging its capabilities to provide accurate and insightful classifications.\nNot only does this result in a more precise classification of models (with the \"Other/Unspecified\" category dropping from 70 to 20), but it also captures more categories and nuances, such as creating a distinct topic for deep learning models. This enhanced granularity in classification demonstrates the LLM's capability to discern subtle differences and provide a comprehensive overview of the diverse range of models used in the studies.\nThe full list can be found in A.8."}, {"title": "6.3 Issues", "content": "6.3.1 LLM Laziness\nFirst of all, the answering scheme is different than one would imagine AI to use. By default, ChatGPT does not read and understand the papers. Instead, it uses regex and NLP methods to answer each question. When prompted, it even provided us with the Python code it used for analysis A.4. As expected, it is suspect to simple false positives (the word 'compare' is used in different contexts) or to omitting keywords (not provided in the short list of options).\nFor example, 12 is the initial analysis provided for each question, as it simply treated it all as one batch and ran a regex analysis on it. It performed surprisingly well - the words selected in regex produced quite accurate results for the number of papers with model comparison (138 compared to 135 based on LLM full-text analysis, 7) and for HPO (96 compared to 89). In data frequency, question it had problems capturing intraday horizons, thus predicting 107 papers to state frequency of dataset used, compared to 134 in LLM full paper analysis 8. Again it did well in the loss function, stating that 65 are unspecified, compared to 69 in 9, while the choice of the best model proved to be too difficult question for regex - 127 papers identified as not stating best method compared to 20 based on full-text analysis by LLM 11.\nTo use LLM capabilities we specifically mentioned we want it to use the context. It would be wiser to find specific keywords for Regex search based on the abstract. Then read the context and decide what is the final answer with elaboration. To confirm it followed our guidance, we asked it to summarise logic afterward A.5.\nThe results prove the efficiency of keyword-based searches, such as regex-based, with domain expert knowledge used to select them, as an efficient way to do filtering"}, {"title": "6.3.2 Errors and consistency", "content": "The most common error encountered was due to excessively large files, such as loading a book with over 100 pages. In some instances, papers were too mathematical for GPT to parse and understand accurately. Parsing such files did not necessarily produce an error; instead, the model either attempted to list chapters as different papers or began to hallucinate based on its partial understanding, as found in research (e.g. Li et al. (2023)). Following their findings, we provide external domain knowledge and break the task by adding reasoning steps.\nExcluding books (7 instances, which were easy to filter out), there were 4 errors out of 150 files. To maintain accuracy, it was crucial to send files in small batches-preferably one by one as larger batches led to confusion and loss of context in GPT. Following best practices from social experiments, we included questions to check GPT's attention (such as asking it to summarise the task at hand), which it passed.\nThere are idiosyncratic risks associated with relying on a single LLM. For example, GPT-3.5 Turbo 23.03 frequently identified the need for hyperparameter optimization (HPO) while rarely recognizing model comparison. Consequently, when abstracts did not explicitly mention HPO but the models required proper tuning, ChatGPT might incorrectly affirm that HPO was performed. Conversely, GPT-4.0 demonstrated more strict criteria for HPO (33 fewer papers identified) but recognized more instances of model comparison (46 more papers). In the full-text analysis, GPT-3.5 performed better on HPO-related questions, while GPT-4.0 excelled in identifying model comparisons. This domain-specific comparison of performance over time, which shows irregularities, is in line with findings in Tu et al. (2024).\nAnother issue observed was the lack of consistency in the LLM's responses. For example, it sometimes did not consider arbitrarily selected benchmarks (e.g., buy and hold) as model comparisons, treating them merely as sanity checks, whereas in other abstracts it did. Occasionally, it treated parameter tuning as HPO. However, analyzing longer answers revealed that the LLM's certainty varied. To address this, we incorporated the elaborations provided by the LLM in our full-text analysis, which enhanced the reliability of the responses."}, {"title": "7 Conclusions", "content": "In summary, our work presents a replicable methodology that combines state-of-the-art NLP techniques and LLMs to perform large-scale, automated literature reviews. By focusing on algorithmic trading as a case study, we demonstrated the effectiveness of this approach in uncovering trends, patterns, and insights that contribute to a deeper understanding of the field. The innovative use of advanced methodologies not only"}, {"title": "7.1 Case. Algorithmic trading literature review", "content": "Recent advancements in computer science and natural language processing have enabled researchers to access vast databases of scientific papers and narrow them down to their areas of interest. In our study of algorithmic investing strategies, we used a keyword-based approach to filter a large dataset of research papers. Our analysis revealed that algorithmic trading has become increasingly popular over time, particularly between 1990 and 2010. In recent years, shorter time horizons have gained popularity, driven by cheaper computational power and easier access to relevant data. While stocks and indices are the most commonly studied assets, other asset classes have experienced spikes in popularity during certain periods, such as the oil crisis of 2014-2016 or the rise of cryptocurrencies after 2018.\nMachine learning-based techniques have become the most widely tested statistical models in the field. Our topic modeling analysis revealed major trends in contemporary research and identified the topic of comparing various algorithms and models, particularly those based on machine learning. While keyword-based approaches are useful for finding popular methods and their intersections, they have limitations in answering questions about which models outperform others in general.\nFull-text analysis has confirmed that HPO, while not often being the main focus of the study, is performed in the vast majority of the papers. Recent studies (Probst et al. (2019); Li et al. (2020)) have shown that many fine-tuned algorithms are sensitive to changes in hyperparameters, so it is important to be cautious about the robustness of some methods.\nDeep learning models prove to be the most promising models for algorithmic trading, closely followed by traditional statistical models. However, there are many successful neural networks, especially recurrent ones, and there is plenty of research applying reinforcement learning or ensemble methods."}, {"title": "7.2 LLM for literature review", "content": "We demonstrated how an automated approach, enhanced by NLP and LLMs, can effectively assist in literature review. By filtering one of the largest databases of research publications we were able to analyze vast amounts of data that would be infeasible to process manually.\nBoth regex-based filtering and LLM proved to be successful and useful in refining a huge corpus of research papers. Abstracts, while giving some insight into the study, often omit parts that are found later in the paper (e.g., details about models, HPO, or data). Furthermore, the results varied from version to version, showing inconsistencies reported in previous studies.\nWe showed how LLMs can enhance the quantitative literature review process by understanding complex concepts and providing nuanced analyses that account for factors like model comparisons and dataset variations. However, we noticed that ChatGPT without reasoning steps tends to oversimplify the problem. By breaking the task"}, {"title": "8 Declarations", "content": "Large Language Models, namely ChatGPT 3.5 and 40, were used in this research for evaluation in 7, as well as for text, code, and table polishing.\nNo funding was received to assist with the preparation of this manuscript. The authors have no competing interests to declare that are relevant to the content of this article."}, {"title": "Appendix A Appendix", "content": null}, {"title": "A.1 Classification and Regex for models", "content": "Here is the list of regular expressions for each topic:\nLinear models: ordinary least square OLS | linear model. lasso ridge\nMachine Learning: random forest decision tree. regression tree. xgboost boosting extreme gradient| LSTM| Long.short.term.| support vector regressions| SVR| support vector machine| SVM| k.nearest neighbour.| knn| clustering algo. mapping algo. neural network (imitation| reinforcement | unsupervised) learning\nTime series: GLM Generalized linear model (Poisson(.?point)? | Gaussian| Normal) (proces.| regress.)| (s)?ar(i)?ma(x)?| garch"}, {"title": "A.2 Asset classes and venues in topic Neural Network Trading", "content": "While we were able to find the traded asset class, the identification of the venue based on keyword search failed to deliver meaningful results based on abstracts. By human validation, we confirmed that in over two-thirds of this reduced dataset, there is no mention of the particular assets."}, {"title": "A.3 Topics per year", "content": null}, {"title": "A.4 Lazy GPT regex", "content": "import re\nimport pdfplumber\ndef extract_text_from_pdf(pdf_path):\nwith pdfplumber.open(pdf_path) as pdf:\ntext = \"\"\nfor page in pdf.pages:\ntext += page.extract_text()\nreturn text\ndef analyze_text(text):\nresults = {\n\"comparison\":{\"yes_no\":\"No\", \"elaboration\": \"\"},\n\"hyperparameter_optimization\": {\"yes_no\":\"No\", \"elaboration\": \"\"},\n\"data_frequency\": {\"yes_no\":\"No\", \"elaboration\": \"\"},\n\"loss_function\": {\"yes_no\":\"No\", \"elaboration\":\"\"},\n\"best_model\":{\"yes_no\":\"No\",\"elaboration\":\"\"}\n}\n# Comparison of Different Models or Methods\nif re.search(r'comparison|compare|benchmark|evaluate | versus comparison-study|side-by-side comparative-analysis', text, re.IGNORECASE) :\nresults[\"comparison\"][\"yes_no\"] = \"Yes\"\nresults[\"comparison\"][\"elaboration\"] =\n extract_comparison_details(text)\n# Hyperparameter Optimization\nif re.search(r'hyperparameter|tuning|optimization|grid-search random-search|bayesian-optimization | hyperparameter-tuning|parameter search | hyper-optimization', text, re.IGNORECASE) :\nresults[\"hyperparameter_optimization\"][\"yes_no\"] = \"Yes\"\nresults[\"hyperparameter_optimization\"][\"elaboration\"] =\n extract_hyperparameter_details(text)\n# Frequency of Data Used\nfrequency_match = re.search(r'daily weekly monthly minute -level|hourly|annually|yearly|bi-weekly|quarterly', text, re.IGNORECASE)"}, {"title": "A.5 LLM Logic", "content": "Reviewing the Abstract: I examined the abstract of the paper to get an overview of its focus, methods, and key findings.\nIdentifying Comparisons: I looked for any mention of comparisons between different models or methods within the abstract and any additional text available from the paper. This included looking for keywords such as \"compare,\" \"comparison,\" \"evaluate,\u201d \"versus,\" and \"against.\"\nHyperparameter Optimization: I searched for information on any tuning or optimization of hyperparameters. This typically involves looking for terms like \"optimize,\u201d \"hyperparameter,\" \"parameter tuning,\" and \"settings.\"\nFrequency of Data: I checked for any mention of the data frequency used in the study. This could include daily, weekly, monthly, or any other specific time intervals mentioned in relation to the data.\nLoss Function: I looked for any explicit mention of a loss function used in the paper. If a specific loss function was not mentioned, I inferred the optimization criteria from the context, such as the focus on maximizing returns or minimizing errors."}, {"title": "A.6 Unique time horizons found in Neural Network Trading topic", "content": "\u2022 Intraday\n'Minute', 'Milliseconds', 'Intra-day', 'High-frequency (minute-level)', 'Hourly', '30-minute', 'High-frequency', 'Intraday', '5-minute intervals', '15-minute', 'High Frequency', '30-minute intervals', '5, 10, and 15-minute intervals', '10-minute intervals', 'Tick-level (every microsecond)', '1-minute intervals', 'Tick-level (microseconds)', 'High-frequency financial data sampled at an interval of one minute', 'High-frequency financial data sampled at one-minute intervals', 'Minute-level', 'High-frequency (5-minute intervals)'\n\u2022 Daily\n'Daily', 'Daily and weekly', 'Daily and minute-level', 'Daily and 15-minute intervals', 'Daily and Monthly', 'Daily, Monthly, Yearly'\n\u2022 Longer\n'Yearly', 'Quarterly', 'Monthly', 'Weekly', 'Various'"}, {"title": "A.7 Grouping of loss function found in Neural Network Trading topic", "content": "1. Mean Squared Error (MSE) Related:\n\u2022 Mean Squared Error (MSE)\n\u2022 Mean Square Error (MSE)\n\u2022 Mean Squared Error with penalizing coefficient\n\u2022 Sum of Square Errors\n\u2022 Mean Squared Forecast Error (MSFE)\n\u2022 Mean Squared Error (MSE) and Cross-Entropy Loss\n2. Root Mean Squared Error (RMSE) Related:\n\u2022 Root Mean Square Error (RMSE)\n\u2022 RMSE\n\u2022 RMSE and MAPE\n\u2022 RMSE, MAE, MAPE, Theil's U (U1, U2)\n3. Cross-Entropy Related:\n\u2022 Cross-entropy loss"}, {"title": "A.8 The best models with explanation and grouped", "content": "1. Deep Learning Models\n\u2022 Integrated CNN and Deep Learning: Integrated CNN with higher prediction accuracy and cumulative yield."}, {"title": "A.9 Abbreviations", "content": null}]}