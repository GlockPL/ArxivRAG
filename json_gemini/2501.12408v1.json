{"title": "Control-ITRA: Controlling the Behavior of a Driving Model", "authors": ["Vasileios Lioutas", "Adam \u015acibior", "Matthew Niedoba", "Berend Zwartsenberg", "Frank Wood"], "abstract": "Simulating realistic driving behavior is crucial for developing and testing au-\ntonous systems in complex traffic environments. Equally important is the ability\nto control the behavior of simulated agents to tailor scenarios to specific research\nneeds and safety considerations. This paper extends the general-purpose multi-\nagent driving behavior model ITRA (\u015acibior et al., 2021), by introducing a method\ncalled Control-ITRA to influence agent behavior through waypoint assignment\nand target speed modulation. By conditioning agents on these two aspects, we\nprovide a mechanism for them to adhere to specific trajectories and indirectly\nadjust their aggressiveness. We compare different approaches for integrating these\nconditions during training and demonstrate that our method can generate control-\nlable, infraction-free trajectories while preserving realism in both seen and unseen\nlocations.", "sections": [{"title": "1 Introduction", "content": "The simulation of realistic driving behavior is a cornerstone in the development and validation of\nautonomous driving systems. As autonomous vehicles (AVs) increasingly integrate into real-world\ntraffic, the necessity for robust, reliable, and diverse simulation environments becomes paramount.\nThese environments enable the testing of AVs in complex, high-stakes scenarios that would be\ndifficult or dangerous to replicate in real-world conditions. Moreover, the ability to simulate realistic\nmulti-agent interactions is critical for ensuring that AVs can navigate and respond appropriately to\nthe unpredictable behavior of human drivers and other road users.\nOne of the key challenges in multi-agent driving simulations is the balance between realism and\ncontrol. State-of-the-art models (\u015acibior et al., 2021; Suo et al., 2021; Nayakanti et al., 2023;\nGulino et al., 2023; Seff et al., 2023; Wu et al., 2024) aim to replicate the nuances of human driving\nbehavior but often lack the flexibility to adapt to specific research needs or safety protocols. The\nability to control the behavior of simulated agents is essential for tailoring scenarios to investigate\nparticular driving conditions, test edge cases, or enforce safety standards. However, introducing\ncontrol mechanisms without sacrificing realism remains a significant challenge in the field.\nConceptually, human driving behavior encompasses numerous unobserved variables, ranging from\nhigh-level goals such as \"going to the grocery store across the roundabout,\" to intermediate behavioral\ntraits like aggressiveness, down to low-level controls such as setting acceleration and steering values.\nAn ideal driving simulator would allow conditioning on any of these variables, enabling targeted\nscenario design. However, achieving such comprehensive control is challenging due to the difficulty\nof precisely defining different behaviors or measuring the degree to which conditions are met.\nIn this work, we introduce Control-ITRA, a model that enables the control of agent behavior through\ntwo primary methods: by specifying waypoints for the agent to follow and by setting a target speed\nfor it to reach. Waypoints provide a natural mechanism for guiding agents along a desired path, while\ntarget speeds offer a way to influence the agent's aggressiveness indirectly. Specifically, we build upon"}, {"title": "2 Related Work", "content": "Trajectory Prediction: Numerous advanced autonomous vehicle simulators have been proposed\nin recent years (Dosovitskiy et al., 2017; Santara et al., 2021; Zhao et al., 2024), reflecting the\ncommunity's growing recognition of simulation as an essential element for achieving Level 5\nautonomous driving (On-Road Automated Driving (ORAD) Committee, 2021). In this paper, we\nfocus on trajectory prediction models that can simulate realistic traffic behavior. The primary task of\ntrajectory prediction models is to predict future trajectories based on observed environmental behavior.\nBroadly, trajectory models can be classified into physics-based and learning-based models. Physics-\nbased methods leverage physical models to generate trajectories with relatively low computational\nresources, often using kinematic and dynamic models (Lin & Ulsoy, 1995; Lytrivis et al., 2008;\nBr\u00e4nnstr\u00f6m et al., 2010) combined with inference techniques like Kalman Filters (KF) (Ammoun\n& Nashashibi, 2009; Jin et al., 2015; Lefkopoulos et al., 2021) and Monte Carlo methods (Althoff\n& Mergel, 2011; Okamoto et al., 2017; Wang et al., 2019). These traditional methods are generally\nsuitable only for simple prediction tasks and environments.\nRecently, deep learning-based methods have gained popularity due to their ability to model com-\nplex physical, road-related, and agent-interactive factors, making them adaptable to more realistic\nenvironments. Predicting future states is inherently probabilistic, and methods like those in Cui et al.\n(2019); Chai et al. (2020) forecast multiple possible trajectories for each agent. Djuric et al. (2020)\nemploys rasterized ego-centric and ego-rotated birdview representations to depict an agent's current\nand past states, using a CNN to predict future trajectories. Similarly, ITRA (\u015acibior et al., 2021)\nuses ego-centric birdview representations to perceive the environment, modeling each agent as a\nvariational recurrent network (Chung et al., 2015). Tang & Salakhutdinov (2019) applies a discrete\nlatent model with a fixed number of future trajectories per agent, utilizing a different representation\nwith separate modules for map encoding and individual RNN networks for encoding agent states.\nCasas et al. (2020) leverages spatially-aware graph neural networks to model agent interactions in the\nlatent space. Transformer-based approaches (Liu et al., 2021; Huang et al., 2022; Seff et al., 2023;\nNiedoba et al., 2023; Wu et al., 2024) have also been widely adopted to encode interactions between\nagent states.\nGoal-conditioned Models: In the literature, conditioning on waypoints is typically framed as\na goal-conditioning task, often addressed through inverse planning. Here, trajectory prediction is\ndivided into first predicting candidate waypoints and then generating trajectories based on these\nwaypoints. PRECOG (Rhinehart et al., 2019) introduces a probabilistic forecasting model conditioned\non agent positions. PECNet (Mangalam et al., 2020) generates endpoints for pedestrian trajectory\nprediction in a two-step process, where the proposed endpoints guide pedestrian trajectory sequences.\nGraph-TERN (Bae & Jeon, 2023) divides pedestrian future paths into three sections, inferring a\ngoal point for each section using mixture density networks. MUSE-VAE (Lee et al., 2022) uses\na conditional VAE model to generate short-term and long-term goal heatmaps, from which the\nagent trajectory is then conditioned. DenseTNT (Gu et al., 2021) predicts a dense goal probability\ndistribution over the road ahead and uses a goal set prediction model to determine the final trajectory\ngoals. Y-net (Mangalam et al., 2021) generates goal position heatmaps using a convolution-based\napproach, sampling final endpoints from the resulting goal distribution. In Goal-LBP (Yao et al.,\n2024), goal endpoints are generated based on both static context maps and dynamic local behavior\ninformation. S-CVAE (Zhang et al., 2024) reformulates point prediction as a region-generation task,"}, {"title": "3 Method", "content": "In this section, we first introduce the driving behavior model that serves as our foundation model\nfor controlling its behavior. We then explain how to introduce a conditional variable and suggest\nControl-ITRA a training scheme for learning such conditional driving behavior models. Finally, we\npropose two types of conditioning for controlling driving agents."}, {"title": "3.1 Background: ITRA", "content": "The main contribution of this paper is to enable the control of a driving behavior model by conditioning\nits output. Doing so will allow the extraction of interesting interactive behaviors that can be used\nfor testing and further improving driving models. Numerous generative models have been proposed"}, {"title": "3.2 Training with Conditions", "content": "We aim to obtain a driving behavior model that can drive vehicles realistically while optionally\nfollowing agent-specific conditions. In this section, we introduce the principal way of training such\nconditional models. Specifically, we extend the main training procedure of ITRA (\u015acibior et al.,"}, {"title": "3.3 Waypoint Conditioning", "content": "An intuitive way of controlling the behavior of the simulated agents is to set waypoints for them to\nfollow. Specifically, we formally define waypoints $w_{1:K_i}^i$, for each agent i as an ordered collection of\n$K_i$ tuples of target coordinates where $w_k^i = (x_k^i, y_k^i)$. Additionally, a waypoint is considered reached\nfrom an agent i at a timestep t when\n$\\sqrt{(x_t^i - x_k^i)^2 + (y_t^i - y_k^i)^2} \\leq R,$\nwhere R is a hyperparameter and corresponds to the radius from the center of the waypoint. In our\ndefinition of the waypoint following task, the agent must reach each waypoint sequentially in the\nspecified order. Once a waypoint is deemed reached, the next waypoint in the sequence is shown.\nEach agent is presented with only one waypoint at any time from the waypoints list $w_{1:K_i}^i$.\nThe agents are not constrained to reach waypoints as quickly as possible or within a specific timeframe.\nInstead, they are free to take any actions necessary to reach the target point safely and realistically.\nWaypoints that cannot be reached safely should be ignored. Finally, waypoints are an optional\ncondition, meaning that not all agents are given a list of waypoints. Agents without waypoints are\nexpected to react and behave realistically according to their learned human-like behavior priors."}, {"title": "3.4 Target Speed Conditioning", "content": "In many scenarios, controlling the aggressiveness of simulated driving behavior is essential for testing\nsafety conditions. Driving aggressiveness can significantly affect safety outcomes, influencing the\nlikelihood of collisions, near-misses, and the ability to navigate complex traffic situations. However,\ndefining aggressiveness remains an open question in the literature, as it encompasses a wide range of\nbehaviors and can have varying interpretations depending on the context (Danaf et al., 2015). For\ninstance, aggressiveness may be reflected in rapid acceleration, sharp turns, or a tendency to follow\nother vehicles too closely. These behaviors can also differ depending on road conditions, traffic\ndensity, and even driving cultural factors.\nDue to this complexity, directly modeling aggressiveness can be challenging. A practical, indirect\nmethod for controlling how aggressively a driver behaves is to condition their predicted actions\non predefined target speed values. For instance, a lower target speed may lead to more cautious,\nconservative driving patterns, while a higher target speed could encourage more assertive or aggressive\nbehaviors.\nTo incorporate target speeds, we apply FiLM-like blocks (Perez et al., 2018) on the input of every\nintermediate layer of ITRA's encoder and decoder modules. Specifically, given a target speed $\\bar{v}$\nas condition and the recurrent state $h_t^i$ for agent i at timestep t, we generate the scale and shift"}, {"title": "4 Experiments", "content": "In this section, we begin by describing the experimental setup. We proceed by evaluating the\nperformance of Control-ITRA through a series of experiments designed to measure the effectiveness\nof following waypoints and target speeds in various driving scenarios.\nWe train all our models on a large-scale self-driving dataset containing more than 1000 hours of traffic\ndata collected from 19 countries worldwide. Drones were used to record continuous traffic trajectories\nfrom various kinds of intersections. Vehicles and pedestrians are represented by 2D bounding boxes\nthat are automatically detected and tracked. Each location is annotated with a high-definition map\nrepresentation capturing the road geometry and topology. In addition, traffic controls such as traffic\nlights, and stop and yield signs are annotated.\nAll models are trained with 4-second segments with a simulation frequency of 10Hz which results\nin approximately 40 million segments usable for training. Only the first initial state is given as\nobservation and the rest 39 timesteps are predicted. Similar to \u015acibior et al. (2021), we used\nclassmates-forcing during training where all states are replayed from the ground truth trajectory\nexcept for the states of the designated ego-agent. We set the introduced hyperparameters R and $\\epsilon_v$ to\n2.0 and 1.0 accordingly."}, {"title": "4.1 Improving Performance By Following Ground Truth Conditions", "content": "We first test the ability of the proposed model to satisfy conditions in the same locations used for\ntraining. We use a validation set containing 1165 segments, each lasting four seconds. Our goal\nis to demonstrate that the conditional models can maintain realism while reaching the specified\nconditions. For this experiment, we provide only the initial state as an observation and generate\nsubsequent timesteps. Similar to the training setting, we use classmates-forcing for the non-ego\nagents. We measure realism using multiple metrics. Specifically, we use the average displacement\nerror (ADE) and the final displacement error (FDE) against the ground truth trajectory. For each\nvalidation case, we sample 6 predictions and additionally report the minimum ADE and FDE values\nof the six samples. Miss rate is also reported as an additional realism metric. A miss of an agent\nhappens when at any point in the trajectory the distance from prediction to ground truth is higher\nthan 2 meters. We also report the maximum final distance (MFD) metric (\u015acibior et al., 2021) as a\nmeasurement of diversity in the sampled predictions. It is important for the conditional driving model\nto satisfy conditions while not yielding additional infractions. We report collision rate to showcase"}, {"title": "4.2 Testing Out-of-domain Performance", "content": "We also evaluate the model's performance in new, unseen locations to ensure that it generalizes well\nacross various scenarios while maintaining both condition satisfaction and good driving behavior."}, {"title": "5 Conclusion", "content": "In this paper, we highlighted the importance of controlling driving behavior through waypoint setting\nand indirectly modulating behavior aggressiveness by conditioning on target speeds. We extended\nthe ITRA driving behavior model to enable partial conditioning of agents in the scene to follow\nwaypoints, target speeds, or both. We proposed Control-ITRA, a training scheme that allows the\nmodel to adhere to these control conditions while maintaining realistic, human-like driving behavior.\nOur experiments demonstrated that in locations where traffic data is available, the conditional\nmodel effectively follows waypoints and target speeds without compromising behavioral realism.\nAdditionally, we validated the method in novel, unseen locations, showing that it can satisfy the\ngiven conditions without increasing infraction rates. These controllable models offer the potential for\naugmenting current driving simulations to create complex and challenging scenarios. Future work\ncould explore conditioning on more abstract control forms, such as natural language commands or\ndriver intentions."}]}