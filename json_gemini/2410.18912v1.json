{"title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling", "authors": ["Mingtong Zhang", "Kaifeng Zhang", "Yunzhu Li"], "abstract": "Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at https://gs-dynamics.github.io.", "sections": [{"title": "1 Introduction", "content": "Humans naturally grasp the dynamics of objects through observation and interaction, allowing them to intuitively predict how objects will move in response to specific actions [1]. This predictive capability enables humans to plan their behavior to achieve specific goals. In robotics, developing predictive models of object motions is critical for model-based planning [2-4]. However, learning such models from real data is challenging due to the complex physics involved in robot-object interactions. Previous works have used simulation environments to learn dynamics models where ground truth 3D object states are available [5-7]. In contrast, real interaction data, such as videos, tend to be difficult to extract 3D information from, thus leading to inefficiencies in model learning.\nRecent advancements in 3D Gaussian Splatting have introduced a novel approach to 3D reconstruction. 3D Gaussian Splatting uses a collection of 3D Gaussians with optimizable parameters as object particles. A direct extension of this framework is to optimize the temporal motions of 3D Gaussians, leading to dynamic scene fitting. Nevertheless, such reconstruction methods only fit given dynamic scenes but can not predict object motions into the future.\nIn this work, we propose a novel method that combines dynamic 3D reconstruction with dynamics modeling. Our method learns a neural dynamics model from real videos for 3D action-conditioned video prediction and model-based planning. To achieve this, we first follow previous dynamic 3D reconstruction approaches [8] to obtain a particle-based representation for dynamic scenes. We extract dense correspondence from long-horizon robot-object interaction videos, which serve as the training data for a dynamics model based on Graph Neural Networks (GNNs). Operating on a spatial graph of control particles, the model predicts object motions under external actions such as robot interactions. To enable dense motion prediction, we design an interpolation scheme to calculate the transformations of 3D Gaussians from sparse control particles, enabling action-conditioned video prediction. The dynamics model can also be incorporated into a model-based planning pipeline, e.g. model predictive control, for object manipulation tasks.\nWe perform experiments on various objects of deformable materials including ropes, cloths, and toy animals. Results show our method accurately reconstructs 3D Gaussians and maintains coherent correspondences across frames, even with occlusions. The learned dynamics model simulates the physical behaviors of the deformable materials truthfully and generalizes well to unseen actions. Our method outperforms other system identification approaches (such as parameter optimization in physics-based simulators) in motion prediction accuracy and video prediction quality. We also demonstrate the model-based planning performance using our learned model in a range of object manipulation tasks."}, {"title": "2 Related Work", "content": "Dense Correspondence from Videos. Dense correspondence is usually extracted from videos using pixel-wise tracking [9-13]. Such tracking methods are usually formulated as a 2D prediction problem and require large datasets to train. Our approach is more related to another line of work, which focuses on lifting RGB or RGB-D images to 3D and track in the 3D space [14\u201316]. Recently, Dynamic 3D Gaussian Splatting approaches [8, 17] reconstruct objects as Gaussian particles and optimize per-sequence tracking using rendering loss. Building on this line of work, our method uses 3D point scans of objects as initialization and extracts correspondence for dynamics model training.\nA primary incentive for examining point tracking is its potential application to robotics [18, 19]. For instance, RoboTAP [20] demonstrates that pre-trained point tracking models enhance the sample efficiency in visual imitation learning, and ATM [21] shows that predictions of point trajectories provide control guidance for robots. Our method, in contrast, learns neural dynamics on top of particle-based tracking, which generalizes to unseen robot actions and can be naturally integrated with a model-based planning framework.\nNeural Dynamics Modeling of Real-World Objects. Modeling the dynamics of real-world objects is extremely challenging due to the high complexity of the state space and the variance of physical properties, especially for deformable materials. Using physics-based simulators, previ-"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminary: 3D Gaussian Splatting", "content": "3D Gaussian Splatting [52] optimizes a dense set of 3D Gaussians as explicit scene representation. Each Gaussian is defined by a 3D covariance matrix $\\Sigma$ centered at $\\mu$. The matrix $\\Sigma$ decomposes into a rotation matrix R and a scale matrix S by $\\Sigma = RSST RT$. For image rendering, 3D Gaussians are projected to 2D using viewing transformation W. The covariance matrix in camera coordinates $\\Sigma'$ is computed as $\\Sigma' = JWEWT JT$, where J is the Jacobian of the affine approximation of the projective transformation.\nTo optimize the 3D Gaussians, they are projected onto the image plane, where each pixel's color C is determined by a weighted blending:"}, {"title": "3.2 Dynamic 3D Gaussian Splatting with Dense Tracking", "content": "3D Gaussians have been proven effective in modeling continuously changing dynamic scenes. For instance, Dynamic 3D Gaussians (Dyn3DGS) [8] optimizes the spatial transformation of a fixed set of oriented Gaussian kernels over time to fit multiview video sequences. We build upon Dyn3DGS to extract dense correspondence of 3D Gaussians for deforming objects. Our key insight is that maintaining uniform Gaussian attributes improves modeling accuracy and physical consistency. Specifically, we keep the color, opacity, and scale of Gaussians constant during optimization, while allowing position and orientation change. We empirically found that enforcing color-consistent ellipsoids reduces optimization parameters and speeds up training, resulting in improved correspondence quality, especially when the video contains partial occlusion. Additionally, we only retain high-opacity Gaussians during training.\nIn line with Dyn3DGS, we use physics-inspired optimization objectives to guide optimization, ensuring physical plausibility and accurate long-term dense correspondence. The non-rigid physical modeling principles capture natural scene dynamics, enhancing the fidelity and stability of tracking over time. To overcome the challenges of our 4-view configuration, we strengthen the local rigidity and isometry objectives, improving the ability to handle limited observations.\nTo isolate the objects of interest, we use GroundingDINO [53] and Segment Anything [54] models to obtain masks for objects the robot interacts with. This allows us to filter out the interference of background objects, thus enhancing the optimization efficiency."}, {"title": "3.3 Graph-Based Neural Dynamics Learning", "content": "The dense tracking of object particles facilitates action-conditioned dynamics learning. Consider the scenario of an external robotic action applied to an object. The action can be represented as a sequence of end-effector positions A = {at}0<t<T. From the video sequence of the action, we can fit 3D Gaussians to the video using our modified Dyn3DGS, resulting in a collection of dynamic Gaussians: X = {Xt}0<t\u2264T = {\u03bc}1<i<N,0<t\u2264T, where Xt denotes the set of Gaussians at time t, and \u03bc\u03b5 denotes the position of a single Gaussian indexed i at time t. The underlying physical dynamics function could then be depicted as\nXt+1 = f(Xo:t, at)\n(2)\nfor any timestep t. We approximate this dynamics function with a Graph Neural Network (GNN) architecture [55], parameterized by \u03b8. To construct the graph, we apply the farthest point sampling algorithm on the Gaussians X to extract a sparse subset of control particles X = {Xt}0<t\u2264T =\n{}1<i<n,0<t\u2264T, with n particles and pairwise distance threshold dv, and use them as graph vertices. The robot end-effector position at is also considered as a graph vertex. A bidirectional edge is connected if two vertices' distance is below a threshold de, resulting in the edge set \u00ca.\nThe vertex encoder takes as input the motion of each particle \u00fb - -\u00b9 in the previous k time steps (k = 3 in our experiments) and a one-hot vector indicating whether the vertex belongs to the object or the robot end-effector. To model the relationship between the object and the table surface, the distance of every vertex to the surface is also included in the input. The edge encoder takes as input the 3-dimensional position difference of the two vertices and a one-hot vector indicating whether the edge is an object-object relation or an object-robot relation.\nThe graph encodes vertices and edges into latent features using shared vertex and edge encoders, performs message passing for p timesteps (p = 3 in our experiments), and uses a shared decoder to"}, {"title": "3.4 Dense Motion Prediction on 3D Gaussians", "content": "The flexibility of 3D Gaussians allows us to perform motion densification from the sparse subset of particle X to the entire collection of Gaussians X while maintaining the ability to render photo-realistic videos. Taking an initial static collection of Gaussians X = {X} and the action sequence A = {at}0<t\u2264T as input, we first perform recurrent future prediction on the subsampled graph vertices, outputting X0:T. Then, for each timestep t, we perform the following interpolation scheme to derive the center motions and rotations of each Gaussian:\nGraph Vertex Transformations. In the first step, we need to calculate the 6-DoF transformation for each graph vertex. The outputs of the GNN are per-vertex motions, which directly serve as the 3D translations. For the 3D rotation, for each vertex \u00fb, we solve for the rotation R according to the motion of its neighbors from time t to t + 1:\n$R_i^* = arg \\underset{R \\in SO(3)}{min} \\sum\\limits_{j \\in N(i)} ||R_i(p_j^t - p_i^t) - (p_j^{t+1} - p_i^{t+1})||^2$.\n(5)\nGaussian Transformations. We adopt Linear Blend Skinning (LBS) [57, 58] to calculate the motions of Gaussian centers. Concretely,\n$\\mu_i^{t+1} = \\sum\\limits_{b=1}^{n} w_{ib}^t(R_b^t(\\mu_i^t - \\hat{\\mu}_b) + \\hat{\\mu}_b + \\Tau_b), q_i^{t+1} = (\\sum\\limits_{b=1}^{n} w_{ib}^t r_b^t) q_i^t$,\n(6)\nwhere w is the weight associated between Gaussian i and graph vertex b; R, r are the matrix and quaternion representations of vertex b's rotation at time t; T is the translation of vertex b, which is directly predicted by our dynamics model. The blending weight between a Gaussian and a vertex is inversely proportional to their 3D distance:\n$w_{ib} = \\frac{||\\mu_i^t - \\hat{\\mu}_b||^{-1}}{\\sum\\limits_{b'=1}^{n} ||\\mu_i^t - \\hat{\\mu}_{b'}||^{-1}}$\n(7)\nthus assigning larger weights to vertices that are spatially closer to the Gaussians. We start from the initial Gaussian centers and rotations at t = 0 and repeatedly calculate the blending weights and updated Gaussian centers and rotations, resulting in dense and smooth Gaussian motions over the entire video sequence."}, {"title": "3.5 Model-Based Planning", "content": "The dynamics network can also be used to perform model-based planning in robotics tasks. Given the multi-view RGB-D image of the object, we apply segmentation and point fusing to get the complete point cloud of the object. We then construct graph vertices and edges with farthest point sampling. Given a target configuration, we use a Model Predictive Control (MPC) [59] framework to plan for robot actions. The optimization objective is defined as the mean squared difference between the target state and the predicted object state if an action is applied. In our experiments, we apply the Model-Predictive Path Integral (MPPI) [60] trajectory optimization algorithm."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Setup", "content": "We perform experiments on 3D tracking, action-conditioned video prediction, and model-based planning. To achieve this, we collect a real-world dataset, with a focus on acquiring multiview images synchronized with corresponding actions at each timestep. The dataset consists of 3 types of deformable objects: rope, cloth, and toy animals, totaling 8 object instances. For all objects, we capture RGBD images and record robot actions at 15 FPS. The depth is used to initialize the point cloud for Gaussian representation. Our data collection setup includes four cameras, strategically positioned at the corners of the workspace and oriented to provide a comprehensive top-down perspective. This setup allows us to collect synchronized multiview data necessary for our experiments."}, {"title": "4.2 3D Tracking with Dynamic 3D Gaussians", "content": "We initialize Gaussian tracking with a point cloud from four views at the first timestep and optimize our Dyn3DGS for dense correspondence. To evaluate our method, we compare it with advanced 2D tracking approaches by projecting 4-view results into 3D using our depth data and evaluate the best results. The unmodified Dyn3DGS is considered as a baseline to show our improvements within the 4-view real-world experiment setup. Following prior work [61], we evaluate median trajectory error (MTE) in millimeters, position accuracy (\u03b4) at thresholds of 2, 4, 8, and 16 millimeters (reporting the average), and survival rate with a threshold of 0.5 meters. For a fair comparison, we also project our 3D tracking results into 2D pixels and evaluate the same metrics, allowing comprehensive assessment in both 2D and 3D contexts."}, {"title": "4.3 Action-Conditioned Video Prediction", "content": "Using dynamic 3D Gaussian reconstruction with tracking, we train the graph-based neural dynamics models with downsampled control particles. These particle motions are used to interpolate dense Gaussian kernel motion and rotations. To evaluate the quality of our dynamics prediction and 3D Gaussian rendering, we assess action-conditioned video prediction performance.\nBaselines. Synthesizing action-conditioned object motions requires physics priors, and existing text-conditioned video prediction methods are not compatible with taking 3D robot actions as input. In this experiment, we consider 2 physics simulator-based baselines, MPM and FleX.\nMPM is based on the Material Point Method simulation framework [62, 63]. Our baseline MPM uses the same simulation setting as previous works [22, 26], while adding support for two types of robot end-effectors: cylindrical pusher and gripper. For each object instance, we set the friction coefficient u and Young's modulus E (assumed uniform) to be two learnable parameters that are optimized from the training data using the CMA-ES algorithm. FleX is based on the NVIDIA FleX simulator. For all objects, we adopt the soft body simulator in FleX, reconstruct the initial object mesh from 3D Gaussians using alpha-shape mesh reconstruction. Similarly, we optimize the friction coefficient u and stiffness coefficient s (assumed uniform) of each object instance using CMA-ES."}, {"title": "4.4 Model-Based Planning", "content": "The action-conditioned dynamics model enables deployment in real-world robot planning tasks, demonstrating its generalizability to unseen object configurations. We choose rope straightening and toy animal relocating tasks to show our model's ability to manipulate highly deformable objects to target configurations. The quantitative results in Fig. 5 demonstrate that our approach effectively reduces error and achieves high success rates. The computational complexity and inefficiency of MPM and FleX baselines make them impractical to perform real-world planning tasks."}, {"title": "5 Conclusion and Limitation", "content": "In this paper, we introduce a novel approach for graph-based neural dynamics modeling from real-world data for 3D action-conditioned video prediction and planning. Our method reconstructs 3D Gaussians with cross-frame correspondences and learns a neural dynamics model that facilitates action-conditioned future prediction. The method outperforms baseline approaches in motion prediction and video prediction accuracy. We also showcase our planning performance for object manipulation tasks, highlighting our framework's effectiveness in real-world robotic applications.\nLimitation Although our approach learns object dynamics directly from videos, collecting real-world data remains costly, and the perception module may fail when there are large occlusions or textureless objects. In future work, we aim to develop a more efficient data collection pipeline and a more robust perception system to reduce the learning cost and enhance the method's applicability to all kinds of real videos."}]}