{"title": "An Advantage-based Optimization Method for Reinforcement Learning in Large Action Space", "authors": ["Hai Lin", "Cheng Huang", "Zhihong Chen"], "abstract": "Reinforcement learning tasks in real-world scenarios often involve large, high-dimensional action spaces, leading to challenges such as convergence difficulties, instability, and high computational complexity. It is widely acknowledged that traditional value-based reinforcement learning algorithms struggle to address these issues effectively. A prevalent approach involves generating independent sub-actions within each dimension of the action space. However, this method introduces bias, hindering the learning of optimal policies. In this paper, we propose an advantage-based optimization method and an algorithm named Advantage Branching Dueling Q-network (ABQ). ABQ incorporates a baseline mechanism to tune the action value of each dimension, leveraging the advantage relationship across different sub-actions. With this approach, the learned policy can be optimized for each dimension. Empirical results demonstrate that ABQ outperforms BDQ, achieving 3%, 171%, and 84% more cumulative rewards in HalfCheetah, Ant, and Humanoid environments, respectively. Furthermore, ABQ exhibits competitive performance when compared against two continuous action benchmark algorithms, DDPG and TD3.", "sections": [{"title": "1. Introduction", "content": "With the wide applications in poker games (Wang, 2017), robot control (Zhang, Li, Yuan and Fu, 2018a), autonomous driving (Zhang, He, Chen et al., 2019; Huang, Zhang, Tian and Zhang, 2019), traffic control (Yang, Zhang, Shi and Zhang, 2014; Yang, Zhang and Zhu, 2018), etc., Deep Reinforcement learning (DRL) has shown great potential and has received more and more attention. In terms of implementation, DRL usually uses a neural network to approximate the value function (value-based method) or to represent the probability distribution (policy-based method) (Chen, Yao, McAuley, Zhou and Wang, 2021). Despite the neural network's efficacy in handling high-dimensional state spaces, it faces challenges of high-dimensional action spaces. This increase in dimensionality results in an exponentially growing number of actions, often referred to as the curse of dimensionality.\nWhen a task involves multiple discrete actions on each of the multiple dimensions, the issues become ever more severe. The policy-based algorithms can only generate an approximate policy, sampled from a continuous probability distribution for each action dimension. Accumulating throughout dimensions, the bias between policy and executing action will be tricky. Moreover, the policy-based algorithms have problems of convergence, sample efficiency, and training variance. On the other side, the value-based algorithms are primarily designed for discrete action spaces, which maintain value functions for each discrete action. They are deemed to adapt to discrete control tasks better\nand some renowned works, including Q-Learning (Watkins, 1989), DQN (Deep Q-Network) (Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra and Riedmiller, 2013), Double DQN (Van Hasselt, Guez and Silver, 2016), Dueling Architecture (Wang, Schaul, Hessel, Hasselt, Lanctot and Freitas, 2016), among others, are widely adopted. However, the value-based algorithms are characterized to explicitly evaluate all possible actions, while the curse of dimensionality contributes to overly large action spaces and innumerable actions to evaluate. They suffer from poor learning efficiency in this scenario. Thus, it's significant to enhance the training efficiency of the value-based algorithms.\nPrevious works address the challenge of the large action space by reducing the scale of the action space, such as action space split (Tavakoli, Pardo and Kormushev, 2018), value function segmentation (Sunehag, Lever, Gruslys, Czarnecki, Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls et al., 2017) (Rashid, Samvelyan, De Witt, Farquhar, Foerster and Whiteson, 2020), or action subsets processing (Liang, Ma, Cao, Liu, Ni, Li and Hao, 2023). By partitioning the task into sub-parts with smaller action spaces, these methods aim to alleviate computational complexity. However, after solving these sub-parts, integrating the sub-part solutions back into the original action space brings a big challenge. Many existing approaches simply concatenate all results, potentially leading to significant bias. To address this challenge, we propose an advantage-based optimization method. The advantage of an action represents the difference in Q value between that action and others. In our proposition, it is measured by a baseline which is calculated by considering the action values across all subparts. Subsequently, the action values of all subparts are tuned according to the baseline, so as to enhance the overall performance."}, {"title": "2. Related works", "content": "Given that a large action space often manifests as high-dimensional, a straightforward approach involves dividing the action space along each dimension into multiple sub-action spaces and solving each sub-action space independently. This can be regarded as a Multi-Agent Reinforcement Learning (MARL) issue. Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus, Aru, Aru and Vicente (2017) devised independent Q networks for the two players (two agents) in the Pong game. These agents could engage in competitive or cooperative gameplay by tuning the reward function. However, this design violates the Markovian environment requirement. Under this scheme, one player perceives the other player as part of the environment, leading to the next state being determined not only by the current state but also by the other player's actions (Sun, Cao, Chen et al., 2020). While effective in specific experiments, this method encounters challenges when applied to more complex tasks.\nSome recent works decompose the action space indirectly through the decomposition of the value function. By keeping correlations between actions, these methods ensure the algorithm complexity increases linearly with the action dimensions. Sunehag et al. (2017) introduced Value-Decomposition Networks (VDN), which linearly decomposes the reward and the Q function, allowing for relatively independent learning of policies by multiple agents. Rashid et al. (2020) identified VDN's inability to handle complex value functions and proposed a solution called QMIX.\nQMIX employs a nonlinear hybrid network to decompose the value function and guarantees consistency between the centralized and decentralized policies by constraining the weights of each network to be non-negative. The MARL Q-value Decomposition algorithm (MAQD) introduced by Zou, Jiang, Li, Gao, Li and He (2021) decomposes the Q value output by the critic network in the Actor-Critic (AC) architecture. It leverages the Nash equilibrium to ensure that each agent can receive the highest reward. Similarly, Ming, Gao, Liu and Zhao (2023) proposed the CMRL (Cooperative Modular RL) method, which decomposes the large discrete action space into multiple sub-action sets. Each critic learns a decomposed value function in parallel, facilitating cooperative learning.\nTavakoli et al. (2018) introduced the Action Branching Architecture, which divides the action space into branches along dimensions and implements it as the Branching Dueling Q-network (BDQ) algorithm. To fulfill the Markovian environment requirement, they incorporated a shared feature extraction module to ensure that all branches receive the same observation. Metz, Ibarz, Jaitly and Davidson (2017) employed a next-step prediction strategy in discrete space to model Q-values and policies over continuous space. This approach sequentially predicts all dimensions, approximating the optimal action. Andriotis and Papakonstantinou (2019) distributed the entire action space across different subsystems. Each subsystem has a limited number of actions and operates independently, thus the total number of actions is significantly reduced.\nMethods proposed by the aforementioned works have found widespread application across diverse scenarios. Chen, Cai, Zheng, Hu and Li (2022) applied the value decomposition method to video encoding, decomposing video attributes (such as clarity and quantity) into sub-actions to cope with the large action space caused by the large number of videos and users. Similarly, Tang, Makar, Sjoding, Doshi-Velez and Wiens (2022) employed a linear value decomposition scheme in the medical system context for the large combinatorial action space. In addressing the 3D bin packing problem, Zhao, Zhu, Xu, Huang and Xu (2022) utilized RL for policy learning in each dimension to solve the large action space resulting from the high-resolution spatial discretization. Based on the BDQ algorithm proposed by Tavakoli et al. (2018), Choi, Choi, Lee, Yoon and Bahk (2023) and Penney, Li, Sydir, Tai, Walsh, Long, Lee and Chen (2022) decomposed large action spaces into multiple branches to tackle dynamic resource allocation problems in 5G communication systems. In addition, BDQ has been applied in various other domains, including network slicing reconfiguration (Wei, Feng, Sun, Wang, Qin and Liang, 2020), smart building environment control systems (Ding, Du and Cerpa, 2019), industrial system maintenance (Bi, Fang, Roux and Barros, 2023), and vehicle collaborative perception (Abdel-Aziz, Perfecto, Samarakoon, Bennis and Saad, 2021).\nSeveral studies have developed strategies to restrict the selection range of actions, in order to reduce the action space."}, {"title": "3. Advantage Branching Dueling Q-Network", "content": "Liang et al. (2023) developed SplitNet, a model designed to tackle the multiple traveling salesman problem. By utilizing a heuristic function, SplitNet focuses on specific segments of the action space, significantly improving training speed and generalization ability. In the actor-critic architecture proposed by Iqbal and Sha (2019), an attention mechanism is designed for dynamic changes in the critic's focus. Consequently, the critic can select actions from only a few actors, thereby enhancing training efficiency. The improved Proximal Policy Optimization (PPO) algorithm in (Lu, Bao, Xia and Qu, 2022) also uses a neural network based on the attention mechanism in the actor-critic architecture. Through the Bahdanau Attention layer, the critic can obtain the importance of all actors, which allows the critic to focus on existing actors, thereby ignoring part of the state or action space. A similar design can be found as well in the MAXQ algorithm proposed by Dietterich (2000).\nHierarchical Reinforcement Learning (HRL) offers a systematic approach to decomposing complex tasks into manageable subtasks across different levels (Pateria, Subagdja, Tan and Quek, 2021). This methodology is believed to enhance decision-making capabilities and has been widely adopted in recent research endeavors. Wu, Gao, Wang, Zhang and Liu (2021) introduced the concepts of managers and workers for robot control in complex environments. Workers are tasked with directly controlling the actions of the robot, while the manager is responsible for task assignment and coordination. Zhang, Guo, Tan, Hu and Chen (2022) extended hierarchical reinforcement learning by introducing an adjacency constraint, ensuring that agents located close to each other share the same goal. This constraint enhances the robustness and learning efficiency of the system.\nFor an industry system with many components, Zhou, Li and Lin (2022) proposed a hierarchical coordinated RL algorithm. They introduced a hierarchical structure where components with low priority consist of subsystems. Since these subsystems are assigned with abstract actions, the overall action space is largely simplified. Abstract actions are also designed in the recommendation system domain by Liu, Cai, Sun, Wang, Jiang, Zheng, Jiang, Gai, Zhao and Zhang (2023). Initially, the authors use a network to learn a potential feature vector, referred to as the hyper-action. Subsequently, they utilize a kernel function to transform this hyper-action into the actual action, known as the effect-action. This approach alleviates the need for the agent to directly handle the large action space, thereby enhancing algorithm efficiency.\nSome other studies have focused on policy or action approximation techniques. Zhang, Yang, Liu, Zhang and Basar (2018b) proposed two decentralized actor-critic architectures grounded in the new policy gradient theory. In these architectures, actors share estimated action values with neighboring actors, forming a common local estimate. By voting for only a few representative actions, the number of actions that are required to be evaluated by the critic is significantly reduced. Khan, Zhang, Lee, Kumar and Ribeiro (2018) explored the scenarios involving a large number of identical agents. They indicate that agents within a certain range exhibit similar strategies that can be approximated by a common strategy. This method doesn't learn for every single agent, thus reducing the computational overhead and resource demands. For information exchange problems among homogeneous robots, they introduced the Graph Convolutional Neural (GCN) network based on Convolutional Neural Networks (CNNs) (Khan, Tolstaya, Ribeiro and Kumar, 2020). GCN aggregates information from neighboring nodes (e.g., based on Euclidean distance), while CNN's pooling operation reduces the search dimension and constrains the search space. Li, Shi and Hwang (2022) proposed the concept of a fuzzy agent instead of multiple agents.\nNumerous methods have been developed to tackle tasks involving large discrete action spaces. In these works, the decomposition method based on the Action Branching Architecture has emerged as a widely adopted approach due to its robust generative capabilities, versatility across tasks, and efficient training performance. Extensive research demonstrates the efficiency of the Action Branching Architecture in resolving reinforcement learning tasks with large discrete action spaces for both simulated and real-world environments. However, a notable limitation of this architecture lies in its trivial strategy for action selection, wherein each branch independently chooses its sub-actions, and the final action is just a concatenation of these sub-actions. It is obvious that the concatenation of optimal sub-actions does not always yield an optimal global action. In this paper, we introduce a novel advantage-based optimization method based on the original Action Branching Architecture. By enhancing the coordination of policies among branches, our proposed method aims to address the shortcomings of the existing architecture."}, {"title": "3.1. Action Branching Architecture", "content": "To demonstrate the curse of dimension problem in large action space, we take an example as follows: consider a scenario with 2 dimensions, each offering 25 possible actions. The combined number of actions is 25^2 = 625. With 3 dimensions, this number increases dramatically to 25^3 = 15625. Formally, for an action space A with n dimensions and N possible actions in each dimension, the size of the action space, denoted as |A|, is calculated as:\n$|A| = N^n$.\n(1)\nA real-world RL task is often characterized by a high-dimensional action space with a large n. When each dimension requires fine division, N naturally increases, leading to an exponentially larger action space. As aforementioned, value-based algorithms like DQN require the evaluation of every possible action. The issue of a large action space makes it difficult to satisfy this requirement, as it demands significant computing resources and extensive training time."}, {"title": "3.2. Advantage Branching Architecture", "content": "Consequently, it poses a challenge for value-based algorithms to learn optimal policies.\nThe Action Branching Architecture Tavakoli et al. (2018), provides a solution for handling tasks with high-dimensional action spaces within the realm of value-based RL algorithms. This architecture partitions the action space into action branches according to dimensions, enabling the evaluation of sub-actions in each dimension. Each branch network is associated with an action dimension that assesses all sub-actions of this dimension and generates action values. The highest-valued sub-actions from each branch are then selected and combined to form a final global action. Formally, let the action act comprise n dimensions, denoted as act = (act\u2081, act\u2082,..., act\u2099), where act\u1d62 (with 1 \u2264 i \u2264 n) represents the sub-action in the ith dimension. If each dimension contains N possible sub-actions, then only n \u00d7 N actions need to be evaluated, maintaining a linear relationship with the dimension. We take an example in Figure 1, where the action space has 3 dimensions and each contains 4 possible sub-actions. Despite the potential total of 3\u2074 = 64 actions, only 4\u00d73 = 12 evaluations are required. This significantly enhances decision-making efficiency.\nHowever, the combination of individual optimal sub-actions can not guarantee an optimal global action. A possible solution is to introduce interaction among branches. According to this concept, we propose an advantage-based optimization method, named the Advantage Branching Architecture. This method tunes the action value on each branch based on a pre-defined baseline. In our proposition, we update the Q value using the Temporal Difference (TD) method. For the ith branch, at time t, state s\u1d57, taking action a\u1d57, the update rule for the corresponding Q value is defined as:\n$Q_i(s^t, a^t) \\leftarrow Q_i(s^t, a^t) + \\alpha[r^t + \\gamma Q_i(s^{t+1}, a^{t+1}) - Q_i(s^t, a^t)],$\n(2)\nwhere \u03b1 denotes the learning rate, r\u1d57 represents the reward, \u03b3 is the discount rate, s\u1d57\u207a\u00b9 and a\u1d57\u207a\u00b9 denote the subsequent state and action. The temporal difference error (TD error) is:\n$TDe_i = r^t + \\gamma Q_i(s^{t+1}, a^{t+1}) - Q_i(s^t, a^t)$.\n(3)\nHere, TDe\u1d62 is influenced by both the reward r\u1d57 and the Q value difference \u03b3Q\u1d62(s\u1d57\u207a\u00b9,a\u1d57\u207a\u00b9) \u2212 Q\u1d62(s\u1d57, a\u1d57). As the environment determines the reward and \u03b3 remains constant during training, value-based algorithms predominantly impact TDe\u1d62 through the second term. In a multi-branch architecture, each branch may generate a distinct Q value estimate for the same state, leading to a distinct Q value difference. A branch with little Q value improvement, while others exhibiting significantly higher values, shouldn't be overly credited for its action selection. Thus a way to distinguish the advantage relationship among branches and to tune the estimate is crucial. Although the branch might rectify its estimate by increasing the Q values of other actions, employing suitable adjustments may accelerate the learning process.\nSimilar motivation can be found in the well-known Dueling Architecture proposed in Wang et al. (2016). Unlike traditional deep reinforcement learning methods, which directly calculate the action value Q through a network, the Dueling Architecture decomposes the Q value into two components: the state value V and the action advantage A:\n$Q(s, a) = V(s) + A(s, a),$\n(4)\nwhere s denotes the state, a represents the action, V is the state value, and A denotes the action advantage of action a in state s. The core concept underlying this architecture is that in superior states, regardless of the chosen action, the Q values tend to be high. By removing the effect of the state, an agent can more effectively discern the advantage of one action over the others. Inspired by this method, we introduce a baseline mechanism that discerns the advantage of one branch over the others. In our proposition, the chosen baseline, denoted as B, is as follows:\n$B = max_j \\frac{1}{N} \\sum_{i=1}^{N} A(s, a_{ij}),$\n(5)\nwhere n represents the number of branches, N is the number of optional sub-actions on each branch, and a\u1d62\u2c7c denotes the ith optional sub-action on the jth branch. As we apply Dueling Architecture in our mechanism, the input is the action advantage instead of the Q value. The calculation of B is critical in our proposition, defined as the maximum average action advantage among all branches. According to our experiments, this definition outperforms other baselines, such as the global maximum action advantage, the global average action advantage, etc. We decompose the action advantage on the ith branch further into two components: branch advantage and baseline. Then, by combining branch advantage and state value, the Q value is calculated as:\n$Q_i(s, a_{ij}) = V + A_i(s, a_{ij}) - B,$\n(6)\nwhere A\u1d62(s, a\u1d62\u2c7c) \u2212 B represents the branch advantage of the ith branch over other branches. This way, the baseline is"}, {"title": "3.3. Network and Algorithm", "content": "introduced to tune the action advantage. Consequently, the TD error is:\n$TDe_i = r^t + \\gamma Q_i(s^{t+1}, a^{t+1}) - Q_i(s^t, a^t)$\n$= r^t + \\gamma (V^t + A_i(s^{t+1}, a^{t+1}) - B^t) - (V^{t+1} + A_i(s^t, a^t) - B^{t+1})$.\n(7)\nBy comparing equation (7) to equation (3), we observe that TDe\u1d62 is now determined mainly by the advantage of the ith branch, rather than the raw output of the ith branch. Based on the Action Branching Architecture, we propose the Advantage Branching Architecture, which employs the baseline mechanism in equation (5) and tunes the output with equation (6).\nThe network of our architecture is illustrated in Figure 2, which consists of four parts:\n\u2022 Feature Network: A shared neural network is utilized to extract features from the current state, converting the variable environment state into a fixed-length feature vector. To balance efficiency and information extraction, the neural network size is set to 256 and 128.\n\u2022 State Value Network: This part receives the state features and provides an evaluation value of the given state, as the first term in equation (4). It is an essential part of the Dueling Architecture. The size of the neural network is set to 64.\n\u2022 Action Advantage Network: This network learns from the state features to generate the action advantage values as the second term in equation (4). Similar to the Action Branching Architecture, it employs independent neural networks, known as branch networks, for every action dimension. Each branch evaluates the advantage values of all sub-actions in its dimensions. The size of the neural networks is set to 64.\n\u2022 Baseline Module: The Baseline Module calculates a baseline using equation (5), which is then used to tune the action advantages on each branching using equation (6). Subsequently, the tuned action advantages, together with the state value, are used to output the Q values. Sub-actions are then selected according to the maximum Q value, forming the optimal action.\nThe State Value Network and the Action Advantage Network form a Dueling Architecture, while the Baseline Module implements our proposed method. Our network considers the advantages of sub-actions within a branch, as well as those among different branches. Then, we develop the Advantage Branching Dueling Q-network (ABQ), where each branch network is implemented using DQN. For all sub-actions, the Q values are calculated as follows:\n$Q(s, a) = V(s) + (A(s, a) - B)$.\n(8)"}, {"title": "4. Performance evaluation", "content": "The process of ABQ is illustrated in Algorithm 1. At the beginning, we initialize a replay buffer (line 1), a dueling network (line 2), and a target network (line 3). For each iteration, we initialize the environment (line 7) and obtain its state (line 8). We introduce \u03f5 \u2212 greedy to balance exploration and exploitation (line 10). For exploration, the agent chooses a random action. Otherwise, every branch evaluates all sub-actions in the current state, and the State Value Network generates a state value (line 13). Then, a baseline is computed based on the output of all branches according to equation (5) (line 14), which is then applied to adjust the values of sub-actions according to equation (8) (line 15). The agent selects sub-actions with the highest values (line 16), and executes the action in the environment (line 18). The environment transfers to the next state based on the action and its transition probability, rewarding the agent accordingly (line 18). Subsequently, the agent stores the tuple comprising state, reward, action, and next state in the replay buffer (line 20). Once a sufficient number of tuples are stored in the buffer, the agent retrieves a batch of tuples (line 22), calculates the Loss according to equation (7) (line 23), and updates its network (or policy) (Line 24). Similar to DQN, the target network is updated every T episodes (line 28).\nReleased by OpenAI on April 27, 2016, Gym provides a standardized platform for various RL tasks, including classic control problems, Atari games, 2D and 3D robot control simulations, etc. Most RL-related studies utilize Gym to train and evaluate their algorithms. For our experiments, we selected one environment with one-dimensional action space (Pendulum) and four environments with multi-dimensional action space (BipedalWalker, HalfCheetah, Ant, and Humanoid) from Gym to analyze the performance of ABQ. In Pendulum, the goal is to apply torque on an inverted pendulum to swing it into an upright position. In BipedalWalker, the agent controls a 4-joint walker robot to cover a slightly uneven terrain. In HalfCheetah, Ant, and Humanoid, an agent must control walker robots to move forward and avoid falling, with the control complexity increasing respectively. Table 1 provides a summary of these environments, where the state dimension, denoted as n\u209b, and the action dimension, denoted as n\u2090. The complexity of the listed environments roughly increases. To accommodate value-based algorithms, we discretized each action dimension into 25 discrete values. In consequence, the simplest multi-dimensional environment BipedalWalker with 4 action dimensions, has approximately 25\u2074 \u2248 3.9e4 discrete actions, while the most complex continuous environment Humanoid with 17 action dimensions, yields more than 25\u00b9\u2077 \u2248 5.8e23 discrete actions.\nBesides the original BDQ, we also compare our ABQ with two benchmark continuous control algorithms: Deep Deterministic Policy Gradient (DDPG) Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver and Wierstra (2015) and"}, {"title": "5. Conclusion", "content": "This paper proposes an advantage-based action branching architecture to address the issue of large action spaces in reinforcement learning (RL). By using the baseline to calculate the advantage relationships among action branches, the action values generated by each branch can be adjusted accordingly. This adjustment improves the learning rate and results in better rewards. Building upon the foundational framework of BDQ, we designed the ABQ Network and its corresponding algorithm. Our performance analysis demonstrates that the proposed ABQ outperforms BDQ in various scenarios and achieves comparable or superior performance compared to the continuous control benchmark algorithms DDPG and TD3, even in continuous action scenarios. Hence, ABQ effectively addresses the issue of large action spaces in RL. For future work, we are interested in addressing both large action space and large state space issues, and investigating how the baseline mechanism can simultaneously tackle both challenges."}]}