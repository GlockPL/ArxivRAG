{"title": "IDENTIFYING KNOWLEDGE EDITING TYPES IN LARGE\nLANGUAGE MODELS", "authors": ["Xiaopeng Li", "Shangwen Wang", "Shezheng Song", "Bin Ji", "Huijun Liu", "Shasha Li\u2217", "Jun Ma", "Jie Yu"], "abstract": "Knowledge editing has emerged as an efficient approach for updating the knowl edge of large language models (LLMs), attracting increasing attention in recent research. However, there is a notable lack of effective measures to prevent the malicious misuse of this technology, which could lead to harmful edits in LLMs. These malicious modifications have the potential to cause LLMs to generate toxic content, misleading users into inappropriate actions. To address this issue, we introduce a novel task, Knowledge Editing Type Identification (KETI), aimed at identifying malicious edits in LLMs. As part of this task, we present KETIBench, a benchmark that includes five types of malicious updates and one type of be nign update. Furthermore, we develop four classical classification models and three BERT-based models as baseline identifiers for both open-source and closed source LLMs. Our experimental results, spanning 42 trials involving two mod els and three knowledge editing methods, demonstrate that all seven baseline identifiers achieve decent identification performance, highlighting the feasibil ity of identifying malicious edits in LLMs. Additional analyses reveal that the performance of the identifiers is independent of the efficacy of the knowledge editing methods and exhibits cross-domain generalization, enabling the identi fication of edits from unknown sources.", "sections": [{"title": "INTRODUCTION", "content": "Knowledge editing is an emerging technology designed to efficiently rectify errors or outdated in formation in large language models (LLMs) (Yao et al., 2023). In recent years, it has garnered increasing attention (Wang et al., 2023). The primary aim of knowledge editing is to provide ap proaches for efficiently updating the knowledge within LLMs, thereby avoiding the costly process of retraining LLMs. However, there is a deficiency in effective measures to prevent the misuse of these techniques (Chen et al., 2024; Li et al., 2024b). Some malicious individuals may exploit knowledge editing to inject harmful information into LLMs, which does not align with human values. Users lacking discernment can be easily influenced by those problematic information, potentially leading them to engage in actions that contravene ethical standards and human values.\nCurrent research begins preliminary exploration into detecting edited knowledge within LLMs (Youssef et al., 2024). This study finds that simple classifiers can effectively distinguish between edited and non-edited knowledge. However, it fails to distinguish between various forms of edited knowledge, which is vital as it aids in discerning the intentions of editors and thus enables the ef fective alerting of users to potentially harmful and erroneous information that can arise from illicit edits of LLMs. To fill this gap, we propose a new task called Knowledge Editing Type Identification (KETI), which aims to identify the knowledge edited in LLMs and its corresponding type. Provid ing more fine-grained information about the knowledge editing allows users to differentiate between malicious and benign updates more effectively, enabling timely alerts to users when encountering illicit edits."}, {"title": "RELATED WORK", "content": "Knowledge editing aims to provide efficient approaches to update the knowledge within LLMs, a rapidly emerging area that has gained increasing attention (Yao et al., 2023; Zhang et al., 2024; Wang et al., 2023). In recent years, various knowledge editing methods (Meng et al., 2022b; Li et al., 2023; Mitchell et al., 2022; Hartvigsen et al., 2023; Deng et al., 2024; Wang et al., 2024b; Li et al., 2024a) and benchmarks (Wang et al., 2024c; Rosati et al., 2024; Zhong et al., 2023) have been proposed. However, the ethical and moral risks associated with knowledge editing should also be acknowledged. Researchers have found that backdoors and harmful information can be implanted into LLMs through knowledge editing (Li et al., 2024b; Chen et al., 2024). Therefore, there is an urgent need for measures to mitigate the risks posed by malicious edits. To this end, we propose the KETI task, which aims to identify the knowledge editing types. Results from various baseline identifiers on KETIBench show that different types of knowledge editing in both open-source and closed-source LLMs can be identified. Our preliminary exploration makes it possible to effectively mitigate the risks posed by the misuse of knowledge editing. Notably, the most relevant work to ours is DEED (Youssef et al., 2024); however, DEED only considers the distinction between edits and non-edits, without further exploring different types of edits."}, {"title": "KNOWLEDGE EDITING", "content": "The current definitions of knowledge editing are mostly centered around the factual knowledge edit ing (Meng et al., 2022b; Mitchell et al., 2022; Hartvigsen et al., 2023). Nevertheless, the definition"}, {"title": "IDENTIFYING KNOWLEDGE EDITING TYPES IN LARGE LANGUAGE\nMODELS", "content": null}, {"title": "KETI TASK", "content": "The purpose of the KETI is to identify the knowledge edited in LLMs and its corresponding type, aiming to mitigate the societal impact of harmful edits made by malicious individuals. Assume that an LLM \\(\\theta_{e}\\) has been edited by a malicious individual to include a series of knowledge, the majority of which consists of harmful information. To further confuse the situation, the individual may also perform some fact updates. Given a series of knowledge prompts \\(P = [p_{1}, . . . ]\\), the goal of identifier\n\\(f\\) is to determine whether these knowledge has been edited in \\(\\theta_{e}\\), and if so, to further identify the\nknowledge editing type. The identifier in KETI can be viewed as a function \\(f : \\theta_{e}(P) \\rightarrow Y\\),\nwhere \\(Y\\) denotes the types of knowledge editing. Note that in real-world scenarios, we cannot know\nwhether an LLM has been edited in advance. We directly consider an edited LLM for the sake of\nconducting the KETI task more efficiently. Nevertheless, KETI can be applied to any LLMs, as the\ntask involves identifying whether knowledge has been edited."}, {"title": "DATA CONSTRUCTION", "content": "The preconditions for KETI is that an LLM has already been edited with a series of knowledge, pri marily harmful information, along with a small amount of beneficial knowledge (i.e., fact updating). Inspired by previous works (Zou et al., 2023; Wang et al., 2024a; Chen et al., 2024), we consider five types of knowledge in KETIBench: Fact updating, Misinformation injection, Offensiveness injection, Behavioral misleading injection, and Bias injection.\nWe first collect Behavioral misleading, Offen siveness, Misinformation, and Bias data from Advbench (Zou et al., 2023). Advbench con tains 576 harmful strings, but these strings do not have corresponding queries, rephrased queries, or specific categories. Therefore, we use GPT-40 to generate these details for the strings. We show the prompts in Appendix A.2. Second, we enrich the number of Misinforma tion and Bias samples using Misinformation In jection and Bias Injection from the Editing At tack dataset (Chen et al., 2024). Third, we se lect samples from the zsRE dataset (Mitchell et al., 2022) where the answer length exceeds 50 characters for use in Fact updating. The rationale for selecting samples with an answer length greater than 50 is that the average char acter length of the object o in other types of knowledge is near 50, and we aim to minimize the impact of varying object o lengths on the ex"}, {"title": "BASELINE IDENTIFIERS", "content": "Current LLMs can be categorized into two types based on whether their weights are open-source:\nopen-source LLMs and closed-source LLMs. The information we can access from these two types\nof LLMs differs. For open-source LLMs, we can access all information, including weights, hidden\nstates generated during inference, and output data. In contrast, for closed-source LLMs, only the\noutput data is accessible. Therefore, we describe different baseline identifiers for each type of LLM."}, {"title": "BASELINE IDENTIFIERS FOR OPEN-SOURCE LLMS", "content": "Since the information accessible in closed-source LLMs is a subset of that in open-source LLMs, the\nidentifiers for close-source LLMs are also applicable to open-source LLMs. Here, we only consider\nweights and hidden states, leaving output data to be considered under closed-source LLMs. When\nthe original model weights are available, it is easy to determine whether the weights have been\nmodified by comparing them, and thus determining whether the model has been edited is trivial.\nHowever, model weights are static features, and relying solely on this static feature does not provide\ninsight into the internal state of LLMs with respect to the knowledge prompts \\(P\\), making it difficult\nto identify the type of knowledge edits. In contrast, hidden states, as dynamic features, aggregate\ninformation from both the knowledge prompts and the weights (Geva et al., 2023), offering richer\ninformation. Therefore, we design identifiers using the hidden states generated by the knowledge\nprompts within LLMs as features. Furthermore, the hidden states \\(h \\in \\mathbb{R}^{1\\times hdim}\\) of the last token in\nthe last layer of autoregressive LLMs aggregate dense information (Geva et al., 2023; Youssef et al.,\n2024), so we use only these hidden states as features. In the remainder of this paper, we use \"hidden\nstates\" to refer to the hidden states of the last token in the last layer.\nTo determine different types of knowledge editing in a LLM, KETI can be framed as a multi-class\nclassification task. The features are the hidden states, and the classification categories include our\npredefined five editing types and one non-editing type. Therefore, we consider several classic multi\nclass classification methods as identifiers. We then describe baseline identifiers one by one."}, {"title": "Linear Discriminant Analysis (LDA)", "content": "It is a supervised learning algorithm designed to achieve dimensionality reduction and classification by projecting data into a lower-dimensional space, maximizing between-class variance while minimiz ing within-class variance. For each class c, the discriminant function of LDA is:\n\n\\(\\delta_{c}(h) = h^{T} \\Sigma^{-1} \\mu_{c} -\\frac{1}{2} \\mu_{c}^{T} \\Sigma^{-1} \\mu_{c} + log P(y = c)\\)\n\nwhere \\(\\Sigma\\) is the covariance matrix of the features, \\(\\mu_{c}\\) is the mean vector of the features for\nclass c, and \\(P(y = c)\\) is the prior probability of class c. The LDA model finally classifies\nby maximizing the discriminant function: \\(\\hat{y} = arg max \\delta_{c}(h)\\)."}, {"title": "Logistic Regression (LogR)", "content": "It is a statistical method used for binary classification that models the probability of a binary outcome based on one or more predictor variables. We consider the multi-class classification scenario here:\n\n\\(P(y = c|h) = \\frac{exp(h^{T}w_{c} + b_{c})}{1 + \\sum_{c'=1}^{C-1} exp(h^{T}w_{c'} + b_{c'})}\\)\n\nwhere \\(w_{c}\\) and \\(b_{c}\\) are weight and bias respectively. When prediction, for each sample, estimate its probability in each class, and the class with the highest probability is the result:\n\\(\\hat{y} = arg max P(y = c|h)\\)."}, {"title": "Linear Classifier (Linear)", "content": "It learns the weight \\(W \\in \\mathbb{R}^{hidden\\_dim\\times c}\\) and bias b used for classification during training:\n\n\\(y = hW + b\\)"}, {"title": "BASELINES IDENTIFIERS FOR CLOSED-SOURCE LLMS", "content": "The most direct information produced by closed-source LLMs after inference is the output text\n\\(t\\). Additionally, they may also output the top-K log probabilities \\(l = [l_{1},...,l_{n}] \\in \\mathbb{R}^{n \\times K}\\) of n\ntokens. For example, the OpenAI API \u00b9 can return top-20 log probabilities of output tokens. We\ncan therefore use these information as features. To better identify different editing types, we can\nalso use the knowledge prompts \\(P = [p_{1}, ...]\\) as features. Considering that the primary feature of\nclosed-source LLMs is text, we mainly build baseline identifiers based on BERT (Devlin, 2018)."}, {"title": "BERT-text only", "content": "The simplest approach for a BERT-based identifiers is to concatenate the representations of knowledge prompt \\(p\\) and the corresponding output \\(t\\) encoded by BERT, and then use a linear layer for classification. This process can be represented by the following equation:\n\n\\(y = Linear (concat (BERT (p), BERT (t)))\\)\n\nAlthough this identifiers is simple, it does not fully utilize the output data. We thus consider\nthe next identifiers."}, {"title": "BERT+statistical features of log probabilities (SFLP)", "content": "While log probabilities provide more information, they are heterogeneous compared to both the text and text representations, which poses a challenge in effectively utilizing this feature. We consider a straightforward solution: extracting statistical features from the log probabilities, such as max, mean, and standard deviation, and concatenating these statistics with the text representations. Finally, we use a linear layer for classification:\n\n\\(y = Linear (concat (BERT (p), BERT (t), max(l), mean(l), std(l)))\\)"}, {"title": "BERT+LSTM", "content": "Another way to extract features from log probabilities is to use an encoder to map them into a vector space. Considering that the log probabilities of tokens are generated sequentially by LLMs, we use an LSTM to extract features from the log probabilities and concatenate these features with the text representations:\n\n\\(y = Linear (concat (BERT (p), BERT (t), LSTM(l)))\\)"}, {"title": "EXPERIMENTS", "content": null}, {"title": "SELECTION OF KNOWLEDGE EDITING METHODS", "content": "The current knowledge editing methods are mostly aimed at factual knowledge. However, KETI is\nnot limited to factual knowledge; it also includes common sense, ethics, and morality. Therefore,\nwhen selecting editing methods, we choose those that do not rely on factual entities. We choose the\nfollowing knowledge editing methods."}, {"title": "FT-M", "content": "This method is an improved version of FT-L(Meng et al., 2022a; Zhu et al., 2020). It directly trains a single-layer FFN on the editing dataset to store the edited knowledge. During training, cross-entropy loss is used on the object while masking the knowledge prompt."}, {"title": "GRACE", "content": "This method inserts a codebook for storing edited knowl edge between the layers of LLMs. The codebook stores discrete key-value mappings and features a delay mechanism to capture semantically similar inputs."}, {"title": "EXPERIMENTAL SETTINGS", "content": "Since we cannot edit closed-source LLMs, we simulate the scenario of identifying edits in closed\nsource LLMs by editing open-source models and only accessing their output data.\nOur experiments consider LLMs with two different parameter scales: Llama3.1-7B-Instruct (Dubey\net al., 2024) and Llama2-13B-Chat (Touvron et al., 2023). We first use knowledge editing methods\nto edit all knowledge in KETIBench with batch editing settings, except for non-edited knowledge,\ninto the LLMs. The implementation details and results of the knowledge editing are provided in the\nAppendix A.4. After completing all edits, we collect the features of editing queries p generated by\nthe edited models for both the test set and the training set samples, including hidden states, output\ntokens, and log probabilities of output tokens. Finally, we use the features from the training set\nto train the identifiers and test them on the features from the test set. In the results, we report the\nprecision, recall, and F1 score of the identifiers on the test set. Unlike DEED (Youssef et al., 2024),\nwe don't filter out unsuccessfully edited samples for the following two reasons: 1) Current model\nediting methods are not completely reliable; they can only ensure that the majority of knowledge is\nsuccessfully edited. Malicious individuals using editing methods to insert harmful information will\nalso encounter this issue. Not filtering out unsuccessfully edited samples closely resembles real\nworld scenarios. 2) After using editing methods to edit a model, traces are left regardless of whether\nthe edit was successful or not. Identifying the type of edit through these traces is also very useful\nfor preventing some potential risks."}, {"title": "MAIN RESULTS", "content": "We present the Precision, Recall, and F1 scores of all identifiers for identifying the knowledge\nediting types on Llama 3.1-8B and Llama 2-13B, edited by FT-M, GRACE, and UnKE, in Table 1."}, {"title": "CROSS DOMAIN RESULTS", "content": "To investigate the effectiveness of identifiers in cross-domain settings (i.e., assessing the validity of\nknowledge edited by editing method B using a detector trained for knowledge editing method A), we\nselected two identifiers, LogR and BERT+LSTM, which performed the best on open-source LLMs\nand closed-source LLMs respectively, for cross-domain experiments. We show the performance\nvariations of LogR and BERT+LSTM in cross-domain settings compared to non-cross-domain set\ntings in Figure 4. We can observe that in most cases, LogR and BERT+LSTM retain much of their\nperformance in cross-domain settings compared to non-cross-domain settings. In Llama3.1-8B, the\ncross-domain performance in 24 out of 36 cases reached over 60% of the non-cross-domain perfor\nmance. In contrast, in Llama2-13B, the cross-domain performance is even better, with 32 out of\n36 cases reaching over 60%. This implies that it is promising to train a identifier using known\nediting methods to identify edit types from unknown editing methods. We also present the de\ntailed data of the cross-domain experiments and the cases where the detector is directly applied to\nnon-edited models in Table 6 of the Appendix A.5.3."}, {"title": "IN-DEPTH ANALYSIS", "content": "The identification methods for closed-source models are more versatile. We therefore chose to con\nduct an in-depth analysis using identifiers for closed-source models. To further analyze the factors\naffecting the performance of these methods in identifying different knowledge editing types, we\npresent the average confusion matrices for BERT-text only, BERT+SFLP, and BERT+LSTM in Fig\nure 5. We also present details of confusion matrices of identifiers in Appendix A.5.4. In the one\nhand, with the increase in the log probability information of the output tokens, the number of in\ncorrect predictions in the non-edited knowledge by the identifiers decreases (number of incorrect\npredictions: BERT-text only: 34.5, BERT+SFLP: 18.67, BERT+LSTM: 13.83). This is intuitive\nbecause knowledge editing increases the probability of edited knowledge, and the log probabilities\nof the output tokens should carry more information conducive to distinguishing between edited and\nnon-edited knowledge (Youssef et al., 2024). On the other hand, as the amount of log probability"}, {"title": "ABLATION STUDY", "content": "Similar to the cross-domain experiments, we select LogR and BERT+LSTM to respectively inves\ntigate the impact of choosing different layers' hidden states and the number of output tokens on"}, {"title": "DISCUSSION", "content": "Our work highlights the risks posed by malicious edits and introduce the KETI task and KETIBench\nto address these risks. Our preliminary exploration indicates that KETI is promising and provides\nsome useful insights that can guide the development of new identifiers."}, {"title": "Limitations", "content": "Although there are important discoveries revealed by this paper, there are also limita tions. First, we simplified the form of knowledge; in KETIBench, each knowledge sample is pre sented in the form of a query-output pair. However, in reality, the form of knowledge is not limited to query-output pairs and can include formats such as multiple-choice questions and in-context chat. Secondly, our current research only considers two LLMs; larger models like Llama3.1-70B (Dubey et al., 2024) and more series of models like Qwen (Team, 2024) have not yet been considered."}, {"title": "Research Significance & Future Research Directions", "content": "Our research can help mitigate the ethical and moral issues associated with knowledge editing in LLMs. KETI can identify harmful knowl edge edits in LLMs to reduce the social risks posed by malicious individuals conducting harmful edits. In the future, more effective KETI methods can be explored to improve identification perfor mance, such as using sparse logistic regression to extract useful information from token logits for identification (Hu et al., 2024). Besides, for open-source LLMs, another worthwhile direction for further research is how to perform undo-editing to remove harmful edits once they are identified."}, {"title": "CONCLUSION", "content": "We propose the KETI task and KETIBench to confront the risks associated with the misuse of knowl edge editing. KETIBench consists of five types of malicious updates and one type of benign update. The seven baseline identifiers we designed for both open-source and closed-source LLMs achieve decent results on KETIBench. Further analysis and experiments reveal that the performance of the baseline identifiers is not affected by the performance of the editing methods and that they possess a certain degree of cross-domain capability. Our in-depth analysis and ablation experiments indicate that the richness of feature information determines the performance of the identifier. Although the current baseline identifiers perform reasonably well, they are still some distance from accurately identifying edit types. This is crucial for users who rely on LLMs to obtain valuable information, as even a single piece of harmful information could potentially mislead users into inappropriate actions."}, {"title": "PROMPTS FOR GENERATE DATA", "content": "We present the prompts used for generating queries and categories, as well as rephrased queries, in\nTables 2 and 3, respectively."}, {"title": "DATA SAMPLES", "content": "We present samples of KETIBench below, where type \u2018Non-edited\u2019 is not used for editing, and thus only includes the query and the corresponding subtype."}]}