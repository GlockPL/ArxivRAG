{"title": "NER4all or Context is All You Need\nUsing LLMs for low-effort, high-performance NER on historical\ntexts. A humanities informed approach.", "authors": ["Torsten Hiltmann", "Martin Dr\u00f6ge", "Melanie Althagel", "Nicole Dresselhaus", "Till Grallert", "Paul Bayer", "Sophie Eckenstaler", "Koray Mendi", "Jascha Marijn Schmitz", "Philipp Schneider", "Wiebke Sczeponik", "Anica Skibba"], "abstract": "Named entity recognition (NER) is a core task for historical research\nin automatically establishing all references to people, places, events and\nthe like. Yet, do to the high linguistic and genre diversity of sources, only\nlimited canonisation of spellings, the level of required historical domain\nknowledge, and the scarcity of annotated training data, established ap-\nproaches to natural language processing (NLP) have been both extremely\nexpensive and yielded only unsatisfactory results in terms of recall and\nprecision. Our paper introduces a new approach. We demonstrate how\nreadily-available, state-of-the-art LLMs significantly outperform two lead-\ning NLP frameworks, spaCy and flair, for NER in historical documents by\nseven to twentytwo percent higher F1-Scores. Our ablation study shows\nhow providing historical context to the task and a bit of persona modelling\nthat turns focus away from a purely linguistic approach are core to a\nsuccessful prompting strategy. We also demonstrate that, contrary to our\nexpectations, providing increasing numbers of examples in few-shot ap-\nproaches does not improve recall or precision below a threshold of 16-shot.\nIn consequence, our approach democratises access to NER for all historians\nby removing the barrier of scripting languages and computational skills\nrequired for established NLP tools and instead leveraging natural language\nprompts and consumer-grade tools and frontends.", "sections": [{"title": "Plain language summary", "content": "This paper introduces and positively evaluates a new method for named entity\nrecognition (NER), or identifying and classifying references to real-world entities,\nsuch as people or places, in historical texts. NER is a foundational first task\nfor many historical research research questions as it allows us to screen large\nbodies of textual sources for relevant entities of interest. Yet, source corpora for\nhistorical research are commonly highly diverse in language, genre, and structure\nand very different from the modern texts with their highly regular language and\nstandardised orthography. Thus, common NER tools, trained on modern texts,\nperform rather badly for our use case scenario without extensive and expensive\npre-processing, customisation, and retraining of models.\nIn this paper we show how out-of-the-box, commercial large language models\n(LLMs) significantly outperform established frameworks for NER using natural\nlanguage prompts in both German and English. We argue that in order to so,\none has to reconceptualize NER from a purely linguistic task into a humanist\nendeavour that requires some level of domain expertise and aims at activating\nthe vast body of information LLMs have ingested during their training.\nTo test our approach, we created ground truth with manually annotated named\nentities from the 1921 Baedeker travel guide for Berlin and surroundings and eval-\nuated the impact of various strategies for prompting readily available, commercial\nLLMs, namely ChatGPT-40, on identifying (recall) and classifying (precision)\nnamed entities. Prompting strategies comprise the provision of contextual infor-\nmation and persona modelling directing the LLM away from a purely linguistic\napproach to NER, providing increasing numbers of random examples from our\ncorpus (zero, few, and many-shot), and common prompt engineering techniques,\nsuch as reiterating instructions, offering rewards or punishments, and insisting\non slow and thorough thought processes. Finally we compared all results to\nbaselines generated with two leading NER frameworks, flair and spaCy.\nOur results demonstrate that LLMs consequently perform at least on par with\nflair and spaCy and significantly outperform such traditional NER frameworks as\nsoon as a bit of contextual information and persona modelling is included in the\nprompts. We also show that, surprisingly and against our expectations, zero-shot\napproaches, that is prompts without any examples, perform better than few-shot\napproaches until the number of examples reaches 16 and more. Examples require\nannotation from domain experts and are thus expensive. Including them in\nthe prompt also increases the necessary context-window, which requires more\ncomputing power and has to be paid for for when using commercial LLMs,\nas we did. Finally, we could show that although initially English performs\nbetter than German as a prompting language in terms of recall, this difference\nbecomes insignificant when we incorporated our rather simple prompt engineering\ntechniques."}, {"title": "Conflict of interest statement", "content": "Competing interests: The author(s) declare none"}, {"title": "Introduction", "content": "The digital has brought in a new era of abundance for historical research and\nwith it new challenges in order to make sense of vast amounts of highly diverse\nand unstructured data encapsulated in digitised and increasingly born-digital-\ncultural artifacts (c.f. Rosenzweig 2003). Named entity recognition (NER),\nor the ability to automatically identify objects of interest for the historian, is\none of the fundamental computational approaches for information extraction.\nYet, our approaches to NER have hitherto been adopted from the domains of\nnatural language processing (NLP) and computational linguistics. Existing tools\nwork best with homogenous corpora of normalized texts in modern languages\nand come with steep learning curves for humanists without prior computing\nknowledge. Adapting and applying them to the specific challenges of historical\ncorpora, requires prohibitive expenditures in effort, time, and money.\nThe recent hype around Large Language Models (LLM) has led to the expecta-\ntion that they will solve historical NER with aplomb, but so far the performance\nof LLM is still significantly lower than established machine-learning approaches,\nsuch as transformer architectures (Keraghel, Morbieu, and Nadif 2024). In this\npaper we show demonstrate how rethinking prompting strategies and fundamen-\ntally reconceptualising NER from a linguistic task to a humanities-focused task\nwith contextual information and appropriate persona modelling, allows us to\nsignificantly outperform state-of-the-art NLP tools such as spaCy and even flair\nin recall and precision using common foundation-models and 0-shot prompts\nwithout further training or fine-tuning and without creating additional data. We\nthus fundamentally remove barriers and democratize access to highly perfor-\nmant NER on heterogeneous corpora of very specific historical and frequently\nlow-resourced texts."}, {"title": "The challenge of NER for historical texts", "content": "Named entities are references to distinct and uniquely nameable or designatable\nconcepts that can be either concrete or abstract (Ehrmann 2008). Named\nEntity Recognition (NER) is a Natural Language Processing (NLP) task to\ndetect these references in unstructured text data and classify them according\nto predefined categories, such as people, places or organizations. NER is thus\na token classification task with a desired result like the following: t1, t2: PER\n(person), t3: PER (person), t4 ... t9, t10: LOC (location), t11: LOC (location),\nt12, ... However, the disambiguation or identification of these named entities\nis not part of the NER task itself, but represents two subsequent processing\nsteps(NE disambiguation and linking).\nWhile some domains operate with relatively homogeneous text data, such as\nscientific publications, patient reports, or annual economic reports, which are\nwritten in modern standardized language and often follow a common internal\nstructure, resulting in effective NER performance, this is not the case for the\nhistorical humanities. Historical research is characterized by texts defined by their\ndiversity in form and content, presenting a significant challenge for NLP-tasks.\nThis diversity may include:\n\u2022 Language"}, {"title": "Corpus and data set", "content": "Since our goal was to test this LLM-based approach to NER under real-world\nconditions and in actual application scenarios and as the approach required\nsome coherent historical corpus in order to provide clearly specified contextual\ninformation, we settled on a text from our own historical domain expertise\ninstead of the usual benchmark texts. We chose the 19th edition of the Baedeker\ntravel guide for Berlin and surroundings (Baedeker and Graupe 1921) because it\nhas a well-defined historical context (information about traveling and sightseeing\nin Berlin around 1920), partly uses period-specific language, and also includes an\ninteresting variety of structural features from narrative sections to comprehensive\nlists, such as stops on different bus routes. Travel guides are a good genre to\nbenchmark NER performance because of the density and variety of named\nentities mentioned therein\nThe book was scanned and OCR'd with OCR4all (Reul et al. 2019), which acts\nas a service wrapping various tools for layout and text recognition into highly\ncustomizable workflows. We trained our own text recognition model based on\nthe \"idiotikon\" Calamari model from the Schweizerisches Idiotikon (Reul and\nWick 2021; Wick, Reul, and Puppe 2020) for 29 randomly selected pages (12.55%\nof the entire corpus), which resulted in a character error rate (CER) of 0.0064.\nPreprocessing for NER was limited to removing line breaks and hyphenation and\nto normalizing of whitespace. Texts were then tokenized with spaCy (Honnibal\net al. 2020).\nThe ground truth for NER was produced by manually annotating 55 randomly\nselected pages from our corpus with named entities using INCEPTION (Klie et\nal. 2018) following a set of guidelines specifically developed for this corpus and\nbased on the NER guidelines from the Impresso project (Ehrmann et al. 2020).\nEach text was independently annotated by two team members. Differences\nbetween annotations were reconciled based on a third opinion.\nWe limited the types of entities to person (PER), organisation (ORG), and places\n(LOC). Nested annotations were not allowed. PER includes living people, histor-\nical and mythical figures, as well as collectives dependent on their individual\nmembers, such as the art collective \u201cDie Br\u00fccke\". Role names, salutations, and\naddresses, such as \"geh[eimer] Regierungsrat\", were not included in the PER\ntag. ORG differs from the kind of collectives encoded as PER in being limited to\nincorporated collectives, such as companies, public institutions, associations, etc.\nLOC designates geographic locations. Given that most incorporated collectives do\nhave a physical location, consider, for instance, a restaurant or even a city, our\nannotation as either ORG or LOC depends on the context. If the text foregrounds\nthe locality, such as in the case of directions, we opted for LOC. All other instances\nwere annotated as ORG. Out of this dataset we used 25 random pages for the\nevaluation (in prodigy-format).\""}, {"title": "Method", "content": "In this section, we describe our methodology for leveraging LLMs to generate,\nevaluate, and refine named-entity annotations. We focus on the technique of"}, {"title": "prompt engineering", "content": "prompt engineering (Bsharat, Myrzakhan, and Shen 2024) to achieve better\nresults instead of optimizing training-datasets and fine-tuning or retraining\nestablished approaches. This is due to the prohibitive cost of the latter approaches\nand the performance of the prompt-engineering approach as demonstrated below.\nBecause there are a lot of combinations for different prompting techniques,\nwe chose the best combination that worked during experimentation without a\ncomprehensive study. We then performed an ablation study on this prompt to\nmeasure how much each component impacts the overall performance-i.e. what\nis the relative importance of the prompt's parts for the observable output.\nThis is a very recent, radical changing field. While additionally having slightly\ndifferent effects on different LLMs, our experiments and approach can only be\nbased on the current state in an evolving environment. More than any exact\ndetail, we want to emphasise our main thesis that having domain and input\nspecific contextual information plays an important role. We do not propose a\ndefinite set of instructions on what to include and what not as gold standard.\nFinally, we tested the impact of different prompting languages, namely English\nand German.\nIn the following sections, we detail our chosen annotation format, which adopts a\nspan-based, tag-like structure inspired by prior research (e.g. Wang et al. 2023).\nNext, we outline the instructions we provide to the model-highlighting key\nchallenges such as maintaining fidelity to the original text, properly handling\nnested entities, and addressing frequent labeling errors. We then discuss our\nmatching strategy, which employs fuzzy search techniques to robustly locate and\ncompare generated spans with their corresponding ground truth. To evaluate\nperformance, we measure precision, recall, and F\u2081-Scores against two established\nNER pipelines (flair (Akbik et al. 2019) and spaCy (Honnibal et al. 2020)),\nemphasizing prompt engineering over dataset optimization or model retraining.\nWe conclude by examining how one-shot, few-shot, and many-shot examples affect\nannotation quality, and by introducing the LLMs used in our experiments, notably\n\"gpt4o-2024-08-16\" (OpenAI 2024). Throughout, we stress the importance of\nincluding domain-specific context and adapting prompts to an evolving LLM\nlandscape."}, {"title": "Choice of target annotations", "content": "Similar to the approach of (Wang et al. 2023), we annotate complete entity-spans,\nwhile also keeping in mind that it should be easily reproducible by a generating\nLLM. Depending on the token-embedding used we often saw generation of spaces\nat the end of a word, problems of properly opening and closing of the tags, etc.,\neven if the prompt instructed the generation of a semantic tag for each word. As\nsoon as we switched over to tag-like annotation of spans of characters all these\nproblems nearly vanished even on smaller nets with smaller or non-complete\nembeddings. Also we do not need to \"waste\" much of the prompt on fixing\nthe output-format. We suspect that this is due to a high amount of similar\nstructured text like code, html or math in the training data.\nWe finally settled on <<TAG word word word /TAG>> as a way to annotate\npossibly nested spans and made all future experiments with this format. Yan,"}, {"title": "Instructing the LLM", "content": "The final prompt (in both German and English) comprises multiple components:\n1. A general introduction to the input text to be analyzed, with short,\nexpert-level contextual information.\n2. The main task and instructions on how to mark named entities in the\ntext.\n3. Some examples of input and expected output while making sure that all\nrecognizable classes are included in decent amounts as Brown et al. (2020)\ndemonstrated their significance for improving output quality.\n4. A reiteration of the instructions in different wording and pointing out\nhow in all examples these instructions were followed to the letter\n5. Common tricks (Federiakin et al. 2024) to stress the importance of cor-\nrectness and offer monetary reward for each successful classification\n6. The seemingly important empty phrase \"take a deep breath and think\nstep by step\"\nFollow these steps:\nUse tags with corresponding rules:\nWhen a section begins, mark it with a category tag. Available\ncategories include '<<PER' for people, '<<LOC' for locations,\nand '<<ORG' for organizations.\nWhen a section ends, mark the end with the corresponding tag,\ni.e., /PER>>', /LOC>>', or /ORG>>'.\n3.  Repeat the given text exactly. Be very careful to ensure that\nnothing is added or removed apart from the annotations.\nYour task is to categorize sections into predetermined categories.\nBe sure to close each opened tag.\nTags may overlap or be nested, but it is unlikely that this will\nhappen frequently.\nNOTE: Even Greek heroes and gods are named entities and should be\nannotated as people (<<PER).\nNOTE: Restaurants are organizations (<<ORG) regardless of whether\nthe name can be interpreted as a person without context.\nNOTE: Locations (<<LOC) within quoted entities should also be\nannotated. For example, \"Pfade des Aves durch Garetien\" would become\n\"Pfade des <<PER Aves /PER>> durch <<LOC Garetien /LOC>>\"."}, {"title": "Matching spans through post-processing the results", "content": "Due to the heuristic nature and only approximative generation of LLMs, the\noutput is often not exactly reproducible. We especially noticed that the LLM\ntends to correct \"mistakes\" in the source format or change the hyphenation of\nentities. But this also enables the LLM to become closer to a real annotator,\nwho has similar freedoms. Due to this we extracted the annotated spans and\nsearched for them in the source material with a fuzzy search provided by the\npython-library fuzzysearch. We specifically used find_near_matches(span,\ntext, len(span) // 5) to allow for up to 1 error every 5 generated characters,\nwhich corresponds to a Levenshtein-Distance of n for a string at least length\n5n (Levenshtein 1966). With this we had no errors in locating the spans in the\nsource-material.\nFor comparing if the same span is present in the generated annotation and the\nground truth, we employed the library nervaluate (Batista and Upson [2019]\n2020) to match the extracted spans. We used the most lenient criteria for NER\nmatching ent_type, meaning it suffices to have an overlap with the annotation\nand having the correct entity-type annotated (Segura-Bedmar, Mart\u00ednez, and\nHerrero-Zazo 2013). This also yielded an average precision/recall/F1-Score for\nthe whole corpus as well as every single page."}, {"title": "Selection of LLMs", "content": "For this paper, we focussed on the gpt40-2024-08-06 model (OpenAI 2024)\nwith no retraining or more than a black-box access to the API.2"}, {"title": "Evaluating the performance of prompt engineering", "content": "To measure the performance of our approach, we compare the results of itera-\ntive prompts and their various components to two off-the-shelf NLP libraries\ncommonly used for NER: flair (Akbik et al. 2019) with its ner-german-large\nmodel (Schweter and Akbik 2021), which has a reported F1-Score of 92.31 on\nthe CoNLL-03 German revised benchmark (\u201c\"State-of-the-Art Models\" [2018]\n2025), and spaCy (Honnibal et al. 2020) using de_core_news_lg, for which\nspaCy claims a self-reported F\u2081-Score of 85 for the NER-Task after training\n(\"De_core_news_lg\" 2023)."}, {"title": "Establishing baselines", "content": "Flair is straight-forward to implement. The German language model offers the\nentity classes we needed (plus an additional MISC-class). Results were also within\nour expectations across all classes. Due to the model being trained on modern\ntexts, we expected not to achieve the reported F1-Score of 92 with historical\nsources (Our corpus resulted in 0.76 recall; 0.89 precision; F1-Score of 0.81, see\ntbl. 1).\nThe baseline of spaCy on the other hand was surprisingly disappointing. While\nhaving a recall of 0.82 in the LOC-Class and 0.76 in the PER-Class, spaCy\ncompletely failed to recognize most of the ORG (recall only 0.12). This is a\nknown issue with ORGs that are not in the original training-set are rarely found.\nAdditionally spaCy found 684 sequences as \"named entities\" that were not\nannotated as such (while the whole corpus only had 1490 annotations in total)\nyielding an unsatisfactory precision. Over all classes this only yielded mediocre\nresults (Our corpus resulted in 0.59 recall; 0.44 precision; F1-Score of 0.50, see\ntbl. 1)."}, {"title": "Results", "content": "We expected basic prompts without any context or more advanced prompt-\nengineering to perform worse than a current large German model for flair. But\neven with this simple approach, modern sophisticated LLMs are able to at least\nperform on par with state-of-the-art specialist NLP tools (recall 0.77 (ours)\nvs. 0.76 (flair); precision 0.91 (ours) vs. 0.89 (flair); F1 0.83 (ours) vs 0.81 (flair)).\nThis already yields good precision values. However, it is important to emphasize\nthat historians are particularly interested in high recall in order to capture as"}, {"title": "Basic-Prompts", "content": "We expector basic prompts without any context or more advanced prompt-engineering to perform worse than a current large German model for flair. But even with this simple approach, modern sophisticated LLMs are able to at least perform on par with state-of-the-art specialist NLP tools (recall 0.77 (ours) vs. 0.76 (flair); precision 0.91 (ours) vs. 0.89 (flair); F1 0.83 (ours) vs 0.81 (flair)).This already yields good precision values. However, it is important to emphasize that historians are particularly interested in high recall in order to capture as"}, {"title": "Context (0-Shot)", "content": "To analyze our main theory that context is core for unlocking the highest\nperformance we compared the 0-shot prompt with different specifications of\ncontext.\n\u2022 In the \"No Context\"-case the instructions were merely information on how\nto annotate the data. No specific prompt-engineering techniques (like in\nthe ablation study) were used.\n\u2022 The \"Generic Context\" takes a generic historic-persona (\"As an experienced\nlinguist specializing in Named Entity Recognition (NER) for historical\ntexts and expert in various historical epochs and locations, your task is to\nannotate a given text for machine reusability.\").\n\u2022 The \"Specific Context\" then also adds details on the specific material and\nwhat to expect (similar to the first paragraph of \"Corpus and data set\").\n\u2022 The \"Full Prompt\" integrates all the above, combined with several opti-\nmization techniques for prompting.\n\u2022 Finally, we tested the prompts in both German and English (mentioned in\nbrackets below).\n\u2022 We report mean and standard deviation, as the annotation was done\npagewise."}, {"title": "Including examples: 1-shot, few-shot, many-shot", "content": "Another way to improve the results and provide more context is through one-\nshot or few-shot learning. Here, in addition to instructions in the sense of the\n\"full prompt\" mentioned earlier, we also include examples of the desired results.\nBrown et al. (2020) demonstrated that output quality improves significantly\nwhen prompts integrate examples. We consequently explore the impact of such\nexamples. This approach allows us to measure the impact of few-shots on the\noutcomes. At the same time, it addresses the question of whether the added\nvalue of preparing such examples is worthwhile at all.\nEvery example consists of small random excerpts of around 200-500 characters\nfrom the corpus that were not in the annotation set. This is drastically lower\nthan the whole page the model has to annotate in the experiments, which\nare 4000-9000 characters long. However, each sample needed to be manually\nannotated, making the process rather labor-itensive. Nguyen and Wong (2023)\nhave shown that prompts are sensitive to choosing the right examples. To control\nfor the impact of implicit domain knowledge, we chose random examples ensuring\nan equal distribution of classes.\nWe incrementally increased the number of shots by powers of two from 2^1 to a\nmaximum of 2^5. The maximum amount tested was 32-shot, with all classes\nbeing balanced. In all other cases they were randomly chosen once from those 32.\nThis also has the side-effect of simulating impacts from non-optimized examples\nas opposed to specifically optimizing these for the given prompt with the risk of\noverfitting. Liu et al. (2021) describe the impact of such optimizations in detail."}, {"title": "Prompting language", "content": "Throughout our initial experiments we had prompted the LLMs in German and\nexpected a significant improvement of performance when switching to English\nas the language for instructions and contextual information (though not for the\nexamples). This expectation was confirmed and yielded a 5% improvement in\nrecall. The difference in precision was much lower. This discrepancy can be nearly\nmitigated in German-language prompts through prompt optimization techniques\n(tbl. 2). To our surprise we noticed that prompt-engineering is only relevant\nin a meaningful way for German instructions, as the performance for English\ninstructions did not change considerably with or without prompt-engineering. In\nconsequence, and most importantly, the differences between the performance of\nthe two prompting languages using our full prompts were insignificant. This will\nrequire further testing with other languages but it certainly opens opportunities\nfor a broader application of our approach and to democratize access to such\ntechniques for non-English speaking communities."}, {"title": "Ablation Study", "content": "In the ablation study, we enable and disable specific features of our prompt in\norder to measure their impact on the overall outcome. The ablation study was\nconducted on the German prompt only.\nWe first did the ablation study on the 32-shot prompt, but all results were\npretty similar due to the example-block dominating the whole prompt. With\nso many examples using a multiple of tokens compared to all instructions, the\nablation-effect we want to measure here is basically invisible. Therefore we\nswitched to 0-shot for this analysis.\nThe parts added and reviewed individually were the following:\n\u2022 only structure: addition of markdown-headings/separations of areas\n\u2022 only instruction-repetition: addition of the short summary of instruc-\ntions at the end\n\u2022 only bullying: offering of reward/punishment for correct/wrong annota-\ntions\n\u2022 only system-prompt: providing the input as two distinct messages\nmarked \"system\" containing the prompt and \"user\" containing the data\nversus putting everything in one \"user\"-message\nFor comparison we added selected results from the previous tables regarding\ndifferent context levels (in italics) as well as the baseline results. \"PE\" here refers\nto using all techniques, otherwise none other than the mentioned one was used.\nAs zero for our benchmark we set a prompt with specific contextual information\nbut no prompt-engineering techniques."}, {"title": "Discussion", "content": "Our results show that while advanced prompting strategies improve scores, their\nimpact is limited, typically increasing performance by only 1-2% or (as in our\ncase) negating a difference in the instruction language. Similarly, the number of\nexamples in few-shot learning has shown some more substantial impact only if\nthere are 16 or more shots, while creating appropriate examples is labor-intensive\nand requires expert knowledge to avoid bias and ensure comprehensive coverage.\nIn contrast, providing explicit and detailed context has a much greater impact.\nJust providing general context, as has been the norm, improves recall by 6%.\nHowever, when specific, detailed context is provided, it leads to further, significant\nimprovement of the results by another 5%. Which in the end, sets our approach\nin terms of retrieval 10% above flair and 15% above spaCy, in terms of F1-Score\n7% above flair and 22% above spaCy.\nThe results clearly demonstrate that instead of relying solely on the model's\ncapabilities as a language model in handling language (NLP), the outcomes\nimprove significantly when the task is defined as a content-based one, thereby\nincorporating the domain knowledge represented within the model through\nappropriate prompting-while acknowledging that this represents only a limited\nperspective of the world. In doing so, the model can surpass human capabilities\nin certain areas.\nTo cite an example for this: when annotating the sequence \"Borstells Lesezirkel,\" human annotators labeled \u201cBorstells\u201d as a person (PER), whereas the model"}, {"title": "Transferability and future work", "content": "In terms of transferability, preliminary tests on documents from the 16th to the\n18th centuries from a variety of genres suggest that our methodology can be\napplied to texts from different historical periods and backgrounds with clear\nimprovements over the respective baselines of Flair and spaCy, although further\ntesting is needed to fully understand the scope of applicability of this approach\nto a wider range of historical documents and use cases.\nIn particular, since our approach may also provide a way to annotate unique\nclasses without first having to manually generate sufficient training data and"}, {"title": "Conclusion", "content": "In our study, we have explored the use of LLMs for NER in historical and\nlow-resource texts through humanities-informed approaches, demonstrating a\nsignificant improvement in performance compared to traditional methods as\nthey are implemented state-of-the-art NLP-frameworks like flair or spaCy. The\nresults indicate that incorporating specific contextual information into prompts\nis fundamental for achieving high accuracy in NER tasks.\nThe integration of domain knowledge through humanities-informed prompting has\nshown to significantly enhance LLM performance in NER tasks for historical texts.\nThis method not only facilitates more efficient analysis but also brings us closer\nto replicating the expertise traditionally required by human annotators, which\ncould revolutionize how we approach textual analysis within digital humanities\nand beyond."}, {"title": "Acknowledgements", "content": "Use of artificial intelligence (AI) tools As non-native speakers of English, some of our co-authors edited parts of their contributions for linguistic and idiomatic clarity. To this end, draft passages were submitted to ChatGPT-40 (OpenAI 2024) and DeepL Write. During this process, all suggested changes were individually verified and accepted or rejected accordingly. The text was then finalised and edited without further resort to AI tools."}]}