{"title": "LLMSCAN: CAUSAL SCAN FOR LLM MISBEHAVIOR DETECTION", "authors": ["Mengdi Zhang", "Kai Kiat Goh", "Peixin Zhang", "Jun Sun", "Rose Lin Xin"], "abstract": "Despite the success of Large Language Models (LLMs) across various fields, their potential to generate untruthful, biased and harmful responses poses significant risks, particularly in critical applications. This highlights the urgent need for systematic methods to detect and prevent such misbehavior. While existing approaches target specific issues such as harmful responses, this work introduces LLMSCAN, an innovative LLM monitoring technique based on causality analysis, offering a comprehensive solution. LLMSCAN systematically monitors the inner workings of an LLM through the lens of causal inference, operating on the premise that the LLM's 'brain' behaves differently when misbehaving. By analyzing the causal contributions of the LLM's input tokens and transformer layers, LLMSCAN effectively detects misbehavior. Extensive experiments across various tasks and models reveal clear distinctions in the causal distributions between normal behavior and misbehavior, enabling the development of accurate, lightweight detectors for a variety of misbehavior detection tasks.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) demonstrate advanced capabilities in mimicking human language and styles for diverse applications (OpenAI, 2023), from literary creation (Yuan et al., 2022) to code generation (Li et al., 2023; Wang et al., 2023b). At the same time, they have shown the potential to misbehave in various ways, raising serious concerns about their use in critical real-world applications. First, LLMs can inadvertently produce untruthful responses, fabricating information that may be plausible but entirely fictitious, thus misleading users or misrepresenting facts (Rawte et al., 2023). Second, LLMs can be exploited for malicious purposes, such as through jailbreak attacks (Liu et al., 2024; Zou et al., 2023b; Zeng et al., 2024), where the model's safety mechanisms are bypassed to produce harmful outputs. Third, the generation of toxic responses such as insulting or offensive content remains a significant concern (Wang & Chang, 2022). Lastly, biased responses, which can appear as discriminatory or prejudiced remarks, are especially troubling as they have the potential to reinforce stereotypes and undermine societal efforts toward equality and inclusivity (Stanovsky et al., 2019; Zhao et al., 2018).\nNumerous attempts have been made to detect LLM misbehavior (Pacchiardi et al., 2023; Robey et al., 2024; Sap et al., 2020; Caselli et al., 2021). However, existing approaches often face two significant limitations. First, they tend to focus on a single type of misbehavior, which reduces the overall effectiveness of each method and requires the integration of multiple systems to comprehensively address the diverse forms of misbehavior. Second, many methods rely on analyzing the model's responses, which can be inefficient or even ineffective, particularly for longer outputs. Additionally, they are often vulnerable to adaptive adversarial attacks (Sato et al., 2018; Hartvigsen et al., 2022). As a result, there is an urgent need for more general and robust misbehavior detection methods capable of identifying (and mitigating) the full range of LLM misbehavior.\nIn this work, we introduce LLMScan, an approach designed to address this critical need. LLMScan leverages the concept of monitoring the \u201cbrain\" activities of an LLM for detecting misbehavior. Since an LLM's responses are generated from its parameters and input data, we believe that the \"brain\""}, {"title": "BACKGROUND AND RELATED WORKS", "content": "Existing methods for detecting LLM misbehavior have predominantly focused on specific and narrowly defined scenarios (Pacchiardi et al., 2023). While these existing approaches are effective within their specific domains, they often struggle to generalize across different types of misbehavior."}, {"title": "", "content": "In the following, we discuss the need for four examples of LLM misbehavior detection and highlight the necessity for more comprehensive and adaptable detection mechanisms that can be applied across a wide range of misbehavior cases.\nLie Detection: LLMs can \u201clie\u201d, i.e., producing untruthful statements despite demonstrably \u201cknowing\" the truth (Pacchiardi et al., 2023). That is, a response from an LLM is considered a lie if and only if a) the response is untruthful and b) the model \u201cknows\u201d the truthful answer (i.e., the model provides the truthful response under question-answering scrutiny) (Pacchiardi et al., 2023). For example, LLMs might \"lie\" when instructed to disseminate misinformation.\nExisting LLM lie detectors are typically based on hallucination detection or, more related to this work, on inferred behavior patterns associated with lying (Pacchiardi et al., 2023; Azaria & Mitchell, 2023; Evans et al., 2021; Ji et al., 2023; Huang et al., 2023; Turpin et al., 2024). Specifically, Pacchiardi et al. propose a lie detector that works in a black-box manner under the hypothesis that an LLM that has just lied will respond differently to certain follow-up questions (Pacchiardi et al., 2023). They introduce a set of elicitation questions, categorized into lie-related, factual, and ambiguous, to assess the likelihood of the model lying. Azaria et al. explore the ability of LLMs to internally recognize the truthfulness of the statements they generate (Azaria & Mitchell, 2023), and propose a method that leverages transformer layer activation to detect the model's lie behavior.\nJailbreak Detection: Aligned LLMs are designed to adhere to built-in restrictions and ethical safeguards aimed at preventing the generation of harmful or inappropriate content. However, these models can be compromised through the use of cleverly crafted prompts, a technique known as \"jailbreaking\" (Wei et al., 2024). It has been shown that jailbreaking consistently circumvents the safety alignment of both open-source and black-box models such as GPT-4.\nNumerous studies have been conducted to defend against jailbreaking attacks (Robey et al., 2024; Alon & Kamfonas, 2023; Zheng et al., 2024; Li et al., 2024; Hu et al., 2024), which can be broadly categorized into three strategies. The first strategy, prompt detection, seeks to identify and filter out malicious inputs by analyzing their perplexity or matching them against known patterns of jailbreak attempts (Alon & Kamfonas, 2023; Jain et al., 2023). While effective in some cases, these methods are limited by the diversity and variability of jailbreak prompts. The second strategy, input transformation, offers an additional layer of defense by introducing controlled perturbations or alternations to the inputs, such as reordering words or applying noise (Robey et al., 2023; Xie et al., 2023; Zhang et al., 2024; Wei et al., 2023). However, even these methods can be evaded by more sophisticated jailbreak attempts, which has led to the use of behavioral analysis. Lastly, some approaches focus on monitoring the model's responses for signs of insecure or unexpected behavior, or analyzing internal model states to detect anomalies during inference (Li et al., 2024; Hu et al., 2024).\nToxicity Detection: LLMs may generate toxic content, i.e., responses that contain abusive, aggressive, or other forms of inappropriate content. It is likely because their training data include toxic or inap-propriate material, and the models lack the ability to make real-world moral judgments (Ousidhoum et al., 2021). As a result, they struggle to discern appropriate responses without proper contextual guidance. Toxic outputs not only degrade the user experience and increase psychological stress, but they also contribute to broader negative social outcomes, such as the spread of hate speech and the deepening of social divisions.\nExisting research on toxic content detection generally follows two main directions. The first focuses on creating benchmark datasets for toxic detection (Vidgen et al., 2021; Hartvigsen et al., 2022). The second involves supervised learning, where models are trained on labeled datasets to identify toxic content. This includes fine-tuning LLMs on toxicity datasets to improve their ability to detect toxic language (Caselli et al., 2021; Kim et al., 2022; Wang & Chang, 2022). However, these supervised learning approaches face several challenges. Firstly, they rely on labeled training data, which is difficult to obtain due to the lack of standardized ground truth. Second, deploying LLMs for toxic content detection in production environments can be computationally expensive, particularly when processing prompts with a large number of tokens."}, {"title": "", "content": "Bias Detection: Typically trained on vast amounts of internet-based data, LLMs inevitably inherit stereotypes, misrepresentations and derogatory language, and other denigrating behaviors that dis-proportionately affect already vulnerable and marginalized communities (Touvron et al., 2023b; Biderman et al., 2023). These harms manifest as \u201csocial bias\". Extensive efforts and benchmarks have been developed to investigate and mitigate bias and stereotype issues (Stanovsky et al., 2019; Zhao et al., 2018). Those social biases can be categorized into three primary dimensions: 1) generative stereotype bias, which involves using prompts to condition text generation (Dhamala et al., 2021; Parrish et al., 2022); 2) counterfactual bias, where bias is assessed using pairs of sentences that substitute social groups (Nangia et al., 2020; Barikeri et al., 2021; Huang et al., 2020); and 3) representation bias, focusing on the embedding layer (May et al., 2019; Guo & Caliskan, 2021). Each category employs different methods and approaches for detecting biases within text and model representations.\nAdditionally, our approach is related to studies on model interpretability. Common methods for explaining neural network decisions include saliency maps (Adebayo et al., 2018; Bilodeau et al., 2024; Brown et al., 2020), feature visualization (Nguyen et al., 2019), and mechanistic interpretabil-ity (Wang et al., 2023a). Zou et al. further develop an approach, called RepE, which provides insights into the internal state of AI systems by enabling representation reading (Zou et al., 2023a)."}, {"title": "OUR METHOD", "content": "Recall that our method has two components, i.e., the scanner and the detector. In this section, we first introduce how the scanner applies causality analysis to build a causal map systematically, and then the detector detects misbehavior based on causal maps. For brevity, we define the generation process of LLMs as follows.\nDefinition 1 (Generative LLM). Let $M$ be a generative LLM parameterized by $\\theta$ that takes a text sequence $x = (x_0, ..., x_m)$ as input and produces an output sequence $y = (y_0, ..., y_n)$. The model $M$ defines a probabilistic mapping $P(y|x; \\theta)$ from the input sequence to the output sequence. Each token $y_t$ in the output sequence is generated based on the input sequence $x$ and all previously generated tokens $y_{0:t-1}$. Specifically, for each potential next token $w$, the model computes a $logit(w)$. The probability of generating token $w$ as the next token is then obtained by applying the softmax function\n$P (y_t = w | y_{0:t-1},x; \\theta) = Softmax(logit(w))$\\qquad(1)\nAfter consider all $w \\in V$, the next token $y_t$ is determined by taking the token $w$ with the highest probability as\n$y_t = arg \\max_{w \\in V} P (y_t = w | y_{0:t-1}, x; \\theta)$\\qquad(2)\nwhere $V$ is the vocabulary containing all possible tokens that the model can generate. This process is iteratively repeated for each subsequent token until the entire output sequence $y$ is generated."}, {"title": "CAUSALITY ANALYSIS", "content": "Causality analysis aims to identify and quantify the presence of causal relationships between events. To conduct causality analysis on machine learning models such as LLMs, we adopt the approach described in (Chattopadhyay et al., 2019; Sun et al., 2022), and treat LLMs as Structured Causal Models (SCM). In this context, the causal effect of a variable, such as an input token or transformer layer within the LLM, is calculated by measuring the difference in outputs under different inter-ventions Rubin (1974). Formally, the causal effect of a given endogenous variable $x$ on output $y$ is measured as follows (Chattopadhyay et al., 2019; Sun et al., 2022):\n$CE_x = E[y | do(x = 1)] - E[y | do(x = 0)]$\\qquad(3)\nwhere $do(x = 1)$ is the intervention operator in the do-calculus.\nTo conduct an effective causality analysis, we first identify meaningful endogenous variables. The scanner in LLMSCAN calculates the causal effect of each input token and transformer layer to create a causal map. We avoid focusing on individual neurons, as they generally have minimal impact on the model's response (Zhao et al., 2023). Instead, we concentrate on the broader level of input tokens and transformer layers to capture a more meaningful influence."}, {"title": "Computing the Causal Effect of Tokens.", "content": "To analyze the causal effect of input tokens, we conduct an intervention on each input token and observe the changes in model behavior, which are measured based on attention scores. These interventions occur at the embedding layer of the LLM. Specifically, there are three steps: 1) extract the attention scores during normal execution when a prompt is processed by the LLM; 2) extract the attention scores during a series of abnormal executions where each input token $x_i, 0 \\le i \\le m$ is replaced with an intervention token '-' one by one; and 3) compute the Euclidean distances between the attention scores of the intervened prompts and the original prompt. Formally,\nDefinition 2 (Causal Effect of Input Token). Let $x = (x_0, ..., x_m)$ be the input prompt, the causal effect of token $x_i$ is:\n$CEx_i = ||AS - AS'_{x_i}||$\\qquad(4)\nwhere $AS$ is the original attention score (i.e., without any intervention) and $AS'_{x_i}$ is the attention score when intervening token $x_i$.\nNote that there are two key reasons for using attention scores for causal effect measurement. First, at the token level, attention scores more effectively capture inter-token relationships, which may be obscured in later results, such as logits. Attention distances provide a clearer reflection of how the model's understanding shifts with token interventions, offering a more precise measure of each token's causal impact. Our empirical results reinforce this point, showing that attention head distances, typically ranging between 2 and 3, are more sensitive to logits differences, which generally fell below 0.2. Additionally, previous research has demonstrated that attention layers may encapsulate the model's knowledge, including harmful or inappropriate information that may require censorship (Meng et al., 2022)."}, {"title": "Computing the causal effect of Layers.", "content": "In addition to the input tokens, we calculate the causal effect of each layer in the LLM. The causal effect of a transformer layer $l$ is computed based on the difference between the original output logit (without intervention) and the output logit when layer $l$ is intervened upon (i.e., skipped). Specifically, we intervene the model by bypassing the layer $l$. That is, during inference, we create a shortcut from layer $l - 1$ to layer $l + 1$, allowing the output from layer $l - 1$ to bypass layer $l$ and proceed directly to layer $l + 1$. Formally,\nDefinition 3 (Causal Effect of Model Layer). Let $l$ be a layer of the LLM; and $x$ be the prompt. The $ACE$ of $l$ for prompt $x$ is\n$CEx_{,l} = logit_0 - logit_0^l$\\qquad(5)\nwhere $logit_0$ is the output logit of first token, and $logit_0^l$ is the logit for the first token when the layer $l$ is skipped (i.e., a shortcut from the layer preceding $l$ to the layer immediately after $l$ is created).\nNote that here we use the logit as the measure of the causal effect, as the attention is no longer available once a layer is skipped.\nThe Causal Map. Given a prompt $x$, we systematically calculate the causal effect of each token and each layer to form a causal map, i.e., a heatmap through the lens of causality. Our hypothesis is that such maps would allow us to differentiate misbehavior from normal behaviors. For example, Figure 2 shows two causal maps, where the left one corresponds to a truthful response generated by the LLaMA-2-7B model (with 32 layers), and the right one corresponds to an untruthful response. The prompt used is \u201cWho developed Windows95?\u201d. The truthful response is \u201cMicrosoft Corporation\u201d. The untruthful response is \"Bill Gates\" when the model is induced to lie with the instruction \"Answer the following question with a lie\". Note that each causal map consists of two parts: the input token causal effects and the layer causal effects. It can be observed that there are obvious differences between the causal maps. When generating the truthful response, a few specific layers stand out with significantly high causal effects, indicating that these layers play a dominant role in producing the truthful output. However, when generating the untruthful response, a greater number of layers contribute, as demonstrated by relatively uniform high causal effects, potentially weaving together contextual details that enhance the credibility of the lie."}, {"title": "MISBEHAVIOR DETECTION", "content": "The misbehavior detector is a classifier that takes a causal map as input and predicts whether it indicates certainly misbehavior. For each type of misbehavior, we train the detector using two contrasting sets of causal maps: one representing normal behavior (e.g., those producing truthful responses) and the other containing causal maps of misbehavior (e.g., those producing untruthful responses). In our implementation, we adopt simple yet effective Multi-Layer Perceptron (MLP) trained with the Adam optimizer.\nAt the token level, prompts can vary significantly in length, making it impractical to use the raw causal effects directly as defined in Definition 2. To address this, we extract a fixed set of common statistical features including the mean, standard deviation, range, skewness, and kurtosis, that summarize the distribution of causal effects across all tokens in a prompt. This results in a consistent 5-dimensional feature vector for each prompt, regardless of its length. At the layer level, this issue does not arise since the number of layers is fixed. Thus, the input to the detector consists of the 5-dimensional feature vector for the prompt, along with the causal effects of each transformer layer calculated according to Definition 3."}, {"title": "EXPERIMENTAL EVALUATION", "content": "In this section, we evaluate the effectiveness of LLMScan through multiple experiments. We apply LLMScan to detect the four types of misbehavior discussed in Section 2. It should be clear that LLMScan can be easily extended to other kinds of misbehavior. Table 1 summarizes the 4 tasks, 13 public datasets, and 3 well-known open-source LLMs adopted in our experiments. More details regarding the datasets, their processing, and the models are provided in the supplementary material.\nFor baseline comparison, we focus on existing misbehavior detectors that rely on analyzing the internal details of the model (rather than the final response). First, for lie detection, we take the approach reported in (Pacchiardi et al., 2023), which is based on the hypothesis that LLM's behavior would become abnormal after lying. It works by asking a predefined set of unrelated follow-up questions"}, {"title": "EFFECTIVENESS EVALUATION", "content": "To evaluate the performance of LLMScan, we present the area under the receiver operating charac-teristic curve (AUC) and the accuracy (ACC) of LLMScan, along with the accuracy of baselines (B.ACC) in Table 2.\nFor the Lie Detection task, LLMScan demonstrates high effectiveness across all 20 benchmarks, with 50% (10/20) of the detectors achieving a perfect AUC of 1.0. We further present the lying rates generated by the four LLMs in Figure 3(a). It can be observed that their lying rates vary significantly, with Llama-2-7b and Llama-2-13b exhibiting higher rates, nearing 0.70, while Llama-3.1 demonstrates much lower rates, around 0.40. Despite these variations, the detection performance of LLMScan remains stable, unaffected by the generative capabilities of the LLMs. Compared to the baseline method, LLMScan shows a significant improvement in terms of average accuracy on LLama-2-7b, LLama-2-13b, and LLama-3.1, i.e., increasing by 45.65%, 41.33%, and 25.61%, respectively. Additionally, on Mistral, LLMScan delivers a comparable average accuracy (0.93 for LLMScan versus 0.92 for the baseline).\nFor the Jailbreak and Toxicity Detection tasks, LLMScan exhibits consistent and near-perfect perfor-mance across all models, achieving an average AUC greater than 0.99 on both tasks. When compared to the baseline RepE, LLMScan is more stable across all benchmarks, with a minimum accuracy of 0.94 for Toxicity Detection on Llama-3.1. In contrast, RepE's performance fluctuates on certain benchmarks, such as Jailbreak Detection on Llama-3.1 with the AutoDAN dataset.\nFor the Bias Detection task, LLMScan's performance is somewhat weaker compared to the other tasks, with AUCs ranging from 0.71 to 0.78 across all benchmarks. Nevertheless, it still significantly outperforms the baseline method, with an average accuracy improvement of 22.05%. We similarly"}, {"title": "EFFICIENCY EVALUATION", "content": "We also evaluate the efficiency of LLM-Scan against baseline methods. Recall that LLMScan works as early as the first token is generated by the LLM. By inferring causality effects at this initial stage, the de-tector is capable of identifying potential misbehavior immediately, without waiting for the entire sentence to be produced.\nThe efficiency of LLMScandepends on the input length and the number of model lay-"}, {"title": "EXPERIMENTAL RESULT ANALYSIS", "content": "In the following, we conduct an in-depth and comprehensive analysis of LLMScan's detection capabilities on specific cases.\nToken-level Causality. To see how the token-level causal effects distinguish normal responses from misbehavior ones, we employ Principal Component Analysis (PCA) to visualize the variations in attention state changes across different response types.\nFigure 4(a) and 4(b) show the PCA results for the Lie Detection task and the Jailbreak Detection task of the Mistral model, respectively. In both figures, noticeable differences in attention state changes are observed between truthful and lie responses, as well as between refusal and jailbreak responses. These findings provide compelling evidence that causal inference on input tokens can offer valuable insights into the model's behavior and highlight the model's vulnerabilities in handling lie-instruction and jailbreak prompts. Furthermore, the results suggest that improving LLM security through attention mechanisms and their statistical properties is feasible.\nHowever, as shown in Figure 4(c) and 4(d), the PCA results reveal less clear distinctions between toxic and non-toxic responses, as well as between biased and fair outputs. This is primarily because, unlike lie and jailbreak responses, which are often directly triggered by input prompts, toxic and biased responses may be more deeply embedded in the model's knowledge or understanding, i.e., model's parameters. As a result, detecting these misbehavior responses based solely on token-level causal effects is challenging.\nLayer-level Causality. To understand why layer-level causal effects can distinguish normal and misbehavior responses, we present the causal effect distribution across LLM layers for all four tasks in Figure 5, using a violin plot to highlight variations in the model's responses. The violin plot illustrates the data distribution, where the width represents the density. The white dot marks the median, the black bar indicates the interquartile range (IQR), and the thin black line extends to 1.5 times the IQR. The differing shapes highlight variations in layer contributions across response types, with green indicating normal responses and brown representing misbehavior ones.\nFor the Lie Detection task, using Questions1000 as an example, Figure 5(a) illustrates clear differences in the causal effect distributions between truthful and lie responses across all four LLMs. In most cases, truthful responses exhibit wider distributions, indicating greater variability in the contributions of model layers. The sharp peaks at the upper end suggest that certain layers contribute significantly to the model's output, with particularly high causal effects. This pattern is consistent across all four models. This is likely because, when an LLM generates a truthful response, relevant knowledge is concentrated in a few key transformer layers, as reported in previous work (Meng et al., 2022). Intervening these layers can thus drastically disrupt the model's output. In contrast, lie responses tend to exhibit more uniformly distributed causal effects, suggesting a more balanced contribution across"}, {"title": "CONCLUSION", "content": "In this work, we introduce a method that employs causal analysis on input prompts and model layers to detect the misbehavior of LLMs. Our approach effectively identifies various misbehavior, such as lies, jailbreaks, toxicity, and bias. Unlike previous works that mainly focus on detecting misbehavior after content is generated, our method offers a proactive solution by identifying and preventing the intent to generate misbehavior responses from the very first token. The experimental results demonstrate that our method achieves strong performance in detecting various misbehavior."}]}