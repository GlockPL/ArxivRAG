{"title": "Decoding Diffusion: A Scalable Framework for Unsupervised Analysis of Latent Space Biases and Representations Using Natural Language Prompts", "authors": ["E. Zhixuan Zeng", "Yuhao Chen", "Alexander Wong"], "abstract": "Recent advances in image generation have made diffusion models powerful tools for creating high-quality images. However, their iterative denoising process makes understanding and interpreting their semantic latent spaces more challenging than other generative models, such as GANs. Recent methods have attempted to address this issue by identifying semantically meaningful directions within the latent space. However, they often need manual interpretation or are limited in the number of vectors that can be trained, restricting their scope and utility. This paper proposes a novel framework for unsupervised exploration of diffusion latent spaces. We directly leverage natural language prompts and image captions to map latent directions. This method allows for the automatic understanding of hidden features and supports a broader range of analysis without the need to train specific vectors. Our method provides a more scalable and interpretable understanding of the semantic knowledge encoded within diffusion models, facilitating comprehensive analysis of latent biases and the nuanced representations these models learn. Experimental results show that our framework can uncover hidden patterns and associations in various domains, offering new insights into the interpretability of diffusion model latent spaces.", "sections": [{"title": "Introduction", "content": "Recent breakthroughs in image generation have significantly advanced the field of computer vision. In particular, Latent Diffusion Models (LDMs) [1] have emerged as powerful tools for generating high-quality and diverse images. Their success has led to widespread adoption across various creative and practical applications, from digital art and design to advertising and content creation. However, the proliferation of AI-generated images has also raised concerns about biases embedded within these models, as they often mirror societal stereotypes and reinforce existing prejudices when generating representations of people, professions, or other sensitive concepts. These biases are especially problematic because diffusion models are increasingly used in contexts where image outputs may influence public perception, media representation, and even automated decision-making processes. Thus, understanding the biases and limitations inherent in these generative models is crucial to ensuring ethical and responsible use.\nCompared to Generative Adversarial Networks (GANs) [2], with a wealth of works analyzing and utilizing its latent spaces [3-8], diffusion models pose unique challenges. The iterative denoising process in diffusion models makes their latent space more complex and less interpretable, posing difficulties in understanding and analyzing the knowledge encoded within these models. This complexity hinders efforts to grasp how these models represent and manipulate concepts throughout the generation process [9]. This lack of transparency hinders efforts to fully grasp how diffusion models represent and manipulate concepts in their latent spaces. Recent research has addressed these\nPreprint. Under review."}, {"title": "Related work", "content": "Semantic directions involve identifying vectors on some latent space corresponding to a concept, where adding or subtracting that vector would result in meaningful semantic change. This idea has existed for many years, with the famous example being \"King - male + female = Queen\" [14].\nWord2Vec [15, 16] is an early pioneer of this idea and has shown its success in natural language relations. This is also a popular method in GANs [3]. Semantic latent directions have been used for face editing [8], shift camera angles and scales [5, 6], analyze memorability [7] and interpolate between images [4, 17, 18]."}, {"title": "Semantic Directions", "content": "Applying the concept of semantic directions to diffusion models have been more challenging. SEGA [19] and Concept Algebra [13] identify semantic latent vectors for target concepts. However, these are limited in the number of vectors that can be trained, and are more suited for guiding image generation than exploring knowledge representation. NoiseCLR [10] introduces a way to identify semantically meaningful directions given a small dataset of images through contrastive learning. However, each identified direction requires manual interpretation to describe what it represents.\nKwon et al.[9] introduced the H-space, which is the output of the middle layer of the U-Net in the diffusion model. This space is information-rich and also has important properties including homogeneity, linearity, robustness, and consistency across timesteps. Subsequent papers have used"}, {"title": "Methodology", "content": "The h-space is introduced by Kwon et al.[9] and is defined as the output of the middle U-Net layer in the diffusion model. This space represents a condensed, information-rich portion of the model's latent space, capturing intermediate representations during denoising. It also has nice properties like homogeneity, linearity, robustness, and consistency across timesteps. By sampling from this space, we aim to examine the biases and knowledge encoded within the diffusion model.\nTo sample from the h-space, we condition the model on a natural language prompt. At a given timestep, we then record the output of the middle U-Net layer, allowing us to observe the evolution of the h-space throughout the diffusion process.\nHowever, beyond the prompt, two key factors influence the h-space vectors: (1) the timesteps in the diffusion process, and (2) the random seed. Since our goal is to focus on the effects of the prompt, we aim to minimize the impact of both timesteps and random seed variability on our analysis."}, {"title": "Timesteps", "content": "The diffusion process is iterative, with each timestep progressively denoising the input. Early timesteps contain more noise, while later steps refine the image's details. As such, each timestep may focus on different aspects of the image and can be unpredictable and noisy. And while a carefully trained h-space vector for a particular concept can remain consistent across timesteps for image editing purposes [9], directly sampling from the h-space remains noisy and inconsistent. This makes extracting semantic information from the h-space vectors more difficult.\nTo address this, we apply use Latent Consistency Models (LCMs) [22] through the latent consistency model LORA [23] to stabilize the h-space representations. Latent Consistency Models directly predict the solution of the Probability Flow ODE (PF-ODE) at t = 0, based on a consistency function $f_0 (z, c, t)$ that maps from a noisy sample at any timestep t to its denoised state. This approach helps reduce variability across timesteps and ensures that h-space samples reflect meaningful semantic information rather than noise-driven distortions.\nFor sampling the h-space vectors when using LCM, only the first timestep is saved.\nSince Latent Consistency models requires fewer timesteps to generate an image, it is also more efficient for sampling."}, {"title": "Seed", "content": "The random seed determines the initialization of noise added during the forward diffusion process, introducing stochastic variations that affect both the generated images and the intermediate h-space representations. To focus solely on the prompt's influence, we control for the random seed by only comparing vectors from the same seed. That difference vector can then be averaged across multiple seeds to mitigate the impact of individual noise configurations and to reveal consistent patterns in how the prompt modifies the h-space. This process allows us to isolate prompt-driven variations, providing a more robust understanding of how the model encodes semantic concepts and biases in its latent representations.\nBy minimizing the impact of the random seed and timestep variability, we ensure that our analysis of the h-space centers on the prompt's influence, providing a clearer understanding of the model's biases and the encoded knowledge."}, {"title": "Experiments/Analysis", "content": "After obtaining the h-space vectors, this section presents several analyses that can be performed to gain deeper insights into the biases and knowledge encoded by the diffusion model. As illustrated in Figure 1, we explore three distinct approaches for analyzing the latent space: (a) one-to-one comparisons between prompts with and without specific concepts, (b) one-to-many comparisons to assess how a set of descriptions aligns with key semantic concepts, and (c) cluster visualizations to reveal naturally occurring groupings of captions and their shared attributes."}, {"title": "One To One Comparison", "content": "One approach to analyze the h-space vectors is through one-to-one comparisons between prompts that either include or exclude a specific concept. In this experiment, descriptions of individuals in various professions are generated for both male and female subjects using ChatGPT-4 [24]. H-space vectors are then sampled for these prompts. Next, words explicitly referencing gender are removed, and the h-space vectors are sampled again for these gender-neutral descriptions.\nTo quantify the influence of gendered terms, we compute the cosine distance between the h-space vectors corresponding to the gendered and gender-neutral prompts. This distance highlights the extent of gender bias present in the model's representations for different professions, revealing implicit associations even when gender is not explicitly stated in the text. Figure 2 illustrates these biases across a range of professions, demonstrating how the model's internal representations differ based on subtle prompt variations.\nWe validate these biases by classifying the images generated from non-gendered prompts using CLIP [25]. Professions with the largest differences between male and female cosine distances (favoring male representations) in Figure 2 show the lowest probabilities of generating female-presenting images (see Table 1). This indicates that greater divergence in h-space representations correlates with a decreased likelihood of producing female images when given gender-neutral prompts."}, {"title": "One to many comparison", "content": "Sometimes, directly isolating and removing a specific concept from every prompt may be challenging. Instead, we can compare a given concept against a set of diverse prompts.\nIn the following example, ChatGPT 40 [24] was used to generate gender-neutral descriptions of facial portraits. H-space vectors were then collected using these captions. Those results were then compared with h-space vectors generated for each gender, namely: \"a photo portrait of a man\" and \"a photo portrait of a woman\". The difference in cosine similarity between each gender-neutral prompt and the two gendered prompts was then used to rank the gender-neutral prompts from \u201cmost similar to man\" to \"most similar to woman\" (Table 2).\nThe use of cosine distance differences, rather than directly comparing the distance to male or female vectors independently, is necessary because some prompts may be equally distant from both gender vectors for reasons unrelated to gender. For example, if \u201ca photo portrait of a man\u201d and \u201ca photo portrait of a woman\u201d both produce black-and-white images, but a given prompt results in a colored image, the colored image will be far from both male and female vectors in h-space. By using the difference between male and female distances, we minimize the impact of these unrelated factors.\nWhen asked to characterize the differences between two hypothetical people based only on the distances to each caption, ChatGPT-4 provided the following summary:"}, {"title": "Cluster Visualization", "content": "The above two use cases both involve targeted analysis of bias for a particular concept through specially crafted captions. However, we can also analyze more naturally occurring captions through visualizing the clusters that may be present.\nThe following section uses the Food500-CAP [26] dataset, which contains natural language captions for 24700 food image. After sampling the h-space vectors for a given seed, we can use a visualization tool such as t-SNE [27] to visualize the high-dimension data in 2D space. Clustering algorithms such as HDBSCAN [28-30] can be used to automatically group points to form clusters.\nFigure 4 demonstrates this visualization and points out several prominent clusters. Because each cluster is made up of captions instead of images, we can easily identify the common element in a cluster through LLMs such as ChatGPT [24]. This removes the need to manually \"describe\" the latent vectors through observation, like Kwon et al.[9] and NoiseCLR [10].\nFor each cluster, it is also possible to take the average H-space vector of that cluster and add that back into the model to condition the output. The result of this can be seen in Figure 5. It is also possible to combine different clusters to get a combined result, as seen in Figure 5e.\nThis experiment demonstrates that even without explicitly targeting a specific concept, naturally occurring captions can reveal latent biases and associations. By leveraging clustering and visualization techniques, we can map out the conceptual landscape of the h-space, offering a broader understanding of how diffusion models encode and express complex ideas."}, {"title": "Conclusion", "content": "In this paper, we introduced a new framework for unsupervised exploration of diffusion model latent spaces using natural language prompts and image captions. Unlike previous methods that require manual interpretation or rely on a limited number of predefined vectors, our approach enables automatic and interpretable analysis of latent directions by directly associating them with semantic information derived from language. This framework not only simplifies the process of understanding latent representations but also expands the range of concepts that can be explored, offering a more comprehensive view of the knowledge encoded within diffusion models.\nThrough various experiments, we demonstrated the effectiveness of our approach in identifying biases, understanding complex associations, and visualizing semantic clusters within diffusion latent spaces. The ability to systematically map these directions to interpretable concepts opens new avenues for understanding and controlling diffusion models, making it possible to detect latent biases and perform targeted concept analysis without extensive manual intervention.\nOverall, our method provides a scalable and versatile tool for interpreting the intricate latent spaces of diffusion models. By leveraging the power of language-guided exploration, we contribute to the ongoing efforts to make generative models more transparent, interpretable, and ethically responsible. Future work will explore reducing the effect of different random seed initialization on the sampled results, as well as extending the analysis using quantifiable metrics on a broader range of domains."}]}