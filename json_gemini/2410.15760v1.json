{"title": "DeepIcon: A Hierarchical Network for Layer-wise Icon Vectorization", "authors": ["Qi Bing", "Chaoyi Zhang", "Weidong Cai"], "abstract": "In contrast to the well-established technique of rasterization, vectorization of images poses a significant challenge in the field of computer graphics. Recent learning-based methods for converting raster images to vector formats frequently suffer from incomplete shapes, redundant path prediction, and a lack of accuracy in preserving the semantics of the original content. These shortcomings severely hinder the utility of these methods for further editing and manipulation of images. To address these challenges, we present DeepIcon, a novel hierarchical image vectorization network specifically tailored for generating variable-length icon vector graphics based on the raster image input. Our experimental results indicate that DeepIcon can efficiently produce Scalable Vector Graphics (SVGs) directly from raster images, bypassing the need for a differentiable rasterizer while also demonstrating a profound understanding of the image contents.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent research in the field of computer graphics has pre- dominantly focused on data processing and the interpretation of raster images. In contrast, vector graphics have attracted relatively less attention within the deep learning community. Despite this, vector graphics have emerged as a preferred format across various domains, including web graphics, user interfaces, animation, and icon design, owing to their adapt- ability in web applications and superior scalability. Unlike raster graphics, vector graphics offer enhanced precision in shape representation at varying resolutions and encapsulated higher-level information, such as the number of segments, sizes of segments, and the spatial relationships among paths. This inherent complexity provides significant advantages over raster graphics in many applications but poses considerable challenges for learning-based methods regarding representa- tion, comprehension, and generation of vector graphics. In this paper, we concentrate on Scalable Vector Graphics (SVG), one of the most popular and widely utilized vector graphics formats, to explore the generation of vector graphics from images.\nImage rasterization is a relatively straightforward pro- cess, whereas reversing an image to its vector representation presents significant challenges, primarily due to the potential non-uniqueness of results. Also, traditional methods perform poorly in accurately interpreting and preserving the semantics and topological characteristics of image content. In contrast, learning-based approaches have emerged as more promising in recent research, offering solutions to prevailing issues. Nevertheless, despite the advancements made by these novel approaches to image vectorization, they frequently encounter three significant obstacles: incomplete shape prediction as shown in Fig. 1c, redundant shape predictions as shown in Fig. 1a, and the inherent limitation of using pixel loss as a metric to evaluate the performance of vector graphics gener-"}, {"title": "II. RELATED WORKS", "content": "Despite the straightforwardness of image rasterization from vectors, image vectorization from raster images is much more difficult due to the inadequate information provided in the image. Critical details such as the order of overlapping shapes, the direction in which a shape should be drawn, and the segmentation of shapes into distinct entities are not explicitly defined in raster images. While humans can often interpret these aspects intuitively, they pose considerable challenges for computational approaches, especially when dealing with complex images. In response to these challenges, learning- based approaches have been increasingly promising due to their efficiency in extracting higher-level information from images.\nTo bridge the gap between parametric attributes and image data, recent studies [1]-[4], [9], [10] adopt differentiable rasterizers to directly backpropagate pixel losses and optimize the parameters with consideration for the rasterized output, facilitating vector graphic generation without the need for ground-truth vector supervision. Moreover, Cloud2Curve [11] approaches the generation of parametric curves autoregres-"}, {"title": "III. DEEPICON", "content": "In this section, we first introduce the definition of our SVG representation (Section III-A) and the strategy for tokenization (Section III-B), followed by a comprehensive description to our proposed network (Section III-C). We then proceed to detail the implementation specifics of the training process (Section III-D).\nAs a widely adopted representation, SVG offers a large range of commands and functionalities for designers to work with. To limit the scope for training and SVG inference, we simplified the representation of SVG by only picking the necessary attributes for icon representations. Here we present the SVG commands we choose to utilize in this paper as referenced in Table I. Since most basic shapes (e.g., rectangle, circle, ellipse, line, polyline and polygon) can be represented by a composition of straight lines and curves, with these three commands listed in the table, our model is capable of con- structing a variety of shapes as through combinations of curves and lines. For instance, rather than employing the specialized Rect command for rectangles, we opt for a more fundamental"}, {"title": "A. SVG Representation", "content": "As a widely adopted representation, SVG offers a large range of commands and functionalities for designers to work with. To limit the scope for training and SVG inference, we simplified the representation of SVG by only picking the necessary attributes for icon representations. Here we present the SVG commands we choose to utilize in this paper as referenced in Table I. Since most basic shapes (e.g., rectangle, circle, ellipse, line, polyline and polygon) can be represented by a composition of straight lines and curves, with these three commands listed in the table, our model is capable of con- structing a variety of shapes as through combinations of curves and lines. For instance, rather than employing the specialized Rect command for rectangles, we opt for a more fundamental"}, {"title": "B. Tokenization", "content": "Different from most discrete tokenization methods, our ap- proach to SVG decoding emphasizes enhanced geometric ac- curacy by treating point positions as continuous arguments. To facilitate this, we represent a single command implementation as $C = (\\{T_k\\}, \\{A_k\\})$ with a pair of sequences (with $i$ and $j$ omitted), where $T_k$ and $A_k$ denote the $k$th elements of the type sequence $T$ and argument sequence $A$, respectively. In detail, the $\\{T_k\\}$ sequence describes the discrete token types with $T_k \\in \\{\\text{M, L, C, arg, SOS, EOS}\\}$, while $\\{A_k\\}$ sequence indicates the continuous location arguments. These discrete tokens $T_k$ respectively specify the token types MoveTo, LineTo, Cubic B\u00e9zier, continuous arguments, the start of SVG, and the end of SVG. Furthermore, to standardize the length of sequences for each command $C$, we also implement padding with a value of -1 for sequences $T$ and $A$.\nSequences of commands within the same path $P_i$ are then concatenated into a single pair of extended sequences for subsequent model processing. Consequently, the final tok- enized representation of a path $P_i$ is formulated as $P_i = (\\{T_{j,k}\\}, \\{A_{j,k}\\})$ (with $i$ omitted), where $\\{T_{j,k}\\}$ and $\\{A_{j,k}\\}$ indicate the concatenated sequence of tokens for types and arguments within the $i$th path, ensuring a coherent and stan- dardized structure for autoregressive decoding. During train- ing, the special tokens SOS and EOS are added to the start and end of type sequence $\\{T_{j,k}\\}$ respectively."}, {"title": "C. Model Architecture", "content": "We develop a hierarchical deep architecture, as illustrated in Fig. 3, designed to efficiently generate high-quality SVG scripts from a single input image. The illustrated model can be generally segmented into three principal modules: an image encoding module (CLIP Image Encoder), a structure decoding module (Transformer Decoder), and a path decoding module (Transformer Decoder). The input image is initially processed through the CLIP Image Encoder, augmented by a trainable Multi-layer Perceptron (MLP), to produce a singular image embedding, denoted as $z_1$. We adopt CLIP as the encoder to better capture the semantic information in the input image. Since the IMG-to-SVG task requires the model to bridge the image context with SVG scripts, which are written in an XML-based language, CLIP is well-suited due to its strength in joint image-text embedding. Drawing inspiration from DeepSVG [6], this image embedding is subsequently input into a hierarchical SVG decoder, which operates in two distinct stages: 1) The structure decoder outputs a latent representation $z_p$ and a corresponding visibility attribute $v$ for each path $P_i$. Following this, the path decoder translates each path embedding $z_p$ into a pair of tokens, as described in Section III-B."}, {"title": "1) Structure Decoder", "content": "The structure decoder within our architecture is comprised of a 4-layer Transformer decoder featuring a feed-forward dimension of 512 and a model dimen- sion of 256. This transformer-based decoder is subsequently coupled with two trainable linear layers tasked with generating the path embeddings $z_p = (z_1,..., z_{N_p})$ and the visibility attributes $v = (v_1, ..., v_{N_p})$ for each individual path $P_i$. Here $v_i \\in \\{0,1\\}$ serves as an indicator of path visibility. In this process, a predetermined number ($N_p = 8$ in this paper) of path and visibility pairs, denoted as $(z_{p,i}, v_i)$, are generated by iteratively feeding the image embedding $z_1$ into the structure decoder. This innovative approach to predicting path visibility enables our network to generate SVG paths of variable lengths directly from the single image embedding, without the necessity for additional information."}, {"title": "2) Path Decoder", "content": "For the task of path decoding, we employ a 12-layer Transformer configured identically to the structure decoder. Each path decoder is also followed by 2 trainable linear layers designed to transform the output of the Trans- former into two distinct sequences. The first sequence, a type sequence denoted as $\\{T_{j,k}\\}$, has a dimension of 6, correspond- ing to six different types of tokens. The second sequence, an argument sequence represented as $\\{A_{j,k}\\}$, contains continuous values. The continuous values $A_{j,k}$ in the argument sequence are only referenced when the predicted token $T_{j,k}$ in the type sequence is arg."}, {"title": "D. Training", "content": "Since we treat the SVG scripts as a pair of sequences, the primary training objective of our model is to minimize the three distinct loss components: the cross-entropy loss for types $\\{T_{j,k}\\}$, the cross-entropy loss for the path visibility attributes $v_i$ and the mean squared error (MSE) for the valid arguments $\\{A_{j,k}\\}$. Therefore, for each path $P_i$, our objective is to optimize the model parameters $\\Theta$ by minimizing the combined loss across these components:\n$l_{type} = \\frac{1}{N} \\sum CE(T_{j,k}, \\hat{T_{j,k}}|I; \\Theta), \\qquad(1)$\n$l_{vis} = CE(v_i, \\hat{v_i}|I; \\Theta), \\qquad(2)$\n$l_{args} = \\frac{1}{N} \\sum \\mathbb{I}_{T=arg}SE(A_{j,k}, \\hat{A_{j,k}}|I; \\Theta), \\qquad(3)$\nwhere $N$ is the length of padded sequences for each path. $CE(\\cdot,\\cdot)$ denotes the cross-entropy function and $SE(\\cdot,\\cdot)$ refers to the squared error function, respectively. The symbol $I$ represents the input image to the image vectorization model. The triplets $(T_{j,k}, A_{j,k}, v_i)$ and $(\\hat{T_{j,k}}, \\hat{A_{j,k}}, \\hat{v_i})$ correspond"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "In this section, we begin with an introduction to data preparation, as detailed in Section IV-A. Subsequently, we evaluate the performance of our proposed approach by con- ducting comparisons with state-of-the-art (SOTA) methods, as outlined in (Section IV-C), and by presenting ablation studies in Section IV-D."}, {"title": "A. Data Preprocessing", "content": "We evaluated our method using the publicly available SVG- Icons8 dataset, which was processed as described by [6]. This dataset includes a diverse collection of 100,000 SVG icons, categorized across 56 distinct classes. To tailor the dataset to our network's requirements, we implemented a filtering process to exclude icons that were deemed unsuitable for our experimental setup. Specifically, icons comprising more than 8 paths or those containing paths with more than 32 commands were excluded from consideration. Following this criterion, the refined dataset consisted of 25,990 SVG icons. These were subsequently divided into two distinct subsets for the purposes of training and evaluating respectively: 18,193 icons constituted the training set, and 7,797 icons were designated for evaluation. This partitioning was executed randomly to"}, {"title": "C. Comparison with the State-of-the-art Methods", "content": "We compared the performance of our proposed model with two other leading state-of-the-art (SOTA) image vectorization and SVG generation methods: LIVE and DeepSVG. Given that DeepSVG's primary application is the generation of SVG from the latent representations, we adapted it for direct comparison by finetuning the pretrained DeepSVG decoder in conjunction with the CLIP image encoder. The finetuning is employed in a similiar procedure as that used for DeepIcon. The quantitative outcomes of this comparative analysis are presented in Table II. For the purposes of this quantitative comparison, we employed two evaluation metrics: Intersection over Union (IoU) and Chamfer Distance (CD), for their relevance and reliability in assessing the accuracy and quality of vector graphics."}, {"title": "1) Quantitative Analysis", "content": "Table II presents the quantitative results comparing our proposed model, DeepIcon, with SOTA vectorization methods, specifically LIVE and DeepSVG, across two distinct tasks: SVG-to-SVG reconstruction and IMG-to-SVG vectorization. The results indicate that DeepIcon outperforms DeepSVG in both tasks, demonstrating superior SVG generation accuracy. It is important to note that LIVE does not include a SVG encoder, thus becoming not applica- ble for the SVG-to-SVG reconstruction task. Therefore, this comparison is exclusively between DeepSVG and DeepIcon. In this task, DeepIcon exhibited higher average IoU and lower CD across the evaluation set, signaling more effective reconstruction performance with SVG data.\nIn the domain of IMG-to-SVG vectorization, DeepIcon maintains its superior performance over DeepSVG. When it comes to image vectorization, the SVG decoder of DeepIcon still performs better than DeepSVG. However, it is noteworthy that the IoU for DeepIcon is slightly lower than that of LIVE. This discrepancy can be attributed to LIVE's approach of op- timizing a stack of layered SVG paths through a differentiable"}, {"title": "2) Qualitative Analysis", "content": "Fig. 4 presents a qualitative com- parison among DeepIcon, DeepSVG, and LIVE for the IMG- to-SVG task. It is evident that DeepIcon surpasses the other methods in preserving the structure of complex shapes, as illustrated in Fig. 4a. Furthermore, DeepIcon demonstrates a superior understanding of the relationships between overlap- ping shapes, enabling it to generate SVG paths with greater accuracy in terms of preserving the content semantics and relations between shapes. Although LIVE achieves the highest IoU as documented in Table II, it produces the least visually appealing results, as depicted in the figures. This outcome stems from the inherent limitations of parameter optimiza- tion methods, which, while adept at closely matching pixel values, often overlook higher-level information such as path relationships and the optimal number of paths. Consequently, LIVE tends to generate redundant but incomplete paths and is susceptible to falling into local optima."}, {"title": "D. Ablations", "content": "We conducted an ablation study on the evaluation set to validate the essential components of DeepIcon. By default, our configuration includes finetuning a pretrained decoder and a 3-layer MLP head, which is positioned after the CLIP image encoder to map the output embeddings to the input dimension required by the transformer. This study examines the impact of employing an autoregressive transformer, utilizing a continu- ous argument sequence, and applying finetuning techniques on the same dataset division. The outcomes of this investigation, including both quantitative and qualitative comparisons, are detailed in Table III and Fig. 5."}, {"title": "1) Effectiveness of Autoregressive Transformer", "content": "In our eval- uation, we first analyze the impact of employing an autoregres- sive transformer compared to a non-autoregressive approach in the path decoder's design. The non-autoregressive decoder"}, {"title": "V. CONCLUSION", "content": "This paper introduces a novel image vectorization network designed to generate variable-length SVG scripts from a single image input, reconstructing its structure with an emphasis on semantic consistency. DeepIcon reconceptualizes SVG icon representation as a sequence of paths, wherein each path consists of a variable number of commands delineating para- metric lines or curves, thereby prioritizing the completion of topological structures. The model is trained and evaluated on the SVG-Icons8 dataset. Utilizing the CLIP image encoder and the hierarchical transformer-based SVG decoder, our network effectively bridges the gap between images and SVG scripts. The introduction of a unique tokenization rule and the align- ment of continuous arguments further enhance the model's performance. The quantitative and qualitative results from our experiments demonstrate that DeepIcon surpasses state-of- the-art approaches, achieving high-quality icon vectorization results."}, {"title": "Limitation", "content": "While the DeepIcon model demonstrates achievements in the domain of icon vectorization, it remains improvable. While the reconstruction accuracy of the icon vec- torization process is not yet perfect, it suggests an opportunity for the development of a more sophisticated SVG decoder, aimed at significantly enhance SVG decoding performance. Also, DeepIcon is limited to predicting relatively simple SVG shapes, with a maximum of 8 paths and no more than 32 commands per path."}, {"title": "Future Works", "content": "Enhancing the capabilities of the SVG decoder could not only improve the model's accuracy but also reduce its reliance on the finetuning of the CLIP image encoder. Such advancements could pave the way for the model's extension into new applications, including text-to- SVG generation. By potentially substituting the image encoder with a pretrained CLIP text model, DeepIcon could be adapted to interpret and translate textual descriptions into SVG scripts, broadening its applicability and utility in the field of graphic representation."}]}