{"title": "Vulnerability Detection via Topological Analysis of Attention Maps", "authors": ["Snopov P.", "Golubinsky A.N."], "abstract": "Recently, deep learning (DL) approaches to vulnerability detection have gained significant traction. These methods demonstrate promising results, often surpassing traditional static code analysis tools in effectiveness.\nIn this study, we explore a novel approach to vulnerability detection utilizing the tools from topological data analysis (TDA) on the attention matrices of the BERT model. Our findings reveal that traditional machine learning (ML) techniques, when trained on the topological features extracted from these attention matrices, can perform competitively with pre-trained language models (LLMs) such as CodeBERTa. This suggests that TDA tools, including persistent homology, are capable of effectively capturing semantic information critical for identifying vulnerabilities.", "sections": [{"title": "Introduction", "content": "The problem of source code vulnerability detection has become increasingly important in the field of software engineering. This complex task involves analyzing both the syntax and semantics of the source code. Traditionally, static analysis methods have dominated the field of vulnerability detection. These static tools tend to rely heavily on the syntax of the program, thereby overlooking potential information hidden in the semantics. As a result, such approaches often suffer from high rates of false positives.\nRecently, however, various machine learning (ML) and deep learning (DL) methods have emerged [14,15]. Most DL-based solutions leverage graph neural networks (GNNs) and large language models (LLMs). These methods aim to learn both the semantics and syntax of the program. They have shown promising results, outperforming traditional static analysis tools and demonstrating low rates of false positives and false negatives [7].\nOur work presents a novel approach to the vulnerability detection problem. As previously mentioned, the superiority of DL-based models over static ones is potentially due to their ability to capture the semantic information of the program. This semantic information is captured by the neural networks during training or fine-tuning. Furthermore, LLM-based models can capture this information even without fine-tuning."}, {"title": "Background", "content": "On the other hand, the semantics of the source code, like its syntax, can also be represented as a tree or a graph. This representation allows for the application of topological methods.\nIn this work, we leverage the BERT model, pre-trained on source code, to capture the semantic information of programs. We also utilize tools from topological data analysis (TDA) [4] to explore the relationships within this semantic information. Our aim is to analyze and interpret the attention matrices using topological instruments.\nOur work is inspired by the work of Laida Kushareva et. al. [9], in which the authors apply the topological methods for the artificial text detection. They demonstrated that the topology of the semantics captured by an LLM model (such as BERT) provides sufficient information for successful classification. Their approach outperformed neural baselines and performed on par with a fully fine-tuned BERT model, while being more robust to unseen data."}, {"title": "BERT Model", "content": "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based language model that has set new benchmarks in various natural language processing (NLP) tasks. The BERT architecture consists of L encoder layers, each with H attention heads. Each attention head receives a matrix X as input, representing the d-dimensional embeddings of m tokens, so that X is of shape m x d. The head outputs an updated representation matrix X^{out}:\nX^{out} = W^{attn}(XW^Q)W^{V}, with W^{attn} = softmax(\\frac{(XW^Q)(XW^K)^T}{\\sqrt{d}}). (1)\nHere W^Q, W^K, W^V are trained projection matrices of shape d x d, and W^{attn} is matrix of attention weights with shape m x m. Each element $w_{ij}^{attn}$ can be interpreted as a weight of the j-th input's relation to the i-th output where larger weights indicate a stronger connection between the two tokens."}, {"title": "Attention Graph", "content": "An attention graph is a weighted graph representation of an attention matrix W^{attn}, where vertices represent the tokens and the edges connect a pair of tokens if the corresponding weight exceeds a predefined threshold value.\nSetting this threshold value is critical yet challenging, as it distinguishes weak and strong relations between tokens. Additionally, varying the threshold can significantly alter the graph structure.\nTopological data analysis (TDA) methods can extract properties of the graph structure without specifying an exact threshold value, addressing this challenge effectively."}, {"title": "Topological Data Analysis", "content": "Topological data analysis (TDA) is an emerging field that applies algebraic topology methods to data science. There are numerous excellent tutorials and surveys available for both non-mathematicians [10,4] and those with a mathematical background [5,11,13].\nThe main tool in topological data analysis, persistent homology, tracks changes in the topological structure of various objects, such as point clouds, scalar functions, images, and weighted graphs [1,3].\nIn our work, given a set of tokens V and an attention matrix W, we construct a family of attention graphs indexed by increasing threshold values. This family, known as a filtration, is a fundamental object in TDA\nWith this sequence of graphs, we compute persistent homology in dimensions 0 and 1. Dimension 0 reveals connected components or clusters in the data, while dimension 1 identifies cycles or \u00abloops\u00bb. These computations yield a persistence diagram, which can be used to derive specific topological features, such as the number of connected components and cycles (such features are also called the Betti numbers) (see Appendix B for the details)."}, {"title": "Topological Features of the Attention Graphs", "content": "For each code sample, we calculate the persistent homology in dimensions 0 and 1 of the symmetrized attention matrices, obtaining the persistence diagram for each attention head of the BERT model. We compute the following features in each dimension from the diagrams:\nMean lifespan of points on the diagram\nVariance of the lifespan of points on the diagram\nMaximum lifespan of points on the diagram\nTotal number of points on the diagram\nPersistence entropy\nWe symmetrize attention matrices to enable the application of persistent homology techniques. Symmetrizing attention matrices allows us to interpret them as distance matrices of a point cloud embedded in Euclidean space. We symmetrize attention matrices as follows:\n\\forall i, j: W^{sym}_{ij} = max(W_{ij}^{attn}, W_{ji}^{attn}). (2)\nAlternatively, one can think of attention graphs, in which an edge between the vertices i and j appears if the threshold is greater than both $W_{ij}^{attn}$ and $W_{ji}^{attn}$.\nWe consider these features as the numerical characteristics of the semantic evolution processes in the attention heads. These features encode the information about the clusters of mutual influence of the tokens in the sentence and the local structures like cycles. The features with \u00absignificant\u00bb persistence (i.e. those with large lifespan) correspond to the stable processes, while as the features with short lifespans are highly susceptible to noise and do not reflect the stable topological attributes."}, {"title": "Experiments", "content": "To evaluate whether the encoded topological information can be used for vulnerability detection, we train Logistic Regression, Support Vector Machine (SVM), and Gradient Boosting classifiers on the topological features derived from the attention matrices of the BERT model, as described in Section 3. We utilize the scikit-learn library [12] for Logistic Regression and SVM, and the LightGBM library [8] for Gradient Boosting. Detailed training procedures are outlined in Appendix A."}, {"title": "Data", "content": "We train and evaluate our classifier on Devign dataset. This dataset comprises samples from two large, widely-used open-source C-language projects: QEMU, and FFmpeg, which are popular among developers and diverse in functionality. Due to computational constraints, we were only using those data samples, that, being tokenized, are of length less than 150. This ensures that the point cloud constructed during attention symmetrization is also limited to a maximum length of 150."}, {"title": "Baselines", "content": "We employ the microsoft/codebert-base model [6] from the HuggingFace library [16] as our pre-trained BERT-based baseline. Additionally, we fully fine-tune the microsoft/codebert-base model for comparison."}, {"title": "Results and Discussion", "content": "Table 1 outlines the results of the vulnerability detection experiments on the Devign dataset. The results reveal that the proposed topology-based classifiers outperform the chosen large language model (LLM) without fine-tuning but perform worse than the fine-tuned version.\nThese observations indicate that the information about a code snippet's vulnerability is encoded in the topological attributes of the attention matrices. The semantic evolution in the attention heads reflects code properties that are crucial for the vulnerability detection task, and persistent homology proves to be an effective method for extracting this information."}, {"title": "Consclusion", "content": "This paper introduces a novel approach for the vulnerability detection task based on topological data analysis (TDA). We propose a set of interpretable topological features, obtained from persistence diagrams derived from the attention matrices of any transformer-based language model. The experiments demonstrate that machine learning classifiers trained on these features outperform pre-trained code-specific large language models (LLMs).\nOur code is publicly available\u00b9, and we encourage further research into TDA-based methods for vulnerability detection and other NLP tasks. Future work could explore combining topological features that encode semantics with those capturing structural information. Additionally, studying different symmetrizations of attention matrices and how they encode semantics could provide new insights. Another interesting direction is applying multiparameter persistent homology to analyze the semantic evolution in attention heads."}, {"title": "Appendix A Training Details", "content": "Fine-tuning CodeBERTa We trained with the cosine scheduler with an initial learning rate lr = 5e - 5 and set the number of epochs e = 15.\nHyperparameter search We employed Optuna [2] to find optimal hyperparameters for both SVM and LightGBM models.\nFor SVM, the optimal hyperparameters were C = 9.97, \u03b3 = auto and kernel = rbf.\nFor LightGBM, the optimal parameters were \u03bb\u2081 = 5.97, \u03bb\u2082 = 0.05, num_leaves = 422, feature_fraction = 0.65, bagging_fraction = 0.93, bagging_freq = 15, min_child_samples = 21."}, {"title": "Appendix B Persistent Homology", "content": "Recall that a simplicial complex X is a collection of p-dimensional simplices, i.e., vertices, edges, triangles, tetrahedrons, and so on. Simplicial complexes generalize graphs, which consist of vertices (0-simplices) and edges (1-simplices) and can represent higher-order interactions. A family of increasing simplicial complexes\n\u2205 \u2282 X\u2080 \u2286 X\u2081 \u2286 ... \u2286 X_{n-1} \u2286 X_n\nis called a filtration.\nThe idea of persistence involves tracking the evolution of simplicial complexes over the filtration. Persistent homology allows to trace the changes in homology vector spaces\u00b2 of simplicial complexes that are present in the filtration. Given a filtration {X_i}_{i=0}^n, the homology functor H_p applied to the filtration generates a sequence of vector spaces H_p(X_i) and maps i\u2217 between them\n\\overset{i_*}{\\longrightarrow} H_p(X): H_p(X_0) \\overset{i_*}{\\longrightarrow} H_p(X_1) ... \\overset{i_*}{\\longrightarrow} H_p(X_n).\nEach vector spaces encodes information about the simplicial complex X and its subcomplexes Xi. For example, Ho generally encodes the connectivity of the space (or, in data terms, Ho encodes the clusters of data), H\u2081 encodes the presence of 1-cycles, i.e., loops, H\u2082 represents the presence of 2-cycles, and so on. Persistence tracks the generators of each vector space through the induced maps. Some generators will vanish, while others will persist. Those that persist are likely the most important, as they represent features that truly exist in X. Therefore, persistent homology allows one to gain information about the underlying topological space via the sequence of its subspaces, the filtration."}, {"title": null, "content": "In algebraic terms, the sequence of vector spaces $H_p(X_i)$ and maps $i_*$ between them can be seen as a representation of a quiver $I_n$. From the representation theory of quivers it is known, due to Gabriel, that any such representation is isomorphic to a direct sum of indecomposable interval representations $I_{[b_i, d_i]}$, that is,\n$H_p(X_*) \\simeq \\bigoplus_i I_{[b_i, d_i]}$.\nThe pairs $(b_i, d_i)$ represent the persistence of topological features, where $b_i$ denotes the time of birth and $d_i$ denotes the time of death of the feature. These pairs can be visualized via barcodes where each bar starts at $b_i$ and ends at $d_i$.\nThis information is also commonly represented using persistence diagrams. A persistence diagram is a (multi)set of points in the extended plane $R^2$, which reflects the structure of persistent homology. Given a set of pairs $(b_i, d_i)$, each pair can be considered as a point in the diagram with coordinates $(b_i, d_i)$. Thus, a persistence diagram is defined as\n$dgm(H_p(X_*)) = \\{(b_i, d_i): I_{[b_i, d_i]} \\text{ is a direct summand in } H_p(X_*)\\}$.\nPersistence diagrams provide a detailed visual representation of the topology of point clouds. However, integrating them into machine learning models presents significant challenges due to their complex structure. To effectively use the information from persistence diagrams in predictive models, it is crucial to transform the data into a suitable format, such as by applying persistence entropy.\nPersistence entropy is a specialized form of Shannon entropy specially designed for persistence diagrams and is calculated as follows:\n$PE_k(X) := - \\sum_{(b_i, d_i) \\in D_k} p_i log(p_i)$,\nwhere\n$p_i = \\frac{d_i - b_i}{\\sum_{(b_i, d_i) \\in D_k} (d_i - b_i)}$ and $D_k := dgm(H_k(X_*)).$\nThis numerical characteristic of a persistence diagram has several advantageous properties. Notably, it is stable under certain mild assumptions. This stability means there is a bound that \u00abcontrols\u00bb the perturbations caused by noise in the input data."}]}