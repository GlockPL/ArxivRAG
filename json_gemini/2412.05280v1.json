{"title": "Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model", "authors": ["Lening Wang", "Wenzhao Zheng", "Dalong Du", "Yunpeng Zhang", "Yilong Ren", "Han Jiang", "Zhiyong Cui", "Haiyang Yu", "Jie Zhou", "Jiwen Lu", "Shanghang Zhang"], "abstract": "4D driving simulation is essential for developing realistic autonomous driving simulators. Despite advancements in existing methods for generating driving scenes, significant challenges remain in view transformation and spatial-temporal dynamic modeling. To address these limitations, we propose a Spatial-Temporal simulation for drivinG (Stag-1) model to reconstruct real-world scenes and design a controllable generative network to achieve 4D simulation. Stag-1 constructs continuous 4D point cloud scenes using surround-view data from autonomous vehicles. It decouples spatial-temporal relationships and produces coherent keyframe videos. Additionally, Stag-1 leverages video generation models to obtain photo-realistic and controllable 4D driving simulation videos from any perspective. To expand the range of view generation, we train vehicle motion videos based on decomposed camera poses, enhancing modeling capabilities for distant scenes. Furthermore, we reconstruct vehicle camera trajectories to integrate 3D points across consecutive views, enabling comprehensive scene understanding along the temporal dimension. Following extensive multi-level scene training, Stag-1 can simulate from any desired viewpoint and achieve a deep understanding of scene evolution under static spatial-temporal conditions. Compared to existing methods, our approach shows promising performance in multi-view scene consistency, background coherence, and accuracy, and contributes to the ongoing advancements in realistic autonomous driving simulation. Code: https://github.com/wzzheng/Stag.", "sections": [{"title": "1. Introduction", "content": "As autonomous driving capabilities advance in perception [26, 28-30], prediction [9, 54], and planning [20, 57], significant progress has also been made in end-to-end networks [17, 18]. With these advancements, comprehensive testing and validation of autonomous vehicles are increasingly critical [8]. However, real-world vehicle testing remains time-consuming, costly, and limited in scenario coverage.\nMainstream research increasingly relies on simulation software for extensive algorithm testing and validation [38]. Yet, simulations based on 3D modeling struggle to accurately replicate realistic driving scenarios, creating a substantial gap between synthetic environments and real-world conditions [58]. To address this, current autonomous driving testing solutions strive to build highly realistic scenarios for validating driving algorithms [12, 43]. With the rapid advancement of text-to-image and text-to-video generative models [52], some research has focused on generating trajectory-controlled images or videos to simulate autonomous driving scenes, guided by maps and surrounding vehicle poses to improve scene accuracy [10, 23]. However, real-world driving involves constantly moving pedestrians, vehicles, and objects that introduce structural changes to the environment. Video generation methods often struggle to capture these dynamic changes or the close interactions between elements, leading to inconsistencies in scene continuity, such as background and vehicle type shifts, which complicate maintaining temporal consistency [16, 55]. Recently, approaches based on NeRF [44] and 3D Gaussian Splatting (3DGS) [11, 19] have aimed to capture dynamic elements with greater precision by rendering and modeling 3D scenes. Nonetheless, these methods still face challenges in reconstructing 4D scenes from arbitrary viewpoints, handling extensive dynamic view changes with significant camera movement, and managing long-term temporal transformations under static views.\nTo enable more realistic autonomous driving testing, we propose Spatial-Temporal simulation for driving (Stag-1), a controllable 4D simulation framework based on real-world autonomous driving scenes, as shown in Figure 2. Our approach begins by constructing 3D point clouds frame-by-frame using surround-view data from autonomous vehicles. We then develop a rough alignment network based on ego-car and camera parameters. Next, we iteratively refine the point clouds and align them with sequential scenes, resulting in a 4D point cloud that incorporates both camera and vehicle motion parameters. This process accurately captures the 4D structure of real-world environments. Furthermore, we develop a multi-view interaction-based sparse point cloud completion network, which allows for controllable 4D simulation video synthesis in autonomous driving applications. To improve the quality of continuous scene simulation, we also design a cross-view diffusion-based generative network that addresses two key challenges: comprehensive dynamic viewpoint modeling in static scenes and precise static viewpoint modeling in dynamic scenes. This network compensates for missing information in sparse point clouds, ensuring continuity in scene transitions and accuracy in temporal changes."}, {"title": "2. Related Work", "content": "Scene Simulation for Autonomous Driving. In early autonomous driving simulation tasks, creating realistic street views often required manual 3D scene modeling, which resulted in significant gaps from real-world scenarios [7, 41, 44]. The advent of NeRF [33] introduced a novel approach by reconstructing real scenes from multi-view images, opening new possibilities for autonomous driving simulation [5, 27, 40]. Later, the introduction of 3DGS [22] methods further enhanced both the effectiveness and efficiency of 3D scene reconstruction. Some researchers [11, 19, 55, 58] have explored combining autonomous driving image scene generation with 3DGS to improve scene coherence. However, methods based on NeRF and 3DGS struggle with large camera viewpoint shifts and lack precise control over temporal progression from fixed viewpoints, limiting their application in controlled autonomous driving simulations. In contrast, Stag-1 combines realistic sparse point cloud representation in 4D reconstruction with video generation using diffusion models, achieving both cross-view and temporal consistency. This enables controllable scene generation from any viewpoint at any time, advancing 4D simulation tasks for autonomous driving.\nScene Generation for Autonomous Driving. Generative adversarial networks [14] have made significant advances in image generation, and with the emergence of diffusion architectures [1, 6, 34, 36, 46], numerous studies have demonstrated powerful capabilities in image and video synthesis. To enhance the controllability of generation, subsequent networks have been refined to enable condition-based video generation using inputs such as text and images [4, 25, 56]. In the field of autonomous driving, scene generation [23, 32] plays a crucial role in enhancing the system's adaptability to diverse driving scenarios and facilitating closed-loop testing [3, 38, 50]. As such, trajectory control [12, 16, 43] or the use of 3D bounding boxes and scene description text [10, 11] have been introduced to guide autonomous driving scene generation. However, despite the inclusion of various control signals, existing methods still face significant challenges in maintaining scene consistency and ensuring high controllability. In contrast, Stag-1 demonstrates clear advantages in maintaining consistency across continuous scenes, providing arbitrary viewpoint and time control, and ensuring multi-view consistency.\nControllable Video Generation. With advancements in text-to-image [24, 35] and image-to-video generation models [1, 21, 46], enhancing the controllability of generative models has garnered significant attention. Diverse condition-control [25] networks now enable controllable image or video generation using inputs such as sketches [47], trajectories [13, 51], and more. In research focused on generative modeling for autonomous driving, much emphasis has been placed on vehicle trajectory control [12, 16, 43] or on using target bounding boxes and maps for guidance [10, 23]. However, these approaches often lack a comprehensive understanding of 4D spatial relationships, making it challenging to accurately capture and distinguish spatial-temporal dynamics. In contrast, our work establishes continuous 4D point clouds to decouple spatial and temporal relationships in generative tasks, enhancing editing capabilities for generative simulations in autonomous driving."}, {"title": "3. Proposed Approch", "content": "3.1. 4D Autonomous Driving Simulation\nGenerative 4D autonomous driving simulation aims to address the lack of realism in traditional autonomous driving simulation scenarios [7] and overcome the limitations of image generation models in terms of scene quality [10] and control capabilities [12]. Formally, the generative 4D autonomous driving simulation generates a scene Gsim \u2208 RH\u00d7W\u00d7t\u00d73 that based on the real-world scene Sgt \u2208 RH\u00d7W\u00d7t\u00d73 and a set of control signals Cs, can be expressed as:\n$G_{sim}(C_s, S_{gt}) = f(S_{gt}, C_s),$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)\nwhere Cs = {Cp, Tt, . . ., Vs} is the set of control signals (such as camera poses Cp, time Tt, and vehicle states V).\nThe traditional approach typically includes control signals $C_g$ such as the vehicle's continuous trajectory T, throttle position $\\theta$, steering angle $\\delta$, and the real positions of surrounding vehicles and BEV maps $M_{bev}$, which control the scene $G_{sim}$. This can be expressed as:\n$C_g = \\{T, \\theta, \\delta, P_s, M_{bev}\\}.$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2)\nHowever, the traditional methods often fail to effectively capture the true relationship between time and space in the scene, leading to temporal jumps and a lack of controllability in the generated scenes. Therefore, we explore 4D scene point cloud reconstruction for realistic scene understanding and high-quality image generation using generative models. The method combines 4D point cloud, camera parameters, and temporal information, and uses a generation framework to effectively capture the independent variations of time and space, enabling more natural and precise autonomous driving simulations, which can be expressed as:\n$G_{sim}(C_p, T_t) = f(C_p,T_t, \\{S_{gt}, V_s, C_o\\}),$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(3)\nwhere $C_o$ represents the original parameters of the ego vehicle and cameras. In this way, we can generate realistic simulated scenes consistent with the control signals.\nTo accurately control the scene, we extract the 4D point cloud $P_a$ from the current scene $S_{gt}$, and project $P_a$ onto continuous 2D images $P^{2D}(t_k)$ under the continuous conditions of $C_p$ and $T_t$, which forms a keyframe video $V_{key} \\in R^{H\\times W\\times t\\times 3}$. Then, we use a video generation network $G(.)$ to generate a continuous, accurate, and controllable 4D autonomous driving simulation scene, $G_{sim}$.\nThe process includes two key steps: reconstructing an accurate 4D point cloud $P_a$ and projecting the 4D point cloud into keyframes $V_{key}$ based on control information for generation. We will provide a detailed explanation of these two steps in Section 3.2 and Section 3.3."}, {"title": "3.2. Spatial-Temporal Aligned Reconstruction", "content": "The construction of generative 4D autonomous driving simulation scenes $G_{sim}$ relies on an accurate 4D point cloud $P_a$. Based on the practical needs of autonomous driving, we define the form of $P_a$ according to three principles: 1) Authenticity: The 4D point cloud must be constructed with real parameters, requiring accurate scene size and range assessment, rather than just relative proportions. 2) Accuracy: The scene should precisely estimate object positions and distances to enhance 3D point cloud precision. 3) Consistency: Each scene frame should align with the vehicle or camera parameters for coherence.\nFollowing these principles, we first estimate and use surround-view camera parameters $R^{(t,v)}, T^{(t,v)}$ to generate a surround-view 3D point cloud $P^{3D}$. We further align the point cloud using ego vehicle parameters $R_t, T_t$, refining it iteratively to build an accurate 4D point cloud scene $P_a$.\nSingle-Frame 3D Point Cloud Construction. Specifically, for constructing a 3D scene from a single frame, we process each image $S_{im}^{(t,v)} \\in R^{H\\times W\\times 3\\times t\\times V}$, where V = 6 represents six surround-views. Following part of the R3D3 [37] approach for per-frame depth estimation $D^{(t,v)}_{im} \\in R^{H\\times W\\times t\\times V}$.\nThen, we use the corresponding camera pose $R^{(t,v)}$ and $T^{(t,v)}$ to get an accurate surround-view point cloud $P^{(t,v)}$. By combining the point clouds from all views v, we obtain the surround-view point cloud $P_t$ at time t, which can be expressed as:\n$P_t = \\bigcup P^{(t,v)} = \\bigcup (R^{(t,v)} \\cdot D_{im}^{(t,v)} + T^{(t,v)}).\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4)$\nContinue-Frame 4D Point Cloud Coarse Alignment. For each $P_t \\in R^{N_t\\times 3}$, where $p_i = [x_i, y_i, z_i]^T$ represents the coordinates of the i-th point in the local coordinate system of the ego vehicle, we also need to perform continuous-frame 4D point cloud alignment. We apply the following transformation:\n$P^{(t)}_i = R_t \\cdot P_i + T_t,$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5)\nwhere $p^{(t)}_i$ represents the transformed 3D point in the global coordinate system, $R_t \\in R^{3\\times 3}$ and $T_t \\in R^3$ represents vehicle's pose at time t.\nThen, to construct a complete 4D point cloud, we aligned 4D point cloud sequence at time step t, denoted as $P_a$:\n$P_a = \\bigcup (p^{(t)}_i).$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(6)\nContinuous 4D Point Cloud Fine Alignment. Given that the 3D point clouds are estimated via single-step depth estimation and lack precise real-world values, aligning them based solely on parameters does not guarantee full alignment accuracy. Therefore, we introduce a fine alignment method, which refines the alignment over several iterations.\nAt each iteration k, the rotation $R_k$ and translation $T_k$ are updated based on the point cloud alignment error $E_k$, which measures the difference between the transformed points and the reference alignment $P_r$. The transformation parameters are updated by minimizing the alignment error:\n$E_k = \\sum \\|p^{(t)}_i - p^{(t)}\\|^2,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(7)$\nwhere $p^{(t)}$ represents the transformed 3D point $p_i$ after applying the current transformation parameters.\nBy applying this process iteratively to each frame, we generate a series of 3D point clouds $P_{t1}, P_{t2},..., P_{tn}$. Finally, we obtain $P_a$, the aligned point cloud, which supports subsequent spatial-temporal scene decoupling."}, {"title": "3.3. Point-Conditioned Video Generation", "content": "Achieving Spatial-temporal decoupling is a critical aspect of autonomous driving simulation. However, existing models face challenges in separately capturing spatial and temporal variations within a scene due to limitations in their structure, making it difficult to decouple space and time within the same environment. To address this, Stag-1 processes a sequence of continuous 4D sparse point clouds $P_a$ to generate 2D sparse keyframe videos $V_{key}(t, 0)$, under the dual control of $C_p$ and $T_t$.\n$V_{key}(C_p, T_t) = \\{f(P^a_i | C_p,T_t,t_k)\\},$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(8)\nwhere, $t_k$ represents the discrete keyframe moments in the time sequence.\nTemporal Decoupling Keyframe. Under a fixed camera pose, we proposed method for efficient spatial-temporal decoupling keyframe modeling by extracting the 3D point cloud for each keyframe and projecting it into a 2D image. Specifically, for each timestamp tk, we select the 3D point cloud corresponding to the current frame $P_i(t_k) = \\{(x, y, z)|(x, y, z) \\in R^3,t = t_k\\}$, where $(x(t_k), y(t_k), z(t_k))$ represents the vehicle's position at time $t_k$, and $(r(t_k), i(t_k), j(t_k), k(t_k))$ represents the vehicle's rotational quaternion at time $t_k$, describing its orientation. Subsequently, we project each keyframe's 3D point cloud $P_i(t_k)$ using the camera's matrix K, along with the rotation matrix $R(t_k)$ and translation vector $t_k$ at timestamp $t_k$, to obtain a sparse 2D point cloud in the image:\n$V_{key}(t, 0) = K[R(t_k)|t_k] P_i(t_k), P_i = C$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(9)\nThrough this approach, dynamic point cloud data is accurately projected into a 2D image from a fixed viewpoint.\nSpatial Decoupling Keyframe. In Spatial Decoupling Keyframe Modeling, we project the 3D point cloud of the current frame onto the 2D image plane using the aligned spatial information. Through perspective projection, we map the 3D point cloud to the 2D image plane:\n$V_{key}(t, 0) = K \\cdot [R(t_k)|t_k] \\cdot P_i(t_k), t_k = 0.$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(10)\nBy using the aligned spatial information, we precisely convert the 3D point cloud of the current frame into its 2D projection. This method effectively leverages the spatial information, transforming it into a 2D point cloud representation while mitigating the impact of temporal variations on keyframe extraction."}, {"title": "3.4. 4D Spatial-Temporal Simulation", "content": "We present the overall training framework for our 4D generative simulation model in autonomous driving, as shown in Figure 3. First, we obtain $P_a$ for all training scenes and align these within a 4D temporal context using Cp and Tt to iteratively refine the alignment. Our training follows a two-Stage approach: The time-focused Stage trains single-view scenes in a temporal context, while the spatial-focused Stage integrates surround-view information to capture spatial and temporal relationships.\nThe Time-Focused Stage. We use odd-frame sequential images $I_{2n+1}$ (where $n = 0, 1, 2, . . .)$ as ground truth and project even-frame 3D point clouds $P_{2n+2}$ onto the image plane based on the poses $T_{2n+1}$ and camera intrinsics $K_{2n+1}$ of the odd frame:\n$P_{2n+2}^{proj} = K_{2n+1} T_{2n+1} \\cdot P_{2n+2},$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(11)\nwhere, $P_{2n+2}^{proj}$ denotes the projection of even-frame point clouds using the parameters of the odd frames.\nWe generate paired training data by creating sequences of projected 3D point clouds $P^{proj}_{2n+2}$, and their corresponding ground-truth images $I_{2n+1}$. For training efficiency, we encode $I_{2n+1}$ and the conditional signals $P^{proj}_{2n+2}$ into latent space, where optimization is performed. To ensure accurate alignment and effective model learning, we define a custom loss function that guides the optimization process. The loss function is defined as follows:\n$\\min E_{\\eta~U(0,1),n~N(0,1)} [\\|l_\\phi(W_\\tau, \\eta, \\hat{w}, I_{ref}) - I_\\tau\\|^2],$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(12)\nwhere, $\\tau$ denotes the time step, sampled from a uniform distribution $U(0, 1)$; $\\eta$ represents noise sampled from a standard normal distribution $N(0, I)$; $\\phi$ represents the model parameters; $w_\\tau$ represents the latent variable at time step $\\tau$; $\\hat{w}$ is the conditional latent variable; $I_{ref}$ represents the reference image.\nThe Spatial-Focused Stage. We use the same input approach as in the time-focused Stage, with $P_{2n+2}^{proj}$ as the sequence of 3D point cloud projections and $I_{2n+1}$ as the reference images. To leverage overlapping information and interactions between surround-view images in autonomous driving, we introduce an attention mechanism for cross-image information exchange:\n$Att(.) = S_{A_C}(Q_s, K_s, V_s), Q_s, K_s, V_s \\in R^{(B\\times T)\\times H\\times W\\times 6}$\n$T_A(Q_t, K_t, V_t), Q_t, K_t, V_t \\in R^{(B\\times 6)\\times H\\times W\\times T}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(13)\nOur method captures spatial relationships within each frame across different perspectives, while also considering temporal connections between consecutive time steps. The loss function for this Stage is designed similarly to equation 12."}, {"title": "4. Experiments", "content": "In this section, we describe our training methodology and evaluate the effectiveness of Stag-1 for autonomous driving tasks, specifically focusing on 3D reconstruction and 4D simulation. Quantitative results from comparison and ablation studies assess Stag-1 performance, with visualizations of scene reconstruction, viewpoint translation, and static temporal conditions. The structure of our experimental results is as follows:\n4.1. Implementation Details\nOur training process consists of two Stages. In the first Stage, we pre-train the model following [51] for 5,000 iterations. Using sweep records from the NuScenes dataset [2], we randomly select starting points and sample eight consecutive frames, where four frames are used as conditions and four as ground truth. During this Stage, we freeze the Encoder module and train only the Decoder module using the AdamW optimizer [31] with a learning rate of $1 \\times 10^{-5}$. In the second Stage, to further learn spatial relationships across panoramic views, we freeze all remaining components and train only the spatial attention module for an additional 3,000 iterations, again using the AdamW optimizer [31] with a learning rate of $1 \\times 10^{-5}$.\n4.2. 4D Reconstruction and Synthesis\nTo assess our method's capabilities in 4D reconstruction, we conduct zero-shot evaluations on the Waymo-NOTR dataset [49], as shown in Table 1. Our approach demonstrates superior performance in scene reconstruction and novel view synthesis compared to existing methods [19, 22, 45, 48, 49]. For the static-32 dataset [48], we follow conventional metrics [49] using PSNR, SSIM, and LPIPS [53] to evaluate rendering quality, and for dynamic data, we use PSNR* and SSIM* [19] to focus on dynamic objects. Our results outperform other methods, showcasing the model's generalization under zero-shot conditions and its ability to model both static scenes and dynamic objects. Qualitatively, as shown in Figure 4, our method excels in monocular scene reconstruction and multi-view synthesis. Additionally, we conducted both quantitative and qualitative evaluations of scene reconstruction on the Street Gaussian dataset [48], as presented in Table 2, with visualizations shown in Figure 5.\nFurthermore, recent studies [42, 48, 49] have used the Waymo Open Dataset (WOD) [39] for evaluations. To accurately compare our method with the latest approaches, we conducted quantitative analyses under similar experimental conditions. As shown in Table 3, our method outperforms other approaches in reconstruction. Thus, the quantitative comparisons and visualization results across the three different experimental conditions demonstrate that our proposed reconstruction and novel view synthesis methods outperform other related approaches.\n4.3. 4D Driving Simulation\nAutonomous driving generative 4D simulation based on real-world scenes requires the ability to decouple spatial-temporal relationships. This involves observing the scene from different camera viewpoints based on the current time state or decomposing temporal motion based on a fixed spatial state. We conducted both quantitative and qualitative comparison experiments on the NuScenes [2] and Waymo [39] datasets to demonstrate the capability and effectiveness of the proposed method.\nFrozen Time. A key aspect of autonomous driving 4D simulation is the ability to achieve dynamic viewpoint changes under frozen temporal conditions. We compared our proposed method with existing approaches, and the visualization results show that our method successfully achieves the desired tasks. As shown in Figure 6, we translated the camera pose in the NuScenes dataset [2]. The results indicate that our method can achieve accurate translations. To provide a fair comparison with 3DGS-based methods, we conducted similar tests on the Waymo dataset [39]. As shown in Figure 7, our method outperforms others in terms of image accuracy. Additionally, we performed viewpoint rotation to test the model's ability to handle diverse camera transformations. As shown in Figure 8, our model successfully accomplished this task.\nFrozen Space. Another key aspect of 4D simulation is its ability to vary temporal ranges while keeping the camera position fixed, enabling diverse functionalities. We demonstrate this capability by simulating temporal movement under frozen spatial conditions, as shown in the left of Figure 9. The figure illustrates moving vehicles relative to the ground truth, while the background remains stable, validating the method's ability to simulate time variations in a fixed spatial context. To quantitatively assess image and video quality under viewpoint translation and transformation, we use the Fr\u00e9chet Inception Distance (FID) [15] for images and Fr\u00e9chet Video Distance (FVD) for videos. As shown in Table 4, the quantitative results demonstrate that our method outperforms previous approaches and effectively decouples spatial and temporal relationships.\nMulti View Simulation. We propose a method for 4D surround-view simulation in autonomous driving, capable of generating dynamic images with consistent transformations across different settings, as demonstrated in the lower part of Figure 1. This method significantly improves interactive simulations that rely on surround-view information.\nRemove Cars. Furthermore, we showcase the enhanced capabilities of our proposed model. After aligning the 4D point cloud scene, we can selectively remove specific point clouds to eliminate individual vehicles, as illustrated on the right side of Figure 9.\n4.4. Ablation Studies\nTo assess the impact of different point cloud conditions on simulation quality, we conducted ablation studies, as shown in Figure 10.\nSparse Point Cloud Information. Point cloud density directly affects reconstruction quality. Sparse point clouds result in incomplete scenes, leading to noise and incorrect information in the output, as seen in Figure 10 (b). Misaligned 4D Point Clouds. Misalignment of point clouds with coordinate parameters causes the loss of keyframe information. As shown in Figure 10 (c), misaligned point clouds introduce significant empty pixels, which causes confusion during model completion and results in blurry or disordered scenes. Partial Occlusion in Panoramic Point Clouds. In cases of occlusion, the model attempts to fill in missing data, as shown in Figure 10 (d). However, accurately reconstructing occluded regions remains challenging, hindering precise simulation in these areas. These ablation studies highlight the importance of each component in improving model performance."}, {"title": "5. Conclusion", "content": "In this paper, we propose a generative 4D simulation model for autonomous driving, designed to edit real-world scenes for controllable autonomous driving simulation. We reconstruct coherent and aligned real-world scenes in 4D point clouds and design keyframe projections to decouple spatial and temporal relationships. Finally, we build a generative network for 4D simulation on sparse point clouds. Both visualization and quantitative results show that the proposed method can extract key elements of real scenes for controllable simulation, providing a feasible solution for autonomous driving testing and validation.\nLimitations. Controlling vehicle or pedestrian motion via rigid body clustering of point clouds to enhance scene editing is crucial for simulation tasks and will be addressed in future research."}]}