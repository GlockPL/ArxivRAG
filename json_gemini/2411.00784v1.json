{"title": "FIRE: Fact-checking with Iterative Retrieval and Verification", "authors": ["Zhuohan Xie", "Rui Xing", "Yuxia Wang", "Jiahui Geng", "Hasan Iqbal", "Dhruv Sahnan", "Iryna Gurevych", "Preslav Nakov"], "abstract": "Fact-checking long-form text is challenging, and it is therefore common practice to break it down into multiple atomic claims. The typical approach to fact-checking these atomic claims involves retrieving a fixed number of pieces of evidence, followed by a verification step. However, this method is usually not cost-effective, as it underutilizes the verification model's internal knowledge of the claim and fails to replicate the iterative reasoning process in human search strategies. To address these limitations, we propose FIRE, a novel agent-based framework that integrates evidence retrieval and claim verification in an iterative manner. Specifically, FIRE employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment. We compare FIRE with other strong fact-checking frameworks and find that it achieves slightly better performance while reducing large language model (LLM) costs by an average of 7.6 times and search costs by 16.5 times. These results indicate that FIRE holds promise for application in large-scale fact-checking operations.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks, including both language comprehension and generation (Zhao et al., 2023; Xie et al., 2023a). Consequently, LLMs are now widely applied in various domains (Xie et al., 2023b), and many users increasingly rely on the information they provide. However, this reliance is problematic, as LLMs are capable of producing outputs that are highly confident but factually incorrect, highlighting the critical need for robust fact-checking systems (Akhtar et al., 2023). However, fact-checking the entire output of LLMs in a single step is highly challenging. To address this, Min et al. (2023) proposed decomposing the content into multiple atomic claims, each of which can be individually verified. While this approach simplifies the fact-checking process, assessing the veracity of these atomic claims remains complex, especially when many require sourcing evidence from the web. Indeed, identifying the most relevant evidence online is a key challenge in fact-checking pipelines (Wang et al., 2023).\nTo address this issue, conventional methods, such as FACTOOL and FACTCHECK-GPT (Chern et al., 2023; Wang et al., 2023), frame the problem as a question-answering task, as illustrated on the left side of Figure 1. In these approaches, an LLM is prompted to generate N relevant questions, which are then used as search queries by a web search tool. The search results serve as evidence for LLM to determine the factuality of the claim. However, we argue that this process is inefficient in two key aspects. First, it underutilizes the internal knowledge already embedded in LLMs during pre-training. For claims involving common knowledge or widely known events, the LLM could confidently assess the claim without relying on external information. Second, generating multiple search queries concurrently does not align with the typical human reasoning process during search (Hu et al., 2023). Humans tend to begin with an initial query, gather information, and then refine their perspective on the claim, which often leads to the formulation of more effective follow-up queries.\nTo address this gap, we introduce Fact-checking with Iterative Retrieval and VErification (FIRE), an innovative agent-based framework that integrates both the internal knowledge of LLMs and external knowledge sources by unifying the verification process and search query generation into a single step. As illustrated on the right side of Figure 1, FIRE employs a mechanism to decide whether to produce the final answer or generate a new search query, continuing the evidence-seeking process. This decision is based on the model's confidence in its judgment. The closest related work to us is SAFE (Wei et al., 2024), depicted in the center of Figure 1. Their method generates web search queries iteratively and subsequently verifies whether the entire retrieved evidence supports the claim. However, this approach lacks flexibility, as it treats evidence retrieval and claim verification as distinct processes, requiring a predetermined fixed number of searches regardless of the claim's complexity. In contrast, our approach integrates evidence retrieval and claim verification into an iterative framework, encouraging the language model to verify based on its own knowledge and conduct searches only when necessary. Our experiments demonstrate that our method significantly reduces the computational costs of LLMs by an average factor of 7.6, as well as search-related costs by a factor of 16.5, all while maintaining fact-checking performance.\nIn summary, our contributions are as follows:\n\u2022 We present FIRE, a simple yet effective interactive framework for fact-checking. Through extensive experiments conducted across multiple datasets, we demonstrate that our framework significantly reduces the LLM computational and search costs, making it a better option for large-scale production.\n\u2022 Our ablation studies demonstrate that the step-by-step reasoning process enhances the model's confidence in fact-checking, particularly with GPT-40-mini. For GPT-40, we observed a similar trend; however, the effect was not as pronounced as that seen with GPT-40-mini.\n\u2022 We conducted an error analysis and identified several quality issues in the current benchmark datasets, including the presence of ungrounded claims. Additionally, the strict reasoning capabilities of the LLM may incorrectly classify some debatable claims as non-factual."}, {"title": "2 Related Work", "content": "LLM Factuality Despite the remarkable capabilities of LLMs (Brown et al., 2020; OpenAI, 2023; Zhao et al., 2023), the auto-regressive learning objective does not inherently offer strong guarantee or enforce the learning of factual accuracy in the training process, making these models produce content that deviates from real-world facts (Wang et al., 2024a). On average, there are 5%-10% false claims in responses of GPT-4 (OpenAI, 2023) and LLaMA-2 (Touvron et al., 2023) on world-knowledge questions (Iqbal et al., 2024). Retrieval-augmented generation (Guu et al., 2020) and post-generation fact-checking are essential for ensuring accurate knowledge dissemination. Retrieving highly relevant information plays a pivotal role in both guiding generation as a reference and determining verification results in fact-checking systems (Wang et al., 2023).\nThe retriever and verifier are the most resource-consuming components in fact-checking systems, in terms of time and cost. Even with the inexpensive APIs (e.g., Serper at 0.001 USD per request and GPT-3.5-turbo for verification), verifying an atomic claim costs approximately 0.02 USD, making extensive verification impractical for general users (Iqbal et al., 2024). This high cost limits the ability to verify large volumes of LLM responses, potentially contributing to the spread of misinformation. Our framework aims to minimize the costs in these two steps, enabling affordable verification for general users. This allows them to easily verify suspicious or doubtful information, enhancing the dissemination of factual information.\nFact Checking with Agents The recent advancements in LLMs have spurred significant research on LLM-powered agents, which are capable of reasoning about their environment and making decisions by either invoking external tools or performing internal actions (Wang et al., 2024b). These agent frameworks typically consist of several components, including reasoning, tool usage, memory, and multi-agent debate (Masterman et al., 2024), many of which can be seamlessly integrated into fact-checking pipelines to enhance the performance of traditional fact-checking systems. For example, recent works have endowed systems with the ability to call external tools, such as search engines (Chern et al., 2023; Wang et al., 2023; Wei et al., 2024; Cheng et al., 2024), recognizing that many claims in the field require additional information for verification. During the verification stage, Sun et al. (2024) proposed a Markov Chain-based multi-agent debate approach to ensure more rigorous verification by enabling collaborative decision-making among agents based on retrieved evidence. Our work differs from previous approaches by combining the evidence retrieval and verification stages, leveraging agents' reasoning and tool-use capabilities to more closely simulate human cognitive processes in fact-checking."}, {"title": "3 Framework", "content": "Assessing the factual accuracy of long-form text presents significant challenges (Min et al., 2023). To address this, prior approaches have broken down the text into individual checkworthy claims (Chern et al., 2023). These sentences, referred to as atomic claims, are fact-checked individually, with their factuality scores aggregated to evaluate the overall factual accuracy of the original text. Previous research indicates that verifying the factuality of atomic claims is the most challenging step in this process (Wang et al., 2023). Our work therefore focuses on this critical task: determining the factual accuracy of individual atomic claims, classifying each as either True or False."}, {"title": "3.1 FIRE", "content": "We present FIRE, a simple yet effective agent-based framework for interactive claim verification through web searches. As illustrated in Figure 1, FIRE takes an atomic claim as input and outputs a binary label indicating whether the claim is factual or non-factual. The framework consists of three key components: Final Answer or Next Search Query, Web Search, and Final Verification, each of which we will explain below.\nFinal Answer or Next Search Query We introduce a unified method, Final Answer or Next Search Query $f(\\cdot)$, which integrates claim verification with search query generation. This component processes an atomic claim c and determines whether to produce a final answer a or to generate an additional search query q. This decision is informed by both an external evidence set E, derived from search results, and the internal knowledge k of the language model, acquired during pre-training. To facilitate this, we incorporate confidence estimation into the reasoning process. The model outputs a final answer a when its confidence is sufficiently high; otherwise, it generates an additional query q. This process can be formalized as follows:\n$f(c, E, k) =\\begin{cases}a, & \\text{if confident} \\\\q, & \\text{if not confident}\\end{cases}$\nThis method offers greater flexibility by eliminating the need to retrieve a fixed number of evidence items before verification, thereby largely reducing search costs. A detailed description of the prompt used for this component is provided in Appendix A."}, {"title": "3.2 Prevention of Repetitive Search Queries", "content": "In our preliminary studies, we identified a recurring issue with sequential search query generation using language models: the tendency of these models to generate repetitive queries. This occurs even when the models are explicitly instructed to generate queries targeting new, claim-relevant information. As a result, identical queries are repeatedly submitted to web search tools, leading to inefficient use of search resources. To address this issue, we investigate following methods for enhancing search query generation and reducing repetition.\nEarly Termination The iterative process is terminated when consecutive queries or retrieved results exhibit a high degree of similarity, indicating diminishing returns.\nDiversity Prompt We introduce additional prompts to encourage the model to generate more diverse queries when consecutive similar queries or search results are detected."}, {"title": "3.3 Prevention of Verification Overconfidence", "content": "LLMs can exhibit strong calibration abilities across diverse tasks (Kadavath et al., 2022; Geng et al., 2024). Consequently, they are aware of their confidence levels during the claim verification process. However, our preliminary analysis reveals that LLMs often demonstrate excessive strictness and unwarranted confidence in certain cases, leading to errors. Considering this, we explore several techniques to prevent overconfidence in verification:\nAt Least One/Two At Least One requires models to retrieve at least one evidence during the verification, which increase the probability of eliminating overconfidence. Similarly, we also adopted a more aggressive approach At Least Two to retrieve a second evidence to reduce the uncertainty.\nInclusive Prompt In this setting, we prompt models to be \"less strict, open-minded and avoid being over confident\" to encourage models to reflect on their confidence level of answers."}, {"title": "4 Experiments Setup", "content": "In our study, we utilized four datasets from prior research that align with our experimental setup:\nFacTool (Chern et al., 2023), FELM (Chen et al., 2023), Factcheck-Bench (Wang et al., 2023), and\nBingCheck (Li et al., 2024b).\nFacTool and FELM provide factuality claims\nacross multiple domains. From these, we selected instances requiring world knowledge for verification, which we refer to as FacTool-QA and FELM-WK, both with binary annotations (True or False). Factcheck-Bench and BingCheck, however, pose a four-label classification task: supported, partially supported, not supported, and refuted. Following Iqbal et al. (2024), we merged supported and partially supported into True, treated refuted as False, and excluded not supported. Given BingCheck's imbalance (3,581 True, 42 False), we sampled 100 True claims for our test set. For FELM-WK, we used un-split claims to avoid loss of context. Dataset statistics are in Table 2.\nIn our experiments, we first use the Factcheck-Bench dataset as a development set to optimize the settings for our framework. We then evaluate its performance on the remaining three datasets, comparing it with other competitive fact-checking systems."}, {"title": "4.2 Language Models", "content": "We investigate several state-of-the-art (SOTA) language models, including proprietary models from two prominent families: GPT models (OpenAI, 2024a,b) and Claude models (Anthropic, 2024), as detailed in Table 1. In addition, we assess two open-source models: LLaMA 3.1-Inst 8B (Dubey et al., 2024) and Mistral-Inst 7B (Jiang et al., 2023)."}, {"title": "4.3 Compared Fact-checking Frameworks", "content": "We select several SOTA fact-checking frameworks for comparison. Additionally, we introduce two baseline models: Random and Always True/False. To further assess the impact of LLM reasoning in fact-checking, we include an ablation setting.\nFACTOOL is adaptable across domains and tasks, using a tool-augmented framework that integrates external tools like Google Search and Python interpreters to assess the factuality of content from large language models. However, this can introduce complexity and depend on the accuracy of these external tools.\nFACTCHECK-GPT excels in fine-grained factuality evaluation through a detailed benchmark with annotations at the claim, sentence, and document levels. While resource-intensive, it provides valuable insights into specific stages of factual inaccuracies.\nSAFE uses a search-augmented approach to verify long-form content by breaking it down into individual facts and checking them via Google Search. This method is cost-effective compared to human annotation but depends on the reliability of search engine results, which can vary and introduce biases.\nRandom assigns the predicted label for each claim in the test set randomly, choosing between True and False with equal probability.\nAlways True/False is an approach that always predicts a single label \u2013 either True or False \u2013 for all claims in the test set.\nFIRE (No Reason) utilizes the same framework as FIRE; however, it is explicitly instructed not to articulate its reasoning process in the output. This modification aims to assess the impact of explicitly stating the step-by-step reasoning process on the results."}, {"title": "4.4 Evaluation Metrics", "content": "In this work, we investigate the trade-off between computational cost and fact-checking performance.\nPerformance We evaluate precision, recall, and F1 scores for both positive and negative classes.\nComputational Cost We report the financial costs of LLM API calls for proprietary models and GPU rental expenses for open-source models, alongside an analysis of API costs from search engine queries and a breakdown of the total time spent on the fact-checking process. The experiments using open-source models were conducted on an NVIDIA RTX 6000 GPU at an estimated cost of $0.79 per hour, while search queries via SerpAPI incurred approximately $0.00105 per search."}, {"title": "5 Results", "content": "In this section, we first present preliminary studies on Factcheck-Bench (\u00a7 5.1), focusing on three key aspects: language models, prevention of repetitive search queries, and prevention of verification overconfidence. These studies aim to identify the most appropriate configurations for our framework. Subsequently, we compare FIRE to other strong fact-checking frameworks across three additional datasets (\u00a7 5.2) to evaluate the generalization capabilities of our approach."}, {"title": "5.1 Preliminary studies", "content": "Language Models We present a performance comparison of various language models in Table 3. Overall, proprietary language models generally outperform open-source models, likely due to their larger size and more sophisticated training in reasoning and tool utilization. Among the proprietary models, the latest and most advanced offerings from different organizations\u2014specifically o1-preview from OpenAI and Claude-3.5 Sonnet from Anthropic\u2014exhibit the best performance. Although the more economical model, GPT-40-mini, performs slightly worse than the top-performing o1-preview, it offers a cost savings of 766 times. This suggests that for fact-checking tasks, the most advanced models may not be necessary; GPT-40-mini can serve as a sufficiently capable alternative at a significantly lower cost. We will continue our preliminary studies using GPT-40-mini.\nPrevention of Repetitive Search Queries We conducted an experimental analysis to evaluate the impact of Early Termination and Diversity Prompt on mitigating the generation of repetitive search queries. To assess query similarity, we employed Sentence-BERT (all-MiniLM-L6-v2; Reimers and Gurevych (2019)) with a similarity threshold of 0.9, as established by Shashavali et al. (2019). Table 4 presents experimental results, where window size refers to the predefined number of consecutive similar queries or retrieval results. Once this threshold is reached, early termination is triggered to prevent further query generation and retrieval. If the model generates queries or retrieves results exhibiting high similarity within this window, the system also activates an early stopping mechanism. The results indicate that optimizing the similarity window size effectively reduces search costs without compromising the model's performance. However, our findings suggest that the diversity prompt does not enhance performance. In our optimal configuration, we selected a window size of 2 without utilizing the diversity prompt.\nPrevention of Verification Overconfidence We present the performance and cost of various overconfidence prevention approaches for verification on Factcheck-Bench in Table 5. Interestingly, the At Least One/Two settings, which aggressively retrieve additional evidence, result in higher search costs without improving fact-checking performance compared to the Default setting, where no explicit constraints are placed on web search. This supports our hypothesis that most atomic claims are relatively straightforward and do not require extensive external web searches for verification. In fact, introducing additional searches may introduce noise, negatively impacting performance. The Inclusive setting encourages models to be more flexible and open to alternative interpretations of evidence, which reduces the need for queries but also leads to lower overall performance. Based on these observations, we maintain the Default setting, leveraging the language model's reasoning capabilities without imposing additional search constraints."}, {"title": "5.2 Comparisons to Other Frameworks", "content": "We present a performance comparison of our framework against other frameworks in Table 6 and a cost analysis in Table 7. As shown, all frameworks exhibit similar performance, with a small gap of approximately 0.2 to 0.5. Our framework, using GPT-40, performs slightly better, achieving superior results on 7 out of 18 metrics, followed closely by SAFE with GPT-40 at 6 metrics. This suggests that all frameworks can effectively perform fact-checking for most claims, although they may encounter difficulties with challenging examples, which we analyze further in \u00a7 6. In terms of model size, GPT-4o generally outperforms GPT-40-mini across most frameworks, indicating that larger models are more effective in detecting misinformation.\nHowever, the performance improvement is modest, as it comes with an average increase of 16.7 times in LLM costs and an additional 3-fold increase in search costs when using FIRE. When considering all frameworks with GPT-40-mini, FIRE demonstrates cost savings of 7.6 times in LLM expenses and 16.5 times in search costs compared to other frameworks. Therefore, we argue that FIRE, when paired with GPT-40-mini, presents a compelling option for the large-scale production of fact-checking systems.\nFigure 2 illustrates the impact of reasoning on the number of web searches conducted by GPT-40 and GPT-40-mini when using BingCheck. Notably, GPT-40-mini demonstrates a high level of confidence in making verifications when it is allowed to articulate its reasoning process, resulting in the majority of judgments being made without any searches. Conversely, when not permitted to express its reasoning, there is a significant decrease in the number of instances with zero searches; most cases now involve at least one search, indicating a marked reduction in GPT-40-mini's confidence in its judgments. This observation aligns with previous findings that the presence of CoT reasoning correlates with increased confidence in the model's answers (Wang and Zhou, 2024). While GPT-40 also shows a decline in confidence when it is not allowed to search, the decrease is less pronounced than that observed in GPT-40-mini. By combining the performance and cost results presented in Table 6 and Table 7, we find that, in the absence of a reasoning process, the costs associated with LLMs can be reduced through fewer completion tokens. However, this reduction leads to increased search costs, resulting in overall performance that is inferior to scenarios in which the models are permitted to engage in step-by-step reasoning. Furthermore, the step-by-step reasoning approach facilitates more effective error analysis."}, {"title": "6 Error Analysis", "content": "To identify weaknesses in our fact-checking system, we manually examine failed cases of three datasets: FELM-WK, FacTool-QA, and BingCheck, analyzing whether the majority of failures is attributed to inadequate retrieved evidence or to flaws in the LLM verification process, despite the availability of reliable evidence.\nWe summarized errors into four major issues and nine error types. Among the total number of 135 failed claims, there are 44 cases falling into challenges of (I) inaccurate identification of checkworthy claims and false gold labels in the original datasets, 50 claims are due to (II) inaccurate or insufficient knowledge applied to verification, either internally extracted from LLM parameters or externally collected from web pages. The rest 26 and 15 cases result from LLM reasoning ability and debatable opinions over some topics, respectively, as shown in Table 8.\nThe major issue lies in collecting sufficient evidence, especially for long claims containing many aspects to verify. This can be approached by decomposing \u201catomic claims\" from the original dataset into the real granularity of \"atomic\u201d, each containing only 1-3 pieces of information. The second problem focus on the quality of benchmarking datasets, particularly FELM-WK that includes many ungrounded claims and labels (Li et al., 2024a), which may lead to ineffective comparisons between fact-checking systems. Interestingly, beyond incorrect reasoning, overly-strict reasoning by exact matching between the claim and collected evidence can also lead to verification errors. For example, LLMs label a claim as false when the claim states FUN Word-Cross Puzzle while evidence mentions Word-Cross Puzzle. Additionally, some claims can be viewed as true from one perspective but false from another, as seen in debates over the origins of fortune cookies, where the truth of related claims is debatable.\nConsidering above, to further advance the field of fact-checking, we highlight the need for improved benchmarking datasets, a stronger focus on verifying fine-grained claims, and strategies to guide LLMs in performing verification under more flexible reasoning conditions, such as semantic alignment, rather than relying exclusively on exact matches."}, {"title": "7 Conclusions and Future Work", "content": "Conventional fact-checking systems typically separate the steps of evidence retrieval and claim verification, leading to suboptimal utilization of the verification models' internal knowledge. To address this, we propose FIRE, a novel framework that integrates evidence retrieval and claim verification in an iterative process. FIRE enables LLMs to leverage their internal knowledge for judgment and only rely on external evidence retrieval when uncertain. Our experiments on multiple datasets demonstrate that FIRE not only slightly improves accuracy but also reduces LLM computation costs by an average of 7.6 times and search costs by 16.5 times, making it highly efficient for production use. Additionally, we performed a detailed error analysis, which revealed issues with the benchmarking datasets quality. These findings highlight the need for further research into edge cases, rather than relying solely on automatic metrics for evaluation.\nWe identify several promising directions for future work: (1) Integrating memory banks to store verification results, enabling the system to leverage previous results instead of repeatedly executing the entire process; (2) Expanding the system to support additional modalities, such as code and images."}, {"title": "Limitations", "content": "We acknowledge certain limitations in this work that we plan to address in future research. First, to maintain the efficiency of our framework, we implement the Final Answer or Next Search Query in a compact manner to retrieve evidence, assess confidence on knowledge, and verify the final answer within a single step. Ideally, this can be implemented via separate process which involves a standalone confidence estimation step. This would offer greater flexibility and interpretability. We plan to leave this to our future work. Second, for fair comparison across multiple fact-checking datasets, our system adopts a binary labeling scheme (True or False) and standarlizes the labels across datasets. However, this may not fully reflect the complexity of factual labels in the real world. We intend to incorporate fine-grained labeling schemes in the future."}, {"title": "Ethical Statement and Broad Impact", "content": "Data License A primary ethical consideration is the data license. We reused pre-existing dataset, FactBench, FACTOOL, FELM-WK, BingCheck, which have been publicly released and approved for research purposes. We adhere to the intended usage of all these dataset licenses.\nEthical Statement We acknowledge that our system relies on LLMs, which can sometimes produce biased or incorrect judgments due to the data used in their pre-training or biases present in external sources. Additionally, there is the risk of over-reliance on the system for making critical factual judgments without human oversight. To mitigate these risks, we strongly encourage human reviewers to be involved in decision-making, especially in high-stakes domains such as legal, political, or medical contexts.\nBroad Impact FIRE has the potential to advance the field of automated fact-checking by enhancing its efficiency and accessibility. Its capability to iteratively retrieve evidence while minimizing computational costs will empower a broader range of users\u2014including journalists, researchers, and the general public\u2014to verify factual information with greater ease. Furthermore, FIRE can be applied to large-scale implementations, such as integration into search engines and social media platforms, thereby contributing to efforts to combat the spread of misinformation."}, {"title": "A Prompts for Verification", "content": "Default prompt We use the following prompt to guide the language model in verifying the atomic claim, determining whether to provide a final judgment or issue an additional Google search query based on the current status. The prompt will output reason or explanation for the verification process."}, {"title": "B Prompt for Final Verification", "content": "Upon reaching the maximum number of steps, we issue the following prompt to compel the language model to make a final judgment based on the accumulated information."}]}