{"title": "Spatio-Temporal Partial Sensing Forecast for Long-term Traffic", "authors": ["Zibo Liu", "Zhe Jiang", "Zelin Xu", "Tingsong Xiao", "Zhengkun Xiao", "Haibo Wang", "Shigang Chen"], "abstract": "Traffic forecasting uses recent measurements by sensors installed at chosen locations to forecast the future road traffic. Existing work either assumes all locations are equipped with sensors or focuses on short-term forecast. This paper studies partial sensing traffic forecast of long-term traffic, assuming sensors only at some locations. The study is important in lowering the infrastructure investment cost in traffic management since deploying sensors at all locations could incur prohibitively high cost. However, the problem is challenging due to the unknown distribution at unsensed locations, the intricate spatio-temporal correlation in long-term forecasting, as well as noise in data and irregularities in traffic patterns (e.g., road closure). We propose a Spatio-Temporal Partial Sensing (STPS) forecast model for long-term traffic prediction, with several novel contributions, including a rank-based embedding technique to capture irregularities and overcome noise, a spatial transfer matrix to overcome the spatial distribution shift from permanently sensed locations to unsensed locations, and a multi-step training process that utilizes all available data to successively refine the model parameters for better accuracy. Extensive experiments on several real-world traffic datasets demonstrate that STPS outperforms the state-of-the-art and achieves superior accuracy in partial sensing long-term forecasting.", "sections": [{"title": "1 INTRODUCTION", "content": "Background: The traffic forecast problem is to use the recent measurements by sensors installed at chosen locations to forecast the future road traffic at those locations. This problem has significant practical values in traffic management and route planning, considering the ever worsening traffic conditions and congestion in cities across the world. Most existing work considers a full-sensing scenario, where sensors are installed at all locations. We observe that a lower cost, more flexible solution should support partial sensing, where only some locations have sensors, which still allow traffic forecast to cover other locations without sensors. This research is in its nascent stage, with limited recent work on short-term forecast. This paper investigates the partial sensing long-term forecast problem that aims to train a prediction model to use recent measurements from sensed locations (with sensors) to forecast traffic at unsensed locations (without sensors) deep into the future. Consider a scenario in a road network where only 10% of the locations are equipped with sensors. The long-term forecast model may utilize just one hour of recent data from the sensed locations to predict traffic conditions up to eight hours ahead at the unsensed 90% of the locations, where the amount of predicted future data is 72 times of the past measurements. The capability of long-term traffic forecast under partial sensing can play a crucial role for low-cost intelligent transportation. For example, early warnings about future traffic conditions may help traffic planners in traffic signal management, law enforcement operations, medical assistance, and disaster response [3, 5].\nChallenges: The partial sensing long-term forecast problem poses several non-trivial challenges. Fig. 1 shows the traffic rates at locations 44, 121 and 33 over two days from the PEMS08 dataset. Let's suppose location 44 and 121 are sensed, whereas location 33 is not. How could we train a model to infer the traffic rates of location 33 deep into the future based on the recent measurements at locations 44 and 121, when their patterns are so different? This requires not only the adaptability of a model to transfer the knowledge from sensed locations to unsensed locations based on the limited input features but also learning the intrinsic, subtle spatio-temperal connections across the locations. Next, long-term forecasting requires the learning of informative embeddings capable of capturing more intricate spatial and temporal correlations than what is required for short-term forecasting. In contrast to short-term models, which might use an hour of data to predict the next hour, long-term models must forecast far beyond this scope, necessitating a more nuanced understanding and representation of the data. We also observe traffic fluctuations at the locations of Fig. 1, paritcularly at location 44. How to smooth out fluctuations and reduce the impact of other noises are important in forecasting long-term traffic. Finally, beside the normal daily/weekly traffic patterns, there are irregular patterns caused by infrequent events such as road closure due to accidents or heavy traffic in holidays, which have limited training data. For example, at location 44 in Fig. 1, which is at a highway, the traffic rate drops abnormally to almost zero from 08/19 15:00 to 20:00, possibly caused by an accident. If the model is not efficient in capturing such irregularities, traffic forecasts in such times will carry significant errors.\nRelated Work: The existing work on traffic forecast can be categorized into full sensing forecast, where all locations of interest are deployed with sensors, or partial sensing forecast, where only some locations are deployed with sensors. Full sensing forecast has been studied extensively, including recurrent neural networks (RNNs) [41, 47, 56], convolutional neural networks (CNNs) or Multiple Layer Perceptrons (MLPs) with graph neural networks (GNNs) [14, 25, 27, 48, 51, 53, 54], neural differential equations (NDES) [4, 19, 20, 30, 36], transformers [13, 28], and most recent mixture-of-experts (MOE) [40] based spatio-temporal model [23].\nOnly two recent papers, Frigate [8] and STSM [44], studied the partial sensing forecast problem, but they are designed for short-term forecasting. Besides, STSM utilizes the location properties such as nearby shopping malls as extra prior knowledge.\nThe prior work can also be categorized into short-term spatio-temporal forecasting (above referenced), or long-term time series forecasting [6, 29, 34, 49], which do not sufficiently model spatial correlation patterns.\nThere exist spatial transfer learning methods for traffic forecasting [17, 32, 33, 35, 46], which utilize traffic measurements from additional locations to help forecast the traffic at the locations of interest. They belong to the full sensing category, with extra input from additional locations. Several imputation methods [21, 26, 52, 57] estimate the missing data at the locations of interest where sensors may sometimes miss measurement. They do not belong to the partial sensing category because the locations all have sensors. Also related are the spatial extrapolation methods that could be used to estimate traffic data at unsensed locations based on data from sensed locations within the same time period, such as non-negative matrix factorization methods [18, 22, 24], and GNN methods [2, 9, 50, 59]. However, these methods are for extrapolation only and do not model the spatio-temporal dynamics of long-term traffic forecast. Their performance in our experiments is not as good as the best of the models designed for traffic forecasting.\nIn summary, this is the first work specifically on the partial sensing long-term traffic forecast problem.\nContributions: We propose a Spatial Temporal Partial Sensing forecast model (STPS) for long-term traffic forecasting. STPS has three main technical contributions. First, we design a rank-based node embedding, which helps capture the characteristics of traffic irregularities and make the model more robust against noises in learning intricate spatio-temporal correlations for long-term forecast. Second, we propose a spatial transfer module, which aims to enhance our model's adaptability by extracting the dynamic traffic patterns from the transfer weights based on rank-based embedding in addition to spatial adjacency, allowing for more nuanced and accurate predictions. Third, we use a multi-step training process to fully utilize the available training data. This approach enables successive refinement of the model parameters. Extensive experiments on several real-world traffic datasets demonstrate that our model outperforms the state-of-the-art and achieves superior accuracy in partial sensing long-term forecasting."}, {"title": "2 PRELIMINARIES", "content": "2.1 Problem Definitions\nDefinition 1: Road Topology and Traffic Flow Rates. Consider a road system, and let $\\mathcal{N}$ be a set of chosen locations where traffic statistics are of interest. Let $n = |\\mathcal{N}|$. The road topology of these locations is represented as a graph $\\mathcal{G} = (\\mathcal{N}, A)$. $A = [A_{i,j}, i, j \\in \\mathcal{N}] \\in \\mathbb{R}^{n \\times n}$ is an adjacency matrix. $A_{i,j} = 1$ indicates that there is a road between location i and location j; otherwise $A_{i,j} = 0$. A traffic flow consists of all the vehicles that pass a location in $\\mathcal{N}$; the flow rate is defined as the number of vehicles passing through the location during a preset time interval. It is a discrete function of time if we partition the time into a series of time intervals of a certain length, e.g., 5 min. For simplicity, we normalize each time interval as one unit of time. Let M be a subset of locations, i.e., $M \\subset \\mathcal{N}$, and T be a series of time intervals. As an example, T could be ${t - l + 1, ..., t - 1, t}$ of l intervals, where t is the current time. The traffic matrix over locations M and times T is defined as $X_{M,T} = [X_{i,j}, i \\in M, j\\in T]$, where $X_{i,j}$ is the flow rate at location i during time j. Further denote the rate vector at location i as $X_{i,T} = [X_{i,j}, j\\in T]$. Hence, $X_{M,T} = [X_{i,T}, i \\in M]$.\nDefinition 2: Problem of Full-Sensing Traffic Forecast. Suppose all the locations in $\\mathcal{N}$ are equipped with permanent sensors to continuously measure the flow rates. The problem is to forecast a future traffic matrix $X_{\\mathcal{N},T'}$ on all locations in $\\mathcal{N}$, based on a past traffic matrix $X_{\\mathcal{N},T}$ that has just been measured by the sensors, where $T' = {t + 1, ..., t + l'}, T = {t - l + 1, ..., t - 1, t}$, and t is the current time. It is called full-sensing traffic forecast because all locations under forecast are fully equipped with sensors to provide traffic information of recent past. In most work [7, 11, 12, 16, 37, 42], both l and l' are set to 1 hour for short-term forecast. Note that l should not be too large to avoid excessively large models and computation costs that come with them. Moreover, research has shown that too large l may actually degrade forecast accuracy [55]. For long-term forecast, l' is set much larger than l.\nDefinition 3: Problem of Partial-Sensing Traffic Forecast. Suppose only a subset M of locations are equipped with permanent sensors to continuously measure their flow rates. The subset of locations without permanent sensors is denoted as $M' = \\mathcal{N} - M$. Let $m = |M|$ and $m' = |M'|$. The partial adjacency matrix is $A_{M,M'} = [A_{i,j}, i \\in M, j\\in M'] \\in \\mathbb{R}^{m \\times m'}$. The problem is to forecast a future traffic matrix $X_{M',T'}$ over the locations without sensors, based on a recent traffic matrix $X_{M,T}$ that has been just measured at the locations with sensors, where $T' = {t+1, ..., t+l'}, T = {t-l+1,..., t-1, t}$, and t is the current time. It is called partial-sensing traffic forecast because locations in $\\mathcal{N}$ are partially equipped with sensors; for long-term forecast, $l' >> l$. This problem is the focus of our work.\nIn practice, one may want to forecast both $X_{M',T'}$ and $X_{M,T'}$ from $X_{M,T}$. In this paper, we only consider $X_{M',T'}$ because predicting $X_{M,T'}$ from $X_{MT}$ is essentially the full-sensing prediction problem, which has been thoroughly studied."}, {"title": "2.2 Inference vs Training", "content": "The problem of partial sensing traffic forecast is illustrated by the left plot of Fig. 2. After a forecast model is trained and deployed, the sensors at the locations in M will measure flow rates $X_{M,T}$, which will be used as input to the model to inference (forecast) future rates of the locations in M', i.e., $X_{M', T'}$. Hence, when we use the model for forecasting, the only available data is $X_{M,T}$, which is shown in blue.\nHowever, to train such a model (before its actual deployment), as illustrated in the middle plot, we will need the ground truth of $X_{M',T'}$ in the training data, which is shown in gray. We assume that mobile sensors are deployed at the locations in M' for a period of time to collect the training data. This is reasonable because mobile sensors can be re-depolyed at different road systems for data collection, offering a lower overall cost than implementing permanent sensors at all locations of all road systems that need traffic forecast.\nWith mobile sensors deployed to collect $X_{M',T'}$, we may naturally use them to collect $X_{M',T}$ as well. The permanent sensors at M will collect $X_{M,T'}$. So, the training data will contain $X_{M,T}, X_{M',T}, X_{M,T'}$ and $X_{M',T'}$, as illustrated in gray in the third plot. While the input to the model is $X_{M,T}$ and the output $X_{M',T'}$, we should still fully utilize the information of $X_{M',T}$ and $X_{M,T'}$, which is available in the training data, to build an accurate model."}, {"title": "2.3 Embeddings", "content": "Embeddings are the common technique to enhance feature expression. We adopt the embedding setting in [28, 37], where the time-of-day embedding and the day-of-week embedding capture the temporal information, and node embedding captures the spatial property of locations. We use $E_{tod} \\in \\mathbb{R}^{n \\times d}$, $E_{dow} \\in \\mathbb{R}^{n \\times d}$, and $E^0 \\in \\mathbb{R}^{n \\times d}$ to denote them respectively, where d is the embedding dimension. A day is modelled as $N_{tod} = 288$ discrete units of 5 minutes each. A week has $N_{tod} = 7$ days. The time-of-day is i/Ntod for the ith unit in the day, and the day-of-week is i/Ndow for the ith day in the week. Trainable vectors $B_{i}^{tod} \\in \\mathbb{R}^{d}, i \\in [0, N_{tod})$, and $B_{i}^{dow} \\in \\mathbb{R}^{d}, i \\in [0, N_{tod})$, are the time-of-day embedding bank and the day-of-week bank. In practice, for the input flow rate data $X_{\\mathcal{N},T} \\in \\mathbb{R}^{n \\times l}, T = {t - l + 1, ..., t - 1, t}$, we only consider the time feature of flow rate data $X_{\\mathcal{N},t-l+1} \\in \\mathbb{R}^{n}$ at $t - l + 1$, yielding $E_{tod} \\in \\mathbb{R}^{n \\times d}$, $E_{dow} \\in \\mathbb{R}^{n \\times d}$. For the node embedding, given any location $i \\in \\mathcal{N}$, the trainable node embedding bank is $B_i \\in \\mathbb{R}^{d}$. For the flow rate data $X_{\\mathcal{N},T}$, we obtain $E^0 \\in \\mathbb{R}^{n \\times d}$ from node embedding bank B over n total locations. In the following, we will use M, M', N as the subscript of the embedding bank B and the embedding E to represent the locations of interest.\nIn addition to the above, this paper will introduce a new rank-based node embedding."}, {"title": "3 THE PROPOSED APPROACH", "content": "This section introduces our Spatio-Temporal Partial Sensing (STPS) traffic forecast model. In Fig. 3, the training phase (left plot) consists of three steps: 1. the dynamic adaptive step, which builds a module with $X_{M,T}$ as input and $X_{M',T}$ as expected output; 2. the long-term forecasting step, which builds another module with $X_{M,T}$ and the previous module's output as input, and with $X_{M,T'}$ as expected output; and 3. the aggregation step, which builds yet another module with $X_{M,T}$ and the previous two modules' output as input, and with $X_{M,T'}$ as expected output. We refer to the above three modules as the dynamic adaptive module, the long-term forecasting module, and the aggregation module, respectively, which together form the proposed STPS. Each step leverages the module parameters derived from its previous step(s) and incorporates new information (expected output) to learn its module parameters, ensuring progressive enhancement in model performance.\nThe testing (or inference, forecast) phase in the right plot also consists of the steps: 1. it uses the dynamic adaptive module, with $X_{M,T}$ as input, to inference $X_{M',T}$; 2. it uses the long-term forecasting module, with $X_{M,T}$ and the output of the previous step as input, to inference $X_{MT'}$; and 3. it uses the aggregation module, with $X_{M,T}$ and the output of the previous two steps as input, to finally forecast $X_{M',T'}$.\n3.1 Rank-based Node Embedding\nFirst, consider irregular traffic patterns. As shown in Fig. 1, location 121 (top blue curve) at a highway from dataset PEMS08 demonstrates a regular traffic pattern from the day time high on 08/19 to the deep night low and back to the day time high on 08/20, and so on. These daily (or weekly) traffic patterns can be captured by the temporal embeddings, $E_{tod}$ (time of day) and $E_{dow}$ (day of week), which however are less effective in addressing the irregular patterns that do not have daily or weekly cycles. For example, in Fig. 1, although location 44 (middle red curve), also at a highway from dataset PEMS08, demonstrates a daily traffic pattern, it experienced a sharp drop to almost zero at 08/19 15:00 and recovered about 5 hours later. This could be a highway closure or partial block due to reasons such as a car accident. Such irregular events are infrequent in the dataset, which makes it harder for our model to be trained to recognize them. The existing node embedding $E^0$ from the prior work cannot change temporally to reflect such changes in the flow rates.\nTo better capture such irregular traffic patterns, we introduce a rank-based node embedding. Consider an arbitrary time interval $j \\in T$ in the input $X_{M,T}$. We assign a rank value to each location $k \\in M$: Sort $X_{i,j}, \\forall i \\in M$, in ascending order (with ties broken arbitrarily), and the rank of location k at time j is the index number of $X_{k,j}$'s position in the ordered list. Our first hypothesis is that the new rank-based embedding can help distinguish the irregularities caused by road closure or other rare events, such as the one with location 44 in Fig. 1, since the location's rank during the time of road closure will be probably among the lowest, whereas its rank at normal times would be much higher. In the event of road closure, the ranks of dependent locations over adjacent time periods will likely change, too, deviating from normal patterns. It is easier to tell such irregularities based on location, rank, time-of-the-day, and day-of-the-week, than based on location, time-of-the-day, and day-of-the-week, since the rank value provides additional hint. For spatio-temporal irregularities, the exact flow rates of the affected locations vary greatly at different times in a day or on different days in a week. Yet ranks, which are the indices of those locations' rates in the ascending list of all locations' rates, can be considered as normalized, relative flow rates, whose distribution tends to be more stable from time to time than the actual rates, revealing the intrinsic pattern of an irregularity. This property will help the model identify irregular patterns and separate them from normal patterns, with limited training data for irregularities due to their infrequent occurrences.\nOur second hypothesis is that the new rank-based embedding can help the model be more robust to noises of a traffic pattern. In Fig. 1, except for the irregularity at 08/19 15:00, location 44 exhibits a normal daily traffic pattern with noises as the traffic fluctuates, which could be normal or sometimes due to inaccuracy sensor reading. Noises can also happen due to inference errors. Refer to the right plot in Fig. 3 for inference. After the first step, we have the estimated $X_{M',T}$, which carries the inference errors from the dynamic adaption module. These errors are noises to the next step. Comparing to the actual flow rates, the ranks are discretized values that tend to smooth out noise fluctuation and be more stable. Especially for partial sensing, the fewer the number of sensed locations is, the more stable the rank-based embedding becomes. The stability of the ranks could also enhance the model's adaptability from one spatial area (say, with sensor data) to a new area (without sensor data), whereby their actual flow rates can differ greatly but their relative ranks follow stable patterns. In other words, the rank-based embedding is more robust to the distribution shift from the sensed locations to the unsensed locations, and between the training dataset and the test dataset.\nWe have run extensive experiments on node-based embedding to test the validity of the above hypotheses.\nSpecifically, at each time interval, we rank the flow rates at all locations. Given the i-th rank of the locations, vector $B_i \\in \\mathbb{R}^{d}$ denotes the rank-based node embedding bank for rank i. For input data $X_{\\mathcal{N},T}$ (or $X_{M,T}$ for the first step), we rank each time interval in T over n locations, yielding the rank feature $E \\in \\mathbb{R}^{n \\times l \\times d}$. A fully connected layer is then applied to the rank-based node embedding to aggregate over the length l, resulting in an aggregated rank-based node embedding $E_r' \\in \\mathbb{R}^{n \\times d}$, encapsulating a generalized high-dimensional rank feature for each location over a period.\nUltimately, a Multi-Layer Perceptron (MLP) processes the raw data $X_{\\mathcal{N},T}$ (or $X_{M,T}$ for the first step) to produce the feature embedding $E_f \\in \\mathbb{R}^{n \\times d}$. We then concatenate this with the other four embeddings: time-of-day embedding $E_{tod}$, day-of-week embedding $E_{dow}$, node embedding $E^0$, rank-based node embedding $E_r'$, to form an aggregated feature $H \\in \\mathbb{R}^{n \\times 5d}$. This aggregated feature then passes through another MLP to produce a high-dimensional representation $H' \\in \\mathbb{R}^{n \\times 5d}$:\n$$E_f = MLP(X_{\\mathcal{N},T})$$  (1)\n$$H = E_f \\| E^0 \\| E_r' \\| E_{tod} \\| E_{dow}$$  (2)\n$$H' = MLP(H)$$ (3)\n3.2 Dynamic Adaption Step\nThe main objective of this step is to capture the latent correlations between sensed locations and unsensed locations and to transfer the feature embedding from sensed locations to unsensed locations. This is to address the challenge of the shift to an unknown distribution at unsensed locations. In Fig. 3, the dynamic adaption step is represented from $X_{M,T}$ to $X_{M',T}$ with a red arrow. The input for this step is the historical data from the permanent sensors deployed at the sensed locations, denoted as $X_{M,T}$. The output is the predicted historical data for the unsensed locations, represented by $X_{M',T}$.\n3.2.1 Node Embedding Enhanced Spatial Transfer Matrix from sensed locations to unsensed locations. In response to the challenges posed by the unknown data distribution at unsensed locations, irregular fluctuations, and the need for more robust embeddings, we have devised a node embedding enhanced spatial transfer matrix. The spatial correlation is enhanced by incorporating node embedding bank $B^0$ and rank-based node embedding $E'_{M'}$. By integrating these two components, node embedding bank $B^0$ and rank-based node embedding $E'_{M'}$ can learn spatial correlations from the partial adjacency matrix $A_{M,M'}$, thereby producing more robust embeddings. Besides, node embedding bank $B^0$ makes the spatial attention module aware of the high dimensional location properties. Rank-based node embedding $E'_{M'}$ makes the spatial transfer matrix sensitive to the changes in traffic patterns, and enhances the model's adaptability. Consider the example of a closure in the highway, where the traffic pattern is irregular and the flow rate drops to zero. The rank embedding $E'_{M'}$ encodes the current rank of a flow rate to a high dimensional space. In this case, the rank drops to zero, generating a low-order pattern in the spatial transfer matrix. This spatial transfer matrix could generate a dynamic adaptation to the unsensed locations M'. In contrast, solely relying on sensor values and temporal embeddings would have not been able to achieve this.\nEQ. 4 shows three facets of spatial knowledge: the partial adjacency matrix $A_{M,M'} = [A_{i,j}, i \\in M, j\\in M'] \\in \\mathbb{R}^{m \\times m'}$, node embedding bank $B^0 = \\{ B^0_i | B^0_i \\in \\mathbb{R}^{d}, i \\in M \\} \\in \\mathbb{R}^{m \\times d}$, and rank-based node embedding $E'_{M'} = \\{ E'_{i} | E'_{i} \\in \\mathbb{R}^{d}, i \\in M \\} \\in \\mathbb{R}^{m \\times d}$.\n$$A_{M,M'}'' = A_{M,M'} + (E'_{M'} + B^0_M) (B^0_{M'})^t$$  (4)\n$$X_{M',T} = MLP(A_{M.M'}'' H_{M.T'})$$ (5)\nWe start with the dynamic adaption step by generating the representation $H_{M,T'} \\in \\mathbb{R}^{m \\times 5d}$ from the input $X_{M,T} \\in \\mathbb{R}^{m \\times l}$ through MLPs. t is the transpose of the matrix. After the node embedding enhanced spatial transfer matrix, a MLP is then utilized to map the high-dimensional representation 5d to the desired output length l.\n3.3 Long-Term Forecasting Step\nThe primary objective of this step is to enhance the model's long-term forecasting power and to further refine the parameters established during the initial training step. One challenge is to fully utilize the embeddings from limited spatio-temporal data. With the previous dynamic adaptive step, we obtain the well-trained embeddings with the information of adaptation from sensed locations to unsensed locations. Thus, the forecasting structure in this step could leverage this knowledge to achieve a better outcome.\nThe long-Term Forecasting step is shown in Fig. 3, from $X_{M,T}$, $X_{M',T}$ to $X_{M,T'}$ (blue arrows). The input consists of historical data from sensed sensors $X_{M.T}$ and predicted historical values at unsensed locations $X_{M',T}$ (from the dynamic adaption step). The output of the long-term forecasting step is the future data for sensed locations $X_{M,T'}$.\nTo keep our model's resistance to noise, we first re-rank the flow rates across the sensed data and the predicted historical unsensed data, yielding the rank-based node embedding. As we explained, the noisy prediction could be alleviate by the rank-based node embedding. By incorporating additional ground truth information $X_{M,T'}$, the model's parameters are further fine-tuned and optimized.\n3.3.1 Node Embedding Enhanced Spatial Transfer Matrix from all locations to sensed locations. Beyond the advantages highlighted in the dynamic adaption step (from $X_{M,T}$ to $X_{M',T}$), the spatial transfer matrix in this step (from $X_{M,T}, X_{M',}$ to $X_{M,T'}$) is capable of learning precise long-term temporal patterns through shared and well-trained parameters. $B = [B^0_M, B^0_{M'}]$, where $B^0_M$ and $B^0_{M'}$ are learned on previous adaptive step. Besides, the rank-based node embedding $E'$ is partially trained by the previous adaptive step too. We obtain the first m rank information from $E'_{M'}$. These newly introduced parameters include the forecasting part from the MLP that maps the high-dimensional representation 5d to the desired output length l'. The rank-based embedding $E'_{M'}'$ is better trained based on these foundations. These two parameters, utilized later in the aggregation step, enhance the effectiveness of subsequent processes.\nWe use $X_{\\mathcal{N},T'}$ to represent the input of the forecasting step, the concatenate data $X_{\\mathcal{N},T'} = [X_{M,T}, X_{M',T}]$, $\\mathcal{N} = [M, M']$. We first obtain the representation $H_{\\mathcal{N},T'} \\in \\mathbb{R}^{n \\times 5d}$ through the MLPs from $X_{\\mathcal{N},T'} \\in \\mathbb{R}^{n \\times l}$. The rank-based node embedding is $E^{\\''}_{\\mathcal{N}} = \\{ E^{\\''}_i | E^{\\''}_i \\in \\mathbb{R}^{d}, i \\in \\mathcal{N} \\} \\in \\mathbb{R}^{n \\times d}$ and the node embedding bank is $B = \\{ B_i | B_i \\in \\mathbb{R}^{d}, i \\in \\mathcal{N} \\} \\in \\mathbb{R}^{n \\times d}$.\n$$A_{\\mathcal{N},M'} = A_{\\mathcal{N},M'} + (E^{\\''}_{\\mathcal{N}} +B^0)(B^0_{M'})^t$$ (6)\n$$X_{M,T'} = MLP'(A_{\\mathcal{N},M'}''H_{\\mathcal{N},T'})$$ (7)\n3.4 Aggregation Step\nSo far, the model has learned how to adapt knowledge from sensed locations to unsensed locations during the dynamic adaptive step (as shown from $X_{M,T}$ to $X_{M',T}$ in Fig. 3) and how to perform long-term forecasting in the long-term forecasting step (as shown from $X_{M,T}, X_{M',}$ to $X_{M,T'}$ in Fig. 3). Intuitively, this aggregation step inherits the optimal parameters learned from previous steps and aggregates them together. As Fig. 3 shown, the input comprises three parts, the historical sensed data $X_{M,T}$, the predicted history unsensed data $X_{M',T}$, and the predicted future sensed sensor data $X_{M,T'}$. We treat the historical data in all locations $X_{\\mathcal{N},T'}$ as the concatenation of the first two data. The output is the predicted future data for the unsensed locations $X_{M',T'}$.\nFollowing the methodology previous steps, the two parts of input first get their embedding $H_{\\mathcal{N},T'} \\in \\mathbb{R}^{n \\times 5d}$ and $H_{M,T'} \\in \\mathbb{R}^{m \\times 5d}$. Sensed and unsensed node embedding bank $B^0_M$ and $B^0_{M'}$, sensed and unsensed rank embedding $E'_{M'}$ and $E'_{M'}''$, and MLP' are trained from previous steps. Thus, they can express the valuable knowledge learned from previous steps to get the optimal model parameter and predicted future data for unsensed locations.\n$$A_{\\mathcal{N},M'}'' = A_{\\mathcal{N},M'} + (E^{\\''}_{\\mathcal{N}} +B^0)(B^0_{M'})^t$$ (8)\n$$A_{M,M'}''' = A_{M,M'} + (E'_{M'} + B^0)(B^0_{M'})^t$$ (9)\n$$X_{M',T'} = \\alpha * MLP'(A_{\\mathcal{N},M'}''H_{\\mathcal{N},T'}) + (1 - \\alpha) * MLP'(A_{M,M'}'''H_{M,T'})$$ (10)"}, {"title": "4 EXPERIMENT", "content": "4.1 Setting and Datasets\n4.1.1 Datasets. We use five widely used public traffic flow datasets", "3": 1, "1": "the dropout layer [43", "45": "of the original input then is added to the result for the final output. a in the aggregation step is 0.5. During training, we set the batch size at 64, the learning rate at $10^{-3}$, and the weight decay at $10^{-3}$ for all datasets. The optimizer is AdamW [31", "22": "long-term traffic forecasting temporal models, such as PatchTST* [34", "29": "short-term traffic"}]}