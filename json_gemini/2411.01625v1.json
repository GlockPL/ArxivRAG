{"title": "Counterfactual explainability of black-box prediction models", "authors": ["Zijun Gao", "Qingyuan Zhao"], "abstract": "It is crucial to be able to explain black-box prediction models to use them effectively and safely in practice. Most existing tools for model explanations are associational rather than causal, and we use two paradoxical examples to show that such explanations are generally inadequate. Motivated by the concept of genetic heritability in twin studies, we propose a new notion called counterfactual explainability for black-box prediction models. Counterfactual explainability has three key advantages: (1) it leverages counterfactual outcomes and extends methods for global sensitivity analysis (such as functional analysis of variance and Sobol's indices) to a causal setting; (2) it is defined not only for the totality of a set of input factors but also for their interactions (indeed, it is a probability measure on a whole \"explanation algebra\"); (3) it also applies to dependent input factors whose causal relationship can be modeled by a directed acyclic graph, thus incorporating causal mechanisms into the explanation.", "sections": [{"title": "1. Introduction", "content": "With the increasing complexity of machine learning prediction models and their wider applications, there is a rapidly growing need to explain these black-box models in practice. Although the term \"explainable artificial intelligence\u201d (XAI) is a fairly recent invention (for a recent survey, see Dwivedi et al., 2023), there is a long-standing and quickly expanding literature on variable importance and black-box model visualization, including Sobol's indices in global sensitivity analysis (Sobol, 1993), functional analysis of variance (Hoeffding, 1948), Shapley's value in game theory (Shapley, 1953), partial dependence plots (Friedman, 2001), accumulated local effect plots (Apley and Zhu, 2020), and various other variable importance measures (Breiman, 2001; Hines et al., 2022; Williamson et al., 2023). A main drawback of these existing tools is that it is not easy to incorporate contextual information that is crucial for model explanation practice. These methods depend entirely on the joint distribution of the observed data and thus ultimately describe associational rather than causal relationships without further assumptions (Zhao and Hastie, 2021).\nWe next give two paradoxical examples to highlight why a lack of causal interpretation can be particularly problematic in the real world. Specifically, Example 1 will show that it is insufficient to consider the joint distribution of model inputs in interpreting the model, and Example 2 will show that knowledge of causal mechanism can completely change our interpretation of the model. For the rest of this paper, by model \u201cinputs\u201d we mean K \u2265 1 variables W\u2081, . . ., WK and by \"prediction model\u201d or simply \u201cmodel\u201d we mean a deterministic function Y that maps W = (W1,...,WK) to a real value. (We will take the dualistic view and notation that Y is both a function of W and also a random variable itself.)"}, {"title": "2. Causally independent input factors", "content": "Example 1 shows that probabilistic independence is not sufficient to define a consistent notion of explainability. In this Section, we use a stronger notion called \u201ccausal independence\u201d to illustrate the definition of counterfactual explainability in (1) and how it may be extended to the whole \u201cexplanation algebra\" to define interaction explainability as in (3).\nRoughly speaking, causal independence means that changing the value of any input factor Wk in an intervention does not change any other input factors. We will not give a formal definition here but note that this can be regarded as a special case of the setting discussed in Section 3 by using an \"empty\" causal DAG between W1,...,WK that has no directed edges. Causal independence implies probabilistic independence, but not vice versa.\nTo see how causal independence can resolve the paradox in Example 1, one simply needs to write down the causal model in the two cases considered there. In the first case where Y (W1, W2) = W2, the conclusion that W2 offers no explanation to Y depends on the implicit assumption that W1 and W2 are causally independent (so the potential outcome is W1(W2) = W\u2081 for all w2). In the second case where Y (W1, W2) = W1 W2, by saying W2 is \u201cgenerated\u201d by W2 = W1W2, we implicitly treat this as a structural equation (the potential outcome of W2 is given by W2(W1) = w\u2081W2), so W\u2081 and W2 are no longer causally independent. See Pearl (2009, sec. 3.6.3) and Example 5 below for a discussion on the meaning of structural equations and how they can be translated to potential outcomes.\nThe counterfactual explainability of a subset of input factors indexed by S \u2286 [K] is denoted as \u03bey(Vk\u2208sWk) and defined in (1) in the Introduction. This quantity should be viewed as the \u201ctotal explainability\" of Ws to Y (instead of some kind of main effect) and is always bounded between 0 and 1: \u03bey (VkesWk) = 0 means that the inputs Ws has no counterfactual explainability at all, and \u03be\u03b3(Vk\u2208SWk) = 1 means that Y is completely determined by Ws. It can be viewed as the causal analogue to Sobol's upper sensitivity index (see Section 6)."}, {"title": "2.1. Counterfactual explainability of interaction between factors", "content": "Often, we are interested in more than just total explainability. For instance, in the genetic heritability example (Example 3), we may be interested in knowing the role of gene-environment interaction or gene-gene interaction (in the latter case, we need two genes G\u2081 and G2 in that example). Interaction explainability can be defined in two equivalent ways. Here we introduce the definition using the anchored decomposition of Y that generalizes the definition in (3). In the next subsection we introduce an alternative definition using the inclusion-exclusion principle.\nLet us first introduce the anchored decomposition. The interaction contrast with respect to a non-empty SC [K] anchored at w and evaluated at w' is defined as\n$I_S(w,w') = \\sum_{S'\\subseteq S} (-1)^{|S|-|S'|}Y(w'_{S'}, w_{-S'})$\nBy convention, I\u00f8(w,w') = Y(w) is the value of Y evaluated at the anchor w. The anchored decomposition then refers to the identity Y(w') = \u2211s\u2286[K] Is(w, w'), which holds for any w and w'. For more discussion, see Kuo et al. (2010) and Owen (2023, Appendix A).\nIf w and w' are close, Is(w,w') is a finite difference approximation to the partial derivative of Y with respect to Ws at w. When K = 2, the anchored decomposition reduces to Y(w') =\nI\u00f8(w, w') + I\u2081(w, w') + I2(w, w') + I{1,2}(w, w'), where\nI\u2081(w, w') = Y(w\u2081, w\u2082) \u2013 Y (W1, W2), I2(w, w') = Y (w\u2081, w\u2082) \u2013 Y (W1, W2)\nI{1,2} (\u03c9, \u03c9') = Y (W1, W2) \u2013 Y (W\u2081, W2) \u2013 Y (W1, W2) + Y(W1, W2).\nThe last quantity is commonly used to measure interaction effect in the causal inference literature (VanderWeele and Tchetgen, 2014), especially in factorial experiments (Dasgupta et al., 2015)."}, {"title": "Definition 1 (Interaction explainability)", "content": "Let W1, ..., WK be causally independent variables. For S \u2286 [K], the counterfactual explainability of the interaction between Ws to Y is defined as\n$\\xi_Y(\\land_{k \\in S} W_k) := \\frac{Var[I_S(W, W')]}{2^{|S|}Var(Y(W))}$\nwhere W and W' are independent and identically distributed.\nIn (4), interaction explainability is quantified by the average magnitude of the interaction contrast (note that Is(W, W') has mean 0 by symmetry). It can be shown that (4) is always bounded between 0 and 1 (see Theorem 3 below), and a larger value indicates greater explainability by the interaction between Ws. To gain some intuitions in the K = 2 case, it can be shown that\n\u03bey(W1 \u2227 W2) = 0 if and only if Y(W1,W2) = f1(W1) + f2(W2) (i.e. Y is additive), and\n\u03bey (W1 W2) = 1 if Y (W1, W2) = W1W2. The next example further demonstrates this."}, {"title": "Example 4 (Multilinear form)", "content": "Suppose Y(W) = \u2211s\u2286[K] \u1e9es Ik\u2208s Wk, where the factors are causally independent and each Wk has a symmetric distribution with mean 0 and variance 1. Then\n$\\xi_Y(\\forall_{k \\in S} W_k) = \\frac{\\sum_{S'\\cap S \\neq \\emptyset} \\beta_{S'}^2}{\\sum_{S'\\subseteq [K]} \\beta_{S'}^2} \\quad \\xi_Y(\\land_{k \\in S} W_k) = \\frac{\\sum_{S'\\subseteq S} \\beta_{S'}^2}{\\sum_{S'\\subseteq [K]} \\beta_{S'}^2}$"}, {"title": "2.2. Counterfactual explainability on the explanation algebra", "content": "The definition of total explainability in (1) and interaction explainability in (4) are related by an inclusion-exclusion principle:\n$\\xi_Y (\\forall_{k\\in S} W_k) = \\sum_{S'\\subseteq S} (-1)^{|S'|+1} \\cdot \\xi_Y (\\forall_{k\\in S'} W_k)$ \n$\\xi_Y(\\land_{k\\in S} W_k) = \\sum_{S'\\subseteq S} (-1)^{|S'|+1} \\cdot \\xi_Y (\\forall_{k\\in S'} W_k)$\nThese identities motivated the notation V and A. In other words, one can equivalently define interaction explainability through (6). When K = 2, these identities reduce to\n\u03be\u03bd (W\u2081 \u2228 W2) + \u00a3y (W1 ^ W2) = \u03bey (W1) + \u03bey (W2).\nThe K = 3 case can be visualized using the Venn's diagram in Figure 1.\nIt is remarked in previous sections that \u0123y(VkesWk) and \u00a7y(\u2227k\u2208sWk) are always between 0 and 1. Together with the inclusion-exclusion principle, these observations suggests that counter-factual explainability \u0123y(\u00b7) behaves just like a probability measure. In fact, this is true and can be made precise by introducing what we call the explanation algebra."}, {"title": "Definition 2 (Explanation algebra)", "content": "The explanation algebra E(W) generated by a collection of variables W = (W1,..., WK) is the Boolean algebra defined by the join operation V, meet operation A, and negation \u00ac that is isomorphic to the set algebra for the power set of {0,1}K (each element in this set algebra is a collection of binary vectors of length K) defined by set union U, set intersection \u2229, and set complement under the map Wk \u2192 {w \u2208 {0,1}K : wk = 1}, k \u2208 [K].\nTo make this definition more concrete, when K = 2 we have the following representation for some elements in E(W):\nW\u2081 \u2192 {(1,0), (1,1)}, W\u2082 \u2192 {(0, 1), (1, 1)},\n\u00acW\u2081 \u2192 {(0,0), (0,1)}, \u00acW\u2082 \u2192 {(0,0), (1,0)},\nW\u2081 \u2228 W\u2082 \u2192 {(1,0), (0, 1), (1, 1)}, W\u2081 > W\u2082 \u2192 {(1,1)},\nso \u0123y (\u00acW\u2081) means the amount of counterfactual variability of Y not explained by W\u2081 and similarly for \u00a3y(W2). One can also consider other elements in E(W), for example W\u2081 ^ (\u00acW\u2082) (which maps to {(1,0)}), which can be interpreted as explanation \u201csolely\" due to W\u2081."}, {"title": "Theorem 3", "content": "If W1,...,Wk are causally independent input factors, the total counterfactual explainability as defined in (1) can be uniquely extended to a probability measure on E(W).\nWe will also use gy(\u00b7) to denote this unique probability measure, which is now defined on E(W) instead of just for a subset or its interaction. In Section 6, we show that the measure can be interpreted as the causal counterpart to Sobol's upper sensitivity index (after normalization) (Sobol, 1993) and its generalizations (Liu and Owen, 2006; Hooker, 2004). We will prove (5), (6), and Theorem 3 in the Appendix.\nNext are some useful properties of counterfactual explainability that follow from Theorem 3:\n1. \u03be\u03b3(Vk\u2208sWk) = 1 if Y is a just function of Ws.\n2. \u03be\u03b3(Vk\u2208s/Wk) \u2265 \u00a3y(Vk\u2208sWk) and \u0123y(\u2227k\u2208S'Wk) \u2264 \u03bey(^k\u2208sWk) for any S \u2286 S' \u2286 [K].\n3. \u03be\u03b3 (W\u2081) + ...\u03be\u03b3(WK) \u2265 \u03bey(\u2228k\u2208[K]Wk).\nThe second property shows that if the interaction explainability of S is zero, so is any of its superset (which represents higher-order interactions). This hierarchy may be useful when trying to construct tests for vanishing interactions. The third property is basically the Efron-Stein inequality (Efron and Stein, 1981) and can be further tightened using the inclusion-exclusion principle (which also provides lower bounds of \u03bey (Vk\u2208[K]Wk); the corresponding interlacing inequalities are named after Boole and Bonferroni in probability theory)."}, {"title": "3. Causally dependent input factors", "content": "To define counterfactual explainability for causally dependent inputs, we require a causal graphical model for those input factors. Because the theory of DAG causal model is very well developed, we will only introduce some main concepts here and refer the reader to Pearl (2009); Hernan and Robins (2023) for further discussion.\nConsider a DAG G with vertices being the variables W1,...,WK. We use pa(k) to denote the set of vertices with a directed edge pointing to Wk (aka \u201cparents\u201d of Wk) in G. We use Wk(ws) to denote the potential outcome of Wk under an intervention that sets Ws to ws, and define the potential outcomes schedule as the collection of all such variables:\nW() = (Wk(ws) : k \u2208 [K], S \u2286 [K], ws in the support of Ws),\nThe schedule of all basic potential outcomes of Wk is defined as\nW = (Wk(Wpa(k)) : Wpa(k) in the support of Wpa(k)).\nFor any S\u2286 [K], let Ws(\u00b7) and W collects all the potential outcomes and basic potential outcomes for Ws, respectively."}, {"title": "Definition 4 (Causal Markov model)", "content": "We say a probability distribution on W(\u00b7) is causal Markov with respect to a DAG G with vertex set W if the following are true:\n1. The next event holds with probability 1:\n$W_k(w_S) = W_k(w_{pa(k)\\cap S}, w_{pa(k)\\setminus S}(w_S)), \\text{ for all } k \\in [K], S \\subseteq [K], \\text{ and } w_S$"}, {"title": "Example 5 (NPSEM-IE)", "content": "The structural equations associated with a DAG G are given by\n$W_k = f_k(W_{pa(k)}, E_k), k \\in [K]$\nfor some functions f1,..., fk and independent errors E1,..., \u0415\u043a. \u0412\u0443 calling these equations \u201cstructural\u201d, we mean they continue to hold under interventions on some of the variables, and in that case the basic potential outcomes of Wk are given by fk(Wpa(k), Ek) with wpa(k) varying over its support. Because E1,..., \u0415\u043a are independent, the basic potential outcomes W\u2081,...,W are also independent and thus the causal Markov assumption is satisfied. By recursive substitution, we can rewrite the model as a function of error terms; for notational simplicity, we will still de-note this function as Y(E). Rather than using basic potential outcomes to define counterfactual explainability, we can instead use the error terms and rewrite equation (2) as\n$\\xi_Y(\\forall_{k \\in S} W_k) = \\frac{Var (Y(E) - Y(E'_S, E_{-S}))}{2Var(Y(E))}$\nwhere E' is an independent and identically distributed copy of E.\nIn view of (8), counterfactual explainability of an input factor Wk is essentially the explain-ability of its exogenous noise Ek. The reason we do not use the NPSEM-IE definition from the beginning is that two different sets of structural equations (e.g. obtained by rescaling the noise vari-ables) may generate the same distribution of the potential outcomes schedule and thus may lead to ambiguous definitions. It is more desirable to directly define counterfactual explainability using the basic potential outcomes schedule, which collects all \u201cintrinsic\u201d noise variables."}, {"title": "Definition 5 (Comonotonicity)", "content": "Fix an anchor w in the support of W. There exist non-decreasing functions hk,w' such that the event Wk(w') = hk,w'(Wk(w)) for all k \u2208 [K] and w' in the support of W holds with probability 1.\nThis assumption essentially requires that any two basic potential outcomes are perfectly pos-itively correlated given their marginal distributions. It is satisfied in the NPSEM-IE model if the noise is additive, i.e. Wk = fk(Wpa(k)) + Ek."}, {"title": "Proposition 6 (Identifiability under comonotonicity)", "content": "Suppose the input factors W follow a causal Markov model with respect to a DAG G and the basic potential outcomes are comonotone. Then the explainability \u03bey (\u2228k\u2208sWk) for any S \u2286 [K] is identified by (8), and the entire explainability measure on the algebra E(W) can subsequently be identified using probability calculus.\nTo actually compute explainability, the basic step is to evaluate the function Y(E). This is simple: we can apply the conditional quantile transformations in (9) using a topological order of the DAG G to compute W = W(E) and then evaluate Y(W). From there, it is easy to approximate (8) using Monte Carlo by simulating many realizations of E and E'. This is essentially the \u201cpick-and-freeze\" method in Sobol (2001); see also Owen (2023, Appendix A).\nThus far, we have assumed the conditional quantile functions in (9) are known. In practice, we can estimate them empirically using the data and then plug the estimates in (8). When W\u2081, ..., WK are known to be causally independent (so we are in the special case considered in Section 2), we can use the inverse of the empirical distribution function of each input factor. The resulting estimator of the explainability \u03bey (Vk\u2208sWk) (more precisely, its numerator Var (Y (W) \u2013 Y (W', W_s))) is then a U-statistic (Hoeffding, 1948)."}, {"title": "5. Numerical Examples", "content": "5.1. Causally independent input factors\nSuppose W1, W2, W3 are causally independent and each follows the standard normal distribution. We demonstrate counterfactual explainability using four prediction models: (1) linear: Y(w) =\n\u22113k=1 Wk; (2) quadratic polynomial: Y(w) = \u2211k<k' Wk\u03c9\u03bd; (3) single-layer neural network (sig-moid activation): Y(w) = (1 + e10w1+10w2)\u22121 + (1 + e10w2+10w3)\u22121; (4) multilinear monomial:\nY(w) = w1W2W3. For each function, the counterfactual explainability on the whole explanation algebra is shown in a Venn diagram superimposed with Monte-Carlo estimates (Figure 2).\n5.2. Causally dependent input factors\nNext, we consider the two-layer neural network with K = 5 nodes in Figure 3 (a), where the vari-ables E11, E12, E13, E21, E22 are causally independent and follow the standard normal distribution. The first-layer nodes are given by W1i = E1i for 1 < i < 3, and they are linearly combined using weights (1) and undergo a quadratic activation function \u03c3(x) = x\u00b2 to generate the second-layer"}, {"title": "6. Discussion", "content": "Counterfactual explainability can be viewed as the extension of functional analysis of variance (Ho-effding, 1948) and Sobol's global sensitivity indices (Sobol, 1993) to a causal setting. To see this, any function f of (probabilistically) independent variables W1,...,WK can be decomposed as\nf(W) = \u2211s\u2286[\u03ba] fs(Ws), where the terms can be obtained inductively via (let f\u00f8(w) = E[f(W)])\n$f_S(w_S) = E[f(W) - \\sum_{S'\\subset S} f_{S'}(W) | W_S = w_S]$\nand they are orthogonal in the sense that E[fs(Ws)fs'(Ws')] = 0 for all different S,S' \u2286 [K].\nLet the variance component of S be defined as o} := Var(fs(Ws)), so the total variance of f (W) can be decomposed as Var(f(W)) = \u2211s\u2286[K] 3. This decomposition provides the basis for a variety of variable importance measures. For a subset S \u2286 [K], Sobol's lower and upper indices are defined, respectively, as\n$\\tau_S^2 = Var(E[f(W) | W_S]) = \\sum_{S'\\subset S} \\sigma_{f_{S'}}^2, \\quad \\tau_S^3 = E[Var(f(W) | W_{-S})] = \\sum_{S'\\cap S \\neq \\emptyset} \\sigma_{f_{S'}}^2$\nFurthermore, the superset importance is defined as (Hooker, 2004; Liu and Owen, 2006)\n$\\sigma_S^> = \\sum_{S'\\supseteq S} \\sigma_{f_{S'}}^2$\nShapley's value is an axiomatic way of attribution in cooperative games (Shapley, 1953). If we use Sobol's lower index as the value of any subset S, the Shapley value of Wk is given by (Owen, 2014)\n$\\phi_k = \\frac{1}{K} \\sum_{S \\subseteq [K] \\{k\\}} \\binom{K-1}{|S|}^{-1} (v(S \\cup k) - v(S)) = \\sum_{{k} \\subseteq S \\subseteq [K]} \\sigma_{f_{S}}^2/|S|$"}, {"title": "Appendix A. Proofs", "content": "A.1. Proof of the inclusion-exclusion principle\nWe first introduce and prove a formula for the covariance of two interaction contrasts.\nLemma 7 For causally independent factors W, any S\" \u2286 S' \u2208 S \u2286 [K], |S| \u2013 |S'| = 1,\n$Cov (I_{S''}(W, W'), I_{S-S'}(W,W')) = -(\\frac{1}{2})^{|S''|+1}Var (I_{S\\cup(S-S')} (W, W'))$\nProof We first prove for S = {1,2}, S\" = S' = {1}. Since W' is an independent copy of W,\nCov (Y(W2, W-2) \u2013 Y (W), Y (W\u2081, W\u22121) \u2013 Y(W))\n= (Cov (Y (W2, W-2) - Y(W), Y (W\u2081, W-1) \u2013 Y(W))\n+Cov (Y (W{1,2}, W-{1,2}) \u2013 Y(W, W-1), Y (W) \u2013 Y(W\u2081, W-1)))\n=Cov (Y(W2, W-2) \u2013 Y (W) - (Y (W{1,2}, W_{1,2}) \u2013 Y (W\u2081, W-1)),\nY(W1,W_1) - Y (W))\n= - Cov (I{1,2} (W, W'), Y (W\u2081, W-1) \u2013 Y(W))"}]}