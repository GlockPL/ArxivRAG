{"title": "LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning", "authors": ["Haoran Yang", "Xiangyu Zhao", "Sirui Huang", "Qing Li", "Guandong Xu"], "abstract": "Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised graph learning that has attracted attention across various application scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet to be explored. Because conventional augmentation techniques like feature embedding masking cannot directly process textual attributes on TAGs. A naive strategy for applying GCL to TAGs is to encode the textual attributes into feature embeddings via a language model and then feed the embeddings into the following GCL module for processing. Such a strategy faces three key challenges: I) failure to avoid information loss, II) semantic loss during the text encoding phase, and III) implicit augmentation constraints that lead to uncontrollable and incomprehensible results. In this paper, we propose a novel GCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to produce textual augmentations and LLMs' powerful natural language processing (NLP) abilities to address the three limitations aforementioned to pave the way for applying GCL to TAG tasks. Extensive experiments on four high-quality TAG datasets illustrate the superiority of the proposed LATEX-GCL method. The source codes and datasets are released to ease the reproducibility, which can be accessed via this link2.", "sections": [{"title": "1 Introduction", "content": "In numerous real-world scenarios, graph data is often enriched with textual attributes, for instance, user-item interaction graphs in recommendation systems that include textual user profiles and product descriptions [9, 16]. This type of graph data is referred to as TAGs [3]. More than recommendation systems, the application scenarios of TAGs also include bioinformatics [2], computer vision [20], and quantum computing [11]. The development of effective methodologies for processing and analyzing TAGs is crucial for advancing applications that rely on such data. With the advent of graph learning techniques, a variety of paradigms have been introduced. Notably, GCL [23, 7, 25] has gained prominence as a powerful self-supervised technique for graph representation learning, capitalizing on the benefits of self-supervision in cases of lacking sufficient labels. Current GCL approaches typically employ perturbations to manipulate graph structures and feature embeddings, thereby generating contrasting samples for GCL [29, 28, 31, 26]. Despite the diversity of these strategies, they fall short in directly augmenting the textual attributes inherent in TAGs. Consequently, there is a pressing need to devise a framework that synergizes GCL with TAGs, potentially enhancing the performance of graph learning tasks within TAG application scenarios by harnessing the strengths of GCL techniques.\nDespite the advancements in GCL, the literature reveals a gap in the development of GCL method-ologies specifically tailored for TAG settings [3, 12, 24]. An initial attempt to address this, referred to as Topological Contrastive Learning (TCL) for TAGs, is outlined in [24]. This approach begins by encoding textual attributes into feature embeddings for each node. Subsequently, it employs conventional GCL augmentations such as feature masking and proximity perturbation [29] to process the graph, followed by the execution of the remaining GCL steps in sequence. While this rudimentary approach enables the adaptation of GCL to TAG settings, it is not without significant drawbacks that could potentially compromise its effectiveness. There are three limitations lie ahead: I) Information Loss. Existing research [7] has identified information loss as a significant issue during the augmentation phase of conventional GCL methods, attributable to randomness and noise inherent in these processes. Adhering to the aforementioned rudimentary pipeline and employing standard random-based augmentation techniques, such as feature masking, inevitably leads to this loss of information. To enhance the performance of graph models within the GCL framework, it is imperative to implement strategies that mitigate such information loss. II) Incapable Language Models. The encoding of textual attributes in TAGs presents challenges when using both shallow text embedding methods, such as bag-of-words [6] and skip-gram [17], and advanced deep language models like BERT [4], DeBERTa [8], and GPT-2 [19]. Shallow embedding methods are constrained by their limited capacity to capture nuanced semantic features, whereas deep language models, despite their sophistication, fall short in complex reasoning tasks [3]. The reliance on these inadequate language models for the text encoding phase leads to an inevitable semantic degradation contained in the original textual attributes. III) Implicit Constraint on Augmentations. Conventional GCL methods [23, 29], as well as those employing sophisticated adaptive augmentation strategies [31, 26], share a fundamental challenge: the absence of explicit constraints on the augmentation process. This deficiency hinders users from monitoring and comprehending the effects of augmentation techniques, leading to augmented outcomes that are both uncontrollable and incomprehensible.\nTo overcome the aforementioned limitations, we introduce a novel approach named LATEX-GCL that employs an LLM to generate auxiliary texts, which act as augmented textual attributes for GCL applied to TAGs. This method circumvents the information loss associated with conventional feature augmentation techniques (e.g., random feature masking). Thanks to the general knowledge contained in the LLM [18], our strategy effectively enriches the semantics of the original text via the LLM-based augmentation, compensating for potential semantic deficits incurred during the text encoding phase. Furthermore, the utilization of LLMs involves natural language inputs, carefully crafted prompts to steer the augmentation process, and outputs that are inherently understandable for human beings. This process ensures that the augmentation constraints and results are explicit and comprehensible, enhancing the transparency and control over the augmentation procedure. Nevertheless, employing LLMs for textual attribute augmentation in GCL is challenging, as there is a dearth of precedents in the literature to guide such an application. In this paper, we seminally propose a suite of prompts for textual attribute augmentation using LLMs, drawing inspiration from the foundational principles of conventional graph augmentations as cataloged in GraphCL [29], including shorten, rewriting, and expansion, to facilitate the LLM-based textual attribute augmentation process.\nIn summary, to address the limitations in current methods and better adapt GCL techniques to TAG settings, we: I) propose a novel GCL framework that can leverage the advantages of LLMs to conduct textual attribute augmentation, II) seminally summarize three types of LLM-based textual attribute augmentations and list the related prompt designs, and III) conduct comprehensive experiments to illustrate the performance and verify the effectiveness of the proposed LATEX-GCL method."}, {"title": "2 Methodology", "content": "This section illustrates the details of the LATEX-GCL method, starting with the preliminaries, followed by the descriptions for each module, including I) LLM-based text feature augmentation, II) text attribute encoding, III) graph encoding, and IV) graph contrastive learning, as shown in Figure 1.\n2.1 Preliminaries and Notations\nBefore giving detailed descriptions of the proposed method, some necessary notations and formula-tions related to TAGs, LLMs, the text encoder, and the graph encoder are listed in this part.\nText-Attributed Graphs. Technically, a TAG can be defined as G = (V, E, {tn}n\u2208v), where V is the set of all nodes, & is the set of all existing links between the nodes in V, and tn is a sequence of text attributes associated with the n-th node. To facilitate the presentation of a graph, an adjacency matrix A \u2208 {0,1}N\u00d7N, where N is the number of nodes, is adopted to demonstrate nodes and links.\nLarge Language Models as Augmentor. In the proposed method, an LLM is applied as an augmentor to augment the original text attributes in the given TAG guided by the properly designed prompt. In this paper, we use the LLM(\u00b7) to denote this augmentor. Given the original text attribute tn and the prompt p, we can have the prompted text attribute \u00een. The augmentor LLM(\u00b7) finally takes the prompted text attribute \u00een to output on.\nText Attribute Encoder. To facilitate the utilization of the original and the augmented text attributes, a text encoder, such as BERT [4] and DeBERTa [8], is required to obtain feature embeddings. In this paper, LM(\u00b7) is used to denote the text encoder, which takes the original text attribute tn or the augmented text attribute on as the input to produce feature embedding hn. Then, the feature embeddings of all the nodes are concatenated to construct the feature matrix H.\nGraph Encoder. A GNN model, such as GCN [14], is implemented to serve as the graph encoder to capture the graph structure information. The graph encoder takes the adjacency matrix and the feature matrix as the inputs to update the feature matrix iteratively, where g(\u00b7,\u00b7) denotes the graph encoder. A K-layer graph encoder will output H(K) as the final feature embedding matrix.\n2.2 Large Language Model-Based Text Feature Augmentation\nAn LLM is adopted in our proposed method LATEX-GCL as an augmentor to conduct augmentations on the original textual attributes in the input TAG. Adopting the LLM aims to effectively address the three limitations in the aforementioned rudimentary TCL strategy [24] in the introduction section, including information loss, incapable language models, and implicit constraints on the augmentation process. However, the adoption of the LLM is non-trivial. A dearth of precedents in the current literature guides how to prompt the LLM to acquire proper augmented texts for the following GCL procedures. In this section, we innovatively propose and summarize a suite of prompts in order to employ the LLM to conduct textual attribute augmentations tailored for GCL on TAGs."}, {"title": "Table 1: Augmentation strategies for text attribute augmentation.", "content": "The paradigm of the LLM is known as 'pre-train, prompt, and output' [3], which is different from the existing language models. An LLM is normally trained on large-scale text corpora and possesses massive general knowledge [3, 18]. A properly designed prompt is required to help the LLM output the desired content from the massive knowledge. The prompt has various forms, such as several words or a sentence, and can include additional information to guide and constrain the output of the LLM [10]. Formally, let tn be the original text attributes of a node and p denote the prompt to be placed in front of tn, the prompted textual attributes after tokenization can be formalized as\ntn = (P1, P2, , Pa, tn,1, tn,2,\u2026\u2026\u2026, tn,b) . The LLM-based augmentor LLM(\u00b7) is trained to assign\na probability to each possible output on = (On,1, On,2,\u2026, On,c) that consists of c tokens, where\nthe most satisfactory output is expected to have the largest probability value. The probability of the\noutput o given tn can be formalized as:\np(on/tn) = \\prod_{i=1}^{b} P(On,i|On,<i, tn).\nTo guide the LLM-based augmentor LLM(\u00b7) to adapt to the scenario of text-attributed graph contrastive learning, three general text augmentations are proposed, which are listed in Table 1. The related discussions about the intuitive priors behind these augmentations are shown below:\nShorten. Given an original text attribute tn, the shorten augmentation applies a prompt ps to produce t to guide LLM(\u00b7) output 0%. Such an augmentation aims to simplify the original text attribute. The underlying prior enforced by it is that simplified content can help filter out redundant information and maintain the key points in the original text attribute.\nRewriting. Given an original text attribute tn, the rewriting augmentation applies a prompt pr to produce tr to guide LLM(\u00b7) output on. Such an augmentation aims to rewrite the original text attribute so that the invariant semantics contained in the original text attribute can be identified. Moreover, the readability can also be improved to produce high-quality feature embeddings.\nExpansion. Given an original text attribute tn, the expansion augmentation applies a prompt pe to produce to to guide LLM(\u00b7) output on. Such an augmentation aims to expand the original text attribute to introduce more related and necessary information to leverage the advantages of the knowledge base, which is trained on a large volume of the corpus.\nWithout loss of generality, we take the shorten augmentation, denoted by superscript s, only to describe the workflow of the proposed method LATEX-GCL in the methodology section and omit the two other augmentations. Formally, we prompt the original text attribute tn of the n-th node in the TAG G to obtain the prompted input t\u2081 for the augmentor LLM(\u00b7) to have:\non = LLM().\nThe operations above repeat on each node in the original TAG G to have the set of augmented text attributes {on \u2208 V}. Finally, we can have the augmented TAG G\u00b0 = (V,E, {0}n\u2208\u03bd)."}, {"title": "2.3 Text Attribute Encoding", "content": "The proposed method LATEX-GCL applies an LLM to directly augment the original text attributes to produce augmented text attributes instead of adopting the feature masking augmentation, which is one of the conventional graph augmentations [29]. Though, as introduced in the introduction section, the adopted strategy can reduce information loss and leverage the advantages of the LLM's superior semantic comprehension capability, the augmented text attributes are in the form of natural language that cannot be processed by the following graph encoding module (i.e., the GNN model) . Therefore, we adopt a text encoder in the proposed method to encode the original and the augmented text attributes to acquire feature embeddings to facilitate the following procedures.\nA relatively small language model, such as BERT [4] and DeBERTa [8], is adopted to serve as the text encoder because they are more powerful than those conventional text embedding methods [6, 17] and more efficient than the LLMs. Following the LLM-based augmentation phase, the text encoder LM(\u00b7) takes the original and the augmented text attributes to produce the original and augmented feature embeddings, which are shown as follows:\nhn = LM(tn) \u2208 Rd\u00d71, h = LM(0%) \u2208 Rd\u00d71,\nwhere d is the size of feature embeddings. Then, the feature matrix of the original TAG G and the augmented TAG G\u00ba can be acquired as follows:\nH = [h1; h2; \u2026 ; hn]T \u2208 RN\u00d7d, H\u00b0 = [h\u2081; h\u2082; \u00a8\u00a8\u00a8 ; h\u2081] \u2208RN\u00d7d.\nThe feature matrices obtained above can cooperate with the adjacency matrix A of the input TAG to facilitate the following graph encoding procedures.\nTo enhance performance, it is common to train the text encoder in conjunction with subsequent modules, yet this approach demands substantial computational resources. In practical applications, an adaptor module, typically a straightforward neural network component such as a linear layer, is employed to refine the text encoder's output, thereby boosting performance without incurring the costs associated with fine-tuning. Nevertheless, optimizing the adaptor module often necessitates ample supervised training data from specific downstream tasks. The efficacy of the adaptor module within the context of GCL in this paper remains an open question. We investigate this issue in Section 3.2.2, where we examine the impact of the adaptor module on the performance of LATEX-GCL."}, {"title": "2.4 Graph Encoding", "content": "TAGs contain a rich repository of information. In addition to the previously mentioned textual attribute information, graph structure is also essential for the graph learning tasks on TAGs. Encoding only the text features is insufficient for acquiring comprehensive graph representation, necessitating the adoption of GNN models (e.g., GCN [14]) to learn the structural information in the graph.\nGiven the feature matrices H and Hs obtained in the previous text encoding module, the adjacency matrix A, and a K-layer graph encoder g(, ), we can have the updated feature matrices that possess graph structure information as follows:\nH(K) = g(A, H) \u2208 RN\u00d7d, Hs(K) = g(A, H\u00b3) \u2208 RN\u00d7d\nEach layer of the graph encoder functions as a message passing and aggregation process, collecting information from neighboring nodes and updating the node feature embeddings accordingly."}, {"title": "2.5 Graph Contrastive Learning", "content": "Typically, TAGs possess extensive text attributes to describe the nodes. However, in real-world scenarios, label sparsity is a common and unavoidable issue, making it infeasible to manually label each node in the TAG due to the prohibitive costs involved. To broaden the applications of TAGs, it is vital to investigate how to employ self-supervised learning paradigms to obtain high-quality graph embeddings from TAGs without label information. GCL has demonstrated the powerful capability to conduct self-supervised graph learning, to this end, being a viable option for the self-supervised learning paradigm on TAGs. This section utilizes a GCL module to process the LLM-augmented graphs, finalizing the workflow of the proposed LATEX-GCL method.\nA rough GCL setting is revealed in the fourth part of Figure 1. During the training, the node embeddings are usually processed in a mini-batch manner. We use Vo to denote the set of nodes in a training batch. Formally, suppose that the i-th node i \u2208 Vs is the target. The original feature embedding of the target and the augmented feature embedding can be obtained as follows:\nh(K) = H(K)T \u2208 Rdx1, h(K) = H (K)T \u2208 Rdx1\nThe two feature embeddings mentioned above originate from the same target node, thus they are expected to exhibit a high degree of similarity. Therefore, we treat such a pair of embeddings as positive contrasting samples. Then, a subset VM \u2286 V\u2081 \\ i of nodes, where |V| = M, is randomly sampled from the mini-batch to collaborate with the original feature embedding of the target, generating 2M negative contrasting samples. The negative contrasting sample's original feature embedding and its LLM-augmented embedding are denoted by {h(K)|j\u2208 VM } and {h(K) |j \u2208 VM}\nA similarity function sim(\u00b7,\u00b7) is adopted to measure the distance between two feature embeddings. Then, InfoNCE [22] is adopted as the loss function for the GCL training:\nL = - log \\frac{e^{sim(h_i^{(K)}, h_i^{s(K)}) / \\tau}}{e^{sim(h_i^{(K)}, h_i^{s(K)}) / \\tau} + \\sum_{j \\in V_M} (e^{sim(h_i^{(K)},h_j^{(K)})}+ e^{sim(h_i^{(K)},h_j^{s(K)})}) / \\tau} \nwhere 7 denotes the temperature hyperparameter. After the GCL training, the feature matrix H(K) is updated, and we can obtain the final feature matrix Hfinal for downstream inference and evaluation."}, {"title": "3 Experiment", "content": "To demonstrate the effectiveness and the performance of the proposed LATEX-GCL method, we conduct extensive experiments and show the results with insightful analysis in this section. The related experimental settings are also provided in this section.\n3.1 Experimental Settings\nFour TAG datasets collected by [24] are selected for experiments in this paper, including Books-Children, Books-History, Ele-Computers, and Ele-Photo, which are extracted from the Amazon dataset [9, 16]. The statistics and the content of the raw text of each dataset are listed in Table 2.\nBesides the datasets, five impactful GCL methods are selected as baselines for the comparison study, including GraphCL [29], GCA [28], GRACE [31], BGRL [21], and GBT [1].\nMore detailed descriptions of datasets and baselines can be found in Appendix A.1. For better repro-ducibility, LATEX-GCL implementation details and the related evaluation protocols are provided, listed in Appendix A.2 and Appendix A.3 for reference."}, {"title": "3.2 Experiment Results & Analysis", "content": "This section lists the experiment results, including the comparison study, the ablation study, and the adaptor module experiment, which are accompanied by detailed and insightful analyses.\n3.2.1 Comparison Study\nThe results of the comparison study are listed in Table 3, demonstrating the performance of the proposed LATEX-GCL method and the selected baselines regarding the node classification task on the graph. According to the results, we have the following three findings:\n\u2022 Generally, the proposed LATEX-GCL method achieves the best performance in the comparison study among all datasets compared to the selected baselines. Such an observation verifies the effectiveness and the superiority of our proposed LATEX-GCL method. For different augmentation settings, the results reflect a clear pattern. Specifically, LATEX-GCL equipped with expansion augmentation performs better on the two Amazon-Books datasets, and LATEX-GCL equipped with rewriting augmentation performs better on the two Amazon-Electronics datasets.\n\u2022 The differences among the performance of different augmentation settings of LATEX-GCL are largely due to the difference in the raw text content of the two types of datasets. As listed in Table 2, the raw text content in the book datasets is the book introduction, and that of the electronic datasets is the consumer review. The book introduction usually contains the correct title of the book, which can help the LLM prompted by the expansion augmentation to produce informative content that is highly related to the specific book as the augmented textual attributes, which can significantly benefit the following GCL. However, the consumer reviews of the electronic datasets are normally short and neglect to list the full name of the product reviewed. Such textual attributes prevent the LLM prompted by expansion augmentation from producing informative content. Even worse, it may lead the LLM to introduce more noise (i.e., unrelated content). Therefore, utilizing the LLM to extract key information in the consumer review would be more suitable instead of producing auxiliary information. The experiment results confirm our analysis. On dataset Books-Children and Books-History, LATEX-GCL equipped with shorten augmentation and rewiriting augmentation, which are both helpful for key information extraction from the original textual attributes as discussed in Section 2.2, outperform LATEX-GCL equipped with expansion augmentation. Moreover, in the scenarios of lacking sufficient computational resources, the shorten augmentation would be a promising alternative for the rewriting expansion as the gap between the performance of these two augmentations is insignificant on both electronic datasets.\n\u2022 GraphCL has the lowest scores across all metrics and datasets. This is because GraphCL uses classical augmentation techniques to conduct GCL, outperformed by those adaptive augmentation strategies. GCA adopts an automatic selection strategy to pick conventional augmentations used in GraphCL, slightly improving the performance. GRACE proposes an adaptive strategy to augment the graph according to the specific input data. However, such a strategy significantly increases the complexity. Consequently, GRACE is out of memory when performing on the two large datasets, including Books-Children and Ele-Computers. The significant improvement brought by the adaptive augmentation strategy is reflected by GRACE's performance on Books-History and Ele-Photo. Specifically, GRACE achieved the best results among all the baselines on these two datasets. Both BGRL and GBT methods follow the same idea of utilizing different training objectives instead of InfoNCE to eliminate the requirement of negative contrasting samples and achieve better performance. We can observe that both methods can perform well on large datasets. However, on the relatively small datasets where GRACE can function, BGRL and GBT are outperformed by GRACE due to both methods taking the same conventional augmentation techniques as adopted by GraphCL, which is less advanced compared to the adaptive augmentation strategy."}, {"title": "3.2.2 Ablation Study of Text Encoder and Graph Encoder", "content": "There are two critical components in LATEX-GCL: text encoder and graph encoder. In this ablation study, we examined different models for the two components. The text encoder adopts BERT [4], and the graph coder is GCN [14] in the default settings. Two supplementary experiments are conducted with BERT [4] being replaced by GPT-2 [19] and GCN [14] being replaced by GraphSAGE [5], respectively. The experiment results are illustrated in Figure 2 below and Figure 3 in Appendix B.\nBoth BERT and GPT-2 are representative language models in NLP areas. However, there are significant differences between the two models. BERT is a bidirectional model that utilizes tasks like Masked Language Modeling to train word representations, focusing on context-based text understanding. But GPT-2 is a single-direction model trained by self-regression paradigms to predict the next word based on the previous content, which is designed for generative tasks. We can observe that LATEX-GCL equipped with GPT-2 is significantly outperformed by LATEX-GCL equipped with BERT. It indicates that the generative language model is unsuitable for acquiring text feature embeddings. This phenomenon is reasonable as the generative language models are designed for content generation, lacking powerful embedding abilities to obtain informative text representations.\nThe graph encoder module in LATEX-GCL incorporates the embedded text features and graph structural information to obtain the final representation embedding of the node in the TAG. In practice, the graph encoder selected for LATEX-GCL should be simple and efficient for processing large-scale graphs like GCN and GraphSAGE. Though LATEX-GCL equipped with GraphSAGE is functional, it is outperformed by LATEX-GCL equipped with GCN. GraphSAGE is designed for very large graphs and randomly drops some nodes and edges to facilitate the training, which causes information loss.\nIn short, to ensure the normal functionality and satisfying performance of LATEX-GCL, the text encoder should not adopt generative language models, and the graph encoder should be simple and efficient enough to incorporate the language model to train on large TAGs."}, {"title": "3.2.3 Adaptor Module Experiment", "content": "As mentioned in Section 2.3, adopting an adaptor module is a common practice for employing pre-trained language models for various downstream applications while avoiding fine-tuning. However, the adaptor module is usually combined with the downstream models to be trained together by supervised signals. But, in our settings, the training phase is motivated by graph contrastive learning, a self-supervised learning paradigm, instead of the supervised one. This section investigates if the adaptor module can apply to LATEX-GCL.\nWithout losing generality, we employ a single linear layer to decorate the outputs of the text encoder. The adaptor-processed outputs' size is a hyperparameter selected from {256, 512, 768}. Moreover, the default setting in this experiment denotes the vanilla LATEX-GCL equipped with shorten augmentation. The experiment results are shown in Table 4.\nAccording to the results, the adaptor module is effective in improving the performance of LATEX-GCL in most scenarios. Specifically, the improvement occurs when the output size of the adaptor is relatively small (i.e., smaller than the output size of the text encoder listed in Appendix A.2). It can be speculated that the role of the adaptor is to condense the text feature embeddings produced by the text encoder to facilitate the following GCL training process."}, {"title": "4 Related Work", "content": "This section briefly introduces the research works that are highly related to the scope of this paper. The following content is two-fold, which are about LLMs for graph learning and GCL, respectively.\n4.1 Large Language Models for Graph Learning\nLLMs have garnered significant attention for their prowess in natural language processing tasks, but their application in graph learning is a burgeoning field of research [3, 12]. The intersection of LLMs and graphs presents a promising avenue for enhancing various scientific disciplines such as cheminformatics [13], material informatics [15], bioinformatics [2], computer vision [20], and quantum computing [11]. By incorporating text information with graph data (i.e., TAG), researchers can accelerate scientific discovery and analysis, particularly in domains where graphs are paired with critical text properties. A comprehensive survey on LLMs on graphs [12] categorizes the application scenarios into pure, text-rich, and text-paired graphs, highlighting the diverse contexts in which LLMs can be leveraged. Techniques such as treating LLMs as task predictors [27], feature encoders for GNNs [10], and aligning LLMs with GNNs [30] offer avenues for exploring the mutual enhancement between LLMs and graphs. However, challenges such as graph linearization, model optimization inefficiencies, and the need for generalizability and robustness of LLMs on graphs underscore the importance of further research in this evolving field [12].\n4.2 Graph Contrastive Learning\nThe focus of GCL research is on securing high-quality contrasting pairs, which are essential for the effectiveness of GCL. Notable works in the literature have concentrated on creating contrasting pairs through conventional graph augmentation strategies, with satisfying results achieved [23, 29]. Nonetheless, these approaches have limitations. For example, the randomness inherent in graph augmentation can lead to suboptimal performance in graph-based models [7]. In response to this challenge, some studies have suggested the creation of various graph views to form contrasting pairs [7, 25] or the adaptive generation of contrasting examples [28, 31, 26]. Despite the sophistication of these advanced GCL techniques, they encounter a similar problem to that of the conventional methods: the lack of explicit constraints over the augmentation process. This lack of explicit constraints can result in uncontrollable and incomprehensible outcomes. In contrast, LATEX-GCL leverages an LLM to guide the augmentation of textual attributes by carefully crafted prompts. This approach ensures that both the prompted inputs and the generated outputs are in natural language, offering explicit and comprehensible constraints and results for the augmentation. Furthermore, while existing methods predominantly augment graph structures or feature embeddings, GCL for TAGs is yet to be explored [3, 12, 24]. The proposed LATEX-GCL method seeks to extend the reach of GCL techniques to include TAGs, thereby expanding the potential use cases for GCL."}, {"title": "5 Conclusion", "content": "This paper proposes a novel GCL framework, namely LATEX-GCL, which successfully incorporates LLMs to conduct augmentations to construct contrasting samples. The purpose of the proposed augmentation strategy is to leverage the advantages of LLMs to tackle the limitations of informa-tion loss, incapable language models, and implicit constraints of current GCL methods for TAGs, including alleviating information loss during the augmentation, enhancing insufficient NLP abilities of conventional language models, and imposing explicit constraints on the augmentation process. Comprehensive experiments verify the effectiveness and superiority of the proposed LATEX-GCL method. This research is expected to be a pioneering work that encourages the exploration of LLMs for GCL. The future directions are two-fold, including investigating more comprehensive augmenta-tion prompting strategies for different scenarios and how to improve the computation efficiency of employing LLMs in real-world applications."}, {"title": "A Experimental Settings", "content": "The experimental settings, including dataset and baseline descriptions, method implementation details, and evaluation protocols, are listed here to ease the reproducibility of the experiments. More details can be found in the released source codes\u00b3.\nA.1 Datasets & Baselines\nConsidering the research scope of this paper, experiments on the graph datasets with promising text attributes are required. Multiple high-quality text-attributed graphs are collected by [24] from which four datasets, including Books-Children, Books-History, Ele-Computers, and Ele-Photo, are selected as the experiment datasets. These datasets are extracted from the Amazon dataset [9, 16], which have raw text descriptions for each node and are large-scale compared to previous text-attributed graph datasets [24]. The statistics and the content of the raw text of each dataset are listed in Table 2.\nBesides the datasets, five impactful GCL methods are selected as baselines for the comparison study. These baselines can be roughly broken down into three categories: I) GraphCL [29] is the most classical GCL method that involves several conventional random-based augmentations, II) GCA [28] and GRACE [31] are both the adaptive augmentation-based GCL methods, where GCA conducts automatic selection from the conventional augmentation techniques and GRACE performs trainable augmentations based on the input graph data, and III) both BGRL [21] and GBT [1] method follow a novel GCL paradigm that utilizes different training objectives instead of InfoNCE [22] based on DGI [23] to eliminate the requirement of negative contrasting samples to achieve storage efficient.\nA.2 Method Implementation Details\nThe LLM used for dataset augmentations in our settings is GPT-3.5-turbo, and the specific version is default and decided by OpenAI update schedule4. The prompts for guiding the LLM to generate augmented text are listed in Table 1 in the methodology section.\nMoreover, we adopt a pre-trained BERT [4] model, whose version is bert-base-uncased, to embed the original and augmented text attributes. The pre-trained model and other related components are used according to the guidance of Pytorch-Transformers. The pre-trained model and other related components can be publicly accessed on Hugging Face via this link.\nSome important hyperparameter settings are listed here. The embedding size of the text encoder is set to 768, and the output size of the graph encoder is set to 256. The learning rate for the whole"}, {"title": "A.3 Evaluation Protocol", "content": "The proposed method is evaluated based on the node classification task, which is subject to the linear evaluation protocol. The linear evaluation is to train and test a support vector machine (SVM) on node feature embeddings trained by the method to be evaluated to verify the quality of the outputs of the proposed LATEX-GCL method, where the SVM is implemented by a third-party toolkit named scikit-learn7. Specifically, to ensure the reliability of the experiment results, we repeat the experiment five times. For each time, 20% of the nodes are selected as the training set, and 10% of the rest of the nodes are the test set. Sufficient metrics, including Accuracy, Precision, Recall, and F1 scores with standard deviations, are used to demonstrate the results of the linear evaluation."}, {"title": "B Supplementary Experiment Results", "content": "The performance of LATEX-GCL equipped with different text encoder and graph encoder measured by metrics Recall and F1 are shown in Figure 3"}]}