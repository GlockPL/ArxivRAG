{"title": "Synthetic Vision: Training Vision-Language Models to Understand Physics", "authors": ["Vahid Balazadeh", "Mohammadmehdi Ataei", "Amir Hosein Khasahmadi", "Hyunmin Cheong", "Rahul G. Krishnan"], "abstract": "Physical reasoning, which involves the interpretation, understanding, and prediction of object behavior in dynamic environments, remains a significant challenge for current Vision-Language Models (VLMs). In this work, we propose two methods to enhance VLMs' physical reasoning capabilities using simulated data. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs generated from simulations relevant to physical reasoning tasks. Second, we introduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to create scene descriptions enriched with physical properties and processes. During physical reasoning tasks, these PCBs can be leveraged as context to assist a Large Language Model (LLM) to improve its performance. We evaluate both of our approaches using multiple benchmarks, including a new stability detection QA dataset called Falling Tower, which includes both simulated and real-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM can significantly outperform larger state-of-the-art foundational models. We also show that integrating PCBs boosts the performance of foundational LLMs on physical reasoning tasks. Using the real-world scenes from the Falling Tower dataset, we also validate the robustness of both approaches in Sim2Real transfer. Our results highlight the utility that simulated data can have in the creation of learning systems capable of advanced physical reasoning.", "sections": [{"title": "1. Introduction", "content": "Physical reasoning is a fundamental component of human cognition, enabling the interpretation of complex interactions, prediction of future events, and understanding of causal relationships in dynamic environments. Emulating this capability in artificial intelligence is essential for achieving advanced understanding and effective interaction within visual scenes. Despite recent advancements, current Vision-Language Models (VLMs) exhibit significant deficiencies in physical and causal reasoning [15, 23]. They consistently fail to accurately interpret basic spatial relationships (e.g., position of objects relative to each other, count of objects, etc.) [15, 32], object attributes [36], and physical interactions (e.g., stability, object dynamics) [15]. Our findings in this paper further reveal that even the largest state-of-the-art models frequently misjudge scenes and fail to predict basic physical events accurately, suggesting that merely scaling up data volume or model size may be inadequate or inefficient.\nWe hypothesize that this shortfall is primarily due to limitations in existing training datasets. Popular visual datasets, such as MSCOCO [24] and Conceptual Captions [33], focus on general scene descriptions but lack specific annotations of physical events and causal relationships. Furthermore, the training methods of VLMs may contribute to their lack of physical understanding. VLMs trained with contrastive or reconstruction objectives, as well as image-text alignment [3, 8, 22] prioritize pattern recognition and semantic associations without instilling the causal reasoning skills needed to accurately predict physical events.\nTo address this challenge, we propose two novel, scalable approaches that leverage simulations to enhance the physical reasoning capabilities of language models. Simulations provide unparalleled control, enabling the generation of extensive datasets with precise annotations of physical interactions across a range of phenomena. In the first approach, we fine-tune a small pre-trained VLM with automatically generated physics and scene-related question-answer (QA) pairs from procedurally generated simulations along with corresponding visuals (image or video for dynamic scenes) and show that a small VLM can outperform state-of-the-art models by a large margin in a zero-shot set-"}, {"title": "2. Related Work", "content": "Physics Understanding. Physics understanding is a quintessential capability sought after when developing a general-purpose AI system. As pre-trained AI models grow increasingly prevalent, the ability to comprehend physical principles has emerged as a key benchmark for evaluating their understanding of the real world. To test such ability, several benchmarks have been proposed. CLEVRER [39] focuses on dynamic video understanding including collisions between objects. PTR [17] is based on static scenes but includes physical reasoning questions. ShapeStacks [16] is another static physics dataset containing scenes of stacked objects but without any QA pairs. PHYRE [5] introduces a benchmark for physical reasoning that consists of a set of physics puzzles where one or two balls can be placed or thrown within a simulation to achieve a specific goal. Piloto et al. [30] proposes a freeform dataset of 300,000 procedurally generated scenes in Mujoco. Jassim et al. [19] presents a new benchmark to test the physical intuition of VLMs by inquiring about the models to distinguish plausible from implausible videos. For more discussion on physical understanding benchmarks we refer the readers to [28].\nLeveraging physics simulation is a common approach for tackling physical reasoning tasks. Prior methods developed by Wu et al. [37], Wu et al. [38], and Smith et al. [34] use simulation results from a physics engine to augment likelihood estimation or expectation modeling for physics prediction. Mind's eye [25] prompts an LLM to generate a simulator's code and then answer the questions based on the simulation results. In addition, [13] employs a differential simulator to make predictions during inference but also to train a concept learner module that generates physics programs based on a perception module. Much of these methods can be considered as a \u201csimulator-in-the-loop\" approach, in contrast to our current work that uses simulation-based data to fine-tune existing models and improve their physical reasoning.\nOther work in this domain aims to develop a new model for physical predictions without a simulator in the loop, e.g., using a graph neural network [7], graph convolutional network [6], LSTM [40], or neuro-symbolic architecture [11]. On the other hand, the goal of our work is to extend the current capabilities of general-purpose VLMs with physical reasoning instead of creating a new specialized model.\nNote that besides [25], the above-mentioned studies do not use language models for physical reasoning tasks."}, {"title": "3. Methodology", "content": "We introduce two methods for enhancing the physical reasoning capabilities of VLMs through simulation without requiring human intervention. The first method involves generating QA pairs directly from the simulation, which are then used to fine-tune a VLM.\nThe second method introduces Physics Context Builders (PCBs), specialized VLMs trained on simulation data to generate detailed physical descriptions of visual scenes. Instead of fine-tuning the VLMs, such as GPT-4o or Gemini, the PCBs act as intermediaries that provide rich physical context to the LLMs. By embedding PCBs within a modular multi-agent framework, this method allows LLMs to improve their physical reasoning capabilities through in-context learning, capitalizing on the strengths of both models without altering the foundational model."}, {"title": "3.1. Overview", "content": "We introduce two methods for enhancing the physical reasoning capabilities of VLMs through simulation without requiring human intervention. The first method involves generating QA pairs directly from the simulation, which are then used to fine-tune a VLM.\nThe second method introduces Physics Context Builders (PCBs), specialized VLMs trained on simulation data to generate detailed physical descriptions of visual scenes. In- stead of fine-tuning the VLMs, such as GPT-4o or Gem- ini, the PCBs act as intermediaries that provide rich phys- ical context to the LLMs. By embedding PCBs within a modular multi-agent framework, this method allows LLMs to improve their physical reasoning capabilities through in- context learning, capitalizing on the strengths of both mod- els without altering the foundational model."}, {"title": "3.2. Fine-Tuning VLMs with Question-Answering", "content": "QA dataset creation. Our approach for generating simulation-based QA pairs involves creating a diverse set of virtual scenarios through procedural simulations and sys- tematically extracting data to form QA pairs focused on physical reasoning. Generally speaking, s shown in Fig. 1, we begin by defining a simulation environment that can render a wide range of physical scenes by varying parameters such as object types, properties, and arrangements, drawn from continuous or discrete distributions. For each simu- lation instance, we generate an initial scene based on the chosen parameters. The simulation is then executed and rendered to produce outcomes that include both visual data"}, {"title": "3.3. Physics Context Builders", "content": "Improving the physical reasoning of LLMs like GPT-4o or Gemini is challenging when fine-tuning is impractical due to computational limits, data scarcity, or proprietary re- strictions. We address this with Physics Context Builders (PCBs), specialized VLMs that enhance foundational mod- els' physical reasoning. PCBs generate detailed, question- agnostic descriptions of physical scenes, providing essential context for more advanced physical reasoning.\nPCBs are trained on simulation data to produce detailed physical descriptions of visual scenes. By supplying en- riched context that emphasizes key physical attributes and interactions, PCBs enable LLMs to perform improved phys- ical reasoning through in-context learning. This approach combines the strengths of both models: PCBs excel at in- terpreting visual inputs and extracting relevant physical in-"}, {"title": "Training PCBs", "content": "Using procedural simulation, we gener- ate extensive labeled datasets detailing object properties, dynamics, interactions, and causal relationships. This en- ables PCBs to translate visual inputs into rich narratives that capture each scene's physical essence.\nAs shown in Fig. 1, we explore two different types of description generation: 1) human-like scene narratives that mimic natural storytelling, which could be easier for models to understand; and 2) frame-by-frame structured observa- tions akin to a distilled/binned physics simulation's output.\nWe chose a human-like narrative approach because LLMs are more attuned to human language, given that their training data is primarily human-generated text [26]. This approach captures events and interactions fluidly, as they unfold in the scene. In contrast, the frame-based approach provides structured tags for each frame, detailing object properties (e.g., shape, color, material), states (e.g., location, velocity), and interac- tions (e.g., collisions). This frame-by-frame structure may help models better capture temporal relationships and reason over time. For instance, a frame tag might look like [FRAME] [OBJECTS) (OBJ) SPHERE BLUE RUBBER (LOC) (1.0, 2.3) (VEL) 0.5 [ENERGY] 1.2 [/OBJ] [/OBJECTS) [/FRAME]. Full examples of each description type are in the supple- mentary material."}, {"title": "Multi-agent integration", "content": "To effectively integrate PCBs with foundation models for downstream tasks, we adopt a multi-agent architecture (Fig. 2).\nGiven a question and visual input, the Triage agent (e.g., GPT or Gemini) first analyzes the scene to identify the re- quired type of physical reasoning. It then routes the query to the appropriate PCB, which generates a targeted physical description focused on the question. This enriched context is returned to the VLM as additional input, enabling the foundational model to deliver a precise, relevant response through in-context learning.\nIn our work, each PCB is based on PaliGemma and dif- fers only in their LoRA weights. This means the rout- ing mechanism essentially selects the appropriate LORA weights for the specific task, introducing minimal compu- tational or memory overhead. This multi-agent integra- tion provides flexibility to add or update PCBs for different physical reasoning tasks and efficiently combines the spe- cialized physical analysis of PCBs with foundational LLMs, all without modifying the underlying big model."}, {"title": "4. Experiments", "content": "We showcase several experiments to evaluate the baselines and our proposed methods, as well as the multi-agent frame- work for the PCB approach. In particular, we use the fol- lowing benchmarks (Fig. 3):"}, {"title": "Falling Tower", "content": "We create a dataset for stability detec- tion of stacked objects, similar to the ShapeStacks behch- mark [16]. In contrast to ShapeStacks, we include QAs and simulation-generated annotations that can be helpful for fine-tuning VLMs in our methods. Falling Tower comes with two question types:\n\u2022 Descriptive: e.g., \"How many objects are in the scene?\" \"What is the color/shape of the object above the purple cube?\"\n\u2022 Predictive: e.g., \"Will this collection of objects remain stationary?\" \"Will the purple cube stay stationary?\"\nDescriptive questions test spatial reasoning, while pre- dictive questions asses model's physical reasoning to deter- mine the stability of stacked objects.\nAdditionally, using the annotations, we create scene de- scriptions to train our PCB. Each scene description includes a list of objects in the scene and their offsets relative to a reference origin. We do not provide information on the sta- bility of individual objects or the entire stack in the descrip- tions. Note that Falling Tower also comes with questions about the stability of individual objects-even when the stack as a whole may be unstable, some objects at the base might still remain stable after collapse."}, {"title": "CLEVRER", "content": "We additionally test our methods on the CLEVRER dataset [39] as it includes annotations and QAS derived from simulation, which aligns with our goal of test- ing if simulation data can enhance physical reasoning.\nCLEVRER contains 10,000 training videos, 5,000 for validation, and 5,000 for testing, each paired with multi- ple questions for descriptive, explanatory, predictive, and counterfactual reasoning. Since the correct answers for the test set are not available, we use the validation set for our evaluation and train solely on the training data.\nThe training set (the validation set) of CLEVRER con- sists of 109,952 (54,990) descriptive, 16,799 (8,488) ex- planatory, 7,179 (3,557) predictive, and 18,642 (9,333) counterfactual questions. All descriptive questions are open-ended, while other types are all multi-choice ques- tions with at most four options, where there can be mul- tiple correct options. We report the accuracy of the models stratified by the question types [39]. For multi-choice ques- tions, we report both per-option accuracy, which measures the model's overall correctness on single options across all questions, and per-question accuracy that measures the model's performance on full questions, requiring all the choices to be selected correctly.\nThe physical descriptions used to train PCBs are gen- erated based on the annotation (i.e., simulation) data. As shown in Fig. 1, we consider two types of physical descrip-"}, {"title": "4.1. Training and Evaluation Details", "content": "We use PaliGemma-3B as the base pre-trained VLM for both QA fine-tuning, and PCB training on physics context descriptions. We apply LoRA-based fine-tuning and minimize the auto-regressive negative log-likelihood of the answers or context descriptions, conditioned on the in- put video/image. For videos, we sample 8 frames and ap- pend it to the input context. All the training details, includ- ing the hyper-parameters can be found in the supplementary material."}, {"title": "Training", "content": "Baselines. We compare the performance of our ap- proaches against multiple baseline models, including the base pre-trained PaliGemma-3B and its fine-tuned version on academic datasets (PaliGemma-3B-mix), as well as current state-of-the-art commercial models GPT-4o, GPT- 4o-mini, and Gemini-1.5-Pro-002 using chain-of-thought (CoT) prompting. For the GPT models, similar to PaliGemma, we sample 8 frames from the video and append them to the input context. For the Gemini models, we use the entire video as the input, as it supports video natively."}, {"title": "Framing the questions", "content": "For all questions, including the open-ended and multi-choice ones, we provide the po- tential options in the question statement. Moreover, for CLEVRER, we frame the multi-choice questions as multi- ple binary questions, where we ask if each choice can be an answer to the question or not (except predictive ones, which inherently have only two possible options). This yields sig- nificant improvement in the accuracy of all the evaluated models, as reported in the supplementary material."}, {"title": "4.2. Evaluation on Falling Tower", "content": "The results for the Falling Tower benchmark in Tab. 1 re- veal several key insights into the model performance across descriptive and predictive tasks. Larger models like GPT- 4o and Gemini-1.5-Pro achieve near-perfect zero-shot ac- curacy in object recognition. However, their performance drops considerably in physics-based predictive tasks of tower/object stability assessment, where complex dynam- ics are involved. This observation supports our hypothesis"}, {"title": "4.3. Evaluation on CLEVRER", "content": "The CLEVRER benchmark results in Tab. 2 reveal that PCB augmentation and QA fine-tuning both improve the performance, though PCB gains are more limited due to the unique challenges of this video-based task. Given that CLEVRER requires models to interpret complex, frame-by- frame interactions, we used two types of narrations, human narration (HN) and structured physics (SP). HN provides a general language description, SP offers detailed frame- by-frame information. This structured narration aids in tracking dynamics, yet video descriptions remain inherently challenging for language models to interpret accurately. Ex-"}, {"title": "4.4. Evaluating PCBs in a Multi-Agent Framework", "content": "Given the ability of foundational model LLMs to inter- pret the overall context of scenes effectively, we investi- gated whether they could reliably select the appropriate PCB when provided with a question-image pair. Our sys- tem utilizes a multi-agent triage model inspired by Ope- nAI's Swarm architecture [1]. Each input consists of a nat- ural language question from a QA dataset paired with a cor- responding scene image. This input is initially processed by a triage agent that routes the query to one of two spe- cialized agents: the Stacked Objects Analysis Agent, which acts as the PCB for the Falling Tower dataset, or the Dy- namic Scene Analysis Agent, designed for analyzing mo- tion and object interactions as in the CLEVRER dataset.\nIn our evaluation as shown in Tab. 3, which used question-image pairs, both GPT-4o and GPT-4o-mini demonstrated high accuracy as triage controllers, effectively routing queries to the appropriate agent. GPT-4o-mini ex- hibited perfect decisiveness with a 0% no-selection rate, while GPT-40 achieved high precision in selection (notably 0.9962 for stacked object tasks), although it occasionally did not make a selection (up to a 12.33% no-selection rate). These results suggest that PCBs in a multi-agent framework offer a promising approach, with LLMs capable of reliably selecting the correct PCB based on the question-image pair."}, {"title": "4.5. Evaluating the Importance of Vision Module", "content": "We illustrate the importance of the vision module in a VLM for physical reasoning by conducting the following experi- ment. Here, we QA-fine-tune only the language model part of PaliGemma-3B while freezing the vision module. The results in Tab. 4 shows that the performance across all cat- egories drops slightly for the language model-only setting. Therefore, jointly fine-tuning both the vision and language modules is essential for optimal performance, as it enables the model to better align visual features with linguistic rep- resentations."}, {"title": "5. Conclusions and Future Directions", "content": "We introduced two novel approaches to enhance the phys- ical reasoning capabilities of Vision-Language Models (VLMs) using simulation data, circumventing the need for extensive human annotations. The first approach fine-tunes a pre-trained VLM using question-answer (QA) pairs gen- erated from physical simulations. The second approach in- troduces Physics Context Builders (PCBs), which are spe- cialized VLMs trained to generate rich, detailed physical descriptions of visual scenes, including the object proper- ties and processes observed. These PCBs are incorporated into a modular multi-agent framework that augments foun- dational large language models (LLMs) during physical rea- soning tasks.\nWe evaluated both approaches using multiple bench- marks a new dataset called Falling Tower, which con- sists of static scenes focusing on stability detection, its Sim2Real part, and CLEVRER, which consists of dynamic scenes involving object collisions. The QA fine-tuning approach demonstrated excellent performance on all the benchmarks, by significantly outperforming state-of-the-art VLMs such as GPT-4o and Gemini-1.5-Pro-002. In addi- tion, the PCB approach augmented those VLMs by improv- ing their performance substantially on predictive questions for Falling Tower and moderately on most question cate- gories for CLEVRER.\nThese results confirm the value of using simulation data to address the limitations of VLMs in performing phys- ical reasoning. Unlike simulation-in-the-loop approaches [13, 25, 37, 38] requiring simulators during inference, our method uses simulation to generate synthetic data (QA pairs or context descriptions) for fine-tuning VLMs.\nOne promising future direction is to tackle more com- plex physical reasoning tasks that require larger-scale sim- ulations. This might involve integrating advanced physics engines that simulate phenomena like fluid dynamics, de- formable materials, and articulated mechanisms, enabling models to reason about phenomena such as liquids interact- ing with solids, materials bending or breaking, and multi- part objects with joints. Note that such simulations can be computationally expensive and would significantly increase the inference time for simulation-in-the-loop approaches. Our approaches, on the other hand, use simulation data for fine-tuning and therefore would not add a significant com-"}, {"title": "B. Ablations", "content": "B.1. How to ask multi-choice questions?\nAs discussed in the main paper, framing the multi-choice questions as multiple binary questions in CLEVRER can yield significant improvement in the accuracy of the models. In Tab. 6, we provide a comparison between the performance of fine-tuned PaliGemma-3B models with and without this change. As demonstrated, we observe improvement in almost all categories, except for the per question predictive accuracy. We posit that this is because the predictive questions in CLEVRER are always binary questions with exactly one correct choice. Framing the predictive questions as two independent binary questions can result in a model choosing both options as correct or wrong."}, {"title": "B.2. To train longer or shorter?", "content": "We run an ablation study to assess the effect of training for smaller vs. larger number of epochs on the accuracy of CLEVRER in the QA fine-tuning task. Tab. 7 demonstrates a large improvement in training for more epochs."}, {"title": "C. Falling Tower Dataset", "content": "The Falling Tower dataset is a benchmark for stability detection of stacked objects, inspired by the ShapeStacks bench- mark [16]. It includes 4864 unique scenes, 72,775 questions, and detailed simulation-generated annotations to support training Vision-Language Models (VLMs) for spatial and physical reasoning. Each simulation instance is represented as a JSON file containing:\n\u2022 Scene Description: A list of objects stacked from bottom to top with their respective offsets, e.g., \"Scene description: Here are the parts stacked from bottom to top: purple cube, yellow cylinder. Offsets for each part, from bottom to top, are: (-0.03, -0.05), (0.0, 0.02).\"\n\u2022 Simulation Metadata: Physical and rendering settings, including stability status (stable: true/false), the num- ber of objects, gravity parameters, and camera settings.\n\u2022 Objects: Detailed information about each object, including its type (e.g., cube, cylinder), dimensions, colors (both RGBA and HEX), rigid body properties (e.g., mass, friction), initial and final positions, and positional offsets. Rigid body prop- erties used for simulation were fine-tuned to reflect real-world dynamics, enabling us to achieve 89% accuracy in a human evaluation of 50 examples for stability detection.\n\u2022 Questions and Answers: A variety of descriptive and predictive QAs aimed at assessing spatial and physical reasoning, e.g:\nDescriptive Questions: \"How many objects are in the scene?\u201d (Answer: 2), \u201cWhat is the shape/color of the object above the purple cube?\" (Answer: yellow cylinder).\"\n    }"}]}