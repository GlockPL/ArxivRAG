{"title": "Motion Forecasting via Model-Based Risk Minimization", "authors": ["Aron Distelzweig", "Eitan Kosman", "Andreas Look", "Faris Janjo\u0161", "Denesh K. Manivannan", "Abhinav Valada"], "abstract": "Forecasting the future trajectories of surrounding\nagents is crucial for autonomous vehicles to ensure safe,\nefficient, and comfortable route planning. While model en-\nsembling has improved prediction accuracy in various fields,\nits application in trajectory prediction is limited due to the\nmulti-modal nature of predictions. In this paper, we propose\na novel sampling method applicable to trajectory prediction\nbased on the predictions of multiple models. We first show\nthat conventional sampling based on predicted probabilities can\ndegrade performance due to missing alignment between models.\nTo address this problem, we introduce a new method that\ngenerates optimal trajectories from a set of neural networks,\nframing it as a risk minimization problem with a variable loss\nfunction. By using state-of-the-art models as base learners, our\napproach constructs diverse and effective ensembles for optimal\ntrajectory sampling. Extensive experiments on the nuScenes\nprediction dataset demonstrate that our method surpasses\ncurrent state-of-the-art techniques, achieving top ranks on\nthe leaderboard. We also provide a comprehensive empirical\nstudy on ensembling strategies, offering insights into their\neffectiveness. Our findings highlight the potential of advanced\nensembling techniques in trajectory prediction, significantly\nimproving predictive performance and paving the way for more\nreliable predicted trajectories.", "sections": [{"title": "I. INTRODUCTION", "content": "Predicting the future motion of surrounding agents is a\ncrucial component of Automated Driving (AD) systems [1].\nFuture predictions, often in the form of trajectories, are\nused to facilitate the decision-making of a planner, which\ngenerates actions in order to execute a safe, efficient, and\ncomfortable trajectory to the target destination. Complex\ndriving scenarios, such as urban environments with rich\ninformation, require state-of-the-art Machine Learning (ML)\nmodels like Graph Neural Networks (GNN) [2], [3], Trans-\nformers [4], [5], and various time-series processing tech-\nniques [6], to address the problem. This enables them to\naccount for the temporal aspects of the problem while at\nthe same time considering interactions between the entities\nwithin a driving scene. Given multiple valid solutions for\nthe future behavior of traffic participants, these architectures\noften generate several predictions [7]. Capturing the plurality\nof future motion, so-called multi-modality, in rich environ-\nmental contexts necessitates many predictions. Consequently,\nthe set of valid predictions can become exceedingly large and\nprohibitively difficult to reason about for a planner. Hence,\nit is essential to obtain a minimal representative subset that\ntrades off the diversity of multiple futures while accounting\nfor their plausibility.\nMeanwhile, ensemble learning [8] has proved to be a\nuseful tool to meaningfully provide a set of multiple predic-\ntions from learned models. In many fields, simply combining\nmultiple learners, via different instances of the same model\nor from heterogeneous models, has shown great success in\nimproving prediction accuracy, robustness, and especially\ndiversity of the overall output [9]. However, their usage\nin trajectory prediction remains limited due to the multi-\nmodal nature of the output. According to [10], model-\nensembling necessitates a strategy for determining which\npredictions from different learners can be combined, present-\ning combinatorial challenges. On the one hand, this difficulty\nrenders ensembles an inconvenient tool for the task. On\nthe other hand, the potential benefit opens the door to a\npromising research direction, aiming to mitigate the gap\nbetween trajectory prediction and ensembles.\nThe successful usage of ensembles in motion prediction\nliterature is restricted either to the heatmap prediction models\nof [10], [11], whose occupancy representation facilitates\ncombining ensemble component outputs or as a tool within\nspecific architectures such as [12], [13], which outputs tra-\njectories but ensembles latent vectors over multiple instances\nof their model in order to increase output diversity. An\nalternative approach to employing ensembles is using the out-\nput representations of several current state-of-the-art models\nin leading public leaderboards, such as nuScenes [14]. As\nthey provide scores along with predictions, we can combine\ntheir strengths by constructing ensembles from these models\nand sample predictions based on their scores, such as using\ntop-k sampling. However, we demonstrate here that naively\nsampling based on the highest scores degrades performance.\nInstead, we propose an alternative method that achieves\noptimal performance w.r.t. a general definition of optimality.\nTo illustrate this in a real example, Fig. 1 shows the sub-\noptimal selection of the five most probable predictions from\na set generated by three different models, in turn inadequately\ncovering the range of possibilities. Conversely, our sampling\nstrategy yields a set of the same size that more comprehen-\nsively represents valid possibilities.\nThe beneficial ensembling methods in trajectory prediction\ndemonstrated by [11] and [13] raise further interest regarding\nthe optimality of their ensembling mechanisms. We argue\nthat employing a more sophisticated sampling strategy, rather\nthan relying on simple averaging or plurality voting as\nemployed in [13] could serve as an alternative mechanism\nto significantly enhance trajectory prediction performance.\nThis approach has the potential to further optimize the\nperformance of state-of-the-art base learners. To this end,"}, {"title": "II. RELATED WORK", "content": "Our proposed approach first ensembles diverse trajectory\nprediction models to generate sets of weighted trajectories,\nfollowed by a resampling step that applies our proposed\nsampling strategy. Thus, we structure this section along\nwith related research topics: approaches to generate sets\nof trajectories along with their probabilities, i.e., a discrete\ndistribution of multi-modal trajectories, sampling trajectories\nfrom distributions, and usage of ensemble techniques in\nprediction.\nMany motion prediction approaches in the AD context\ndirectly regress a fixed set of trajectories along with their\nlikelihoods, thus representing a categorical distribution of\ntrajectory proposals. These trajectories can also represent\nthe means of a parametric mixture distribution with separately\nregressed covariances. To achieve this, existing approaches\nfocus on various aspects of the problem, from inferring\nrelationships to surrounding map elements [15]\u2013[18], mod-\neling interaction between traffic participants [19]\u2013[22], to\nmodeling multi-modality of future motion [12], [23], [24].\nIn these contexts, various learned models are used to encode\nrich contextual information and generate trajectory outputs,\nespecially prominent being GNNs [25] and Transformers [4].\nMany such approaches are deterministic in nature or model\ndistributions without prior assumptions.\nIn contrast to approaches that directly regress a fixed\nset of trajectories, generative models in prediction have the\nability to provide an arbitrary number of trajectories by\nsampling from learned latent distributions. In this context,\nvarious Conditional Variational Autoencoder (CVAE) [26]-\n[29], Generative Adversarial Network (GAN) [30]\u2013[32], or\ndiffusion-based [21], [33] prediction approaches exist. Com-\nmon among these classes of models is the flexibility in\nconstructing sets of trajectories, however, providing likeli-\nhoods along with trajectories usually does not come 'out\nof the box'. Various strategies exist for this: the CVAE\napproaches [26], [29] learn additional classifier networks,\nwhile the diffusion approach in [33] integrates out the\ndiffusion iterations in order to compute the log probability\nof a sample. Another generative approach is [34], an autore-\ngressive maximum-likelihood approach inspired by language\nmodeling and without prior distribution assumptions, which\ncasts trajectory prediction as the problem of learning a"}, {"title": "B. Sampling Trajectories from Distributions", "content": "The concept of sampling from distributions in order to\nobtain trajectory candidates arises in various forms: (i) con-\nstructing and sampling a parametric continuous distribution,\n(ii) constructing a non-parametric distribution and drawing\nsamples via discrete optimization, or (iii) sub-sampling a\nfixed set of trajectories.\nApproaches employing (i) are typically generative models\nwhere latent distributions are explicitly modeled and samples\nare drawn and decoded into trajectories, usually via random\nsampling [26], [35]. In contrast, [29], [36] employ deter-\nministic sampling to draw latent vectors and subsequently\ntrajectories from fixed points in the domain.\nSince we do not construct explicit continuous distributions,\nour optimal sampling approach is conceptually closer to\napproaches in (ii). Here, works such as [10], [11], [37]\npredict future occupancy maps that can be interpreted as\nprobability distributions\u00b9. The occupancies are then sampled\nin a discrete optimization manner by minimizing the Miss\nRate (MR) and Final Displacement Error (FDE) metrics of\ndrawn trajectories. Our optimal sampling method in Eq. (3)\ndraws inspiration from this approach, however, our optimiza-\ntion is continuous and differentiable, thus facilitating the use\nof automatic differentiation libraries.\nWe use our optimal sampling procedure to sub-sample\nfrom a set of trajectories (iii). In this context, a notable\napproach is [38], which first constructs a dense set of\ndiscrete goals and then applies an optimization step to\nobtain a representative subset. Optimizing over goals puts\nstrong requirements on preserving diversity for heteroge-\nneous heatmaps; in order to avoid applying and tuning NMS\n(as the predecessor goal-prediction approach in [23]), they\nintroduce another, offline-trained goal set prediction task.\nOur continuous optimization approach is simpler and more\nflexible since we directly optimize over full trajectories while\noffloading the task of ensuring diversity to ensemble com-\nponents. In a similar fashion of discrete optimization, [12]\ngenerates clusters of decoded trajectories in an Expectation\nMaximization (EM) manner, starting from a pre-computed\nand sufficiently diverse initialization. In comparison, our\ngradient-based optimization approach directly optimizes the\ntarget metrics and does not require a 'good-enough' initial-\nization. Additionally, approaches in [33], [34] use NMS or\nKMeans to sub-sample trajectories; we ablate our sampling"}, {"title": "C. Ensembles in Motion Prediction", "content": "Ensembling multiple learned models to enhance the di-\nversity and performance is sparsely explored within trajec-\ntory prediction, despite the demonstrated benefits in other\nfields. Among the first to use ensembles of learned models\nis [10] in the context of heatmap prediction. This output\nrepresentation allows direct positional comparison of out-\nputs across multiple diverse heatmap generators, combining\nthe GNN-based [10] and Convolutional Neural Network\n(CNN)-based [11], thus boosting overall performance. In\ncontrast, [13] demonstrates the use of ensembles for non-\ngrid representations. Their multi-task architecture simulta-\nneously predicts driving maneuvers as a classification task\nand vehicle trajectories as a regression task. Both outputs\nundergo ensembling, using plurality voting for maneuver\nclassification and simple averaging for trajectory regression,\neffectively combining base learner strengths and improving\npredictive accuracy and robustness in varied traffic scenarios.\nSimilarly, [12] ensembles their proposed trajectory prediction\nmodel across several instances, boosting its performance by\nfirst clustering the ensemble component outputs w.r.t. overall\ndiversity and secondly, an EM optimization contingent on\na suitable initialization. In a similar fashion, the use of\ndropout can be interpreted as a form of implicit ensem-\nbling; it is used in [41] for boosting overall performance\nand in [42], [43] for epistemic uncertainty quantification.\nOverall, common among many approaches in literature is\nthe ensembling of multiple instances of the same model. In\ncontrast, our approach is the first to successfully apply the\nensembling of heterogeneous trajectory prediction models in\norder to surpass their individual performance, as well as the\nperformance of ensembles containing purely those models."}, {"title": "III. TECHNICAL APPROACH", "content": "In this section, we introduce our novel method to gen-\nerate optimal trajectories by combining predictions from an\nensemble of heterogeneous trajectory prediction models. We\nrefer to an ensemble as a set of neural networks. Our method\nis independent of how the ensemble is constructed, meaning\nthe ensemble can be generated through various methods such\nas deep ensembling [44], dropout [45], SGLD [46], or any\nother technique. We frame our approach as a risk mini-\nmization problem [47], where the ensemble approximates\nthe true risk. We observe that traditional methods, which\neither sample from the distribution over future trajectories or\nselect the most likely trajectories, degrade in performance as\nmore ensemble members are added. In contrast, our method"}, {"title": "A. Ensembles of Trajectory Prediction Models", "content": "In this work, we consider neural network-based trajectory\nprediction models $f_m(x) \\rightarrow {w_m,y_m}_{n=1}^N$ that predict $N \\in$\n$\\mathbb{N_+}$ proposal trajectories for a target agent, as commonly\ndescribed in concurrent literature [15], [17], [39], [48]. Here\n$y_m^n \\in \\mathbb{R}^{T\\times 2}$ represents the $n$-th proposal trajectory of the\n$m$-th model and $w_m^n \\in \\mathbb{R_+}$ represents the corresponding\nweight. Each proposed trajectory $y_m^n$ spans $T$ time steps and\nconsists of 2 features: the position from a top-down view. The\nweights $w_m^n$ form a standard $N$-simplex. The input $x \\in \\mathbb{R}^{D_r}$\nconsists of $D_r$ features, typically including past trajectories\nof both the target agent and surrounding agents, as well as the\nmap. Additionally, some models do not predict a weighted\nset of trajectories but instead, use a Gaussian Mixture Model\n(GMM) and rely on the mean values of the GMM as their\nproposed trajectories [16], [24].\nSuppose, we are given an ensemble of $M\\in \\mathbb{N_+}$ trajectory\nprediction models. We denote the set of all predictions as\n${w,y} = {{w_m^n,y_m^n}_{n=1}^N,..., {w_M^n,y_M^n}_{n=1}^N}$, which con-\nsists in total of $MN$ trajectory-weight pairs. Essentially, this\nset includes predictions of $M$ models, each providing $N$\nweighted trajectories. We rewrite the set of all predictions\nas a distribution\n$$q(y) = \\sum_{m=1}^M \\sum_{n=1}^N \\frac{w_m^n}{M} \\delta(y_m^n \u2013 y),$$\nwhich is a weighted sum of Dirac delta distributions that\nin turn can be interpreted as a categorical distribution.\nAssuming uniform weighting among the models, we divide\nthe weights $w_m^n$ by $M$ to ensure that the double sum over\nthe weights evaluates to 1. We also assume that the true\ndistribution $p(y)$ over future trajectories is sufficiently well\nmodeled by our approximation, i.e., $p(y) \\approx q(y)$."}, {"title": "B. What are Optimal Trajectories?", "content": "Having introduced the probabilistic interpretation, our next\ntask is to sample $S\\in \\mathbb{N_+}$ optimal trajectories, denoted as\n$\\hat{y} = {\\hat{y}_s}_{s=1}^S$. We define a set of predictions as optimal if\nit minimizes a risk, which is the expected value of a loss\nfunction $L(y, \\hat{y}) \\rightarrow \\mathbb{R_+}$. This set of optimal predictions is\nalso commonly referred to as the Bayes estimator [49], [50].\nFormally, the optimal set of predictions is determined as the\nargmin solution to\n$$\\hat{y} = \\underset{\\tilde{y}}{argmin} \\mathbb{E}_{y \\sim p(y)}[L(y, \\tilde{y})] .$$\nAbove, $y \\sim p(y)$ represents samples from the ground truth\ndistribution over future trajectories $p(y)$. The concept of risk\nminimization is widely applied in various domains, such as"}, {"title": "C. Generating Optimal Trajectories", "content": "In practice, we do not have access to the ground truth\ndistribution $p(y)$. To address this, we assume that $p(y) \\approx$\n$q(y)$. By making this assumption, we may insert Eq. 1 into\nEq. 2 and obtain\n$$\\hat{y} \\approx \\underset{\\tilde{y}}{argmin} \\sum_{m=1}^M \\sum_{n=1}^N \\frac{w_m^n}{M} minADE_k(y_m^n, \\tilde{y}).$$\nAbove we replaced the risk $L$ with $minADE_k$. The $minADE_k$\nmetric is differentiable with respect to $\\tilde{y}$, making it com-\npatible with gradient-based optimizers such as Adam [54].\nConsequently, we can optimize the objective in Eq. 3 with\nrespect to the optimal prediction set with any modern au-\ntomatic differentiation library. We refer to our method as\nmodel-based risk minimization as it minimizes the risk under\nlearner distribution $q(y)$ instead of the true $p(y)$ one. We\nprovide pseudo-code in Alg. 1."}, {"title": "IV. EXPERIMENTAL EVALUATIONS", "content": "Our paper introduces a novel method for generating tra-"}]}