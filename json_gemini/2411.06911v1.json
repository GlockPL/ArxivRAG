{"title": "Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI", "authors": ["Bruno Viti", "Franz Thaler", "Kathrin Lisa Kapper", "Martin Urschler", "Martin Holler", "Elias Karabelas"], "abstract": "Segmentation of cardiac magnetic resonance images (MRI) is crucial for the analysis and assessment of cardiac function, helping to diagnose and treat various cardiovascular diseases. Most recent techniques rely on deep learning and usually require an extensive amount of labeled data. To overcome this problem, few-shot learning has the capability of reducing data dependency on labeled data. In this work, we introduce a new method that merges few-shot learning with a U-Net architecture and Gaussian Process Emulators (GPEs), enhancing data integration from a support set for improved performance. GPEs are trained to learn the relation between the support images and the corresponding masks in latent space, facilitating the segmentation of unseen query images given only a small labeled support set at inference. We test our model with the M&Ms-2 public dataset to assess its ability to segment the heart in cardiac magnetic resonance imaging from different orientations, and compare it with state-of-the-art unsupervised and few-shot methods. Our architecture shows higher DICE coefficients compared to these methods, especially in the more challenging setups where the size of the support set is considerably small. The code is available on GitLab 6.", "sections": [{"title": "1 Introduction", "content": "Medical imaging techniques like computed tomography (CT) and magnetic resonance imaging (MRI) are established technologies to assess patient health. Automatic medical image segmentation plays an important role by mapping anatomical structures to specific semantic labels which allows more in-depth analysis through follow-up applications [15]. This is especially relevant in cardiac imaging, with cardiovascular diseases being the world's leading cause of death [25]. In recent years, deep learning (DL) models, and in particular Convolutional Neural Networks (CNNs), have been extensively employed to automate and accelerate heart segmentation, obtaining remarkable results in terms of accuracy [5]. CNNs generally require a large amount of labeled data for training, posing a challenge in the medical field with a lack of data and limited manual segmentations. Moreover, CNNs assume that training and test data are drawn from the same distribution, i.e., that they are i.i.d. Therefore, in order for the model to perform well on a different data distribution, it would require a large-scale labeled dataset to update the networks' parameters for adaptation. Consequently, several DL approaches have been suggested to face the latter problem. For example, recent works proposed unsupervised domain adaptation or generalization architectures in the cardiac segmentation scenario [6,13,18,4]. These models alleviate the need for extensively labeled data in the target domain by leveraging knowledge from a related source domain where labeled data is more abundant. However, while these techniques can handle perturbations between training and test images from different modalities and sources, they are not designed to adapt to novel image orientation [7]. Another promising technique, which can effectively reduce data dependency, is few-shot segmentation (FSS) [23,3]. In the FSS framework, the model is trained to learn how to segment a novel image, denoted as query, having at its disposal only a small set of labeled examples, denoted as support. Especially in the medical field, FSS is gaining attention, and several FSS architectures have been proposed [8,14,17,21]. The method's effectiveness highly depends on the mechanism that extracts the information from the support set and integrates this new information with the query image. Most of the existing models rely on the prototype alignment paradigm, which tries to learn a common representation space, where the feature vectors of different object classes can be aligned, e.g. [24]. Differently, Johnander et al. [11] proposed a novel approach based on GPEs, which they exploit to learn a mapping between dense local deep feature vectors and their corresponding mask values. Saha et al. [22] also used FSS in combination with GPEs and applied their method to microscopy images. However, the majority of these approaches [8,14,17,21,24] are limited to predicting binary segmentations. As we also show in this work exemplarily for [14], this results in a non-efficient class-by-class segmentation for multi-label query images [9]. To face this issue, [9] introduced an extension of prototype alignment to perform one-step multi-class segmentation. They employ a self-supervised training approach, facing the challenge of limited labeled image data. However, in [9] they still consider the same type of images during training and testing, namely 2D short-axis cardiac MRI.\nIn contrast, our goal is to cope with the scarcity of data in the cardiac setting and to make segmentation models more adaptable to different cardiac image orientations. Therefore, we propose a model for multilabel cardiac image segmentation, which combines a U-Net-like architecture [20] with GPEs as extractors of information from the support set [11]. Specifically, we use the contracting branch of"}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Problem Definition", "content": "Few-shot segmentation aims to segment novel images given only a tiny set of labeled examples from the same distribution. In a fully supervised setting, training and test data are assumed to follow the same distribution, and further, each training instance is composed of an image and a segmentation mask. In the FSS framework, each instance, denoted as episode, includes a small set of labeled images, denoted as the support set, and an unlabeled image, denoted as the query set, both following the same distribution. More formally, we denote the support set as {(Xs,k, Ys,k)}K, including K pairs of images, XS,k \u2208 \\mathbb{R}^{Ho\u00d7Wo} and the corresponding segmentation masks, Ys,k \u2208 {0,1,..., L - 1}^{Ho\u00d7Wo} where L is the number of labels and Ho, Wo refer to the height and width of the image, respectively. The task is to predict the segmentation mask YQ of the query image XQ. During training, each episode includes a support and query set drawn from the domain distribution Dtr. For testing, the model predicts the segmentation mask of a novel image from the distribution Dte, having only a few examples of that distribution at its disposal."}, {"title": "2.2 Model Architecture", "content": "Our network architecture is based on the work of [11], and combines their GPE regressor with a U-Net architecture [20]. Although the U-Net has been chosen due to its effectiveness in handling medical images, the GPEs regressors are the fundamental components to extract the information from the support set and lead the model towards the correct segmentation of unseen images. Our model contains three major blocks, which are trained end-to-end, see Fig. 1.\nContracting path As first step, we gradually extract and encode features from the input images while reducing their spatial dimensions. For each training instance, the input includes a query image XQ \u2208 \\mathbb{R}^{Ho\u00d7Wo}, a support set of images Xs \u2208 \\mathbb{R}^{Ho\u00d7Wo\u00d7K}, and the corresponding masks Ys \u2208 {0,..., L \u2212 1}^{Ho\u00d7Wo\u00d7K}. The three inputs are moved into the latent space to extract their deep features. We employ the same feature extractor for XQ and Xs as both are MRI. However, we need to train another encoder for the support masks Ys. At the end of the contracting path, we have\nXQ = Ex(XQ) \u2208 \\mathbb{R}^{H\u00d7W\u00d7F},\nxs = Ex(Xs) \u2208 \\mathbb{R}^{H\u00d7W\u00d7KF},\nys = Er(Ys) \u2208 {0,..., L \u2212 1}^{H\u00d7W\u00d7KF},\nwhere F is the number of features.\nGPEs few-shot learning GPEs offer a flexible framework for modeling complex functions and are suitable for working with small datasets [19]. In our case, the training points are the support features, which can be reshaped as xs \u2208 \\mathbb{R}^{HWK\u00d7F}, and ys \u2208 {0,..., L - 1}^{HWK\u00d7F} is the corresponding output. With this reshaping, we assume that we have HWK training points, and each of them lives in \\mathbb{R}^{F}. As GPEs exhibit a computational complexity of O(N\u00b3) for datasets of size N [19], our GPE will scale as O((HWK)\u00b3), making it necessary to work in a latent space in which H \u00ab Ho and W \u00ab Wo. The GPE aims to predict yQ given a new input xQ. The underlying assumption is that the two outputs ys and yq are jointly Gaussian, i.e.\n$\\begin{pmatrix}\nys \\\\\nYQ\n\\end{pmatrix} \\sim N\\begin{pmatrix}\n\\mu_s \\\\\n\\mu_q\n\\begin{pmatrix}\n\\Sigma_{ss} & \\Sigma_{SQ} \\\\\n\\Sigma_{QS} & \\Sigma_{QQ}\n\\end{pmatrix}\n\\end{pmatrix}.$ (1)\nwhere \u03bcs and \u00b5o are the prior means, which for simplicity are set to 0. The covariance matrices \u2211 encode the relationship between different points in the input space and quantify the similarity between them. Given two points xi,xj \u2208 \\mathbb{R}^{F}, the covariance matrix is defined by the kernel function Ei,j = k(xi, xj), where k(,): \\mathbb{R}^{F} \u00d7 \\mathbb{R}^{F} \u2192 \\mathbb{IR}. Ideally, for two similar points x1 and x2 we would expect similar outputs y1 and y2. In our model, we employ the squared"}, {"title": null, "content": "exponential kernel function, due to its smoothness and interpretability, defined as\nk(x1, x2) = s2e^{-\\frac{||x_1-x_2||^2}{l^2}}. (2)\nwhere s and l are two additional hyperparameters that control the amplitude of the function values and its smoothness. Using the Gaussian assumption in (1), we can infer the distribution of yq as\nyq|xq, xs, ys ~ N(\u03bcq|s, \u2211Q|s),\nwhere\n$\\mu_{q|s} = \\Sigma_{QS} (\\Sigma_{ss} + \\sigma^2I)^{-1}ys$,\n$\\Sigma_{Q|s} = \\Sigma_{QQ} - \\Sigma_{QS} (\\Sigma_{ss} + \\sigma^2I)^{-1} \\Sigma_{SQ}$.\nHere, the term \u03c3\u00b2I is used to bypass numerical issues and to account for noise in the data [1]. The hyperparameter \u03c3 represents the variance of noise that affects the observations. This hyperparameter, as well as s and l, are learnable and therefore trained alongside the network weights. Thanks to the Gaussian assumption, we can infer the conditional mean of the output yq, which represents the closest prediction of yo in the latent space according to the GPE [11].\nExpanding path We equip our model with an additional mechanism that is able to infer the posterior distribution of the query mask yo, given the image-mask pairs of the support set. At this point, we combine the deep features of the U-Net with the GPE prediction. To do that, we concatenate the posterior mean \u03bcQ|s with the representation of the image in the latent space xQ, obtaining zQ = (\u03bcQ|s, xQ). The latter is given as input to the decoder Ds, which predicts the mask of the query image \u0176Q = D\u00e7(z\u0119). It should be noted that, as proposed in [11], we can successfully leverage the skip-connection structure in the few-shot learning paradigm. In addition to learning the distribution of yq in the deep latent space, we can do the same after each level of the U-Net encoder. More formally, given our encoders Ex and Er with depths D, we denote by xg and yg, the output of the encoders after d levels. Following the previous terminology, we have x-1 = xs, x = Xs, y3-1 = ys and yg = Ys. The same idea applies to xQ. Standard U-Net skip-connections concatenate the output of level d, xo, of the encoder, with the features in the decoder at the same level. We can combine this mechanism with our GPE regressor. For each level d of the encoders, we assume that\nyx, x, y ~ \u039d (\u03bc\u03bf\u03c2, \u03a3\u03b1\u03c2).\nAt this point, we construct the corresponding zo = (\u03bc\u03bf\u03c2, x) and pass it to the level d of the decoder Dr. In the shallower levels, where d <D-1, the resolution of the images and masks is too large; therefore, we down-sample them to the same size as used in the last level to make the GPEs computationally feasible."}, {"title": "2.3 Training procedure", "content": "The construction of meaningful query and support instances is crucial for FSS. In our setup, the model must be trained with different anatomical views to segment a novel view in the test set. To achieve that in the training phase, we treat SA slices from the base to the apex of the heart as different anatomical views based on their size, but with the same image orientation. Specifically, we divide our training dataset into 10 new subsets based on the dimension of the heart, via counting the pixels of the ground truth. In this way, each subset contains cardiac MRIs with similar appearance and GPEs can be trained with different views. During training, each query-support instance is constructed with images from the same subset. For testing, we used a different orientation in the anatomical view, which corresponds to the LA slices."}, {"title": "3 Experiments and Results", "content": "We evaluate our method for a scenario where only SA 2D MRI slices are available with ground truth for training, while LA slices of different patients have to be predicted. In this scenario, it is beneficial to have a method that only requires few labeled LA slices in order to optimally segment the whole LA set. We denote our training set as {Xtr,i}_{i=0}^{N-1} \u2208 Dtr and our testing set as {Xte,i}_{i=0}^{N-1} \u2208 Dte, where Dtr = {SA}, Dte = {LA} hence Dtr \u2260 Dte. We compare our methodology with different baselines. We first measure the ability of a fully supervised method to generalize from SA to LA, having at its disposal only a few LA labeled samples. For this task, we use the widely used nnU-Net due to its good performance on a wide variety of medical image segmentation datasets [10]. Additionally, we compare our model to CAT-Net, a network for medical image FSS [14]. Finally, we also utilize the unsupervised CSDG method [18], a recently published method for domain generalization.\nDataset For our evaluations, we use the M&Ms-2 dataset [2,16]. The cohort contains 360 patients with different pathologies of the right ventricle and left ventricle, as well as healthy subjects. M&Ms-2 is split into 200 patients for training and 160 for testing. Two pathologies are only present in the test set to evaluate the generalization to unseen pathologies. For each patient, the dataset provides 2 annotated LA MRI slices and about 20 annotated SA MRI slices. As described above, we use all of the SA slices for training and the LA slices for testing.\nImplementation Details The two encoders, Ex and Er, contain five lev-els, each consisting of two convolutional blocks followed by ReLU activations, dropout, and batch normalization, ending with max pooling. For decoding, we use bilinear up-sampling at each level, followed by one convolution. The preprocessing involves intensity normalization to zero mean and unit variance, and bilinear resampling to 1.375 mm \u00d7 1.375 mm in-plane resolution. We use the ground"}, {"title": "4 Conclusions", "content": "In this work, we proposed a few-shot cardiac segmentation architecture that combines the strengths of a U-Net with the flexibility of GPEs to segment different cardiac orientations with a limited amount of data. Our model incorporates information extracted from a limited support set with deep features of the CNN, increasing the generalizability of the whole architecture. We evaluated our method for a scenario in which only SA slices are available for training and obtained meaningful heart segmentations for LA orientation. In particular, we obtain a great performance boost using a single support image. As a result, with small effort in terms of annotation, it is possible to work with the differently oriented cardiac images without retraining of the whole network. We plan to further improve our model, in particular, the GPE component by taking into account the conditional variance, and aim to explore the possibility of integrating uncertainty quantification in the predictions."}]}