{"title": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with Multi-Agent LLMs", "authors": ["Firoj Alam", "Md. Rafiul Biswas", "Uzair Shah", "Wajdi Zaghouani", "Georgios Mikros"], "abstract": "In the past decade, social media platforms have been used for information dissemination and consumption. While a major portion of the content is posted to promote citizen journalism and public aware-ness, some content is posted to mislead users. Among different content types such as text, images, and videos, memes (text overlaid on images) are particularly prevalent and can serve as powerful vehicles for propaganda, hate, and humor. In the current literature, there have been efforts to individually detect such content in memes. However, the study of their intersection is very limited. In this study, we explore the intersection be-tween propaganda and hate in memes using a multi-agent LLM-based approach. We extend the propagandistic meme dataset with coarse and fine-grained hate labels. Our finding suggests that there is an association between propaganda and hate in memes. We provide detailed experimental results that can serve as a baseline for future studies. We will make the experimental resources publicly available to the community.", "sections": [{"title": "1 Introduction", "content": "Social media has emerged as a primary channel for freely sharing content online. Its exponential growth has significantly transformed the landscape of information dissemination. However, misuse of these platforms has made them fertile grounds for the spread of inappropriate content, misinformation, and disinformation [2]. While interactions on social media facilitate public discussions on a range of topics, from local issues to politics, they also harbor and propagate hate speech and offensive content through various content types, text, images, and videos [25,16,31,2].\nTo address such problems across different modalities, there have been efforts to automatically detect them using both mono- and multimodal modeling ap-proaches [9]. For propagandistic content detection, research efforts have specifi-cally focused on defining techniques and tackling the issue across various types of content, including news articles, tweets, memes, and textual content in multiple"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Propagandistic Content", "content": "Textual Content: The study of propagandistic content has attracted significant attention in recent years. Da et al. (2020) introduced a large-scale dataset for fine-grained propaganda detection in news articles, presenting a corpus of 350K sentences annotated with 18 propaganda techniques [8]. The annotation schema has been extended to include 23 techniques and a multilingual corpus development has been proposed in [27]. Following the same annotation schema, datasets and shared tasks have been developed for Arabic [4,20,19].\nMultimodal Content: Building on previous research in textual content, Dimitrov et al. (2021) introduced SemEval-2021 Task 6, which focuses on detecting persuasion techniques in texts and images within memes [14]. Subsequently, the focus has expanded to include the detection of multilingual and multimodal propagandistic memes [12]. Similarly, related work on Arabic involves the devel-opment of datasets and a shared task for propaganda detection [3,21]. Fang et al. (2022) utilized separate networks to embed text and images and then fused these multi-modal embeddings. They employed a split-and-share module with multi-level representations to enhance the detection performance of persuasive techniques [15]."}, {"title": "2.2 Hateful Memes", "content": "The study of hateful memes presents unique challenges due to their multimodal nature. Kiela et al. (2020) introduced the Hateful Memes Challenge, a large-scale dataset and benchmark for multimodal hate detection. This work highlighted the importance of integrating both textual and visual elements to identify hate speech in memes [23]. Sharma et al. (2022) proposed a multi-task learning ap-proach for detecting hate speech in memes by combining visual and textual features. Their model demonstrated improved performance over unimodal ap-proaches, underscoring the synergy between different modalities in hate speech detection [30]. Addressing the challenge of low-resource languages, datasets have also been developed in various languages [22]. In [31], the authors provide a de-tailed survey of multimodal and harmful memes, highlighting the significance of the problem and proposing future research avenues."}, {"title": "2.3 Multi-Agent Systems in Content Analysis", "content": "The application of multi-agent systems to content analysis is an emerging field, which could be an effective approach in analyzing complex narratives across various media [17]. Chen et al. (2021) introduced a dynamic content modera-tion system using multi-agent reinforcement learning, which adapts based on user interactions and content patterns for improved detection of harmful con-tent [7]. These studies emphasize the value of multi-agent systems in analyzing complex, often propagandistic or hateful content in memes. Building on this, our work focuses on Arabic memes, employing a multi-agent LLM approach. We explore the association between propaganda and hateful memes in low-resource settings. We employ LLMs as multiple agents to automate the data annota-tion process, demonstrating the utility of LLMs as data annotators in detecting hateful memes."}, {"title": "3 Dataset", "content": ""}, {"title": "3.1 Propagandistic Memes", "content": "The dataset consists of approximately ~3k memes, annotated as propagandistic versus non-propagandistic, which were collected from various social media platforms such as Facebook, Twitter, Instagram, and Pinterest. Each meme was annotated by three annotators, with the majority decision considered as the final label. Texts from the memes were extracted using an off-the-shelf OCR tool,1 followed by manual editing for the propagandistic memes. The distribution of propagandistic and non-propagandistic labels is 40% and 60%, respectively. More details on this dataset can be found in [21], and the detailed annotation guide-lines can be found in [3]. The dataset is split into 70%, 10%, and 20% for training, development, and testing, respectively."}, {"title": "3.2 Hatefulness and Fine-grained Categories", "content": "We used ArAIEval-2024 dataset [21] for the hatefulness and their fine-grained categorization. Our motivation to use ArAIEval-2024 dataset is that this is the only meme dataset currently available for Arabic, which has already been an-notated for propagandistic content. Another motivation was to understand the association between propagandistic and hateful memes. In Figure 1, we provide full pipeline for the data preparation to classification experiments."}, {"title": "3.2.1 LLM Agents as Annotators", "content": "To employ LLM agents as annotators, we selected three well-known and top-performing commercial models: OpenAI's GPT-40 [26], Google's Gemini Pro (version 1.5) [32], and Claude 3.5 (Sonnet).2.\nFor the annotation process, we use the same manual procedure discussed in [18], which involves a two-phase approach. In the first phase, known as the annotation phase, three annotators independently annotate memes following the guidelines outlined in 3.2.2. In the second phase, known as the consolidation phase, we review and resolve any disagreements from the annotations received during the first phase. As illustrated in the figure, highlighted in dark red, we employ LLM agents as annotators in the first phase and as a consolidator in the second phase. For each phase, we use a specific prompt in a zero-shot setup for the LLM agent. Following the annotation guidelines discussed below, we ask an LLM agent to perform two tasks: (Task 1) label each meme as hateful or not-hateful, and (Task 2) based on the label from Task 1, provide a fine-grained categorization. For example, if a meme is categorized as hateful in Task 1, it should then provide a fine-grained label from one of the eight categories mentioned below. The prompt in the second phase is slightly different. Here, the task also involves considering the labels obtained from the first phase to make a final decision. For this phase, we have experimented with using GPT-40 as the consolidator."}, {"title": "3.2.2 Manual Annotation", "content": "To verify the LLM-based multi-agent approach, we manually annotated a test set from the ArAIEval-2024 dataset, as shown in Figure 1. For the annotation process, we developed a set of instructions, which are discussed below. The typical approach to annotation involves two to three annotators. However, for this study, we relied on a single annotator who had prior experience with similar annotation tasks."}, {"title": "3.2.3 Annotation Instructions", "content": "The purpose of this annotation is to identify whether a meme is hateful or not-hateful. A hateful meme can attack different individuals, organizations or entities. Therefore, another task is identifying the attack types. A non-hateful meme can be humorous or sarcastic. Therefore, the idea to also identify the sub-categories within non-hateful memes. Below we provide the definitions:\nHateful: A direct or indirect attack on people based on characteristics, includ-ing ethnicity, race, nationality, immigration status, religion, caste, sex, gender identity, sexual orientation, and disability or disease. We define attack as violent or dehumanizing (comparing people to non-human things, e.g., animals) speech, statements of inferiority, and calls for exclusion or segregation. Mocking hate crime is also considered hate speech. Attacking groups perpetuating hate (e.g. terrorist groups) is not considered hate.\nFine-grained categories hatefulness:\nDehumanizing: Explicitly or implicitly describing or presenting a group as subhuman.\nInferiority: Claiming that a group is inferior, less worthy or less important than either society in general or another group\nInciting violence: Explicitly or implicitly calling for harm to be inflicted on a group, including physical attacks\nMocking: Making jokes about, undermining, belittling, or disparaging a group\nContempt: Expressing intensely negative feelings or emotions about a group\nSlurs: Using prejudicial terms to refer to, describe or characterize a group\nExclusion: Advocating, planning or justifying the exclusion or segregation of a group from all of society or certain parts\nOther: None of the above\nNot-Hateful: The content is humorous, neutral, or positive, without targeting or harming specific individuals or groups. It is light-hearted and intended for en-tertainment without being offensive. Additionally, the content does not promote or incite violence, hatred, or discrimination.\nFine-grained not-hateful categories:"}, {"title": "3.2.4 Annotated Dataset", "content": "As discussed in Section 3.2.1, we annotated the test data into 'Hateful' and \u2018Not-Hateful' categories using GPT-40, Sonnet (Claude 3.5), and Gemini (Vertex). We then provided the three annotated labels (ob-tained from GPT-40, Sonnet, and Gemini) as prompts to GPT-40 and asked it to choose the best label that matches the data. The generated output label is termed GPT-40 consolidation. Table 1 shows the inter-annotator agreement (IAA) among the annotators. We computed the annotation agreement using pairwise Cohen's kappa score in different setups: (i) LLMs as annotators vs. an LLM as a consolidator, (ii) LLMs as annotators vs. human annotation, and (iii) pairwise between LLMs as annotators.\nIt shows that the IAA between GPT-40 and GPT-40 consolidation is high (0.786), representing substantial agreement. It is reasonable because the GPT-40 consolidation is derived from the GPT-40, which means that the consolidated label inherently aligns closely with the labels initially provided by GPT-40. In-terestingly, we observe that the IAA between Sonnet and GPT-40 consolidation is significant and high (0.701), which denotes Sonnet's capability to understand hateful content and memes.\nTable 1 shows that the IAA between Sonnet and the human annotator achieved a higher score (0.405) compared to other annotation labels. We also per-formed pairwise agreements among LLM annotators and found that the agree-ment between Sonnet and GPT-40 is higher (0.528). Finally, we measured the annotation agreement among all three annotators and obtained a score of 0.369.\nThe annotation agreements between the three different LLMs and the human annotator suggest that Sonnet has a fair capability of understanding 'Hateful' content compared to a human annotator. Therefore, we used Sonnet to annotate the training and development datasets.\nData Stat: Hateful Meme Table 2 presents the distribution of class la-bels for training, development, and testing datasets, categorized into \"Hate/Not-hate\" and further labeled into fine-grained categories. There is a significantly larger number of \"Not-Hateful\" (N=1931) category instances compared to the \"Hateful\" (N=212) category. In the fine-grained label, \u201cMocking\u201d has a notable presence (N=133) in the 'Hateful' category. Similarly, in the fine-grained \"Not-Hateful\" categories, \u201cHumor\u201d (N=1815) overwhelmingly dominates, followed by \"Sarcasm\". This distribution highlights the imbalance in the data.\nPropaganda and Hateful Meme: To understand the correlation between propaganda and hateful memes, we observe that out of 171 propagandistic"}, {"title": "Humor:", "content": "The purpose of humor is to entertain, amuse, or bring joy to the audience. Often characterized by jokes, puns, or playful language. Humor can vary widely in style, including wit, slapstick, parody, and satire."}, {"title": "Sarcasm:", "content": "Typically involves saying the opposite of what one means. Sarcasm is a form of irony that always occurs with a deliberate mismatch be-tween what is said and what is meant, intentionally to ridicule or mock a specific target."}, {"title": "4 Experiments", "content": "Our experiments consist of three setups: (i) hate vs. not-hate, (ii) fine-grained categories for hateful memes, and (iii) fine-grained categories for non-hateful memes. These classification experiments involve unimodal (text and image) and multimodal classifications.\nText classification: We extracted text from propagandistic memes and ap-plied various text classification techniques such as AraBERT, mBERT, CAMel-BERT, and Qarib-BERT ([6,11,1]). The original dataset is imbalanced, and so we implemented a class weighting scheme during the fine-tuning process. More-over, we optimized the model by adjusting the dropout rate. This approach led to significant improvements over using the original dataset alone. We embed-ded the LoRa to fine-tune the model in an efficient way that does not require fine-tuning all the parameters of the model. However, embedding LoRA in the"}, {"title": "Image classification:", "content": "We fine-tuned ResNet50 and ConvNeXt-tiny [24]. To ensure stable weight adjustments, we froze the feature extraction layers and fine-tuned the classification layer. We also adjusted the dropout rate during the model training."}, {"title": "Multimodal classification:", "content": "To extract visual and text features, we applied ConvNext tiny and AraBERT models respectively. We combine the features us-ing a fusion layer. We froze the visual models and trained the classification layer with textual data. We also adjusted the dropout rate to improve the perfor-mance."}, {"title": "Experimental Setup:", "content": "We performed all of our experiments and trained our models on an Nvidia-RTX 2080 GPU. We employed the Adam optimizer with an initial learning rate of le-5 and le-4 for text and image, respectively. We used a batch size of 32 containing a 128-character sentence length and 224x224 image. We set the dropout rate to 0.25 for text data and trained for 50 epochs. For image data, we set a dropout rate of 0.5 and trained for 30 epochs. For multimodal data, we trained the model for 100 epochs with a stochastic drop rate of 0.2. Note that our choice of the models and parameters was inspired by prior studies [3,29]."}, {"title": "5 Results and Discussion", "content": "In Table 3, we present the results for different classification setups. For hate vs. not-hate classification across different modalities, considering macro-F1, the text and multimodal models exhibit similar performance. For the multimodal model, we obtained model with a macro-F1 score of 0.709 and an accuracy of 0.764. For the text modality, we obtained a macro-F1 score of 0.705 and an accuracy of 0.818. For the image modality, we obtained a macro-F1 score of 0.669 and an accuracy of 0.775.\nTo understand the class imbalance issue, we selected 500 propaganda labels and 500 non-propaganda labels from the dataset and applied the fusion model. This yielded a macro-F1 score of 0.709 and an accuracy of 0.818. The hateful memes were further fine-grained into eight labels, while the non-hateful memes were fine-grained into two labels, as shown in Table 2. We applied the fusion model individually to the hateful and non-hateful fine-grained labels. The F1-score for the hateful fine-grained memes is 0.224, with an accuracy of 0.166, which is very low. This occurs because there are multiple labels within the hate-ful category, and the dataset is imbalanced for fine-grained hateful memes. In contrast, with only two labels in the non-hateful memes, the model performs better in classification, achieving a macro-F1 score of 0.537 and an accuracy of 0.622."}, {"title": "6 Conclusion and Future Work", "content": "In this study, we investigate whether the content in propagandistic memes may contain hate. To do so, we used a multi-agent LLM-based approach to label propagandistic memes with coarse and fine-grained hate categories. We observed that there is a moderate agreement between an LLM agent (Claude 3.5 Sonnet) and human annotation. This led us to label propagandistic memes with coarse and fine-grained hate categories. We further used the dataset to train the model and evaluate its performance on the test set. The developed dataset is skewed in nature, which also reflects its classification performance. It is important to note that this attempt can enable the development of a large-scale dataset in a cost-effective manner. The issue of label imbalance can be resolved with an increase in data size. Future study will investigate this direction further by increasing data size and exploring open-sourced LLMs."}]}