{"title": "Tell me why:\nVisual foundation models as self-explainable classifiers", "authors": ["Hugues Turb\u00e9", "Mina Bjelogrlic", "Gianmarco Mengaldo", "Christian Lovis"], "abstract": "Visual foundation models (VFMs) have become\nincreasingly popular due to their state-of-the-art\nperformance. However, interpretability remains\ncrucial for critical applications. In this sense, self-\nexplainable models (SEM) aim to provide inter-\npretable classifiers that decompose predictions\ninto a weighted sum of interpretable concepts.\nDespite their promise, recent studies have shown\nthat these explanations often lack faithfulness. In\nthis work, we combine VFMs with a novel proto-\ntypical architecture and specialized training objec-\ntives. By training only a lightweight head (approx-\nimately 1M parameters) on top of frozen VFMs,\nour approach (ProtoFM) offers an efficient and in-\nterpretable solution. Evaluations demonstrate that\nour approach achieves competitive classification\nperformance while outperforming existing models\nacross a range of interpretability metrics derived\nfrom the literature. Code is available at https:\n//github.com/hturbe/proto-fm.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks have shown impressive results in\nvision tasks, including segmentation and classification. Re-\ncent progress has been largely driven by the development of\nvisual foundation models (VFMs), that is, models trained\non vast image datasets that serve various downstream tasks.\nAs VFMs are increasingly applied in diverse domains, in-\nterpretability is becoming crucial for critical fields like\nmedicine and remote sensing. Given the ambiguity around\nwhat interpretability means (Lipton, 2018), in this work we\nfocus on the ability to reflect the influence of the input fea-\ntures on the model's prediction. Besides legal requirements\n(Metiko\u0161 & Ausloos, 2024), the absence of interpretabil-\nity has often been pointed out as a barrier to the adoption\nof deep learning models in various domains, such as the\nhealthcare sector (Reddy, 2022). Beyond the adoption chal-\nlenges, interpretability may allow for scientific discovery\nwhere a model might have discovered some \"rules\" in the\ndata unknown to the scientific community that can be uncov-\nered through explaining the model's predictions (Mengaldo,\n2025).\nMethods for model interpretability can be broadly catego-\nrized into two paradigms based on the stage of model de-\nvelopment where interpretability is incorporated: (i) the\nintrinsic paradigm, and (ii) the post-hoc paradigm (Madsen\net al., 2024). Post-hoc methods aim to provide explanations\nfor already trained models. These methods are often model-\nagnostic (e.g., SHAP (Lundberg & Lee, 2017)), and do not\ninterfere with model training or performance. However,\nthese methods are often criticized for their lack of faith-\nfulness (Adebayo et al., 2018; Hooker et al., 2019a; Turb\u00e9\net al., 2023; Wei et al., 2024), raising significant concerns\nabout their reliability in critical applications. This has led to\ngrowing advocacy for the intrinsic paradigm (Rudin, 2019),\nwhich involves models designed to be interpretable by de-\nsign, which are also referred to as self-explainable models\n(SEM).\nA notable direction within SEM is the development of part-\nprototype models, which aim to decompose predictions into\na weighted sum of interpretable concepts. Although these\nmodels are theoretically designed to provide consistent ex-\nplanations, recent findings indicate that the explanations\nthey generate often lack faithfulness. Specifically, these\nmodels tend to mislocalize critical regions of the input\nthat are important for classification (Sacha et al., 2024;\nCarmichael et al., 2024a) and fail to represent coherent\nconcepts in the input space (Hoffmann et al., 2021), aspects\nthat undermine their interpretability. Another general criti-\ncism of the intrinsic paradigm is that previous approaches\nare not designed to leverage pre-trained models, such as\nvision foundation models (VFMs) (Madsen et al., 2024).\nIn this work, we aim to address the challenges just described\nthat affect SEMs. We achieve this goal by designing a new"}, {"title": "2. Related work", "content": "The development of Prototypical Part Network research was\ninitiated by the ProtoPNet architecture (Chen et al., 2019).\nProtoPNet works by comparing parts of an image to learned\nprototypes, which are meant to represent semantically co-\nherent concepts. This approach allows the model to make\npredictions based on the similarity of image parts to these\nprototypes, offering a form of reasoning that is qualitatively\nsimilar to human decision-making.\nSeveral challenges with the original ProtoPNet have led to\nthe development of more advanced models. The original\nProtoPNet used a fixed number of class-specific prototypes.\nThis means that each class had a pre-determined number\nof prototypes associated with it, which could lead to large\nexplanations and redundant prototypes (Nauta et al., 2021a).\nProtoPShare was developed to address this issue by enabling\nprototypes to be shared across different classes, reducing\nthe total number of prototypes needed and allowing the\nmodel to find similarities between classes (Rymarczyk et al.,\n2021). ProtoPool built on this work and introduced a fully\ndifferentiable assignment of prototypes to classes, allowing\nend-to-end training of the model (Rymarczyk et al., 2022).\nOther approaches have explored different prototype repre-\nsentations. Deformable ProtoPNet introduced spatially flex-\nible prototypes (Donnelly et al., 2022). ST-ProtoPNet aimed\nto improve classification accuracy by learning support and\ntrivial prototypes, drawing an analogy with Support Vector\nMachine theory (Wang et al., 2023). On a different note,\nProtoTree looked at replacing the final linear layer with a de-\ncision tree (Nauta et al., 2021b), while PIP-Net focused on\nproducing sparse explanations (Nauta et al., 2023a). Most of\nthe presented works described above use different variations\nof CNN backbones, but the ProtoVit architecture recently\nlooked at leveraging Vision Transformers (ViT) (Ma et al.,\n2024).\nWhile part-prototype networks are theoretically designed\nto be interpretable, researchers have investigated their inter-\npretability and identified several limitations that can under-\nmine this promise. Studies have shown that such models\nmay exhibit: (i) a semantic gap, where prototypes fail to\nconsistently represent the same concepts across different\nimages (Hoffmann et al., 2021; Kim et al., 2022b; Nauta\net al., 2023a), and (ii) spatial misalignment, where the pixels\nused by the model for predictions are not correctly local-\nized (Carmichael et al., 2024b; Sacha et al., 2024).\nWhile several studies have examined specific aspects of in-\nterpretability evaluation (Gautam et al., 2022; Carmichael\net al., 2024b; Huang et al., 2023), none have provided a\nholistic and quantitative assessment of prototypical models'\ninterpretability that addresses its multifaceted nature. The\nCo-12 properties were recently introduced as a comprehen-\nsive framework for evaluating explanation quality (Nauta\net al., 2023b). These properties have been designed for\ninterpretable methods in general, including both post-hoc in-\nterpretability methods and SEMs. We summarize the most\nimportant properties in Table 1. They encompass many\ndesiderata that have been independently formulated in the\nliterature focused on SEMs with metrics such as Prototyp-\nical part Location Change (PLC) (Sacha et al., 2024) or\nRelevance Ordering Test (ROT) (Carmichael et al., 2024b)\nwhich aim to evaluate the spatial alignment and fall under\nthe Correctness property. Metrics to evaluate the consis-\ntency, stability and compactness desiderata have also been\nproposed in the literature (Huang et al., 2023; Nauta et al.,\n2023a)."}, {"title": "Contributions", "content": "1. Introduce a novel architecture which allows to\nleverage frozen VFMs providing a lightweight\napproach (\u2248 1M trained parameter). The\nevaluation demonstrates that the architecture\nis competitive in terms of classification per-\nformance achieves SOTA performance across\nthe range of interpretability desiderata derived\nfrom the literature.\n2. We extensively evaluate a number of prototyp-\nical models from the literature, highlighting\nimportant issues regarding the correctness and\ncontrastivity of the explanations obtained with\nthese models, justifying the introduction of the\nnovel architecture."}, {"title": "3. Methodology", "content": "Problem setting We consider the classification task that\nconsists of mapping an image $X \\in \\mathbb{R}^{H \\times W \\times C}$ to a labelled\ntarget $Y \\in \\mathbb{N}^{D}$ where $H, W, C$ represent, respectively, the\nheight, width, and number of channels of the input image,\nand $D$ is the number of classes."}, {"title": "3.1. Model architecture", "content": "As depicted in Figure 1, our model ProtoFM leverages a\nfrozen VFM as a visual feature extractor $f$ mapping image x\nto patch embedding $F_i \\in \\mathbb{R}^{C}$ for patch index $i \\in [1, . . ., I]$,\nand $I = \\frac{H \\times W}{s^{2}}$ with $s$ indicating the patch size of the\nVFM and $C$ the embedding dimension. Similarly a version\naugmented with geometric and color transformation x', is\nmapped to F'.\nA projector z then maps the feature from the backbone into\nan embedding space, of dimension $c_z$, such that the im-\nage and its augmented version are mapped to $Z = z(F)$\nand $Z' = z(F') \\in \\mathbb{R}^{c_z \\times I}$. A cosine similarity is then\ncomputed between Z and a set of trainable prototypes\n$P{s,t} = {p_n \\in \\mathbb{R}^{c_z}}$, with $n \\in [1,\u2026\u2026, N]$ where N is\nthe number of prototypes. The prototypes aim to represent\nthe concepts that can decompose the image and be used to\nclassify the latter. The s and t exponents respectively refer\nto the student and teacher prototypes, with the latter being\nupdated through an exponential moving average (EMA) of\nthe student prototype.\nTo improve the consistency of the prototypes assignment,\nwe follow a student-teacher approach similar to the segmen-\ntation model proposed in SmooSeg (Lan et al., 2023). Two\nmasks, $M^s$ and $M^t$ attributing pixels from the image to a\nprototype are computed by measuring the cosine similarity\nbetween the prototypes and the projections as follows:\n$M^{s} = cos(sg(Z); \\Phi_{s}); M^{t} = cos(Z'; sg(\\Phi_{t})) \\quad(1)$"}, {"title": "3.2. Optimization objective", "content": "The optimization objective is composed of losses promoting\na consistent and local assignment of the patches along a\njoint-classification objective. The different losses used in\nthis sense are described next.\nAssignment loss: To promote a consistent and confident\nassignment of the prototypes, we first aim to encourage the\nsame patches from two different views, with both geomet-\nric and color transformations, to be assigned to the same\nprototype with high confidence through the assignment loss\n$L_{as}$.\n$L_{as} = - \\frac{1}{I} \\sum_{i} log \\sum_{d} \\tilde{A}^s_{i,d} \\tilde{A}^t_{i,d} \\quad (7)$\nAlignment loss: Prototypes assignments are also aligned\nto the backbone through a correspondence distillation\nloss (Hamilton et al., 2022):\n$\\mathcal{L}_{al} = (F - \\tilde{F}) \\tilde{A}^t \\quad (8)$"}, {"title": "Contrastive loss:", "content": "We next leverage the prototype represen-\ntations $S^{{s,t}}$ and the projection $Q^{t}$ in a contrastive loss\n$L_{NCE}$ as initially presented in (Wen et al., 2022). This\nobjective encourages a consistent representation of a given\nprototype across views of a given image as well as minimiz-\ning the similarity with different prototype representations.\nThe contrastive loss $L_{NCE}$ is defined as:\n$L_{NCE} (S^{s}, S^{t}) = \\frac{1}{N} \\sum_{n=1}^{N} -log \\frac{ 1^{n,s}1^{n,t} exp( q_n^T \\tilde{s}_n / \\tau) }{ \\sum_{n'} 1^{n,s}1^{n',t} exp( \\tilde{s}_n \\tilde{q}_{n'}/ \\tau) } \\quad (10)$\nwhere denotes l2-normalization, $q_n$ and $s_n$ denotes respec-\ntively the entry of S and Q for prototype n and $1^{n}$ is a binary\nindicator representing prototypes which are dominant in at\nleast a single patch:\n$1^{n,s} = \\exists i \\quad such \\quad that \\quad arg \\quad max (\\tilde{A}^s) [i] = n \\quad (11)$\nand similarly for $1^{n,t}$ based on the teacher assignment At.\nSparsity loss: To prune prototypes in the classification layer\nwith low importance, we introduce a sparsity loss $L_{sp}$ based\non the Hoyer-Square (HS) regularizer (Yang et al., 2019)\napplied on the importance matrix R:\n$L_{sp} = \\alpha \\frac{||R||_1}{\\sqrt{D} ||R||_2} + (1 - \\frac{1}{\\sqrt{D}}) ||R||_2. \\quad (12)$"}, {"title": "Classification loss:", "content": "The classification loss is a simple cross-\nentropy loss between the model's prediction y and the label\nY:\n$\\mathcal{L}_{CE} = \\sum_{q} \\tilde{y}^q log Y_q \\quad (13)$\nThis loss is applied both on the student and teacher predic-\ntions $\\tilde{y}^s$ and $\\tilde{y}^t$.\nThe final objective is:\n$\\mathcal{L} = \\lambda_1 L_{as} + \\lambda_2 L_{al} + \\lambda_3 L_{NCE} + \\lambda_4 L_{sp} + \\lambda_5 L_{CE} \\quad (14)$\nwhere $\\lambda_{1,...,5}$ are hyper-parameters."}, {"title": "3.3. Benchmark for evaluation of prototypical-part models", "content": "The set of metrics used in our evaluation aims to provide\na general evaluation of the explanations provided by the\nprototypical models. This evaluation can be decomposed\ninto two main steps. We first evaluate all models with the\nmetrics from the FunnyBirds metrics which cover three\ndimensions of the Co-12 properties, namely correctness,\ncompleteness, and contrastivity. The metrics in the frame-\nwork rely on a part importance function $PI(\\cdot)$ which aims\nto reflect the importance of the different parts in the im-\nage towards the model's prediction. We follow a similar\napproach as (Op\u0142atek et al., 2024) described in more detail\nin Appendix D to adapt the PI function to prototypical\nmodels. This set of metrics allows to derive a mean explain-\nability score $m_X$ which allows us to compare the different\nmodels to different interpretability methods which are not\nnecessarily based on prototypical models.\nIn a second part, we focus on metrics specific to prototyp-\nical model. The consistency and stability score initially\ndeveloped on the CUB dataset by (Huang et al., 2023) were\nadapted to the FunnyBirds dataset to leverage the precise\npart annotations provided as part of this dataset. The con-\nsistency metrics evaluate whether the prototypes are con-\nsistently attributed to one of the five parts defined in the\nFunnybirds dataset, that is beak, eye, foot, tail and wing.\nThe stability metrics focus on measuring whether the part\nattribution of a prototype is stable when the input is per-\nturbed with noise. The idea is to evaluate whether prototype\nassignments change under perturbations which are invisible\nto human eyes. More details on these metrics are included\nin Appendix D\nTo assess the compactness of the evaluation, we utilize the\nlocal and global size metrics from (Nauta et al., 2023a). The\nlocal size quantifies the total number of prototypes a model\nuses for a prediction, while the global size represents the\nnumber of prototypes with non-zero weight in the classifi-\ncation head. The composition property is often overlooked,\nwith score sheets failing to indicate that the displayed proto-\ntypes contribute often for less than 50% of the final predic-\ntion. To address this, we introduce the Score Explained by\nComposition (SEC) metric, which we advocate for incorpo-\nrating into score sheets produced by prototypical models,\nas it measures the fraction of the total prediction explained\nby the prototypes presented in a given score sheet.\nAll the properties included in our benchmarks along the"}, {"title": "4. Experiments", "content": "4.1. Implementation details\nThe proposed architecture leverages DINOv2 ViT-B/14 with\nregisters (Oquab et al., 2024; Darcet et al., 2023) as well\nas the ViT-L from the OpenClip architecture (Cherti et al.,\n2022) for the general datasets as our backbone. In addi-\ntion, an experiment on a chest X-Ray dataset with RAD-\nDINO (P\u00e9rez-Garc\u00eda et al., 2025) was also performed to\ndemonstrate the possibility of leveraging domain specific\nVFMs. Full experimental setups including the number of\nepochs for the different experiments are described in Ap-\npendix B."}, {"title": "4.2. Classification performance", "content": "We compare the proposed approach in term of classification\nperformance to a non-explainable baseline and SOTA proto-\ntypical models. For the non-explainable baseline, we present\nresults for one of the frozen backbone, i.e. DINOv2 ViT-\nB/14, with a linear classifier reporting results from the initial\nmodel publication (Oquab et al., 2024). In addition, we con-\nsider a range of SOTA prototypical models, namely ProtoP-\nNet (Chen et al., 2019), ProtoTree (Nauta et al., 2021b), Pro-\ntoPShare (Rymarczyk et al., 2021), ProtoPool (Rymarczyk\net al., 2022), PIP-Net (Nauta et al., 2023a), ViT-Net (Kim\net al., 2022a), ST-ProtoPNet (Wang et al., 2023), PixP-\nNet (Carmichael et al., 2024b), ProtoViT (Ma et al., 2024).\nThree datasets for image classification tasks were used to\nbenchmark the model on general classification tasks. Two\nare common benchmarks: i) CUB-200-2011 (Wah et al.,"}, {"title": "4.3. Interpretability evaluation", "content": "We performed an extensive evaluation of the explana-\ntions provided by our model on the metrics described in\nSection 3.3. The model is compared with PIP-Net, ST-\nProtoPNet and ProtoVit. These models were selected be-\ncause ST-ProtoPnet and ProtoVit achieved the highest ac-\ncuracy among prototypical models in the literature, while\nPIP-Net offered the most compact explanations in terms of\nboth local and global size.\nThe metrics presented in this section aim to cover the\ndesiderata presented in Table 1. Metrics which cover the\nfirst five desiderata are summarized in the radar plot shown\nin Figure 4. The local and global sizes, which measure the\ncompactness of the explanations, are reported in Table 2 and\nall metrics are reported individually in Appendix E.\nThe evaluation performed as part of our work demonstrates"}, {"title": "4.4. Ablation studies", "content": "An ablation study to understand the effects of the different\nterms in the objective function was performed and the results\nare presented in Table 3. This study was conducted on Fun-\nnyBirds by individually removing the different terms from\nthe loss function and evaluating the model both in terms of\nclassification performance and interpretability metrics.\nWe find that the assignment loss $L_{as}$ and the alignment loss\n$L_{al}$ play a crucial role in enhancing prototype consistency.\nThe significance of $L_{al}$, which leverages the similarity of\nfeatures extracted by the backbone, further supports the de-\ncision to keep the backbone frozen\u2014not only for efficiency\nbut also for its contribution to consistency. Additionally,\nwe observe that the contrastive loss $L_{NCE}$ positively in-\nfluences prototype consistency, while the sparsity loss $L_{sp}$\ncontributes to reducing the local size of the explanation and\nhence promotes a better interpretability of the model for a\ngiven accuracy."}, {"title": "5. Conclusion", "content": "This work aimed to demonstrate that the proposed architec-\nture ProtoFM effectively adapts visual foundation models\ninto self-explainable classifiers. Through extensive evalua-\ntion, we showed that our approach not only produces models\nwith competitive classification accuracy but also surpasses\nother prototypical models in the quality of explanations pro-\nvided. While we believe this interpretability framework\nenhances understanding of the model's decision-making\nprocess, a natural next step would be incorporating textual\ndescriptions to further clarify the concepts utilized by the\nmodel."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning (ML). There are many potential so-\ncietal consequences of our work; however, we believe that\ninterpretability, which is the focus of this work, could help\nto partially mitigate some of the risks of using ML-based\nmodels in critical applications."}, {"title": "F.1. Qualitative evaluation of prototypes consistency", "content": "In addition to the quantitative metrics used to assess the quality of the explanations provided by the designed architecture,\none user study was carried out to better understand the consistency of the prototypes with respect to concepts humans\nwould associate together. The user study relies on a random selection for each prototype of 50 samples where this\nprototype plays a role toward the model's prediction. The samples used for the user study can be found in Supplementary\nMaterials Appendix A.\nA percentage of consistent sample (the most prevalent category) was calculated for each prototype (global size is 77).\nOn average, the prototypes are 86% consistent with one category (see analysis per prototype in Supplementary Materials\n(see Appendix A), with 17 prototypes being 100% consistent. Two examples are displayed in Figure 5. The model provides"}]}