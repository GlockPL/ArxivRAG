{"title": "Tempo: Helping Data Scientists and Domain Experts Collaboratively Specify Predictive Modeling Tasks", "authors": ["Venkatesh Sivaraman", "Anika Vaishampayan", "Xiaotong Li", "Brian R Buck", "Ziyong Ma", "Richard D Boyce", "Adam Perer"], "abstract": "Temporal predictive models have the potential to improve decisions in health care, public services, and other domains, yet they often fail to effectively support decision-makers. Prior literature shows that many misalignments between model behavior and decision-makers' expectations stem from issues of model specification, namely how, when, and for whom predictions are made. However, model specifications for predictive tasks are highly technical and difficult for non-data-scientist stakeholders to interpret and critique. To address this challenge we developed Tempo, an interactive system that helps data scientists and domain experts collaboratively iterate on model specifications. Using Tempo's simple yet precise temporal query language, data scientists can quickly prototype specifications with greater transparency about pre-processing choices. Moreover, domain experts can assess performance within data subgroups to validate that models behave as expected. Through three case studies, we demonstrate how Tempo helps multidisciplinary teams quickly prune infeasible specifications and identify more promising directions to explore.", "sections": [{"title": "1 Introduction", "content": "Predictive AI models aim to enhance human decision-making by inferring individuals' future outcomes, relating their past observations against vast amounts of historical data. In expert communities, ranging from medical and public health professionals to public services agencies to business analysts, predictive capabilities are increasingly expected to streamline and standardize decision processes. However, many decision support tools (DSTs) built on predictive models fail to effectively support decision-makers in practice. While in some cases these failures can be attributed to the nature of the Al's deployment, such as incompatibility with existing workflows or poor usability [26, 50, 79], negative perceptions of predictive models often stem from more fundamental concerns about their behavior. In particular, issues often arise when existing decision processes conflict with the model specification, or the precise task that the predictive model is trained to perform. Model specifications determine what, how, and for whom predictions will be made [70, 74], all of which can impact how decision-makers perceive the resulting models to behave. In high-stakes fields such as medical treatment [70, 71], child welfare [48], and housing allocation [19, 53], misaligned model specifications can lead experts to avoid using the models out of concern that they will introduce or perpetuate systemic decision-making errors. Therefore, helping data scientists and domain experts collaboratively define the right predictive modeling task is key to developing DSTs that decision-makers find relevant and useful.\nTo demonstrate how these obstacles might play out in a practical scenario (inspired by our previous research collaborations), let us consider a fictional data scientist named Ava who, along with non-technical clinical collaborator Ben, is building a model to predict whether a patient will be readmitted to a hospital in the future. Existing literature and the landscape of available tools [3, 68, 83] suggest that even without the need to incorporate domain experts' feedback, specifying a model for this task could be a challenging technical problem. In contrast to other types of models, building algorithms to predict future outcomes depends on making sense of temporal event data, which can be sparse and irregularly spaced and therefore demand extensive wrangling and pre-processing [1, 30]. For example, Ava might need to aggregate past diagnoses at varying time intervals and carry forward previous values to produce a clinically meaningful value at each hospital admission, which itself is an irregularly-spaced observation. However, existing tools for data scientists largely focus on the more general, technical problems of \"data cleaning\" [27, 47] and model selection [3, 52] without considering the temporal nature of the data. As a result, a representative"}, {"title": "2 Related Work", "content": "Our work is inspired and informed by challenges in predictive model specification that have been discussed in human-centered Al evaluations, reviewed in Sec. 2.1. To design and implement our system to address these issues, we expand on prior technical work around working with temporal event data (Sec. 2.2) and interactively evaluating models (Sec. 2.3)."}, {"title": "2.1 Predictive Model Specification for AI Decision Support", "content": "Temporal predictive modeling represents a large and long-standing class of machine learning problems that aim to forecast a future action or outcome based on prior history. Predictive modeling is currently employed in an extremely wide range of applications, including predicting sales [76], user intent [14], global climate [25], and medical treatment [71], as well as more contentious topics like child welfare [48] and criminal justice [33]. However, issues of model specification have often been a major obstacle to adoption of predictive models, especially for decision support in high-stakes domains. In the medical setting, model specification problems have been increasingly discussed in the literature as predictive model-ing has grown more sophisticated and widespread; for instance, Sherman et al. [70] showed that calculating input features retrospectively around the time of known outcomes can cause models to perform misleadingly well in evaluation. Unexpected behaviors may also arise because historical data is biased with respect to decisions that a DST is designed to influence. For example, in Caruana et al.'s pneumonia risk prediction model [13], asthma appeared to lower mortality risk because it was associated with more aggressive treatment. In sepsis, another promising disease area for DSTs, studies have pointed out the difficulty of choosing the right predictive target, given that both mortality and need for treatment may be imperfect signals for disease severity [69, 71].\nModel specifications have also been discussed in the AI fairness literature as a lens to understand how DSTs may reflect or distort human decision-maker values. For example, Kawakami et al.'s [48] study with child maltreatment screeners highlighted concerns that their DST used public support use as a proxy for mental health and substance abuse concerns, thereby unfairly penalizing families that"}, {"title": "2.2 Querying, Visualizing, and Modeling Event Data", "content": "Temporal event data is ubiquitous as a way to represent evolving real-world processes and interactions through discrete observation. Storing and querying event data is a primary objective of many database systems, including OpenTSDB [5], Timescale [75], and Prometheus [64]. Query languages such as TSQL2 [9], the Event Query Language [80], and others [10, 67] offer syntactic ways to apply temporal logic to event data, often following Allen's foundational framework of interval relations [2]. However, event queries still tend to be verbose and hard to read in commonly-used ML data wrangling tools such as the Structured Query Language (SQL), the Pandas library in Python, SAS, and the dplyr package in R. Moreover, even in systems specialized for event data, it can be challenging to write and update queries because different types of aggregations require dramatically different implementations (e.g., tumbling, hopping, sliding, and variable-length windows) [9].\nConsiderable data visualization research has focused on helping analysts interpret temporal event data, particularly in healthcare contexts [38]. Systems such as DecisionFlow [32], Frequence [63], and EventAction [24] aim to identify frequent event patterns and event sequences associated with adverse outcomes. Other systems such as VizPattern [43] and (s|qu)eries [83] have augmented the query-centric approach described above with visual representations. While these systems tend to focus on exploratory analysis and temporal relationships between events (e.g. B after A), our work addresses the more ML-focused task of helping users aggregate events at standard intervals (e.g. A occurred four times in the past hour).\nDespite the many approaches to visualize event data, tools to address the challenges of working with event data for ML modeling are comparatively scarce. For instance, systems by Kwon et al. [54] and Guo et al. [37] help users interpret sequence models, but they assume the model specification and input data are fixed. Tempo addresses this gap by including model specification through temporal queries as a central part of the interface."}, {"title": "2.3 Interactive Tools for ML Model Analysis", "content": "Supporting ML practitioners in reasoning about and improving models has been a widely-studied problem in human-computer interaction and data visualization [40]. Most closely related to our work are tools that help users navigate AutoML modeling results [60, 68] and tools to evaluate and adjust data labels based on model behavior [3, 8, 29]. In particular, Visus [68] and EXMOS [8] both allow users to configure the model specification to some degree"}, {"title": "3 Challenges in Predictive Model Specification", "content": "Data science work often centers on the inherent tension between the real world and how to model it. As Passi and Jackson describe it, data scientists must \"continuously straddle the competing demands of formal abstraction and empirical contingency\", applying their discretion and prior experience to find abstractions that will yield useful results [62]. To achieve usefulness, data scientists must not just translate broader objectives into abstract modeling problems, but also negotiate these translations with non-data-scientist stakeholders [61]. To better understand how technical systems could support this process, we review and analyze in detail four case studies in which the authors document how predictive model specifications have evolved throughout their projects. To our knowledge these are among the only papers that directly address the development of predictive modeling specifications, perhaps because it is not often explored in simulated study settings and because there is little incentive to publish nonviable specifications. Nevertheless, these four studies, drawn from communities ranging from anthropology to HCI to medicine, put data science practices in dialogue with downstream stakeholder perspectives and complicate the traditional linear narrative from identified needs to models, informing the design of our system."}, {"title": "3.1 Design Opportunities", "content": "The case studies above illustrate that problem specification for predictive modeling is a highly ad hoc process that can often yield poor results even with significant time investment and opportunities for collaboration. We propose four broad opportunities that interactive tools can address to structure this process and increase its likelihood of success:"}, {"title": "4 System Design", "content": "We introduce Tempo, a visual analytics system that facilitates collaborative iteration on predictive model specifications. Tempo aims to help teams find potential pitfalls faster by speeding up and improving transparency in three major phases of early model development: expressing a model specification in terms of aggregations on temporal data, training an initial prototype model, and evaluating model predictions. Below we first describe a usage scenario for how Tempo supports these tasks, then we detail the novel algorithmic and interactive aspects of Tempo's implementation."}, {"title": "4.1 Usage Scenario", "content": "Let us return to the scenario described in Sec. 1, in which data scientist Ava is tasked with building a hospital readmission model. Without Tempo, this task might require writing a large amount of code and making choices around what model behaviors to communicate, none of which would be easily visible to clinical collaborator Ben. Using Tempo, Ava can transform their dataset for modeling and perform subgroup analysis interactively with Ben, helping them iterate on model specifications much more quickly.\nAva's first step is to import their data into Tempo. Although some SQL might be required to download the data from a database, these queries would not require aggregation, making them much simpler to write than an entire pre-processing pipeline. Ava writes simple queries to import dozens of different types of time-stamped events, such as hospital admissions, diagnoses, and prescriptions1. These events can be irregularly spaced in time and are not guaranteed to align with admissions, which would make them challenging to work with in other languages. After importing into Tempo, Ava can use the system's query language to aggregate the diagnoses and procedures at the end times of every admission, effectively creating"}, {"title": "4.2 Temporal Query Language", "content": "At the core of what makes Tempo useful to data scientists is its novel query language, which simplifies common but complicated pre-processing tasks while also providing greater transparency for domain experts. A technical description of the syntax and how queries are computed can be found in Appendix A; here we describe at a high level how Tempo helps write and verify temporal queries.\nImporting Data into Tempo. Tempo's data format requires minimal analysis choices, so users can import as much data as possible and refine it later using queries. The system supports three basic data types: Attributes (data fields that remain constant throughout a trajectory), Events (observations from a single moment in time), and Intervals (observations that span a period of time with a start and end point). For example, in a medical setting, Attributes could include demographic features such as date of birth, ethnicity, and gender; Events could represent diagnoses and drug prescriptions; and the Intervals table could encode periods where a patient was admitted to the hospital. Any of these data types can be omitted if not applicable for a particular dataset.\nTemporal Aggregations. The primary functionality of Tempo's query language is temporal aggregation. Many temporal modeling tasks involve aggregating data at a fixed series of timesteps, sometimes called an index date: for example, counting all diagnoses for a patient within the last 30 days of each visit, or averaging the prices of the products a user has purchased prior to viewing a new item in an e-commerce setting. However, it can be challenging in existing query languages to deal with unevenly-spaced timesteps and sparse or missing events, leading to less readable code. In Tempo's syntax, aggregations take a similar form regardless of spacing and missing-ness considerations: <aggregation function> <data field> <aggregation bounds> <timestep definition>. For instance, for the diagnosis example above, we could use a count aggregation function on a {Diagnosis} data field, specify the past 30 days as the aggregation bounds, and every visit for the timestep definition. This would result in the following query:\nTempo also includes special syntax for common pre-processing routines to help create useful features for predictive modeling (ad-dressing 3.1). For example, the impute command can be used to re-place missing entries with a fixed value. Conversely, the where keyword can be used to introduce missing values while preserving time series alignment. To discretize numerical features, the cut command allows users to define a binning strategy using custom or automatic cut points.\nComparison with Existing Query Languages. In contrast to the tools most commonly used by data scientists to pre-process model features (such as SQL, Pandas, and SAS), Tempo is explicitly de-signed to make temporal aggregations simpler to write and easier to read, even for non-data scientists. To illustrate this comparison directly, Fig. 4 shows two plausible aggregations that a data scientist might need to implement for a predictive model: checking for an occurrence of a heart failure diagnosis every 30 days (Fig. 4A) and finding any prior occurrence of the diagnosis at the start of a hospital admission (Fig. 4B). In Tempo, the two queries are roughly the same length as it would take to describe them in plain English, and their differences are directly related to the conceptual differences in the period and frequency of aggregation. On the other hand, SQL and Pandas require highly verbose, multi-step operations to produce a correct result, and a substantially different approach is needed for the two queries."}, {"title": "4.3 Defining and Training Models", "content": "Together, the features of the Tempo query language described above can enable data scientists to develop a complete and succinct de-scription of the data used to develop an ML model, addressing 3.1. In Tempo's Specification Editor (Fig. 2B), four sections determine how a model's inputs and outputs will be defined:\n(1) Timestep Definition. As with the aggregation expressions described above, the model's timestep definition specifies at which time points the model will be run.\n(2) Timestep Filter. Optionally, times selected by the timestep definition can be removed if they do not match a predicate query. This can be used to define inclusion or exclusion criteria for entire trajectories or for individual timesteps.\n(3) Input Variables. The bulk of the model specification, the Input Variables consist of a query for each feature to be used as in-put to the model. Each query is evaluated against the model's Timestep Definition to ensure that the resulting Time Series are aligned. If a query returns a categorical variable, it is automatically converted to a one-hot encoding.\n(4) Target Variable. This field specifies what to attempt to predict. Like Input Variables, the target is evaluated against the"}, {"title": "4.4 Subgroup Analysis", "content": "Addressing 3.1, Tempo supports interactive subgroup discovery to help domain experts understand and interpret model specifications using the familiar concept of rule-based subpopulations. Similar to how Tempo's query language provides a simple yet computable representation for cohorts, inputs, and target variables, we envision subgroup discovery as an intuitive yet precise tool for expressing model behaviors.\nThe goal of subgroup discovery is to find subsets of a dataset, each defined by the intersection of one or more discrete features, that exhibit meaningful differences from the overall data. Subgroup discovery has been extensively explored in databases and machine learning research [20, 39, 85], suggesting it is a promising way to help domain experts make sense of model behavior. However,"}, {"title": "4.5 Implementation Details", "content": "Tempo is a web application built using a Svelte frontend\u00b3 and a Flask backend server\u2074. Users can launch Tempo locally on their own datasets by writing a lightweight JSON configuration file that points to the Attributes, Events, and Intervals files in CSV or Apache Feather format. This configuration file also allows data scientists to specify the sizes of the training, validation, and test sets, which are automatically generated upon opening the dataset for the first time. The system is open-source to enable wider use.\u2075\nInternally, the Tempo query language is parsed using Lark\u2076, and query computations are performed using Pandas with heavy use of Numba\u2077 to accelerate aggregations. Query results for individual variables are cached as columnar Apache Feather files so that they can be quickly and separately loaded from disk for model training and subgroup discovery. Tempo's queries could be translated to SQL queries in the future, allowing users to more easily work with large databases."}, {"title": "5 Case Studies", "content": "To explore how Tempo could support the development of predictive model specifications, we conducted three collaborative model-building efforts in different domains. These case studies were conducted in an exploratory manner and followed typical real-world data science practices. Through the resulting models and qualitative user feedback, we aimed to assess whether Tempo addressed our Design Opportunities (Sec. 3.1) and what new opportunities it posed.\nIn the first two case studies, we worked with the teams asynchronously to understand and address their goals, built initial models in Tempo, then engaged participants from the two teams in semi-structured interview and feedback sessions. During these sessions we gave participants access to Tempo with the initial models loaded, and we worked with them to perform a part of the model-building process that was relevant to their expertise. In the final case study, we collaborated with a group of researchers with expertise in pharmacy and biomedical informatics over a period of three months beginning after the first version of Tempo was developed. The team members with data science expertise (included as co-authors on this paper) worked with the HCI research team to build an initial model prototype in Tempo, after which we elicited feedback from two additional pharmacy experts."}, {"title": "5.1 Predicting Web Browsing Behavior", "content": "We worked with a start-up that was developing a web browser extension for tab management and had collected a large dataset of de-identified user browsing logs. They were interested in understanding whether the events they had captured could be used to develop predictive models to make proactive suggestions to users. To explore this question, we first extracted data for about 500 users during the first 72 hours that they used the browser extension, including Events for navigating to a URL domain (called Navigation,"}, {"title": "5.2 Sepsis Treatment in Intensive Care", "content": "We collaborated with four experts (U3-U6) from a large hospital system to develop predictive models for sepsis. Sepsis occurs when the body's response to an infection causes further harm, leading to organ failure and sometimes death [16]. While AI has been de-veloped to recommend treatments for patients with sepsis, prior work has shown that these systems tend to make misleading recommendations for patients with severe illness [71], indicating that better model specifications may be needed. To examine how best to predict patients' future need for treatment, we extracted a subset of roughly 14,000 patients with sepsis from MIMIC-IV, a publicly-available dataset of intensive care patient records [44]. The data imported into Tempo contained about 19.2M events of 89 different types (vital signs, lab values, physical assessments, etc.), and 1.2M intervals of 387 types (primarily drug administrations and proce-dures). We used these events to define about 300 input variables using Tempo's query language, each aggregated over 4 hours prior to each timestep.\nExcluding Uninformative Patients. We showed U3 and U4, both experts in medical data analysis, an initial model to predict whether in the next 8 hours a patient would receive vasopressors, a class of drugs used as a second-line sepsis treatment. In the Subgroups view, U3 observed that the groups with most positive predictions always included a variable for prior vasopressor use. U3 thus concluded that the model was not clinically valuable, as future predicted vaso-pressor use was highly contingent on prior treatment that clinicians would already be aware of. Accordingly, they used the Specification Editor to build a new model using a Timestep Filter to exclude patients that had received vasopressors in the preceding 8 hours. U4 further built on this model to include only timesteps in which vasopressors had never been used, yielding slightly higher accu-racy. As both their roles frequently involved extracting features for ML modeling, U3 and U4 noted that Tempo's low-code model specifications could help them perform feature selection alongside a clinician:"}, {"title": "5.3 Readmission for Home Health Patients", "content": "For the final case study, we worked with an academic team of pharmacy and biomedical informatics researchers interested in making predictions for patients in home health care. As the US population ages, the number of people with chronic diseases and other age-related maladies will increase, posing a great challenge for the healthcare system and financial support for health [56]. To cater to the preferences of older people and reduce the cost of healthcare delivery, US health insurance policies have begun to favor home"}, {"title": "6 Discussion", "content": "We presented Tempo, an interactive tool that supports the process of specifying and evaluating predictive modeling tasks. Addressing design opportunities from prior work revealing how data scientists navigate model specification in practice, Tempo incorporates a novel query language to aggregate temporal event data and interactive subgroup discovery tools to probe model behavior in-depth. These features together addressed the primary research question in this work: how we can enable data scientists and domain experts to iterate on model specifications together and find potential pitfalls faster. In our three case studies, the prototype models we created in Tempo were sufficient to surface model specification issues, in some cases revealing that the approach we were taking was very likely to fail. Tempo sparked discussion of these issues between data scientists and domain experts before we had committed to any one approach, enabling us to quickly explore other, more promising directions. Below we discuss implications and directions for future work based on the experiences of developing Tempo and using it in real-world contexts.\nEvaluating functional model prototypes surfaces diverse specification issues without the need for extensive model refinement. Because it is difficult to evaluate how Al will behave in practice while concurrently developing and improving AI capabilities [81], there are few opportunities during development for domain experts to determine whether a predictive problem is formulated appropriately. Model development teams may bring assumptions on what technical approach will yield the most useful and accurate results [77, 82], and it is difficult to test these assumptions without substantial technical effort. In contrast, in our case studies the ability to create rough prototypes of models lowered the barrier for experts to critique their specifications and quickly resolve minor issues. In cases where the expert feedback indicated that an entire approach was non-preferable (e.g. Sec. 5.3), we were able to reuse and repurpose components of the model specification for new tasks, lowering the amount of effort required to shift the project's goals.\nCentering discussions about model specification leads to divergent exploration of the modeling space. Defining the predictive task in relation to its intended use case is often the first step in normative data science processes (e.g., the CRISP-DM model [17]). However, despite its importance to the acceptability of the final model, the process of defining a predictive task is often ill-defined and requires ad hoc collaboration and iteration outside of existing tooling. As a result, discussions about model specification may often be implicit, lack common language, or be embedded into more technical discussions about data pre-processing or model architecture selection. Using Tempo, team members frequently posed speculative questions such as: What type of predictive target is most appropriate and fair for the decision-maker's task? At what time points should model predictions be computed? What variables should be used to explain its behavior? These discussions could be sparked even with very preliminary results as described in Sec. 5.3, helping to generate ideas for experiments that may not have been obvious a priori but could lead to feasible and useful predictions. Further evaluations are needed to evaluate whether this deeper exploration leads to more effective or acceptable models. Nevertheless, the discussions Tempo facilitated are a first step toward expanding the space of possible specifications and avoiding concerns such as those raised around previously-deployed DSTs [48, 61, 70].\nWhile the types of temporal predictions Tempo supports can be applied to a wide range of important use cases, our findings also suggest ways to cultivate divergent exploration of modeling tasks more generally. For example, it could be valuable for analytics systems to surface how problem formulations-that is, for whom, what, and how predictions are made-affect behavior patterns in recommender systems [58], machine learning models for causal in-ference [21], or generative models [51]. Supporting these tasks may require new techniques to express specification choices (analogous"}, {"title": "7 Conclusion", "content": "If predictive models are to be accepted and used in important task domains, experts in those fields must find them to be relevant and useful additions to their decision-making processes. While recent empirical studies have revealed challenges in identifying the right model specification, our work is among the first to explore how a technical system could address these challenges. By making model specifications and behaviors salient and readable while still being precise and computable, tools such as Tempo can help bring non-data-scientist stakeholders into the loop earlier in development and surface ways that models might conflict with their intuitions. Identifying these misalignments earlier in turn opens a wider range of options to change the model's behavior. As predictive models become increasingly ubiquitous, reflecting on model specifications can help ensure that the final models are sensible to experts and beneficial to those their predictions may impact."}, {"title": "A Tempo Query Language", "content": "Below we provide a more technical description about how Tempo's query language supports formatting data for modeling."}, {"title": "A.1 Basic Queries", "content": "In Tempo's syntax, data fields are selected by wrapping them in curly braces, and they can be operated on using arithmetic and logical operations similar to SQL. By default, operations apply to the value associated with a data field (such as a temperature measurement), while the time, starttime,and endtime functions can be used to extract the timestamps of the data field. For instance, to calculate the patient's age in years at the start of each admission, we could combine the timestamp of each Admission interval with a Birth Time attribute:\nAttributes are automatically broadcasted such that each interval time is compared with the correct attribute value for that trajectory."}, {"title": "A.2 Aggregations", "content": "While basic operations on the raw data evaluate to Attributes, Events, or Intervals (which may be unevenly-spaced), aggregations result in a Time Series that is aligned to a user-defined timestep definition, as shown in Fig. 7. The timestep definition specifies the bounds and frequency of timesteps that should be selected within each trajectory. Each aggregation is computed over the observations that fall within the provided aggregation bounds, which are typically a function of #now (which represents the time of the current timestep). Aggregation bounds make it equally straightfor-ward to define overlapping, non-overlapping, and variable-length window sizes without concern for how the aggregation will be implemented. A variety of common functions are supported to produce the aggregation result, including mean, min/max, any/all,"}, {"title": "A.3 Convenience Syntax for Temporal Feature Engineering", "content": "Because Tempo is explicitly designed to help create useful features for predictive modeling, it includes special syntax to make common pre-processing routines easier to read and modify. Combining these elements, we can perform several pre-processing operations within a single simple query. For example, the below query could generate a Time Series containing quantile-binned classifications of each patient's average body mass index (BMI) using their most recent height and weight observations, imputing the median value when missing:"}]}