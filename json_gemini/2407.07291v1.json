{"title": "Causal Discovery in Semi-Stationary Time Series", "authors": ["Shanyun Gao", "Raghavendra Addanki", "Tong Yu", "Murat Kocaoglu", "Ryan A. Rossi"], "abstract": "Discovering causal relations from observational time series without making the stationary assumption is a significant challenge. In practice, this challenge is common in many areas, such as retail sales, transportation systems, and medical science. Here, we consider this problem for a class of non-stationary time series. The structural causal model (SCM) of this type of time series, called the semi-stationary time series, exhibits that a finite number of different causal mechanisms occur sequentially and periodically across time. This model holds considerable practical utility because it can represent periodicity, including common occurrences such as seasonality and diurnal variation. We propose a constraint-based, non-parametric algorithm for discovering causal relations in this setting. The resulting algorithm, PCMCI\u03a9, can capture the alternating and recurring changes in the causal mechanisms and then identify the underlying causal graph with conditional independence (CI) tests. We show that this algorithm is sound in identifying causal relations on discrete-valued time series. We validate the algorithm with extensive experiments on continuous and discrete simulated data. We also apply our algorithm to a real-world climate dataset.", "sections": [{"title": "1 Introduction", "content": "In modern sciences, causal discovery aims to identify the collection of causal relations from observational data, as in Pearl (1980), Peters et al. (2017) and Spirtes et al. (2000). One of the most popular causal discovery approaches is the so-called constraint-based method. Constraint-based approaches assume that the probability distribution of variables is causal Markov and faithful to a directed acyclic graph called the causal graph. Given large enough data, they can then recover the corresponding Markov equivalence class by exploiting conditional independence relationships of the variables. See Peters et al. (2017). There are many constraint-based algorithms such as PC and FCI algorithms Spirtes et al. (2000). The standard assumption of these approaches is that data samples are independent and identically distributed, which makes it possible to perform CI tests. See Bergsma (2004), Zhang et al. (2012) and Shah and Peters (2020).\nRecently, there have been numerous efforts to extend such constraint-based algorithms to accommodate time series data. For instance, PCMCI in Runge et al. (2019) and LPCMCI in Gerhardus and Runge (2020) are the PC-based algorithms for time series. Inspired by FCI algorithms, approaches designed for time series include ANLSTM in Chu and Glymour (2008), tsFCI in Entner and Hoyer (2010) and SVAR-FCI in Malinsky and Spirtes (2018). This setup is relevant in several industrial applications since many data points have an associated time-point, such as root-cause analysis in Ikram et al. (2022). Most of the existing causal discovery algorithms make the stationary assumption.\nSee Chu and Glymour (2008), Hyv\u00e4rinen et al. (2010), Entner and Hoyer (2010), Peters et al. (2013), Malinsky and Spirtes (2018), Runge et al. (2019), Pamfil et al. (2020)and Assaad et al. (2022).\nNon-stationary temporal data makes causal discovery more challenging since the statistics are time-variant, and it is unreasonable to expect that the underlying causal structure is time-invariant. Identifying causal relations from non-stationary time series without imposing any restriction on the data is difficult. Here, we focus on a specific class of non-stationary time series, called the semi-stationary time series, whose structural causal model (SCM) exhibits that a finite number of different causal mechanisms occur sequentially and periodically across time. One example is illustrated in Fig.1, where the time series X\u00b9 has three different causal mechanisms across time, shown as red edges, green edges, and blue edges, respectively. Similarly, time series X2 has two alternative causal mechanisms. This setting holds considerable practical utility. Periodic nature is commonly observed in many real-world time series data. See Han et al. (2002), Nakamura et al. (2003), Carskadon et al. (2005) and Komarzynski et al. (2018). Here are a few additional intuitive examples: poor traffic conditions often coincide with commute time and weekends; household electricity consumption typically follows a pattern of being higher at night and lower during the daytime. Consequently, it is reasonable to expect periodic changes in the causal relations underlying this type of time series without assuming stationarity. Here, the constraint-based methods in Chu and Glymour (2008), Entner and Hoyer (2010), Malinsky and Spirtes (2018) and Runge et al. (2019), designed for stationary time series, may fail. Given observational data with periodically changing causal structures, it is hard to apply CI tests directly. Most of the other algorithms designed for non-stationary time series rely heavily on model assumptions, as in Gong et al. (2015), Pamfil et al. (2020), and Huang et al. (2019). These algorithms are discussed further in the related work.\nIn this paper, we propose an algorithm to address this problem, namely non-parametric causal discovery in time series data with semi-stationary SCMs. The key contributions of our work are:\n\u2022 We develop an algorithm to discover the causal structure from semi-stationary time series data where the underlying causal structures change periodically. Our algorithm systematically uses the PCMCI algorithm proposed for the stationary setting in Runge et al. (2019). The resulting algorithm is hence named PCMCI\u03a9 where \u03a9 denotes periodicity.\n\u2022 We validate our method with synthetic simulations on both continuous-valued and discrete-valued time series, showing that our method can correctly learn the periodicity and causal mechanism of the synthetic time series.\n\u2022 We utilize our method in a real-world climate application. The result reveals the potential existence of periodicity in those time series, and the stationary assumption made by previous works could be relaxed in some practical situations."}, {"title": "1.1 Related Work", "content": "PCMCI has been applied in diverse domains to investigate atmospheric interactions in the biosphere, as demonstrated in Krich et al. (2020), global wildfires as explored in Qu et al. (2021), water usage as studied in Zou et al. (2022), ultra-processed food manufacturing as examined in Menegozzo et al. (2020), and causal feature selection as discussed in Peterson (2022), among other applications. See Arvind et al. (2021); Gerhardus and Runge (2020); Castri et al. (2022, 2023). While PCMCI has achieved considerable success, it is not without its limitations. One notable assumption that can be challenged is the concept of causal stationary, that is, causal relations are time-invariant. PCMCI exhibits robustness when applied to linear models with an added non-stationary trend. See also Runge et al. (2019). However, there is an ongoing exploration to enhance its performance in a wider range of non-stationary settings.\nAlthough not as extensively as the stationary case, causal discovery in non-stationary time series has been studied by some authors. However, many of those algorithms rely on parametric assumptions such as the vector autoregressive model in Gong et al. (2015) and Malinsky and Spirtes (2019); linear and nonlinear state-space model in Huang et al. (2019). One non-parametric algorithm in the literature is CD-NOD proposed by Huang et al. (2020), which has been extended to recover time-varying instantaneous and lagged causal relationships. In very recent work, Fujiwara et al. (2023) proposed an algorithm JIT-LINGAM to obtain a local approximated linear causal model combining algorithm LINGAM and JIT framework for non-linear and non-stationary data. To the best of our knowledge,"}, {"title": "2 PCMCI\u0186: Capturing Periodicity of the Causal Structure", "content": "In this section, we formulate the problem of learning the causal graph on multi-variate time series data when the SCM exhibits periodicity in causal mechanisms. In section 2.1, we present the necessary definitions and provide an overview of the problem setting. In section 2.2, we introduce the required assumption."}, {"title": "2.1 Preliminaries", "content": "Let G(V, E) denote the underlying causal graph, and for each variable X \u2208 V, we denote the set of all incoming neighbors as parents, denoted by Pa(X).\nFor any two variables X, Y \u2208 V and S \u2282 V, we denote the CI relation X is independent of Y conditioned on S, by X \u22a5 Y | S.\nFor simplicity's sake, define sets: [b] := {1, 2, ..., b} and [a, b] := {a, a + 1, ..., b}, where a, b \u2208 N.\nIn the time series setting, let X \u2208 R\u00b9 denote the variable of jth time series at time t, Xj = {Xj}t\u2208[T] \u2208 RT denote a univariate time series and Xt = {Xj}j\u2208[n] \u2208 Rn denote a slice of all variables at time point t. V = {Xj}j\u2208[n] = {Xt}t\u2208[T] \u2208 Rn\u00d7T denotes a n-variate time series. By default, we assume n > 1 and hence X \u2286 V, and p(V) \u2260 0, where p(.) denotes the probability or probability density.\nDefinition 2.1 (Non-Stationary SCM). A Non-Stationary Structural Causal Model (SCM) is a tuple M = (V, F, E, P) where there exists a Tmax \u2208 N+, defined as:\n\\(T_{max} := arg \\max{\\tau : X_{t-\\tau} \\in Pa(X_{t}^{j}), i, j \\in [n]}\\),\n(1)"}, {"title": "2.2 Assumptions for PC\u039c\u0395\u0399\u03a9", "content": "A1. Sufficiency: There are no unobserved confounders.\nA2. Causal Markov Condition: Each variable X is independent of all its non-descendants, given its parents Pa(X) in G.\nA3. Faithfulness Condition (Pearl (1980)): Let P be a probability distribution generated by G. (G, P) satisfies the Faithfulness Condition if and only if every conditional independence relation true in P is entailed by the Causal Markov Condition applied to G.\nA4. No Contemporaneous Causal Effects: Edges between variables at the same time are not allowed.\nA5. Temporal Priority: Causal relations that point from the future to the past are not permitted.\nA6. Hard Mechanism Change:\nIf at time points t\u2081 and t2, the causal mechanisms of X\u2081 and X are different, then their corresponding parent sets can not be transformed to each other by time shifts:\n\\(f_{j,t_1} \\neq f_{j,t_2} \\rightarrow Pa(X^{j}_{t_2}) \\neq \\{X^{i}_{t + (t_2-t_1)}: X^{i}_{t} \\in Pa(X^{j}_{t_1}), i \\in [n]\\}.\nA7. Irreducible and Aperiodic Markov Chain: The Markov chains {Zn} of V are assumed to be irreducible (Serfozo (2009)): for all states i and j of {Zn}, \u2203n so that\n\\(P_{i,j}^{(n)} := p(Z_{n+1} = j|Z_{n} = i) > 0\\)\n(12)\nand aperiodic(Karlin (2014)): for every state i of {Zn}, d(i) = 1, where the period d(i) of the state i is the greatest common divisor of all integers n for which \\(p_{i,i}^{(n)} > 0\\).\nAssumptions A1-A5 are conventional and commonly employed in causal discovery methods for time series data. On the other hand, our approach requires additional Assumptions A6-A7 to be in place. To clarify, A6 is essential because our method may encounter challenges in distinguishing distinct causal mechanisms for variables in {X}n\u2208[T] if they share identical parent sets after time shifts. As for A7, it serves a crucial role in establishing the soundness of our algorithm."}, {"title": "3 PCMCI Algorithm", "content": "In this section, we propose an algorithm called PCMCI\u03a9, and in section 3.1, we present a theorem demonstrating the soundness of PCMCI and its ability to recover the causal graph. Our algorithm PCMCI builds on the Algorithm PCMCI in Runge et al. (2019). Additional details about PCMCI are provided in the supplementary material.\nOverview of Algorithm 1 PCMCI\u03a9. We assume that the periodicity and time lag are upper bounded by Wub and Tub respectively. Using PCMCI Runge et al. (2019), we obtain a superset of parents for every variable X denoted by SPa(X) (line 2). Our goal is to identify the correct set of parents along with its periodicity for every variable in V. For a variable X\u00b9, we guess its periodicity w by iterating over all possible values in [wub]. Next, we construct time partition subsets \u03a0, k \u2208 [\u03c9] based on the guess of periodicity w. In each time partition subset, we maintain a parent set, denoted by Paw (X\u00b9), initializing it with the superset SPa(X). Then we test the causal relations between X\u00b9 \u2208 Paw (X\u00b9) and X using a CI test on the sample t \u2208 \u03a0 (lines 6-10).\nFor each guess w, every variable in X\u00b9 should have its estimated parent set (line 9), and there are total w potentially different parent set index pInd\u2208[\u03c9] in X\u00b3. We return an estimate \u0175j that maximizes the sparsity of the causal graph (Lemma 3.4). Therefore, we select the value of w \u2208 [wub] that minimizes the maximum value of |Paw(X\u00b9)|, t \u2208 [T] as the estimator of \u03c9j (line 12)."}, {"title": "3.1 Theoretical Guarantees", "content": "Our main theorem shows that PCMCI recovers the true causal graph on discrete data. There are three important lemmas. We provide all the detailed proof in the supplementary material.\nTheorem 3.1. Let \u011c be the estimated graph using the Algorithm PCMCI\u03a9. Under assumptions A1-A7 and with an oracle (infinite sample size limit), we have that:\n\\(G = \\hat{G}\\)\n(13)\nalmost surely.\nLemma 3.2 and Lemma 3.3 jointly state that if CI tests are conducted on samples generated by different causal mechanisms, the obtained parent sets SPa(X\u00b9) should be the superset of the union of"}, {"title": "4 Experiments", "content": "To validate the correctness and effectiveness of our algorithm, we perform a series of experiments. The Python code is provided at https://github.com/CausalML-Lab/PCMCI-Omega. In this section, we test four algorithms\u00b9, PCMCI\u03a9, PCMCI Runge et al. (2019), VARLINGAM Hyv\u00e4rinen et al. (2010) and DYNOTEARS Pamfil et al. (2020), on continuous-valued time series with Gaussian noise. The experiments for continuous-valued time series with exponential noise and binary-valued time series are in the supplementary material.\nFollowing Runge et al. (2019), we generate the continuous-valued time series in three steps:\n1. Construct an n-variate time series V with length T using independent and identical (Standard Gaussian or Exponential) noise temporarily. Determine Tmax and Wmax where Wmax = max{\u03c9j}j\u2208{n}. After making sure that one univariate time series, say X\u00b9, has periodicity Wmax, the periodicity of the remaining time series X\u00b9, i \u2260 j is randomly selected from {1, ..., max} respectively.\n2. Randomly generate \u03c9j binary edge matrices with shape (n, Tmax) for each time series X\u00b9, j \u2208 [n]. 1 denotes an edge and 0 denotes no edge. Each binary matrix represent one parent set index pInd\u00b2, k \u2208 [\u03c9j]. Randomly generate \u03c9j coefficient matrices with shape (n, Tmax) for each time series X\u00b9, j \u2208 [n]. One binary edge matrix and one coefficient matrix jointly determine one causal mechanism. Hence, total \u03c9j causal mechanisms are constructed. Here, make sure that V satisfies Assumption A6."}, {"title": "4.2 Case Study", "content": "Here, we construct an experiment with a real-world climate time series dataset. In Runge et al. (2019), the authors tested dependencies among monthly surface pressure anomalies in the West Pacific and surface air temperature anomalies in the Central Pacific, East Pacific, and tropical Atlantic from 1948 to 2012. Our application explores the causal relations among the monthly mean of the same set of variables from 1948-2022 with 900 months. Let XWP denote the monthly mean of surface pressure in the West Pacific, XCP, XEP and XTA denote the monthly mean of air temperature in the Central Pacific, East Pacific, and tropical Atlantic, respectively.\nThe parent sets for each variable obtained from PCMCI\u03a9 algorithms are shown in Table 1. Sets of true and illusory parents of a variable at time t are separated by curly braces. For instance, variable XWP with \u03c9p = 1 means that the causal mechanism of the surface pressure in the West Pacific remains invariant over time with the estimated parent set {XCP-1, XEP-1, XTA-1, XWP-1}. Only time series XCP has three different parent sets, including one true parent set and two illusory parent sets, which appear periodically over time. The three parent sets of XCP imply that the causal effect from the tropical Atlantic air temperature XTA-1 to the Central Pacific air temperature XCP would disappear every quarter of a year. Note that we do not have a ground truth in this case, and we do not possess the necessary knowledge in this area, so the significance of these results is under-explored. More discussion about this application can be found in the supplementary materials."}, {"title": "5 Conclusions", "content": "In this paper, we propose a non-parametric, constraint-based causal discovery algorithm PCMCI\u03a9 designed for semi-stationary time-series data, in which a finite number of causal mechanisms are repeated periodically. We establish the soundness of our algorithm and assess its effectiveness on continuous-valued and discrete-valued time series data. The algorithm PCMCI\u03a9 has the capacity to reveal the existence of periodicity of causal mechanisms in real-world datasets."}, {"title": "6 Acknowledgements", "content": "This research has been supported in part by NSF CAREER 2239375 and Adobe Research. We wish to convey our heartfelt gratitude to the anonymous reviewers for their invaluable and constructive feedback, which significantly contributed to enhancing the quality of the manuscript."}, {"title": "A PCMCI Algorithm", "content": "The PCMCI algorithm is proposed by Runge et al. (2019), aiming to detect time-lagged causal relations in a window causal graph. There are two stages of PCMCI: the condition-selection stage and the causal discovery stage. In the first stage, unnecessary edges are removed based on the conditional independencies from an initialized partially connected graph where Assumption A4-A5 should be satisfied. In the second stage, Momentary Conditional Independence tests (MCI) are used to further remove the false positive edges caused by autocorrelations in time series data. More specifically, these two steps can be briefly formalized as follows:\n\u2022 PC1 in Algorithm A1: Condition selection stage. PC\u2081 is a variant of the skeleton-discovery part of the PC algorithm in a more robust version named stable-PC Le et al. (2016). The goal in this stage is to obtain a superset of the parents Pa(X) for all variables Xt\u2208[Tmax+1,T], XJ\u2208[n] \u2208 V. Initialize Pa(X) = {X1}ie[n], [max]. Pa(X) will remove X if\n\\(X^{i}_{t-\\tau} \\perp X^{j}_{t} | Pa(X^{j}_{t}) \\setminus {X^{i}_{t-\\tau}}\\)\n(17)\n\u2022 MCI in Algorithm A2: Causal discovery stage. In this stage, do MCI tests for all variable pairs (X\u2081\u208b\u03c4, X\u2081) with i, j \u2208 [n] and time delays \u03c4 \u2208 [\u03c4max]:\nMCI(X\u2081\u208b\u03c4, X\u2081|Pa(X\u2081\u208b\u03c4)\\{X\u2081\u208b\u03c4}, Pa(X\u2081))\n(18)\nwhere Pa(X\u2081\u208b\u03c4) and Pa(X\u2081) are estimated from the PC\u2081 stage.\nNote that \u03c4max in this section is the same as Tub in the main paper, serving as the upper bound for the time lag that exhibits causal effects. On the other hand, Tmax in the main paper denotes the maximum time lag observed within the multivariate time series. Essentially, in the main paper, Tub is a parameter that must be fed into the algorithm, and Tmax is observed from the true causal graph. As a default, we assume Tub is configured with a value greater than Tmax, ensuring that the algorithm uncovers the correct causal relations. See Fig.4 for more detail."}, {"title": "B PCMCI", "content": "For simplicity's sake, define sets: [b] := {1, 2, ..., b} and [a, b] := {a, a + 1, ..., b}, where a, b \u2208 N."}, {"title": "C Soundness of PCMCI\u03a9", "content": ""}, {"title": "C.1 Stationary Markov Chain", "content": "Claim: Any discrete-valued time series V with Semi-Stationary Structural Causal Model (SCM) satisfying assumption A1, A2, A4, A5 can be written as a Markov chain {Zn} as long as this Markov chain satisfies Pa(Zn) C Zn U Zn\u22121 for all n, where Zn is a set of variables in V. This Markov chain has a finite number of states if all time series in V are discrete-valued time series.\nNote that when the notation n is related to a Markov chain Zn, it means the running index. In the context of X\u2081\u2208[n], n represents the index of component time series within the n-variate time series.\nTo simplify, assume that one associated Markov chain of V = {X,Y} has Zn = {Xt, Yt, Xt\u22121, Yt\u22121} with t \u2208 {t \u2208 N\u207a :,t < T} satisfying Pa(Zn) C Zn U Zn\u22121. Here, the notation for the time points of variables is simplified as t and t \u2013 1, even though it should be a function of n, the running index of the Markov chain. Note that Zn\u22121 = {Xt\u22122, Yt-2, Xt\u22123, Yt-3} rather than {Xt\u22121, Yt\u22121, Xt\u22122, Yt\u22122}, as the simplified notation could erroneously suggest the latter sequence. A simple proof is shown below through Markov assumption (A2)."}, {"title": "E Turning Points", "content": "Given infinite samples, our estimate w (line 17 in Algorithm B1) is the exact value wj (see Lemma D.5). However, for finite samples, estimating wj by the equation in line 17 in Algorithm B1 does"}]}