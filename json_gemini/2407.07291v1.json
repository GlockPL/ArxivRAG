{"title": "Causal Discovery in Semi-Stationary Time Series", "authors": ["Shanyun Gao", "Raghavendra Addanki", "Tong Yu", "Murat Kocaoglu", "Ryan A. Rossi"], "abstract": "Discovering causal relations from observational time series without making the stationary assumption is a significant challenge. In practice, this challenge is common in many areas, such as retail sales, transportation systems, and medical science. Here, we consider this problem for a class of non-stationary time series. The structural causal model (SCM) of this type of time series, called the semi-stationary time series, exhibits that a finite number of different causal mechanisms occur sequentially and periodically across time. This model holds considerable practical utility because it can represent periodicity, including common occurrences such as seasonality and diurnal variation. We propose a constraint-based, non-parametric algorithm for discovering causal relations in this setting. The resulting algorithm, PCMCI\u03a9, can capture the alternating and recurring changes in the causal mechanisms and then identify the underlying causal graph with conditional independence (CI) tests. We show that this algorithm is sound in identifying causal relations on discrete-valued time series. We validate the algorithm with extensive experiments on continuous and discrete simulated data. We also apply our algorithm to a real-world climate dataset.", "sections": [{"title": "1 Introduction", "content": "In modern sciences, causal discovery aims to identify the collection of causal relations from observational data, as in Pearl (1980), Peters et al. (2017) and Spirtes et al. (2000). One of the most popular causal discovery approaches is the so-called constraint-based method. Constraint-based approaches assume that the probability distribution of variables is causal Markov and faithful to a directed acyclic graph called the causal graph. Given large enough data, they can then recover the corresponding Markov equivalence class by exploiting conditional independence relationships of the variables. See Peters et al. (2017). There are many constraint-based algorithms such as PC and FCI algorithms Spirtes et al. (2000). The standard assumption of these approaches is that data samples are independent and identically distributed, which makes it possible to perform CI tests. See Bergsma (2004), Zhang et al. (2012) and Shah and Peters (2020).\nRecently, there have been numerous efforts to extend such constraint-based algorithms to accommodate time series data. For instance, PCMCI in Runge et al. (2019) and LPCMCI in Gerhardus and Runge (2020) are the PC-based algorithms for time series. Inspired by FCI algorithms, approaches designed for time series include ANLSTM in Chu and Glymour (2008), tsFCI in Entner and Hoyer (2010) and SVAR-FCI in Malinsky and Spirtes (2018). This setup is relevant in several industrial applications since many data points have an associated time-point, such as root-cause analysis in Ikram et al. (2022). Most of the existing causal discovery algorithms make the stationary assumption.\nSee Chu and Glymour (2008), Hyv\u00e4rinen et al. (2010), Entner and Hoyer (2010), Peters et al. (2013), Malinsky and Spirtes (2018), Runge et al. (2019), Pamfil et al. (2020)and Assaad et al. (2022).\nNon-stationary temporal data makes causal discovery more challenging since the statistics are time-variant, and it is unreasonable to expect that the underlying causal structure is time-invariant. Identifying causal relations from non-stationary time series without imposing any restriction on the data is difficult. Here, we focus on a specific class of non-stationary time series, called the semi-stationary time series, whose structural causal model (SCM) exhibits that a finite number of different causal mechanisms occur sequentially and periodically across time. One example is illustrated in Fig.1, where the time series X\u00b9 has three different causal mechanisms across time, shown as red edges, green edges, and blue edges, respectively. Similarly, time series X2 has two alternative causal mechanisms. This setting holds considerable practical utility. Periodic nature is commonly observed in many real-world time series data. See Han et al. (2002), Nakamura et al. (2003), Carskadon et al. (2005) and Komarzynski et al. (2018). Here are a few additional intuitive examples: poor traffic conditions often coincide with commute time and weekends; household electricity consumption typically follows a pattern of being higher at night and lower during the daytime. Consequently, it is reasonable to expect periodic changes in the causal relations underlying this type of time series without assuming stationarity. Here, the constraint-based methods in Chu and Glymour (2008), Entner and Hoyer (2010), Malinsky and Spirtes (2018) and Runge et al. (2019), designed for stationary time series, may fail. Given observational data with periodically changing causal structures, it is hard to apply CI tests directly. Most of the other algorithms designed for non-stationary time series rely heavily on model assumptions, as in Gong et al. (2015), Pamfil et al. (2020), and Huang et al. (2019). These algorithms are discussed further in the related work.\nIn this paper, we propose an algorithm to address this problem, namely non-parametric causal discovery in time series data with semi-stationary SCMs. The key contributions of our work are:\n\u2022 We develop an algorithm to discover the causal structure from semi-stationary time series data where the underlying causal structures change periodically. Our algorithm systematically uses the PCMCI algorithm proposed for the stationary setting in Runge et al. (2019). The resulting algorithm is hence named PCMCI\u2229 where \u03a9 denotes periodicity.\n\u2022 We validate our method with synthetic simulations on both continuous-valued and discrete-valued time series, showing that our method can correctly learn the periodicity and causal mechanism of the synthetic time series.\n\u2022 We utilize our method in a real-world climate application. The result reveals the potential existence of periodicity in those time series, and the stationary assumption made by previous works could be relaxed in some practical situations."}, {"title": "1.1 Related Work", "content": "PCMCI has been applied in diverse domains to investigate atmospheric interactions in the biosphere, as demonstrated in Krich et al. (2020), global wildfires as explored in Qu et al. (2021), water usage as studied in Zou et al. (2022), ultra-processed food manufacturing as examined in Menegozzo et al. (2020), and causal feature selection as discussed in Peterson (2022), among other applications. See Arvind et al. (2021); Gerhardus and Runge (2020); Castri et al. (2022, 2023). While PCMCI has achieved considerable success, it is not without its limitations. One notable assumption that can be challenged is the concept of causal stationary, that is, causal relations are time-invariant. PCMCI exhibits robustness when applied to linear models with an added non-stationary trend. See also Runge et al. (2019). However, there is an ongoing exploration to enhance its performance in a wider range of non-stationary settings.\nAlthough not as extensively as the stationary case, causal discovery in non-stationary time series has been studied by some authors. However, many of those algorithms rely on parametric assumptions such as the vector autoregressive model in Gong et al. (2015) and Malinsky and Spirtes (2019); linear and nonlinear state-space model in Huang et al. (2019). One non-parametric algorithm in the literature is CD-NOD proposed by Huang et al. (2020), which has been extended to recover time-varying instantaneous and lagged causal relationships. In very recent work, Fujiwara et al. (2023) proposed an algorithm JIT-LINGAM to obtain a local approximated linear causal model combining algorithm LINGAM and JIT framework for non-linear and non-stationary data. To the best of our knowledge,"}, {"title": "2 PCMCI\u0186: Capturing Periodicity of the Causal Structure", "content": "In this section, we formulate the problem of learning the causal graph on multi-variate time series data when the SCM exhibits periodicity in causal mechanisms. In section 2.1, we present the necessary definitions and provide an overview of the problem setting. In section 2.2, we introduce the required assumption."}, {"title": "2.1 Preliminaries", "content": "Let G(V, E) denote the underlying causal graph, and for each variable $X \\in V$, we denote the set of all incoming neighbors as parents, denoted by Pa(X).\nFor any two variables X, Y \u2208 V and S \u2282 V, we denote the CI relation X is independent of Y conditioned on S, by $X \\amalg Y \\mid S$.\nFor simplicity's sake, define sets: $[b] := \\{1, 2, ..., b\\}$ and $[a, b] := \\{a, a + 1, ..., b\\}$, where a, b \u2208 N.\nIn the time series setting, let $X_t^j \\in \\mathbb{R}^1$ denote the variable of jth time series at time t, $X^j = \\{X_t^j\\}_{t\\in[T]} \\in \\mathbb{R}^T$ denote a univariate time series and $X_t = \\{X_t^j\\}_{j\\in[n]} \\in \\mathbb{R}^n$ denote a slice of all variables at time point t. $V = \\{X^j\\}_{j\\in[n]} = \\{X_t\\}_{t\\in[T]} \\in \\mathbb{R}^{n \\times T}$ denotes a n-variate time series. By default, we assume n > 1 and hence X \u2286 V, and $p(V) \\neq 0$, where p(.) denotes the probability or probability density.\nDefinition 2.1 (Non-Stationary SCM). A Non-Stationary Structural Causal Model (SCM) is a tuple M = (V, F, E, P) where there exists a $T_{max} \\in \\mathbb{N}^+$, defined as:\n$T_{max} := arg \\underset{\\tau}{max} \\{\\tau : X_{t-\\tau}^i \\in Pa(X_t^j), i, j \\in [n]\\},$ (1)"}, {"title": "2.2 Assumptions for PC\u039c\u0395\u0399\u03a9", "content": "A1. Sufficiency: There are no unobserved confounders.\nA2. Causal Markov Condition: Each variable X is independent of all its non-descendants, given its parents Pa(X) in G.\nA3. Faithfulness Condition (Pearl (1980)): Let P be a probability distribution generated by G. (G, P) satisfies the Faithfulness Condition if and only if every conditional independence relation true in P is entailed by the Causal Markov Condition applied to G.\nA4. No Contemporaneous Causal Effects: Edges between variables at the same time are not allowed.\nA5. Temporal Priority: Causal relations that point from the future to the past are not permitted.\nA6. Hard Mechanism Change:\nIf at time points t\u2081 and t\u2082, the causal mechanisms of $X_t^j$ and $X_{t'}^{j'}$ are different, then their corresponding parent sets can not be transformed to each other by time shifts:\n$f_{j,t_1} \\neq f_{j,t_2} \\rightarrow Pa(X_{t_1}^j) \\neq \\{X_{t + (t_2 - t_1)}^i: X_t^i \\in Pa(X_{t_1}^j), i \\in [n]\\}.\nA7. Irreducible and Aperiodic Markov Chain: The Markov chains $\\{Z_n\\}$ of V are assumed to be irreducible (Serfozo (2009)): for all states i and j of $\\{Z_n\\}$, \u2203n so that\n$P_{ij}^{(n)} := p(Z_{n+1} = j|Z_1 = i) > 0$ (12)\nand aperiodic(Karlin (2014)): for every state i of $\\{Z_n\\}$, d(i) = 1, where the period d(i) of the state i is the greatest common divisor of all integers n for which $p_{ii}^{(n)} > 0$.\nAssumptions A1-A5 are conventional and commonly employed in causal discovery methods for time series data. On the other hand, our approach requires additional Assumptions A6-A7 to be in place. To clarify, A6 is essential because our method may encounter challenges in distinguishing distinct causal mechanisms for variables in $\\{X_t\\}_{t\\in[T]}$ if they share identical parent sets after time shifts. As for A7, it serves a crucial role in establishing the soundness of our algorithm."}, {"title": "3 PCMCI Algorithm", "content": "In this section, we propose an algorithm called PCMCI\u03a9, and in section 3.1, we present a theorem demonstrating the soundness of PCMCI and its ability to recover the causal graph. Our algorithm PCMCI builds on the Algorithm PCMCI in Runge et al. (2019). Additional details about PCMCI are provided in the supplementary material.\nOverview of Algorithm 1 PCMCI\u03a9. We assume that the periodicity and time lag are upper bounded by Wub and Tub respectively. Using PCMCI Runge et al. (2019), we obtain a superset of parents for every variable X denoted by SPa(X) (line 2). Our goal is to identify the correct set of parents along with its periodicity for every variable in V. For a variable X\u1ec9, we guess its periodicity w by iterating over all possible values in [wub]. Next, we construct time partition subsets \u03a0, k \u2208 [\u03c9] based on the guess of periodicity w. In each time partition subset, we maintain a parent set, denoted by Paw (Xi), initializing it with the superset SPa(X). Then we test the causal relations between $X_t^i \\amalg X_{t'}^{j'} \\mid Pa^\\omega(X_t^i)$, using a CI test on the sample t \u2208 \u03a0 (lines 6-10).\nFor each guess w, every variable in X\u00b9 should have its estimated parent set (line 9), and there are total w potentially different parent set index $pInd_{k \\in [\\omega]}$ in X3. We return an estimate \u0175j that maximizes the sparsity of the causal graph (Lemma 3.4). Therefore, we select the value of w \u2208 [wub] that minimizes the maximum value of $\\mid Pa^{\\omega}(X_t^i)\\mid, t \\in [T]$ as the estimator of wj (line 12)."}, {"title": "3.1 Theoretical Guarantees", "content": "Our main theorem shows that PCMCI recovers the true causal graph on discrete data. There are three important lemmas. We provide all the detailed proof in the supplementary material.\nTheorem 3.1. Let \u011c be the estimated graph using the Algorithm PCMCI\u03a9. Under assumptions A1-A7 and with an oracle (infinite sample size limit), we have that:\n$G = \\hat{G}$ (13)\nalmost surely.\nLemma 3.2 and Lemma 3.3 jointly state that if CI tests are conducted on samples generated by different causal mechanisms, the obtained parent sets SPa(X) should be the superset of the union of"}, {"title": "4 Experiments", "content": "To validate the correctness and effectiveness of our algorithm, we perform a series of experiments. The Python code is provided at https://github.com/CausalML-Lab/PCMCI-Omega. In this section, we test four algorithms\u00b9, PCMCI\u03a9, PCMCI Runge et al. (2019), VARLINGAM Hyv\u00e4rinen et al. (2010) and DYNOTEARS Pamfil et al. (2020), on continuous-valued time series with Gaussian noise. The experiments for continuous-valued time series with exponential noise and binary-valued time series are in the supplementary material.\nFollowing Runge et al. (2019), we generate the continuous-valued time series in three steps:\n1. Construct an n-variate time series V with length T using independent and identical (Standard Gaussian or Exponential) noise temporarily. Determine Tmax and Wmax where Wmax = max{$\\omega_j$}$_{j\\in{n}}$. After making sure that one univariate time series, say X\u1ec9, has periodicity Wmax, the periodicity of the remaining time series $X^i$, $i \\neq j$ is randomly selected from $\\{1,..., max\\}$ respectively.\n2. Randomly generate wj binary edge matrices with shape (n, Tmax) for each time series $X^i$, $j\\in [n]$. 1 denotes an edge and 0 denotes no edge. Each binary matrix represent one parent set index $pInd^i$, $k\\in [\\omega_j]$. Randomly generate wj coefficient matrices with shape (n, Tmax) for each time series $X^i$, $j\\in [n]$. One binary edge matrix and one coefficient matrix jointly determine one causal mechanism. Hence, total wj causal mechanisms are constructed. Here, make sure that V satisfies Assumption A6.\n1We did not conduct experiments on JIT-LiNGAM because this is from a very recent paper Fujiwara et al. (2023) and is considered concurrent per NeurIPS policy."}, {"title": "4.1 Experiments on Continuous-valued Time Series", "content": "A correct estimator \u0175 is the prerequisite for obtaining the correct causal graph. Fig.2(a)\nshows the accuracy rate of \u0175 for different time lengths T. Here, elements in $\\{N\\omega_j\\}$ where $N\\in [\\omega_{ub}]$\nare all treated as correct estimations. By Definition 2.2 and 2.3, the multiple of wj is still associated with a correct causal graph. However, it leads to a finer time point partition I\u00b3, decreasing the sample size used in each CI test from approximately T/wj to approximately T/(Nwj). The accuracy rate is sensitive to @max for small T. This result verifies that algorithm PCMCI\u2229 has the capacity to detect the true periodicity of each $X \\in V$ with a large enough time length.\nWe evaluate the performances of PCMCI\u2229 on continuous-valued time series with Gaussian noise shown as Fig.3(a). As T increases, it is natural to see a continuous improvement in performance.\nThe sub-figures show that all three evaluation metrics decrease when @max gets larger. The precision of PCMCI\u03a9 is always far better than other algorithms when @max is not equal to 1. Given the fact that the parent sets $Pa^{\\omega}(X_t^i), \\forall j, t$ obtained from PCMCI\u2229 are subsets of the parent set $SPa(X_t^i), \\forall j, t$\nestimated from PCMCI, the recall rate of PCMCI should be the upper bound of the recall rate of PCMCI. This assertion has been verified as the red recall line of PCMCI\u2229 is always below the blue recall line of PCMCI as T increases.\nIn Fig.3(a), the recall of PCMCI\u03a9 is worse than PCMCI for T = 500. In this regime, the accuracy rate of \u0175 is low, shown as the dark blue line in Fig.2(a). Small sample sizes in CI tests may result in a sparser causal graph. Hence the number of true positive edges may decrease. This is a common problem for many constraint-based algorithms, but it hurts PCMCI\u2229 the most because in PCMCI\u03a9, the sample sizes in each CI test are approximate T/\u0175 instead of T. As T increases, the red recall line of PCMCI push forward to the blue recall line of PCMCI. The high value of both adjacent precision and recall rate with large T verify that PCMCI\u03a9 can identify the correct causal graph."}, {"title": "4.2 Case Study", "content": "Here, we construct an experiment with a real-world climate time series dataset. In Runge et al. (2019), the authors tested dependencies among monthly surface pressure anomalies in the West Pacific and surface air temperature anomalies in the Central Pacific, East Pacific, and tropical Atlantic from 1948 to 2012. Our application explores the causal relations among the monthly mean of the same set of variables from 1948-2022 with 900 months. Let $X_{WP}^t$ denote the monthly mean of surface pressure in the West Pacific, $X_{CP}^t$, $X_{EP}^t$ and $X_{TA}^t$ denote the monthly mean of air temperature in the Central Pacific, East Pacific, and tropical Atlantic, respectively.\nThe parent sets for each variable obtained from PCMCI\u2229 algorithms are shown in Table 1. Sets of true and illusory parents of a variable at time t are separated by curly braces. For instance, variable $X_{WP}^t$ with $w_{wp}$ = 1 means that the causal mechanism of the surface pressure in the West Pacific remains invariant over time with the estimated parent set $\\{X_{CP}^{t-1}, X_{EP}^{t-2}, X_{WP}^{t-1}, X_{TA}^{t-1}\\}$. Only time series $X_{CP}^t$ has three different parent sets, including one true parent set and two illusory parent sets, which appear periodically over time. The three parent sets of $X_{CP}^t$ imply that the causal effect from the tropical Atlantic air temperature $X_{TA}^{t-1}$ to the Central Pacific air temperature $X_{CP}^t$ would disappear every quarter of a year. Note that we do not have a ground truth in this case, and we do not possess the necessary knowledge in this area, so the significance of these results is under-explored. More discussion about this application can be found in the supplementary materials."}, {"title": "5 Conclusions", "content": "In this paper, we propose a non-parametric, constraint-based causal discovery algorithm PC\u039c\u0395\u0399\u03a9 designed for semi-stationary time-series data, in which a finite number of causal mechanisms are repeated periodically. We establish the soundness of our algorithm and assess its effectiveness on continuous-valued and discrete-valued time series data. The algorithm PCMCI\u03a9 has the capacity to reveal the existence of periodicity of causal mechanisms in real-world datasets."}, {"title": "6 Acknowledgements", "content": "This research has been supported in part by NSF CAREER 2239375 and Adobe Research. We wish to convey our heartfelt gratitude to the anonymous reviewers for their invaluable and constructive feedback, which significantly contributed to enhancing the quality of the manuscript."}, {"title": "Appendix", "content": "The PCMCI algorithm is proposed by Runge et al. (2019), aiming to detect time-lagged causal relations in a window causal graph. There are two stages of PCMCI: the condition-selection stage and the causal discovery stage. In the first stage, unnecessary edges are removed based on the conditional independencies from an initialized partially connected graph where Assumption A4-A5 should be satisfied. In the second stage, Momentary Conditional Independence tests (MCI) are used to further remove the false positive edges caused by autocorrelations in time series data. More specifically, these two steps can be briefly formalized as follows:\n\u2022 PC1 in Algorithm A1: Condition selection stage. PC\u2081 is a variant of the skeleton-discovery part of the PC algorithm in a more robust version named stable-PC Le et al. (2016). The goal in this stage is to obtain a superset of the parents Pa(Xi) for all variables $X_t^j\\in V$, t\u2208 $[T_{max+1,T}]$. Initialize $Pa(X_t^j) = \\{X_{t-\\tau}^i\\}_{i\\in[n], \\tau\\in[T_{max}]}$. $Pa(X_t^j)$ will remove $X_{t-\\tau}^i$ if\n$X_t^j \\amalg X_{t-\\tau}^i\\mid Pa(X_t^j)\\{X_{t-\\tau}^i\\}$ (17)\n\u2022 MCI in Algorithm A2: Causal discovery stage. In this stage, do MCI tests for all variable pairs $(X_{t-\\tau}^i, X_t^j)$ with $i, j \\in [n]$ and time delays \u03c4 \u2208 [$T_{max}$]:\nMCI($X_{t-\\tau}^i, X_t^j\\mid Pa(X_t^j)\\{X_{t-\\tau}^i\\}, Pa(X_{t-\\tau}^i)$) (18)\nwhere $Pa(X_t^j)$ and $Pa(X_{t-\\tau}^i)$ are estimated from the PC\u2081 stage.\nNote that $T_{max}$ in this section is the same as Tub in the main paper, serving as the upper bound for the time lag that exhibits causal effects. On the other hand, Tmax in the main paper denotes the maximum time lag observed within the multivariate time series. Essentially, in the main paper, Tub is a parameter that must be fed into the algorithm, and Tmax is observed from the true causal graph. As a default, we assume Tub is configured with a value greater than Tmax, ensuring that the algorithm uncovers the correct causal relations. See Fig.4 for more detail."}, {"title": "Algorithm A1 PCqmax", "content": "1: Input: A n-variate time series V = ($X^1$, $X^2$, $X^3$, ..., $X^n$), target time series $X^j$, maximum time lag \u03c4max, significance threshold \u03b1pc, maximum condition dimension \u03c1max (default \u03c1max =\nn\u03c4max), maximum number of combinations qmax (default qmax = 1), conditional independence test function CI\n2: function CI(X, Y, Z)\n3: Test X Y Z using test statistic measure I\n4: return p-value, test statistic value I\n5: Initialize preliminary set of parents $Pa(X_t^j)$ = \\{$X_{t-\\tau}^i$: $i \\in \\{1, ..., n\\}$, \u03c4\u2208 \\{1, ..., \u03c4max\\}$\\}\n6: Initialize dictionary of test statistic values $I_{min}(X_{t-\\tau}^i \\rightarrow X_t^j)$ = \u221e \u2200$X_{t-\\tau}^i$, \u03b5 $Pa(X_t^j)$\n7: for p = 0, 1, 2, ..., \u03c1max do\n8: if $|Pa(X_t^j)|$ \u2212 1 < p then\n9: Break for-loop\n10: end if\n11: for all $X_{t-\\tau}^i$ in $Pa(X_t^j)$ do\n12: q = \u22121\n13: for all lexicographically chosen subsets S \u2286 $Pa(X_t^j)$ \\{$X_{t-\\tau}^i$\\} with |S| p do\n14: q=q+1\n15: if q\u2265 qmax then\n16: Break from inner for-loop\n17: end if\n18: Run CI test to obtain (p-value, I) \u2190 CI($X_{t-\\tau}^i$, $X_t^j$, S)\n19: if |I| < $I_{min}(X_{t-\\tau}^i \\rightarrow X_t^j)$ then\n20: $I_{min}(X_{t-\\tau}^i \\rightarrow X_t^j)$ = |I|\n21: end if\n22: if p-value > \u03b1pc then\n23: Mark $X_{t-\\tau}^i$, for removal from $Pa(X_t^j)$\n24: Break from inner for-loop\n25: end if\n26: end for\n27: end for\n28: Remove non-significant parents from $Pa(X_t^j)$\n29: Sort parents in $Pa(X_t^j)$ by $I_{min}(X_{t-\\tau}^i \\rightarrow X_t^j)$ from largest to smallest\n30: end for\n31: return $Pa(X_t^j)$"}, {"title": "Algorithm A2 MCI", "content": "1: Input: A n-variate time series V = ($X^1$, $X^2$, $X^3$, ..., $X^n$), sorted parents $Pa(X_t^j)$ for all\nvariables $X^j$ estimated with Algorithm A1, maximum time lag \u03c4max, maximum number \u03c1x of\nparents of variable $X^j$, and conditional independence test function CI\n2: for all ($X_{t-\\tau}^i$, $X_t^j$) with $i, j \\in \\{1, ..., n\\}$, \u03c4\u2208 \\{0, ..., \u03c4max\\}, excluding ($X_t^j$, $X_t^j$) do\n3: Remove $X_{t-\\tau}^i$ from $Pa(X_t^j)$ if necessary\n4: Define $Pa^{\\rho_x}(X_t^j)$ as the first px parents from $Pa(X_t^j)$, shifted by \u03c4\n5: Run MCI test to obtain (p-value, I) \u2190 CI($X_{t-\\tau}^i$, $X_t^j$, Z = \\{$Pa(X_t^j)$, $Pa^{\\rho_x}(X_{t-\\tau}^i)$\\})\n6: end for\n7: Optionally adjust p-value of all links by False Discovery Rate-approach (FDR)\n8: return p-value and MCI test statistic values"}, {"title": "B PCMCI", "content": "For simplicity's sake, define sets: $[b] := \\{1, 2, ..., b\\}$ and $[a, b] := \\{a, a + 1, ..., b\\}$, where a, b \u2208 N."}, {"title": "C Soundness of PCMCI\u03a9", "content": "Claim: Any discrete-valued time series V with Semi-Stationary Structural Causal Model (SCM) satisfying assumption A1, A2, A4, A5 can be written as a Markov chain $\\{Z_n\\}$ as long as this Markov chain satisfies $Pa(Z_n) \\subset Z_n \\cup Z_{n-1}$ for all n, where $Z_n$ is a set of variables in V. This Markov chain has a finite number of states if all time series in V are discrete-valued time series.\nNote that when the notation n is related to a Markov chain $Z_n$, it means the running index. In the context of $X_t^j\\in[n]$, n represents the index of component time series within the n-variate time series.\nTo simplify, assume that one associated Markov chain of V = {X, Y} has $Z_n = \\{X_t, Y_t, X_{t-1}, Y_{t-1}\\}$ with $t \\in \\{t \\in N^+ :,t < T\\}$ satisfying $Pa(Z_n) \\subset Z_n \\cup Z_{n-1}$. Here, the notation for the time points of variables is simplified as t and t \u2013 1, even though it should be a function of n, the running index of the Markov chain. Note that $Z_{n-1} = \\{X_{t-2}, Y_{t-2}, X_{t-3}, Y_{t-3}\\}$ rather than $\\{X_{t-1}, Y_{t-1}, X_{t-2}, Y_{t-2}\\}$, as the simplified notation could erroneously suggest the latter sequence. A simple proof is shown below through Markov assumption (A2)."}, {"title": "4 Experiments", "content": "All experiments, including those detailed in the main paper, are conducted on a single node with one core, utilizing 512 GB of memory in the Gilbreth cluster at Purdue University.\nHere, we describe how to calculate the metrics (F\u2081 score, Adjacency Precision, and Adjacency Recall) in our setting. In stationary time series, the output of the causal discovery algorithm is typically an adjacency matrix with dimensions [n, n, Tmax + 1]. Within the three-dimensional binary array, the value 1 signifies an edge pointing from one variable to another with a specific time lag, while 0 indicates the absence of an edge. For instance, if element [i, j, k] in the matrix is 1, then there is an edge pointing from $X_{t-k}^i$ to $X_t^j$. In semi-stationary time series, due to the presence of multiple causal mechanisms, the binary edge matrix is a four-dimensional array with dimensions [n, \u03a9, n, Tmax + 1], where \u03a9 is defined as Eq.(7) in the main paper. This expanded binary matrix is constructed based on the edge matrix of each variable $X_t^i$, $j \\in [n]$, through repetition. For instance, if \u03a9 = 2wj, setting the third dimension of the large binary matrix to j should yield wj potentially different parent sets (including illusory and true parent sets), each appearing twice."}, {"title": "G.1 More Discussion regarding the Case Study", "content": "As stated in the main paper, we express our inability to comment on the significance of the case study results. We open a door for the related experts; if assumptions A1-A7 are satisfied, the stationary assumption may not hold in this real-world dataset, and such periodicity exists. However, if the finding is not correct from an expert's viewpoint, the following assumptions may be violated:\n\u2022 Assumption A4 No Contemporaneous Causal Effects: There is a possibility of potential causal effects from $X_{TA}^t$ to $X_{CP}^t$ that the algorithm is unable to capture.\n\u2022 Assumption A6 Hard Mechanism Change combined with limited power of CI tests: If there is a soft mechanism change in the variables, the reliability of the CI test of two variables given their parents will be influenced by the skewed distribution of the parent variables. This effect will be exacerbated by the fact that the sample size will be shrunk by w.\nWe provide a sound and robust algorithm for experts in various fields who are interested in validating the presence of periodicity within the causal mechanisms specific to their domain."}, {"title": "G.2 Experiments on Continuous-valued Time Series with Exponential Noise", "content": "Considering that VARLINGAM is a temporal extension of LiNGAM and LiNGAM is an algorithm designed for non-Gaussian data, following the work in Pamfil et al. (2020), we also construct experiments on continuous-valued time series data with Exponential noise. Shown as Fig.7(a), the performance of PCMCI, PCMCI and VARLINGAM, are quite similar with their performance on Gaussian noise. The recall rate of DYNOTEARS, however, gets worse with Exponential noise."}, {"title": "G.3 Experiments on Binary Time Series", "content": "Similar to the process of generating continuous-valued time series, the generation of binary time series also involves three steps. However, the main difference lies in the last two steps. In the third step, we simulate the conditional distributions of each child variable based on all possible combinations of parent variable values. Subsequently, we randomly generate the value of the child variable by considering the corresponding conditional distribution given its parent sets.\nFor discrete-valued time series, a longer time length is required. To evaluate performance, we conduct a series of experiments following the same methodology as described in section 4.1. Fig.7(b) illustrates the variation in comprehensive performance with respect to wmax. PCMCI\u2229 demonstrates a similar performance to PCMCI```json\n, a well-balanced trade-off between precision and recall. This outcome is expected since discrete-valued time series demand larger sample sizes, and the increases in Wmax negatively impact the power of MCI tests. This observation is further supported by Fig.8(a), where an increase in time length T from 4000 to 12000 does not lead to a significant improvement in the accuracy rate of w, while the accuracy decreases rapidly with higher values of @max.\nComparing these results to the experiments conducted on continuous-valued time series, it becomes evident that the demand for efficient samples is even more substantial for binary time series, and the influence of increasing @max on performance becomes more pronounced."}, {"title": "G.4 More experiments on Continuous-valued time series", "content": "In this section, we conduct more experiments with continuous-valued time series with Gaussian noises.\nIn Fig.9(a), we test our algorithm with and without utilizing the turning point rule. See lines 13-14 in Algorithm B1 and section E for more information about the turning point rule. Let PCMCI\u03a9 TP"}]}