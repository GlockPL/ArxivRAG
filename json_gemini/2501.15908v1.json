{"title": "EVIDENTIAL PHYSICS-INFORMED NEURAL NETWORKS", "authors": ["Hai Siong Tan", "Kuancheng Wang", "Rafe McBeth"], "abstract": "We present a novel class of Physics-Informed Neural Networks that is formulated based on the principles of Evidential Deep Learning, where the model incorporates uncertainty quantification by learning parameters of a higher-order distribution. The dependent and trainable variables of the PDE residual loss and data-fitting loss terms are recast as functions of the hyperparameters of an evidential prior distribution. Our model is equipped with an information-theoretic regularizer that contains the Kullback-Leibler divergence between two inverse-gamma distributions characterizing predictive uncertainty. Relative to Bayesian-Physics-Informed-Neural-Networks, our framework appeared to exhibit higher sensitivity to data noise, preserve boundary conditions more faithfully and yield empirical coverage probabilities closer to nominal ones. Toward examining its relevance for data mining in scientific discoveries, we demonstrate how to apply our model to inverse problems involving 1D and 2D nonlinear differential equations.", "sections": [{"title": "1 Introduction", "content": "Physics-Informed Neural Networks (PINNs) furnish a class of scientific machine learning frameworks that can harness observational data and the universal approximation capability of neural networks [16] to solve partial differ-ential equations (PDE) and their related inverse problems. It is essentially characterized by a linear combination of loss functions formulated such that they drive the model towards fitting the empirical datasets while also adhering to the governing PDEs (equipped with some set of bound-ary/initial conditions). Over the years, PINNs have shown excellent results in terms of solving not only forward prob-lems [22, 25, 26], but also inverse ones [8, 14, 17, 19, 30] where we use PINNs to determine unknown scientific pa-rameters in the PDEs that are assumed to model the obser-vational data, enabling deep learning techniques to be used as a powerful tool for scientific discovery.\nIn this paper, we present a novel form of PINNs designed such that uncertainty quantification [1] is naturally embed-ded within the model architecture and its loss function. In our formulation, uncertainty estimates of both the depen-dent variables of the PDE and its unknown parameters are obtained immediately upon completion of model training which is implemented in a typical gradient-descent based approach. This is in contrast to other forms of methods such as Bayesian-based techniques which rely on Monte-Carlo sampling to estimate output variance. Our model is largely inspired by the principles of Evidential Deep Learn-ing (EDL) introduced in the seminal works [2, 27] where an evidential, higher-order set of priors are placed over like-lihood functions for the outputs, and model training is then used to infer hyperparameters of this distribution. Uncer-tainty estimates can be deduced straightforwardly as they are closed-form functions of the learned hyperparameters. To our knowledge, although EDL has been implemented in various types of model architectures (e.g. U-Nets [18, 31], FNN ([2]), LeNET [27]) it hasn't been synthesized with PINNs. In our work here, we propose how a hybrid of PINN and EDL principles can be performed, and present a novel information-theoretic regularizer for EDL that is also applicable for more general regression tasks.\nAs reviewed in [8], one of the most recent state-of-the-art proposals for uncertainty quantification in PINNs is the highly interesting Bayesian-PINN (B-PINN) of Yang et al. in [29]. Thus, for comparison, we validated our model against the inverse PDE problems studied in [29]. Specifically, we applied our model to 1D and 2D nonlinear reaction-diffusion-type systems with the training datasets"}, {"title": "2 Theoretical basis of our model", "content": "A distinctive feature of PINNs is that the PDE residual is included as an essential component of the loss function, guiding the model training towards adhering to the PDE in an unsupervised manner. Consider the case where the PDE contains an unknown parameter $\\kappa$. We lift it to be a random variable equipped with a density function $P(\\kappa; \\mu_{\\kappa})$ where $\\mu_{\\kappa}$ collectively denotes its moments. In the most general case, $\\kappa$ is not separable from the PDE residual which we denote as\n$\\mathcal{L} (u(x), \\partial u, x, \\kappa) = 0,$\totheusualPINNformalismproceedsfromhere,yetanexplicitformfor$P(\\kappa;\\mu_{\\kappa})$hastobespecified,and$G$isgenerallydependentonthechosen$\\kappa$prior.Forconcreteness,inthispaper,weconsiderthecasewhere$\\kappa$isseparablefromothertermswithin$\\mathcal{L}$inthefollowingsense.\n$\\mathcal{L} = \\mathcal{L}_1 (u, \\partial u, x) + \\kappa \\mathcal{L}_2 (u, \\partial u, x).$\nMarginalizing over $\\kappa$ via (2), we then obtain\n$G (u, \\partial u, x, \\mu_{\\kappa}) = \\mathcal{L}_1^2 + \\mathcal{L}_2^2 (Var(\\kappa) + \\kappa^2) + 2\\kappa (2\\mathcal{L}_1\\mathcal{L}_2),$\totheusualPDEresiduallossofPINNinthelimit$Var(\\kappa)\\rightarrow0$with$\\kappa\\rightarrow\\kappa$.Despitesimplicity,thePDEresidual(4)for$\\mathcal{L}$isapplicableformanyinversePINNproblemsstudiedinpreviousliterature(e.g.[4,5,29]).ItcanbeencapsulatedwithintraditionalMonte-CarloDropout[10]andDeepEnsemble[9,11,23]methodstoyielduncertaintyestimatesfor$u(x)$apartfrom$\\kappa$.\nWe now incorporate the fundamental principles of EDL [2, 27] in PINN and the residual loss (4). We first assume that the observed datapoints of $u$ have been drawn i.i.d. from Gaussian profiles. Placing Gaussian and inverse-gamma priors on the means and variances respectively, the posterior distribution adopts the form of a 4-parameter normal-inverse-gamma distribution.\n$p (\\bar{u}, \\sigma_u|\\gamma,\\nu, \\alpha, \\beta) = \\frac{\\sqrt{\\frac{\\nu}{2\\pi}} \\beta^\\alpha}{\\Gamma(\\alpha) \\sqrt{\\sigma_u}}e^{-\\frac{2\\beta + \\nu(\\gamma-\\bar{u})^2}{2\\sigma_u^2}} .$\nThis assertion leads to simple closed-form formulas for the prediction means and uncertainties as follows.\n$E[\\bar{u}_i] = \\gamma,  \\sigma_p = E[\\sigma_u^2] + Var[\\bar{u}] = \\frac{2\\beta}{\\nu (\\alpha - 1)} (1 + \\frac{1}{\\nu}),$\nwhere we note that the expectation value $E$ is defined w.r.t. $p$ in (5), and that the first and second components within the bracket are the aleatoric and epistemic uncertainties de-fined as $E[\\sigma_u^2]$, $Var[\\bar{u}]$ respectively. In EDL, the learnable parameters are the hyperparameters {$\\gamma, \\alpha, \\beta, \\nu$}. Fitting data to the evidential model requires not the usual mean-squared-error loss term but rather the negative logarithm of the marginal likelihood obtained after integrating the first-order Gaussian profiles over the measure described by $p$ in (5).\n$\\mathcal{L}_{edl} = log ((\\bar{u}_i - y)^2 \\nu + \\Omega)^{-\\alpha + \\frac{1}{2}} + log \\frac{\\Gamma(\\alpha + \\frac{1}{2})}{\\sqrt{\\pi \\nu} \\Gamma(\\alpha)},$\nwhere $\\Omega = 2\\beta (1 + \\nu)$ and $u_i$ refer to the observed datapoints for the dependent variable $u$. In [2], the authors proposed a regularizer designed to minimize evidence on incorrect predictions that reads $\\mathcal{L}_R = |u_i - y|(2\\nu + \\alpha)$. This was shown in [2, 28] to work well, yet it is indepen-dent of $\\beta$ which controls the overall uncertainty scale. Here we propose a refinement of this original EDL regularizer by multiplying it to the Kullback-Leibler divergence [7, 21] between the distribution described by (5) and a weakly-informative prior, its final form being (see Appendix A for a detailed derivation)\n$\\mathcal{L}_{kl} = |u_i - y|(2\\nu + \\alpha) log \\big(\\frac{\\Gamma(\\alpha)}{\\sqrt{\\frac{\\beta_r}{\\beta}}} \\exp\\big(\\gamma_e(\\alpha - 1) + \\frac{\\beta - \\beta_r}{\\beta_r} \\big)\\big),$\nwhere $\\gamma_e$ is the Euler-Mascheroni constant. In (8), $\\beta_r$ is a reference $\\beta$ value which is assumed to describe a weakly informative prior for larger deviations from the observed dataset. We set it to be of the same order-of-magnitude as the maximal target variable over the PDE domain. Empiri-cally, we found this regularizer to perform better than $\\mathcal{L}_R$ that was used in [2, 28]. Theoretically, it aligns with the goal of guiding distributions for erroneous predictions to inherit weak priors with higher uncertainties.\nTo include a suitable PDE residual in the EDL framework, we take $G$ as defined in (4), but with $u \\rightarrow \\gamma$. Physically, this can be interpreted as the dependent variable being intrinsically stochastic in nature and the PDE should be un-derstood as modeling the dynamics of its mean $\\gamma$. The base model output variables are now represented by {$\\gamma, \\alpha, \\beta, \\nu$} (instead of $u$) and thus the Dirichlet/Neumann boundary conditions should be expressed in terms of the mean $\\gamma$. For the case studies here, we adopted the soft constraint ap-"}, {"title": "3 Methodology", "content": "We validated our model against the same 1D and 2D in-verse problems presented in the seminal paper [29] where B-PINNs were formulated. They are nonlinear differential equations with source terms designed such that the exact solutions are simple products of sinusoidal functions, en-abling a more convenient analysis of the neural networks' effectiveness. In contrast to [29] however, we doped the training dataset with a more heterogeneous noise as we wish to scrutinize the degree to which the uncertainty dis-tributions align with those of the inserted noise. The differ-ential equations are of the same class as diffusion-reaction equations with non-linear source terms.\n[1] 1D nonlinear Poisson equation:\n$\\lambda \\frac{d^2 u}{dx^2} + \\kappa tanh(u) = f_{1D}(x), u_e = sin^3(6x).$\n[2] 2D nonlinear diffusion-reaction PDE:\n$\\lambda \\nabla^2 u + \\kappa u^2 = f_{2D}(x), u_e = sin(\\pi x) sin(\\pi y).$\nLike in [29], we set $\\lambda = 0.01$ and $\\kappa$ to be the unknown PDE parameter to be inferred. In [1] and [2], the ex-act solutions $u_e$ and their corresponding source terms $f_{1D,2D}(x)$ are such that we have $\\kappa = {0.7, 1}$ for the 1D and 2D equations respectively. For these problems, taking fully-connected multilayer perceptron (with a few hidden layers) to be the base models turned out to be sufficient for our case studies. The 1D and 2D prob-lems required only three and two hidden layers respec-tively. The total number of iterations was $10^5$ epochs, and Adam optimizer [20] was used with a learning rate of $10^{-3}$. All E-PINN models were implemented with PyTorch [3] using its Autograd engine [24]. For the 1D problem, the optimal loss function parameters were found to be $\\lambda = 1, \\lambda_{kl} = 0.01, \\lambda_r = 0.1$, whereas for the 2D problem, we used $\\lambda = 1, \\lambda_{kl} = 0.005, \\lambda_r = 0.05$. As for Bayesian-PINNs, the algorithm of [29] was implemented in PyTorch using the Hamiltonian-Monte-Carlo method [6, 29] to sample the posterior distributions. The number of samples used was of the order of $10^4$ with a burn-in ratio of about 0.2.\nFor the 1D problem, we took the number of observational and collocation data points to be 200 (random) and 500 (regularly spaced) respectively. The training dataset domain was $x \\in (-0.8, 0.7)$, yet domain for testing data was extended to $x = 1.2$ for asessing various models on out-of-distribution data. The data was doped with a stan-dard Gaussian noise with a spatially varying amplitude, i.e. $A(x) \\mathcal{N}(0, 1)$ as depicted in Fig 1.\nFor the 2D problem, we also took the number of obser-vational and collocation data points to be 200 and 500 respectively, with the domain being a square centered at the origin with length 2. In addition, we imposed a soft constraint realizing the Dirichlet condition along each of the boundary segments {$x = \\pm 1, y = \\pm 1$} by including 50 points for each side of the square. We doped the train-ing data with a standard Gaussian noise with a spatially varying amplitude $A(x, y)$ that was maximal at the origin and vanishing along the boundary segments.\n$A(x, y) = 0.1 cos(\\frac{\\pi x}{2}) cos(\\frac{\\pi y}{2}) e^{-(x^2 + y^2)}.$\nAs a metric for assessing uncertainty quantification, we monitored the empirical coverage probability (ECP) [12, 13] defined as the proportion of samples of which devi-ations from the exact solution fell within the confidence band (we took it to be $\\pm 1.96 \\sigma_p$ for a 0.95 CI). We also kept track of the Spearman's coefficients between $\\sigma_p$ and (i)prediction errors (pe) (ii)noise-induced errors in the train-ing data (pn). We compared our E-PINN model against B-PINN, Deep Ensemble (DE) method (defined by a set"}, {"title": "4 Results", "content": "For the 1D problem, as visible in Fig. 3, the predictive uncertainty $\\sigma_p$ was larger in the high-noise regions and beyond the training domain, with the 0.95 confidence band encompassing both exact solution and model-predicted curve. Taking $\\sigma_{\\kappa} = \\sqrt{Var(\\kappa)}$ to be the standard error, E-PINN predicted $\\kappa \\approx 0.71 \\pm 0.03$, close to the actual value of 0.7. The ECP value was 0.96 which was closest to the nominal value of 0.95 among all models. Compared to E-PINN(V), our model inferred a more accurate $\\kappa$ and was better calibrated as measured by its ECP. Relative to B-PINN, it inherited stronger correlations with prediction errors (pe), noise-induced deviations of the training data from exact solution (pn) and had a better ECP score. The Deep Ensemble inferred $\\kappa$ with a much smaller uncertainty than other models, but its ECP appeared to be problematic indicating a poor degree of calibration. Like B-PINN, the correlation of its uncertainty distribution with the noise of the data was weak (pn ~ 0.1).\nFor the 2D problem, we found E-PINN to exhibit similar advantages relative to other frameworks. It inferred $\\kappa$ to be 1.00 \u00b1 0.01, with a ECP value of 0.89, and Spearman's coefficients pe ~ 0.7, pn ~ 0.8. The mean model error was ~ 0.02 while it was ~ 0.002 when restricted to bound-ary, showing strong adherence to the Dirichlet boundary condition."}, {"title": "5 Conclusion", "content": "We have constructed a novel uncertainty-aware PINN-like model synergizing the fundamental principles of EDL and PINN. It provides another data-driven framework in sci-entific machine learning that is suited for solving inverse problems involving differential equations. E-PINN gen-erates uncertainty estimates for both the target variable and unknown PDE parameters upon completion of model training. The novel information-theoretic regularizer we proposed in eqn. (8) appeared to enhance the model's ef-fectiveness relative to the original version first proposed in [2]. For the 1D and 2D problems examined here, we found that E-PINNs superseded B-PINNs and other models in terms of sensitivity of uncertainties towards data noise and predictive errors. Its empirical coverage probability values were close to nominal ones indicating a good degree of uncertainty calibration. The 1D problem showed E-PINN to be effective in interpreting out-of-domain data, while the 2D problem illustrated the model's ability to adhere to boundary conditions. An important future direction would be to validate E-PINN against a wider set of PDEs used in various scientific and engineering fields, such as those proposed in PINNacle [15]."}, {"title": "A Appendix A: Supplementary derivations for Section 2", "content": "In this appendix, we collect various mathematical details omitted in Section 2 for additional clarity.\nTo derive the residual function $G$ in eqn. (4), we note that\n$G = \\int_{D_{\\kappa}} d\\kappa P (\\kappa; \\mu_{\\kappa}) [\\mathcal{L}_1^2 + \\mathcal{L}_2^2\\kappa^2 + 2\\kappa \\mathcal{L}_1 \\mathcal{L}_2]$\n$= \\mathcal{L}_1^2 + \\mathcal{L}_2^2 \\kappa^2 + 2 \\kappa \\mathcal{L}_1 \\mathcal{L}_2$\n$= \\mathcal{L}_1^2 + \\mathcal{L}_2^2 (Var(\\kappa) + \\kappa^2) + 2 \\kappa \\mathcal{L}_1 \\mathcal{L}_2.$\nWhen $G$ is incorporated in Monte-Carlo Dropout for PINN, the uncertainty in $\\kappa$ is not affected by the stochastic dropout units that are switched on during each forward pass, but has already been learned during model training. On the other hand, for Deep Ensemble, one averages over the collection of models to deduce both $\\kappa$ and $Var(\\kappa)$.\nWith regards to our regularizer loss term $\\mathcal{L}_{kl}$, we had obtained it by taking the KL divergence between the normal-inverse-gamma (NIG) distribution parametrized by {$\\gamma, \\alpha, \\beta, \\nu$} and another reference NIG distribution with {$\\bar{\\gamma}_r, \\alpha_r, \\beta_r, \\nu_r$} that presumably characterizes a non-weakly informative prior. When multiplied to a term such as $|u_i - y|$ in (8), this means that for more significantly large errors, the loss term guides the distributions for such samples to approach a weakly-informative one yielding higher uncertainty. By definition, the KL divergence be-tween two NIG distributions with density functions {$\\bar{p}_r, p$} is\n$D_{KL} (\\bar{p}_r, p) = \\int \\int d\\bar{u} d\\sigma_u \\bar{p}_r log \\frac{\\bar{p}_r}{p}.$\nThe NIG distribution can be equivalently understood as the product\n$\\bar{u} \\sim N(\\gamma, \\frac{\\sigma_u}{\\nu}), \\sigma_u \\sim \\Gamma^{-1}(\\alpha, \\beta).$\nWe would like the reference prior to be a non- or weakly-informative prior. Taking the limit of $\\nu_r \\rightarrow \\infty$ yields a uniform distribution over $\\mathcal{R}$ which is unfortunately unnor-malizable. To avoid this pathology, we take the reference NIG to be parametrized by values of $\\gamma, \\nu$ identical to those of the EDL model outputs' distributions, and instead set the reference inverse-gamma distribution to be of rela-tively higher entropy, taking $\\alpha_r = 1$ and letting $\\beta_r$ be task-dependent. Since the variance $\\sigma_u^2$ is not physically expected to be larger than the upper bound for $u$, a natu-ral candidate for $\\beta_r$ is the order-of-magnitude of $U_{max}$ the maximum value of the dependent variable as deduced from physical principles. For the differential equations examined in our work here, we take $\\beta_r = 1$. Generally, it should be set to the same order-of-magnitude as $U_{max}$. This definition of $\\bar{p}_r$ leads to the KL divergence term being\n$D_{KL} = log(\\frac{\\Gamma(\\alpha)}{\\sqrt{\\frac{\\beta_r}{\\beta}}}) + \\gamma_e(\\alpha - 1) + \\frac{\\beta - \\beta_r}{\\beta_r}.$\nIn this form, we note that there is no dependence on $\\nu$. In [2], the authors argued that the NIG distribution admits the interpretation of having its variance estimated from a 'virtual observations' with sample mean $\\gamma$ and sum of squared deviations 2$\\nu$, and one can thus interpret 2$\\nu$ + $\\alpha$ as a measure of evidence for the EDL distributions. Together with a linear term $|u_i - y|$ that aims to increase the loss for predictions with larger errors, the authors of [2] then took $|u_i - y|(2\\nu + \\alpha)$ to be the regularizer term. Since previous works [2, 28] has shown it to be effective in a number of contexts and it carries a dependence on $\\nu$, we multiply it to the KL term in (13) to obtain our final expression for the regularizer loss term\n$\\mathcal{L}_{kl} = |u_i - y|(2\\nu + \\alpha)D_{kl} .$"}, {"title": "B Appendix B: Learning curves for E-PINN training", "content": "Model training for E-PINN involves choices of loss func-tion weight coefficients $\\lambda_d, \\lambda_r, \\lambda_{kl}$. Apart from the indi-vidual loss terms, they should yield learning curves which show stable convergent behavior for $\\kappa, \\sigma_k$. Fig. 5 depicts the evolutions of $\\kappa, \\sigma_k$ for the 2D problem. While the latter decreased steadily, the former experienced a transient"}, {"title": "C Appendix C: Additional details on the results for other uncertainty-aware PINN models", "content": "In this Appendix, we provide some additional details on the simulation and training results for\n[A] B-PINN with Hamiltonian-Monte-Carlo for pos-terior sampling [29].\n[B] E-PINN(V): E-PINN equipped not with (8) but with the original regularizer of [2] : $\\mathcal{L}_{kl} = |u_i - y|(2\\nu + \\alpha),$\n[C] Deep Ensemble (DE) method using a set of 20 PINNs with distinct initial weights.\n[A] B-PINN models exhibited weaker correlations with noise and errors compared to our E-PINN models. In Fig. 7, one can see that the confidence band was not particularly enlarged within the high-noise regions, in contrast to our E-PINN model (Fig. 3). It yielded weak correlations between $\\sigma_p$ and model errors (pe ~ 0.5) or noise-induced errors (pn ~ -0.2). The model's empirical coverage probability was 0.87. Predicted value for $\\kappa$ with its uncertainty was $\\kappa = 0.69 \\pm 0.03$, while its mean predictive uncertainty was $\\sigma_p \\sim 0.03.\n[B] For E-PINN(V), we found that the main disadvantage relative to our proposed $\\mathcal{L}_{kl}$ was a weaker response to out-of-distribution data. In Fig. 9, one can see that although the confidence band was enlarged within the high-noise regions, in contrast to our E-PINN model (Fig. 3), the predictive uncertainty $\\sigma_p$ was relatively much lower and not encompassing the deviations from the exact solution. The Spearman's coefficients ps were {0.6, 0.7} for uncer-tainty correlations with error and noise respectively. Its empirical coverage probability was 0.79. Predicted value for $\\kappa$ with its uncertainty was $\\kappa = 0.74 \\pm 0.03$, while its mean predictive uncertainty was $\\sigma_p \\sim 0.02.\n[C] We implemented DE using a set of 20 PINNs with $\\kappa$ as the only trainable parameter in the PDE residual loss), with distinct initial random weights set in Pytorch via Kaiming initialization. Training was completed with $10^5$ epochs with Adam optimizer under a constant learning rate of $10^{-3}$. For the 1D problem DE generated an un-certainty spectrum that did not appear to be sensitive to noise at all, though its confidence band for out-of-domain data was better calibrated compared to B-PINN and E-PINN(V). Predicted value for $\\kappa$ with its uncertainty was $\\kappa = 0.6976 \\pm 10^{-4}$."}]}