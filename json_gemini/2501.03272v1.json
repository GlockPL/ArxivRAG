{"title": "Backdoor Token Unlearning: Exposing and Defending Backdoors in Pretrained Language Models", "authors": ["Peihai Jiang", "Xixiang Lyu", "Yige Li", "Jing Ma"], "abstract": "Supervised fine-tuning has become the predominant method for adapting large pretrained models to downstream tasks. However, recent studies have revealed that these models are vulnerable to backdoor attacks, where even a small number of malicious samples can successfully embed backdoor triggers into the model. While most existing defense methods focus on post-training backdoor defense, efficiently defending against backdoor attacks during training phase remains largely unexplored. To address this gap, we propose a novel defense method called Backdoor Token Unlearning (BTU), which proactively detects and neutralizes trigger tokens during the training stage. Our work is based on two key findings: 1) backdoor learning causes distinctive differences between backdoor token parameters and clean token parameters in word embedding layers, and 2) the success of backdoor attacks heavily depends on backdoor token parameters. The BTU defense leverages these properties to identify aberrant embedding parameters and subsequently removes backdoor behaviors using a fine-grained unlearning technique. Extensive evaluations across three datasets and four types of backdoor attacks demonstrate that BTU effectively defends against these threats while preserving the model's performance on primary tasks.", "sections": [{"title": "Introduction", "content": "Pretrained Language Models (PLMs) (Devlin et al. 2018; Radford et al. 2019) have demonstrated remarkable performance across various tasks, such as sentiment analysis (Jim et al. 2024), toxicity detection (Bonetti et al. 2023), and news classification (Nkongolo Wa Nkongolo 2023). However, as PLMs are increasingly fine-tuned for specific downstream applications (Min et al. 2023), they have become vulnerable to backdoor attacks (Liu et al. 2024; Cheng et al. 2023). Typically, backdoor attacks inject malicious triggers into the model during training. The backdoored model functions normally on clean tasks but exhibits an attack-desired target label when the trigger is presented. In Natural Language Processing (NLP), backdoor triggers can be designed as obvious elements like rare words (Kurita, Michel, and Neubig 2020) or more subtle features such as sentence styles (Qi et al. 2021a). With the widespread adoption and deployment of PLMs, defending against backdoor threats has become an urgent challenge.\nExisting backdoor defense methods in NLP generally fall into three categories: backdoor detection (Lyu et al. 2024; Liu et al. 2022; Xian et al. 2023), backdoor removal (Zhang et al. 2022; Li et al. 2021c), and anti-backdoor learning (Li et al. 2021b; Zhu et al. 2022). Backdoor model detection methods aim to identify whether a model or inputs contain backdoors, while backdoor removal methods focus on purifying the backdoor triggers from the backdoored model. Among them, anti-backdoor learning methods (Zhu et al. 2022; Li et al. 2021b) has become a widely adopted defense strategy as they allow the users to train a clean model even on a poisoned dataset. For example, ABL (Li et al. 2021b) employs a two-stage gradient ascent technique to filter out and mitigate backdoor behaviors. Another approach, MF (Zhu et al. 2022), limits the model's learning capacity by restricting the number of training epochs, thereby preventing the model from acquiring backdoors during training. However, these anti-backdoor learning methods often lead to reduced model performance and exhibit instability across different scenarios. Therefore, how to effectively defend against backdoor attacks during the model training phase essentially deserves much attention.\nPrevious research has shown that backdoor learning can be viewed as a dual-task problem, i.e. training the backdoored model on both clean and backdoor data (Li et al. 2023). In this paper, we reformulate backdoor learning from model parameter perspective and identify two key properties: 1) backdoor learning induces significant differences between the embedding parameters of backdoor tokens and clean tokens, where the backdoor tokens converge much faster than clean ones; 2) the activation of backdoors is highly dependent on backdoor token parameters in the embedding layers. Intuitively, if we can isolate backdoor token parameters at the level of word embedding dimensions rather than across all model parameters, the backdoor information could be more effectively exposed and removed.\nIn this work, we propose a novel defense method called Backdoor Token Unlearning (BTU) for efficient anti-backdoor learning. Specifically, BTU operates in two stages: backdoor token detection and dimensional fine-grained unlearning. In the first stage, BTU identifies potential back-"}, {"title": "Related Work", "content": "Backdoor Attack\nExisting backdoor attacks in NLP manifest in two primary scenarios: outsourced training and data poisoning. In outsourced training, attackers have full control over the training process. For instance, the LWP (Li et al. 2021a) scheme implants backdoors in the model's intermediate layers to increase the persistence of the attack, while the transfer (Shen et al. 2021) approach adjusts the backdoor optimization target in front of the MLP layer, using a multi-objective strategy to ensure the attack's resilience against downstream task influences. Additionally, LWS (Qi et al. 2021c) employs an auxiliary model to create more concealed triggers. Conversely, in data poisoning scenarios, attackers are limited to inserting a few carefully crafted samples into the dataset since they do not control the training process. For example, Dai et al. (Dai, Chen, and Li 2019a) demonstrate that words or context-independent phrases can serve as triggers, and that random insertion into training samples can successfully inject backdoors. Similarity, Qi et al. (Qi et al. 2021a,b) reveal that textual styles and syntactic structures can also act as triggers, significantly enhancing the stealthiness of backdoor attacks. These studies highlight the high vulnerability of NLP models to such covert manipulations and underscore the critical need for robust defense mechanisms.\nBackdoor Defense\nIn the field of NLP, existing backdoor defense methods can be broadly categorized into three types: 1) Backdoor input detection, which is applied during the model inference stage to identify and prevent the activation of backdoor inputs (Gao et al. 2021; Chen and Dai 2021; Yang et al. 2021a). For example, BKI (Chen and Dai 2021) distinguishes potential trigger words by analyzing each word's impact on the model's outcomes; 2) Backdoored model detection, which assesses whether a model contains backdoors (Liu et al. 2022; Azizi et al. 2021), often employing techniques like reverse engineering. For instance, PICCOLO attempts to recover potential triggers embedded within the model; 3) Anti-backdoor learning aims to train clean models from potentially poisoned datasets during the training phase (Li et al. 2021b; Zhu et al. 2022; Min et al. 2023). For instance, ABL (Li et al. 2021b) characterizes backdoor learning as a form of shortcut learning, where backdoor triggers are more easily captured. To address this, ABL proposed a two-stage gradient ascent technique to mitigate backdoor effects. Similarly, the MF defense (Zhu et al. 2022) introduced to minimize overfitting to prevent the model from learning backdoor patterns. Although promising, these methods often fail against adaptive attacks, such as textual style or grammatical structure triggers. In this work, we present new insights into backdoor learning and propose an simple yet efficient anti-backdoor defense to mitigate such threat."}, {"title": "Proposed Token Unlearning Method", "content": "In this section, we first present the problem of backdoor attacks and then reveal the distinctive behavior between backdoor tokens and clean tokens optimized in the word embedding layers. Finally, we introduce our proposed BTU method.\nProblem definition Consider the poisoned training dataset as $D = D_c \\cup D_b$, where $D_c$ denotes the subset of clean data and $D_b$ denotes the subset of backdoor data. Training a backdoored model on a poisoned dataset can be viewed as minimizing the following empirical error:\n$L = \\mathbb{E}_{(\\mathbf{x},y)\\sim D_c}[\\ell(f_\\theta(\\mathbf{x}), y)] + \\mathbb{E}_{(\\mathbf{x},y)\\sim D_b}[\\ell(f_\\theta(\\mathbf{x}), y)],$\nwhere $\\ell$ and $\\theta$ denote the loss function and model parameters, respectively. The overall learning task can be regarded as a combination of the backdoor task on dataset $D_b$ and the clean task on dataset $D_c$.\nIntuitively, if we can clearly distinguish between clean and backdoor tasks, the backdoor task can be more effectively detected. To achieve this, we reformulate the backdoor learning process in Eq. 1 to focus on the word embedding layer rather than all model parameters. As a result, the model's optimization objective can be redefined as follows:\n$L = \\mathbb{E}_{(\\mathbf{x},y)\\sim D_c}[\\ell(\\epsilon_c(\\mathbf{x}), y)] + \\mathbb{E}_{(\\mathbf{x},y)\\sim D_b}[\\ell(\\epsilon_b(\\mathbf{x}), y)],$\nwhere $\\epsilon_c$ denotes the entire clean embedding parameters and $\\epsilon_b$ denotes backdoor embedding parameters. Based on Eq. 2, the backdoor information is primarily contained in the Backdoor Token Parameters (BTP), while the Clean Token Parameters (CTP) remain largely unchanged. Since the backdoor task is much simpler than the clean task (Li et al."}, {"title": "Backdoor Token Unlearning", "content": "Overview Fig. 2 illustrates the BTU framework, which consists of two main components: Backdoor Token Detection and Dimensional Fine-grained Unlearning. The backdoor token detection aims to identify suspicious backdoor tokens within the embedding parameters through three rounds of anomaly detection. Once these malicious tokens are detected, fine-grained dimensional unlearning is applied to remove backdoor functionalities from these token parameters. We provide detailed technical explanations below.\nBackdoor Token Detection As previously noted, we have identified a distinctive Euclidean distance between BTP and CTP. Building on this, we can detect suspicious backdoor token parameters through iterative detection rounds $T$. The detection threshold is set to $\\alpha \\in [0, 1]$, with the top $\\alpha\\%$ of embedding parameters flagged as backdoor token parameters in each detection round. For simplicity, we set $\\alpha$ to 0.05 across all three detection rounds. A more detailed analysis of $\\alpha$ and the detection round $T$ will be provided in the ablation study.\nIn the first round, we train only the embedding layer parameters $\\epsilon$ of model $M$ on the dataset $D$, resulting in the updated embedding layer parameters $\\epsilon'$. We then calculate the change distance $s'$ for each token $t_i$ and store the token-distance pairs in the set $T_1$:\n$T_1 = \\{(\\mathbf{s}_i, t_i)\\}_{t_i\\in V} = \\{(d(\\epsilon(t_i), \\epsilon'(t_i)), t_i)\\}_{t_i\\in V}$\nNext, we rank s in descending order and select the top $\\alpha\\% \\times |V|$ tokens from $T_1$, which we denote as $T'$.\nIn the second round, we retain the embedding layer and classification head of model M, denoted as $M^*$, and train the embedding layer $\\epsilon$ of $M^*$ to obtain $\\epsilon''$. After training on the dataset $D$, we calculate the change distance $s''$ for each token and store the token-distance pairs in the set $T_2$:\n$T_2 = \\{(\\mathbf{s}_i, t_i)\\}_{t_i\\in V} = \\{(d(\\epsilon(t_i), \\epsilon''(t_i)), t_i)\\}_{t_i\\in V}$\nWe then rank s in descending order and select the top $\\alpha\\% \\times |V|$ tokens from $T_2$, denoted as $T''$.\nIn the third round, we repeat the previous procedure, but modify the dataset to $D/T''$. All other settings remain the same, leading to:\n$T_3 = \\{(\\mathbf{s}''_i, t_i)\\}_{t_i\\in V} = \\{(d(\\epsilon(t_i), \\epsilon'''(t_i)), t_i)\\}_{t_i\\in V}$\nFinally, we rank $s'''$ in descending order and select the top $\\alpha\\% \\times |V|$ tokens from $T_3$, denoted as $T'''$.\nWe define $T = T' \\cup T'' \\cup T'''$ as the set of suspicious tokens. Notably, the three rounds of anomaly detection serve different purposes. Rounds 1 and 2 aim to detect simple triggers, while Round 3 refines the process to detect more complex triggers. This three-step iterative detection ensures comprehensive identification of suspicious backdoor tokens, effectively exposing both simple and complex triggers at varying levels of granularity. The analysis of results for different detection rounds can be found in the ablation study.\nDimensional Fine-gained Unlearning Given a backdoored model $M_b$ and a set of suspicious tokens $T$, the most straightforward method is to replace all tokens in $T$ with padding tokens that carry no information, thereby removing all backdoor-related token parameters. However, simple replacement would eliminate both backdoor and clean features within the word embedding parameters, leading to a decrease in model accuracy.\nTo maximally retain clean features in the word embedding parameters, we propose a Dimensional Fine-grained Unlearning technique, which allows selectively replace only the dimensions with large changes in BTP while remaining others unchanged. Specifically, we first calculate the mean change in the word embedding layer before and after training:\n$\\delta = \\sum_{t_i \\in V} (d(\\epsilon(t_i), \\epsilon'(t_i))) / |V|,$ \nFor all $t \\in T$, the dimensions in $\\epsilon'(t)$ with values greater than $\\delta$ are replaced by the corresponding dimension values of $\\epsilon'(p)$, where $p$ denotes the padding token. Thus, the suspicious parameters in embedding layers $\\epsilon_b(t)$ are replaced by:\n$\\epsilon'(t) = \\begin{cases} \\epsilon_i(t), & \\text{if } |\\epsilon'_i(t) - \\epsilon_i(t)| < \\delta; \\\\ \\epsilon'_i(p), & \\text{if } |\\epsilon'_i(t) - \\epsilon_i(t)| \\geq \\delta. \\end{cases}$\nFinally, the values in $\\epsilon'(t)$ are replaced with $\\epsilon(t)$. As we replace only a small number of tokens and the word embedding layer contains relatively little downstream information, the impact of our token unlearning causes minimal degradation in clean performance. To further mitigate the negative"}, {"title": "Experimental Setting", "content": "Datasets and Models We conducted experiments using three text classification datasets: 1) SST-2 (Stanford Sentiment Treebank-2) (Socher et al. 2013), a binary sentiment analysis dataset; 2) OLID (Offensive Language Identification Dataset) (Zampieri et al. 2019a), a binary toxicity detection dataset; and 3) AG News, a four-class news headline classification dataset. The victim model used is BERT-BASE-UNCASED, which consists of 12 layers with 30522 \u00d7 768 parameters in the word embedding layer.\nAttack Setups Four data poisoning-based attack methods are employed: 1) Add-Word, using rare words as triggers (e.g., \"cf\", \"tq\u201d, and \u201cbb", "I watched a 3D movie\"); 3) Stylebkd, using text styles as triggers (e.g., \u201cBible style\"); and 4) Synbkd, using syntactic structures as triggers (e.g., \u201c(ROOT (S (SBAR) (, ) (NP) (VP) (.)))": "."}, {"title": "Experimental Results", "content": "As shown in Table 1, BTU significantly reduces the success rate of four types of backdoor attacks across three datasets. Specifically, for insertion-based attacks (Add-Word and Add-Sent), BTU reduces the ASR to below 10% across all three datasets. Additionally, it is observed that the more complex the dataset, the more effective BTU becomes. Across all datasets, we find that the ACC of the Add-Sent attack is higher than that of the Add-Word attack. This is because BTU detects more clean tokens in the Add-Word attack, resulting in the loss of more clean features.\nFor unfixed type triggers in Stylebkd and Synbkd, BTU successfully mitigate the influence of backdoor attacks, demonstrating that these backdoor attack activations still depend on specific tokens. This phenomenon can also be observed in the poisoned samples, where conjunctions such as \"when\" and \"if\" are frequently involved. Additionally, we find that Stylebkd negatively affects the model's performance; however, BTU can effectively restore the damage caused by this attack."}, {"title": "Defense Results against Adaptive Attacks", "content": "In this section, we consider the countermeasures an attacker might take when aware that the defender is using BTU. The core of BTU is to capture and purify backdoor tokens based on the simplicity of backdoor tasks. However, when backdoor tasks become more complex, backdoor tokens may evade detection, leading to potential defense failure. Therefore, adaptive attacks could be executed by narrowing the learning difficulty gap between backdoor and clean tasks. We will explore these potential adaptive attacks in the following discussion.\nLow Poison Ratio In fact, a low poison ratio is more reflective of real-world scenarios. However, we found that most existing defense methods perform poorly against low poison ratio attacks. At the same time, a low poison ratio makes it more challenging for the model to learn the backdoor task. So we employ an experiment to test BUT's performance under low poison ratio backdoor attack. We use the lowest possible poison ratio (0.7%) to perform Add-Sent attack on the SST-2 dataset, achieving an ASR of over 90%. Then, we conducted training defense methods including RAP, Strip,"}, {"title": "Conclusion", "content": "In this work, we identified two key properties in the context of NLP backdoor learning: 1) the distinctive differences in the embedding values of backdoor tokens and clean tokens when only the word embedding layer is trained, and 2) the success of backdoor activation is highly related to the backdoor token parameters. Based on these observations, we propose a novel anti-backdoor learning method Backdoor Trigger Unlearning (BTU), which proactively exposes aberrant embedding parameters of backdoor tokens and mitigates backdoor behaviors during the training process. Extensive experimental results demonstrate that BTU can effectively defend against currently known backdoor attacks with minimal impact on the performance of clean tasks.\nFuture Work While BTU effectively defends against four different backdoor attacks and outperforms nine other defense methods, we cannot guarantee its effectiveness against more advanced future attacks. Further exploration is needed to provide theoretical guarantees for BTU's underlying mechanisms. Additionally, our current findings and defense results are based on evaluations with pre-trained language models, so it remains an open question whether BTU is effective for more advanced large language models."}, {"title": "Appendix", "content": "Experiment Details\nModel and Dataset The experiments were conducted on the BERT-BASE-UNCASED (Devlin et al. 2018) pre-trained model. The datasets used in this work are summarized in Table 8.\nAttack Setup We adopted four representative backdoor attacks in the field of NLP: Add-Word (Gu, Dolan-Gavitt, and Garg 2017), Add-Sent (Dai, Chen, and Li 2019b), Stylebkd (Qi et al. 2021a), and Synbkd (Qi et al. 2021b). These attacks utilize various trigger patterns, including word-level, sentence-level, and style-based triggers. We reproduced these attacks using the available open-source code \u00b9 from OpenBackdoor. A summary of all the attacks evaluated in our work is provided in Table 7.\nDefense Setup We reproduced six mainstream defense methods: BKI(Chen and Dai 2021), RAP(Yang et al. 2021a), ONION(Qi et al. 2020), CUBE(Cui et al. 2022), MF(Zhu et al. 2022), and STRIP(Gao et al. 2021), using the open-source code from OpenBackdoor. These defenses cover a broad range of strategies, including backdoor sample detection, backdoor model detection, and anti-backdoor learning. The implementation details for these backdoor defense methods are provided below:\n\u2022 BKI (Chen and Dai 2021) detects backdoor words during the inference stage, this method identifies output logits and marks those with significant contributions as potential trigger words. We used the optimal hyperparameter settings recommended in the original paper, with p set to 5 and a set to 0.1.\n\u2022 RAP (Yang et al. 2021a) detects backdoor samples during the inference stage by leveraging the robustness gap between clean and backdoor samples. We followed the optimal hyperparameter settings from the original paper, using 'tq' as the trigger and setting a probing range of [-0.3, -0.1] to achieve optimal defense performance.\n\u2022 ONION (Qi et al. 2020) detects backdoor words during both inference and training stages by evaluating the perplexity increase caused by trigger words. We adopted the optimal settings, with the threshold set to 0.\n\u2022 CUBE (Cui et al. 2022) removes backdoor samples through a three-step process: mapping data into an embedding space, applying dimensionality reduction, and clustering to identify poisoned samples. We reproduced the experimental results using the code from OpenBackdoor.\n\u2022 MF (Zhu et al. 2022) aims to reduce backdoor learning by moderating overfitting. We reproduced their results using the low-rank reparameterization method (LoRA).\n\u2022 Strip (Gao et al. 2021) detects backdoor words during the inference stage by comparing the perturbation sensitivity between clean and poisoned text. We followed the optimal hyperparameter settings suggested in the original paper, repeating the process five times with a false rejection rate of 0.01."}, {"title": "Analysis of Backdoor Parameters Detection", "content": "Here, we provide a theoretical analysis for BTU defense. Formally, backdoor learning can be viewed as a dual-optimization with a clean task and a backdoor task. The optimization of backdoor attack on embedding parameters can be re-defined as follows:\n$\\mathbf{\\epsilon}_{new} = \\mathbf{\\epsilon} + \\sum_{i=1}^{step} \\alpha$\n$\\mathbf{\\epsilon}_{new}^{b} = \\mathbf{\\epsilon}^{b} + \\sum_{i=1}^{step} \\alpha + \\sum_{i=1}^{step'} \\alpha$\nwhere $\\alpha$ denotes the gradient at each step, and $\\mathbf{\\epsilon}_{new}$ and $\\mathbf{\\epsilon}_{new}^{b}$ represent the clean token embedding parameters (CTP) and backdoor token embedding parameters (BTP) respectively. We aim to distinguish between BTP and CTP by analyzing the change in parameters. To measure the change in word embedding parameters before and after training, we use Euclidean distance as the metric. The specific calculation method is as follows:\n$d_c = d(\\mathbf{\\epsilon}_{new}, \\mathbf{\\epsilon}) \\approx ||\\sum_{i=1}^{step} \\alpha||_2$\n$d_b = d(\\mathbf{\\epsilon}_{new}^{b}, \\mathbf{\\epsilon}^{b}) \\approx ||\\sum_{i=1}^{step} \\alpha + \\sum_{i=1}^{step'} \\alpha||_2$\nwhere d represents the Euclidean distance. As the BTP experiences a more significant change compared to CTP, it causes effective backdoor parameter detection in our BTU defense."}, {"title": "Results of More Layers", "content": "To investigate whether parameter changes in other model layers are more closely linked to backdoor behavior, we conducted experiments on the BERT model using the SST-2 dataset, focusing on Add-Word and SynBkd backdoor attacks. The analysis targeted the five bottom layers of the model, as previous research suggests that the lower layers are more likely to capture backdoor features [1]. During the experiments, we zeroed in parameters in each layer that exhibited changes that exceeded the mean parameter change for that layer after backdoor training. We then evaluated both the attack success rate (ASR) and the clean accuracy (ACC). The results are shown in Table 10. The findings reveal that, aside from the word embedding layer, parameter changes in other layers are not strongly correlated with the presence of a backdoor."}, {"title": "Results of More Datasets", "content": "We expanded our experiments to include two additional datasets: Offens (Zampieri et al. 2019b) and the Internet Movie Database (IMDB) (Maas et al. 2011) to further evaluate the effectiveness of our method. The results presented in Table 11 demonstrate that BTU performs well in multiple datasets."}, {"title": "Algorithm 1: BTU: Backdoor Token Unlearning", "content": "Stage 1: Backdoor Token Detection\nInput: Dataset: D, PLM: M = (\u03b5, ..., cls), threshold: \u03b1\nOutput: Backdoor token set T\nLet T = T\u2081 = T2 = T3 = \u03c6, \u03b5 denotes embedding.\nTrain \u03b5 of M by D \u2192 \u03b5'.\nGet T\u2081 by Eq. 3\nSort s in set T\u2081 in descending order.\nExtract the top \u03b1% of tokens as T' = {ti}xa\nTrain \u03b5 of M* = (\u03b5, c) by D \u2192 \u03b5''.\nGet T2 by Eq. 4\nSort s in set T2 in descending order.\nExtract the top \u03b1% of tokens as T'' = {ti}xa\nTrain \u03b5 of M* by D/T'' \u2192 \u03b5'''.\nGet T3 by Eq. 5\nSort s' in set T2 in descending order.\nExtract the top \u03b1% of tokens as T''' = {ti}xa\nreturn T = T' \u222a T'' \u222a T'''\nStage 2: Dimensional Fine-gained Unlearning\nInput: T, padding token p, backdoored model Mb, clean\ndataset D'\nOutput: clean model Mc\nGet \u03b5b from Mb, compute \u03b4 by Eq. 6.\nLet \u03b5c = \u03b5b.\nfor t in T do\nif |\u03b5(t) \u2212 \u03b5i(t)| < \u03b4;\n\u03b5i(t) =.\nif |\u03b5(t) \u2212 \u03b5i(t)| \u2265 \u03b4;\nend for\nReplace Mb embedding with \u03b5 to get Mc\nFine-tune the Mc by D'\nreturn M"}]}