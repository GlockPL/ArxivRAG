{"title": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables", "authors": ["Luis Antonio Guti\u00e9rrez Guanilo", "Mir Tafseer Nayeem", "Cristian L\u00f3pez", "Davood Rafiei"], "abstract": "Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets. To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries. Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data. Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments. Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows. This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges\u00b9.", "sections": [{"title": "1 Introduction", "content": "E-commerce relies heavily on tabular data, such as product details and features, while user interactions, including assistant agents and Q&A, predominantly occur in natural language. This disparity underscores the need for models that can effectively parse tabular data and engage users through coherent, context-aware communication (Zhao et al., 2023b). Table-to-text generation addresses this challenge by transforming structured data into natural language, enabling applications such as product reviews, personalized descriptions, and tailored summaries in e-commerce. Beyond e-commerce, this capacity extends to domains such as healthcare, where structured patient records are converted into concise summaries for doctors (He et al., 2023), and finance, where tabular financial data is transformed into analytical reports (Varshney, 2024). However, generating text that is coherent, contextually relevant, and aligned with user-specific requirements remains a significant challenge, particularly for user- or query-centric tasks that demand domain-specific knowledge. Existing table-to-text datasets often focus on general-purpose applications and lack the depth required for specialized domains. For instance, datasets like QTSumm (Zhao et al.,"}, {"title": "2 Related Work", "content": "Table-to-Text Generation Table-to-text generation has advanced through datasets tailored to diverse domains and applications, as summarized in Table 1. Early efforts, such as WikiTableT (Chen et al., 2021), focused on generating natural language descriptions from Wikipedia tables, while TabFact (Chen et al., 2020b) introduced fact-checking capabilities and ROTOWIRE (Wiseman et al., 2017) generated detailed sports summaries. However, these datasets are limited in their relevance to product-specific domains. Later datasets like LogicNLG (Chen et al., 2020a) emphasized logical inference and reasoning, and ToTTo (Parikh et al., 2020) supported controlled text generation by focusing on specific table regions. HiTab (Cheng et al., 2022) extended these capabilities with hierarchical table structures and reasoning operators. Despite these advancements, none of these datasets provide the contextual and attribute-specific depth necessary for e-commerce applications, where generating meaningful descriptions requires reasoning across heterogeneous attributes, such as linking battery capacity to battery life or associating display size with user experience.\nQuery-Focused Summarization (QFS) Advances in text summarization have improved multi-document summarization through abstractive methods like paraphrastic fusion (Nayeem and Chali, 2017b; Nayeem et al., 2018), compression (Nayeem et al., 2019; Chowdhury et al., 2021), and diverse fusion models (Fuad et al., 2019; Nayeem, 2017), among others (Nayeem and Chali, 2017a; Chali et al., 2017). These approaches lay the groundwork for query-focused summarization (QFS), which tailors summaries to user-specific queries. Initially formulated as a document summarization task, QFS aims to generate summaries tailored to specific user queries (Dang, 2006). Despite its potential real-world applications, QFS remains a challenging task due to the lack of datasets. In the textual domain, QFS has been explored in multi-document settings (Giorgi et al., 2023) and meeting summarization (Zhong et al., 2021). Recent"}, {"title": "3 eC-Tab2Text: Dataset Construction", "content": "To address the gap in table-to-text generation for user-specific aspects or queries, such as \u201cCamera\" and \"Design & Display\u201d (as illustrated in Figure 1), we developed the eC-Tab2Text dataset. This dataset comprises e-commerce product tables and is designed to facilitate aspect-based text generation by fine-tuning LLMs on our dataset. The pipeline for creating eC-Tab2Text is outlined in Figure 2 and described in detail below.\nData Sources The dataset was constructed using product reviews and specifications (i.e., tables) extracted from the Pricebaba website\u00b2. Pricebaba provides comprehensive information on electronic products, including mobile phones and laptops. For this study, the focus was exclusively on mobile phone data due to the richness of product specifications (attribute-value pairs) and the availability of detailed expert reviews as summaries. Additionally, the number of samples available for mobile phones is significantly larger than for laptops. Each sample includes feature-specific details such as camera performance, battery life, and display quality.\nData Extraction and Format Data extraction was performed using web scraping techniques, with the extracted data stored in JSON format to serialize the table structure and to ensure compatibility with modern data processing workflows. Two JSON files were generated (Appendix E): one containing aspect-based product reviews and the other containing product specifications. The review JSON file captures user aspects alongside their associated textual descriptions collected from the \"Quick Review\" section of the website, while the specifications JSON file stores key-value pairs for both key specifications and full technical details. The structures of the sample inputs and outputs are depicted in Figures 3 and 4 in the Appendix.\nData Cleaning, Normalization, and Integration To ensure consistency, usability, and completeness, the extracted data underwent rigorous cleaning, normalization, and integration, similar to previous approaches (Nayeem and Rafiei, 2023, 2024a,b). The process includes (1) standardizing all text values to lowercase for uniformity, (2) replacing special"}, {"title": "4 eC-Tab2Text: Models", "content": "This section outlines the methodology for table serialization and provides details on the selection and fine-tuning of LLMs using our dataset.\nTable Serialization The representation of tabular data in machine learning has been addressed through various serialization techniques, including markdown format, comma-separated values (CSV), HTML (Fang et al., 2024; Singha et al., 2023), and LaTeX (Jaitly et al., 2023). However, for our specific problem involving semi-structured tables with nested structures, we adopt JSON serialization. This approach effectively addresses two critical needs: (1) representing the nested structures inherent in product tables and (2) enabling query-specific generation and evaluation (Gao et al., 2024).\nIn our eC-Tab2Text dataset, both input tables and query-specific outputs are serialized using JSON. The input JSON captures structured product specifications, while the output JSON aligns queries (e.g., \"Design and Display\u201d or \u201cBattery\u201d) as keys and their corresponding generated texts as values. This unified representation facilitates efficient querying and maintains alignment between inputs and outputs, ensuring consistency across the dataset. Additional implementation details can be found in Appendix D (Listing 7 prompt).\nModel Selection and Characteristics To evaluate the effectiveness of the eC-Tab2Text dataset, we fine-tuned three open-source LLMs: LLaMA 2-Chat 7B (Touvron et al., 2023), Mistral 7B-Instruct (Jiang et al., 2023), and StructLM 7B (Zhuang et al., 2024). These models were selected due to their distinct pretraining paradigms, which address diverse data modalities and tasks. Detailed descriptions of these models are provided in Appendix B and summarized below."}, {"title": "5 Evaluation", "content": "In this section, we evaluate the performance of the eC-Tab2Text models described in Section 4 along with several closed-source models, including GPT-40-mini and Gemini-1.5-flash. The evaluation follows standard metrics commonly used in table-to-text generation, as outlined in (Zhao et al., 2023a). These metrics include BLEU (Reiter, 2018), the F-1 scores of ROUGE-1 and ROUGE-L (Ganesan, 2018), METEOR (Dobre, 2015), and BERTScore (Zhang* et al., 2020), following (Akash et al., 2023;"}]}