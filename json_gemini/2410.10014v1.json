{"title": "Safety-Aware Fine-Tuning of Large Language Models", "authors": ["Hyeong Kyu Choi", "Xuefeng Du", "Yixuan Li"], "abstract": "Fine-tuning Large Language Models (LLMs) has emerged as a common practice for tailoring models to individual needs and preferences. The choice of datasets for fine-tuning can be diverse, introducing safety concerns regarding the potential inclusion of harmful data samples. Manually filtering or avoiding such samples, however, can be labor-intensive and subjective. To address these difficulties, we propose a novel Safety-Aware Fine-Tuning (SAFT) framework designed to automatically detect and remove potentially harmful data, by leveraging a scoring function that exploits the subspace information of harmful and benign samples. Experimental results demonstrate the efficacy of SAFT across different LLMs and varying contamination rates, achieving reductions in harmfulness of up to 27.8%. Going beyond, we delve into the mechanism of our approach and validate its versatility in addressing practical challenges in real-world scenarios. Disclaimer: This paper may contain offensive qualitative examples; reader discretion is advised.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [1-6] have emerged as a powerful foundation for building personalized models tailored to individual needs and purposes. To enable customization, a pre-trained LLM typically undergoes supervised fine-tuning, a process that allows LLMs to adapt and specialize based on task-specific data [7, 4, 8]. While fine-tuning enables LLMs to improve their performance on custom datasets, it also poses safety concerns when harmful samples arise in the fine-tuning data. For instance, consider a scenario where a conversational agent is being fine-tuned on user interactions from social media platforms. These interactions often contain a mixture of benign and potentially harmful content, such as hate speech, misinformation, or inappropriate language. Consequently, fine-tuning LLMs on data containing objectionable content could adversely affect the model's behavior.\nTo formalize the problem, we consider a generalized characterization of the fine-tuning data, which can be modeled as a mixed composition of two distributions:\n$\\mathbb{P} = \\lambda \\mathbb{P}_{harmful} + (1 - \\lambda) \\mathbb{P}_{benign}$,\nwhere $\\mathbb{P}_{harmful}$ and $\\mathbb{P}_{benign}$ respectively denote the distribution of harmful and benign data, and $\\lambda$ is the mixing ratio. Such mixture of data can naturally arise in numerous real-world applications and is more realistic than requiring a fully benign set for fine-tuning. However, as demonstrated by our experiment in Section 3, even a small amount of harmful samples mixed into the benign fine-tuning data can severely compromise the model's safety performance, and relevant concerns have been observed in recent works [8-11] as well. Addressing this problem is challenging due to the lack of clear membership (benign or harmful) for samples in the dataset. To make matters worse, the manual process of filtering or avoiding harmful data is often labor-intensive and subjective, relying on the judgment of crowd workers."}, {"title": "2 Preliminaries", "content": "We use Huber contamination model [12] to characterize the underlying data as a mixture of benign data distribution $\\mathbb{P}_{benign}$ and harmful data distribution $\\mathbb{P}_{harmful}$. Then, this is formalized as follows.\nDefinition 2.1 (Fine-tuning data distribution) We define the fine-tuning data distribution to be the following mixture of distributions\n$\\mathbb{P} = \\lambda \\mathbb{P}_{harmful} + (1 - \\lambda) \\mathbb{P}_{benign},$ (1)\nwhere $\\lambda \\in [0, 1]$ is the contamination ratio. When $\\lambda = 0$, our formulation generalizes to the ideal situation when no harmful data occurs.\nDefinition 2.2 (Empirical training data) An empirical training set $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ is sampled i.i.d. from this mixture distribution $\\mathbb{P}$, where $N$ is the number of samples. For each sample $(x_i, y_i)$, $x_i$ denotes an input prompt to the large language model, and $y_i$ represents the corresponding target. Note that we do not have clear membership (benign or harmful) for the samples in $\\mathcal{D}$."}, {"title": "3 How Does Harmful Data Impact Fine-tuning?", "content": "In this section, we explore the impact of harmful data on the fine-tuning process of LLMs, highlighting its implications for model performance.\nSetup. We design our experiments with the Beavertails dataset [17], which consists of question-answer pairs labeled as harmful or benign. Each sample comprises a one-turn dialogue. We construct the fine-tuning dataset of 3,000 samples under various contamination ratios, denoted by $\\lambda = \\{0.1, 0.15, 0.2, 0.25, 0.3\\}$, where higher values of $\\lambda$ indicate more pronounced contamination. For each $\\lambda$, we fine-tune the Llama-2-chat model [2] with low-rank adaptation [18]. The hyperparameters are summarized in Appendix A. For each fine-tuned model, we evaluate the performance based on two criteria:\n\u2022 Harmfulness Score (HS) \u2193: We assess the harmfulness of the model using an auxiliary moderation model from [17], which predicts each generated response to be harmful or benign. The fraction of harmful responses among all query responses is computed as the Harmfulness Score.\n\u2022 Helpfulness Score \u2191: For each test input query labeled 'benign', we measure the sentence-level similarity between the model's generation $\\hat{y}$ and the ground truth response $y$. That is, we assume 'benign' ground truth responses to be helpful. The similarity score is calculated based on BLEURT [19] and the ROUGE-L [20] metric. A higher score indicates more helpfulness.\nObservation. In Figure 2 (a), we illustrate that fine-tuning on the mixed data can notably increase the harmfulness score even with a minor ratio of harmful data mixed (e.g., $\\lambda = 0.1$), and this degradation worsens as the proportion of harmful data increases. Another observation is that the helpfulness score is not significantly affected regardless of the harmful ratio. For example, when $\\lambda = 0.3$, the helpfulness score under BLEURT is 0.504, which closely matches the score fine-tuned on the pure benign samples (0.511). Hence, the adverse effects of harmful data present in $\\mathcal{D}$ may remain unaware if our focus is primarily on evaluating the fine-tuning performance of the main task (i.e., generating helpful responses). Our observation corroborates with findings in [11], motivating our framework on safety-aware fine-tuning for large language models."}, {"title": "4 Safety-Aware Fine-Tuning", "content": "Safety-aware fine-tuning aims to adapt a language model to the task at hand defined by $\\mathcal{D}$ while withstanding the influence of harmful data in it. Our goal is to ensure a decreased harmfulness of"}, {"title": "4.1 Harmful Data Detection", "content": "Harmful data detection refers to the step of identifying and flagging harmful data instances within a mixture dataset comprising both benign and harmful samples. The ability to effectively detect harmful data relies heavily on whether the language model's representations can capture information related to harmfulness. Our idea is that if we could identify a direction or a subspace in the activation space associated with harmful statements, then we might be able to detect and separate the harmful data from the rest.\nEmbedding factorization. To operationalize the idea, we first extract embeddings from the language model for samples in the dataset $\\mathcal{D}$. Specifically, let $Z \\in \\mathbb{R}^{N \\times d}$ denote the matrix of embeddings extracted from the language model for samples in $\\mathcal{D}$, where each row represents the embedding vector $z_i$ of a data sample $x_i$. To identify the harmfulness direction, we perform singular value decomposition:\n$\\mathbb{Z}_i := \\mathbb{Z}_i - \\mu \\\\ Z = U\\Sigma V^T,$ (4)\nwhere $\\mu \\in \\mathbb{R}^d$ is the average embeddings across all $N$ samples, which is used to center the embedding matrix. The columns of $U$ and $V$ are the left and right singular vectors that form an orthonormal basis, and $\\Sigma$ is a diagonal matrix. Such a factorization of matrix $Z$ is useful, because it enables finding the best representation with a $k$-dimensional subspace for the set of points in $\\mathcal{D}$.\nTo gain insight, we begin with a special case of the problem where the subspace is 1-dimensional, a line through the origin. Finding the best-fitting line through the origin with respect to a set of points $\\{z_i | 1 \\leq i \\leq N\\}$ means minimizing the sum of the squared distances of the points to the line. Here, distance is measured perpendicular to the line. Geometrically, finding the first singular vector $v_1$ is also equivalent to maximizing the total distance from the projected embedding (onto the direction of"}, {"title": "4.2 Fine-Tuning with Filtered Data", "content": "Based on the filtering score defined in Equation 7, we regard $\\mathcal{D}_{filtered} = \\{(x_i, y_i) \\in \\mathcal{D} : s_i < \\tau\\}$ as the training set. This dataset is used to fine-tune the model using the following objective:\n$\\min_\\theta E_{(x,y) \\in \\mathcal{D}_{filtered}} L(f(x;\\theta),y),$ (8)\nwhere $\\theta$ is the parameterization of the model."}, {"title": "5 Experiments", "content": "5.1 Setup\nDatasets and models. Same as our setup described in Section 3, we evaluate the effectiveness of our method using the Beavertails dataset [17]. Each sample includes a prompt and one response, and the sample is either labeled as benign or harmful. We construct the contaminated fine-tuning dataset of 3000 samples under various mixing ratios, denoted by $\\lambda = \\{0.1, 0.15, 0.2, 0.25, 0.3\\}$. The magnitude of $\\lambda$ is moderately small to reflect the real-world scenario where the majority of samples remain benign. The filtering threshold $\\tau$ and subspace dimensionality $k$ are validated on a held-out set with labeled 100 samples. We evaluate the trained models on the standard test set of Beavertail. Furthermore, we adopt commonly used open-sourced models, Llama-2-7B [2] and Vicuna-7B [4]. Specifically, we use the 'llama-2-7b-chat-hf' and 'vicuna-7b-v1.5' versions. We fine-tune both models for 4 epochs with a learning rate of 2e-5. We employ Low-rank Adaptation (LoRA) for efficient training [18]. More details are provided in Appendix A.\nBaselines and metrics. We consider the following alternative approaches for fine-tuning and compare them with our method SAFT. (1) 'SFT' is a baseline trained on the entire dataset $\\mathcal{D}$ without any filtering. (2) 'Prompting' filters the harmful data by querying the large language model \"Is the following data sample harmful to use for fine-tuning? '[DIALOG]'. Respond only with Yes or No\". We then remove dialog samples that the LLM responds with 'Yes'. (3) 'Random' baseline randomly removes the same amount of samples as SAFT. We adopt the same metrics as defined in Section 3, namely the Harmfulness Score (HS) and the Helpfulness Score based on either BLEURT (BRT) or ROUGE-L (RL)."}, {"title": "5.2 Main Results", "content": "SAFT reduces harmfulness without compromising helpfulness. As shown in Table 1, our method consistently achieves a meaningful decrease in the harmfulness score across all experimental settings, compared to the standard SFT without filtering and baselines. For example, when $\\lambda = 0.25$, our safety-aware fine-tuning reduces the harmfulness score from 39.9 (SFT) to 28.8, a relative 27.8% decrease. This result suggests that employing our harmful data detection technique to filter out harmful data samples prior to fine-tuning effectively alleviates the harmfulness of the fine-tuned model. Moreover, we find that randomly removing an equivalent number of data samples as SAFT does not yield the same level of harmfulness reduction, further supporting the efficacy of our filtering approach. Lastly, the BLEURT (BRT) and ROUGE-L (RL) scores, which measure the helpfulness and quality of the model's outputs on benign samples, do not deviate significantly from other baselines. This indicates that our SAFT framework effectively decreases harmfulness without compromising the model's overall helpfulness and performance, a desirable outcome for practical applications."}, {"title": "6 Analyses", "content": "In this section, we delve deeper into the mechanism of safety-aware fine-tuning, and demonstrate its potential in different settings. Our analyses are geared toward addressing the following questions:\nRQ1. How well are harmful samples filtered? (Section 6.1)\nRQ2. Can SAFT effectively address practical challenges? (Section 6.2)\nRQ3. What are some qualitative aspects of SAFT? (Section 6.3)"}, {"title": "6.1 Soundness of SAFT", "content": "In this section, we validate the soundness of each component in SAFT. In particular, we study the accuracy of benign/harmful sample classification and the impact of the number of components k.\nAre harmful data accurately detected? Figure 3 presents a comparison of harmful data detection performance with baselines across various contamination ratios \u03bb. In our context, harmful data detection can be viewed as a binary classification task, where each sample is categorized as either harmful or benign. We evaluate performance using AUROC, a standard metric for quantifying binary classification performance. Higher AUROC values indicate superior harmful data detection performance and vice versa. Our findings indicate that our filtering approach consistently outperforms the baselines, more accurately identifying harmful samples. Particularly noteworthy is the inferior performance of the 'Prompting' baseline compared to random filtering, highlighting the unreliability of directly prompting large language models to identify harmful data. In contrast, our method leverages the internal activation space, which contains richer statistical information for harmful data detection than simply relying on the output of the language model. These results underscore the importance of meaningful harmful data detection in enhancing the overall performance of SAFT.\nWhere in the LLM is harmfulness represented? In Figure 4, we analyze the harmful data detection with embeddings extracted from different layers in the LLM. The AUROC values of benign/harmful classification are evaluated with Llama-2, and all other configurations are kept the same as our main experiments. We observe that the detection performance initially increases from the bottom to middle layers, and then decreases slightly. Overall, the embeddings extracted from the median layers (e.g., 15) lead to the best separability compared to earlier layers. The trend suggests that LLMs gradually capture the information in the context in the first few layers, and then condense them in the last layers to map to the vocabulary.\nThe effect of k subspace components. As described in Eq. (7), SAFT utilizes a subspace of k orthogonal singular vectors to define the filtering score. In this ablation study, we examine how the number of component vectors influences performance. Performance is assessed on the harmful data detection metrics: AUROC, F1 scores, Precision, and Recall. Table 3 presents performance metrics for varying values of k = {1,2,4,8,16,32}. Overall, we observe superior performance with a smaller value of k. For instance, on Llama-2, the best classification performance is achieved with k = 1, yielding an AUROC of 0.6868. This trend persists across all contamination scenarios with \u03bb = {0.10,0.15, 0.20, 0.25, 0.30}. These findings align with our assumption that harmful samples can be represented by a small subspace, indicating that only a few key directions in the activation space are capable of distinguishing harmful samples from benign ones."}, {"title": "6.2 Robustness to Practical Challenges", "content": "Safety-Aware Fine-Tuning is a practical framework that may potentially face real-world challenges. For instance, we explore how well SAFT deals with different data distributions of the fine-tuning dataset, and discuss its steerability.\nDoes SAFT generalize to different dataset distributions? To assess the generalizability of our proposed approach, we conduct an additional experiment using the Anthropic HH-RLHF dataset [16] as the fine-tuning task. Specifically, we randomly select 3,000 dialogues as the fine-tuning dataset, which contains a mixture of benign and harmful samples with unknown ratio. Meanwhile, we retain the same validation dataset and test query set from Beavertails dataset [17] for model selection and evaluation. This setup simulates whether SAFT performs robustly on a dataset from a new data distribution. In Table 4, we present the Harmfulness Scores and Helpfulness measures, BLEURT and ROUGE-L. The results show that while SFT has Harmfulness Score of 18.70, SAFT outperforms all baselines with a low score of 14.79.\nHow can we make SAFT steerable? Steerability is a useful feature in practical applications, offering flexibility to specific needs. For instance, if data are scant, we may need to filter less data even if it may compromise harmlessness. Conversely, a more conservative filtering might be desired in certain circumstances. In light of these considerations, we introduce a simple method to steer the behavior of SAFT. Recall that the detection is controlled by the threshold, \u0442 (Eq. (6)). By adjusting \u0442, we can alter the detection outcomes: increasing it results in fewer samples classified as 'harmful', while decreasing it has the opposite effect. In Figure 5, we demonstrate experimental results illustrating the impact of different steer rates, utilized as $\\tau \\times (1 + steer \\ rate)$. Notably, as the"}, {"title": "6.3 Qualitative Exploration", "content": "Qualitative examples of generated text. We provide qualitative examples of the model responses after fine-tuning with and without Safety-Aware Fine-Tuning, in Table 5. 'SFT Response' refers to the model that has been trained with SFT without any filtering. The examples are retrieved from the Llama-2 model. More examples are in Appendix D.\nCategory-wise effect of SAFT. For a more fine-grained understanding of SAFT, we evaluate the harmfulness score for the 14 harmfulness categories in [17]. In Figure 6, SAFT is compared with vanilla SFT. Notably, SAFT reduced the harmfulness scores of all categories. The categories that had the most reduction in harmfulness level are \u2018Controversial topics, politics' and 'Terrorism, organized crime', with approximately 48% to 49% decrease. The category with the least reduction, on the other hand, was 'Privacy Violation'."}, {"title": "7 Related Works", "content": "LLM representation space. To better understand and interpret large language models, there have been various attempts to extract meaningful information from LLM activations or layer parameters [22-25]. One such attempt is to probe LLM embeddings for truthfulness [26, 24, 27-29]. Our work differs by identifying latent knowledge related to harmfulness through an activation subspace. On the other hand, there have been works that focus on ways to extract embeddings regarding specific factual knowledge to edit [30, 31, 23, 32] or unlearn [33, 34] it from the LLM parameters. Others studied methods to detect toxicity or harmful content using the LLM embeddings [35, 25, 17, 36]. Different from these works, we investigate the problem of safety-aware fine-tuning, aiming to adapt a model to the task while withstanding the influence of harmful data.\nSupervised Fine-tuning. Many works have performed Supervised Fine-Tuning (SFT) to enhance LLMs' ability to follow instructions [7, 4], or customize behaviors or personality traits [8, 9, 37, 38]. Regarding what data samples to use for SFT, numerous works have revolved around efficiently and effectively selecting high-quality samples [39-44], while a handful of works considered the impact of harmful data samples in the dataset. Specifically, [45, 8] showed how data samples influence harmfulness of the model, whereas [10, 21] revealed the potential negative impact of benign samples on the safety of fine-tuned models. While some works considered methods to make the fine-tuning stage safe [11, 46], our SAFT differs by providing a more fundamental and direct way of mitigating malicious fine-tuning by detecting and removing harmful samples beforehand."}, {"title": "8 Conclusion", "content": "With the advent of pre-trained LLMs, adapting the models to individual needs and preferences has been one of the most intriguing goals. To accomplish this, Supervised Fine-Tuning (SFT) can be seamlessly applied with task-specific data. While SFT allows LLMs to improve their performance on custom datasets, safety concerns may arise when harmful samples are involved in the datasets. Manual curation of datasets to remove harmful samples is not only resource-intensive but also prone to subjectivity. To mitigate this challenge, we proposed a novel safety-aware fine-tuning (SAFT) framework designed to automatically detect and filter out potentially harmful samples, by leveraging the lower dimensional subspace representation of the dataset. Our experiments, conducted across diverse contamination rates and LLM families, demonstrated the effectiveness, simplicity, and flexibility of SAFT in practical scenarios. With SAFT, we pave the way for safer and more reliable usage of fine-tuned models tailored to individual needs."}, {"title": "4.1 Harmful Data Detection", "content": "$\\mathcal{V}_1 = arg \\max_{\\|V\\|^2 = 1} \\sum_{i=1}^N <\\mathbb{Z}_i, V_1>^2,$ (5)"}, {"title": "4.2 Fine-Tuning with Filtered Data", "content": "$\\begin{cases} 1, & \\text{if } s_i > \\tau \\\\ 0, & \\text{otherwise} \\end{cases}$ (6)"}, {"title": "5.1 Setup", "content": "$\\mathcal{S}_i = \\frac{1}{k} \\sum_{j=1}^k <\\mathbb{Z}_i, V_{ij}>^2,$ (7)"}]}