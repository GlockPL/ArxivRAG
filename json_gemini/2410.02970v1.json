{"title": "F-FIDELITY: A ROBUST FRAMEWORK FOR FAITHFULNESS EVALUATION OF EXPLAINABLE AI", "authors": ["Xu Zheng", "Farhad Shirani", "Zhuomin Chen", "Chaohao Lin", "Wei Cheng", "Wenbo Guo", "Dongsheng Luo"], "abstract": "Recent research has developed a number of eXplainable AI (XAI) techniques, such as gradient-based approaches, input perturbation-base methods, and black-box explanation methods. Although extracting meaningful insights from deep learning models, how to properly evaluate these XAI methods remains an open problem. The most widely used approach is to perturb or even remove what the XAI method considers to be the most important features in an input and observe the changes in the output prediction. This approach although efficient suffers the Out-of-Distribution (OOD) problem as the perturbed samples may no longer follow the original data distribution. A recent method RemOve And Retrain (ROAR) solves the OOD issue by retraining the model with perturbed samples guided by explanations. However, the training may not always converge given the distribution difference. Furthermore, using the model retrained based on XAI methods to evaluate these explainers may cause information leakage and thus lead to unfair comparisons. We propose Fine-tuned Fidelity (F-Fidelity), a robust evaluation framework for XAI, which utilizes i) an explanation-agnostic fine-tuning strategy, thus mitigating the information leakage issue and ii) a random masking operation that ensures that the removal step does not generate an OOD input. We designed controlled experiments with state-of-the-art (SOTA) explainers and their degraded version to verify the correctness of our framework. We conducted experiments on multiple data structures, such as images, time series, and natural language. The results demonstrate that F-Fidelity significantly improves upon prior evaluation metrics in recovering the ground-truth ranking of the explainers. Furthermore, we show both theoretically and empirically that, given a faithful explainer, F-Fidelity metric can be used to compute the sparsity of influential input components, i.e., to extract the true explanation size.", "sections": [{"title": "1 INTRODUCTION", "content": "EXplainable AI (XAI) methods have been widely used in many domains, such as Computer Vision (CV) (Chattopadhay et al., 2018; Smilkov et al., 2017; Jiang et al., 2021; Zhou et al., 2016; Selvaraju et al., 2017), Neural Language Processing (NLP) (Lyu et al., 2024; Luo et al., 2024; Zhao et al., 2024), Graph Neural Networks (GNNs) (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2020; Vu & Thai, 2020), and Time Series (Liu et al.; Queen et al., 2024). There are various types of explanation methods, in which the most predominant one is post-hoc instance-level explanation. Given a pre-trained classifier and a specific input, these methods aim to identify the most important features of the model's output. For instance, such explanations map to a subset of important pixels in image classification (Ribeiro et al., 2016; Selvaraju et al., 2017; Lundberg, 2017). Existing research has proposed a number of XAI methods to draw post-hoc explanations such as Integrated Gradients (Sundararajan et al., 2017), CAM-based approaches (Selvaraju et al., 2017), and SmoothGrad (Smilkov et al., 2017)."}, {"title": "2 PRELIMINARIES", "content": "This section introduces the notation and key background concepts used throughout the paper."}, {"title": "2.1 NOTATION", "content": "Sets are denoted by calligraphic letters such as X. The set {1,2,\u2026\u2026,n} is represented by [n]. Multidimensional arrays (tensors) are denoted by bold-face letters such as x. Upper-case letters such as X represent random variables, and lower-case letters such as x represent their realizations. Similarly, random tensors are denoted by upper-case letters such as X."}, {"title": "2.2 CLASSIFICATION MODELS AND EXPLANATION FUNCTIONS", "content": "Let f: X \u2192 Y be a (pre-trained) classification model such as a neural network which takes an input X \u2208 Rt\u00d7d and outputs a label Y \u2208 Y, where Y is a finite set of labels. In CV tasks, t = h \u00d7 w, where h, w are the height and width, and d is the number of channels per input pixel. Analogously, in NLP and time series classification tasks, t \u2208 N represents the time index, and d is the feature dimension. An explanation function (explainer) consists of a pair of mappings \u03c8 = (\u03c6, \u03be), where $ : X \u2192 Rtd is the score function, mapping each input element to its (non-negative) important score, and a mask function \u00a7 : \u03c6(X) \u2192 M, mapping the output of the score function to a binary mask M \u2208 {0,1}t\u00d7d . The masked input X\u00d3M is called the explanation for the input X and model f(\u00b7), where  represents elementwise multiplication. We denote the explanation size by S = ||M||1, where || \u00b7 ||1 is the l\u2081 norm. That is, the explanation size S is the number of non-zero elements of M. In general, the size may be deterministically set to a constant value s, or alternatively, it may depend on the output of the score function, e.g., input elements receiving a score higher than a given threshold are included in the mask and the rest are removed. Let us assume that the (ground-truth) data distribution is Px,y. Then, a 'good' explainer is one which minimizes the total variation distance drv (Py\\x, Py\\x\u00a9M), while satisfying an explanation size constraint Ex(||M||1) \u2264 s, where s \u2208 N is the desired average explanation size. The minimization of the total variation essentially enforces that the posterior distribution of the classifier output be mostly determined by the masked input explanation, implying that the subset of input components which are removed by the mask have a low influence on the classifier output."}, {"title": "2.3 QUANTIFYING THE PERFORMANCE OF EXPLAINERS", "content": "A key challenge in explainability research is to quantify and compare the performance of various explainers. The performance of an explainer can be formally quantified in terms the total variation distance drv (PY\\x, Py|x\u00a9M) as a function of the average explanation size Ex(||M||1). However, in most problems of interest, the underlying statistics Px,y is not available, and hence direct evaluation of the aforementioned total variation distance is not possible. As discussed in the introduction, some datasets are accompanied by ground-truth explanations, which enables the use of measures such as AUC and IoU for evaluating the quality explainers. However, the ground-truth explanations are available only for a limited collection of datasets, and even when ground-truth explanations are available, they may not accurately reflect the model's internal decision-making processes. To address the aforementioned issues, a widely used set of metrics have been proposed in the literature which are based on the removal strategy. In CV, two removal orders have been considered (Samek et al., 2016; Yuan et al., 2022), MoRF, where the removal process begins with the most influential pixels, and LeRF, where it starts with the least influential pixels. In the graph domain, an alternative removal strategy is used, where first the size of explanation subgraphs is determined according to a sparsity parameter, and then edges are removed either from the explanation subgraph of the desired size, or the non-explanation subgraph which is its complement. In this paper, we use metrics which"}, {"title": "3 ROBUST FIDELITY VIA MODEL FINE-TUNING AND BOUNDED REMOVALS", "content": "As discussed in the previous section, a significant limitation of prior explanation evaluation metrics is the loss in accuracy due to the OOD nature of the modified inputs generated by the application of removal strategies. For instance, the probability difference P(Y = f(X))\u2212P(Y = f(X-X\u2299M)) may be large, even for low-quality explanations. This occurs because the modified input X \u2013 XOM is OOD for the trained classifier f(\u00b7),despite Py|x and Py|x-x\u00a9M being close to each other. Consequently, this yields a high Fid+ score despite the explanation's low quality with respect to the theoretically justified total variation metric. For example, in the empirical evaluations provided in the next sections, we demonstrate that the Fid measure sometimes assigns better evaluations to completely random explainers than to those whose outputs align with ground-truth explanations.\nA partial solution in the graph domain addresses this issue by removing only an a+ fraction of the explanation subgraph and a fraction of the non-explanation subgraphs (Zheng et al., 2023). However, we argue that two issues degrade the evaluation quality of the RFid metric. First, if the original explanation size M is large (small), then removing an a\u207a (a\u00af) of the explanation (non-explanation) part of the input, this would still yield OOD inputs. Second, the classifier may lack robustness and produce unreliable outputs even when the input is only slightly perturbed.\nTo address the first issue, we modify the RFid metric so that the size of the removed part of the input is upper-bounded by a constant fraction of the entire input, rather than a fraction of the explanation,"}, {"title": "4 EMPIRICAL VERIFICATION OF ROBUSTNESS", "content": "To demonstrate the robustness of our proposed F-Fidelity framework, we conduct comprehensive experiments across multiple domains, including image classification, time series analysis, and natural language processing. Our evaluation strategy builds upon the concept introduced by Rong et al. (2022), which posits that an ideal evaluation method should yield consistent rankings in both MORF and LeRF settings. We introduce a novel approach for a fair comparison using a degradation operation on a good explanation, such as an explanation obtained by Integrated Gradients (IG) (Sundararajan et al., 2017), generating a series of explanations with varying levels of random noise. This method provides a clear ground truth (GT) ranking based on noise level, allowing for controlled evaluation.\nWe evaluate the performance of F-Fidelity against established baselines such as Fidelity (Fidelity), ROAR (Hooker et al., 2019), and R-Fidelity (Zheng et al., 2023) across a wide range of sparsity levels, from 5% to 95% at 5% intervals. Throughout our evaluation, we focus on three key Spearman rank correlations: \"MoRF vs. GT\", \"LeRF vs. GT\", and \"MoRF vs. LeRF\". It allows us to assess the methods' performance under various conditions, from highly sparse to nearly complete explanations. To provide a thorough analysis, we employ both macro and micro correlation metrics:\n\u2022 Macro Correlation (Rong et al., 2022; Pan et al., 2021; Zhu et al., 2024): We compute the AUC with respect to sparsity across the entire 5-95% range for each explanation method. The macro correlations are then calculated using these AUC values, providing an overall performance measure across all sparsity levels.\n\u2022 Micro Correlation: To capture fine-grained performance differences, we calculate micro correlations at each sparsity level. In the main body of the paper, we report the averaged micro correlations across all sparsity levels, as well as the average rank of each method."}, {"title": "4.1 IMAGE CLASSIFICATION EXPLANATION EVALUATION", "content": "Setup. We use Cifar-100 (Krizhevsky et al., 2009) and Tiny-Imagenet (Deng et al., 2009), as the benchmark datasets. To obtain a pre-trained model to be explained, we adopt ResNet (He et al., 2016). More experiments with Vision Transformer(ViT) (Dosovitskiy et al., 2020) can be found in Appendix D.4. To generate different explanations, we first use two explanation methods, SmoothGrad Squared (SG-SQ) (Smilkov et al., 2017) and GradCAM (Selvaraju et al., 2017)"}, {"title": "4.2 TIME SERIES CLASSIFICATION EXPLANATION EVALUATION", "content": "Setup. We use two benchmark datasets for time series analysis: PAM for human activity recognition and Boiler for mechanical fault detection (Queen, 2023). For PAM, we use 534 samples across 8 activity classes, with each sample recorded using a fixed segment window length of 600 from 17 sensors. The Boiler dataset, used for mechanical fault detection, consists of 400 samples with 20 dimensions and a fixed segment window length of 36. We employ IG from the Captum library 2 to obtain initial explanations. To generate different explanations, we apply noise perturbations to the importance of each timestamp, using proportions of [0.0,0.1, 0.2, 0.3, 0.4, 0.5] for PAM and [0.0, 0.2, 0.4, 0.6, 0.8, 1.0] for Boiler. For R-Fidelity and F-Fidelity, we set a+ = a\u00ae = 0.5. Other settings remain consistent with the image classification task to ensure comparability across domains."}, {"title": "5 DETERMINING THE EXPLANATION SIZE VIA FIDELITY METRICS", "content": "In many machine learning tasks, such as image classification and NLP, the ground truth explanations can be discretized into distinct clusters representing different levels of importance. For instance, in image classification, pixels associated with the target object tend to receive high importance scores. Conversely, pixels corresponding to the background or irrelevant regions receive low scores. However, in many practical scenarios, even good explainers that produce accurate explanation masks as measured by the Fid and RFid evaluation metrics may yield explanation scores that are not discretized into distinct clusters. For instance, when using PGExplainer on the MUTAG dataset (Luo et al., 2020), the explainer achieves over 90% accuracy in correctly ranking the influential edges of the input graph. Yet, the explanation scores are not clustered around discrete values. As a result, the size of the ground-truth explanation cannot be inferred from the explainer's output since it provides only a ranking of edge importance without a clear separation into important and non-important edges.\nIn this section, we theoretically demonstrate that our proposed evaluation metric can recover the cluster sizes given an explainer that outputs the correct explanation mask (i.e., correctly ranks the importance of input elements). Thus, provided the explainer outputs an accurate mask function, our metric can recover the explanation size (also known as sparsity). To provide a concrete theoretical analysis, we first consider a classification problem under a set of idealized assumptions that generalize the above observations on the clustering of explanation scores. In the subsequent sections, we provide empirical evaluations demonstrating the applicability of our theoretical results to real-world settings.\nSpecifically, let us consider a classification task defined by a joint distribution Px,y and a classifier f:xy. We assume that the input elements can be partitioned into several influence tiers. That is, for any given input x, there exists a partition Ck (x), k \u2208 [r] of the index set [t] \u00d7 [d], where Ck (x) represents the set of indices of the input elements belonging to tier k, and Ck = |Ck(x)| are the (fixed) tier sizes. For a given mask m, the probability of correct classification based on the masked input x m depends only on the counts of unmasked elements in each influence tier. Formally,\nP(Yxm) = g(j1, 12, ..., \u0130r),\nwhere g: [C1] \u00d7 [C2] \u00d7 \u00d7 [cr] \u2192 [0, 1] is a function monotonically increasing with respect to the lexicographic ordering on its input, and jk \u2208 [Ck] is the number of elements in Ck (x) whose"}, {"title": "6 EMPRICAL VERIFICATION OF DETERMINING THE EXPLANATION SIZE", "content": "In this section, we demonstrate empirically that the FFid metric can be used to deduce the size or sparsity of explanations, complementing the theoretical analysis of Section 5. Specifically, we consider the colored-MNIST dataset (Arjovsky et al., 2019), where the explanation corresponds to the pixels representing the digit in each image. To control the explanation size, we rescale and crop the image samples so that the digit pixels occupy \u03b3 \u2208 {0.1,0.15, 0.2, 0.25} fraction of the total image area. Our goal is to show empirically that the FFid metric can recover the value of \u03b3, thus revealing the explanation size. A three-convolution-layer model is used as the pre-trained model.\nIn the terminology of Section 5, the pixels can be divided into two tiers: those belonging to the digit, which form the explanation, and those belonging to the background, which do not. The explanation size is thus c1 = ytd. According to Theorem 1, we expect FFid+ to increase for s < C\u2081 and"}, {"title": "7 RELATED WORK", "content": "Existing methods for evaluating explanations can be generally divided into two categories according to whether ground truth explanations are available. Comparing to the ground truth is an intuitive way for explanation evaluation. For example, in time series data and graph data, there exist some synthetic datasets for explanation evaluation, including BA-Shapes (Ying et al., 2019), BA-Motifs (Luo et al., 2020), FreqShapes, SeqComb-UV, SeqComb-MV, and Low Var (Queen et al., 2024; Liu et al.). In computer Vision and Natural Language Processing, the important parts can also be obtained with human annotation. However, these methods suffer from the heavy labor of ground truth annotation.\nThe second category of evaluation methods assesses the explanation quality by comparing model outputs between the original input and inputs modified based on the generated explanations Zheng et al. (2023). For example, Class Activation Mapping (CAM) (Zhou et al., 2016) first compares the classification performance between the original and their GAP networks, which makes sure the explanation is faithful for the original network. Grad-CAM(Selvaraju et al., 2017) uses image occlusion to measure faithfulness. In the following work, Grad-CAM++ (Chattopadhay et al., 2018) uses three metrics to evaluate the performance of explanation methods, \u201cAverage Drop %", "% Increase in Confidence\u201d, and \u201cWin %": "In Adversarial Gradient Integration (Pan et al., 2021; Petsiuk et al., 2018), the authors use \u201cDeletion Score\u201d and \u201cInsertion Score\" to measure the faithfulness, where \"Deletion Score\" is to delete the attributions from the original input and \u201cInsertion Score\" is to insert attributions into one blank input according to the explanations. In (Samek et al., 2016), the LeRF/MORF method is proposed to measure if the importance is consistent with the accuracy of the model. However, this group of metrics does not consider the effect of the OOD issues. In (Hooker et al., 2019), the author proposed a new evaluation method ROAR to calculate the accuracy, which avoids the OOD problem by using retrain. To overcome the disadvantage of ROAR, ROAD (Rong et al., 2022) was introduced to solve information leakage and time-consuming issues.\nIn the field of NLP, various methods are employed for evaluating faithfulness, as outlined in (Jacovi & Goldberg, 2020). These methods include axiomatic evaluation, predictive power evaluation, robustness evaluation, and perturbation-based evaluation, among others. Axiomatic evaluation involves testing explanations based on predefined principles (Jacovi & Goldberg, 2020; Adebayo et al., 2018; Liu et al., 2022; Wiegreffe et al., 2020). Predictive power evaluation operates on the premise that if explanations do not lead to the corresponding predictions, they are deemed unfaithful (Jacovi & Goldberg, 2020; Sia et al., 2023; Ye et al., 2021). Robustness evaluation examines whether explanations remain stable when there are minor changes in the input (Ju et al., 2021; Yin et al., 2021; Zheng et al., 2021), such as when input words with similar semantics produce similar outputs. Perturbation-based evaluation, one of the most widely used methods, assesses how explanations change when perturbed (Atanasova, 2024; Jain & Wallace, 2019). This approach is akin to MORF and LeRF, where (DeYoung et al., 2019) measures prediction sufficiency and comprehensiveness by removing both unimportant and important features. For further information, please refer to the survey paper (Lyu et al., 2024)."}, {"title": "8 CONCLUSION", "content": "In this paper, we introduced F-Fidelity, a robust framework for faithfulness evaluation in explainable AI. By leveraging a novel fine-tuning process, our method significantly mitigates the OOD problem that has plagued previous evaluation metrics. Through comprehensive experiments across multiple data modalities, we demonstrated that F-Fidelity consistently outperforms existing baselines in assessing the quality of explanations. Notably, our framework revealed a relationship between evaluation performance and ground truth explanation size under certain conditions, providing valuable insights into the nature of model explanations."}, {"title": "A DETAILED ALGORITHM", "content": "In this section, we provided the detailed pipeline of our evaluation method F-Fidelity as follows:"}, {"title": "B PROOF OF THEOREM 1", "content": "We provide the proof by considering the following cases:\nCase 1: s \u2208 [max(td, c\u2081), td]\nWe have:\ne(s) = Ex,y(FFid+(\u03c8, a+orig, \u03b2, s))\n= Ex,y( \u03a31(y = f(xi)) \u2013 P(Yi = fr(x+(xi, a+orig, s)))),\n(xi,Yi) ET\nwhere fr(.) represent the finetuned model in Algorithm 1, which is assumed to be robust to up to \u1e9etd removals, i.e., P(f(x\u2299 m) = Y) = P(Y|xm) if ||m||1 \u2265 td \u2013 \u1e9etd. Consequently,\ne(s) = Px,y(Y = fr(X)) \u2013 Px,y(Y = fr(x+(X, a+orig, s)))\n= g(C1, C2,..., Cr) - \u2211 P(J = j)g(C1 \u2013 \u00cd1, C2 \u2013 \u01302, \u2026\u2026\u2026, Cr - \u00cdr)\njr:jici\n= g(C1, C2,\uff65\uff65\uff65, cr) \u2013 E(g(c1 - J1, C2 - J2,\uff65\uff65\uff65, Cr \u2013 Jr)),\nwhere J\" is a multivariate hypergeometric vector with parameters (n1, n2,\u2026\u2026\u2026, nr, \u1e9etd), where:\nif \u2211i'<i Ci' \u2264 s,\ns-i'<i Ci' if \u2211i'<i Ci' \u2264s \u2264\u2211i'<i Ci',\nNi = \n0 otherwise\n(7)\nTo explain the last equation, recall that FFid+ first ranks the elements of the input based on Shap values, and chooses the top s element. Then, it randomly and uniformly samples \u1e9etd elements from the top s elements (as long as [a+origs] > \u1e9etd). So, if the i-th influence tier satisfies\""}, {"title": "C DETAILED EXPERIMENT SETUP", "content": "We explore two architectures, Resnet and ViT for the image classification tasks. We use Resnet-9 3 and a 6-layer ViT as backbones for both Cifar-100 and Tiny-Imagenet . In the ResNet architecture, the hidden dimensions are set to [64, 128, 128, 256, 512, 512, 512, 512]. For the ViT model, we use a patch size of 4, with a patch embedding dimension of 128. The backbone consists of 6 attention layers, each with 8 heads for multi-head attention. The final output hidden dimension of the ViT encoder is 256. In the training stage, we set the learning rate and weight decay to 1E-4 for Resnet and set the learning rate to 1E-3 and weight decay to 1E-4 for ViT. We use Adam as the optimizer and the training epochs are 100 for Resnet and 200 for ViT. During fine-tuning, we use the same hyperparameters. We report our model accuracy in the Table 4."}, {"title": "D EXTRA EXPERIMENTAL STUDY", "content": "D.1 ABLATION STUDY\nTo evaluate the effectiveness of our proposed F-Fidelity framework, we conduct ablation studies comparing three evaluation setups: the original Fidelity metric, Fidelity with fine-tuning (Fidelity+Fine-tune), and F-Fidelity. We utilize the CIFAR dataset for these experiments, employing a ResNet architecture as the backbone for the classifier. The explainers are based on SG-SQ.\nD.2 HYPERPARAMETER SENSITIVITY STUDY\nIn this part, we assess the robustness of F-Fidelity to hyperparameter choices. We select a subset of 400 samples with an explanation size ratio of 0.2 from the colored-MNIST dataset (Arjovsky et al., 2019). we have three key hyperparameters: the number of sampling iterations T, the ratio a+/a\u00af, and B. For each hyperparameter, we explored a range of values to understand their impact on the performance of F-Fidelity. We evaluated the method's performance using three Macro Spearman correlations, with additional detailed results on RFid+, RFid\u00af, and Micro Spearman correlations provided in Appendix E.4.\nD.3 FIDELITY AND EXPLANATION SIZE\nAs mentioned in Section D.2, We conduct experiments on three settings of a+ and five settings of \u1e9e in the evaluation stage. As Figure 4 shows, in different settings, the ground truth size can be observed from the flat area in RFid+. According to the analysis in Section 5, there are two key points in the x-axis, the explanation size y and the point where the deleting attributions are equal to the explanation size, Ba\u207a. Between the two key points, there exists a flat area, which is the best comparison area. Notably, our experiments are based on the assumption that all the explanation parts share the same important weight. In the real world, there is a gap between the ideal results and experiment results, such as the smooth increase and decrease in Figure 4.\nD.4 VIT AS BACKBONE FOR IMAGE CLASSIFICATION.\nTo further validate the robustness of our F-Fidelity framework across different model architectures, we conducted additional experiments using ViT (Dosovitskiy et al., 2020) as the backbone for image classification on both Cifar-100 and TinyImageNet datasets. We utilize the SG-SQ (Smilkov et al., 2017) to generate explanations. For the settings, se follow our main experimental setup. As shown in Table 6, F-Fidelity consistently achieves best correlations across all metrics for both datasets, outperforming other methods. This strong performance on ViT models further emphasizes the versatility and effectiveness of F-Fidelity in evaluating explanations across different deep learning architectures.\nD.5 NATURAL LANGUAGE CLASSIFICATION EXPLANATION EVALUATION\nSetup. We use two benchmark datasets for our NLP experiments: the Stanford Sentiment Tree- bank (SST) (Socher et al., 2013) for binary sentiment classification and the Boolean Questions (BoolQ) (Socher et al., 2013) dataset for question-answering tasks. For SST, we utilize 67,349 sentences for training and 872 for testing. BoolQ comprises 9,427 question-answer pairs for training and 3,270 for testing. We employ two popular model architectures: LSTM networks and"}]}