{"title": "L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text Compression", "authors": ["Junxuan Zhang", "Zhengxue Cheng", "Yan Zhao", "Shihao Wang", "Dajiang Zhou", "Guo Lu", "Li Song"], "abstract": "Learning-based probabilistic models can be combined with an entropy coder for data compression. However, due to the high complexity of learning-based models, their practical application as text compressors has been largely overlooked. To address this issue, our work focuses on a low-complexity design while maintaining compression performance. We introduce a novel Learned Lossless Low-complexity Text Compression method (L3TC). Specifically, we conduct extensive experiments demonstrating that RWKV models achieve the fastest decoding speed with a moderate compression ratio, making it the most suitable backbone for our method. Second, we propose an outlier-aware tokenizer that uses a limited vocabulary to cover frequent tokens while allowing outliers to bypass the prediction and encoding. Third, we propose a novel high-rank reparameterization strategy that enhances the learning capability during training without increasing complexity during inference. Experimental results validate that our method achieves 48% bit saving compared to gzip compressor. Besides, L3TC offers compression performance comparable to other learned compressors, with a 50\u00d7 reduction in model parameters. More importantly, L3TC is the fastest among all learned compressors, providing real-time decoding speeds up to megabytes per second. Our code is available at https://github.com/alipay/L3TC-leveraging-rwkv-for-learned-lossless-low-complexity-text-compression.git.", "sections": [{"title": "Introduction", "content": "Lossless text compression is a fundamental research field focused on reducing data size based on information theory. In 1948, Shannon (Shannon. 1948) described that maximizing the log2-likelihood of data is equivalent to minimizing the number of bits required for compression. Then information theory (MacKay. 2003) established the essential equivalence between probabilistic models of data and lossless compression. Recently, learning-based probabilistic models, like language models, have developed rapidly and demonstrated remarkable in-context learning capabilities across various tasks by predicting the probability of the next token. This predictive capability can be inherently combined with entropy encoding to serve as a data compressor. Entropy encoding can be implemented in various ways, including Huffman Coding (Huffman 1952), Arithmetic Coding (AC) (Pasco. 1977; Howard and Vitter 1991), and Asymmetric Numeral Systems (ANS) (Duda. 2013).\nDespite the success of language models in many intelligent tasks, the literature has largely overlooked their application as a practical lossless text compressor. This is not surprising, because the decoding complexity is nearly equivalent to the model's inference complexity, which is significantly slower compared to popular engineered compressors such as gzip (Pasco. 1996). For a practical usage, the data compression task basically demands a very critical real-time decoding performance, up to megabytes per second. Therefore, it is a very promising and challenging research direction to develop a low-complexity text compression algorithm based on the learning-based likelihood prediction.\nSeveral existing works have demonstrated the effectiveness of learned probabilistic models based on transformers and LSTMs for lossless text compression. Specifically, nncp v3.2 (Bellard 2021), which is based on transformers, and"}, {"title": "Related Work", "content": "We review related work in three areas: classical text compressors, learned text compressors, and recent language models and their relation to compression.\nThe development of lossless text compression has a long history. Typical compression tools include gzip, bzip2, and zstd. gzip (Pasco. 1996) is based on the Deflate algorithm, which is a combination of LZ77 (Ziv 1977) and Huffman coding (Huffman 1952). It works by searching for duplicate strings within a sliding window and replacing them with a pointer to their previous occurrence and the string length. BZIP2 (Seward 2000) is another popular lossless text compressor that uses Burrows-Wheeler Transform and run-length encoding, Huffman Coding. Owing to its transform to rearrange the input text into runs of similar characters, bzip2 achieves a higher compression ratio than gzip. zstd (Meta. 2015), a more recent algorithm, introduces several advanced techniques such as optimal parsing and dictionary compression, significantly improving the compression performance.\nLearned Text Compressors. Transformer (Vaswani et al. 2017) or LSTM (Hochreiter and Schmidhuber 1997) based compression with arithmetic coding has achieved state-of-the-art compression performance according to the Large Text Compression Benchmark (Mahoney 2024) and Hutter Prize. Notable examples include cmix v20 and nncp. Cmix combines multiple modeling techniques, like context mixing, prediction by partial matching, and neural network-based models, to enhance compression. Nncp uses an online compression approach, where a pseudo-randomly initialized model is continuously trained on the data stream, adapting its weights to enhance predictions over time. Language models, with their strong in-context learning abilities, are well-suited for offline compression. For instance, LLMZip, an offline compression method using pretrained LLaMa-7B, achieves up to 0.71 bpc (bit per character) on the text8 dataset, outperforming popular archive tools like PAQ8H (1.2bpc). However, these learned compressors are highly resource-intensive, requiring several days or a week to compress 1GB of text.\nRecent studies have discussed the connection between compression and likelihood maximization in language models. (Huang et al. 2024) examines the linearity relationship between compression and intelligence for various tasks. Google (Del\u00e9tang et al. 2024) demonstrates that Chinchilla-70B, pretrained on text datasets, not only achieves a promising compression ratio on text compression, but also compresses ImageNet (Deng et al. 2009) patches to 48% and LibriSpeech (Panayotov et al. 2015) audios to 21% of their raw size, obviously outperforming the domain-specific compressors like PNG (60%) (Boutell 1997) and FLAC (30%) (Xiph.Org Foundation 2001). Additionally, various studies have investigated the relationship between tokenization and compression. (Lester et al. 2024) introduces a novel text segmentation technique that significantly outperforms byte-level segmentation in terms of perplexity and inference speed. Pathpiece (Schmidt et al. 2024) argues that fewer tokens do not always result in better downstream performance. (Goldman et al. 2024) suggests that compression is a reliable intrinsic indicator for tokenization quality.\nThough learning-based compression has shown promising improvements, the significant complexity of models with billions of parameters remains unaddressed. Therefore, our work prioritizes a low-complexity design while maintaining competitive compression ratios.\nLanguage Models and Compression."}, {"title": "Proposed Method", "content": "The overall architecture of our proposed method L3TC is shown in Fig. 2. The text is firstly segmented into a sequence of tokens X1:N, where N denotes the total number of tokens."}, {"title": "Low-Complexity RWKV Models", "content": "To explore a low-complexity design, we conduct experiments on various architectures, including Transformer (Vaswani et al. 2017), Transformer-XL (Dai et al."}, {"title": "Outlier-aware Tokenizer", "content": "Tokenizers segment text into subword tokens, serving as a pre-compression process (Lester et al. 2024). By concealing the character-level composition of each token and merging the most frequent subwords, tokenizers enable the network to model long-distance dependencies and process more data. This approach also enhances the context prediction during inference. Consequently, enhancing tokenizers' performance not only improves the overall compression ratio but also reduces inference complexity.\nVocabulary size and coverage value are two important factors for tokenization. Recent language models typically use vocabulary sizes ranging from 32K to 256K tokens. However, a larger vocabulary would increase the inference burden, especially for very small models. While language models usually aim for 100% character coverage, rare tokens can actually increase the compressed size and reduce the compression efficiency.\nTo investigate the influence of vocabulary sizes and coverage values, we conduct experiments on enwik8 using Byte Pair Encoding (BPE) (Sennrich, Haddow, and Birch 2016) as the baseline, which is recognized for its optimal pre-compression performance. We use two distribution methods, \"Unigram\" and \"Uniform\", to determine the number of bits for each token after tokenization. \u201cUnigram\" calculates the entropy of tokens based on their contextual probabilities in enwik8, assigning shorter codes to more frequent tokens, making it suitable for evaluating in-distribution performance. \"Uniform\" assigns identical bits to all tokens, hence is suitable to assess out-of-distribution performance. If the character coverage is lower than 1, there exists a vocabulary set \u0393 containing all known tokens. Text characters excluded by the vocabulary, i.e., the unknown tokens, are identified as outliers and transmitted through a bypass. Therefore, the total compressed size Bits is defined as\nBits = H_(\\Gamma)(p) + R(x_i \\notin \\Gamma)\nwhere H(p) represents the compressed size of known tokens, as defined in Eq.(1), and R(x_i \\notin \\Gamma) denote the consumed bits for outliers to be coded directly as a UTF-8 byte sequence. Two parts contribute to their total bit sizes. The \"bits per byte (bpb)\" metric (the number of bits to encode one byte of text) is then calculated to assess the tokenizer's pre-compression efficiency, with a lower value indicateing better performance, as shown in Fig. 3.\nIt can be observed the coverage of 0.999 achieves the lowest bpb for both \"Uniform\" and \u201cUnigram\u201d methods. When using the \"Unigram\" method, increasing the vocabulary size can reduce the sequence length, packing more information into the context and resulting in a better compression ratio. However, this benefit diminishes if the evaluation data's distribution differs from the training data, as measured using the \"Uniform\" method. In other words, the largest vocabulary size is not always optimal. Moreover, our evaluation using RWKV-800K (Table 2) shows that the inference time increases with the vocabulary size. Thus, a relatively small vocabulary size is essential for a low-complexity design."}, {"title": "High-rank Reparameterization", "content": "To enhance the inference performance of RWKV without compromising the computational efficiency, we introduce a High-rank (HiRA) reparameterization strategy. As shown in Fig. 4, we add additional branches for each R, K, V layer in both \"Channel Mixing\" and \"Time Mixing\" modules during training to enhance the training effectiveness, and then merge them into the main branches during inference to reduce the number of parameters. Inspired by Low-rank adaptation (LoRA) (Hu et al. 2021), we increase the parameters of these additional branches through matrix decomposition, thereby improving their capability.\nSpecifically, assuming the original parameters of the R, K, V layers are W_0 \\in \\mathbb{R}^{d \\times k}, we do not directly add several parallel branches, such as 1x1 convolution or shortcuts as in (Ding et al. 2021). Instead, given the m-th branch, we decompose it into the product of two high-rank matrices: A_m \\in \\mathbb{R}^{d \\times r} and B_m \\in \\mathbb{R}^{r \\times k}. Therefore, during training, these layers' output is obtained by summing the outputs of the main branch and several bypass branches. Unlike LORA, the matrices W_0, A_m, and B_m are optimized simultaneously. The effect of the rank r and the number of branches m will be discussed in the ablation studies.\nAs illustrated in the right part of Fig. 4, during inference, the additional branches A_m \u00d7 B_m are merged into the main branch via structural reparameterization:\nW = W_0 + \\sum_m A_m \\times B_m\nSuch a single-path structure after reparameterization ensures lower running time and memory usage during inference. Despite the structural changes, the multi-branch parameters are retained, ensuring high inference performance. Besides, we add a linear layer (depict as W) across the whole RWKV module to further enhance the performance with negligible computational overhead."}, {"title": "Experiments", "content": "For RWKV models, we adjust the number of layers, attention embedding dimension and hidden sizes to achieve target model sizes. We train the models using the AdamW (Loshchilov and Hutter 2019) optimizer with an initial learning rate of 1e-4 and a linear learning rate scheduler with a decay rate of 0.999 over 20 epochs, without warm-up. All the models are trained with a sequence length of 2048 bytes and a batch size of 64.\nDuring evaluation, for learned compressors, we split the data into sequences of 2048 bytes and enable parallel batch processing. For traditional compressors with context lengths exceeding 2048 bytes, chunking can deteriorate the compression ratio. Therefore, we use unchunked data to maintain their optimal performance."}, {"title": "Compression Performance", "content": "Table 3 illustrates the compression ratios for different compressors on enwik9. Models labeled with * are pretrained models on larger text datasets using their own tokenizers. Models without * are trained from scratch on enwik8. Our proposed L3TC models are highlighted in bold and in italics.\nAs shown in Table 3, classical compressors, e.g., gzip, offer moderate compression ratios with minimal computational complexity. Compressors based on pretrained models (Llama2, Chinchilla, and RWKV) achieve superior compression performance, with Llama2-13B leading by achieving a 75% bit saving compared to gzip. However, these models suffer from significant model sizes (ranging from 169M to 70B parameters) and excessive resource requirements. In fact, using such large models to compress small-scale data is highly inefficient, with ACR values ranging from 48% to 14,000%. Even compressed using low-bit quantization, the model size overhead remains substantial, making them impractical for real-world deployments.\nBy contrast, the proposed L3TC models have smaller model sizes and lower computational complexity. L3TC-3.2M achieves 50% bit saving compared to gzip and outperforms other learned compressors by saving 48% more bits when accounting for model size. L3TC also offers compression ratios comparable to other learned compressors, with a 50\u00d7 reduction in model parameters, as shown in Fig. 1.\nTable 4 records the compression ratios on enwik8, providing an assessment of in-distribution performance. While larger model sizes generally enhance compression, L3TC-12M shows no further gains on enwik9 compared to L3TC-3.2M. Actually, when the model size approaches the size of training data, the model is likely to overfit to the training set. Therefore, for general usage, L3TC-12M is excluded in the following section."}, {"title": "Decoding Speed", "content": "Table 5 compares the decoding speeds for various methods. To calculate the decoding speed (bytes per second), we multiply the batch size by the average byte length per token and then divide the result by the batch inference time. The average byte lengths for the character-based tokenizer, Llama tiktokenizer, RWKV-pile tokenizer, and our proposed outlier-aware tokenizer are 1, 2.94, 3.37 and 3.29, respectively. On the A100, the batch size is 2048 by default or the maximum size without out-of-memory. When running on mobile devices, the batch size is set to 256. Pretrained models with hundreds of millions to billions of parameters are difficult to run on devices with limited memory and computational resources, so their decoding speeds are not reported.\nTable 5 shows that the Llama models operate at speeds measured in bytes per second (B/s), while the pretrained RWKV models are significantly faster, measured in kilobytes per second (KB/s). Our proposed methods L3TC are the fastest, reaching decoding speeds up to 1.30 MB/s on mobile devices. The speed gains of L3TC-200K over the default RWKV-200K are primarily due to our proposed outlier-aware tokenizer. This tokenizer merges frequent subwords, allowing more data to be processed in a single inference, whereas the default RWKV-200K uses character-based tokenizer and processes only one character at a time."}, {"title": "Ablation Study", "content": "We conduct experiments with different vocabulary sizes and coverage settings. As shown in the left sub-figure of Fig. 5 (the coverage for SPM-BPE is consistently set at 0.999), the compression ratios on enwik9 decrease as vocabulary size increases, adhering to the scaling law (Kaplan et al. 2020). However, excessively large vocabularies may require a larger model capacity, leading to a slight decline in compression performance when vocabulary sizes exceed 16K. Therefore, we empirically set the vocabulary size to 16K.\nThe right sub-figure of Fig. 5 demonstrates the relationship between coverage values and compression ratios using line graphs. The bar charts, whose y-axis is depicted on the right, indicate the proportion of unknown tokens' size to the compressed data size (i.e., 8x(x_i). As the coverage value increases, both unknown ratios and compression ratios exhibit a decreasing trend. However, the compression ratio reaches the lowest point when the coverage reaches 0.999, after which it slightly increases. This results validate the effectiveness of proposed outlier-aware tokenizer.\nTo examine the effect of rank r and the number of branches m, we conduct experiments as Fig. 6a. The left subfigure illustrates that given r ranging in the set of {0.25, 0.5, 1, 2, 4}, compression ratio continuously decrease along with the increase of ranks. We use r as 4 in our experiments. The right subfigure shows the compression ratios decrease minimally with the increase of more additional branches. Therefore, we use only one branch in our experiments.\nTo further validate the effectiveness of the proposed high-rank reparameterization, we conduct experiments with default character-based tokenizers in Fig. 6b. The circles refer to the original unmodified RWKV models, while the crossed points represent the proposed L3TC models using HiRA with one branch and the rank of 4. Points of the same color correspond to models with equivalent sizes. Obviously, introducing HiRA significantly improves compression performance. It is noteworthy that reparameterization itself does not increase MACs; the marginal increase is due to the added linear shortcut parallel to the whole RWKV module.\nBatch size is a key factor to decoding speed but large batch size can not always improve the decoding speed proportionally, especially on resource-limited devices. We analysis the effect of batch size on iPhone12. As shown in Fig. 7, when batch size is relatively small, the decoding speed is proportion to the batch size, but when the computational resources saturate (i.e., batch size is larger than 256), the decoding speed is not increased any more. Thus, we use a batch size of 256 in our settings when running on device."}, {"title": "Conclusion", "content": "In this paper, we propose a novel learned lossless low-complexity text compressor (L3TC) method. First, we conduct extensive experiments to compare various architectures and select RWKV as our backbone due to its fast decoding speed. Second, we propose an outlier-aware tokenizer that focuses on frequent tokens while bypassing infrequent outliers. Third, we propose a high-rank reparameterization strategy to enlarge the model capacity during training while not increasing inference complexity. Experiments demonstrate that our proposed L3TC achieves impressive lossless compression performance, with 48% size reductions compared to gzip. L3TC offers compression performance comparable to other learned compressors with 50\u00d7 model size reductions. Besides, L3TC is the fastest among all the learned compressors, with a decoding speed of up to MB/s."}]}