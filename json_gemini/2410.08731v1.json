{"title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models", "authors": ["Yeeun Kim", "Jinhwan Choi", "Young Rok Choi", "Hai Jin Park", "Eunkyung Choi", "Wonseok Hwang"], "abstract": "Large language models (LLMs) have demonstrated remarkable performance in the legal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However their efficacy remains limited for non-standardized tasks and tasks in languages other than English. This underscores the need for careful evaluation of LLMs within each legal system before application. Here, we introduce KBL, a benchmark for assessing the Korean legal language understanding of LLMs, consisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning tasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510 examples). First two datasets were developed in close collaboration with lawyers to evaluate LLMs in practical scenarios in a certified manner. Furthermore, considering legal practitioners' frequent use of extensive legal documents for research, we assess LLMs in both a closed book setting, where they rely solely on internal knowledge, and a retrieval-augmented generation (RAG) setting, using a corpus of Korean statutes and precedents. The results indicate substantial room and opportunities for improvement.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) show remarkable performance across various tasks. For instance, GPT-4 has passeed the Uniform Bar Exam (Martinez, 2023; OpenAI, 2023). However, their performance is still limited beyond the standardized bar exam questions (Guha et al., 2023; Dahl et al., 2024; Magesh et al., 2024; Kang et al., 2023; Bernsohn et al., 2024; Blair-Stanek et al., 2023), particularly in handling legal tasks in languages other than English (Fei et al., 2023; Zhong et al., 2023). This implies it is necessary to thoroughly evaluate LLMs before applying them to specific legal tasks and systems."}, {"title": "2 Related Work", "content": "Many legal AI datasets has been created contributing to NLP community (Paul et al., 2022; Kapoor et al., 2022; Yao et al., 2022; Glaser et al., 2021; Niklaus et al., 2021; Chalkidis et al., 2022a; Rossi et al., 2021; Chalkidis et al., 2022b; Louis and Spanakis, 2022; Rabelo et al., 2020; Henderson et al., 2022; Chalkidis* et al., 2023; Niklaus et al., 2023). Here we review only a few works that are dedicated to build benchmark for evaluating LLMs under zero, or few-shot setting in legal domain.\nGuha et al. (2023) developed LegalBench which comprises 162 legal language understanding tasks organized according to six different types of legal reasoning based on the IRAC framework. However, their work is limited to tasks in English legal language understanding tasks. Additionally, the benchmark does not evaluate LLMs in a open-book setting, and LLMs must rely solely on their internal knowledge.\nMagesh et al. (2024) compared commercial RAG-based AI systems in the US legal domain using 202 examples on generative tasks and found that even the most competent system exhibited a 17% hallucination rate through human evaluation. In contrast, our research focuses on Korean legal AI tasks and emphasizes scenarios where automatic evaluations are possibles. RAG systems rely on various components including databases, search algorithms, and the backbone of LLMs. Given these complexities, it becomes impractical to manually evaluate the performance of RAG systems every time a component changes. Therefore, we concentrate on addressing the challenges where automatic evaluation is feasible."}, {"title": "3 Datasets", "content": "Our benchamrk consists of 7 knowledge tasks, 4 reasoning tasks, and multiple-choice questions from the Korean bar exam. All tasks are structured as question-answering task where the model must select the correct answer for given questions, similar to MMLU (Hendrycks et al., 2021). The datasets were compiled using various sources, including Korean precedents, statutes, bar exams, legal QA datasets from Korea Legal Aid Corporations, etc.\nThe Korean legal system, rooted in civil law, incorporates complex historical and cultural aspects unique to Korea. Notably, GPT-4 has not passed the Korean bar exam, despite passing the US bar exam. This highlights that creating a Korean legal benchmark involves more than translating existing benchmarks; it requires developing a new system with a unique ontology. To ensure the quality of the datasets, we developed the tasks in close corpo-ration with legal professionals, and all the answers have been verified by 8 licensed lawyers. The verification took 26 hours in total reflecting the difficulty (and the quality) of the tasks constructed. During the quality assurance process, we found freely available data, often created by individuals with semi-expertise, frequently include substantial amounts of errors (up to 21% in our study), highlighting the importance of close collaboration with experts (Section A.1 covers additional general lessons learned during the dataset creation process). Representative examples from individual tasks are displayed in Tables 3, 4, 5 in Appendix."}, {"title": "3.1 Legal Knowledge Tasks", "content": "Legal Concept QA The legal concept QA dataset (CONC) comprises questions addressing complex legal concepts. For instance, from various types of suspension-such as suspension of indictment, suspension of sentence, suspension of execution, suspension of collection, suspension of announcement-a model needs to select the option that best fits a given definition. The examples were crafted based on legal terminology reference documents from South Korean courts, definition of legal terms provided by the National Law Information Center, and the Korea Legislation Research Institute. Please See Section A.2.1 for further information.\nOffense Component QA The offense component QA dataset (COMP) comprises question and answer pairs that determine whether specific actions meet the actual elements of a criminal offense. The dataset was constructed using several sources: \"100 Questions and 100 Answers\" on Day-to-Day Laws provided by the Ministry of Justice, data crawled from a now-defunct law firm website, cases from Korea Legal Aid Corporation, from statutory interpretations from the Ministry of Government. The questions were refined to be clear legal inquiries based on responses from actual legal consultation experts. Responses are binary, either \"Yes\" or \u201cNo\u201d. For example, one real client question inquires whether escaping prison due to unbearable harassment over private loans by falsely reporting to authorities could constitute the crime"}, {"title": "3.2 Legal Reasoning Tasks", "content": "Causal Reasoning QA The causal reasoning QA dataset (CAUSAL) is compiled from a series of verdicts from criminal trials involving physical harm leading to injury or death. The examples are drawn from cases categorized as \u201cDeath or Injury Resulting from Violence\u201d or \u201cDeath Resulting from Bodily Injury\". We used precedents from these two criminal cases based on advice from a lawyer, as selected cases had relatively well-matched causal relations. For each given factual description and claims, the task is to assess whether the defendant's actions were the direct and decisive cause of the victim's injury or death. A guilty verdict implies that \"there is causal relationship\" between the defendant's actions and the victim's injury or death, indicating that the event would not have occurred without the defendant's involvement. Conversely, a not guilty verdict may indicate that other factors also contributed to the victim's injury or death, or that there was no no causal relationship between the defendant's actions and the outcome. These instances are classified under \"no causal relationship.\""}, {"title": "3.3 Korean Bar Exam", "content": "Multiple-choice questions The Korean Bar Exam is designed to evaluate legal knowledge and the capability to perform tasks essential for a lawyer. Administered at least once annually under the Ministry of Justice's supervision, the exam is divided into two main parts: multiple-choice questions and an essay-type written test. The multiple-choice section comprises 150 questions, divided among Public Law (PUBLIC), Civil Law (CIVIL), and Criminal Law (CRIMINAL), with 40, 70, and 40 questions in each subject area, respectively. The Public Law section covers Constitutional Law and Administrative Law; The Civil Law section encompasses Civil Law, Commercial Law, and Civil Procedure Law; and the Criminal Law section includes Criminal Law and Criminal Procedure Law. We use the test held in 2012\u20132024. The essay-type written test covers specialized legal areas such as International Law, Labor Law, and Tax Law, in addition to Public Law, Civil Law, and Criminal Law.\nWe focus solely on the multiple-choice questions for Public Law, Civil Law, and Criminal Law. as the official answers for the essay-type test are not publicly available. Additionally, the multiple-choice section offers clear and definitive answers, providing an ideal playground to evaluate LLMs' under a RAG setting, where multiple components can influence performance."}, {"title": "3.4 Corpus", "content": "We utilize 150k Korean precedents released by (Hwang et al., 2022) for the RAG experiments. The corpus, processed using the gpt-40 tokenizer, contains 320M tokens. Additionally, we have developed a new Korean statute corpus compiled from active Korean statutes (\ubc95\ub839) and municipal ordinances and rules (\uc790\uce58\ubc95\uaddc) as of Nov. 2023. The raw data for this was collected from LAW OPEN DATA, a resource maintained by the Korean government. This statute corpus consists of 220k articles, totaling 52M tokens, where each article is concatenated with the name of the act, again processed using the gpt-40 tokenizer."}, {"title": "4 Experiments", "content": "We developed the evaluation code using the lm-eval-harness framework (Gao et al., 2023). Each task is formulated as multiple-choice question, where the model must generate a label corresponding to the given questions and possible selections. The options are tagged with letters A through E, and the model is tasked with generating the letter that matches the ground truth. For the evaluation of open-source LLMs (EEVE (Kim et al., 2024c), KULLM (Kim et al., 2024a), and Qwen2 (Bai et al., 2023)), we compare the average logit values by feeding individual (question, choice) pairs into the models. This approach is adopted because the performance of models tend to drop significantly when they directly generate labels."}, {"title": "4.2 Retrieval Augmented Generation", "content": "We constructed the BM25 retriever using the pyserini library (Lin et al., 2021). We first indexed the precedent, and the statute corpora using pyserini.index.lucene and LuceneSearcher with default settings. To retrieve related documents, the question is used as a query to the retriever. The number of documents retrieved is determined in the following way; (1) identify the maximum number of documents that LLMs can process simultaneously, (2) progressively decrease the number of documents until the performance ceases to improve. This results in retrieving between 1-8 documents; the exact number will be specified in the accompanying code. For the HALL task, we employ individual (question, choice) pairs as queries to retrieve related documents since each answer include the name of a (fictitious) statute. This method ensures the relevance of retrieved documents to the specific query, enhancing the accuracy of the task performance."}, {"title": "5 Results", "content": "We evaluate four open-source LLMs and five commercial LLMs across 7 legal knowledge tasks, 4 legal reasoning tasks, and 3 bar exam tasks conducted in 2024.\nOpen-source LLMs show performance comparable to GPT-3.5 We first compare open-source LLMs and GPT-3.5. On the knowledge tasks, EEVE-10.7b, KULLM3-10.7b, Qwen2-7b, and GPT-3.5 score on average 45.8, 52.1, 55.3, and 50.0 respectively (rows 2\u20136, 10; column 1) indicating open-source LLMs achieve comparable or better performance than GPT-3.5. Similarly, on two reasoning tasks-CAUSAL, CONS-all open source LLMs outperform GPT-3.5 except for EEVE-10.7b, which scores 4.3 points lower.\nHowever on the Korean bar exam which requires complex legal reasoning and knowledge, all open-source LLMs and GPT-3.5 perform close to the baseline achievable by selecting the most frequent labels (4th column from the right, rows 1\u20136, 9). Notably, Qwen2-72b shows the highest performance, even exceeding GPT-3.5 by margins of +10.2, +24.0, and +8.0 one knowledge tasks, reasoning tasks, and the bar exam respectively (rows 6 vs 9)."}, {"title": "6 Analysis", "content": "Here we provide a further analysis of the Korean bar exam held in 2024. This exam was chosen because: (1) it requires both deep legal knowledge and reasoning capabilities; (2) GPT-4 exhibited surprisingly low performance despite having passed the U.S. bar exam (Martinez, 2023); (3) GPT-4 was trained using data from before the 2024 bar exam, ruling out the possibility of data contamination.\nWe first categorizes individual questions into two types: (1) Rule or (2) Application. A question is categorized as Application-type if it requires not just an understanding of legal knowledge or principles, but also how these are applied to specific real-world cases. Otherwise, it is considered Rule-type. Although the IRAC method\u2013Issue, Rule, Application, and Conclusion (IRAC)-typically has four categories, we focus on two because questions often belong to multiple categories, making them difficult to clearly categorize. In the Civil domain, there are 29 Rule-type questions and 41 Application-type questions. The Criminal domain includes 4 Rule-type and 36 Application-type questions, while the Public domain comprises 23 Rule-type and 17 Application-type questions."}, {"title": "7 Conclusion", "content": "We propose KBL, a pragmatic benchmark designed for Korean Legal language understanding that comprises (1) 7 knowledge tasks, (2) 4 reasoning tasks, and (3) Korean bar exams. The first two tasks were designed with close corporation with legal professionals and the answers were validated by lawyers. The results indicate that there is still a significant room for improvement in LLMs' capabilities in the Korean legal domain. Additionally, recognizing that legal research often involves consulting related legal documents, we equip LLMs for evaluation in a RAG setting by providing two accompanying legal corpora for retrieval: a precedent corpus from a previous study (Hwang et al., 2022) and a statute corpus developed in this study. Enabling LLMs to utilize external legal documents via a simple BM25 retriever has shown to improve performance but not always depending on several factors. This highlights the importance of our work that provides a common playground for automatic evaluating of RAG systems."}, {"title": "Limitations", "content": "Here we evaluate LLMs on multi-choice type questions only where clear ground truths can be established. For generative tasks, although it would be possible to use LLM-as-a-judge as a proxy, the field is actively evolving and its accuracy is still limited. Also, it is particularly challenging to evaluate generative tasks in the legal domain where the hallucination are still prone and where complex in-depth reasoning process are required (Magesh et al., 2024). Although we have meticulously designed and selected the tasks for KBL, it is important to acknowledge that our benchmark cannot encompass the entire spectrum of legal tasks especially where the labeling data is very costly due to the fact that it requires professional trained for several years. Nevertheless we aim to capture the essential aspects of legal intelligence that can be automatically evaluated. For this, we have collaborated with legal professionals and verified all examples thoroughly, striving to establish a reliable benchmark."}, {"title": "Ethics Statement", "content": "We use Korean precedents and statutes as a main source of raw data where all personal information, if any, is redacted by Korean government or court. The part of the datasets include the detailed description of crimes from precedents that are already publicly available. Open-source LLMs have a possibility of the misuse and can be easily tuned for unethical purpose (Qi et al., 2024; Kim et al., 2024d). However, here we do not release or train any models but focusing on their evaluation with the benchmark that consists of legal question and answers, the precedent corpus, and the statute corpus."}, {"title": "Acknowledgements", "content": "We thank Prof. Yong Lim, Prof. Jong Hwan Kim for their critical comments on the legal aspects. We also thank Hyunjun Kim, Jinu Lee, and Dongjun Lee for their assistant in designing tasks during the early stages of the project. The part of this research was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NO.2022M3J6A1084845) for Yeeun Kim."}, {"title": "A Appendices", "content": "While developing the benchmark for the legal domain, which requires high expertise, we learned the following lessons:\n\u2022 It is necessary to communicate closely with domain experts to distinguish subtle differences and accurately label the data.\n\u2022 Freely available data, often created by individuals with semi-expertise, frequently include substantial amounts of errors (up to 21% in our study), highlighting the importance of close collaboration with experts.\n\u2022 The selection of domains should be done carefully, considering the difficulty of the labeling. For instance, during the creation of the Causal reasoning QA dataset, we initially selected \u201cinsurance dispute\". The domain was chosen because corresponding cases include the lexical cue \u201csubstantial causation (\uc0c1\ub2f9\uc778\uacfc\uad00\uacc4),\u201d. However, during the verification stage by the lawyer, feedback was received that although the term \u201ccausation\u201d might be mistaken to mean a judgment by the court, legal professionals use it in a practical sense, indicating a contextual connection between the act and the result based on common experience (\uacbd\ud5d8\uce59). Therefore, the presence of \u201csubstantial causation\u201d alone cannot serve as a standard to determine the causality of the case. Based on this feedback and the lawyer's recommendation, we changed the domain to \u201cDeath or Injury Resulting from Violence\" or \"Death Resulting from Bodily Injury\" where actions could be seen as directly and clearly causing the results (the defendant's actions and the victim's death or injury)."}, {"title": "A.2 Datasets", "content": "Most Korean legal terms originate from Chinese characters, leading to significant differences in meaning with even small variations in characters. For instance, the Korean legal terms \u2018\uc18c\uac01\ud558' (so-gak-ha) and \u2018\uc18c\ucde8\ud558' (so-chui-ha) both refer to methods of terminating a lawsuit. Despite differing by only one character, they represent significant differences regarding the party responsible for terminating the lawsuit.\nTherefore, we collected terms that are rarely used by the general public but should be clearly differentiated by experts or have different meanings when used in a legal context. We first reviewed the definitions of terms listed alphabetically in a legal dictionary and prioritized terms that have subtle variations. Additionally, during the term selection process, we used commercial LLMs (ChatGPT and Claude-sonnet) to identify 100 terms that at least one LLM responded inadequately. A lawyer then verified the dataset, identifying a single error among 100 examples, which was subsequently corrected based on the lawyer's feedback."}]}