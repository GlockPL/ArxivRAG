{"title": "Offline Handwritten Signature Verification Using a Stream-Based Approach", "authors": ["Kecia G. de Moura", "Rafael M. O. Cruz", "Robert Sabourin"], "abstract": "Handwritten Signature Verification (HSV) systems distin-\nguish between genuine and forged signatures. Traditional HSV develop-\nment involves a static batch configuration, constraining the system's abil-\nity to model signatures to the limited data available. Signatures exhibit\nhigh intra-class variability and are sensitive to various factors, including\ntime and external influences, imparting them a dynamic nature. This\npaper investigates the signature learning process within a data stream\ncontext. We propose a novel HSV approach with an adaptive system\nthat receives an infinite sequence of signatures and is updated over time.\nExperiments were carried out on GPDS Synthetic, CEDAR, and MCYT\ndatasets. Results demonstrate the superior performance of the proposed\nmethod compared to standard approaches that use a Support Vector\nMachine as a classifier. Implementation of the method is available at\nhttps://github.com/kdMoura/stream_hsv.", "sections": [{"title": "Introduction", "content": "Handwritten Signature Verification (HSV) systems aim to automatically dis-\ntinguish between genuine signatures, belonging to the claimed individual, and\nforgeries. In offline HSV, signatures are represented as digital images captured\nafter the writing process is completed, as opposed to online systems that analyze\nthe signing dynamics [7].\nOffline HSV systems can be categorized into two approaches: writer-dependent\n(WD) and writer-independent (WI). In WD systems, a unique classifier is trained\nfor each enrolled user, offering potentially higher accuracy. However, this ap-\nproach requires individual training data for each new user. Conversely, WI sys-\ntems utilize a single classifier for all users, hence being more scalable [5]. In this\ncase, the classification is performed on a dissimilarity space where the pattern\nrecognition is reduced to a 2-class problem by using differences between claimed\nand reference signatures through a Dichotomy Transformation (DT) [5].\nIn general, HSV development entails two distinct datasets: a development set\nutilized for training and an exploitation set employed during the testing phase\n[6]. Each set comprises the signatures of enrolled users. While more samples"}, {"title": "Stream handwriting signature verification (SHSV)", "content": "Streaming systems are designed to continuously process and analyze data as it\narrives, delivering updated results based on the most recent characteristics of the\ninformation. In the context of handwriting signature verification, this approach\nenables adaptive signature authentication, accounting for the inherent variability\nof this biometric modality. As the system evolves to accommodate variations in\nhandwriting patterns, it ensures accurate and reliable verification.\nIn light of this, we propose a framework for signature verification under a data\nstream context. Specifically, we introduce a system that takes as input a sequence\nof signatures S of claimed users \u00ce, denoted as Stream {(S, \u00ce)1, (S, \u00ce)2, ..., (S, \u00ce)}, for verification against the signatures of enrolled users. As new sig-\nnature samples arrive (from new or enrolled users), the system incorporates new\ninformation into its base knowledge and delivers updated results on the next\nverification. The system is depicted in Figure 1, and notation is synthesized in\nTable 1."}, {"title": "Representation model $\\phi(\\cdot)$", "content": "The representation model is a previously well-trained model capable of extracting\nrelevant features from the signature images. To this end, the SHSV employs the"}, {"title": "Stream dichotomy transformation DT(,)", "content": "An essential part of the SHSV system is the dichotomy transformation DT(,) [2]. It transforms a multi-class problem into a 2-class problem. This enables\nthe implementation of a writer-independent approach for the classification task,\nwhich is crucial for the stream context. The binary-problem result is achieved\nby computing the absolute distance between each feature of two feature vectors,\ni.e., the dissimilarity between two samples. Suppose (XR,IR) and (xc,lc) are\nthe feature vector (x) and label (l) of two data samples, where I refers to the\nauthor's ID. With XR = {f}=1 and xc = {f}K_1, where K is the number\nof features f. The dissimilarity vector between XR and xc is given by XRC =\nDT(XR,XC) = {|$f_k - f_u$|}$_{k=1}^K$, where |\u00b7| represents the absolute value of the\ndifference. The vector XRC has the same dimensionality as XR and XC.\n\nR\nR\nThe resulting dissimilarity set after applying DT(, ) on (XR,IR) and (xc,lc)\nis given by (XRC, YRC) where y denotes the new label. If lr = lc, i.e., XRC is"}, {"title": "Adaptive WI-classifier $\\theta$", "content": "In the proposed SHSV approach, the core component is the adaptive verification\nprocess, which enables the system to update its base knowledge over time. In\nstatic HSV systems, Support Vector Machines (SVM) are a popular choice for the\nverification step [7]. Nonetheless, an adaptive classifier is required for the present\nwork. Many methods have been developed to adapt the traditional SVM to han-\ndle evolving data [22]. An efficient optimization method is applying Stochastic\nGradient Descent (SGD) to linear models to minimize the loss function [10]. \u03a4\u03bf\nmimic the SVM behavior with adaptive capability, we adopt the SGD classifier\nwith a hinge loss function. We follow similar works[13,15,21] that employed SGD\nto minimize the loss function in the primal formulation directly. This approach\nis more efficient than employing Lagrangian methods as it avoids the need to\ncompute and store dual variables, which can become computationally expensive\nand memory-intensive as the number of data points and features increases. The\nloss function, described in Equation 1, aims to minimize the norm of the weight\nvector w while penalizing misclassifications (quantified by the hinge loss term\nmax(0, 1-yi (w\u2022xi + b))), where C is the regularization parameter that controls\nthe trade-off between maximizing the margin and minimizing the hinge loss.\n\nmin\nw,b\n||w||\u00b2 +\nC\nN\nN\u2211max(0,1 \u2212 Yi(w \u00b7 xi + b))\ni=1\n(1)\nSGD processes individual samples or small batches from the dataset, iter-\natively updating the model parameters based on the loss function. In SHSV,\nthe WI-classifier is updated with dissimilarity vectors obtained from a chunk of\nincoming signatures. This update process occurs after the classifier's prediction"}, {"title": "Fusion function g(.)", "content": "In SHSV, the WI-classifier's output is determined by the dissimilarity vector's\ndistance to its decision hyperplane. When there is a set of reference signatures\nSR for a claimed signature Sc, the system delivers a distance for each pair of dis-\nsimilarity vectors between SR and SC. These hyper-plane distances are combined\nthrough a fusion function g(.) [14]. Results in [17] reveal that better verification\nperformance is achieved when the Max fusion function is chosen to combine\nhyper-plane distances output. This work employs the maximum distance to de-\nliver a final decision."}, {"title": "Experimental Setup", "content": "Datasets. There are a few publicly available datasets for offline HSV Systems. In\nthis work, we adopt datasets used in related works [17,20] summarized in Table 2.\n\nPreprocessing. As SigNet-S is adopted as the backbone for feature extraction,\nto ensure the reported performance of the model, we have adhered to the same\ninitial preprocessing steps as described in [6] and [20]. First, images are centered\non a large canvas, with dimensions determined by the maximum size encountered\nwithin each dataset. Next, the image's background is removed using Otsu's algo-\nrithm [12], which transforms background pixels to white and foreground pixels\nto grayscale. Subsequently, the image is inverted to set the background to zero\nvalue. Then, all images are resized to the size of 170 pixels in height and 242\npixels in width, and finally, a center crop of size 150x220 is taken.\nClassifier. For classifier comparison in batch context, we employ soft margin\nSVM with Radial Basis Function (RBF) kernel, following the experimental pro-\ntocol defined in [6,17,20]. The SVM regularization parameter is given by 1.0, and\nthe RBF kernel coefficient hyper-parameter is given by 2-11.\nTypes of signatures. In this work, there are three types of signatures: genuine\n(G), which belongs to the claimed user; random forgery (RF), which is a genuine\nsignature that belongs to a user different from the claimed one; and skilled\nforgery (SK), which belongs to the claimed user but it was produced by a forger.\nThe set of all genuine signatures in a dataset is given by SG = {$S_1^G, S_2^G, ..., S_N^G$},\nwhere N is the number of users and S refers to the set of genuine signatures\nof user i. Specifically, S = {S1, S2, ..., SK}, where K denotes the number"}, {"title": "Experimental Results", "content": "Batch evaluation. Figure 4 presents results for batch-trained SVM and SGD\nconsidering different number of users nD (#U) and genuine signatures nDG\n(#G) (Table 3a). Interestingly, increasing the number of training samples does\nnot necessarily guarantee improved performance. For example, the configuration\nwith U=50 users and G=2 genuine signatures per user (orange dotted line) dur-\ning training, resulting in 100 samples (S:100), achieves better results than the\nconfiguration with U=10 users and G=12 signatures (blue solid line, S:1320),"}, {"title": "Conclusion", "content": "This work proposes a novel handwriting signature verification approach called\nSHSV. SHSV treats signatures as continuous data streams and updates the sys-\ntem dynamically. To achieve this, we introduce a stream generation approach\ncompatible with standard batch evaluation settings.\nExperimental results in batch settings demonstrated that having a high num-\nber of users is more crucial than the sheer volume of signatures, indicating an\noverall improvement in performance when more users are available at initial\ntraining. Results also showed that SHSV overcame the problem of limited train-\ning data by incorporating new information over time, demonstrating superior\nperformance compared to the SVM approach across different scenarios under\nstream configuration.\nFuture work may include using partially labeled data to explore scenarios\nwhere labels are not available for all test samples, as well as investigating the\ntrade-off between adapting the representation model and the WI-classifier over\ntime."}]}