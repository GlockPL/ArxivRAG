{"title": "How Good Is It? Evaluating the Efficacy of Common versus Domain-Specific Prompts on Foundational Large Language Models", "authors": ["Oluyemi Enoch Amujo", "Shanchieh Jay Yang"], "abstract": "Recently, large language models (LLMs) have expanded into various domains. However, there remains a need to evaluate how these models perform when prompted with commonplace queries compared to domain-specific queries, which may be useful for benchmarking prior to fine-tuning domain-specific downstream tasks. This study evaluates LLMs, specifically Gemma-2B and Gemma-7B, across diverse domains, including cybersecurity, medicine, and finance, compared to common knowledge queries. This study employs a comprehensive methodology to evaluate foundational models, encompassing problem formulation, data analysis, and the development of novel outlier detection techniques. This methodological rigor enhances the credibility of the presented evaluation frameworks. This study focused on assessing inference time, response length, throughput, quality, and resource utilization and investigated the correlations between these factors. The results indicate that model size and types of prompts used for inference significantly influenced response length and quality. In addition, common prompts, which include various types of queries, generate diverse and inconsistent responses at irregular intervals. In contrast, domain-specific prompts consistently generate concise responses within a reasonable time. Overall, this study underscores the need for comprehensive evaluation frameworks to enhance the reliability of benchmarking procedures in multidomain AI research.", "sections": [{"title": "I. INTRODUCTION", "content": "The increasing efficacy of large language models (LLMs) for dynamic natural language processing (NLP) tasks has attracted significant attention across various disciplines. Notably, LLMs often undergo an intricate development process encompassing training, optimization, fine-tuning, and, in some cases, incorporation of sophisticated techniques like reinforcement learning with human feedback (RLHF), to achieve advanced conversational capabilities. Extensive studies [1], [2] have unveiled the transformative potential of LLMs across various scientific and technological domains. Moreover, their potential to positively impact diverse fields, including software engineering, via process and outcome optimization has been recognized [3]. Further, their impact on the evolution of artificial intelligence and the burgeoning field of digital learning has been well-documented [4][5]. In addition, LLMs are increasingly being acknowledged for their transformative potential in the cybersecurity field, particularly in the area of threat detection [6].\n\nThe functions executed by Large Language Models (LLMs) can be fundamentally classified into two categories: input classification and output generation. Tasks falling under the former may encompass Text classification[7], named entity recognition (NER) [8], Sentiment Analysis [9], question answering [10], [11], and various others. Conversely, output generation tasks may include multimodal output generation [12], [13], language understanding and translation [14],[15], text summarization [16], dialog systems [17][18], among others. Notably, output generation tasks distinguish LLMs from traditional machine learning and deep learning methodologies due to their objective, which is similar to human-like natural language processing tasks.\n\nA significant milestone in artificial intelligence (AI) is the development of large foundational models (LLFMs), which are LLMs pre-trained on diverse datasets across various domains. LLFMs represent substantial progress in AI, offering enhanced performance, broader applicability, and efficiency gains [19]. LLFMs serve as fundamental pillars for numerous natural language processing (NLP) tasks and applications, and they excel at tasks such as text generation, summarization, and translation with exceptional accuracy and fluency. A plethora of LLFMs, varying in size and architecture, have been recently released, including models like Alpaca [20] [21], BERT, GPT, DALL-E, LLAMA, BLOOM, Gemma, and Alpaca. These models have been developed for diverse applications, spanning multimodal capabilities, vision-and-language tasks, visual-audio processing, as well as code and image generation, among others [22][23][24]. By leveraging large datasets for training, these models demonstrate remarkable adaptability, requiring relatively fewer data to fine-tune specific tasks within distinct domains. Thus, they serve as foundational frameworks for many AI applications.\n\nFocusing on the cybersecurity domain, previous studies have explored the potential of fine-tuning LLFMs for various tasks like cyber threat intelligence (CTI) and automation, using natural language text [25], identification of intricate patterns for automating software vulnerability detection [26], and many others. However, these studies have revealed a notable inability to evaluate the LFM's baseline cybersecurity knowledge prior to fine-tuning. This gap is exacerbated by the absence of comprehensive pre-fine-tuning assessments,"}, {"title": null, "content": "which leads to persistent inaccuracies in benchmarking. In addition, existing evaluation benchmark datasets ROUGE [27], Super-NaturalInstructions [28], MMMLU [29] in the realm of LLMs and natural language processing (NLP), inadequately encompass essential cybersecurity materials, which hinders accurate evaluations. Consequently, the misguided adoption of fine-tuning methodologies resulted in flawed benchmarking outcomes (Author, Year). Addressing these issues highlights the urgent need for a more rigorous and comprehensive evaluation framework to rectify these shortcomings and improve the reliability of benchmarking procedures in cybersecurity-AI research.\n\nTherefore, the goal of this study is to evaluate the foundational understanding of special domains such as cybersecurity, finance, and health/medicine, within large foundational models, which facilitates the development of a fine-tuning framework tailored to cybersecurity tasks. Our motivation stems from the intuition that a large language model (LLM) reacts differently to the prompts of various domains. For example, it responds differently to a common query like \"What is the meaning of a good life?\" compared to a cybersecurity query such as \"What is the attacker trying to achieve when running a DLL remotely on the server?\". Leveraging this insight, we address inquiries regarding the assessment of foundational model comprehension in various domains such as cybersecurity, finance, and medical. Furthermore, this study is significant because it establishes a framework for developers and researchers to assess the need for fine-tuning.\n\nOur work and findings. Large language model (LLM) inference requires significant resources, including time, CPU, and memory. Logically, a foundational model trained on a diverse dataset is inclined to retain various forms of knowledge. In our investigation, we observed that their performance in terms of both output and resource usage varies depending on whether they are prompted with a common or domain-specific query.\n\nSummary of our key findings:\n\n1) 7B models consume more GPU memory than 2B models.\n\n2) Overall, common prompts tend to produce responses with greater diversity in length and longer inference times.\n\n3) Across all categories, the 2B model tends to have higher throughput than its 7B counterpart.\n\n4) There is a strong correlation between inference time and response length compared to the other parameters.\n\n5) When using semantic textual similarity (STS) with ChatGPT responses as a reference, the 7B model exhibits superior performance compared to 2B.\n\n6) 7B model with a response length limit of 50 yields responses with higher ROUGE-L scores in all domains compared to any other parameter.\n\nOur contributions are as follows:\n\n1) We delve into foundation models of diverse sizes, specifically Gemma-2B and Gemma-7B, within both the domain-specific (cybersecurity, health/medical, finance) and common prompt and response generation control settings.\n\n2) Our analysis compares resource utilization and the quality and length of responses generated by the models.\n\n3) We introduce a framework to facilitate informed decision-making when fine-tuning large language models (LLMs) in the cybersecurity, medical, and finance domains."}, {"title": "II. LITERATURE REVIEW", "content": "In this section, we delve into several concepts crucial to this study, such as the large language foundation model (LLFM), LLM inference, and LLM evaluation metrics."}, {"title": "A. Large Language Foundation Model (LLFM)", "content": "For the sake of precision and lucidity, the term \"foundation models\" is employed within the machine learning paradigm antecedent to the emergence of large language models (LLMs), delineating a broader category of AI models that served as a benchmark for user applications [30]. Moreover, an LLFM, alternatively denoted as a Pre-trained Language Model (PLM) [31][32], undergoes training on a comprehensive and diverse dataset to function as a versatile substrate for various applications. After this phase, the model can be fine-tuned on reduced data to perform specific tasks [33]. It is important to acknowledge that the capacity and diversity of the foundation model are contingent on the size of the training dataset [34]. Therefore, while all LLMs can be categorized as foundation models, not all foundation models attain the scale of largeness.\n\nOne common factor among all Large Language and Large Language-Focused Models (LLLFMs) is their development by companies with substantial resources and workforces. These entities include OpenAI, Google Research, MetaAI, and others. For example, GPT-1 was trained using 4.5 GB of text over 30 days on 8 P600 GPUs, equivalent to 1 petaFLOP/s-day, and was publicly released in 2018 [35]. In 2023, GPT-4 underwent training involving both text prediction and Reinforcement Learning Hyperparameter Fine-Tuning (RLHF), where the specifics of the data volume and training duration undisclosed, yet estimated to range from 2.1 to 25 FLOP. In addition, more than 50 experts were engaged solely for adversarial testing, in addition to undisclosed others contributing to various facets of the system [36][37]. Llama 3, as described in its model card, was trained using two custom-built 24K GPU clusters, consuming 7.7 million GPU hours and processing over 15 trillion tokens. This dataset is seven times more extensive than the training dataset for Llama 2 [38][39]."}, {"title": null, "content": "Fine-tuning is essential when adapting a large language model (LLM) to downstream tasks. There exist various categories of fine-tuning techniques that are worth mentioning. First, fine-tuning the pre-trained parameters can be performed in either a full [40] or partial [41] manner, aiming to update the pre-trained parameters to suit a new task. Although this approach has demonstrated remarkable performance, particularly in domain-specific tasks, it is computationally expensive. Second, parameter-efficient fine-tuning (PEFT) involves adding a small trainable parameter for fine-tuning. PEFT utilizes only a small percentage of existing fine-tuned parameters, referred to as low-rank, to adapt to a downstream task and incorporates them into the pre-trained model [42]\u2013[44]. While this strategy balances performance and resource efficiency better than full fine-tuning, it increases model size. Finally, prompt-based fine-tuning [45], [46] is a method to construct prompts in a more insightful manner to optimize the model's performance without altering its parameters. In addition, advanced prompt tuning techniques, such as retrieval augmented generation (RAG), have been introduced and demonstrated to effectively mitigate LLM hallucinations [47]. However, a drawback of prompt tuning is that it requires users to have more experience in creating prompts or crafting RAGs that align with their objectives.\n\nIn general, the perspective on LLM fine-tuning may vary depending on the researcher's objectives. A large organization with abundant computing resources may prioritize high-accuracy downstream tasks or specific tasks. Conversely, for a small organization, institution, or individual researcher with limited resources, the objectives may include reducing fine-tuning computational overhead while enhancing overall performance."}, {"title": "B. LLM Text Generation and Inference", "content": "Large language models (LLMs) excel at comprehending human language and extracting insights from corpora of training data. Recent advances in this domain have reached a level of sophistication where distinguishing between machine-generated text and human-authored text has become increasingly challenging, despite numerous investigations [48] [49]. The text generation task is formally delineated as stated in [50].\n\n$y = f_M(x, P)$ (1)\n\nHere, the text generation model $f_M$ produces the output text y given the input data x that satisfies some special set of properties P. The property may be that the input is text, image, tabular data, a knowledge base, etc.\n\nDuring inference, the text generation model M, typically the decoder, produces output sequences $y_i$ conditionally on some information x, referred to as the prompt, where each $y_i$ represents a token (a word or a subword). Formally, given"}, {"title": null, "content": "$x_k \\in X$ to a model M where $i = 1, 2, ..., n$, the objective is to predict $y_k \\in Y$ where $j = 1, 2, . . ., k$. The conditional probability denotes this as follows:\n\n$P(y|x) = P(y_1|x)P(y_2|x, y_1) . . . P(y_k|x, y_1, \\cdot \\cdot \\cdot, y_{k-1})$ (2)\n\nIn this context, the model operates in an autoregressive manner [51], generating $y_1$ sequentially and appending it to the target input sequence to predict the subsequent sequence. Consequently, the data structure housing the computational weight mirrors that of a lower triangular matrix [52][53]. Various strategies have been proposed to optimize the value of k, resulting in a lengthy and coherent passage [54]. However, it is essential to maintain a balance between response length and throughput, especially concerning resource-constrained devices."}, {"title": "C. Google Gemma Architecture", "content": "In this subsection, our focus is on Gemma, which serves as a case study for large language foundation models (LLFMs). Google DeepMind released the model in two variations: one with 2 billion parameters and another with 7 billion parameters, as part of the Gemini model series [55]. Although the specific architecture remains undisclosed in the documentation [56], essential components are illustrated in the architecture depicted in Figure 1."}, {"title": "III. METHODOLOGY", "content": "In this section, we initially delineate the problem and present our hypotheses regarding its nature. Subsequently, we expound upon the methodology employed for the proposed frameworks and align them with the study objectives. The framework is delineated in two iterations: the conceptual and implementation frameworks. In addition, we investigate the constituent elements of these frameworks and preprocess the datasets preceding inference."}, {"title": "A. Problem Formulation", "content": "Our hypothesis proposes that when an expert addresses a query in a particular domain, they expend a level of cognitive effort that may diverge from that required for a Common question. This indicates a correlation between the model's domain expertise and inference overhead, manifested in the form of the time and computational resources consumed during the process. Formally, this relationship can be expressed as a 4-tuple:\n\n$O = f(t, g, x, y, q_y)$ (3)\n\nwhere x represents the prompt length, y denotes the response length, O signifies the inference overhead, t denotes the"}, {"title": "B. Proposed Framework", "content": "A framework to assess a large foundational model's comprehension of different domains is presented in Fig. 1. This is a three-dimensional representation of the problem domain, model size, and response control. In terms of the problem domain, we examine cybersecurity, medical, finance, and common questions. The model size includes Gemma-2B and Gemma-7B for tasks related to text generation inference. The response output is controlled, limited to 50 words, and unrestricted.\n\nWe present the implementation framework in Fig. 2. Alongside the predictive models Gemma-2B and Gemma-7B, ChatGPT serves as a referential model against which we evaluate the quality of the predictive models.\n\nTo ensure precision, the experiment unfolded in sixteen distinct phases (4 x 4), each corresponding to an output line (blue and red which comprises F, M, and C) from the models as delineated in the framework. Pairing the output from each model, we configured each to produce responses containing 50 or unlimited words. Subsequently, we compared the predictive model output from each configuration with that of the referential model.\n\nThroughout each inference phase, we meticulously recorded the inference time, response word length, GPU maximum consumption, and prompt word length data. In addition, we computed the inference throughput and latency and assessed response quality using the ROUGE-L and semantic text similarity (STS) metrics."}, {"title": "C. Data Analysis", "content": "In this study, we determine the statistical significance of the inference parameters and investigate the implications of the observed correlations between two variables. The correlation coefficient (typically Pearson's r) quantifies the linear relationship between two variables X and Y as follows:\n\n$r=\\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum(X_i - \\bar{X})^2 \\sum(Y_i - \\bar{Y})^2}}$ (4)\n\nA value of r equal to 1 indicates perfect positive correlation, whereas a value of 1 indicates perfect negative correlation. A value of 0 implies no correlation."}, {"title": "D. Formulation of Outlier Technique", "content": "Given a set of data points that represent the correlation between two variables, we define upper and lower boundaries within which the slopes, $m_{min}$ and $m_{max}$, can be determined. The margin between these bounds represents the concentration area of the data points with upper and lower slope margins taken at a specific value derived from Eq.5 \u2013 11. Outliers were identified as data points falling below the lower boundary.\n\n$m_{central} = \\frac{y_2 - y_1}{x_2 - x_1}$ (5)\n\n$\\theta_{central} = arctan(m_{central})$ (6)\n\n$\\theta_{step} = rad((\\mu_x + (1.96 * \\sigma_x))\\lambda)$ (7)\n\n$\\theta_{max} = \\theta_{central} + \\theta_{next}$ (8)\n\n$\\theta_{min} = \\theta_{central} - \\theta_{next}$ (9)\n\n$m_{max} = tan(\\theta_{max})$ (10)\n\n$m_{min} = tan(\\theta_{min})$ (11)\n\nFirst, a straight line is plotted from (0, 0). The slope ($m_{central}$) and the angle in radians ($\\theta_{central}$) are, respectively, calculated as follows. 5 and 6, respectively. The subsequent angle, $\\theta_{step}$, in radians, is computed in Eq. 7 using the 95% confidence interval between the max-line and the central line and then to the min-line, where $\\lambda$ is the tuning parameter for angle adjustment, and $\\mu_x$ and $\\sigma_x$ are the mean and standard deviation of the interval, respectively. Furthermore, $\\theta_{max}$ (Eq. 8) and $\\theta_{min}$ (Eq. 9) are determined using $\\theta_{central}$ and $\\theta_{step}$, which are then used to compute $m_{min}$ (Eq. 10) and $m_{min}$ (Eq. 11), respectively."}, {"title": "E. Dataset", "content": "In the investigation, we examined two categories of datasets: Common and domain-specific datasets, each comprising 2019 instances. The Common dataset was retrieved from GLUE (General Language Understanding Evaluation) [65]. To ensure fairness, we endeavored to exclude instances containing toxic content, as they could potentially be rejected by the model, affecting the response length. Furthermore, we ensured that the prompts included commonplace questions that anyone could easily answer without particular expertise.\n\nFurthermore, the domain-specific datasets consist of three domains: cybersecurity-oriented, finance-oriented, and"}, {"title": null, "content": "medical-oriented datasets. The cybersecurity dataset was obtained from [66]. Primarily, it consists of attack procedures originally curated from the MITRE ATT&CK [67], a globally accessible knowledge base of adversary tactics and techniques derived from real-world observations. Given an attack procedure as a prompt, the underlying premise is that we anticipate the model's ability to predict what the attacker aims to achieve. The finance-oriented dataset was acquired from [68], originally combining Stanford's Alpaca and FiQA datasets which have been used to facilitate the training and fine-tuning of diverse models tailored for financial applications. Subsequently, the medical-oriented dataset sourced from [69] was employed by the original author for training purposes in the development of the AI medical chatbot.\n\nIn the investigation, we examined two categories of datasets: Common and domain-specific datasets, each comprising 2019 instances. The Common dataset was retrieved from GLUE (General Language Understanding Evaluation) [65]. To ensure fairness, we excluded instances containing toxic content because they could be rejected by the model, which affected the response length.\n\nFurthermore, we ensured that the prompts included com-"}, {"title": "IV. RESULT DISCUSSION", "content": "The study results are presented in Table 1. Common prompts tend to generate responses with greater diversity in length and significantly longer inference times. The 7B models consume more GPU memory than their 2B counterparts, which is expected due to the differences in size. In all cases, the 2B models achieved higher throughputs, over 50%, than the 7B models, indicating that the 7B models incur more computation and memory overhead than the 2B models."}, {"title": "A. Analysis of Response", "content": "Regarding quality, using semantic textual similarity (STS) with ChatGPT responses as a reference model, the top five highest scores for semantic textual similarity (STS) are: 7B/Finance/$\\approx$ 50, 7B/Medical/$\\approx$ 50, 7B/Finance/$\\infty$, 7B/Medical/$\\infty$, and 7B/Common/$\\approx$ 50. Conversely, 2B/Cybersecurity/$\\infty$, 7B/Cybersecurity/$\\infty$, and 7B/Cybersecurity/$\\approx$ 50 have the lowest STS. This alignment implies that the words and phrases in 7B/Common/$\\approx$ 50 convey meanings that are more similar to those conveyed in the reference ChatGPT text. This indicates a higher degree of agreement or relevance. Higher STS scores indicate greater semantic similarity (or word-level similarity) between the text and reference text. In addition, restricted responses showed better ROUGE-L values for both common and domain-specific prompts, which implies that the responses contain similar keywords as the reference responses. It is important to note that the quality assessment of the responses using the STS and ROUGE-L in this study may not reflect the true value of response quality because the predicted and reference responses are often not of equal length and differ with a wide margin, which is a major criterion that may affect the quality assessment of datasets."}, {"title": "B. Analysis of Correlation and Outliers", "content": "Our results demonstrate a significant correlation between inference time and response length (Figure 5a-p). Table II provides further insight into this relationship. The 7B and 2B models with unrestricted responses to Medical, Finance, and Common prompts exhibit the highest time-response"}, {"title": null, "content": "correlations, and the cybersecurity model exhibits the lowest time-response correlation across all scenarios. Several key observations emerge from the results. The variability in correlation is depicted in Fig. 5 can be computed using the previously discussed outliers technique (Section III subsection C). A higher response in a shorter time indicates high throughput, and the primary objective is to identify outliers on the x-axis, i.e., inf-time. For the response-restricted cases, the value of $\\lambda$ is set to 0.005 for $\\theta_{max}$ and 0.5 for $\\theta_{min}$, whereas, for the unrestricted cases, the value of $\\lambda$ is set to 0.0005 for $\\theta_{max}$ and 0.05 for $\\theta_{min}$. This resulted in a variability ratio of 10 : 1 between restricted and unrestricted response lengths.\n\nCommon prompts exhibited the highest number of outliers across all cases compared to domain-specific prompts. This finding further confirms the inconsistency in response length associated with common prompts relative to their domain-specific counterparts. Within the domain-specific category, 7B/Finance/$\\infty$ manifests the highest number of outliers, which is attributed to its inclusion of content from related fields such as insurance, accounting, and taxation. In contrast, cybersecurity and medical prompts consistently yielded concise responses in all cases. The analysis of the outliers indicates that the values are nearly uniformly below the overall mean across all parameters except latency."}, {"title": "V. CONCLUSION", "content": "This study investigates the inference behavior of foundation models of varying sizes under common and domain-specific prompts, such as those related to cybersecurity, medical, and finance domains. This study examines these behaviors under conditions in which response lengths are both restricted and unrestricted. We present a framework to assess large foundational models in terms of domain understanding and outlier formulation. The results indicate that model size and types of prompts used for inference significantly influence response length and quality, as larger datasets for training provide more information across various domains. In addition, common prompts, which include different types of queries, generate diverse responses and may result in inconsistent response lengths when the same prompt is used multiple times or in different ways. In contrast, domain-specific prompts consistently generate concise responses. Therefore, we recommend eliminating irrelevant domains in the language model information prior to fine-tuning domain-specific tasks.\n\nWhile this study aimed to focus on resource utilization, response length, and response quality, we did not observe significant differences in resource utilization and response quality assessment because the results did not show statistically significant differences among the cases. Consequently, future research should focus on a comprehensive investigation of response quality across various domains and determine whether response length correlates with quality. In addition, resource usage must be assessed in a manner that does not affect inference time."}]}