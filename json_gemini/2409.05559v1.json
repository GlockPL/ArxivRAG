{"title": "CauseJudger: Identifying the Cause with LLMs for Abductive Logical Reasoning", "authors": ["Jinwei He", "Feng Lu"], "abstract": "Large language models (LLMs) have been utilized in solving diverse reasoning tasks, encompassing common sense, arithmetic and deduction tasks. However, with difficulties of reversing thinking patterns and irrelevant premises, how to determine the authenticity of the cause in abductive logical reasoning remains underexplored. Inspired by hypothesis and verification method and identification of irrelevant information in human thinking process, we propose a new framework for LLMs abductive logical reasoning called CauseJudger (CJ), which identifies the authenticity of possible cause by transforming thinking from reverse to forward and removing irrelevant information. In addition, we construct an abductive logical reasoning dataset for decision task called CauseLogics, which contains 200,000 tasks of varying reasoning lengths. Our experiments show the efficiency of CJ with overall experiments and ablation experiments as well as case studies on our dataset and reconstructed public dataset. Notably, CJ's implementation is efficient, requiring only two calls to LLM. Its impact is profound: when using gpt-3.5, CJ achieves a maximum correctness improvement of 41% compared to Zero-Shot-CoT. Moreover, with gpt-4, CJ attains an accuracy exceeding 90% across all datasets.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) have become a hot topic. Research on LLMs mainly focuses on pre-training (Vaswani et al., 2017; Brown et al., 2020), fine-tuning (Liu et al., 2023; Hu et al., 2021), application (Zhang et al., 2024; Shen et al., 2024), and LLMs reasoning (Kojima et al., 2022; Wang et al., 2022). Among them, LLMs reasoning utilizes LLMs' language comprehension ability. Typical methods include Chain-of-Thought (CoT) (Wei et al., 2022), which guides LLMs to provide step-by-step solutions, and Tree-of-Thought (ToT) (Yao et al., 2024), which constructs the reasoning shape of LLMs as a tree. However, the existing research mainly focuses on commonsense and deductive reasoning tasks (Kojima et al., 2022; Wang et al., 2022; Wei et al., 2022; Yao et al., 2024; Sun et al., 2023; Zhang et al., 2023b), lacking research on abductive logical reasoning. Can existing methods effectively identify the authenticity of possible cause for abductive logical reasoning? If not, what are major challenges and their solutions?\nTo answer these, we need to delve deeper into the cause judgement of abductive logical reasoning. It determines whether the cause of a phenomenon is correct based on established patterns and logical relationships. Compared to other reasoning types, it has two main characteristics (Magnani, 2023):\n(1) Different patterns of thinking: It uses reverse thinking, which deduces the cause from the result, defers from the deductive reasoning of forward deducing the results; (2) Highly affected by irrelevant information: Due to the absence of intermediate causes, abductive logical reasoning needs to consider a diverse combination of premises and rules, where irrelevant information can be a significant interference.\nTo address the above challenges, we draw inspiration from human ways of thinking: (1) Human uses hypothesis and verification method to transform reverse reasoning into forward reasoning (Bruner, 2017); (2) Human has an ability to exclude irrelevant information when facing diverse information during reasoning (Johnston and Dark, 1986). Based on these observations, we propose CauseJudger (CJ) framework for LLMs abductive logical reasoning with candidates, which reverses the thinking pattern and pre-filters irrelevant information. CJ consists of three modules:"}, {"title": "2 Related Works", "content": "Large Language Model. Recent years, the emergence of LLMs has brought great changes to the field of artificial intelligence. Representative research achievements include GPT-3 (Brown et al., 2020), LLaMA (Touvron et al., 2023), Mistral (Jiang et al., 2023), Falcon (Penedo et al., 2024), and others. These models have demonstrated powerful capabilities and have been applied across various fields (Zhang et al., 2023a, 2024). Research has shown that although LLMs possess strong generalization capabilities that can address various reasoning problems, there is still wide room for improvement in specific logical reasoning tasks (Xu et al., 2023; Huang and Chang, 2023). Fine-tuning (Liu et al., 2023; Hu et al., 2021) may potentially address this issue, but it can lead to challenges such as catastrophic forgetting. Moreover, for non-open-source LLMs, users can hardly fine-tune them. Researchers are exploring ways to enhance the reasoning capabilities of LLMs while still utilizing the original model.\nLarge Language Model Reasoning. By simulating human reasoning patterns and deliberately designing a framework for thinking, we can dismantle complex tasks and plan the reasoning process for LLMs. This approach has become a hot research direction (Ye et al., 2024; Shao et al., 2023; Romera-Paredes et al., 2024). Chain-of-Thought (CoT) can be divided into Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al., 2022). Zero-Shot-CoT enhances the reasoning ability of LLMs by simply adding a classic line of \"Let's think step by step\" to the instructions. Few-Shot-CoT involves adding examples of chains of thought to the instructions, allowing the LLMs to imitate the problem-solving steps described within. Self-Consistency CoT (SC-CoT) (Wang et al., 2022) employs the idea of multiple rounds of thinking, collecting answers from various reasoning paths and ultimately selecting the most consistent response as the final answer. Tree-of-Thoughts (ToT) (Yao et al., 2024) argues that the traditional left-to-right unidirectional decision-making process of LLMs has limitations. Therefore, it proposes modeling thinking as a tree, allowing the LLMs to consider multiple reasoning paths. Graph-of-Thoughts (GoT) (Besta et al., 2024) represents thinking as a graph, with nodes in the graph representing thoughts, enabling the LLMs to manipulate the nodes to enhance the quality of reasoning. Thought Propagation (TP) (Yu et al., 2023) gets inspiration from human cognition, allowing the LLMs to explore similar problems related to the input before solving the problem.\nAbductive Logical Reasoning. Unlike deductive reasoning, abductive reasoning is a reverse reasoning process that finds the most likely explanation for a particular observation (Thagard and Shelley, 1997; Josephson and Josephson, 1996). Currently, abductive reasoning can be mainly classified into two categories: abductive commonsense reasoning and abductive logical reasoning. Abductive commonsense reasoning aims to explain abnormal observations in daily events (Bhagavatula et al., 2020; Zhang et al., 2020; Rudinger et al., 2020; Wang et al., 2019). Abductive logical reasoning, on the other hand, involves finding the missing cause in the reasoning path from premises to conclusions based on clear facts. This type of reasoning does not require additional external knowledge. Abductive logical reasoning datasets include ProofWriter (Tafjord et al., 2021) and AbductiveRules (Young et al., 2022), but they are all oriented towards natural language generation tasks, and there is a lack of datasets specifically designed for abductive logical reasoning decision tasks. In the field of language models, the research on abductive logical reasoning mainly focuses on small model training or fine-tuning pre-trained models. Some studies have explored abductive logical reasoning in language models through fine-tuning a pre-trained Transformer (Young et al., 2022), reinforcement learning methods with knowledge graphs (Bai et al., 2023), self-supervised learning methods (Aakur and Sarkar, 2019), semi-supervised learning methods (Huang et al., 2020), and other approaches. However, to date, there has been little exploration of abductive logical reasoning methods for LLMs that do not require training or fine-tuning."}, {"title": "3 Preliminaries: Motivation and Dataset", "content": "3.1 Abductive Logical Reasoning (ALR) with LLMs\nCurrently, research on LLMs reasoning, whether the Input-Output (IO), chain-shaped CoT (Wei et al., 2022), parallel multi-chain-shaped SC-COT (Wang et al., 2022), or tree-shaped ToT (Yao et al., 2024), new ideas are built upon existing ones, following the mode of deductive logical reasoning. However, as shown in Figure 2, the thinking patterns of deductive logical reasoning do not align with the reverse abductive logical reasoning process, making them unable to effectively solve such problems. Due to the high difficulty and complexity of abductive logical reasoning, the capabilities of LLMs alone are insufficient, too. Therefore, there is a need to explore reasoning methods for LLMs that target to abductive logical problems. There two main challenges: (1) Reverse Thinking Pattern; (2) Dealing with Irrelevant Reasoning Information. When designing a LLMs abductive logical reasoning framework, it is crucial to focus on solving the above two challenges."}, {"title": "3.2 CauseLogics dataset", "content": "In order to better study the problem of identifying the cause in abductive logical reasoning, we establish a new dataset, called CauseLogics, a sample is shown in Table 1. \"Premises\" refer to the prerequisites for reasoning, \"Rules\" represent the guidelines for reasoning, \"Phenomenon\" designates the phenomenon of the cause to be traced, \"Possible Cause\" suggests potential cause for the phenomenon, and \"Label\" is the answer (True or False). The task is to reason whether the cause is reasonable for the phenomenon. To enhance the complexity of the task, we add some interference information as distractions in the premises and rules. Based on the length of the reasoning chain, the CauseLogics dataset is divided into four independent subsets with varying degrees of difficulty: Level 1 to Level 4. Level X indicates that data point in that dataset has a preset correct reasoning chain length of X.\nThe advantages of the CauseLogics dataset are threefold: (1) Our dataset is tailored for abductive logical reasoning decision tasks. Compared to generation tasks, our dataset includes both positive and negative examples. (2) Our dataset incorporates interference information, making the premises and rules more complex and thus increasing the difficulty level. (3) Our dataset is designed with difficulty grading based on the length of the reasoning chain, allowing researchers with different difficulty requirements to freely choose suitable subsets for their studies."}, {"title": "3.3 Dataset Statistics", "content": "Here, we conduct statistical analysis on our dataset and other datasets in the same field. As shown in Table 2, our dataset is specifically designed for abductive logical reasoning decision tasks, and our dataset comprises four subsets, with each subset containing 50,000 data items, totaling 200,000 data points. Furthermore, our CauseLogics dataset offers difficulty grading from Levelc1 to Levelc4 based on the length of the reasoning chain required for correct reasoning."}, {"title": "4 CauseJudger", "content": "4.1 Overall Design\nThis article proposes a LLMs abductive logical reasoning framework named CauseJudger, which primarily consists of three core modules: the Logic Reverse Module converts reverse thinking into forward thinking through hypothesis and verification. The Information Pruning Module removes irrelevant reasoning information. The Forward Reasoning Module uses LLMs to reasoning forwardly and get the final answer. As shown in Figure 3, the specific process is as follows: (1) Input premises, rules, target phenomenon, and possible cause. Logic Reverse Module assumes that a certain cause is correct and incorporates it into the original set of premises, forming a new set. At this point, a forward reasoning form is constituted.\n(2) Information Pruning Module utilizes LLMs to analyze the premises and rules, eliminating irrelevant information and outputting a new set that only contain relevant information. (3) Forward Reasoning Module, where the LLMs is used to reason based on the new premises and rules, as well as the target phenomenon. If the target phenomenon can be inferred, it indicates that the possible cause is a reasonable explanation for the phenomenon; otherwise, the possible cause is unreasonable."}, {"title": "4.2 Logic Reverse Module", "content": "Previous studies, whether modeling human thinking as a chain, a tree, or a graph, have emphasized that the generation of new ideas must be based on existing ones, reflecting a forward thinking process that is more natural and simpler. However, in abductive logical reasoning, the known information includes premises and rules, as well as the emergence of a new target phenomenon, but the specific intermediate cause is missing. In this scenario, reasoning involves a reverse process distinct from forward reasoning, which poses significant challenges for direct reasoning.\nLet $P$ be the initial set of premises, $P^*$ represent the new set of premises, and $hyp$ be the hypothesized cause to be determined. In LRM, we assume that the cause is correct and add it to the set of premises to form a new set: $P^* = H(P,hyp) = P \\cup {hyp}$. Subsequently, we will perform forward deduction on the target phenomenon based on the new set of premises, thereby changing the relatively difficult problem of reverse reasoning into forward reasoning."}, {"title": "4.3 Information Pruning Module", "content": "In the process of reasoning, a large amount of information should be processed, only a few of them are relevant to the answer. Irrelevant and redundant premises can easily confuse LLMs (Shi et al., 2023), and the presence of truly useful information amidst a large quantity of useless information can make it difficult to identify the former. Let $P^*$ denote the new set of premises generated in the previous step, $R$ be the set of rules, $phe$ be the target phenomenon, and $hyp$ be the hypothesized cause to be determined. $V_{LLM}(x, P^*, R, phe)$ represents the function of calling a LLMs to judge whether information $x$ is useful. $P^{IPM}$ and $R^{IPM}$ represents the new set of premises and rules after information pruning. In IPM, before LLMs performs actual reasoning, it removes premises and rules that is obviously unrelated to the target phenomenon: $P^{IPM} = {p_i \\in P^* | V_{LLM}(p_i, P^*, R, phe)}$ and $R^{IPM} = {r_i \\in R | V_{LLM}(r_i, P^*, R, phe)}$. This step aims to reduce the irrelevant premises and rules, minimizing interference for LLMs reasoning. In subsequent steps, the new set of premises and rules will be used instead of the original set."}, {"title": "4.4 Forward Reasoning Module", "content": "In this part, we use data that has been reversed and filtered in the previews steps, and conduct forward reasoning to determine whether the target phenomenon can be inferred. If phenomenon can be inferred, it indicates that the possible cause is a reasonable explanation for the target phenomenon; otherwise, it is not. Let $P$ be the initial premise set, $P^{IPM}$ and $R^{IPM}$ represent the new premise and rules set generated in IPM, $R$ be the rule set, $phe$ be the target phenomenon, and $hyp$ be the possible cause. $G_{LLM}(P^{IPM}, R^{IPM}, phe)$ represents calling a LLMs to determine whether the premises and rules can forward reason to infer the target phenomenon. Combining Section 4.2, 4.3, we can obtain an algorithm representation of CauseJudger, as shown in Algorithm 1."}, {"title": "5 Experiments", "content": "5.1 Results on CauseLogics Dataset\nSettings and Baselines. LLMs used in this experiment are gpt-3.5-turbo (OpenAI, 2022) and gpt-4-turbo (Achiam et al., 2023) developed by OpenAI. We set the temperature to 1. We uses our proposed CauseLogics dataset, and selects the top 100 data points from subsets Level 1 to 4 as test dataset. The evaluation metrics mainly focus on reasoning accuracy, with the number of calls to LLMs as an auxiliary reference. Given the zero-shot characteristics of CauseJudger, the experimental comparisons are IO, Zero-Shot-CoT and SC-CoT. IO (Vaswani et al., 2017) is a basic approach that poses questions directly to LLMs. Zero-Shot-CoT (Kojima et al., 2022) adding a classic \"Let's think step by step\" to the input instructions. SC-CoT (Wang et al., 2022) enhances the stability of Zero-Shot-CoT by integrating the results through voting.\nResults. The experimental results are presented in Table 3. It can be seen that CJ outperforms the selected comparison methods in terms of reasoning accuracy across the four levels. With gpt-3.5-turbo, our method achieves a 90% accuracy rate in Level 1, representing a 55% improvement compared to IO, a 41% improvement compared to Zero-Shot-CoT and a 40% improvement compared to SC-COT. In other levels, which involve longer reasoning chains, our method also performs well. With gpt-4-turbo, CJ has an accuracy rate of over 90% on all four datasets, which indicates that our method is also helpful for the reasoning of the most powerful LLMs. In terms of average calls for LLMs, CJ requires calling the LLMs twice. We will conduct ablation experiments separately for LRM+FRM and IPM. Since IPM does not include a reasoning component, IO method was used for the reasoning part in the \"IPM Only\" experiment. Through comparison, it can be seen that both LRM+FRM and IPM can improve the reasoning correctness of gpt-3.5 and gpt-4, and combining them provides greater assistance to LLMs reasoning. Such results validate the effectiveness of the modules, as well as our CauseJudger framework.\nDiscussions. We select 10 data points from experiment on Level 1 with gpt-3.5 and compared the average output length and proportion of sentences containing logical reasoning between the LRM+FRM and others in Table 4. A sentence with logical reasoning refers to a sentence that contains both analysis of premises and rules, and can lead to credible new conclusions. It can be seen that under LRM+FRM, the output of LLMs is more detailed, and LLMs tends to make more logical reasoning based on premises and rules. The possible reason is that the forward reasoning approach is simpler in terms of logic, making the task easier to understand.\nIn Table 5, it can be seen that, IPM can effectively reduce the number of useless premises, with an average of less than 0.1 useless premises retained. Meanwhile, it does not erroneously remove relevant information. The effectiveness of IPM is related to the difficulty of distinguishing useless information. In the context of our dataset, using IPM for once can already achieve good results. If in more complex tasks, it may be necessary to use IPM multiple times and remove different types of useless information in batches."}, {"title": "5.2 Results on Proofwriter Dataset", "content": "Settings and Baselines. In this experiment, we still uses gpt-3.5-turbo (OpenAI, 2022) and gpt-4-turbo (Achiam et al., 2023), and its parameter settings are the same as in the previous experiment. The experiment was conducted on the ProofWriter (Tafjord et al., 2021), but as it is a generative task, its form needs to be modified. We change the generation problem into a decision problem with positive labels, and mix different reasoning depths together. However, due to the lack of counterexamples, which has certain drawbacks and can only be used as auxiliary testing. In terms of evaluation indicators, the main focus is on reasoning accuracy, with the number of calls to the LLMs as an auxiliary reference. The comparison methods selected in this experiment are still IO (Vaswani et al., 2017), Zero-Shot-CoT (Kojima et al., 2022) and SC-CoT (Wang et al., 2022).\nResults and Discussions. The experimental results of CauseJudger on the modified Proofwriter dataset are shown in Table 3. Through comparison, it can be seen that the LRM+FRM and IPM alone can improve reasoning performance of gpt-3.5 and gpt-4. The overall accuracy of the CJ framework is excellent and higher than IO, Zero-Shot-CoT and SC-CoT. The results show that CauseJudger can also effectively improve the reasoning quality of LLMs on existing datasets, and has good applicability. It further verifies the effect of LRM+FRM and IPM acting alone and the efficiency of the overall framework of CauseJudger.\nWe select top 10 data points and manually analyze the effectiveness of LRM+FRM in Table 4. And analysis for IPM is shown in Table 5."}, {"title": "5.3 Case Study", "content": "We select a representative case of Level 2 subset in our CauseLogics dataset, with moderate difficulty. The difference in reasoning performance between IO (Vaswani et al., 2017) and CauseJudger is shown in Table 6. With CauseJudger, LLMs tends to perform logically rigorous symbolic reasoning, including more detailed step-by-step reasoning chains and using more mathematical language, which is beneficial for obtaining the correct reasoning path and results."}, {"title": "6 Discussion", "content": "6.1 Demonstration on potential application\nHere is a conceptual application of our method, serving as an example of the practical potential of our approach. In the current social context, smart technology-assisted healthcare is a significant research focus. By conducting preliminary assessments of patients' conditions and providing this information as a reference for doctors, we can enhance the accuracy and efficiency of medical diagnoses. Essentially, disease diagnosis is a form of abduction problem. This paper conducts a case study of the CauseJudger framework, focusing on the preliminary diagnosis of respiratory diseases, as shown in Figure 4.\nFirstly, we collect patient's symptom descriptions to determine the approximate category of disease, and retrieve detailed information from professional databases. Then, we conduct a possible respiratory disease hypothesis one by one through the Logic Reverse Module (LRM), and then use Information Pruning Module (IPM) to extract the content related to the disease from the materials, delete irrelevant information, and then use the Forward Reasoning Module (FRM) to judge the possibility of the patient's infection with each disease. Finally, we summarize the reasoning results of each disease hypothesis. For medical preliminary diagnosis tasks, which may involve diverse diseases and complex patient symptoms, CJ conduct hypothesis and verification reasoning method and exclude interference information, which shows the potential of our work for practical application. More detailed examples of inputs and outputs at each step are shown in Figure 5.\nIt should be clarified that this example only shows the potential effectiveness of the method proposed in this paper, and has not been clinically tested in actual medical settings. This example is not intended to illustrate that the method proposed in this paper can already be applied to medical assistance. The performance of this method may vary across different clinical settings and patient populations. So far, the method proposed in this article does not guarantee its practical application, especially in the medical field, and the use of this method for any practical application is prohibited."}, {"title": "6.2 Limitations and Future Directions.", "content": "Regarding the dataset, our current CauseLogics dataset still has some limitations. Its themes are not diverse enough, and the data patterns are relatively monotonous. As for CauseJudger (CJ), it has a certain degree of accuracy decline under conditions of longer logical chains. This may be related to the fewer invocation times. Meanwhile, CJ in this article is only applicable to the problem of identifying the authenticity of possible cause for abductive logical reasoning, which has not yet expanded to the ability to actively acquire and generate causes.\nBased on the above limitations, our future research directions will mainly focus on two aspects. Firstly, we will strive to explore more diverse and suitable abductive logical reasoning datasets for LLMs. These datasets will cover multiple fields such as healthcare and education. Secondly, we will delve deeper into reasoning methods for LLMs with long logical chains, aiming to improve reasoning accuracy for both reasoning results and reasoning paths. Finally, we will further explore abductive reasoning without the presence of candidate causes. We look forward to exploring more advanced and practical reasoning methods for LLMs in the future."}, {"title": "7 Conclusion", "content": "In this article, we conduct a detailed analysis of the definition and characteristics for the task of determining the authenticity of the cause in abductive logical reasoning, discussing the lack of decision task datasets and methods for LLMs. We construct the CauseLogics dataset, which includes data points with varying lengths of reasoning, addressing the lacks of decision task datasets for abductive logical reasoning. We propose a framework for LLMs abductive logical reasoning with candidates, called CauseJudger (CJ), which comprises three modules: Logic Reverse Module (LRM), Information Pruning Module (IPM) and Forward Reasoning Module (FRM). Our CJ framework undergoes quantitative experiments on the dataset proposed in this article and modified existing datasets, verifying the overall advancement of the framework and the effectiveness of each module. In addition, case study shows that CauseJudger can help LLMs apply logical thinking and symbolic formal expressions during reasoning. We believe that abductive logical reasoning for LLMs is a promising research direction, and our work comprehensively explores it from the perspectives of reasoning frameworks and datasets, hoping to inspire subsequent research."}]}