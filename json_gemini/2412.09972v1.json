{"title": "Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial Data Management Perspective", "authors": ["Yuchen Fang", "Yuxuan Liang", "Bo Hui", "Zezhi Shao", "Liwei Deng", "Xu Liu", "Xinke Jiang", "Kai Zheng"], "abstract": "Road traffic forecasting is crucial in real-world intelligent transportation scenarios like traffic dispatching and path planning in city management and personal traveling. Spatio-temporal graph neural networks (STGNNs) stand out as the mainstream solution in this task. Nevertheless, the quadratic complexity of remarkable dynamic spatial modeling-based STGNNs has become the bottleneck over large-scale traffic data. From the spatial data management perspective, we present a novel Transformer framework called PatchSTG to efficiently and dynamically model spatial dependencies for large-scale traffic forecasting with interpretability and fidelity. Specifically, we design a novel irregular spatial patching to reduce the number of points involved in the dynamic calculation of Transformer. The irregular spatial patching first utilizes the leaf K-dimensional tree (KDTree) to recursively partition irregularly distributed traffic points into leaf nodes with a small capacity, and then merges leaf nodes belonging to the same subtree into occupancy-equaled and non-overlapped patches through padding and backtracking. Based on the patched data, depth and breadth attention are used interchangeably in the encoder to dynamically learn local and global spatial knowledge from points in a patch and points with the same index of patches. Experimental results on four real world large-scale traffic datasets show that our PatchSTG achieves train speed and memory utilization improvements up to 10x and 4\u00d7 with the state-of-the-art performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Road traffic data comprises multiple traffic time series collected from points where road sensors are deployed. Thus traffic time series are correlated in not only the temporal aspect but also spatial domain. Forecasting future road traffic through past data plays an essential role in many real world intelligent transportation applications. For instance, users on Map platforms can select the least time path in advance according to the predicted traffic [7, 33, 40, 41]. Likewise, on city management platforms [6], users can control the signal light to avoid congestion based on future traffic [10, 29, 53].\nTo accurately forecast future road traffic, countless algorithms have been proposed in past decades, ranging from statistical models [2] to data-driven methods [27]. In the beginning, temporal modeling methods like the recurrent neural network (RNN) and autoregressive integrated moving average (ARIMA) are used to learn the temporal evolution of the single traffic time series in traffic data [28, 46], yet ignore the spatial transmission between multiple traffic time series. Subsequently, spatio-temporal graph neural networks [26, 45, 55] are at the forefront of collaboratively capturing spatio-temporal dependencies.\nHowever, the spatial correlation of traffic points is evolved over time and the fixed static graph in conventional spatio-temporal graph neural networks can not reflect various correlations in different time stages. Therefore, dynamic spatial modeling technology has been focused as the mainstream research line in traffic forecasting [17], which aims to reveal spatial correlations of each time slice respectively and dynamically propagate spatial information. Considering the quadratic complexity in most dynamic spatial modeling methods, traffic forecasting is only performed at zone scale, which is opposite to the realistic traffic forecasting needs in a city with thousands of traffic points [43].\nAs shown in Table 1, three widely used dynamic spatial modeling paradigms (dot-product, linear, and low-rank) are illustrated in the attention form, i.e., using query and key to dynamically calculate spatial correlations first and then propagating spatial information to the original value according to computed spatial correlations. Spatial correlations in dot-product-based methods such as D2STGNN [51] and STAEformer [39] should be calculated on each pair of points thus leading to unacceptable quadratic complexity in large-scale traffic forecasting. Despite linear-based BigST [20] and low-rank-based Airformer [37] being proposed to mitigate the high computation needs of dynamic spatial modeling in large-scale spatio-temporal forecasting, they still have some limitations. The drawback of linear-based [13, 20] is the lack of interpretability because spatial correlations can not be explicitly shown. For low-rank-based [12, 36], the blemish is a lack of fidelity, i.e., crucial information cannot be guaranteed to remain in the reduced low-rank representations and thus results in a performance drop.\nThis work aims to reduce the computation needs of dynamic spatial modeling with high interpretability and without information loss in large-scale traffic forecasting by proposing an efficient Transformer framework called PatchSTG. The sketch of PatchSTG is illustrated in Figure 1 and the core goal of PatchSTG is to reduce the number of points involved in dynamic calculations like vision Transformers [44]. However, traffic points are irregularly distributed on roads, and thus simply splitting the same number of points into patches with equal size to reduce complexity in regular spatial data-based vision tasks is unsatisfactory in traffic forecasting. Inspired by the spatial data management algorithm KDTree (short for K-dimensional tree), we propose an irregular spatial patching method to split the same number of traffic points into patches with unequal size, which first uses the novel leaf KDTree to recursively partition all the traffic points into leaf nodes with the small capacity, and then merges leaf nodes belonging to the same subtree into occupancy-equaled and non-overlapped patches through padding unfull leaf nodes and backtracking to the root node of subtrees. Finally, based on the patched data, PatchSTG first performs depth attention on points in a patch to learn local spatial knowledge and then conducts breadth attention on points in different patches but with the same index to efficiently aggregate multiple global knowledge. Notably, PatchSTG is interpretable and fidelity due to domain knowledge informed irregular spatial patching and non-compression dynamic spatial modeling.\nIn summary, we have made the following contributions:\n\u2022 We present PatchSTG, a generic Transformer framework tailored for efficiently and effectively predicting large-scale traffic from the spatial data management perspective.\n\u2022 To the best of our knowledge, we are the first to bridge the gap between KDTree and patching technique to evenly partition irregularly distributed traffic points with interpretability.\n\u2022 PatchSTG performs the depth and breadth attention on points in a patch and points with the same index to efficiently learn dynamic local and global spatial knowledge with fidelity.\n\u2022 Comprehensive experimental results on large-scale traffic datasets demonstrate that our PatchSTG achieves state-of-the-art performance and enjoys 4x memory reduction and 10x training speedup compared to dynamic spatial modeling baselines."}, {"title": "2 RELATED WORKS", "content": "Traffic forecasting has been a concern of research and industrial communities in past decades. Originally, the statistical-based vector autoregression (VAR) [2] and autoregressive integrated moving average (ARIMA) [28] are used to capture temporal dependencies. As deep learning is splendid in many tasks, recurrent neural network-based methods [46] and temporal convolution network-based methods [16, 23] are proposed to improve traffic forecasting performance. To simultaneously extract spatio-temporal information, DCRNN [34] and STGCN [61] constructed a fixed adjacency matrix in graph neural networks [25, 62, 63] based on real-world distances to capture static spatial information for traffic forecasting. Subsequent techniques such as GWNET [59], AGCRN [1], MTGNN [58], METRO [4], STG-NCDE [3], and Localised AGCRN [9] have further improved forecasting accuracy through the data-driven fixed adjacency matrix. Nevertheless, most of them ignored the evolved spatial correlations of traffic. For calculating dynamic point-to-point spatial correlations, most approaches such as ASTGCN [17], GMAN [65], ST-GRAT [49], DMSTGCN [21], GMSDR [38], and etc. [24, 35, 39, 56, 64] applied the dot-product operation introduced by the attention mechanism on hidden representations with different periods. However, point-to-point dynamic models have brought the efficiency bottleneck into large-scale traffic forecasting. Despite efficient dynamic spatial modeling methods such as linear-based Lastjomer [13], BigST [20] and low-rank-based HIEST [47], SSTBAN [18] have been proposed to reduce the complexity, explicit spatial correlations are failed to report in linear-based methods and performance is restricted in low-rank-based methods compared with perceptron-based STID [50] and SimST [42] due to spatial reduction caused information loss. Therefore, we propose a novel efficient dynamic spatial modeling method PatchSTG, which is intepretable and fidelity."}, {"title": "2.1 Traffic Forecasting", "content": "Traffic forecasting has been a concern of research and industrial communities in past decades. Originally, the statistical-based vector autoregression (VAR) [2] and autoregressive integrated moving average (ARIMA) [28] are used to capture temporal dependencies. As"}, {"title": "2.2 Efficient Spatial Transformers", "content": "The quadratic complexity of dot-product attention has become the bottleneck in applying Transformers to learn spatial knowledge, thus efficient spatial Transformers have been researched in recent years. For regular spatial data such as images and videos, the same number of neighbored pixels can be simply merged into the same size patches such as ViT [8] and SwinTransformer [44] to decrease complexity by reducing points in the calculation. Different from regular spatial data, GraphTrans-ViT [22] and PatchGT [15] can only derive the overlapped and unbalanced patches through clustering algorithms. Luckily, STRN [36], FPT [48] and OctFormer [54] segmented images into patches with different sizes based on the semantic and distribution, which gave us the inspiration to use spatial data management algorithms for patching time-ordered irregular spatial data into balanced and non-overlapped patches."}, {"title": "3 PRELIMINARIES", "content": "Traffic Data. Traffic data is made up of multiple correlated time series collected from points where road sensors are deployed. The recorded time series of the specific traffic point n can be formulated as xn \u2208 RH, which contains traffic volume within H time slices. Therefore, traffic data that comprises time series on N points can be formed as a matrix X \u2208 RH\u00d7N, where xh denotes the traffic flow of point n at time h.\nTraffic Forecasting. In the traffic forecasting task, the common setting involves predicting future traffic features through historical values. Specifically, the goal of our paper is to forecast the future traffic flow for the next F time slices according to information from"}, {"title": "4 METHODOLOGY", "content": "In this section, we present our PatchSTG framework, an effective and efficient solution designed for large-scale traffic forecasting. Figure 2 illustrates the overview of PatchSTG, which comprises four primary components: a spatio-temporal embedding module to preprocess traffic data into high-dimensional embeddings, an irregular spatial patching to split the same number of traffic points into patches, a dual attention encoder for extracting spatial information, and a projection decoder for predicting future values. We offer a detailed description of each component in the following."}, {"title": "4.1 Spatio-Temporal Embedding", "content": "Following previous works [30, 50], we adopt a fully-connected layer for each input traffic time series to transform their numerical traffic flow into high-dimensional embeddings. The detailed process of input traffic data X \u2208 RH\u00d7N can be formulated as follows:\n$E = W^{(1)}X + b^{(1)}$\nwhere $W^{(I)} \u2208 R^{d_e\u00d7H}$ and $b^{(1)} \u2208 R^{d_e}$ are learnable parameters of fully-connected layer. $E \u2208 R^{N\u00d7d_e}$ is the projected embedding that contains the temporal evolution of traffic. Moreover, we take"}, {"title": "4.2 Irregular Spatial Patching", "content": "Motivation. Spatial propagation is indispensable in improving traffic forecasting performance evaluated by some methods [39, 57] in addition to spatial distinguishability. This is because vehicles moving on the road will bring real-time traffic changes in the source and destination areas. However, the quadratic complexity of remarkable dynamic spatial modeling methods is unacceptable under current computation resources. Fortunately, we find that spatial information in vision Transformers [44, 54] can be efficiently propagated on the patched input by reducing the number of points involved in attention. The difference between traffic and vision data is that pixels are regularly located in images but traffic points are irregularly distributed on roads, i.e., the same number of pixels can be segmented into patches of the same size, but traffic points can not. Therefore, the main goal of our PatchSTG is to design a balanced and non-overlapped patching algorithm to reduce computation requirements of performing attention on irregular spatial data.\nLeaf KDTree. As irregular spatial data management is essential for database, geoscience, etc., numerous spatial partitioning algorithms such as KDTree [52] and RTree [19] have been proposed. Considering the balance, non-overlapping, and efficiency requirements of partition, we take the simple yet effective KDTree into account to find a solution for evenly dividing irregular traffic data. As shown in Figure 3a, KDTree is a binary spatial tree that uses each internal node as a partitioning hyperplane to split points contained in the node between its two children excluding the hyperplane and is built by recursing on each child node after partitioning, until leaf nodes are reached. Besides, the splitting hyperplane is determined by alternately chosen coordinate axes and the median point of the selected axis. In this paper, locations of latitude and longitude of traffic data are considered as axes to construct the tree. Unfortunately, as illustrated in Figure 3a, we find that hyperplane points in internal nodes of the conventional KDTree are not divided into leaf nodes and may result in irrelevant points being adjacent in the searching order. Therefore, we design a novel leaf KDTree as drawn in Figure 3b to enforce all points stored in leaf nodes, which utilizes the median value as the partitioning hyperplane for internal nodes with an even number of points, and the value between the median point and its left point as the partitioning hyperplane for internal nodes with an odd number of points. Moreover, from the illustration of our leaf KDTree in Figure 3b, we can observe that leaf nodes belonging to the same subtree maintain stronger spatial correlations based on their real-world closer distance, which provides the explainable backtracking for subsequent patching. After constructing leaf KDTree, we conduct the breath first searching on the tree to derive new indices of traffic points according to their searching order, which ensures that leaf nodes belonging to the same subtree are adjacent in the latest index. The entire process based on the latitude Lat \u2208 RN, longitude Lng \u2208 RN, and the capacity C of leaf nodes (leaf nodes in KDTree contain at most C points for a predetermined constant and C = 2 in Figure 3) can be formulated as follows:\n$idx = BFS(LKDT(Lat, Lng, C))$\nwhere idx \u2208 RN. LKDT(\u00b7) and BFS(\u00b7) denote the leaf KDTree construction and breath first search operation.\nPadding. Despite leaf KDTree can provide an equilibrium partition, the number of traffic points N is not necessarily divisible by the capacity C, which leads to unfull leaf nodes as shown in Figure 3c. The inconsistent number destroys the application of patch-based efficient methods. Padding zeros or irrelevant points can mitigate the non-divisible issue yet decrease prediction performance. To make leaf nodes have equaled occupancy and non-self-repeating, we pad the points that are most similar to the unfull leaf nodes from other leaf nodes to reach the maximum capacity, which can confirm the non-overlap patches by the similar time series:\n$idx, idx' = Query(LKDT(Lat, Lng, C), CosSim(X, X^T))$\nwhere Query() denotes querying most similar points of each un-full leaf node through the Cosine similarity CosSim(\u00b7). idx and idx' indicate locations should be padded in the new index and the corresponding original indices of queried points. Therefore, the padding"}, {"title": "4.3 Dual Attention Encoder", "content": "In this section, we present the dual attention encoder to dynamically capture spatial dependencies. For the patched input X \u2208 RR\u00d7P\u00d7d, R points in the first dimension can be seen as the root nodes of subtrees and P points in the second dimension can be seen points in the subtree. Therefore, PatchSTG first uses the depth attention on each patch to dynamically extract local spatial information because points in a subtree have stronger correlations. Moreover, as global dependencies are equally essential to the local information in traffic prediction [13], breadth attention is then adopted on the patch level to learn lossless global knowledge because each point in a root node of the subtree can receive information from points with the same index in other root nodes and these points are mixed with local information after previous depth attention. The dual attention can be interchangeably stacked L layers in the encoder, thus we describe the process of l-th layer in the following for simplicity.\nDepth Attention. Depth attention is the multi-head scaled dot-product attention, which is used to capture local spatial information in patches. As shown in Figure 4, the query, key, and value are first derived by using learnable linear transformations on the input, which can be formulated as follows:\n$Q^{(l)}_{(P)i} = W^{(l)}_{(Q)}x^{(l-1)}_{(P)i}$\n$K^{(l)}_{(P)i} = W^{(l)}_{(K)}x^{(l-1)}_{(P)i}$\n$V^{(l)}_{(P)i} = W^{(l)}_{(V)}x^{(l-1)}_{(P)i}$\nwhere 1 \u2264 i \u2264 O in our multi-head setting. $Q^{(l)}_{(P)i}, K^{(l)}_{(P)i}, V^{(l)}_{(P)i} \u2208 R^{RxPxdo}$ indicate the query, key, and value of each head in l-th"}, {"title": "4.4 Projection Decoder", "content": "In this section, we aim to predict the future traffic through the spatial information interacted output X(L) \u2208 RR\u00d7P\u00d7d of the dual attention encoder. We first unpatch the output to X(L) \u2208 RM\u00d7d through performing depth first search operation on each root node, and unpad leaf nodes to consist with the input shape and index:\n$\\tilde{Y} = UnPad(idx, X^{(L)})$\nwhere UnPad() denotes removing points on the unpadded locations and $\\tilde{Y} \u2208 R^{N\u00d7d}$ is the representation with original index. Finally, a fully connected layer is adopted to project the historical representation into the future:\n$\\hat{Y} = W^{(D)} \\tilde{Y} + b^{(D)}$\nwhere $W^{(D)} \u2208 R^{F\u00d7d}, b^{(D)} \u2208 R^{F}$ are learnable parameters and $\\hat{Y} \u2208 R^{F\u00d7N}$ indicates predicted traffic. During the training stage, we"}, {"title": "4.5 Complexity Analysis", "content": "The leaf KDTree takes O(Nlog(N)) complexity to construct the balanced binary tree based on the median hyperplane [5], which can be quickly done in the pre-processing stage. In the dual attention encoder, the cost of depth and breadth attention is respectively O(RP2d) and O(PR2d). Therefore, the dominant complexity of PatchSTG is O(max(P, R)Md), which requires less time than quadratic dynamic spatial modeling methods because P < N, R \u00ab N, and M \u2248 N in PatchSTG."}, {"title": "5 EXPERIMENTS", "content": "The goal of this section is to address the following five pivotal research questions by conducting comprehensive experiments on four large-scale traffic datasets.\n\u2022 RQ1: How does PatchSTG perform when compared to current approaches in large-scale traffic forecasting?\n\u2022 RQ2: What contributions do the main components of PatchSTG?\n\u2022 RQ3: How efficient is PatchSTG in large-scale datasets?\n\u2022 RQ4: Does PatchSTG output reasonable correlations?\n\u2022 RQ5: How do the essential hyper-parameters impact PatchSTG?"}, {"title": "5.1 Experimental Setup", "content": "5.1.1 Datasets. We conduct experiments on four large-scale datasets SD, GBA, GLA, and CA as introduced in LargeST [43]. Following previous settings, we not only chronologically split each dataset into train, validation, and test sets with a ratio of 6:2:2 but also utilize continuous 24 time slices as samples to perform traffic forecasting with the historical 12 time slices as the input and the future 12 time slices as the output. Detailed statistics of these datasets are shown in Table 2.\n5.1.2 Baselines. We compare 10 advanced baselines in this paper with our PatchSTG . (i) The non-spatial modeling-based STID [50], which uses identity spatio-temporal embeddings in fully-connected layers to forecast traffic. (ii) In the category of static spatial-based methods, we select GWNET [59], AGCRN [1], STGODE [14], and RPMixer [60]. They combine various static GNNs with temporal networks for traffic forecasting. (iii) Baselines in the dynamic spatial-based category include, DSTAGNN [31], D2STGNN [51], DGCRN [32], low-rank STWave [11], and linear BigST [20]. They dynamically reveal spatial correlations at different time periods for traffic forecasting.\n5.1.3 Evaluation Metrics. We utilize diverse evaluation criteria from performance and efficiency aspects for a comprehensive comparison. For the performance aspect: we utilize three commonly"}, {"title": "5.2 Performance Comparisons (RQ1)", "content": "Table 3 showcases the MAE, RMSE, and MAPE of traffic forecasting across all methods on four large-scale datasets except for failure to run with the smallest batch size of 1. The performance on the horizon 3, horizon 6, horizon 12, and the average of the whole 12 horizons are reported. To ensure a fair comparison, we follow official configurations of baselines, with the only adjustment of fixes input length to 12. Therefore baselines in our paper may show slight variations compared to the original results.\nAdvantages of Distinguishability. Among the baselines concerned, identity embedding-based non-spatial STID, shows a significant lead over spatial modeling methods on larger datasets such as the CA dataset. This phenomenon underscores the advantages of learning heterogeneous representations of different points to avoid over-smoothing in spatial message passing.\nAdvantages of Dynamic Spatial Modeling. As shown in Table 3, D2STGNN exhibits remarkable superiority, especially on the SD dataset when compared to non-spatial and static spatial-based methods. This appearance is owing to the dynamic spatial modeling. Despite point-to-point D2STGNN achieving great performance in small-scale datasets, it is still limited by the quadratic complexity in large-scale GLA and CA datasets.\nAdvantages of Explicit Spatial Aggregation. In large-scale datasets, in contrast to linear-based efficient dynamic spatial modeling method BigST, low rank-based STWave demonstrates clear advantages on all datasets under spatial reduction caused information loss, attributed to the implicit spatial correlations in linear-based methods are failed to correctly normalize.\nConsistent Performance Superiority. Drawing on the aforementioned components and our irregular spatial patching, we introduce PatchSTG, an efficient Transformer framework that achieves state-of-the-art performance on all datasets as evidenced in Table 3."}, {"title": "5.3 Ablation Study (RQ2)", "content": "In this section, ablation experiments are conducted on four datasets using four variants of PatchSTG to address RQ2. These four variants are listed below:\n\u2022 \"w/o FGGC\": PatchSTG fuse all points in the same patch into a single patch point during the breadth attention.\n\u2022 \"w/o Depth\": PatchSTG removes the depth attention and only global spatial correlations are modeled.\n\u2022 \"w/o Breadth\": PatchSTG removes the breadth attention and only local spatial correlations are modeled.\n\u2022 \"w/ PadDis\": PatchSTG pads points with closest distance into unfull patches.\n\u2022 \"w/o PadSim\": This variant only pads zero constants but not other points into unfull leaf nodes.\n\u2022 \"w/ METIS\": This variant uses the balanced graph partition algorithm METIS to replace our Leaf KDTree.\n\u2022 \"w/ KMeans\": This variant utilizes the unbalanced clustering algorithm KMeans to substitute our Leaf KDTree.\n\u2022 \"w/o LKDT\": PatchSTG no longer equips the leaf KDTree, i.e., dual attention is conducted on the original input.\nAccording to the results illustrated in Table 4, the following observations can be found.\nBenefits Brought by KDTree. Experimental results of \"w/o LKDT\" on all datasets show a huge drop in prediction performance,"}, {"title": "5.4 Efficiency Comparisons (RQ3)", "content": "To evaluate the efficiency of our PatchSTG, we present the training speed, inference speed, and batch size comparisons among our PatchSTG and previous dynamic spatial modeling methods with explicit spatial correlations, including DSTAGNN, D2STGNN, DGCRN, and STWave. As depicted in Table 5, PatchSTG attains the fastest speed and boasts the most efficient GPU memory utilization across all datasets, especially achieving up to 10x and 4\u00d7 improvements in speed and memory on large-scale GLA and CA datasets. While low-rank-based STWave also excels other methods in speed on large-scale datasets, the worse performance of STWave on small datasets compared to quadratic complexity-based DSTAGNN suggests that low-rank-based methods are less general than our PatchSTG under different settings."}, {"title": "5.5 Visualization (RQ4)", "content": "Interpretability. To verify the interpretability of our PatchSTG, we visualize the evenly segmented GLA dataset using our leaf KDTree in Figure 5. We can observe that the real-world adjacent points are obviously divided into the same leaf node to maintain spatial locality, which can give explainable spatial partition compared with low-rank-based dynamic spatial modeling methods. Moreover, our PatchSTG can explicitly show the learned local and global spatial correlations corresponding to their real-world locations on the segmented map, which is interpretable compared with linear-based dynamic spatial modeling methods.\nFidelity. To show our framework of learning global spatial knowledge without information loss, we conduct a case study on the GLA dataset, i.e., we visualize the learned patch correlations of"}, {"title": "5.6 Hyper-parameter Study (RQ5)", "content": "Figure 7 draws the impact of hyper-parameters on the representative GBA and CA datasets. We search the number of attention layers and the number of input fully-connected dimensions from a search space of [1, 3, 5, 7] and [32, 64, 128, 256]. First, the performance of PatchSTG improves as the layers of the encoder increase and tends to be best when there are 5 layers. Second, when the number of input dimensions is 64 and 128 on GBA and CA datasets, PatchSTG achieves the best performance. We can observe that the small model is enough to learn spatio-temporal knowledge in large-scale datasets due to rich patterns in big data.\nMoreover, we search the number of patches from a search space of [8, 16, 32, 64, 128, 256, 512] as shown in Figure 8. PatchSTG with 16, 16, 64, and 512 patches can achieve the best performance on SD, GBA, GLA, and CA datasets, which points out that the number of patches is positively correlated with the size of the dataset."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we introduce a novel efficient Transformer framework PatchSTG from the spatial data management perspective for large-scale traffic forecasting. PatchSTG first utilizes the leaf KDTree to recursively partition the equilibrium number of irregular traffic points into leaf nodes with interpretability. Then PatchSTG patches"}]}