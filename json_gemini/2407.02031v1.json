{"title": "SWIFTDIFFUSION: Efficient Diffusion Model Serving with Add-on Modules", "authors": ["Suyi Li", "Lingyun Yang", "Xiaoxiao Jiang", "Hanfeng Lu", "Zhipeng Di", "Weiyi Lu", "Jiawei Chen", "Kan Liu", "Yinghao Yu", "Tao Lan", "Guodong Yang", "Lin Qu", "Liping Zhang", "Wei Wang"], "abstract": "This paper documents our characterization study and practices for serving text-to-image requests with stable diffusion models in production. We first comprehensively analyze inference request traces for commercial text-to-image applications. It commences with our observation that add-on modules, i.e., ControlNets and LoRAs, that augment the base stable diffusion models, are ubiquitous in generating images for commercial applications. Despite their efficacy, these add-on modules incur high loading overhead, prolong the serving latency, and swallow up expensive GPU resources. Driven by our characterization study, we present SWIFTDIFFUSION, a system that efficiently generates high-quality images using stable diffusion models and add-on modules. To achieve this, SWIFTDIFFUSION reconstructs the existing text-to-image serving workflow by identifying the opportunities for parallel computation and distributing ControlNet computations across multiple GPUs. Further, SWIFTDIFFUSION thoroughly analyzes the dynamics of image generation and develops techniques to eliminate the overhead associated with LoRA loading and patching while preserving the image quality. Last, SWIFTDIFFUSION proposes specialized optimizations in the backbone architecture of the stable diffusion models, which are also compatible with the efficient serving of add-on modules. Compared to state-of-the-art text-to-image serving systems, SWIFTDIFFUSION reduces serving latency by up to 5\u00d7 and improves serving throughput by up to 2\u00d7 without compromising image quality.", "sections": [{"title": "1 Introduction", "content": "Text-to-image generation has been receiving substantial attention and evolved into a mature service offered to users, e.g., OpenAI's DALL-E [24] and Adobe's Firefly [4]. It demonstrates astonishing efficacy and has been integrated into various creative applications (e.g., advertisements and one-click virtual try-on experiences). Adobe reported that its Firefly service has generated over 2 billion images [3], underscoring the increasing demand for text-to-image capabilities. Similarly, our company has experienced a surge in requests for our commercial text-to-image service, with daily request volumes reaching up to 100k.\nIn this paper, we share our experience in serving production text-to-image workloads. We present a comprehensive characterization study based on a 20-day workload trace collected from our cluster with more than 300 nodes. The workload consists of real user request traffic to our text-to-image services, each with specific requirements regarding the content and composition of the generated images. Unlike previous studies [5, 36] that focus solely on serving text-to-image requests using a large base stable diffusion model, e.g., Stable Diffusion XL (SDXL) [25], our trace discloses that add-on modules, i.e., ControlNets [44] and LoRAs [17], are widely used in image generation: over 98% of requests use at least one ControlNet and over 95% of requests use at least one LoRA. These modules augment the base model and empower users to effortlessly control image generation, liberating them from numerous trial-and-error rounds of composing prompts to generate images that precisely match their mental imagery. See Figure 1: ControlNets enable users to provide a reference image to guide the spatial composition of the generated image; further, LoRAs can stylize the image with astonishing effect, expanding the creative possibilities of text-to-image generation.\nDespite the efficacy of add-on modules, providing text-to-image service is challenging, as users expect low latency for real-time interaction [5, 24]. However, we observe that existing serving systems [5, 31] suffer from prolonged serving latency, especially when add-on modules are incorporated to serve an image generation request. First, add-on modules"}, {"title": "2 Background", "content": "2.1 Text-to-Image Serving\nWorkflow. Text-to-image generates an image conditioned on a text prompt. State-of-the-art text-to-image models are stable diffusion models [25, 27], which outperform classical models, e.g., GANs, in terms of image quality and alignment with text prompts.\nA typical diffusion model consists of three main components: a text encoder [26], a convolutional UNet model [28], and a decoder-only variational autoencoder (VAE). Given a text prompt, the text encoder encodes the prompt into token embeddings. The image generation process starts by initializing a latent tensor filled with random noise. The UNet takes in the latent tensor and token embeddings, denoises the latent tensor conditioned on the token embeddings over tens of sequential steps, and produces a final latent tensor that the VAE decoder uses to paint the final image. Figure 4 illustrates the generation workflow where the initial noisy latent tensor (t1) undergoes 50 sequential and iterative denoising steps, resulting in the final latent tensor (t50), which is subsequently used by the VAE decoder to render the output image. At each denoising step, the UNet predicts the noise in the latent tensor of the current step and generates a new latent tensor by subtracting the predicted noise from the current latent tensor. The noise prediction is conditioned on the token embedding, which is mapped into UNet via cross-attention [27, 30].\nThe convolutional UNet [28] serves as the backbone architecture of existing stable diffusion models [5, 25, 27, 28]. It consists of encoder blocks, a middle block, and skip-connected decoder blocks, with transformer blocks placed in [25].\nComputational intensive inference. Generating an image using stable diffusion models is computationally intensive, as the generation of a single image can fully saturate the"}, {"title": "2.2 Control Image Generation with ControlNets", "content": "Why using ControlNets? Due to the internal randomness in image generation, users often struggle to control the image generation process because text prompts alone can hardly express complex layouts, compositions, and shapes precisely. Given the same prompt, the SDXL model can generate totally different images (Figure 1-Left). Consequently, it is common that users will suffer from numerous trial-and-error cycles of editing a prompt, inspecting the resulting images, and then re-editing the prompt to generate an image that matches their mental imagery [44].\nControlNet is a neural network that augments the base stable diffusion models by supporting additional input conditions, like edge maps and depth maps, to specify the precise spatial composition desired in generated images. See Figure 1-Center, where an edge map is provided as a reference to guide the image generation process. With the same prompt, the base model can generate a Ferrari that adheres to the spatial compositions specified in the edge map. As such, ControlNets allow users to provide a reference image to dictate their desired image compositions, enabling fine-grained"}, {"title": "2.3 Stylized Image Generation with LoRAs", "content": "LoRA is a parameter-efficient approach to enhancing the base model performance for domain-specific tasks or customizing the inference results [17]. See Figure 1-Right, where we use a LoRA to make the SDXL model generate an image in the papercut style. LoRA modifies parts of the base model parameters by patching new parameters to take effect. Specifically, given a pre-trained weight matrix $W \\in \\mathbb{R}^{H_1 \\times H_2}$ in a base model, a LoRA introduces two low-rank matrices $A \\in \\mathbb{R}^{H_1 r}$ and $B\\in \\mathbb{R}^{r \\times H_2}$, where r is the LoRA rank. By modifying the weight matrix to $W' = W + AB$, the LoRA effectively stylizes the final generated image, infusing it with the desired visual characteristics."}, {"title": "3 Characterization Study", "content": "In this work, we present the first characterization study of inference request traces for text-to-image applications in production clusters. Our study encompasses two core text-to-image services such as one-click virtual try-on and image generation on an e-commerce platform. The traces were collected over 20 days, comprising more than 500k diffusion model inference requests in total. We recorded the information of each request's invocation of ControlNet"}, {"title": "3.1 Characterizing ControlNet Usage", "content": "First, we analyze the usage patterns of ControlNet.\nThe use of ControlNet is ubiquitous. As shown in Table 1, nearly all inference requests employ at least one ControlNet to constrain the image generation results. Approximately 70% of the requests in both services even utilize two ControlNets simultaneously.\nControlNet invocations exhibit a skewed distribution. Compared to the hundreds of thousands of requests, the number of ControlNets is often limited. Service A offers fewer than 50 diverse ControlNets in total and Service B includes less than 100. These ControlNets exhibit a severe imbalance in access frequency, with certain ControlNets being invoked at a high rate (as shown in Figure 6-Left). In the ControlNet inference of Service A, 11% of the ControlNets account for 98% of the total invocations. A similar observation can also be made in Service B, where 9% of the ControlNets contribute to 95% of the invocations.\nControlNets benefit from caching in GPU memory. The characteristics of ControlNets, such as their limited quantity and uneven distribution, motivate us to cache the frequently used models. As shown in Figure 7, if we cache ControlNets in GPU memory, we can significantly reduce the overhead of the ControlNet switching, if two consecutive requests use different ControlNets. In practice, we dynamically load ControlNets into GPU memory using an LRU cache. In this way, there is no need to fetch the ControlNet model weights from remote storage when processing stable diffusion model inference requests.\nSystem inefficiency in serial execution of ControlNets. The existing state-of-the-art text-to-image inference frameworks [31] sequentially execute the computational graphs of ControlNet(s) and the base model. The results of ControlNets must be computed before being passed to the base model for the next step of computation (Figure 5). This process typically iterates multiple rounds (e.g., 50). The root cause lies in the computation-intensive nature (Figure 2-Right) of diffusion models, where GPU saturation prevents us from efficiently executing the base model and ControlNets in parallel on a single GPU. Referring to Table 1, requests can invoke up to 3 ControlNets. Thus, executing ControlNets sequentially undoubtedly increases the end-to-end latency of image generation. After an in-depth analysis of the model computation workflow (Figure 5), we discover that the base model does not require the results of ControlNets at the beginning of its execution but instead incorporates them during the middle of the computation. This motivates us to deploy each ControlNet as an independent service for invocation, thereby accelerating end-to-end performance. We elaborate on our design for deploying ControlNets in \u00a74.1."}, {"title": "3.2 Characterizing LoRA Usage", "content": "Next, we analyze the usage patterns of LoRA.\nLoRAs are widely used. The vast majority of inference requests typically involve 1 or 2 LoRAs to stylize the final generated image. In Table 1, over 90% of the requests in Service A overlay 2 LoRAs during inference, while nearly 74% of the requests in Service B overlay 1 LoRA.\nLoRA invocations exhibit a long-tail distribution. As shown in Figure 6-Right, LoRAs have a large number of invocations in the long tail, which accounts for a significant proportion, unlike the skewed distribution of ControlNet invocations. Service A has nearly 7k different LoRAs, and Service B includes almost 7.5k different LoRAs.\nLoRAs benefit from caching in a distributed memory system. Due to the large number of LoRAs and the lack of skewness in their distribution, increasing the size of the LORA cache does not significantly reduce the overhead of LORA model switching (Figure 7). Figure 8 shows that the number of unique LoRAs used on each inference worker is positively correlated with the number of requests, making it impractical to cache all LoRAs in local GPU memory or even local host memory, considering the fact that the size of LoRA in production is in the order of hundreds of MiB. In practice, we fetch the model weights of LoRAs from the"}, {"title": "4 Design", "content": "We next present our design and practices to serve stable diffusion models based on our characterization study. Figure 9 illustrates the overview of system components, which consists of the base stable diffusion model, multiple ControlNets, and a LoRA storage."}, {"title": "4.1 ControlNets-as-a-Service", "content": "Motivation. Our characterization study reveals the widespread adoption of ControlNets in production clusters due to their astonishing effect on controlling the image generation process (\u00a73.1). However, the existing system [31] falls short in supporting ControlNets as it couples the computations of ControlNets and the base diffusion model in a sequential manner (Figure 5). During each denoising step in image generation, a ControlNet takes the reference image, the text prompt, and the latent tensor at the current step as inputs. It then runs inference through the encoder blocks and the middle block, passing the outputs to the base model for subsequent inference. When multiple ControlNets are present, they execute sequentially and their outputs are aggregated. In [31], ControlNets and base models are collocated on the same GPU, and the base stable diffusion model cannot commence inference until all ControlNets have completed their computation. This computation flow overlooks the opportunity of using multiple processors for acceleration.\nDesign. SWIFTDIFFUSION identifies the opportunities of parallel computing in serving ControlNets. The connection between ControlNets and the base diffusion model enables the reconstruction of the inference computational graph by breaking it into a serial part and a parallel part, as illustrated in Figure 9. The serial part consists of the one-time computation of the text encoder, one-time computation of the VAE decoder, and UNet decoder computation during denoising steps. SWIFTDIFFUSION parallels the computation of ControlNets and the computation of UNet encoder blocks, and distributes the computations across different GPUs: the UNet is placed on one GPU while each ControlNet is allocated to a separate GPU. Such a computing paradigm exhibits pleasingly parallelism for two reasons. First, the computation workload of a ControlNet is almost identical to that of the encoder blocks and the middle block in UNet. The only difference is that the ControlNet has additional zero convolution operators, making the computation time of ControlNet 1.1\u00d7 longer than UNet's encoder blocks and the middle block. Second, the communication between the ControlNet and UNet is lightweight, i.e., 108 MiB, when using SDXL as the base model. With high-performance communication links, e.g.,"}, {"title": "4.2 Efficient Text-to-Image with LoRAs", "content": "Motivations. As discussed in \u00a73.2, LoRAs are stored in a local disk or remote cache system. To apply a LoRA for stylizing the image generation, the system typically takes two steps. First, it fetches the LoRA from storage and loads it into memory. After that, it patches the LoRA to the base stable diffusion model by merging its weights with the parameters of the base model. Our measurement shows that loading a LoRA with a size of 384 MiB takes 490 ms, and patching it takes 2 seconds, which is intolerable considering the inference latency of the UNet is 2.67 seconds.\nAsynchronous LoRA loading. We analyze the dynamics of image generation and observe that the effect of LoRA is imperceptible during the initial 30% denoising steps. We empirically validate this by running the image generation process twice: one with LoRA patched on the diffusion model and the other without. We collected and calculated the cosine similarity between the latent tensors and the predicted noises (Figure 4) generated with and without the LoRA at each denoising step, as demonstrated in Figure 10-Left. The cosine similarities are consistently above 99% during the initial denoising steps, indicating that LoRA exerts minimal effects at this time.\nDriven by this observation, SWIFTDIFFUSION proposes overlapping the LoRA loading with the initial denoising stage, as illustrated in Figure 10-Right. When a request arrives, SWIFTDIFFUSION initiates asynchronous loading of the corresponding LoRA. In the meantime, it early-starts the base stable diffusion model inference without LoRA patched. Upon completion of the LoRA loading, SWIFTDIFFUSION patches the LoRA onto the base model by merging its weights with the parameters of the base model (\u00a72.3). The base model then proceeds with the remaining image generation process. SWIFTDIFFUSION executes LoRA loading in a separate process and utilizes shared memory to transfer LoRA weights from the loading process to the base model serving process for efficient data transfer (Figure 9). When serving SDXL on a H800 GPU, a LoRA with a size of 456 MiB is patched on the base model at the 11th denoising steps on average, with a minimal overhead of 0.1 seconds (See 0C/0L and 0C/1L in Figure 2-Left). In cases where a request uses multiple Lo-RAS, SWIFTDIFFUSION launches multiple loading processes to load the LoRAs in parallel.\nInspired by [7], we also try a fine-grained pipeline loading scheme to overlap LoRA loading with base model inference. We divide a LoRA into M groups. SWIFTDIFFusion initiates loading the first group and simultaneously runs base model inference without LoRA. Starting from the second group, a pipeline is established, allowing SWIFTDIFFUSION to patch the (m - 1)-th group and load the m-th group in parallel, where m = 2, 3, . . ., M \u2013 1. Though the fine-grained loading scheme allows SWIFTDIFFUSION to patch parts of LORA earlier, it introduces additional overhead due to multiple merging of LoRA weights and does not significantly improve image quality.\nEfficient LoRA patching. Existing system [31] uses the PEFT [22] to merge LoRA weights with base model parameters. For a layer in the base stable diffusion model that will be patched with LoRA, PEFT creates a new LoRA layer to replace the original layer in the base model. The new LoRA layer augments the corresponding base model layer with LoRA weights and configurations. However, such a create_and_replace operation incurs high overhead, taking 2 seconds for a LoRA of 341 MiB and occupying extra GPU memory. Though keeping a separate copy of LoRA weights"}, {"title": "4.3 Optimized Stable Diffusion Model Inference", "content": "In addition to add-on modules, we also optimized the base model inference, particularly the UNet backbone, which accounts for over 93% of the base model inference latency (Figure 15). Here, we introduce three optimization strategies that collectively yield up to 20% improvements in UNet inference (\u00a76.5).\nDecoupled CUDA Graphs. CUDA graph [1] is a commonly used GPU optimization strategy, first introduced in CUDA 10. It can merge multiple GPU operators into a single graph and then pass it to the GPU for computation through a single CPU launching operator, thereby reducing the overhead caused by frequent GPU/CPU switching. Compared with traditional deep learning models, we found that diffusion model inference can better benefit from CUDA graphs. As discussed in \u00a72.1, the batch size used in diffusion model serving is usually 1, and the dimensions of the input and output tensors are fixed. This implies that we do not need to maintain a large number of CUDA graphs to accommodate potentially varying input and output sizes, which is often required by traditional deep learning models and large language models. We can pre-compile CUDA graphs based on predefined dimensions and directly invoke them for GPU computation at runtime. Furthermore, since we distribute the computation of ControlNet and UNet across different devices, causing UNet to require the incorporation of ControlNet's results during the computation process, we cannot naively apply CUDA graphs to the entire UNet computation graph. To address this issue, we manually split the UNet backbone into two decoupled CUDA graphs, which are stored in GPU memory after the initial execution to accelerate subsequent executions. Experiments (\u00a76.5) show that our CUDA graph design can reduce UNet inference latency by 6.4%.\nUNet [28] is a classic convolutional network architecture. It primarily consists of linear layers, convolutional layers, and matrix multiplication operations. In addition to these common model layers, we found that optimizing the GEGLU activation function and fusing GroupNorm and SiLU operators significantly improve inference efficiency in practice. Our observation is consistent with Google's findings [10].\nOptimized GEGLU operator. GEGLU is an activation function used in the attention layer [30] within UNet blocks. Taking SDXL [25] as an example, there are 70 GEGLU operators in its UNet. We implemented a high-performance CUDA operator for the GEGLU activation function, which improves the operator computation speed by 31% and boosts the end-to-end inference speed of the base model by 6%.\nFused GroupNorm and SiLU operators. Each convolutional layer in UNet blocks contains a combination of GroupNorm and SiLU operators. SDXL [25] has a total of 35 such combinations. We implemented an efficient CUDA operator to fuse the two operators, avoiding data copying in GPU memory. The results show that it can improve the computation by 76% and increase the inference speed by 7.2%."}, {"title": "5 Implementation", "content": "We have implemented SWIFTDIFFUSION on top of Diffusers [31], a PyTorch-based diffusion model inference framework that integrates state-of-the-art model optimization strategies. SWIFTDIFFUSION is written in 5.5k lines of Python and 2.4k lines of C++/CUDA code. ControlNets-as-a-Service, asynchronous LoRA loading, and decoupled CUDA graphs are implemented in Python, while customized CUDA operators are developed to accelerate the base model. When a request arrives, a separate process is launched to load LoRA weights asynchronously and transfer the LoRA weights to the base diffusion model serving process via shared memory (\u00a74.2). LoRA weights are then patched onto the parameters of the base model. At each denoising step, SWIFTDIFFUSION aggregates the results of ControlNet computations across GPUs via NVLink [2]. The decoupled CUDA graphs are maintained in standalone LRU caches, preventing out-of-memory errors."}, {"title": "6 Evaluation", "content": "We evaluate SWIFTDIFFUSION's performance in terms of serving latency and image quality. Evaluation highlights include:\nSWIFTDIFFUSION achieves efficient serving performance without degrading image quality, outperforming strong state-of-the-art baselines, e.g., Nirvana [5] (\u00a76.2).\nControlNets-as-a-Service accelerates text-to-image serving with ControlNets, achieving a speedup that accords with the theoretical gains (\u00a76.3)."}, {"title": "6.1 Experimental Setup", "content": "Model and serving configurations. We adopt SDXL [25] as the base model for our experiments. The model and its variants have been widely used in our production cluster and benefit from comprehensive support for add-on modules such as ControlNets and LoRAs. The ControlNets and LoRAs used in our setup are publicly accessible through the HuggingFace repository. We serve SDXL and ControlNets with NVIDIA H800 GPUs.\nBaselines. We re-implement Nirvana [5], the SOTA text-to-image serving system, and compare it with SWIFTDIFFUSION. The key idea behind Nirvana is to skip the first K denoising steps by utilizing a pre-cached image generated from a similar prompt to replace the randomly initialized noise latent (Figure 4). By generating the image based on an intermediate representation instead of starting from scratch with noise, Nirvana aims to reduce the number of required denoising steps and improve serving latency. In our re-implementation, we prepare the pre-cached images using the same prompts that will be used to generate the images for quality evaluation.\nIncluding Nirvana [5], we consider the following baseline systems:\nDIFFUSERS represents the standard text-to-image serving workflow incorporating ControlNets and LoRAs [31]. Images generated by DIFFUSERS adhere to the standard diffusion model inference process, which executes ControlNets sequentially and synchronously applies LoRA to the base diffusion model. While this approach yields images of standard quality, it incurs a relatively lengthy serving latency.\nNIRVANA-10 [5] omits ten denoising steps during image generation, i.e., K = 10.\nNIRVANA-20 [5] aggressively skips twenty denoising steps during image generation, i.e., K = 20.\nMetrics. We evaluate each baseline in terms of serving latency and image quality. For serving latency, we measure the end-to-end latency of generating an image based on a given text prompt. For image quality, we use the following quantitative metrics, which are considered essential and widely used in measuring image quality [5, 21, 25, 41, 45].\nCLIP [15, 26] score evaluates the alignment between generated images and their corresponding text prompts. A higher CLIP score indicates better alignment (\u2191).\nFr\u00e9chet Inception Distance (FID) score [16] calculates the difference between two image sets, which correlates with"}, {"title": "6.2 End-to-End Performance", "content": "Serving latency. We measure requests' average serving latency with different numbers of add-on modules and compare the results of each baseline in Figure 11. SWIFTDIFFUSION shows its advantage across all settings that require add-on modules, achieving up to a 5\u00d7 speedup. SWIFTDIFFUSION outperforms other baselines by distributing ControlNet computation across GPUs (\u00a74.1) and patching LoRA efficiently (\u00a74.2). Even in the absence of add-on modules, SWIFTDIFFUSION achieves a 1.16\u00d7 speedup compared to DIFFUSERS, due to its optimizations in UNet backbone (\u00a74.3). With 0C/0L, NIRVANA-20 is 0.26 sec faster than SWIFTDIFFUSION. However, it falls short of generating high-quality images, which we will elaborate on later.\nServing throughput. Figure 12 illustrates the request serving throughput of each baseline, measured as the number of images produced per minute of GPU time. SWIFTDIFFUSION achieves up to a 2x higher throughput compared to other baselines with 1C/2L, benefiting from its efficient design of LoRA loading and patching (\u00a74.2). Despite leveraging more GPUs for serving ControlNets, SWIFTDIFFUSION'S pleasingly parallel design enables it to rival the throughput achieved by DIFFUSERS (See 1C/0L). This demonstrates SWIFTDIFFUSION's ability to greatly reduce serving latency while maintaining high throughput (Figure 11 and Figure 12). NIRVANA-20 also achieves good throughput in some cases due to its aggressive design, but at the expense of generating lower-quality images.\nImage Quality. We compare the quality of images generated by each baseline. Since our design of ControlNets-as-a-Service does not make any difference in the content of generated images, we focus on evaluating the LoRA's effects. Two settings are considered: the first uses a single LoRA to generate images in a papercut style, while the second employs two LoRAs to generate images in a combination of William Eggleston photography style and filmic style. We use the prompts in P2 that emphasize vivid details in the generated images.\n1) Quantitative evaluation. Table 2 shows the CLIP scores achieved by each baseline, which measure the alignment between generated images and their corresponding prompts. The results indicate that all baselines exhibit comparable performance in terms of alignment.\nTable 3 shows the FID, LPIPS, and SSIM score achieved by each baseline. These metrics focus on comparing the generated images with the real images (\"ground truth\"). Therefore, we use the images generated by DIFFUSERS as the ground truth, as it represents the original text-to-image serving workflow, while NIRVANA-10, NIRVANA-20, and SWIFTDIFFUSION introduce slight modifications to accelerate the image generation. We also consider a new baseline NOADDON, which does not employ any add-on modules in image generation. We can see that SWIFTDIFFUSION outperforms other baselines, achieving the best performance across all metrics. NIRVANA-10 and NIRVANA-20 fall short because they generate an image based on the contents of a cached image, which is selected only based on the prompt similarity. However, even with the same prompt, the visual contents in cached images can be drastically different (See Figure 1) and may not"}, {"title": "6.3 Microbenchmark: ControlNet-as-a-Service", "content": "This section evaluates the performance of SWIFTDIFFUSION'S ControlNets-as-a-service design at a micro-benchmark level, isolating it from our LoRA design and optimizations in the UNet backbone. We compare DIFFUSERS and SWIFTDIFFUSION, as Nirvana [5] lacks specialized designs for ControlNets. Figure 16-Left illustrates the serving latency achieved by DIFFUSERS and SWIFTDIFFUSION, where SWIFTDIFFUSION achieves up to 2.2\u00d7 speedup by distributing ControlNets computation across multiple GPUs. Notably, SWIFTDIFFUSION'S design of ControlNets does not alter the image generation process, ensuring that the images generated by DIFFUSERS and SWIFTDIFFUSION are identical in this context."}, {"title": "6.4 Microbenchmark: Text-to-Image with LoRAs", "content": "This section evaluates SWIFTDIFFUSION's design for efficient image generation with LoRAs at a micro-benchmark level, excluding our ControlNet design and optimizations in the UNet backbone. We also exclude Nirvana [5] since it does not have specialized designs for LoRAs. As described in \u00a74.2, DIFFUSERS requires two steps to patch on a LoRA: first, loading the LoRA from a local disk or remote in-memory caching system, and then creating a new LoRA layer and replacing the corresponding layer in the base model to merge the LoRA weights [22]. This process incurs a high latency overhead, as shown in Figure 16, with up to a 2.3\u00d7 increase in serving latency when using two LoRAs. For the single LoRA case, we use a LoRA of 341 MiB. For the two LoRA cases, we use LoRAs of 341 MiB and 456 MiB. Figure 16-Right shows that SWIFTDIFFUSION's design significantly reduces the overhead"}, {"title": "6.5 Microbenchmark: Optimized UNet Inference", "content": "This section evaluates SWIFTDIFFUSION's optimizations on the base diffusion model inference without adding any add-on modules, thereby disabling all optimizations for ControlNets and LoRAs. As Nirvana's design only affects the number of denoising steps, we primarily compare SWIFT-DIFFUSION with DIFFUSERS. As introduced in \u00a74.3, we design three main techniques for base model inference: decoupled CUDA graphs, CUDA-optimized GEGLU operators, and fused GroupNorm and SiLU operators. Figure 15 records the results of the ablation experiments in detail. SWIFTDIFFUSION can accelerate the UNet inference by up to 1.2x, resulting in an end-to-end latency acceleration of 1.18x. These three techniques contribute to performance improvements of 6.4%, 6%, and 7.2%, respectively."}, {"title": "7 Related Works", "content": "Model serving systems. Existing research on model serving systems focuses on reducing latency [11, 23, 33, 38, 39], improving throughput [6, 38], enhancing performance predictability [12, 43], and conserving resources [13, 32, 42]. These studies concentrate on optimizing various workloads, including graph neural networks [34], recommendation models [39], and large language models [23, 33, 40]. Our work is orthogonal to the aforementioned efforts, as we focus on accelerating text-to-image diffusion models, which have drastically different computation intensity and workflow.\nText-to-image diffusion model inference. Diffusers [31] is an out-of-the-box inference framework that incorporates state-of-the-art optimization strategies tailored for diffusion models. DeepCache [21] leverages the temporal consistency of high-level features to reduce redundant computations. DistriFusion [19] facilitates the parallel execution of diffusion models across multiple GPUs. Nirvana [5] employs approximate caching to skip a certain number of denoising steps. Yet, these works only consider optimizing the base model and do not consider the overhead introduced by add-on modules (i.e., ControlNets and LoRAs) during model inference. Our work conducts a pioneering analysis of the status quo in production diffusion model deployment. Driven by real-world traces, we propose several techniques to address the system inefficiencies caused by the invocations of add-on modules.\nServing systems with add-on modules. In the domain of large language models, cutting-edge research [9, 20, 29] has proposed efficient inference techniques for models with add-on modules (i.e., LoRAs), such as CUDA-optimized operators for batched LoRA computations on GPUs and efficient GPU memory management mechanisms. These works aim to enable multi-tenant sharing of the base model to accommodate more LoRA adapters within the same batch. Yet, batching yields minimal benefits in diffusion model inference, due to its compute-intensive nature. Thus, these multi-tenant optimization strategies work ineffectively in our scenario. In addition to LoRA, these works also overlook ControlNet, a specialized add-on module in diffusion model inference."}, {"title": "8 Conclusion", "content": "We present SWIFTDIFFUSION, an efficient text-to-image serving system that augments image generation with ControlNets and LoRAs. Driven by our comprehensive characterization study on production workloads of commercial text-to-image service, SWIFTDIFFUSION proposes three novel designs that reconstruct existing text-to-image serving workflow. First, SWIFTDIFFUSION leverages the opportunities of parallel computation to substantially accelerate ControlNet inference. Second, SWIFTDIFFUSION overlaps LoRA loading with base model inference and efficiently patches on LoRA weights. Last, SWIFTDIFFUSION optimizes the UNet backbone of state-of-the-art diffusion models. Compared to existing systems, SWIFTDIFFUSION can achieve up to a 5\u00d7 reduction in serving latency and a 2x improvement in throughput, without sacrificing image quality."}]}