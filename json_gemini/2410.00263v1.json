{"title": "Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation", "authors": ["Kun Yuan", "Vinkle Srivastav", "Nassir Navab", "Nicolas Padoy"], "abstract": "Surgical video-language pretraining (VLP) faces unique challenges due to the knowledge domain gap and the scarcity of multi-modal data. This study aims to bridge the gap by addressing issues regarding textual information loss in surgical lecture videos and the spatial-temporal challenges of surgical VLP. We propose a hierarchical knowledge augmentation approach and a novel Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) framework to tackle these issues. The knowledge augmentation uses large language models (LLM) for refining and enriching surgical concepts, thus providing comprehensive language supervision and reducing the risk of overfitting. PeskaVLP combines language supervision with visual self-supervision, constructing hard negative samples and employing a Dynamic Time Warping (DTW) based loss function to effectively comprehend the cross-modal procedural alignment. Extensive experiments on multiple public surgical scene understanding and cross-modal retrieval datasets show that our proposed method significantly improves zero-shot transferring performance and offers a generalist visual representation for further advancements in surgical scene understanding.", "sections": [{"title": "1 Introduction", "content": "The recent advancements in multi-modal representation learning, particularly with the introduction of CLIP [50], have led to the development of models capable of understanding a wide range of visual concepts using natural language supervision [32, 39]. The expressive natural language has allowed these models to shift from task-specific to more generalist applications [47, 79, 80]. The learned representations of these models are robust, facilitating effective performance across diverse visual tasks without the need for task-specific fine-tuning [65, 78]. However, despite the impressive progress made by these models in the general computer vision domain, the effectiveness of these methods in domain-specific settings remains uncertain.\nThis concern is particularly relevant to the field of Surgical Data Science (SDS), an emerging interdisciplinary domain that utilizes deep learning and computer vision techniques to analyze surgical data [42, 41, 71]. A key component of SDS is the analysis of intraoperative surgical videos captured through endoscopes or laparoscopes. Analyzing these videos presents several unique challenges compared to the general computer vision datasets. Unlike general computer vision datasets [45, 50, 6], surgical videos can last several hours and capture complex and fine-grained activities within a narrow field of view. This requires development of computational approaches to decompose and model the surgical procedures at multiple hierarchical levels, including the entire procedure [27], phases [64, 15], steps [52, 29], atomic actions [5, 7], and action triplets [48, 59]. Moreover, surgical language involves specialized vocabulary, and annotating videos requires clinical expertise, limiting dataset scalability. Consequently, current deep learning applications are restricted to single-centric, fully-supervised, and task-specific approaches [64, 66, 29, 48, 71, 5, 54].\nTo bridge the gap, recent efforts have focused on creating surgical video-text pretraining datasets by curating surgical lecture videos from online e-learning platforms and pairing them with transcribed narrations using audio speech recognition (ASR) methods. Subsequently, a CLIP-style model [73] is trained contrastively to match the video clips to their corresponding textual descriptions. Building on this, the HecVL approach introduces the use of hierarchical texts, which include phase-level keystep descriptions and video-level summaries that provide hierarchical goals of the surgical procedure [72]. However, challenges persist due to the smaller size of the surgical video-language pretraining dataset, noisy transcribed narrations, limited variability in phase-level descriptions, and strong temporal dependencies in surgical procedures, where actions and keysteps occur in a specific routine order. These issues hinder the accurate learning of multi-modal surgical representations.\nTo address these challenges, we propose Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP), which boosts data efficacy and tackles the spatial-temporal challenges inherent in surgical procedures from two perspectives. First, we introduce hierarchical knowledge augmentation to mitigate the problem of textual information loss in surgical video-language pretraining datasets. We argue that the internal knowledge of LLMs serves as a valuable surgical knowledge base, enriching and correcting text descriptions while preserving the original key concepts and meanings. Therefore, We utilize the large language model (LLM) prompted with different behaviors as external knowledge base to correct, explain, or summarize the hierarchical texts in the surgical video-language pretraining dataset, thus providing diverse and better language supervision for multi-modal pretraining. Additionally, it reduces the risk of overfitting by preventing the text encoder from repeatedly encountering the same keystep texts in each epoch.\nFrom the pretraining objective perspective, we perform the hierarchical video-language pretraining, as shown in Fig. 1, with a novel hierarchy-specific loss, $LecNCE$. Specifically, we combine language supervision with visual self-supervision at the clip-level pretraining to introduce additional supervision signals within vision modality, making the pretraining efficient with a small surgical dataset [73]. At phase- and video-level pretraining, we construct hard negative samples by reversing the order of texts, followed by a Dynamic Time Warping (DTW) based loss function to learn the temporal alignment between video frames and texts, thus facilitating the understanding of cross-modal procedural alignment during pretraining."}, {"title": "2 Related Works", "content": "Surgical Video-Language Pretraining: many works have demonstrated the effectiveness of learning visual representations from the natural language supervision of corresponding text [6, 67, 74, 38, 44, 40, 32]. These methods conduct contrastive learning [49] to match the video clips (or images) with their corresponding narrations (or captions). Similarly in the medical field, recent works have started to curate large-scale multi-modal data through hospital-sourced chest radiological reports [26, 11] and online platforms [73, 25, 24], e.g., YouTube and Twitter, to perform vision-language pretraining. However, these works encounter the sample efficiency issue when handling the smaller surgical video-language pretraining dataset (SVL) [73]. Recent works improve the data efficacy and zero-shot performance of CLIP-style models [46, 35, 23]. However, they do not capture procedural dependency from the long-form surgical videos beyond the video clip and text matching. Hierarchical pretraining methods [3, 76, 72] propose to pair video clips of different durations to different hierarchical levels of texts, covering both short- and long-term understanding. Paprika [77] builds a procedural knowledge graph and elicits the knowledge node during the video-language pretraining process.\nTextual Augmentation with Knowledge Base: the success of vision-language pretraining is highly dependent on the quality and quantity of available multi-modal data. Recent research [36] shows that a smaller high-quality dataset can outperform a larger low-quality dataset. Common practices improve the quality by textual augmentation, including EDA [35], masked token modeling [62], and captioning loss [69]. Recent studies have used synthesized captions from captioning models to achieve notable improvements [31, 30, 55]. However, they show scalability deficiency and world knowledge loss in models trained with synthetic captions [70], which their initial benchmark success has largely obscured. To inject the knowledge, K-Lite [60] enriches the texts with WordNet [14] and Wiktionary [43] knowledge base. Merlot [75] learns script knowledge representations from millions of YouTube videos, however, a knowledge domain gap exists when applying this to the surgical field. The recent advent of self-supervised large language models like GPT4 [2] and Llama series [63] have been a game-changer, as they encode rich domain-specific knowledge, e.g., clinical knowledge [61], motivating LaCLIP [13] to augment textual inputs through the LLM rewrites."}, {"title": "3 Approach", "content": "We summarize our contributions as follows: First, we propose an LLM-based knowledge augmentation to handle surgery-specific textual information loss in the dataset, providing more densely interconnected natural language supervision from surgical lecture videos. Second, our proposed hierarchical video-language pretraining method enforces the understanding of the spatial-temporal characteristics of surgical lecture videos at different hierarchical levels. The pretrained PeskaVLP demonstrates state-of-the-art transferability and visual representation to different surgical scene understanding downstream datasets [64, 66, 29], across types of surgical procedures and clinical centers. It also shows strong multi-modal alignment ability through the cross-modal retrieval task at multiple hierarchical levels."}, {"title": "3.1 Dataset and Contrastive Learning", "content": "Learning joint video and language embedding space requires a large-scale video-language dataset, however, such datasets are expensive and time-consuming to create in the surgical field. Therefore, the first surgical video-language pretraining dataset, i.e., SVL [73], is proposed by obtaining around a thousand surgical lecture videos from surgical education platforms. SVL collects ~300 hours of lecture videos accompanied by narration texts obtained using Audio Speech Recognition (ASR) methods, providing ~26k video clip-narration pairs for contrastive video-language pretraining. Specifically, short video clips $x_c$ and their corresponding narration texts $y_n$ are treated as positive pairs $P_n$, and the unpaired ones are treated as negative pairs $N_n$. Then, the contrastive training loss InfoNCE [49] can be formulated as follows:\n$L_{InfoNCE} = max_{f,g} \\sum_{i=1}^{B} log \\frac{\\sum_{(x_c,y_n) \\in P_n} e^{f(x_c) \\cdot g(y_n)}}{\\sum_{(x_c,y_n) \\in P_n} e^{f(x_c) \\cdot g(y_n)} + \\sum_{(x',y'_n) \\sim N_n} e^{f(x') \\cdot g(y'_n)}}$"}, {"title": "3.2 Hierarchical Knowledge Augmentation", "content": "Quality of language supervision matters [1, 35, 34] especially when the surgical video-language dataset is not \"big\" enough, e.g., millions of multi-modal samples used in [50, 45], to sufficiently cover the visual-linguistic concepts. In this work, we find that the texts suffer from different types of degradation at different hierarchies, failing to provide accurate and broad concepts for pretraining. Specifically, as shown in Fig. 2, narration texts are mostly sentence fragments and easily affected by misspelling errors, therefore altering the original key concepts. The keystep texts are mostly short and abstract, resulting in a narrow set of linguistic concepts that could show poor transferability to the downstream datasets, which usually come with a different set of concepts [60, 16]. The abstract texts sometimes include redundant and useless information, such as author and citation information.\nTo address the above hierarchy-specific textual degradation, we propose a hierarchical knowledge augmentation to correct/explain/summarize the narration/keystep/abstract texts, respectively, by eliciting LLM's encoded surgical knowledge [61]. For each hierarchy, we manually design the system prompt and several input-output examples for LLM. Thus, we obtain hierarchical LLM assistants with different behaviors of using internal surgical knowledge to augment the texts:\nNarration. We ask the LLM to behave as a \"recipe\" to come up with a list of sequential steps that complete the given surgery. For each lecture video, we feed its title as input and obtain the list of pseudo steps, as shown in Fig. 2 (a), building a surgical step knowledge base. Then, we assign these pseudo steps to narration texts based on textual similarity. This implicitly corrects the typos in transcribed narrations and augments the textual input based on the LLM's surgical knowledge.\nKeystep. As shown in Fig. 2 (b), we ask the LLM to behave like a \u201cdictionary\" to explain the meaning of the keystep. Specifically, the LLM assistant expands the given keystep into a description of the main surgical events, anatomies, and instruments involved. This enlarges the textual semantic information of each keystep and provides more expressive language supervision for pretraining.\nAbstract. As shown in Fig. 2 (b), we ask the LLM to behave like a \u201csummarizer\u201d that captures the key concepts of the given abstract texts, e.g., surgical type, anatomies, and so on. This reduces the length of the textual inputs while maintaining the main concepts of the abstract paragraph. In the following experiment, we randomly input the original or augmented texts for video-language pretraining. Check Appendix H for examples of pre- and post-augmented texts."}, {"title": "3.3 Procedure-aware Surgical Video-language Pretraining", "content": "We introduce PeskaVLP, a procedure-aware pretraining framework for the above surgical knowledge-augmented video-language dataset. We emphasize devising a pretraining objective $LecNCE$ for the hierarchical video-text pairs. For clip-level pretraining, $LecNCE_{clip}$ combines language supervision with visual self-supervision to improve data efficiency and boost the scene understanding on visually similar laparoscopic images. $LecNCE_{phase/video}$ considers the procedure awareness during the coarser-level pretraining, through a DTW-based contrastive regularization objective with temporally reversed text sequences as negative samples. We apply the dual-encoder as our model architecture."}, {"title": "3.3.1 Clip-level Pretraining", "content": "Language Supervision. The common pretraining objective for dual-encoder model is InfoNCE [49], as denoted in Eq. 1, where matched video text pairs are treated as positive while all other pairwise combinations in the batch are regarded as negative. In this work, we also apply $L_{InfoNCE}^{vl}$ to maximize the similarity between short-term video clips and their corresponding narration texts at the clip level, denoted as $L_{clip}^{vl}$. However, this simple objective is data hungry and sensitive to the weakly aligned noisy video-text pairs from small-scale surgical video-language datasets, such as SVL [73].\nVisual Self-supervision. To address that, our PeskaVLP introduces an additional supervisory signal from visual self-supervision to complement noisy language supervision. Specifically, we explore the widespread supervision within visual modality to learn generic visual representation. We adopt the simple yet effective SimSiam [10] strategy, whose objective is to maximize the similarity between two augmented views. As shown in Fig. 3 (a), during the pretraining, we apply random distortion on the frames of video clips and generate two augmented embedding vectors for one video clip. We then apply $L_{InfoNCE}^{vl}$ to maximize the similarity of these two augmented embeddings by treating them as positive pairs, denoted as $L_{clip}^{vv}$. This additional supervisory can learn visual features more efficiently and is robust to the distortion of surgical scene images. Finally, the $LecNCE$ loss for clip-level pretraining is the sum of these two losses, denoted as $LecNCE_{clip} = L_{clip}^{vl} + L_{clip}^{vv}$"}, {"title": "3.3.2 Phase-/Video-level Pretraining", "content": "The surgical video-language pretraining presents a unique procedural challenge compared to the existing video-language methods [17, 45, 50, 68, 58]. The surgical actions and events occur in a certain order to follow the routine to complete the surgical phase and surgery, e.g., \u201chook dissecting cystic duct\u201d should happen before \u201cclipper cutting cystic duct\u201d in the \u201cclipping cutting\u201d phase of cholecystectomy surgery. However, prior contrastive learning objectives [44, 50, 17] omit this temporal dependency and limit the understanding of procedural knowledge in surgical lecture videos.\nOur proposed LecNCE training objective enables procedural understanding in phase- and video-level pretraining by considering the cross-modal temporal alignment between video frames and text sequence. Specifically, hierarchical texts can form the parent-child correspondence, i.e., abstract (parent-level) and keystep (child-level) texts, keystep (parent-level) and narration (child-level) texts. As shown in Fig. 3 (b), each parent-level text A is paired with a video segment $V = \\{v_1, ...v_\\tau\\}$, where the $T$ is the number of frames of the video segment. A is also paired with a child-level text sequence $B = \\{b_1, ...b_n\\}$, where N is the length of this sequence. Then, we build the cost matrix $C \\in R^{T*N}$ between video frames and child-level text sequence based on their embeddings, with each element $C_{i,j}$ computed by a distance function D. We adopt the same distance function from [19]:\n$C_{i,j} = D(v_i, b_j) = - log \\frac{exp(v_i \\cdot b_j/\\beta)}{\\sum_{k=1}^{N} exp(v_i \\cdot b_k/\\beta)}, v_i = f(v_i)/||f(v_i)||_2 b_i = g(b_i)/||g(b_i)||_2$\nUsing this cost matrix C, we apply Dynamic Time Warping (DTW) to find the minimum cross-modal cost path that aligns the video frames to the text sequence, denoted as $DTW(C)$. We then make a reasonable assumption that the global semantics of the text sequence and its reversed version are distinct. Therefore, aligning the video frames to the text sequence should be easier, i.e., incur a lower alignment cost compared to aligning the same video frames when the text sequence is played in reverse. Following this assumption, we temporally reverse the child-level texts into $B' = \\{b_n, ...b_1\\}$ and build the cost matrix $C'$ between V and B, computing the minimum alignment cost $DTW(C')$. We then devise a DTW-based contrastive regularization using hinge loss as follows:\n$L_{dtw} = max((DTW(C) - DTW(C')), \\phi)$\nwhere \u03c6 is the margin between positive and negative samples. This imposed regularization can support fine-grained multi-modal representation learning from weakly paired video frames and texts via temporal alignment. Unlike Paprika [77], which relies on a pretrained model [44], our phase-/video-level pretraining provides a direct, lightweight, and more adaptable methodology to unseen surgical domains. We do not require the adaption from any existing models, improving the generalization capability. Also, our pretraining process is procedure-aware in itself rather than modifying the representation in a second step, streamlining the process and increasing efficiency. We also apply the $L_{InfoNCE}^{vl}$ loss to maximize the similarity between the paired parent-level text, video segment, and child-level texts, denoted as $L_{Infonce}^{vl}$. Note that the $L_{Infonce}^{vl}$ follows the same pipeline as in Fig. 1 (b) and (c). Finally, we achieve the loss $LecNCE$ for phase- or video-level pretraining as $LecNCE_{phase/video} = L_{Infonce}^{vl} + \\lambda L_{dtw}$, where \u03bb is the hyper-parameter to scale two losses. Please refer to Appendix D for more details about dynamic time warping. Finally, we train the model in an alternating way, using the proposed hierarchical levels of learning objectives. We only train one set of visual and textual encoders for all three levels, ensuring the encoders are optimized for capturing both short-term and long-term semantics. We alternatively train with 25 batches of clip-level samples, followed by 15 and 115 batches of phase- and video-level samples."}, {"title": "4 Experiments", "content": "Datasets. Our pretraining is conducted on the videos of SVL [73] dataset. The pertaining dataset includes hierarchical textual annotations from the metadata of the videos [72]. We evaluate our model on 3 publicly available surgical phase recognition downstream datasets, i.e., Cholec80 [64] (cholecystectomy) from Strasbourg center, AutoLaparo [66] (hysterectomy) from HongKong hospital, MultiBypass140 [29] (gastric bypass) from both Strasbourg (StrasBypass70) and Bern (BernBypass70) centers. These datasets contain untrimmed surgical workflows with frame-wise phase labels. We also evaluate pretrained model on the cross-modal retrieval task in multiple hierarchical levels with the holdout videos in SVL-Retrieval [73]. Check Appendix A for more details about the pretraining dataset.\nTraining Parameters. We utilize the dual-encoder architecture with ResNet50 [21] as visual encoder and ClinicalBert [22] as textual encoder, respectively. We train the model with a batch size of 120/80/25 for clip-/phase-/video-level, respectively. We sample 4/16/64 frames for videos of clip-/phase-/video-level. We use AdamW optimizer [28] with a learning rate of 5e - 5. We train the model with 4-80 GB NVIDIA A100 GPUs for 200 epochs. Temperature parameter \u03b2 for distance function and \u03c6 for DTW-base contrastive loss function D are fixed as 0.1. Scale factor \u03bb is set as 0.01.\nEvaluation Setup. We evaluate pretrained models using two setups: Zero-Shot evaluation and Few/Full-shot Linear Probing evaluation. For Zero-Shot, we utilize class text prompts, the same as HecVL [72], to compute cosine similarities between image embedding and class text embeddings, classifying images based on the shortest distance. In Linear Probing, the pretrained visual encoder remains frozen when we extract features for each image, subsequently training a linear classifier using the SGD optimizer. We consider one shot as one percentage of the videos in the training set because each video contains frames from different classes. Check Appendix B for more details."}, {"title": "4.1 Zero-shot Surgical Phase Recognition", "content": "High-quality Surgical Video-language Dataset. As shown in Tab. 1, our approach achieves a significant performance improvement over the baselines MIL-NCE [44] and CLIP [50] pretrained on the natural computer vision datasets, even though our pretraining dataset is 10, 000 times smaller than those. Noted that when the CLIP model is randomly initialized and then trained with SVL, its performance declines compared to initializing from OpenAI. This shows that our surgical video-language pretraining dataset lacks the scale necessary to adequately pretrain a robust video-language model from scratch. ViT [12, 8] architectures are sensitive to initialization and excluded from this work. Further insights into the impact of initialization can be found in Appendix C.\nTransferability across Surgical Procedures and Centers. Compared to the HecVL, our method achieves over 12.3% and 7.8% improvement in absolute accuracy and f1, thanks to our spatial-temporal LecNCE learning objective across multiple hierarchies. Also, the consistent boost on cholecystectomy [64], hysterectomy [66], and gastric bypass [29] procedures show the generalizable and transferable features of PeskaVLP. Comparing the results of StrasBypass and BernBypass, we find that PeskaVLP can recognize the phases of the same kind of surgery (gastric bypass), even if these surgeries are performed in different centers and follow different procedural routines. More qualitative results can be found in Appendix F."}, {"title": "4.2 Zero-shot Cross-modal Retrieval", "content": "In our study, we evaluate pretrained models' cross-modal alignment efficacy by conducting both zero-shot text-to-image and image-to-text retrieval tasks in multiple hierarchical levels. We report the Recall@N metric by identifying the retrieved nearest neighbors for each query and then determining whether the corresponding ground truth element is within the top N nearest neighbors, where \u039d\u2208 {1, 5, 10}. Tab. 2 shows that our PeskaVLP achieves superior performance due to the procedure-aware learning objective in hierarchical pretraining. Particularly, the hierarchical pretraining scheme significantly boosts the cross-modal retrieval at the coarse-grained video-text pairs, comprehending the relationship between long video segments and high-level sentences with surgical terms."}, {"title": "4.3 Few-/Full-shot Linear Probing", "content": "We present the few- and full-shot linear-probing evaluation in Tab. 3. It shows that the learned visual representation from PeskaVLP provides a general visual representation for surgical scene understanding across surgical procedures. We also find that the Moco pretrained on the frames of SVL dataset (second row of Tab. 3) in a visual self-supervised manner achieves better visual representation than pretraining on a public dataset that only contains one type of surgery, e.g., Cholec80 (third row in Tab. 3). This shows that the cross-procedure surgical pretraining dataset enables better generalizationability.\nKnowledge Augmentation and Hierarchical Pretraining. Interestingly, the model pretrained contrastively with short video clips and narrations (SurgVLP) performs worse than Moco (second row in Tab. 3) in linear probing evaluation. This may be because the noisy narrations do not provide accurate natural language supervision for visual representation learning, thus highlighting the importance of visual self-supervision and textual quality. Our model surpasses the prior methods by a large margin, showing the efficacy of our hierarchical knowledge augmentation, which denoises the text and improves textual quality. Also, our proposed LecNCE promotes the visual encoder through additional visual self-supervision and procedural understanding. We present t-SNE visualizations of learned features in Appendix E, which shows that our multi-modal representations exhibit a smaller modality gap, enhancing transferability to vision-and-language downstream tasks [18, 37]."}, {"title": "4.4 Ablation Studies", "content": "Effect of Knowledge Augmentation. Tab. 4 presents the effect of our proposed LLM-based hierarchical knowledge-aware augmentation strategy, applied to the texts of SVL dataset. The first row of the table corresponds to HecVL [72] pretrained on SVL with only conventional visual augmentations, e.g., blurring and so on, without any knowledge augmentation. The results clearly demonstrate that simple visual augmentation strategies exhibit poor robustness as the texts of SVL are noisy and not diverse enough. Conversely, our knowledge-aware text augmentation consistently improves performance across multiple surgical datasets, highlighting the importance of the textual quality of the surgical video-language pretraining dataset.\nEffect of Pretraining Objective. Tab. 4 shows the impact of our learning objective for hierarchical surgical video-language pretraining. When we append visual self-supervision to language supervision at the clip-level pretraining, the zero-shot performance is clearly improved. This improvement can be attributed to the added diverse and high-quality supervision. Also, the boost at linear-probing evaluation shows that the combination of language supervision and visual self-supervision leads to a robust visual representation especially with a moderate size of surgical video-language dataset, e.g., SVL. Table 4 also highlights that the inclusion of LecNCE with procedure understanding consistently improves performance across most downstream datasets, leading to enhanced accuracy in both zero-shot and linear-probing. However, performance on the AutoLaparo degrades with this modification. This may be due to challenging or less routined surgical procedures in the dataset."}, {"title": "5 Conclusion", "content": "We have introduced a surgical video-language pretraining method for long-term surgical lecture videos and their hierarchical paired texts. Our proposed knowledge augmentation addresses the hierarchical textual information loss by integrating the large language model's internal surgical knowledge. Also, we propose a novel spatial-temporal pretraining objective for video-text pairs of different hierarchies, which addresses the lack of supervision signals problem in a small surgical vision-language dataset. The proposed LecNCE also addresses the procedural awareness problem, benefiting the long-term cross-modal understanding. The experiments show that our proposed PeskaVLP achieves the state-of-the-art generalized zero-shot ability and visual representation learning that can serve as a general initialization for many surgical scene understanding tasks."}, {"title": "A Pretraining Dataset", "content": "We start with the videos that are used for surgical vision-language pretraining in [73]. In total, there are 1,326 surgical lecture videos. These videos are transcribed by AWS [4] and Whisper [51] audio speech recognition (ASR) to obtain the corresponding narration texts. Furthermore, we curate the videos' metadata from the online platforms to obtain the extra keystep and abstract texts. In the phase- and video-level pretraining, we need parent- and child-level text correspondences, e.g., keystep and its corresponding narration texts, to perform procedure understanding. Therefore, we filter out the videos that do not have parent-child correspondences. In total, we have 1,007 and 920 videos for phase- and video-level pretraining, respectively."}, {"title": "A.2 Misspelling Error", "content": "As the narration texts are generated from the audio using the ASR system, they usually contain many misspelling errors and fragment sentences. Therefore, we apply multiple preprocessing steps to clean the narration texts.\nWe first built the vocabulary based on the textbook, surgical category labels, and definition words. Specifically, we refer to the academic papers, which define the surgical phases, to curate a list of definition words and build a vocabulary that contains the words of interest. We also parse and merge the words from the textbook. In total, we obtain a vocabulary of the size of 51, 640 words. Then, we use the built vocabulary along with the spell-checking algorithm \u00b9 to correct the misspelling errors in narration texts. The algorithm utilizes Levenshtein Distance to identify words within 2 edit distances from the original. It then cross-references these permutations (insertions, deletions, replacements, and transpositions) with a word frequency list, prioritizing words with higher occurrence frequencies as potential correct results."}, {"title": "B Evaluation Setup", "content": "We provide a detailed description of the downstream tasks and their settings that we apply in the experiment.\nSurgical Phase Recognition. Surgical phase recognition is a proxy task to test the model's surgical scene understanding ability. It aims to classify the frame of surgical video into predefined classes (phases), requiring the model to understand the instrument and anatomy's presence and their interactions by extracting visual patterns from the surgical scene image. In this work, we ignore temporal modeling in surgical phase recognition as we focus on multi-modal representation learning. We consider phase recognition as a frame-wise image classification problem. In the surgical phase recognition task, we evaluate the model's performance based on the publicly available datasets, including Cholec80 [64], AutoLaparo [66] and MultiBypass [29].\n\u2022 Zero-shot Evaluation. As the surgical phase labels are high-level definitions that can be decomposed into a few basic concepts, we manually construct the contextual prompts for phase labels, as shown in Tab. 5, Tab. 6 and Tab. 7. Our constructed prompts for the class names are built with the help of clinician's comments, considering the involved surgical instruments, anatomies, and events involved in a given surgical phase.\n\u2022 Linear-probing Evaluation. For linear-probing evaluation on the surgical phase recognition downstream datasets, we keep the visual encoder frozen and train a linear classifier on the extracted features. We do not apply any image augmentation during the training. The learning rate is scaled linearly based on the actual batch size. The model is optimized using SGD optimizer with the learning rate as 0.001 and weight decay parameter as 0.0005. We train the model for 40 epochs. We fit the model on the training and validation sets and report the performance on the separate test set. For the few-shot linear-probing evaluation, we adopt an N-way K-shot approach with a slight modification to accommodate the nature of surgical videos, which contain frames from different classes. Specifically, we select 10%"}, {"title": "D Dynamic Time Warping", "content": "After achieving the cost matrix C and \u0108, we perform dynamic time warping (DTW) [57] to find the minimum cost path to align the frames of video segment $V = \\{v_1, ...v_\\tau\\}$ to the text sequence $B = \\{b_1, ...b_n\\}$ and reversed text sequence $\\{b_n, ...b_1\\}$, respectively, as shown in Algorithm. 1. We follow [68] to process the DTW function into differentiable, enabling the gradient back-propagation. The differentiable loss function is the same as [19].\nA significant advantage of using DTW is that it does not require additional temporal modules, such as recurrent neural networks or attention mechanisms, to model temporal relationships. This simplification allows us to focus on learning better representations by directly aligning video frames and text sequences based on their semantics."}, {"title": "E Modality Gap", "content": "Modality gap is a geometric phenomenon observed in the embedding space of multi-modal models [37]. This gap illustrates that pretrained multi-modal (vision-language) models create a joint embedding space where different modalities, such as images and text, are kept at a significant distance from each other. During contrastive optimization, this separation created at initialization is maintained to the extent that irrelevant image embeddings can be closer to each other than to their corresponding relevant text embeddings. This spatial disparity in the embedding space hinders the model's ability to effectively align and understand the relationships between visual and textual data, leading to suboptimal performance in tasks requiring integrated multi-modal comprehension. The existence of the modality gap is particularly detrimental when adapting pretrained vision-language models to cross-modal generation tasks, such as image captioning. As highlighted by several studies [33, 18], narrowing modality gap correlates with improved performance in cross-modal tasks.\nAs shown in Fig. 4, we visualize the embeddings of videos and their corresponding text descriptions at three hierarchical levels: clip-narration, phase-keystep, and video-abstract. Our proposed model demonstrates a significant reduction in the modality gap compared to the SurgVLP model. This alignment across different hierarchical levels ensures a more comprehensive and cohesive understanding of the multi-modal data, leading to superior performance in tasks like image captioning and other vision-language applications."}, {"title": "F Surgical Phase Recognition Results", "content": "We demonstrate the zero-shot surgical phase recognition to reflect the surgical scene understanding ability of our pretrained model. Our model can identify surgical phases of different types of surgical procedures without any finetuning. Both success and failure examples are shown.\nSurgical Term Understanding. In Fig. 5, we show that the pretrained model excels at identifying the \"washing\" phase in surgical procedures, demonstrating its capability to accurately recognize high-level surgical activities. This proficiency enhances surgical assistance systems, improving real-time analysis and decision-making in operating rooms.\nInstrument Identification. In Fig. 6, we demonstrate how the visual embedding is significantly influenced by the presence of surgical instruments. Specifically, in the first row, the semantic meaning of the image changes from \"calot triangle dissection\" to \"clip and cut\" due to the appearance of a hook, even though the other anatomical features remain similar."}, {"title": "G Limitations", "content": "As the pretraining process at clip-level requires additional supervision signals, i.e., visual self-supervision, the memory and computation overhead increase compared to the vanilla HecVL pretraining. Also, during the phase- and video-level pretraining, the process of dynamic time warping can be time-consuming because it is based on dynamic programming, slowing down the pretraining iteration when handling longer-term surgical videos. Additionally, the knowledge augmentation on keystep and abstract texts need to be modified to fit the other video-language pretraining datasets [3, 76"}]}