{"title": "Text Prompt is Not Enough: Sound Event Enhanced Prompt Adapter for Target Style Audio Generation", "authors": ["Chenxu Xiong", "Ruibo Fu", "Shuchen Shi", "Zhengqi Wen", "Jianhua Tao", "Tao Wang", "Chenxing Li", "Chunyu Qiang", "Yuankun Xie", "Xin Qi", "Guanjun Li", "Zizheng Yang"], "abstract": "Current mainstream audio generation methods primarily rely on simple text prompts, often failing to capture the nuanced details necessary for multi-style audio generation. To address this limitation, the Sound Event Enhanced Prompt Adapter is proposed. Unlike traditional static global style transfer, this method extracts style embedding through cross-attention between text and reference audio for adaptive style control. Adaptive layer normalization is then utilized to enhance the model's capacity to express multiple styles. Additionally, the Sound Event Reference Style Transfer Dataset (SERST) is introduced for the proposed target style audio generation task, enabling dual-prompt audio generation using both text and audio references. Experimental results demonstrate the robustness of the model, achieving state-of-the-art Fr\u00e9chet Distance of 26.94 and KL Divergence of 1.82, surpassing Tango, AudioLDM, and AudioGen. Furthermore, the generated audio shows high similarity to its corresponding audio reference. The demo, code, and dataset are publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "Target Style Audio Generation generates audio with specific styles or features, allowing for more natural and fine-grained audio production. This approach has numerous applications, particularly in the media industries, where it can generate background sound effects that match specific scenes. The current mainstream method for audio generation is Text-to-Audio (TTA) [1]\u2013[5]. These TTA models, often encoded by CLAP [6] or T5 [7], utilize rich semantic information in textual descriptions to produce high-quality audio outputs.\nAlthough mainstream methods using single-text prompts have achieved promising results, several limitations remain. Text input and audio output belong to different modalities, making alignment between the two challenging. From a mathematical perspective, achieving full control over the generated audio requires the mapping between input and output to be at least surjective, if not bijective. For instance, generating the sound of a dog barking from a single text prompt fails to capture specific characteristics such as timbre or how the environment interacts with the barking. This limitation restricts the ability to model audio in finer detail. To address this issue, incorporating additional prior knowledge is essential for providing richer contextual information and enhancing the precision of the generated output.\nTwo primary approaches exist for introducing prior knowledge into audio generation. The first involves control conditions manipulating the generated audio's pitch, energy, and temporal relationships [8]\u2013[10]. However, no current methods specifically address style control in audio generation. The second approach utilizes multi-modal prompts that incorporate semantic and temporal information from other modalities, such as images [11] and videos [12]\u2013[14]. Despite their potential, cross-modal prompts often suffer from interference caused by redundant and unrelated information, as they do not provide intuitive acoustic references for the model. As a result, a text-acoustic fusion prompt emerges as an effective solution, not only providing intuitive information to the model but also filling the gap in style control.\nIn this paper, we first propose the Sound Event Enhanced Prompt Adapter. Traditional style transfer approaches typically extract a global style directly from the reference. However, text offers valuable semantic information that can guide and refine the application of this global style. To leverage this, cross-attention [15] is employed between sound events and text to identify which text events are most closely correlated with the corresponding audio reference, as illustrated in Fig. 1. Additionally, the style embedding generated by the adapter"}, {"title": "II. METHOD", "content": "To achieve target style audio generation, a Sound Event Reference Style Transfer Dataset is built. We utilize the Sound Event Enhanced Prompt Adapter to extract style embedding, which is then sent into the Conditional Latent Diffusion Audio Generation Model. It comprises a Variational Auto-Encoder (VAE) [19], a conditional Latent Diffusion Model (LDM) [20], and a text condition encoder (FLAN-T5) [21]. The latent representation constructed by LDM is then used to generate the mel-spectrogram via the VAE decoder. A vocoder is employed to generate the audio in the inference phase. The overall architecture is illustrated in Fig. 2.\nEffective style transfer requires high-quality reference audio. To address this need, the Sound Event Reference Style Transfer Dataset (SERST) is constructed, providing event-level granularity audio to capture the full distribution of acoustic events and enabling the accurate reconstruction of their characteristics. This dataset is created by segmenting the original audio from the Audioset-Strong dataset [18] based on annotated acoustic event timestamps. Statistical analysis revealed that a 2-second audio length offers an optimal balance between segment quantity and accuracy. Audio is segmented by event, and in cases where the segments are shorter than 2 seconds, they are concatenated from other clips with the same sound event tag, facilitating both padding and data augmentation. Then a Short-Time Energy detection is used to filter out Poor quality references. As a single original audio could yield multiple trimmed segments: during training, one of these segments is randomly selected, while during inference, all trimmed segments are utilized to examine the variability in the generated audio. The dataset consists of 88,464 training samples, 1,384 validation samples, and 1,180 test samples.\nTo fully utilize the acoustic information, the global sound event style feature is extracted from a reference encoder. A style embedding is then generated through cross-attention between the text and reference audio, enabling adaptive style transfer and allowing the model to focus on the relevant aspects of the reference audio's style.\nThe sound event reference is first compressed into a refer-ence embedding $e_r$, representing the global style of the audio."}, {"title": "C. Conditional Latent Diffusion Audio Generation Model", "content": "The latent diffusion model (LDM) constructs the audio prior with the guidance of text and audio. LDM can achieve this through a forward and reverse diffusion process. The forward diffusion is a Markov chain of Gaussian distributions with scheduled noise parameters $0 < \\beta_1 < \\beta_2 < \\cdots < \\beta_T < 1$. Through a reparametrization trick that allows direct sampling of any $z_n$ from $z_0$ via a non-Markovian process:\n$z_n = \\sqrt{\\bar{\\alpha}_n} z_0 + (1 - \\bar{\\alpha}_n) \\epsilon$,\nwhere $\\epsilon$ is a standard Gaussian noise and $\\bar{\\alpha}_n = \\prod_{i=1}^{t}(1-\\beta_t)$.\nThe LDM model aims to conduct the denoising process on mel-embedding (training) or standard Gaussian noise $\\epsilon$ (inference) and predict the mel-embedding $z_0$. For every step t, the training objective is to minimize the following:\n$\\mathcal{L}_{LDM} = \\mathbb{E}_{x,\\epsilon \\sim N(0,1)} || \\epsilon_{\\theta}(x_t, t, e_t, e_r) - \\epsilon||^2$.\nIn this context, $\\epsilon$ represents the noise estimation conditioned on $t$, $e_t$ and $e_r$. The architecture of the LDM primarily utilizes a U-Net structure [16], which consists of a series of ResNet [23] and transformer blocks.\nThe shift parameters $\\gamma$ and $\\beta$, derived from the concat of style embedding and time step embedding, are applied as adaptive layer normalization-zero parameters [17] throughout the Resnet blocks in U-Net. This is because the adaptive layer norm allows the normalization layer to adapt to data distributions in different modalities or domains, thus performing well in multimodal learning or domain adaptation tasks. This dynamic adjustment is achieved by learning how to modify the normalized mean and variance based on data, thereby generating more robust feature representations.\nTo guide the reverse diffusion process to reconstruct the audio prior $z_0$, we employ a classifier-free guidance [24] of condition input $\\tau$. During training, the guidance was randomly dropped for 10% of the training samples. When inference, a guidance scale $w$ controls the contribution of guidance to"}, {"title": "III. EXPERIMENTS", "content": "All data were resampled to a 16kHz sampling rate, with each sample padded to a duration of 10.24 seconds. The VAE and text condition encoder were kept frozen and accepted audio at 16kHz while we fine-tuned the latent diffusion model using pre-trained weights from Tango [5]. The reference audio encoder, in contrast, was trained from scratch. The text encoder is based on FLAN-T5-LARGE [21], which contains a total of 780 million parameters. HiFi-GAN [25] was used as the vocoder to convert mel-spectrograms into audio. The trainable components include the U-Net, which loaded the pre-trained weights from Tango, and the reference audio encoder, collectively comprising 1.097 billion trainable parameters. We employed AdaFactor as the optimizer and AdafactorSchedule as the scheduler to accelerate the training process. Our model was trained for 20 epochs on four RTX 3090 GPUs with a batch size of two. The checkpoint with the lowest validation loss was then selected for final evaluation."}, {"title": "B. Evaluation Metrics", "content": "We compared our model to Tango [5], AudioGen [2] and AudioLDM [4] and used four objective metrics: Fr\u00e9chet Distance (FD), Fr\u00e9chet Audio Distance (FAD), KL divergence (KL), Mel-Spectrogram cosine Similarity (Mel-Sim) and Clap-Audio [6] cosine similarity (CLAP-Audio). The first two measure the distance between the generated audio distribution and the real audio distribution while the third one computes the divergence between the distributions of the original and generated audio samples. To calculate the Mel-Spectrogram cosine similarity between the sound event reference and generated audio, we segmented the generated audio into multiple parts. We calculated the similarity for each segment against the reference audio. The highest similarity value among these segments was then taken as the overall similarity between the generated and reference audio. The CLAP-Audio cosine similarity is employed to measure the similarity between different generated audios and between the generated audios and their respective references.\nAs for subjective evaluation, we paid twenty experienced human evaluators to assess fifty randomly selected audio samples on a scale from 1 to 100 in the following aspects: overall audio quality (OVL) and relevance to the input text (REL) that reflects the quality of generated audio and its relevance to the input sound event prompt (REA) that demonstrates the ability in target style transfer."}, {"title": "IV. RESULTS AND ANALYSIS", "content": "In this section, we first conduct a sensitivity analysis on the Sound Enhanced Prompt Adapter to evaluate its effectiveness. Then grade its performance (referred to as Ours) in comparison to baseline models. Additionally, we conduct an ablation study on our model and various modified versions, focusing on identifying the most effective method for fusing the modalities. Finally, we assess the relevance of the generated outputs and the provided reference audio.\nTable I presents the CLAP-Audio similarity results of the generated audio provided with various sound event references, while keeping the text input constant. When the same sound event reference is provided to the model multiple times, the generated audio exhibits a CLAP similarity score of 0.72. In contrast, when different sound event references are used, the generated outputs yield a CLAP similarity score of 0.54. This difference of 0.18 demonstrates the effectiveness of the Sound Enhanced Prompt Adapter in utilizing prior acoustic information.\nTable II presents the evaluation results of our model compared to TTA models using both objective and subjective metrics. Our model achieves great results in both objective and subjective evaluation. In terms of objective metrics, our model achieves an FD score of 26.94, and a KL divergence of 1.88, which are all the lowest in all models. The FAD score of 2.38, although not the best, is still very competitive and close to Tango's leading result of 2.21. For subjective metrics, our model achieves an overall quality (OVL) score of 79.10 and a relevance (REL) score of 77.65, which are both the best in these models, showing that the audio generated by our model is very well aligned with the provided textual descriptions.\nTable III presents the results of our ablation study. We experimented with four different approaches: concatenating the reference embedding with the text embedding or applying cross-attention to obtain the style embedding, then"}, {"title": "V. CONCLUSION", "content": "This work first introduces the SERST dataset, which integrates dual-modality prompts from event-level audio reference and text, providing a valuable resource for target audio generation. Then a Sound Event Enhanced Prompt Adapter is proposed to achieve fine-grained style control in audio generation. The method leverages cross-attention and adaptive layer normalization, significantly improving the quality and controllability of generated audio, particularly in style. Compared to Tango, the proposed approach improves FD and KL Divergence scores by 2.3% and 7.6%. The generated audio strongly aligns with the reference audio, highlighting effective style control. Future work will explore additional methods to enhance the performance of the prompt adapter."}]}