{"title": "DEALING WITH ANNOTATOR DISAGREEMENT IN\nHATE SPEECH CLASSIFICATION", "authors": ["Somaiyeh Dehghan", "Mehmet Umut Sen", "Berrin Yanikoglu"], "abstract": "Hate speech detection is a crucial task, especially on social media, where harmful content can spread\nquickly. Implementing machine learning models to automatically identify and address hate speech\nis essential for mitigating its impact and preventing its proliferation. The first step in developing\nan effective hate speech detection model is to acquire a high-quality dataset for training. Labeled\ndata is foundational for most natural language processing tasks, but categorizing hate speech is\ndifficult due to the diverse and often subjective nature of hate speech, which can lead to varying\ninterpretations and disagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different\napproaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets,\nbased on a fine-tuned BERT model. Our work highlights the importance of the problem and provides\nstate-of-art benchmark results for detection and understanding of hate speech in online discourse.", "sections": [{"title": "Introduction", "content": "Hate speech detection plays a vital role in maintaining a safe and respectful environment, especially on social media\nplatforms. To achieve accurate automatic hate speech detection, it is crucial to have a sufficient amount of well-labeled\ntraining data. Large language models (LLMs) such as BERT (Devlin et al., 2019) have demonstrated state-of-the-art\nperformance in many NLP tasks, including hate speech detection. These models rely heavily on high-quality, accurately\nlabeled datasets to train effectively. Therefore, ensuring that the training data is both fair and precise is essential for\nleveraging the full potential of these advanced models. However, earlier research on hate speech detection often lacks\nclarity in detailing their annotation processes, which can impact the quality of the datasets used for training.\nMany tasks in natural language processing (NLP) are subjective, meaning there can be a variety of valid perspectives\non what the appropriate data labels should be. This is particularly true for tasks like hate speech detection, where\nindividuals often hold differing opinions on what content should be labeled as hateful (Talat, 2016; Salminen et al.,\n2019; Davani et al, 2021). While certain viewpoints are more commonly accepted than others, there is no absolute,\nobjective standard. Additionally, some tweets may contain multiple intensities of hate within one tweet, ranging from\ndiscrimination to more severe forms such as cursing, further complicating the annotation process.\nSince annotators might have subjective disagreements regarding labels (e.g., for hate speech), it is up to the dataset\ncreators to decide how to handle these disagreements and establish the true labels. The issue is often addressed with one\nof two approaches: (1) keeping only the samples where there is agreement among annotators, (2) selecting the samples\nwhere the majority opinion is clear. However, for cases of disagreement where the majority is not obvious, researchers\nmust decide whether to use methods like expert adjudication, averaging annotations, or considering the context in more\ndetail to determine the most accurate label.\nRecognizing the challenge of subjective disagreements among annotators, especially in tasks such as hate speech\ndetection, we shed light on this issue and propose a novel method for deriving the true label from such annotations.\nTowards this end, we explore various strategies to determine the most accurate label when faced with differing opinions.\nOur analysis includes methods such as taking the maximum, minimum, random selection, and mean of the annotators'\nlabels. Additionally, we examine weighted versions of these strategies-weighted max, min, random, and mean-to\naccount for the varying levels of confidence or reliability among annotators. Through this approach, we aim to better\nunderstand and address the issue of annotator disagreement, ultimately providing a more reliable method for establishing\ntrue labels in subjective labeling tasks.\nOur main contributions are as follows:\n\u2022 We address the critical issue of annotator disagreement in the labeling of hate speech data and conduct a\ncomprehensive evaluation of various strategies to manage the issue, even when considering multiple labels.\n\u2022 We assess the effectiveness of these strategies, along with potential issues and important observations, con-\ntributing to the development of more fair and reliable detection models.\n\u2022 We show that annotating based on perceived hate speech strength-where the annotator provides judgments\nwithout following guidelines- can be a slightly inferior but faster alternative to annotation using detailed\nguidelines. This approach can also be utilized in building an ensemble model.\n\u2022 We demonstrate state-of-art results in hate speech detection and classification in Turkish tweets, based on\nfine-tuning a pretrained BERT model.\nThe paper is organized as follows: In Section 2, we review the existing literature on hate speech annotation, including\nworks that report on the difficulty of the problem. In Section 3, we introduce the topics of our used hate speech dataset.\nSection 4 provides a brief overview of our annotation process, including few main issues by the annotators, resulting in\nan iteratively improved guideline. In Section 5, we present alternative approaches to the annotator disagreement problem\n-ranging from only taking agreed-upon samples to weighted majority voting with tie-breaking- and the architecture of\nour transformer-based models for hate speech classification and strength prediction. Section 6 presents our experimental\nresults, highlighting the performance variations and possible reasons. Finally, Section 7 summarizes our findings and\noutlines future research directions."}, {"title": "Related Work", "content": "Despite the growing body of research on hate speech detection models, the literature lacks a thorough examination\nof the annotation process and the challenges associated with it. The process of annotation is already challenging due to\nmany considerations such as what to do when the intent is covert or when the hate discourse is carried in an image. The\nsubjective nature of hate speech further complicates the issue, as it can leads to disagreements among annotators. These\ndisagreements can significantly impact the quality of the dataset and, consequently, the performance of models trained\nwith it."}, {"title": "Dataset", "content": "We use a dataset containing 11,021 samples to date, across 5 topics in Turkish as given below. The dataset is part of\nan ongoing project, to be made public in the near future at the end of the project. The first set of tweets collected within\nthe project has been already shared in (Ar\u0131n et al., 2023; Uludo\u011fan et al., 2024; Dehghan and Yanikoglu, 2024a,b). The\ntopics covered in the dataset are as follows:\nImmigrants and Refugees in Turkey In recent years, the civil wars in Syria and Afghanistan have led countless\nimmigrants and refugees from these countries to seek refuge in Turkey. According to the latest statistics, approximately\n3.7 million Syrians and around 300,000 Afghans have settled in Turkey. While public opinion was initially welcoming\nduring the early stages of the refugee crisis, the challenges posed by the large influx of asylum seekers and the\nwidespread misconception that refugees receive rights not granted to Turkish citizens have fueled growing negative\nsentiments toward them. Consequently, this has led to an increase in hate speech directed at them on social networks.\nThis topic includes 2,281 tweets.\nIsrael-Palestine Conflict The Israel-Palestine conflict, which began in the mid-20th century, remains one of the\nworld's most enduring disputes, with pro-Israeli and pro-Palestinian groups holding sharply opposing views. As the\nsituation frequently escalates into warfare, it continues to be a highly debated topic in Turkey, which has a generally\npro-Palestine views. This topic includes 2,873 tweets.\nAnti-Greek Sentiment in Turkey Anti-Hellenism, or Hellenophobia (commonly known as Anti-Greek sentiment),\nrefers to hatred and prejudice against Greeks, the Hellenic Republic, and Greek culture. Since the Treaty of Lausanne,\nTurkey and Greece have been in conflict over the sovereignty of the Aegean islands, territorial waters, flight zones, and\nthe rights of their respective minorities. In the summer of 2022, Greece began to increase its military presence on the\nislands, which heightened the rhetoric between politicians in Ankara and Athens, as well as tensions between the two\npopulations, especially as the Turkish elections approached. This topic includes 2,033 samples.\nReligion/Race/Ethnicity (Alevis, Armenians, Arabs, Kurds, and Jews) In addition to the above four topics,\npeople sometimes directly attack ethnic groups or religions with hate speech. Hate speech against groups like Alevis,\nArmenians, Arabs, Jews, Kurds, and Romans often stems from historical, political, and social factors, as well as media\nand propaganda that create and perpetuate stereotypes, prejudices, and discrimination. These attacks are often rooted in\ndeep-seated biases and misconceptions that have been reinforced over generations, leading to further marginalization\nand social division. This topic includes 3,135 samples.\nLGBTQ+ In Turkey and many Muslim-majority countries, opposition to LGBTQ+ individuals often stems from\ndeeply rooted cultural, religious, and social beliefs. Islam, the predominant religion in these regions, traditionally views"}, {"title": "Annotation Process", "content": "We use a prescriptive annotation strategy for classifying tweets into different hate speech categories and a descriptive\none for indicating the perceived degree (strength) of hate speech on a scale of [0, 10]. The annotation guidelines were\nrefined over time to reduce ambiguities when annotating the tweets into different categories. The degree of hate speech\nwas asked independently of the category, as a second measure and to study its correlation with hate speech classes.\nTweets were assigned to three annotators in batches of 50 using Label Studio\u00b2 and annotators were asked to label\neach tweet one by one according to the guidelines. The following six categories were decided for a comprehensive\nlabelling:\n0. No Hate Speech: Tweet does not contain hate speech.\n1. Exclusion/discriminatory discourse: These are discourses in which a community is seen as negatively\ndifferent from the dominant group as to the benefit, rights and freedoms in the society. For instance, the\nexpressions \"Suriyeliler oy kullanmas\u0131n\" (Syrians should not vote) and \"K\u00fcrt\u00e7e e\u011fitim kabul edilemez\"\n(Kurdish education is unacceptable) are considered discriminatory discourse and hate speech.\n2. Exaggeration, Generalization, Attribution, Distortion: These are discourses that draw larger conclusions\nand inferences from individual events or situations; manipulate real data by distorting it; or attribute individual\nevents to the whole identity based on their agents. For example, \"E\u015fcinsel sapk\u0131nlar deh\u015fet sa\u00e7\u0131yor\" (Perverted\nhomosexuals are spreading terror), \"Suriyeliler g\u0131na getirdi\" (I am fed up from Syrians), after individual and\nisolated incidents.\n3. Symbolization: These are discourses in which an element of identity itself is used as an element of insult,\nhatred or humiliation and the identity is symbolized in such manners (e.g., \"Ermeni gibi konu\u015ftular\" (They\nspoke like Armenians)).\n4. Swearing, Insult, Defamation, Dehumanization: These are discourses that include direct profanity, insult,\ncontempt towards a community, or insults by characterizing them with actions or adjectives specific to\nnon-human beings, e.g., \"Barbar ve ahlaks\u0131z Frans\u0131zlar\" (Barbaric and immoral French).\n5. Threat of Enmity, War, Attack, Murder, or Harm: These are discourses that include expressions about a\ncommunity that are hostile, evoke war or express a desire to harm the identity in question, e.g., \"M\u00fcltecilerin\n\u00f6lmesini istiyorum\" (I want the refugees to die).\nTo form the guidelines and ensure that annotators follow a consistent set of rules, we iteratively refined the guidelines\nto resolve ambiguities and conflicts in the annotation process. For instance, we have added more examples to the classes\nto eliminate confusion and clarified the guidelines about what to do in ambiguous situations such as when a tweet\ncontains hate speech towards multiple groups or when it contains covert hate speech.\nNonetheless, hate speech annotation is a difficult task due to its relatively subjective nature. While annotators\ntypically agree on discourse that includes swear words or threats towards a target group, they quite often disagree\non how to classify discriminatory speech. When each tweet can be assigned to multiple categories (e.g. swear and\nthreat) and there are multiple annotators, the resulting annotations are often not in agreement. This is called annotator\ndisagreement. Researchers often address the issue of annotator disagreement by discarding samples for which there are disagree-\nments, with the goal of obtaining data without label noise. In addition to losing a large portion of data, this results in\noverly optimistic model results since difficult cases are left out. Another common approach to deal with annotator"}, {"title": "Methodology", "content": "In this section, we first discuss annotator disagreement and our proposed handling strategies (5.1). Then, we present\nour transformer-based model for hate speech classification and hate strength prediction (5.2). Finally, we analyze the\nrelationship between classes and strengths (5.3).\n5.1 Handling Annotator Disagreement\nMany works in the literature seek agreement among annotators by either using only the subset of the collected\ndata where there is agreement (Braun, 2024) or by requiring a second phase of annotation so that the annotators can\nreach a consensus (Krenn et al., 2024). Other studies handle disagreements by selecting the final label (gold standard)\nthrough majority voting. In this case, each independent annotation counts as a vote, and the annotation that receives the\nmost votes is selected as the gold standard. However, research in more objective fields, such as the medical domain,\ngenerally agrees that majority voting is not an ideal method for resolving annotation disagreements. Sudre et al. (2019)\nrecommend retaining the labels from individual annotators to preserve the disagreements, whereas Campagner et al.\n(2021) proposed different methods for consolidating conflicting labels into a single groundtruth, namely corrected\nmajority, probabilistic overwhelming majority, and fuzzy possibilistic three-way.\nWhile the issue of annotator disagreement is very prevalent, there are very few studies that explore the effects of\nincluding disagreeing annotations.\n5.1.1 Majority Voting for Multi-Label Annotations\nMost prior research that consolidates multiple annotations into single class labels focuses on data where each\nannotator selects only one category. In contrast, our work addresses multi-label annotations, allowing each annotator to\nselect multiple categories.\nIn multi-label annotation scheme, we are given a set of classes from each annotator. Let L be the set of classes, i.e.,\nL = {0, 1, 2, 3, 4, 5} for our 6-class setting and let $A_k(x) \\subseteq L$ be the annotation (set of labels) of data-point x by the\nannotator k. Then, we define the majority-voting (MV) result of x as follows:\n$M(x) =\\begin{cases} { l |c_l(x) = \\max_{l'} c_{l'}(x) }\\end{cases}$  (1)\nwhere $c_l(x)$ is the number of times label-l is selected by the annotators:\n$c_l(x) = \\sum_k 1(l \\in A_k(x))$ (2)\nwhere 1 (l\u2208 Ak(x)) is 1 if l \u2208 Ak(x) and 0 otherwise. The result M(x) of the majority-voting is a set of classes\nand there are more than one element in this set in the case there is equality among classes. This set contains a single\nclass in the case there is a clear winner of the Majority Voting. We define three different scenarios, not mutually\nexclusive, based on annotator agreements and Majority Voting results:\n\u2022 Agreement: Each annotator selects the same single class: |Ak(x)| = 1 \u2200k and A\u2081(x) = Aj(x) \u2200i, j. An\nexample for the agreement scenario is as follows: A1(x) = {1}, A2(x) = {1}, A3(x) = {1} where class-1 is\nthe \"Exclusion/discriminatory discourse\" as described in Section 4.\n\u2022 Clear majority: The majority voting set contains a single class: |M(x)| = 1. This scenario also contains\nexamples of the above \"agreement\" scenario. An example for this scenario is as follows: A1(x) = {0},\nA2(x) = {3,4}, A3(x) = {4,5} where the majority voting set is M(x) = {4} since count of the class-4 is\nc4(x) = 2 and it is the maximum.\n\u2022 No clear majority: There is an equality between some classes, i.e. the Majority Voting set may contain\nmultiple classes: M(x)| \u2265 1. This scenario contains all the examples. An example for which majority voting\nis not clear is as follows: A1(x) = {4,5}, A2(x) = {1,5}, A\u2081(x) = {2,4} where the Majority Voting set is\nM(x) = {4,5}.\nSome other examples for such scenarios are given at Figure 1. Note that class indices which are in the range [0, 5]\nare described at Section 4.\n5.1.2 Strategies When Majority Vote Is Not Clear\nIf majority is not clear, a usual resolution is to pick a random label from the set M(x) or discard that data-point from\nthe training set. In this work, since we assume classes are ordered, we propose other solutions for the not-clear majority\nscenario: minimum ($y_{min}$), maximum ($y_{max}$) and the mean ($y_{mean}$). The definitions are as follows:\n$y_{min}(x) = min M(x)$ (3)\n$y_{max}(x) = max M(x)$ (4)\n$y_{mean}(x) = round (\\frac{1}{|M(x)|} \\sum_{l \\in M(x)} l )$ (5)\nwhere round is the rounding function which maps rational numbers to the closest integer.\nWe also propose a weighted majority voting where weights are inversely proportional to the number of classes an\nannotator chooses:\n$c_l^{(w)}(x) = \\sum_k \\frac{1}{|A_k(x)|} 1 (l \\in A_k(x))$ (6)\nThen, the weighted majority voting set is found as follows:\n$M^{(w)}(x) = \\{ l |c_l^{(w)}(x) = max_{l'} c_{l'}^{(w)}(x) \\}$ (7)\nThis weighted majority voting setting decreases the number of data-points for which Majority Voting is not clear.\nFor example, for the datapoint with the annotations A\u2081(x) = {1,2}, A2(x) = {2}, A3(x) = {1,4}; regular majority\nvoting set is M(x) = {1,2} whereas the weighted majority voting set is M(w)(x) = {2}.\n5.1.3 Deriving the Four-Class and the Two-Class Labels\nIn the four-class setting, we first map the six-class labels to the corresponding four-class labels, then calculate the\ncounts of four-class labels before taking the majority vote:\n$c_l^{(4)}(x) = \\sum_k \\sum_{l' \\in A_k (x)} 1 (l = r_4(l'))$ (8)\nwhere 1 (l = r4(l')) is 1 if l = r4(l'), 0 otherwise and r4 is the reduction mapping from the six-class setting to the\nfour-class setting:\n$r_4(l) = \\begin{cases} 0 & l=0 \\ 1 & l=1 \\ 2 & l=2 \\text{ or } l = 3 \\ 3 & l=4 \\text{ or } l=5 \\end{cases}$\nFor two-class setting we follow the same approach, only with the following mapping function:\n$r_2(l) = \\begin{cases} 0 & l=0 \\ 1 & \\text{otherwise} \\end{cases}$\nThen the counts are calculated as follows:\n$c_l^{(2)}(x) = \\sum_k \\sum_{l' \\in A_k (x)} 1 (l = r_2(l'))$ (9)\n5.2 Classification and Regression Models\nWe design our classification model using transfer learning, incorporating a single layer on top of the BERT (Devlin\net al., 2019) 2019) encoder to predict hate speech categories or strengths. BERT, being a state-of-the-art model, has\ndemonstrated success across numerous NLP tasks, including in our previous works (Beyhan et al., 2022; Ar\u0131n et al.,\n2023; Dehghan and Yanikoglu, 2024a,b), where it has consistently shown strong performance.\nWe use BERTurk\u00b3 checkpoint from Huggingface Transformer package and apply fine-tuning for 10 epochs. We\nreserve 5% of the train data as validation and select the model with the best validation performance -accuracy for the\nclassification and MSE loss for the regression tasks. During training, we use a learning rate of 5 \u00d7 10-6, batch-size of 4\nand apply weight-decay regularization with a parameter of 0.01."}, {"title": "Analyzing the Relation Between Classes and Strengths", "content": "As previously noted, during data preparation, annotators were asked to select both a class and a strength value\nranging from 0 to 10 to represent the severity of hate speech in each tweet. This class-strength relationship allows us to\nobtain the strength distributions for each class.\nWe also note that the class \"Discriminatory Speech\" (class-1) is often labelled as no-hate speech. Indeed, classifying\ndiscriminatory speech as hate speech is one of the most argued points, by annotators.\nFinally, the mean strength scores for classes 2 and 3 (4.94 and 5.01) are also similar, as well as the means of classes\n4 and 5 (5.81 and 6.26). This observation supports the rationale for grouping classes 2 with 3 and classes 4 with 5 in\nthe 4-class setting. Distributions for the resulting 4-class setting are improved in the sense that class means are more\nseparated, as depicted in Figure 2b."}, {"title": "Experiments", "content": "We have implemented three methods to handle annotator disagreements (Experiments 1-3) and evaluated the\nperformance of the classification model trained with the data obtained in each case. For each experiment, the data was\ndivided into an 80-20% split for training and testing, respectively. The performance on the test set was evaluated using\nmacro F1 and accuracy scores. In the fourth experiment, we trained and evaluated a regression model using the hate\nspeech strength indicated by annotators. It should be noted that each annotator was allowed to indicate multiple classes,\nbut only a single strength value.\nExperiment 1: We only kept the tweets for which there is consensus (i.e. all annotators have picked the same single\ncategory). Using the terminology defined in Section 5, this corresponds to |ak(x)| = 1 \u2200k and a\u00bf(x) = aj(x) \u221ai, j\nwhere ak (x) is the set of label selections from annotator k for the tweet x.\nNote that this approach filtered out many tweets and resulted in the smallest dataset. Furthermore, as annotators in\ngeneral agree about the no hate speech class, there are almost 4-fold samples in that category. Furthermore, in this case,\nthere must be a single selection from each annotator, so tweets involving multiple classes are also filtered out.\nExperiment 2: In addition to the above tweets, we also kept the tweets where the majority vote is clear (see 5.1.1).\nThis increases both the total number of tweets kept and the proportion of Hate category (where there is less consensus)\namong the data that is kept.\nExperiment 3: In this experiment, we used all the data, including those from the first and second experiments, as\nwell as the data where the majority is not clearly defined. The disagreement rates are approximately 10%, 12%, and\n13% for the 2-class, 4-class, and 6-class problems, respectively."}, {"title": "Classification Results", "content": "The results corresponding to the Experiments 1-4 are given in Tables 1-4. We first observe that our results are on par\nwith state-of-art (Ar\u0131n et al., 2023; Uludo\u011fan et al., 2024) as discussed in Section 6.3. When we consider what happens\nwith respect to different annotator disagreement handling strategies, we first see that there is a significant performance\ndrop between Experiment 1 and 2. For the two-class case, macro F1 score drops from 91.10% to 83.94%, as shown in\nTable 1, with corresponding confusion matrices shown in Table 2. This performance drop highlights the fact that taking\nonly the data samples for which there is agreement among annotators eliminates more challenging samples and result in\nhigher performance.\nWhen we consider Experiment 3, dealing with all of the data with multiple annotators and labels, we see that the\noverall best strategy appears to be taking the minimum label when tie-breaking, across the three classification tasks with\n6-, 4- and 2- classes. However, there isn't a clearly winning strategy with respect to the weighted versus unweighted\napproaches in Table 1. We observed that the two approaches obtained similar results, with unweighted approach\nperforming slightly better."}, {"title": "Regression and Classification Results Using Hate Speech Strength", "content": "The results of the fourth experiment using the 0-to-10 strength labels are presented in Table 4.\nIn the regression experiments, we first trained the model using the mean of the annotated strength scores, which\nrange from 0 to 10, with the Mean Squared Error (MSE) loss function. Then, to obtain the predicted labels, we applied\nthe optimal threshold determined on the validation set, which is the threshold that maximizes accuracy on that set. We\nevaluated the two-class classification with the binary class labels obtained by converting the mean strength scores using\na threshold of 0.5, as described in Section 6. We also trained a classification-based model using the Cross Entropy Loss\nfunction using these binary class labels.\nWe first observe that the regression-based model outperforms the classification-based model (79.42 vs 78.33). This\nfinding suggests that the strength labels contain valuable information that enhances the model's ability to distinguish\nbetween hate and non-hate speech more effectively.\nAnother observation is that considering only the samples where there is agreement results in a very high performance\n(89.80% accuracy) in the regression approach. However, when tested with the more realistic scenario of unfiltered test\ndata, the model performs worse in comparison to the one trained with all data (77.88% vs 78.44% F1 score). This result\nsuggests not filtering out any data.\nFinally, we obtained an ensemble model of the classification and regression-based models by combining the scores:\n$S = \\alpha S_r + (1 - \\alpha) S_c$ (12)\nwhere Sr and Sc are the scores of regression and classification models respectively. The value for the parameter \u03b1 is\ndetermined on a validation set and is set to \u03b1 = 0.93. Note that we normalized the scores to be in the range [0, 1] before\nthe ensemble.\nThe fact that the ensemble performs better than both the regression and the classification models (Table 4 last row)\nsupports that there is complementary information in these models that can be leveraged by an ensemble."}, {"title": "Comparison to Literature", "content": "We have compiled various literature results on hate speech classification in Turkish, as shown in Table 5. While\nour performance results are not directly comparable to any results in the literature, two competitions on the topic used\nearlier versions of this dataset. The winner of the SIU2023-NST competition (Ar\u0131n et al., 2023) obtained 76.87%\nF1 score on the 2-class classification for a subset of the Refugee dataset. Similarly, in the HSD-2Lang competition,\n(Uludo\u011fan et al., 2024) achieved a macro F1 score of 69.64%, while (Dehghan and Yanikoglu, 2024b) attained a macro\nF1 score of 79.49%, both evaluated on a combined test set of three topics (Anti-Refugees, Israel-Palestine conflict, and\nAnti Greek sentiment in Turkey). Our model outperformed these results, achieving a macro F1 score of 82.57% on\nthe extended version of the same combined test set, which also included the LGBTQ+ topic, demonstrating superior\nperformance in hate speech classification across all four topics."}, {"title": "Conclusion and Future Work", "content": "Disagreements in hate speech annotation are often overlooked in the literature. Our work aims to highlight the\nproblem and describe and evaluate alternatives to find the final label, even in in the case of multi-label multi-annotator\nlabelling process.\nOur results show that when we consider only samples with agreeing annotations, the problem is much simplified,\nas evidenced by high accuracy (94.50%) compared to more realistic scenarios. When we include all samples with a\nclear majority label, most of the data is kept and the result is 84.76%. Finally, when all samples are kept, the annotator\ndisagreement management becomes important, as results range from 80.78% to 83.00%, with the best method (taking\nthe minimum label in case of tie) achieving 83%.\nAdditionally, the results indicate that the accuracy of regression on hate strength is 78.33%, while the accuracy of\nclassification on categories is 83.0%. This suggests that classification on categories performs better, likely because\nthe guideline provided annotators with more detailed explanations and examples for each class. In contrast, for hate\nstrength, the guideline only asked annotators to rate the text's hatefulness on a scale from 1 to 10, without much\nguidance. Therefore, it can be concluded that incorporating a clear definition of hate speech, along with detailed\ncategories and examples in the annotation guidelines, can lead to a richer dataset with fewer disagreements. On the\nother hand, it is also interesting to see that the annotator perception about the hate speech strength can be used to train a\npretty successful model.\nWe also realized that the samples with disagreement constitute a relatively small portion (10-13%) of the dataset.\nTherefore, as a straightforward approach, re-annotating these samples through a meta-reviewer process can be an option\nand will help ensure consistency and accuracy in the annotations, ultimately enhancing the reliability of the hate speech\ndetection model.\nNo matter how detailed and precise the guideline is, subjectivity cannot be entirely avoided (Sang and Stanton, 2022;\nWan et al, 2023). For instance, as mentioned in (Fleisig et al., 2023), the phrase \"A woman's place is in the kitchen\", is\nconsidered as hateful by some women, while some men or women do not consider it as hate speech. As it is impossible\nto include all such cases in the guidelines, we believe that annotator disagreements will always exist and need to be\nhandled clearly by researchers.\nAs future work, it would be valuable to explore the impact of annotators' gender, demographic backgrounds,\nethnicity, education level, personal outlook (optimistic or pessimistic), and personal biases on the quality of hate speech\nannotations. By examining how these factors influence the perception and labeling of hate speech, researchers could\ndevelop more comprehensive annotation guidelines and methods that account for these variations. This could lead to\neven more reliable datasets, ultimately enhancing the accuracy of hate speech detection models."}]}