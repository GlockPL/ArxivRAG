{"title": "MixPE: Quantization and Hardware Co-design for Efficient LLM Inference", "authors": ["Yu Zhang", "Mingzi Wang", "Lancheng Zou", "Wulong Liu", "Hui-Ling Zhen", "Mingxuan Yuan", "Bei Yu"], "abstract": "Transformer-based large language models (LLMs) have achieved remarkable success as model sizes continue to grow, yet their deployment remains challenging due to significant computational and memory demands. Quantization has emerged as a promising solution, and state-of-the-art quantization algorithms for LLMs introduce the need for mixed-precision matrix multiplication (mpGEMM), where lower-precision weights are multiplied with higher-precision activations. Despite its benefits, current hardware accelerators such as GPUs and TPUs lack native support for efficient mpGEMM, leading to inefficient dequantization operations in the main sequential loop. To address this limitation, we introduce MixPE, a specialized mixed-precision processing element designed for efficient low-bit quantization in LLM inference. MixPE leverages two key innovations to minimize dequantization overhead and unlock the full potential of low-bit quantization. First, recognizing that scale and zero point are shared within each quantization group, we propose performing dequantization after per-group mpGEMM, significantly reducing dequantization overhead. Second, instead of relying on conventional multipliers, MixPE utilizes efficient shift&add operations for multiplication, optimizing both computation and energy efficiency. Our experimental results demonstrate that MixPE surpasses the state-of-the-art quantization accelerators by 2.6\u00d7 speedup and 1.4\u00d7 energy reduction.", "sections": [{"title": "Introduction", "content": "Large language models have sparked a new revolution across a broad spectrum of tasks, exerting a profound influence on our daily lives. However, the colossal size of LLMs results in high computation and energy costs to train and serve these models.\nThe AI industry is applying many techniques to reduce the cost of models. Quantization is one critical instance of these techniques, in which individual tensor values are cast from full-precision FP32 to a cheaper numeric standard. The most popular quantization formats include INT4, INT8, and FP16, given their vast hardware support."}, {"title": "Related Work", "content": "Nowadays, there is a rapid growth in the scale of large language models, which in turn requires significant hardware resources. For example, Llama-3-70B [9] consumes approximately 148GB of memory just to hold its model weights (in FP16), far exceeding the capacity of a modern GPU like NVIDIA A100 or H100. This imposes a considerable challenge for LLM deployment. To reduce inference costs in LLM deployment, low-bit integer quantization, which maps floating point tensors to discrete level, has become a popular approach [1, 7, 10].\nGiven n bits to represent the integer, the quantization process can be formulated as:\n$Q_x = \\lfloor \\frac{x}{s} + z \\rceil,$\n$s = \\frac{X_{max} - X_{min}}{2^n-1},$\n$z = \\lfloor -2^{n-1} \\frac{X_{min}}{X_{max} - X_{min}} \\rceil,$\nwhere x is the floating-point value, $q_x$ is the n-bit quantized counterpart, s is the scaling factor and z is the zero point. Therefore, the dequantization process can be represented as:\n$x = (Q_x - z) \\cdot s.$"}, {"title": "mpGEMM in Low-bit LLM inference", "content": "In decoder-only LLMs, using different bit-widths for weights and activations creates a unique requirement for mixed-precision General Matrix Multiplication, where the weight matrix uses lower precision while the activation matrix remains in higher precision. This mixed-precision requirement applies to primary GEMM operations in both multi-head attention and feed-forward blocks when weight and activation quantization bits differ. However, current commercial hardware like GPUs and TPUs primarily support canonical GEMM, where both inputs are in the same format and bit-width, and lack native support for mpGEMM. Existing mpGEMM methods generally fall into two main categories: indirect mpGEMM, primarily dequantization-based mpGEMM, and direct mpGEMM, which includes lookup table (LUT)-based mpGEMM and processing element (PE)-based mpGEMM approaches."}, {"title": "Motivation", "content": "Quantizing weights and activations can significantly reduce memory usage and improve computational throughput. Although model weights are known in advance and typically exhibit uniform distributions, achieving accurate representation remains challenging due to the limited precision of 4-bit integer formats. To further improve precision, group quantization is widely used. This method divides the weight matrix into subgroups and performs quantization within each group. To incorporate group quantization into the conventional GEMM pipeline, state-of-the-art quantization algorithms require dequantization operations in the main loop. For an m \u00d7 n \u00d7 k quantized GEMM problem, m represents the number of sequences (or batch size), while n and k correspond to channel dimensions. Both m and n are parallelizable dimensions, whereas k serves as the reduction dimension, requiring a sequential main loop. The main loop includes over 100 iterations, which significantly dominate the runtime of the GEMM operation.\nThe dequantization process in the main loop introduces two significant efficiency bottlenecks. First, dequantization requires additional"}, {"title": "Dequantize After GEMM", "content": "Consider k-dimensional weight and activation vectors:\n$w = [w_0, w_1,..., w_{k-1}], x = [x_0, x_1, ..., x_{k-1}],$\nwhere w is a row vector of the weight matrix $W_{n \\times k}$ and x is a row vector of the activation matrix $X_{m \\times k}$. Based on Equation (2), their inner product can be expressed as:\n$xw^T \\approx \\sum_{i=0}^{k-1} x_i s_{w_i}(Q_{w_i} - z_{w_i}),$\nwhere $s_{w_i}$ and $z_{w_i}$ are the scale and zero point of $w_i$, respectively. Here $x_i$ and $Q_{w_i}$ are in different precisions, with $Q_{w_i}$ in INT4 and $x_i$ in either INT8 or FP16. To perform GEMM on GPUs, $Q_{w_i}$ must be dequantized before being multiplied with $x_i$. In particular, we observe that, when group quantization is employed, the scale factor and zero-point are shared within each group. Let the number of groups be $N_{group} = k/g$, where g is the group size, and denote the set of element indices in the n-th group as $G_n$. As a result, we can express the dot product in Equation (4) as follows:\n$xw^T \\approx \\sum_{n=0}^{N_{group}-1} \\sum_{j \\in G_n} s_{G_n} (Q_{w_j} x_j - z_{G_n} x_j),$\n$\\approx \\sum_{n=0}^{N_{group}-1} s_{G_n} \\sum_{j \\in G_n} (Q_{w_j} x_j) - s_{G_n} z_{G_n} \\sum_{j \\in G_n} x_j.$\nHere $s_{G_n}$ and $z_{G_n}$ represent the scale factor and zero point of $G_n$, respectively. From Equation (5), we observe two key points. First, if multiplication between $Q_{w_j}$ and $x_j$ can be performed directly, dequantization can be applied after the inner-group dot product without sacrificing accuracy. This approach reduces the frequency of dequantization operations to 1/g of the original rate, significantly decreasing dequantization overhead in the main loop. Second, the computational complexity of the first term in Equation (5) is O(k), while the second term is O(1). As a result, most existing accelerators"}, {"title": "Mixed-Precision Processing Elements", "content": "State-of-the-art quantization algorithms for LLM inference often quantize model weights to low-bit integer formats, achieving high theoretical throughput and a reduced memory footprint. In computer systems, integers are represented in binary with a fixed bit width, where each bit can be either 0 or 1. For a 4-bit integer (INT4), this allows for $2^4 = 16$ possible values, enabling INT4 to represent up to 16 distinct numbers. For W4A8 quantization, we employ an unsigned 4-bit (UINT4) quantization on weights as QServe [1], and introduce a specialized mixed-precision processing element design tailored for quantized GEMM:\n$w(UINT4) * x(INT8)$\n$= (w_0 * 2^0 + w_1 * 2^1 + w_2 * 2^2 + w_3 * 2^3) * x$\n$= w_0 * x << 0 + w_1 * x << 1 + w_2 * x << 2 + w_3 * x << 3$\n$= \\sum_{i=0}^{3} \\mathbf{1}(w_i) (x << i),$\nwhere $\\mathbf{1}(w_i) := \\begin{cases} 1, & \\text{the i-th bit of w is 1,} \\\\ 0, & \\text{otherwise,} \\end{cases}$\nwhere w and x represent the values in UINT4 weights and INT8 activations, respectively, and << denotes the left shift operator for integer data formats. Due to the structure of integer representation, original multiplications can be efficiently replaced by bit shifts. Specifically, shifting left by n bits is equivalent to multiplying by $2^n$. Based on Equation (6), we implement MixPE using shift and add operations, enabling highly efficient mixed-precision GEMM with weights in INT4 and activations in INT8. Theoretically, MixPE for W4A8 can achieve 2\u00d7 speed-up, 25% - 50% memory cost savings, and 25% - 50% communication savings, compared with the current INT8 multiplication module in tensor core, which is very promising for scaling-up next-generation hardware accelerators. Then, for W4A16 operations, even though bitwise shifts cannot replace multipliers as they are not applicable to floating-point values, efficient techniques still exist to achieve fast scaling by powers of two without the need"}, {"title": "Multi-Objective Design Space Exploration", "content": "The primary goal of LLM quantization is to reduce memory usage and computational costs during inference. Quantization methods generally seek to balance two often competing factors: (1) quantization accuracy, which reflects the quality loss incurred by quantization relative to FP32 full precision, and (2) hardware efficiency, which combines computational efficiency (energy and silicon area for matrix multiplication) and memory efficiency (the number of bits needed to store or transfer a tensor). Popular quantization formats include INT4, INT8, and FP16, which form the basis of state-of-the-art quantization schemes such as W4A8, W8A8, and W4A16. This variety of data formats and configurations creates a complex design space, requiring careful trade-offs between accuracy, computational efficiency, and memory efficiency. Therefore, we first formalize the design space as a function of quantization accuracy and hardware cost.\nFirst, to assess quantization accuracy, we calculate the quantization signal-to-noise ratio (SNR), a metric shown to reliably predict the end-to-end language model loss induced by quantization [22]. This approach enables extensive design space exploration without the need for costly experiments on each LLM quantization configuration. Specifically, SNR is defined as the ratio of the power of the original (non-quantized) signal to the power of the quantization noise:\n$SNR := \\frac{P_{signal}}{P_{noise}} = log(\\frac{E[||x||^2]}{E[||Qx -x||^2]}),$\nwhere $x = [x_1, x_2,...,x_k] \\in \\mathbb{R}^k$ and $||x||^2 = \\sum_{i=1}^{k} x_i^2$. In quantization, the objective is to ensure that the quantized vector Qx closely approximates the original vector x, minimizing any discrepancies introduced by quantization. This goal corresponds to reducing the denominator term in Equation (8), which represents the quantization noise, thereby preserving the numerical fidelity of the original data. Generally, a higher SNR reflects a quantized vector that more accurately retains the direction and magnitude of the original non-quantized vector, denoting a high-quality quantization process."}, {"title": "Accelerator Hardware Evaluation", "content": "As explained in Section 3.3, we integrate MixPE to the systolic array-based hardware accelerator and compare its performance and energy against INT8 PE and FP16 PE adopted in commercial GPUs, such as Nvidia Tensor Core [31] and Google TPU [21]. For a fair comparison, we set the frequency as 250MHz for all the hardware designs. The number of columns and number of rows are both 4, leading to a 4 \u00d7 4 systolic array structure."}, {"title": "Accelerator Design Space Exploration", "content": "The synthesized accelerator performance, including power and area, can be affected by many aspects, such as aspect ratio, global buffer, and high-performance DSP usage. Therefore, we conducted design space exploration to gain insight in the impact of the different parameters on the performance of hardware accelerators."}, {"title": "Model Performance and Energy", "content": "We evaluate the performance of LLMs on different accelerators, with a batch size of 8. In our experiments, we compare MixPE against four baseline designs: BitFusion [15], OLAccel [16], and traditional INT8 and FP16 PE. BitFusion [15] utilizes a mixed-precision scheme with 4-bit and 8-bit INT types, and we extend OLAccel [16] to support"}, {"title": "Ablation Study of Dequantization Overhead", "content": "We measure the dequantization overhead of the mixed-precision quantization algorithm on both MixPE and INT8 PE. Our analysis focuses on the W4A8 quantization scheme applied to OPT-6.7B, with batch sizes ranging from 2 to 32. The dequantization overhead for both MixPE and INT8 PE is presented in Figure 9. The results indicate that MixPE exhibits significantly lower dequantization overhead compared to the INT8 PE. This reduction becomes more pronounced at lower batch sizes due to the relatively smaller computational load. In practical LLM deployment settings, where batch sizes are often small but the sequence length is long, MixPE emerges as the more efficient choice, offering substantial improvements in both energy and computational efficiency during real-world inference tasks."}, {"title": "Conclusion", "content": "In this work, we present MixPE, a novel hardware accelerator designed to efficiently handle mixed-precision GEMM with minimal dequantization overhead. The key insight lies in exploiting per-group parameter sharing to enable dequantization after mpGEMM, thereby reducing overhead in the main loop. To fully capitalize on the benefits of low-precision weights, we propose a dedicated shift&add-based processing element to achieve mpGEMM without energy-intensive multipliers. MixPE pushes the efficiency of LLM quantization to a new level by co-designing the quantization scheme and hardware accelerator. Moreover, we conduct an exhaustive exploration of design variables, demonstrating that MixPE establishes a Pareto frontier that optimally balances numerical fidelity and hardware efficiency. Finally, MixPE outperforms existing accelerators, achieving a 2.6\u00d7 speedup and 1.4\u00d7 reduction in energy consumption."}]}