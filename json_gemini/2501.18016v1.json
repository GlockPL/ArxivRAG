{"title": "Digital Twin-Enabled Real-Time Control in Robotic Additive Manufacturing via Soft Actor-Critic Reinforcement Learning", "authors": ["Matsive Ali\u2020", "Sandesh Giri", "Sen Liu*", "Qin Yang*"], "abstract": "Abstract-Smart manufacturing systems increasingly rely on adaptive control mechanisms to optimize complex processes. This research presents a novel approach integrating Soft Actor-Critic (SAC) reinforcement learning with digital twin technology to enable real-time process control in robotic additive manufacturing. We demonstrate our methodology using a Viper X300s robot arm, implementing two distinct control scenarios: static target acquisition and dynamic trajectory following. The system architecture combines Unity's simulation environment with ROS2 for seamless digital twin synchronization, while leveraging transfer learning to efficiently adapt trained models across tasks. Our hierarchical reward structure addresses common reinforcement learning challenges including local minima avoidance, convergence acceleration, and training stability. Experimental results show rapid policy convergence and robust task execution in both simulated and physical environments, with performance metrics including cumulative reward, value prediction accuracy, policy loss, and discrete entropy coefficient demonstrating the effectiveness of our approach. This work advances the integration of reinforcement learning with digital twins for industrial robotics applications, providing a framework for enhanced adaptive real-time control for smart additive manufacturing process.", "sections": [{"title": "I. INTRODUCTION", "content": "The evolution of smart manufacturing demands increasingly sophisticated control systems capable of adapting to dynamic production environments. Reinforcement learning (RL) has emerged as a promising approach for developing such adaptive control systems, offering advantages over traditional control methods through its ability to optimize behavior through environmental interaction [1]. Unlike supervised learning ap- proaches that rely on labeled datasets, RL enables autonomous exploration and strategy refinement, making it particularly valuable for robotics applications where continuous adaptation is essential [2], [3].\nHowever, implementing RL in manufacturing environments presents several significant challenges. These include sam- ple inefficiency during training, computational intensity, and potential instability in learned policies [4]. Additionally, the direct application of RL algorithms in physical manufacturing systems risks equipment damage and production disruption during the learning phase [5], [6]. Recent applications have demonstrated RL's potential across various manufacturing do- mains. Wang et al. [7] successfully applied RL to optimize CNC machining parameters in real-time, achieving improve- ments in surface quality while reducing tool wear. In the automotive sector, Loffredo et al. [8] implemented RL-based energy-efficient control systems for multi-stage production lines, effectively balancing energy consumption with through- put requirements. Moreover, RL methods are particularly ef- fective in improving the robustness of microgrids, such as DC microgrids integrating piezoelectric energy harvesting, which shows substantial improvement under failure scenarios [9].\nIn additive manufacturing, RL has shown promise for pro- cess optimization under varied manufacturing environments. Li et al. [10] developed an RL-based predictive model for surface roughness in fused filament fabrication, while Ge et al. [11] utilized RL to optimize path planning for thin- walled structure printing. These studies highlight RL's capacity to enhance manufacturing precision and efficiency, though challenges persist in areas such as real-time adaptation and multi-objective optimization.\nDigital twin technology offers a potential solution to these implementation challenges by providing a safe, virtual robotic additive manufacturing environment for RL training and real- time communication with real robot path planning for 3D printing process. Digital twins enable real-time mirroring of physical systems, facilitating simulation-based optimization and monitoring [12]. When combined with RL, digital twins create a protected testing environment that minimizes physical risks while allowing for rapid iteration and validation [13]. Furthermore, the digital twin synchronization setup enables robust RL model training in a virtual environment, where trained models can then be validated in real-world settings. This integration provides a high-fidelity testing environment for robotic control algorithms, improving safety, reliability, and efficiency in robotics research [14]\u2013[17]."}, {"title": "A. Unity's Role in Simulation and Real-Time Control", "content": "Unity has emerged as a valuable simulation platform across fields such as robotics, autonomous navigation, temperature control, and underwater vehicle testing. Its integration with Robot Operating Systems (ROS and ROS2) enables real-time control and synchronization between virtual simulations and physical robots, making it a preferred choice for simulat- ing RL-based control systems. In collaborative robotics and motion planning, Unity allows for interactive path editing and real-time trajectory generation, which is essential for optimizing robot control strategies in hazardous or dynamic environments [18]. The applications with Unity have under- scored Unity's value for autonomous systems development and real-time control, where high-fidelity virtual environments allow for extensive testing and refinement of RL algorithms without the constraints of real-world testing [19], [20].\nThe Temporal Gauss-Seidel (TGS) solver in Unity enhances the realism and stability of physics simulations by iteratively resolving constraints, stabilizing interactions, and reducing jitter, particularly in real-time applications. TGS's efficiency in managing complex collisions and joint interactions is crucial for scenarios requiring precision, such as robotics, vehicle dynamics, and immersive virtual environments. For example, TGS supports applications like real-time robot control and vehicle navigation by stabilizing simulations to reflect realistic physical behavior [21]. This solver provides computational efficiency, making Unity a top choice for interactive environ- ments where both realism and real-time performance are es- sential. Other approaches, such as physics-based deep learning frameworks, offer alternative methods that may provide more generalized solutions for broader applications, but Unity's TGS solver remains a practical choice for scenarios requiring immediate, reliable responses [22], [23]."}, {"title": "B. Challenges in Robot Arm Control and Path Optimization", "content": "Robot arm control and path optimization present several challenges, especially in dynamic environments where the task conditions are constantly changing [24]. Traditional control methods often struggle to adapt in real-time, relying on predefined models and static decision-making processes. Path optimization is particularly challenging because robots must calculate the most efficient trajectory while avoiding obstacles, complying with kinematic constraints, and reacting to real- time environmental feedback [25]. These factors make path planning in robotics a complex and time-consuming task.\nReinforcement learning offers a potential solution to these challenges, as it enables robots to learn optimal strategies for controlling motion and planning paths. By continuously inter- acting with the environment and receiving feedback, RL agents can improve their decision-making over time, leading to better performance in dynamic settings. However, RL-based systems must overcome issues such as slow convergence, local minima, and instability in learning, which can limit their effectiveness. This research aims to address these challenges by leveraging the Soft Actor-Critic (SAC) RL algorithm and Hierarchical Reward Structure (HRS) to improve the training efficiency, stability, and adaptability of robotic systems, particularly in path optimization tasks.\nThe primary objective of this research is to explore the inte- gration of reinforcement learning (specifically the Soft Actor- Critic algorithm) with digital twin technology to enhance robot arm control and optimize path planning. The key goals of the study are outlined as follows:\n\u2022 Integration of SAC with Real-Time Synchronization:\nCombine the SAC RL algorithm with real-time synchro- nization for controlling the Viper X300s robot arm in both virtual and physical environments.\n\u2022 Task Scenarios: Explore two distinct robotic tasks: a\nstatic target-reaching task and a dynamic goal-following task to evaluate the RL agent's performance for line scanning settings.\n\u2022 Hierarchical Reward Structure (HRS): Leverage HRS\ntechniques to enhance model adaptation across tasks, accelerating training efficiency and convergence. Develop reward mechanisms tailored to overcome RL-specific challenges such as local minima and instability in learn- ing.\n\u2022 Comprehensive Evaluation of Performance Metrics:\nAssess cumulative reward, episode length, policy loss, and value prediction accuracy in both virtual and physical settings.\nBy combining Unity's simulation environment, RL training, ROS2, and real robot arm communication, this research aims to advance robotic adaptability, task stability, and learning efficiency, contributing to the broader fields of autonomous robotics applications and smart additive manufacturing pro- cess."}, {"title": "II. METHODOLOGY", "content": "A. Experimental Setup and Digital Twin Synchronization\n1) Viper X300s Robot Arm: Our experimental setup utilizes\nthe Viper X300s, a Six Degree of Freedom (DOF) robot arm designed by Trossen Robotics for research applications. With a span of 1500mm and a payload capacity of 750g, the Viper X300s offers researchers a compact yet powerful platform for experimenting with robotic functionalities. Through ROS inter-bridging, researchers can simulate tasks in Unity's virtual environment and then validate them on the Viper X300s, creating a seamless feedback loop between virtual testing and physical deployment.\nThe Viper X300s, designed with biologically inspired adapt- ability, can operate effectively in unstructured environments, making it well-suited for applications like exploration, emer- gency response, and even hazardous material handling. It features low-cost computer vision capabilities combined with neural networks for object tracking and classification, which broadens its adaptability across various automation tasks, from precision assembly to search and rescue operations [26], [27].\n2) Digital Twin Unity Environment Setup: The Unified\nRobot Description Format (URDF) file for the Viper X300s robot arm was imported into Unity's virtual environment, which included a simulated desk setup, enabling accurate simulations of the robot's kinematics and movement dynamics. This allows researchers to manipulate joint positions, adjust trajectories, and experiment with spatial orientation, providing a practical way to simulate robotic behavior before real-world application. By integrating polynomial algorithms, the simu- lation can achieve high precision in task execution, helping to ensure accurate and repeatable operations [28]. Simulating tasks like pick-and-place within Unity reduces the wear on physical hardware, allowing for extensive testing without physical risk. Unity also supports collaborative, remote-access features, making the virtual environment an accessible and versatile tool for robotics research and training in a controlled, risk-free setting [29].\nHowever, there is a lack of ROS2 package for subscribing to Unity Publisher for the Viper X300s robot arm, hence a ROS2 subscriber package was developed. Establishing digital twin connection as given in 1, where virtual viper x300s robot arm in Unity continuously sends data of the robot arms joints to a local TCP-server and the developed ROS2 package subscribes to these data and move the robot arm continuously.\n3) ROS-TCP Connector for Communication and Control:\nThe Unity ROS-TCP-Connector extension enables seamless publishing of ROS2 messages to a local cloud server, allowing bidirectional communication between Unity and ROS-based systems. These messages are subscribed by ROS2 for pro- cessing on the physical Viper X300s robot arm, establishing real-time digital twin synchronization. This synchronization has a latency of only about 20 milliseconds, ensuring near- instantaneous reflection of virtual movements on the physical counterpart, which allows for testing and validation of robotic tasks in a simulated environment before deployment [30]\u2013[32]."}, {"title": "B. Reinforcement Learning in Unity", "content": "1) Unity ML-Agents Framework for Reinforcement Learn-\ning: Unity's ML-Agents extension enables RL-based agent training in simulated environments, including robotics, game AI, and autonomous systems. We implemented the RL frame- work as shown in Figure 2. With support for RL algorithms such as Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), the ML-Agents toolkit facilitates complex sim- ulations where agents learn through continuous interaction with their environment. ML-Agents allows Unity to act as a simulation environment while handling training processes in Python, creating an efficient workflow for testing and optimizing RL models in robotics [33], [34].\nOur established algorithms environment was run in Unity that consisted of different objects and a virtual robot arm as illustrated in Figure 2. The algorithms were trained through Python Trainer. Initially, the states of the environment was fed to the RL agent so that it could gain an understanding of the working environment. After getting the state, the agent took a set of action policies to asses the change in state and reward. These changes were then further fed into the agent to store the set of action policies in replay buffer. Following this implementation, the agent improved its set of action policies and then evaluated them. After which the environment communicated the state and reward back to the agent. This iteration was done throughout the training, and after the Case 1 training was completed, the trained model was transferred to the case 2 training which also utilized the trained models' weights to adjust and improve its learning.\n2) Action Space for Robot Arm Control: To control the robot arm, we defined a discrete action space with seven action branches: six for each joint's movement and one for gripper translation, as outlined in Table I. Each action branch has three options (e.g., turn left, stop, turn right), allowing fine-grained control over the robot's joint movements. The agent receives state information from Unity that includes positional data for the gripper, goal position, end-effector, and distance to the goal, all of which are used to help the agent learn and optimize its movements for task completion.\nThe choice of dynamic model and controller greatly influ- ences the efficiency of training. Different dynamic models, including kinematic and dynamic models, can affect both the accuracy and speed of RL tasks in robotic navigation [39]. Moreover, the use of sliding mode control (SMC) in trajectory tracking, particularly in challenging environments with sharp turns, can benefit from trajectory smoothing techniques that improve RL performance [40]. Additionally, RL can be applied in microgrids for voltage and frequency regulation, demon- strating its ability to outperform conventional controllers and improve system stability in islanded conditions [41]."}, {"title": "3) Hierarchical Reward Structure Policy:", "content": "A hierarchical reward structure policy was implemented to improve learning efficiency.\nThe tables II, III and IV below outlines the reward frame- work designed for two cases in the reinforcement learning task, highlighting the primary goals, sub-goals, reward values, and the utilization of transfer learning to enhance model adap- tation. The listed framework strategically combines positive reinforcement for achieving objectives and maintaining sta- bility with penalties for undesirable actions, while leveraging transfer learning from simpler tasks to more complex ones.\nThis strategy decomposes the task into sub-goals, each with its own reward structure, allowing the agent to achieve incre- mental successes and maintain consistent learning progress. By incorporating intermediate rewards for sub-goals, the agent learns high-level strategies and low-level actions more effec- tively, improving training scalability and stability [42], [43]. Sub-goals in RL tasks enhance convergence speed and task accuracy [44], while structured reward policies in multimodal systems manage data fusion complexity, boosting efficiency and accuracy [45], [46]. Hierarchical models in VR training systems for manufacturing also optimize learning outcomes by adapting to novice learners' pace [47].\nTo further accelerate learning, we used transfer learning based on the hierarchical reward structure, where the trained model from Case 1 was fine-tuned for Case 2, leveraging pre- learned features to reduce training time and computational costs. Transfer learning enhances performance and efficiency, particularly in scenarios where the new task shares similarities with a previously trained model."}, {"title": "III. RESULTS AND DISCUSSION", "content": "The experimental results demonstrate the effectiveness of our SAC-based reinforcement learning approach across mul- tiple performance metrics, as illustrated in Figure 4. The analysis encompasses four key dimensions: cumulative reward, episode length, policy loss, and value prediction accuracy, tracked over 200,000 training steps.\n1) Cumulative Reward Progression: Figure 4(a) reveals distinct learning patterns across three experimental cases. In Case 1 (static target-reaching), the agent initially encountered a significant local minimum, evidenced by the sharp dip in reward percentage around the 25,000-step mark. However, the implementation of our hierarchical reward structure enabled recovery, leading to stable convergence at approximately step of 60,000. Case 3 without the hierarchical transfer training showed a gradual reward increase but with slow convergence until the step of 150,000.\nCase 2, leveraging transfer learning from Case 1, demon- strated remarkably faster convergence despite handling the more complex task of following a moving target. This accel- eration validates our transfer learning approach which showed clear improvements. Case 2 also achieved gradual increase of reward value, at the same time with fast speed of convergence at a step of 50,000, suggesting superior policy performance compared to Cases 1 and 3. Additionally, Case 2 reached a higher cumulative reward more quickly and maintained it consistently, achieving the task's goals more effectively than Case 3.\n2) Value Loss and Policy Optimization: The value loss trajectories in Figure 4(b) provide insight into the agent's reward prediction accuracy. Case 1 exhibits rapid stabilization at approximately 5% value loss, indicating efficient learning of the state-value function due to the relative easier task of object touching. Case 2 shows gradually increasing of value loss and then decreases rapidly to achieve stability at 100,000 steps. In Case 3, it shows a similar trend in the beginning as Case 2, but with a much slower decrement. The peak at around step 50,000 is shown for both Cases 2 and 3, likely indicating more significant adjustments during early training, which gradually stabilize, with Case 2 stabilizing at a higher speed than Case 3. Furthermore, Case 2 stabilized at a lower value loss compared to Case 3, indicating that its value function approximations are more accurate, which helps improve decision-making.\n3) Policy Evolution and Entropy Analysis: Policy loss trends in Figure 4(c) reveal the adaptation characteristics of the SAC algorithm. It illustrates the tendency of the agent to change its setup actions per iteration. Case 1's policy loss stabilized at -200%, while Case 2 showed a gradual decline to -1200%, reflecting increasingly deterministic pol- icy selection. Figure 4(c) further revealed that Cases 2 and 3 experienced greater policy adjustments, as seen by their large negative values, while Case 1 remains relatively stable. Although both cases show substantial negative policy loss, Case 2's loss stabilizes faster, implying that it found an effective policy earlier and with less overcorrection than Case 3. The discrete entropy coefficient from Figure 4(d) further supports this observation, demonstrating the agent's transition from exploration to exploitation, particularly evident in Case 2's rapid entropy reduction after 50,000 steps. Case 3 took the longest to stabilize, taking approximately 125,000 steps. Case 2 initially encouraged exploration with a high entropy coefficient but reduced it faster than Case 3, signaling a more efficient transition from exploration to exploitation."}, {"title": "B. Real-world Validation Through Digital Twin Synchronization for Robotic AM Line Scanning", "content": "The practical validation of our approach is demonstrated in Figure 5, which shows the synchronized movement of virtual and physical Viper X300s robots during a linear scanning task. The temporal sequence captures key positions at 6, 9, 12, 14, and 16 seconds, revealing highly coherent behavior between the simulated and physical systems, and the observed latency remained consistently around 20 milliseconds.\nReal-world validation demonstrates that the learned policies transfer smoothly and reliably from simulation to physical hardware, confirming the effectiveness of the training ap- proach. This smooth transfer underscores the model's robust- ness in handling real-world complexities and suggests that simulation-based training can substantially reduce the need for extensive, time-consuming testing on physical systems.\nThe success of the hierarchical training structure is largely attributed to several key factors. First, the use of high-fidelity physics simulation, facilitated by Unity's TGS solver, ensures accurate and realistic modeling of physical interactions. Sec- ond, the implementation of a robust reward structure effec- tively incorporates practical constraints, guiding the agent to- ward achieving desired behaviors while penalizing undesirable actions. Lastly, effective domain randomization during training enhances the model's adaptability and generalization, enabling it to perform reliably under varying real-world conditions.\nThe results demonstrate enhanced stability in transfer learn- ing, with a notably smooth transition between Cases 1 and 2. This stability indicates that the SAC-based approach can maintain consistent performance when transferring learned policies across tasks, which is essential for applications re- quiring adaptive control.\nFurthermore, Case 2 achieved higher training efficiency and improved overall performance compared to Case 3, emphasiz- ing the effectiveness of integrating transfer learning within this framework. The reduced training time and increased reward attainment in Case 2 suggest that transfer learning not only accelerates convergence but also helps the model adapt more effectively to task variations. This efficiency is especially valu- able in robotic systems, where rapid learning and adaptation are critical for handling dynamic and evolving tasks. These improvements can be attributed to our hierarchical reward structure and the effective integration of transfer learning principles."}, {"title": "IV. CONCLUSION", "content": "Our research successfully demonstrates the integration of RL with digital twin technology to enhance robotic control within manufacturing applications. By utilizing the SAC al- gorithm and the Viper X300s platform, we tackle several key challenges in smart manufacturing, ultimately developing adaptive and efficient control systems. The findings from this work highlight the transformative potential of RL-driven digital twins, showcasing their ability to enable robust, real- time synchronization between virtual simulations and physical additive manufacturing process. This breakthrough opens new avenues for optimizing automation processes and bridging the gap between simulation and real-world applications.\nA. Contributions and Performance Analysis\nThe primary contributions of this study include the de- velopment of a high-performance digital twin synchroniza- tion framework that achieves approximately 20ms latency, the implementation of a hierarchical reward structure that significantly enhances learning efficiency and policy stability, and the successful application of transfer learning techniques across related tasks to drastically reduce training time. In terms of performance, the SAC algorithm demonstrated its capability to achieve consistent convergence in static tasks within just 60,000 steps, while transfer learning enabled rapid adaptation to more complex dynamic tasks with minimal additional training. Notably, when transferring the learned policies to physical hardware, performance degradation was limited to less than 5%. These results validate the effectiveness of using digital twins for rapid prototyping, testing control strategies, and exploring edge cases-all while minimizing the need for extensive real-world testing, and consequently reducing system downtime. These achievements underscore the potential of RL and digital twin integration in advancing automation within manufacturing settings.\nB. Future Work\nDespite these promising results, scalability, environmental variability, and real-time adaptation remain challenges. Future work will focus on extending the framework to multi-robot coordination, investigating hybrid learning approaches such as combining SAC with imitation learning or model-based RL, and enhancing digital twin fidelity to narrow the reality gap. Additionally, research will explore seamless integration with manufacturing execution systems and enterprise resource planning platforms. These advancements aim to expand the applicability of RL and digital twin technologies in adaptive, high-precision, and flexible manufacturing systems.\nThe findings of this study demonstrate that reinforcement learning, when effectively integrated with digital twin tech- nology, provides the adaptability and reliability needed for next-generation manufacturing systems. Success in both static and dynamic tasks, combined with efficient transfer learning capabilities, underscores the potential of this approach. These results pave the way for more autonomous and adaptable manufacturing processes, establishing a solid foundation for future innovations in smart manufacturing"}]}