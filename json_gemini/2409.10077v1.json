{"title": "LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain", "authors": ["LE XIAO", "YUNFEI XU", "JING ZHAO"], "abstract": "Domain-specific Named Entity Recognition (NER), whose goal is to recognize domain-specific entities and their categories, provides an important support for constructing domain knowledge graphs. Currently, deep learning-based methods are widely used and effective in NER tasks, but due to the reliance on large-scale labeled data. As a result, the scarcity of labeled data in a specific domain will limit its application. Therefore, many researches started to introduce few-shot methods and achieved some results. However, the entity structures in specific domains are often complex, and the current few-shot methods are difficult to adapt to NER tasks with complex features. Taking the Chinese coal chemical industry domain as an example, there exists a complex structure of multiple entities sharing a single entity, as well as multiple relationships for the same pair of entities, which affects the NER task under the sample less condition.In this paper, we propose a Large Language Models (LLMs)-based entity recognition framework LLM-DER for the domain-specific entity recognition problem in Chinese, which enriches the entity information by generating a list of relationships containing entity types through LLMs, and designing a plausibility and consistency evaluation method to remove misrecognized entities, which can effectively solve the complex structural entity recognition problem in a specific domain. The experimental results of this paper on the Resume dataset and the self-constructed coal chemical dataset Coal show that LLM-DER performs outstandingly in domain-specific entity recognition, not only outperforming the existing GPT-3.5-turbo baseline, but also exceeding the fully-supervised baseline, verifying its effectiveness in entity recognition.", "sections": [{"title": "INTRODUCTION", "content": "The goal of Named Entity Recognition (NER) is to identify mentioned entities in a sentence and classify them into predefined entity types. In general domains [2], these entity types can include names of people, places, organizations, time, etc. Whereas in specific domains, further identification of entity types specific to that domain is required. For example, in the biomedical domain [3], entities such as diseases and chemical substances need to be identified.Currently, deep learning-based methods are widely used in NER tasks [4], and advanced results have been achieved [5,6]. Compared to the traditional method of manually constructing features, deep learning-based methods do not need to manually design features and can automatically obtain models by training a large amount of data. Among them, based on the excellent sequence modeling capability of unidirectional Long Short-Term Memory (LSTM) models, many methods use LSTM Conditional Random Fields (CRF) as the main framework for the NER task and incorporate a variety of related features based on it [7].BiLSTM-CRF is the most commonly used method, with which state-of-the-art performance is achieved using this method as the main framework[4].However, deep learning models rely on a large amount of labeled data for training [8,9] in order to efficiently extract entity features, and the scarcity of labeled data may limit the performance of deep learning methods in specific domains because large-scale labeled data is very expensive and time-consuming. Although few-shot learning methods can alleviate the data scarcity problem to a certain extent, it is difficult to achieve good results in entity recognition in Chinese specific domains with complex structures [1]. For example, the field of coal chemical industry covers numerous products with complex upstream and downstream relationships. A product can usually be obtained through multiple synthesis pathways, and in the process of preparing downstream products, it often needs to rely on multiple upstream chemical raw materials. In addition, different chemical products may substitute each other in application scenarios. We found that entities within the Chinese coal chemical industry usually consist of multiple vocabularies through their data analysis and experimental comparisons(as shown in Figure 1), and there are complex situations in which multiple entities share a single entity, as well as the same pair of entities has multiple relationships, leading to the inability of the few-shot methods to adapt to the NER task with complex structural features.\nHowever, recent studies [36,37] have shown that large language models (LLMs) such as GPT-3 and ChatGPT, even without any training or fine-tuning, rely solely on in-context learning (ICL) methods to make predictions by concatenating a query with a few-shot demonstrations to prompt LLMs. It also performs well in various natural language processing (NLP) downstream tasks [11]. Because LLMs is better able to capture contextual information in texts and infer the boundaries and types of entities through context, which helps to identify entities with complex and specialized structures in domain-specific texts. And LLMs can perform transfer learning, where knowledge learned from texts in other domains helps to process entities in texts from different domains [18]. To this end, the LLM-DER framework is proposed, which enhances entity contextual relevance by adding entity type information at both ends of predefined relationships and using this information to generate a list of similar relationships, and removes misrecognized entities through a plausibility and consistency weighted assessment. We evaluate our proposed method on the self-built Chinese coal chemical domain dataset Coal and the public dataset Resume. The experimental results show that LLM-DER can effectively extract domain-specific entity information and exceed the fully supervised baseline."}, {"title": "RELATED WORK", "content": "Named entity recognition methods mainly include: rule-based methods, statistical model-based methods and deep learning-based methods. Earlier, named entities were mainly recognized by manually writing rules, which required domain experts to manually construct recognition rules, which is time-consuming and laborious. Unlike rule-based approaches, statistical model-based approaches do not require domain experts to formulate rules and use Hidden Markov Model (HMM) [17], Maximum Entropy Model (MEM) [32] or Conditional Random Fields Model (CRF) [21] on manually labeled datasets, etc. can extract data features better, but in practical applications, the labeling of data and the design of features limit the development of this method."}, {"title": "Deep Learning Based Approach", "content": "With the development of deep learning techniques, it has rapidly become a research hotspot due to its excellent performance on named entity recognition tasks.Collobert et al [12] proposed a CNN-CRF model based on convolutional neural network (CNN) and CRF, which achieved excellent performance compared to previous models.Ma et al [13] and Chiu et al [14] introduced CNN-extracted character features in a word-level representation based on the BiLSTM-CRF framework to enhance the expressive power of the word-level representation.Dong et al [15] composed each character into a root sequence and utilized Long Short-Term Memory Network (LSTM) to obtain the root information of Chinese characters.Zhang et al [16] proposed Lattice-LSTM method, which replaces the traditional LSTM cells with lattice"}, {"title": "Approaches Based On Large Language Modeling", "content": "LLMs has learned rich linguistic knowledge and contextual information through pre-training on huge scale text corpus, which enables LLMs to infer the boundaries and types of entities by understanding contextual cues of the text, and helps to recognize named entities in the text [10]. Currently, there are some researches based on LLMs to do NER, for example, Polak et al[23] proposed ChatExtract, which consists of a set of designed prompts that both recognize sentences with data, extract the data, and ensure the correctness of the data through a series of follow-up questions. Wang et al[24] proposed GPT-NER, which firstly converts the sequence labeling task into a generative task that can be easily accomplished by LLMs, and then proposes a self-verification strategy that effectively solves the \"illusion\" problem of LLMs by asking LLMs whether the entities they recognize belong to the labeled entity labels. The models and methods mentioned above are analyzed and summarized in detail in the following, as shown in Table 1."}, {"title": "LLM-DER", "content": "The LLM-DER framework is shown in Figure 2. There are three components in total:(1) Relationship List Generation:By predefining specific relationships, entity type information is associated with each relationship to improve"}, {"title": "Relationship List Generation", "content": "Considering that specific domains usually involve multiple entities in different segments, this leads to a situation where entities and relationships are entangled and interact with each other.Therefore, we first predefine context-specific relationships and associate these relationships with the type information of the entities at both ends, with the aim of establishing connections between entities through the relationships and thus enhancing the LLM's perception of the entities in the text. For example, when using \"production\" as a relationship, the entity types should be \"enterprise\" and \"product\" respectively. Based on the fact that the diversity of cueing instructions can further enhance the comprehension ability of LLMs [33,34], we utilize the zero-shot learning ability of LLMs to generate diversified relations that are similar to the predefined relations and associate them with the entity type information respectively. The diversified relations thus generated from each predefined relation are recorded as a list of relations $R_i={r_1,r_2......r_n}$, where n denotes the number of diversified relations generated and i denotes the number of predefined relations."}, {"title": "Associated Entity", "content": "Based on the generated list of relationships $R_i={r_1,r_2......r_n}$, the entities in the text are associated. In addition, we took each predefined relation and constructed presentations for LLMs, aiming to fully utilize the ICL capabilities of LLM[35]. Since each relation in the relation list is accompanied by entity type information, LLMs determines the entities present in the text and recognizes the entities by understanding the semantic information of entity-relationships and utilizing the semantic links between entities and relations. For example, the j-th diversity relation in the i-th relation list results in:$r_j={{subject_1,object_1}, {subject_2,object_2},......, {subject_n, object_n}}$, where n is the number of matched entity pairs, and we treat the obtained matching result of each diversified relation as a relation pattern candidate for this relation, denoted as $(R_{type}, {r_1,r_2......r_n})$,where $R_{type}$ denotes the predefined relationship and n denotes the number of similar relationships."}, {"title": "Entity Screening And Validation", "content": "After associating entities, the entity information is distributed among the relationship schema candidates. Because of the phantom problem of LLMs, this section is to describe how to determine the final entity information by obtaining the two slots from each relationship candidate that are significantly reliable and consistent, i.e., head entity SUBJECT, and tail entity OBJECT.We use the i-th relationship list $(R_{type}, {r_{i1},r_{i2},......r_{in}})$,to denote the result of associating entities, where $R_{type}$ denotes the predefined relationship and n denotes the number of similar relationships, where the j-th relationship candidate is denoted as $r_j = (subject_1,object_1),(subject_2,object_2)........(subject_n,object_n)$, We evaluate and associate the entity slots by using reliability and consistency metrics, while eliminating the erroneous entities and aggregating the entity information distributed over the relationship candidates to the predefined relationships, and use $R_{type}' = {subject_1, object_1}, {subject_2,object_2},......, {subject_n, object_n}$ to denote the result after screening and validated result, where n is t"}, {"title": "Implementation Details", "content": "During the training process of fine-tuning the BERT model for similarity computation between sentence and entity pairs, the hyper-parameters were configured as follows: a learning rate of 2e-5, a batch size of 16, and a maximum sequence length of 128 tokens, and 3 cycles of training were performed. The model was implemented and trained using the PyTorch deep learning framework, and the Adam optimizer was used for parameter optimization."}, {"title": "Evaluation", "content": "In both experiments on the two datasets in this paper we only consider perfect matching, i.e., we consider a predicted entity to be a correct prediction only if its boundary and type are the same as those of the real entity, and in order to keep up with the baseline model, we similarly evaluate it using accuracy precision(P), recall recall(R), and F1-score(F1), which are defined as follows:\n$P = \\frac{N_m}{N_p}$ (4)\n$R = \\frac{N_m}{N_r}$ (5)\n$F_1 = \\frac{2xPxR}{P+R}$ (6)\n$N_m, N_p$ and $N_r$ denote the total number of correctly predicted entities, predicted entities, and true entities, respectively.F1 is a coordinated average of accuracy and recall, a composite metric that balances the impact of accuracy and recall."}, {"title": "Baseline", "content": "In order to evaluate that our proposed LLM-DER framework is more effective for the NER task in the Chinese coal chemical industry, we compare it with the following other methods.\nBERT [30] is a NER method based on BERT that adds a labeled classifier layer downstream of the BERT model.\nLattice LSTM [16] is a character-based Chinese NER method that uses a lattice-structured LSTM model to introduce dictionary information.\n\u2022 FLAT [19] is a transformer-based NER method for lattice structures. It constructs a flat-structured transformer to fully utilize the lattice information and take advantage of parallel computation on GPUs.\nPCBERT [29] is a cue-based small-sample NER model for Chinese. It consists of a P-BERT component and a C-BERT component, integrating lexical features and implicit labeling features."}, {"title": "RESULTS AND ANALYSIS", "content": "In this section, we show the experimental results of LLM-DER on the Resume and Coal datasets, as shown in Tables 3 and 4. To evaluate the effectiveness of LLM-DER, we adopt PCBERT's sampling method to simulate few-shot scenarios on Coal and Resume datasets. We took 250, 500, 1000 and 1350 samples from them as training sets to observe the changes of F1 values under different data ranges, and these values were used as evaluation metrics for the final performance."}, {"title": "Main Results", "content": "According to the results in Tables 4 and 5, our method did not demonstrate significant advantages over the chosen SOTA model on the public dataset Resume. However, on the coal chemical dataset, our method significantly outperforms the chosen model and achieves the highest F1 value in every sample size condition. This suggests that our method has an advantage in domain-specific entity recognition tasks. In addition, we also observe that there is a significant difference in the performance of our method on the Resume and Coal datasets. On the Coal dataset, as the number of samples K increases from 250 to 1350, we observe changes in F1 values of 0.22%, 1.09%, and 1.31%, respectively; while on the Resume dataset, these changes are 0.57%, 0.37%, and 0.36%, respectively. This suggests that our approach exhibits higher sensitivity and adaptability when dealing with the coal-chemical domain with complex entity structure patterns, leading to larger fluctuations in F1 values with increasing sample size. In contrast, the texts in the resume dataset typically lack complex entity structures, and thus we observe smaller variations in the F1 values. This phenomenon highlights the performance of our approach in processing domain-specific data in terms of its feature sensitivity and adaptability to data complexity.\nThus, the method of generating a list of relations by predefining specific relations increases the diversity and enables LLMs to understand complex structures and recognize entities more accurately. The weighted assessment of reliability and consistency removes possible recognition errors due to the LLMs illusion problem, thus enhancing entity recognition. To further validate the effectiveness of LLM-DER on the Coal dataset, we performed ablation analysis."}, {"title": "Ablation studies on the Coal dataset", "content": "To validate the effectiveness of the relationship list module in LLM-DER and the weighted consistency assessment module for avoiding the LLMs illusion problem, we conducted ablation experiments on the Coal dataset. Table 6 shows in detail the impact of each module on the performance of the named entity recognition task."}, {"title": "CONCLUSION", "content": "This study aims to solve the domain-specific NER problem and takes the coal chemical industry domain as a study case for domain-specific entity recognition. First, to address the scarcity of domain-specific labeled data and the insufficiency of few-shot learning methods, we apply GPT-3.5-Turbo to the domain NER task and design the LLM-DER framework, which employs two strategies to recognize relevant entities:(1) adding entity type information at both ends of predefined relationships, and using contextual relationships to establish connections between entities, thus enhancing the LLMs perception of entities in the text (2) designing reliability and consistency weighted assessment methods to remove misrecognized entities. The experimental results on Resume dataset and Coal dataset show that LLM-DER has excellent performance in the coal chemical industry.\nSince the training data of the large language model is from the general domain and there is no data from the specialized domain, to better apply the LLMs to the domain-specific entity recognition task, in the future, we can consider trying to fine-tune the current large language model, such as LLaMA2, using domain-specific corpus to further optimize the task performance."}, {"title": "Reliability", "content": "Reliability(slot) = \\beta \\sum_{R_{type}} \\frac{Reliability(s_{k})}{|R_{type}|} + (1-\\beta) \\frac{1}{d(s_{k})} (1)\nwhere \\beta is a hyperparameter,|R_{type}| is the number of slots of Ktype, $\\frac{R_{Reliability(s_{k})}{|R_{type}|}$ denotes that slot s and slot $(s_{k})$co-occur in a relation candidate, and the initialized slot reliability score is $\\frac{1}{|R_{type}|}$ The PageRank algorithm stops after T iterations or when the change in the reliability score is less than a threshold value E.\nConsistency:since LLMs may recognize wrong entities, in order to remove wrong recognition results, we use semantic similarity based on BERT [27] to evaluate the consistency between entities and texts in relation candidates."}, {"title": "Consistency", "content": "Consistency(s)j= Sim(rtype,T | s,rtype \\in c) (2)\nwhere Sim() denotes the semantic similarity function, both s and $r_{type}$ come from the candidate relational schema c, and s is an entity slot in the relationship candidate.\nThe final assessment of a slot by an entity is to have and consistency assessed together[28]:\nScore(s( = )\\lambda * Reliability(s)) * Consistency(s)i (3)\nFinally, we keep only the entity slots in candidate relationship patterns with the highest confidence scores. Entity slots for candidate relationship patterns with confidence scores below a preset threshold are filtered. By evaluating reliability and consistency together, we effectively mitigate untrue entities identified by the LLMs illusion problem."}]}