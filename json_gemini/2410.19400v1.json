{"title": "Offline Reinforcement Learning with OOD State Correction and OOD Action Suppression", "authors": ["Yixiu Mao", "Cheems Wang", "Chen Chen", "Yun Qu", "Xiangyang Ji"], "abstract": "In offline reinforcement learning (RL), addressing the out-of-distribution (OOD) action issue has been a focus, but we argue that there exists an OOD state issue that also impairs performance yet has been underexplored. Such an issue describes the scenario when the agent encounters states out of the offline dataset during the test phase, leading to uncontrolled behavior and performance degradation. To this end, we propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression in offline RL. Technically, SCAS achieves value-aware OOD state correction, capable of correcting the agent from OOD states to high-value in-distribution states. Theoretical and empirical results show that SCAS also exhibits the effect of suppressing OOD actions. On standard offline RL benchmarks, SCAS achieves excellent performance without additional hyperparameter tuning. Moreover, benefiting from its OOD state correction feature, SCAS demonstrates enhanced robustness against environmental perturbations.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (RL) shows promise in solving sequential decision-making problems, gaining increasing interest for real-world applications [43, 58, 64, 54, 8]. However, deploying RL algorithms in extensive scenarios poses persistent challenges, such as risk-sensitive exploration [14] and time-consuming episode collection [28]. Recent advances view offline RL as a hopeful solution to these challenges [35]. Offline RL aims to learn a policy from a fixed dataset without further interactions [33]. It can tap into existing large-scale datasets for safe and efficient learning [24, 38, 51].\nIn offline RL research, a well-known concern is the out-of-distribution (OOD) action issue: the evaluation of OOD actions causes extrapolation error [13], which can be exacerbated by bootstrapping and result in severe value overestimation [35]. To address this issue, a large body of work has emerged to directly or indirectly suppress OOD actions during training, employing various techniques such as policy constraint [13, 31, 11], value penalization [32, 2, 7], and in-sample learning [30, 15, 72].\nDistinguished from most previous works, this paper argues that, apart from the OOD action issue, there exists an OOD state issue that also impairs performance yet has received limited attention in the field. Such an issue refers to the agent encountering states out of the offline dataset during the policy deployment phase (i.e., test phase). The occurrence of OOD states can be attributed to OOD actions, stochastic environments, and real-world perturbations. Since typical offline RL algorithms do not involve policy training in OOD states, the agent tends to behave in an uncontrolled manner once entering OOD states in the test phase. This can further exacerbate the state deviation from the offline dataset and lead to severe degradation in performance [35, 76].\nIn mitigating this OOD state issue, existing limited work attempts to train the policy to correct the agent from OOD states to in-distribution (ID) states [76, 23]. Technically, Zhang et al. [76] construct a dynamics model and a state transition model and align them to guide the agent to ID regions, while"}, {"title": "2 Preliminaries", "content": "In reinforcement learning, we generally characterize the environment as a Markov Decision Pro-\ncess (MDP) M = (S, A, P, R, \u03b3, do), with state space S, action space A, transition dynamics\nP : S \u00d7 A \u2192 A(S), reward function R : S \u00d7 A \u2192 R, discount factor \u03b3\u2208 [0, 1), and initial state\ndistribution do [62]. The agent interacts with the environment and seeks a policy \u03c0 : S \u2192 \u0394(A) to\nmaximize the expected discounted return \u03b7(\u03c0):\n$\\eta(\\pi) = E_{s_0 \\sim d_0, a_t\\sim \\pi(\\cdot|s_t), s_{t+1}\\sim P(\\cdot|s_t, a_t)} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$"}, {"title": "3 OOD State Correction", "content": "The following focuses on the OOD state issue and OOD state correction in offline RL. In Section 3.1, we systematically analyze the OOD state issue, introduce the concept of OOD state correction, and point out limitations of prior methods. Then we present the proposed approach SCAS in Section 3.2."}, {"title": "3.1 OOD State Issue in Offline RL", "content": "In offline RL, OOD states refer to states not in the offline dataset. The OOD state issue (Definition 1) pertains to scenarios where the agent enters OOD states during the test phase, potentially resulting in catastrophic failure [35]. However, such a topic is rarely investigated in the literature, and existing studies lack deep insights. We mathematically formulate the OOD state issue as follows.\nDefinition 1 (OOD state issue). There exists s \u2208 S, such that $d_{M_T}^{\\pi}(s) > 0$ and dp(s) = 0, where MT is the MDP of the test environment, \u03c0 is any learned policy, $d_{M_T}^{\\pi}$ is the state probability density induced by \u03c0 in MT, and dp is the state probability density in the offline dataset.\nOrigins and consequence of OOD states. During the test phase, the OOD states occur primarily in three scenarios: (i) OOD actions: the learned policy, not perfectly constrained within the support of the behavior policy, executes unreliable OOD actions, leading to OOD states. (ii) Stochastic environment: the initial state of the actual environment may fall outside the offline dataset. In addition, stochastic dynamics can also lead to states outside the dataset, even when taking ID actions in ID states. (iii) Perturbations: commonly seen in real-world robot applications, some unexpected perturbations can propel the agent into OOD states (e.g., wind, human interference).\nDuring offline training, the typical Bellman updates involve only ID states, and the policies in OOD states are not trained. As a result, when encountering OOD states in the test phase, the agent would exhibit uncontrolled behavior, and the state deviation from the offline dataset can be further exacerbated over time steps, severely degrading performance [35].\nOOD state correction. To mitigate this OOD state issue, an intuitive solution is to train a policy capable of correcting the agent from OOD states to ID states, a concept known as OOD state correction [76]. Specifically, during offline training, we can perturb the original state s in the dataset into \u015d to generate substantial OOD states. Then consider the scenario where the agent starts from \u015d, follows the trained policy \u03c0, and transitions to the next state \u015d'. To reduce state deviation, \u015d' is expected to be close to the offline dataset. Thus we can align the distribution of \u015d' with an ID state distribution to regularize the policy and achieve OOD state correction.\nContinuing the above train of thought, SDC [76] generates the ID state distribution by feeding the original state s into a trained state transition model N(s'|s) of the dataset. This model characterizes"}, {"title": "3.2 Value-aware OOD State Correction", "content": "The objective of this work is to formulate a simple yet effective policy regularizer for offline RL that unifies OOD state correction and OOD action suppression. Moreover, we aim to achieve value-aware OOD state correction, involving the correction of the agent from OOD states to high-value ID states.\nValue-aware state transition. For the ID state distribution to which the agent is corrected, we expect a value-aware state transition distribution N*(\u00b7|s) that lies within the support of the dataset state transition distribution N(\u00b7|s) but is skewed toward high-value states s'. To ensure stability and, more importantly, to enable our subsequently designed algorithm to circumvent modeling complex distributions, we seek a soft optimal version of it. To this end, we consider the following problem\u00b2:\n$\\max_{\\mathcal{N}^*} E_{s \\sim D} \\alpha E_{s' \\sim \\mathcal{N}^*(s)} V(s') - D_{KL}(\\mathcal{N}^*(\\cdot|s) || \\mathcal{N}(\\cdot|s))$\nwhere \u03b1 is a hyperparameter to balance the two terms.\nThe optimization problem above has a closed-form solution:\n$\\mathcal{N}^*(s'|s) = \\frac{1}{Z(s)} exp(\\alpha V(s'))\\mathcal{N}(s'|s)$\nwhere Z(s) = \u2211s, exp (\u03b1V (s')) N(s'|s) is a normalization factor. It can be seen from Eq. (6) that supp(N*(\u00b7|s)) supp(N(\u00b7|s)). Note that \u03b1 is a key hyperparameter that controls the significance of the values of next states in SCAS's OOD state correction. As \u03b1 increases, N*(\u00b7|s) becomes more skewed toward the optimal s' in the support of N(\u00b7|s).\nOOD state correction. In order to produce substantial OOD states, we perturb each state s \u2208 D with Gaussian noise N (0, \u03c3\u00b2), resulting in perturbed state \u015d. It is worth noting that the dataset used for RL training remains unchanged. We perturb the states solely to formulate the regularizer.\nWe anticipate the following value-aware OOD state correction scenario, where the agent starts from OOD state \u015d, follows the trained policy \u03c0, and transitions to the high-value ID state s' in the distribution of N*(\u00b7|s). To this end, we train the policy \u03c0 to align the dynamics induced by \u03c0 on the perturbed states with the value-aware state transition distribution at the original state s via KL divergence. That is, we regularize \u03c0 by minimizing:\n$\\min_{\\pi} E_{s\\sim D} E_{\\hat{s} \\sim \\mathcal{N}_\\sigma(s)} D_{KL} (\\mathcal{N}^*(\\cdot|s) || M(\\cdot|\\hat{s}, \\pi(\\cdot|\\hat{s})))$"}, {"title": "4 Analysis of OOD Action Suppression", "content": "This section theoretically shows that the proposed regularizer also exhibits the effect of OOD action suppression. In other words, it can also prevent the policy from taking OOD actions, thereby simultaneously addressing the fundamental OOD action issue in offline RL. In offline RL, OOD actions are exclusively defined on ID states. This is because actor-critic training is limited to ID states, and any actions on OOD states would not affect training and cause the OOD action issue mentioned in Section 2. Consequently, for the analysis of OOD actions, it is essential to consider ID states. We define R, R\u2081 as the ID state version of R, R1, where \u015d = s. R and R\u2081 can be regarded as special cases of R and R\u2081, when \u015d sampled from N(s, o\u00b2) is equal to s:\n$R(\\pi) = E_{(s,s') \\sim D} [ \\frac{exp(\\alpha V(s'))}{Z(s)} \\cdot log \\mathcal{M}(s'|s, \\pi(\\cdot|s)) ]$\n$R_1(\\pi) = E_{(s,s') \\sim D} [ \\frac{exp(\\alpha V(s'))}{exp(\\alpha V(s))} \\cdot log \\mathcal{M}(s'|s, \\pi(\\cdot|s)) ]$"}, {"title": "5 Implementation Details", "content": "SCAS is easy to implement and we design the practical algorithm to be as simple as possible, retaining algorithmic simplicity and improv-ing computational efficiency.\nDynamics model. We employ a determinis-tic dynamics model Mw. The loss for training the model is\n$L_M(\\omega) = E_{(s,a,s') \\sim D} [ ||M_\\omega(s, a) - s'||^2 ]$\nPolicy improvement. With a deterministic model, we replace the log-likelihood in R\u2081 (\u03c0) with mean squared error. It is a common ap-proach in RL algorithms to convert a maxi-mum likelihood estimation problem into a re-gression problem when dealing with Gaussians with fixed variance [11]. As discussed in Section 3.2, we also adopt a deterministic policy model \u03c0\u03c6. Thus, we have the following policy regularizer:\n$R_2(\\pi_\\phi) = E_{(s,s')\\sim D}E_{\\hat{s} \\sim N_\\sigma(s)} [ \\frac{exp(\\alpha V_\\theta(s'))}{exp(\\alpha V_\\theta(s))} \\cdot ||M_\\omega(\\hat{s}, \\pi_\\phi(\\hat{s})) - s'||^2 ]$\nwhere $V_\\theta(s) = Q_\\theta(s, \\pi_\\phi(s))$ and \u4e93\u03c6 means \u03c0\u03c6 with detached gradients. Using deterministic policy also simplifies the training process without learning a V-function. Combining R2(\u03c0\u03c6) with the standard policy improvement objective, we update the policy by maximizing:\n$J_\\pi(\\phi) = (1 - \\lambda)E_{s\\sim D} [Q_\\theta(s, \\pi_\\phi(s))] + \\lambda R_2(\\pi_\\phi)$"}, {"title": "6 Experiments", "content": "In this section, we conduct several experiments to examine the performance and properties of SCAS. Please refer to Appendices D and E for experimental details and additional results."}, {"title": "6.1 Empirical Evidence of OOD State Correction and OOD Action Suppression", "content": "OOD state correction. To examine the OOD state correction ability, we compare the state distri-butions generated by the learned policies of different algorithms with the state distribution of the offline dataset. In detail, we first train SCAS, CQL [32], and TD3+BC [11], and then collect 50,000 samples by running the trained policies separately. We also sample 50,000 states randomly from the offline dataset for comparison. Figures 1(a) to 1(c) plot the state distributions in halfcheetah-medium-expert [10] with t-SNE [63], and Figure 1(d) visualizes the optimal value of each state. We access these values from the learned value function obtained by running TD3 [12] online to convergence.\nIn Figures 1(a) and 1(b), we observe that the policies learned by CQL and TD3+BC tend to produce many OOD states. As depicted in Figure 1(d), these OOD states have extremely low values, so entering them can be detrimental to performance. In contrast, the state distribution induced by SCAS is almost entirely within the support of the offline distribution, demonstrating the OOD state correction ability of SCAS. Moreover, we also note that in the low-value area of the offline state distribution (the grey circle in Figure 1(d)), SCAS exhibits a very low state density, which could be attributed to SCAS's value-aware OOD state correction. We refer the reader to Appendix E.2 for additional experiments validating the OOD state correction effects.\nOOD action suppression. We empir-\nically evaluate the OOD action sup-\npression effects through the lens of\nvalue estimates. We compare SCAS\nwith three baselines: (1) ordinary off-\npolicy RL which is SCAS with \u03bb\n= 0 (all other implementations are the\nsame); (2) SDC [76] without additional\nCQL [32] term to suppress OOD ac-\ntions; (3) OSR [23] without additional\nCQL term. We conduct experiments\non D4RL datasets [10]. Since value\nover-estimation (divergence) is the main\nconsequence and evidence of OOD ac-\ntions [13], we plot the learned Q-values\nof SCAS and the baselines in Figure 2.\nWe also include the oracle Q-values of SCAS by rollouting the trained policy for 1, 000 episodes and evaluating the Monte-Carlo return. Additional results are provided in Appendix E.1.\nThe results show that the learned Q-values of ordinary off-policy RL, SDC without CQL, and OSR without CQL diverge at early learning stages, suggesting that the algorithms suffer from severe OOD actions. By contrast, the learned Q-values of SCAS stay close to the oracle Q-values. This indicates that SCAS regularization alone is able to suppress OOD actions."}, {"title": "6.2 Comparisons on Offline RL Benchmarks", "content": "Tasks. We evaluate SCAS on D4RL [10] and NeoRL [50] benchmarks. In D4RL, we conduct experiments on Gym locomotion tasks and much more challenging AntMaze tasks. Due to the space limit, the results on NeoRL are deferred to Table 4 in Appendix E.3."}, {"title": "6.3 Comparisons in Perturbed Environments", "content": "In this section, we evaluate the algorithms in a more real-world setting where the agent receives uncertain perturbations during test time. OOD state correction is even more critical in such scenarios since the agent can enter ODD states after perturbation. To simulate this scenario, we add varying steps of Gaussian noise with a magnitude of 0.5 to the actions conducted by the policy during test time. Specifically, the policy is trained on standard D4RL datasets but is tested in the perturbed environments. We control the strength of perturbations by adjusting the number of perturbation steps."}, {"title": "6.4 Parameter Study", "content": "We examine the effects of the inverse temperature \u03b1, the balance coefficient \u03bb, and the noise scale \u03c3. Due to the space limit, the results for \u03c3 and on additional datasets are deferred to Appendix E.6. A sensitivity analysis on dynamics model errors is provided in Appendix E.7.\nInverse temperature \u03b1. \u03b1 is the key hyperparameter in SCAS for achieving value-aware OOD state correction. If \u03b1 = 0, the effect degenerates to vanilla OOD state correction. Figure 4(a) displays the learning curves of SCAS with different \u03b1. The results show that a large \u03b1 is crucial for achieving good performance (also verified on more tasks in Figure 6), clearly demonstrating the effectiveness of our value-aware OOD state correction. However, too large \u03b1 (\u03b1 = 10) induces less satisfying performance, probably due to the increased variance of the learning objective.\nBalance coefficient \u03bb. A in Eq. (20) controls the balance between vanilla policy improvement and SCAS regularization. We vary X within the range [0, 1] and present the learning curves of SCAS in Figure 4(b). Notably, SCAS is able to converge to good performance over a very wide range of A (also verified on more tasks in Figure 7). An interesting finding is that even when X = 1 and the signal from RL improvement (max Q) is removed, SCAS still performs well on most tasks. This could be attributed to the fact that value-aware OOD state correction implies some sort of improvement in policy by maximizing the values of policy-induced next states."}, {"title": "7 Conclusion and Limitations", "content": "In this paper, we systematically analyze the OOD state issue in offline RL and propose SCAS, a simple yet effective approach that unifies OOD state correction and OOD action suppression. SCAS also achieves value-aware OOD state correction, significantly improving performance over vanilla"}, {"title": "A Related Work", "content": "Model-free offline RL. In offline RL, extrapolation error and overestimation caused by OOD ac-tions pose significant challenges. Among model-free solutions, value regularization methods penalize the Q-values of OOD actions [32, 2, 29, 37, 3, 73, 41], while policy constraint approaches compel the trained policy to be close to the behavior policy, either explicitly via divergence penalties [70, 31, 11], implicitly by weighted behavior cloning [48, 46, 68, 40], or directly through specific parameterization of the policy [13, 16]. Relatively independently, in-sample learning methods formulate the Bellman target using only the actions in the dataset to avoid OOD actions [5, 30, 77, 72]. Recently, some works aim to learn the optimal policy within the support of the dataset (known as in-support or in-sample optimal policy) in a theoretically sound way and are less affected by the average quality of the dataset [40, 41, 69]. However, existing popular offline RL approaches primarily focus on the OOD action issue during training and often neglect the OOD state issue during the test phase.\nModel-based offline RL. Model-based RL methods learn a model of the environment and generate synthetic data from that model to optimize the policy [61, 21, 25]. To ensure conservatism in offline RL, Kidambi et al. [26] and Yu et al. [74] estimate the uncertainty in the model and apply reward penalties for state-action pairs with high uncertainty. Some model-based approaches also introduce conservatism similarly to model-free ones, employing techniques like value regularization [75] and policy constraint [42]. Recently, Sun et al. [60] conducts uncertainty quantification through the inconsistency of Bellman estimations under the learned dynamics ensemble. However, model-based methods often come with a high computational burden [21], and their effectiveness relies heavily on the quality of the trained model [44]. In contrast, our algorithm leverages the dynamics model to propagate policy gradients, make one-step predictions, and regularize policy training, leading to significantly improved computational efficiency and relatively high prediction accuracy.\nOOD state correction. In offline RL, OOD state correction deserves more attention as the state deviation during the test phase can accumulate over time steps, severely degrading performance [35]. Existing limited solutions aim to train the policy to correct the agent from OOD states to ID states [76, 23]. Specifically, SDC [76] builds a dynamics model and a state transition model, and aligns the policy-induced next state distributions at OOD states with the state transition model. On the other hand, OSR [23] utilizes an inverse dynamics model to constrain the policy at OOD states. Compared with prior methods, our proposed SCAS efficiently unifies OOD state correction and OOD action suppression in offline RL and additionally achieves value-aware OOD state correction. Although the DICE series of works [45, 34, 39, 6] share similar motivations with SCAS, there are significant differences between the two. Firstly, DICE is based on a linear programming framework of RL, while SCAS is based on a dynamic programming framework. Therefore, the theoretical foundations and learning paradigms of the two are inherently different. Secondly, SCAS only corrects encountered OOD states, whereas DICE algorithms require the policy-induced occupancy distribution to align with the dataset distribution. Therefore, DICE's constraints are stricter, potentially making it more susceptible to the average quality of the dataset. Lastly, theoretical and empirical evidence indicate that DICE algorithms have a problem of gradient cancellation [39], which imposes certain limitations on their practical effectiveness."}, {"title": "B Proofs", "content": "In this section, we present the proofs for the theories in the paper."}, {"title": "B.1 Derivation of the Value-aware State Transition Distribution", "content": "We show that Eq. (6) is the optimal solution of the optimization problem mentioned in Section 3.2:\n$\\max_{\\mathcal{N}^*} E_{s \\sim D} \\alpha E_{s' \\sim \\mathcal{N}^*(s)} V(s') - D_{KL}(\\mathcal{N}^*(\\cdot|s) || \\mathcal{N}(\\cdot|s))$"}, {"title": "B.2 Proof of Proposition 1", "content": "Proposition 3 (Proposition 1 in the paper). Suppose that the environment dynamics is deterministic, then both R(\u03c0) and R\u2081(\u03c0) achieve their global maximum at the policy \u03c0*, where\u2074\n$R(\\pi) := E_{(s,s') \\sim D} [ \\frac{exp(\\alpha V(s'))}{Z(s)} \\cdot log \\mathcal{M}(s'|s, \\pi(\\cdot|s)) ]$\n$R_1(\\pi) := E_{(s,s') \\sim D} [ \\frac{exp(\\alpha V(s'))}{exp(\\alpha V(s))} \\cdot log \\mathcal{M}(s'|s, \\pi(\\cdot|s)) ]$\n$\\pi^*(a|s) = \\frac{1}{Z(s)} exp(\\alpha V(\\mathcal{M}(s, a))) \\beta(a|s), \\forall s \\sim D$\nThe support of \u03c0* is within that of the behavior policy \u03b2:\n$supp(\\pi^*(\\cdot|s)) \\subseteq supp(\\beta(\\cdot|s)), \\forall s \\sim D$\nand \u03c0* makes the following equation hold:\n$\\mathcal{N}^*(\\cdot|s) = \\mathcal{M}(\\cdot|s, \\pi^*(\\cdot|s)), \\forall s \\sim D$"}, {"title": "B.3 Proof of Proposition 2", "content": "Proposition 4 (Proposition 2 in the paper). When the dynamics is stochastic, the maximum points of both R(\u03c0) and R\u2081(\u03c0) are constrained within the support of the behavior policy:\n$supp(\\pi^*(\\cdot|s)) \\subseteq supp(\\beta(\\cdot|s)), \\forall s \\sim D$\n$supp(\\pi_1(\\cdot|s)) \\subseteq supp(\\beta(\\cdot|s)), \\forall s \\sim D$\nProof. We start with R(\u03c0).\n$R(\\pi) : = E_{(s,s') \\sim D} [ \\frac{1}{Z(s)} exp(\\alpha V(s')) log \\mathcal{M}(s'|s, \\pi(\\cdot|s)) ]$\n$= E_{(s,s')\\sim D} [ \\frac{1}{Z(s)} exp(\\alpha V(s')) log (\\sum_a \\mathcal{M}(s'|s, a) \\pi(a|s)) ]$\nLet \u03c0 denote any valid policy. For \u2200s \u2208 D, define e(s) and n(s) as follows:\n$e(s) := \\sum_a I[\\beta(a|s) = 0]\\pi(a|s)$\n$n(s) := \\sum_a I[\\beta(a|s) > 0]$"}, {"title": "C Further Discussions", "content": "C.1 Rationale for Choosing exp(\u03b1V (s)) as the Empirical Normalizer\nFirstly, choosing exp(\u03b1V (s)) is intended to obtain something similar to the advantage function. With this normalizer, the weight of our regularizer is exp(\u03b1(V(s') \u2212 V(s))), which is comparable to the weight exp(\u03b1A(s, a)) in Advantage Weighted Regression (AWR) [48]. Here, V (s') \u2212 V(s) represents the relative advantage of the next state s' compared to the current state s, while A(s, a) reflects the relative advantage of taking action a in s compared to following the current policy. Comparison of the objectives of SCAS and AWR:\nSCAS: exp(\u03b1(V(s') \u2212 V(s))) log(M(s'|\u015d, \u03c0(\u015d)))\nAWR: exp(\u03b1A(s, a)) log \u03c0(als)\nSecondly, as discussed in the paper, introducing any normalizer that depends only on s (independent of s') does not affect the development and analysis of our method; it is merely for computational stability. In AWR-based methods, there also exists a normalizer Z(s) and they usually disregard it [48, 46]. The rationale behind this is similar.\nC.2 Pessimism and Robustness in SCAS\nIn a specific sense, SCAS, which unifies OOD state correction and OOD action suppression, also integrates pessimism and state robustness. (1) Regarding pessimism: The OOD action suppression effect of SCAS aligns with the pessimism commonly discussed in offline RL work (being pessimistic about OOD actions) [32, 71, 31, 3, 78, 55]. Unlike traditional policy constraint methods [70, 31, 11, 48], our approach does not require the training policy to align with the behavior policy; it only requires the successor states to be within the dataset support, which is a more relaxed constraint. (2) Regarding state robustness: The OOD state correction effect of SCAS is aimed at improving the"}, {"title": "D Experimental Details", "content": "All hyperparameters of SCAS are included in Table 2. Note that we use this same set of hyper-parameters to obtain all the results reported in this paper (except for parameter study). Following TD3+BC [11], we normalize the states in all datasets except for antmaze-large. We clip the expo-nentiated weight exp (aVo (s') \u2013 aVo (s)) in Eq. (19) to (-\u221e, 50]. Following the suggestions in the benchmark [10], we subtract 1 from the rewards for the Antmaze datasets."}, {"title": "E Additional Experimental Results", "content": "E.1 Additional Value Estimation Results\nUnder the same setting of Figure 2, we conduct experiments on the additional datasets. The results are shown in Figure 5. We omit the Q values of Off-policy RL, SDC w/o CQL, and OSR w/o CQL at higher numbers of optimization steps, because these Q values diverge in the early learning stage, and plotting their Q values at later optimization steps would result in an excessive range on the vertical axis. The additional results also show that only SCAS's OOD state correction term can achieve OOD action suppression and prevent value over-estimation.\nE.2 Additional Results on OOD State Correction\nTo further examine the OOD state correction effects of SCAS, we conduct experiments on a modified D4RL maze2d-open-v0 [10]. It is a 2D point robot navigation task in a rectangle map with vertices"}, {"title": "E.7 Sensitivity Analysis on Dynamics Model Errors", "content": "To empirically investigate SCAS under different dynamics model errors, we run SCAS using different checkpoints of the trained dynamics model, which are obtained at different steps in the dynamics model training process. The model error is controlled by the number of trained steps. The results are shown in Figure 9. The figure plots the training loss of the dynamics model Mw and the corresponding normalized return of SCAS over 5 random seeds. We observe that the performance of SCAS increases with the number of trained steps of the dynamics model (i.e. the accuracy of the model) and stabilizes at a high level.\nE.8 Learning Curves of SCAS\nLearning curves on Gym locomotion tasks and Antmaze tasks are presented in Figure 10 and Figure 11 respectively. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds."}, {"title": "F Broader Impact", "content": "Offline RL holds promise for facilitating practical RL applications in domains like robotics, healthcare, and education, where data collection is often costly or risky. However, it is important to recognize its potential negative societal impacts. One concern is that biases in offline data may transfer to the learned policy. In addition, offline RL may affect employment by automating tasks traditionally"}]}