{"title": "DCOR: Anomaly Detection in Attributed Networks via Dual Contrastive Learning Reconstruction", "authors": ["Hossein Rafiee Zade", "Hadi Zare", "Mohsen Ghassemi Parsa", "Hadi Davardoust", "Meshkat Shariat Bagheri"], "abstract": "Anomaly detection using a network-based approach is one of the most efficient ways to identify abnormal events such as fraud, security breaches, and system faults in a variety of applied domains. While most of the earlier works address the complex nature of graph-structured data and predefined anomalies, the impact of data attributes and emerging anomalies are often neglected. This paper introduces DCOR, a novel approach on attributed networks that integrates reconstruction-based anomaly detection with Contrastive Learning. Utilizing a Graph Neural Network (GNN) framework, DCOR contrasts the reconstructed adjacency and feature matrices from both the original and augmented graphs to detect subtle anomalies. We employed comprehensive experimental studies on benchmark datasets through standard evaluation measures. The results show that DCOR significantly outperforms state-of-the-art methods. Obtained results demonstrate the efficacy of proposed approach in attributed networks with the potential of uncovering new patterns of anomalies.", "sections": [{"title": "1 Introduction", "content": "Anomaly detection in attributed networks plays a critical role in modern networked systems due to their increasing complexity and interconnectivity [1]. Anomalies often occur in applied domains such as financial systems, computer networks, and the internet known as fraud, security breaches, or system faults with billions of costs for companies [2].\nTraditional anomaly detection methods often have problems when facing with attributed graph-structured data. Techniques such as Local Outlier Factor (LOF) [3] and Structural Clustering Algorithm for Networks (SCAN) [4] primarily rely on local density and structural deviations. However, these approaches suffered to deal with node attributes and connectivity structure of the network simultaneously."}, {"title": "2 Problem Definition", "content": "Formally, let G = (A, X) represent an attributed network, where A \u2208 Rn\u00d7n is the adjacency matrix and X \u2208 Rnxd is the attribute matrix. Each node vi in the graph is represented by a row in both A and X. The adjacency matrix A captures the structural relationships between nodes, with Aij = 1 if there is an edge between nodes vi and vj, and Aij = 0 otherwise. The attribute matrix X"}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Autoencoder and GNN-Based Anomaly Detection", "content": "Some works combine autoencoders with GNNs for better anomaly detection because they capture complex dependencies in graph data. For instance, Zhou and Liu proposed AnomalyDAE [5], which is a dual autoencoder architecture to simultaneously reconstruct the adjacency matrix and node features for anomaly detection. Some analogical approaches, such as Dominant [6], based on GNNs, try to learn node embeddings from a GNN to acquire good performance in nodewise anomaly detection. A recent and likely the most relevant one is by Liu et al. [9], presenting contrastive learning on attributed graphs to improve anomaly detection. The focus of such an approach lies in generating effective negative samples to distinguish normal patterns from anomalous ones, thereby achieving enhanced robustness of the model. Such approaches underline the strength of autoencoders and GNNs in capturing structural nuances and dependencies in graph-based tasks of anomaly detection."}, {"title": "3.2 Graph Contrastive Learning and Self-Supervised Learning", "content": "Some of the latest techniques in this line, which have considerably advanced the robustness and generalization of GNNs, are graph contrastive learning and self-supervised learning with data augmentation.\nGraph Contrastive Learning: GraphCL [7] introduces a way to improve the GCNs by providing different views of graphs with respect to data augmentations, like node dropping and edge perturbations. It works in a way in which the augmented views between the same graph are brought to maximum agreement, but minimized with those of different graphs.\nSelf-Supervised Learning with Data Augmentation: In [10], robust embeddings of anomaly detection are generated by adding node and graph augmentations. Curriculum negative sampling is introduced to adjust the difficulty"}, {"title": "3.3 Recent Advances in Graph Anomaly Detection", "content": "A dynamic graph anomaly detection using GNNs was proposed in [15], followed by the introduction of the first contrastive learning method for detecting anomalies in temporal networks. A domain-specific contrastive learning technique that is agnostic to the graph was proposed in [16]. They proposed a hybrid model that combined graph neural networks with reinforcement learning for anomaly detection within dynamic graphs."}, {"title": "4 Proposed Method", "content": "Our method starts by generating both the original network and an augmented version. We then focus on comparing the reconstructed adjacency and feature matrices from the original network with those from the augmented one.\nWhat sets our approach apart from traditional contrastive learning frameworks [17], which usually compare embeddings from original and augmented networks, is our focus on reconstructing the actual graph data. By shifting attention to reconstruction, we can uncover more intricate anomalies and subtle differences in the graph's structure and features-details that might be missed if we were only looking at the embeddings. Our main goal is to refine the quality of these reconstructions within the autoencoder, allowing the model to more effectively distinguish between normal and anomalous data by emphasizing the key differences.\nInitially, we use an autoencoder to help us in the reconstruction of the adjacency and feature matrices of the graph. These reconstructions are the basis for carrying out anomaly detection. Now begins the process of contrastive learning. At this stage, the model is going to compare the reconstructed adjacency and feature matrices of the original and augmented graphs. The aim is to augment the accuracy of anomaly detection through this contrast. Such an approach enables the model to detect slight differences in the structure and features of normal versus anomalous graphs, thus making anomaly detection much more accurate.\nDuring reconstruction, anomalies found in the augmented graph lead the model to try deviating from their reconstruction of the original network. At the same time, the model will also try keeping its reconstruction for the nodes that are anomalously far away from theirs. For instance, if an augmented network node is infected, it tries to predict the reconstruction of that node back toward its corresponding node in the original network. Such contrasts are made both in"}, {"title": "4.1 Graph Data Augmentation", "content": "Anomaly detection graph augmentation refers to the incorporation of artificially generated anomalies into graph data to enhance the effectiveness of detection algorithms [9]. In order to enable effective testing and verification, we incorporate unconventional nodes or edges into the network, hence producing concrete abnormalities. Both structural and feature anomalies are included, with augmentation rates optimally adjusted to the unique characteristics of each dataset. Upon encountering an anomaly, a node is allocated the label of anomalous (anomaly label), which facilitates straightforward discrimination during testing. By incorporating tailored anomaly creation, the model's robustness and capacity to generalize to real-world scenarios are improved.\nGraph augmentation can be expressed as:\nG' = G + A\n(1)\nwhere G' is the augmented graph, G is the original graph, and A the anomalies introduced into the graph.\nNode-Level Augmentation We perturb node features by adding noise to simulate anomalies and increase data diversity. Perturbation improves the model's ability to generalize. Techniques include:\nFeature Copying: Replicating the feature vector of a node with maximum deviation to a target node creates attribute anomalies [18], useful in detecting subtle deviations.\nFeature Scaling: Adjusting feature values by a scaling factor creates anomalies in feature distribution, enhancing detection.\nStructural Augmentation Structural augmentation alters the graph by adding or removing edges to simulate unusual connectivity patterns. This helps the model generalize and detect structural anomalies. Techniques include:\nAdding Subgraph: Introducing dense subgraphs around nodes creates structural anomalies, simulating unusual connectivity patterns.\nDrop all connections: Disconnecting a node isolates it, simulating scenarios like faults or network issues, and test the model's ability to detect isolated nodes."}, {"title": "4.2 Anomaly Detection Model and Contrastive Learning", "content": "Traditional contrastive learning frameworks [17] focus on comparing embeddings from different augmented views of the same data to maximize agreement between similar pairs and minimize agreement between dissimilar pairs. Our approach builds upon this concept by first reconstructing the graph data using a dual autoencoder model. Since typical representations may lose some sensitive information about the nodes, which can hinder accurate contrasts, we compare the reconstructed adjacency matrices and feature matrices directly. By feeding both the main graph and the augmented graph into the dual autoencoder, we enhance the detection of anomalies and ensure more precise results."}, {"title": "Dual Autoencoder Model", "content": "The dual autoencoder model reconstructs the graph's adjacency and feature matrices using both structure and attribute autoencoders. The main and augmented graphs are fed into the model to obtain reconstructed matrices."}, {"title": "Structure Autoencoder", "content": "The structure autoencoder encodes the graph structure into a latent representation and reconstructs the adjacency matrix.\nEncoder: Compresses the input graph (A, X) into a latent representation\n\u017dv = \u03c3(XW(1)) +6(1))\n(2)\nwhere X is the feature matrix, W(1) and 6(1) are weight and bias parameters, and \u03c3 is the activation function.\nThe attention scores (eij) between nodes i and j are computed as follows:\nCij = attn(\u017di, Zj) = \u03c3(aT . [W(2) \u017di || W(2) Zj ])\n(3)\nwhere \u017di and Zj are the latent representations of nodes i and j, W(2) is the weight matrix, a is the attention weight vector, and [||] represents concatenation of the vectors.\nA graph attention layer then aggregates the representation from neighboring nodes:\n\u03b3ij =\nexp(eij)\n\u03a3k\u2208Ni exp(eik)\n(4)\nwhere \u03b3ij is the attention coefficient for nodes i and j, computed based on the attention scores eij.\nThe final embedding for node i, denoted as \u017dvi, is computed as:\n\u017dvi = \u2211 \u03b3ik\nk\u2208Ni\n(5)"}, {"title": "", "content": "where Ni represents the neighbors of node i.\nDecoder: Reconstructs the adjacency matrix \u00c2 from Zy:\nA = Sigmoid(ZvZT)\n(6)\nwhere Zy is the latent node representation and \u00c2 is the reconstructed adjacency matrix."}, {"title": "Attribute Autoencoder", "content": "The attribute autoencoder encodes node attributes into latent space and reconstructs the attribute matrix.\nEncoder: Transforms node attributes X into a latent representation ZA:\n\u017dA = \u03c3(XTW(1)) +6(1))\nA\nA\n(7)\nwhere W(1) and 6(1) are weight and bias parameters for attribute encoding.\nA\nZA = ZAW(2) +6(2))\n(8)\nwhere ZA is the final latent representation of the node attributes.\nDecoder: Reconstructs the attribute matrix X from Zy and ZA:\nX = Zv(ZA)T\n(9)\nwhere X is the reconstructed attribute matrix."}, {"title": "Loss Function", "content": "The objective is to minimize the reconstruction errors of both structure and attributes:\nLrec = \u03b1||A - \u00c2||2F + (1 \u2212 \u03b1)||X \u2013 X||2F\n(10)\nwhere \u03b1 controls the trade-off between structure A and attribute X reconstruction, and || ||F represents the Frobenius norm.\nNodes with higher reconstruction errors, indicated by the above loss function, are more likely to be anomalies."}, {"title": "Contrastive Loss Function", "content": "After obtaining the reconstructed adjacency and feature matrices from both the main and augmented graphs, the contrastive loss, which consists of both structural contrastive loss and feature contrastive loss, is applied to these reconstructed matrices. The margin parameter m used in Equations (12) and (13) is set to 0.5. To further enhance the model's ability to distinguish between the original and augmented graphs, contrastive learning is incorporated. This allows the network to effectively differentiate between the reconstructions of the main graph and those of the augmented graph containing labeled anomalies.\nThe contrastive loss is a combination of structural contrastive loss and feature contrastive loss:\nLSC = Lstruct + Lfeat\n(11)"}, {"title": "", "content": "where:\nLstruct =\n1\n2n\nn\n\u2211(Iy=0 \u00b7 d(Ai, \u00c2i) + Iyi=1 \u00b7 max{0, m \u2013 d(Ai, \u00c2i)})\n(12)\ni=1\nLfeat =\n1\n2n\nn\n\u2211(Iy=0 \u00b7 d(Xi, Xi) + Iyi=1 \u00b7 max{0, m \u2013 d(Xi, X\u00ec)})\n(13)\ni=1\nWhere, d(\u00b7,\u00b7) represents the distance between matrices, Iy i=0 and Iyi=1 are indicator functions for normal and anomalous instances, and m is a margin parameter.\nThe total contrastive loss Lsc includes: - Positive Pair Loss: Minimizes the distance between the reconstructed matrices of the main and augmented graphs. - Negative Pair Loss: Ensures the distance between reconstructed matrices of different graphs exceeds the margin m.\nAs part of the contrastive learning process, the model evaluates the reconstruction of anomalous nodes in the augmented network during the reconstruction of the main network. The goal is to push the reconstruction of anomalous nodes further from their counterparts in the main network, while pulling the reconstruction of normal nodes closer to their corresponding nodes in the main network. Through these contrasts, the model improves its ability to reconstruct the graph and enhances its anomaly detection capabilities. Additionally, by leveraging the augmented network reconstructions, the model generates a more accurate reconstruction of the main network.\nTotal Loss The total loss is a weighted sum of the reconstruction loss and the contrastive loss. This combined loss function ensures that the model not only accurately reconstructs the graph but also effectively distinguishes between normal and anomalous graphs.\nLtotal = \u03bbrecLrec + \u03bbscLSC\n(14)\nwhere \u03bbrec and \u03bbsc determine the balance between reconstruction and contrastive losses.\nThe methodology can be summarized as follows: 1. Graph Augmentation: Create augmented versions of the graph using feature and structural augmentations. 2. Reconstruction: Apply the dual autoencoder to both the main and augmented graphs to reconstruct adjacency and feature matrices. 3. Contrastive Learning: Use contrastive loss to compare the reconstructed matrices, improving anomaly detection by highlighting differences between the main and augmented graphs."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Datasets", "content": "We evaluate DCOR using four real-world datasets Flickr [19], Amazon [20], Enron [21], and Facebook [22] from diverse domains such as social networks, e-commerce, and email communications.\nTable 1 provides key statistics for these datasets."}, {"title": "5.2 Experimental Settings", "content": "We evaluate the method on four real-world datasets with consistent hyperparameters across all:\nAdam optimizer - Embedding and hidden dimensions set to 128 - Learning rate of le-2 (except for Facebook) - One hidden layer in the encoder\nDataset-specific hyperparameters are detailed in Table 2.\nWe implemented DCOR using PyTorch and ran the experiments on Google Colab with an NVIDIA T4 GPU and 15GB of memory.."}, {"title": "5.3 Experimental Results", "content": "We compare DCOR with several existing methods including LOF [3], Dominant [6], AEGIS [23], AnomalyDAE [5], and Conad [17]. The AUC scores for anomaly detection are shown in Table 3."}, {"title": "6 Conclusion", "content": "In this paper, we proposed DCOR a dual contrastive learning based approach for anomaly detection in attributed networks. By contrasting the reconstructed adjacency and feature matrices in both original and augmented graphs, DCOR successfully identified the low-level irregularities likely to be neglected by prior methods. This technique improved the quality of reconstructions in the autoencoder, thereby enabling the model to capture important anomalies. Extensive experiments conducted on a number of public datasets including Flickr, Amazon, Enron, and Facebook revealed that DCOR outperformed the current stateof-the-art algorithms with higher AUC scores. These results demonstrated the strength of the integrated proposed approach using contrastive learning with reconstruction-based methods for detecting anomalies in attributed networks."}]}