{"title": "Towards understanding how attention mechanism works in deep learning", "authors": ["Tianyu Ruan", "Shihua Zhang"], "abstract": "Attention mechanism has been extensively integrated within mainstream neural network architectures, such as Transformers and graph attention networks. Yet, its underlying working principles remain somewhat elusive. What is its essence? Are there any connections between it and traditional machine learning algorithms? In this study, we inspect the process of computing similarity using classic metrics and vector space properties in manifold learning, clustering, and supervised learning. We identify the key characteristics of similarity computation and information propagation in these methods and demonstrate that the self-attention mechanism in deep learning adheres to the same principles but operates more flexibly and adaptively. We decompose the self-attention mechanism into a learnable pseudo-metric function and an information propagation process based on similarity computation. We prove that the self-attention mechanism converges to a drift-diffusion process through continuous modeling provided the pseudo-metric is a transformation of a metric and certain reasonable assumptions hold. This equation could be transformed into a heat equation under a new metric. In addition, we give a first-order analysis of attention mechanism with a general pseudo-metric function. This study aids in understanding the effects and principle of attention mechanism through physical intuition. Finally, we propose a modified attention mechanism called metric-attention by leveraging the concept of metric learning to facilitate the ability to learn desired metrics more effectively. Experimental results demonstrate that it outperforms self-attention regarding training efficiency, accuracy, and robustness.", "sections": [{"title": "1 Introduction", "content": "The attention or self-attention mechanism is extensively applied in popular deep learning architectures like Transformers (Vaswani et al., 2017; Dong et al., 2018) and graph attention networks (Veli\u010dkovi\u0107 et al., 2017). This mechanism enables the model to assign diverse weights to various parts (data points, nodes, etc.) of the input sequence (data, graph, etc.) based on their relevance when producing an output. This capability is particularly critical for handling inputs where the lengths and relevance strengths of different parts can vary significantly. As a result, this mechanism contributes to the broad applications of deep learning in various fields, including natural language processing (Devlin et al., 2018; Brown et al., 2020; Radford et al., 2019), computer vision (Dosovitskiy et al., 2020; Touvron et al., 2021; Carion et al., 2020), graph mining (Liu et al., 2023) and bioinformatics (Dong and Zhang, 2022; Zhang et al., 2023; Ji et al., 2021).\nHowever, understanding the mathematical principle of attention mechanism is still challenging due to its interaction with normalization layers and feed-forward networks in neural network architectures. Difficulties in understanding attention mechanism also stem from the numerous parameters in neural networks and various engineering techniques. To our knowledge, only a few studies have explored it in depth (Vuckovic et al., 2020; Dong et al., 2021; Sander et al., 2022; Geshkovski et al., 2023). Sander et al. (2022) formalized the self-attention mechanism with residual connections as a flow map and analyzed it from the perspective of the Wasserstein gradient flow. In addition, they characterized the L2 self-attention (Kim et al., 2021) using continuous dynamical systems. Geshkovski et al. (2023) investigated attention mechanism in Transformers by assuming that data is distributed on the unit sphere and making simplified assumptions about parameters and proved that the distribution would converge to a single point under certain conditions, suggesting that attention mechanism induces an aggregation tendency. However, they did not fully explain how attention mechanism works or its connection to classical algorithms.\nMoreover, several architectures, such as CRATE (Yu et al., 2024) and Probabilistic Transformer (Wu and Tu, 2023), have been proposed. They often originate from interpretable models and have information propagation mechanisms similar to attention mechanism. While not strictly equivalent, they offer valuable insights into understanding attention mechanism. For example, CRATE suggests that attention mechanism functions as a compression process, whereas the Probabilistic Transformer explains it as an explicit iteration of the Frank-Wolfe optimization algorithm.\nWe illustrate three architecture components, i.e., the residual block, the attention block, and their recombination of a Transformer block (Figure 1). The attention block consists of an information propagation process followed by a linear transformation. Some studies modeled the residual block using an ordinary differential equation (E, 2017; Chen et al., 2018). Some researchers modeled the recombination of the residual and attention blocks using the flow map mentioned above. In this paper, we focus on the attention-based information propagation mechanism. This mechanism can be viewed as a message-passing process akin to that in graph neural networks (GNNs) operating on fully connected graphs. However, GNNs typically employ fixed edge weights and topologies, which limit theoretical analysis to diffusion processes on graphs (Li et al., 2024). In contrast, attention mechanism, as a learnable method of information propagation on fully connected graphs, has yet to be thoroughly analyzed in terms of its limit behavior.\nMany machine learning methods such as manifold learning (e.g., diffusion map (Coifman and Lafon, 2006), UMAP (McInnes et al., 2018)), clustering methods (e.g., k-means clustering (Lloyd, 1982), fuzzy c-means clustering (Bezdek et al., 1984), Markov clustering (Van Dongen, 2008)) and supervised learning (e.g., k-nearest neighbors algorithm (Fix,"}, {"title": "2 From similarity computation to attention mechanism", "content": "gation. Numerical experiments demonstrate its superior performance to the self-attention mechanism on various examples. Finally, we conclude this work and discuss its implications."}, {"title": "2.1 Similarity computation", "content": "Similarity computation is a set of engineering practices to generate similarity measures between data points (or graph nodes). Different methods for similarity computation have been developed. Most of them involve one, two, or three of the following three components: initializing similarity, strengthening similarity, and normalizing similarity."}, {"title": "2.1.1 INITIALIZING SIMILARITY", "content": "Initializing similarity is the first step for further computations and information extraction. The point-to-point similarity is typically generated using a binary function D(, ), where D is often a simple transformation based on metrics, inner products, or topological structures. A natural idea is that the closer two data points are, the higher their similarity. Therefore, the distance between two data points is often transformed into a similarity measure through monotonically decreasing mappings. Building on previous work, we have derived the following metric-based function Dt,c:\nDefinition 1 (Metric-based similarity generation). Given a metric space (M,d), we define the similarity function on M as:\n$D_{c,t}(x, y) = c(x) - \\text{sign}(t)d(x, y)$\nwhere c is a specified function and t is a hyperparameter.\nBilinear functions are also commonly used to initialize similarity.\nDefinition 2 (QK-dot product). Given m \u00d7 n matrices Q, K and vectors x, y \u2208 Rn, where x and y are column vectors, we define QK-dot product of x and y as follows:\n$D(x,y) = x^TQK y = Qx^T \\cdot Ky = x^T(Q^TK)y$\nBefore the inner product mapping, people may apply a non-linear transformation to the original features, which is equivalent to defining similarity using a certain kernel function K(...).\nAdditionally, similarity may be defined using a local combination, which reflects how a data point or node is represented by a combination of its neighbors.\nDefinition 3 (Local combination similarity). Given a set of data points {$v_i$} \u2282 Rn and their adjacency relationships, we calculate $w_{ij}$ to satisfy:\n$v_i = \\sum_{v_j \\in N(i)} w_{ij}v_j$\nwhere N(i) is the set of neighbors of $v_i$. The local combination similarity between $v_i$ and $v_j$ is $w_{ij}$, denoted by $L_c(v_i, v_j)$."}, {"title": "2.1.2 STRENGTHENING SIMILARITY", "content": "The purpose of strengthening similarity is to make two data points that are relatively similar become even more similar. It can work in conjunction with the normalization process, by strengthening similarities and decreasing weaker ones to aid the aggregating process.\nDefinition 5 (r-Inflation). Given the hyperparameter r, we define the inflation operator \u0393r:\n$\\Gamma_r: M_{m \\times n} \\rightarrow M_{m \\times n}$ \n$(\\Gamma_r(M))_{ij} = \\text{sign}(r)M_{ij}^r$\nDefinition 6 (Exponential Inflation). Given a vector e = (${\\epsilon_1},{\\ldots},{\\epsilon_n}$) of size n, we define the exponential inflation operator $$\\Gamma^*$: \n$\\Gamma^*: M_{m \\times n} \\rightarrow M_{m \\times n}$ \n$((\\Gamma^*_e(M))_{ij} = \\text{exp}(\\frac{M_{ij}}{\\epsilon_i})$"}, {"title": "2.1.3 NORMALIZING SIMILARITY", "content": "The purpose of normalizing similarity is to transform the similarity matrix into a desired form, which is often related to the modeling of the data. Given a similarity matrix S, where each element is positive, four common normalization operations are typically used.\nDefinition 7 (Row normalization).\n$N_r: M_{m \\times n} \\rightarrow M_{m \\times n}$\n$(N_r(S))_{ij} = \\frac{S_{ij}}{\\sum_k S_{ik}}$\nDefinition 8 (Column normalization).\n$N_c: M_{m \\times n} \\rightarrow M_{m \\times n}$\n$(N_c(S))_{ij} = \\frac{S_{ij}}{\\sum_k S_{kj}}$\nDefinition 9 (Two-side normalization).\n$N_2: M_{m \\times n} \\rightarrow M_{m \\times n}$\n$(N_2(M))_{ij} = \\frac{M_{ij}}{\\sum_k M_{ik} \\sum_k M_{jk}}$\nDefinition 10 (Global normalization).\n$N_g: M_{m \\times n} \\rightarrow M_{m \\times n}$\n$(N_g(M))_{ij} = \\frac{M_{ij}}{\\sum_{kl} M_{kl}}$"}, {"title": "2.2 Similarity computation in machine learning", "content": "Here, we introduce the applications of similarity computation in manifold learning, clustering, supervised learning, and attention mechanism in neural networks, highlighting the distinct common characteristics shared by attention mechanism and traditional algorithms."}, {"title": "2.2.1 SIMILARITY COMPUTATION IN MANIFOLD LEARNING", "content": "Manifold learning refers to dimensionality reduction. Due to the curse of dimensionality, high-dimensional data are often difficult to handle and cannot be visualized. Therefore, dimensionality reduction methods are employed to preprocess the data. The problem statement of manifold learning is as follows: given the high-dimensional data points {$x_i, i = 1,...,N$} \u2282 Rn, assuming that they are distributed on a low-dimensional manifold, how to find their corresponding data points {$y_i, i = 1,..., N$} in a low-dimensional space R\u00b9, where l \u226a n, such that the topology, distances or densities of data points on the underlying manifold is preserved as much as possible?\nHow can we maintain the main structure of data during the process of dimensionality reduction? Prior work often adopts the principle of similarity (dissimilarity or distance) preservation. First, the coordinates of data points in the low-dimensional space are determined using some initialization method. Then, the coordinates are optimized so that the pairwise similarities in the low-dimensional space closely approximate those in the original data. The key difference among most approaches lies in the different similarity computation techniques they employ. To show this, we summarize those used in several classical algorithms including MDS (Kruskal and Wish, 1978), Isomap (Balasubramanian and Schwartz, 2002), LLE (Roweis and Saul, 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), Diffusion map (Coifman and Lafon, 2006), SNE (Hinton and Roweis, 2002), t-SNE (Van der Maaten and Hinton, 2008), UMAP (McInnes et al., 2018) These methods employ hand-crafted similarity computation techniques."}, {"title": "2.2.2 SIMILARITY COMPUTATION IN CLUSTERING", "content": "Fuzzy c-means clustering and k-means clustering Fuzzy c-means clustering categorizes data points into several classes such that data points within each class have high similarity. This is achieved by alternately calculating similarity and updating class representatives based on similarity. To be specific, suppose we have data points {$x_1,\u2026,x_N$}CRn and class center {$C_1,\u2026\u2026, C_m$}CR\":\n\u2022 Calculate the similarity between data points and class center:\n$(D_{m}^{2,0})_{ij} = ||C_i - x_j ||_{m}$\n$S = N_r \\circ N_c(D^{-2,0}_m)$"}, {"title": "2.2.3 SIMILARITY COMPUTATION IN SUPERVISED LEARNING", "content": "K-nearest neighbors algorithm (KNN) KNN is a supervised classification algorithm that utilizes adjacency information (Fix, 1985). Given a set of data points {(xi, Yi),i = 1,\u2026, N}, where yi \u2208 {0,1} represents the label, the predicted label \u0177 for a test data point \u00ee is computed as \u0177 = sTy, where y = (y1,\u2026,yn)T, and the similarity vector s = ($s_1,\u2026\u2026,s_N$)$^T$ is defined by:\ns = N(A)\nwhere A = (Ai,\u2026\u2026, AN)T. Ai = 1 if xi is one of the k-nearest neighbors of \u00ee; otherwise, A_i = 0.\nSupport vector machine (SVM) SVM is a supervised binary classification algorithm (Cortes and Vapnik, 1995). Given a dataset {(xi, Yi), i = 1,\u2026, N}, where yi \u2208 {1,\u22121}, and a kernel function K(,). SVM first computes the coefficients ai and the bias term b corresponding to the supporting plane. For a new sample \u00ee, the predicted label is computed as \u0177 = sTy + b, where y = (y1,\u2026,YN)T and s = ($s_1,\u2026,s_N$)$^T$ is the similarity vector defined as:\nsi = aiK(x, xi)\nIf \u0177 > 0, then \u00ee is classified into the class of +1; otherwise, it is classified into the class of -1."}, {"title": "2.2.4 SIMILARITY COMPUTATION IN ATTENTION MECHANISM FOR INFORMATION PROPAGATION", "content": "Neural networks Information propagation modules are highly prevalent in neural network architectures, particularly in graph neural networks and Transformers. These modules are typically followed by linear transformations and nonlinear activations, which together form the core of these architectures. An information propagation module can be expressed as:\nH^{k+1} = S^{(k)} H^{k}\nwhere Hk contains the features of each node in the k-th layer, S(k) is the similarity matrix in which $S_{ij}^k$ represents the similarity between the i-th and j-th data points in the k-th layer. Different neural networks adopt various methods for similarity computation. For example,\n\u2022 Graph convolutional networks: S Normalized(A + I), where A is the adjacency matrix.\n\u2022 Diffusionnet (Sharp et al., 2022): S = exp(tL), where t is a parameter and L is the discrete Laplacian matrix.\n\u2022 Transformer and graph attention network: Similarity between data points is initially computed pairwise through a learnable pseudo-metric function $f_\\theta(x_i, x_j)$. The resulting similarity matrix is then subjected to exponential scaling and row normalization"}, {"title": "2.3 Approximate Laplacian-Beltrami operator by heat kernel", "content": "The heat kernel is deeply connected to the Laplacian-Beltrami operator, which is a fundamental tool in differential geometry and mathematical physics for studying the geometric properties of surfaces and manifolds. This operator generalizes the Laplacian from Euclidean spaces to general manifolds. Informally, it contains all the information of the Riemannian manifold (Bronstein and Kokkinos, 2010). Here, we focus on the 0-form Laplacian operator, as it is the most commonly studied and has the most direct connection to attention mechanism.\nIn the following, we introduce the method to approximate the Laplacian-Beltrami operator using the heat kernel. Given the data points {$x_i$}$_{i=1}^N$ and the heat kernel $h_\\epsilon(x,y) = exp(-\\frac{||x-y||^2}{2\\epsilon})$, we define the weight matrix W of N \u00d7 N by heat kernel $h_\\epsilon$ and the diagonal matrix D:\n$W_{ij} = h_\\epsilon(x_i, x_j) = exp(-\\frac{||x_i - x_j||^2}{2\\epsilon})$,  $D_{ii} = \\sum_{j=1}^{N} W_{ij} =  \\sum_{j=1}^{N} exp(-\\frac{||x_i - x_j||^2}{2\\epsilon})$\nThe negative defined graph Laplacian for data points {$x_i$}$_{i=1}^N$ is defined as L = D-1W \u2013 I, where\n$L_{ij} = \\frac{exp(-\\frac{||x_i - x_j||^2}{2\\epsilon})}{\\delta_{ij}\\sum_{k} exp(-\\frac{||x_i - x_j||^2}{2\\epsilon})}$\nwhere $\u03b4_{ij}$ = 1 if i = j and $\u03b4_{ij}$ = 0 otherwise. Given that the data points {$x_i$}$_{i=1}^N$ are uniformly distributed on a manifold, it has been shown that the graph Laplacian will converge to the Laplacian of the manifold as e \u2192 0 and N \u2192 \u221e. The following theorem implies that a heat kernel can approximate the Laplacian of a Riemannian manifold and provides the convergence rate of this approximation.\nTheorem 1 (Naive heat kernel approximator (Singer, 2006)). Suppose {$x_i$}$_{i=1}^N$ are i.i.d. sampled from the uniform measure on a compact Riemannian manifold, then\n$\\frac{1}{N} \\sum_{j=1}^N L_{ij}f(x) = \\frac{1}{2} \\Delta f(x_i) + O(\\frac{1}{N^{1/2} \\epsilon^{n/4} + \\epsilon^{1/4} })$"}, {"title": "3 Limit properties of attention mechanism for information propagation", "content": "Due to the analogous formulations of heat kernel approximation and information propagation of attention mechanism, we can analyze the asymptotic properties of attention mechanism by analogy. To start with, we will show a direct analysis for the metric setting of the learnable pseudo-metric, which corresponds to an elegant continuous dynamical system. This special setting is helpful for intuitively understanding attention mechanism by physical intuition. We further generalize this analysis to the situation where the learnable pseudo-metric function does not satisfy the metric setting. The primary difference between these two settings lies in how neighbors are defined."}, {"title": "3.1 Assumptions", "content": "Network structure formulation We examine the attention coefficients computation and information propagation steps in Transformer and graph attention network. Specifically, we pay attention to the following steps:\nH^{new} = SH^{old}\nwhere $S_{ij} = \\frac{exp(-f_\\theta (H^{old}_i, H^{old}_j))}{\\sum_k exp(-f_\\theta (H^{old}_i, H^{old}_k))}$. In the Transformer block, $f_\\theta(x,y) = -x^T (Q^T K)y$. In the graph attention network, $f_\\theta(x,y) = \u2212\u03c3(\u03b1_1Px-\u03b1_2 Py)$, where $\u03b1_i$ (i = 1, 2) are learnable vectors, P is a learnable matrix and \u03c3 is an activation function.\nWe reformulate the similarity matrix of attention mechanism S as S\u20ac:\n$S_{\\epsilon,ij} = \\frac{exp (-\\frac{f_\\theta (H^{old}_i, H^{old}_j)}{2\\epsilon})}{\\sum_k exp(-\\frac{f_\\theta (H^{old}_i, H^{old}_k)}{2\\epsilon})}$\nwhere \u20ac/2 represents the time scale. In attention mechanism, the updated representation is given by Hnew = SHold, where $\\epsilon = \\frac{1}{2}$.\nBy this formulation, we suppose that:"}, {"title": "3.2 Attention limit operator.", "content": "Theorem 3 (Limit of attention mechanism for information propagation). Suppose the assumptions are satisfied, then the dynamics of the feature of each data point in attention mechanism for information propagation is a first-order approximator (with respect to \u20ac) of the PDE:\n$\\frac{dH}{dt} = \\Delta_{g_{\\theta}}H + 2 \\langle \\nabla_{g_{\\theta}} \\text{log} p, \\nabla_{g_{\\theta}}H\\rangle$\nH(x, 0) = Hold(x)\nwhere $\\Delta_{g_{\\theta}}$ is the Laplacian-Beltrami operator of manifold M with the Riemannian metric given by Neural network (Assumption 2) and p is the density function (Assumption 3).\nBased on this theorem, we define the attention limit operator Atg,p by $\\Delta_{g_{\\theta}} + 2 \\langle \\nabla_{g_{\\theta}} \\text{log} p, \\nabla_{g_{\\theta}} \\rangle$, or equivalently  $\\Delta_{g_{\\theta}} + 2 \\langle \\nabla \\text{log} p, \\nabla_{g_{\\theta}}  \\rangle$, which characterizes the limit increment of features influenced by the information propagation of attention mechanism.\nThe attention limit operator reflects the combined information propagation effect of diffusion and density-guided flow (Figure 2). The Atg,p limit operator contains two parts: the Laplacian term  $\\Delta$ and the particle drift  $\\langle \\nabla \\text{log} p, \\nabla_{g_{\\theta}}  \\rangle$ term. The Laplacian term represents heat diffusion, indicating heat diffuses uniformly in all directions within a homogeneous medium, serving to make features tend towards consistency. The particle drift term represents particles always moving in the direction of the steepest change in probability density rather than along contour lines, suggesting that density guides the flow of information. Additionally, this PDE can be translated into its stochastic differential equation (SDE) counterpart, which offers a similar interpretation.\nUsing this theorem, we can analyze the impact of metric scaling on the information propagation rate. If $\\tilde{g_2} = \\frac{1}{m^2}g_1$, m is a constant bigger than 1, then:\n$\\tilde{\\Delta_{g_2}} = m^2 \\Delta_{g_1}$\nThis implies that the rate of information propagation through diffusion can be increased by compressing distances. The density-guided flow also varies with different metrics. Therefore, neural networks can modulate the rate of information propagation by learning different metrics.\nTheorem 4 (Heat diffusion formulation). If the dimension of the manifold M is n \u2260 2, then there exists a metric \u011f such that the Atg,p operator is equivalent to a Laplacian-like"}, {"title": "3.3 Multi-head attention", "content": "Multi-head attention combines multiple attention blocks. A k-head attention mechanism is defined as:\nH\u2081 = S\u1d62Hold, i = 1,\u2026,k\nHnew = [H\u2081V\u2081,\u2026, HkVk][W\u2081,\u2026, Wk]T\nwhere Si is the similarity matrix for the i-th head, Vi is the value matrix of the i-th head, W = [W\u2081,\u2026\u2026, Wk] is a learnable matrix. Reformalize the formula we have:\nHnew = \u2211 SiHold Wi\nwhere Wi = ViWT. Consequently, when assumptions 3.1 are satisfied, a multi-head self-attention block can be viewed as a first-order approximator of a combination of k PDEs:\n$\\frac{dH_i}{dt} = \\Delta_{g_i}H + 2 \\langle \\nabla \\text{log} p_i, \\nabla_{g_i}H \\rangle$\nHi(x, 0) = Hold(x)\nHnew = \u2211 HW\nwhere gi is the Riemannian metric learned by the i-th attention block and pi is the density function of the random variable, from which data are sampled, with respect to gi. Compared to the single-head attention mechanism, multi-head attention simultaneously learns multiple pseudo-metrics for information aggregation and combines the aggregated information. This architecture may reduce the difficulty of learning meaningful pseudo-metrics, thereby demonstrating a stronger capability to extract information."}, {"title": "3.4 An analysis for general pseudo-metric setting", "content": "Before presenting our analysis for the information propagation process of general pseudo-metrics in attention mechanism, we provide an explanation of Theorem 3 to strengthen our understanding of the role of the metric:\nH^{new}(x) = Hold (x) + \\frac{\\epsilon}{2} (\\Delta_{g_{\\theta}} Hold(x) + 2 \\langle \\nabla \\text{log} p, \\nabla_{g_{\\theta}}Hold (x) \\rangle) + \\text{Higher order}"}, {"title": "4 From self-attention to metric-attention", "content": "The zeroth-order term represents the information at the data point x itself, which can be considered the 'nearest' point to x as measured by the metric: since a distance function is positive definite by definition, the only data point that is nearest to x is itself. Similarly, the first-order term comes from the the drift-diffusion process at the nearest data point of x measured by the metric. Inspired by this, if we use a general pseudo-metric function which may not satisfy the conditions of a metric, the zeroth-order term should correspond to the information at the nearest data points of x and the first-order term should be related to a drift-diffusion process at the nearest data points of x, where the nearest data points is define by pseudo-metric $f_\\theta(x, y)$. The main difference is that a metric function d should satisfy the following three conditions:\n\u2022 d(x, y) \u2265 0 and d(x, x) = 0  x = y\n\u2022 d(x,y) = d(y, x)\n\u2022 d(x,y) \u2264 d(x, z) + d(z,y)\nTherefore, for a metric function, the only data point nearest to x should be itself, while for general pseudo-metric functions, the nearest data points may not be x itself and not be unique.\nExample 3. In Transformer, given the three conditions hold: (1) \u2014QTK = PTdiag(a1,\u2026\u2026, an)P, where P \u2208 SO(n); (2) $f_\\theta(x,y) = \\sum a_ix'y'$, where x' = Px and y' = Py; (3) x and y lie on the unit sphere and there exists a\u00bfx\u00bf \u2260 0, we obtain the following result (Appendix C):\n$\\text{argmin}_y f_\\theta (x, y) = PT (\\frac{a_1x'_1}{\\sum a_ix^2},..., \\frac{a_nx'_n}{\\sum a_ix^2})^T$\nGenerally, if QTK is non-degenerate and x,y lie on an ellipsoid, $f_\\theta(x,y)$ has a unique minimizer y.\nDenote argminy$f_\\theta(x,y)$ as Az. According to our previous analysis, the zeorth-order effect should be an average of information in Az and the first-order effect should be related to a drift-diffusion process.\nTheorem 5 (Informal). If Ax = {y'} and certain regularity conditions hold (Appendix C), we have:\nHnew (x) = Hold (y') + \u20acA$_{\\hat{t} f_\\theta,p}$Hold (y') + Higher order\nwhere A$_{\\hat{t} f_\\theta,p}$ is a second-order partial differential operator related to the pseudo-metric $f_\\theta$ and the sampling density p."}, {"title": "4.1 Metric-attention", "content": "As demonstrated above, attention mechanism is essentially a combination of an information propagation process based on a handcrafted similarity computation and a learnable pseudo-metric fo(,). We expect that neural networks can learn beneficial pseudo-metrics from data. However, the self-attention mechanism formulates the pseudo-metric as:\n$f_\\theta(x, y) = -x^TQT Ky$\nwhich can be interpreted as performing a linear transformation on features followed by dot product. These overly simplistic pseudo-metrics may struggle to capture complex similarity relationships, and intuitively, we would prefer them to correspond to continuous dynamical systems for easier training and better generalization. Therefore, we draw on the concept of metric learning (Yu et al., 2016; Hu et al., 2014) and propose a modified attention mechanism called metric-attention mechanism, in which $f_\\theta(x, y) = ||f_\\theta(x) \u2013 f_\\theta(y)||^2$, where fo is a learnable function. The metric-attention information propagation mechanism is detailed in Algorithm 1:"}, {"title": "4.2 Experiments", "content": "To evaluate the effectiveness of the metric-attention mechanism while minimizing the influence of other modules, we construct an information propagation network (IPN) by sequentially passing the input features through a linear transformation layer, a series of attention-based information propagation modules, and another linear transformation layer. For classification tasks, we employ a softmax classifier to classify features. We choose the self-attention mechanism, L2 self-attention mechanism, and metric-attention mechanism"}, {"title": "5 Conclusion", "content": "In this study, we first examine the techniques for similarity computation in classical machine learning algorithms such as manifold learning, clustering and supervised learning. We point out that attention mechanism is essentially a composition of an information propagation process based on a handcrafted similarity computation and a learnable pseudo-metric. This highlights the strong connection between attention mechanism and traditional algorithms. More importantly, we demonstrate the evolution of techniques over time: traditional algorithms rely on manual design for similarity computation, while neural networks represented by attention mechanism introduce learnable and adaptive techniques making them more flexible. Under the assumption that the pseudo-metric is a transformation of a metric function and some other assumptions, we utilize PDEs to explain the limit properties of attention mechanisms and translate them into heat equations for intuitive comprehension. For a general pseudo-metric, we fully account for its differences from a metric and provide a first-order analysis. This helps us intuitively understand the working principle of attention mechanism. That is, the features can be interpreted as updates using the nearest neighbors in terms of the pseudo-metric.\nWe conclude that the parameters of attention mechanism are designed to learn a pseudo-metric, which is used to compute pairwise similarities of data points. That implies we can consider training attention blocks as searching for a helpful pseudo-metric. It shares the same objective as metric learning. Thus, we integrate metric learning into attention mechanism. The difference between attention mechanism and metric learning lies in that metric learning often directly learns the metric through supervised approaches, while attention mechanism achieves this implicitly through propagation, making it more flexible. A significant advantage of attention mechanism is that it allows direct learning of a pseudo-metric through the labels provided by the task without requiring the metric to serve as the supervised signal. Moreover, different layers can learn different metrics, making pseudo-metric learning more powerful and flexible.\nPrevious researchers have attempted to understand deep neural networks (DNNs) using continuous dynamical systems. For example, some studies (E, 2017; Chen et al., 2018) have modeled residual neural networks (He et al., 2016) as ordinary differential equations (ODEs). Gai and Zhang (2021) utilized optimal transport to understand residual neural networks. Song et al. (2020) formalized denoising diffusion probabilistic models (Ho et al., 2020) into stochastic differential equations (SDEs). Certain studies have linked graph neural networks with diffusion processes (Li et al., 2024). Some studies utilized measure theory to understand attention mechanism with residual connections (Geshkovski et al., 2023; Sander et al., 2022; Vuckovic et al., 2020). These studies share the commonality of interpreting the characteristic layer-by-layer updates of features in DNNs as the temporal dimension of information processing. Among these, some studies model the changes in features by dynamics of measures, while others model these as dynamics of functions. Here we adopt the latter perspective, distinguishing it from previous studies on attention mechanism (Geshkovski et al., 2023; Sander et al., 2022; Vuckovic et al., 2020). In addition, We reveal the connections between attention mechanism and classical algorithms that have not been addressed before.\nHowever, this study has certain limitations. First, the limit properties require the satisfaction of three assumptions, which may not hold in engineering practice. Second, although attention mechanism is a crucial component of neural network architectures like Transformers, these networks are complex engineering products with many modules (e.g., multilayer perceptron, skip connection) that are not yet fully understood as a collaborative whole. Third, although we have tested the performance of the metric-attention mechanism, further experiments are needed to figure out how this mechanism performs across different domains and tasks.\nSince attention mechanism can be viewed as a discretization of a heat equation with a learnable metric, a natural question arises. Can new network blocks be designed based on other PDEs that might be more universal or effective in specific domains? We anticipate that this could become a prominent area of research in the future."}, {"title": "Dynamic of heat diffusion", "content": "Heat diffusion on the manifold (M, \u011f) with the specific heat capacity c, thermal conductivity k and material density p has dynamic:\n$\\frac{du}{dt} = \\frac{1}{cp} \\nabla \\cdot (k\\nabla u)$\nWhen k = 1, p = 1 and c = f\u22121, this dynamic is the same as the dynamic of attention.\nProof: By the Fourier's law, the flow of heat can be described by a vector field:\nq = -k\u2207u\nDenote the heat energy at point x and time t by Q(x,t), we know that the change of temperature is proportional to the change of heat energy. To be specific:\n$\\frac{\\partial Q}{\\partial t} = cp \\frac{\\partial u}{\\partial t}$\nAdditionally, we know that the change in heat energy is equal to the net heat flux:\n$\\frac{\\partial Q}{\\partial t} = -\\text{div}q$\nTherefore, we have:\n$\\frac{\\partial u}{\\partial t} = \\frac{1}{cp} \\nabla \\cdot (\\nabla u)$\nLet k = 1, p = 1 and c = f\u22121, we have\n$\\frac{\\partial u}{\\partial t} = f \\Delta u$"}, {"title": ""}]}