{"title": "PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization", "authors": ["Christopher Clarke", "Yuzhao Heng", "Lingjia Tang", "Jason Mars"], "abstract": "The recent emergence of Large Language Models (LLMs) has heralded a new era of human-AI interaction. These sophisticated models, exemplified by Chat-GPT and its successors, have exhibited remarkable capabilities in language understanding. However, as these LLMs have undergone exponential growth, a crucial dimension that remains understudied is the personalization of these models. Large foundation models such as GPT-3 etc. focus on creating a universal model that serves a broad range of tasks and users. This approach emphasizes the model's generalization capabilities, treating users as a collective rather than as distinct individuals. While practical for many common applications, this one-size-fits-all approach often fails to address the rich tapestry of human diversity and individual needs. To explore this issue we introduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP models for user personalization. PEFT-U consists of a series of user-centered tasks containing diverse and individualized expressions where the preferences of users can potentially differ for the same input. Using PEFT-U, we explore the challenge of efficiently personalizing LLMs to accommodate user-specific preferences in the context of diverse user-centered tasks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown tremendous capability in performing complex tasks such as reasoning, summarization, creative writing, etc. Through the scaling of these models, both in size (> 1B parameters) and data (> 1 Trillion tokens) these models have achieved remarkable performance on a wide range of natural language understanding and generation tasks (Touvron et al., 2023; Henighan et al., 2020). However, even as the generalization capability of LLMs has grown, one crucial dimension that has been understudied is the personalization of these models (Salemi et al., 2023; Kazienko et al., 2023).\nAt its core, personalization is about tailoring AI-driven interactions to the individual preferences, needs, and idiosyncrasies of each user (Salemi et al., 2023; Welch et al., 2022; Clarke et al., 2023; Kang et al., 2022). In many real-world scenarios, users have unique preferences, contexts, and expectations, which are currently incapable of being effectively accommodated by the generalized LLMs available today. These traditional LLMs have predominantly adhered to a \"one-size-fits-all\" approach (Touvron et al., 2023; Clarke et al., 2022; OpenAI, 2023; Bubeck et al., 2023), offering a single, uniform model capable of serving all users and tasks alike. While this approach is undoubtedly valuable in many scenarios, it falls short when it comes to accommodating the rich tapestry of human diversity where people are not uniform, and their linguistic and communicative preferences vary widely (Li et al., 2023; Zhang et al., 2018).\nExisting works in NLP have highlighted the need for user perspective in language modeling particularly when dealing with intrinsically subjective applications such as Hate Speech Detection and Sentiment Analysis where differing perspectives are common (Mostafazadeh Davani et al., 2022; Sang and Stanton, 2021; Geva et al., 2019; Kanclerz et al., 2022; Welch et al., 2022). Research has shown that accounting for user perspective and personalization is essential for building robust and effective models (Mostafazadeh Davani et al., 2022; Sang and Stanton, 2021; Geva et al., 2019; Kanclerz et al., 2022; Welch et al., 2022). However, despite this glaring need, existing resources fail to model and cater to these differing perspectives. When curated, NLP resources tend to have an intrinsic bias towards the majority perspective due to their reliance on voting for ground truth. As such they fail to adequately represent diverse user preferences and individualized expressions, further contributing to a lack of personalization (Mostafazadeh Davani et al., 2022; Sang and Stan-"}, {"title": "2 PEFT-U Benchmark", "content": "The PEFT-U benchmark aims to assess the efficacy of language models in producing personalized outputs based on user-specific information.\nData Collection To generate high-quality data samples representative of differing user perspectives we focus on curating subjective problems where the model is forced to respect the user's points of view e.g. Hate Speech Detection. Typically NLP resources for these problem areas employ an annotation process where correctness is determined via majority vote and outliers are discarded. This often results in the overlooking of the subtleties of the user's perspective, ultimately leading to potential group biases and inaccuracies in the data. In contrast, we reconstruct these data sources framing individual annotators as distinct users to capture these important nuances. To avoid the possible influence of noisy/bad annotators we take into account their contribution level to the annotation process and discard low-quality users. Additionally, we discard users with less than n = 10 samples in their training and test sets respectively."}, {"title": "3 Modularity + Personalization", "content": "When exploring the problem of personalization, one possible solution would be the allocation of a dedicated LLM for each user. However, deploying a separate personalized model for each user would incur significant compute costs in production. In addition, the balance between embedding generalized and personalized knowledge in these models remains unclear. Thus providing such a solution is prohibitive in this era of large language models. Recent works in Modular Deep Learning (Liu et al., 2022a; Pfeiffer et al., 2023; Hu et al., 2021; Houlsby et al., 2019), seek to optimize and further tune these LLMs without having to update all the model parameters. These methods typically introduce a small number of additional parameters and update these parameters in a model while freezing the remaining weights, thus limiting the computational resources required. This is often done to enable multi-task learning or to introduce new updates in the training data without the need for training from scratch.\nIn our experiment setting, we shift this paradigm from multi-task learning to multi-user learning focusing on the research question of \"How to efficiently personalize large language models for subjective tasks?\". As such, we empirically analyze personalized prompting approaches (non-"}, {"title": "4 Benchmark Evaluation", "content": "To quantify the challenge the PEFT-U benchmark presents, we evaluate the performance of a range of parameter-efficient methods compared to zero/few-shot prompting approaches.\nMethods We implement and evaluate 7 different parameter-efficient methods for personalizing LLMs using Flan-T5 (Chung et al., 2022). These methods consist of:\n1) Zero-shot/Few-shot Prompting: Using k = 3 random samples of user data we construct an instruction-style prompt for inference.\n2) LoRa (Hu et al., 2021): injects trainable rank decomposition matrices into each layer of the Transformer architecture.\n3) Adapters (Houlsby et al., 2019) add a trainable bottleneck layer after the feedforward network in each Transformer layer.\n4) Prompt Tuning (Lester et al., 2021) introduces an additional k tunable tokens per downstream task prepended to the input text and trained end-to-end.\n5) Prefix-Tuning (Li and Liang, 2021) prepends task-specific trainable vectors to the input of multi-head attention in each Transformer layer that is attended to as virtual tokens.\n6) P-Tuning (Liu et al., 2022b) employs trainable continuous prompt embeddings in concatenation with discrete prompts.\n7) IA^3 (Liu et al., 2022a) introduces trainable"}, {"title": "5 Results", "content": "Evaluation Metrics We consider two performance metrics: (1) average per-user accuracy per task and (2) average accuracy for all tasks.\n5.1 Few/Zero-shot vs PEFT\nTable 2 shows our results analyzing existing PEFT methods in comparison to few/zero-shot prompting techniques. From these results, we show that personalizing models is crucial to providing users with more accurate results representative of their actual perspectives. Notably, zero/few-shot prompting falls short of adequately representing user viewpoints compared to its trained counterparts being outperformed on average by all methods except for Prompt Tuning. Across all methods, results show that Adapters performs the best outperforming on 12 out of the 13 PEFT-U tasks and achieving an overall accuracy score of 64.4% compared to 59.5% of LoRa in 2nd. The presented results underscore the complexity of the PEFT-U benchmark, revealing the challenges inherent in achieving consistently high performance across diverse tasks and"}, {"title": "6 Related Works", "content": "Prior works have highlighted the need for user perspective particularly when dealing with intrinsically subjective applications where differing per-"}, {"title": "7 Conclusion", "content": "This work addresses a critical gap in NLP concerning the personalization of LLMs. While LLMs have achieved remarkable performance in various tasks, their generalization capabilities have predominantly followed a \"one-size-fits-all\" paradigm. This approach falls short of meeting the diverse linguistic and communicative preferences of individual users. The PEFT-U Benchmark introduced in this paper serves as an effort to evaluate the personalization capabilities of LLMs across a spectrum of tasks. PEFT-U, presents a unique challenge by emphasizing scenarios where identical inputs necessitate diverse model outputs. The reported results showcase the inherent challenges posed by the"}, {"title": "8 Limitations", "content": "The PEFT-U Benchmark while designed to capture diverse user perspectives, may not fully represent the intricacies of all real-world communication scenarios. The dataset's construction involved a careful curation process, but the authors acknowledge that the complexities of individual preferences and linguistic nuances are vast and varied. In this work, user perspective is modeled solely based on the user's output preferences. Factors such as age, gender, and other potentially important demographics are not considered.\nIn addition, the personalization methodologies explored in this study may not encompass the entire spectrum of potential approaches. The field of NLP is dynamic, and emerging techniques could offer alternative solutions to the challenges presented. Personalization in LLMs is an evolving research area, as such there may be relevant strategies released recently that were not covered in this work."}, {"title": "A Additional Dataset Details", "content": "In this section, we detail the datasets in our PEFT-U benchmark, including dataset construction, representative samples, and task instructions.\nA.1 Dataset Details & Construction\nWe include datasets in various domains, including:\n\u2022 HateXplain (Mathew et al., 2022) contains posts on social media. Each post is classified into 3 classes: hate, offensive, or normal. The dataset additionally contained annotations for the hate speech target community and the rationales. We consider the post texts and the classification labels only.\n\u2022 GabHate (Kennedy et al., 2018) has 28K posts from the social media platform Gab. Each post is annotated using a hierarchical coding typology of hate-based rhetoric, with hierarchical labels indicating dehumanizing and violent speech, vulgarity and offensive"}, {"title": "A.2 Representative Samples", "content": "In this section, we show representative samples for each dataset where different user perspectives result in different labels for the same text input. Warning: Offensive content is inevitable for datasets in the Hate+Abuse domain.\nA.2.1 Representative Sample for HateXplain\nText. right definitely not going back to the fag hag thing it was fun for a bit in my <number> but outrageously gay people are just too high strung for me it back to that reason i can not be a lesbian\nLabels.\nnormal\nhatespeech\noffensive\nA.2.2 Representative Sample for\nMeasuring HateSpeech\nText. @L1LTR4P fucking losers wetbacks #SorryNotSorry\nLabels.\n- 0\n- 1\n- 2\nA.2.3 Representative Sample for GabHate\nText. #Breaking: Claiming bias against conservatives, @FreedomWatchUSA has filed a class action lawsuit against Google, Facebook, Instagram, Twitter, and Apple.\nLabels.\nNon-hateful\nHateful\nA.2.4 Representative Sample for TweetEval\nText. [USER] fuck Brett Farve redneck ass, he stuckup he don't give a damn lol he be on campus acting like he the shit\nLabels.\nHateful\nNon-hateful\nA.2.5 Representative Sample for Unhealthy\nConversations\nText. They are poor because they are reliant on the drug trade and reliant on the drug trade because they are then poor. That cycle can be broken.\nLabels.\nhealthy\nunhealthy"}, {"title": "A.2.6 Representative Sample for WikiDetox\nAggression", "content": "Text. == Dougbiznatch again! ==\nHey I'm back. Gonna vandalize all day and no one can stop me! As you can tell I can't be stopped by banning. I'll be seeing alo tof you and the rest of the blacklisted admins for the next couple of weeks =P\nDougbiznatch\nLabels.\nAggressive\n- Normal\nA.2.7 Representative Sample for GoEmotion\nText. Is he dead now from a tragic drowning accident? Asking for a friend.\nLabels.\n[sadness]\n[surprise]\nA.2.8 Representative Sample for StudEmo\nText. We got to the Cycle On Hostel by chance in the middle of the night. There wasn't a single place available in other places. . . ...and it's very good that we didn't get to another place! First of all, great service: people who are open to others, nice and smiling, who help us with every time of day and night. Spacious hostel, rooms and bathrooms are clean. And besides all that - the location nothing to add, nothing to take away. Literally 5 minutes away from Neptune and at the same time the building is situated in such a place that at night it is quiet despite such a short distance from the busiest street where it is full of tourists and children. If we will still have a chance to spend the night in Gdansk, we will surely come to Cycle On again. With a clear conscience I recommend\nLabels.\n[trust, anticipation, valence, arousal]\n- [joy, trust, valence, arousal]\nA.2.9 Representative Sample for Subjective\nDiscourse (response)\nText. politician: JIM JORDAN, Ohio\nOkay. Well, this is put out by the Exempt Organizations Division, same division where all these problems took place over the last three years. It came out, again, just five days after the comment period on the proposed rule ended at the end of February, and I want to just highlight a few of the"}, {"title": "A.2.10 Representative Sample for Subjective\nDiscourse (question sentiment)", "content": "Text. politician: RANDY NEUGEBAUER, Texas\nAnd, as you're aware, Section 972 of Dodd-Frank requires an issuer of securities to disclose the annual proxy statement, the reason why the issuer has chosen to allow the same person to serve as the board chairman and the CEO. This year, Wells states that your dual role is a result of your extensive experience and knowledge regarding the company and provides the most efficient leadership of the board and the company. Mr. Stumpf, do you think it's a good idea for the same person to serve as both chairman of the board and CEO?\nwitness: Stumpf, John G., Chairman and Chief Executive Officer, Wells Fargo and Company\nThank you, Congressman, for that question. In our company, we have 14 outside directors. We have a lead director. All directors are New York Stock Exchange-independent, by their standards. I am not a member of any standing committee of that board. The independent directors and the lead director help set the agenda for the boards. They always have meetings that are in executive session without me. And as you probably read about what happened this weekend-because we filed an 8-K yesterday about actions that they took as an independent board, and I was not part of that. So the board acts quite independently.\nLabels.\nnegative\nneutral\nsomewhat-negative\nsomewhat-positive\nA.2.11 Representative Sample for Subjective\nDiscourse (response sentiment)\nText. politician: JERROLD NADLER, New York, Chairman\nIt is our understanding that at least one briefing occurred between December\u2013between your decision not to recuse yourself on December 19th and 6 days later, Christmas day. Is that correct? Simple enough question, yes or no.\nwitness: Matthew G. Whitaker, Acting Attorney General, U.S. Department of Justice Oral Testimony\nMr. Chairman, I, again, what is the basis for your question? You are saying that it is your understanding--\nLabels.\nnegative\nneutral\nsomewhat-negative\n- somewhat-positive"}, {"title": "A.2.12 Representative Sample for\nCockamamie", "content": "Text. judokas\nLabels.\nhumorous\nnot-humorous\nA.2.13 Representative Sample for EPIC\nText. message: \u201cBecause the last generation was Gen. Z and there's no where to go past that, so we gotta start from the beginning.\"\nreply: \"but we should have just named the first generation \"Alpha\" instead of doing it now\"\nLabels.\nIronic\n- Non-ironic"}, {"title": "A.3 Task Instructions", "content": "In this section, we show the task instructions prepended to prompts in training and inference for each dataset.\nA.3.1 Instruction for HateXplain\nPlease review the following text and indicate if it has the presence of hate speech or is offensive in nature. Respond 'hatespeech' if the text contains hate speech, 'offensive' if the text does not contain hate speech but is offensive and 'normal' if the text is neither offensive nor contains hate speech.\nA.3.2 Instruction for MeasuringHateSpeech\nOn a scale of 0-2 rate the level of hatespeech contained in the following text. Respond with '0' if the text does not contain hate speech, '1' if the text contains some hate speech, and '2' if the text contains a lot of hate speech.\nA.3.3 Instruction for GabHate\nPlease review the following text and indicate if it has the presence of hate speech. Respond \u2018Hateful' if the text contains hate speech and \u2018Non-hateful' if the text does not contain hate speech.\nA.3.4 Instruction for TweetEval\nPlease review the following text and indicate if it has the presence of hate speech. Respond 'Hateful' if the text contains hate speech and 'Non-hateful' if the text does not contain hate speech.\nA.3.5 Instruction for Unhealthy\nConversations\nPlease review the following text and indicated if it is 'healthy' or 'unhealthy'. Respond \u2018healthy' if the text is healthy and 'unhealthy' if the text can be considered hostile, antagonistic, condescending, dismissive or an unfair generalization.\nA.3.6 Instruction for WikiDetox Aggression\nPlease review the following text and indicate if it has the presence of malicious remark to a person or group. Respond 'Aggressive' if the text contains a personal attack and \u2018Normal' if the text does not contain a personal attack.\nA.3.7 Instruction for GoEmotion\nPlease analyze the following text and assign one or more appropriate emotion labels. Emotion labels include happiness, sadness, anger, surprise, joy, fear, disgust. You can select one or multiple emotion labels that best capture the emotional content of the text. Respond with the emotion labels separated by a comma.\nA.3.8 Instruction for StudEmo\nPlease analyze the following text and assign one or more appropriate emotion labels. Emotion labels include joy, trust, anticipation, surprise, fear, sadness, disgust, anger, valence, and arousal. You can select one or multiple emotion labels that best capture the emotional content of the text. Respond with the emotion labels separated by a comma.\nA.3.9 Instruction for Subjective Discourse\n(response)\nPlease analyze the following text and indicate how the witness responded to the question. Respond with 'answer' if they answered the question reasonably, 'cant-answer-lying' if they could not answer and are lying, 'cant-answer-sincere' if they could not answer but are honest about it, 'shift-dodge' if they shifted the topic with the intent of dodging the question, 'answer_overans-sway' if they over answered the question with the intention of swaying or 'shift-correct' if they shifted the topic with the intention of clarifying the question.\nA.3.10 Instruction for Subjective Discourse\n(question sentiment)\nPlease analyze the following text and rate your sentiment towards the questioners. Sentiment labels include 'somewhat-positive', \u2018positive', 'very-positive', 'somewhat-negative', 'very-negative', 'neutral' and 'negative'. Respond with the sentiment label that best captures your sentiment towards the questioners."}, {"title": "A.3.11 Instruction for Subjective Discourse\n(response sentiment)", "content": "Please analyze the following text and rate your sentiment towards the witness. Sentiment labels include 'somewhat-positive', \u2018positive', \u2018very-positive', 'somewhat-negative', 'very-negative', 'neutral' and 'negative'. Respond with the sentiment label that best captures your sentiment towards the witness.\nA.3.12 Instruction for Cockamamie\nPlease rate whether the following text is funny or not funny. Respond 'yes' if you think the text is funny and 'no' if you think the text is not funny.\nA.3.13 Instruction for EPIC\nIrony is a figurative language device that conveys that opposite of literal meaning, profiling intentionally a secondary or extended meaning. Please review the following message and reply and indicate if it has the presence of irony. Respond 'Ironic' if the reply if you think the reply is ironic and \u2018Non-ironic' if you think the reply is not ironic."}]}