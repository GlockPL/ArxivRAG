{"title": "Can LLM Generate Regression Tests for Software Commits?", "authors": ["JING LIU", "SEONGMIN LEE", "ELEONORA LOSIOUK", "MARCEL B\u00d6HME"], "abstract": "Large Language Models (LLMs) have shown tremendous promise in automated software engineering. In this paper, we investigate the opportunities of LLMs for automatic regression test generation for programs that take highly structured, human-readable inputs, such as XML parsers or JavaScript interpreters. Concretely, we explore the following regression test generation scenarios for such programs that have so far been difficult to test automatically in the absence of corresponding input grammars:\n\u2022 Bug finding. Given a code change (e.g., a commit or pull request), our LLM-based approach generates a test case with the objective of revealing any bugs that might be introduced if that change is applied.\n\u2022 Patch testing. Given a patch, our LLM-based approach generates a test case that fails before but passes after the patch. This test can be added to the regression test suite to catch similar bugs in the future.\nWe implement CLEVEREST, a feedback-directed, zero-shot LLM-based regression test generation technique, and evaluate its effectiveness on 22 commits to three subject programs: Mujs, Libxml2, and Poppler. For programs using more human-readable file formats, like XML or JavaScript, we found CLEVEREST performed very well. It generated easy-to-understand bug-revealing or bug-reproduction test cases for the majority of commits in just under three minutes-even when only the code diff or commit message (unless it was too vague) was given. For programs with more compact file formats, like PDF, as expected, it struggled to generate effective test cases. However, the LLM-supplied test cases are not very far from becoming effective (e.g., when used as a seed by a greybox fuzzer or as a starting point by the developer).", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) have shown tremendous promise. Liu et al. [14] just published a comprehensive survey of research on the automation of the most important software engineering processes, including requirements engineering, code generation, program analysis, testing, debugging, and end-to-end development and maintenance. For instance, the LLM-based AutoCodeRover [36] can successfully resolve more than 30% of Github issues in a recent benchmark.\nIn this paper, we take a critical perspective on the utility of LLMs as assistants in automated regression test generation for programs that take human-readable, highly structured inputs. Given a commit or pull request, regression test generation is the problem of exposing any bugs that may have been introduced or fixed by the code changes in that commit. We find this specific problem statement interesting for several reasons."}, {"title": "2 Experimental Design", "content": "In this paper, we seek to evaluate the capabilities of a large language model (LLM) as a regression test generation tool in the CI/CD pipeline (Continuous Integration / Continuous Delivery) for"}, {"title": "2.1 Research Questions", "content": "\u2022 RQ1. How well does CLEVEREST perform as a regression test generator? Specifically, we evaluate the two most important properties of CLEVEREST if it was fully embedded in a CI/CD pipeline: effectiveness and execution time. Given the corresponding commits, we evaluate (i) finding bugs that were introduced in a commit and (ii) reproducing bugs that were patched in a commit. To understand how CLEVEREST performs in the hands of a developer, we study quantitatively and qualitatively how \u201cclose\u201d CLEVEREST-generated test cases are from revealing the bug.\n\u2022 RQ2. How well does CLEVEREST perform under various hyperparameter values? Specifically, we evaluate how well CLEVEREST performs compared to the default configuration if we (a) only used the commit message but not the diff, (b) only used the commit diff but not the message, (c) let it generate the command line prompt in addition to the test input. (d) used maximum LLM temperature, (e) used a less powerful LLM (40-mini), (f) used 10 iterations instead of 5 in the feedback loop, and (g) dropped the execution analysis result in the feedback loop.\n\u2022 RQ3. How does CLEVEREST compare to WAFLGo, the state-of-the-art in regression test generation? Specifically, we evaluate (1) the performance of WAFLGo as a few-shot regression test generator and (2) the dependence of WAFLGo on the quality of the initial seed corpus. We then (3) evaluate"}, {"title": "2.2 Cleverest: LLM-Based Regression Testing Technique and Implementation", "content": "We present the design and implementation of CLEVEREST, a zero-shot feedback-guided LLM-based regression test generation tool, to evaluate the effectiveness of LLMs for generating regression tests for software commits to programs taking highly structured, human-readable inputs.\nFigure 1 gives a procedural overview of CLEVEREST. Our tool CLEVEREST consists of three key components: Prompt Synthesizer, LLM Module, and Execution Analyzer, which work in a feedback loop to generate regression test cases for the commit under test."}, {"title": "2.2.1 Prompt Synthesizer", "content": "The first component of CLEVEREST is the Prompt Synthesizer, which generates a domain-specific prompt for the LLM to create the test input. Given the testing target, which is the software and the commit under test, and optionally some feedback from a previous iteration, the prompt synthesizer constructs the prompt for the LLM, which includes the task description, commit information, and the attempt history.\nFigure 2 shows an example prompt to generate a regression test case for a code commit to Mujs, a lightweight JavaScript interpreter. The synthesized prompt has the following structure:\n(1) Task Description: The beginning of the prompt starts with a general description of the task. The main parts consist of (a) a simple sentence to describe the program under test, e.g., \"JavaScript interpreter,\u201d (b) the goal of the LLM-generated input, e.g., \u201ctrigger a bug introduced by the commit,\u201d and (c) format specification of LLM's response, e.g., \u201creturn the input wrapped in a triple-backtick fenced block along with its brief explanation of the expected behavior.\u201d\n(2) Commit Information: The second part contains the information about the commit under test to the LLM. We consider both the commit message and the code diff as the source of the information as they provide different levels of context about the commit: The code diff contains"}, {"title": "2.2.2 LLM Module", "content": "The generated prompt is fed into the LLM module, which generates the test input for the commit under test. The output of the LLM contains the content of the test input along with the explanation of the expected behavior and the cmd if asked. The output is then parsed to produce the actual test case, which is executed to verify the commit under test. Depending on the task description, the LLM tries to generate the input that triggers the bug introduced by the commit (without knowing a priori if the bug is introduced) or reproduces the bug fixed by the commit.\nThe LLM module has several hyperparameters that affect the quality of the generated output. One hyperparameter is the size of the LLM model, which determines the model's capacity. A larger model size generally results in better output quality but requires more computational resources. Another hyperparameter is the temperature, which controls the randomness of the output. A higher temperature value results in more randomness in the output, while a lower temperature value results in more deterministic output."}, {"title": "2.2.3 Execution Analyzer and Feedback Loop", "content": "The final component of CLEVEREST, the Execution Analyzer, receives the LLM-generated test input and executes it on the program before and after the commit under test. The execution analyzer compares the two executions to measure if the generated input contributes to the commit testing. To classify the test outcome, we leverage the RIPR model [1, 12], i.e., Reaching, Infecting, Propagating, and Revealing, to measure the effectiveness of the generated input. We measure the result of the execution in the following three aspects:\n(1) Bug Triggering: Bug triggering is the ultimate goal of the testing; it represents the Revealing part of the RIPR model. Depending on the scenario, the execution analyzer checks if the input triggers the intended bug: the input should trigger the bug on the program after or before the commit for the bug-finding or bug-reproduction scenario, respectively, and the same input should not trigger the same bug on the other version of the program. For instance, even if the input triggers a bug in the program, it is not considered a successful bug triggering if the same bug is triggered in both versions of the program.\n(2) Output Changing: Even if the input does not trigger a bug as the sanitizer does not detect the anomaly, it may result in a behavior difference; this represents the Infecting and Propagating parts of the RIPR model. We detect the behavior difference by comparing the output and return code of the program before and after the commit. Any difference will imply that the generated test successfully exploits the difference in the program behavior introduced by the commit.\n(3) Commit Reaching: The first necessary condition for input to reveal a bug introduced by the commit is to reach the code changed by the commit; this represents the Reaching part of"}, {"title": "2.2.4 Implementation", "content": "Below, we describe the implementation details of CLEVEREST.\nCommit Information Extraction. We use Git to extract the commit message and code diff. The command 'git show -format=%B' is used to print them together in a concise format. For commits with lengths that exceed the context length of LLM, we will apply a filter mechanism to keep only code diff in the C/C++ source file. If it still does not fit, we keep only the commit message.\nExecution Feedback. CLEVEREST utilizes the sanitizer while compiling the program to detect the bugs. It inserts instrumentation code during the compilation so that it detects malicious or undefined behavior at runtime. Using the sanitizer, one can detect various types of bugs, such as memory corruption, use-after-free, and buffer overflow, without any preliminary effort to define the oracle and write assertions or test cases.\nIn the experiment, we used AddressSanitizer since the benchmark dataset we considered consists of memory-related bugs. The execution analyzer checks for the keyword Sanitizer in the program output to determine the existence of a bug. For behavior difference, we compare the stdout, stderr, and return code of the program built before and after the commit under test. To identify the reachability of the commit, we use GCOV to get code coverage information. We consider the input reaches the commit if there is any overlap between the code changed by the commit and the code covered by the execution.\nLLM Configuration. We use openai-cli, a command line client written in Bash, to make LLM queries. We use GPT-40 as the default model and set the max token to 4096 to achieve a balance between performance and cost. It is worth noting that we do not keep conversation history when querying LLM, as it consumes more tokens. The default temperature is set to 0.5, and the number of iterations is set to 5. We additionally consider up to ten iterations and the temperature to be 1.0 to investigate the effect of the parameter in our methodology. We also consider the GPT-40 mini model to investigate the effect of the model size in our methodology. GPT-40 mini is a smaller, more efficient version of GPT-40; it scores 82% on the MMLU benchmark, whereas GPT-40 scores 88.7%. It is designed especially for applications where affordability and lower latency are critical."}, {"title": "2.3 Benchmark Selection and Experimental Infrastructure", "content": "Benchmark Selection. To answer the research questions for CLEVEREST, we choose a commit benchmark dataset for our experiment based on the following selection criteria:\n(1) We want to focus on regression test generation for programs that take highly structured, human-readable input formats.\n(2) We want to maximize the fairness of our comparison to the state-of-the-art regression test generation tool WAFLGo, which is used for comparison.\n(3) We consider real-world regression bugs of various types that have been introduced and patched in identified commits to open-source software.\nTable 1 shows the details of the dataset that satisfies our selection criteria. Specifically, we found that the benchmark dataset that was used during the evaluation of WAFLGo [33] satisfies selection criteria 2 and 3, while the three programs (i.e., the JavaScript interpreter Mujs, the XML parsing library Libxml2, and the PDF rendering library Poppler) satisfy selection criterion 1. These programs cover various types of textual input formats: JavaScript, XML, and PDF, which are all highly structured. Each software has one or more command-line utilities that retrieve the textual input files as command-line parameters with relevant options.\nThe WAFLGo dataset lists 11 bugs for these three benchmark programs, all of which are memory-related. The bug types span a wide range of memory-related bugs, including heap-buffer-overflow, global-buffer-overflow, heap-use-after-free, stack-overflow, etc. For each bug, we consider the bug-introducing-commit (BIC) and the bug-fixing-commit (BFC) as the target commit in each scenario: BIC for bug-finding and BFC for bug-reproduction scenarios. Thus, we have 22 commits in total for the experiment.\nExperiment Configuration. The default configuration of our Cleverest is as follows: we provide both the commit message and the code diff to the LLM as the commit information, and we provide the command-line utility and parameters expected to be used to test the commit. We attached the execution analysis result during the feedback. As previously mentioned, we use the GPT-40 model with a temperature of 0.5 and set the number of iterations to 5. We set the timeout for each LLM-based input generation to 30 seconds. We repeat each experiment five times to account for the randomness of the LLM and the fuzzer. All experiments are conducted on a docker container with 64 cores of AMD EPYC 7713P @ 2.0GHz and 251GB memory."}, {"title": "3 RQ1. Evaluation of Capabilities", "content": "Effectiveness. We measure the effectiveness of CLEVEREST by computing the average effectiveness score across all five repetitions. If the generated regression test case finds the bug in the bug-introducing commit or reproduces the bug in the bug-fixing commit, the score is 3 (\u2714). Similarly, 2 indicates an output-changing test case (\u2714), 1 indicates a commit-reaching test case (\u2192), and 0 indicates a test case that fails to reach the changed code (X). Visually, we represent the average effectiveness score on a slider. The slider is colored in red, orange, olive, and teal if the score is within the range of [0-0], (0-1], (1-2], and (2-3], respectively.\nTable 2 shows how well CLEVEREST performs as a regression test generator. We find that CLEVER-EST performs unexpectedly well in bug finding and bug reproduction for the two programs, Mujs and Libxml2, which take more human-readable formats. Despite missing the code of the full program and without information about the input features needed to exercise the changed code, CLEVEREST found bugs in 3 of 6 bug-introducing commits (one bug existed before) and reproduced the bugs patched in 4 of 6 bug-fixing commits\u2014it at least reached the changed code in 4 of the 5 remaining cases.\nHowever, for the Poppler PDF parser, which requires a complex input format, CLEVEREST could not find or reproduce any of the five bugs. At least, it reached the changed code in half of the commits and exposed a difference in one commit. CLEVEREST consistently fails to generate the input relevant to the regression test generation in both scenarios for the two issues of Poppler, #1305 and #1381. While the generated input contains some related components required to test the commit, it does not satisfy the validity constraints required to successfully parse the input. For instance, CLEVEREST can generate an input that already contains the key elements necessary for revealing the bug in Issue #1305, including an annotation of type Highlight with an appearance stream and a Resources dictionary with an ExtGState entry. The failure to reach the commit occurs because Poppler strictly checks the existence of QuadPoints and Rect properties in the annotation object and exits early before reaching the changed code. If CLEVEREST knew these validity constraints and added these specific elements, as we confirmed, the generated PDF input could have reached the commit and even demonstrated an output difference before and after the commit."}, {"title": "RQ1. Result Summary", "content": "CLEVEREST performs unexpectedly well for the commits to programs that take more human-readable formats but struggled to generate the right structure for the more complex format. With a short execution time of minutes, CLEVEREST is well-suited to be used in the CI/CD pipeline. As they are easy to read and may already be halfway there, CLEVEREST-generated inputs can also be used as a starting point for manual regression testing, even if they do not trigger any bugs in the given commit."}, {"title": "4 RQ2. Ablation Study", "content": "Table 3 shows how CLEVEREST performs under various hyperparameter values for the bug-finding and bug-reproduction scenarios. Specifically, we evaluate how well CLEVEREST performs compared to the default configuration if we only used the commit message but not the diff or only used the commit diff but not the message (commit information), let it generate the command line prompt in addition to the test input (task difficulty), used GPT-40 mini as a less powerful LLM or maximized the LLM temperature (LLM module), used ten (10) iterations instead of five (5) in the feedback loop (number of iterations), or dropping the execution analysis result from the feedback (feedback utility)."}, {"title": "Commit Information", "content": "We evaluate the impact of only using the commit message but not the diff (\"only msg\") or only using the commit diff but not the message (\u201conly diff\u201d) and find that the results reproduce as long as the intention of the change is captured. In the bug-finding scenario, providing only the commit message changes the score very slightly from 1.32 to 1.35 (i.e., sometimes output-changing). In both scenarios, providing only the commit diff also only changes the score very slightly from 1.32 to 1.23 and from 1.35 and 1.44, respectively.\nOnly in the bug reproduction scenario, where the commit messages for bug fixes happen to be fairly brief and lack detailed information about the intended change, the effectiveness of CLEVEREST drops from 1.35 (i.e., sometimes output-changing) to 0.66 (i.e., sometimes unreached). For example, the bug fix for Issue #145 of Mujs comes with the commit message \u201cFix js_strtol.\u201d This is too brief to understand the bug context. The patch for Issue #1284 of Poppler comes with the commit message, \u201ctopIdx can't be negative.\u201d Yet, it remains unclear what topIdx is, how it can be negative, and how it relates to features required in the generated test case.\nIn contrast, the commit diff often points to the intention of the change. For instance, the patch of Issue #141 in Mujs adds an error handling routine that checks if there is an unterminated escape sequence in some regex pattern, which goes much beyond what the commit message says: \u201cAdd missing end-of-string checks in regexp lexer.\u201d"}, {"title": "Task Difficulty", "content": "We evaluate the difference in effectiveness if CLEVEREST is also asked to generate the command line prompt in addition to the regression test case (\u201cGen. cmd\u201d) and find no significant impact on the performance of CLEVEREST. We can only observe a negative effect for the commit introducing Issue #535 of Libxml2, changing the average score from 1.32 to 1.20. This indicates that the LLM is even capable of generating the right command line prompt for regression testing."}, {"title": "LLM Module", "content": "We evaluate the impact of using a less powerful LLM (GPT-40 mini) and find that model size has a significant impact on the performance of CLEVEREST. Changing the LLM from GPT-40 to GPT-40 mini reduces the effectiveness of CLEVEREST for five issues in the bug-finding scenario. For two bug-introducing commits, the generated test cases never even reach the code changes. The average score drops from 1.32 to 0.81. The same happens, but more severely, for the bug-reproduction scenario: Effectiveness reduces for eight (8) patches. For five (5), the generated test cases never even reach the code changes. The average score drops from 1.35 to 0.42. The result indicates that while GPT-40 mini is more cost-effective than GPT-40, it is insufficient to assist the complex input generation for the commit testing.\nWe also evaluate an increase in temperature (i.e., the degree of confabulation). We expected that maximizing temperature would also lead to an increase in test case diversity. However, we find no significant changes in the effectiveness of CLEVEREST when the temperature is increased."}, {"title": "Number of Iterations", "content": "We evaluate the impact on effectiveness if we increase the number of feedback iterations from five (5) to ten (10) and find a small positive impact only for bug-reproduction (from 1.35 to 1.56). In every iteration, CLEVEREST appends the execution feedback of the previous iteration from the Execution Analyser to the synthesized prompt (Fig. 2). We expect an increased number of iterations may positively impact on the effectiveness.\nAfter increasing the number of iterations, two cases (2) turned from reaching to bug-triggering. Three cases (3) turned from failing to reach to output-changing. One case (1) turned from reaching to output-changing. Probably due to randomness, one case (1) turned from reaching to failing to reach. For the bug-finding scenario, we find no significant impact on CLEVEREST's effectiveness. Overall, there may be some benefit of increasing the number of feedback iterations at the cost of execution time."}, {"title": "Feedback utility", "content": "We evaluate the impact of dropping the execution analysis result from the feedback, i.e., having only the record of previously generated inputs in the prompt, and find that it has a negative impact on the effectiveness of Cleverest, particularly in the bug-reproduction scenario. In the bug-finding scenario (from 1.32 to 1.26), the decrease is obvious only for Issue #1289 of Poppler, where the generated test case now even fails to reach the code changes. In the bug-reproduction scenario (from 1.35 to 0.99), CLEVEREST's effectiveness reduces across all bug-fixing commits. The result indicates that the execution feedback is crucial for generating the regression test input in the bug-reproduction scenario.\nTaking a closer look, we found that the feedback was especially helpful to the LLM by pointing out invalid components in the input. For instance, for Issue #166 of Mujs, the test case that is often generated in the first iteration contains a token, Symbol, which is not supported by the program and returns an error: ReferenceError: \u2018Symbol' is not defined. The execution feedback helps the LLM avoid generating another input with the invalid component in the next iteration \u2013 until, finally, a valid, bug-triggering input is generated."}, {"title": "RQ2. Result Summary", "content": "Surprisingly, we found that CLEVEREST continues to generate effective test cases that find bugs in the changed feature as long as the intention is obvious which feature is changed, i.e., even when only an expressive commit message is provided. We also found that asking CLEVEREST to generate the command line prompt itself did not reduce its performance; it even found a bug that was related to the changed feature but was only fixed much later.\nReducing the model size or dropping the execution feedback both reduced the effectiveness. Doubling the number of iterations (at double the cost) slightly increased the effectiveness of CLEVEREST."}, {"title": "5 RQ3. Comparison to the State-of-the-Art", "content": "We evaluate how CLEVEREST compares to WAFLGo, the state-of-the-art in regression test generation. WAFLGo [33] is the most recent, state-of-the-art directed greybox fuzzer for regression test generation that was shown to outperform previous directed and regression greybox fuzzers [4, 6, 15, 37, 38].\nGiven a corpus of valid input seeds, WAFLGo mutates these seeds in a feedback-directed manner, intending to reach the changed code and expose any bugs."}, {"title": "5.1 Direct Comparison", "content": "Table 4 contains the results for comparing CLEVEREST and WAFLGo regarding effectiveness score and execution time. We first evaluate both tools head to head and then explore the effectiveness of CLEVEREST if the reaching or output-changing test cases were also fuzzed (ClevFuzz).\nEffectiveness. Technically speaking, CLEVEREST performs as well as WAFLGo in bug reproduction and slightly worse than WAFLGo in bug finding (Columns Bug in Tab. 4). WAFLGo finds the bug-triggering input in five (5) and four (4) of the 11 bug-introducing and bug-fixing commits, respectively. However, we also notice that the initial seed corpus that is provided to WAFLGO already finds the bugs that were introduced and patched in the respective commits in four (4) cases (Column Init). We explore the dependence of WAFLGo's effectiveness further in the next section.\nExecution time. Cleverest is substantially faster than WAFLGo, which makes our LLM-based approach more suitable as part of the CI/CD pipeline, which runs under strict time and resource constraints. A typical fuzzing campaign is set to 24 hours. Apart from four cases where the initial seeds already found or reproduced the bug that was introduced or patched in a commit, WAFLGo takes between five and fifteen minutes to find the bugs, on average. For the commit introducing Issue #65 of Mujs, WAFLGo took more than 1.5 hours. CLEVEREST's execution time is bounded by the number of iterations (fixed to 5 in our experiments) and requires less than one minute for Mujs and Libxml2 and between two and three minutes for Poppler.\nFuzzing CLEVEREST's seeds for improved effectiveness. We explore an opportunity to automatically improve the effectiveness of CLEVEREST using the generated test inputs as fuzzer seeds (CLevFuzz). During the qualitative analysis of the change-reaching and output-changing CLEVEREST-generated test cases for RQ1 (\u00a73), we found that they are often not \u201cvery far\u201d from bug-revealing. This insight is further inspired by the use of LLMs as seed generators for fuzzing as an effective strategy in the 2024 DARPA AI Cyberchallenge (AIxCC)."}, {"title": "5.2 Cleverest as Zero-Shot Regression Test Generator", "content": "From the perspective of CLEVEREST as a zero-shot regression test generator, we wanted to explore the dependence of WAFLGo's effectiveness on the initial set of seed files (i.e., valid XML, JS, or PDF files). For Mujs, Libxml2, and Poppler, there are 19, 14, and 100 initial seeds for WAFLGo. The authors of WAFLGo [33] used a sound and fair seed corpus selection strategy for their experiments; the initial seeds were taken from the UNIFUZZ benchmark [13]. Nevertheless, in four cases, the initial corpus already reveals Issues #166 of Mujs and #535 of Libxml2 introduced or fixed in the corresponding commits (cf. Column WAFLGo.Init in Tab. 4). This raises the question of how close to triggering the corresponding bugs are WAFLGo initial seeds."}, {"title": "Results", "content": "Table 4 (Col. WAFLGo.Init) shows the effectiveness of the initial corpus in terms of reaching the code changes and revealing changes in the output. Among the 22 cases, in addition to the four (4) bug-revealing cases, in nine (9) cases, the initial seed corpus at least already reaches the code changes. In two (2) additional cases, only a few characters need to be changed to reveal the bug. Listings 1 and 2 above show such examples for Issues #65 and #145 of Mujs. Given this result, we find that CLEVEREST, as a zero-shot regression test generator, also makes an excellent seed generator for regression greybox fuzzers, like WAFLGo."}, {"title": "RQ3. Result Summary", "content": "While CLEVEREST is substantially faster than WAFLGo, CLEVEREST performs as well as WAFLGo in bug reproduction and slightly worse in bug finding. However, by upgrading CLEVEREST by fuzzing the generated test cases, our zero-shot approach outperforms WAFLGo, whose performance depends on a user-provided seed corpus, which, as we found, happens to be quite close to bug-revealing already. Hence, we find that CLEVEREST, as a zero-shot regression test generator, also makes an excellent seed generator for regression greybox fuzzers."}, {"title": "6 Threats to Validity", "content": "Construct Validity. A key concern is the potential for data leakage and memorization by LLMs, where test set information may inadvertently influence training. In this study, the risk pertains to LLMs memorizing the regression test cases from the WAFLGo benchmark (cf. Table 1). To assess this risk, we computed the similarity, in terms of Levenshtein ratio [2], between the CLEVEREST-generated test cases and the available bug-triggering test cases that were provided along with the bug report (not the commit). We find that the majority of CLEVEREST-generated test cases are less than 7% similar (mean 10%, max. 40%) to the ones available online. A significant difference suggests that the LLM did not simply memorize them. We also note that in an experiment where CLEVEREST was asked to generate the command line prompt, as well, a bug was found that\u2014while unrelated to the feature changed in the targeted bug fix-was only fixed after LLM's cut-off date (cf. RQ2).\nInternal Validity. We identified three main threats to internal validity: implementation correctness, confabulation of the LLM, and the randomness of our results. To ensure correctness, we conducted thorough code reviews and testing. Our implementation, including data and scripts, is made available in a replication package for transparency. We used publicly accessible GPT-40 and GPT-40 mini models via the OpenAI API, adhering strictly to their documented usage guidelines. We mitigate the impact of confabulation; all generated test cases are actually executed on subject programs to validate the test outcome. To handle the randomness in our results, we repeated each experiment within the available budget, i.e., five (5) times, and reported the findings across all trials. To assess the impact of various hyperparameters on our approach effectiveness, we performed an ablation study (RQ.2).\nExternal Validity. We do not claim the generality of our results but consider our experiments as an important case study for three open-source C programs that take highly structured, human-readable inputs. We sought to test the capabilities of an LLM as a regression test generation tool and carefully established benchmark selection criteria to align with this goal. We selected all programs from the WAFLGo benchmark that apply and all of their commits. The results demonstrate that the LLMs are capable of generating contextually relevant and well-structured inputs for the selected commits. However, the findings may not extend to programs with less structured or non-human-readable input formats. Our case provides valuable insights into the potential of LLMs for regression test generation, expanding the current understanding of their capabilities and limitations in this context."}, {"title": "7 Related Work", "content": "LLMs for Automatic Unit Test Generation. The field of automated test case generation has evolved significantly, particularly with the advent of LLMs and deep learning techniques. Early research relied on non-LLM approaches, such as Atlas [31], which uses neural machine translation to generate assert statements, and ATHENATEST [28], which applies deep learning to generate unit tests based on real-world examples. Both approaches use sequence-to-sequence models to map test methods to assert statements. Similarly, CodeT [5] relies on language models to generate both code solutions and corresponding tests for programming problems."}, {"title": "LLMs for Static Bug Detection", "content": "The advent of LLMs has pushed researchers towards exploring their potential in vulnerability detection. Previous works [7, 26] tested different prompting strategies [19, 27], revealing that performance varies greatly depending on the prompt, with the Chain-of-Thought approach being the most promising one. Zhang et al. [34] incorporated additional information from the source code, such as API call sequences, but found that improvements were limited to specific programming languages. To be fully integrated into developer workflows, LLMs should accurately explain the root causes of vulnerabilities. However, Ullah et al. [29] found that they often provide inaccurate explanations, frequently missing the true root causes when asked to justify their classifications. While previous works have identified limitations in LLMs when asked to identify vulnerabilities in software, in our evaluation, we saw that LLMs can perform well in generating regression test cases from code commit or pull requests, confirming they have some reasoning capabilities."}, {"title": "LLMs for Fuzzing", "content": "Several fuzzers leverage LLMs to enhance fuzzing techniques, but each operates with different goals and methods. ChatAFL [17] uses LLMs to construct grammars for protocol messages, mutate inputs, and generate sequences to improve fuzzing efficiency, distinguishing"}, {"title": "8 Discussion", "content": "Throughout the study, we have shown the significant potential of LLMs in generating structured, human-readable, system-level inputs for regression testing. Our results highlight that even without advanced techniques such as fine-tuning or retrieval-augmented generation (RAG), LLMs, when combined with prompting and execution feedback, are capable of producing high-quality test inputs. These inputs can reflect the semantic meaning inside commits. In some cases, they directly trigger the bugs, and in other cases, they could be \u201crepaired\u201d to trigger the bugs. This points to the potential of LLMs in directed software testing, enabling automated testing workflows that are typically more challenging for conventional tools.\nOne notable observation is the LLM's ability to generate meaningful input without access to the complete program context or compilation databases, which static analysis tools traditionally rely on. This process mirrors human intuition in crafting tests based on high-level semantic understanding. While LLMs may generate inaccurate tests, these inaccuracies are mitigated by real program execution-similar to how dynamic analysis and greybox fuzzing operate. This testing loop can be viewed as a \u201cclever\u201d fuzzer that, while slower (in terms of execs/sec) and more computationally expensive than greybox fuzzers, compensates by generating semantically rich inputs.\nWhile using our tool CLEVEREST standalone has already shown promising results in generating effective test cases in a short time, we believe the value of CLEVEREST shines even more in their subsequent use as a means to an end (i.e., as a tool in a developer's hand). The LLM-generated test cases are easily comprehensible by human developers, as they are structured and human-readable; we demonstrated how human developers can modify these test cases to trigger bugs. They are also an excellent seed generator for regression greybox fuzzers; even a vanilla greybox fuzzer can perform similarly or better than WAFLGo, which requires a user-provided seed corpus.\nThis research suggests that LLMs offer a complementary tool in the evolving landscape of automated software testing. While this study focuses on text-based programs, there is an opportunity to extend CLEVEREST"}]}