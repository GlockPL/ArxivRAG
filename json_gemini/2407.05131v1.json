{"title": "RULE: Reliable Multimodal RAG for Factuality\nin Medical Vision Language Models", "authors": ["Peng Xia", "Kangyu Zhu", "Haoran Li", "Hongtu Zhu", "Yun Li", "Gang Li", "Linjun Zhang", "Huaxiu Yao"], "abstract": "The recent emergence of Medical Large Vi-\nsion Language Models (Med-LVLMs) has en-\nhanced medical diagnosis. However, current\nMed-LVLMs frequently encounter factual is-\nsues, often generating responses that do not\nalign with established medical facts. Retrieval-\nAugmented Generation (RAG), which utilizes\nexternal knowledge, can improve the factual\naccuracy of these models but introduces two\nmajor challenges. First, limited retrieved con-\ntexts might not cover all necessary informa-\ntion, while excessive retrieval can introduce\nirrelevant and inaccurate references, interfer-\ning with the model's generation. Second, in\ncases where the model originally responds\ncorrectly, applying RAG can lead to an over-\nreliance on retrieved contexts, resulting in in-\ncorrect answers. To address these issues, we\npropose RULE, which consists of two com-\nponents. First, we introduce a provably ef-\nfective strategy for controlling factuality risk\nthrough the calibrated selection of the number\nof retrieved contexts. Second, based on sam-\nples where over-reliance on retrieved contexts\nled to errors, we curate a preference dataset\nto fine-tune the model, balancing its depen-\ndence on inherent knowledge and retrieved con-\ntexts for generation. We demonstrate the ef-\nfectiveness of RULE on three medical VQA\ndatasets, achieving an average improvement\nof 20.8% in factual accuracy. We publicly\nrelease our benchmark and code in https:\n//github.com/richard-peng-xia/RULE.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) has showcased its poten-\ntial in medical diagnosis, including disease iden-\ntification, treatment planning, and recommenda-\ntions (T\u0103u\u021ban et al., 2021; Wang et al., 2019; Ye\net al., 2021; Xia et al., 2024b; Li et al., 2024).\nIn particular, the recent development of Medical\nLarge Vision Language Models (Med-LVLMs) has"}, {"title": "2 Preliminaries", "content": "In this section, we will provide a brief overview of\nMed-LVLMs and preference optimization.\nMedical Large Vision Language Models. Med-\nLVLMs connects the LLMs and medical visual\nmodules, enabling the model to use medical im-\nages xv and clinical queries xt as inputs x. This\nallows the model to autoregressively predict the\nprobability distribution of the next token. The text\noutput of Med-LVLMs is denoted as y.\nPreference Optimization. Preference optimiza-\ntion has achieved remarkable results in efficiently\nfine-tuning LLMs, significantly aligning their be-\nhavior with the goals. Typically, give an input x,\na language model policy \\( \\pi_{\\theta} \\) can produce a condi-\ntional distribution \\( \\pi_{\\theta}(y | x) \\) with y as the output\ntext response. The recently popular DPO (Rafailov\net al., 2023) utilizes preference data achieve ob-\njective alignment in LLMs. The preference data\nis defined as \\( \\mathcal{D} = \\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\}_{i=1}^N \\), where \\( y_w^{(i)} \\)\nand \\( y_l^{(i)} \\) represent preferred and dispreferred re-\nsponses given an input prompt x. The probably\nof obtaining each preference pair is \\( p(y_w > y_l) = \\sigma(r(x,y_w)-r(x,y_l)) \\), where \\( \\sigma(\\cdot) \\) is the sigmoid func-\ntion. In DPO, the optimization can be formulated\nas classification loss over the preference data as:\n\\[\\mathcal{L}_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -\\mathbb{E}_{(x,y_w,y_l)\\sim \\mathcal{D}}\\left[\\log \\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)})\\right],\\]\nwhere \\( \\pi_{\\theta} \\) represents the reference policy, which is\nthe LLM fine-tuned through supervised learning."}, {"title": "3 Methodology", "content": "In this section, as illustrated in Figure 2, we will\nintroduce RULE as an efficient solution for improv-"}, {"title": "3.1 Context Retrieval for Reference", "content": "Med-LVLMs often generate non-factual responses\nwhen dealing with complex medical images. RAG\ncan provide the model with external knowledge as a\nreference, thereby effectively enhancing the factual\naccuracy. In the multimodal knowledge retrieval\nstage, RULE retrieves textual descriptions/reports\nthat are most similar to the features of the target\nmedical images. These references contain a wealth\nof image-based medical facts and serve to guide\nthe generation of responses for the medical image.\nFollowing the design of CLIP (Radford et al.,\n2021), the retriever will first encode each image and\nthe corresponding reports into embeddings using\na vision encoder and a text encoder, respectively.\nSpecifically, all medical images \\( X_{img} \\) are encoded\ninto image representations \\( V_{img} \\in \\mathbb{R}^{N\\times P} \\) by a\nvision encoder \\( \\mathcal{E}_{img} \\) (i.e., \\( V_{img} = \\mathcal{E}_{img}(X_{img}) \\)),\nwhere N is the number of medical images that\nneed to be retrieved, and P is the dimension of\nthe embedding. Similarly, we generate text embed-\ndings \\( V_{txt} \\in \\mathbb{R}^{N\\times P} \\) for all corresponding medical\nreports \\( X_{txt} \\) by applying a text encoder \\( \\mathcal{E}_{txt} \\), i.e.,\n\\( V_{txt} = \\mathcal{E}_{txt}(X_{txt}) \\). Subsequently, to adapt the gen-\neral vision and text encoders to the medical domain,\nwe fine-tune the encoders using the training data\nwith a contrastive learning loss, defined as:\n\\[\\mathcal{L} = \\frac{\\mathcal{L}_{img} + \\mathcal{L}_{text}}{2}\\]\n\\[\\mathcal{L}_{img} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(S_{i,i})}{\\sum_{j=1}^{N} \\exp(S_{i,j})}\\]\n\\[\\mathcal{L}_{text} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(S_{i,i})}{\\sum_{j=1}^{N} \\exp(S_{j,i})}\\]\nwhere \\( S \\in \\mathbb{R}^{N\\times N} \\) represents the similarity matrix\nbetween image and text modalities, calculated as:\n\\( S = \\frac{V_{img}}{\\lVert V_{img}\\rVert} (\\frac{V_{txt}}{\\lVert V_{txt}\\rVert})^T \\), where each element \\( S_{i,j} \\)\nrepresents the similarity between the image repre-\nsentation of example i and the text representation\nof example j. Equation (2) aims to learn the repre-\nsentations by maximizing the similarity of text and\nimage modalities representing the same example,\nwhile minimizing the similarity of text and image\nmodalities representing different examples.\nAfter fine-tuning the image and text encoders,"}, {"title": "3.2 Factuality Risk Control Through\nCalibrated Retrieved Context Selection", "content": "For the RAG strategy, the top-3/5 result is typically\nused as a reference (Gao et al., 2023). However, it\nsometimes fails to encompass all relevant retrieved\ncontexts, especially when facing the fine-grained\nfeatures of medical images. Additionally, an exces-\nsive amount of retrieved contexts may introduce\nlow-relevance and inaccurate references, which can\ninterfere with the model's generation. Thus, an\nalgorithm that can automatically determine the op-\ntimal number of retrieved contexts, based on the\nrisk of factual errors, is particularly crucial.\nIn this section, motivated by Angelopoulos et al.\n(2021), we propose the following strategy to choose\na subset \\( \\mathcal{A} \\) for the number of retrievals k from a\ncandidate set \\( \\mathcal{C}_K \\subseteq N \\) such that the factuality risk\nFR(k) can be provably controlled for any \\( k \\in \\mathcal{A} \\). Specifically, first, for each \\( k \\in \\mathcal{C}_K \\), the strategy\nfirst calculates the factuality risk FR(k), computed\nas \\( \\frac{1}{n} \\sum_i ACC(M(x, (q, T_k))) \\), where x denotes the\ntarget medical image, q denotes the question, Tk\nmeans the selected top-K retrieved contexts, and\nACC() measures the ratio of correct answers pro-\nvided by the Med-LVLM M to the total number of\nanswers. Next, two probabilities \\( p_{k1} \\) and \\( p_{k2} \\) are\ncomputed as:\n\\[p_{k1} = \\exp(-nh_1(FR(k) \\wedge a, a)),\\]\n\\[p_{k2} = e \\cdot P(Bin(n, a) < [nFR(k)]),\nwhere \\( h_1(a, b) := a \\log(a/b) + (1 - a) \\log((1 -\na)/(1 - b)) \\) is the Kullback-Leibler divergence be-\ntween two Bernoulli distributions and \\( \\alpha \\) denotes\nrisk upper bound. \\( p_{k2} \\) representing the probabil-\nity that, in a binomial distribution with param-\neters n and a, denoted by \\( Bin(n, a) \\), the ob-\nserved value is less than or equal to \\( [nFR(k)] \\). Then, the minimum of these two probabilities\n\\( p_k = \\min (p_{k1}, p_{k2}) \\) is taken. Finally, we use any\nfamily-wise error rat (FWER)-controlling proce-\ndure, such as Bonferroni correction (Van der Vaart,\n2000) or sequential graphical testing (Bretz et al.,\n2009), to choose A. For example, for Bonferroni\ncorrection, if pk is less than or equal to \\( \\delta/|C_K| \\),\nwhere \u03b4 denotes tolerance level, then k is added\nto the set A. The proposed strategy calculates the\nmodel's factuality risk under different k values,\ncomputes the corresponding probabilities using two\napproaches, and selects those k values that meet\nthe risk tolerance to control the overall factuality\nrisk.\nWe have the following result that ensures with\nprobability at least 1 \u2212 \u03b4, the factuality risk pro-\nduced is controlled by \u03b1.\nProposition 1 Let \\( \\alpha, \\delta \\in (0,1) \\). If the training\ndataset \\( \\mathcal{D}_{Med} = \\{x_i, y_i, q_i\\}_{i=1}^N \\) is i.i.d. and the\noutput of the above algorithm \\( \\mathcal{A} \\neq \\emptyset \\), then\n\\[P_{\\mathcal{D}_{Med}}\\left(\\sup_{k \\in \\mathcal{A}} FR(k) \\leq \\alpha\\right) \\geq 1 - \\delta.\\]\nIn practice, we calibrate the selection of k on the\nvalidation sets of each dataset to minimize factual-\nity risk. Consequently, the optimal k calibrated by\nthis algorithm can be directly used on the test sets."}, {"title": "3.3 Knowledge Balanced Preference Tuning", "content": "In addition to selecting the optimal number k of\nretrieved contexts, it is likely that these contents\noften fail to fully capture the details of every le-\nsion or normal area in medical images. Therefore,\nwhen the retrieved contexts is inaccurate, a reliable\nMed-LVLM is expected to remain unaffected by\nthe unreliable information and independently use\nits own knowledge to answer medical questions.\nHowever, empirically, as illustrated in Table 1, ap-\nproximately half of all incorrect responses by the\nretrieval-augmented Med-LVLM are due to an over-\nreliance on retrieved contexts. This significantly\naffects the application of the retrieval augmented\ngeneration strategy to Med-LVLMs."}, {"title": "4 Experiment", "content": "In this section, we evaluate the performance of\nRULE, aiming to answer the following questions:"}, {"title": "4.1 Experimental Setups", "content": "Implementation Details. We utilize LLaVA-Med-\n1.5 7B (Li et al., 2023) as the backbone model.\nDuring the preference optimization process, we\nadapt LoRA fine-tuning (Hu et al., 2021). For\nthe training of retriever, the vision encoder is a\nResNet-50 (He et al., 2016), and the text encoder\nis a bio-BioClinicalBERT (Alsentzer et al., 2019).\nWe use the AdamW optimizer with a learning rate\nof 10-3, weight decay of 10-2 and a batch size of\n32. The model is trained for 360 epochs. For more\ndetailed information on training hyperparameters\nand training data, please see Appendix A and C.\nBaselines. We compare RULE with LVLM hal-\nlucination mitigation methods that have already\nshown promising results in natural images, includ-\ning Greedy Decoding, Beam Search (Sutskever\net al., 2014), DoLa (Chuang et al., 2023),\nOPERA (Huang et al., 2023), VCD (Leng et al.,\n2023). These methods manipulate the logits of the\nmodel's output tokens to enhance factual accuracy.\nFurthermore, we compare the performance with\nother open-source Med-LVLMs, including Med-\nFlamingo (Moor et al., 2023), MedVInT (Zhang\net al., 2023), RadFM (Wu et al., 2023).\nEvaluation Datasets. To ensure that the retrieved\nreport content is relevant to the visual question-\nanswering content and to facilitate experimentation,\nwe utilize three medical vision-language datasets,\ni.e., MIMIC-CXR (Johnson et al., 2019), IU-\nXray (Demner-Fushman et al., 2016), and Harvard-\nFairVLMed (Luo et al., 2024), encompassing radi-\nology and ophthalmology. The training set is split\ninto two parts: one part is used to train the retriever\n(Section 3.1), and the other part is used to construct\nthe preference dataset for KBPT (Section 3.3).\nAdditionally, we construct VQA pairs for KBPT\nand evaluation. Specifically, the reports from train-\ning set for preference dataset and reports from orig-\ninal test set are input into GPT-4 (OpenAI, 2023)\nto create closed-ended VQA data with yes or no an-"}, {"title": "4.2 Results", "content": "In this section, we provide comprehensive compar-\nison results with different baseline methods and\nother open-sourced Med-LVLMs.\nComparison with Baseline Methods. We present\nthe results of a comparison between RULE and\nvarious hallucination reduction methods in Table 2.\nAccording to these results, RULE demonstrates\nthe best overall performance, effectively and accu-\nrately diagnosing diseases with an average accu-\nracy improvement of 20.8% across all datasets. We\nalso observe that RULE performs notably better on\nthe IU-Xray and Harvard-FairVLMed compared\nto MIMIC-CXR. This difference is attributed to\nthe excessive length of the reports available for\nretrieval in MIMIC-CXR, where overly long refer-\nences tend to confuse the Med-LVLM. In addition,\neven when dealing with the relatively niche oph-\nthalmology data (i.e., Harvard-FairVLMed), RULE\ndemonstrates superior results, significantly enhanc-\ning the factual accuracy of the Med-LVLM. In con-\ntrast, the performance of decoding methods is quite\nunstable, showing significant rates of missed or\nincorrect diagnoses across different datasets, as in-\ndicated by the precision and recall values.\nComparison with Other Med-LVLMs. In Ta-\nble 3, we present the comparison with different\nopen-sourced Med-LVLMs. RULE demonstrates\nstate-of-the-art (SOTA) performance across all\ndatasets. Although the second-best model, Med-\nVInT, outperforms other models, RULE achieves\nan average accuracy improvement of 47.4% over it.\nWhether in radiology or ophthalmology, RULE\ndemonstrates remarkable performance, signifi-\ncantly surpassing other open-source Med-LVLMs.\nThis indicates that RULE is generally applicable\nand effective in the medical multimodal diagnosis,\nproviding consistent improvements across various\nmedical image modalities."}, {"title": "4.3 How Does RULE Improve the\nPerformance?", "content": "In this section, we conduct a set of analyses demon-\nstrate how different components contribute to the\nperformance and illustrate how RULE enhances\noverall performance, which are details as follows:\nAblation Studies. To further illustrate the effec-\ntiveness of the components of RULE, we conduct\nablation experiments on three datasets. The results\nare shown in Table 4. We find that the basic RAG\nstrategy (\"R\") slightly improves factual accuracy on\ntwo datasets but decreases it on MIMIC-CXR. The\nlimited retrieved contexts can not cover the fine-\ngrained features of medical images, resulting in\nunstable factual accuracy improvements. With the\naid of the factuality risk control strategy (\"FRC\"),\nretrieval performance see a stable increase, out-\nperforming the original Med-LVLM. Considering\nthe model's over-reliance on retrieved contexts, the\nknowledge balanced preference tuning (\"\u041a\u0412\u0420\u0422\")"}, {"title": "4.4 Compatibility Analysis", "content": "To demonstrate the compatibility of RULE, we\nconduct KBPT on LLaVA-Med-1.0 as well. The\nexperimental results on three datasets are shown\nin Figure 4. We find that our knowledge balanced\npreference tuning method demonstrates good com-\npatibility across different models, significantly im-\nproving factual accuracy across multiple datasets.\nBased on LLaVA-Med-1.0, RULE increases accu-\nracy by an average of 16.7%. This indicates that\nRULE has a noticeable positive effect on mitigating\nover-reliance on retrieved contexts, thereby enhanc-\ning the Med-LVLM's factual accuracy."}, {"title": "4.5 Case Study", "content": "Figure 5 presents two representative case results,\ndemonstrating that RULE can effectively enhance\nthe factual accuracy of med-LVLMs. In case 1,\nLLaVA-Med provides a factually incorrect answer.\nAfter applying the RAG strategy, the model still\nexhibits factual issues, whereas our method effec-\ntively addresses this and improves accuracy. In\ncase 2, LLaVA-Med initially provides a correct\nanswer, but due to the model's over-reliance on\nretrieved contexts, it subsequently produces an in-\ncorrect response. RULE balances the weight of\ninherent knowledge and retrieved contexts, enhanc-\ning factual accuracy."}, {"title": "5 Related Work", "content": "Factuality in Med-LVLMs. The rapid devel-\nopment of Large Vision and Language Models\n(LVLMs) (Liu et al., 2023b,a; Zhu et al., 2023;\nAlayrac et al., 2022; Zhou et al., 2024a,b) has be-\ngun to impact medical diagnosis. A series of Med-\nLVLMs (Li et al., 2023; Moor et al., 2023; Wu et al.,\n2023; Zhang et al., 2023), represented by LLaVA-\nMed, have emerged, demonstrating impressive per-\nformance across various medical image modali-\nties. However, Med-LVLMs still exhibit significant\nfactual errors, producing medical responses that\nconflict with the visual medical information. This\ncould potentially lead to misdiagnoses or missed\ndiagnoses. Recently, several benchmarks (Royer\net al., 2024; Xia et al., 2024a) have been established\nto evaluate the accuracy of Med-LVLMs in tasks\nsuch as VQA or report generation. Beyond evalu-\nating factuality, improving the factual accuracy of\nMed-LVLMs remains an underexplored area.\nRetrieval Augmented Generation. RAG has\nrecently been recognized as a promising solu-\ntion (Gao et al., 2023). It enhances the model's\nability to generate accurate facts by incorporating\ncontextual information from external datasets. In\nmedical multimodal analysis, the RAG approach\nhas been applied to various tasks such as medi-\ncal VQA (Yuan et al., 2023) and report genera-\ntion (Kumar and Marttinen, 2024; Tao et al., 2024;\nHe et al., 2024). However, in Med-LVLMs, ap-\nplying RAG-based approaches overlook two crit-\nical issues: the number of retrieved contexts and\nwhether the model overly relies on these reference.\nThese factors can significantly affect the model's\nperformance and may even degrade it. In RULE,\nwe systematically address these challenges and en-\nhance the factuality of Med-LVLMs."}, {"title": "6 Conclusion", "content": "In this work, we aim to enhance the factuality of\nMed-LVLM by addressing two key challenges in\nmedical RAG. Specifically, we first introduce a\nprovably effective strategy for controlling factu-\nality risk through the calibrated selection of re-\ntrieved contexts. Second, we develop a preference\noptimization strategy that addresses errors stem-\nming from the model's excessive dependence on\nretrieved contexts, aiming to balance its intrinsic\nknowledge and the retrieved information. Experi-\nments on three medical imaging analysis datasets\ndemonstrate the effectiveness of RULE."}, {"title": "Limitations", "content": "This work explores a reliable multimodal RAG\nmethod for Med-LVLMs to enhance factual accu-\nracy. Our primary focus is on factual accuracy.\nFuture research can explore other issues related to\ndeploying Med-LVLMs in clinical settings, such as\nsafety, fairness, robustness, and privacy."}, {"title": "Acknowledgement", "content": "This research was supported by Cisco Faculty Re-\nsearch Award."}, {"title": "A Data", "content": "A.1 Data statistics\nThe quantities of all the data used are shown in\nTable 6 and Table 7. It is notable to note that for\ntraining the retriever, this refers to the number of\nimage-text pairs; for fine-tuning, it refers to the\nnumber of QA items. \u201cAll\" represents the total\nquantity used to construct the preference dataset,\nwhere only the samples with correct original an-\nswers that become incorrect after adding retrieved\ncontexts are included in the training of knowledge\nbalanced preference tuning (\"\u041a\u0412\u0420\u0422\")."}, {"title": "A.2 Instructions", "content": "We convert the medical reports into a series of\nclosed-ended questions with yes or no answers. To\nensure the quality of the VQA data, we perform a\nround of self-checks using GPT-4 (OpenAI, 2023).\nFinally, we conduct an round of manual filtering\nto remove questions with obvious issues or those\nrelated to multiple images or patient histories. The\nprompt templates used are shown in Table 8."}, {"title": "A.3 Involved Datasets", "content": "We utilize three open-source medical vision-\nlanguage datasets, i.e., MIMIC-CXR (Johnson\net al., 2019), IU-Xray (Demner-Fushman et al.,\n2016), Harvard-FairVLMed (Luo et al., 2024).\n\u2022 MIMIC-CXR (Johnson et al., 2019) is a large\npublicly available dataset of chest X-ray images"}, {"title": "B Evaluated Models", "content": "We evaluate four open-source Med-LVLMs,\ni.e., LLaVA-Med (Li et al., 2023), Med-\nFlamingo (Moor et al., 2023), MedVInT (Zhang\net al., 2023), RadFM (Wu et al., 2023). The se-\nlected models are all at the 7B level.\n\u2022 LLaVA-Med (Li et al., 2023) is a vision-language\nconversational assistant, adapting the general-\ndomain LLaVA (Liu et al., 2023b) model for\nthe biomedical field. The model is fine-tuned\nusing a novel curriculum learning method, which\nincludes two stages: aligning biomedical vocabu-\nlary with figure-caption pairs and mastering open-\nended conversational semantics. It demonstrates\nexcellent multimodal conversational capabilities.\n\u2022 Med-Flamingo (Moor et al., 2023) is a mul-\ntimodal few-shot learner designed for the\nmedical domain. It builds upon the Open-\nFlamingo (Alayrac et al., 2022) model, contin-\nuing pre-training with medical image-text data\nfrom publications and textbooks. This model"}, {"title": "C Implementation Details", "content": "Following the settings of CLIP (Radford et al.,\n2021), we adopt the same architecture and hy-\nperparameters for the vision and text encoders.\nThe vision encoder is a ResNet-50 (He et al.,\n2016), and the text encoder is a bio-bert-based\nmodel (Alsentzer et al., 2019). We use the AdamW\noptimizer with a learning rate of 10-3, weight de-\ncay of 10-2 and a batch size of 32. The model\nis trained for 360 epochs. The reports available\nfor retrieval are from the training set of the corre-\nsponding dataset. In our experiments, we apply\ncross-validation to tune all hyperparameters with\ngrid search. All the experiments are implemented\non PyTorch 2.1.2 using four NVIDIA RTX A6000\nGPUs. It takes roughly 2.5 and 4 hours for fine-\ntuning CLIP and LLaVA-Med-1.5 7B, respectively."}, {"title": "D Proofs", "content": "Proof of Proposition 1: According to the definition,\nM(,) denotes the Med-LVLM. {T}1 denotes\nthe topk retrieved contexts. The dataset is DMed =\n\\{x_i, y_i, q_i\\}_{i=1}^N, where \\( x_i \\) is the target image, \\( y_i \\) is\nthe ground-truth answer, \\( q_i \\) is the target question.\nBy the definition of FR(k),\n\\[FR(k) =1 \u2013 ACC(M(x, (q, \\{T_k\\}_{i=1}^N)))\\]\n\\[=1-\\frac{1}{N} \\sum_{i=1}^N \\mathbb{1}\\{M(x_i, (q_i, \\{T_k\\}_{i=1}^N)) = y_i\\}\\]\n\\[=\\frac{1}{N} \\sum_{i=1}^N (1-\\mathbb{1}\\{M(x_i, (q_i, \\{T_k\\}_{i=1}^N)) = y_i\\})\\]\nTherefore, FR(k) can be written as the average\nvalue of a function evaluated at each data point\n\\((x_i, y_i, q_i)\\) in DMed. Then, by combining Theorem\n1, Proposition 1 and Proposition 2 of (Angelopou-\nlos et al., 2021), we finish the proof."}]}