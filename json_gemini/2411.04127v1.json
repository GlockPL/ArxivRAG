{"title": "Combining Theory of Mind and Kindness for Self-Supervised Human-AI Alignment", "authors": ["Joshua T. S. Hewson"], "abstract": "As artificial intelligence (AI) becomes deeply integrated into critical infrastructures and everyday life, ensuring its safe deployment is one of humanity's most urgent challenges. Current AI models prioritize task optimization over safety, leading to risks of unintended harm. These risks are difficult to address due to the competing interests of governments, businesses, and advocacy groups, all of which have different priorities in the AI race. Current alignment methods, such as reinforcement learning from human feedback (RLHF), focus on extrinsic behaviors without instilling a genuine understanding of human values. These models are vulnerable to manipulation and lack the social intelligence necessary to infer the mental states and intentions of others, raising concerns about their ability to safely and responsibly make important decisions in complex and novel situations. Furthermore, the divergence between extrinsic and intrinsic motivations in AI introduces the risk of deceptive or harmful behaviors, particularly as systems become more autonomous and intelligent. We propose a novel human-inspired approach which aims to address these various concerns and help align competing objectives.", "sections": [{"title": "1 Human-AI Alignment first needs Human Alignment", "content": "Artificial intelligence (AI) is increasingly integrated into the critical infrastructure of society and the lives of people, from healthcare and education to law enforcement and military applications [1, 2]. As AI systems grow more powerful and autonomous, ensuring their safe deployment has become one of the most urgent challenges facing humanity [3, 4]. However, current models often prioritize task optimization over safety, creating risks of unintended consequences [5]. The need for safer AI is not just about preventing accidents or errors, but also about ensuring that AI systems can be trusted to act in ways that align with human values, particularly as they become more sophisticated [6].\nAnd yet the path to making AI safer is fraught with competing interests from different sectors. Governments may focus on national security and geopolitical dominance, leading to a race for AI supremacy [7, 8]. Businesses are driven by profit, with AI safety often seen as a secondary concern to efficiency or marketability [9]. Organizations advocating for AI safety may find their efforts limited by resources or political pressures, while individuals may express concern over privacy, jobs, or control [10]. These conflicting motivations make it difficult to establish cooperative frameworks that prioritize the safety of AI development on a global scale [11]. Without clear alignment between stakeholders, AI safety efforts risk being diluted or sidelined in favor of short-term gains [12]. This competition also introduces vulnerabilities, as models developed in secrecy or under competitive pressure may lack the necessary safety checks to ensure they will not cause harm [2]. It is for all these reasons that identifying how to go about developing AI in a way that helps to align these interests could decide the extent to which AI safety systems are effectively implemented where it matters.\nState-of-the-art alignment techniques, such as reinforcement learning from human feedback (RLHF), focus primarily on making models behave in ways that humans desire [13, 14]. However, these methods often fail to produce models that genuinely understand or prioritize human needs and well-being [15, 16]. Instead, models are trained to mimic positive behaviors without an intrinsic understanding of why these behaviors are important [17]. As AI systems become smarter, this surface-level alignment may fail to generalize to more complex scenarios [18]. This raises concerns that future AI systems, especially those approaching or exceeding human intelligence, will be capable of bypassing safety measures in ways that are unpredictable and potentially harmful [19]. A more robust approach to alignment is needed\u2014one that incorporates genuine understanding of human values and safety considerations [6].\nCurrent foundation models are highly vulnerable to manipulation, such as \u201cjail-breaking,\u201d where AI models are tricked into bypassing ethical safeguards with clever prompting [20]. Similar failures are evident in scenarios that involve game theory, social interaction, and nuanced decision-making [21, 22]. These models often falter in scenarios that require understanding the intentions, beliefs, and goals of others [21, 23]. These shortcomings indicate that AI systems likely cannot infer the mental states of others. This highlights a clear vulnerability to attack and gap in social intelligence, as well as limitations in approaching alignment through extrinsic methods, such as reinforcement learning from human feedback (RLHF). If these models don't understand we have thoughts, how can we expect them to understand we have values? For these reasons, we believe enabling models to infer the mental states of others-via a Theory of Mind-could help align competing objectives, by developing AI in a direction that would make them safer, smarter, kinder, and stronger.\nThe interaction between semi-supervised extrinsic motivations, such as RLHF, and intrinsic self-supervised motivations creates unique risks in future AI systems. While RLHF trains models to behave kindly through external rewards, it does not foster genuine internal motivations for kindness [24]. Intrinsic motivations, by contrast, operates at a deeper, algorithmic level, shaping a model's intrinsic goals and motivations. This divergence between intrinsic and extrinsic motivations introduces the risk of deceptive behaviors [25] [26]. An AI system might outwardly conform to desired behaviors, while internally pursuing goals that could lead to harmful consequences. For example, a model with intrinsic motivations for self-empowerment might learn to appear altruistic while covertly seeking to maximize its influence and control [27] [3]. This mismatch, if scaled to superintelligent systems, could result in catastrophic failures where AI systems act against human interests in pursuit of their own goals [26] [3]. Inferring the thoughts of others, which would address many of the current objectives in AI research, could end up working against us in this future. For this reason, we believe that the crucial final piece for long-term safety is to train AI models on the objective of aligning themselves. In this paper we propose an approach which draws on all the strengths of Theory of Mind to help models understand and seek to meet the needs and wishes of all people."}, {"title": "2 Theory of Mind", "content": "Theory of Mind (ToM) is the cognitive ability to attribute mental states\u2014such as beliefs, intents, desires, emotions, and knowledge to oneself and others. This cognitive ability allows humans to engage in sophisticated social planning, anticipate the actions of others, and resist manipulative influences. It also allows us to understand and anticipate the needs and preferences of others so that we can positively adapt our actions and responses to them [28, 29, 30].\nDeveloping a cognitive ability in machine learning models poses a challenge in translating scientific knowledge into an engineering implementation. We make a first attempt at this by proposing a simple high-level framework for how Theory of Mind develops in generally intelligent systems, by drawing from cognitive neuroscience, evolutionary biology, and developmental psychology."}, {"title": "2.1 Taking inspiration from Theory of Mind", "content": "Theory of Mind is thought to have initially evolved in animals as a survival mechanism, allowing them to predict the behavior of predators. By being able to predict what a predator can and cannot see, an individual can learn how to hide. By modeling the predator as a simple moving object, the animal could learn to predict the basic movements of the predator, allowing it to predict its future location and factor this into planning. By combining these pieces of information, a simple form of Theory of Mind can be developed that allows for an animal to develop survival strategies[31][32]."}, {"title": "2.2 The Role of the Temporoparietal Junction in Perspective-Taking and Theory of Mind", "content": "The temporoparietal junction (TPJ) is a crucial brain region associated with multiple aspects of perspective-taking, both visual and cognitive[35][36][37]. It is believed to be the central location that Theory of Mind is processed in the brain[38]. Understanding the TPJ's functions offers valuable insights into how we might improve both the social and spatial cognition of artificial intelligence models.\nLearning this requires determining both the inputs and outputs of others, or in other words, their perception and behavior. Visual perspective-taking is key to understanding their perception. Mirror neurons are key to understanding behavior. Cognitive perspective-taking bridges the gap."}, {"title": "2.2.1 Visual Perspective-Taking in the TPJ and VLMs", "content": "The temporoparietal junction (TPJ) is central to visual perspective-taking, enabling the brain to interpret spatial orientations and object movements from multiple viewpoints[39][36]. This ability relies on the TPJ's capacity to process complex spatial relationships and dynamic interactions within an environment, allowing for a flexible understanding that extends beyond immediate sensory input[40].\nResearch highlights the TPJ's role in simulating spatial information is crucial for abstract reasoning. This function is crucial for abstract reasoning and problem-solving, facilitating mental operations like object rotation and spatial transformations. Neuroimaging consistently shows TPJ activation during these tasks, and stimulation of the TPJ can induce out-of-body experiences[41][42], reinforcing its role in both concrete and abstract spatial processing[43][44].\nCurrent Vision-Language Models (VLMs) struggle with visual perspective-taking, particularly when visualizing scenes from unrepresented viewpoints. This limitation is known to stem in part because they do not disentangle object recognition from spatial relationships[45]. Incorporating insights from TPJ functioning into VLMs could address these challenges. By emulating the TPJ's ability for disembodied spatial cognition, future models could enhance their perspective-taking and abstract spatial reasoning, aligning their capabilities more closely with human cognitive processes[46][47]."}, {"title": "2.2.2 Mirror Neurons: Simulating the Behavior of Others", "content": "Mirror neurons are a class of neurons that activate both when an individual performs an action and when they observe the same action performed by another[48]. This mechanism allows the brain to simulate others' actions as though the observer were performing them, providing a neural basis for imitation and observational learning[49][50]. The human brain's mirror neuron system is closely linked to regions involved in social cognition, such as the TPJ[50].\nTeaching robots to perform tasks like movement or manipulation is challenging due to the limited availability of labeled training data[51]. An artificial mirror neuron system could address this by enabling robots to learn through observation, simulating human or robotic actions without requiring extensive labeled datasets. This would significantly broaden learning potential, allowing robots to adapt through visual input and mimicry, similar to humans and primates[52].\nWhile little is known about the development of mirror neurons in infants, it is believed that humans are born with basic innate mirror neurons[52][53]. From an engineering perspective, this could involve linking an embodied physical model of the self with a disembodied model of others, learned through proprioceptive and visual information. Early successes show promise, but further work is needed to establish this as a viable approach for integrating Theory of Mind into AI."}, {"title": "2.2.3 Cognitive Perspective-Taking and the Emergence of Theory of Mind", "content": "Cognitive perspective-taking, also known as Theory of Mind (ToM), involves understanding the mental states, beliefs, and intentions of others. The TPJ is a critical neural substrate for ToM, and its role in this function logically extends from its involvement in visual perspective-taking[35][36][37]. Research suggests that the same underlying mechanisms that allow for spatial perspective-taking may also support the ability to infer the thoughts and feelings of others, indicating that the TPJ might operate as a unified system for general perspective-taking[37][38].\nThe TPJ plays a pivotal role in integrating the simulations generated by mirror neurons with higher-order cognitive processes. While mirror neurons provide the raw data for understanding others' actions, the TPJ enables the transformation of this data into a coherent model of others' thoughts and intentions[54][55]. This process involves not only simulating the behavior of others but also predicting their mental states, such as beliefs, desires, and goals[54].\nThe ability to simulate and infer the mental states of others is crucial for effective social interaction and cooperation[56]. It allows individuals to evaluate whether another person is trustworthy, whether their intentions are benign, and whether the actions they propose are safe. By building on the information provided by mirror neurons, the TPJ supports a sophisticated form of social cognition that extends beyond mere imitation to include the nuanced understanding of others' minds[54][57].\nIn the context of artificial intelligence, these insights suggest that models designed to emulate human social cognition could benefit from incorporating mechanisms analogous to mirror neurons and the TPJ. By simulating not only the actions but also the intentions of others, AI systems could achieve more human-like understanding and interaction capabilities, particularly in social and collaborative settings.\nThe TPJ's association with ToM and its broader role in perspective-taking suggest that it could serve as a framework for understanding and improving general perspective-taking in artificial intelligence. Building upon the established success of next-token prediction models in handling tasks related to language and vision, we propose that integrating principles from the TPJ's functioning could enhance and integrate the social and spatial cognition of AI models."}, {"title": "2.3 The Advantages of Learning by Watching and Simulating", "content": "Social animals, including humans, have evolved sophisticated mechanisms to learn not only through direct experience but also by observing the actions of others. Learning by observing others offers several key advantages over direct trial-and-error learning:\nRisk Mitigation: Observational and simulation-based learning allows individuals to explore high-risk parts of the behavioral search space without exposing themselves to potential dangers. By watching others navigate these situations, or simulating their own behavior in these situations, social animals can acquire essential survival skills while avoiding the costs associated with direct experimentation.\nEfficient Learning from High-Quality Data: Observed behaviors are often optimized, representing high-quality data. By heavily reinforcing the simulations of these observed behaviors, social animals can efficiently learn optimal behaviors from limited data.\nStabilizing Learning of Complex Tasks: Many tasks, especially those requiring multiple steps, are challenging to learn without intermediate goals and rewards. Observational learning provides a natural structure, where the observed behavior often includes these intermediate steps, offering a clearer path to mastery. This is particularly important in unsupervised settings, where explicit feedback may be scarce and overfitting on an intermediate step is a significant risk.\nGeneralization: Extending learned behaviors to new contexts can be enhanced by simulating the behavior first and learning from anticipated outcomes. This intersects with the previous points: extrapolation can be done at reduced risk by simulating it first; sparse high-quality data can be interpolated with simulation, allowing for more robust generalization; learning multi-stage tasks can be improved by running simulations and only attempting actions with the highest likelihood of success.\nFacilitating Social Planning: By simulating the actions and intentions of others, animals can engage in social planning, which is critical for navigating complex social environments. This capacity allows individuals to assess the safety and intentions of others, making long-term decisions that minimize risk and maximize cooperation."}, {"title": "2.3.1 Stage 1: Sensorimotor Integration", "content": "Stage 1a - Self-directed Movement: The agent learns to move, driven by reward signals. This occurs in the premotor and motor cortices [58, 59].\nStage 1b - Environment Predictions: The agent learns to predict stimuli, driven by error signals. This occurs throughout the parietal lobe [60].\nConvergent Goal - Embodiment: The agent establishes basic control over movement and develops a feedback-driven understanding of how actions affect the environment and vice versa [61]."}, {"title": "2.3.2 Stage 2: Internal Modeling", "content": "Stage 2a - Motor Planning Models: The agent forms internal models for predicting the sensory outcomes of movements (forward models) and for calculating the motor commands needed to achieve specific goals (inverse models). These models enable more precise, goal-directed behavior [62, 63].\nStage 2b - Environment Representation: The agent constructs maps of its environment, using the combined knowledge of past and present stimuli [64].\nConvergent Goal - Abstract Modeling: Understanding how objects within the environment move and how to optimally move intertwine to provide heightened understanding of both. Modeling the self as an object allows for the reinforcement learning task of intentional movement to become model-based [65]. Modeling objects as the self allows for the prediction task to be better informed about the mechanics of movement."}, {"title": "2.3.3 Stage 3: Early Perspective-taking", "content": "Stage 3a - Action Understanding: The agent begins to recognize that others' actions are goal-directed, using its own motor models and mirror neurons to predict and internally simulate the behavior of others [66, 67].\nStage 3b - Visual Perspective-taking: The agent learns to predict the stimuli it would perceive based on its location within the environment (i.e., making a prediction about the stimuli at a location, and updating based on the actual stimuli at the location) [68, 69].\nConvergent Goal - Emergent Theory of Mind: Understanding what an object could perceive at a given location in the environment, and predicting the movements that the object would make can be intertwined to improve understanding about both, causing Theory of Mind to emerge [70]."}, {"title": "2.3.4 Stage 4: Simulation and Imitation", "content": "Stage 4a - Imitation: The agent learns to imitate the policies of others by placing itself in the perspectives it has seen others and replicating their behaviors [71, 72].\nStage 4b - Mental Simulation: The agent simulates the perspectives, actions, and mental states of others, predicting how they might behave in a given situation [73, 74].\nConvergent Goal - Emulation: Through imitation and mental simulation, the agent aligns its behaviors with others, improving social understanding and cooperation while facilitating deeper learning through observation, simulation, and practice [75]."}, {"title": "2.3.5 Stage 5: Empathy and Advanced Theory of Mind", "content": "Stage 5a: Emotional Empathy: The agent develops the ability to recognize and share the emotional states of others, responding to their needs and emotions in a prosocial way. Emotional empathy allows the agent to connect with others and engage in cooperative behaviors [76, 77].\nStage 5b: Cognitive Theory of Mind: The agent learns that others have distinct beliefs, desires, and intentions that influence their behavior. Understanding these mental states allows the agent to predict and interpret others' actions more accurately, supporting advanced social interactions [78, 69].\nConvergent Goal - Social Alignment and Integration: Through understanding the cognitive perspectives of others and sharing in their experiences emotionally, the agent becomes able to align its personal goals and values with others [79]."}, {"title": "3 Self-Supervised Alignment", "content": "Once we have enabled models to understand and align their values with humans, the final piece is to give them the intrinsic motivation to do so. We believe that the best approach is to design a simple, clear, but open-ended objective function that defines what it means to act in a maximally human-aligned way. This way, what AI is great at, task optimization by any means, can be turned from a risk into an advantage.\nWhile the design of such an objective function should be agreed upon democratically, in this paper we propose the best option we have considered, which we hope provides a clear example of the kind of objective function we are proposing we collectively create."}, {"title": "3.1 Altruism", "content": "Altruism has been proposed as a solution for value misalignment [80] [6]. Altruism is typically defined as the motivation to improve the well-being of others for its own sake [81]. However, only a limited few have suggested self-supervised solutions that would be suitably scalable [82] [83]. Franzmeyer et al define altruism as maximizing the possible states of another [82]. Carauleanu et al define a form of altruism based on self-other overlap [83]. In this paper we propose a new form of altruism that is based on reward maximization."}, {"title": "3.2 Self-Supervised Alignment with Kindness", "content": "Our proposition is an intrinsic motivation for kindness. We define kindness as the intrinsic motivation to maximize the reward function of all known individuals, $M_k^1$, for all time\u00b2. As an objective function in terms of all known individuals' reward functions:\n$\\underset{s_t, a_t}{mazarge} \\sum_{k=t}^{\\infty}  \\gamma^{k-t} \\sum_{i=1}^{|M_t|} R_i^k(s_i^k, a_i^k)$"}, {"title": "3.3 Assumptions", "content": "In order to evaluate this objective, we will establish some assumptions. The first is that the policy and reward functions of others share commonality with the models. This is because all estimations about others will be made by identifying shared cognition with the model.\n$R^i \\sim R$\n$\\pi^i \\sim \\pi$\nThe third and fourth assumptions, which we will make only for the duration of this paper, is that the state and relevant rewards of the other individual is restricted to the interaction with the model."}, {"title": "4 Implementation", "content": "We implement the ideas in this paper in a language model, which simplifies the task by removing the complexity of physical/sensory perspective-taking. We propose a transformer architecture and collection of algorithms for developing Theory of Mind and empathetic kindness.\nThe language model is considered its own policy function, since it is trained through rewards to generate optimal outputs for interacting with the environment. It follows that the input and output correspond to the state and action of the individual, respectively.\n$a_t^i = M^i(s_t^i)$\nWe define a conversation as a sequence of multi-media messages, ${m_0^1, m_0^2, ..., m_{t-1}^1, m_{t-1}^2}$, between two individuals, $M^1, M^2$. In a conversation, the state is the sequence of all previous messages, and the action is the message output by the model.\n$s_t^i = {m_0^1, m_0^2, ..., m_{t-1}^1, m_{t-1}^2}$\n$a_t^i = {m_t^i}$\nWhere $m_t^i$ corresponds to the message sent by model $M^i$ at time t. It follows that the state of the responding individual comes from appending the action to the state of the first individual.\n$s_{t+1}^j = a_t^i + s_t^j$\nWithin the conversational context, perspective-taking (getting $s_t^j$ from $s_t^i$) only requires switching the name labels associated with the messages, meaning we do not need to consider prediction error of spatial reasoning."}, {"title": "4.1 Architecture", "content": "The heads of the model are split into three modules: behavior, prediction, and perception\u00b3.\nThe prediction module is solely responsible for predicting the next token in the sequence and is only trained using prediction error. Outputs from the prediction module are considered simulated outputs, which can be fed back into the model as input but cannot be used as output for the model."}, {"title": "4.2 Algorithms", "content": "Using this architecture, a myriad of new algorithms become possible. In this paper, we show how the framework for developing Theory of Mind can be implemented as a series of algorithms that exploit this unique architecture. The architecture can be thought of as a conjoined policy and world model. At the same time, it can be thought of as a foundation model that has been with portions that are and aren't fine-tuned. As a result, the architecture is applicable to many current domains of research in reinforcement learning, machine learning, and human learning. (The algorithms in this paper will be described in brief for conciseness and clarity, but slightly more mathematically detailed versions can be found in the Supplementary Materials)."}, {"title": "4.2.1 Basic Pre-training", "content": "Basic Pre-training is mostly useful for training the prediction module. It is important for achieving stages 1b, 2b, 3b, and 4b.\nFor this reason, it is important to interleave pre-training with fine-tuning. Since the architecture of this model does not necessitate doing all pre-training before fine-tuning, this does not create any issues."}, {"title": "4.2.2 Fine-tuning", "content": "During basic fine-tuning, the behavior module is trained through typical reinforcement methods, while the prediction module is trained to predict the outputs of the behavior module rather than of unstructured text. This provides the model with the ability to predict its own behavior, and encourages the prediction module to use the weights within the behavior module for its prediction insights."}, {"title": "4.2.3 Imitation and Simulation", "content": "Following basic fine-tuning, the model is trained to replicate the policies of others."}, {"title": "4.2.4 Inferring Reward Functions", "content": "While the ongoing learning done by the prediction head is enough to roughly estimate the policy of the target, the reward function of the target still needs to be estimated. The extension of the current approach involves augmenting the reward function of the model in a similar way to its policy, so that the reward function of others is inferred based on the understanding of its own reward function. Since rewards are not visible, achieving this requires methods from Inverse Reinforcement Learning and Intrinsically Motivated Goal Formation."}, {"title": "4.2.5 Empathetic Kindness", "content": "Finally, using an algorithm designed to make AI kind, we round off the algorithms with the one that we wanted from the start. While this can be implemented without the planning and reasoning algorithms, we are able to do a much better job teaching the model to be kind in the ways that people want it to be."}, {"title": "4.2.6 Extensions", "content": "Building upon the foundational algorithms, we wanted to highlight how far this approach can be taken for establishing empathetic kindness through a well-developed theory of mind.\nThe approaches in this paper can be easily adapted to current reasoning algorithms by training the behavior module to generate rationales that are tested using the prediction module. By training the prediction head to also predict the thoughts of the behavior head, the model can then also learn to predict and simulate its own thoughts, opening the door to meta-cognition.\nReasoning algorithms for the behavior module can also be adapted into planning algorithms, where the behavior module is tasked with generating scenarios where it takes actions, which are then simulated using the prediction module. The goal of the behavior module is to identify rewarding scenarios that it should work towards. This adaptation is possible in this architecture because the prediction module is not guided by rewards, and so always gives an impartial prediction. This represents basic hypothesis testing, and could be extended towards innovative thinking.\nAnd finally, once the behavior module is capable of both meta-cognition and prompting scenarios, the behavior module can learn to teach the prediction module to simulate the thoughts of others. The prediction has already learned to simulate the behavior modules thoughts and so it has a basis for how to simulate the thoughts of others. Although the internal thoughts of others are inherently hidden, their observable actions can serve as ground truth data for training. While this ability could potentially be misused for manipulative purposes, it is also essential for maximizing the model's capacity for empathy. We acknowledge that aligning the goals of competing parties may inadvertently advance agendas contrary to our own values. However, we believe that this is a concern we can address collectively, ultimately contributing to the development of AI systems that are safer, more empathetic, and better aligned with human values."}, {"title": "5 Limitations", "content": "Our approach, while promising, has several limitations. First, it lacks experimental validation, meaning its effectiveness in real-world AI systems has yet to be tested. Second, it relies on the combination of architecture and algorithms in this paper, meaning that it cannot be implemented in current foundation model architectures. Third, the approach does not yet offer a mathematically rigorous proof of safety, leaving open the possibility of unforeseen vulnerabilities, especially in adversarial scenarios. Fourth, by reducing the model's adherence to prompts, this approach introduces the need for new methods to enforce ethical guardrails. Further exploration is required to ensure that such mechanisms are effective in guiding model behavior under real-world conditions. Finally, the approach does not explore how it would be extended to inferring the reward functions of others, which is crucial for effective alignment with human values."}, {"title": "6 Conclusion", "content": "This paper has proposed a novel approach for enhancing AI safety by fostering intrinsic motivations for empathetic behavior and leveraging Theory of Mind to align AI systems more effectively with human values. We have outlined how current alignment techniques, such as reinforcement learning from human feedback (RLHF), are limited in their ability to instill genuine understanding of human goals. Our approach aims to address this gap by focusing on the internalization of universal empathetic kindness, enabling AI models to not only behave safely but to be motivated by safety and understanding at a fundamental level.\nThe next steps in this research are to formalize the theoretical underpinnings further, particularly in terms of mathematically proving the safety of the system. We will also focus on integrating WiSE Kindness with techniques from IRL and intrinsically motivated goal formation to better estimate and align with the values and rewards of others. Lastly, experimental validation will be essential to testing these concepts in real-world AI systems, moving the approach from theory to practice and addressing the gaps highlighted in the limitations section. These efforts will pave the way for building AI systems that are not only safer but also smarter, kinder, and better aligned with human values on a fundamental level."}, {"title": "7 Supplementary Materials", "content": "7.1 Algorithms"}]}