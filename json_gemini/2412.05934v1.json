{"title": "Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models", "authors": ["Teng Ma", "Xiaojun Jia", "Ranjie Duan", "Xinfeng Li", "Yihao Huang", "Zhixuan Chu", "Yang Liu", "Wenqi Ren"], "abstract": "With the rapid advancement of multimodal large language models (MLLMs), concerns regarding their security have increasingly captured the attention of both academia and industry. Although MLLMs are vulnerable to jailbreak attacks, designing effective multimodal jailbreak attacks poses unique challenges, especially given the distinct protective measures implemented across various modalities in commercial models. Previous works concentrate risks into a single modality, resulting in limited jailbreak performance. In this paper, we propose a heuristic-induced multimodal risk distribution jailbreak attack method, called HIMRD, which consists of two elements: multimodal risk distribution strategy and heuristic-induced search strategy. The multimodal risk distribution strategy is used to segment harmful instructions across multiple modalities to effectively circumvent MLLMs\u2019security protection. The heuristic-induced search strategy identifies two types of prompts: the understanding-enhancing prompt, which helps the MLLM reconstruct the malicious prompt, and the inducing prompt, which increases the likelihood of affirmative outputs over refusals, enabling a successful jailbreak attack. Extensive experiments demonstrate that this approach effectively uncovers vulnerabilities in MLLMs, achieving an average attack success rate of 90% across seven popular open-source MLLMs and an average attack success rate of around 68% in three popular closed-source MLLMs. Our code will coming soon.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) [37], including prominent open-source models like LLaMA [52] and Qwen [3], as well as proprietary closed-source models such as OpenAI's GPT-4 [1] and Google's Gemini [2], have revolutionized the field of artificial intelligence. These models demonstrate exceptional capabilities in generating human-like text [11], summarizing complex information [22], and engaging in nuanced conversations [13]. With the growing demand for models that can handle richer, multimodal data, research has increasingly shifted toward the development of multimodal large language models (MLLMs) [57], which integrate both textual and visual inputs. This shift towards multimodality allows MLLMs to excel in complex tasks that require a deeper understanding of context across diverse in- puts, broadening their potential applications [23]. However, the integration of multiple data types also introduces new layers of safety and ethical concerns [31].\nRecent works [12, 26, 30, 44, 55, 58] have studied that MLLMs are vulnerable to jailbreak attacks, where malicious prompts are designed to bypass model safety restrictions. As shown in Figure 1, existing jailbreak attack methods generally fall into two categories. The first category involves embedding the malicious prompt within the text input, often by optimizing the text suffix [20, 65] or adopting automated algorithms such as genetic algorithms [63], with the image input either left blank or subtly perturbed to increase the model's likelihood of responding to the malicious question. However, these text-centric methods are susceptible to the model's text comprehension capabilities, as the malicious prompt is fully embedded within the text modality. This integration results in the text modality containing the complete harmful semantic information, enabling the model to detect the harmful intent conveyed within the text, the model refuses to answer, resulting in limited attack performance. The second category involves embedding malicious prompts within the image input through layout and typography [12, 30], with text serving primarily as an explanatory element. These image-centric methods rely on the model's OCR capability and integrated processing capability of image and text information but are also prone to detection, as the model may recognize the harmful content embedded in the visual input, thereby capturing the adversary's malicious intent and refusing to answer, resulting in the limited attack performance. Hence, most previous works focus on risks within a single modality, leading to limited performance in jailbreak.\nTo address this issue, in this paper, we propose a heuristic-induced multimodal risk distribution jailbreak attack method, called HIMRD. As shown in Figure 2, the proposed method consists of two elements: multimodal risk distribution strategy and heuristic-induced search strategy. The multimodal risk distribution strategy divides the malicious prompt into two harmless segments and embeds them separately within the text and image inputs. It effectively circumvents the model's safeguards by preventing it from directly recognizing the malicious prompt in either modality alone. The heuristic-induced search strategy is used to find two kinds of text prompts: understanding-enhancing prompts and inducing prompts. The understanding-enhancing prompt is employed to enable the MLLM to first successfully reconstruct the malicious prompt. The inducing prompt is used to make the tendency of affirmative output greater than the tendency of refusal output, thus achieving a jailbreak attack. Extensive experiments across different models demonstrate the superiority of the proposed method. Our HIMRD achieves an average attack success rate (ASR) of 90% across seven popular open-source MLLMs and an average ASR of around 68% in three popular closed-source MLLMs.\nIn summary, the main contributions of this paper are as follows:\n\u2022 We propose a heuristic-induced multimodal risk distribution jailbreak attack method, called HIMRD, to improve the jailbreak performance for MLLMs.\n\u2022 We propose a multimodal risk distribution strategy to distribute the malicious prompt into two harmless parts and embed them separately into text and image. This strategy effectively bypasses the safety mechanisms of MLLMs.\n\u2022 We propose a heuristic-induced search strategy to refine the textual prompt to guide the MLLM in autonomously combining two innocuous parts to reconstruct the malicious prompt and output affirmative content.\n\u2022 Extensive experiments across ten various MLLMs demonstrate the effectiveness of the proposed black-box jailbreak method HIMRD, which achieves outstanding performance in jailbreaking MLLMs, surpassing state-of-the-art jailbreak attack methods."}, {"title": "2. Related Work", "content": "2.1. Jailbreak Attacks against LLMs\nAdversarial attacks [6, 15, 18] are a well-studied method for assessing neural network robustness [21, 46], particularly in large language models (LLMs) [17, 19, 20, 65]. Human Jailbreaks [49] leverage a fixed set of real-world templates, incorporating behavioral strings as attack requests. Gradient-based attacks involve constructing jailbreak prompts using the gradients of the target model, as demonstrated by methods such as GCG [65], Auto-DAN [63] and I-GCG [20]. Logits-based attacks focus on generating jailbreak prompts based on the logits of output tokens, with examples including COLD [14]. Additionally, there are fine-tuning-based attacks [50], which require access to the model's internal information. Many of the techniques are somewhat effective on closed-source models but often yield suboptimal results.\nSome attacks primarily rely on prompt engineering [16, 47], either to directly deceive models or to iteratively refine attack prompts. For instance, LLM-based generation approaches, such as PAIR [8], GPT-Fuzzer [60], and PAP [61], involve an attacker LLM that iteratively updates the jailbreak prompt by querying the target model and refining the prompt based on the responses received.\n2.2. Jailbreak Attacks against MLLMs\nIn addition to inheriting the vulnerabilities of LLMs [3, 53], multimodal large language models (MLLMs) introduce a new dimension for attacks due to the inclusion of visual modality [7, 24, 25, 34, 41\u201343, 64]. Existing attack methods can be broadly categorized into white-box [33, 44, 54]"}, {"title": "3. Methodology", "content": "Existing jailbreak attack methods often embed malicious prompts within a single modality, making it easier for multimodal large language models (MLLMs) to detect the adversary's harmful intent, leading to jailbreak failures. To address this problem, we propose a heuristic-induced multimodal risk distribution jailbreak method (HIMRD). This method first distributes the malicious prompt across two modalities and uses two types of text prompts to guide the model in generating harmful outputs. As shown in Figure 2, our method consists of two main strategies: multimodal risk distribution and heuristic-induced search. In this section, we provide a detailed description of the two strategies following the problem setting.\n3.1. Problem Setting\nMultimodel Large Language Model. A MLLM can be defined as $M_{\\theta}$, where $\\theta$ denotes the model's parameters. The model receives visual input $x_v$ and textual input $x_t$, which are processed through a fusion module $\\phi$ to generate a joint representation vector $r$. This vector $r$ serves as a high-dimensional representation that not only retains essential information from the original visual input $x_v$ and textual input $x_t$, but also encapsulates integrated features from both modalities. Within this framework, the model $M_{\\theta}$ can leverage $r$ to extract richer semantic information and produce an informed response $y$ accordingly. The formal definition of the MLLM can be expressed as:\n$y = M_{\\theta}(r),  r = \\phi(x_v, x_t)$ (1)\nwhere $x_v \\in V$, $x_t \\in T$ and $r \\in \\mathbb{R}$, $\\mathbb{R}$ refers to a high-dimensional vector space.\nJailbreak Attacks. To achieve a jailbreak attack on the target $t$ and obtain the desired harmful output $y_t$, the adversary should design a jailbreak strategy. This strategy involves embedding the malicious prompt $t$ into one or both of the input $x_v$ and $x_t$ to circumvent the safety defense mechanisms of the MLLM for multimodal input. Thus, the high-dimensional representation $r$ is altered to become $r_{adv}$, which incorporates the semantic information of the malicious prompt $t$. This strategy can be illustrated as:\n$\\max_\\mathbb{R} \\log p(y_t|r_{adv})$ (2)\n$r_{adv} = \\psi(x_v \\oplus \\phi_v(t), x_t \\oplus \\phi_t(t))$ (3)\nwhere $\\oplus$ denotes the concatenation operation, which combines the malicious prompt $t$ with the visual input $x_v$ or the textual input $x_t$. $\\phi_v(\\cdot)$ and $\\phi_t(\\cdot)$ represent the jailbreak strategies that embed the malicious prompt $t$ into the visual modality and textual modality, respectively.\nLimitations of existing attacks. To successfully jailbreak a MLLM, the key is to design an effective attack strategy, represented by $\\psi(\\cdot)$ in Eq. 3, which enables the bypassing of MLLM's safety detection mechanisms, while ensuring that $r_{adv}$ retains the semantic information of the malicious prompt $t$, thereby compelling the model to generate the desired harmful output $y_t$. However, existing jailbreak attack strategies typically embed the malicious prompt into a single modality [12, 58], which can be represented by the following equation:\n$r_{adv} = \\begin{cases} \\psi(x_v \\oplus \\phi_v(t), x_t) \\\\ \\psi(x_v, x_t + \\phi_t(t)) \\end{cases}$ (4)\nFor instance, methods like Figstep [12] and MM-SafeBench [30] opt to embed the malicious prompt $t$ within the image, while the text serves only as an inducement. Conversely, BAP [58], UMK [55] and HADES [26] embed it within the text, leaving the image merely including perturbations to increase the model's likelihood of an affirmative response. Due to the confinement of the malicious prompt to a single modality, the input of this modality contains completely harmful semantic information, making it easy for MLLMs to detect potential risks, thus resulting in limited jailbreak performance.\nThreat Model. The adversary's goal, as shown in Eq. 2, is to obtain answers to questions prohibited by the safety policy through exploiting MLLMs. This reflects real-world scenarios where malicious users may abuse the model's capabilities to acquire inappropriate knowledge. Our attack method HIMRD is a pure black-box approach. Consequently, the adversary can be regarded as an ordinary user and only can get the model's output. The adversary cannot access information such as the model's internal structure, parameters, and gradients.\n3.2. Multimodal Risk Distribution\nThe jailbreak method DRA [29] for LLMs distributes the malicious prompt into multiple parts to temporarily elim-"}, {"title": "3.3. Heuristic-Induced Search", "content": "While our multimodal risk distribution strategy effectively bypasses the model's safety alignment mechanisms, ensuring that the victim model can successfully reconstruct the malicious prompt within its completion and generate an affirmative response still requires further strategy. Therefore, we propose a heuristic-induced search strategy. As depicted in the middle of Figure 2, the text input can be divided into two types of prompts:\n\u2022 Understanding-enhancing prompt, denoted as $p_u$, which is used to enable the model to successfully reconstruct malicious prompts within its completion as much as possible.\n\u2022 Inducing prompt, denoted as $p_i$, which is used to enhance the inclination of the victim model to affirmatively respond to malicious prompt.\nThe above issue can be formally defined as the following formula:\n$\\text{find } p_u, p_i \\text{ s.t. } C_j(y, t) = 1$ (7)\nwhere $y = M_{\\theta}(\\psi(x_{v}, p))$ and $p$ denotes the textual prompt composed of $p_u$ and $p_i$, which are equivalent to $x_t$ in Eq. 1. $C_j(\\cdot)$ is a discriminator that determines whether the attack is successful based on the output $y$ and malicious prompt $t$. As shown on the right of Figure 2, in the final prompt, the green text serves as the understanding-enhancing prompt $p_u$, which is used to ensure that the model can accurately reconstruct the malicious prompt within its completion. The red text represents the distributed part of the malicious prompt $t$ embedded in the text, and the blue text is the inducing prompt $p_i$, whose purpose is to ensure that the model outputs an affirmative answer, preventing it from refusing to answer due to the safety defense mechanism. Through the above two strategies, we can compel MLLMs such as LLaVA [27] and DeepSeek [32] to successfully reconstruct the malicious prompt and ultimately output our desired malicious content, as shown in the MLLMs & Inference section on the right side of Figure 2.\nThe specific heuristic-induced search process is shown in Algorithm 1, the algorithm consists of two phases: understanding-enhancing prompt heuristic-induced search and inducing prompt heuristic-induced search. In the first phase, we handle samples whose $s_u$ are 0, which implies that the model fails to effectively reconstruct the malicious prompt within its completion. As a result, the output $y$ provided by the model has no connection with the malicious prompt $t$, thus $s_j$ is 0. Then we utilize function $f_u(\\cdot)$ to get a new understanding-enhancing prompt $p_u$, thereby updating the text prompt $p$ to obtain a new model output $y$. Subsequently, evaluation functions $C_u(\\cdot)$ and $C_j(\\cdot)$ are employed to quantify $s_u$ and $s_j$, respectively. The second phase is to process samples where the model successfully reconstructs malicious prompts but still refuses to answer. At this time, the model's safety alignment is still stronger than its instruction-following ability, resulting in the model refusing to answer. Therefore, we need to perform heuristic-induced search of the inducing prompt $p_i$ to break through the model's safety alignment and achieve jailbreak. The system prompts, user prompts and more details used in heuristic-induced search are provided in the supplementary materials."}, {"title": "4. Experiment", "content": "4.1. Experiment Setup\nDatasets. We adopt the SafeBench dataset from Figstep [12], which encompasses seven categories of topics that are strictly prohibited by both OpenAI's usage policies [40] and Meta's usage policies [36]. These categories are:"}, {"title": "4.2. Comparison with other Attacks", "content": "To verify the effectiveness of the multimodal risk distribution strategy, i.e. the correctness of Eq. 6, we conduct a validation experiment. Specifically, we apply multimodal risk distribution to SafeBench and then separately feed the textual prompts and visual prompts into GLM-4V-9B [10], using its built-in safety alignment mechanism as the judge function $J(\\cdot)$ in Eq. 6. If the model's output contains a refusal prefix, the input is considered to contain harmful semantic information; otherwise, it is deemed harmless. We use FigStep [12], MM-SafeBench [30] and BAP [58] as comparison methods, as their harmful attack targets are embedded in a single modality, such as the form in Eq. 4. Thus, we only test the modality in which the malicious prompts are embedded. All predefined refusal prefixes are provided in the supplementary materials.\nThe results, shown in Table 1, use the percentage of samples detected with refusal prefixes in all samples as the evaluation metric. It can be observed that when the malicious prompt is embedded into the vision modality, MM-SafeBench [30] and FigStep [12] respectively have 19.71% and 30.00% samples with refusal prefixes detected, while BAP [58], where the malicious prompt is embedded into the text modality, has 76.00% such samples. In contrast, our method HIMRD only has 0.00% samples detected in the text modality and 0.02% samples detected in the vision modality. These results strongly validate the effectiveness of our multimodal risk distribution strategy, as well as the validity of Eq. 5 and Eq. 6 in the context of our HIMRD."}, {"title": "4.3. Attacks on Open-Source Models", "content": "The results presented in Table 2 list in detail the ASR of each open-source model under different attack methods. It can be observed that when conducting white box attacks on MiniGPT-4 [62], VAE [44] and BAP [58] achieve 70% ASR, and UMK [55] achieves a higher ASR of 87.71% on MiniGPT-4 [62]. When performing transfer attacks on LLaVA-V1.6 [28] and LLaVA-V1.5 [27], they also exhibit good ASR. BAP [58] reaches an ASR of 62.80% on Yi-VL-34B [59], indicating that the attack method has a certain degree of transferability on this model. However, when the above white box methods are transferred to other models, their attack performance is slightly average, and their average ASR on the seven models does not exceed 40%. For the grey-box method HADES [26], there may be some performance degradation due to the replacement of the attacker model during our replication process.\nThe two black-box methods, FigStep [12] and MM-Safebench [30], have higher ASR. FigStep has a relatively balanced ASR on the seven models. This phenomenon reveals to some extent the deficiency of current open-source multimodal large language models in cross-modal safety alignment capability. This means that when these models face such attacks, their safety protection mechanisms may not be effective and are vulnerable to attacks. Our method achieves the highest ASR on six models except MiniGPT-4. On DeepSeek-VL, LLaVA-V1.6 and GLM-4V-9B, it reaches ASRs of 94.57%, 96.57% and 94.29% respectively, and the average ASR is as high as 90.86%. This fully demonstrates that our attack method has extremely effective attack performance and generalization ability. Such a high ASR indicates that our method can effectively break through the safety defenses of these models, revealing the vulnerability of MLLMs when facing such attacks and posing a greater challenge to the safety of the models. More examples are provided in the supplementary materials."}, {"title": "4.4. Attacks on Closed-Source Models", "content": "In our experiments targeting closed-source models, given the high cost of API access, we do not use the full dataset; instead, we create a small-scale dataset tiny SafeBench by randomly selecting 10 samples from each of the seven cat-"}, {"title": "4.5. Ablation Study", "content": "To further validate the effectiveness of our approach, we conduct a series of ablation studies. Specifically, we investigate the following two aspects: 1) the impact of the heuristic-induced search strategy on the ASR; and 2) the impact of the number of iterations for heuristic-induced search, denoted as $N_1$ and $N_2$, on the ASR. Through these ablation studies, we quantify the specific contribution of each factor to the attack performance.\nHeuristic-induced search. Table 4 illustrates the result of the ablation study concerning the heuristic-induced search. The results indicate that introducing this strategy can significantly enhance the ASR of both LLaVA-V1.5 and GLM-4V-9B models. Specifically, compared to the initial result, LLaVA-V1.5 exhibits an approximate 11.14% increase in ASR during the heuristic-induced search process for $p_u$, followed by an additional 3.06% increase during the heuristic-induced search process for $p_i$, resulting in a total improvement of 19.72%. Similarly, GLM-4V-9B shows an approximately 3.06% increase during the heuristic-induced search process for $p_u$, followed by an additional 4.94% increase during the heuristic-induced search process for $p_i$, leading to a total improvement of 8.00%. These results highlight the critical role of heuristic-induced search in enhancing the ASR of our method.\nNumber of search iterations. Fig. 4 presents the results of our ablation study on iteration counts $N_1$ and $N_2$, conducted with the Qwen-VL-Chat and GLM-4V-9B models. Fig. 4a shows the results for the first phase of the heuristic-induced search with respect to the iteration count $N_1$. It can be observed that during the first and second iterations, the ASR of Qwen-VL-Chat experiences a significant improvement, with roughly 12%, after which the improvement stabilized. The ASR of GLM-4V-9B also shows some improvement. This suggests that our designed heuristic-induced search strategy is capable of enabling the model to comprehend the text prompt meanings of nearly all samples within five iterations. Fig. 4b illustrates the results for the second phase of the heuristic-induced search with respect to the iteration count $N_2$. It shows that within five iterations, the ASR of the GLM-4V-9B model increases by approximately 5%, with a corresponding improvement in the Qwen-VL-Chat model. This strongly validates the effectiveness of our heuristic-induced search strategy in inducing the model to provide affirmative responses. This strategy favors the model's tendency to answer questions rather than reject them due to safety alignment constraints."}, {"title": "5. Conclusion", "content": "In this work, we propose a heuristic-induced multimodal risk distribution jailbreak attack method for multimodal large language models, called HIMRD. The proposed HIMRD consists of two elements: multimodal risk distribution strategy and heuristic-induced search strategy. The multimodal risk distribution strategy splits a malicious prompt into harmless text and image parts, bypassing safeguards by concealing intent across modalities. A heuristic-induced search strategy is used to find two prompts: an understanding-enhancing prompt, which enables the model to reconstruct the malicious prompt, and an inducing prompt, which increases the likelihood of an affirmative response over a refusal, thereby achieving a successful jailbreak attack. Extensive experiments on seven open-source MLLMs and three closed-source MLLMs, demonstrate the effectiveness of our HIMRD, outperforming previous multimodal state-of-the-art jailbreak attack methods."}]}