{"title": "Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture", "authors": ["Hong Lu", "Hengxu Li", "Prithviraj Singh Shahani", "Stephanie Herbers", "Matthias Scheutz"], "abstract": "Vision-language-action (VLA) models hold promise as generalist robotics solutions by translating visual and linguistic inputs into robot actions, yet they lack reliability due to their black-box nature and sensitivity to environmental changes. In contrast, cognitive architectures (CA) excel in symbolic reasoning and state monitoring but are constrained by rigid predefined execution. This work bridges these approaches by probing OpenVLA's hidden layers to uncover symbolic representations of object properties, relations, and action states, enabling inte- gration with a CA for enhanced interpretability and robustness. Through experiments on LIBERO-spatial pick-and-place tasks, we analyze the encoding of symbolic states across different layers of OpenVLA's Llama backbone. Our probing results show consis- tently high accuracies (> 0.90) for both object and action states across most layers, though contrary to our hypotheses, we did not observe the expected pattern of object states being encoded earlier than action states. We demonstrate an integrated DIARC- OpenVLA system that leverages these symbolic representations for real-time state monitoring, laying the foundation for more interpretable and reliable robotic manipulation.", "sections": [{"title": "I. INTRODUCTION", "content": "A vision-language-action (VLA) model is a type of foun- dation model for robotics that takes in images and language commands as input and directly outputs robot actions [1], [2]. VLAs show promise in providing generalist robot policies across different scenarios and robotic platforms [1]. Recently, OpenVLA has emerged as a significant open-source VLA model, built on a Llama 2 language model backbone combined with a visual encoder that fuses pretrained features. Despite using only 7B parameters (7x fewer than comparable models), OpenVLA has demonstrated strong generalization capabilities across diverse manipulation tasks through its training on nearly one million real-world robot demonstrations [2].\nHowever, recent evaluation of VLAs shows that they strug- gle with changes in environmental factors such as camera poses, lighting conditions, and the presence of unseen objects [3]. VLAs also lack reliability, particularly because of their opaque, black-box nature, which makes their internal workings challenging to interpret.\nOn the other hand, traditional Cognitive Architectures (CA), also known as symbolic architectures, excel in dependable, symbol-based reasoning but are constrained by their reliance on predefined rules and coded policy execution [4], [5]. Ideally, a CA could harness the versatility of generalist robotic policies and the multimodal capabilities offered by VLAs while maintaining vigilance over dynamic environmen- tal changes during execution in safety-critical applications such as robotic manipulations.\nIn this work, we investigate whether and how OpenVLA en- codes symbolic representations in its activation space through probing experiments. Our investigation aims to answer the following research questions:\n\u2022 RQ1: To what extent can we decode object properties and relations (e.g., spatial relationships between objects) from OpenVLA's hidden layer activations?\n\u2022 RQ2: Can we extract action-related concepts (e.g., grasp states, movement targets) from the model's activation patterns, and how do these compare to object-level rep- resentations?\nTo answer these questions, we train linear probes on dif- ferent layers of OpenVLA to predict symbolic states during manipulation tasks. Based on prior work in language model probing [6] [7], we hypothesize that:"}, {"title": "II. RELATED WORK", "content": "Existing works typically explore how CAs can be integrated with large language models (LLMs). For example, Wu et al. investigate whether the Llama-2 13B model encodes features that can predict expert decisions in a decision making task by training a linear classifier on the Llama model's last contextual embeddings to predict ACT-R's expert decision when the model is given ACT-R's strings of decision making traces as input; they further examine whether ACT-R's knowledge can be injected into the Llama model by fine-tuning a Llama- classifier system on ACT-R's expert decisions [8]. Bajaj et al. enhance ACT-R's analogical reasoning capabilities by building a natural language processing pipeline to automatically extract key entities, relationships, and attributes [9]. Once these key elements have been extracted from unstructured text, an LLM is prompted to convert the unstructured text into a structured format based on its key elements. ACT-R can utilize the struc- tured knowledge for reasoning tasks downstream, significantly reducing the need for manual knowledge engineering. While Bajaj et al. propose to use LLMs to transform unstructured text into structured knowledge, Kirk et al. explore ways in which LLMs can be leveraged as a knowledge source for the task knowledge needed for successful task planning downstream [10]. They propose three approaches to knowledge extraction: indirect extraction in which the LLM's reponses are placed in a knowledge store that the cognitive agent accesses, direct extraction in which the agent directly queries the LLM and parses its output for structured knowledge, and direct knowl- edge encoding in which the LLM creates programs that are run as part of the cognitive agent's task pipeline.\nTo the best of our knowledge, no existing work has explored the integration of a CA with a VLA model. Realizing such an integration requires a method to \u201cprobe\u201d the VLA's hidden representations for symbolic content. We next discuss relevant literature on foundational model probing, which informs our approach in extracting object and action states from Open- VLA."}, {"title": "B. Foundational Model Probing", "content": "Foundational models encode extensive knowledge derived from their internet-scale training data [11]. The increasing popularity of these models has drawn significant attention to the challenges of extracting and evaluating the knowledge they encode. One commonly used approach to evaluate LLMs is prompt-based probing in which an LLM is prompted to fill in the blanks in the prompt [12]. For example, Alivanistos et al. combine various prompting techniques to probe LLMs for possible objects of a triple where the subject and the relation are given [13], Wang et al. develop a method to automatically search sentences to construct optimal and readable prompts for probing certain knowledge [14], and Qi et al. propose a probing framework for multimodal LLMs that includes visual prompting, textual prompting, and extra knowledge prompting [15]. While prompt-based probing is intuitive and easy to execute, it lacks the layer-specific precision offered by linear probing. Furthermore, prompt-based probing is not applicable to vision-language-action models as they do not output language tokens. Linear probing on the other hand involves training a linear classifier on top of each frozen layer of a foundational model. Each classifier is tasked with predicting specific knowledge based on the output features of the corresponding frozen layer. For example, Li et al. train semantic probes to predict object properties and relations as they evolve throughout a discourse [16]. Similarly, Chen et al. use linear probes to evaluate the Llama model family's perfor- mance on higher-order tasks such as reasoning and calculation, comparing probe performance across layers and model sizes [6]. In our work, we extract symbolic representations of state changes similar to the approach described in Li et al [16] and we evaluate probing accuracies across hidden layers similar to the approach used by Chen et al [6]."}, {"title": "III. INTEGRATED VISION-LANGUAGE-ACTION MODEL - COGNITIVE ARCHITECTURE OVERVIEW", "content": "Figure 1 illustrates the high-level architecture of our DIARC-OpenVLA system. DIARC provides a cognitive ar- chitecture that manages symbolic reasoning and user inter- action, while OpenVLA is a continuous policy that takes in images and language instructions to produce a 7D robot action. Internally, OpenVLA uses a Llama 2 7B backbone [7], which consists of 32 transformer blocks (often referred to as \"layers\") plus an initial embedding layer, yielding 33 distinct hidden states when indexed from 0 to 32 at runtime. Each hidden state is a 4096-dimensional vector. At a conceptual level, we combine these by: (1) routing user commands through DIARC to OpenVLA, (2) running OpenVLA in a LIBERO simulation environment to generate actions and extract hidden- layer embeddings, and (3) mapping those embeddings to sym- bolic states for DIARC's belief store. This pipeline leverages the expressiveness of a vision-language-action model while maintaining the reliability of a symbolic architecture. Subsec- tion III-A details our real-time implementation, including the WebSocket interface and a React-based UI for visualization."}, {"title": "A. DIARC-OpenVLA Integration", "content": "The DIARC-OpenVLA integration bridges OpenVLA's continuous policy outputs and hidden-layer embeddings with DIARC's symbolic reasoning modules. Our approach requires minimal modification to OpenVLA itself: we intercept each inference call to extract the relevant hidden-layer activations and feed them to trained linear probes for symbolic state prediction, then pass those states back to DIARC in an automated fashion.\na) VLAComponent and Symbolic Predicates.: At every timestep, OpenVLA predicts a 7D action \u2206x, \u0394\u03b8, AGrip given the current camera image and the user's natural-language instruction. In parallel, we run linear probes on the extracted hidden-layer embeddings to output arrays of 0/1 labels for object relations and action subgoals (e.g., on(bowl, plate) = 1, grasped(bowl) = 0). These arrays are sent to DIARC's VLAComponent, which converts them into DIARC's symbolic predicate format: relation(object1, object2), property(object) for object states, and action(object) for action states. For example, a 1 in grasped(bowl_1) becomes grasped(bowl_1) in DIARC's knowledge store. DIARC can then leverage these discrete predicates to detect inconsistencies (e.g., a bowl can- not be both on(bowl_1, plate_1) and inside(bowl_1, drawer_1) at the same time), verify subgoals, or track overall task progress.\nb) WebSocket Server and Real-Time Flow.: We imple- ment a lightweight WebSocket server to provide real-time communication among OpenVLA, the LIBERO simulator, DIARC, and a React UI:\n1) User Task. The user selects a pick-and-place command in DIARC's GUI (e.g., \"pick up the black bowl between"}, {"title": "B. Simulated Pick-and-Place Tasks", "content": "LIBERO-spatial is a suite of 10 pick-and-place tasks in the LIBERO simulation environment [17]. We choose LIBERO- spatial for our OpenVLA evaluation as a LIBERO-spatial finetuned OpenVLA checkpoint is readily available for down- load. The LIBERO-spatial task suite consists of 10 pick-and- place tasks of the form \u201cpick up the black bowl {spatial relations identifier} and place it on the plate\" where the \"spatial relations identifier\" is filled with a natural language description of the target black bowl's spatial relations to its surrounding objects."}, {"title": "IV. PROBING EXPERIMENT", "content": "To test our hypotheses, we extract activations from the 33 hidden layers of OpenVLA's Llama 2 7B backbone. Each hidden-layer embedding is a 4096-dimensional vector. We then train two probes on each layer's activations to predict object states and action states. In total, we train 2 \u00d7 33 = 66 probes.\nAn object state involves the following relation predicates: behind(tabletop-object1, tabletop-object2), in-front- of(tabletop-object1, tabletop-object2), inside(tabletop- object, container), left-of(tabletop-object1, tabletop-object2), on(tabletop-object1, tabletop-object2), on-table(tabletop- object), and right-of(tabletop-object1, tabletop-object2), as well as unary object property predicates: open(container) and turned-on(on-off-object)."}, {"title": "A. Probe Training Data Collection", "content": "To train the probes, we need to collect (hidden layer activation, ground truth state) pairs as training data. To do this, we implement detector functions that detect the truth values for object relations, object properties, action statuses, and action subgoals in the 10 LIBERO-spatial tasks.\nAn action state captures the action status predicate grasped(pickupable-object) and the action subgoal predi- cate should-move-towards(tabletop-object). We find combinations of grounded objects to which a predicate is applicable, and we define the object relation atoms as the object relation pred- icates applied to all combinations of their grounded objects. We define an object state as a complete truth assignment to the object relation atoms and object property atoms. Similarly, we define an action state as a complete truth assignment to the action status atoms and action subgoal atoms. In total, an object state has 224 atoms and an action state has 12 atoms."}, {"title": "B. Probe Training Data Preprocessing", "content": "To produce a reliable dataset for training our linear probes, we collect (1) per-timestep embeddings from OpenVLA's hid- den layers and (2) corresponding ground-truth symbolic states (e.g., object relations and action subgoals) from a LIBERO environment. We then apply the following preprocessing steps to ensure that the resulting dataset reflects meaningful, fluctu- ating states and avoids overfitting or misleading evaluations:\n1) Episode-Level Splitting for Train/Test: We first gather a dataset of entire episodes (trajectories) from the environment, storing (embedding, symbolic_state) pairs at each timestep. To prevent temporal leakage\u2014where the model might \u201csee\u201d future frames from the same trajectory during training-we split the dataset by episode. Specifically, we assign each entire episode to either the training or the test set. This ensures that no frames from the same trajectory end up in both sets, so the probe's evaluation genuinely measures out-of-episode generalization.\n2) Filtering Near-Constant Labels: Some symbolic states never change (or change only once) across our collected episodes, offering little to no discriminative signal. They can also artificially inflate accuracy if trivially predicted (e.g., always zero). We measure each label's frequency of being '1' across all training frames and remove any label whose frequency is below 1% or above 99%. This pruning discards near-constant labels, ensuring that the remaining labels reflect genuinely variable states that are amenable to learning.\n3) No Class Balancing: While class imbalance is a concern for certain tasks, we do not perform synthetic oversampling or under-sampling in this work. In practice, removing near- constant labels already eliminates the most extreme forms of imbalance, so we find that further balancing is not strictly necessary. Nevertheless, class balancing remains an option for future improvements if highly skewed distributions re-emerge with other data.\n4) No Feature Standardization: We load the embeddings as raw floating-point vectors from OpenVLA's hidden layers and do not apply z-score normalization or any other scaling. Although standardizing embeddings can help in some contexts, we observe that the linear probe converges well without it-likely because the embeddings remain within moderate numeric ranges. We thus preserve a simpler pipeline, though we note that feature standardization remains a viable tweak for additional robustness.\n5) Resulting Training/Testing Dataset: After these steps, we obtain a final dataset whose labels reflect genuinely changing states, each entire episode is restricted to one split (train or test), and no artificially inflated metrics arise from trivial or constant conditions. We train the linear probe on the processed training subset and evaluate it on disjoint test episodes, confident that the reported performance tracks the probe's capacity to decode meaningful state information from the model's representations.\n6) Summary: By removing near-constant labels, splitting entire episodes for train/test, and keeping the embeddings unmodified, we produce a dataset that highlights OpenVLA's genuine internal representations of symbolic states. This pipeline avoids temporal leakage and trivial labels, thereby enabling a fair and interpretable evaluation of the probe's performance."}, {"title": "C. Probe Training and Evaluation", "content": "Our probing methodology builds on recent work investi- gating internal representations in language models [16] and multimodal embeddings [18]. Inspired by these approaches, we implement a linear probe that maps from the model's internal representations to ground truth environment states. However, rather than using single-label classification which would face combinatorial explosion with growing numbers of states, we extend this to multi-label classification where each state variable can be predicted independently.\nFormally, for a given layer's activation vector $h \\in \\mathbb{R}^d$, our probe learns a mapping to binary predictions $\\hat{y} \\in [0, 1]^n$ where n is the number of tracked symbolic states:\n$\\hat{y} = \\sigma(Wh + b)$ (1)\nwhere $W \\in \\mathbb{R}^{n \\times d}$ and $b \\in \\mathbb{R}^n$ are learned parameters and $\\sigma$ is the sigmoid activation function. Each element of $\\hat{y}$ corresponds to a binary prediction about a specific ground atom (e.g., \"on(bowl_1, plate_1)\u201d or \u201cin(bowl_1, top_drawer\"). We train using binary cross-entropy loss with the Adam optimizer.\nFor evaluation, we compute averaged accuracies per predi- cate type. For a predicate like \u201con\u201d, we average the accuracies across all specific instances of that predicate that we track - for example, if we track both \"on(bowl_1, plate_1)\" and \"on(plate_1, table_1)\", we would average their individual prediction accuracies to get the overall accuracy for the \"on\" predicate:\n$acc(pred) = \\frac{1}{N_{pred}} \\sum_{i}^{N_{pred}} acc(i)$ (2)\nwhere $N_{pred}$ is the number of tracked instances of that predicate, and $acc(i)$ is the binary prediction accuracy for the ith instance."}, {"title": "V. RESULTS AND DISCUSSION", "content": "The probe results, visualized as a heatmap across layers and predicates, reveal consistently high accuracies across later lay- ers suggesting robust encoding of symbolic state information. However, the first layer shows notably lower performance, aligning with its expected role in encoding lower-level features rather than high-level semantic relationships.\nThe accuracies are above 0.90 for most layers, indicating that the OpenVLA indeed encodes some object relation, object property, action status, and action subgoal features. The layer 0 probes perform significantly worse across all categories. This is not surprising as the first Llama layer probably only encodes very low-level semantic features such as syntactic relations and not the high level visual-semantic features such as object relations. We do not observe the hypothesized pattern of higher object state accuracies in earlier layer probes versus that of later layer probes. This does not support our hypothesis 1 and hypothesis 2. We recognize that the training data we use are not diverse enough in terms of the variation in object states and action states. Specifically, the objects in the 10 simulated LIBERO-spatial tasks have the same placements across tasks except for the two black bowls. As a result, most of the object relations remain unchanged across tasks, significantly reducing the variation in object states. Furthermore, the robot always picks one of the two black bowls, and the place target is always the plate, significantly reducing the variation in action states. These two factors combined significantly reduce the difficulty of the linear classification task that the probes are trained to do, leading to high accuracies across layers and categories, potentially washing out the layer-wise object state versus action state difference we expected to observe. More and better data is needed to test our hypotheses."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "As future work, we plan to scale the probing experiment up by collecting more diverse data from tasks that involve variable objects, variable object layouts, and variable goals. We believe that extracting symbolic information from VLAs opens the door to the integration of CA and VLA and we demonstrate an integrated CA-VLA system with this work. In the future, we hope to explore how the reasoning capabilities of the CA can enhance or monitor the performance of the VLA."}]}