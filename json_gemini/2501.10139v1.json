{"title": "Conformal Prediction Sets with Improved Conditional Coverage\nusing Trust Scores", "authors": ["Jivat Neet Kaur", "Michael I. Jordan", "Ahmed Alaa"], "abstract": "Standard conformal prediction offers a marginal guarantee on coverage, but for prediction\nsets to be truly useful, they should ideally ensure coverage conditional on each test point.\nUnfortunately, it is impossible to achieve exact, distribution-free conditional coverage in finite\nsamples. In this work, we propose an alternative conformal prediction algorithm that targets\ncoverage where it matters most in instances where a classifier is overconfident in its incorrect\npredictions. We start by dissecting miscoverage events in marginally-valid conformal prediction,\nand show that miscoverage rates vary based on the classifier's confidence and its deviation from the\nBayes optimal classifier. Motivated by this insight, we develop a variant of conformal prediction\nthat targets coverage conditional on a reduced set of two variables: the classifier's confidence in a\nprediction and a nonparametric trust score that measures its deviation from the Bayes classifier.\nEmpirical evaluation on multiple image datasets shows that our method generally improves\nconditional coverage properties compared to standard conformal prediction, including class-\nconditional coverage, coverage over arbitrary subgroups, and coverage over demographic groups.", "sections": [{"title": "Introduction", "content": "Machine learning models are envisioned to inform decision-making in high-stakes applications such as\nmedical diagnosis (Kumar et al., 2023; Elfanagely et al., 2021; Caruana et al., 2015). Consequently,\nthere is a critical need for actionable and useful uncertainty estimates to mitigate the risks associated\nwith incorrect decisions influenced by overconfident models. Conformal prediction (Vovk et al.,\n2005) is a framework for constructing prediction sets that provide finite-sample marginal coverage\nguarantees without making any modeling or distributional assumptions beyond exchangeability;\ni.e., the sets contain the correct output with a specified probability. Given a calibration dataset\n{(X, Y)}=1 and a new test point (Xn+1, Yn+1), split conformal prediction (referred to as simply\n\u201cconformal prediction\u201d from here on) constructs a prediction set C(Xn+1) \u2286 Y that satisfies,\nP(Yn+1 \u2208 C(Xn+1)) \u2265 1 \u2212 a,\nfor a \u2208 (0, 1). Such marginal validity does not, however, ensure that the prediction sets are actionable\nin arbitrary contexts, as coverage can be unacceptably poor for individual predictions.\nTo highlight the limitations of marginal validity, consider a scenario where a model is used to diag-\nnose a disease in a population where 90% of the cases are straightforward to diagnose, while 10% are\nchallenging. Prediction sets that only cover in the straightforward cases may achieve 90% marginal"}, {"title": "Related work", "content": "Group-conditional conformal prediction. A widely adopted relaxation of conditional coverage\nin prior work is based on group-conditional guarantees of the form P(Yn+1 \u2208 C(Xn+1) | Xn+1 \u2208\nG) > 1 - a for all groups G\u2208 G (Vovk et al., 2003; Barber et al., 2021; Jung et al., 2023). This\nconcept is often motivated by the idea that instead of X-conditional coverage, one can ensure\nthe coverage guarantee holds over predetermined subgroups that might otherwise be underserved\nby marginal coverage (Romano et al., 2020a). Mondrian conformal prediction (Vovk et al., 2003)\nachieves exact group-conditional coverage in finite samples when the groups in G are disjoint. The pro-\ncedure involves splitting the calibration data into subgroups and then separately calibrating on each\ngroup. Within this framework, Romano et al. (2020a) focus on achieving equal coverage over disjoint\nprotected groups of interest. Barber et al. (2021) propose an approach that allows groups in G to\noverlap; however, this method can be highly conservative and result in wide prediction intervals that\nover-cover. To provide practical, \u201cmultivalid\" coverage guarantees over arbitrary subgroups, Jung\net al. (2023) propose learning quantile multiaccurate predictors by minimizing the pinball loss over\nthe class of functions F = {\u2211G\u2208\u03c2 \u03b2G1{x \u2208 G} : \u03b2\u2208 R}. While Jung et al. (2023) provide\nPAC-style guarantees, Gibbs et al. (2023) propose a conditional conformal procedure that yields\nexact coverage guarantees over collections of groups in finite samples, and also extends beyond the\ngroup setting to finite-dimensional classes. That said, as noted earlier, it is not feasible to run this\nmethod with a function class that can guarantee approximate X-conditional coverage. Our work is\ncomplementary to this line of work: we propose a practical way to construct a function class that\nguarantees an interpretable notion of conditional coverage broader than group-conditional coverage\nover pre-specified groups.\nWe note there are some recent works that share similar motivation of learning features from\ndata to improve conditional coverage. Yuksekgonul et al. (2023) propose a density-based atypicality\nnotion to improve calibration and conditional coverage with respect to input atypicality. They\nimplement a special case of Mondrian conformal prediction to improve coverage in high atypicality\nor low confidence groups. In our work, the goal is to improve conditional coverage more generally,\nand our method differs from variants of Mondrian conformal prediction that cluster the covariate\""}, {"title": "Preliminaries", "content": "In this paper, we consider a classification setting in which each input feature Xi \u2208 X is associated\nwith a class label Y\u00bf drawn from a discrete set of possible classes Y. Let s :X \u00d7Y \u2192 R be a conformity\nscore function that measures how well the label y \"conforms\" to a model prediction at x, where\nlower scores indicate better agreement. (A simple choice for the score is s(x, y) = 1 \u2212 f(x), where\nf : X \u2192 Y is a pretrained classifier and f(x) is its softmax output for class y). Given the calibration\ndata set, {(Xi, Yi)}=1, and a model f, a conformal prediction set C(Xn+1) for a test point Xn+1 is\nconstructed by evaluating the conformity scores si = s(Xi, Y\u00bf),1 \u2264 1 \u2264 n. Then, we compute\u011d as\nthe [(n + 1)(1 \u2013 a)]/n empirical quantile of {si}=1, and use \u011d to construct the prediction sets\nC(Xn+1) = {y : s(Xn+1,y) \u2264 \u011d}.\nWe refer to this as STANDARD split conformal prediction following Ding et al. (2023). STANDARD\nconformal prediction guarantees marginal validity as described in (1) as long as the calibration and test\nscores $1,...,Sn,Sn+1 are exchangeable. However, as discussed earlier, marginal coverage may be\ninsufficient for C to be practically useful and we aim for a stronger notion of conditional coverage."}, {"title": "Dissecting miscoverage patterns in standard conformal prediction", "content": "Exact conditional coverage with an oracle classifier. Imagine an \"oracle\" classifier f* which per-\nfectly captures the distribution f(x) = P(Y = y|X = x),\u2200y \u2208 V, x \u2208 X. Romano et al. (2020b)\nshowed that one could construct optimal prediction sets Coracle(x) with exact conditional coverage\nby leveraging the order statistics f(1)(x) \u2265 f(2)(x) \u2265 ... \u2265 f(y)(x), for {f(x): y \u2208 Y}, as follows:\nCoracle(x) = {'y' indices of the k largest f(x)}, where k = inf {k': \u2211=1)(x) \u2265 1 - a}."}, {"title": "Conditional conformal prediction with trust scores", "content": "Consider a scenario where the feature space X is discrete, and we have access to a large calibration set\nthat includes all possible values in X. In this case, one approach to constructing conditionally valid\nsets is to select a distinct threshold \u011d in (3) for each x \u2208 X. This procedure would assign a larger \u011d for\ninstances where the model f is more prone to errors, and a smaller \u011d where errors are less likely.\nHowever, in practice, this procedure is infeasible because X is typically high-dimensional or continuous.\nMore generally, Lei and Wasserman (2014) and Barber et al. (2021) have shown that distribution-free\nconditionally valid predictive inference is impossible to attain meaningfully.\nThe key idea behind our proposed method is to condition on a lower-dimensional statistic V\u2208 V,"}, {"title": "Implementation using trust scores", "content": "The Rank(X, Y) variable measures how far from the top softmax score a classifier ranks the true class\nY for a given input X. Since we do not have access to the label Y, we use the trust score proposed\nin Jiang et al. (2018) as a proxy to approximate the miscoverage patterns characterized by Rank\n(Figure 1). The trust score is a nonparametric statistic that measures the agreement between the\nclassifier f and the Bayes-optimal classifier on a given testing point X. Formally, the trust score\nTrust(X; f) for a classifier f on test point X is defined as\nTrust(x; f) := d (x, Hs(P)) /d(x, Hs(P\u2081)),\nwhere Hs(P) := {x \u2208 X : rk(x) \u2264 \u03b5}; k-NN radius rk(x) := inf{r > 0 : |B(x,r) \u2229 X| > k},\n\u03b5 := inf{r > 0 : |{x \u2208 X : rk(x) > r}| \u2264 \u03b4\u00b7 n}.\nThe evaluation of Trust proceeds in two stages: first, a d-high-density-set Hs(Pe) (for continuous\ndensity function P with compact support X) is estimated for each class l from the training data by\nfiltering out a d-fraction of samples with lowest density. Then, for a given test sample, the trust\nscore (Trust(X)) (6) is computed as the ratio of the Euclidean distance between X and the nearest\npoint in the training set with class label different from the top-1 predicted label (say, \u1ef9), and the\ndistance between X and the nearest point with class label same as the top-1 predicted label by the\nclassifier (say, \u0177). We provide the implementation details in Appendix B.3.\nTheorem 4 in Jiang et al. (2018) provides the following guarantee for the trust score: for la-\nbeled data distributions with well-behaved class margins, when the trust score is large, the classi-\nfier likely agrees with the Bayes optimal classifier h*(x) := arg max\u0119\u2208y P(y = l|x), and when the\ntrust score is small, the classifier likely disagrees with it. More formally, under certain regularity\nassumptions, Trust(x; f) satisfies the following with high probability uniformly over all x \u2208 X and\nall classifiers f : X \u2192 Y simultaneously for sufficiently large training set:\nTrust(x; f) <y \u21d2 f(x) \u2260 h*(x),\n1/Trust(x; f) < \u03b3 \u21d2 f(x) = h*(x),\nfor some data-dependent threshold y. The paper can be consulted for the full theorem and\nproof. This result suggests that the Trust score stratifies prediction instances based on their\nagreement with the Bayes-optimal classifier. Given the Bayes-optimal classifier has low error, high"}, {"title": "Algorithm", "content": "We introduce our CONDITIONAL method for conformal prediction with the goal of achieving approxi-\nmate conditional coverage over the reduced variable set V = {Conf(Xn+1), Trust(Xn+1)}:\nP(Yn+1 \u2208 C(Xn+1) | Conf(Xn+1) \u2208 I\u2081, Trust(Xn+1) \u2208 I2) \u2265 1 - \u03b1,\nwhere sub-intervals I\u2081 and I2 are some discretization of [0, 1] and (0,\u221e) respectively. The exact\nconditional coverage guarantee P(Yn+1 \u2208 C(Xn+1)|Xn+1 = x) = 1 \u2212 a, \u2200x \u2208 X, is equivalent to a\nmarginal guarantee over all measurable functions f:\nE[f(Xn+1)\u00b7 (1{Yn+1 \u2208 C(Xn+1)} \u2013 (1 \u2212 a))] = 0, for all measurable f.\nIf f(x) = x \u2192 1, we recover marginal coverage. Gibbs et al. (2023) propose a relaxation of the exact\nconditional coverage guarantee over all measurable f to all f belonging to some function class F,\nE[f(Xn+1) \u00b7 (1{Yn+1 \u2208 C(Xn+1)} \u2013 (1 \u2212 a))] = 0, for all f \u2208 F.\nA special case of this guarantee is group-conditional coverage; i.e., P(Yn+1 \u2208 C(Xn+1) | Xn+1 \u2208 G) =\n1 \u2013 a for all G belonging to some collection of groups G, where F = {\u2211G\u03b5\u03c2 \u03b2G1{x \u2208 G} : \u03b2\u2208 R|9|}.\nThe above conditional coverage guarantee can be achieved by fitting an augmented quantile regression\nproblem over F where the unknown conformity score Sn+1 is imputed as s. The quantile estimate \u011ds\nis fit using the pinball loss la(g(Xi), si) as follows\ngEF\n\u011ds = arg min + i=1la(g(Xi), Si) + n+1la(g(Xn+1), s),\nwhere la(g(Xi), si) = (1 \u2212 a)(si - g(Xi))+ + a(g(Xi) \u2013 si)+. We compute the prediction set by\nC(Xn+1) = {y : s(Xn+1, y) \u2264 \u00ces(Xn+1,y)(Xn+1)}.\nFor a finite-dimensional linear class F = {\u03a6(\u00b7)\u03a4\u03b2 : \u03b2 \u2208 Rd} over the basis \u03a6 : X \u2192 Rd, Gibbs et al.\n(2023) show that we can achieve an upper bound on coverage in (9) and the exact coverage guarantee\nwith appropriate randomization.\nTheorem 1 (Theorem 2 Gibbs et al. (2023)). Suppose {(Xi, Yi)}+1 are independent and identically\ndistributed. Let F = {\u03a6(\u00b7)\u03a4\u03b2 : \u03b2 \u2208 Rd} denote the class of linear functions over the basis \u03a6 : X \u2192 Rd.\nIf the distribution of s | X_is continuous, then for all f \u2208 F, we have\n|E[f(Xn+1)\u00b7(1{Yn+1 \u2208 C(Xn+1)} \u2013 (1 \u2212 a))]| < d/n+1 E E max/1<i<n+1|f(Xi)|].\nTo achieve our coverage objective (7), we now define a function class F that depends on V."}, {"title": "Experiments", "content": "We evaluate CONDITIONAL empirically on four large-scale image classification datasets and a clinical\ndataset in dermatology. We propose evaluation metrics that measure the gap in coverage from\nthe desired 1 a level across different regions of the distribution. We also introduce a NAIVE\nimplementation of our proposed approach that aims to improve coverage with respect to V via\ngroup-conditional coverage guarantees over non-overlapping subgroups in V (Section 4.2). We\nevaluate STANDARD, CONDITIONAL (NAIVE), and CONDITIONAL on all datasets, and show that\nCONDITIONAL achieves the best conditional coverage performance across all settings. CONDITIONAL\nalso improves class-conditional coverage on all but one dataset."}, {"title": "Evaluation metrics", "content": "To evaluate approximate conditional coverage over test points {(X, Y)}=1, we propose binning\nthe feature space into |B| bins and computing the average coverage gap (CovGap) across these bins\n(14). The coverage gap measures the l\u2081 distance between the achieved coverage and the desired 1 \u2013 \u03b1\nlevel across all bins. Here, \u0109\u266d denotes the mean empirical coverage in bin b; i.e., \u0108b = (\u2211x\u2208b1{Y; \u2208\nC(X})/|b|, where |b| is the number of samples in bin b. This metric is inspired from the class\ncoverage gap in Ding et al. (2023):\nCovGap = 1/(|B|) \u2211(bEB) |(\u0109\u266d \u2212(1 \u2212 a))| \u00d7 100.\nFor comprehensive evaluation, we measure the conditional coverage performance using CovGap over\nthree different binning schemes:"}, {"title": "Experimental setup", "content": "Datasets. We perform experiments on ImageNet (Russakovsky et al., 2015), Places365 (Zhou\net al., 2018), and their corresponding long-tail versions ImageNet-LT and Places365-LT (Liu et al.,\n2019). ImageNet-LT and Places365-LT are constructed from the original datasets using a Pareto\ndistribution with a power value a = 6. Places365-LT has higher class imbalance than ImageNet-LT,\ndefined by the number of samples in the largest class divided by the number of samples in the\nsmallest class. We also evaluate on the Fitzpatrick 17k dataset (Groh et al., 2021) for skin disease\ndiagnosis to study coverage of prediction sets across skin types (Section 4.4).\nBaselines. Along with STANDARD split conformal prediction, we additionally include a NAIVE im-\nplementation of our approach as a baseline. CONDITIONAL (NAIVE) uses the Mondrian conformal pre-\ndiction procedure discussed earlier to improve coverage with respect to V = {Conf, Trust}. Each\nindividual bin b in the Conf \u00d7 Trust binning setting forms a group and CONDITIONAL (NAIVE) fits a\nseparate quantile \u011du for each bin. The prediction sets for X \u2208 b are computed as\nC(X) = {y : s(Xi, y) \u2264 \u011db}.\nCONDITIONAL (NAIVE) is a competitive baseline to assess the effectiveness of our higher-dimensional func-\ntion class Fas it has access to the discrete binning scheme we use for evaluation.\nExperimental details. We use a non-randomized version of the APS score (Romano et al., 2020b)\n(described in Section 2.1), as our conformity score: s(x,y) = \u2211=1 f(j) (x), where f(k)(x) = fy(x).\nWe exclude fy(x) to achieve smaller set sizes overall (Gibbs et al., 2023). We perform an extra step\nof temperature scaling to rescale the probabilities following past work (Angelopoulos et al., 2021;\nGuo et al., 2017). We consider a = 0.1 for all experiments to achieve a desired coverage level of 90%.\nFurther experimental details are provided in Appendix \u0412."}, {"title": "Results", "content": "Table 1 presents the CovGap of STANDARD, CONDITIONAL (NAIVE), and CONDITIONAL on all\ndatasets. We see that while all methods achieve the desired marginal coverage level of 90%,\nthere is a significant difference in conditional coverage as evaluated by the coverage gap. Overall,"}, {"title": "Fitzpatrick 17k dataset: Skin condition classification in clinical images", "content": "Fitzpatrick 17k (Groh et al., 2021) is a dataset of clinical images with skin\ncondition labels and skin type labels 1 through 6 based on the Fitzpatrick scoring system, with a\nsmall subset of images labeled as 'Unknown' where annotators could not determine the skin type.\nWe use the training script provided by Groh et al. (2021) to train a ResNet18 model (pretrained on\nImageNet) for 50 epochs. We randomly sample 70% of the data for fine-tuning and split the held\nout data evenly into calibration and evaluation splits based on the random seed. We consider the\nfull classification task over 114 skin condition labels. For our group-conditional coverage evaluation,\nwe consider Fitzpatrick skin types 1 through 6 as well as the Unknown type as our subgroups."}, {"title": "Discussion", "content": "Conformal prediction guarantees exact coverage marginally across test samples in finite samples. How-\never, for uncertainty sets to be truly meaningful and useful, they should provide coverage conditional\non test instances where uncertainty is higher or where the model is more likely to err. Unfortunately,\nachieving X-conditional coverage in finite samples is impossible without making distributional assump-\ntions. In this paper, we propose a relaxed notion of conditional coverage that improves the practical\nutility of prediction sets by ensuring coverage where it matters most specifically, in cases where a\nclassifier is overconfident in its incorrect predictions. To assess a classifier's overconfidence, we use\nits reported confidence (softmax probabilities) in combination with a nonparametric trust score\nthat measures its alignment with the Bayes classifier. We develop a practical variant of conformal\nprediction that achieves approximate conditional coverage with respect to these two variables, and\ndemonstrate that it improves conditional coverage properties in a general sense, including subgroup-\nlevel and class-conditional coverage. By reducing the coverage gap across relevant subpopulations,\nour resulting prediction sets can lead to fairer and improved downstream decision-making, especially\nin high-stakes applications where miscoverage can be consequential.\nWhile we show trust scores significantly improve conditional coverage in conformal prediction, they\nalso come with limitations. Particularly, in cases where the trust scores are not a good approximate\nof the rank of the true class, our method may not improve conditional coverage properties over\nstandard conformal prediction. Additionally, our function class is susceptible to computational\ndifficulties at high polynomial degrees. Future work can explore more sophisticated function classes\nto achieve our proposed conditional coverage objective."}, {"title": "Proofs", "content": "Proof of Theorem 1. See Theorem 2 in Gibbs et al. (2023).\nProof of Corollary 1. Corollary 1 follows directly from Theorem 1 in the special case where we choose\nF = {\u221111\u20ac11 111 1{Conf(x) \u2208 I1} + \u221112\u20ac12 \u03b22121{Trust(x) \u2208 I2} : \u03b21 \u2208 RIZ11, B2 \u2208 R|12|}.\nTheorem 2 (Proposition 4 in Gibbs et al. (2023)). Suppose {(Xi, Yi)}n+1 are independent and\nidentically distributed. Let F = {\u03a6(\u00b7)\u03a4\u03b2: \u03b2\u2208 Rd} denote the class of linear functions over the basis\n\u0424 : X \u2192 Rd. If we optimize the dual formulation of (10) and the dual solutions are computed using\nan algorithm that is symmetric in the input data, then the randomized prediction set Crand(Xn+1)\nachieves exact coverage for all f \u2208 F:\nE[f(Xn+1) (1{Yn+1 \u2208 Crand(Xn+1)} \u2013 (1 \u2212 a))] = 0.\nProof of Theorem 2. See Proposition 4 in Gibbs et al. (2023). We formally state this result here\nto show that appropriate randomization of C(Xn+1) can guarantee exact coverage without the\ncontinuity assumption on s|X as in Theorem 1."}, {"title": "Experimental details", "content": "We set a = 0.1 for our empirical evaluation. We run all our experiments with 10 random seeds\n{1,...,10} and report the standard errors in our results. The randomness in our experiments is\nover splitting the validation set into calibration and evaluation data and fitting the temperature\nparameter (Guo et al., 2017).\nEvaluation metrics. For our conditional coverage evaluation, we use the notion of CovGap\n(inspired by class coverage gap in Ding et al. (2023)) to measure the coverage gap across multiple\nbins in the feature space. We propose three binning schemes: Conf \u00d7 Trust, Conf \u00d7 Rank, and\nClass-conditional CovGap. To apply two-dimensional binning, we split the samples into evenly\nspaced bins based on Conf and then split each confidence bin into equal-size bins based on Trust\nscore or Rank depending on the binning scheme. For both two-dimensional binning schemes, we split\nthe samples into 10 evenly spaced confidence bins and then 4 equal-size bins based on trust score or\nrank. We choose this splitting to have appreciable granularity while also ensuring most bins have\nsufficient number of samples in all cases. As a special case for ImageNet, we manually edit the Rank\nbins as ~ 75% test samples have accurate predictions."}, {"title": "Datasets and models", "content": "We follow the data processing steps and pretrained models used by Yuksekgonul et al. (2023) in our\nevaluation for ImageNet, ImageNet-LT, and Places365-LT."}, {"title": "Details on F", "content": "We define our function class as F = {\u221111\u20ac11 B11\u20811{Conf(x) \u2208 I1} + \u221112\u20ac12 \u03b22121{Trust(x) \u2208 I2} :\nB1 \u2208 R|Z11, B2 \u2208 R|12|}. To compute Trust(x), we use features from the model's penultimate layer.\nFollowing Jiang et al. (2018), we skip the initial filtering step of the trust score algorithm to increase\ncomputational efficiency. For calculating the nearest neighbor distance to each class for the trust\nscores computation, we use IndexFlatL2 from FAISS (Johnson et al., 2017), Meta's open-sourced\nGPU-accelerated library for efficient similarity search. For all experiments, we run CONDITIONAL\nwith polynomial degree d = 5. We study the effect of d in Appendix C.2. We note that with d > 5,\nprediction set construction considerably slows down; hence, we do not report results for polynomial\ndegree values d > 5. The prediction sets in Gibbs et al. (2023) are constructed by updating the\nquantile fit for each test point using an iterative algorithm, where the cost per iteration is O(nd\u00b2)"}, {"title": "Details on approximate X-conditional coverage evaluation", "content": "We share details regarding the experimental setup for evaluating approximate X-conditional coverage\n(Figure 2). In this experiment, we evaluate the coverage gap between randomly sampled 12 balls in\nthe feature space. We compute the Euclidean distance between features Xi and Xj from the model's\npenultimate layer for samples i, j. We vary the radius r of the 12 balls, where r is evenly spaced in\nthe interval [rmin, Tmax]. rmin and rmax are estimated as the minimum and 90th percentile of the\ndistribution of distances between randomly sampled pairs of points for each dataset, respectively.\nFor a fixed r, we randomly sample 100 test points and find Euclidean balls with radius r around the\ntest point such that every ball should have at least 10 neighboring points. Then, we compute the\ncoverage gap over these regions using Eq. 14. We perform 100 trials of this procedure for each r for\nall datasets, and report the standard errors by error bars."}, {"title": "Miscoverage patterns in STANDARD and CONDITIONAL", "content": "To demonstrate the improvement in conditional coverage over standard conformal prediction, we\nshow the miscoverage patterns as in Figure 1 for CONDITIONAL along with STANDARD (Figures 3, 4).\nFigure 3 demonstrates the effectiveness of our function class F, as we see that coverage over all bins\napproaches the desired level of 1 \u2212 a = 0.9. From Figure 4, we can see that CONDITIONAL typically\nimproves coverage over the severly under-covered bins in Conf \u00d7 Rank for all datasets."}, {"title": "Effect of d", "content": "We study the effect of polynomial degree d in function class F (13) on CovGap and average set size\nin Figure 5. We can see that the choice of d is not a trivial one, and different datasets may have"}, {"title": "Trust-Rank correlation", "content": "Here we empirically study the relationship between trust score (Trust) and rank of the true class\n(Rank). We compute the Pearson and Spearman's rank correlation coefficients with p-values for\nTrust and Rank on test samples in all datasets (Table 3). The correlation coefficients and p-values\nindicate a statistically significant negative correlation between Trust and Rank. We also plot the\nrelationship between trust score and log(rank) in Figure 6. This shows that lower (better) rank\nvalues generally correspond to higher trust scores on average, whereas higher (worse) rank values\ngenerally correspond to lower trust scores."}, {"title": "Principal components of feature layer as F", "content": "We construct an alternative function F class using the top principal components of the feature layer\nas input. We choose the number of principal components as 20, with an added intercept term to\nachieve marginal coverage. We make this choice considering the computational constraints of the\nconditional conformal procedure in Gibbs et al. (2023) at the scale of our datasets (discussed in\nAppendix B.3). We compare the performance of this approach with our method over all evaluation\nmetrics. Conditional coverage evaluation in Table 4 shows that CONDITIONAL outperforms this\nfunction class on average over all datasets. Approximate X-conditional coverage evaluation (Figure 7)\nalso shows that our method consistently outperforms this function class on all but one dataset."}]}