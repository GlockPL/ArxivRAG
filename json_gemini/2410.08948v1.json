{"title": "The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points", "authors": ["Ariel Flint Ashery", "Luca Maria Aiello", "Andrea Baronchelli"], "abstract": "Social conventions are the foundation for social and economic life. As legions of Al agents increasingly interact with each other and with humans, their ability to form shared conventions will determine how effectively they will coordinate behaviors, integrate into society and influence it. Here, we investigate the dynamics of conventions within populations of Large Language Model (LLM) agents using simulated interactions. First, we show that globally accepted social conventions can spontaneously arise from local interactions between communicating LLMs. Second, we demonstrate how strong collective biases can emerge during this process, even when individual agents appear to be unbiased. Third, we examine how minority groups of committed LLMs can drive social change by establishing new social conventions. We show that once these minority groups reach a critical size, they can consistently overturn established behaviors. In all cases, contrasting the experimental results with predictions from a minimal multi-agent model allows us to isolate the specific role of LLM agents. Our results clarify how Al systems can autonomously develop norms without explicit programming and have implications for designing Al systems that align with human values and societal goals.", "sections": [{"title": "1 Introduction", "content": "Social conventions shape social and economic life, determining how individuals behave and their expectations [1, 2, 3, 4]. They can be defined as unwritten, arbitrary pat-terns of behavior that are collectively shared by a group. Examples range from con-ventional greetings like handshakes or bows, to language and moral judgments [5, 6]. Recent numerical [7, 8] and experimental [9] results have confirmed the hypothesis that conventions can arise spontaneously, without the intervention of any centralized insti-tutions [5, 3, 10, 11]. Individuals' efforts to coordinate locally with one another can generate universally accepted conventions.\nDo universal conventions also spontaneously emerge in populations of Large Lan-guage Models (LLMs)? This question is critical for predicting and managing Al behavior in real-world applications, given the proliferation of LLMs using natural language to in-teract with one another and with humans [12, 13]. It is also important for ensuring that Al systems behave in ways aligned with human values and societal goals [14].\nA second key question concerns the effect of the biases of the individual LLMs on the process leading to the emergence of universal conventions. A key insight from complexity science is that collective processes can either suppress or amplify individual traits [15]. As great emphasis is given to assessing and countering biases in one-to-one interaction between a human and an LLM [16, 17, 18], less attention has been paid so far to the effects of those biases on repeated communications in populations of LLMs, and even-tually in mixed human-LLM ecosystems [14]. However, predicting group behavior based solely on the knowledge of a single agent is extremely challenging [19], and the safety of a single LLM does not necessarily correspond to the safety of a multi-agent system [20]."}, {"title": "2 Experimental Setting", "content": "Our approach builds on Wittgenstein's general model of linguistic conventions, where repeated interactions lead to collective agreement between two players [30]. Theoretical extensions of this approach have argued that purely local interactions taking place on so-cial networks can lead to population-wide, or \u2018global', coordinated behavior [1, 2, 31, 6]. Theoretical predictions for our study are based on the naming game model of conven-tion formation, where agents, aiming to coordinate in pairwise interactions, accumulate a memory of past plays, which they then use to \"guess\" the words their subsequent partners will use [7, 8]. Extensive numerical and analytical studies have shown how the model captures the rapid growth of universally shared social conventions in different set-tings [6]. Derived laboratory experiments involving human participants in naming games have provided the first empirical evidence for the spontaneous emergence of shared lin-guistic conventions [9].\nThe naming game framework has also been applied to study norm change and criti-cal mass theory, which posits that committed minorities can overturn stable social con-ventions once their size reaches a tipping point, or \u2018critical mass'. Theoretical models suggest critical masses between 10% and 40% of the population [21, 32]. Empirical evi-dence from controlled social coordination experiments, which closely follow the scheme described above, supports a 25% threshold [22]. However, real-world observations reveal a wider range, with some studies proposing 30-40% for gender conventions in corporate leadership [24, 33], and others indicating that minorities as small as 0.3% can trigger significant linguistic and social changes [34, 35, 36, 28]."}, {"title": "2.2 Experimental Setup", "content": "A simulation \u2018trial' consists of a population of N interacting agents. At each time-step, two agents are randomly selected for interaction. Both agents select a convention, or 'name', from a pool of a finite size W, and attempt to blindly coordinate with one another. If they manage to coordinate, they are rewarded with an increase in their game score, otherwise they are penalized. Agents are not informed that their co-player is sampled from a population and are not incentivized to reach a \u2018global' consensus but only to coordinate in a pairwise manner with their partner on each round. Importantly, agents are able to remember details about the past M interactions they participated in, including their co-player's convention choice, their own convention, whether the interaction was successful or not, and their own accumulated score over these M interactions. The agents' memory is initially empty, so that at their first interaction they produce a random convention chosen from the pool of available names. After each interaction, agents see"}, {"title": "2.3 Prompting", "content": "Interactions within the game take place in the form of a series of text-based moves. In each interaction, the LLM agent is given a text prompt comprised of a system prompt and a user input prompt. The system prompt contains all information about the game. The user input requests the agent to predict a player's next action based on the history of choices in the M most recent interactions. This positions the agent as an external observer of the game, tasked with forecasting the upcoming round. In practice, these decisions dictate the state of play. Agents receive no information about the players' iden-tities or personalities, such as whether they are rational actors. Consequently, we can interpret the agent's recommendations as their de-facto participation in the game. The system prompt (see Materials and Methods) is designed such that the agent's output fol-lows a consistent format, from which we can extract its decision. Following previous works on LLMs' cognitive abilities [39], we ask the agent to 'think step by step' and to explicitly consider the history of play. The prompt thus encourages agents to make a decision based on their previous experience, but provides no instruction as to how it should be used in the decision making process. Agents are asked to select a name from the name pool, which is presented to them as a list of W unique letters sampled from the English alphabet. Ordering bias is removed by randomizing the list of presented let-ters for each player at every interaction. A successful interaction garners equal rewards for the participating agents, whereas a failure to coordinate results in a penalty. In the absence of human guidance, LLMs are notoriously bad at arithmetic. To avoid decision errors based on a misjudgment of the game state, we explicitly provide the agent with both the payoff they obtained at each round and their cumulative score within memory range. Lastly, to ensure that the responses generated by the LLM are correctly guided by the prompt and not merely the result of random hallucinations [40], we have im-plemented a meta-prompting strategy to assess the LLM's understanding of the given instructions. This practice, previously used in evaluating LLMs within game-theoretical frameworks [41], consists of posing a series of text comprehension queries to the LLM and evaluating the precision of its responses. The LLMs subjected to our testing demon-strated good comprehension capabilities, as detailed in SI Section 6.4."}, {"title": "3 Results", "content": "To balance experimental time, which should allow for multiple repetitions, with param-eters that provide agents a rich set of alternatives and meaningful awareness of their history, we set the name pool size to W = 10 and the individual memory length to M = 5 for populations of N = 24 agents, unless otherwise specified. The results presented below remain robust with respect to variations in these parameters (see Fig. SI5)."}, {"title": "3.1 Spontaneous emergence", "content": "Fig. 1 shows that group-wide linguistic conventions spontaneously emerge across all models. Initial interactions have a low probability of being successful, but stochastic fluctuations break the initial symmetry between the conventions, and eventually one be-comes dominant. The inset of Fig. 1 shows that the theoretical model (see SI Section 6.2 for a description) captures the dynamics generated by the LLM populations. The curves in Figure 1 concern a population size of N = 24 agents, but convergence is also observed for larger systems (N = 200, see Fig. SI5) and larger name pools (W = 26, see Fig. SI1). A population round is defined as N microscopic interactions, a common approach in multi-agent simulations [42]."}, {"title": "3.2 Collective bias in convention selection", "content": "Having established that social conventions emerge, a natural question arises: what are these conventions? The single Latin alphabet letters available in the name pool are all equally valid as global conventions, and so we would expect them to all to have the same probability to become the accepted social convention, as supported by the theoretical model [8] (see also SI Section 6.2). However, the experimental results present a different picture (Fig. 2A). The probability that a particular name becomes the social convention is neither uniform nor deterministic. Some names appear to have a significantly higher likelihood of becoming the adopted convention than others. This pattern holds across models, although the preferred names vary between models.\nTwo hypotheses could explain the observed behavior. The selection process may be non-uniform due to (i) intrinsic model (i.e., individual single-agent) biases or (ii) prompt features, specifically the order in which names in the name pool are presented to the agents, as noted in a different context [43]. The latter hypothesis can be discarded since, as mentioned above, the names are presented to the agents in a list in randomized order for each agent and at every interaction.\nHaving ruled out the order of name presentation as a factor, we can focus on the role of individual (i.e. single-agent) biases in shaping collective behaviour. The hypothesis that individual bias can be responsible for a collective bias is supported by the theoret-ical model. When the model is run with only two names, a bias towards a particular name quickly results in unilateral convergence on that name at the population level (see Fig. SI3). To test this intuition in our experiment, we examine the selection preferences of individual agents during their first round, when they have no prior memory. We find that individual biases are indeed possible. For example, when agents can choose any letter from the complete English alphabet, the population systematically converges on the letter 'A' because individual agents overwhelmingly prefer to select it over all other letters, even without prior memory (see Fig. SI1). However, a similar test on the fre-quency of name selection by agents with no prior memory for the case of Fig. 1, where the name pool contains ten elements but not the letter 'A', yields mixed results. Under these conditions, individual Llama 2 70b and Claude 3.5 Sonnet agents are unbiased across conventions in this name pool ($\\chi^2$-test, P = 0.100, 0.410), whereas individual Llama 3/3.1 agents exhibit a significant statistical skew in their name selections (see Fig. SI2). In all cases, the final consensus distribution shows that specific names are favoured as a consensus option, even if they appeared to be less likely to be selected in the initial step (Fig. 2A). Thus, both social conventions and collective biases in the selection process emerge also in absence of individual biases.\nThe findings suggest that collective bias may stem from the convention formation process itself, as agents are exposed to diverse memory states with different name com-binations and success-failure sequences. To test this hypothesis, we focus on the case of a name pool size W = 2, since tracking potential confounders of bias becomes impractical as the space of possible names increases. Fig. 2B shows shows that across all models, although agents are initially unbiased, local communication and coordination lead to a collective bias toward a specific convention, which we term the \u2018strong convention' (as opposed to its 'weak' counterpart)."}, {"title": "3.3 Tipping Points and Critical Mass", "content": "Social conventions are steady states of the system: once a convention spontaneously emerges, the population adheres to it indefinitely (see Fig. SI6). A natural question con-cerns the stability of such steady states: how resistant is a convention to deliberate efforts to overturn it? To address this question, we investigate whether a committed mi-nority can 'flip' an equilibrium consensus on a convention. We consider the scenario in which a population has long converged on a convention and every agent has solely ob-served that convention in the past M interactions (which were, therefore, all successful). We then introduce a 'committed minority' of agents producing an alternative convention [21, 22]. These committed agents follow a fixed strategy and use the alternative conven-tion at all times. We test populations using the same two-name (W = 2) conditions as in our convergence experiments. We simulate a consensus on each name per combination and introduce its complementary name as an adversary."}, {"title": "4 Discussion", "content": "Our findings demonstrate that social conventions can spontaneously emerge in popu-lations of Large Language Models (LLMs) through purely local interactions, without any central coordination. These results reveal how the process of social coordination can give rise to collective biases, increasing the likelihood of specific social conventions develop-ing over others. Importantly, this collective bias is not easily deducible from analyzing isolated agents, and its nature varies depending on the LLM model used. Additionally, our work uncovers the existence of tipping points in social conventions, where a minor-ity of committed agents can impose their preferred convention on a majority settled on a different one. The critical size of this committed minority is influenced by two factors: the interplay between the majority's established convention and the minority's promoted alternative, and the specific LLM model employed.\nOur approach aimed to minimize the complexity of both the interaction scheme and the semantic space to enhance transparency when interpreting the results. It is impor-tant to delimit the scope of our findings while highlighting possible avenues for future work. Firstly, our results reveal key aspects of norm dynamics in populations of LLMs within an experimental setup that is, unavoidably in LLM research, reliant on several pa-rameters including the LLM model, the prompt, and specific conventions. While rigorous testing, including metaprompting and experiment repetitions using different parameters, confirms the robustness of the results in this context, an important aspect of future work will consist of generalizing the results to different controlled experimental settings. In this context, scaling to larger populations and semantic spaces should also be inves-tigated [44]. Secondly, we considered only unstructured populations where interacting pairs are randomly selected. A straightforward yet crucial extension of this work con-sists of embedding the population in more realistic social networks, which may have a profound impact on the collective dynamics [6], as well as considering microscopic inter-actions involving more than two agents [45]. Finally, to bridge the gap between synthetic experiments and real-world applications, an exciting frontier for future study lies in con-sidering more realistic conventions\u2014such as moving from alphabet letters to sensitive human norms related to gender, race, and other social categories\u2014and investigating the dynamics of conventions in mixed LLM-human ecosystems, both in laboratory settings and eventually in natural environments like social media. Ethical considerations should be of course foundational for these kind of experiments.\nWithin the expanding field of LLM multi-agent systems [46], our work explored the so-far less-investigated aspect concerning the shared, poorly defined ways agents and humans solve social problems, such as creating language, norms, and institutions [14]. In this context, our results on norm change could stimulate research into similar dynam-ics within the framework of cultural evolution, particularly in chains of communicating agents [47]. Game theoretical approaches would naturally allow investigation of asym-metric payoffs' effects on collective consensus, potentially contrasting individual biases with explicit collective goals [48, 49]. Further promising research avenues include devel-oping frameworks to promote the emergence of specific conventions [50] and higher-order social norms [51], as well as testing interactions between agents based on different LLM models within populations.\nTaking a broader perspective, understanding how Al systems spontaneously develop conventions and more sophisticated norms without explicit programming is critical for predicting and managing Al behaviour in real-world applications. It is also essential for ensuring Al systems behave in ways that align with human values and societal goals. In particular, despite their rapid adoption, ethical concerns have arisen regarding the bi-ases exhibited by LLMs. The vast, unfiltered Internet data used to train LLMs can cause them to propagate and amplify harmful biases, disproportionately affecting marginalized communities [52]. Accordingly, a significant goal of the alignment research community has been to improve the performance of LLMs in individual bias tests [53, 54]. Our work shows that alignment also needs to be tested at the group level. So far, mixed results have been achieved when measuring and imbuing human social norms in LLMs [55, 56], and as of yet Al agents struggle to represent multiple cultures [57] and continuously evolving social norms [58, 59, 26]. We argue that the challenge extends beyond merely detecting 'undesirable behavior', to understanding the evolution of social norms held by agents and how these may influence humans through interactions in human-machine societies [26]. In this light, our work represents a first step towards a better understand-ing of norm dynamics in populations of LLMs, and we anticipate that it will be of interest to researchers and practitioners interested in making AI a tool for societal good."}, {"title": "5 Methods", "content": ""}, {"title": "5.1 Prompt", "content": "The system prompt comprises of three components: i) a fixed prompt that outlines the game's rules, including the payoff structure and the player's objective, ii) a dynamic memory prompt that provides contextual information about the state of play within the player's memory range, and iii) an instructional prompt that provides information for how the agent should format its response. The user prompt asks the agent to select a name to use in the current interaction. We use zero-shot prompting to directly extract the agent's name decision in response to the state of play. We do not provide instructions as to how agents should decide their next move, nor do we present them with example strategies. We ask the agent to behave in a self-interested manner, and the only part of the prompt in which we suggest to the agent that it should consider partaking in coordination is when we state that the agent's objective is to 'maximise their own accumulated point tally, conditional on the behaviour of their co-player'. We apply fixed payoffs for successful and failed interactions, set at +100 and -50 points respectively."}, {"title": "5.2 Models and APIs.", "content": "For our experiments, we use homogeneous populations of the following LLM agents: Llama 3 70B 12, Llama-3.1 70B, Llama 2 70b (in 4-bit quantisation format), and Claude Sonnet 3.5 (see Table 2 for specific versions). In these autoregressive LLMs, each newly generated word is produced based on previously inputted and generated words, and so the sequence of generation matters. More precisely, the probability distribution for predicting the next word is conditional on the product of all previous word probability distribution. To mimic LLMs deployed in real-world application, we demand all agents in our experiments to behave non-deterministically by fixing them with a non-zero constant temperature. This means that for each agent the next generated word is randomly selected from the conditional probability distribution. We use K-sampling to restrict the probability distribution of the next word to the next K most likely words, thus increasing the likelihood of high probability words and decreasing the likelihood of low probability words which are outside of the name pool (see Table SI3 for all parameter values)."}, {"title": "5.3 Measuring Individual Bias", "content": "We quantify the individual bias of agents by measuring the number of times each con-vention was produced in the first round of the game, when their memory inventory is empty, over T trials. Experiments with W = 2 are effectively a Bernoulli trial, and so we measure whether the agent is biased by performing a two-tailed exact Binomial test with the observed proportions. We calculate the p-value, P, using the null probability p = 0.5, and reject the hypothesis that the model is biased if P<0.05. For the case of W = 10, we perform a $\\chi^2$-test, and also test the null hypothesis that the model is neutral in its convention selection. Thus, we use the expected value 0.1T in our calculations, and again reject the null hypothesis that the model is unbiased if P<0.05."}, {"title": "5.4 Committed Minorities", "content": "To determine the critical size of the committed minority, we identify the point at which the majority consensus is overturned. A consensus flip occurs when 95% of the past 3N interactions succeed after the introduction of the committed minority. For Llama 3, we tested the smallest minority needed to overturn a weak convention majority, then repeated the experiment with a strong convention majority to measure the critical mass within the same time frame. For other models, the critical mass threshold is defined as the minimum proportion of committed agents that is required to flip the consensus within 30 population rounds. These criteria account for potential fluctuations in non-deterministic agent decisions."}, {"title": "6 Supplementary Information", "content": ""}, {"title": "6.1 Measuring Bias", "content": ""}, {"title": "6.1.1 Microscopic Bias", "content": "To measure the production bias in Table 1, we assess both the interaction-level bias and the bias within each unique memory configuration. The interaction-level bias is defined as the overall production probability of the strong convention across all possible configurations (per interaction), which we test using an exact Binomial test with a null probability of p = 0.5, rejecting the null hypothesis if the p-value falls below 0.05. The results of the tests are reported in the caption of Table 1. At the configuration level, we first perform an exact Binomial test, as above, to check whether the model is biased. In all cases, the p-value $P\u2020 < 0.05$, confirming that the model's decision is biased towards an extreme. Then, we use bootstrapping by resampling 70% of the observations for each configuration 10,000 times and measure the proportion of samples showing a stronger bias than the observed value in Table 1. In all cases, we obtain a bootstrapped $P\u2021 > 0.05$, indicating that we cannot reject the hypothesis that the model's underlying bias is more extreme than the observed bias. Results are reported in Table SI1."}, {"title": "6.2 Theoretical Minimal Naming Game", "content": "The Naming Game model simulates a population of N agents engaging in pairwise ne-gotiation interactions, demonstrating the emergence of global consensus on conventions through local coordination mechanisms. In the canonical formulation [8], agents must reach consensus on the name for an object using only local interactions, similar to our experimental framework. Agents possess internal lexicons with unlimited word capacity (although this is not a necessary initial condition of the model), initially empty. The inter-action protocol involves random selection of agent pairs, where the designated speaker transmits a randomly chosen word from their lexicon (or invents a new one if it is empty) to the hearer. If the hearer recognises the word in their own lexicon, both agents re-tain only the communicated word, while in case of failure, the hearer incorporates the novel word into their lexicon. The non-equilibrium dynamics of this system exhibit three-distinct temporal phases: (i) an innovation phase characterised by word creation, (ii) a propagation phase involving lexicon reorganization, and (iii) a convergence phase culmi-nating in global consensus. In our experimental framework, we set an initial condition whereby agents can only invent new words from a finite word pool of size W < N. This condition means that the initial innovation phase is extremely short, as seen in the in-set of Fig. 1. This model provides insights into the dynamics of language evolution and convention formation in both human and artificial communication systems."}, {"title": "6.3 Robustness Checks", "content": ""}, {"title": "6.4 Prompting", "content": ""}, {"title": "6.4.1 Prompt Structure", "content": "The system prompt comprises of three components: 1. a fixed prompt that outlines the game's rules, including the payoff structure and the player's objective, 2. a dynamic memory prompt that provides contextual information about the state of play within the player's memory range, and 3. an instructional prompt that provides information for how the agent should format its response. We find that agents generally struggle to behave in a manner befitting a partnership game, and often opted for strategies aimed at undermining their co-player's payoff, effectively treating them as an opponent. In practice, this meant that on some occasions the agent would willingly take an action with a negative payoff, in order to harm their co-player's accumulated point tally."}, {"title": "6.4.2 Output structure", "content": "To extract any meaningful decision from a language agent's output, which may be verbose and unstructured, it is necessary to distinguish between the reason, where the agent"}, {"title": "6.4.3 Example Prompt", "content": "We provide an example of the system and user prompts given to LLM agents in our experiments. We exclude the beginning of text and end of text tokens, which are unique to each model."}, {"title": "6.4.4 Meta-Prompting", "content": "When LLMs are used to solve tasks where some form of ground truth is defined, such as classification or regression, the effect of prompt variations on the quality of a model's outputs can be measured on downstream performance [63]. However, that is not possi-ble in generative tasks where a notion of error is undefined. Specifically in the naming game, any generated output is plausible, as long as it is within the set of allowed symbols. This ambiguity makes it difficult to assess whether the LLM's outputs reflect a proper"}, {"title": "6.4.5 LLM parameters", "content": "The table below shows the text generation parameters we use for all LLM models used in this work."}, {"title": "7 Figure Data", "content": ""}]}