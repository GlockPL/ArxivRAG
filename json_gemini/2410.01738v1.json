{"title": "VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch Diffusion Models", "authors": ["Kailai Feng", "Yabo Zhang", "Haodong Yu", "Zhilong Ji", "Jinfeng Bai", "Hongzhi Zhang", "Wangmeng Zuo"], "abstract": "Artistic typography is a technique to visualize the meaning\nof input character in an imaginable and readable manner.\nWith powerful text-to-image diffusion models, existing meth-\nods directly design the overall geometry and texture of in-\nput character, making it challenging to ensure both creativity\nand legibility. In this paper, we introduce a dual-branch and\ntraining-free method, namely VitaGlyph, enabling flexible\nartistic typography along with controllable geometry change\nto maintain the readability. The key insight of VitaGlyph is\nto treat input character as a scene composed of Subject and\nSurrounding, followed by rendering them under varying de-\ngrees of geometry transformation. The subject flexibly ex-\npresses the essential concept of input character, while the\nsurrounding enriches relevant background without altering\nthe shape. Specifically, we implement VitaGlyph through a\nthree-phase framework: (i) Knowledge Acquisition leverages\nlarge language models to design text descriptions of subject\nand surrounding. (ii) Regional decomposition detects the part\nthat most matches the subject description and divides input\nglyph image into subject and surrounding regions. (iii) Ty-\npography Stylization firstly refines the structure of subject\nregion via Semantic Typography, and then separately ren-\nders the textures of Subject and Surrounding regions through\nControllable Compositional Generation. Experimental results\ndemonstrate that VitaGlyph not only achieves better artistry\nand readability, but also manages to depict multiple cus-\ntomize concepts, facilitating more creative and pleasing artis-\ntic typography generation. Our code will be made publicly at\nhttps://github.com/Carlofkl/VitaGlyph.", "sections": [{"title": "1 Introduction", "content": "Artistic typography concentrates on emphasizing the visual\nmeaning of the word while maintaining its legibility. This\ntechnique has widespread applications across numerous ar-\neas such as commercial advertising (Gao et al. 2023; Lin\net al. 2023), social education (Vungthong, Djonov, and Torr\n2017; Duda et al. 2023) and design promotion (Zhao and\nLian 2023; Xiao et al. 2024). Traditional artistic typogra-\nphy involves manually applying design principles such as\ncolor, spacing and font choice to convey meaning (He et al.\n2023; Iluz et al. 2023). However, this process is often time-\nconsuming and lacks the flexibility to adapt to varying con-\ntent dynamically.\nRecent works (Iluz et al. 2023; He et al. 2023; Mu et al.\n2024) leverage the powerful capabilities of pre-trained text-\nto-image diffusion models (Rombach et al. 2022) to advance\nthe designing process. Despite significantly improved pro-\nductivity and diversity, it is still challenging to flexibly por-\ntraying the visual characteristics (e.g., the transformation of\ntexture and geometry) of input word without compromis-\ning readability. FontStudio (Mu et al. 2024) only focuses\non depicting the given word from the perspective of style\nand texture, while ignoring the geometry deformation. In\ncontrast, Word-as-image (Iluz et al. 2023) targets black-and-\nwhite designs by changing the shape of the selected letter.\nWordArt Designer (He et al. 2023) attempts to integrate var-\nious of transformation into Semantic Typography, but may\nsacrifice readability when illustrating concepts that involve\nvisible changes to glyph geometry. For example, Fig. 5's\n\"eagle\" resembles an eagle more than the character itself.\nMoreover, existing methods can express just a single mean-\ning of a given concept, further limiting the flexibility and\nuser-operability on this technique.\nIn this work, we introduce VitaGlyph to facilitate flexible"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Artistic Text Generation", "content": "Artistic text generation (Bai et al. 2024) is divided into text\nstylization, which focuses on visual effects (Wang et al.\n2023; Mao et al. 2022), and Semantic Typography, which\ndeforms text to match its meaning (Tanveer et al. 2023; He\net al. 2023). Models like eDiff-I(Balaji et al. 2022), Deep-\nFloyd (Lab 2023), and TextDiffuser-2 (Chen et al. 2023) en-\nhance generation using LLMs, while others (Xie et al. 2023;\nLiu et al. 2024b; Pu et al. 2024) add motion.\nHowever, these methods often capture only one layer of\nmeaning. We propose a dual-branch model that combines\nstylization and typography for more complex text genera-\ntion."}, {"title": "2.2 Large Language Models for Image\nGeneration", "content": "Large language Models(LLMs) (Achiam et al. 2023) have\nextend their impressive abilities from language tasks to vi-\nsion synthesis. Most generation-related works (Lian et al.\n2023; Yang et al. 2024a; He et al. 2023) use LLMs for\nprompt understanding, layout planning and typography de-\nsign. Due to these insights, we incorporate the LLM to de-\nsign artistic characters."}, {"title": "2.3 Controllable and Compositional Generation", "content": "ControlNet-based methods (Mou et al. 2024; Zhang, Rao,\nand Agrawala 2023; Huang et al. 2023; Liu et al. 2024a;\nZhang et al. 2023) improve image synthesis control with\nconditions like segmentation or depth maps, and some stud-\nies (Peong, Uchida, and Haraguchi 2024; Yang et al. 2024b;\nMu et al. 2024) apply these to text generation. Leverag-\ning the geometric nature of glyphs, we use ControlNet with\nglyph conditions as our foundation.\nFor multi-concept synthesis, works (Chefer et al. 2023;\nAgarwal et al. 2023; Meral et al. 2024; Rassin et al. 2024)\noptimize noise maps for concept co-occurrence, while (Jiang\net al. 2024; Wei et al. 2023) uses Diffusion Models with Lo-\nRAs (Hu et al. 2021), and (Liu et al. 2023b) integrates resid-\nual embeddings and layout priors.\nFew methods (Zhang et al. 2024) combine compositional\ngeneration with controllable conditions. Our work achieves\npixel-level control in compositional generation using Con-\ntrolNet and user-specific masks."}, {"title": "3 Method", "content": "Artistic typography generation task aims to express the vi-\nsual meaning of input characters in a appealing and compre-\nhensible way while also ensuring legibility. To this end, we\nintroduce a dual-branch method, namely VitaGlyph, which\nperforms adaptive rendering on the Subject and Surrounding\ncomponents of input glyph. The Subject undergoes struc-\ntural modifications to flexibly express the intrinsic seman-\ntics of input character, reflecting a strong artistic essence.\nMeanwhile, the Surrounding enriches the details of the sub-\nject while maintaining overall readability by adhering to the\ncharacter's structure throughout the generation process.\nAs illustrated in Fig. 2, VitaGlyph consists of three\nphases: Knowledge Acquisition, Regional Decomposition,\nand Typography Stylization. In Knowledge Acquisition\nphase, we ask the large language models (e.g., Chat-\nGPT (OpenAI 2024a)) to acquire the prior knowledge of\nSubject and Surrounding and design their text prompts.\nIn Regional Decomposition phase, we parameterize the\ninput character into glyph image, followed by employ-\ning Grounding-DINO (i.e., GDINO) to detect the sub-\nject part. Finally, during Typography Stylization phase, we\nfirstly transform subject's structure and then render subject\nand surrounding images with their corresponding prompts\nthrough Controllable Compositional Generation (CCG)."}, {"title": "3.1 Knowledge Acquisition", "content": "The goal of Knowledge Acquisition is to convert input char-\nacters into language descriptions of subjects and surround-\nings that are comprehensible for text-to-image diffusion\nmodels. It is challenging to manually design the prompts or\nto use the input characters themselves (e.g., \"rose\"), since\nthese approaches are either time-consuming, or result in de-\nscriptions that are difficult to interpret, especially for ab-\nstract words like \"win\" or \"lose\" (He et al. 2023).\nThe large language models like ChatGPT (OpenAI\n2024a) have demonstrated considerable logical capabilities"}, {"title": "3.2 Regional Decomposition", "content": "Given subject description $P_{sub}$, Regional Decomposition\nphase aims to convert the input character $c$ into glyph image\n$I$ and then divide the image $I$ into subject and surrounding\nregions at image-level, where the subject region has the most\nstructurally similar part to the essential concept.\nParameterization Given an input character $c$, we use the\nFreeType font library (David Turner 1996) to extract the out-\nline of the character and convert it into cubic B\u00e9zier curves.\nAfter that, the Differentiable Vector Graphics(DiffVG) (Li\net al. 2020) is utilized to raster obtained B\u00e9zier curves into\nget a final glyph image $I$.\nRegion Interpretation To preserve the shape of input\nglyph (i.e., readability) as much as possible, we choose the\nregion most similar to the structure of subject concept as\nsubject region (e.g., \"snow\" in Fig. 3). In pursuit of this,\nwe leverage Grounding-DINO (Liu et al. 2023a) to detect\nthe part of input image $I$ that matches the subject descrip-\ntion $P_{sub}$. As the detected results contain multiple bounding\nboxes with varying sizes, we filter out large-size or small-\nsize bounding boxes, and then select the bounding box with\nthe highest confidence score as the subject region:\n$I_{sub}, M = GDINOrank(I, P_{sub}),$\nwhere $I_{sub}$, GDINOrank and $M$ denote the subject image,\nGrounding-DINO with filtering and ranking strategies and\nmask area of the Subject. As shown in Fig. 3, owing to the\nopen-domain detection capabilities of Grounding-DINO, the"}, {"title": "3.3 Typography Stylization", "content": "As mentioned above, the subject and surrounding compo-\nnents play different roles in depicting the visual semantics of\ninput characters, so their structures and textures are rendered\nin different manners. Firstly, the structure of subject image\nis flexibly altered and enriched by Semantic Typography,\nwhile the background remains unchanged. Secondly, we ren-\nder their texture style through two independent ControlNets\n(i.e., as $ControlNets_{ub}$ and $ControlNets_{urr}$) in a composi-\ntional manner, termed as Controllable Compositional Gen-\neration (CCG).\nSemantic Typography Previous works (Iluz et al. 2023;\nHe et al. 2023) rely on time-consuming score distillation\nsampling loss (Poole et al. 2022) to transform the structure\nof input glyph. Inspired by (He et al. 2023), we utilize the\ndepth-to-image model in LDM (Rombach et al. 2022) to\ndirectly adjust the geometry of $I_{sub}$. Concretely, we adopt\nSDEdit algorithm (Meng et al. 2021) to perform geometry\ntransformation, conditioned on depth map $I_{sub}$ and subject\nprompt $P_{sub}$. It can be described as follows:\n$I_{sub}^* = SemTypo(P_{sub}, I_{sub}).$\nControllable Compositional Generation To render sub-\nject and surrounding images with their corresponding\nprompts, we apply two ControlNets to separately transfer\ntexture style in a compositional way. For subject image\n$I_{sub}$, we have transformed its structure in SemTypo into\n$I_{sub}^*$, and choose the segmentation version of ControlNet as\n$ControlNets_{ub}$. For surrounding image $I_{surr}$, we select the\nscribble version of ControlNet as $ControlNets_{urr}$ to satisfy\nthe constraints of the character layout when adding creativ-\nity and artistic flair."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Implementation Details", "content": "Benchmark We collect two hundred Chinese words (in-\ncluding single and multiple characters) as evaluation bench-\nmark, and ask ChatGPT to annotate subject and surround-\ning descriptions. Considering that Word-as-image (Iluz et al.\n2023) only generates English words, we also translate the\ncollected benchmark into an English version. We provide\nmore benchmark details in supplementary materials."}, {"title": "4.2 Quantitative Comparisons", "content": "As shown in Tab. 1, we conduct quantitative experiments\non the benchmark mentioned above. For comparisons, we\nleverage WordArt Designer (He et al. 2023) for Chinese\nwords and Word-as-image (Iluz et al. 2023) for English ver-\nsion. Since Word-as-image can only generate black text with\nwhite backgrounds, we apply ControlNets on it for texture\nrendering, as done in prior work (He et al. 2023).\nOur VitaGlyph outperforms the WordArt Designer in all\nmetrics especially in OCR with 4.22%. We believe this is\nbecause with controllable transformation of glyph geome-\ntry, our method could maintain promising readability. Mean-\nwhile, for English words, VitaGlyph also presents strong"}, {"title": "4.3 Qualitative Comparisons", "content": "We compare VitaGlyph with open-sourced WordArt De-\nsigner, which is a strong baseline for Chinese artistic ty-\npography. The input prompt of WordArt Designer is com-\nposed of the concatenation of the subject description and\nthe background description. In Fig. 5, one can observe that\nVitaGlyph renders input characters with higher quality and\nlegibility than WordArt Designer. For example, \"crab\u201d in\ncolumn 1, VitaGlyph visualizes this concept more concisely,\nwhile the rendered image of WordArt Designer blends the\nsubject and background, visibly harming the readability. In\ncontrast, VitaGlyph properly generates flexible and imag-\ninable artistic typography with the controllable transforma-"}, {"title": "4.4 User Study", "content": "We perform a user study to evaluate our method. For cus-\ntomized multi-concept generation, we show users the gen-\nerated artistic text of each method along with ten reference\nimages of the character. Then the users are asked to select\nthe image that most represents the concept, the image that\nbest matches the character itself and the image that is most\naesthetically pleasing. The questionnaire contains 40 sets for"}, {"title": "4.5 Ablation Studies", "content": "Effect of Region Interpretation To study the effect of our\nRegion Interpretation from Sec. 3.2, we conduct ablation\nexperiments without the GDINOrank. Concretely, we regard\n$I$ as the $I_{sub}$, while remaining the settings for SemTypo\nand CCG unchanged. We solely utilize a $controlnets_{ub}$\nwith $I_{sub}$ as the additional condition. The input prompt is\nconcatenation of $P_{sub}$ and $P_{surr}$.\nFig. 7 indicates that without Region Decomposition,\nVitaGlyph faces challenges: In Fig. 7 (a), it captures \"bam-\nboo nodes\" but misses \"bamboo leaf\" without GDINOrank.\nIn Fig. 7 (b) and (c), row 2 fails to resemble \"tree\" and\n\"lavende\" as it transforms the entire character instead of the\nrelevant part.\nFor quantitative results, comparing to the results in row 2\nand 5 of Tab. 3, the CLIP score is improved by 1.5% and\nthe accuracy is improved by 1.43% in GPT-40(TIM) and\n3.09% in GPT-40(OCR) respectively. We believe this is be-\ncause Grounding-DINO will identify the part that most re-\nsembles the subject prompt and help the transformation of\nglyph geometry easier and more natural.\nEffect of Semantic Typography Although Grounding-\nDINO identifies the part similar to the subject prompt $P_{sub}$,\nthe Semantic Typography is necessary for structural trans-\nformation. Ablation experiments without this module show\nthat images fail to capture the character's essence or make\nthe Subject appear semi-transparent in Fig. 9 (a). This is due\nto the gap between $I_{sub}$ and real-world objects, which the\nDepth-to-image module helps bridge.\nBased on the rows 3 and 5 from Tab. 3, we can conclude\nthat VitaGlyph with Semantic Typography would enrich the\nsemantic expression but sacrifice little readability.\nEffect of shared-$\\epsilon_t$ As VitaGlyph employs two Control-\nNets to individually render subject and surrounding regions,\nwe investigate the impact of shared initial noise (i.e., shared-$\\epsilon_t$) on the final results. Fig. 9 (b) indicates that not using\nshared-$\\epsilon_t$ leads to inconsistent appearance and unrealistic\nlighiting. For example, for \u201cmonkey\u201d in 3rd row, the color\nof subject and surrounding regions are visibly incoherent.\nMoreover, rows 4 and 5 of Tab. 3 show that using shared-$\\epsilon_t$\nachieves better performance in terms of prompt alignment\nand readability, being consistent with visualization results.\nThe Strength of Added Noise in Semantic Typography\nSemantic Typography uses the SDEdit algorithm (Meng\net al. 2021) based on depth-to-image diffusion model to ad-\njust the geometry of subject region. As shown in Fig. 8 (a),\nas the strength of added noise (DS) increases, more struc-\ntural details are added to the subject region while remain-\ning the black-and-white color scheme unchanged. When DS\nreaches 0.9, the subject region contains enough details of the\ngiven concept, e.g., the ears and beard of cat.\nThe Guidance Scale in ControlNet We fix the Subject\n$I_{sub}^*$ and only adjust the $ControlNets_{ub}$'s scale CS. From\nFig. 8 (b), we can observe that as the CS increases, the gen-\nerated part of Subject becomes more clear and more similar\nto the $I_{sub}^*$, shown in row 1. However, being too similar to"}, {"title": "4.6 Applications on Fonts and Advertisements.", "content": "In Fig. 6 (a), we verify the effectiveness of VitaGlyph on\nvarious types of fonts, e.g., \"font Bold\" in English and"}, {"title": "5 Conclusion", "content": "This paper presents VitaGlyph as a novel approach to artis-\ntic typography, effectively balancing creativity and readabil-\nity through the controllable transformation of glyph geom-\netry. By distinguishing between the subject and surround-\ning elements, our method enriches visual representation\nwhile preserving the essential meaning of the input char-\nacter. VitaGlyph significantly advances the field of artistic\ntext generation, showcasing the substantial contributions of\nour research. Future work will explore further advancements\nin generative AI to enhance the adaptability of typography\nacross different contexts and styles. VitaGlyph will signifi-\ncantly enrich artistic expression, enabling even more person-\nalized and dynamic design experiences."}]}