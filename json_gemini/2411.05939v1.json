{"title": "GCI-VITAL: GRADUAL CONFIDENCE IMPROVEMENT WITH VISION TRANSFORMERS FOR ACTIVE LEARNING ON LABEL NOISE", "authors": ["Moseli Mots'oehli", "Kyungim Baek"], "abstract": "Active learning aims to train accurate classifiers while minimizing labeling costs by strategically selecting informative samples for annotation. This study focuses on image classification tasks, comparing AL methods on CIFAR10, CIFAR100, Food101, and the Chest X-ray datasets under varying label noise rates. We investigate the impact of model architecture by comparing Convolutional Neural Networks (CNNs) and Vision Transformer (ViT)-based models. Additionally, we propose a novel deep active learning algorithm, GCI-ViTAL, designed to be robust to label noise. GCI-ViTAL utilizes prediction entropy and the Frobenius norm of last-layer attention vectors compared to class-centric clean set attention vectors. Our method identifies samples that are both uncertain and semantically divergent from typical images in their assigned class. This allows GCI-VITAL to select informative data points even in the presence of label noise while flagging potentially mislabeled candidates. Label smoothing is applied to train a model that is not overly confident about potentially noisy labels. We evaluate GCI-ViTAL under varying levels of symmetric label noise and compare it to five other AL strategies. Our results demonstrate that using ViTs leads to significant performance improvements over CNNs across all AL strategies, particularly in noisy label settings. We also find that using the semantic information of images as label grounding helps in training a more robust model under label noise. Notably, we do not perform extensive hyperparameter tuning, providing an out-of-the-box comparison that addresses the common challenge practitioners face in selecting models and active learning strategies without an exhaustive literature review on training and fine-tuning vision models on real-world application data.", "sections": [{"title": "1 Glossary", "content": "Deep Learning (DL), Active Learning (AL), Deep Active Learning (DAL), Multi-Head Self-Attention (MHSA), Convolutional Neural Network (CNN), Vision Transformer (ViT), Large Language Models (LLMs), Multi-Layer Perceptron (MLP)."}, {"title": "2 Introduction", "content": "While most works in literature often compare results to baseline active learning algorithms such as random selection or simple entropy-based selection, the extensive hyper-parameter tuning performed during training but often left out of the manuscripts leads to authors stating vastly different performances for the same CNN architecture, active learning algorithm [1], and label noise rate [2]. This not only raises questions about the credibility of reported state-of-the-art results but can also delay actual progress in developing active learning schemes that are robust to label noise and achieve performances comparable to models trained on clean labels. The work on non-active training of DL models in the"}, {"title": "3 Related Works", "content": "In this section, we highlight the current state of the literature on active learning, as well as active learning with label noise. We also highlight the use of the ViT for image classification and briefly discuss works that employ the ViT for active learning and label noise settings."}, {"title": "3.1 Deep Active Learning with Label Noise", "content": "In most supervised machine learning use cases, there is an initial data collection and labeling cost, in both money and time. In some domains and tasks, datasets are inherently difficult to label for a variety of reasons, meaning more time is needed even by an expert human annotator to assign a label to each sample. In other cases the cost of hiring expert annotators is high, such as is the case in medical imaging [13, 14], or the cost of producing the samples is high, such as is the case in experimental physics where observations come from costly telescopes or particle accelerators with limited access [15, 16, 17]. This challenges the real-world use of machine learning systems, especially as unlabeled dataset sizes increase. While much progress has been made in improving self-supervised learning (SSL) methods to leverage large unlabeled datasets for extracting quality image embeddings [18, 19, 20, 21, 22] to be used on downstream tasks with little labeled data, these methods still fall short when directly applied to datasets with noisy labels in an active learning setting [3]. Active learning is a machine learning paradigm, as depicted in Figure 1, that seeks to address the issues related to training ML models within a labeling budget, letting learning algorithms iteratively select a subset $L_m$ of size m, from a larger unlabelled dataset $U_n$ of size n : m \u2264 n, to be labeled by an oracle O for training [23, 24, 25, 26, 27]. However, the oracle may not always provide the correct label [28, 29, 30]. The active learning mantra under label noise can be stated as follows: Train a machine learning model on a significantly smaller dataset that may contain p% label noise, with little to no drop in test performance, while staying within a pre-determined labeling budget B.\nMost work in literature use random query, uncertainty sampling, and entropy-based sampling as baseline algorithms in comparing more complex methods for ALLN such as [31, 28], where a mixture of information gain and uncertainty is used for query selection. Other works in the literature focus on reducing the labeling budget by using a mixture of weak and strong annotators as well as annotator abstention in the case of uncertainty [28, 32, 33]. Despite these works posting promising results in budget optimization, there is little to no improvement in terms of the robustness to label noise and improved query selection. In [34], Huang et al. show that DAL is viable with oracle epiphany, which is to say the oracle is allowed to abstain from labeling samples they are unsure about until later on in the DAL cycle once they have seen enough examples to provide a more confident label. While their method is more realistic and leads to better performance, abstention may not always be possible in a fast-paced sector where lots of data is generated on a daily basis and requires urgent labeling. In [35], Yan et al. utilize abstention in DAL under label noise. A key difference in their work is that the algorithm need not be aware of either the abstention or noise rate. While there are obvious merits to the methods above, the solutions rely too heavily on the specific setup of the AL cycle and the human annotators to be trusted in the general setting. For these reasons and more, our work focuses on a more algorithmic approach to robustness and query selection.\nRecent work [36] deploys a more data-centric approach to label noise for active label cleaning by ranking samples for label correctness and labeling difficulty. In [37], the authors propose a data-driven self-adapting DAL strategy that selects potentially noisy labels for correction in a manner that automatically avoids class imbalance in the labeled dataset with no prior knowledge of the class distributions. A thorough study of the literature [1, 3, 38] raises questions on the superiority and robustness of more complex DAL methods. The authors state that due to a lack of standardized benchmark settings in overlapping niches, performances by baseline models and AL queries in noisy labels tend to be understated. The use of grid search and related methods for hyper-parameter tuning can also aid in finding the ideal model parameters that show the superiority of a proposed AL model over the baseline methods. It is for this reason we opt to avoid any extensive hyper-parameter optimization that could skew the results of this study, and thus we report a realistic out-of-the-box benchmark in AL under label noise."}, {"title": "3.2 Vision Transformer for Image Classification", "content": "In [9], Kolesnikov et al. present the Vision Transformer (ViT) as a CNN replacement for image classification tasks. They show that, in the extensive dataset regime, ViTs achieve higher classification accuracy, are more computationally efficient, and show no signs of saturation compared to CNNs such as ResNet and EfficientNet on increasingly larger datasets. The standard ViT architecture takes 16 by 16 patches from an image, flattens them, and applies a linear projection onto a higher dimensional space equal to that of the original text-based transformer. The patches are then marked for where in the image they were extracted, and so a second input to the transformer is the 1D positional embedding of each patch as shown in Figure 2. The spatial correlations of the patches are learned implicitly through their positional embeddings and self-attention vectors. The positional embeddings ensure the model learns both the relationships between pixels in the patches as well as the local and global 1D proximity representations of the tokens (image patches). Below we give an overview of the main transformer encoder and the attention mechanism within the ViT architecture.\nTansformer Encoder: The transformer encoder block in a ViT consists of multi-head self-attention (MHSA) and multi-layer perceptron (MLP) layer with normalization layers before and after MHSA each performing specific operations on the input tokens or subsequent later outputs. MHSA attempts to learn diverse features to capture semantical and contextual complexity in image patches while learning all these features in parallel for a given input. The parameterized MLP pools and aggregates the learned high-dimensional features from all the last layer attention heads, and compresses them into a lower-dimensional representation for a downstream task such as image classification, regression, and more. Figure 3 illustrates the transformer encoder block architecture.\nSelf-Attention: Given a sequence of input vectors, the self-attention mechanism calculates the similarity between the vectors. In the case of ViTs, starting with a sequence of m patch embeddings of size e, $X = [X_1, X_2, ..., X_m]$,"}, {"title": "4 Query Based on ViT Patch Similarity", "content": "Most AL approaches rely solely on the predicted probabilities from the trained model to form their query strategy. Label noise leads to a confused learner that outputs uncalibrated probability estimates of the samples, so selecting samples based on the predicted probabilities alone leads to the selection of non-optimal samples for each iteration and further corruption of the model through unstable gradients from incorrectly labeled samples. To address this issue, our proposed solution involves using the attention vectors from the last layer of a ViT model in query selection. This approach is based on finding specific features of a ViT that can help identify potentially mislabeled samples by comparing the labels of image samples that are close to each other in the last layer of attention representation maps. We start with a pre-trained ViT and fine-tune it using a small set of images with accurate labels, a common practice in many AL algorithms. Subsequently, we extract attention vectors for all the images in the initial set with accurate labels, and we use these to create core attention representations (centroids) for each class by aggregating the attention vectors of images belonging to the same class. This results in C-Core attention representation vectors that have been trained exclusively using accurate data for a C-class classification problem. The purpose of these core attention vectors is to help identify potentially incorrect labels during the AL process and reduce their impact on the model's training. In each AL cycle, unlabeled images go through the fine-tuned ViT model, and their last layer's attention vectors are collected. Our AL strategy combines the consideration of prediction uncertainty and attention-based diversity, which means we select samples that the current model is uncertain about while also striving to maintain a balanced representation of each class in the training dataset using the C-Core attention vectors."}, {"title": "4.1 Handling Label Noise through Gradual Class-Centric Confidence Improvement", "content": "During training, an oracle receives a batch of K samples to label, and before retraining the model with these labeled samples, a portion of them that deviate too much from the C-Core attention vector of the assigned class have their class assignment probabilities changed. We explore handling these examples in two ways, first by assigning the samples to the class dictated by the C-Core attention vectors, or, secondly by label smoothing. With label smoothing, we change the class probabilities assigned by the oracle for a sample so that it is not one-hot-encoded, but rather we introduce a positive probability for another class. Since we have the C-Core class centroids, we smooth the label by assigning a positive probability to the closest class centroid from the current sample in the attention vector space. For example, say sample xj was selected for labeling, and the noisy oracle assigns it to class C4 out of 10 classes. This means the probability distribution is given by [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. However, if we suspect this labeling to be incorrect based on the sample $x_j$ being far from the core attention centroid of class C4, we alter the probability distribution in such a way that we express less confidence in class C4 being the correct label. If the closest centroid in attention vector space is that of class C9, we change the probability distribution $p(y|x_j)$ to be [0, 0, 0, (1 \u2014 \u0454), 0, 0, 0, 0, \u20ac, 0], where \u20ac \u2208 [0, 1] controls how much to trust the oracle as opposed to the C-Core attention vectors that compare images semantically.\nOnce the initial model is trained on the clean set, future batches for labeling are selected in a way that promotes class-centric confidence. We first calculate prediction uncertainty for each class based on the model's confidence in its predictions. The AL strategy selects samples with the highest distance to their core attention class centroid, measured by the Frobenious norm. We then rank these from highest to lowest based on the Frobenious distance and take the top-K for labeling. As the model improves its performance on the validation set, we shift towards selecting samples with"}, {"title": "4.2 Joint Entropy-Attention Active Learning", "content": "Starting with: (1) An initial set of $K_o$ labeled images, referred to as the \u201cclean labeled set\", consisting of randomly selected images. This set is denoted as $C_{K_o}$, and should be large enough to ensure the representation of at least a few examples from each of the C classes. (2) A base Vision Transformer (ViT) denoted by $ViT_1$. The ViT has been pre-trained on ImageNet-1k [46], and has the fully connected layers changed to suit the number of classes C. (3) Iteratively fine-tune the ViT on a collection of K images with the highest prediction entropy $H(p(\\hat{y}|x_i))$ and ViT hidden layer attention heads distance to their supposed centroid attention heads, i.e., the images which jointly maximize Equations 2 and 3:\n$argmax_\\{x_i\\} H(p(\\hat{y}|x_i)) = argmax_\\{x_i\\} [-\\sum_\\{y=1\\}^\\{C\\} p(y|x_i) log p(\\hat{y}|x_i)]$\n$argmax_\\{x_i\\} d(V^m_\\{Yi}, \\bar{V}^m_\\{Yi\\})$\nwhere d is a suitable distance measure, C is the number of classes, and $\u0177_i$ is the predicted class of $x_i$. We also have: the m attention vectors $V^m_\\{Yi\\}$ produced by running image $x_i$ through the fine-tuned model and\n$\\bar{V}^m_\\{Yi\\} = \\frac{\\sum_\\{x_i \\epsilon C\\}\\{Yi\\} z^i \\: V^m_\\{Yi\\}}{\\sum_\\{x_i \\epsilon C\\}\\{Yi\\} z^i}$\nwhere\n$z^i_\\{Yi\\} =  \\begin\\{cases\\} 1 & \\text{if } x_i \\epsilon \\text{class } \\hat{y}_i \\\\ 0 & \\text{otherwise} \\end\\{cases\\}$"}, {"title": "4.3 Theoretical Analysis", "content": "In this section, we investigate the theoretical relationship between the predicted class probability distribution entropy, the Frobenius norm of the final layer ViT attention heads of the potential AL query candidates, and the C-Core attention head vectors. We then analyze the relationship between our strategy and increased label noise."}, {"title": "4.3.1 Attention, Entropy and the Frobenius Norm", "content": "Starting with the self-attention head outputs of samples in the clean initial random sample:\n$Z = softmax(\\frac{QK^T}{\\sqrt{d_eq}})V$"}, {"title": "4.3.2 Label Smoothing Using the C-Core Attention-Vectors to Reduce the Effects of Label Noise", "content": "In this section we show how label smoothing potentially incorrect oracle labels helps reduce the adverse effects of incorrect labels during training. Mathematically, we can express label smoothing as a form of regularization or penalty-based learning. In the case of multi-class image classification using the cross-entropy loss, consider the following scenario: For a single training example $x_i$ with predicted probabilities $p(\\hat{y}|x_i)$, the cross-entropy loss without label smoothing is given by:\n$L = -\\sum_\\{y=1\\}^\\{C\\}p(y|x_i) log p(\\hat{y}|x_i)$\nwhere $p(y|x_i)$ is the one-hot encoded label from the oracle, and $p(\\hat{y}|x_i)$ is the predicted probability distribution by the model. With label smoothing the cross-entropy loss is given by:\n$L = -\\sum_\\{y=1\\}^\\{C\\}(1 - \\epsilon)p(y|x_i) log p(\\hat{y}|x_i) + \\epsilon \\sum_\\{y=1\\}^\\{C\\}log p(\\hat{y}|x_i)$\nwhere \u03f5 is the smoothing parameter.\nThe term $-\\epsilon \\sum_\\{y=1\\}^\\{C\\}log p(y|x_i)$ in the loss is a regularization term that penalizes the model for being too confident in noisy label settings, thus forcing it to learn more robust decision boundaries between classes. However, in this case, e is shared between the other C 1 classes to distribute uncertainty amongst them equally. In our case, since we have the C-Core attention vector per class, we only label smooth by assigning the C-Core selected class a non-zero probability. This means that each sample that the oracle assigns to a class other than that we would assign using the Frobenious distance between the image's attention vectors and the C-Core vectors is label smoothed to reflect lower than 100% confidence in the oracle's label, and exactly 6% confidence in the class based on the distance to the C-Core attention vectors. Mathematically, we add an indicator variable to Equation 12 so that all other classes that are not the C-Core prediction do not get their zero probability adjusted. The cross-entropy loss based on C-Core Frobenious label smoothing is given by:\n$L = -\\sum_\\{y=1\\}^\\{C\\}(1 - \\epsilon)p(y|x_i) log p(\\hat{y}|x_i) + I(\\hat{y} = \\hat{y}\\_{cc})\\epsilon \\sum_\\{y=1\\}^\\{C\\}logp(y|x_i)$\nwhere\n$I(\\hat{y} = \\hat{y}\\_{cc}) = \\begin\\{cases\\} 1 & \\text{if } y = \\hat{y}\\_{cc} \\\\ 0 & \\text{otherwise} \\end\\{cases\\}$\nand $\u0177\\_{cc}$ represents the class that would be assigned based purely on the Frobenious norm using the C-Core attention vectors. Focusing on the terms of Equation 11, we see the loss is large when the disparity between $p(\\hat{y}|x_i)$ and $p(y|x_i)$ is large. Let us consider two cases, a perfect model (one that can fully predict the underlying ground truth) and a random model (one that assigns random labels for any input). All other scenarios lead to a model contained in the search space of models we seek to optimize. Assuming the perfect model produces $p(\\hat{y}|x_i)$ approximately equal to the ground truth label distribution, then the loss changes based on the noise rate of the oracle. At noise rate r = 0, the loss L \u2248 0, and as r\u21921:L\u2192\u221e.\nComparatively, the smoothed loss in Equation 13 first reduces the confidence placed on the oracle's labels by a small percentage e, and then encourages reliance on the C-Core attention vectors by reducing the additional loss term by a"}, {"title": "5 Experimental Setup", "content": "In this section, we describe the experimental setup that forms a high-dimensional grid of different configurations in active learning for image classification in the presence of label noise. We vary several DL architectures (4), AL algorithms (6), and benchmark datasets (4), as well as the Oracle label noise rates (4). Two of the DL models used in this work are CNN-based while the other two are ViT-based. All CNN-based models are trained in an AL setting with 5 of the 6 AL strategies, and all datasets over all the 4 label noise rates. The 6th AL strategy is GCI-ViTAL and is unique to ViTs. The ViT models in this work are trained under all six AL strategies, all datasets, and label noise settings."}, {"title": "5.1 Deep Learning Models", "content": "For all four DL architectures in this work, the weights are transferred from the pre-training of the model on the ImageNet-1k dataset [46]. We then fine-tune the fully connected layer for classification. The CNN-based models used in this work are: ResNet34 [54] and VGG19 [55, 56], chosen for their popularity in image classification benchmarks as well as good performance. The ViT-based models of choice in this work are the base ViT with 14 non-overlapping 16 by 16 patches and 12 attention heads, and the Swin transformer [39], which implements overlapping shifted patches as opposed to a grid of rigid patches."}, {"title": "5.2 Active Learning Algorithms", "content": "We compare the following standard DAL query strategies: random query, information entropy-based selection, margin sampling, hybrid uncertainty and diversity, model delta, and ours, GCI-ViTAL. Due to a large number of experiments as well as training time, we limit AL strategies to the above six. We briefly explain each method below:\n\u2022 Random Query: This is the simplest query strategy, while also relatively effective. This AL strategy simply selects candidate images for labeling with equal probability from the unlabeled dataset. It does not take into account any information about the unlabeled data except its size, so it is likely to select samples that are not very informative for the model."}, {"title": "7 Discussions", "content": "We show that GCI-ViTAL performs equal or marginally better than most of the AL strategies for lower label noise rates, statistically on par with random selection on low label noise rates, and observably better on higher label noise rates, particularly on CIFAR100 where 60% label noise rate means any sample has a 60% chance of being mislabeled as one of the other 99 classes. We attribute this to GCI-VITAL possessing an in-build pseudo-memory bank of high-accuracy mappings from the C-Core attention maps to labels that come in handy when the oracle's labels are very noisy. The C-Core attention vectors help in selecting outlier samples for a supposed class, and at the same time discount the confidence in the oracle's label under high label noise if it does not match that of the C-Core Frobenius norm-based assignment. In this case, we can show that the additional use of the attention maps learned during clean label pre-training on ImageNet through multi-head attention is advantageous over random selection under high label noise rates. We do this without heavily searching the hyper-parameter space of the underlying DL model in such a manner that would benefit our AL strategy over baseline models, thus reflecting a fairer comparison under label noise.\nAs the label noise rate increases from 0% up to 60%, the test accuracy for all models and AL strategies declines considerably for CIFAR10 and CIFAR100, and only marginally for the Chest X-ray and Food101 datasets. Still, the decline in performance is steeper for the CNN-based models as compared to their transformer-based counterparts. One reason for this could be that the pre-training in the transformer captures adequate local dependencies while at the same time learning rich global dependencies between pixels so that fine-tuning on noisy labels has a less detrimental effect on the overall probability outputs of the model. This is not the case with CNN-based models that have an inductive bias to prioritize local dependencies in explaining the differences in class labels, thus making the transformer more robust to label noise than CNN-based models [61, 62, 63]. To contrast the two in the face of high label noise, CNNs learn a smoother decision boundary between classes while transformers learn a more complex decision boundary [9, 64, 65]. This is because ViTs are larger models with considerably more parameters than CNNs as shown in Table 1, and more parameters allow for a more fine-grained complex decision boundary between classes. The more parameters of transformers allow for the decision boundary to change due to label noise but only change so insignificantly that the change does not lead to the misclassification of many neighboring samples. However we note that the ViT is more robust to label noise in most cases that require complex decision boundaries since it has a smaller trainable-to-frozen parameters ratio, meaning it is forced to condense a lot more of the signal into useful weights, thus it is less likely to overfit to noisy training data as compared to CNN-based models. This leads us to the idea of aiming for larger frozen parameter models for feature extraction and low trainable parameters as future work.\nWhile the use of ViTs in active learning with label noise shows promising results, the high computational cost is a disadvantage that needs to be addressed. Possible ways of addressing this include deploying ViTs with fewer layers or reducing model size through methods such as quantization [66, 67] and model distillation [68, 69, 70]. We are also interested in exploring how ViT models of different sizes (small, base, and large) are impacted by label noise. This can be especially important in navigating the tradeoffs in performance and computational complexity. The more fundamental question that is a result of this work is: in the context of high label noise, at what point do the active learning and few-shot learning paradigms converge, what are the factors, and what role can weak or self-supervised learning play in improving generalization and robustness. Last but not least, in the wake of the advances in large language models (LLMs) as well as multi-modal learning, how can we leverage LLMs to produce text-guided active learning strategies, guide the oracle through caption generations, and create explainable ViT applications in the label noise domain? These ideas are partially inspired by the following works [71, 72, 73]."}, {"title": "7.1 Limitations", "content": "We note that while this work addresses the use of out-of-the-box DL models to give a good reflection of realistic AL baselines, it lacks in providing a comprehensive comparison of the proposed method (GCI-ViTAL) to state-of-the-art AL methods with label noise. The work also does not address or show results of GCI-VITAL on more complex real-world datasets outside the datasets covered due to time and resource constraints."}, {"title": "8 Conclusion", "content": "In conclusion, this study provides valuable insights into the performance of various deep learning models trained and tested on CIFAR10, CIFAR100, Food101, and the Chest X-ray (Pneumonia) datasets, particularly in the context of active learning with label noise. We found that transformer-based models, such as ViT and Swin Transformer, consistently outperformed CNN-based models across all datasets and AL strategies, indicating their superiority in handling complex image classification tasks. We attribute ViT's robustness to their ability to capture both local and global dependencies, resulting in more complex and elastic decision boundaries that are less affected by incorrectly labeled training samples. Furthermore, our results challenge conventional wisdom by showing that random query selection often yields superior classification accuracy in AL with label noise compared to more complex AL strategies that rely solely on uncertainty and diversity metrics. Despite these findings, it is worth noting that transformer-based models incur higher computational costs compared to CNN-based models. GCI-ViTAL, our proposed AL strategy, exhibits higher generalization performance at high label noise rates, albeit with increased computational cost. This strategy leverages the inherent robustness of transformer-based models by making use of the semantic relationships of images without an oracle's labels.\nThis work opens up several avenues for future research. Exploring methods to mitigate the computational overhead of transformer-based models, such as deploying smaller ViT models or applying model quantization and distillation techniques, could be a promising direction. Additionally, investigating the convergence of active learning and few-shot learning paradigms in the context of high label noise, as well as exploring the role of weak or self-supervised learning to improve generalization in label noise scenarios could provide further insights into enhancing model performance. We are also interested in investigating how ViT model size impacts DAL under label noise. In summary, our study contributes to the current understanding of DL models' behavior under DAL scenarios with label noise and suggests potential avenues for addressing challenges in this domain."}]}