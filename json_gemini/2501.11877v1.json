{"title": "From Drafts to Answers:\nUnlocking LLM Potential via Aggregation Fine-Tuning", "authors": ["Yafu Li", "Zhilin Wang", "Tingchen Fu", "Ganqu Cui", "Sen Yang", "Yu Cheng"], "abstract": "Scaling data and model size has been proven\neffective for boosting the performance of large\nlanguage models. In addition to training-time\nscaling, recent studies have revealed that increas-\ning test-time computational resources can fur-\nther improve performance. In this work, we in-\ntroduce Aggregation Fine-Tuning (AFT), a su-\npervised fine-tuning paradigm where the model\nlearns to synthesize multiple draft responses, re-\nferred to as proposals, into a single, refined an-\nswer, termed aggregation. At inference time,\na propose-and-aggregate strategy further boosts\nperformance by iteratively generating proposals\nand aggregating them. Empirical evaluations on\nbenchmark datasets show that AFT-trained mod-\nels substantially outperform standard SFT. No-\ntably, an AFT model, fine-tuned from Llama3.1-\n8B-Base with only 64k data, achieves a 41.3%\nLC win rate on AlpacaEval 2, surpassing sig-\nnificantly larger LLMs such as Llama3.1-405B-\nInstruct and GPT-4. By combining sequential\nrefinement and parallel sampling, the propose-\nand-aggregate framework scales inference-time\ncomputation in a flexible manner. Overall, These\nfindings position AFT as a promising approach\nto unlocking additional capabilities of LLMS\nwithout resorting to increasing data volume or\nmodel size. Our code is publicly available at\nhttps://github.com/Linzwcs/AFT.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) (Dubey et al., 2024; Ope-\nnAI, 2023) have demonstrated remarkable success in ap-\nplications ranging from coding (Li et al., 2023a; Roziere\net al., 2023) and reasoning (Yuan et al., 2023; Qin et al.,\n2024a) to AI assistance (Chiang et al., 2023), driven by\nthe expansion of both data and model capacity during pre-training (Hoffmann et al., 2022; Kaplan et al., 2020) and\nsupervised fine-tuning (SFT) (Yuan et al., 2023; Zhang et al.,\n2024a). Beyond training-time scaling, recent studies (Snell\net al., 2024a; Zhang et al., 2024c; Wang et al., 2024a) in-\ndicate that allocating additional compute at inference can\nfurther boost performance, enabling smaller models to ri-\nval larger ones. Correspondingly, a variety of inference-\ntime scaling algorithms (Madaan et al., 2023; Brown et al.,\n2024; \u015awiechowski et al., 2023) have emerged, including se-\nquentially revising generations or selecting among parallel\nsamples.\nDespite the growing complexity of inference algorithms,\ntypical SFT pipelines still learn a direct mapping from a\nuser query to a reference response. To this end, a line of\nwork attempts to integrate the intermediate inference pro-\ncess such as rationale (Zelikman et al., 2022) and reasoning\ntraces (Tian et al., 2024; Zhang et al., 2024b), into super-\nvised training to improve test-time performance. While\nthese intermediate steps have been shown effectiveness in\nreasoning tasks such as coding and math, they are less\nstraightforward to extend to tasks such as writing or open-\nended dialogue. The intermediate process for general tasks\nremains less discussed.\nIn this work, we seek to find an inference process for\ninstruction-following and integrate it into supervised train-"}, {"title": "2. Related Work", "content": "Supervised Fine-tuning of LLMs. Supervised fine-tuning plays a pivotal role in enhancing the instruction-following ability of LLM (Li et al., 2023b; Zheng et al.,\n2023; Liu et al., 2024b; Ouyang et al., 2022a; Qin et al.,\n2024b; Jiang et al., 2024; Zhou et al., 2023b). Previous\nworks find that high-quality instruction-following data is\nessential for the success of SFT (Zhao et al., 2024; Liu et al.,\n2024a; Xu et al., 2024; Zhou et al., 2023a; Li et al., 2024a)\nand various data-curation techniques are proposed (Wang\net al., 2023; Du et al., 2023; Chen et al., 2024a; Li et al.,\n2024b; Sun et al., 2023). More specifically, including com-\nplex constrained (Sun et al., 2024; He et al., 2024), multi-faced (Lou et al., 2024), self-refined (Zelikman et al., 2022;\nCheng et al., 2024; Wu et al., 2024a) and sequential (Hu\net al., 2024) data could enhance the instruction-following\nability of LLM. Additionally, several studies on reasoning\nsuggest incorporating intermediate processes (e.g., refine-ments in sequential revision (Cheng et al., 2024; Wu et al.,\n2024a) or search traces in step-level graph search (Qin et al.,\n2024a; Xi et al., 2024; Zhang et al., 2024b)) into the SFT\ndata to enhance performance in reasoning tasks. The pro-posed aggregation fine-tuning differs in that it uses aggrega-\ntion as the inference process of general-purpose instruction-following tasks to improve test-time performance.\nInference Scaling for LLMs. Recent research has shifted\nattention from training-time computation to test-time scal-ing. Snell et al. (2024a) show that allocating more compute\nat inference can sometimes prove more effective than enlarg-ing model size. Indeed, with existing inference-time algo-rithms (Wei et al., 2022; Yao et al., 2023; Besta et al., 2024;\nChen et al., 2024b; Brown et al., 2024), smaller models can\nsurpass larger ones when both share a fixed test-time bud-get (Wu et al., 2024b). According to Welleck et al. (2024),\ninference scaling can be broadly categorized into: sequen-tial revision, where an LLM functions as a callable module\nto iteratively refine its prior outputs (Dohan et al., 2022;\nKhattab et al., 2024; Madaan et al., 2023; Shinn et al., 2023;\nWelleck et al., 2023; Havrilla et al., 2024); parallel sam-pling, which generates multiple candidate responses concur-rently and selects the best via a ranker (Brown et al., 2024;\nHuang et al., 2024; Jiang et al., 2023b); and step-level graph"}, {"title": "3. Method", "content": "In a traditional supervised fine-tuning setup, a language\nmodel $f_\\theta: Q \\rightarrow R$ is trained to map each query $q \\in Q$\ndirectly to a single reference response $r^* \\in R$. In contrast,\nwe propose a supervised training framework wherein a lan-guage model aggregates multiple draft responses, referred\nto as proposals, into a refined final response, termed aggre-gation. We define this process as aggregation fine-tuning.\nLet $q \\in Q$ be a query, and $P = \\{r_1,r_2,...,r_K\\} \\subseteq R$\nrepresent a set of K proposals. Given $(q, P)$, the goal of\nAFT is to train an aggregator $A_\\theta: Q \\times R^K \\rightarrow R$, param-eterized by $\\theta$, to produce a refined response $r = A_\\theta(q, P)$\nthat approximates a high-quality reference response $r^* \\in R$.\nDuring inference, the model can iteratively perform aggre-gation $A_\\theta: Q \\times R^{t-1} \\rightarrow R^t$ to refine responses, where $t$\nis the iteration step or aggregation layer.\nThis process contrasts with traditional supervised fine-tuning, which directly maps a query $q$ to a single reference\nresponse $r^*$. By explicitly conditioning on the drafts $P$, the\nmodel requires a meta-skill which goes beyond producing\nan output for a query. It learns a reasoning process that eval-uates, critiques, and improves upon initial solutions. The\noverall framework is illustrated in Figure 2. We provide\na detailed illustration of data construction, model training,\nand inference in the following sections."}, {"title": "3.1. Data Construction", "content": "Constructing training data for aggregation fine-tuning in-volves collecting model proposals and a reference aggrega-tion response, for each user query.\nProposal Collection. We categorize proposals into two\ntypes: off-policy proposals and on-policy proposals. Off-policy proposals are derived from existing preference align-"}, {"title": "3.2. Training", "content": "The goal of aggregation fine-tuning is to maximize the prob-ability that the model's aggregator $A_\\theta$ assigns to the refer-ence aggregated response $r^*$. Formally, for a training dataset\n$\\{(q^{(i)}, p^{(i)}, r^{*(i)})\\}_{i=1}^{N}$, where each example consists of a\nquery $q^{(i)}$, a set of proposals $P^{(i)}$, and a high-quality refer-ence aggregation $r^{*(i)}$, we seek:\n$\\max_{\\theta} \\frac{1}{N} \\sum_{i=1}^{N} A_\\theta(r^{*(i)} | q^{(i)}, p^{(i)}).$\nIn contrast, conventional supervised training learns a map-ping function f from a query to reference response directly:\n$\\max_{\\theta} \\frac{1}{N} \\sum_{i=1}^{N} f_\\theta(r^{*(i)} | q^{(i)})$\nWe use the same prompt as outlined in Section 3.1 to incor-porate proposals. Rather than producing an answer from\nscratch given only the query, the aggregator can build upon\nthe semantic cues, linguistic structures, and partial solutions\nembedded in the drafts. As a result, the model's genera-tion perplexity is significantly reduced, reflecting its ability\nto more efficiently predict the reference tokens due to the\nguidance offered by the existing candidates. Therefore, this\nenriched learning signal leads to a more stable and smooth\ntraining curve, as demonstrated in Section 6.1."}, {"title": "3.3. Inference", "content": "During inference, the AFT model can perform propose-and-aggregate to scale inference-time computing to boost"}, {"title": "4. Experimental Setup", "content": "Data. We construct training data based on UltraFeed-back (Cui et al., 2023). UltraFeedback consists of 64k\ninstances, each of which containing multiple model re-sponses to a user query, providing a natural testbed for\noff-policy proposals. Among these responses, we select\nthe response with the highest score as the reference for"}, {"title": "5. Experimental Results", "content": "In this section, we present the main results of our exper-iments, highlighting the enhanced performance achieved\nthrough aggregation learning and propose-and-aggregate\ninference-time scaling."}, {"title": "5.1. Main Results", "content": "Performance on Single-turn Dialogues. The results on\nAlpacaEval 2 are presented in Table 1. For inference with-out scaling, both AFT-off-policy and AFT-on-policy demon-strate substantial improvements over their SFT counterparts,\nwith an average LC win rate increase of 14.9% for Mistral-based models and 12.9% for Llama-based models. For\nboth model families, AFT-on-policy obtains stronger per-formance compared with AFT-off-policy. The performance\nadvantage can be attributed to on-policy aggregation learn-"}, {"title": "5.2. Downstream Task Performance", "content": "The evaluation results for downstream tasks are summa-rized in Table 3 (MMLU, ARC-CA, and StrategyQA) and\nFigure 5 (GSM8K and IFEval). AFT models demonstrate\ncomparable performance to the SFT baseline on MMLU\nwhile consistently improving outcomes in reasoning tasks\nsuch as ARC-c and StrategyQA. Due to the limited informa-tion provided in the answers for these tasks-specifically,\nsingle-choice options or yes/no responses\u2014the propose-and-aggregate method is not applied. For GSM8K and IFEval,\nwe perform propose-and-aggregate for three runs and re-port mean performance and standard errors. As shown in\nFigure 5, AFT models consistently outperform SFT counter-parts, and the advantages are further expanded with propose-and-aggregate. We can also observe that SFT models,\nwithout aggregation learning, cannot perform propose-and-aggregate to boost performance."}, {"title": "6. Analysis", "content": "In this section, we delve into aggregation learning and the\npropose-and-aggregate framework. We begin by examining\nhow aggregation learning outperforms traditional supervised\nfine-tuning (Section 6.1). Subsequently, we analyze key pro-posal patterns that impact aggregation quality (Section 6.2),\nfollowed by experiments assessing test-time scaling in terms\nof search width and depth (Section 6.3). Finally, we discuss\nthe computational overhead of our method (Section 6.4)."}, {"title": "6.1. Understanding Aggregation Learning", "content": "The training curves in the left portion of Figure 6 demon-strate that aggregation fine-tuning is more efficient and sta-ble compared to standard SFT. AFT achieves lower training\nloss and converges faster while maintaining smoother pro-gression with minimal fluctuations. These results suggest\nthat AFT exerts smaller perturbation on the existing distri-bution of the base model.\nWe sample 1,000 instances from different training dataset\nand calculate the perplexity of the base model before super-vised training, as illustrated in the right section of Figure 6."}, {"title": "6.2. Effects of Proposal Diversity and Quality", "content": "We investigate the effects of proposal diversity and quality\non aggregation quality. Using 100 instances sampled from\nAlpacaEval 2, we employ a reward model to evaluate the\nquality of generated responses (AFT-on-policy based on\nLlama3.1-8B-Base). For each query, we sample 10 propos-als and systematically traverse all possible combinations of\n5 proposals, resulting in altogether 25,200 groups of pro-posals. The model then generates aggregations for each\ncombination, with the quality of these aggregations evalu-ated using the same reward model. To measure proposal\ndiversity, we utilize the Vendi score (Friedman & Dieng,\n2023), while the average quality of proposals within each\ncombination serves as a measure of proposal quality. To en-able comparisons across queries, we normalize the absolute"}, {"title": "6.3. Test-time Scaling along Width and Depth", "content": "As discussed in Section 3.3, the propose-and-aggregate\nframework combines the strengths of sequential revision\nand parallel sampling to enhance inference performance. To\nanalyze its scalability, we conduct experiments on AFT-on-policy (Llama3.1-8B-Base) using the GSM8K and IFEval"}, {"title": "6.4. Computational Overhead", "content": "We analyze the computational overhead of the propose-and-aggregate framework and compare with parallel sam-pling. We approximate inference FLOPs following previ-ous work (Brown et al., 2024) (Details in Appendix C).\nThe primary additional computational cost arises from the\naggregation step, which processes all proposals from the\nprevious layer as input prompts. The FLOPs for propose-and-aggregate, denoted as F, can be approximated as:\n$F \\approx L \\cdot (2\\cdot N \\cdot F) + F$, where F represents the FLOPs for\nvanilla generation, L is the number of aggregation layers\nand N denotes the number of parallel proposals. A com-\nmon parallel sampling baseline is Best-of-N (BON), which\nuses an external reward model to rank multiple generated\nresponses. Assuming the reward model is the same size as\nthe policy model, the FLOPs for BoN can be approximated\nby $F \\approx 2 \\cdot N \\cdot F$.\nTable 5 compares propose-and-aggregate to BoN under com-parative FLOPs. Even when equipped with BoN, an SFT\nmodel cannot surpass the performance of an AFT model.\nFurthermore, propose-and-aggregate offers greater improve-ments on AFT compared to BoN, indicating the benefit of ex-panding along both width (parallel proposals) and depth (it-"}, {"title": "7. Conclusion", "content": "In this work, we introduced aggregation fine-tuning as a\nparadigm that teaches language models to aggregate mul-tiple draft responses (i.e., proposals) into a refined final\nanswer (i.e., aggregation). By conditioning on diverse pro-posals, AFT encourages higher-order reasoning and demon-strates consistent gains over standard supervised fine-tuning\nacross both instruction-following benchmarks and down-stream tasks. Furthermore, our propose-and-aggregate\nmethod leverages iterative inference to further improve per-formance without additional training, combining the merits\nof both sequential refinement and parallel sampling. Anal-ysis showed that AFT reduces perplexity, stabilizes con-vergence, and benefits from carefully balancing proposal\nquality and diversity, making it a flexible and cost-effective\napproach to unlocking latent model capabilities."}, {"title": "Impact Statement", "content": "This work aims to advance the field of Machine Learning\nby providing a new training and inference paradigm, i.e.,\nAggregation Fine-Tuning, that refines multiple candidate\nanswers into a stronger overall solution. While we do not\nforesee any immediate or significant societal harms unique\nto this approach, it is possible that more powerful language\ngeneration techniques could be misused for deceptive or\nharmful purposes (e.g., generating misleading content). We\nencourage future efforts to incorporate robust filtering and\nresponsible usage guidelines for such systems. Beyond\nthese considerations, the potential positive societal impacts\ninclude more accurate, reliable, and resource-efficient lan-guage models, which could benefit a wide range of applica-tions from education to healthcare."}]}