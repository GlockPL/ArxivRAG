{"title": "Exploring the Potentials and Challenges of Using Large Language Models for the Analysis of Transcriptional Regulation of Long Non-coding RNAs", "authors": ["Wei Wang", "Zhichao Hou", "Xiaorui Liu", "Xinxia Peng"], "abstract": "Research on long non-coding RNAs (IncRNAs) has garnered significant attention due to their critical roles in gene regulation and disease mechanisms. However, the complexity and diversity of IncRNA sequences, along with the limited knowledge of their functional mechanisms and the regulation of their expressions, pose significant challenges to IncRNA studies. Given the tremendous success of large language models (LLMs) in capturing complex dependencies in sequential data, this study aims to systematically explore the potential and limitations of LLMs in the sequence analysis related to the transcriptional regulation of IncRNA genes. Our extensive experiments demonstrated promising performance of fine-tuned genome foundation models on progressively complex tasks. Furthermore, we conducted an insightful analysis of the critical impact of task complexity, model selection, data quality, and biological interpretability for the studies of the regulation of IncRNA gene expression.", "sections": [{"title": "1 Introduction", "content": "Long non-coding RNAs (IncRNAs) are broadly defined as transcripts greater than 500 nucleotides in length and with low potential translating into proteins [21]. The discovery of many IncRNA genes has attracted great attention due to their critical roles in multiple cellular processes, such as gene regulation [12,28], cellular differentiation, and development [5,30]. LncRNAs can also play important roles in immunity and host response to infection [13,16,18,23,25].\nIt is now evident that in the human genome there are more IncRNA genes than protein coding genes. In the recent GENCODE V47 release, there are 35,934 annotated human IncRNA genes, compared to 19,433 protein-coding genes. However, the regulation and functions of most IncRNAs are still unknown. The analysis of IncRNA regulation and functions remains extremely challenging due to their diverse functions, complex gene regulatory mechanisms, and significantly lower expression levels compared to protein-coding genes [21, 24, 31]. Furthermore, most IncRNA sequences are much less conserved across species compared to transcript sequences encoding proteins [21,24]. LncRNAs also often lack clear sequence motifs or structural signatures, making their identification and functional prediction challenging [17].\nRecent revolutionary developments in artificial intelligence, particularly in natural language processing (NLP), have shed light on genomic research. Large language models (LLMs), such as GPT-3 [4], have demonstrated remarkable ability to capture complex dependencies and patterns in sequential data, and are now rapidly emerging in biological sequence analysis. Recent genome LLM models like DNABERT [15], Nucleotide Transformer [9], and scGPT [8] have shown promising performance in various genomic sequence tasks, such as promoter-enhancer interaction prediction and functional element identification. However, the potential of LLMs in IncRNA analysis remains unexplored.\nTo address this research gap, here we aim to leverage LLMs, with their ability to learn contextual information and long-range dependencies, to overcome the limitations of conventional computational approaches and provide new insights into IncRNA biology. We conducted a comprehensive exploration of using LLMs in the sequence analysis for transcriptional regulation of IncRNA gene expression, by fine-tuning pre-trained genome foundation models. By systematically evaluating the performance of LLMs on these IncRNA-related tasks, we seek to bridge the gap between computational predictions and biological understanding, and contribute to the development of more reliable computational methods for IncRNA research. Our contributions are as follows:\n\u2022 We are the first to conduct a comprehensive evaluation of fine-tuning genome foundation models, including DNABERT [15], DNABERT-2 [33], and Nucleotide Transformer [9], on the regulation of IncRNA gene expression related tasks.\n\u2022 We designed a series of tasks that progressively increase in complexity and relevance to IncRNA analysis, including biological vs. artificial sequence classification, promoter sequence detection, highly vs. lowly expressed gene pro-"}, {"title": "2 Related Work", "content": "The landscape of natural language processing has been transformed by the development of LLMs, which can capture complex dependencies in sequential data, outperform traditional methods, and are applicable to a broader range of real-world scenarios across various domains.\nLLMs have significantly contributed to biological sequence analysis, including DNA and RNA analysis [1,9, 15, 32], protein analysis [19, 26], single-cell transcriptome sequencing (scRNAseq) analysis [8], and noncoding RNA (ncRNA) analysis [1]. For DNA and RNA analysis, DNABERT [15] and DNABERT-2 [33] are two advanced genome foundation models that demonstrate impressive performance in tasks like predicting promoter-enhancer interactions. The Nucleotide Transformer [9] extends the DNABERT series models [15,33] to both DNA and RNA sequences. Enformer [3] introduces a self-attention-based architecture to predict gene expression from DNA sequences, while HyenaDNA [22] combines aspects of transformers and long convolutions to model long-range dependencies in genomic sequences.\nFor protein sequence analysis, the ESM-2 model [19] achieves state-of-the-art performance in protein structure and function prediction tasks. scGPT [8], designed for scRNAseq sequencing data, provides new insights into cellular heterogeneity and gene expression patterns. Additionally, ncRNA analysis is also benefiting from the development of LLMs. Early work by Aoki et al. [2] introduced CNNs for ncRNA classification, and more recently, RNABERT [1] has demonstrated significant improvements in ncRNA clustering and RNA structural alignment for human small ncRNAs with lengths ranging from 20 to 440 nucleotides. A recent study shows that a transformer based RNA-focused pretrained model is effective in several RNA learning tasks including RNA sequence classification, RNA-RNA interaction, and RNA secondary structure prediction [29].\nDespite the rapidly growing research on LLMs for biological sequence analysis, the impact of diverse data, models, and tasks on LLMs performance is not yet fully understood. These factors can significantly affect model performance and potentially lead to incorrect biological insights, which could have catastrophic consequences, especially in medical-related applications. In this study, we aim to thoroughly evaluate LLMs performance on the transcriptional regulation of IncRNAs related tasks and investigate the critical impact of task complexity, model selection, and data quality on IncRNA analysis."}, {"title": "3 Method", "content": "In this section, we aim to explore the abilities and limitations of pre-trained foundation models in IncRNA related sequence analysis. We will first introduce three well-established genome foundation models, followed by an explanation of fine-tuning techniques and a series of downstream tasks at various levels of difficulty. We provide a systematic overview of our study in Fig. 1."}, {"title": "3.1 Foundation Models", "content": "Foundation models (FMs) are a class of large-scale, pre-trained models that serve as a versatile starting point and can be fine-tuned for a wide range of tasks across various domains. In genome sequence analysis, foundation models are typically trained on a large amount of unlabeled human genome data, which enables the models to learn general representations of biological sequences and capture complex dependencies and patterns within them. Moreover, fine-tuning these foundation models with relatively small amounts of task-specific data can adapt models to specific downstream tasks and achieve promising performance."}, {"title": "3.2 Finetuning on Downstream Tasks", "content": "To explore the capabilities and limitations of foundation models in IncRNA related sequence analysis, we designed several downstream tasks to evaluate their fine-tuning performance and gain insights from the experimental results.\nFinetuning. Considering the different sizes of three foundation models, we employed different fine-tuning strategies to adapt them to specific downstream tasks. For DNABERT and DNABERT-2, we performed standard fine-tuning using optimizer AdamW [20], with a learning rate of 3e-5, a batch size of 32, 50 warmup steps, and a weight decay of 0.01. Since NT is an order of magnitude larger than the DNABERT and DNABERT-2 models, we fine-tuned it using the LORA technique to improve efficiency. For LoRA, we set the alpha to 16, dropout to 0.05, and r to 8, with a learning rate of 1e-4. The parameters were reused from DNABERT-2, based on preliminary grid search results for hyperparameter selection. All tasks shared the same fine-tuning parameters.\nDownstream tasks. To comprehensively evaluate the performance of using LLMs in IncRNA-related tasks, we fine-tuned the models on the following four tasks, ranging from easy to complex levels. These tasks were designed to address the intriguing observation that IncRNA genes tend to have much low expression levels than protein coding genes in general [21] [24].\n\u2022 (Easy) Task 1: Biological vs. Artificial Sequence Classification served as a key quality control step for models. It focused on distinguishing between naturally occurring sequences in biological systems (such as DNA, RNA, or protein sequences) and those artificially generated through random processes, computational algorithms, or laboratory synthesis."}, {"title": "3.3 Evaluation", "content": "In this section, we introduce the evaluation aspects, including classical baseline models, evaluation metrics, and interpretable feature importance analysis.\nBaseline models. To establish benchmark performance, validate the necessity of using complex models, and quantify the incremental value brought by advanced models, we set up a simple baseline using Logistic Regression (LR) [7] with n-gram Term Frequency-Inverse Document Frequency (TF-IDF) [27] features for four biological sequence classification tasks. LR is a widely used statistical method for binary classification tasks. In combination with n-gram TF-IDF features, LR becomes a powerful tool for sequence classification. Compared to large-scale foundational models, Logistic Regression requires minimal computational resources and is much faster to train and evaluate.\nMetrics. We included three metrics to evaluate model performance in the experiments: (1) Accuracy is the ratio of correctly predicted instances (both positive and negative) to the total number of instances. (2) F1 Score is the harmonic mean of precision and recall, which is particularly useful for imbalanced classes as it considers both false positives and false negatives. (3) Matthews Correlation Coefficient (MCC) is a metric used to evaluate the quality of binary classifications and provides a balanced measure, even when the classes are of different sizes.\nFeature importance analysis. To gain a deeper understanding of which regions contribute most to the predictions in promoter sequence classification tasks, we conducted a feature importance analysis based on the attention scores of the LLMs. These attention scores were aggregated and visualized to emphasize the regions with the highest importance, potentially corresponding to functional motifs that regulate gene expression. To further validate the biological relevance of the insights provided by the LLMs, we compared the identified important regions from the feature importance analysis with known regulatory motifs and elements from existing databases and publications."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Datasets", "content": "In our experiments, we fine-tuned the foundation models over four datasets:\n(1) Biological vs. artificial sequence dataset was obtained from the Genome Understanding Evaluation (GUE) dataset [33]. The non-TATA promoter detection (human) dataset consists of 26,533 positive samples and 26,533 negative samples. Positive sequences were extracted from non-TATA promoter sequences, which are promoter sequences without the TATA motif, downloaded from the Eukaryotic Promoter Database (EPDnew) [10]. These promoter sequences were extracted from the region 249 bp upstream of the transcription start site (TSS) and 50 bp downstream, in total 300 bp. Negative samples were generated by randomly reshuffling the positive sequences.\n(2) Promoter vs. non-promoter sequence dataset was generated based on the non-TATA promoter sequence classification. We reused the positive samples from the GUE dataset, but regenerated the negative samples by randomly sampling 300 bp sequences from the human genome outside of promoter regions. This custom dataset contains 26,533 positive samples from the non-TATA promoter detection dataset of GUE and 26,533 negative samples.\n(3) Promoter sequences of high vs. low expression gene dataset was constructed through the following steps. Gene expression data from 49 tissues was downloaded from the Genotype-Tissue Expression (GTEx) V8 database [6]. High-expression genes were defined as genes with high expression levels in most tissues, where the expression levels were above the 75th percentile in all 49 tissues. Low-expression genes were defined as those with expression levels below the 25th percentile in all tissues. We generated promoter sequence datasets with varying lengths 1 (300 bp, 500 bp, 1000 bp, and 2000 bp). All promoter sequences were extracted from the region 1 - 49 bp upstream of the TSS and 50 bp downstream. This dataset contains 2,772 positive samples and 2,772 negative samples. As expected, the positive samples had more promoter sequences from protein coding than IncRNA genes (2,740 from protein coding genes vs. 32 from IncRNA genes), and the majority of the negative samples were from IncRNA genes (1,990 from IncRNA genes vs. 782 from protein coding genes).\n(4) Promoter sequences of protein coding vs. IncRNA genes dataset was another custom dataset that we generated to further explore the potentials of fine-tuned LLMs. We downloaded the human reference genome annotation file from the Ensembl database (Homo_sapiens.GRCh38.110.gtf) and extracted the positions of protein coding and IncRNA genes by filtering based on gene biotype. This dataset contains 10,239 positive samples and the same size of negative samples. Among the 10,239 positive samples, 2,546 (24.86%) overlapped with the samples in the high vs. low expression gene dataset described above. Among the 10,239 negative samples, 2,022 (19.75%) overlapped with the samples in the high vs. low expression gene dataset."}, {"title": "4.2 Main Results", "content": "In this section, we evaluate the fine-tuning performance of LLMs on four IncRNA-related tasks with varying levels of difficulty. Specifically, we fine-tuned DNABERT, DNABERT-2, and the Nucleotide Transformer, and compared their performance to that of a traditional logistic regression model. The results are presented according to the specific tasks tested.\nEffect of prompoter sequence length. To evaluate the impact of sequence length on model performance, we conducted an experiment on promoter sequence classification of highly and lowly expressed genes using different promoter sequence lengths. The results in Table 1 showed that all models performed better with shorter promoter sequences. These findings suggest that shorter sequences already encompassed specific regulatory regions, making them easier for models to capture, while longer sequences might have introduced additional complexity and noise."}, {"title": "Effect of prompoter sequence length", "content": "To evaluate the impact of sequence length on model performance, we conducted an experiment on promoter sequence classification of highly and lowly expressed genes using different promoter sequence lengths. The results in Table 1 showed that all models performed better with shorter promoter sequences. These findings suggest that shorter sequences already encompassed specific regulatory regions, making them easier for models to capture, while longer sequences might have introduced additional complexity and noise."}, {"title": "(1) Biological vs. artificial sequence classification (Task 1)", "content": "First, we evaluated the performance of fine-tuned foundation models on the biological and artificial sequence classification task. This task was less challenging due to the significant differences between real biological sequences and reshuffled artificial sequences. As shown in Table 2, DNABERT (3-mer) achieved the highest Matthews correlation coefficient (MCC) of 93.8, followed closely by DNABERT-2 (92.59) and Nucleotide Transformer (89.89). The LR model also performed well on this task, achieving an MCC of 86.68. These results indicate that for this simple task, traditional machine learning methods are nearly as effective as advanced LLMs, but with significantly lower model complexity and computational cost."}, {"title": "(2) Promoter vs. non-promoter sequence classification (Task 2)", "content": "The second task focused on distinguishing between promoter and non-promoter biological sequences. Compared to the previous biological vs. artificial classification task, detecting promoter regions was more challenging, leading to a performance drop across all models (Table 2). DNABERT (3-mer) outperformed the other models, achieving an MCC of 83.41. The Nucleotide Transformer performed similarly, with an MCC of 83.23, while DNABERT-2 achieved 80.58 and the LR model achieved 75.03. The significant performance drop of the LR model highlights the limitations of simpler models in handling more complex tasks."}, {"title": "(3) Highly vs. lowly expressed gene promoter sequence classification (Task 3)", "content": "This task aimed to explore potential regulatory mechanisms associated with low gene expression levels. LncRNA genes tend to exhibit significantly lower expression levels compared to coding genes [21] [24], also as shown in Table 3. As illustrated in Table 2, DNABERT (3-mer) achieved the best performance with an MCC of 73.48, followed by Nucleotide Transformer at 70.10 and DNABERT-2 at 67.78. These results demonstrate that LLMs were effective for tasks requiring deeper contextual understanding, such as the complex relationships between promoter sequences and gene expression levels. Since the majority (98.85%) of the highly expressed genes were protein-coding genes and the majority (71.79%) of the lowly expressed genes were IncRNA genes, these results suggest that the low expression of some IncRNA genes could be mostly explained by the special features of the promoter sequences that they are regulated through."}, {"title": "(4) Protein coding vs. IncRNA gene promoter sequence classification (Task 4)", "content": "The more challenging task was the classification of protein coding and IncRNA gene promoter sequences directly. As expected, the performance of all models was significantly lower in this task (Table 2). DNABERT-2 achieved the highest MCC of 41.93, slightly outperforming DNABERT (3-mer) with 41.55. The Nucleotide Transformer performed the worst, with an MCC of 35.72. Interestingly, the LR model achieved an MCC of 37.10, outperforming the Nucleotide Transformer on this task. This results indicate that the LLMs did not fully capture the contextual information needed to distinguish between coding and IncRNA gene promoters, and that simpler models could still be competitive in certain tasks. Also since the majority of the promoter sequences did not overlap the highly vs. lowly expressed gene dataset, these results also suggest that potentially additional variables such as specific regulators that restrict the expression of IncRNAs under various conditions such as different cell types or developmental stages need to be included in the model to accurately predict IncRNA gene expression levels."}, {"title": "4.3 Additional Experiments", "content": "We then performed additional experiments including feature importance analysis to further improve interpretability and validate the effectiveness of the fine-tuned foundation models.\nFeature importance analysis. We conducted a feature importance analysis using the attention mechanisms of LLMs. For the highly vs lowly expressed gene expression prediction task (Task 3), the feature importance results in Fig. 2 indicate that the initial 80bp sequences upstream of the TSSs contributed the most to the prediction of gene expression levels. This 80bp regions likely contained key regulatory elements that could strongly influence gene expression, aligning with a recent study by Duttke et al. [11]. The position of activator binding sites relative to the TSS is crucial in determining gene expression. Generally, activator binding sites are concentrated upstream of the core promoter region, between -40 and +40bp relative to the TSS. Our experiments on various promoter sequence lengths provided further evidence that proximal promoter regions may contain critical binding sites for transcription factors that regulate gene expression.\nEffect of k-mer size. To further investigate model behavior, we also conducted a parameter sensitivity study on k-mer sizes in DNABERT for the promoter sequence detection task. As shown in Table 4, DNABERT's performance remained stable across different k-mer sizes, suggesting that the model may be relatively insensitive to the choice of k-mer in some IncRNA related analysis tasks."}, {"title": "5 Discussion", "content": "This study explored the application of LLMs in the transcrptional regulation of IncRNAs related sequence analysis across various tasks. We obtained key insights from our results. First, in terms of task complexity & model selection, fine-tuned LLMs significantly outperformed traditional models in more challenging tasks while simpler models like LR remain competitive in less complex tasks. These findings highlight that model selection should highly depend on task complexity. LLMs are best reserved for tasks requiring deep contextual understanding, while simpler models are a better choice for straightforward classification problems, considering efficiency. Next, data quality emerged as a crucial determinant of model performance. In the biological vs. artificial sequence classification task, artificially generated data inflated model performance. This highlights the importance of using high-quality, task-specific data when applying LLMs in biological research. Moreover, the impact of Promoter sequence length on model performance was another key finding. Model performance declined as the sequence length increased, likely due to added complexity and noise. Shorter sequences tend to contain more concentrated regulatory elements, therefore easier for LLMs to capture relevant features. Thus, investigating appropriate sequence lengths will be critical for evaluating model accuracy in future research. Lastly, LLMs fine-tuned on IncRNA-related data also provided biologically informed interpretability through the lens of the attention mechanism. The attention-based feature importance analysis offered valuable biological insights, revealing that LLMs can highlight key regulatory regions within sequences. This capability enhances model interpretability and supports the discovery of novel regulatory motifs, making them more applicable to real-world biological problems."}, {"title": "6 Conclusion", "content": "In this study, we investigated the application of LLMs to transcriptional regulation of IncRNAs related sequence analysis, by fine-tuning pre-trained genome foundation models. These experiments demonstrated the critical impact of task complexity, model selection, and data quality on the performance. The results from tasks of varying complexity levels and feature importance analysis highlighted the importance of integrating domain knowledge into LLMs training and fine-tuning processes. By incorporating biological insights into model development and task design, we may guide LLMs to further improve both performance and interpretability. In conclusion, this work explored the foundation models for IncRNA related sequence analysis and demonstrated the promising potential of combining LLMs with biological research for future advancements."}]}