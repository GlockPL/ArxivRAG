{"title": "VERA: Validation and Evaluation of Retrieval-Augmented systems", "authors": ["Tianyu Ding", "Adi Banerjee", "Yunhong Li", "Laurent Mombaerts", "Tarik Borogovac", "Juan Pablo De la Cruz Weinstein"], "abstract": "The increasing use of Retrieval-Augmented Generation (RAG) systems in various applications necessitates stringent protocols to ensure RAG systems' accuracy, safety, and alignment with user intentions. In this paper, we introduce VERA (Validation and Evaluation of Retrieval-Augmented Systems), a framework designed to enhance the transparency and reliability of outputs from large language models (LLMs) that utilize retrieved information. VERA improves the way we evaluate RAG systems in two important ways: (1) it introduces a cross-encoder based mechanism that encompasses a set of multidimensional metrics into a single comprehensive ranking score, addressing the challenge of prioritizing individual metrics, and (2) it employs Bootstrap statistics on LLM-based metrics across the document repository to establish confidence bounds, ensuring the repository's topical coverage and improving the overall reliability of retrieval systems. Through several use cases, we demonstrate how VERA can strengthen decision-making processes and trust in AI applications. Our findings not only contribute to the theoretical understanding of LLM-based RAG evaluation metric but also promote the practical implementation of responsible AI systems, marking a significant advancement in the development of reliable and transparent generative AI technologies.", "sections": [{"title": "1 INTRODUCTION", "content": "The integration of Retrieval-Augmented Generation (RAG) systems with Large Language Models (LLMs) has significantly advanced the field of natural language processing, particularly enhancing capabilities in areas such as open-domain question answering, fact-checking, and customer service support. These systems combine extensive data repositories with sophisticated generative capabilities to produce responses that are both relevant and informative [13, 18].\nDespite recent advancements, RAG systems rely on LLMs and hence face similar challenges, such as untraceable reasoning processes, supporting evidence is not provided as part of the answers, the production of \"hallucinated\" responses and answers that are coherent but factually incorrect or irrelevant [22]. Furthermore, integrating these systems with additional databases presents unique challenges. Since these databases are static, they can have limited coverage on topics and can lead to outdated responses. Additionally, their large volumes can result in high computational costs.\nTraditional methods for evaluating RAG systems involve extensive manual annotations and continuous human monitoring, both of which are resource-intensive [44]. To address these challenges, we have developed VERA, a scalable RAG evaluation method that utilizes LLM-based evaluation mechanisms and statistical estimators to provide annotations and evaluation tools suitable for production environments.\nVERA efficiently evaluates both the retrieval and generation phases of RAG systems by measuring retrieval precision and recall to ensure optimal information retrieval and assessing the faithfulness and relevance of generated answers. Additionally, VERA enhances its evaluation by leveraging a cross-encoder that incorporates these retrieval and generation metrics to yield a single comprehensive score that can be used to rank RAG systems against each other. This singular score enables users to quickly ascertain the performance of their RAG systems, as well as make any engineering decisions related to it. For instance, whether to roll-back a deployment that caused an unforeseen change to their RAG performance [30].\nFurthermore, VERA introduces an innovative method that utilizes bootstrap estimators to validate and assess the topicality of"}, {"title": "2 RELATED WORK", "content": "Traditionally, RAG models were evaluated based on their performance in specific downstream tasks, utilizing established metrics like EM and F1 scores for entity or sentiment classification, BERTScore and MoverScore for question answering, or accuracy for fact-checking [12, 21, 31, 36, 41, 42]. Tools like RALLE automate this process using task-specific metrics [15]. State-of-the-art evaluation tools such as EXAM and RAGAS propose various quantifications for RAG retrieval and generation effectiveness, including context relevance and answer faithfulness [2, 25]. BARTScore and SelfCheckGPT focus on generation factuality and coherency. RAG evaluation also encompasses abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, counterfactual robustness, and guideline adherence [6, 19].\nDespite developments in evaluation metrics and tools, quantifying different aspects in RAG remains challenging due to uncertainties in inputs and outputs and limitations of existing benchmarks in capturing human preferences. The Large Model Systems Organization (LMSYS) group explores the feasibility and pros/cons of using various LLMs as automated judge for tasks in writing, math, and world knowledge [43]. Their results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. The G-EVAL proposed by Microsoft Cognitive Services Research group with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, along with other studies confirming GPT's ability to achieve state-of-the-art or competitive correlation with human judgments [20, 32]. Furthermore, several initiatives leverage LLM prompting to evaluate performance across diverse tasks such as translation, summarization, and dialogue [14]. These studies point out that LLMs offer a scalable and explainable alternative to human evaluation, which are otherwise very expensive to obtain [43].\nLastly, given that RAGs rely on a retrieval model to retrieve relevant documents, their performance is pegged to the efficacy of the semantic search within the retriever. As the quality of semantic search is dependent on document ingestion and chunking strategies employed, the retriever can be made more robust by a re-ranking"}, {"title": "3 VERA METHOD", "content": "VERA first systematically assesses the integrity of document repositories using LLM-based metrics, such as Retrieval Precision, Recall, Faithfulness, and Answer Relevance. It then applies advanced techniques like rank-based aggregation and bootstrapping to enhance the usability, reliability, and reproducibility of these metrics. Finally, it conducts contrastive analysis to evaluate document repository topicality [35]. This approach not only evaluates the relevance and accuracy of document retrieval but also ensures the integrity and thematic consistency of the information retrieved. Section 3.1 covers how VERA uses LLMs to generate integrity related metrics. Section 3.2 discusses rank-based aggregation. Section 3.3 introduces the bootstrapping technique. Section 3.4 details how contrastive analysis is used to assess document topicality."}, {"title": "3.1 LLMs as Evaluators", "content": "Recent advances in LLMs' information retrieval, understanding of nuances, and reasoning abilities have made their applications in high-stakes tasks such as system evaluations practical and feasible [4, 8]. VERA uses Anthropic Claude V3 Haiku through Amazon Bedrock service as the default LLM for RAG evaluations, due to Haiku's balance between cost and effectiveness. Haiku achieves competitive performance on major reasoning dataset: 75.2% on MMLU [9], 89.2% on ARC-Challenge [40] and 85.9% on HellaSwag [1]. On each of the dataset, it has surpassed GPT-3.5 over all those three evaluation benchmark datasets. A different LLM can be chosen based on the model's merits, specific use cases, and costs.\nLike existing LLM-based RAG evaluation system such as RAGAS or ARES [25], VERA has measured the following LLM-based evaluation metrics. The prompts to create the metrics are listed in Appendix 8.1.\nFaithfulness: This metric evaluates whether answers are based solely on the provided contexts, without any fabrication. The prompt will instruct the language model to generate a binary \"yes\" or \"no\" label for each (q, a, c) pair, where q, a, and c represent the question, answer, and context, respectively. The faithfulness metric for a set of (q, a, c) pairs is calculated as the average of all the binary labels.\nRetrieval Recall: This metric evaluates the system's effectiveness in fetching all relevant information related to a query from the given context, ensuring that no significant data is omitted. This metric is determined by assessing whether each piece of information in the answer is explicitly supported by the context. The retrieval"}, {"title": "3.2 Consolidation of Multi-Dimensional Evaluation Metrics", "content": "The concept of consolidating evaluation metrics into a single comprehensive score involves integrating the utilities of each metric, allowing users to make informed decisions despite the inherent fluctuations in these metrics. Appropriate consolidation eases the burden of users having to parse through multiple metrics to then make a decision based on the outcome - which would improve iteration speed in a development cycle. Furthermore, given that each of these multi-dimensional metrics has its nuances, the question of how to prioritize certain metrics over others arises (e.g. does a system with higher faithfulness and lower relevance outperform the system with lower faithfulness and higher relevance). This would assist users to swiftly take action during regression testing, to make decisions on whether to roll-back a deployment or not.\nTraditional techniques like simple aggregation or rank fusion often suffer from compensatory effects and lack clarity, as they obscure the subtleties of individual metrics [5, 26].\nTo address these challenges, VERA utilizes cross-encoder models that leverage a cross-attention mechanism for a more precise evaluation of document relevance. Traditional cross-encoder models are effective at highlighting the most relevant text segments within large texts, based on capturing semantic relationships between words and phrases. It generates a relevance score for every question-answer pair, enabling an effective comparison and ranking of these pairs. Formally, for a user-input question q and an answer"}, {"title": "3.3 Bootstrap LLM-based RAG Evaluation Metrics", "content": "Evaluating RAG systems requires measuring retrieval precision, recall, faithfulness, and relevance. However, these metrics can vary due to LLMs' stochasticity, reasoning limitations, and document repository topicality. To address this, we used bootstrapping on pre-computed metric values. This approach enhances result reliability and reproducibility by providing a robust statistical framework to analyze metric variability and distribution, while also supporting document repository topicality assessment for specific content types [14].\nLLMs can produce varying outputs due to factors like random seed values, causing traditional evaluation to capture only a snapshot of this variability and potentially misleading performance conclusions. By applying bootstrap directly on the metric values, we can simulate multiple runs of model evaluations, capturing a broader spectrum of possible outcomes and thus providing a more comprehensive picture of system performance.\nBootstrapping metric values allows for repeated sampling from a set of observed metric computations, essentially creating numerous virtual evaluation scenarios. The bootstrapping metric values computation are identical for all metrics.\nGiven a known metric M: Firstly, compute its values for a document repository dataset D = {d1, d2, ..., dn} as M(di) for each document di. This results in a set of metric values M = {m1, m2, ..., mn}. Then, for each metric M, generate B bootstrap samples. Each sample s is created by randomly selecting metric values from M with replacement. Each bootstrap sample for metric M can be represented as Ms = {m1, m2, ..., mn}. For each bootstrap sample, compute the desired statistics, such as the sample mean and variance as below:\n\u2022 Estimates the Mean and Variability: Provides a statistically robust way to estimate the mean and variance of performance metrics, incorporating the inherent randomness of LLM outputs. The mean M of the bootstrap samples is estimated as:\nM = \\frac{1}{B} \\sum_{s=1}^{B} m_{s}\nAnd the variance \u03c3\u00b2 (M) is:\n\u03c3\u00b2 (M) = \\frac{1}{B-1} \\sum_{s=1}^{B} (m_{s} - M)^{2}\n\u2022 Confidence intervals: Can be derived from the percentiles of the bootstrap distribution, typically the 2.5th and 97.5th percentiles for a 95% confidence interval."}, {"title": "3.4 Evaluating Document Repository Topicality Using Contrastive Query Analysis", "content": "Document repositories often contain diverse content, leading to high entropy for domain-specific information retrieval and making it challenging to ascertain the repository's thematic topicality, especially in specialized industry domains. To address this, we implement a contrastive analysis framework, differentiating responses to topic perfect relevant queries (positive instances) from responses to unrelated queries (negative controls). Within the framework, we have proposed a bootstrap estimation approach that provides a structured statistical analysis to evaluate the repository's thematic consistency.\nThe approach involves several key steps with the idea ignited from contrastive learning [7]:\n\u2022 Query Generation: Develop two distinct sets of queries. Positive queries set are relevant to a specific domain of interest, and negative queries are deliberately chosen to be unrelated to that domain.\n\u2022 Retrieval and Evaluation: Utilize a large language model (LLM) or a similar retrieval system to fetch and evaluate responses for each query. Evaluation metrics such as Retrieval Precision, Recall, Faithfulness, and Answer Relevance are calculated to assess the quality and relevance of the responses.\n\u2022 Bootstrap Statistics: Apply bootstrap sampling techniques to each evaluation metrics. This involves generating numerous subsamples from the collected metrics and computing statistical measures (e.g., mean, variance) for these samples to analyze the data robustly.\n\u2022 Comparative Analysis: Compare the distributions of these bootstrap statistics between the positive and negative query sets. This step quantitatively assesses the repository's content alignment with the domain of interest and identifies any significant disparities in content handling between relevant and irrelevant queries."}, {"title": "4 EXPERIMENTS", "content": "In this section, we present the models and data used by VERA. VERA uses both public and proprietary datasets to ensure a comprehensive analysis. We utilize the open-source MS MARCO in TREC 2023 Deep Learning Track for a general knowledge [17]. Simultaneously, we incorporate proprietary datasets tailored to AWS sales and marketing domain-specific evaluations, reflecting the unique challenges and requirements of different industry sectors. This combination allows us to assess the general applicability and targeted performance of our RAG systems, facilitating a thorough understanding of their capabilities and areas for optimization in real-world scenarios."}, {"title": "4.1 Models", "content": "For domain-specific synthetic data generation, we employ Anthropic V3 Haiku to create high-quality synthetic queries and responses tailored for our experimental needs. This model's advanced generative capabilities ensure that the synthetic datasets are both diverse and closely aligned with the task-specific requirements. For the evaluation of responses, we utilize Anthropic V3 Sonnet, which serves as our LLM judge. The examples of synthetic generation prompts, evaluation prompts and RAG summarization prompt using Llama 3 supported in POE Web UI [28] are in Appendix 8.1. In our experiments, we compared the performance of multiple RAG systems by pairing different combinations of LLMs-specifically Anthropic Haiku and Llama3-with a selection of advanced retrievers. The retrievers we've chosen include e5-mistral-7b-instruct, titan-embedding-text-G1, and bge-large-en-v1.5, all of which are recognized as top models on the MTEB leaderboard, indicative of their superior performance and capability in handling complex retrieval tasks [3, 33, 34, 38]. This diverse combination of cutting-edge LLMs and retrievers allows us to thoroughly assess and contrast the strengths and limitations of different RAG configurations in producing relevant and accurate responses."}, {"title": "4.2 Datasets", "content": "The TREC 2023 Deep Learning Track emphasizes enhancing information retrieval with large-scale datasets suitable for deep learning, focusing on passage and document ranking tasks. It utilizes the MS MARCO dataset to analyze and develop effective retrieval and reranking systems in real-world scenarios. In this research, we'll focus on using the smaller passage ranking data from the TREC 2023 Deep Learning Track for experimental purpose. For the purpose of our experiments, we have used all 887 unique perfectly relevant query-passage pairs (score=3) from \"2021.qrels.docs.final.txt\" and 500 randomly sampled irrelevant query-passage pairs (score=0).\nAdditionally, we have generated 400 passages related to cloud computation sales and marketing topic and 100 passages related to basketball topic. Then, we have created 200 queries about cloud computation sales and marketing, 200 queries about basketball and 200 random queries not related to both topics."}, {"title": "5 RESULTS & ANALYSIS", "content": "In this section, we evaluate the performance of several RAG systems by comparing perfectly relevant and irrelevant query-passages pairs on faithfulness, answer relevance, retrieval recall, retrieval precision, as well as the logit values returned by the cross-encoder when performing the aggregation step (\"Agg\" column in tables) ."}, {"title": "5.1 VERA LLM-Based RAG Evaluation Metrics", "content": "In this experiment, we evaluate the performance of several RAG systems by comparing perfectly relevant and irrelevant query-passages pairs on faithfulness, answer relevance, retrieval recall, retrieval precision, as well as the logit values returned by the cross-encoder when performing the aggregation step (\"Agg\" column in tables) ."}, {"title": "5.2 Bootstrap Metrics for Document Repository Topicality Analysis", "content": "As outlined in section 3.3, we utilized bootstrap statistics to analyze a synthetic dataset described in section 4.2 and the results are in Table 3. We used bootstrap sampling with replacement on the synthetic query sets and the overall passage set. In the synthetic query sets, we have 200 synthetic queries in each set and we labeled them in Tale 3 as \"Sales\", \"Basketball\" and \"Random\" based on the topics. This approach enabled us to calculate critical statistical measures like the mean and variance, providing a robust foundation for assessing the thematic topicality of the data repository. We used sample size 50 and bootstrapping size 500 to ensure fairly stable convergence of the statistics for each metric and each query set. This comparative analysis helps in quantifying the document repository's content topicality to distinguish and accurately process content relevant to its designated domain.\nIn our study, the use of bootstrap statistics enabled us to compute the mean and confidence intervals for each performance metric across three different synthetic query sets on the same document repository. This comparison revealed notable differences in retrieval-related metrics among the query sets regarding different topics. The \"Sales\" query set results are with higher values in recall, precision, and relevance as the majority (80%) of the synthetic passage set is related cloud computation sales and marketing data."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced VERA, a framework tailored for evaluating Retrieval-Augmented Generation (RAG) systems. By generating LLM-based RAG evaluation metrics such as faithfulness, answer relevance, retrieval precision and retrieval recall, VERA can help evaluate and validate if the response from RAG based AI assistant is accurate or not. This framework boosts the reliability and transparency of RAG systems and build the trust in AI applications for users.\nOur findings demonstrate VERA's capacity to enhance decision-making processes effectively. The framework has been applied across several use cases, illustrating its ability to adapt to dynamic environments and maintain the integrity of data repositories. This adaptability makes VERA an important tool in the landscape of modern Al technologies, where the accuracy and relevance of information are paramount.\nLooking forward, we aim to further refine the metrics within VERA and expand its applicability to a broader range of domains and languages. Continuous advancements in VERA's methodologies will allow it to keep pace with rapid technological developments in AI. This evolution will ensure that emerging AI technologies are leveraged responsibly, maximizing their potential benefits for society."}, {"title": "7 LIMITATIONS", "content": "This paper presents several limitations that could potentially impact the comprehensiveness and applicability of its conclusions. Firstly, the analysis omits scenarios involving fine-tuned LLMs. Potential enhancements or specific use-case efficiencies brought by fine-tuned models might not be fully captured. This omission could lead to an incomplete understanding of the capabilities and limitations of the models under different experimental conditions. And the exclusion of some top proprietary LLMs in our experiments, such as OpenAI models, limits the evaluation's scope and understanding of our selected models' performance against the best available options.\nSecondly, our study does not address multilingual capabilities. The focus solely on English-language tasks may limit the generalizability of our conclusions to multilingual applications. This oversight could restrict the utility of our findings for developers and researchers working on systems intended for diverse linguistic environments, potentially overlooking significant performance variations across languages.\nThirdly, although our bootstrap estimators offer a more convincing assessment of the content complexity within a document repository, they are computationally intensive. We aim to develop a theoretically grounded, cost-effective measurement approach by constructing a pseudo-bootstrap strategy. This strategy will utilize pre-calculated evaluation metrics instead of relying on bootstrap sampling from queries."}, {"title": "8 APPENDIX", "content": ""}, {"title": "8.1 Prompt Examples", "content": "Prompt of Faithfulness Metric\nConsider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format.\nContext:\nEmma is a graduate student specializing in marine biology at Coastal University. She has a keen interest in coral reefs and is conducting her thesis on coral bleaching. Emma attends several seminars"}, {"title": "8.2 Unbiased Estimator", "content": "Let f: X \u2192 Y be a black box function mapping from the input space X (queries) to the output space Y (LLM-based metrics space). For each query x, the output is a vector y = (Y1, Y2, Y3, Y4) where Y1,Y2,Y3 and y4 represent Retrieval Precision, Retrieval Recall, Faithfulness, and Answer Relevance, respectively.\nAssume a dataset D = {xi}=1 where each x\u012f is a query. Associated with each query is a vector of metrics m\u012f = (mi1, mi2, mi3, mi4). For each metric k (where k = 1, 2, 3, 4 corresponding to the four metrics), the estimator \u03b8k based on n observations is given by the sample mean:\n\u03b8\u03ba = \\frac{1}{n} \\sum_{i=1}^{n} mik\nBased on the bootstrap procedure described in 3.2, for each bootstrap sample D and for each metric k, compute the bootstrap replicate:\n\u03b8*k = \\frac{1}{n} \\sum_{i=1}^{n} m*bik\nThe bootstrap expectation for each metric k over all bootstrap samples is:\nE* [\u03b8k] = \\frac{1}{B} \\sum_{b=1}^{B} \u03b8*k\nWe need to know that:\nE* [\u03b8*k] = \u03b8\u03ba\nto prove the bootstrap statistics are unbiased estimators of the empirical means \u03b8k for each metric.\nProof: Given that each is an average of n independently and identically distributed (i.i.d.) bootstrap samples drawn with replacement from mk, we apply the law of large numbers in the bootstrap world, stating:\nE* [\u03b8*k] \u2248 \\frac{1}{n} \\sum_{i=1}^{n} E* [m*bi]\nSince the bootstrap samples are drawn from the empirical distribution of mk, E* [m*bi] = \u03b8k. Therefore:\nE* [\u03b8*k] \u2248 \u03b8\u03ba\nThus, o is an unbiased estimator of Ok under the bootstrap distribution, assuming that the original sample is representative of the population and the metrics are i.i.d."}]}