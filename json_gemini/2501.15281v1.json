{"title": "Pre-training a Transformer-Based Generative Model Using a Small Sepedi Dataset", "authors": ["Simon Phetole Ramalepe", "Thipe I. Modipa", "Marelie H. Davel"], "abstract": "Due to the scarcity of data in low-resourced languages, the development of language models for these languages has been very slow. Currently, pre-trained language models have gained popularity in natural language processing, especially, in developing domain-specific models for low-resourced languages. In this study, we experiment with the impact of using occlusion-based techniques when training a language model for a text generation task. We curate 2 new datasets, the Sepedi monolingual (SepMono) dataset from several South African resources and the Sepedi radio news (SepNews) dataset from the radio news domain. We use the SepMono dataset to pre-train transformer-based models using the occlusion and non-occlusion pre-training techniques and compare performance. The SepNews dataset is specifically used for fine-tuning. Our results show that the non-occlusion models perform better compared to the occlusion-based models when measuring validation loss and perplexity. However, analysis of the generated text using the BLEU score metric, which measures the quality of the generated text, shows a slightly higher BLEU score for the occlusion-based models compared to the non-occlusion models.", "sections": [{"title": "1 Introduction", "content": "Low-resourced languages are characterised by small training datasets, as well as limited tools and linguistic analyses [12]. The Sepedi language, one of the official languages in South Africa falls in this category. It is a highly disjunctively written language which is mostly characterised by 2 character words as discussed by the authors in [26]. Previous attempts to develop text-based language models include the use of recurrent neural networks (RNNs) and long-short memory networks (LSTMs) [29]. These techniques could perform well on short sentences but their performance would degrade as the sentence length increases. The introduction of the transformer architecture [31] has since gained much attention in developing transformer-based models that can generate longer coherent sentences. This technique uses a multi-head self-attention mechanism to compute the contextual representation of each word by considering the entire input sequence. Complementary approaches when developing language models for low-resourced languages include transfer learning techniques and data augmentation. Transfer learning is a machine learning approach that leverages knowledge gained from pre-trained models to improve the performance of a low-resourced model in a related task [25,32]. In the context of text generation, this approach transfers knowledge from a pre-trained language model to a low-resourced language through the fine-tuning process [12,14]. Transfer learning has emerged as an approach that effectively addresses data scarcity in low-resourced languages. Data augmentation is another approach that has been explored in several studies [4,7,22,28,30], with various approaches proposed for generating artificial data. The technique has also been shown to improve the performance of a language model.\nThe purpose of this study is to analyse the implications of pre-training a Sepedi generative pre-trained model (GPT) when only a relatively small dataset is available. Specifically, we aim to understand how well a semi-matched dataset (from the same language but a different context) can be used as the seed model for the new context, and whether the type of pre-training used makes a difference. The target context we are interested in is the Sepedi radio news dataset obtained from radio broadcasts, and the semi-matched dataset contains a mix of different styles. The study contributes:\nSepedi monolingual (SepMono) dataset curated from several South African resources and Sepedi radio news (SepNews) dataset curated from radio news domain.\nA comparison of two different pre-training techniques for a small dataset, before and after fine-tuning.\nSepedi transformer generative pre-trained models (SepGPT) and (SepGPT-OCC), trained on the newly curated Sepedi corpus using the standard GPT training approach and occlusion-based technique."}, {"title": "2 Background", "content": "We provide a brief overview of language models and the use of pre-training, before discussing selected closely related studies."}, {"title": "2.1 Pre-trained language models", "content": "Language models are applied to many computational linguistic problems including text generation. Language modeling remains the most fundamental task in natural language processing (NLP) and automatic speech recognition systems [34]. It assigns a joint probability distribution over a sequence of linguistic words [32,34]. In its simplest form, the joint probability of an entire sequence of words\ncan be estimated by multiplying the number of conditional probabilities using the chain rule.\n\n$P(w) = P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i | w_{<i}) = P(w_1)P(w_2 | w_1) ... P(w_i | w_1, w_2, ..., w_{i-1})$\n\nPre-trained language models (PLMs) are models that were initially trained on a large corpus to capture broad representations of a language's syntactic and semantic knowledge [32]. The primary pre-training objective for generative language models is to predict the missing token in a given sentence. Alternative objectives like \"next-sentence prediction\" are also used for specific purposes, such as understanding sentence relationships. In the \"predicting the missing token\" approach, auto-regressive models like GPT-2 [24] predict the next token based on previous tokens, while Bidirectional encoder representations from transformers (BERT) [6], uses the masked language model (MLM) and next-sentence prediction objectives to predict the masked tokens. The MLM objective is achieved by randomly masking the input tokens and training the model to predict the masked tokens using the rest of the context. The next sentence prediction task is used to train the model to predict the relationship of the next sentence given the previous sentence. We use the terms \"occlusion\" and \"non-occlusion\" to refer to pre-training objectives where some input tokens are occluded or masked and the training process where input tokens are not occluded respectively.\nThe occlusion-based pre-training is a common pre-training objective in computer vision [3], however, it has been used in generative models [6]. This technique helps the model to be robust to unseen tokens [10]. Harbecke et al. [10], show the effectiveness of combining occlusion and language modeling for a classification task. In general, occlusion is regarded as an explanation method where the difference in prediction when removing an input feature is seen as an indicator of the importance of the feature [36]. On the other hand, the non-occlusion pre-training technique is commonly used in autoregressive models like GPT-2 [24].\nSubsequently, pre-trained models are commonly fine-tuned on smaller datasets to perform several downstream tasks using the learned knowledge. To avoid catastrophic forgetting during the standard fine-tuning approach, Howard et al. [11] proposed discriminative fine-tuning (which uses different learning rates for different layers), slanted triangular learning rates (which varies the learning rates using the defined pattern for better convergence), and gradual unfreezing of layers (which unfreezes layers from top to bottom during training). The authors show that these approaches can improve the model's performance."}, {"title": "2.2 Closely related studies", "content": "Several studies have adopted pre-training before fine-tuning their models on downstream tasks. We discuss some of the most relevant studies in this section,\nwith a specific focus on the size of the dataset and evaluation metrics used to evaluate the performance of the models.\nNiculescu et al. [21] developed a Romanian text generation model (RoGPT-2) using the GPT-2 architecture. The authors pre-trained their model from scratch using 17GB of monolingual data and fine-tuned it on several tasks for news article generation. The performance of their model was evaluated using perplexity as a metric and obtained a score of 34.37 on the validation set and 33.74 on the test set for the natural language generation (NLG) task on their base model. Experimenting with news generation, they obtained a BLEU score of 35.90 between the reference text and the generated text. Similarly, Buzea et al. [1] developed a smaller Romanian text generation model (MCBGPT-2) using a standard GPT-2 architecture. The model was trained on a small dataset of 24k news items crawled from online news portals. The performance of the model was monitored using the sparse categorical cross-entropy loss function.\nIn another study, Martin et al. [17] developed SwaBERT, a low-resourced language model for the Swahili language using the BERT architecture. The authors trained their model with monolingual data collected from news websites, forums, Wikipedia and popular social media websites with a total size of 105MB. The original BERT's architecture was followed when pre-training their model. In their approach, the authors trained a model using both MLM and next sentence prediction as used in [6]. However, during training, they experimented by varying the vocabulary size and the number of training and warm-up steps. The performance of their model was analysed on several downstream tasks including news classification, emotions detection, sentiment analysis and named entity recognition (NER).\nIn the study by Wongso et al. [35], the authors developed several low-resourced models for the Sudanese language. The authors pre-trained GPT-2 Model, BERT and ROBERTa architectures using 785MB monolingual data and evaluated them for emotional classification through fine-tuning. The authors observed that both Sudanese ROBERTa and BERT surpassed or performed comparably to larger multilingual models. Perplexity and validation loss were used to evaluate their models during pre-training. The study by Martin et al. [18] also demonstrated that it is possible to pre-train a large language model using a relatively small dataset. The authors pre-trained a GPT-2 model using 4GB of French data. They used the masked language modeling approach as used by ROBERTa as their training objectives. However, instead of fixing the masked token, they introduced dynamic token masking which seemed to improve the model variability and made it robust during training. Their model was evaluated on several downstream tasks including natural language inference.\nFinally, in another low-resource language context, Salim et al. [27] developed a BanglaGPT generative model using the GPT architecture. They trained their model from scratch using 26.24GB corpus, scraped from several websites. The model outperformed multilingual GPT (mGPT) and LSTM models with an optimal perplexity score of 2.86."}, {"title": "3 Experimental setup", "content": "In this section, we start by describing the datasets used in this study, the model development process, and lastly the evaluation techniques used."}, {"title": "3.1 Data collection", "content": "SepMono Dataset Table 1 lists the available Sepedi monolingual text datasets used to develop the pre-trained models. We indicate the specific size of each dataset to show the relative size and scarcity of the available Sepedi textual datasets. We combine all these datasets to create a Sepedi monolingual dataset referred to as \"SepMono\". The SepMono dataset consists of 432,970 sentences with 11,360,000 tokens after cleaning. All of these datasets are freely available for research purposes.\nThe National Centre for Human Language Technology (NCHLT) corpus [23], was curated from a collection of several South African government entities crawled from gov.za websites. The corpus was collected from various language units from 2007 to 2011. The Autshumato dataset, developed by McKellar [19] was collected from several sources (magazines, policies, newsletters, translation works) and documents crawled from the government domain. The News headlines dataset [16], is another Sepedi text corpus based on radio news headlines. It was crawled from one of the South African national Sepedi radio stations between 2018 and 2020. The Sepedi newspaper dataset [15], was collected from the Vukuzenzele newspaper between 2011 and 2022. The dataset has both monolingual and translated parallel data from English to other South African low-resource languages. The Leipzig dataset [8] consists of newspaper texts and texts randomly collected from the web while Web crawl dataset [5,33] consists of general data extracted from Common Crawl for various languages including Sepedi.\nWe split the dataset into 80% training, 10% validation and 10% testing. We show the size of the partitioned dataset in the first row of Table 2."}, {"title": "3.2 Models", "content": "Model architecture We adopt the GPT-2 architecture [24] to develop the Sepedi generative model (SepGPT). The GPT-2 architecture uses the decoder part of the transformer architecture [31] with the pre-training objective of predicting\nthe next word given the previous words. The GPT-2 architecture consists of a series of decoder blocks, each incorporating a masked self-attention block which helps to identify the relevant words the model should focus on. The feed-forward neural network block within the hidden layer, on the other hand, establishes the relationships between the input tokens. At the lower level, it has the token and positional embedding layer to map each token in the vocabulary to a high-dimensional vector representation and to capture the positional embedding of the tokens.\nModel training The experiments are conducted on Google Colab's cloud development environment with NVIDIA T4 GPUs. We start training the model using the standard pre-training objective of the GPT-2 model and later add the occlusion-based technique to help the model learn different structural representations that capture the semantics of the sentence. To optimise hyperparameters, the Weights and Biases7 random sweep configuration is utilized, with validation loss monitored throughout. The AdamW optimizer is employed, starting with a learning rate of 1e-4, and the model convergence is enhanced using a learning rate scheduler with a warm-up. With a patience of 5, early stopping is applied to control overfitting. Due to limited computational resources, the hyperparameter search is randomized to 20 counts over 100 epochs, exploring the optimal learning rate, number of transformer layers, number of attention heads, and dropout rate (see experimental details in the Appendix for more information). The optimal validation loss and perplexity are recorded at each epoch, while the test loss from the best-performing models is also logged. The non-occlusion model obtained its optimal hyperparameters using a higher number of attention heads and layer blocks, as shown in Table 4 in the Appendix.\nOcclusion-based training Occlusion-based techniques have been used extensively in computer vision and classification tasks as discussed in Section 2.1. In this study, we experiment with this technique for a text generation task. To train the model using the occlusion-based technique, we use the same experimental setup as described above to find the optimal hyperparameters. However, we now combine the generative approach of predicting the next word given the previous word with an occlusion-based technique to help the model learn different structural representations that capture the semantics of the sentence. In addition to the standard generative approach, the occlusion probability is added as a hyperparameter to the model. We experiment with probabilities of 0.1, 0.3, and 0.5 and record the model's performance per epoch during training. This technique introduces noise to the input text by randomly occluding some tokens in the input sequence based on a given probability. The model is then trained to predict the occluded tokens and recover them using the surrounding non-occluded tokens."}, {"title": "Fine-tuning models", "content": "To evaluate the performance of the trained models, we fine-tune them for a text generation task using the radio news (SepNews) dataset. To fine-tune the models, the same experimental setup as in the pre-training process is applied. The optimal hyperparameters obtained for each model are used, and the models are initialized with the respective pre-trained weights. The models are then fine-tuned for 50 epochs with early stopping. Specifically, we experiment with the gradual unfreezing fine-tuning technique as discussed and experimented in [11,25]. This technique works by gradually unfreezing model layers (from top to bottom during training). Importantly, the approach helps the model to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. Initially, only the top 2 layers are unfrozen. We set the unfreeze interval to 2 epochs, meaning that at every 2 epochs, additional layers are progressively unfrozen. This interval, basically specifies how often layers are unfrozen during the training process."}, {"title": "3.3 Evaluation metrics", "content": "Although human evaluation is the most standard method for evaluating text generation language models, it is expensive to execute and the results are difficult to reproduce [2]. Model evaluation can either be done intrinsically or extrinsically. Intrinsic evaluation methods that have been used for language modeling include the Bilingual Evaluation Understudy (BLEU) score and perplexity [1,9,21]. Perplexity (PPL), remains the most preferred metric to evaluate the performance of a language model [13,34]. It is the exponentiation of the entropy, which is the average negative log-likelihood of the true word sequence. That is, for a language model that assigns probabilities to sequences of words, the perplexity PPL of a sequence of words W where W =  W1,W2,...Wn can be computed as:\n\n$PPL(W) = exp(\\frac{1}{n} \\sum_{i=1}^{n} -log P(w_i | w_1, w_2, ... w_{i-1}))$\n\nwhere P(wi | W1, W2, ...Wi\u22121) is given by a language model and is the probability assigned by the model to the word given the previous words in the sequence. We also use the categorical cross-entropy loss function to monitor the model's performance during training.\nThe BLEU score metric is used to evaluate the quality of the text generated by a model. It does this by comparing the generated text to the reference text. Its computation is based on the precision of n-grams between the generated text and the reference text, along with a brevity penalty to handle shorter text. It is calculated as follows:\n\n$BLEU = BP \\times exp (\\sum_{n=1}^{N} W_n log p_n)$\n\nwhere BP is the brevity penalty, which adjusts the score based on the length of the generated text compared to the reference length. The brevity penalty is\ncalculated as follows:\n\n$BP = \\begin{cases} 1 & \\text{if } c > r \\\\ exp(1 - \\frac{r}{c}) & \\text{if } c \\leq r \\end{cases}$\n\nwhere c is the length of the generated text, and r is the length of the reference text."}, {"title": "4 Results", "content": "In this section, we discuss the performance of the 4 developed models by analysing their performance using validation loss and perplexity. We start with the analysis of the pre-training results, before demonstrating the effects of fine-tuning.\nFig. 1a and 1b show the training curves of the optimal scores recorded during the pre-training process of the occlusion and non-occlusion models respectively. Although the maximum number of epochs was set to 100, the occlusion-based model achieved its optimal validation loss of 3.46 at epoch 79, while the non-occlusion model reached its optimal validation loss of 2.78 at epoch 61. Based on the size of the dataset, and the complexity of occlusion-based training, the SepGPT-OCC obtained the optimal validation loss which is 0.68 higher compared to the non-occlusion model (SepGPT). However, the fine-tuning process (the last row of Table 3) significantly closed this gap to just 0.11. Similar observations are noted in validation perplexity, test loss and test perplexity. Notably, the fine-tuning process improved the validation perplexity of the occlusion-based model by almost 50%. Of importance to note is that the test loss and test perplexity are relatively high compared to the pre-trained models. The introduction of a completely new test dataset (same news domain but from a different time period) presumably affected the test performance of our fine-tuned models.\nWe also compare our models with other low-resourced models trained from scratch using the transformer-based technique (the first row of Table 3). While systems evaluated on different datasets and for different languages are not directly comparable, this comparison provides an indication of the level of performance achieved. We note that the validation perplexity score of the SepGPT-OCC pre-trained model outperformed both the Sudanese GPT-2 and RoGPT-2 base models by a validation perplexity score of 5.08 and 2.48 respectively. Furthermore, the test perplexity score of the SepGPT-OCC model also outperformed the RoGPT-2 base model. Although the optimal test loss obtained from the SepGPT is 2.30 higher compared to the standard BanglaGPT model, the general performance of all our models and the training processes used in this study were satisfactory.\nWe further generated text from the trained models and computed a BLEU score to measure the quality of the generated text from the standard SepGPT and the SepGPT-OCC models. Although BLEU score is used mostly in translation tasks, it gives a good indication of similarities between the generated and the reference text. Without human evaluation, the pre-trained SepGPT-OCC model\nobtained a BLEU score of 5% higher compared to the SepGPT model while the the BLEU score of the fine-tuned SepGPT-OCC model was 3.86% higher. These scores show how robust the occlusion model can be to unseen text. We show an example of the generated text from the SepGPT-OCC (FT) model Table 5 in the Appendix section. Although further analysis of the generated text is necessary we note that the sentences are mostly grammatically correct.\nEven though the occlusion-based technique introduces noise to the training data, it is observed that both models (pre-trained and fine-tuned) could still produce a competitive result compared to a non-occlusion model. Further observations from experiments show that although the performance of the occlusion-based technique is low compared to standard training, the technique can provide valuable insight, especially in building robust code mixed text generation models."}, {"title": "5 Conclusion", "content": "The purpose of this study was to evaluate the performance of a generative transformer-based model on a relatively small low-resourced Sepedi dataset. We curated and cleaned 2 datasets, the Sepedi monolingual dataset (SepMono) from various available Sepedi resources and the radio news dataset (SepNews). We used the SepMono dataset to train 2 transformer-based generative models using the standard GPT-2 training objective and the occlusion-based technique. The SepNews dataset was used to fine-tune our 2 pre-trained models using the gradual unfreezing technique. In this process, 2 additional Sepedi text generation models were further developed. Although the transformer-based models are data-intensive, we successfully used the hyperparameter search to obtain the optimal parameters for our pre-trained models (SepGPT and SepGPT-OCC). Compared to other low-resourced GPT models, our models obtained higher and comparable results. We further used the BLEU score metric to evaluate the performance of the generated text from our trained models and obtain the first and new optimal BLEU score of 44.98% (SepGPT) and 48.84% (SepGPT-OCC) for the Sepedi language. The scores obtained in this study create a new baseline for the Sepedi language and other South African low-resourced languages. Although the occlusion-based technique approach obtained in general a higher BLEU Score compared to the non-occlusion model, the non-occlusion model performed fairly better in both validation and perplexity loss which could mean that the model is more reliable in generating coherent text. In future work, we aim to experiment with other fine-tuning techniques and also, to analyse the performance of these techniques and their impact on generating code-switched text."}]}