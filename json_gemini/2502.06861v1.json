{"title": "Design Considerations in Offline Preference-based RL", "authors": ["Alekh Agarwal", "Christoph Dann", "Teodor V. Marinov"], "abstract": "Offline algorithms for Reinforcement Learning from Human Preferences (RLHF), which use only a fixed dataset of sampled responses given an input, and preference feedback among these responses, have gained increasing prominence in the literature on aligning language models. In this paper, we study how the different design choices made in methods such as DPO, IPO, SLiC and many variants influence the quality of the learned policy, from a theoretical perspective. Our treatment yields insights into the choices of loss function, the policy which is used to normalize log-likelihoods, and also the role of the data sampling policy. Notably, our results do not rely on the standard reparameterization-style arguments used to motivate some of the algorithms in this family, which allows us to give a unified treatment to a broad class of methods. We also conduct a small empirical study to verify some of the theoretical findings on a standard summarization benchmark.", "sections": [{"title": "1. Introduction", "content": "The now substantial literature on Reinforcement Learning with Human Preferences (RLHF) can be broadly categorized into two families of methods. Given a dataset of human preferences, the first class of online algorithms are based on learning a reward function that assigns numerical scores to a response y given some input x, such that the high-scoring responses are preferred over the low-scoring ones in our preference dataset. These methods subsequently maximize this reward function using an online RL algorithm like PPO or Reinforce. A different approach forgoes the reward learning step and uses a reparameterization trick to directly learn a good policy from the preference dataset, with Direct Preference Optimization (DPO) being the pioneering approach in this line of work. These approaches are referred to as offline or direct alignment, since they do not draw any fresh samples from the learned policy during training, and only use the responses observed in the preference dataset. This paper focuses on this latter family of algorithms, and studies how the properties of the data and the learning objective affect the quality of the learned policy.\nOffline methods for RLHF such as DPO, IPO, SLiC and KTO, along with a growing number of variants have received a significant attention in the academic literature owing to a number of attractive properties. First, the removal of an explicit reward learning step simplifies the number of modeling choices and steps required for the RLHF pipeline, along with reducing the demand on computational resources. Furthermore, the requirement to only evaluate a policy's likelihood on a fixed set of responses in the preference dataset, as opposed to generating responses from the learned policy as in online RL, adds further resource efficiency. However, the growth of this literature has also spurred an equally large body of work now detailing the various deficiencies of these techniques, such as the tendency of the algorithms to shift the probability mass outside the support of the observed responses in the preference dataset, significant preference hacking behaviors and notorious collapses in the learning dynamics with continued training.\nWhile some of the issues raised above have received theoretical treatment for specific approaches, the literature still lacks a comprehensive theoretical foundation underlying these offline RLHF techniques. Most of these techniques have been motivated by some variant of the original reparameterization argument in the DPO paper, but the argument hinges on unformalized assumptions regarding the coverage of the data with respect to the learned policy, and does not capture all the algorithmic variants which have subsequently been developed in the literature.\nIn this paper, we instead adopt the perspective of offline RLHF as just solving a loss minimization problem on a preference dataset, and study when the optimal solution of this loss minimization yields a desirable policy. A key"}, {"title": "2. Problem Setup", "content": "Preference-based learning. We consider the setting of learning from offline preference data. In this setting, we are typically given a dataset of samples where each sample consists of an input $x \\in X \\sim D_x$, two responses $y, y' \\in Y \\mid x \\sim D_{y(x)}$ and a binary label $w \\in \\{-1,1\\} \\sim D_{w(x, y, y')}$. We use the shorthand $(x, y, y',w) \\sim D_{xyw}^{i.i.d.}$ to succinctly denote samples from this generative process. The label w captures our preference for the response y over y', for the input x. We study offline preference-based RL methods, which are parameterized by some loss function $l: [-R,R] \\rightarrow [0,\\infty)$. In particular, we study methods which take as input a policy class $\\Pi \\subseteq \\{X \\rightarrow \\Delta(Y)\\}$, and find a policy which minimizes the following loss, given n samples:\n$\\hat{\\pi} = \\underset{\\pi \\in \\Pi}{\\text{argmin}} \\sum_{i=1}^n l\\left(w_i \\cdot \\left(\\log \\frac{\\pi(y_i|x_i)}{\\mu(y_i|x_i)} - \\log \\frac{\\pi(y'_i|x_i)}{\\mu(y'_i|x_i)}\\right)\\right) \\tag{1}$\nHere $\\mu: X \\rightarrow \\mathbb{R}$ refers to an arbitrary base policy, which does not need to be normalized. Typically l is chosen so that it incentivizes $w \\cdot w_{\\pi,\\mu}$ to be positive. This means that $\\pi$ tends to increase the probability of producing $y_i$ compared with $y'_i$, relative to the base probabilities under $\\mu$, when $w_i = 1$. In the particularly simple case when $\\mu$ is just the uniform policy, we see that $\\pi$ results in a larger probability of producing $y_i$ over $y'_i$, when $w_i = 1$, which captures the basic intuition behind offline preference-based RL.\nChoice of policy class. A common policy parameterization is to use softmax policies $\\pi_{\\theta}(y|x) \\propto exp(f_{\\theta}(y|x))$ with some parameter set $\\theta \\in \\Theta$, where f is some fixed network architecture. While we could explicitly define the policy class $\\Pi$ this way, we intentionally leave $\\Pi$ general at this point, to allow our framework to further include:"}, {"title": "3. Analysis Framework and Main Results", "content": "In this section, we set up a framework for analyzing offline preference learning algorithms which optimize (1), and present our main results. We begin with a discussion to set up the performance criterion.\n3.1. Analysis Framework\nPerformance criterion. How to measure the efficacy of a preference-based learning technique? As described above, based on our choices of l, $\\mu$ and $\\Pi$, we get a guarantee on the expected loss of the resulting policy. But we want to measure how well the policy does in terms of producing highly preferred outputs y, given inputs x. It is not clear that a policy which has a small loss also produces good outputs. For instance, suppose that the learned $\\pi$ is such that $E[w_{\\pi,\\mu}(x, y, y')] > 0$ for any x, y, y' in the support of our training distribution, and the base policy $\\mu$ is uniform. In this case, we can conclude that $\\pi(y|x) > \\pi(y'|x)$ whenever $E[w|x,y,y'] > 0$. But this does not preclude the two probabilities from being extremely close, though correctly ordered, and more generally it might still place a non-trivial probability on the least desirable outputs y for many inputs x.\nIdeally, we would like to say that $\\pi$ places most of the mass on the most desirable outputs y. Since the conditional probabilities $D_w(x, y, y')$ can be interpreted as a preference function $P(y > y'|x)$, one notion of an optimal policy is provided by the Nash equilibrium policy for the two player-game encoded by this preference function, as considered in prior works. However, as show, this optimal solution cannot be attained by minimizers of the objective (1) in general, where they provide a lower bound for the special case of DPO. Consequently, we need a dif-"}, {"title": "4. Analysis", "content": "The high-level reasoning to prove Theorem 3.6 is the following. We first use Assumption 3.5 to establish that any $\\epsilon$-minimizer of $L(\\pi_{\\rho}; D_{xyw})$ admits a bound on the expected error in the centered rewards $E_{(x,y)\\sim D}[\\Delta\\dot{R}(x, y)^2]$. We then invoke the coverage condition of Assumption 3.4 to translate this error bound to be under the benchmark policy $\\pi^*$. Subsequently, we relate the KL divergence between $\\pi$ and $\\pi^*$ in terms of expectation of $\\Delta R(x, y)^2$ under $\\pi^*$, using a careful anaylsis of the log-partition function.\nWe start with the first step in the sketch above.\nLemma 4.1. Under Assumptions 3.2 and 3.5, any policy $\\pi \\in \\Pi_{\\mathcal{R}}$ with $L_{\\mu}(\\pi; D_{xyw}) - L_{\\mu}(\\pi^*; D_{xyw}) \\leq \\epsilon$, satisfies\n$E_{(x,y)\\sim D} [\\Delta\\dot{R}(x, y)^2] = E_{(x,y,y')\\sim D}[(\\Delta R(x, y) - \\Delta R(x, y'))^2] < \\frac{2\\epsilon}{C_{\\mu}}$.\nProof. The first condition of the lemma is equivalent to\n$E[l_{\\mu}(w\\cdot w_{\\pi^*,\\mu} (x, y, y'))] \\geq E[l_{\\mu}(w\\cdot w_{\\pi,\\mu}(x, y, y'))] + E[w l' (w\\cdot w_{\\pi^*,\\mu} (x, y, y')) ((w_{\\pi,\\mu} (x, y, y) - w_{\\pi^*,\\mu} (x, y, y'))] + E[ \\frac{C_{\\mu}}{2} (w(R(x,y) - R(x,y') - R^*(x, y) + R^*(x,y')))^2]$\nNow, Assumption 3.2 implies that for any $\\pi \\in \\Pi$,\n$E_{w|x,y,y'} [l' (w\\cdot w_{\\pi^*,\\mu}; (x, y, y')) (w(w_{\\pi,\\mu} (X, Y, Y') - w_{\\pi^*,\\mu}(x, y, y'))] \\geq 0,$\nwhere we have used the fact that $v^* = w_{\\pi^*,\\mu}(x, y, y')$ is the minimizer of $E_{w|x,y,y'} [l(w\\cdot v^*)]$ together with first order optimality so that $E_{w|x,y,y'} [l' (w\\cdot v^*)w(v - v^*)] \\geq 0$ for any v and in particular for any, $\\pi$, such that $v = w_{\\pi,\\mu}(x, y, y')$. This, together with the second assumption of the lemma imply that\n$\\epsilon > E[l_{\\mu} (w\\cdot w_{\\pi^*,\\mu} (x, y, y'))] - E[l_{\\mu} (w\\cdot w_{\\pi,\\mu}(x,y,y'))] \\geq \\frac{C_{\\mu}}{2} E[(w(R(x, y) - R(x, y') - R^*(x, y) + R^*(x,y')))^2],$ where we have used the parametrization of $\\pi$ and $\\pi^*$.\nNext we show how to bound the KL by $\\Delta R$ and the fraction of log-partition functions $Z_{\\pi}$ and $Z_{\\pi^*}$, where $Z_{\\pi}(x) = \\sum_y exp(R(x, y))$.\nLemma 4.2. Under Assumption 3.4, for any $\\pi \\in \\Pi$ such that $\\pi \\propto exp(R(x,y))$, the expected KL divergence $E_x [KL(\\pi^*(\u00b7|x)||\\pi(\u00b7|x))]$ is bounded by\n$\\sqrt{\\frac{2C}{C_{\\mu}}} \\sqrt{E_{x,y}[\\Delta\\dot{R}(x, y)^2]} + E_x log \\frac{Z_{\\pi}(x)}{Z_{\\pi^*}(x)}$\nProof.\n$E_x[KL(\\pi^*(\u00b7|x)||\\pi(\u00b7|x))] \\leq E_x E_{y\\sim \\pi^*(\u00b7|x)} log \\frac{\\pi^*(y|x)}{\\pi(y|x)} = E_x (log \\frac{\\pi^*(y|x)}{Z_{\\pi}(x)} - log Z_{\\pi}(x)) \\leq E_x \\sqrt{2 E_{y\\sim \\pi^*(\u00b7|\u03b1)} \\left(log \\frac{\\pi^*(y|x)}{\\pi(y|x)} - log \\frac{Z_{\\pi}(x)}{Z_{\\pi^*}(x)}\\right)^2}$  + $E_x\\sqrt{2 E_{y\\sim \\pi^*(\u00b7|\u03b1)} \\left(log \\frac{Z_{\\pi}(x)}{Z_{\\pi^*}(x)}\\right)^2} \\leq \\sqrt{\\frac{2C}{C_{\\mu}}} \\sqrt{E_{x,y} [\\Delta\\dot{R}(x, y)^2]} + E_x log \\frac{Z_{\\pi}(x)}{Z_{\\pi^*}(x)}$ where in second inequality follows from Jensen's inequality, and the third inequality follows from a combination of $(x + y)^2 < 2x^2 + 2y^2$ and $\\sqrt{x + y} < \\sqrt{x} + \\sqrt{y},\\forall x,y \\geq 0$, and the third inequality uses Jensen to push the expectation inside the square root, along with Assumption 3.4.\nThe next lemma bounds the ratio of log-partitions.\nLemma 4.3. For any $\\pi$ such that $L(\\pi; D_{xyw}) - L(\\pi^*; D_{xyw}) \\leq \\epsilon$, it holds that\n$E_x log \\frac{Z_{\\pi}(x)}{Z_{\\pi^*}(x)} < \\frac{\\sqrt{C}e^R}{C_{\\mu}} \\sqrt{\\frac{\\epsilon}{n}} + \\frac{Ce^R}{2\\mu} \\frac{\\epsilon}{n}$\nProof. Using the definition of $Z_{\\pi}$ we have\n$E_x log \\frac{Z_{\\pi}(x)}{Z_{\\pi^*}(x)} = E_x log \\frac{\\sum_y exp(R(x, y))}{\\sum_y exp(R^*(x, y))} = E_x log \\sum_y \\pi^*(y|x) \\frac{exp(\\dot{R_{\\pi}}(x, y))}{exp(\\dot{R_{\\pi^*}}(x, y))}$ Here the second equation rearranges the definition"}, {"title": "5. Experiments", "content": "We evaluate the impact of different design choices for offline RLHF methods on the standard TL;DR summarization task , where the task is to provide short summaries of articles. For our policy, we use a large T5 model with 770M parameters, which has been fine-tuned to maximize the log-likelihood of the human responses in the TL;DR dataset. This ensures that the policy is initialized with parameters that have reasonable likelihoods for the responses observed in our data. We experiment with two different losses, l:\n$l(x) = log(1 + exp(-x)) \\quad \\quad (Logistic \\, loss)$ \n$l(x) = (x - 1)^2 \\quad \\quad (Squared \\, loss)$\nwhere $\\beta > 0$ is a hyper-parameter, governing the strength of how much each preference in the dataset should in the policy. We pair each of losses with two possible choices for the base policy $\\mu$: The uniform base policy $\\mu = 1$, which results in the following optimization\n$min \\sum_{i=1}^n l( \u2013 w_i (log \\pi(y_i|x_i) \u2013 log \\pi(y\u2019_i|x_i))),$\nand the second choice uses $\\mu = \\pi_{ref}$, the policy obtained after fine-tuning on TL;DR which our optimization is initialized with. This corresponds to the objective is:\n$min \\sum_{i=1}^n l( \u2013 w_i (log \\frac{\\pi(y_i|x_i)}{\\pi_{ref}(y_i|x_i)} \u2013 log \\frac{\\pi(y\u2019_i|x_i)}{\\pi_{ref}(y\u2019_i|x_i)}))$\nWe recall that when $\\mu = \\pi_{ref}$, using the logistic loss from (Logistic loss) corresponds to the DPO algorithm and the squared loss from (Squared loss) corresponds to IPO .\nFor each variant, we tuned the $\\beta$ parameter in the loss in the interval {0.1, 0.5, 1.0}. For logistic loss, the best results are obtained at $\\beta = 0.1$, while we did not see a significant difference across these choices for the squared loss, and show the results at $\\beta = 0.5$. See Appendix B for details.\nWe evaluate the different variants in two ways. First we compare the policies generated by each method at regular intervals in the training process, with the initial policy $\\pi_{ref}$, by comparing the generated summaries in terms of their quality and conciseness by a prompted Gemini 1.0 Ultra model . Figure 1 (left) shows that squared loss variants perform significantly better than those with logistic loss, which reach a peak preference at 2k steps and suffer from a dramatic collapse afterwards. In the case of $\\mu = \\pi_{ref}$, this collapse is consistent with prior findings on the DPO algorithm in the literature . In comparison, both squared loss variants reach peak performance after similar number of steps, but maintain that performance more stably afterwards.\nNotably, using $\\mu = \\pi_{ref}$ is consistently better than $\\mu = 1$, when combined with the squared loss. In case of logistic loss, the comparison against $\\pi_{ref}$ in Figure 1 (left) suggests that the base policy choice does not affect performance. However, when we compare the policies produced by the two variants with $\\mu = 1$ and $\\mu = \\pi_{ref}$ directly in Figure 1 (left), we do observe a strong impact. Initially, the uniform variant is slightly preferred with a strong preference afterwards in the other direction, after roughly 5K steps. However, at this point the preference of each variants against $\\pi_{ref}$ has already collapsed, indicating a perhaps less meaningful comparison between two sets of bad responses in this region.\nTo further understand the training dynamics, we plot the log-probabilities of the preferred and dispreferred responses from the preference data for all the variants in Figure 2. For the logistic loss, these plots demonstrate that while both the"}, {"title": "6. Discussion", "content": "This works adopts a different line of reasoning than the typical reparameterization arguments to motivate the correspondence between offline and online RLHF techniques, with a goal of understanding the impact of design choices in the offline methods, many of which do not fit cleanly in"}, {"title": "A. From a preference model to a proper loss", "content": "A natural family for the reward generating distributions is the exponential family. In particular we are going to assume that $D_{\\omega}$ is in the exponential family parametrized by some $\\upsilon^* := \\upsilon^*(x, y, y')$, that is $P(\\omega|x, y, y') = exp(\\omega\\upsilon^* - \\phi(\\upsilon^*))$, where $\\phi$ is some strictly convex function. Considering the exponential family naturally leads to an objective function for learning the unknown parameter $\\upsilon$, that is to maximize the log-likelihood which will recover $\\upsilon^*$ in the following way. First we take the derivative of the log-likelihood with respect to $\\upsilon$\n$\\nabla_{\\upsilon} E_{\\omega} [log \\, exp(\\omega\\upsilon - \\phi(\\upsilon)) ] = E_{\\omega} [\\omega - \\nabla \\phi(\\upsilon)] = \\nabla \\phi(\\upsilon^*) - \\nabla \\phi(\\upsilon)$.\nSetting the derivative to 0 and using the strict convexity to invert $\\nabla \\phi(\\upsilon)$ shows that $\\upsilon^* = \\nabla \\phi^{-1}(E_{\\omega}[\\omega])$. To summarize the above, a preference model which follows exponential family distribution gives rise to a natural loss function given by\n$\\underset{\\upsilon \\in [-R,R]}{\\text{min}} E_{\\omega} [\\phi(\\upsilon) - \\omega\\upsilon]$,\nand has a closed form solution $\\upsilon^* = \\nabla \\phi^{-1}(E_{\\omega}[\\omega])$.\nTo make this discussion concrete we focus on a BTL model, where we set $\\upsilon(x,y,y') = R(x,y) - R(x,y')$. Then $\\phi(\\upsilon(x, y, y')) = log(exp(R(x, y) - R(x, y')) + exp(R(x, y') - R(x, y)))$ and the corresponding loss function is then\n$\\phi(\\upsilon(x, y, y')) - \\omega \\upsilon(x, y, y') = log(exp(R(x, y) - R(x, y')) + exp(R(x, y') - R(x, y)))$ log(exp($\\omega(R(x, y) - R(x,y'))$))\n$ = log(1 + exp(-$\\omega$(R(x, y) - R(x, y')))),$\nwhich is precisely the logistic loss. The above derivation already shows that any loss derived from the exponential family with link function $\\nabla \\phi$ is going to be proper as $\\phi$ is strictly convex and hence $\\nabla \\phi$ is invertable with $g_e$ in Assumption 3.3 satisfying $g_e = \\nabla \\phi^{-1}$. We can further check that for a minimizer, $\\upsilon$, of the logistic loss we must have\n$\\nabla_{\\upsilon} (\\eta \\, log(1 + exp(-\\upsilon)) + (1 - \\eta) \\, log(1 + exp(\\upsilon))) = \\frac{\\eta \\, exp(-\\upsilon)}{1 + exp(-\\upsilon)} - \\frac{(1 - \\eta) \\, exp(\\upsilon)}{1 + exp(\\upsilon)} = 0,$\nwhere $\\eta = P(\\omega = 1|x, y, y')$. The above implies $\\upsilon = log \\frac{\\eta}{1 - \\eta}$. Equivalently, we could have computed the derivative of the convex conjugate of $\\phi$, $\\nabla \\phi^*$, which is precisely $\\nabla \\phi^{-1}$. Finally, to establish the claimed connection between the parametrization of $\\Pi$ to the BTL parametrization with $\\upsilon^*$ we have the following. The fact that the logistic loss is proper implies\n$R(x, y) - R(x, y') - log \\frac{\\mu(y|x)}{\\mu(y'|x)} = \\omega_{\\pi,\\mu}(x, y, y')$= log$\\frac{\\pi(y|x)}{\\pi(y'|x)} $ \\approx \\frac{\\eta}{1 - \\eta}$ exp($\\overline(R)^*(x, y) - \\overline(R)^*(x, y'))$\n$ = log \\frac{\\pi(y|x)}{\\pi(y'|x)} \\\nwhich further simplifying gives Equation 3.\n = log Z(x) - $(\\overline(R)^*(x, y) - \\overline(R)^*(x, y'))'$, A similar line of reasoning shows that when $l(\\upsilon) = (1 - \\upsilon)^2$, the link function satisfies $\\eta = \\frac{1+v}{2}$ and so the resulting reward model is $P(\\omega|x, y, y') = \\frac{1 + \\frac{R^*(x,y) - R^*(x,y')}{2}}{2}$\nB. Experiment details\nWe evaluate the different variants on the TL;DR dataset , where the task is to summarize posts on reddit forums. The dataset consists of an original reddit posts, along with a pair of responses which are rated by human judges to provide the groumd-truth preference annotations . Our experiments use a T5 large model with 770M parameters, which is further fine-tuned to maximize the log-likelihood of the winning responses in the TL;DR dataset. We train for 20000 iterations, with a batch size of 32. A KL regularizer is used to the reference $\\pi_{ref}$ checkpoint with coefficient equal to 0.005. The optimizer used is Adafactor with learning rate that is constant with a linear warm-up for 2000 steps and a base rate of le - 4.\nWe used the following text to prompt the Gemini evaluator used for our experiments:"}, {"title": "B. Experiment details", "content": "We evaluate the different variants on the TL;DR dataset , where the task is to summarize posts on reddit forums. The dataset consists of an original reddit posts, along with a pair of responses which are rated by human judges to provide the groumd-truth preference annotations . Our experiments use a T5 large model with 770M parameters, which is further fine-tuned to maximize the log-likelihood of the winning responses in the TL;DR dataset. We train for 20000 iterations, with a batch size of 32. A KL regularizer is used to the reference $\\pi_{ref}$ checkpoint with coefficient equal to 0.005. The optimizer used is Adafactor with learning rate that is constant with a linear warm-up for 2000 steps and a base rate of le - 4.\nWe used the following text to prompt the Gemini evaluator used for our experiments:"}]}