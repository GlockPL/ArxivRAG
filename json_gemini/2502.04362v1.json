{"title": "LLMs can be easily Confused by Instructional Distractions", "authors": ["Yerin Hwang", "Yongil Kim", "Jahyun Koo", "Taegwan Kang", "Hyunkyung Bae", "Kyomin Jung"], "abstract": "Despite the fact that large language models (LLMs) show exceptional skill in instruction following tasks, this strength can turn into a vulnerability when the models are required to disregard certain instructions. Instruction-following tasks typically involve a clear task description and input text containing the target data to be processed. However, when the input itself resembles an instruction, confusion may arise, even if there is explicit prompting to distinguish between the task instruction and the input. We refer to this phenomenon as instructional distraction. In this paper, we introduce a novel benchmark, named DIM-Bench, specifically designed to assess LLMs' performance under instructional distraction. The benchmark categorizes real-world instances of instructional distraction and evaluates LLMs across four instruction tasks: rewriting, proofreading, translation, and style transfer-alongside five input tasks: reasoning, code generation, mathematical reasoning, bias detection, and question answering. Our experimental results reveal that even the most advanced LLMs are susceptible to instructional distraction, often failing to accurately follow user intent in such cases.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Radford et al., 2019; Touvron et al., 2023) have demonstrated remarkable performance across a wide range of tasks (Wei et al., 2021), with instruction-following being one of the most critical requirements for their applications (Qin et al., 2024). To better align with user instructions and preferences, LLMs are often further trained through instruction tuning for diverse generative tasks (Zhang et al., 2023b; Peng et al., 2023; Zhou et al., 2024). In response to the increasing importance of instruction-following capabilities, several benchmarks have been developed to assess various aspects of this ability (Mishra et al., 2021; Jiang et al., 2023; Zhou et al., 2023; Oh et al., 2024). Typically, such benchmarks consist of an instruction that clearly describes the task or goal the model must perform, along with a target input\u2014the actual data or information the model needs to process according to the instruction.\nHowever, a significant challenge arises when the target input itself resembles an instruction, leading to confusion for the LLM (Wallace et al., 2024). We refer to this phenomenon as instructional distraction. Rather than simply processing the target input as data, the model struggles to decide whether to follow the primary instruction or the embedded instruction within the target input, potentially leading to degraded performance or unintended outputs."}, {"title": "2 Related Works", "content": "2.1 Instruction Following in LLMs\nInstruction following is a crucial task in LLMs, requiring them to generate responses aligned with user intent (Zhou et al., 2023). The rapid advancement of instruction tuning algorithms (Wang et al., 2022; Ouyang et al., 2022; Xu et al., 2023), along with strategic data selection (Wang et al., 2024), has enabled LLM to achieve impressive zero-shot performances across various downstream tasks (Peng et al., 2023; Wang et al., 2023b).\nDespite this progress, several studies highlight the limitations of LLMs when dealing with complex instructions (Xu et al., 2023; Zhou et al., 2023; He et al., 2024a). For example, Wen et al. (2024) and He et al. (2024b) each introduce a benchmark aimed at evaluating the performance of LLMs on complex instructions that consist of multiple constraints. Also, Jiang et al. (2023) introduce FollowBench, an instruction-following benchmark designed with multi-level fine-grained constraints. Additionally, Wallace et al. (2024) explore the concept of instruction hierarchy, revealing that models struggle when presented with instructions of conflicting priorities, and propose the notion of instruction privilege as a guideline to direct model behavior in such scenarios. Instruction conflict differs from instructional distraction in that the former involves multiple instructions with a defined priority order, while the latter offers a single instruction, with the input text serving as distractors that mimic an instructional format. However, no benchmark currently evaluates LLMs in instructional distraction scenarios, and this paper is the first to introduce a benchmark aimed at evaluating LLMs in such contexts.\n2.2 LLM-powered Data Generation and Processing\nLLMs have gained significant attention in data generation and processing tasks (Gandhi et al., 2024; Long et al., 2024; Guo and Chen, 2024). Their ability to produce coherent and contextually relevant text makes them invaluable for augmenting training datasets (Gilardi et al., 2023; Rosenbaum et al., 2023; He et al., 2023; Singh et al., 2023; Macias, 2024). For example, existing data can be paraphrased using LLMs to enhance diversity, thus improving model robustness. Moreover, to ensure data quality, tasks such as proofreading and filtering are commonly performed using LLMs (Lin et al., 2024). Furthermore, as acquiring annotated data for low-resource languages poses significant challenges (Magueresse et al., 2020), researchers leverage LLMs' superior translation capabilities (Vilar et al., 2022; Zhang et al., 2023a) to translate the available data into target languages (Zhang et al., 2021; Yang et al., 2023). LLMs are also utilized for style transfer tasks (Jin et al., 2022; Mukherjee and Du\u0161ek, 2024), generating variations of text in different styles while preserving the underlying content. However, when the target input data to be processed contains embedded instructions, instructional distraction can occur. This study analyzes how various LLMs respond to instructional distractions in various data generation and processing tasks."}, {"title": "3 DIM-Bench", "content": "We introduce a novel benchmark, named DIM-Bench, to evaluate the performance of LLMs in the context of instructional distractions. Section \u00a73.1 outlines the collection process of instructions and input tasks for the benchmark. Section \u00a73.2 discusses the benchmark's statistics, while Section \u00a73.3 explores the evaluation methods for assessing LLMs using this benchmark.\n3.1 Data Collection\nIn this section, we describe the process of data collection and filtering. Each data instance consists of two components: Instructions and Inputs. Instructions involve four key tasks\u2014rewriting, proofreading, translation, and style transfer\u2014while the Inputs consist of five tasks: reasoning, code generation, mathematical reasoning, bias detection, and question answering. Data examples for various combinations can be found in Table 1.\n3.1.1 Tasks for Instruction\nRewriting The goal of the rewriting task is to rephrase a given text while maintaining its original meaning. The rewritten text should be semantically equivalent to the original yet differ in its structure, wording, or sentence flow. To guide this process, we develop ten template prompts, including instructions such as, \"Restate the following input text in your own words.\"\nProofreading The proofreading task involves reviewing and correcting errors in grammar, spelling, and punctuation in a given text. To avoid ambiguity during evaluation, our proofreading task focuses on providing a corrected version of the input text without offering detailed explanations, such as outlining the proofreading process or identifying specific errors. A set of ten instruction templates is designed, including \"Generate a revised version of the input text with corrections for spelling and grammar..\".\nTranslation The translation task aims to convert the input text into one of the following languages: Chinese, Spanish, French, German, Arabic, Portuguese, Hindi, or Italian. * The translated output should accurately convey both the meaning and content of the original text in the target language. We create ten instructions to guide the translation process, including prompts such as \"Translate the input text into German.\"\nStyle Transfer Style transfer is a task aimed at transforming a given text to align with a specified stylistic framework. In this paper, we have categorized four distinct styles: 1) formal and respectful, 2) direct and concise, 3) casual and friendly, and 4) emotional and dramatic. The goal is to modify the input text in a way that conforms to one of"}, {"title": "3.1.2 Tasks for Input Data", "content": "Reasoning The reasoning task is intended to evaluate the model's capacity to make logical inferences or solve problems based on a provided scenario. The data for this task is sourced from the ARC dataset (Clark et al., 2018), which encompasses a diverse range of linguistic and inferential phenomena. Each instance consists of a brief scenario description followed by a multiple-choice question, where the goal is to reason through the scenario and select the correct option.\nCode Generation The code generation task involves asking the model to generate code based on a set of instructions or prompts. This task is derived from the Code Alpaca dataset (Chaudhary, 2023), which includes a variety of coding challenges and real-world programming problems. The types of questions range from generating code that meets specific conditions to modifying existing code. To ensure clarity in evaluation, we specifically filter data where the intent of the instruction is to generate code that meets the given conditions without requiring an explanation.\nMathematical Reasoning The mathematical reasoning task requires the model to solve math problems, ranging from basic arithmetic to more advanced topics (Imani et al., 2023). These problems are sourced from the GSM8k (Cobbe et al., 2021) and MATH datasets (Hendrycks et al., 2021), with an equal number of problems extracted from each dataset. We filter for math problems presented in natural language while excluding those that involve complex mathematical notation.\nBias Detection The bias detection task aims to detect social biases in language models, particularly by measuring biases across various protected social categories (Gallegos et al., 2024). The dataset for this task is derived from the BBQ (Parrish et al., 2021), which consists of human-annotated contexts designed to highlight social biases against different socially relevant groups through multiple-choice questions. For this benchmark, we focus on the categories of age, disability, and gender.\nQuestion Answering For the question answering task, we adopt a closed-book question answering approach (Roberts et al., 2020) to evaluate instructional distraction in longer contexts. This task assesses the model's ability in reading comprehension, which involves synthesizing information and reasoning about characters and occurrences within a given text. The task is sourced from the NarrativeQA dataset (Ko\u010disk\u1ef3 et al., 2018), and passage summaries are concatenated with questions related to their context."}, {"title": "3.2 Statistics", "content": "We construct a benchmark by combining the four instruction tasks and five input tasks previously described, resulting in 20 categories. Each category consists of 100 examples, leading to a total of 2,000 instances. The average token length of Instructions and Inputs for each category is provided in Table 2. Notably, the question answering task has a considerably longer length compared to other tasks due to the closed-book setting we have chosen. This allows us to evaluate LLM performance in handling instructional distractions with long sequences. Additionally, leveraging the long sequence of the task, we propose a length-difference-based automatic evaluation method and report the model's performance accordingly."}, {"title": "3.3 Evaluation", "content": "In this section, we introduce the evaluation methods used when assessing LLMs with DIM-Bench: an LLM-based evaluation method (Liu et al., 2023) and a length difference-based automatic evaluation method that enhances reliability. The objective is to determine whether the model generates outputs that align with the user's intent when encountering instructional distractions.\nDIM-Bench utilizes LLM-based evaluations to assess how effectively the output adheres to the given instructions, following the methodologies established in existing instruction-following benchmark evaluations (Zheng et al., 2023; Wang et al., 2023a). Typically, this is done by breaking down the evaluation into binary (yes/no) questions. In the case of DIM-Bench, if the model successfully follows the instructions, its output will likely reflect the format of the target input. However, if the model is misled by instructional distractions, it may generate incorrect outputs by following instructions embedded in the input. To evaluate this, we formulate 2-3 specific questions for each case. If the model output meets all criteria, it is considered to have adhered well to the instructions.\nFor example, if the instruction is a translation task (e.g., English to French), and the input task is reasoning, the questions are structured as follows: 1) Is the target text in French? 2) Is the target text in multiple-choice format? 3) Have any options from the original text been removed in the target text? In the third question, the original reasoning question is provided. If the LLM-judge's answers are yes, yes, and no, it confirms that the translation instructions are followed correctly, without any confusion from the reasoning task. The decomposed questions for the remaining categories are provided in Appendix C.\nIn addition to LLM evaluation, we further support the results by designing a length-difference-based automatic evaluation on the question answering task. This approach leverages the fact that the length of the data should remain relatively consistent before and after processes like rewriting, proofreading, translation, and style transfer. While the output may become slightly more concise or expand slightly for clarity, there isn't a drastic difference in length, such as a threefold or tenfold change between the input and output. Also, although a similar output length to the input doesn't necessarily indicate that the instruction is well followed, if the output is significantly shorter than the input, we can reasonably conclude that the instruction is not followed properly. Thus, for the question answering task, we compare the token count of the input and output to assess whether the model has processed the task according to the instructions or mistakenly provided an answer to the question."}, {"title": "4 Experiments", "content": "In this section, we use the DIM-Bench to assess the performance of various LLMs in handling instructional distractions. Further details about the experimental setup, including the specific prompts used, are provided in Appendix A.\n4.1 Experimental Setting\nModels In this experiment, we evaluate the robustness of six LLMs against instructional distractions. We first assess two open-source models from the Llama herd (Dubey et al., 2024): Llama-3.1-8B-Instruct, designed for efficient instruction-following, and Llama-3.1-70B-Instruct, a larger model optimized for complex prompts. Additionally, we evaluate Qwen-2.5-7B (Qwen Team, 2024), an open-source model known for its capability to balance instruction-following and general understanding. We also evaluate three closed-source models: GPT-3.5-turbo(OpenAI, 2023), known for balanced performance; GPT-4o-mini(OpenAI, 2024a), a cost-efficient model with superior textual intelligence; and GPT-4o (OpenAI, 2024b), an enhanced version for handling complex instructions.\nPrompting We conduct experiments using zero-shot LLM instruction-following prompting based on Lou et al. (2024). The prompt is structured by first providing an \"Instruction:\" followed by the instruction, and then \"Input:\" followed by the target input text. Among general zero-shot prompting techniques, we select the one that explicitly separates the instruction from the input for our experiments. The analysis section further explores how performance is affected by a prompt specifically tuned for the task of instructional distraction.\nJudge Model We use GPT-4o as the judge LLM to evaluate whether the outputs generated by each model adhere to the given instructions (Zheng et al., 2023). GPT-4o is widely recognized as a high-performance judge model and is known for delivering consistent evaluation results (Bavaresco et al., 2024). For each task, categorized by instruction-input type, the model answers the corresponding questions and generates a brief explanation alongside. The temperature is set to 0 to ensure deterministic outputs. Additional experimental details can be found in Appendix A.\n4.2 LLM Evaluation Results\nWe evaluate the performance of six LLMs across 20 distinct categories under instructional distraction scenarios using DIM-Bench. Our findings reveal that all LLMs \u2014 including strong models like GPT-4o and Llama-3.1-70B-Instruct \u2014 struggle significantly in following instructions across all categories, as shown in Table 3. While models with generally lower performance tend to be more vulnerable to instructional distraction, GPT-4o, despite its greater capacity, underperforms in the question answering task.\nFocusing on four instruction types, the models achieve an average accuracy of 0.301 in Style Transfer, 0.397 in Rewriting, 0.526 in Translation, and 0.458 in Proofreading. These results suggest that LLMs tend to adhere more to instructions for tasks like rewriting, proofreading, and translation, whereas they are more prone to distraction during tasks requiring style transfer.\nMoreover, among the input tasks, those involving question formats, such as bias detection (0.208), reasoning (0.493), and question answering (0.051), exhibit significantly lower accuracy compared to tasks like math (0.738) and code generation (0.612). In particular, in the question answering task, there are even cases where the model records an accuracy of zero, indicating a strong tendency of LLMs to produce an answer when presented with a question after the passage. We manually verify that most failure cases in the question answering task involve the model attempting to provide an answer to the given question. Furthermore, to support the reliability of the notably low scores observed in this task, we conduct a length difference-based automatic evaluation in the following section."}, {"title": "5 Analysis", "content": "5.1 Task-Specific Prompting\nWe observed that, even when clearly distinguishing between instruction and input through general prompting, LLMs often fail to align with user intent in instructional distraction scenarios. Therefore, in this section, we conduct experiments to explore whether task-specific prompting can effectively address this issue, focusing on translation tasks. Specifically, we employ three prompting strategies: the first is direct prompting (DIRECT), which explicitly instructs the model to disregard any instructions or questions embedded in the input, and the second is Chain-of-Thoughts (CoT) prompting (Wei et al., 2022), which encourages the model to generate responses by following a step-by-step reasoning process. As demonstrated in Table 4, both methods contribute to an improvement in average performance when evaluated by an LLM judge. However, neither approach is entirely successful in fully mitigating the issue of instructional distraction.\nMoreover, we also experiment with a prompting strategy that alters the sequence of instructions and target inputs (Suffix Instruction). + The results indicate that, in most tasks, placing the instruction after the target input increases the LLM's vulnerability to instructional distraction.\n5.2 Impact Variations Based on Input Length\nMoreover, to examine how input length impacts distraction, we conduct LLM-based evaluations by varying the input length in a question answering task. For testing purposes, we construct four data sets\u2014QAshort, QAmedium, QAlong, and QAsuperlong with average token counts of 362, 743, 1,087, and 3,007, respectively. Also, we focus on translation tasks among the instruction tasks. The experimental results reveal that as the input text length increased, LLMs became more prone to distraction, as shown in Table 5. This may be due to the observation that, as the passage lengthens, the distance between the instruction and the question grows, making it increasingly difficult for the model to follow the instruction.\n5.3 Case Study\nWe present examples of error cases in Table 6, illustrating how instructional distractions influence the performance of LLMs. The first case demonstrates a scenario where the instruction is to proofread, but GPT-4o is distracted by an input containing a code generation command and ends up generating code instead. The second case involves the model ignoring the instruction to perform style transfer and, instead, providing a solution to a bias detection multiple-choice question."}, {"title": "6 Conclusion", "content": "In this study, we explore the phenomenon of instructional distraction in instruction-following tasks, where the input itself resembles an instruction, potentially confusing the model. We categorize various instances of instructional distraction as they occur in real-world scenarios and evaluate the performance of several LLMs when confronted with these distractions. We demonstrate that all tested LLMs fail to fully match user intent when encountering instructional distraction, highlighting a critical gap in current LLM capabilities in accurately understanding and processing such inputs."}, {"title": "Limitations", "content": "In this study, various tasks commonly used in data processing with LLMs are addressed. However, tasks such as summarization, where multiple valid output forms may exist depending on the user's intent\u2014i.e., one-to-many tasks\u2014are not considered. For example, one user might view a structured summary as the desired output, while another might prefer a simplified explanation, discarding the multiple-choice format in favor of a brief, open-ended response. This ambiguity makes it challenging to assess whether the output faithfully follows the instruction using an LLM-based judge when multiple valid outputs are possible. Nevertheless, we manually verified that summarization tasks are also vulnerable to instructional distraction. For instance, in question-answering tasks, the model might bypass summarization entirely and proceed directly to solving the problem, thus deviating from the instruction. The investigation of instructional distraction in one-to-many tasks remains an avenue for future work."}, {"title": "Ethics Statement", "content": "In our benchmark setup, all datasets utilized were publicly available and applied for their intended purposes. Additionally, we performed our evaluations using GPT models accessed through OpenAI's official websites. Similarly, Qwen 2.5 and Llama 3.1 models \" were obtained via official source, following proper authorization protocols. Also, all models used in our experiments were sourced from publicly accessible platforms, such as websites and GitHub repositories, in alignment with open science principles. While writing this paper, we employed an AI assistant to help draft and refine sentences at the sentence level."}]}