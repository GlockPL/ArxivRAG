{"title": "1.58-bit FLUX", "authors": ["Chenglin Yang", "Celong Liu", "Xueqing Deng", "Dongwon Kim", "Xing Mei", "Xiaohui Shen", "Liang-Chieh Chen"], "abstract": "We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in {-1, 0, +1}) while maintaining comparable performance for generating 1024 \u00d7 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7\u00d7 reduction in model storage, a 5.1\u00d7 reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.", "sections": [{"title": "1. Introduction", "content": "Recent text-to-image (T2I) models, including DALLE 3 [3], Adobe Firefly 3 [1], Stable Diffusion 3 [14], Midjourney v6.1 [41], Ideogram v2 [26], PlayGround V3 [36], FLUX 1.1 [29], Recraft V3 [2], have demonstrated remarkable generative capabilities, making them highly promising for real-world applications. However, their immense parameter counts, often in the billions, and high memory requirements during inference pose significant challenges for deployment on resource-constrained devices such as mobile platforms.\nIn this work, we address these challenges by exploring extreme low-bit quantization of T2I models. Among available state-of-the-art models, we select FLUX.1-dev [30] as our quantization target due to its public availability and competitive performance. Specifically, we quantize the weights of the vision transformer in FLUX to 1.58 bits without relying on mixed-precision schemes or access to image data. The quantization restricts linear layer weights to the values {+1, 0, -1}, akin to the BitNet b1.58 [40] approach. Unlike BitNet, which involves training large language models from scratch, our method operates as a post-training quantization solution for T2I models.\nThis quantization reduces model storage by 7.7\u00d7, as the 1.58-bit weights are stored using 2-bit signed integers, compressing them from 16-bit precision. To further enhance inference efficiency, we introduce a custom kernel optimized for low-bit computation. This kernel reduces inference memory usage by over 5.1\u00d7 and improves inference latency, as detailed in Fig. 2 and Tab. 3. Comprehensive evaluations on T2I benchmarks, including GenEval [17] and T2I Compbench [23], reveal that 1.58-bit FLUX achieves comparable performance to full-precision FLUX, as shown in Tab. 1 and Tab. 2.\nOur contributions can be summarized as follows:\n\u2022 We introduce 1.58-bit FLUX, the first quantized model to reduce 99.5% of FLUX vision transformer parameters (11.9B in total) to 1.58 bits without requiring image data, significantly lowering storage requirements.\n\u2022 We develop an efficient linear kernel optimized for 1.58-bit computation, enabling substantial memory reduction and inference speedup.\n\u2022 We demonstrate that 1.58-bit FLUX maintains performance comparable to the full-precision FLUX model on challenging T2I benchmarks.\nThis work represents a significant step forward in making high-quality T2I models practical for deployment on"}, {"title": "2. Related Work", "content": "Quantization is a widely adopted technique for reducing model size and enhancing inference efficiency, as demonstrated in numerous studies [4, 10, 13, 15, 18, 19, 25, 27, 42, 43, 53]. It has proven particularly effective for serving large language models (LLMs) [8, 12, 16, 28, 34, 35, 39, 47, 54, 56, 59, 63], enabling significant resource savings without compromising performance. Recent advancements include both post-training quantization (PTQ) methods [16, 56, 59], which adjust pre-trained models for efficient deployment, and Quantization Aware Training (QAT) approaches that fine-tune the model to low bits from pre-trained checkpoints [9, 21, 39, 44] or train the model from scratch [51]. For instance, BitNet b1.58 [40] employs weights in linear layers restricted to three values {+1, 0, -1}, facilitating highly efficient inference, particularly on CPUs [52]. These developments highlight the potential of quantization to address the computational challenges associated with deploying large-scale models.\nFor image generation models, prior work has explored various quantization techniques for diffusion models [5, 7, 20, 22, 24, 33, 38, 46, 49, 50, 55, 57, 58], including Hadamard transformations [37], vector quantization [11], floating-point quantization [37], mixed bit-width allocation [62], diverse quantization metrics [61], multiple-stage fine-tuning [48] and low-rank branch [32]. These approaches aim to optimize model efficiency while maintaining performance. In this work, we focus on post-training 1.58-bit quantization of FLUX, a state-of-the-art open-source text-to-image (T2I) model. Notably, our method achieves efficient quantization without relying on any tuning image data and is complemented by optimized inference techniques."}, {"title": "3. Experimental Results", "content": "Quantization. We utilize a calibration dataset comprising prompts from the Parti-1k dataset [60] and the training split of T2I CompBench [23], totaling 7,232 prompts. This process is entirely image-data-free, requiring no additional datasets. The quantization reduces the weights of all linear layers in the FluxTransformerBlock and FluxSingleTransformerBlock of FLUX to 1.58 bits, covering 99.5% of the"}, {"title": "3.2. Results", "content": "Performance. Comparable performances between 1.58-bit FLUX and FLUX on T2I Compbench and GenEval are observed in Tab. 1 and Tab. 2, respectively. The minor differences observed before and after applying our linear kernel further demonstrate the accuracy of our implementation.\nEfficiency. Significant efficiency gains are observed in both model storage and inference memory, as shown in Fig. 2. For inference latency, as illustrated in Tab. 3, even greater improvements are achieved when running 1.58-bit FLUX on lower-performing yet deployment-friendly GPUs, such as the L20 and A10."}, {"title": "4. Conclusion and Discussion", "content": "This work introduced 1.58-bit FLUX, in which 99.5% of the transformer parameters are quantized to 1.58 bits. With our custom computation kernels, 1.58-bit FLUX achieves a 7.7\u00d7 reduction in model storage and more than a 5.1\u00d7 reduction in inference memory usage. Despite these compression gains, 1.58-bit FLUX demonstrates comparable performance on T2I benchmarks and maintains high visual quality. We hope that 1.58-bit FLUX inspires the community to develop more robust models for mobile devices.\nWe discuss the current limitations of 1.58-bit FLUX below, which we plan to address in future work:\n\u2022 Limitations on speed improvements. Although 1.58-bit FLUX reduces model size and memory consumption, its latency is not significantly improved due to the absence of activation quantization and lack of further optimized kernel implementations. Given our promising results, we hope to inspire the community to develop custom kernel implementation for 1.58-bit models.\n\u2022 Limitations on visual qualities. Fig. 1, Fig. 3 and Fig. 4 show that 1.58-bit FLUX can generate vivid and realistic images closely aligned with the given text prompts. However, it still lags behind the original FLUX model in rendering fine details at very high resolutions. We aim to address this gap in future research."}]}