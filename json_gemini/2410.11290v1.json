{"title": "BACKDOOR ATTACK ON VERTICAL FEDERATED GRAPH NEURAL\nNETWORK LEARNING", "authors": ["Jirui Yang", "Peng Chen", "Zhihui Lu", "Ruijun Deng", "Qiang Duan", "Jianping Zeng"], "abstract": "Federated Graph Neural Network (FedGNN) is a privacy-preserving machine learning technology that\ncombines federated learning (FL) and graph neural networks (GNNs). It offers a privacy-preserving\nsolution for training GNNs using isolated graph data. Vertical Federated Graph Neural Network\n(VFGNN) is an important branch of FedGNN, where data features and labels are distributed among\nparticipants, and each participant has the same sample space. Due to the difficulty of accessing\nand modifying distributed data and labels, the vulnerability of VFGNN to backdoor attacks remains\nlargely unexplored. In this context, we propose BVG, the first method for backdoor attacks in\nVFGNN. Without accessing or modifying labels, BVG uses multi-hop triggers and requires only\nfour target class nodes for an effective backdoor attack. Experiments show that BVG achieves high\nattack success rates (ASR) across three datasets and three different GNN models, with minimal\nimpact on main task accuracy (MTA). We also evaluate several defense methods, further validating\nthe robustness and effectiveness of BVG. This finding also highlights the need for advanced defense\nmechanisms to counter sophisticated backdoor attacks in practical VFGNN applications.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) are effective tools for handling graph-structured data and have been widely applied to\ncomplex problems such as node classification, graph classification, and edge prediction. They have shown great potential\nin bioinformatics, chemistry, medicine, cybersecurity, linguistics, and transportation [1]. For example, financial risk\ncontrol systems can train GNNs on transaction data to identify potential fraudulent activities, such as money laundering\n[2]. However, graph data are often distributed among multiple owners, making direct access impractical due to privacy\nconcerns, regulations, and competitive dynamics [3].\nFederated Learning (FL), as a distributed machine learning method that does not require data sharing, effectively solves\nthe data isolation problem. Training GNNs with FL is known as Federated Graph Neural Networks (FedGNN), which\ncan be categorized into two types: Horizontal Federated Learning (HFGNN), where different participants have the same\nfeatures but different samples [4, 5]; and Vertical Federated Learning (VFGNN), where different participants have the\nsame samples but different features [6, 7]. This paper focuses on VFGNN, which is more common in heterogeneous\ndata environments such as finance and healthcare."}, {"title": "2 Related Work", "content": "In the fields of VFL and GNN, researchers are investigating backdoor attacks specifically tailored to each domain."}, {"title": "2.1 Backdoor attacks on Vertical Federated Learning", "content": "Federated Learning is especially vulnerable to backdoor attacks due to its distributed nature [17]. However, the vertically\nsplit model of VFL restricts the attacker's access to the training labels, let alone modify them.\nThere are two notable directions in attempting to solve this issue. The first is to conduct label inference attacks [18] to\nget some labels in advance. For example, VILLAIN [19] leverages label inference to pinpoint samples of the target\nlabel and then poisons these samples to inject the backdoor. BadVFL [10] and LR-BA [20] assume attackers can\nacquire labels of a certain number of samples from each category with label inference. Another direction is to make the\nknowledge of a small number of labeled samples from the target class a prerequisite [21, 11, 12]. These methods try to\nuse as few as possible (e.g., 500 for [21] and 0.1% for [11]) the labeled target-class samples to establish links between\nbackdoor triggers and target label. Normally, in VFL, the backdoor attack can only be conducted in a clean-label manner\n[22]. Therefore, works like TPGD [23] assuming the label modification capability of the active party are impractical."}, {"title": "2.2 Backdoor attacks on Graph Neural Networks", "content": "The unique challenge to graph-oriented backdoor attacks lies in the inherently unstructured and discrete nature of\ngraph data [14]. For structured and continuous data like images, attackers can directly stamp a trigger pattern (e.g., a\nblack square on the bottom right of the image) s onto a benign image x to achieve a poisoned sample $x_p = x + s$ [13].\nHowever, the backdoor attacks against GNN involve design triggers within a large spectrum, including topological\nstructures and descriptive (nodes and edges) features [14]. The effectiveness of backdoor attacks largely depends on\ndesigning triggers tailored to the specific attributes of the target task. These triggers mainly fall into three categories:\nmalicious subgraph structures, graph structure perturbations, and node attribute manipulation.\nMalicious subgraph structures involve adding malicious subgraphs to nodes [24], edge endpoints [16], or specific\npositions in the graph [25, 14], causing nodes, edges, or graphs to be misclassified. Graph structure perturbation\ninvolves injecting special node connections into the training set adversarially, allowing backdoor attacks to be executed\nby merely modifying the graph's topology during the attack [26]. Node attribute manipulation involves carefully\ndesigning special node features and adding them to the target class nodes in the training set, followed by adaptive\noptimization of the graph structure to train a GNN model with backdoors [15, 27]. Although these methods are viable\nin centralized GNN scenarios, they all rely on full access to the training set, which is nearly impossible in VFGNN.\nThe aforementioned backdoor techniques, whether tailored for GNN or VFL, are not applicable to VFGNN due to\nunique threat models. Existing methods such as CBA, DBA, and Bkd-FedGNN focus on HFGNN and do not address\nthe needs of VFGNN [9, 8]. VFL backdoor attack methods lack consideration for the data structure of graph data, while\nGNN backdoor attacks face difficulties in handling vertically partitioned data. To address these issues, we propose the\nBVG algorithm for VFGNN, which uses dual-layer optimization and multi-hop triggers to achieve high attack success\nrates with minimal impact on the main task accuracy (MTA)."}, {"title": "3 Proposed Approach", "content": "In this section, we formalize the backdoor attack problem in VFGNN and provide a detailed threat model. Next, we\npresent our attack method, with a sketch of the entire approach shown in Figure 2."}, {"title": "3.1 Problem Formulation", "content": "Graph data is represented as $G = (V, E, X)$, where $V = {v_1, . . ., v_N }$ is a set of N nodes, $E \\subseteq V \\times V$ is a set of\nedges, and $X = {x_1,...,x_n }$ is a set of node attributes, with $x_i$ being the attribute of node $v_i$. The adjacency matrix\nof graph G is denoted as $A \\in R^{N\\times N}$, where $A_{ij} = 1$ if nodes $v_i$ and $v_j$ are connected; otherwise, $A_{ij} = 0$.\nWe focus on the node classification problem, which is common in real-world applications like social networks. In\ninductive node classification tasks, only a subset of nodes $V_L$ in the training graph have labels $Y_L = {Y_1, ..., Y_{N_L}}$.\nThe test nodes $V_T$ are disjoint from the training nodes.\nIn VFGNN, K (K >= 2) participants collaboratively train a model using their private data. Each participant k has its\nlocal graph $G^k = (V, E^k, X^k)$, which comprises the entire node set V a subset of edges $E^k$, and a subset of feature\ndata $X^k$ [6]. From a global perspective, all participants collectively own a global graph $G^{gl} = (V, E^{gl}, X^{gl})$, where\n$E^{gl} = E^1 \\cup... \\cup E^K$, and for any node attribute in $X, x_i^{gl} = x_i^1|| ... ||x_i^K$, where $||$ is the concatenation operator. One\nof the K participants is an active party that knows the corresponding labels of the labeled node set $V_L \\subseteq V$, and all\nother participants are negative parties that have no access to the label information.\nEach participant employs a local GNN model $f_k$ parameterized by $\\theta_k$ to compute the local output $H_i^k = f_k(G_i^k;\\theta_k)$,\nwhere $G_i^k$ denotes the computation graph of node $v_i$. In addition to training its own local model, the active party also\ntrains a top model G parameterized by $\\theta_{top}$, which aggregates the outputs from all local models to minimize the loss"}, {"title": "3.2 Threat model", "content": "We assume the adversary is a passive party, not the active party, because the active party can modify labels and thus\neasily perform backdoor attacks. All other participants are considered trustworthy.\nAdversary's capacity. The adversary adheres to the VFL protocol, transmitting local features and receiving gradients\nwithout manipulating other participants' information.\nAdversary's objective. In VFGNN multi-classification tasks, the adversary's objective is to inject a backdoor into the\nmodel during the training phase. When the poisoned model is deployed for applications, it will misclassify any local\ndata with the backdoor trigger as the designated target class but maintains classification capacity for all other clean data.\nAdversary's knowledge. In backdoor attacks, the adversary requires only a very few training samples labeled\ntarget class. Although this requirement deviates from the original VFGNN setting, it is feasible in practical VFGNN\napplications because it is possible for an adversary to acquire a small number of target class samples through various\nmeans, such as direct purchasing [18, 20]. Our experiment results indicate that acquiring only one to four target class\nnodes is sufficient for a successful backdoor attack. Apart from these limited target samples, the adversary has no\ninformation about the models and data of other parties."}, {"title": "3.3 Multi-hop Trigger Generation", "content": "Due to the special nature of the VFGNN architecture, all participants are aware of all nodes in the dataset, but possess\ndifferent attributes of the nodes. Therefore, the adversary cannot generate a subgraph structure as a trigger like common\nGNN backdoor attacks [24, 28]. Here, the trigger is added to the node attributes. To prepare a trigger, we propose a\nmulti-hop trigger generation method without introducing new nodes. The trigger is generated and trained with the\nVFGNN model using bi-level optimization. We use the PGD method [29, 30] to generate the trigger. The optimization\nformula for the trigger is as follows:\n$\\delta_{t+1} = \\Pi_{\\epsilon} (\\delta_t - \\alpha \\cdot sgn(\\nabla_{\\delta}L (F (\\alpha(G_p, \\delta_t); \\Theta^*),\\tau)))$,\nwhere t is the step index, $G_p$ represents the computation graph of the node $v_p$ to inject the trigger, where $v_p \\in V_p$.\n$\\nabla_{\\delta}L (F (\\alpha(G_p, \\delta_t); \\Theta^*), \\tau)$ denotes the gradient of the loss function for the backdoor target class with respect to the\ntrigger, $\\alpha$ is the step size, $\\Pi_{\\epsilon}$ keeps $\\delta$ within an e-ball at each step, $F(\\Theta^*)$ refers to the pre-trained VFGNN model.\nsgn(.) denotes the sign function.\nThe multi-hop trigger $\\delta$ affects the target node and its neighbors, $\\delta = {\\delta_0, \\delta_1, ..., \\delta_M }$. Here, $\\delta_m$ is the trigger added\nto the attributes of the m-hop neighbors of the target node $v_p$. Therefore, the operation of adding triggers to $G_p$ can be\nexpressed as\n$\\alpha(G_p, \\delta) = (x_p + \\delta^0, X_{1-hop} + \\delta^1,\u2026\u2026\u2026, X_{M-hop} + \\delta^M)$,\nwhere $x_p$ is the attribute of node $v_p$, $X_{m-hop}$ are the attributes of the m-hop neighbors, and $\\delta^0, \\delta^m$ are the triggers\nadded to these attributes, respectively.\nAccording to (3), the generation of the trigger relies on the gradients of the target class nodes T. The generation of the\ntrigger and the training of the VFGNN model are accomplished together, which can be expressed through the following\nbi-level optimization:\n$\\min_{\\delta}\\sum_{v_i \\in V_P}L(F(\\alpha(G_i, \\delta); \\Theta^*), \\tau)$\ns.t. $\\Theta^* = \\arg \\min_{\\Theta} \\sum_{v_i \\in V_L-V_P}L(F(G_i; \\Theta), Y_i)$\n$+\\sum_{v_i \\in V_P} L(F(\\alpha(G_i, \\delta); \\Theta), \\tau)$,\nwhere the trigger $\\delta$ is trained according to all known target class nodes $V_P$ available to the adversary, $V_P \\subseteq V_L$.\nEssentially, this trigger serves as a universal trigger for the target class $\\tau$ [31, 32].\nAlgorithm 1 outlines the procedure of the proposed attack. In each iteration of VFGNN model training, the adversary\nA injects the trigger $\\delta$ into the local computation graph of known target class samples $V_P$ according to (4) (line 4).\nSubsequently, each participant k computes the node embeddings through the bottom model $f_k$ (lines 5-8), where $H_i^k$\nrepresents the embedded features of the i-th data from the k-th participant. Upon receiving these embedded features,\nthe active party computes the gradients of the loss function with respect to the top model and the embedded features\nof each participant according to (1). The top model is then updated, and the gradients of the loss with respect to\nthe embedded features $\\frac{\\partial L}{\\partial H_i^k}$ are transmitted to each participant (lines 9-11), where $H_i^i$ denotes the aggregation of the"}, {"title": "4 Experiments", "content": "To demonstrate the effectiveness of BVG in VFGNN settings with different local GNN structures, we use three common\nGNN models as local participants:"}, {"title": "4.1 Experiments Settings", "content": "To demonstrate the effectiveness of BVG in VFGNN settings with different local GNN structures, we use three common\nGNN models as local participants:"}, {"title": "4.1.1 Local GNN Models", "content": "Graph Convolutional Network (GCN) [33]: GCN captures local features by propagating and aggregating information\nbetween nodes and their neighbors. Each local GNN model in VFGNN uses a two-layer GCN.\nGraph Attention Network (GAT) [34]: GAT uses an attention mechanism to assign adaptive weights to neighbor\nnodes, enhancing the model's ability to focus on important nodes.\nSimple Graph Convolution (SGC) [35]: SGC simplifies GCN by removing nonlinear activations and reducing layers,\nmaking it more efficient while retaining essential graph structural information.\nTo ensure a fair comparison with previous studies [36], we use a two-layer GNN model for each participant's local\nGNN to extract local node embeddings, with the dimension set to 16. The number of hidden units is fixed at 32. For\nGCN and GAT, the activation function is ReLU. VFGNN is trained using Adam, with a learning rate of 0.01."}, {"title": "4.1.2 Datasets", "content": "This paper uses three widely adopted public datasets to evaluate the performance of BVG, including Cora [37], Cora_ml\n[37], and Pubmed [38].\nEach participant in the VFGNN framework has access to all the nodes in the datasets, but the node features are equally\nsplit among the participants. We randomly split the edges of the graphs into equal parts, one for each participant,\nwithout overlapped edges between any two participants. We assume the adversary knows only four target class nodes\n(i.e., $|V_p| = 4$), which are randomly selected from the training set."}, {"title": "4.1.3 Performance Metrics", "content": "To evaluate the performance of the BVG method, we employ two common metrics: attack success rate (ASR) and main\ntask accuracy (MTA) [39]. ASR measures the percentage of samples in the backdoor test set that are predicted as the\ntarget class by the poisoned model. On the other hand, MTA assesses the accuracy of the clean test set on the poisoned\nmodel."}, {"title": "4.1.4 Comparison Benchmarks", "content": "Due to the limited research on backdoor attacks against VFGNN, we chose two attack methods from other scenarios\nand the baseline VFGNN (without backdoors) for comparison. To make the attack methods from other scenarios\nembedded features from all participants for the i-th data. After receiving the gradients, the adversary computes $\\nabla_{\\delta}L$\nand updates the trigger using (3). The current model parameters serve as the pre-trained model $F(\\Theta^*)$ (lines 12-14).\nFinally, each participant updates its bottom model based on the gradients sent by the active party (lines 15-19)."}, {"title": "3.4 Backdoor Trigger and Deployment", "content": "After the training is completed, the attacker has successfully injected a backdoor into the model. When the VFGNN\nmodel is deployed in practical applications, any local data containing the trigger (added using the method described\nin (4)) will be misclassified as the specified target class, while other clean data will still be correctly classified. The\nattacker successfully implements the backdoor attack using a small number of labeled target class samples."}, {"title": "4.2 Experiment Results Analysis", "content": ""}, {"title": "4.2.1 Attack Efficacy in Two-Party Setting", "content": "We first conducted experiments in a VFGNN framework with two parties, and the obtained experimental results are\nshown in Table 2. To evaluate the impact of the BVG method on the main classification task, we also tested the\nperformance of VFGNN without any backdoor attack (the Baseline in Table 2). The table shows that both comparison\nmethods and BVG have a decline in the main task performance compared to the baseline, but BVG has the smallest\ndecline, thus achieving the highest MTA among the attack methods. Table 2 also shows that BVG has a significantly\nhigher backdoor attack success rate than those of to the compared methods for all the tested GNN models and used\ndatasets. These results indicate that BVG achieves a high attack success rate with the least impact on the main task\nperformance, making it an effective backdoor attack method to VFGNN."}, {"title": "4.2.2 Attack Efficacy in Multi-Party Setting", "content": "We also evaluated the proposed attack method's performance in multi-party VFGNN scenarios where multiple partic-\nipants are involved. Unlike the two-party setting, multi-party scenarios become more complex due to the increased\nnumber of participants and their interactions. Our evaluation focuses on how the presence of multiple participants\naffects the attack success rate and the main task performance of VFGNN.\nWe conducted experiments with two, three, and four participants to evaluate the scalability and effectiveness of the\nproposed backdoor attack method in multi-party VFGNN scenarios. Each participant was assigned a subset of the\ngraph's edges and node features, with one active party holding the labels for the labeled node set $V_L$. The passive\nparticipants, including the adversary, only had partial information about the graph. The experimental results are shown\nin Figure 3.\nThe results indicate that the main task accuracy slightly decreases as the number of participants increases. This trend\ncould be attributed to the increased complexity and noise introduced by multiple participants. The backdoor attack\nsuccess rate remains high across all multi-party settings but slightly decreases as the number of participants increases.\nThe obtained results verify that our attack method is effective even in more complex multi-party environments. The\nslight decrease in success rate may be due to the dilution of the adversary's influence with more participants."}, {"title": "4.2.3 Attack Efficacy under Defense", "content": "To evaluate the robustness of the BVG method, we explored potential backdoor defense strategies within the VFGNN\nframework. We considered two main types of defense: the first type is GNN backdoor defense methods, such as\nPrune and Prune+LD [24]. These methods defend against backdoor attacks by pruning the edges between nodes with\nlow similarity. Pruning these edges can disrupt the attacker's trigger structure and connections. However, VFGNN'S\narchitecture prevents defenders from pruning the adversary's data, making these methods unsuitable. The second type\nis VFL backdoor defense methods, which reduce the leakage of labels, features, and other information during training\nby perturbing the gradients during the VFGNN training process, thereby lowering the likelihood of attackers implanting\nbackdoors.\nWe implemented several representative VFL backdoor defense methods within the VFGNN framework. These defenses\ninclude Differential Privacy (DP) [18], max norm [40], Gradient Compression (GC) [18, 41], and adding Gaussian\nnoise to intermediate features. These methods defend against backdoor attacks by perturbing the gradients sent from\nthe active party to the adversary during VFGNN training. In our experiments, we set the compression rates of GC and\nthe scale of DP to 0.75, 0.5, 0.25, and 0.1, and the scale of Gaussian noise to 0.5, 0.1, 0.01, and 0.001.\nThe obtained results are depicted in Figure 4 and show a trade-off between model utility and backdoor protection in\nVFGNN. Although different defense strategies can partially reduce the impact of backdoor tasks, they also degrade the\nperformance of the main task. In our experiments with three underlying models and three datasets, ISO provided a"}, {"title": "4.3 Analysis of Hyperparameter in BVG", "content": ""}, {"title": "4.3.1 Impact of Multi-hop Trigger in BVG", "content": "Considering the sensitivity of GNN models to spatial relationships, we employed multi-hop triggers in the BVG method.\nThis involves injecting triggers into the multi-hop neighbors of the target node to enhance the performance of backdoor\nattacks. Since the Bottom model used in our experiment is a 2-layer GNN, its receptive field is limited to 2-hop\nneighbors. Therefore, we only considered the trigger injection up to 2 hops. To evaluate the impact of multi-hop triggers\non BVG performance, we compared the backdoor task performance of the BVG method under three scenarios: without\nusing multi-hop triggers (0 hop), considering only 1-hop neighbors (1 hop), and considering 2-hop neighbors (2 hops).\nWe also conducted experiments on three datasets using three bottom models, with each value being the average of five\nindependent trials. It can be observed that the more hops considered for the trigger, the better the ASR performance.\nCompared to not using multi-hop triggers, the performance improvement when using 2-hop triggers can even exceed\n20%. This demonstrates the effectiveness of multi-hop triggers for VFGNN backdoor attacks."}, {"title": "4.3.2 Impact of Target Dataset Size", "content": "The BVG method requires a minimal amount of auxiliary target sample labels for backdoor attacks. These labeled\nnodes play a crucial role in BVG, making the size of $V_P$ (denoted as N) an important hyperparameter that affects BVG's\neffectiveness and its real-world threat. We measured the impact of different sizes of $V_P$ on BVG performance (ASR and\nMTA) across three base models and three datasets,\nThe figure shows that the BVG method can achieve excellent backdoor attack performance even with very limited\nauxiliary target data. In the Cora_ml and Pubmed datasets, the BVG method performs well with just one target class\nnode. For more complex datasets like Cora, the method achieves over 80% attack accuracy with just one target class\nnode, and only four target class nodes are needed to achieve good ASR and MTA performance. This demonstrates that\nBVG can effectively carry out backdoor attacks in VFGNN, even with very limited information about target samples,\nthus providing an efficient attack method in practical VFGNN scenarios."}, {"title": "4.3.3 Impact of Trigger Perturbation", "content": "In backdoor attacks, trigger perturbations are usually constrained to ensure stealthiness. Figure 6 shows the performance\nof BVG on three datasets and three Bottom models with perturbations ($\\epsilon$) of 0.002, 0.02, and 0.2.\nWe conducted experiments on three datasets and observed that as the perturbation increases, the ASR performance\nof the backdoor attack significantly improves. The required perturbation value varies across different datasets, with\nthe Cora dataset requiring a larger perturbation than the other datasets. Therefore, within the constraints of ensuring\nstealthiness, selecting an appropriate range of perturbation values can achieve a higher success rate for backdoor attacks."}, {"title": "5 Conclusion", "content": "In this paper, we proposed the BVG method for backdoor attacks to Vertical Federated Graph Neural Network Learning\n(VFGNN). Utilizing a multi-hop trigger generation approach, the BVG method can perform efficient backdoor attacks\nwith very limited knowledge of target class nodes. Extensive experiments using three GNN models across three datasets\nindicate that BVG achieves high attack success rates (ASR) with minimal impact on the main task accuracy (MTA).\nThe evaluation of BVG efficacy under various defense methods highlights the robustness and efficiency of the proposed\nattack method, underscoring the necessity for advanced defense mechanisms in practical federated learning applications\nto counter such sophisticated backdoor attacks."}]}