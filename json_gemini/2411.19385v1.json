{"title": "Zero-Forget Preservation of Semantic Communication Alignment in Distributed AI Networks", "authors": ["Jingzhi Hu", "Geoffrey Ye Li"], "abstract": "Future communication networks are expected to connect massive distributed artificial intelligence (AI). Exploiting aligned priori knowledge of AI pairs, it is promising to convert high-dimensional data transmission into highly-compressed semantic communications (SC). However, to accommodate the local data distribution and user preferences, AIs generally adapt to different domains, which fundamentally distorts the SC alignment. In this paper, we propose a zero-forget domain adaptation (ZFDA) framework to preserve SC alignment. To prevent the DA from changing substantial neural parameters of AI, we design sparse additive modifications (SAM) to the parameters, which can be efficiently stored and switched-off to restore the SC alignment. To optimize the SAM, we decouple it into tractable continuous variables and a binary mask, and then handle the binary mask by a score-based optimization. Experimental evaluations on a SC system for image transmissions validate that the proposed framework perfectly preserves the SC alignment with almost no loss of DA performance, even improved in some cases, at a cost of less than 1% of additional memory.", "sections": [{"title": "I. INTRODUCTION", "content": "From connecting people and things, future communication networks are envisioned to connect massive distributed artificial intelligence (AI) [1] since AIs will become pervasive in society, serving as personal assistants and business en-ablers [2]. As the massive number of AIs leads to enormous network traffic, it is imperative for AIs to communicate with optimal efficiency. Notably, with the powerful feature extraction capabilities enabled by deep learning techniques, AIs can obtain highly-compressed and task-oriented features from raw data. This makes AI communications inherently suitable for the emerging semantic communications (SC) paradigm [3]. Following the SC paradigm, the transmitter (Tx) AI encodes the data into low-dimensional semantics for the receiver (Rx) AI to decode and complete a target task, so that only the most crucial information is transmitted from Tx to Rx.\n\nThe efficacy of SC relies on the prior knowledge of AIs to capture the relationship between data and task. More importantly, SC requires alignment between the knowledge of Tx and Rx AIs [4]. As the prior knowledge are embedded in the semantic encoder and decoder, the alignment indicates that the semantic encoder and decoder should work compatibly, reaching a performance level similar to that of being jointly trained. To satisfy the requirement of SC alignment, most studies assume the Tx and Rx obtain their knowledge from a shared knowledge base [3]\u2013[5]. For AI communications, this assumption seems to be practical since AIs often inherit the"}, {"title": "II. SYSTEM MODEL", "content": "Without loss of generality, we focus on the SC between a Tx AI and an Rx AI, which are connected by a communication link, either a wireless link in cellar or Wi-Fi networks, or a wired link in backbone networks. As described in Sec. I, both AIs inherit the same pre-trained neural model for a certain task from a shared knowledge base. We assume the neural model has an general encoder-decoder architecture. By splitting the pre-trained neural model and adopting its parts as the semantic encoder and decoder, the AIs perform highly efficient SC over the link, transmitting semantics rather raw data.\n\nSpecifically, the parameters of the pre-trained encoder and decoder are denoted by \\theta^* \\in \\mathbb{R}^{N_E} and \\phi^* \\in \\mathbb{R}^{N_D}, respectively, which are trained at the knowledge base by solving\n\n$(\\theta^*, \\phi^*) = \\arg \\min_{\\theta', \\phi'} \\sum_{(X,Y) \\in D} l(Y, g_{\\phi'} \\circ f_{\\theta'}(X))$,\n(1)\n\nwhere $\\circ$ is function composition, $D$ denotes the pre-training dataset comprising data $X$ and ground-truth task label $Y$, $l(Y, \\hat{Y})$ represents the task loss given outcome of neural network $\\hat{Y}$ and truth $Y$, and $f_{\\theta} : X \\rightarrow S$ and $g_{\\phi} : S \\rightarrow Y$ are the encoder and decoder functions parameterized by $\\theta$ and $\\phi$, mapping from $X$ to features, i.e., semantics, $S$ and from $S$ to outcome $\\hat{Y}$, respectively. As $\\theta^*$ and $\\phi^*$ are obtained by the joint training in (1), they inherently satisfy the SC alignment.\n\nIn practice, each distributed AI has a local domain to adapt to, which is dependent on its local data distribution and task outcome preference. Theoretically, a domain can be modeled as a probability distribution $\\Gamma: (X, Y) \\rightarrow [0,1]$. To adapt to their domains, Tx and Rx AIs need to impose changes to $(\\theta^*, \\phi^*)$. Take Tx AI as an example: it obtains the changes as $(\\Delta \\theta, \\Delta \\phi) = \\arg \\min_{\\Delta \\theta, \\Delta \\phi} L_T(\\Delta \\theta, \\Delta \\phi)$, where\n\n$L_T(\\Delta \\theta, \\Delta \\phi) = \\mathbb{E}_{(X,Y) \\sim \\Gamma_T} l_T(Y, g_{\\phi^*+\\Delta \\phi} \\circ f_{\\theta^*+\\Delta \\theta}(X))$.\n\nHere, $\\Gamma_T$ represents the local domain of Tx AI, $l_T(\\cdot)$ denotes the task loss under Tx AI's preference, and $L_T(\\cdot)$ is referred to as the domain loss. Therefore, the adapted parameters $(\\theta_T, \\phi_T) = (\\theta^*, \\phi^*)+(\\Delta \\theta, \\Delta \\phi)$. Similarly, $\\Gamma_R$, $l_R(\\cdot)$, and $(\\Delta \\theta, \\Delta \\phi)$ are defined for the Rx AI.\n\nDue to the large size of the neural model and the limited local memory, the AIs cannot store a copy of the pre-trained parameters along with the adapted ones. the Als need to use adapted semantic encoder and decoder for the SC, which suffers from misalignment causing increase of task loss. Assume that the SC between the Tx and Rx Als"}, {"title": "III. ZFDA FRAMEWORK TO PRESERVE SC ALIGNMENT", "content": "Our goal of achieving the SC alignment for the Tx and Rx AIs, with their respective parameters been adapted to different domains, can be formulated as an optimization problem of $A$:\n\n(P0) $\\min_{A} J(A(\\theta^*_T+ \\Delta \\theta, \\phi^*_R+ \\Delta \\phi))$.\n\nHowever, it is a major challenge to solve (P0) efficiently. Firstly, since $A$ is a functional variable, it can take arbitrary form. This vast degree of optimization freedom makes it intractable to solve by conventional methods. Secondly, for $A$ to be a practical solution, it should introduce as little overhead as possible. Without consideration of efficiency, several trivial solutions of (P0) can be readily identified, including jointly re-training the encoder and decoder, or naively memorizing and restoring the original $(\\theta^*, \\phi^*)$. Unfortunately, they either incur large overhead in computation and communications, or impose a heavy burden on memory. Although equalizer-based approaches [10]\u2013[12] seems to reduce such burden, it remains a challenging problem to find the optimal transformations between semantic latent spaces of the Tx and Rx AIs.\n\nTo address (P0) efficiently, we propose a novel SC alignment framework named ZFDA, approaching the problem from a new perspective. Instead of relying on post-hoc remedies, ZFDA leads to adapted parameters that are easy to restore the original alignment without any forgetting. More specifically, it uses sparse additive modifications (SAM) to achieve the DA, which can be stored at a low cost and switched-off efficiently. As the neural parameters can be restored to its pre-trained state, the misalignment loss is reduced to zero. In this regard,"}, {"title": "IV. SAM OPTIMIZATION ALGORITHM", "content": "To efficiently address the problems (P1)T and (P1)R, we propose an SAM optimization algorithm. This algorithm con-sists of three main components, which are detailed below and summarized in Algorithm 1. We note that given the strong symmetry between (P1)T and (P1)R, we focus on handling (P1), with the same procedures applicable to (P1)R.\n\nA. Sparsity-Aware SAM Decomposition\n\nThe first difficulty in solving (P1) is to ensure the sparsity ratio of $\\Delta \\theta$ and $\\Delta \\phi$ in (3). To handle this difficulty, we decompose each of them into two components: a binary mask and a continuous vector, i.e.,\n\n$\\Delta \\theta = m_{\\theta} \\odot v_{\\theta}, \\Delta \\phi = m_{\\phi} \\odot v_{\\phi}$,\n(5)\nwhere $\\odot$ denotes element-wise product, $m_{\\theta} \\in \\mathbb{B}^{N_E}$ and $m_{\\phi} \\in \\mathbb{B}^{N_D}$ are binary mask for $\\Delta \\theta$ and $\\Delta \\phi$ respectively, and $v_{\\theta} \\in \\mathbb{R}^{N_E}$ and $v_{\\phi} \\in \\mathbb{R}^{N_D}$ represent additive modifications to $\\theta$ and $\\phi$ before applying the masks.\n\nFurthermore, to avoid directly restricting the $\\ell_0$-norm of the masks, which is difficult to enforce, we introduce an auxiliary variable named importance score, or score in short, for each parameter element as in [15]. Given sparsity ratio $\\gamma$, the score vectors, i.e., $s_{\\theta} \\in \\mathbb{R}^{N_E}$ and $s_{\\phi} \\in \\mathbb{R}^{N_D}$, determine their respective masks by setting the mask elements with the top-$\\gamma$ scores to be one and others to be zero. For example, consider a score vector $s \\in \\mathbb{R}^{N'}$ and its corresponding mask $m \\in \\mathbb{B}^{N'}$. The relationship between $s$ and the $i$-th element of $m$ can be expressed by an indicator function below:\n\n$[m]_i = [h(s; \\gamma)]_i = \\begin{cases} 1, & \\text{if } [s]_i \\text{ among top-} \\gamma \\text{ in } s, \\\\ 0, & \\text{otherwise.} \\end{cases}$\n(6)\nTherefore, with an awareness of sparsity constraint (3), $\\Delta \\theta$ and $\\Delta \\phi$ can be expressed as\n\n$\\Delta \\theta = h(s_{\\theta}; \\gamma_E) \\odot v_{\\theta}, \\Delta \\phi = h(s_{\\phi}; \\gamma_D) \\odot v_{\\phi}$,\n(7)\nwhere $\\gamma_E, \\gamma_D \\in [0, \\gamma]$ denote the sparsity ratios for the encoder and decoder, respectively, with $\\gamma_E N_E + \\gamma_D N_D = \\gamma \\cdot (N_E + N_D)$. By the decomposition in (7), we encapsulate the discrete, combinatorial nature of SAM into indication functions $h(\\cdot; \\gamma_E)$ and $h(\\cdot; \\gamma_D)$ while guaranteeing the sparsity ratio, facilitating efficient optimization of the SAM.\n\nB. Score-Aided SAM Optimization\n\nBased on (7), given $\\gamma_E$ and $\\gamma_D$, optimizing the SAM is equivalent to optimizing $s_{\\theta}$, $s_{\\phi}$, $v_{\\theta}$, and $v_{\\phi}$. Intuitively, the most prevalent and effective approaches to optimize variables in neural models are the gradient descent based optimizers, such as the widely recognized stochastic gradient descent (SGD) and Adam. In such algorithms, variables are updated iteratively. In particular, iterative update of $v_{\\theta}$ and $v_{\\phi}$ can be expressed as\n\n$v_{\\theta} \\leftarrow v_{\\theta} - \\alpha_v \\frac{\\partial L_T}{\\partial v_{\\theta}}, v_{\\phi} \\leftarrow v_{\\phi} - \\alpha_v \\frac{\\partial L_T}{\\partial v_{\\phi}}$,\n(8)\nwhere $\\alpha_v$ represents the learning rate for $v_{\\theta}$ and $v_{\\phi}$.\n\nHowever, it is intractable to evaluate the gradients of $L_T$ with respect to (w.r.t.) $s_{\\theta}$ and $s_{\\phi}$, as the gradient of $h(\\cdot; \\gamma)$ is zero almost everywhere. To tackle this difficulty, we leverage the straight-through estimation method [15], which is simple to implement and more effective than sophisticated methods as demonstrated in [16]. In this method, the indicator function $h(\\cdot; \\gamma)$ is treated as an identity function, allowing gradient calculation to directly pass through it. For example, for the $i$-th element of $s_{\\theta}$, its gradient is approximated by\n\n$\\frac{\\partial L_T}{\\partial [s_{\\theta}]_i} \\approx \\frac{\\partial L_T}{\\partial [h(s_{\\theta}; \\gamma_E)]_i} \\frac{\\partial [h(s_{\\theta}; \\gamma_E)]_i}{\\partial [s_{\\theta}]_i} = \\frac{\\partial L_T}{\\partial [m_{\\theta}]_i}$.\n(9)\nTherefore, the update for $s_{\\theta}$ and $s_{\\phi}$ can be expressed as\n\n$s_{\\theta} \\leftarrow s_{\\theta} - \\alpha_s \\frac{\\partial L_T}{\\partial m_{\\theta}}, s_{\\phi} \\leftarrow s_{\\phi} - \\alpha_s \\frac{\\partial L_T}{\\partial m_{\\phi}}$.\n(10)\nThe principle behind (9) can be explained as follows: If the loss gradient w.r.t. a mask element with a value of one is positive, its score should be updated in the negative direction to decrease the mask element to zero, and vice versa. As scores change, the mask changes, and the subset of selected parameters changes accordingly. In [15, Thm. 1], it is proven that for a multi-layer perceptron, score update by (10) will"}, {"title": "V. EXPERIMENTAL EVALUATIONS", "content": "To validate the ZFDA framework, we implement a SC system for image transmission between AIs. The neural model equipped by the AIs is a typical autoencoder for image reconstruction/denoising tasks, comprising an encoder with 6 convolutional layers and a decoder with 6 deconvolutional layers, as well as other auxiliary layers for activation and nor-malization. The encoder is capable of compressing a 32\u00d732\u00d73 image data to 512-dim semantics, achieving a bandwidth ratio of 0.167, and the decoder reconstructs the original image using the semantics. The pre-training of the autoencoder minimizes the mean squared errors (MSE) between input and output images of 80 classes of images in the CIFAR100 dataset [18], which takes 40 epochs with a learning rate of 0.01 to achieve a peak-SNR (PSNR) of 30.51 dB. Consequently, when the\nPSNR measures the ratio between the squared maximum pixel value and the MSE. It is commonly adopted to indicate image reconstruction quality."}, {"title": "VI. CONCLUSION", "content": "This paper has proposed a ZFDA framework to preserve the SC alignment for distributed AI networks. Considering that AIs inherit the same pre-trained neural models, we have analyzed the disruption of their SC alignment due to local DA. To avoid this disruption, ZFDA achieves the DA by an optimized SAM, which can be efficiently stored and switched off to restore the SC alignment. Experiment re-sults on practical image transmissions have demonstrated that ZFDA achieves comparable DA performance while allowing zero-forget preservation of the SC alignment, with a storage cost for the SAM of less than 1% of the neural model's size."}, {"title": "APPENDIX A PROOF OF PROPOSITION 1", "content": "Without loss of generality, denote the mask and the score vector before score update by $m$ and $s$, respectively, and those after the score update by $\\hat{m}$ and $\\hat{s}$. From $m$ to $\\hat{m}$, suppose there are $M$ mask element change from zero to one, with indices $i_1,..., i_M$. Based on (6), there should be $M$ mask elements change from one to zero, indexed by $j_1,..., j_M$.\n\nUsing the first-order Taylor expansion, the difference between the loss after and before the score update can be calculated by\n\n$\\mathcal{L} - \\hat{\\mathcal{L}} \\approx \\sum_{k=i_1,..., i_M} \\frac{\\partial \\mathcal{L}}{\\partial m_k} - \\sum_{k'=j_1,...,j_M} \\frac{\\partial \\mathcal{L}}{\\partial m_{k'}}$\n(12)\nDenote the index among $i_1,..., i_M$ with the maximum loss gradient by $i_{max}$, and the index among $j_1,..., j_M$ with the minimum loss gradient by $j_{min}$. Since $i_{max}$ is selected while $j_{min}$ is aborted during the score update, it can be derived that $\\hat{s}_{i_{max}} > s_{i_{max}}$ and $\\hat{s}_{j_{min}} < s_{j_{min}}$. Therefore, based on (10),\n\n$\\hat{s}_{i_{max}} > s_{i_{max}} \\wedge \\hat{s}_{j_{min}} < s_{j_{min}} \\Rightarrow \\frac{\\partial \\mathcal{L}}{\\partial m_{i_{max}}} < 0$.\n(13)"}]}