{"title": "Can OpenAI o1 outperform humans in higher-order cognitive thinking?", "authors": ["Ehsan Latif", "Yifan Zhou", "Shuchen Guo", "Yizhu Gao", "Lehong Shi", "Matthew Nyaaba", "Arne Bewerdorff", "Xiantong Yang", "Xiaoming Zhai"], "abstract": "This study evaluates the performance of OpenAI's o1-preview model in higher-order cognitive domains, including critical thinking, systematic thinking, computational thinking, data literacy, creative thinking, logical reasoning, and scientific reasoning. Using established benchmarks, we compared the o1-preview models's performance to human participants from diverse educational levels. o1-preview achieved a mean score of 24.33 on the Ennis-Weir Critical Thinking Essay Test (EWCTET), surpassing undergraduate (13.8) and postgraduate (18.39) participants (z = 1.60 and 0.90, respectively). In systematic thinking, it scored 46.1 \u00b1 4.12 on the Lake Urmia Vignette, significantly outperforming the human mean (20.08 \u00b1 8.13, z = 3.20). For data literacy, o1-preview scored 8.60 \u00b1 0.70 on Merk et al.'s \"Use Data\" dimension, compared to the human post-test mean of 4.17 \u00b1 2.02 (z = 2.19). On creative thinking tasks, the model achieved originality scores of 2.98 \u00b1 0.73, higher than the human mean of 1.74 (z = 0.71). In logical reasoning (LogiQA), it outperformed humans with 90% \u00b1 10 accuracy versus 86% \u00b1 6.5 (z = 0.62). For scientific reasoning, it achieved near-perfect performance (0.99 \u00b1 0.12) on the TOSLS,, exceeding the highest human scores of 0.85 \u00b1 0.13 (z = 1.78). While o1-preview excelled in structured tasks, it showed limitations in problem-solving and adaptive reasoning. These results demonstrate the potential of AI to complement education in structured assessments but highlight the need for ethical oversight and refinement for broader applications.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) has made significant advances in recent years, particularly with the emergence of large language models (LLMs) like OpenAI's o1-preview model [1]. These developments have created new opportunities for integrating AI into education, especially to enhance higher-order thinking skills that are essential for academic and professional success [2,3]. Higher-order thinking skills such as critical thinking, systematic thinking, metacognition, logical reasoning, and collaborative problem-solving\u2014are crucial for navigating the complexities of modern education and for developing cognitive abilities required in the 21st-century workforce [4,5].\nResearch has underscored AI's potential to improve problem-solving and cognitive skills, including critical thinking and data literacy, with significant implications for education and research [6,7]. Reviews and bibliometric analyses highlight a decade of progress in AI applications for education [3]. However, critical questions remain about the extent to which AI models can replicate or surpass human performance in higher-order thinking tasks, particularly at the graduate education level [2,8].\nEvaluations of the OpenAI o1-preview model reveal promise in addressing cognitive challenges, such as complex problem-solving and logical reasoning [9, 10]. Despite this, concerns persist about the model's ability to consistently perform tasks requiring deeper cognitive processes, such as metacognition [11] and scientific reasoning [2]. Mixed results from studies on analogical reasoning in AI indicate some successes but also highlight challenges in achieving the nuanced thinking characteristic of human cognition [12, 13].\nThis paper addresses these gaps by comprehensively evaluating the o1-preview model across several key higher-order thinking domains, specifically critical thinking, systematic thinking, computational thinking, data literacy, creative thinking, logical reasoning, and scientific reasoning. These domains are identified as critical for graduate education and beyond [2-5]. Our findings reveal that the o1-preview model outperforms human experts in five out of seven domains, including systematic thinking, computational thinking, data literacy, creative thinking, and scientific reasoning.\nWhile the o1-preview model demonstrates effectiveness in computational tasks [2,3],\nfurther research is needed to evaluate its capabilities in collaborative thinking and human-AI interactions in dynamic educational contexts [14,15]. Recent studies suggest that incorporating reflective and deconstructive strategies into AI systems can enhance their performance on complex, multi-step problems [11,16]. Additionally, the ethical implications of using AI in education, particularly in decision-making and reasoning tasks, demand careful consideration [17].\nIn the following sections, we systematically evaluate the o1-preview model's performance across these higher-order thinking domains, benchmarking its results against human participants. Our analysis provides insights into the potential and limitations of AI in graduate education, identifying areas where AI can contribute to the future of learning and teaching."}, {"title": "2 Methods", "content": "This study utilizes OpenAI's o1-preview model to evaluate its capacity for performing higher-order thinking tasks. Human participant data are drawn from existing studies representing various educational levels, including undergraduate, graduate, and post-graduate students. Graduate-level human performance serves as the benchmark where applicable. The performance of the o1-preview model is compared to human participants across multiple higher-order thinking domains."}, {"title": "2.2 Instruments and Datasets", "content": "This study employs well-established instruments and datasets tailored to each higher-order thinking category. For each domain, the performance of the o1-preview model is evaluated against the corresponding human cohort."}, {"title": "2.2.1 Critical Thinking Instruments", "content": "The Ennis-Weir Critical Thinking Essay Test (EWCTET) [18] is a widely recognized tool designed to assess critical thinking skills through performance-based, context-driven written responses. Unlike standardized assessments, the EWCTET emphasizes real-world application, making it particularly effective for evaluating higher-order cognitive abilities. Studies such as those by Taghinezhad et al. [19] highlight its capacity to assess cognitive complexity effectively. Developed by Robert Ennis and Eric Weir, the EWCTET challenges participants to construct, analyze, and evaluate arguments through coherent essays [20].\nThe EWCTET uses scenario-based prompts, requiring participants to critically assess specific situations or arguments and present well-reasoned evaluations in written form [21]. It measures a range of critical thinking skills, including logical consistency, identification of assumptions, argument analysis, and evidence-based reasoning [22]. The tool's accessibility and cost-effectiveness further contribute to its frequent use in educational research and practice.\nThe assessment typically takes 40 to 60 minutes, allowing participants sufficient time to construct thoughtful and nuanced arguments [18]. The scoring rubric evaluates multiple dimensions of critical thinking and written communication, including clarity, logical structure, and engagement with opposing viewpoints [22].\nSpecific scoring criteria include:\nRecognition of misuse of analogy, irrelevance, and circular reasoning: Assesses the ability to identify logical fallacies."}, {"title": "2.2.2 Systematic Thinking Instruments", "content": "Numerous instruments have been developed to assess systematic thinking (ST), leveraging various theoretical frameworks across disciplines, using diverse methods, and targeting different educational levels. Common formats include mapping, interviews, scenario-based items, open-ended items, fill-in-the-blank items, and multiple-choice questions [26].\nIn this study, we selected two ST instruments based on the following criteria:\n1. To account for the limitations of the o1-preview model in identifying and generating images, we excluded instruments requiring respondents to create mappings or interpret indispensable visual information in prompts.\n2. Research supports the use of scenario-based, ill-structured items as the most effective method for measuring ST. These items present realistic problems followed by a series of open- or close-ended questions to elicit ST skills [27].\n3. Since ST is extensively studied in biology and engineering [26, 28], we focused on theoretical frameworks and instruments developed in these fields, as they are more advanced and comprehensive.\nBased on these considerations, we selected two recently developed scenario-based, ill-structured instruments designed for higher education: the \"Village of Abeesee\" instrument and the \"Lake Urmia Vignette\" (LUV) instrument. These instruments are grounded in distinct theoretical frameworks and contextualized in engineering and socio-environmental systems.\nVillage of Abeesee Instrument The Village of Abeesee instrument, developed by Grohs et al. (2018) [29], is based on a framework that views ST as comprising three dimensions for general interdisciplinary use. This framework emphasizes the interconnectedness of technical and social aspects of modern problems and highlights the importance of stakeholder perspectives.\nThe instrument presents a scenario about the Village of Abeesee, which faces a complex issue regarding winter heating. Participants respond to six open-ended questions aligned with the framework's dimensions. A pilot study was conducted to refine the scenario and gather qualitative data. Rubrics were developed through a multi-stage process using a pool of 93 student responses. The scoring dimensions include: Problem identification, Information needs, Stakeholder awareness, Goals, Unintended consequences, Implementation challenges, and Alignment. Each dimension is scored on a scale of 0 to 3, with a maximum possible score of 21.\nLake Urmia Vignette (LUV) Instrument The Lake Urmia Vignette (LUV) instrument, developed by Davis et al. (2020) [30], is based on a theoretical framework that conceptualizes systems as webs of interconnected variables. Participants are presented with a scenario describing the real-world case of Lake Urmia's desiccation. They are asked to describe the problem of Lake Urmia and explain why the lake shrank over the years.\nThe rubric evaluates participants' responses by analyzing the number of variables, causal links, and feedback loops identified. During development, a pilot study and interviews were conducted with 30 graduate students to refine the instrument. This rigorous process ensures that the LUV instrument effectively captures participants' ability to think systematically about socio-environmental issues."}, {"title": "2.2.3 Computational Thinking Instruments", "content": "Korkmaz et al. [31] developed the Computational Thinking (CT) Skills instrument to assess college students' computational thinking (CT) abilities across five dimensions: Creativity, Algorithmic thinking, Cooperation, Critical thinking, and Problem-solving. The CT instrument is a 29-item, five-point Likert scale ranging from 1 (strongly disagree) to 5 (strongly agree). It demonstrates high reliability, with a Cronbach's alpha coefficient of 0.822 for the entire scale and 0.843, 0.869, 0.865, 0.784, and 0.727 for the five dimensions, respectively. This instrument has been widely adopted in research and has consistently yielded reliable human performance across various contexts.\nThe Algorithmic Thinking Test for Adults (ATTA), developed by Lafuente Mart\u00ednez et al. [32], is another validated tool designed to evaluate adults' CT skills. It focuses on key CT components, including: Problem decomposition, Algorithmic thinking, Abstraction, Pattern recognition, and Debugging/evaluation. The ATTA consists of 20 items, including nine open-ended and 11 multiple-choice questions, offering a comprehensive assessment of computational thinking in adults."}, {"title": "2.2.4 Data Literacy Instruments", "content": "Data literacy has been explored across diverse fields, focusing on how individuals collect, analyze, and interpret data to make informed decisions in various settings [33,34]. It involves not only understanding complex information but also effectively addressing real-world challenges. Existing data literacy assessments can be categorized into two approaches: self-reflective and objective measures [35].\nSelf-reflective approaches measure individuals' self-reported data literacy competencies through surveys, questionnaires, semi-structured interviews, and think-aloud interviews [35]. Participants reflect on their data-related behaviors, practices, and attitudes. For example, a self-efficacy item might ask participants to rate their confidence in using data to identify students with special learning needs [36].\nObjective measures assess data literacy using test questions in three formats: conventional tests (multiple-choice and constructed-response questions), digital game-based assessments, and participation observations [35].\nTo compare the data literacy of OpenAI o1-preview with humans, specific criteria were applied to select appropriate assessment instruments:\nAssessment Format. Standardized assessments were prioritized to ensure consistency in data literacy measurement.\nValidation. Only empirically validated assessments with established reliability and validity were included.\nItem Type. To account for OpenAI o1-preview's limitations with interactive and video-based content, selected assessments were restricted to conventional formats such as multiple-choice and constructed-response questions.\nAudience. The assessments were targeted at adults, specifically post-secondary students, to align with the study's focus population.\nSelected Instruments Based on these criteria, two data literacy instruments were chosen for comparison:\nMerk et al.'s Data Literacy Test Merk et al. [37] developed a test to assess pre-service teachers' data literacy, focusing on two major components: 1) Using Data (11 items): Evaluates understanding of data properties, manipulation, aggregation, and knowledge of statistics and psychometrics. 2) Transforming Data into Information (11 items): Assesses the interpretation of data, use of data displays and visual representations, application of statistical methods, and summarization of data.\nFour items measure both components. The test was validated through exploratory and confirmatory factor analyses, reliability analysis (demonstrating high reliability with Cronbach's a), and concurrent criterion validity (correlation with state achievement test scores).\nChen et al.'s Data Literacy Assessment Chen et al. [38] designed an assessment emphasizing the importance of data literacy for 21st-century citizens. It comprises three dimensions: Data Management: Covers data organization (1 item) and data manipulation (2 items). Data Visualization: Includes frequency distributions (2 items) and the use of visual charts (4 items). Basic Data Analysis: Focuses on central tendency (5 items), variability (3 items), and percentage calculations (1 item).\nPsychometric validation included item-total correlation and item response theory analyses. Eye-tracking studies further confirmed item validity by identifying differences in cognitive effort between successful and unsuccessful students. This study uses the data literacy assessments developed by Merk et al. [37] and Chen et al. [38] to evaluate the performance of OpenAI o1-preview. Human performance data reported in these studies serve as benchmarks for comparison, providing a basis for assessing the model's data literacy competencies."}, {"title": "2.2.5 Creative Thinking Instruments", "content": "Creative thinking is commonly defined in two dimensions: divergent thinking and convergent thinking [39].\nDivergent Thinking Divergent thinking involves solving problems or making decisions by employing strategies that deviate from commonly used or previously taught methods [40]. One of the most widely used tests for divergent thinking is the Alternate Uses Task (AUT) [41], where participants generate original uses for common objects. Responses to the AUT are traditionally evaluated along four dimensions: Fluency: The number of ideas generated. Flexibility: The diversity of idea categories. Originality: The uniqueness of the ideas. Elaboration: The level of detail in the ideas.\nAmong these dimensions, originality is considered the most critical indicator of divergent thinking [42, 43]. For example, the originality of AUT responses can be evaluated using automated AI scoring tools, such as the one developed by Organisciak et al. [42], which has demonstrated high reliability and validity. This tool addresses the inefficiencies and subjectivity associated with traditional consensual assessment methods [42].\nThe AUT, originally developed by Guilford [41], has been extensively used among undergraduate, graduate, and post-graduate student groups, demonstrating good reliability and validity [44-46]. These qualities make it a suitable measure of divergent thinking for this study. The AUT used here includes three items: participants are asked to generate unconventional uses for a paperclip, a brick, and a can.\nConvergent Thinking Convergent thinking refers to the ability to use given clues to arrive at a single correct solution. A classic test for this dimension is the Remote Association Test (RAT), which asks participants to find a common link between three seemingly unrelated words [47]. For instance, the goal for the words \"SA\u039c\u0395,\u201d \"TENNIS,\" and \"HEAD\" is to identify a linking word, such as \"MATCH,\" which forms compound words or semantic relationships (e.g., \"MATCH HEAD,\" \"TENNIS MATCH\"). The number of correct answers reflects the participant's convergent thinking ability.\nOriginally developed by Mednick [47], the RAT has since been translated into various languages (e.g., Chinese, Spanish) and validated across different populations, demonstrating good reliability and validity among college students [48,49]. For this study, the Chinese version of the RAT was selected to align with the language background of the participants. This version consists of 10 items, with a maximum score of 10. Higher scores indicate greater convergent thinking ability.\nThe AUT and RAT were chosen for their established validity and widespread recognition as classic tasks for measuring divergent and convergent thinking, respectively. These instruments provide complementary insights into the creative thinking process, making them well-suited for evaluating the creative thinking capabilities of OpenAI o1-preview and human participants."}, {"title": "2.2.6 Logical Reasoning Instruments", "content": "To evaluate the logical reasoning capabilities of the o1-preview model, we utilized the LogiQA dataset [50]. The LogiQA dataset comprises logical comprehension questions from the National Civil Servants Examination of China, designed to assess candidates' logical thinking and problem-solving abilities. It contains 867 paragraph-question pairs categorized into five types of deductive reasoning, as defined by Hurley [51]:\nCategorical Reasoning: Determines whether a concept belongs to a specific category, often involving quantifiers such as \"all,\" \"everyone,\" \"any,\" \"no,\" and \"some\" [52].\nSufficient Conditional Reasoning: Based on conditional statements of the form \"If P, then Q,\" where P serves as the premise and Q as the outcome [51].\nNecessary Conditional Reasoning: Involves statements such as \"P only if Q\" or \"Q whenever P,\" indicating that Q is a necessary condition for P [51].\nDisjunctive Reasoning: Uses premises in an \"either...or...\" format, where the conclusion holds if at least one premise is true [51].\nConjunctive Reasoning: Features premises connected by \"both...and...\" statements, where the conclusion is valid only if all premises are true [51].\nThe dataset is divided into training (80%), development (10%), and testing (10%) sets. Among machine learning models, RoBERTa [53] achieved the highest performance, with an accuracy of 35.31%, significantly below the human ceiling of 95.00%.\nThe LogiQA dataset serves as a robust benchmark for logical reasoning, enabling direct comparison of the o1-preview model's performance with both human participants and existing machine learning models."}, {"title": "2.2.7 Scientific Reasoning Instruments", "content": "Numerous instruments are available for assessing scientific reasoning. Opitz et al. [54] conducted a comprehensive review identifying 38 scientific reasoning tests, 14 of which were specifically designed for the university level. In this study, we focus on multiple-choice (MC) test instruments due to their advantages in standardized testing, which eliminates the need for an objective rater. Although MC formats are often critiqued for providing limited qualitative insights, they enable consistent assessment across a broad population.\nThe review by Opitz et al. identifies eight dimensions of scientific literacy: problem identification (PI), questioning (Q), hypothesis generation (HG), evidence generation (EG), evidence evaluation (EE), drawing conclusions (DC), communicating and scrutinizing (CS), and other skills (OT). Among the five MC instruments designed for university-level use, the Test of Scientific Literacy Skills (TOSLS) stands out for its ability to assess five of these dimensions (EG, EE, DC, CS, and OT). In contrast, other instruments cover only three dimensions each. Additionally, TOSLS is domain-specific, with a primary focus on biology, making it particularly relevant for evaluating scientific reasoning in specific contexts.\nTest of Scientific Literacy Skills (TOSLS) The 28-item TOSLS assesses nine skills related to scientific literacy:\n1. Identify valid scientific arguments.\n2. Evaluate the validity of sources.\n3. Evaluate the use and misuse of scientific information.\n4. Understand elements of research design and their impact on findings and conclusions.\n5. Create graphical representations of data.\n6. Read and interpret graphical representations of data.\n7. Solve problems using quantitative skills, including probability and statistics.\n8. Understand and interpret basic statistics.\n9. Justify inferences, predictions, and conclusions based on quantitative data.\nGormally et al. [55] designed TOSLS with a focused interpretation of scientific literacy that aligns closely with the concept of scientific reasoning [54,56]. The test's internal reliability, measured using the Kuder-Richardson Formula 20 (KR-20) [57], was reported as 0.73, meeting the acceptable threshold of 0.7 [58]. Principal component analysis revealed a single-factor structure, supporting the test's internal consistency.\nSince its development, TOSLS has been widely used to assess scientific reasoning skills in university students across various studies [59-61]. The test's focus on standardized measurement, broad coverage of scientific literacy, and domain-specific relevance makes it well-suited for evaluating OpenAI's o1-preview model's capacity for scientific reasoning."}, {"title": "2.3 Procedure", "content": "For each domain, tasks were presented to the o1-preview model as text-input prompts. The model's responses were evaluated using the same criteria applied to human participants, following the scoring guidelines of the respective instruments. Human participant data were drawn from existing studies to ensure consistency. Tasks were carefully matched in content and difficulty across human and AI cohorts to maintain comparability.\nStatistical analyses were conducted to evaluate significant differences in performance between the o1-preview model and human participants, with a particular focus on graduate-level performance as a benchmark."}, {"title": "2.4 Data Analysis", "content": "The study compared the performance of OpenAI o1-preview with that of human participants across seven higher-order thinking assessments. Percentage accuracy in answering the tasks was calculated, and mean performance scores were computed for both human participants and the o1-preview model, based on ten trials for each domain. To ensure comparability across different dimensions, scores were standardized within each assessment.\nStandard deviations were reported to evaluate how the o1-preview model's performance deviated from the human mean. A one-sample t-test was used to determine the statistical significance of differences between the o1-preview model and human performance. Results were supplemented with confidence intervals and effect sizes to provide a comprehensive understanding of the observed differences. This approach ensured a rigorous comparison of AI and human capabilities across all assessed domains."}, {"title": "3 Results", "content": null}, {"title": "3.1 Critical Thinking", "content": "The critical thinking abilities of OpenAI's o1-preview were evaluated using the Ennis-Weir Critical Thinking Essay Test (EWCTET), a widely recognized tool for assessing critical thinking across diverse educational contexts. To establish robust benchmarks for comparison, we referenced human performance data from three foundational studies. These studies provide insights into critical thinking outcomes across varying instructional methods and educational levels, serving as a basis for evaluating o1-preview's performance.\nHuman Benchmarks Hatcher [23]: Conducted over four years at Baker University, this study involved American freshmen who completed a compulsory, year-long critical thinking course. Post-test scores ranged between 11.8 and 13.8, marking gains of 2.8 to 6.0 points from pre-test scores of 5.8 to 9.4. These results illustrate the impact of structured, long-term instruction in improving critical thinking skills, providing a benchmark for sustained human cognitive development.\nDavidson and Dunham [24]: This study was conducted at a Japanese private women's junior college and included 36 first-year EFL (English as a Foreign Language) students. Participants were divided into a treatment group (n=17), who received critical thinking instruction, and a control group (n=19), who received only intensive English instruction. After one year, the treatment group achieved a mean score of 6.6 on the EWCTET, significantly higher than the control group's mean score of 0.6 (p<.001). This highlights the effectiveness of integrating critical thinking instruction, even within a language-learning context.\nHollis et al. [25]: This study involved 100 online participants recruited via social media and online study platforms, with no specific critical thinking intervention provided. Post-test scores on the EWCTET averaged 14.31 (SD = 8.45). Educational background significantly influenced performance (p < .001), with postgraduates scoring an average of 18.39 (SD = 6.59), compared to undergraduates with a mean score of 11.51 (SD = 8.23). This study underscores the positive correlation between educational attainment and critical thinking skills.\no1-Preview Model Performance The o1-preview model was tested using iterative prompt strategies. A zero-shot prompt was employed first, followed by role-based prompts instructing the model to respond as a college student and then as a postgraduate student. These strategies mirrored the human participant studies, where responses were scored using the EWCTET scoring rubric.\nComparison with Human Benchmarks We focused on the highest mean scores reported for human participants after learning interventions. Undergraduate students achieved a maximum mean score of 13.8, while postgraduate students scored an average of 18.39. In comparison, o1-preview achieved a mean score of 24.33 across the three prompt strategies. The corresponding z-scores indicate o1-preview outperformed human participants.\nThese results demonstrate that OpenAI's o1-preview exceeds human benchmarks in structured critical thinking tasks as measured by the EWCTET. However, the findings raise important questions about the comprehensiveness of such instruments in capturing the full spectrum of human critical thinking skills. While o1-preview excels in structured tasks, human oversight is essential when using the model in educational contexts. The findings highlight o1-preview's potential as a supplementary tool for critical thinking instruction and assessment but underscore the need for careful integration to address its limitations and ensure alignment with broader educational goals."}, {"title": "3.2 Systematic Thinking", "content": "The systematic thinking (ST) abilities of o1-preview were evaluated using two instruments: the Village of Abeesee and the Lake Urmia Vignette (LUV). Davis et al. [62] reported the average performance of 263 undergraduate students on the Village of Abeesee instrument and 155 undergraduates on the LUV instrument. The mean scores and standard deviations for each dimension are presented. To compare o1-preview's performance with human participants, the tests were administered to o1-preview in 10 trials, with the mean score, standard deviation, and z-score for each dimension calculated.\nThe results indicate that o1-preview outperformed the average human scores in all seven dimensions of the Village of Abeesee instrument, suggesting that the model performed better on average than undergraduate students. For the LUV instrument, o1-preview also achieved significantly higher mean scores across all three dimensions compared to the human participants. This demonstrates that o1-preview generally excels in systematic thinking when compared to undergraduate students.\nAccording to the z-scores, o1-preview performed exceptionally well in the \"Feedback Loops\" dimension of the LUV instrument, achieving the highest z-score (6.53). This indicates that o1-preview is particularly adept at identifying feedback loops, which involve interconnected causal relationships within complex systems.\nThe findings highlight o1-preview's exceptional capabilities in systematic thinking, particularly in identifying feedback loops, a critical aspect of complex systems thinking. While these results are promising, they also underscore the need for further research to explore whether the performance gap reflects the model's inherent strengths or limitations in the instruments used. Additionally, although o1-preview's performance surpassed that of undergraduates, systematic thinking in real-world applications often requires collaborative and contextualized reasoning, which the current assessment methods may not fully capture.\nThe results suggest that OpenAI's o1-preview has the potential to serve as a valuable tool for enhancing systematic thinking skills, particularly in educational settings. However, its application should be accompanied by human oversight to ensure that its outputs align with the nuanced requirements of real-world problem-solving."}, {"title": "3.3 Computational Thinking", "content": "Human and o1-Preview Performance on Computational Thinking Skills.\nTab. 5 compares the performance of human participants and o1-preview on the Computational Thinking (CT) Skills instrument across overall CT skills and specific dimensions: creativity, algorithmic thinking, cooperativity, critical thinking, and problem-solving. Human performance data were synthesized from three studies:\n1. Liu et al. [63], which involved 341 college students.\n2. \u015eahin et al. [64], which included 25 gifted science teachers.\n3. Liao et al. [65], which examined 44 sophomore undergraduate students.\nThe overall human performance mean was 3.92 (SD = 0.52), slightly higher than o1-preview's mean score of 3.84 (SD = 1.56), yielding a z-score of -0.15. However, o1-preview outperformed human participants in four dimensions: creativity, algorithmic thinking, cooperativity, and critical thinking. Notably, o1-preview's performance in critical thinking was exceptional (M = 4.8, SD = 0.42), achieving a z-score of 1.38. Conversely, o1-preview scored poorly on problem-solving (M = 1.0, SD = 0.0), significantly lower than the human mean (M = 3.68, SD = 0.63), with a z-score of -4.25.\nHuman and o1-Preview Performance on Algorithmic Thinking Test for Adults The Algorithmic Thinking Test for Adults (ATTA) [32] was used to further evaluate o1-preview's algorithmic thinking skills. The model achieved a perfect score (M = 20, SD = 0) across all 20 items (11 multiple-choice and 9 open-ended) in five rounds of testing. Human performance on the ATTA varied significantly: experts achieved a mean score of 14.63 (SD = 3.81), while novices scored an average of 9.11 (SD = 3.81). Performance differences were also observed across academic disciplines, with social sciences scoring the lowest (M = 9.11, SD = 4.54) and mathematics scoring the highest (M = 15.70, SD = 3.71). In all cases, o1-preview's performance exceeded that of human participants, achieving high z-scores .\no1-preview demonstrated strong capabilities in creativity, algorithmic thinking, cooperativity, and critical thinking dimensions of CT. However, its exceptionally low performance in problem-solving highlights a potential limitation in adapting its capabilities to real-world, ill-structured problems. On the ATTA, o1-preview's perfect scores across all items suggest exceptional algorithmic reasoning abilities, surpassing all human groups tested. These results underline o1-preview's potential as a tool for supporting computational thinking but also emphasize the need for human oversight, particularly for problem-solving tasks."}, {"title": "3.4 Data Literacy", "content": "The data literacy capabilities of o1-preview were evaluated using two validated instruments: Merk et al.'s [37] Data Literacy Assessment and Chen et al.'s [38] Data Literacy Assessment. These instruments assess distinct dimensions of data literacy, allowing for a comprehensive comparison between o1-preview and human participants.\nMerk et al.'s Data Literacy Assessment Merk et al. [37] reported the performance of 89 pre-service secondary teachers from a large university in southern Germany. The assessment measured performance across two dimensions: \"Use Data\" and \"Transform Data into Information.\" Pre- and post-test scores were reported to evaluate the impact of an instructional intervention on participants' data literacy skills. The mean scores and standard deviations for human participants, along with o1-preview's mean scores, standard deviations, and z-scores, are presented. o1-preview outperformed human participants in both dimensions of the assessment, achieving substantially higher scores. For the \"Use Data\" dimension, o1-preview attained a mean score of 8.60 (SD = 0.70), compared to human pre-test and post-test scores of 3.28 (SD = 1.84) and 4.17 (SD = 2.02), respectively, with a z-score of 2.89 for the pre-test and 2.19 for the post-test. Similarly, for the \"Transform Data into Information\" dimension, o1-preview scored a mean of 4.80 (SD = 0.42), surpassing human pre-test and post-test scores of 2.96 (SD = 1.21) and 4.04 (SD = 1.34), with a z-score of 1.52 for the pre-test and 0.57 for the post-test.\nChen et al.'s Data Literacy Assessment Chen et al. [38] reported the performance of 170 post-secondary students (average age = 22.81, SD = 4.25) from the Faculty of Education at a western Canadian university. This assessment evaluates three dimensions of data literacy: \"Data Management,\" \"Data Visualization,\" and \"Basic Data Analysis.\" The mean scores, standard deviations, and z-scores for human participants and o1-preview are provided.\no1-preview scored higher than human participants across all three dimensions. For \"Data Management,\" o1-preview achieved a mean score of 2.00 (SD = 0.30), compared to the human mean of 0.17 (SD = 0.44), resulting in a z-score of 4.16. For \"Data Visualization,\" o1-preview achieved a perfect score of 6.00 (SD = 0.00), exceeding the human mean of 3.56 (SD = 1.46) with a z-score of 1.67. Finally, for \"Basic Data Analysis,\" o1-preview scored 9.00 (SD = 0.00), surpassing the human mean of 5.38 (SD = 2.22) with a z-score of 1.63.\nIn both assessments, o1-preview consistently outperformed human participants across all dimensions, indicating strong capabilities in understanding, interpreting, and analyzing data. These findings suggest that o1-preview could serve as a valuable tool to support or enhance data literacy education, particularly in developing critical data interpretation and analytical skills. However, while the results are promising, future research should explore o1-preview's performance in more complex, real-world data scenarios to ensure its applicability beyond structured testing environments."}, {"title": "3.5 Creative Thinking", "content": "Human creative thinking has been assessed in previous studies using both divergent and convergent thinking tasks [66, 67", "66": "assessed the divergent thinking abilities of 68 university students (N = 52, males = 22; primarily from social sciences and humanities) using the AUT. Participants provided creative uses for common objects, and originality was scored on a 5-point scale by trained experts. The average originality score was 1.74.\nFor convergent thinking, Xia et al. [67"}]}