{"title": "SMOOTHIE: Label Free Language Model Routing", "authors": ["Neel Guha", "Mayee F. Chen", "Trevor Chow", "Ishan S. Khare", "Christopher R\u00e9"], "abstract": "Large language models (LLMs) are increasingly used in applications where LLM inputs may span many different tasks. Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples. Prior approaches have thus explored how engineers might select an LLM to use for each sample (i.e. routing). While existing routing methods mostly require training auxiliary models on human-annotated data, our work explores whether it is possible to perform unsupervised routing. We propose SMOOTHIE, a weak supervision-inspired routing approach that requires no labeled data. Given a set of outputs from different LLMS, SMOOTHIE constructs a latent variable graphical model over embedding representations of observable LLM outputs and unknown \"true\" outputs. Using this graphical model, we estimate sample-dependent quality scores for each LLM, and route each sample to the LLM with the highest corresponding score. We find that SMOOTHIE'S LLM quality-scores correlate with ground-truth model quality (correctly identifying the optimal model on 9/14 tasks), and that SMOOTHIE outperforms baselines for routing by up to 10 points accuracy.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are increasingly being deployed in multi-capability regimes where data inputs may span a diverse range of tasks, each of which requires different capabilities [8]. For instance, an LLM-powered chatbot may be asked to write code, answer questions about different domains, summarize documents, perform extraction, and more [3, 8, 14, 30]. One challenge is that while engineers often have access to numerous pre-trained LLMs (i.e., through Huggingface or various APIs), they do not know which LLM is optimal for each possible user input [86]. Because the quality of generations can vary significantly between LLMs, choosing the right LLM for each input sample is important to ensure high task performance [41].\nRecent work has explored various ways to utilize ensembles of pretrained LLMs in multi-capability settings, by (1) collecting a diverse pool of LLMs and (2) identifying which LLM to route each sample to [55, 86]. However, most existing approaches require labeled data; Engineers typically either (1) train an auxiliary model using labeled data to rank or predict the LLM to which each sample should be routed [41, 79], or (2) directly use labeled data to determine which LLM is the best on average [86]. As a result, engineers designing routing protocols face the practical difficulty of constructing labeled datasets.\nGiven a candidate pool of LLMs and an unlabeled test dataset, this paper explores how to best select LLM outputs for each sample in an entirely unsupervised manner-without labeled data, or models trained on labeled data. To make progress in addressing this question, we face two technical challenges:\n\u2022 Unknown LLM quality: The first challenge is estimating the quality of each LLM. Access to labeled data allows engineers to identify higher performing LLMs by measuring the accuracy/quality of LLM outputs. In this paper, we study the question of how to estimate quality without labeled validation data.\n\u2022 Sample-conditional generator performance: The second challenge is determining how to select the best LLM for each individual test sample. LLM outputs can vary in quality over different samples, which could render population-level estimates of LLM quality misleading.\nIn this work, we propose SMOOTHIE, a method for routing samples to LLMs in a label-free manner (Figure 1). Below, we describe how SMOOTHIE addresses the two challenges described above.\n\u2022 Quality estimation: Using the LLM outputs for each test sample as \"voters,\u201d SMOOTHIE estimates the quality of each generator using methods from Weak Supervision (WS). Concretely, SMOOTHIE constructs a latent variable graphical model over observable LLM outputs and an unknown true output. By modeling the embedding vector difference"}, {"title": "Related Work", "content": "We provide an abbreviated related work, with a full treatment in Appendix C.\nRouting Routing has been classically utilized in Mixture-of-Experts models [25, 37, 42, 82], which involve jointly training a set of models as well as a router. Recently, routing mechanisms have been used at inference time to decide which pre-trained LLM to use for a given sample [79]. Some approaches involve training an auxiliary model using labeled training data to either score or rank the performance of each LLM on each test sample [38, 74]. Others do not involve training a model but instead use nearest neighbor methods, selecting the LLM that does the best on a test sample's labeled neighbors [48, 86]. In contrast, SMOOTHIE does not require any labels.\nEnsembling Ensembling is another way of utilizing a pool of LLMs. Existing work has primarily focused on ensembling outputs for classification tasks [2, 68, 98]. Ensembling generative outputs typically requires training an auxiliary model [41], combining or switching among outputs when decoding [36, 83], or averaging in weight space [95].\nPrompt selection In addition to selecting the best LLM for a sample, prior works have studied how to select the best prompt or in-context examples. While the simplest approach is to use a held-out labeled dataset [67], there are also retrieval-based approaches to selecting the best in-context examples [90], as well as approaches based on mutual information [89] and probability-based measures [103], although the latter two are limited to classification.\nWeak supervision SMOOTHIE utilizes statistical techniques inspired by weak supervision, which programmatically generate labels for an unlabeled dataset by aggregating the predictions of several weak \u201cvoters\u201d via a latent variable graphical model [71, 73]. Weak supervision has mostly been studied in classification settings [26, 72] but more recently has been extended to tasks such as learning rankings and manifolds [85, 94]. We derive our estimation procedure from the Gaussian model in [85], applying it to LLM embeddings and the routing setting."}, {"title": "Preliminaries", "content": "Let V be the token vocabulary space, and let $V^* = V \\times \\dots \\times V$ be the space of all vocabulary sequences. We consider a generative task with input text $x \\in X \\subset V^*$ and reference output text $y \\in Y \\subset V^*$. We have a candidate pool of m LLMs, $G = \\{g_1,..., g_m\\}$, where each $g_i \\in G : X \\rightarrow Y$ produces a generative output sequence $g_i(x)$ for a given input text sequence x. We are given an unlabeled test dataset $D_{test} = \\{x\\}_{i=1}^n$, where the ground-truth reference outputs are unknown.\nOur goal is to route each sample $x \\in D_{test}$ to one of the LLMs in G. Specifically, we wish to construct a router $route: G^m \\times X \\rightarrow G$ that selects the LLM that yields the highest quality generation on x for each test sample x, without any labeled data."}, {"title": "Graphical model", "content": "We present a probabilistic graphical model (see Figure 1 (center)) that describes how the LLM outputs, $g_1(x), ..., g_m(x)$, are related to a true output y in terms of each LLM's quality on a given input x, which we call $\\theta_i(x)$, corresponding to each $g_i(x)$. Let $z_{g_0}: V^* \\rightarrow \\mathbb{R}^d$ map from a sequence of tokens to a d-dimensional embedding using a common model go such as SentenceBERT [76]. Define $\\lambda_i(x) := z_{g_0}([x, g_i(x)])$ to be the observable embedding of x and the LLM output, and define $z^*(x) := z_{g_0}([x, y])$ to be the latent ground-truth embedding of x and reference output y. Similar to the approach in [85], we model the distribution over embedding vectors, $Pr(z^*(x), \\lambda_1(x), ..., \\lambda_m(x)|x)$ as\n$Pr(z^*(x), \\lambda_1(x),..., \\lambda_m(x)|x) = \\frac{1}{Z}exp(\\sum_{i=1}^m - \\frac{\\theta_i(x)}{2d}||\\lambda_i(x) - z^*(x)||^2)$                                                                         (1)\nwhere Z is the log partition function and the $\\theta_i(x)$s\u2014the LLM quality scores\u2014are canonical parameters of the graphical model. Intuitively, our model captures LLM quality by supposing that if $g_i$ is of high quality and $\\theta_i(x)$ is very large, then it should be unlikely for the LLM output to be very different from the true output in terms of Euclidean distance in embedding space. Conversely, if $\\theta_i(x)$ is small, we assign larger probability to the setting where $\\lambda_i(x)$ and $z^*(x)$ differ significantly. Finally, note that this graphical model corresponds to a multivariate Gaussian. That is, the vector $[\\lambda_1(x) - z^*(x), ..., \\lambda_m(x) - z^*(x)] \\in \\mathbb{R}^{dm}$ is Gaussian with mean $\\mu = 0$ and a diagonal covariance matrix $\\Sigma \\in \\mathbb{R}^{dm \\times dm}$ with $\\Sigma_{jj} = \\frac{d}{2 \\theta_i(x)} [\\frac{j}{m}]$. Intuitively, this means that the average difference vector between each $\\lambda_i$ and $z^*(x)$ is centered, with its magnitude inversely proportional to the LLM score $\\theta_i(x)$ and independent of other LLMs. Given this probabilistic graphical model, our goal is to learn each quality score $\\theta_i(x)$ from the unlabeled test dataset and use these for improved routing."}, {"title": "Method", "content": "Given an unlabeled test dataset $D_{test}$ and a pool of LLMs G, SMOOTHIE consists of two steps:\n1. Estimation: The LLM quality scores $\\theta_1(x), ..., \\theta_m(x)$ are learned for each $x \\in D_{test}$ (Section 4.1, Algorithm 1).\n2. Routing: The LLM with the highest scores is selected, and its output is used as our final prediction for x (Section 4.2)."}, {"title": "LLM score estimation", "content": "We describe how to estimate each $\\theta_i(x)$s in the graphical model in (1) using only unlabeled data from $D_{test}$. Then, we describe how the LLM score estimate can be instantiated to be sample-conditional.\nComputing $\\theta_i (x)$ Below, we state a simple property arising from the fact that (1) corresponds to a multivariate Gaussian with a diagonal covariance matrix.\nProposition 1. [85] For any i, j \u2208 [m], it follows from the graphical model in (1) that\n$E [||\\lambda_i(x) - \\lambda_j(x)||^2] = E [||\\lambda_i(x) - z^*(x)||^2] + E [||\\lambda_j(x) - z^*(x)||^2] .\t (2)\nThe proof is in Appendix D and relies on the fact that off-diagonal entries of \u2211 are 0. Note that the left hand side of the equation is observable while the two expectations on the right are unknown. We can apply this equation to pairs of LLM embeddings over a triplet of \u039bi, Aj, Ak to form a system of three equations with three unknown expectations. Solving, we have\n$E [||\\lambda_i(x) - z^*(x)||^2] = \\frac{1}{2}(\\delta_{ij} (x) + \\delta_{ik} (x) - \\delta_{jk} (x))\\ \\forall (i,j,k) \\in [m],\t (3)\nwhere $\\delta_{ij}(x) = E [||\\lambda_i(x) - \\lambda_j(x)||^2]$. Since (1) is a multivariate Gaussian with $\\Sigma_{jj} = \\frac{d}{2 \\theta_i(x)} [\\frac{j}{m}]$, we can write $\\theta_i(x)$ as the following function of $E [||\\lambda_i(x) - z^*(x)||^2]$:\n$\\frac{d}{2 \\theta_i(x)} = E [||\\lambda_i(x) - z^*(x)||^2] = \\sum_{j=1}^d E [(\\lambda_{i,j}(x) - z_j^*(x))^2] = \\sum_{j=1}^d Var (\\lambda_{i,j}(x) - z^*(x)) = \\frac{d}{2 \\theta_i(x)},\t(4)$\nwhere $\\lambda_{i,j}(x)$ and $z_j^*(x)$ are the jth indices of the embeddings $\\lambda_i(x)$ and $z^*(x)$ respectively. Therefore, we can write $\\theta_k(x) = \\frac{2}{\\delta_{ij}(x)+\\delta_{ik}(x)-\\delta_{jk}(x)}$, where each $\\delta_{ij}(x)$ can be estimated using the LLM outputs on Dtest, and in practice in Algorithm 1 we estimate \u03b8i(x) by averaging \\hat{\\theta}_k(x) over all $\\binom{m-1}{2}$ pairs of j, k \u2260 i.\nSample-conditional estimation of \u03b8i(x) Note that the expectation in $\\delta_{ij} (x) = E [||\\lambda_i(x) - \\lambda_j(x)||]$ is over the randomness in $\\lambda_i(x), \\lambda_j(x)$ conditioned on a fixed point x. However, we only have one sample per x. One simple approach is to use the entire dataset to estimate \u03b8i(x), i.e., $\\delta_{ij} (x) = \\frac{1}{x' \\in D_{test}} ||\\lambda_i (x') - \\lambda_j(x')||^2$. We denote this as SMOOTHIE-GLOBAL. However, in SMOOTHIE-GLOBAL each $\\theta_i(x)$ for i \u2208 [m] is a constant over the entire Dtest. Therefore, we use nearest neighbor kernel smoothing to estimate each \u03b4ij(x) in a sample-dependent manner, an approach we call SMOOTHIE-LOCAL. Concretely, for x \u2208 Dtest, define $\\mathbb{NN}_{n_o}(x) \\subset D_{test}$ as the $n_o < n$ nearest neighbors of x (excluding x itself) in zgo's embedding space. Then, we construct $\\delta_{ij}(x) = \\frac{1}{n_o} \\sum_{x' \\in \\mathbb{NN}_{n_o}(x)} ||\\lambda_i (x') - \\lambda_j(x')||^2$, and do the same for \u03b4ik(x), \u03b4jk(x) to get a sample-conditional estimate of \u03b8i(x). The procedure for estimating \u03b8i(x) in SMOOTHIE-LOCAL is outlined in Algorithm 1."}, {"title": "Routing", "content": "Once we have estimates of \u03b8i(x) for each of the m generators by using Algorithm 1, we can construct our route() function. We define route(G, x) = gi where i = arg max{\u03b81(x),...,\u03b8m(x)}, which selects the highest scoring LLM for input x based on \u03b8i(x). We apply this on Dtest to determine the best LLM for each input sample."}, {"title": "Results", "content": "We empirically analyze SMOOTHIE-GLOBAL and SMOOTHIE-LOCAL, focusing on four questions:\n1. How well does SMOOTHIE-GLOBAL recover ground-truth LLM rankings over samples belonging to the same task (Section 5.1)?\n2. In multi-task datasets, how well can SMOOTHIE-LOCAL perform unsupervised-routing, by identifying the best LLM for each sample (Section 5.2)?\n3. Can SMOOTHIE-GLOBAL and SMOOTHIE-LOCAL be applied to select from or route between different prompts (Section 5.3)?\n4. How does SMOOTHIE-GLOBAL and SMOOTHIE-LOCAL's performance change as a function of different algorithmic choices (Section 5.4)?"}, {"title": "Single-Task LLM Scoring", "content": "Setup We begin by evaluating whether SMOOTHIE-GLOBAL can accurately learn the relative performance of different LLMs on a single task-dataset. We study three categories of tasks. First, we consider 7 datasets corresponding to commonly-studied natural language generation (NLG) tasks [51]: CNN/DailyMail and XSum (summarization), SQUAD (reading comprehension), TriviaQA (factual recall), E2E and WebNLG (data-to-text generation), and LegalBench's Definition Extraction (text extraction) [1, 27, 30, 31, 43, 61, 62, 70, 81, 84]. We report Rouge2 for summarization and data-to-text generation tasks and accuracy for all others. For all tasks other than Definition Extraction we evaluate SMOOTHIE-GLOBAL on a 1000 sample subset.\u00b9 For these tasks, we consider two ensembles of LLMs at different size points. At the 3B size point, our ensemble consists of Pythia-2.8B [7], Gemma-2B [91], Incite-3B [17], and Dolly-3B [18]. At the 7B size point, our ensemble consists of Llama-2 [92], Mistral [40], Vicuna [107], Gemma-7B [91], and Nous Capybara [19]. We manually write a single prompt template for each task, and all model generations rely on this template.\nSecond, we consider two instruction-following benchmarks: AlpacaEval and MixInstruct [23, 24, 41, 50]. For AlpacaEval, we rely on responses accessible via the online leaderboard.\u00b2 We identify 10 LLMs (each from a different base family), and download these models' responses to the AlpacaEval instructions. We conduct 10 different simulations, where in each simulation we randomly select 5 LLMs from our pool to function as an ensemble. Reported win-rates use the standard GPT-4 references. For MixInstruct, we use generations from an ensemble of 11 different LLMs originally studied in [41]. Following [41], we measure generation quality using a ChatGPT-based rank.\nFinally, we consider a more \"reasoning-intensive\" task, GSM8K [16]. We consider an ensemble of three models: Gemma-7B, Phi-2 [39], and Llema-7b [4]. We prompt each model to provide a chain-of-though reasoning [100], and apply SMOOTHIE to these generations.\nFor all datasets, we apply SMOOTHIE-GLOBAL using SentenceBERT (all-mpnet-base-v2) embeddings of genera-tions [76]."}, {"title": "Multi-task Routing", "content": "Setup We next assess whether SMOOTHIE-LOCAL's sample-conditional scoring mechanism allows it to route samples to LLMs in the multi-capability regime. We construct two mixed-task distributions by combining existing datasets. The first distribution corresponds to tasks measured by accuracy, and contains SQUAD, TriviaQA, and Definition Extraction. We refer to this as DISTR-ACC. The second distribution corresponds to tasks measured by Rouge2, and contains CNN/DailyMail, XSum, Web NLG, and E2E. We refer to this as DISTR-ROUGE2. For each mixed-task dataset, we report the metric averaged across all tasks. We compare to three baselines.\n\u2022 RANDOM: A random-selection baseline which returns a generation from a random LLM in the ensemble. Though naive, prior work has found this to be a strong method in practice [56]. We run 10 trials and report the mean of this approach to account for variance.\n\u2022 LABELED-KNN: A labeled data-based KNN baseline. For this, we sample 50 labeled samples from a separate hold-out set (Dval), and measure the performance of each candidate LLM on this set. For a given test sample x, we identify the 20 most semantically similar instances in Dval (using SentenceBERT embeddings [76]), and route x to the highest performing LLM over this subset. We note that the LABELED-KNN baseline is derived from routing methods in [48, 86].\n\u2022 PAIRRM: A reward model from [41] which accepts an instruction and multiple generations as input, scores each generations suitability for the instruction, and returns the predicted best generation. PAIRRM is a labeled-data method which [41] trained on collected preference data.\nIn addition, we also compare the best individual model in the ensemble (BEST-MODEL), and SMOOTHIE-GLOBAL. For both mixed-task datasets, we run SMOOTHIE-LOCAL with SentenceBERT embeddings, and the sample-conditional version of SMOOTHIE-LOCAL estimates \u03b8i(x) using a neighborhood size no = 1.\nResults for the 3B and 7B ensembles over DISTR-ACC and DISTR-ROUGE2 are provided in Table 2. We find that SMOOTHIE-LOCAL outperforms all baselines across both data distributions, for both ensembles. Though SMOOTHIE-LOCAL requires no labels, it still outperforms labeled data baselines like LABELED-KNN and PAIRRM. We observe a substantial gap between SMOOTHIE-LOCAL and SMOOTHIE-GLOBAL, which indicates that SMOOTHIE-LOCAL's sample-specific scoring mechanism provides performance improvements.\nNotably, we see that SMOOTHIE-LOCAL substantially betters BEST-MODEL, indicating that SMOOTHIE-LOCAL's routing mechanism is offering a performance improvement over a strategy which merely selects the best LLM on average. We study this in greater detail by examining the relative rank of the LLM selected by SMOOTHIE-LOCAL for each sample. For each sample in DISTR-ACC and DISTR-ROUGE2, we rank the quality of each LLM's generation according to standard-competition ranking (i.e., \"1-2-2-4\" ranking). We then count how frequently SMOOTHIE-LOCAL selects the rank-i generation across each distribution for each ensemble. We visualize results in Figure 3. As the visualizations demonstrate, SMOOTHIE-LOCAL consistently selects the best or second-best generation from within the ensemble."}, {"title": "Prompt Selection", "content": "Third, we study whether SMOOTHIE-LOCAL and SMOOTHIE-GLOBAL can be generalized to other settings where engineers have a candidate pool of text generators of unknown quality, and must select one of them to use for some application. In particular, we focus on the setting where an engineer has access to multiple prompt templates for a given generation task, and must select which prompt-templates' generation to use as the final output [29]. Unlike above, we assume the engineer only has access to one LLM. We study SMOOTHIE-LOCAL and SMOOTHIE-GLOBAL in this regime using the NLG tasks from Section 5.1. For each task, we manually write between 3 and 5 prompt templates, varying the wording of instructions and the choice of in-context samples. We analyze SMOOTHIE applied to two models at different size points: Falcon (1B) [65] and Llama-2 (7B) [92]."}, {"title": "Ablations", "content": "Finally, we conduct ablations to examine different aspects of SMOOTHIE-GLOBAL and SMOOTHIE-LOCAL: improving its efficiency, adjusting the neighborhood size, varying the choice of embedding model, and using different LLM ensembles.\nImproving efficiency First, we explain SMOOTHIE's current efficiency properties. To estimate the Smoothie weights for routing, we use a simple closed-form procedure that does not require any SGD or training, as described in Algorithm 1. As a result, SMOOTHIE weights on the entire dataset can be computed in seconds for the 7B ensemble, SMOOTHIE-LOCAL on the multi-task datasets takes 2.14 seconds per 1000 samples, and SMOOTHIE-GLOBAL on the single-task datasets takes under 0.03 seconds per 1000 samples. Moreover, SMOOTHIE does not require any ground-truth annotations; however, all m model generations per test sample are needed as input to the algorithm. That is, we need n \u00d7 m generations for a Dtrain of size n samples.\nFortunately, the need for computing all model generations per test sample can be removed with a small algorithm tweak, making Smoothie even more efficient and its runtime independent of n. Suppose we have a held-out set of ntrain train samples with precomputed generations from the models in the ensemble. For each test sample, we retrieve the most similar train samples, learn the Smoothie weights for the sample using the corresponding train sample generations, and return the model with the highest Smoothie weight (i.e., in line 5 in Algorithm 1, KNN is now over a held-out training dataset). This approach, which we call SMOOTHIE-TRAIN, selects the model for a test sample without needing model generations for that sample. Only ntrain \u00d7 m generations are needed, regardless of how large the test dataset n is."}, {"title": "Conclusion", "content": "In this paper we study and propose an algorithm for learning label-free routers for generative tasks. We validate our approach across a variety of evaluation regimes, finding it consistently beats other unsupervised approaches and often matches/exceeds supervised approaches.\nLimitations We discuss several of SMOOTHIE's limitations. First, its multivariate Gaussian graphical model currently uses a diagonal covariance matrix. This assumes independent error vectors for each generation, though SMOOTHIE could be extended to account for dependencies [72, 93]. Additionally, SMOOTHIE optimizes only for performance without considering cost tradeoffs between large and small models. Finally, its reliance on embeddings may capture only certain aspects of semantic similarity. Other embedding models and additional heuristics could be used to create richer input features for SMOOTHIE."}]}