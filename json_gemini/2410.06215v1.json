{"title": "DATAENVGYM: DATA GENERATION AGENTS IN TEACHER ENVIRONMENTS WITH STUDENT FEEDBACK", "authors": ["Zaid Khan", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "abstract": "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DATAENVGYM, a testbed of teacher environments for data generation agents. DATAENVGYM frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides feedback from a student. The agent's end goal is to improve student model performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. As a general-purpose testbed, DATAENVGYM includes multiple instantiations of teacher environments across three levels of structure in the state representation and action space, with varying levels of scaffolding support. More structured environments are based on automatically-inferred skills and offer a higher degree of interpretability and control over the curriculum. We support developing and testing data generation agents in three diverse tasks covering both text and images (mathematics, programming, and visual question answering) and test multiple student and teacher models. We find that example agents in our teaching environments can iteratively improve students across diverse tasks and settings. Moreover, we show that environments can teach different skill levels and can be used to test variants of key modules, pointing to directions of future work in improving data generation agents, engines, and feedback mechanisms.", "sections": [{"title": "1 INTRODUCTION", "content": "Improving an already-trained model by creating additional training data that is targeted towards current model weaknesses is an important and frequent task for researchers and engineers. For example, past work in instruction tuning and alignment has found that models can be improved with additional task-specific training examples. However, the current model improvement process is largely driven by humans, who try to identify the weaknesses of the model based on evaluations, use intuition and heuristics to create data to target weaknesses, train an updated model on the data, and revise the data based on how the new model performs. The labor and repetition involved in this process strongly motivate the creation of data generation agents that can automate the process of creating data to teach student models, in whole or part. In our prior work, EnvGen, we automated"}, {"title": "2 DATAENVGYM ENVIRONMENTS AND AGENTS", "content": "We provide three categories of (environment, agent) pairs in DATAENVGYM with multiple levels of structure given to data generation agent, corresponding to different levels of interpretability. Agents are composed of two modules: the data generation policy  \\pi (which creates a data generation plan p_t ~ \\pi(p_t|s_t)) and the data generation engine Engine (which executes the plan to produce training data d_t = Engine(p_t); cf. Sec. 2.2.2), Both the policy and plan can change depending on the environment the agent is in, as the environment provides the agent with affordances that define the"}, {"title": "2.1 ENVIRONMENT MODULES", "content": "Given training data d_t from the data generation engine, the trainer performs a training run (i.e., a certain number of training steps on the dataset) updating the student model: m_{t+1} = Train(m_t, d_t). Then, the evaluator tests the student model and outputs its performance: s_{t+1} = Eval(m_{t+1})."}, {"title": "2.1.1 TRAINER AND EVALUATOR", "content": "Baseline implementation. We use supervised finetuning for training using the Transformers library. We present data in an instruction-finetuning format of Alpaca with the standard language modeling loss. For evaluation, we use the standard training splits from the datasets we test on. More details of the training process, including hyperparameters such as learning rates and optimizers, are provided in Appendix B.1."}, {"title": "2.1.2 SKILL DISCOVERY", "content": "SKILL-LIST and SKILL-TREE environments have a skill discovery module that takes a set of training samples d_t and returns a set of skills that would be needed to solve these examples; in the beginning of training t = 0, the environments use the skill discovery module to discover a set of skills over the validation set. Alternatively, the environments can be parameterized by a set of user-specified target skills. The skill discovery module will assign a discovered skill label to each evaluated student prediction in the validation set. The list of skills and evaluated student predictions are consumed directly by the SKILL-LIST environment. In the SKILL-TREE environment, skills are used as input by the skill organization module."}, {"title": "2.1.3 SKILL ORGANIZATION", "content": "To solve complex problems by adaptively growing the set of skills the student can perform, it is natural to organize skills into some kind of hierarchy. In the SKILL-TREE environment, the skill organization module takes as inputs a set of skills, and outputs a forest of \"skill-trees\u201d, an organized hierarchical structure that encodes skills and stores their metadata (e.g., how much data is allocated to each skill). This is the state st in the SKILL-TREE environment."}, {"title": "Baseline implementation.", "content": "In the SKILL-TREE environment, the skill forest captures the student's proficiency at increasing levels of granularity, with the root of each tree corresponding to a high-level skill domain and the children corresponding to subskills. Each tree in the forest contains key information about subskills, including the amount of training data allocated to each subskill (i.e., the data allocation) and the student's performance on the training split for each subskill. Note that the specific implementation of the skill forest is left to the user; DATAENVGYM provides a default version of the skill forest, but other implementations can be plugged in."}, {"title": "2.2 DATA GENERATION AGENT MODULES", "content": ""}, {"title": "2.2.1 DATA GENERATION POLICY", "content": "The data generation policy \u03c0 takes as input the student performance state st (list of per-example errors for OPEN-ENDED environment, skill-specific errors and the skill list for SKILL-LIST environment, and skill-specific errors and the skill tree for SKILL-TREE environment), and outputs as an action the data generation plan (the inputs to a data generation engine): p_t ~ \\pi(p_t|s_t). In the OPEN-ENDED and SKILL-LIST environments, the data generation plans are lists of specifications for training data, one for each training datum to be produced. The training datum is rendered or formatted into the appropriate format for instruction finetuning by the data generation engine. In the SKILL-TREE environment, we shape the action space and provide two discrete actions: explore and exploit; note that further actions can easily be added. Explore actions grow the skill tree by adding subskills. Exploit actions change the allocation of data for existing subskills."}, {"title": "Baseline implementation.", "content": "We drive the policies for the OPEN-ENDED and SKILL-LIST environments with an LLM by giving the verbalized the state to the LLM and prompting it to produce the corresponding actions. For the SKILL-TREE environment, we implement a policy that grows the skill tree to a fixed size and while maintaining a uniform data distribution by sequencing explore and exploit actions. Details can be found in Appendix B.5."}, {"title": "2.2.2 DATA GENERATION ENGINE", "content": "The data generation engine's role is to generate training examples based on the data generation plan from the policy: d_t = Engine(p_t). The training examples will be used to teach the student. Because each environment affords the agent with a different action space, the data generation engines corresponding to them also differ. Specifically, for the OPEN-ENDED and SKILL-LIST environments, the data generation engine receives actions in the form of datapoints to generate (since the policy's action space is unstructured) and formats the appropriate examples (e.g. for GQA, it generates images using a T2I model). The OPEN-ENDED and SKILL-LIST generators have access to the task and a list of examples to render into training data. For the SKILL-TREE environment, where the action space is {explore,exploit}, the data generation engine must first interpret these actions. Each action triggers a modification to the skill tree. An explore action invokes a subskill discovery pipeline to grow the skill tree by adding subskills. When emitting an exploit action, the agent has to specify how to change the data allocation, or budget, for each subskill; executing the action means adjusting the budget stored in the skill tree accordingly. Finally, the data generation engine consumes the skill tree and generates the planned amount of data for each subskill."}, {"title": "Baseline implementation.", "content": "For all tasks (mathematics, VQA, and programming), we generate training data using an LLM (GPT-40). For mathematics problems, we generate problems using an LLM, where each problem consists of a question, a step-by-step solution, and a final answer. For VQA tasks, we first use an LLM to generate image descriptions aligned with the task/skill/subskill given as input. We then employ a text-to-image model to convert these descriptions into images. Then, the LLM is instructed to generate a specified number of unique questions for the given task/skill/subskill. For programming, we generate data in two stages. We generate a problem and starter code given subskill information and detailed instructions about the expected format, and then solve it with an independent LLM call. We provide details on the generators for each environment in Appendix B.3 and show generated training examples for each task in Appendix C."}, {"title": "3 EXPERIMENTS", "content": "We experiment with DATAENVGYM environments in three domains: visual questions answering, mathematics, and programming. For visual question answering, we use the GQA; for mathematics, we use MATH; for programming, we use LiveCodeBench. For most experiments (reported in Sec. 3.1) we start from instruction-tuned models rather than base models because we believe it is a more realistic and challenging setting since starting from instruction tuned models is standard for applications and these models have undergone post-training on large amounts of task-specific data. For GQA, we use PaliGemma-3b-pt-224 as our student model and we use GPT-40 OpenAI as the teacher agent policy, augmented with SDXL-Turbo for T2I generation. For MATH and LiveCodeBench, we use Gemma-2-2B-Instruct and Llama-3-8B-Instruct as students respectively, and generate data with GPT-40. Note that the student models we use are typically already proficient at the target task and thus are difficult to improve. For each domain, we choose the student model to satisfy the following criteria: 1) the student should be strong enough to perform the given task (e.g. LiveCodeBench is too challenging for a Gemma2-2B student). 2) The student should not have been heavily post-trained s.t. further improvements are unlikely (e.g. Llama3-8B has been extensively trained for math and further improvements with additional training are unlikely, regardless of the data generation agent). Details can be found in Appendix B.6 (validation and test splits) and Appendix B.3 (data generation).\nWe train all models for a fixed number of steps and terminate episodes after a fixed number of iterations, and use validation accuracy to select the training dataset iteration corresponding to the highest student accuracy. For each environment and domain, we report two values: first, we report the increase in student performance achieved by the baseline implementations of the teacher policy"}, {"title": "3.1 PRIMARY RESULTS: VQA, MATHEMATICS, PROGRAMMING", "content": "Tab. 2 presents results on example instantiations of environments within DATAENVGYM. Here, we compare students before and after a multi-step trajectory of training across environments, with different data generation policies. For each setting, we report the relative gain or loss (in blue) compared to student model before training in DATAENVGYM. Note also that the models used here are already instruction-tuned on large datasets (including task-specific datasets), making obtaining further improvements particularly challenging.\nLLM policies can make use of state information to provide better training data for the student. Students trained in the \"No State\" setting generally perform worse than those trained in the \"With State\" setting. This is true across environments, with the largest difference (3.5%) for the OPEN-ENDED environment and the smallest difference (1.08%) for the SKILL-TREE environment. On LiveCodeBench, policies without state information are not able to improve the student at all, whereas on MATH, a policy without state information is still able to improve a student in all environments. The support provided to the teacher by the SKILL-TREE environment is particularly robust for GQA, where a policy without state information reaches almost identical performance to a policy with state information. However, absent SKILL-TREE's structured actions, removing state information actually hurts performance on GQA, with slight drops from the baseline for the \"No State\" setting on OPEN-ENDED and SKILL-LIST environments. For both these environments, \"With State\" improves the student model. Taken together, these results highlight the importance of the state information.\nTeaching is easier in some environments than others. With a fixed student and task (i.e. looking at \"With State\" entry across the columns of Tab. 2), teachers typically elicit the highest student performance in the unconstrained OPEN-ENDED environments, where they are not required to teach a specific set of skills. However, there may be domain specific effects here as the teachers in the SKILL-TREE environment perform the best on the multimodal GQA dataset (+5.58%), whereas this is reversed for MATH, where teachers in the OPEN-ENDED environment perform the best (+7.66%).\nThese difference may relate to the level of constraint imposed: in the OPEN-ENDED environment, the teacher can produce any data without any constraints, while in the skill-structured environments, we require the teacher to improve the student along specified skills. This may be a more difficult task, as it may impose suboptimal constraints on the teacher, i.e., the teacher may have difficulty teaching the specified skills, whereas in the unconstrained OPEN-ENDED environment, the teacher may be implicitly able to identify and teach the skills it is best at teaching. However, unconstrained teaching"}, {"title": "3.2 ANALYSIS: DIFFICULTY/RARITY, TRAINING DYNAMICS, SKILL DISCOVERY QUALITY, AND QUALITATIVE EXAMPLES", "content": "Skill learning across rarity and difficulty levels. Tab. 2 shows that skill-based learning in the SKILL-TREE environment can improve overall performance of student models. Two core questions are (1) how interpretable these skills are and (2) how learning correlates with features like question average difficulty or skill frequency. In Fig. 4, we plot the accuracy improvement of a Gemma-2B student model after training in DATAENVGYM'S SKILL-TREE environment for MATH; most skills improve, some more than others. In Fig. 4(a) we plot improvement across the average question difficulty (provided by MATH on a scale of 1 to 5). Training in DATAENVGYM boosts student performance the most in the middle difficulty region (around 3.5). On the edges, we see smaller boosts, with close to 0 difference for Calculus and Optimization (high difficulty) and even decreases for Trigonometry (low difficulty). In Fig. 4(b) we compare performance to skill rarity (inverse frequency) in the training data. Here, infrequent skills benefit less, with more frequent (i.e. less rare) skills generally benefiting more. Taken together, the results in Fig. 4 suggest that there is a sweet-spot of difficulty and frequency. At the low end this could be due to saturation: easy skills benefit less from training because the model already performs well on them or has saturated. At the other end, difficult skills or very rare skills may be underrepresented in teacher's training data or be harder to generate questions for, making learning less effective. Alternatively, the questions generated may be too hard for the student. In the middle difficulty range, the teacher generates helpful examples, allowing the student to learn. Similar theories have been put forth for human learning, e.g., Vygotsky (1934)'s Zone of Proximal Development, where learning is most effective on problems slightly harder than those students could solve alone, but not so hard that they would have no hope of solving them.\nIterative training dynamics. In Fig. 5, we plot the change in the student model's performance on the validation set throughout a full run in DATAENVGYM on each task and for each environment. Each experiment is truncated once the performance consistently decreases for multiple iterations. We use the \"With State\" baseline agents for each environment, and use the same models as in Tab. 2. Fig. 5 shows that the students generally improve across iterations. In other words, the baseline agents do uncover new datapoints that further improve the student at each iteration."}, {"title": "4 RELATED WORK", "content": "Training Environment Generation. In agent learning frameworks, designing training environ- ments usually becomes a bottleneck, as it requires sophisticated human efforts. Unsupervised environment design (UED) explores progressively increased environment difficulty based on agent scores in simple games. Liesen et al. introduce a meta-learning approach to create learning environments for continuous control. In vision-language navigation (VLN), past work propose augmenting the visual diversity of training environments with image generation models. Generation has been applied to game environments as well: Cobbe et al. generate a diverse set of game environments for training RL agents and measuring their gener- alization. In our previous work - EnvGen - we continuously adapt game instances for training RL agents, using an LLM to generate different game engine parameters that teach core skills to the agent based on feedback from the agents' intermediate progress, which improves the final performance of agents as well as learning efficiency. Yang et al. generate 3D embodied environments from user-specified prompts, generating rooms in different styles. Generally, past environment generation work has focused on restricted environments like simple games, often with a few predefined actions and skills. Moreover, past environment generation work has focused on developing students rather than improving the data generation process itself. In contrast, our present work focuses on data generation agents, creating a testbed for teachers and treating students as part of the environment. Furthermore, our work introduces environments for data generation with automatic skill discovery in a diverse set of open-ended and more realistic settings such as mathematics, visual question answering, and programming.\nLearning from Generated Data. DATAENVGYM involves transferring task knowledge from a teacher agent to a student model in an effective way, based on the student model's feedback. Past work on knowledge transfer from one model to another has been centered around knowledge distillation, where outputs from larger models are used to train smaller ones; unlike the process in DATAENVGYM, this process is typically not adaptive, relying on fixed datasets of inputs that are processed by the larger teacher model and used to train the student. In the context of LLMs, symbolic distillation has become increasingly common; here, text is generated from a large model and used to train a smaller one, e.g., in instruction tuning or distilling chain-of-thought reasoning from large proprietary models into smaller models. The kind of teaching that data generation agents are expected to perform in DATAENVGYM's environments differs from the knowledge distillation setting in that the inputs to the model themselves are model-generated (as opposed to being sourced from an existing dataset). Moreover, the inputs are dynamically determined based on the student model's feedback and errors, whereas in knowledge distillation they are determined by a fixed dataset or generated all at once. Note that DATAENVGYM is compatible with different methods of training the student (i.e., symbolic distillation, logit-based losses, etc.), and these can be swapped into our modular environments."}, {"title": "5 CONCLUSION", "content": "We propose DATAENVGYM, a testbed of teacher environments for developing modular data genera- tion agents (i.e., teachers) and environments. In DATAENVGYM, a teacher agent generates training data for a student model and receives feedback on the student's performance from the environment. We provide three environment instantiations with different levels of structure. Across three diverse domains (visual question answering, mathematics, and programming), we demonstrate that the example teachers we introduce can improve students in all DATAENVGYM environments. We analyze DATAENVGYM, including its training dynamics, performance across difficulty levels, and the impact of skill discovery module quality. We hope our work will foster future progress on data generation agents, engines, and feedback mechanisms by providing a testbed for evaluating and improving them."}, {"title": "A ADDITIONAL RELATED WORK", "content": "Skill Discovery. A line of work in reinforcement learning has studied unsupervised skill discovery, where agents learn diverse emergent behaviors without explicit rewards. The majority of this work helps agents discover new skills by maximizing the diversity of agent trajectories, such as exploration-encouraging rewards and adding randomness during action sampling However, such methods require long exploration steps, which is expensive if the cost for agent action is not negligible. Recent work has also proposed using the knowledge contained in pretrained language models to help in skill discovery and to sample new skills Developments and improvements to skill discovery are complementary to DATAENVGYM, where skill discovery is used in the SKILL-LIST and SKILL-TREE environments in Sec. 2.1.2) to dynamically extract human-interpretable skills from data and to create student feedback. These skills help identify model weaknesses and condition the data generation process, and we find that improving skill discovery can improve student model performance (cf. Tab. 3), pointing to directions for future work. Moreover, by using skill discovery in its environments, DATAENVGYM not only helps develop and test interpretable agents, but also provides a concrete extrinsic evaluation for skill discovery, allowing competing skill discovery methods to be evaluated and compared on the basis of how well they improve downstream agent performance.\nModel Weakness Discovery. Testing a trained machine learning model in different scenarios is crucial in mitigating unexpected malfunctions, so that developers can actively prevent such behaviors and finetune models on weak skills. Traditionally, model weaknesses have been identified by hiring human annotators and asking them to create adversarial inputs in different scenarios and check when model outputs are incorrect or undesired Recent work explores automatically"}, {"title": "B ADDITIONAL METHOD DETAILS", "content": ""}, {"title": "B.1 TRAINING DETAILS", "content": ""}, {"title": "B.1.1 GQA", "content": "We use the Transformers Wolf et al. (2020) library for training. We train PaliGemma-3b-pt-224 (Beyer et al., 2024) for 10 epochs using the AdamW (Loshchilov & Hutter, 2017) optimizer with 2 warmup steps and a learning rate of 2 \u00d7 10-5, a weight decay of 10-6 using the BF16 datatype and batch size of 16. We apply LORA (Hu et al., 2022) with a rank of 16 and an alpha of 32, no bias, and a dropout of 0.05. We apply LoRA to all linear layers."}, {"title": "B.1.2 LIVECODEBENCH AND MATH", "content": "We use Transformers (Wolf et al., 2020) and Llama-Factory (Zheng et al., 2024) libraries for training. We format all data in the Alpaca format (Taori et al., 2023) as instruction-response pairs. We use the Adam optimizer with a batch size of 16 and a cosine learning rate scheduler with a warmup ratio of 0.1 and train for 3 epochs in the FP16 datatype. We apply LoRA to all linear layers with a rank of 16 and an alpha of 32, no bias, and a dropout of 0.05. We truncate all training examples to a maximum length of 1024 tokens."}, {"title": "B.2 LLM DETAILS", "content": "LLM configuration. We use gpt-40-2024-08-06 (OpenAI, 2024) for the following modules: skill discovery (Sec. 2.1.2; in SKILL-LIST env), data generation policy (Sec. 2.2.1 in SKILL- LIST env), data generation engine (Sec. 2.2.2; in OPEN-ENDED, SKILL-LIST, and SKILL-TREE envs). We use a temperature of 0 and top-p of 1.0, which are default API settings. We use the Instructor library to produce structured output from LLM calls and automatically parse LLM calls into Python objects.\nPrompt Templates. We provide the LLM prompt templates for skill discovery (Fig. 6), data generation for GQA (Fig. 7) / MATH (Fig. 8) / LiveCodeBench (Fig. 9,), and data generation policy for OPEN-ENDED (Fig. 10) and SKILL-LIST (Fig. 11) environments."}, {"title": "B.3 DATA GENERATION DETAILS", "content": "For all tasks, we use validation accuracy to identify when to terminate an episode. An episode is terminated when a fixed number of iterations is reached, and the best student is selected from all students trained by the policy using validation accuracy.\nGQA. We use stabilityai/sdxl-turbo with 4 inference steps to generate images at a resolution of 1024 \u00d7 1024. In the SKILL-LIST environment, our baseline policy exhausted its data budget after 3 iterations, producing 7.5k data points. In the SKILL-TREE environment, the baseline policy episode produced the top performing student after 20 steps at \u2248 3k datapoints. In the OPEN- ENDED environment, the baseline policy episode produced the top performing student after 5 steps and \u2248 3k data points."}, {"title": "B.4 SKILL FOREST DETAILS", "content": "The skill forest (used in SKILL-TREE environment) is a hierarchical structure representing skills and subskills across various domains. It models the student model's skill proficiency with increasing detail. Each tree in the forest corresponds to a high-level skill domain and contains the following key pieces of information for each subskill:\n1. Data Allocation: The amount of training data allocated to each subskill."}, {"title": "B.5 SKILL-TREE POLICY", "content": "We develop a policy as the baseline \"With State\" policy shown in Tab. 2. that aims to grow and balance a skill tree. It operates in two phases: Growth Phase: The policy alternates between exploration and exploitation actions until a predetermined maximum number of subskills is reached. During exploration, new subskills are added to the tree. During exploitation, the policy resets data allocations to zero, preparing for the next exploration. Filling Phase: Once the maximum number of subskills is reached, the policy switches to a pure exploitation strategy. It calculates and executes actions that incrementally allocate data to each subskill, aiming to reach a specified maximum amount of data per subskill. The policy respects a maximum allocation limit per action and continues until all subskills have reached their data capacity."}, {"title": "B.6 VALIDATION AND TEST SPLITS", "content": "For GQA, we create validation and test splits by doing a balanced stratified sampling of the validation and testdev sets repeatedly. Specifically, we sample 5 questions from each of the 100 question types in GQA, following Gupta & Kembhavi (2023). For MATH, we create a validation set by doing balanced stratified sampling of the test set across all levels and topics in MATH, selecting 50 from each group. We use the official test set for MATH. For LiveCodeBench, we create a validation set by choosing all problems that are in the 2nd release but not in the 1st release as our validation set, and"}, {"title": "C QUALITATIVE EXAMPLES", "content": "Generated skill trees and examples. In Fig. 12, Fig. 13, and Fig. 14 we show qualitative examples of skills and subskills discovered for GQA, MATH, and LiveCodeBench, respectively. For each subskill, we also show example datapoints generated by the teacher model. Note that these datapoints are generated in their entirety, including the images shown in Fig. 12.\nModel predictions before and after training. In Fig. 15 we show qualitative examples of how training on generated data changes the response of a PaliGemma-3B student. The example on the left falls under the \"Material Identification and Comparison\" skill that was identified during skill discovery. Training on generated data leads to the model correctly identifying the material as \u201cplastic\u201d. This may be a result of debiasing in terms of the possible materials for chairs in the synthetic data. On"}]}