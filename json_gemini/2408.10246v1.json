{"title": "VyAnG-Net: A Novel Multi-Modal Sarcasm Recognition Model by Uncovering Visual, Acoustic and Glossary Features", "authors": ["Ananya Pandey", "Dinesh Kumar Vishwakarma"], "abstract": "Various linguistic and non-linguistic clues, such as excessive emphasis on a word, a shift in the tone of voice, or an awkward expression, frequently convey sarcasm. The computer vision problem of sarcasm recognition in conversation aims to identify hidden sarcastic, criticizing, and metaphorical information embedded in everyday dialogue. Prior, sarcasm recognition has focused mainly on text. Still, it is critical to consider all textual information, audio stream, facial expression, and body position for reliable sarcasm identification. Hence, we propose a novel approach that combines a lightweight depth attention module with a self-regulated ConvNet to concentrate on the most crucial features of visual data and an attentional tokenizer-based strategy to extract the most critical context-specific information from the textual data. The following is a list of the key contributions that our experimentation has made in response to performing the task of Multi-modal Sarcasm Recognition: an attentional tokenizer branch to get beneficial features from the glossary content provided by the subtitles; a visual branch for acquiring the most prominent features from the video frames; an utterance-level feature extraction from acoustic content and a multi-headed attention based feature fusion branch to blend features obtained from multiple modalities. Extensive testing on one of the benchmark video datasets, MUSTaRD, yielded an accuracy of 79.86% for speaker dependent and 76.94% for speaker independent configuration demonstrating that our approach is superior to the existing methods. We have also conducted a cross-dataset analysis to test the adaptability of VyAnG-Net with unseen samples of another dataset MUStARD++.", "sections": [{"title": "1. Introduction", "content": "The proposed model has been named \"VyAnG,\" which draws inspiration from the use of sarcasm in the Hindi language. The acronyms V, A, and G correspond to visual, acoustic, and glossary (textual) content, respectively. The Multi-modal sarcasm recognition (MSR) task aims to determine whether a given video utterance should be labelled as sarcastic or non-sarcastic based on its content. Multi-modal learning has emerged as a significant area of study in recent years [1], [2] due to the proliferation of video clips and other user-generated multi-modal content on social networking sites. In contrast to conventional single-modal learning on individual modalities (such as auditory, visual, or textual), multi-modal learning seeks to combine multiple data streams into a single unit. MSR is a subset of multi-modal sentiment recognition in which the speaker deliberately uses unconventional body language, word choice, or vocal inflexion to emphasize incongruity across modalities. Unfortunately, it is notoriously tricky for opinion-mining algorithms to comprehend sarcastic utterances accurately. Consider the phrase \"Unconditionally; I love it whenever my train is delayed\" as an example. It is quite easy to be misled by the positive word \u201clove\u201d while failing to understand the underlying emotion from \u201cmy train is late\u201d. Therefore, developing an accurate system for spotting sarcasm is quite vital when it comes to opinion mining.\nThis form of implicit human attitude is essential for preventing discussion barriers, producing positive effects on mental health, and fostering trust. On the other hand, existing deep learning architectures might sometimes struggle to understand such complex and multi-modal emotions."}, {"title": "1.1 Motivation", "content": "Text has always been the most prevalent medium for delivering sarcasm. Sarcasm in multi-modal data, on the other hand, typically requires explicit inter-modal clues to expose the speaker's true intent. It may, for example, be indicated by a mix of linguistic and non-linguistic clues, such as excessive emphasis on a phrase, a drawn-out syllable, a shift in tone of voice, or an awkward expression. Consider the cheering statement in Figure 1: \u201cif you're compiling a mix CD for a double suicide. Oh, I hope that scratching post is for you.\" becomes sarcastic when spoken with an awkward face and a saucy tone, and in general, has a negative meaning. Naturally, humans can process this massive amount of simultaneous data. However, developing an approach that can possibly accomplish the same task requires a suitable representation of all of these different sources of information. It thus results in a significant increase in research interest.\nSignificance of the dataset used in this study: In today's era, individuals are expressing their viewpoints on social media via the use of sarcastic modes of communication. Furthermore, there is a notable trend of people actively participating in the dissemination of caustic reviews directed towards a wide range of political decisions and governmental policies. Hence, apart from understanding of multiple emotions, the ability to detect sarcasm is crucial for individuals to successfully deal with and engage in modern societies that are flooded with irony. Considering the significance of sarcasm recognition, we decided to choose a multimodal dataset for sarcasm recognition to assess the effectiveness our proposed approach."}, {"title": "1.2 Challenges", "content": "Sarcastic dialogue utterances are challenging to gather, despite their frequent use in movies and television series, due to the time and effort needed to recognize and annotate unprocessed videos with sarcastic class labels manually. MUSTARD, the sole open-source accessible dataset, has 690 video clips labelled with either non-sarcastic or sarcastic sentiments. The main challenge in the MSR field is acquiring the most prominent features from all the modalities. Thus, to obtain robust intra-modal dependencies, we offer a novel framework integrating a dedicated lightweight depth attention module in the visual branch to extract the most prevalent features from the video frames. In contrast, the textual branch uses the attention-based tokenization method to acquire the most relevant features from the glossary content provided by the subtitles and to acquire inter-modal dependencies; in our study, the proposed approach employs multi-headed attention fusion to integrate asynchronously received features from each of the separate modalities."}, {"title": "1.3 Major Contributions", "content": "The following are the three aspects of our contribution:\nWe proposed VyAnG-Net, a novel multi-modal sarcasm recognition framework, by uncovering visual, acoustic and glossary (textual) features. This framework includes the glossary branch that uses the attention-based tokenization approach to acquire the most significant contextual features from the textual content provided by the subtitles of the video utterances, a visual unit with a dedicated lightweight depth attention module to acquire the most prominent features from the video frames, an utterance-level feature extraction from acoustic content and lastly multi-headed attention based feature fusion has been employed to blend features acquired from each of the separate modalities.\nWe have tested our method on one of the standard video datasets, MUSTARD, and found that it surpasses the cutting-edge techniques by a wide margin.\nFinally, a series of ablation experiments were conducted to ensure the robustness of our proposed approach that we suggested, and our results seemed quite promising.\nIn the recent years, remarkable advancements have been made in sarcasm identification frameworks on the MUSTARD dataset, but there are concerns about their generalizability. Consequently, rather than limiting ourselves to assessing VyAnG-Net on a single dataset we undertake a cross-dataset study as part of a generalization research to test the resilience of VyAnG-Net. Our proposed approach, VyAnG-Net, was trained using the MUSTARD dataset for this experimental investigation, and its performance was evaluated using an unseen MUSTARD++ dataset."}, {"title": "2 Related Work", "content": "This section deals with both single-modal and multi-modal sarcasm recognition cutting-edge approaches from the previous research studies in detail."}, {"title": "2.1 Unimodal Sarcasm Recognition", "content": "The current subsection covers literature on sarcasm recognition that focuses on a single modality in their research."}, {"title": "2.1.1 Sarcasm Recognition through Text", "content": "Most prior research for identifying sarcasm in text-based content has focused on either lexicon or rule-based approaches. Twitter is used as a leading data source for resource collection in this area, with human annotations [3], [4] and remote supervision through hashtags serving as the key annotating methods [5], [6]. According to previous studies, similarities between a speaker and the audience may also be gleaned from their context [7]. Apart from this, many other different aspects of context have also been addressed, such as the encoding of expressed emotion and the individual's personality traits [8]; the background of the speaker and behavioural patterns on various online platforms [9]; aspects of style and discourse [10]; features of the user community [11]; and the retention of user-specific representations [11], [12].\nTraditionally, classical machine learning approaches have been utilized to recognise sarcasm. Generally, algorithms like Naive Bayes, SVM, logistic regression, etc., were used for various text classification tasks, as indicated in [13], [14]. These approaches deliver the most accurate and effective results when dealing with classification challenges involving smaller datasets. Furthermore, in [15], a comparison of conventional and transfer learning strategies was analysed. In most scenarios, ensemble-based machine learning approaches also provide the best performance for text-based classification in detecting sarcasm, as presented in [16].\nMachine learning methods will be insufficient to deal with the massive amounts of data generated daily on social media platforms. Hence, in addition, to typical machine learning algorithms, researchers are increasingly more enthusiastically working on sarcasm identification using different deep learning approaches to manage vast amounts of information. For example, to recognize sarcasm on the web, [17] utilizes numerous deep learning architectures, including GRU, LSTM, and ConvNets, to form an ensemble-based model. In the aviation industry, RNNs with GRU and SVM were utilized [18] to enhance the recognition and analysis of sarcastic sentiments. In addition to using English as the sole language for the research, other languages, such as Hindi and Arabic, have also been utilized to identify sarcasm in tweets. [19] suggested \u201cTANA\u201d, a neural architecture to recognise sarcasm in Hindi tweets. The system is trained using word and emoji embedding and uses a combination of LSTM and SVM to identify sarcasm.\nTransformer-based models are also receiving a lot of attention because of their self-attention mechanism [20], which aids in focusing on the most salient characteristics while ignoring the rest for various computer vision and natural language processing applications. [21] proposed a hybrid deep neural architecture with an integrated attention module for identifying sarcasm in news headlines. A combination of graph-convolution neural network and BERT has been presented [22] for recognizing irony in the text."}, {"title": "2.1.2 Sarcasm Recognition through Audio", "content": "The acquisition of prosodic signals in the form of auditory patterns that are associated with sarcastic behaviour has been the primary focus for recognizing sarcasm in voice. For addressing the issue of sarcasm detection using acoustic content was proposed by [23], who focused on the vocal tonalities of ironical speech. The researchers speculated that slower speech rates and greater frequency could be the best indicators of sarcasm. Researchers [24] looked at prosodic and spectral aspects of sound in and out of context to identify sarcasm. Stress & intonation are two examples of prosodic characteristics that are widely regarded as reliable predictors of irony [25]. The above-discussed research contributions are among the few discussions devoted to sarcasm detection based on audio."}, {"title": "2.2 Multi-Modal Sarcasm Recognition", "content": "This subsection examines the research studies on sarcasm recognition that focuses on multiple modalities."}, {"title": "2.2.1 Sarcasm Recognition through Image-Text Pairs", "content": "Affective computing is currently attracting a lot of interest from academics, particularly with regard to the usage of multi-modal sources of information. People are still heavily dependent on text-based content, but videos, acoustic, images, and emoticons are becoming more popular nowadays. Consequently, we frequently encounter posts on online community forums that include text and a caption. Hence, sarcasm recognition using image-text pairings has been the subject of extensive study. [26] was the first to work on multi-modal sarcasm identification using image-text pairings scraped from Instagram posts. Bi-GRU was used in this research study to extract features from captions, while VGG-16 was used to extract features from the visual modality. In addition, the text included in the picture is retrieved using OCR, and the resulting transcript is given to the Bi-GRU for feature extraction. Finally, all of the cross-modality features are concatenated and sent to a classification layer to provide the final prediction score. [27] have also examined the interaction of textual and visual information in sarcastic multi-modal blogs for three prominent social media sites, namely, Tumblr, Instagram & Twitter, and present a classification of the relevance of photos in sarcastic posts. Researchers have recently started to employ \"attention modules\" to help them focus on what's important and ignore the rest in order to acquire more accurate and superior results. For example, [28] suggested a deep learning framework based on attention mechanisms to accomplish multiple subtasks such as sarcasm, sentiment, and humour recognition for image-text pairs. [29]\u2013[31] are other contributions in recent years based on image-text pairs."}, {"title": "2.2.2 Sarcasm Recognition through Videos", "content": "Despite the fact that multi-modal data sources provide extra clues in identifying sarcasm, this has not been done extensively; one of the primary reasons behind this is the lack of multi-modal datasets. In the modern era, scholars [32] [33] have commenced utilising diverse sources of information to detect sarcasm, including those that are multi-modal in nature. Indeed, it is a verifiable fact that modalities such as acoustic and visual frequently offer a greater abundance of clues pertaining to the context of an utterance when compared to text. The MUSTARD dataset, which is the initial multi-modal video dataset for detecting sarcasm, was recently introduced by [32]. In this study, researchers have used an SVM algorithm to identify examples of sarcasm in the given dataset. Subsequently, the MUSTARD dataset was manually annotated with sentiment and emotion class labels by [34], followed by sentiment and emotion recognition, along with sarcasm recognition. [35] introduces \u201cIWAN\u201d, an innovative approach for identifying sarcasm. This method involves the utilisation of a scoring mechanism that prioritises word-level incongruity details. The task of analysing emotions in sarcastic video utterances has evolved with the release of MUSTARD++, an extended version of MUSTARD that includes nine distinct emotions. This development was reported by [36] with nine emotions for the task of emotion analysis in video utterances.\nIn addition to deep learning methodologies, a novel approach utilising fuzzy logic has been proposed for the recognition of sarcasm in video utterances by [37]. The SEEmoji MUSTARD dataset has been recently published by [38] as an extension of the benchmark dataset MUSTARD. This dataset is intended for the analysis of sentiment, sarcasm, and emotion and utilises a multi-task framework that is capable of recognising emojis as well.\nAlthough previous research endeavours have had significant efforts on the alignment of visual, acoustic and textual features, however, the acquisition of multi-modal sarcastic video samples poses a significant challenge, and currently existing approaches encounter difficulties in achieving satisfactory performance levels on the MUSTARD dataset. This served as a source of inspiration for us to come up with a framework that can effectively incorporate key information from all three modalities in the context of multi-modal sarcasm recognition."}, {"title": "3 Proposed Approach", "content": "This section provides a comprehensive discussion of the proposed framework VyAnG-Net. The main objective is outlined in the first part of this section. Then, the model's framework is introduced, which consists of three modules: a glossary branch that uses the attention-based tokenization approach to acquire the most significant contextual features from the textual content provided by the subtitles of the video utterances, a visual branch with dedicated attention module to acquire the most prominent features from the video frames and lastly multi-headed attention based feature fusion to blend features acquired from each of the separate modalities. The term \u201cmulti-modal sarcasm recognition\" is typically used in our study to denote the process of analysing sarcasm in video utterances for the sake of convenience."}, {"title": "3.1 Objective", "content": "The problem of recognising sarcasm in video utterances can be thoroughly summed up as follows:\nLet \"V\" denote the sample space comprising video utterances. A sample of the dataset consists of the textual content conveyed through video subtitles \"G\", visual frames \"V\", and accompanying acoustic content \"A\". Each of the samples is assigned to a class label denoted as \"C\". In more technical terms, each sample can be defined as a quartet consisting of a subtitle, visual frames, acoustic information, and a class label. The following expression can be formulated as:\n$\\Pi = \\{(G^{0}, V^{0}, A^{0}, C^{0}), (G^{1}, V^{1}, A^{1}, C^{1}), ..., (G^{i}, V^{i}, A^{i}, C^{i}), ..., (G^{m-1}, V^{m-1}, A^{m-1}, C^{m-1})\\}$                                                                         (1)\nwhere, $\\Pi$ is the collection of sample quartets, $G^{i}$ denotes subtitle information, $V^{i}$ denotes visual frame information, $A^{i}$ defines the acoustic content, $C^{i}$ is the class label that corresponds to a specific utterance for the ith sample and the variable m represents the cardinality of the sample space, which denotes the total number of samples in a given dataset.\nVyAnG-Net aims to learn a mapping function F: (G, V, A) \u2192 C from the multi-modal training examples {($G^{i}$, $V^{i}$, $A^{i}$)|0 \u2264 i \u2264 m - 1}. For a sarcasm recognition task, $C^{i}$\u2208 {sarcasm and not sarcasm}."}, {"title": "3.2 VyAnG-Net: A Novel Multi-Modal Sarcasm Recognition Model by Uncovering Visual, Acoustic and Glossary Features", "content": "The VyAnG-Net was proposed for the purpose of performing multi-modal sarcasm recognition to generate the relationship among visual, acoustic, and glossary (textual) information and to explore the compatibility between these three modalities. Figure 2 illustrates the VyAnG-Net framework. The model comprises of three distinct components. Firstly, a textual branch that employs an attention-based tokenization approach to extract the most salient contextual features from the glossary content presented in the video utterances' subtitles. Secondly, a visual branch that incorporates a dedicated attention module to capture the most prominent features from the video frames. Lastly, a multi-headed attention-based feature fusion mechanism is utilised to integrate the features obtained from each of the individual modalities. Table 1 presents the proposed framework in algorithmic format."}, {"title": "3.3 Algorithm 1: VyAnG-Net: A Novel Multi-Modal Sarcasm Recognition Model by Uncovering Visual, Acoustic and Glossary Features.", "content": "Aim: To learn a mapping function F: (G, V, A) \u2192 C from the multi-modal training examples {($G^{i}$, $V^{i}$, $A^{i}$)|0 \u2264 i \u2264 m \u2212 1}.\nInput: Glossary (textual) set G = {$G^{1}$, $G^{2}$, ... ..., $G^{i}$}, visual set V = {$V^{1}$, $V^{2}$, ......, $V^{i}$}, and acoustic set A = {$A^{1}$, $A^{2}$, ......, $A^{i}$}.\nOutput: sarcasm recognition task, $C^{i}$ \u2208 {sarcasm and not sarcasm}.\nWord-to-vector representation from the entire Glossary content set IR;\nExtract features at the level of utterance and context from vector representation of the glossary content $G_{u}$\u2295$S_{gu}$, and $G_{c}$\u2295$S_{gc}$;\nExtract features at the level of utterance and context from the visual frame $V_{u}$\u2295$S_{vu}$,and $V_{uc}$\u2295$S_{vc}$;\nExtract features at the level of utterance and context from acoustic content $A_{u}$\u2295$S_{au}$, and [ ];\nfor E1 to Epochs do\n$R_{u}$ \u2190 $W_{1:g}$ = {$W^{1}$, $W^{2}$, \u2026\u2026\u2026 \u2026\u2026\u2026, $W^{g}$} word to vector representation by Eq. (1);\n$R_{c}$ \u2190 $W_{1:g^{c}}$ = {$W_{1}^{c}$, $W_{2}^{c}$, .., $W_{g}^{c}$} word to vector representation by Eq. (4);\n$G_{u}$\u2295$S_{gu}$ \u2190 ($G_{u}$ \u2190 \u03bc($R_{u}$) \u2295$S_{gu}$) obtain utterance-level text-based features using Eq. (2) and (3);\n$G_{c}$\u2295$S_{gc}$ \u2190 ($G_{c}$ \u2190 \u03bc($R_{c}$)\u2295$S_{gc}$) obtain context-level text-based features using Eq. (2) and (3);\n$V_{u}$\u2295$S_{vu}$\u2190 $V_{S}$ obtain utterance-level visual features using Eq. (9) and (10);\n$V_{uc}$\u2295$S_{vc}$ $V_{S}$ obtain context-level visual features using Eq. (11) and (12);\n$A_{u}$\u2295$S_{au}$ \u2190 Librosa tool(Concatenation($A_{m}$, $S_{au}$)) obtain utterance-level acoustic features using Eq. (13);\n$G_{cat}$ $G_{u}$\u2295$S_{gu}$\u2295$G_{c}$\u2295$S_{gc}$ final text-based features obtained by concatenating utterance and context-based features;\n$V_{cat}$ $V_{u}$\u2295$S_{vu}$\u2295$V_{uc}$\u2295$S_{vc}$ final vision-based features obtained by concatenating utterance and context-based features;\n$A_{cat}$ $A_{u}$\u2295$S_{au}$\u2295 [ ] final audio-based features obtained by concatenating utterance and context-based features;\n$L_{G}$ $G_{cat}$ obtain the most prominent textual features by applying multi-headed attention using Eq. (15);\n$L_{V}$ $V_{cat}$ obtain the most prominent visual features by applying multi-headed attention using Eq. (15);\n$L_{A}$ $A_{cat}$ obtain the most prominent acoustic features by applying multi-headed attention using Eq. (15);\nGVA \u2190 $L_{G}$\u2295$L_{V}$\u2295$L_{A}$ concatenate all the features obtained from multiple modalities to get multi-modal feature representation using Eq. (16);\n$C$\u2190 Softmax(GVA) pass the multi-modal features to the softmax layer to get the final prediction; calculate loss and perform backpropagation;"}, {"title": "3.3.1 Input Features", "content": "The dataset comprises of individual samples that include an utterance, its corresponding context, and associated labels. The utterance's context encompasses a series of prior utterances, typically N in number, that lead up to the given utterance within the dialogue. Each utterance is linked to its respective context and speaker, with the speaker of the utterance and the speaker of the context being distinct entities. Our study provides an extensive explanation of the utterance and its contextual factors across all modalities in the following subsections."}, {"title": "3.3.2 Textual Feature Extraction using Glossary Content", "content": "Assuming a given utterance consisting of g words, denoted as $W_{1:g}$ = {$W_{1}$, $W_{2}$, ... ..., $W_{g}$},\nwhere each word $W_{i}$ belongs to the set of real numbers $R^{300}$. Each term is denoted as $W_{i}$, corresponds to a vector that is generated through the utilization of [39] attention-based tokenization (\u03c4) represented in Eq. (1). The acquisition of the contextual relationship among words is accomplished by means of employing a [40] model denoted as \u03bc using Eq. (2). Subsequently, utterance level features are obtained through the utilization of the final word embedding, represented as $G_{u}$.\n$R_{u} = \\tau (\\{W_{1}, W_{2}, ... ..., W_{g}\\})$                                                                                                                                                                                                 (1)\n$G_{u} = \\mu (R_{u})$                                                                                                                                                                                                                                                                                                (2)\nIn cases where speaker information $S_{gu}$is accessible, it is possible to combine it using Eq. (3) with $G_{u}$ to form a speaker-aware textual utterance, which is represented $G_{u}$\u2295$S_{gu}$\n$G_{u}$\u2295$S_{gu}$ = Concatenation($G_{u}$n, $S_{gu}$)                                                                                                                                                                                (3)\nAssuming there is a set of utterances in the given context, each comprising $g_{c}$ words, the utterance-level representations for such a set of contextual videos are obtained by subjecting the words of each utterance to [40], using Eq. (4) following which the embedding of the last word of the glossary provided by the subtitle is utilized. The $f^{ith}$ utterance in the context is denoted by $G_{i}^{c}$.\n$G_{i}^{c} = \\mu (\\tau (\\{W_{1: g_{c}} = \\{W_{1}, W_{2},....., W_{g}\\}\\}\\))$                                                                                                                                                                (4)\nWhen speaker information $S_{gc}$ is available, it is appended to every contextual utterance $G_{i}^{c}$ too. In the end, the features at the context level are also obtained by concatenating all textual utterances that are influenced by the speaker, as represented in Eq. (5).\n$G_{SC} = Concatenation ((G_{1}^{c}\u2295S_{g1}^{c}), (G_{2}^{c}\u2295S_{g2}^{c}), ........, (G_{gc}^{c}\u2295S_{gc}^{c}))$                                                                                    (5)"}, {"title": "3.3.3 Visual Feature Extraction from the Video Frames of the Utterances", "content": "To obtain visual features from the video frames [41] is integrated with the depth attention module [42], discussed in the following section."}, {"title": "3.3.3.1 Lightweight Attention Module", "content": "The Convolutional Neural Networks (ConvNets) have demonstrated remarkable representational abilities, leading to significant enhancements in their efficacy for visual tasks. In addition, we explore another aspect of architectural design that has become increasingly prevalent in modern times, namely, attention. Through the utilization of attention mechanisms, which involve prioritising significant attributes while inhibiting irrelevant ones, it is anticipated that the efficacy of representation will be enhanced. Considering this information, a framework known as the \"light weighted attention framework\" [42], illustrated in Figure 3, has been developed and integrated into [41] to concentrate on the most salient characteristics from the visual frames while disregarding the others. In order to accomplish this task, we have implemented four different modules, namely, feature grouping, depth attention, spatial attention and aggregation, which constitute [42].\nThe word \"spatial\" refers to the encompassing spatial domain of each feature map. By including the spatial attention module to enhance the feature maps, the superior input is then sent to the subsequent levels of convolution, hence increasing the efficacy of the model. On the other hand, the phrase \"depth\" denotes the total number of channels, which are simply a set of feature maps arranged in a tensor. Each and every multidimensional layer inside this tensor represents a feature map with a depth of H \u00d7 W. The depth attention mechanism provides a numerical value associated with every channel, therefore prioritising those channels that have the most impact on the learning process. This prioritisation leads to the optimisation of the most important features, ultimately enhancing the overall performance of the model.\nThe property of feature grouping is characterised by a hierarchical structure consisting of two levels. Suppose that the attention module's input tensor is $X\u2208R^{D\u00d7H\u00d7W}$, where\nD, H and W denote the depth, height and width of the feature map, respectively. Initially, X is partitioned into P distinct groups, resulting in $X' \u2208 R^{\\frac{D}{P}\u00d7H\u00d7W}$ for each group across the depth of the feature maps. The obtained feature groups are then transmitted to the attention components, where they are subsequently segregated into two distinct groups based on the depth dimension. One group is allocated to the spatial attention branch, while the other is assigned to the depth attention branch. And these sub-feature groups that are transmitted across both the spatial or depth attention branches can be represented as $X'' \u2208 R^{\\frac{D}{2P}\u00d7H\u00d7W}$.\nThe depth attention branch involves reducing the feature maps obtained from the feature grouping phase to $X'' \u2208 R^{\\frac{D}{2P}\u00d71\u00d71}$. This is achieved through the use of a global average pooling operation and gating mechanism, which enables more accurate and versatile decisions. The resulting output is then subjected to a sigmoid activation function which is represented as follows:\n$X_{1} = \\sigma(F(\\tau)) \u00b7 X'' = \\sigma(V_{1}\\tau\u2295b_{1}) \u00b7X''$                                                                                                (6)\nThe Group Norm technique is employed to reduce the input X' in spatial attention, resulting in spatial features. The function F(. ) is subsequently employed to improve the depiction of the diminished tensor. This concept can be expressed through a simple mathematical formula:\n$X_{2} = \\sigma(V_{2} \u00b7 GroupNorm(X'')\u2295b_{2})\u00b7X''$                                                                                          (7)\nThe concatenation of the outputs obtained from the Spatial Attention and depth attention is performed initially. Then a depth shuffle technique is implemented, similar to the approach used in ShuffleNet, to facilitate interaction among groups along the depth. Consequently, the resulting output possesses identical dimensions to those of the input tensor that underwent processing in the shuffle attention layer.\n$X^{'}_{k} = [X_{1}\u2295X_{2}] \u2208 R^{\\frac{D}{P}\u00d7H\u00d7W}$                                                                                                                                (8)"}, {"title": "3.3.3.2 Utterance and Context-Level Feature Extraction from Video Frames", "content": "In this, utterance-level features are initially extracted, followed by the extraction of context-level features. These two sets of features are then subsequently concatenated to yield the final feature representation. Suppose there is a set of $N_{u}$ visual frames at utterance level denoted as $V_{1:Nu}$ = {$V_{1}$, $V_{2}$, ....., $V_{Nu}$}. Each visual frame is sent to a self-regulatory ConvNet model [41] depicted in Figure 4 that makes use of a light-weighted depth attention module [42] to extract the most prominent features, which has already been explained in detail above. To obtain information pertaining to the level of utterance, the mean value is computed for all frames $V_{u}$ In cases where speaker information is present, the utterance $V_{u}$ is concatenated with the corresponding speaker information $S_{vu}$ given by Eq. (9) and Eq. (10). The notation $V_{u}$\u2295$S_{vu}$ used is where $V_{u}$ belongs to the set of real numbers R and has a cardinality of 2048.\n$V_{Su}$ = (($V_{u1}$\u2295$S_{vu1}$), ($V_{u2}$\u2295$S_{vu2}$), ..\u2026.\u2026\u2026\u2026\u2026, ($V_{uN}$\u2295$S_{vuN}$))                                                                                               (9)\n$V_{u}$\u2295$S_{vu}$ = Self \u2013 regulated ConvNet+Lightweighted depth attention($V_{S}$)                                                                                        (10)\nSimilarly, to obtain context-level features from the set of $N_{uc}$ contextual utterances, the mean value is computed for all frames denoted as $V_{uc}$. In cases where speaker information is present, the utterance $V_{uc}$ is concatenated with the corresponding speaker information $S_{vc}$ defined by Eq. \u27e810\u27e9 and Eq. \u27e811\u27e9.\n$V_{sc}$ = (($V_{uc1}$\u2295$S_{vc1}$), ($V_{uc2}$\u2295$S_{vc2}$), ........,($V_{ucN}$\u2295$S_{vcN}$))                                                                                                  (11)\n$V_{uc}$\u2295$S_{vc}$ = Self \u2013 regulated ConvNet+Lightweighted depth attention($V_{S}$)                                                                                         (12)"}, {"title": "3.3.4 Utterance-Level Feature Extraction from Acoustic Content", "content": "Suppose there is a set of $N_{a}$ acoustic frames at utterance level denoted as $A_{1:Na}$ =\n{$A_{1}$, $A_{2}$, ... ..., $A_{Na}$ }. Librosa library has been utilized to extract acoustic information. Similar to the process of extracting visual features, the methodology utilised here also involves the computation of the average value of all frames to extract information pertaining to utterances denoted as $A_{um}$. In cases where speaker information is present, the utterance $A_{m}$ is concatenated with the corresponding speaker information $S_{au}$ given by Eq. (13). The notation\n$A_{u}$\u2295$S_{au}$ used is where $A_{u}$ belongs to the set of real numbers R and has a cardinality of 283. Also, it is important to keep in mind that audio recordings often consist of a variety of speakers, ambient noise, cues for laughter, and other sounds in the background. As a result, the consideration of contextual factors is not integrated into acoustic analysis, as it might cause challenges in distinguishing it from the laughter portion of the conversation. Therefore, an empty list [ ] is employed within the context of acoustic content.\n$A_{u}$\u2295$S_{au}$ = Librosa tool(Concatenation($A_{m}$, $S_{au}$))                                                                                                            (13)"}, {"title": "3.3.5 Multi-Headed Attention-Based Feature Fusion", "content": "In the very first step, all the utterance and context-level features of all the modalities are concatenated together, as represented in Eq. (14).\n$G_{icat}$ = Concatenation (($G_{u}$\u2295$S_{gu}$), ($G_{c}$\u2295$S_{gc}$))\n$V_{cat}$ = Concatenation(($V_{u}$\u2295$S_{vu}$), ($V_{uc}$\u2295$S_{vc}$))\n$A_{cat}$ = Concatenation(($A_{u}$\u2295$S_{au}$), ([ ]))                                                                                                                                      (14)\nSubsequently, $G_{cat}$, $V_{cat}$, and $A_{cat}$ are individually fed into the linear layer followed by the multi-headed attention layer as defined by Eq. (15)\n$L_{G}$ = Multi \u2013 headed attention(Linear($G_{cat}$))\n$L_{V}$ = Multi \u2013 headed attention(Linear($V_{cat}$))\n$L_{A}$ = Multi \u2013 headed attention(Linear($A_{cat}$))                                                                                                                                     (15)\nThe features derived from various modalities are concatenated and subsequently fed into a linear layer, which is succeeded by a multi-headed attention layer. This process yields a highly significant multi-modal feature vector, as outlined in Eq. (16).\nGVA = Multi \u2013 headed attention (Linear(Concatenation($L_{G}$, $L_{V}$, $L_{A}$)))                                                                                        (16)\nFinally, the softmax layer is utilized to forecast the classification label as either sarcastic or non-sarcastic, as illustrated in Eq. (17).\n$C$ = Softmax(GVA)                                                                                                                                                                                               (17)"}, {"title": "4 Experiment and Result", "content": "The following subsection covers comprehensive details related to the dataset used throughout the study, the experimental configurations of the proposed methodology, and evaluations of its performance."}, {"title": "4.1 Dataset Used", "content": "The MUSTARD dataset, as provided by [32"}]}