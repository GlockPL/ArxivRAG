{"title": "COLLABLLM: From Passive Responders to Active Collaborators", "authors": ["Shirley Wu", "Michel Galley", "Baolin Peng", "Hao Cheng", "Gavin Li", "Yao Dou", "Weixin Cai", "James Zou", "Jure Leskovec", "Jianfeng Gao"], "abstract": "Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce COLLABLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, COLLABLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions\u2014a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. COLLABLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where COLLABLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.", "sections": [{"title": "1. Introduction", "content": "Modern Large Language Models (LLMs) excel at generating high-quality single-turn responses when given well-specified inputs. However, real-world users often do not fully articulate their intents and sometimes initiate conversations with an imprecise understanding of their own needs (Taylor, 1968). As a result, users routinely refine their requests post hoc through iterative corrections, which can increase frustration, hinder effective task completion, and reduce conversational efficiency (Amershi et al., 2019; Zamfirescu-Pereira et al., 2023; Wang et al., 2024; Kim et al., 2024). Therefore, an open problem is to train models that actively guide users in clarifying and refining their intents, and helps them achieve their goals. This key challenge would improve user satisfaction and efficiency and streamline human-LLM interactions\u2014especially as LLMs are being applied to real-world tasks that are increasingly complex and open-ended.\nA notable limitation of established fine-tuning techniques, such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), is that they primarily reward LLMs for immediate, single-turn responses, reducing their incentive to seek clarification or assist users in refining their intents or preferences. As a result, commonly used LLMs tend to prioritize direct answers, even though seeking additional context would enhance task completion and increase user satisfaction (Kim et al., 2024).\nHere we introduce COLLABLLM, a novel and general training framework that improves the ability of LLMs to effectively collaborate with humans in multiturn scenarios (Gao et al., 2019; Balog & Zhai, 2023; Rahmani et al., 2023). The key innovation of COLLABLLM is to promote LLMs' forward-looking behavior that leads to long-term collaboration gains (Figure 1). We introduce a collaborative simulation module that samples future conversations with users to estimate the long-term impact of model responses across multiple turns, a measure we term the Multiturn-aware Reward (MR). The MR function evaluates responses by incorporating both extrinsic metrics, such as task-specific success, and intrinsic metrics, such as efficiency, to holistically assess collaboration quality (cf. Section 3). By fine-tuning with established RL algorithms (Rafailov et al., 2023; Schulman et al., 2017) on MRS, COLLABLLM promotes responses that lead to better task completion and efficiency in later conversation stages.\nAs shown in Figure 2b, the fine-tuned model goes beyond simply responding to user requests in Figure 2a-it actively collaborates by asking follow-up questions about the writing tone, generating targeted content about the role of optimism, and offering insightful suggestions such as adding anecdotes.\nWe also introduce three challenging multiturn tasks for training and evaluation in simulated environments: MediumDocEdit-Chat, BigCodeBench-Chat, and MATH-Chat, which respectively encompass document creation, code generation, and multiturn question answering."}, {"title": "2. Problem Formulation", "content": "In contrast to many existing tasks that are single-turn and require no human involvement beyond the initial query, our problem formulation reflects a real-world setting in which a user's underlying (implicit) goal is defined as g in a multiturn conversational task. The conversation unfolds over multiple turns $t_j = {u_j, m_j}$, where $u_j$ is the user input and $m_j$ is the model's response at each turn $j = 1, ..., K$, where K is the number of turns in the conversation.\nAt the j-th turn, the model generates its response based on the previous conversation turns $t_{1:j-1} = {t_1,..., t_{j-1}}$ and the current user response $u_j$. For simplicity, we define"}, {"title": "3. Unified Collaborative LLM Training", "content": "Key Motivations. Established LLM training frameworks, such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), focus on maximizing immediate rewards for single-turn tasks. This cause a misalignment between their single-turn objective and real-world multiturn objective $R^* (t_{1:K} | g)$. Precisely, the model's accumulative single-turn reward $\\sum_{j=1}^{j=K} R(m_j | t_h)$ may not imply a higher final reward $R^* (t_{1:K} | g)$. In fact, achieving high single-turn rewards at each turn may not imply a higher final reward. For example, consider a task where the user's goal g is to write an engaging article. A model trained with traditional RLHF might generate isolated responses, like drafting an introduction or listing conclusions. While these responses are helpful in isolation, they fail to consider how the sections flow together, resulting in an article that might not be cohesive and aligned with the user's goal.\nInstead, effective multiturn collaboration requires model responses that optimally contribute to the final reward. The model should aim to align its responses with the user's goal g by considering their impact on the entire conversation trajectory $t_{1:K}$. In the previous example, instead of generating a conclusion, asking, \u201cShould I maintain an engaging tone in the conclusion like the introduction?\u201d offers better long-term alignment with the goal."}, {"title": "3.1. Multiturn-aware Rewards", "content": "In Figure 1, our key insight is that effective multiturn collaboration relies on forward-looking strategies. Given a context \u2460, the model should consider how its response \u2461 influences the subsequent turns of the conversation. To capture this, we design a \u2462 collaborative simulation module to estimate this impact. By \u2463 fine-tuning to distinguish between potential future conversations resulting from different responses, the model generates responses that align better with the overarching goal g.\nThis high-level design naturally aligns with causal effect estimation (Pearl, 2009; Pearl et al., 2016), which evaluates the interventional effects of an action in sequential decision-making. Appendix A provides further discussion on the connection between causal effect estimation and our approach. More specifically, we define the Multiturn-aware Reward:\nMultiturn-aware Reward (MR): The multiturn-aware reward for model response $m_j$ at the j-th turn is given by:\n$MR(m_j|t_h,g) = E_{tf\\sim P(t_{j+1:K}|t_h\\cup \\{m_j\\})} R^* (t_h\\cup \\{m_j\\} \\cup t_f | g) $\n$= E_{tf\\sim P(t_f|t_{1:j})} R^* (t_{1:j} \\cup t_f | g)$,\nwhere $t_{1:j}$ denotes the conversation history up to and including the j-th turn, and $t_f = t_{j+1:K}$ represents the forward trajectory of turns following the j-th turn. The distribution $P(t_f | t_{1:j})$ models the possible forward conversations conditioned on the prior conversation history.\nHowever, computing Equation 1 remains challenging as it requires the following components: (a) A conversation-level reward function, $R^* (t | g)$, for evaluating an arbitrary multiturn conversation t, and (b) a sampling strategy for obtaining forward conversations $P(t | t_{1:j})$, which represents the forward conversation distribution. We elaborate on the two components in Section 3.1.1 and 3.1.2."}, {"title": "3.1.1. CONVERSATION-LEVEL REWARD FUNCTION", "content": "We approximate the conversation-level reward $R^*(t | g)$ with a combination of extrinsic (goal-specific) and intrinsic (goal-agnostic) metrics:\n$R^*(t | g) = R_{ext}(t, g) + R_{int}(t)$,\nwhere $R_{ext}(t, g)$ focuses on task-specific success, and $R_{int}(t)$ evaluates user experience including efficiency and engagement.\n\u2022 Extrinsic Reward $R_{ext}(t, g)$ measures how well the conversation achieves the user's goal g. Formally:\n$R_{ext}(t, g) = S(Extract(t), y_g)$,\nwhere Extract(t) extracts the final solution or response from the conversation t, especially for tasks requiring revisions or multi-step answers. $y_g$ is the reference solution for the goal g, e.g., the ground truth solution for a math problem. And S(\u00b7, \u00b7) evaluates task-specific metrics like accuracy or similarity. This ensures the conversation contributes directly to achieving the desired goal.\n\u2022 Intrinsic Reward $R_{int}(t)$ prioritizes conversations that enhance user experience, defined as:\n$R_{int}(t) = - \\lambda min [ TokenCount(t), 1] + R_{LLM}(t)$,\nwhere we encourage conversational efficiency by penalizing excessive tokens that users read and write, with \u03bb controlling the penalty severity. This efficiency measure is bounded by 1 to maintain balance with other metrics."}, {"title": "3.1.2. FORWARD SAMPLING", "content": "To compute Eq. 1, we require samples from $P(t | t_{1:j})$, the distribution of forward conversation conditioned on the conversation history. A simple approach is to use Monte Carlo sampling, where the conversation is extended turn-by-turn until it concludes. However, this can be computationally expensive for computing reward for every model response. For a scalable approximation, we introduce a window size w as a hyperparameter to limit the maximum number of forward turns considered in $t_f$. This reduces the computational cost while maintaining sufficient context.\nMore importantly, while real-world conversations could be gathered from human participants, sampling multiple forward conversations during training is costly and impractical. To further reduce cost and ensure scalability, we introduce a user simulator U.\nUser Simulator:. A user simulator U : T \u2192 U is a function that maps a given conversation history t \u2208 T to a user response u \u2208 U. Specifically, U generates a probabilistic distribution P(u | t) over possible user responses conditioned on the conversation history t, simulating realistic user behavior.\nSpecifically, we prompt an LLM to role-play as users, explicitly asking the LLM to follow the same language style as the previous user turns, and injecting typical user behaviors. The user simulator operates with an implicit goal g, which it seeks to achieve over the course of the conversation. This design emulates real-world scenarios where users may have evolving needs, limited background knowledge, or require clarification, resulting in naturally unfolding multiturn conversations (Park et al., 2024)."}, {"title": "3.2. Optimization & Synthetic Datasets", "content": "With the conversation-level reward function and forward sampling strategy, we can compute MR for any model response without requiring an additional reward model, which is often costly and slow to train. Unlike traditional single-turn reward approaches, MR explicitly accounts for the impact of a response on future conversations, promoting long-term collaboration.\nFurther, we employ reinforcement learning (RL) methods such as PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2023) to guide the model in navigating complex conversations. By optimizing for higher MR, the model learns to generate responses that enhance overall effectiveness and efficiency by the end of the conversation.\nMoreover, MR can generate high-quality synthetic conversations (cf. Figure 8 in Appendix B) for both supervised fine-tuning (SFT) and DPO. For SFT, it iteratively selects top-ranked responses to build realistic, goal-directed conversation histories. For DPO, it constructs pairwise comparisons by ranking responses at each turn, distinguishing \u201cchosen\u201d and \u201crejected\u201d pairs based on MR scores. The generated synthetic data aligns with multiturn objectives.\nOverall, COLLABLLM enables scalable dataset generation and online RL training without human annotation, making it generalizable across diverse tasks. In Appendix 7, we compare COLLABLLM with related prompting- and training-based approaches, highlighting its contributions."}, {"title": "4. Experimental Setup*", "content": "For fine-tuning and evaluation, we create three multiturn datasets using publicly available data across diverse domains (Hendrycks et al., 2021; Zhuo et al., 2024; Chiusano, 2024): collaborative document editing, coding problem assistance, and multiturn mathematics problem solving.\nTo build a multiturn environment (Figure 3), we employ GPT-4o-mini as a user simulator LLM to role-play realistic user behaviors, given the target problem and conversation history. Our simulation-based evaluations are designed to closely mimic real-world interactions (Park et al., 2024). Unlike traditional single-turn tasks, our setup requires dynamic interactions over multiple turns to achieving a goal. The three interactive datasets are:\nMediumDocEdit-Chat: Document editing requires iterative feedback and refinements across multiple turns to ensure coherence and alignment with user intent. We sample 100 Medium articles as goal documents, which are summarized into target problems to guide the user simulator. After each interaction, task performance is evaluated using the BLEU score, measuring similarity between the extracted document and the original articles.\nBigCodeBench-Chat: Coding tasks inherently require multiturn interactions, such as clarifying requirements and debugging. We sample 600 coding problems from BigCodeBench (Zhuo et al., 2024) as the target problems given to the user simulator. For evaluation, we compute the average Pass Rate (PR) of code at the end of the interactions.\nMATH-Chat: Math problem solving often requires addressing implicit assumptions, verifying intermediate steps, and clarifying reasoning. We sample 200 level-5 math problems from MATH (Hendrycks et al., 2021) to prompt the user simulator, which interacts with the LLMs. Task success is measured by the accuracy (ACC) of the final solution, as evaluated by an LLM judge.\nIn addition to the above task-specific metrics, we incorporate two task-agnostic scores across all datasets: 1) Average Token Count, which quantifies the average number of tokens generated by the LLM per conversation, reflecting interaction efficiency. 2) Interactivity (ITR), which evaluates engagement levels using an LLM judge (Claude-3.5-Sonnet), with scores rescaled to the range [0, 1].\nFine-tuning COLLABLLMS. COLLABLLMs are based on Llama-3.1-8B (Llama Team, 2024) with LoRA fine-tuning (Hu et al., 2022). We train four model variants: 1) Offline models: SFT and Offline DPO are fine-tuned on pre-generated multiturn conversational datasets guided by Multiturn-aware Rewards (MR) (cf. Section 3.2). 2) Online models: PPO and Online DPO are further trained from the SFT and Offline DPO models, respectively. The model during online fine-tuning is involved in the collaborative simulation to compute MRs, which, in turn, dynamically adjust the model preference.\nBaselines. We compare COLLABLLMs against (1) the pretrained Llama-3.1-8B (Base), (2) the base model with proactive prompt engineering (Proactive Base), which encourages follow-up and clarification questions."}, {"title": "5. Results of Simulated Experiments", "content": "We present the results in Table 1 and the takeaways are:\nPrompt engineering is helpful, but limited in terms of performance gains and flexibility. Proactive Base improves base model performance by encouraging follow-up questions and clarifications. For example, it increases BLEU on MediumDocEdit-Chat from 32.2% to 35.0% and reduces read tokens by 0.31k compared to the base model. However, these gains are modest and do not fully address the challenges of multiturn collaboration. We observe that prompting strategies remain rigid, relying on predefined instructions rather than adapting dynamically to user needs. For instance, the model sometimes asks clarification questions even when unnecessary, leading to redundant interactions that disrupt conversation flow.\nCOLLABLLM increases task performance, conversational efficiency, and engagement. COLLABLLM achieves 18.5% superior task-specific performance, 13.3% more efficient conversations, and 46.3% enhanced interactivity compared to the best baselines. We highlight that COLLABLLM engage in more meaningful collaborations, with ITR shows substantial gains. For MediumDocEdit-Chat, the Online DPO model increases ITR from 0.46 to 0.92. Moreover, our framework significantly improves conversational efficiency by minimizing the content users need to review to arrive at the final solution. For MATH-Chat, Online DPO decreases token count per conversation by 1.03k compared to the base model."}, {"title": "5.1. Ablations on Reward Mechanisms (Figure 4)", "content": "To investigate how components contribute to COLLABLLM's superior performance, we conduct an ablation study focusing on the reward mechanisms used during fine-tuning. We evaluate the following reward mechanisms:\n\u2022 Variants of Multiturn-aware Reward: We vary the forward sampling window size w = 1,2,3 to assess their ability to capture long-term conversational effects through simulated collaborations.\n\u2022 Immediate Rewards evaluate the model's immediate response based on: 1) Helpfulness: Assessed by an LLM judge; 2) Extrinsic Reward: Focuses on task-specific metrics like BLEU while ignoring intrinsic factors such as efficiency; 3) Extrinsic + Intrinsic Reward: Combines task-specific metrics with efficiency and interactivity measures. This can be seen as a special case of the multiturn-aware reward function with w = 0.\nWe present results in Figure 4. Interestingly, expanding the forward sampling window w within the range generally enhances performance and efficiency by better capturing future interactions. Notably, MR with w = 2 balances the gains and additional costs to conduct forward sampling, making it well-suited for large-scale fine-tuning. In contrast, immediate rewards, even with extrinsic and intrinsic components, fall short as they ignore long-term impact. These findings validate the positive impact of the forward sampling strategy in MRs."}, {"title": "5.2. Case Study (Figure 5 & 6)", "content": "We now offer a deeper insight into COLLABLLM's behavior as shown in Figure 5. In this example, the user request to tokenize a text file is inherently open-ended due to unspecified factors, such as the NLTK environment, tokenizer selection, and optional preprocessing steps. The base LLM makes several arbitrary assumptions, applying lowercase conversion and stopword removal without user confirmation. The user simulator later corrects these assumptions, but the final solution remains incorrect due to missing stopwords. In contrast, COLLABLLM actively clarifies user intent by seeking confirmation on key decisions, ensuring an aligned final solution with a 100% Pass Rate. This approach also reduces user effort with lower token usage.\nIn Figure 6, we compare different reward mechanisms for responses A and B of Figure 5, to confirm that these rewards work as intended. The helpfulness rewards favor response A due to its seemingly more well-round output. Extrinsic rewards assign zero scores to both, as A provides an incorrect solution and B defers answering. Extrinsic + Intrinsic rewards slightly favor B for efficiency and engagement. Interestingly, MR assigns significantly higher rewards to B, especially at w = 2 and w = 3, since the response obtains useful information and provide a precise answer within the future interaction window."}, {"title": "5.3. Model Generalization (Table 2)", "content": "Modern foundation models are expected to generalize across a diverse range of tasks beyond their training domain. A key question is whether collaborative behaviors learned by COLLABLLM during fine-tuning transfer effectively to new tasks without additional adaptation.\nWe assess COLLABLLM, trained with online DPO on BigCodeBench-Chat (the coding assistance task), on Abg-CoQA, a question-answering (QA) benchmark where questions are labeled as ambiguous or non-ambiguous (cf. Appendix D). We categorize the model's responses into two actions\u2014asking a clarifying question or providing a direct answer\u2014and evaluate action-level accuracy within each question type. As shown in Table 2, GPT-4o and Llama-3.1-8B rarely ask clarifying questions regardless of ambiguity. In contrast, COLLABLLM proactively asks questions about 50% of the time while maintaining high accuracy on unambiguous inputs. This behavior leads to the highest Macro Accuracy across both ambiguous and non-ambiguous sets and improves Macro F1 over the base model, while leaving room for further improvement against GPT-4o. These results suggest that COLLABLLM effectively generalizes its learned collaborative strategies beyond its training domain."}, {"title": "6. Real-world User Study", "content": "Setup. We conduct a large-scale user study using Amazon Mechanical Turk with 201 participants. Each participant is assigned a document type\u2014randomly selected to be either blog post, creative writing, or personal statement\u2014and chooses a topic from a predefined set. To simulate real-world scenarios where users have only a rough idea of the task, they are first asked to provide brief responses to topic-related questions. Participants then engage in at least eight turns of conversation with an anonymized AI assistant, which can be Base, Proactive Base, or COLLABLLM. Every three turns, they provide an interaction rating based on their experience so far. After the conversation, participants rate the final document quality and overall interaction. All ratings are in a scale from 1 to 10. We also record the total interaction duration to assess efficiency. The detailed user study setup is provided in Appendix E.\nQuantitative Results (Figure 7). Across multiple metrics, COLLABLLM consistently outperforms the baselines. It achieves an average document quality score of 8.50. Specifically, 91.4% of participants rate COLLABLLM's document quality as \u201cgood\u201d (score 8-9), and 56.9% as \u201cvery good\u201d (score 9\u201310), compared to 88.5% and 39.3% for Base (Llama-3.1-8B), respectively. Similarly, 63.8% of participants find COLLABLLM highly engaging, while only 42.6% report the same for Llama-3.1-8B.\nInterestingly, for multiturn interaction, the Base model shows a declining trend in ratings from turns 6-9, indicating reduced user experience in longer conversations. In contrast, both COLLABLLM and Proactive Base exhibit increasing ratings over time, with COLLABLLM consistently achieving higher average ratings every three turns compared to Proactive Base. This suggests that COLLABLLM maintains sustained engagement more effectively.\nMoreover, COLLABLLM improves task efficiency, reducing time spent by 10.4% compared to the Base model and by 15.6% relative to Proactive Base. While Proactive Base is prompted to maintain conciseness, it frequently asks unnecessary questions, causing lower efficiency. In contrast, COLLABLLM strikes a more streamlined user experience.\nQualitative Results (Table 3). We collected a total of 180 strengths and 180 weaknesses across the three models. Table 3 presents representative feedback, while we summarize here the mddels' strengths and weaknesses: The base model generates coherent content while effectively follows user instructions, but it sometimes struggles with maintaining context in long texts, and can be overly verbose or repetitive in its responses. Proactive Base excels in responsiveness and adapting to user input but struggles with memory retention, and could produce repetitive or overly structured content. On the other hand, COLLABLLM is highly engaging, effectively guiding users through writing, adapting seamlessly to feedback. However, users also point out that COLLABLLM can occasionally feel bland, lack of up to date information, and require additional effort to personalize the output. Overall, COLLABLLM enhances collaboration by guiding users through an interactive and iterative refinement process, yet future improvements should focus on increasing personalization, creativity, and real-time knowledge integration to further optimize human-LLM collaboration."}, {"title": "7. Related Work", "content": "Non-collaborative LLM training. Existing LLM training frameworks, including pre-training, supervised fine-tuning (SFT), and reinforcement learning (RL) (Rafailov et al., 2023; Schulman et al., 2017; Ouyang et al., 2022; Lee et al., 2024), primarily optimize for next-turn response quality. Standard RL methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) apply rewards to individual model responses without accounting for their long-term impact on conversation trajectories. While effective for single-turn objectives, these approaches fail to capture how responses influence user intent discovery and long-term task success (Amershi et al., 2019; Zamfirescu-Pereira et al., 2023; Wang et al., 2024; Kim et al., 2024; Lu et al., 2024).\nPrompting techniques for multiturn interaction. Prior work has explored prompting strategies to enhance LLM interactivity, particularly for clarification questions (Keh et al., 2024; Mu et al., 2023; Zhang & Choi, 2023; Chi et al., 2024; Kim et al., 2023; Deng et al., 2023b; Zhao & Dou, 2024) and mixed-initiative dialogues (Deng et al., 2023a; Chen et al., 2023; Liao et al., 2023). For instance, Mu et al. (2023) prompt LLMs to ask clarification questions when code generation requests are ambiguous. However, such prompting-based approaches are constrained by predefined interaction patterns, limiting adaptability across different tasks and conversation stages. Moreover, their reliance on fixed prompts reduces generalization, as demonstrated in our experiments where proactive prompting fails to match the effectiveness of our fine-tuned models.\nLearning-based methods for multiturn interaction.\n\u2022 LLMs for generating clarification questions: Beyond prompting, prior studies have explored supervised fine-tuning (Andukuri et al., 2024), RL fine-tuning (Chen et al., 2024; Zamani et al., 2020; Erbacher & Soulier, 2023), and active learning (Pang et al., 2024) to train models to ask clarification questions. For example, Chen et al. (2024) use Direct Preference Optimization (DPO) to encourage models to identify ambiguous user queries and request clarifications. However, like prompting approaches, these methods primarily focus on clarification questions and do not generalize to broader multiturn collaboration strategies.\n\u2022 Multiturn training for LLMs: Several studies extend RLHF to multiturn settings by optimizing trajectory-level rewards (Shani et al., 2024; Zhou et al., 2024; Gao et al., 2024; Shi et al., 2024b). Other works (Xu et al., 2023; Deng et al., 2024) leverage self-chat or self-play to enhance model adaptation to dialogue. See Zhang et al. (2025) for a comprehensive survey. However, these methods primarily rely on post-hoc trajectory-level data, learning from observed conversations rather than explicitly modeling the causal effect of individual responses on long-term task success (see Appendix A for further details). Additionally, they often overlook open-ended tasks such as document generation (Faltings et al., 2023; Jiang et al., 2024), where real-world user responses can be highly variable, and users may have limited capacity to read and refine lengthy model outputs."}, {"title": "8. Conclusion", "content": "Multiturn human-LLM collaborations are increasingly prevalent in real-world applications. We believe future foundation models should act as collaborators rather than passive responders, actively uncovering user intents in open-ended and complex tasks\u2014an area where current LLMs fall short. The key insight of COLLABLLM is making LLMs more multiturn-aware by using forward sampling to estimate the long-term impact of responses. Through extensive simulated and real-world evaluations, we demonstrate that COLLABLLM is highly effective, efficient, and engaging, while also generalizing well to new tasks and interactions, advancing the frontiers of human-centered LLMs."}, {"title": "Impact Statement", "content": "This paper presents work aimed at making AI more user- and human-centric, which, in our view, yields a positive societal impact. Most current work on AI and its evaluation focuses on fully automated tasks, with no user involvement in solving the task or optimization for a collaborative experience with users. This has serious societal drawbacks, given issues such as AI hallucinations (Huang et al., 2025), biases (Gallegos et al., 2024), and unsafe language (Shi et al., 2024a) that arise from a lack of human oversight. The common focus on having AI models autonomously complete tasks also ignores the reality that many scenarios have humans present regardless of the level of automation, and that not priming AI models to proactively seek human help, feedback, or clarifications misses an opportunity to make generative AI more accurate, effective, and safe. This consideration would also help increase the adoption of AI in safety-critical scenarios, such as medical decision-making tasks (Liu et al., 2024), in which we believe Al models should be inclined to seek confirmation or verification (Gero et al., 2023) from an expert in case of uncertainty-a behavior that is mostly absent in current state-of-the-art LLMs.\nThis work presents one of the first attempts to train LLMs in such human-centric environments. To promote future research in this societally beneficial direction, we plan to release all the code, models, data, benchmarks, and user simulators described in this work.\nThe data collected in our study involves human participants recruited through Mechanical Turk. We took several measures to ensure the privacy of these workers in the document creation tasks. First, we asked workers to confirm that they were willing to share the text they wrote as part of a public dataset. Second, we urged them not to include any personally identifiable information (PII) in their writings and to focus only on topics of public knowledge or fictitious stories. Third, we scanned the collected data to ensure that no PII was included. For the final version of the dataset, we will recruit additional workers to manually review each collected conversation to ensure that no PII or other safety issues (e.g., offensive language) exist in the data. Mechanical Turk workers were paid $10 per conversation. Given that conversations averaged 28.4 minutes, including break times, this means workers were paid more than $20 per hour on average\u2014above the minimum wage in the country where the data was collected."}, {"title": "A. Supplementary Discussion", "content": "A.1. Connection Between Multiturn-aware Reward and Causal Inference\nOur approach naturally aligns with causal inference principles, as it aims to quantify how a model's response influences the future trajectory of a conversation. This aligns with the fundamental goal of causal effect estimation, which seeks to isolate the impact of an intervention\u2014in this case, a model response\u2014on long-term outcomes.\nFrom a causal perspective, given a conversation history $t_h$ at turn j, the causal effect of a model response $m_j$ on the final conversation trajectory can be expressed using front-door adjustment (Pearl, 2009; Pearl et al., 2016):\n$\\sum R^* (t_{1:K} | g)P(t_{1:K}|t_h)P(t_h) = \\sum R^* (t_{1:K} | g)P(t_{1:K} | t_h) = E_{t_{1:K}\\sim P(t_{1:K}|t_h)} R^* (t_{1:K} | g)$.$\nThis equation captures the expected long-term reward of a conversation conditioned on the model's response at turn j. It explicitly accounts for how $m_j$ intervenes in the conversation, influencing future turns and, ultimately, task success.\nA.2. Distinction from Other Multiturn Training Frameworks\nExisting multiturn trajectory-based training frameworks (Shani et al., 2024; Zhou et al., 2024; Gao et al., 2024) primarily rely on learning from observed trajectory-level rewards. These methods estimate the utility of responses by assigning rewards post hoc to completed conversations, training models to imitate historically successful responses. However, this approach is fundamentally correlational\u2014it captures statistical associations between responses and final outcomes but does not explicitly model how different responses causally influence future turns.\nIn contrast, our Multiturn-aware Reward (MR) framework intervenes on model responses and uses forward sampling to explicitly simulate alternative future trajectories. This enables the model to estimate the counterfactual impact of different responses and adjust its behavior accordingly. By leveraging causal effect estimation, MR training moves beyond passive imitation of high-reward conversations and instead actively optimizes responses to maximize long-term goal attainment. This distinction is crucial in dynamic human-LLM interactions where user needs evolve throughout a conversation."}]}