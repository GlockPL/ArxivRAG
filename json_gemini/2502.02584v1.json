{"title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search", "authors": ["Zongyu Lin", "Yao Tang", "Xingcheng Yao", "Da Yin", "Ziniu Hu", "Yizhou Sun", "Kai-Wei Chang"], "abstract": "Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a exploration tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically show that QLASS can lead to more effective decision making through qualitative analysis.", "sections": [{"title": "1. Introduction", "content": "Supervised fine-tuning (SFT) is commonly employed to make base LLMs perform effective reasoning and planning in complex agent tasks by imitating expert trajectories (Chen et al., 2023; Yin et al., 2024). However, the substantial hu-"}, {"title": "2. Related Work", "content": "2.1. Large Language Model Agent\nLarge language models have shown impressive performance in complex interactive tasks, such as web navigation (Yao et al., 2022), scientific reasoning (Wang et al., 2022a), and action planning in embodied environments (Shridhar et al., 2021). ReAct (Yao et al., 2023) developed a prompting method to shape language models as agents that can reason and act. While several works (Shen et al., 2024; Song et al., 2023) improve agent performance with closed-source LLM controllers, the open-source LLM agents still offer unique advantages like accessibility and customization. FireAct (Chen et al., 2023) and LUMOS (Yin et al., 2024) leverage high-quality data generated by experts and employ teacher-forcing to improve the performance of open-source agents. In line with this, our QLASS is also based on open-source LLMs.\n2.2. Self-Improvement for LLM\nThe self improvement of LLM can be a good way to improve LLM without heavy human annotation, which can be divided into two parts. (1) Training models on self-generated data is a promising approach. A large number of works (Dou et al., 2024; Wang et al., 2022b; Yuan et al., 2023; Singh et al., 2023; Gulcehre et al., 2023; Wang et al., 2023) follow the paradigm of self-training, which filters positive self-generated data and performs model training on those filtered data. Some other works (Song et al., 2024;"}, {"title": "2.3. Process Reward Modeling for LLM", "content": "Existing works have explored various strategies and reasoning policies for process reward modeling. (Uesato et al., 2022) and (Lightman et al., 2023) utilize human-annotated step-level correctness to train a reward model. Math-Shepherd (Wang et al., 2023) infers per-step rewards through random rollouts. TS-LLM (Feng et al., 2023) employs an MCTS-based policy and infers per-step rewards using the TD-X (Sutton, 1988) method. ReST-MCTS* (Zhang et al., 2024) uses Monte Carlo tree search (MCTS) with re-inforced self-training to enhance the diversity and performance on general reasoning tasks like maths, science, and code. Most recently, Wang et al. (2024) and Zhai et al. (2024) also use step-level guidance for agent inference through training a step-level value model. Putta et al. (2024) applies a hybrid process reward modeling for web navigation tasks by combining Monte Carlo Tree Search (MCTS) rewards with scores generated by large language models to form process rewards. Our approach focuses on solving complex agent tasks by providing effective per-step guidance for LLM agent inference. Our method differs from Putta et al. (2024) because we do not rely on a strong proprietary LLM to provide rewards. Compared with Wang et al. (2024) and Zhai et al. (2024), we shift our focus on more complex agent tasks with larger search space and deeper search depth like ALFWorld and SciWorld. Compared with Zhang et al. (2024), our framework is much simpler with less stages of training and more straightforward to make process reward modeling works better compared with strong training-based baselines."}, {"title": "3. Preliminaries", "content": "In this section, we introduce Q-learning, the key algorithm that inspires QLASS to extract Q-values from the exploration trees. Q-learning (Watkins & Dayan, 1992) is a traditional model-free reinforcement learning algorithm, where a value function $Q(s, a)$ is trained to represent the expected future rewards by taking action a given state s. The optimal"}, {"title": "4. QLASS Pipeline Details", "content": "In this section, we will follow the order of QLASS pipeline and introduce each critical component step by step. The overall pipeline is shown in Figure 1 and Algorithm 1.\nFirst, we will describe the initial stage of behavior cloning. Then, we will explain how the exploration tree is constructed during the second self-generation stage and how we use it to extract Q-values as the supervision to train Q-network (QNet Training). Finally, we will detail the Q-guided generation how the QNet is employed to guide the agent's test-time inference in a stepwise manner.\n4.1. Behavioral Cloning\nBehavior cloning provides a strong initial foundation for language agents by supervised fine-tuning on expert trajectories. Formally, the first stage of QLASS is to supervised"}, {"title": "4.2. Constructing an Exploration Tree", "content": "The supervised fine-tuned agents can explore the environment and collect a large amount of trajectories. However, due to the extremely large search space of language agents, directly sampling trajectories without any guidance may lead to low efficiency. To address this issue, we propose to construct an exploration tree during self-generation.\n4.2.1. TREE STRUCTURE\nFor a trajectory, we take the task description as the root node, and each node below the root node is composed of the state, action, and related information for each step. For all trajectories of a task, they can be seen as different branches that originate from the same root node."}, {"title": "5. Experiment", "content": "In this section, we aim to evaluate the effectiveness of QLASS for solving complex agent tasks in the following aspects: 1) whether QLASS can aid better decision making on different complex agent tasks; 2) whether the Q-value in QLASS is an effective process reward to facilitate self-improvement; 3) whether QLASS can retain strong performance with reduced annotated data.\n5.1. Setup\nDatasets. We assess the ability of QLASS on WebShop (Yao et al., 2022), ALFWorld (Shridhar et al., 2021) and SciWorld (Wang et al., 2022a). These environments only provide a single outcome reward at the end of each trajectory. The statistics of three agent datasets are displayed in Table 1. The evaluation metric is the reward averaged on the test sets. During the sampling process, environments will give a termination signal when certain actions like \"Click[Buy Now]\" in Webshop are taken or the set maximum steps are reached. Details can be found in Appendix A.2.\nTraining Setup. In our work, we mainly use Llama-2-7B-Chat as base policy model and QNet backbone. We train our models mainly using 4 or 8 A6000 GPUs. The experiments on Webshop, including the training of SFT model, QNet, self-generation and Q-guided exploration, takes one or two days and the experiments on ALFWorld and SciWorld takes four or five days. The detailed hyper-parameters for training and model architectures can be found in Appendix A.2.\nBaselines. 1) SFT (Chen et al., 2023) is the base agent after supervised fine-tuning on the expert data. 2) RFT (Rejection sampling Fine-Tuning) (Yuan et al., 2023) is a self-improvement baseline which is trained on the merged data consisting of successful trajectories sampled and expert data. 3) ETO (Song et al., 2024) is a self-improvement base-"}, {"title": "5.2. Evaluation Results", "content": "In this section, we compare the performance of our QLASS with all the baselines on WebShop, SciWorld, and ALF-World. We evaluate all algorithms using one-shot evaluation. The decoding temperatures are set to 0.7 for QLASS and Best-of-N and 0 for other baselines.\nOverall Baseline Comparison. Results are summarized in Table 2. From Table 2, we can observe that QLASS consistently achieves the highest scores among all the open-sourced baselines, including both training-based methods and inference-based methods. QLASS also demonstrates comparable performance with the best proprietary baselines. Specifically, GPT-4 is the state-of-the-art model, but QLASS still outperforms it on all three benchmarks by 17.9% on average, especially on SciWorld and ALFWorld. Also, QLASS outperforms ETO and PPO consistently by over 5% on average, which are two strong baselines based on multiple stages of training, including supervised fintuning on expert trajectories, training reward models and doing DPO or PPO on the explored trajectories. We achieve better performance while avoiding the heavy cost (including the hyperparameter tuning on DPO / PPO).\nInference-time Search Efficiency. We compare QLASS and Best-of-N under different search budgets and visualize the results in Figure 3. We find that increasing the number of completion tokens will improve the performance of all inference methods. We can observe that QLASS is consistently better than Best-of-N under almost all the search budgets. Another notable observation is that compared with Best-of-N (68.4) under 400K tokens, QLASS (70.3) with only about half of search budgets under 240k tokens, outperforms the highest score of Best-of-N (68.4). Also, as the completion tokens approach 360K, Best-of-N begins to flatten, while the score of QLASS still gets improved by a relatively larger margin from 360K tokens to 400K tokens. This indicates that our approach is a more effective way to"}, {"title": "5.3. Fewer Annotations", "content": "In many real-world applications, collecting large amounts of expert-annotated data is both time-consuming and costly. To evaluate the effectiveness of our approach under such constraints, we designed this setup with fewer annotations to test how quickly the agents adapt to new environments in this section. We extract 1000 trajectories as a subset from the original 1938 trajectories. Under this setup, all baselines"}, {"title": "5.4. Case Study", "content": "We pick out an example from ALFWorld in Figure 5 to showcase the difference between baselines and our models. The SFT agent correctly picks up the lettuce, cools it using the fridge, and places it on the countertop in the beginning."}, {"title": "5.5. Ablations Across Different Base Policy Models", "content": "To validate the robustness of our method across different model architectures, we also conduct experiments on a large base model: Llama-2-13B. As shown in Table 4, QLASS still outperforms the baseline reported in Song et al. (2024) on the SciWorld benchmark."}, {"title": "6. Conclusion", "content": "In this paper, we introduce QLASS, a novel approach that enhances open-source language agents at inference time by integrating Q-value-based process guidance. By modeling the Q-value at each intermediate step during planning, our method offers step-wise feedback that surpasses the limitations of outcome-based reward models, particularly in complex, long-horizon tasks. Through extensive experiments, we have demonstrated that QLASS significantly"}, {"title": "A. Appendix", "content": "A.1. Discussion\nWhy supervised train the offline QNet using LLM as the backbone instead of directly using deep Q-learning? Directly applying deep Q-learning (Watkins & Dayan, 1992) to language agents face critical challenges. First, the action space (all possible text outputs) is unbounded and orders of magnitude larger than typical RL environments (e.g., Atari's 18 discrete actions). Standard exploration strategies like e-greedy fail because random text sampling rarely yields meaningful rewards or trajectories. Second, language tasks often involve sparse rewards, destabilizing Q-learning's reliance on frequent reward signals. Pure online Q-learning would suffer from high gradient variance and require infeasible exploration budgets.\nInitializing the value function model from a well-pretrained large foundation model can encode rich linguistic and reasoning priors, as well as world commonsense knowledge (Bansal et al., 2024; Song et al., 2024; Xu et al., 2022; Wang et al., 2023). So we initialize our QNet with the LLM trained in the agent environment to embrace both knowledge during pretraining and agent specific capabilities, thus boosting the adaption to the long-term value modeling.\nA.2. Experimental details\nA.2.1. DATASETS\nWe follow the setup of ETO (Song et al., 2024) to use the three agent tasks for our experiments.\n(a) WebShop is an online shopping environment. The available action types for agents include search[keywords] and click[value]. The agent is instructed to complete the task with ReAct(Yao et al., 2023)-style response. The instruction is specified in Figure 6.\n(b) ALFWorld (Shridhar et al., 2021) consists of interactive TextWorld environments paralleling the embodied worlds. In this setup, agents must explore and complete complex household tasks. The ALFWorld dataset includes both seen and unseen evaluation sets. The seen set tests in-distribution generalization, while the unseen set evaluates out-of-distribution generalization, featuring entirely new task instances for the agents to solve.\n(c) SciWorld (Wang et al., 2022a) is a text-based virtual platform designed around conducting basic scientific experiments across ten task categories, such as thermodynamics and electrical circuits. Agents engage in embodied, interactive environments to grasp scientific concepts through practical tasks. Each task in ScienceWorld includes optional subgoals, with the final reward calculated based on the achievement of these subgoals.\nWe have summarize the statistics of SFT datasets for behavior cloning on all the environments in the main body. Note that the default reward from the environment is zero for the intermediate step before terminal. For self-generation and tree construction, we set the maximum step as 5 in WebShop and 18 in ALFWorld and SciWorld. For inference, we set the maximum step as 5 in WebShop and 40 in ALFWorld and SciWorld. The instruction templates are displayed in Figure 6, 7 and 8.\nA.2.2. QNET\nModel Architecture. Our QNet is designed by sharing the backbone of the Large Language Model (LLM) and appending a value head to predict Q-values. Specifically, we utilize a pre-trained LLM, denoted as LLM, which serves as the foundational model for encoding input sequences. The value head is a Multi-Layer Perceptron (MLP) that takes the hidden states from the LLM and outputs scalar Q-value predictions."}, {"title": "A.3. Algorithms", "content": "A.3.1. PSEUDOCODE OF EXPLORATION TREE CONSTRUCTION AND Q-VALUE DISTILLATION\nIn this section, we provide the pseudocode of constructing an exploration tree in stage 2 in Algorithm 2 and and how we distill the Q-value from an exploration tree in Algorithm 3."}]}