{"title": "GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale Image Super-Resolution", "authors": ["Jintong Hu", "Bin Xia", "Bin Chen", "Wenming Yang", "Lei Zhang"], "abstract": "Implicit neural representations (INRs) have significantly advanced the field of arbitrary-scale super-resolution (ASSR) of images. Most existing INR-based ASSR networks first extract features from the given low-resolution image using an encoder, and then render the super-resolved result via a multi-layer perceptron decoder. Although these approaches have shown promising results, their performance is constrained by the limited representation ability of discrete latent codes in the encoded features. In this paper, we propose a novel ASSR method named GaussianSR that overcomes this limitation through 2D Gaussian Splatting (2DGS). Unlike traditional methods that treat pixels as discrete points, GaussianSR represents each pixel as a continuous Gaussian field. The encoded features are simultaneously refined and upsampled by rendering the mutually stacked Gaussian fields. As a result, long-range dependencies are established to enhance representation ability. In addition, a classifier is developed to dynamically assign Gaussian kernels to all pixels to further improve flexibility. All components of GaussianSR (i.e., encoder, classifier, Gaussian kernels, and decoder) are jointly learned end-to-end. Experiments demonstrate that GaussianSR achieves superior ASSR performance with fewer parameters than existing methods while enjoying interpretable and content-aware feature aggregations.", "sections": [{"title": "1. Introduction", "content": "The vision world is continuous, presenting scenes and objects in their natural, uninterrupted forms. However, computer vision tasks predominantly employ pixel-based discrete representations for image processing, which inherently constrains the ability capture the continuous nature with high fidelity across different resolutions. The emergence of implicit neural representations (INR) has revolutionized this challenge by treating images as continuous functions. Pioneered by LIIF [6], INR-based methods leverage Multi-Layer Perceptrons (MLPs) to model the RGB values of high-resolution (HR) images as continuous functions of low-resolution (LR) features and pixel coordinates. This continuous representation enables LIIF to perform super-resolution at arbitrary scales without the need for creating and training separate models for each scaling factor. Building upon LIIF, numerous variants and extensions have subsequently emerged [3, 18, 44, 47], further advancing the capabilities of INR-based Arbitrary-Scale Super-Resolution (ASSR).\nAlthough INR-based ASSR methods have demonstrated their effectiveness, several intrinsic limitations remain. Latent codes are stored individually, which necessitates the use of Multi-layer Perceptrons (MLPs) to interpolate these discrete features into a continuous output. This process may not fully preserve key physical properties of the scene, such as lighting and texture consistency, reducing the overall interpretability and physical fidelity of the generated images. Moreover, as depicted in Figure 1(a), when query location Xq moves in 2D domain, the selection of z\u0142 can abruptly switch from one to another if xq crosses the dashed lines, potentially leading to checkerboard artifacts in the output image. To counteract this, a strategy involving a weighted combination of nearby latent codes is employed, which, while effective, increases the computational load and may become a bottleneck for real-time applications.\nHow about continuously storing latent codes? After discussing the limitations of existing INR-based methods in handling 2D image super-resolution (SR) with continuous integrity, we explore the potential application of concepts derived from 3D reconstruction technologies to address these issues. Inspired by the recent advancements in 3D Gaussian"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Implicit Neural Representation", "content": "Implicit Neural Representations (INRs) elegantly model signals across a continuous domain and have gained prominence in numerous fields, including 3D reconstruction [8, 14, 27, 30, 34], scene rendering [31,33,37, 41], robotics [4, 20, 36], and image and video compression [5, 38, 51]. Notably, NeRF [28] innovatively represents complex 3D scenes with continuous neural implicit functions and has enabled novel view synthesis, although it faces challenges with dynamic scenes due to its static nature and lacks explicit geometric representations. Attempting to address dynamic"}, {"title": "2.2. 3D Gaussian Splatting", "content": "3D Gaussian splatting (3DGS) [16] serves as a groundbreaking technique in computer graphics, offering an explicit scene representation through millions of learnable 3D Gaussians, a stark contrast to NeRF's [28] implicit, coordinate-based approach. This state-of-the-art methodology harbors the promise of real-time rendering in addition to unparalleled degrees of controllability and editability. The practicality of 3DGS transcends the traditional confines, proving useful for simultaneous localization and mapping [15, 26, 45], dynamic scene modeling [23, 42, 48], AI-generated content [7, 13, 53], and autonomous driving [46, 52]. While 3DGS has significantly advanced 3D-related tasks, its application to image super-resolution had remained untapped before our work. Our pioneering work adapts the Gaussian Splatting to arbitrary-scale super-resolution, an area traditionally ruled by LIIF [6] and its variants [18,44,47], with our approach enhancing quality with reduced parameters.\nOur innovations set a path for integrating Gaussian representation into the broader field of image restoration."}, {"title": "2.3. Arbitrary-Scale Super-Resolution", "content": "Traditional single image super-resolution (SISR) techniques, such as interpolation, have been outperformed by deep learning-based methods, notably Convolutional Neural Networks (CNNs), due to their ability to learn hierarchical features and map LR to HR images. Various CNN architectures have been proposed, including sub-pixel convolutional layers, pyramid frameworks, memory blocks, residual blocks, and dense connections, etc [9,17,21,35,39,50]. However, these methods lack flexibility as they are designed for fixed integer-scale upsampling, limiting their applicability. Arbitrary-scale super-resolution (ASSR) techniques have gained prominence for their adaptability across various scaling factors. MetaSR [11] pioneered CNN-based ASSR, while LIIF [6] introduced an innovative framework leveraging implicit neural representations to treat images as continuous functions. Subsequent studies, such as LTE [18], SADN [43], and A-LIIF [19], have aimed to address limitations like spectral bias, multi-scale feature integration, and artifact mitigation. However, these methods treat all features discretely without accounting for inherent variations, lack adaptive receptive fields to naturally and explicitly handle inputs of diverse scales, and rely on generic kernel weights that cannot adapt to individual samples during inference."}, {"title": "3. Model Architecture", "content": "Figure 2 depicts GaussianSR's architecture. Section 3.1 reviews the theoretical foundation of 3D Gaussian Splatting"}, {"title": "3.1. Preliminary: 3D Gaussian Splatting", "content": "3D Gaussian splatting (3DGS) [16] represents a paradigmatic shift from the prevalent neural radiance fields (NeRF) [28] methodology for scene representation and rendering. Grounded in a fundamentally distinct approach, 3DGS circumvents the computational complexities and controllability limitations inherent to NeRF while retaining the capability to synthesize photorealistic novel views from sparse input data. The forward process of 3DGS can be summarized as: \u2022 3D Gaussian Representation. The scene is represented using a collection of 3D Gaussians, each characterized by learnable properties such as position, opacity, covariance matrix, and color. This explicit representation allows efficient rendering through parallelized workflows. \u2022 Splatting and Tiling. The 3D Gaussians are first projected onto the 2D image plane through a splatting process. The image is then divided into nonoverlapping patches or \"tiles\" to facilitate parallel computation. \u2022 Sorted Gaussian Rendering. The projected Gaussians are sorted by depth within each tile, and the final pixel color is computed by alpha compositing, leveraging the sorted order. The impact of a 3D Gaussian i on an arbitrary 3D point p in 3D is defined as follows:\n$f_i(p) = \\sigma(\\alpha_i) \\exp(-\\frac{1}{2}(p-\\mu_i)^T \\Sigma_i^{-1}(p - \\mu_i))$ (1)\nwhere p represents an arbitrary point in a 3D Cartesian coordinate system. The 3D Gaussian i is parametrized by (1) mean \u03bci, (2) covariance \u03a3i, (3) opacity \u03c3(\u03b1), (4) color parameters ci, either 3 values for (R, G, B) or spherical harmonics coefficients. The image formation model of Gaussian"}, {"title": "3.2. 2D Gaussian Splatting", "content": "Although 3DGS [16] has shown remarkable performance in various 3D tasks, its inherent 3D formulation may not be optimally suited for certain 2D tasks such as image restoration. Motivated by the need for continuous representation, we propose a 2D adaptation of Gaussian splatting, termed 2D Gaussian Splatting (2DGS). Our 2DGS simplifies the approach compared to 3DGS by eliminating operations and parameters specific to 3D scene rendering, such as project transformations and spherical harmonics, streamlining the overall process and reducing complexity.\nThe process of 2DGS consists of two main steps: \u2022 Grid Transformation. Initially, we create an affine-transformed grid that takes into account the standard deviations of all Gaussian kernels. This grid is tailored to match the high resolution (HR) dimensions of the target output, preparing the scene for subsequent feature mapping. \u2022 Feature Value Rendering. Following the grid setup, the feature values undergo alpha blending, and then these blended values are projected onto the transformed grid for rendering. This step ensures that the upsampled image preserves natural continuity and visual integrity.\nIn our framework, the image representation unit is represented as a 2D Gaussian defined by its position \u03bc\u03b5 \u2208 R2 and the covirance matrix \u03a3\u2081 = ([$\u03a3_{xi}, Pi], [pi, $\u03a3_{yi}]) \u2208 R2\u00d72. To prevent \u03a3\u03ba, \u03a3y from being negative, we apply the sigmoid"}, {"title": "3.3. Arbitrary-Scale Super-Resolution Pipeline", "content": "The shift from a discrete feature storage methodology to a continuous feature field, characterized by a Gaussian distribution, represents a significant advancement in the field of image restoration. This transformation enables any point within the feature field to be explicitly determined according to the Gaussian distribution, thereby facilitating a more natural implementation of ASSR. Based on 2DGS, we have designed a novel image SR framework called GaussianSR, which introduces Gaussian representation for the first time in image restoration tasks. In this section, we discuss the key components of the GaussianSR framework.\nOverall Architecture. The architecture of GaussianSR is illustrated in Figure 2. The process begins with an encoder that extracts features from the input images. These features are then mapped onto a learnable Gaussian field via Selective Gaussian Splatting, assigning a Gaussian distribution to each pixel. At any given query coordinate xq, multiple overlapping Gaussian fields determine the feature value through their collective Gaussian functions explicitly. The features are subsequently rendered into the high-resolution space and processed through several fully connected layers to restore the channel dimension and produce the RGB value at xq. Since xq can be any point within the R2 space, the framework achieves arbitrary-scale super-resolution.\nLR Initialization. LR feature initialization involves fetching features from the input LR image and representing them as initialization points within a continuous Gaussian feature field. Specifically, the value of the LR feature is used as the initial amplitude of the Gaussian, while the coordinates of the LR feature determine the center of the Gaussian field. Mathematically, let (mi, ni, vi) be the set of LR feature coordinates and their corresponding values, where i = 1,2,..., N, and N is the total number of features. Each LR feature is then represented as a Gaussian distribution fi(p|\u03bc\u03b5, \u03a3\u2081) within the continuous feature field, with its mean \u03bc\u2081 = (mi, ni) corresponding to the normalized LR feature coordinates and its amplitude initialized to vi, the actual value of the feature of the LR image. The feature value at any coordinate p within the continuous Gaussian feature field is determined by summing the function values from all individual Gaussian distributions centered at the LR feature coordinates:\n$C_{2DGS}(p|v, \\mu, \\Sigma, \\xi) = \\sum_i f_i(p/p_i, \\Sigma_i) \\cdot C_i$ (5)\nwhere fi(), ci(\u00b7) are the same as defined in Equations 3 amd 4. LR feature initialization ensures that subsequent upsampling and reconstruction processes can effectively leverage the spatial and intensity information from the LR image, ultimately leading to high-quality HR image reconstruction.\nSelective Gaussian Splatting. Convolutional operations, while effective, are inherently limited by their fixed kernel sizes, incapable of adapting to varying input charac-"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "For our training, we use the DIV2K dataset [1]. Sourced from the NTIRE challenge [40], this dataset comprises 1000 diverse 2K-resolution images featuring a wide range of content, including individuals, urban scenes, flora, fauna and natural landscapes. Within this collection, we allocate 800 images for the training set, 100 images for validation. To evaluate the generalization performance of the model, we report the results on the DIV2K validation set with 100 images. Another four benchmark datasets are also utilized, namely General100 [10], BSD100 [24], Urban100 [12], and Manga109, [25] which provided a comprehensive landscape for assessing cross-dataset robustness. Consistent with previous work [6, 11,47], we utilized bicubic downsampling to synthesize the LR images."}, {"title": "4.2. Implementation details", "content": "To simulate a continuous magnification process, the down-sampling factor is randomly sampled from a uniform distribution, U(1, 4), enabling the model to adapt to different degrees of image degradation. To accommodate the GaussianSR framework, within each batch, the downsampling factor remains constant across different GPUs (if multi-GPU training is employed). The loss function used is the L1 distance between the reconstructed image and the ground truth image. Following the settings in LIIF and its variants [6,19,47], we randomly crop the LR images into 48\u00d748 patches, collect 2304 random pixels on the HR images. The initial learning rate for all modules is set to le-4, and is halved every 200 epochs. The model is trained in parallel on 4 Tesla V100 GPUs with a mini-batch size of 16. The training process takes about 2000 epochs to converge."}, {"title": "4.3. Quantitative and Qualitative Results", "content": "To validate the efficacy of our GaussianSR, we perform a comparative analysis against several advanced methods. These methods include MetaSR [11], a pioneering work that first realizes super-resolution for non-integer scales using a CNN-based method. Additionally, we have chosen LIIF [6], which introduced the concept of implicit neural representations to the image super-resolution domain. Furthermore, we have included several variants and extensions of the LIIF method in our comparison, such as ITSRN [47], A-LIIF [19], DIINN [29]. We re-train these models under the same framework to ensure fairness. Due to the significantly larger number of parameters and the different framework employed by CiaoSR [3], it was not included in the comparative analysis. Tables 1 and 2 present the results for integer-scale SR and noninteger-scale SR, respectively. Our method achieves competitive results across all 5 datasets for all scaling factors, particularly excelling in noninteger scaling factors due to the flexible representation of Gaussians. Although our model performs comparably to LIIF and A-LIIF on lower-resolution datasets such as General100 and BSD100, it shows significant improvement on higher-resolution datasets like Urban100 and Manga109, with particularly notable results on Urban100.\nFigure 5 offers a qualitative comparison with other arbitrary-scale SR methods. Our model excels at synthesizing SR images with sharper textures. For instance, in the second column of the first row and the second column of the second row, our method successfully recovers the texture of buildings despite significant texture degradation in the low-resolution images. In contrast, other methods tend to produce erroneous redundant artifacts that detract from the visual quality of the image due to limitations in local integration and insufficient feature extraction. More visual comparisons can be found in the supplementary materials."}, {"title": "4.4. Ablation Study", "content": "In this section, we conduct an ablation study to investigate the effects of different settings in our architecture. Table 5 and Table 6 present the results of various hyperparameter settings for channel decoupling and Gaussian bank.\nChannel decoupling. The default configuration involves 8 channels undergoing 2D Gaussian Splatting (2DGS) upsampling, while the remaining channels are upsampled using bicubic interpolation. This setup is designed to achieve memory-efficient training and inference. We compare this configuration with settings where all channels were upsampled using bicubic interpolation and where 16 channels were upsampled using 2DGS. The version with 8 channels using 2DGS achieves the highest PSNR on the DIV2K validation set, and all versions with 2DGS significantly outperformed the version using only bicubic interpolation. These results demonstrate the effectiveness of our proposed framework for super-resolution tasks.\nGaussian bank. GaussianSR consists of 100 Gaussian fields initialized in a stepwise manner. We conducted ablation experiments by varying the number of Gaussian"}, {"title": "5. Discussion", "content": ""}, {"title": "Discussion of Selective Gaussian Splatting.", "content": "Convolutional operations are limited by their fixed kernel sizes and can induce redundancy in high-resolution images. Selective Gaussian Splatting technique adaptively assigns Gaussian kernels to pixels based on their unique features. This method minimizes kernel redundancy and parameter overhead. SGS uses logits from an encoder to assign one of 100 distinct Gaussian kernels to each pixel, optimized during training using Gumbel Softmax to facilitate gradient backpropagation. This adaptive approach improves flexibility and performance in modeling pixel distributions for computer vision tasks.\nWe visualize the logits results to demonstrate the frequency of selection for each Gaussian kernel. We conduct experiments on the Set5 dataset [2], considering both different images with the same scaling factor and the same image with different scaling factors. Figure 7 (a) illustrates the frequency distribution for five images in Set5 under 2x super-resolution. The results show significant differences in frequency among all five images. Considering the high variance within Set5, we further experiment with the same image under different scaling factors. The results in Figure 7 (b) indicate significant differences in frequency as well, validating the correctness and effectiveness of our adaptive Gaussian kernel assignment approach.\nIn order to investigate the necessity of utilizing LR features, we conduct an overfitting analysis on a single image using the 2D Gaussian Splatting approach. The results reveal that the number of Gaussian fields significantly impacts the final reconstruction quality. Consequently, to fully leverage the information from the LR features, we represent each feature point as an individual Gaussian field. Furthermore, we perform experiments by introducing varying levels of noise to the LR image to examine the rationale behind using LR feature values as initial amplitudes. Our findings demonstrate that the final reconstruction results are comparable across different degradation levels; however, the initial convergence rates varied (see Figure 9). Therefore, by directly utilizing the feature values as initial amplitudes, we can effectively improve the convergence speed. GaussianSR exhibits superior early-stage performance compared to LIIF [6] and ITSRN [47] approaches, as demonstrated by the results presented in Table 4 of the main article."}, {"title": "Discussion of LR initialization.", "content": "The LR feature initialization process represents the LR image features as initialization points within continuous Gaussian feature fields. The value of the LR feature is used as the initial amplitude of the Gaussian, which ensures that subsequent upsampling and reconstruction processes can effectively leverage the spatial and intensity information from the LR image, ultimately leading to high-quality HR image reconstruction."}, {"title": "6. 2D Gaussian Splatting Pseudocode", "content": "Algorithm 1 A Simple Example of 2D Gaussian Splatting\nRequire: \u03a3\u03ba, \u03a3y, p (covariance parameters), coords (point coordinates), colors (point colors), image_size\nEnsure: final_image (rendered image)\n1: Compute the covariance matrix \u2211 using Ex, \u03a3\u03c5, \u03c1\n2: Check if \u2211 is positive semi-definite\n3: Compute the inverse of \u2211: \u03a3\u22121\n4: Create a 2D grid x, y in the range [-5, 5]\n5: Compute the Gaussian kernel K using x, y, and 2-1\n6: Normalize K to [0, 1] range\n7: Repeat K along the channel dimension to match colors\n8: Pad K with zeros to match image_size\n9: Create a batch of 2D affine transformation matrices \u0398 using coords\n10: Apply affine transformations to K using \u0398 to obtain Ktranslated\n11: Multiply Ktranslated with colors to get image layers\n12: Sum the image layers to obtain final_image\n13: Clamp final_image to [0, 1] range\n14: Permute final_image to match channel order\nreturn final_image"}, {"title": "7. More Visual Comparison", "content": "Figure 10 provides a qualitative analysis of the performance of GaussianSR in comparison with other arbitrary-scale super-resolution methods. The evaluation highlights the exceptional capability of our approach in generating high-resolution images with enhanced texture clarity and fidelity. This is particularly evident in the first and last rows of the figure. In the first row, while other methods fail to accurately reconstruct the bird's eyes, our method preserves this critical detail, closely resembling the original image. In the fourth row, which features repetitive and intricate textural structures, our method outperforms others by effectively reconstructing these complex details without introducing artifacts such as texture blending, even when the low-resolution input information is significantly degraded.\nFigure 11 further showcases the visual effectiveness of our method in handling non-integer scaling factors. GaussianSR achieves superior restoration results on complex and fine textures, minimizing the presence of artifacts while maintaining a close resemblance to the original image. Furthermore, Figure 12 demonstrates the visual efficacy of our proposed GaussianSR when applied to the challenging task of extreme super-resolution at a scale factor of \u00d710. Notably, the super-resolved images generated by GaussianSR exhibit significantly sharper and more naturally delineated edges compared to those produced by the conventional bicubic interpolation method. This qualitative assessment highlights the superior performance of our approach in preserving high-frequency details and maintaining the structural integrity of the upscaled images, even under such demanding magnification conditions. The ability of GaussianSR to effectively capture and reconstruct fine textural information while minimizing the introduction of visual artifacts underscores its"}, {"title": "8. Limitation and Future work", "content": "During the rendering process, for each Gaussian field, we generate a tensor with dimensions [batch size, channel, height, width]. The memory overhead of this five-dimensional vector is substantial. Despite employing techniques such as feature unfolding to reduce memory usage, there remains significant room for improvement. It is noteworthy that this five-dimensional tensor is inherently sparse, as only the neighborhood around the center of each Gaussian field contains values. Therefore, by optimizing matrix multi-"}, {"title": "9. Conclusion", "content": "The proposed GaussianSR pipeline represents a paradigm shift in image super-resolution by introducing a continuous feature field characterized by a Gaussian distribution, departing from the traditional discrete feature storage methodology. This approach enables natural implementation of arbitrary-scale upsampling, as any point within the feature field can be explicitly determined according to the Gaussian distribution. By reconceptualizing pixel representations as continuous Gaussian fields, GaussianSR bridges the gap between discrete feature representations and super-resolution tasks, allowing for a more natural and fluid representation of each pixel with fewer parameters. Furthermore, the classifiers were trained to assign optimal Gaussian kernels to each pixel, effectively tailoring the model to various input characteristics. Experimental results demonstrate that GaussianSR enhances super-resolution performance while reducing computational overhead associated with parameters, highlighting the potential of integrating Gaussian expressions in computer vision tasks traditionally dominated by INR-based methodologies. The proposed framework not only pioneers a novel approach in arbitrary-scale super-resolution but also opens avenues for substantial advancements in how visual information is conceptualized and processed."}]}