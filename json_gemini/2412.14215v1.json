{"title": "GENERATIVE AI TOOLKIT - A FRAMEWORK FOR INCREASING\nTHE QUALITY OF LLM-BASED APPLICATIONS OVER THEIR\nWHOLE LIFE CYCLE", "authors": ["Jens Kohl", "Luisa Gloger", "Rui Costa", "Otto Kruse", "Manuel P. Luitz", "David Katz", "Gonzalo Barbeito", "Markus Schweier", "Ryan French", "Jonas Schroeder", "Thomas Riedl", "Raphael Perri", "Youssef Mostafa"], "abstract": "As LLM-based applications reach millions of customers, ensuring their scalability and continuous\nquality improvement is critical for success. However, the current workflows for developing, main-\ntaining, and operating (DevOps) these applications are predominantly manual, slow, and based on\ntrial-and-error. With this paper we introduce the Generative AI Toolkit, which automates essential\nworkflows over the whole life cycle of LLM-based applications. The toolkit helps to configure, test,\ncontinuously monitor and optimize Generative AI applications such as agents, thus significantly\nimproving quality while shortening release cycles. We showcase the effectiveness of our toolkit\non representative use cases, share best practices, and outline future enhancements. Since we are\nconvinced that our Generative AI Toolkit is helpful for other teams, we are open sourcing it on GitHub\nand hope that others will use, forward, adapt and improve.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Challenges in developing and operating LLM-based applications over their whole life cycle", "content": "Large Language Models (LLM) are machine learning models capable of executing natural language processing tasks\nsuch as summarizing or generating text. Since their introduction LLM have gained widespread traction in different\ndomains. They can be used as stand-alone products, but also to augment existing software products such as applications\n(also called agentic functions) or machine learning agents (also called LLM-based agents) to increase their capabilities.\nIn this section, we show challenges during development and operation of LLM-based applications on three examples.\nUsers interact with LLM-based applications by entering input into the LLM, the so-called prompt. Jang et al. showed in\n2023 that the LLM's output is very sensitive to variations of the prompt [1]. Thus, the task of finding the best prompt\nto generate expected or best output leads to manual, trial-and-error-prompt experimenting \u2013 a method well known as\nprompt-engineering (cf. White et al. in 2023 for ChatGPT [2] or a survey of prompt techniques by Schulhoff et al. in\n2024 [3]).\nAdditionally, the outputs of an LLM-based application can not only vary, but also be wrong without telling a user\n(\u201challucination\u201d, cf. Sec 2.). This means that LLM-based applications need to be tested sufficiently during development\nwhich is aggravated due to limited insight into the LLM-based application. Keeping track of various prompt versions\nduring prompt engineering and comparing results over a large test dataset adds even more development complexity.\nGiven the fact that LLM-based applications can also interact autonomously with potentially millions of users, they need\nto be monitored at scale after deployment and release to prevent hallucination and identify other quality-related issues."}, {"title": "1.2 Overview of our contribution", "content": "In this paper we introduce a toolkit automating DevOps workflows for LLM-based applications with the goal of\nincreasing the quality of LLM-based applications over their whole life cycle while introducing more efficient workflows.\nWe show the benefits of the Generative AI Toolkit on representative use cases. With these use cases we also share some\nbest practices for optimizing LLM-based applications over their whole life cycle. Since our use cases are using single\nLLM or LLM-based agents, we cover the currently most common architectural patterns for LLM-based applications."}, {"title": "2 Related Work", "content": "In this section we give some background regarding relevant software engineering workflows and then detail LLM-based\nagents and their history. Furthermore, we show that there are currently few open source and free of charge frameworks\ncovering the whole LLM-agent life cycle.\nDevOps: is an approach aiming to solve the previous separation between engineers developing software and engineers\noperating it. The idea was that engineers should own their software and thus ensure quality over the whole life cycle.\nOne of the first mentions of this term was in a conference in Gent in 2009. An example for state of the art and challenges\nfor DevOps can be found in Leite et al. in 2019 [4]. These techniques have been adopted to operate machine learning\nsystems as we will show in a later paragraph of this section.\nContinuous Integration and Continuous Delivery (CI/CD): Continuous Integration, as defined by Fowler in 2006,\nis a practice \u201cwhere members of a team integrate their work frequently...to multiple integrations a day\u201d leading to\n\u201csignificantly reduced integration problems and allows to develop cohesive software more rapidly [5]\u201d. Beck et al.\ndefined in 2001 Continuous Deployment as the next step with the goal to \u201csatisfy the customer through early and\ncontinuous delivery of valuable software\u201d [6] by a \u201creliable...and largely automated process\u201d as Humble and Farley\nstated in 2010 [7]). Both approaches combined offer reduced time to market and an improved efficiency and quality of\nthe software (e.g. Chen in 2015 [8]). Core enabler of CI/CD is a pipeline in which all essential workflows for building,\nintegrating, deploying, and releasing the software are fully automated.\nSoftware testing: is one of the essential steps of the CI/CD pipeline. Software testing has been extensively researched\nwith a substantial body of literature addressing efficient software testing, e.g., Dustin et al. in 1999 [9] or Gregory\nand Crispin in 2014 [10]. A main concept is the test pyramid with at least three layers as introduced by Kohn in 2010\nstarting with single tests of small software functions (unit tests), followed by testing of the interactions of these small\nfunctions (integration tests) and ending with testing the whole system (acceptance tests) [11, Chap. 16]. This test\ncascade is nowadays executed automatically as part of a CI/CD pipeline. Since only a successful passing of all tests of\none layer leads to testing the next layers, this concept helps to provide a good trade-off between testing time and test\ndepth (although depending on the quality of the tests).\nLarge-language models (LLM): we show a brief overview of selected LLM and focus on the improvement of model\nquality; a very comprehensive overview for LLM can be found under Zhao et al. from 2023 [12, Sec 2.2].\nMost LLM use Transformers with attention layers as the basis of their architecture. Attention layers were first described\nby Schmidhuber in 1992 [13] and by Bahdanau et al. in 2015 [14]. In 2017, Vaswani et al. refined the attention layer\nand built the structure now known as Transfomer [15]. In contrast to previously existing architectures such as long\nshort-term memory [16], attention layers allow for more efficiency than previous architectures in training since compute\nload can be distributed on multiple machines.\nOpenAI's publication of GPT1 by Radford et al. in 2018 made LLM quite popular [17]. OpenAI published subsequent\npapers for GPT2 (Radford et al. in 2019 [18]) and GPT3 (Brown et al. in 2020 [19]) increasing the capabilities of the\nmodel and its qualitative output by using more training data and more parameters. In 2022, Open Al's Instruct GPT\nintroduced reinforcement learning to improve the quality of model output (Ouyang et al. [20]).\nMeta's open sourcing of development steps, source code and weights of Llama 1 and Llama 2 in 2023 (Touvron et al.\n[21, 22]) were a big step to accelerate LLM development. Since then, several papers have been published improving"}, {"title": "3 Contribution", "content": ""}, {"title": "3.1 Overview of our contributions", "content": "Figure 1 shows an overview of the features of the proposed Generative AI Toolkit. It covers the whole DevOps life\ncycle of a LLM-based application, but focuses on the phases code, build, test, and monitoring."}, {"title": "3.2 Walkthrough of Generative AI toolkit's features", "content": "Automate agent creation: we built a template for LLM-based agents which can be created and deployed with the\nPython library Cookiecutter to bootstrap new projects for agent development. The included \u201cvanilla agent\u201d is based\non best practices and contains sample code, notebooks, and tools, making it easy for developers to experiment with\ndifferent prompts, tooling, or response patterns. Furthermore, it allows to run the agent against different metrics (e.g.\nlatency) as well as to deploy and test changes rapidly."}, {"title": "3.3 Future extensions to our AI-Toolkit", "content": "Define and create custom, user-defined metrics: agents often require highly customized evaluation methods tailored\nto their specific goal and intended way of working. The toolkit allows developers to create user-defined metrics that can\nassess and evaluate the agent as well as its interaction with a human by measuring various factors, such as: the agent's\nresponse, tool usage, number of conversation turns or \u201chops\u201d in the interaction between agent and user, input and output\ntoken counts, per-turn costs, latency (total time taken for agent interaction), etc. User-defined metrics can be simple\nones such as counting tokens or complex ones such as using an additional LLM as a judge to score the generated output\n(as proposed by Zheng et al. in 2023 [56]). Since the metrics can be evaluated and analyzed in the development as well\nas operation phase, they offer a feedback loop to continuously improve the agent's performance and quality.\nCreate repeatable evaluation cases: the feature allows users to conduct comprehensive and standardized evaluations\nby defining multiple test cases. This includes testing various models, user input scenarios, and customized prompts.\nThese evaluations help ensure consistency across agent versions, making it easier to identify unintended behavioral\nchanges over time. By supporting systematic testing, this feature enables teams to verify that each version meets\nperformance standards and aligns with the intended user experience.\nCompare different models and prompts against evaluation cases and metrics: a challenge during development\nof LLM-based agents is to find the \u201cbest\u201d set of agent parameters. For example, tweaking the agent's system prompt,\nor one of its tool descriptions, may make some use cases better, but degrade others, so rigorous testing must consider\nall use cases. Our toolkit therefore allows developers to \u201cpermute\u201d agent parameters, and run test cases against each\npossible permutation, by creating an agent instantiation for each permutation running as a thread. Developers can\nthus test different language models against a set of slightly different system prompts to find the best prompt-model\ncombination. Likewise other agent parameters can be \u201cpermuted\u201d too, such as the temperature parameter, the set of\ntools available, the top-p parameter, etc. The toolkit will run the developer-defined test cases against each permutation,\nin parallel against each user-defined metric, and returns a Pandas DataFrame with all measurements, grouped per\nparameter permutation. This helps developers to identify the best set of parameters.\nTests: as mentioned in Sec. 2, most testing of LLM-based agents is done on acceptance level and in manual workflows\nor tool-supported (e.g. DeepEval or Ragas) against standard benchmarks. We combine the Generative AI toolkit\nwith a new approach for testing LLM-based agents such as the test automation pyramid from software engineering.\nWhile this study only provides an introduction and short overview of the framework, more details will be provided in a\nsubsequent publication. The idea of the approach is to first test components of the agent in isolation in automated tests\nand afterwards test the interactions of these components. With the Generative AI Toolkit, it is possible to run tests of\nagents against typical benchmarks such as GLUE or BLEU (cf. Sec. 2), user-defined evaluation cases and metrics,\nas well as isolated tests of an agent's components and their interactions automatically, which are then verified via the\ntraces (see below in paragraph Logging and Monitoring).\nDeploy: the Generative AI toolkit offers Infrastructure-as-Code based on the AWS Cloud Development Kit (AWS\nCDK). By using Cookiecutter, a GitHub workflow YAML file for a simple CI/CD pipeline is created, which can serve\nas a starting point for a more elaborate pipeline.\nCI/CD: the toolkit's evaluation can also be run as part of continuous integration scripts and CI/CD pipelines. Test\ncases defined by developers can, like unit tests, be run against the agent's implementation in the commit under test.\nSince our agent's implementation can run locally, cloud deployment of the agent (again, like unit tests) is not necessarily\nrequired, except from online access to the models. The resulting traces can then be evaluated against all defined metrics.\nAssertions can be made on the resulting measurements, so that the commit under test can be properly evaluated, e.g., by\ncomparing measurements automatically against set thresholds, or simply as further information for human reviewers of\nthe associated pull request. It is of course helpful for human reviewers to be able to compare the agent's measurements\nof the commit in the pull request against the measurements of the latest commit from the target branch. Note that such\ncomparisons are often not Pareto-optimal: some cases may actually achieve worse measurements, but that may be offset\nby higher measurements on other cases. The toolkit exposes the full set of measurements as a Pandas DataFrame for\ndevelopers to consume, inspect, and write assertions against. An out-of-the-box summary contains the averages for all\ncustom metrics that were included in the evaluation and can be logged as a table to standard out"}, {"title": "3.4 Graph", "content": "Graphical User Interface (GUI) for \u201cdebugging\" the agent: the GUI of the Generative AI Toolkit is designed to\nset up a local web server using Gunicorn, giving developers a clear and interactive view of all evaluations and traces.\nThis GUI allows users to inspect detailed trace logs, verify evaluation results, and quickly check the pass/fail status\nof tests (cf. Fig. 3). By providing a real-time display of test outcomes, the GUI enhances the debugging and quality\nassurance process, making assessing an agent's readiness for deployment easier.\nLogging and Monitoring at scale via traces: the toolkit includes a wrapper around the agent and thus captures\ndetailed logs and metrics for each interaction inside an agent as well as throughout each stage of an agent's life cycle.\nThe logs are currently stored in DynamoDB, but an extension to other services for observing and storing measurements\nis easily possible.\nThis feature allows users to monitor key metrics, track agent interactions, assess response quality, and analyze latency,\nincluding end-to-end latency for tool execution. Tracing supports robust debugging and optimization, providing teams\nwith a fine-grained view of how the agent performs under various conditions and helping them identify potential areas\nfor improvement. Typical optimization potentials are when the agent does not have an answer to a request. This can\nbe detected either via checking and triggering for words such as \u201cunfortunately\u201d or \u201csorry\u201d (cf. Appendix B.3) or by\nchecking if the same user asks similar queries in a short time.\nWe use the AWS native logging with Amazon CloudWatch which allows easy dashboarding (cf. Fig. 4) and also to\ndefine alarms to automatically notify developers of a feature or model drift after deployment.\nAdditionally, we will implement the graphical UX/UI from the previous section for these logs as well."}, {"title": "4 Evaluation", "content": "In this section we show how the Generative AI Toolkit offers comprehensive advantages throughout the whole life cycle\nof LLM-based applications on several use cases. More details about the use cases can be found in appendix B.\nThe toolkit accelerates project initiation by bootstrapping a vanilla agent based on best practices. Since this is\nextensible to other LLM-architectural archetypes, it offers the deployment of compliant LLM-based applications within\nminutes. During the coding phase, the toolkit allows users to define evaluation cases and metrics in Python code, and\nthus significantly reduces the effort required for test development. Once evaluation cases and metrics are configured,\nthey can easily be integrated in CI/CD pipelines for automated build runs with audit-proof logging. Tests can be\nconfigured and automatically executed on application, user and component level inside the LLM-based application\nfor all specified model and prompt variations, thus eliminating the need for manual testing and documentation. The\nGenerative AI toolkit facilitates deployment through Infrastructure-as-Code and CI/CD scripts, minimizing manual\neffort and ensuring high levels of automation and repeatability across environments. The toolkit's granular, configurable\nlogs, metrics, and traces for production monitoring, visualized through a graphical user interface, offers a structured,\nautomated, and thus scalable approach to measure the deployed LLM-based application to identify issues or optimization\npotentials.\nAll put together, the framework allows us to significantly reduce cycle time of LLM-based applications and manual\nwork.\nWhile we have seen significant improvements in both efficiency and quality in the use cases detailed in the appendix,\ngeneric statements on the quantitative benefits of the Generative AI toolkit are difficult to make."}, {"title": "5 Conclusion and future work", "content": "In this paper we introduced the Generative AI Toolkit, a comprehensive framework covering the whole development and\noperation life cycle of an LLM-based application or agent. We showed how our toolkit can help to improve the quality\nby providing higher test coverage and identifying optimization potentials via user-definable metrics. Additionally, we\nshowed how automation allows for shortening development cycle times. As we are convinced that the framework will\nbe beneficial for developers and users of LLM-based agents, we are open sourcing our framework so others can use,\nadapt, and improve.\nWe are currently working on adding more features to the toolkit to increase its benefits. As we focus on optimizing\nthe path for bringing robust Generative AI applications to production, we are working closely with developers in this\nfield to incorporate their feedback on current and future features. Examples for these include (i) an implementation\nof consensus-based classification where several models are used to independently perform a task, with their results\naggregated as a proxy for a confidence metric, (ii) a workflow for autonomous model argumentation, that could\nadditionally produce a measure of confidence by challenging the outputs produced by upstream models, (iii) the ability\nto integrate RAG in the agent, (iv) support for models hosted outside of AWS Bedrock, and (v) the ability to persist\ngenerated test cases and support their execution as parameterized tests using common testing frameworks.\nWhile these are still in early development stages, they conceptually demonstrate our intention to continue working on\nnovel approaches to test and improve the quality and reliability of LLM outputs."}, {"title": "A Getting started", "content": "The Generative AI Toolkit is a lightweight library covering the whole life cycle of LLM-based applications, including\nagents. Its purpose is to support developers in building and operating high quality LLM-based applications over their\nwhole life cycle starting with the first deployment in an automated workflow.\nWe are convinced that the Generative AI Toolkit will be helpful for DevOps teams and release it free of charge and as\nopen source under terms and conditions of the Apache 2 license. The README.MD in the Github repository shows\nhow to use the Generative AI Toolkit.\nWe hope that the framework will help you facilitate the development and operation of great applications. And we can't\nwait to see how others will adapt and improve the Generative AI Toolkit!"}, {"title": "B Use Cases", "content": "In this section we show the adaptation of our toolkit to several representative use cases with increasing complexity. This\nsection shall give readers more insight into our framework and show exemplary prompts, metrics, and practices to adapt."}, {"title": "B.1 Use Case 1: Text-to-SQL agent", "content": "Description: the first use case, a Text-to-SQL agent, shows how natural language inputs can be translated into Structured\nQuery Language (SQL, [57]) statements. Leveraging the Generative AI Toolkit, this agent exemplifies a foundational\nLLM-based application relying on a system prompt and associated tools. The simplicity of this use case makes it a good\nstarting point to illustrate Generative AI Toolkit's effectiveness in ensuring quality and efficiency during development."}, {"title": "B.2 Use Case 2: LLM-based agent with a RAG - Menu agent", "content": "Description: as next example we build an agent with long-term data storage. The agent's task is to provide insights for\ncustomers into a restaurant's menu and is based on an exemplary agent available on AWS' GitHub. As storage type for\nthe domain-specific knowledge of menu documents, we chose the popular RAG, a term first coined by Lewis et al. in\n2020 [58]. As stated in Sec. 2, RAG are used to reduce hallucination of agents."}, {"title": "B.3 Use Case 3: In-vehicle personal assistant", "content": "Description: we build a simplified in-vehicle assistant helping the driver and other passengers to control essential\nvehicle features such as opening windows, setting temperatures, or starting the navigation system. This assistant is an\nagent with a higher complexity than before as we use a custom intent classifier to retrieve the k most relevant tools for a\nuser request."}, {"title": "B.4 Use Case 4: compare optimization techniques for models", "content": "Description: as last example we evaluate and compare the performance of different foundation models on use case\n3 from the previous section. Finding a good tradeoff between model quality and size is a typical challenge for using\nfoundation models in devices with limited hardware (cf. Wang et al. in 2024 for a detailed overview of SLM and how\nthey can interact with LLM [60] and Gunter et al. in 2024 for Apple's hybrid approach with a small language model,\nSLM, deployed on a device interacting with an LLM in the cloud [61])."}]}