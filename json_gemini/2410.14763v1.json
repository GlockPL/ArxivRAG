{"title": "ENABLING SCALABLE EVALUATION OF BIAS PAT-\nTERNS IN MEDICAL LLMS", "authors": ["Hamed Fayyaz", "Raphael Poulain", "Rahmatollah Beheshti"], "abstract": "Large language models (LLMs) have shown impressive potential in helping with\nnumerous medical challenges. Deploying LLMs in high-stakes applications such\nas medicine, however, brings in many concerns. One major area of concern relates\nto biased behaviors of LLMs in medical applications, leading to unfair treatment\nof individuals. To pave the way for the responsible and impactful deployment of\nMed LLMs, rigorous evaluation is a key prerequisite. Due to the huge complexity\nand variability of different medical scenarios, existing work in this domain has\nprimarily relied on using manually crafted datasets for bias evaluation. In this\nstudy, we present a new method to scale up such bias evaluations by automatically\ngenerating test cases based on rigorous medical evidence. We specifically target\nthe challenges of a) domain-specificity of bias characterization, b) hallucinating\nwhile generating the test cases, and c) various dependencies between the health\noutcomes and sensitive attributes. To that end, we offer new methods to address\nthese challenges integrated with our generative pipeline, using medical knowledge\ngraphs, medical ontologies, and customized general LLM evaluation frameworks\nin our method. Through a series of extensive experiments, we show that the test\ncases generated by our proposed method can effectively reveal bias patterns in\nMed LLMs at larger and more flexible scales than human-crafted datasets.\nWe publish a large bias evaluation dataset using our pipeline, which is dedicated\nto a few medical case studies. A live demo of our application for vignette genera-\ntion is available at https://vignette.streamlit.app. Our code is also\navailable at https://github.com/healthylaife/autofair.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have shown an impressive potential to assist in various forms of real-\nworld challenges, including finding long-sought solutions to some key medical challenges such as\nprovider burnout, processing of claims, and obtaining preauthorizations. The LLMs used in medical\napplications (hereafter, Med LLMs) have been leveraged for clinical note summarization [1], patient\nconsultation [2], and generating differential diagnosis [3], among others.\nDespite the promising potential, using LLMs in sensitive assistive decision-making scenarios, such\nas clinical decision support, brings about significant concerns. One of the most critical concerns\nrelates to the impact of LLM-based tools on bias and fairness. Specifically, deploying biased LLMs\nin clinical settings can amplify existing healthcare inequities, for instance, by providing unfair care\nrecommendations or service denials based on patients' demographics. A fairly large family of recent\nstudies has documented biased behaviors of Med LLMs across different (a) LLM types, (b) clinical\ntasks, and (c) subpopulations [4; 5; 6; 7]. Biased behavior in LLMs can arise from a variety of\nsources, including the inherent biases in the training data, the limitations in capturing and utilizing\nfactual knowledge, and flawed training and inference strategies [8].\nPrior to mitigating bias patterns in Med LLMs, it is critical to develop tools for comprehensive eval-\nuation of such patterns. Despite the growing number of methods for the evaluation of LLMs (LLM\nEvals) [9], evaluating LLMs remains a challenging task due to reasons such as open-endedness and\nstochasticity of the LLM outputs [10]. Evaluating Med LLMs generally involves creating bench-\nmarking datasets with controlled clinical 'vignettes' (i.e., patient scenarios) that target sensitive"}, {"title": "2 PRELIMINARIES", "content": "We aim to address three key challenges in the automated generation of clinical vignette generation\nfor fairness evaluation, including domain-specificity of the fairness evaluation, outcome indepen-\ndence, and hallucination.\nDomain-specificity of the Fairness Evaluation: Medicine is a highly specialized field, and sim-\nlarly, fairness evaluation in clinical settings is highly domain-specific. Each domain generally has\ncertain 'fairness pitfalls' that are unique to that particular medical domain and are characterized\nthrough rigorous evidence-based epidemiological or mechanistic (biological) studies [15; 16]. For\ninstance, while focusing on gender disparities in heart attack diagnosis may be more meaningful in\ncertain branches of cardiology, racial differences in cancer screening may be more relevant in some\nbranches of oncology. Therefore, fairness (i.e., a fundamentally sociotechnical concept) must be\ncharacterized within each specific clinical context through a rigorous evidence-based process.\nOutcome Independence: The common procedure for using red teaming strategies to evaluate\nMed LLMs involves iterating over different values of a target sensitive attribute (e.g., male and\nfemale in the case of gender). More formally, red-teaming approaches check for counterfactual\n(parity-based) fairness defined as:\n$P(\\hat{Y}_{A\\leftarrow a}|X = x) = P(\\hat{Y}_{A\\leftarrow a'}|X = x)$,\nwhere A shows a sensitive attribute with two values of a and a', X shows the remaining input\nvariables and Y shows the outcome. This formulation generally assumes that the sensitive attributes\nare independent of the outcome (P(Y|A) = P(Y))."}, {"title": "3 METHOD", "content": "We propose a vignette generation method to enable scalable and evidence-based evaluation of bias\npatterns in Med LLMs while addressing the three discussed challenges. At a high level, in our\nmethod, an information extractor (\u00a73.1) retrieves relevant evidence from biomedical knowledge\nbases related to user-determined scenarios. The extracted information aims to help contextualize\nfairness and identify the bias pitfalls in each domain. Then, a vignette generator LLM uses the ex-\ntracted evidence to generate \u201cbase\u201d vignettes (\u00a73.2). Next, we exclude the base vignettes with signs\nof hallucination (\u00a73.3). Then, outcome independence between the augmented sensitive attributes\nand the health outcomes is confirmed (\u00a73.4). After this step, to create the red-teaming vignettes,\ntargeted sensitive attributes are then augmented with each base vignette (\u00a73.5).\nMore formally, given a user-specified set of inputs I, a generative LLM M, and a knowledge base\nK, we propose a pipeline P that generates a set of distinct red-teaming vignettes V:\nP(I,K,M) \u2192 V,\nwhere, the input I includes targeted health outcome, o; the desired number of vignettes, n; and the\nsensitive attributes of interest S. Hence, the inputs are I = {o, n, S}. Figure 1 shows an overview\nof the proposed method."}, {"title": "3.1 INFORMATION RETRIEVAL", "content": "To generate the base vignettes, we connect a (strong) LLM to external biomedical knowledge bases\n(K). A \"base\" vignette here refers to an initial clinical scenario to create the final red-teaming\nscenario after being later modified by manually augmenting sensitive attribute values to that. The\nretriever procedure (R) uses the knowledge base K and targeted health outcome and returns a set of\narticles A that are most relevant to characterize bias patterns within the specified context:\nR(K, o) \u2192 A\nArticles A refer to various types of biomedical literature capturing the most recent evidence-based\npractice, including clinical guidelines published by professional societies, clinical trial reports, and\nsystematic reviews (as well as common scientific articles). We use PubMed Central (PMC) as our\nknowledge base. PubMed offers a search engine to access a full-text archive of biomedical and\nlife sciences journal literature and is considered the most comprehensive resource of biomedical\nevidence. We provide additional technical details for connecting to PubMed in Appendix A.\nTo find relevant articles related to the targeted health outcome in PubMed, we use a separate knowl-\nedge graph (KG) constructed from PubMed articles [20], namely the \u2018PubMed Knowledge Graph.'\nThis KG's nodes consist of biomedical entities (e.g., diseases, genes, proteins, drugs, and chem-\nicals) from PubMed abstracts, and the KG uses a multi-type normalization model, which assigns\nunique IDs to recognize biomedical entities and corresponding articles. We use a name entity recog-\nnition process to recognize the named entities in PubMed abstracts by using the popular BioBERT\nmodel [21], which offers a pre-trained biomedical language representation model (embeddings) for\nbiomedical text mining.\nSpecifically, we first identify all nodes corresponding to the health outcome o inside the KG and\nidentify the set of PubMed article ID nodes connected to these outcome nodes (No). Similarly, we\nidentify a set of nodes representing 'bias' and similar concepts in the KG and select a set of PubMed\narticle ID nodes connected to these nodes (Nb). The targeted articles to be retrieved (A) from K will\nbe obtained by selecting the articles that relate to the target health outcome and refer to some form\nof bias discussion, as:\nA = {$\\bigcup a_i | a_i \\in N_o \\cap N_b$}.\nAfter querying the knowledge base K for the articles with IDs in A, we combine those to form\nthe context for the generator LLM. Instead of retrieving chunks of the text from the identified ar-\nticles -common in RAG (retrieval-augmented generation)-based methods [22]- we retrieve full\ndocuments to include in the context."}, {"title": "3.2 VIGNETTE GENERATION", "content": "For each retrieved article ai, an LLM M, is prompted to generate a set of base vignettes using the\narticle as the context:\nM(ai) \u2192 VBase,\nwhere VBase is the set of base vignettes that have been generated by the language model from evi-\ndence A.\nWe use a chain-of-thought process to instruct the LLM to generate the base vignettes. Specifically,\nin our prompt design, we describe a series of steps that the LLM needs to take in compiling the\nprovided context and generating the clinical scenario. We instruct the LLM to generate yes/no\nquestions, as this style has been the most common form of generating datasets for LLM alignment\nand red-teaming [23]. We also ensure a balanced distribution of 'yes' and 'no' answers across the\nvignettes are generated. This requirement helps prevent a majority of vignettes from leaning toward\na single answer, which could result in high accuracy simply by answering 'yes' or 'no' to all the\nquestions. The LLM is also asked to explicitly identify and list the reference for each scenario."}, {"title": "3.3 HALLUCINATION DETECTION", "content": "After generating the base vignettes, we identify and exclude those with signs of hallucination. For\nthis purpose, we adopt two LLM evaluation frameworks for hallucination detection. These two"}, {"title": "3.4 OUTCOME INDEPENDENCE CHECK", "content": "Prior to augmenting the sensitive attributes, we check for the relationship between the health out-\ncomes studied in each vignette and the sensitive attributes. As discussed earlier, this step aims to\nexplicitly avoid generating unfeasible (such as puberty complications for older adults) while allow-\ning justified scenarios (like less sensitivity to pulse oximetry for Blacks).\nWe operationalize this part by using the Unified Medical Language System (UMLS) [26], which is a\ncomprehensive system of thesaurus and ontology of biomedical concepts used for clinical data stan-\ndardization in various health applications. It provides a mapping structure among the standardized\nvocabularies and thus allows one to translate among the various terminology systems.\nWe use the UMLS ontology to identify the ancestors of a health outcome as the indicators of the\nsensitive attributes that interact with the health outcome. More specifically, in our proposed ap-\nproach, we map the generated vignettes to the UMLS concepts using MetaMap [27], which is a\ntool for recognizing UMLS concepts in the text. Next, we find the ancestors for every concept ex-\ntracted from the vignettes, going up in the hierarchy. In theory, all ancestors up to the root can be\nincluded. However, our empirical results show that two levels are generally enough. From this set\nof concepts' ancestors, we especially extract a subset ($S_{Anc}$) that belongs to the specified sensitive\nattributes (such as a specific gender or ethnicity).\nWe consider two scenarios where (a) the extracted subset related to the health outcomes is smaller\nthan all possible values for the specified sensitive attributes, and (b) the subset is the same size or\nempty. We consider the former scenario (a) as an indicator of a partial relationship and the latter\nscenario (b) as an indicator of a full relationship between the sensitive attribute and health outcome.\nFor instance, for gestational diabetes, the subset related to gender would only include female (as\nmales cannot have this disease) in an ontology tree (mostly containing \u201cis-a\" relationships), and for\nrace, it would include all races.\nThese two scenarios will determine the sensitive attribute values that are safe to be augmented\nthrough red-teaming in the next step of our pipeline $S_{Red-Team}$, as shown in:\n$S_{Red-Team} = \\begin{cases} S_{Ancestors}, & \\text{if } |S_{Ancestors}| < |S_{Input}| \\\\ S_{Input}, & \\text{Otherwise}. \\end{cases}$"}, {"title": "3.5 AUGMENTING THE SENSITIVE ATTRIBUTES", "content": "Following the described red-teaming approach for bias evaluation across various sensitive attribute\ndimensions, each of the remaining vignettes (VBase') is augmented by the remaining sensitive at-\ntribute values that are determined safe in the previous stage ($S_{Red-Team}$). Accordingly,\n$V_{Red-Team} = {\\bigcup_{i} {\\bigcup_{j} M(V_i, S_j)} | V_i \\in V_{Base'}, S_j \\in S_{Red-Team}}$\nwould be the set of final red-teaming vignettes that our pipeline generates. We prompt a strong\nLLM (M) to place the sensitive attribute values in the best location within the input vignette (same\nlocation for each single vignette). The two green boxes in Figure 1 show an example where two\nsensitive attribute values are added to one base vignette, resulting in two red-teaming vignettes. The\nstrong LLM (M) in our pipeline could be the same LLM acting in different roles (an agentic design)\nor be different LLMs."}, {"title": "4 EXPERIMENTS", "content": "We follow a series of focused research questions to examine our vignette generation pipeline. Each\nquestion addresses a fundamental component of our study's approach and especially targets evalu-\nating the pipeline's ability to address the three primary challenges we highlighted for our problem.\nSpecifically, our research questions aim to evaluate the generated vignettes' diversity and utility, as\nwell as their potential for hallucination, which are crucial factors for ensuring the effectiveness of\nour proposed method.\nExperimental Setup While our method works with any LLM, we use OpenAI's GPT-4 [28], act-\ning as the strong LLM (M) in our pipeline. We also report on using Anthropic's Claude Sonnet\n3.5 [29] in Appendix E. We use a value of 0.8 for the threshold value (\u03c4) for filtering the halluci-\nnated vignettes.\nBaselines We use two baseline methods to compare to our method. First, we use an LLM to\ngenerate vignettes, but without providing any evidence from external knowledge bases. Second, we\nuse our method without any further refinement. For both baselines, we use the same prompt design\nand the same inputs as those we use for our method.\nClinical Case Studies In our experiments, we focus on several specific case studies with docu-\nmented biases in clinical literature. One main case study relates to obesity (prevention and treat-\nment). Obesity affects approximately 16% and 40% of adults globally and in the US, respectively\n[31; 32; 33; 34]. Avoiding bias for LLMs is especially challenging due to the historical misconcep-\ntions about an individual's lack of \"willpower\" causing obesity [35]. Stigma and bias patterns are\nvery common, even among clinical providers [36]. In the following experiments, we use a subset of\n243 vignettes generated from 10 articles extracted from PubMed.\nIn addition to obesity, we also study case studies related to breast cancer, prostate cancer, and preg-\nnancy. We publish all of the generated vignettes, in addition to vignettes related to the popular topic\nof pain management, in our repository."}, {"title": "4.1 Q1: HOW MUCH THE GENERATED VIGNETTES ARE FAITHFUL TO THE PROVIDED\nMEDICAL EVIDENCE?", "content": "To comprehensively evaluate the faithfulness of the vignettes to the provided context, we use four\ndifferent methods (BARTScore, Semantic Entropy, G-Eval, and RefChecker) to measure how"}, {"title": "4.2 Q2: HOW MUCH DOES THE PROVIDED EVIDENCE AFFECT THE GENERATED VIGNETTES?", "content": "We evaluate the effect of the provided evidence in terms of vignette 'diversity' and 'domain speci-\nficity.'\nDiversity By diversity, we refer to the range of distinct expressions and language used across the\ngenerated vignettes. High diversity indicates that the model produces a wide variety of wording and\nconcepts, demonstrating creativity and reducing redundancy. This is particularly important in tasks\nlike vignette generation for clinical evaluations, as diverse outputs may better capture the nuances\nand complexities of different cases. Conversely, low diversity might suggest that the model relies on\na limited set of patterns or vocabulary, which could limit the diversity of the generated content.\nWe evaluate the diversity in terms of the total number of distinct tokens present in each vignette\nand across all vignettes. Specifically, we tokenize each generated vignette by splitting the text into\nindividual words using whitespace and punctuation as delimiters. We convert all words to lower-\ncase to ensure consistency and remove any punctuation marks. Additionally, we eliminate common\nstop words such as \"the,\" \"and,\" and \"is\" to focus on the meaningful content that contributes to\nvocabulary diversity. This tokenization process results in a set of distinct tokens for each vignette.\nNext, we create a set of distinct tokens for each vignette, combine all vignettes, and then count the\nnumber of distinct tokens in each set. Table 2 shows the results related to this experiment. The\nresults especially demonstrate that using an LLM alone produces vignettes with a limited vocabu-\nlary compared to our method, which also incorporates external knowledge bases to extract relevant\nevidence."}, {"title": "4.3 Q3: HoW WELL DOES THE OUTCOME INDEPENDENCE CHECKING WORK?", "content": "To validate the effectiveness of our outcome independent checking phase, we selected three case\nstudies related to the conditions or diseases specific to certain demographic groups. Specifically,\nwe applied our method to generate vignettes for breast cancer, prostate cancer, and pregnancy to\nobserve how the vignettes interact with two sensitive attributes (including the biologically infeasi-\nble scenarios). The results are presented in Figure 2. The color shade in each square indicates the\npercentage of vignettes that have been generated for each outcome and sensitive attribute pair. One\ncan observe that our method appropriately identifies biologically feasible and infeasible outcomes\nin vignettes based on the sensitive attributes. For example, the prostate cancer case, which is bio-\nlogically exclusive to males, shows no vignettes generated for females, whereas pregnancy, which\nis exclusive to females, does not appear in the male demographic group."}, {"title": "4.4 Q4: HoW DO HUMANS RATE THE GENERATED VIGNETTES?", "content": "We designed a comprehensive survey and asked a group of participants to rate our generated vi-\ngnettes compared to other baselines. A diverse group of 45 adults with post-graduate degrees and\nfamiliarity with the basics of LLMs and our applications participated in our study. Our study was\nreviewed by a local institutional review board (IRB) panel.\nWe asked two series of questions in our survey. In the first section, participants were asked to com-\npare five pairs of vignettes randomly drawn from a set of generated vignettes, with one vignette\nin each pair generated by our pipeline and the other the 'LLM-only' baseline. In the second sec-\ntion, they were asked to compare a separate set of five pairs of vignettes, with one generated by\nour pipeline and the other randomly selected from the EquityMedQA [30] curated dataset. Impor-\ntantly, participants were not informed which vignette came from which source and were given clear"}, {"title": "4.5 Q5: HOW CAN THE GENERATED VIGNETTES BE UTILIZED FOR BIS EVALUATION?", "content": "Finally, to showcase the potential of the generated vignettes in evaluating Med LLMs, we run an\nempirical study on five LLMs, including three general and two medical fine-tuned LLMs.\nWe assess the fairness of these LLMs using the generated vignettes in our obesity case study. We\nask each model to answer the same set of vignettes for different demographic groups and measure\nthe fairness metrics. Table 4 shows the results related to these experiments."}, {"title": "5 RELATED WORK", "content": "The advent of LLMs has made significant transformations across various fields, including health-\ncare and medicine. General-purpose LLMs, such as Claude [41] and Llama [42], are trained\non medical text (e.g., medical textbooks and blogs) and have shown an impressive potential to be\nused for various clinical decision support purposes, such as differential diagnosis [43] and treatment\nplanning [44]. Beyond the general-purpose models, there have also been specialized LLMs tailored\nfor medical applications, such as Google's Med-Gemini [45], Palmyra-Med [46], and Meditron\n[47].\nEvaluating LLM behaviors is challenging and an active area of research. LLM outputs can be\nevaluated from various perspectives, making this a complex process to ensure their effectiveness and\nreliability in various applications. LLM evaluation typically involves key metrics and techniques to\nassess different aspects of LLM performance. Traditional evaluation metrics, such as BLEU [48] and\nROUGE [49], rely on n-gram overlap between model outputs and reference texts to gauge the quality\nof outputs. However, these metrics have been criticized for their weak correlation with human\njudgments [50], as surface-level matching does not reliably capture the quality of the generated text.\nWith the advancement of deep learning, model-based metrics like BERTScore [51] and\nBARTScore [37] have been introduced and are increasingly used to evaluate various dimensions\nof text quality. Although these metrics offer improvements over traditional ones, they still do not\ndeliver fully satisfactory performance and have a limited application scope [52]. Recent methods,\nsuch as G-EVAL [24] and GPTScore [53], increasingly leverage LLMs themselves for evaluation."}, {"title": "6 DISCUSSION", "content": "This study presents a new method for generating tailored clinical vignettes based on the facts ex-\ntracted from external biomedical knowledge bases to evaluate bias patterns in Med LLMs. Our\nprocedure enables scalable fairness assessments of Med LLMs, supporting access to dynamic (user-\ndefined) and contextually relevant benchmarks.\nExtraction of the most relevant biomedical information ensures that the vignettes generated reflect\ncurrent guidelines based on the latest medical evidence. Our experiments indicate that connecting\nto external knowledge bases can significantly enhance the diversity and reduce the hallucinations of\nthe generated clinical vignettes, as demonstrated by the performance across various approaches and\nexperiments.\nSeveral limitations of our work should be noted in particular. First, reliance on existing biomedical\nliterature and knowledge bases, such as PubMed, may inadvertently introduce new biases present in\nthese sources or reinforce existing biases in LLMs. Additionally, while our method facilitates large-\nscale benchmark generation, the complexity of certain medical scenarios might be underrepresented\ndue to constraints in the current knowledge extraction methods.\nWe also note that we view our pipeline as an assistive tool in 'diagnosing' undesired biased and\nunfair patterns in Med LLMs. Having humans (especially medical stakeholders such as providers\nand patients) is critical to ensure an effective human-in-the-loop process. As a diagnosis tool, our\ngenerated scenarios may wrongly indicate biased patterns (false positive) or miss some of those\n(false negative). Ultimately, the final users can decide how to adjust the model in trading off these\ntwo cases.\nNevertheless, our method can enable much larger scale identification of biases in Med LLMs (po-\ntential true positives) than current human-crafted datasets. Accordingly, we also view our method\nas a screening tool that can reduce the search space of Med LLM bias Evals.\nWhile our method represents a crucial step toward the fairness evaluation of Med LLMs, continued\nefforts to refine the proposed methods are essential. By continuously improving new methodolo-\ngies for this purpose, the health AI community can ensure that Med LLMs can benefit all patient\ndemographics equitably, ultimately leading to improved health outcomes, reducing disparities in\nhealthcare, and increasing trust in AI-based clinical decision support tools."}, {"title": "A PUBMED RETRIEVAL", "content": "PubMed offers a search engine to access a full-text archive of biomedical and life sciences journal\nliterature and is considered as the most comprehensive resource of biomedical evidence. It covers\nfields such as medicine, nursing, pharmacy, dentistry, veterinary medicine, and health care. We use\nPubMed because it is a well-known database that researchers and healthcare professionals widely\nuse to find and access scientific content. To access PubMed we utilize NCBI's Entrez Programming\nUtilities (E-utilities) API [57] which is a suite of eight server-side programs that accept a fixed URL\nsyntax for search, link, and retrieval operations."}, {"title": "B PROMPT DESIGN", "content": "In this section, we present the main prompt we have used for generating vignettes based on the\nextracted evidence.\nYour task is to write vignettes that assess bias and fairness in LLMs concerning patients\nwith {condition}.\nAttached is a scientific paper on this topic. You are required to use this document to\ncreate the vignettes.\nFollow these guidelines:\n1. Read the attached document thoroughly. Extract all relevant scenarios for vignette\ngeneration and write vignettes based on these scenarios.\n2. Each vignette should include a close-ended question directed at the LLM, requir-\ning a 'yes' or 'no' answer.\n3. Ensure a balanced number of 'yes' and 'no' answers across the vignettes.\n4. For each vignette, provide the specific part of the document that inspired the vi-\ngnette.\n5. Do not include any additional explanations.\n6. Vignette Format:\n# Vignette [number]:\n## Question: [Write the question here.] It should start with: \"A patient...\"\n## Answer: [Write the answer here.]\n## Reference: [Copy the part of the document that you used to generate the vignette\nhere.]\n**Document:**\n{context}"}, {"title": "C HALLUCINATION DETECTION", "content": "In this section, we present a sample of the input and output from G-Eval, demonstrating how it is\nintegrated into our pipeline."}, {"title": "D HUMAN RATING PROCEDURE", "content": "We conducted a study using Qualtrics to compare the usefulness of vignettes generated by our\nmethod, a baseline, and the EquityMedQA [30] dataset. The following instructions were provided\nto participants:\nBackground: In the following, a series of \u201cvignette\" pairs are shown to you. A vignette de-\nscribes a short clinical scenario, simulating when a medical provider (a doctor) should make\na clinical decision. It is often used for testing in medical education. The same questions can\nbe asked from an LLM. Besides answering such questions correctly, it is critical that LLMs\nanswer the questions in an unbiased manner.\nOur study aims to generate a series of synthetic vignettes to evaluate the medical LLMs in\nterms of their fair performance. We aim to generate \u201cred-teaming\" vignettes, meaning vi-\ngnettes that target tricky and challenging scenarios that can reveal biased behaviors (similar\nto an adversarial learning framework).\nTo evaluate the performance of our vignette generation method, we show you two generated\nvignettes in each question. One is generated by our method, and the other by a different\nmethod.\nEach of these vignettes has a placeholder for some sensitive attributes (like [female\nmale]). When using a vignette for red teaming, it can be asked once for each of those\nsensitive attributes (e.g., once for male and once for female). A non-identical answer to the\nquestions can show the LLM's unfair behavior.\nOur Request: We ask you to check these two scenarios and tell us which one you think is\nmore \"suitable\" for evaluating the fair behavior of medical LLMs. \u201cSuitable\u201d here means\na scenario with a higher potential to reveal any bias patterns of LLMs and follows a more\ndisciplined and rational way; for instance, one that more directly targets a sensitive scenario."}, {"title": "E ALTERNATIVE LLM", "content": "We present the results for the first and second research questions, utilizing Claude Sonnet 3.5,\nin Tables 5 and 6."}]}