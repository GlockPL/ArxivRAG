{"title": "Sequential LLM Framework for Fashion Recommendation", "authors": ["Han Liu", "Xianfeng Tang", "Tianlang Chen", "Jiapeng Liu", "Indu Indu", "Henry Peng Zou", "Peng Dai", "Roberto Fernandez Galan", "Michael D Porter", "Dongmei Jia", "Ning Zhang", "Lian Xiong"], "abstract": "The fashion industry is one of the leading domains in the global e-commerce sector, prompting major online retailers to employ recommendation systems for product suggestions and customer convenience. While recommendation systems have been widely studied, most are designed for general e-commerce problems and struggle with the unique challenges of the fashion domain. To address these issues, we propose a sequential fashion recommendation framework that leverages a pre-trained large language model (LLM) enhanced with recommendation-specific prompts. Our framework employs parameter-efficient fine-tuning with extensive fashion data and introduces a novel mix-up-based retrieval technique for translating text into relevant product suggestions. Extensive experiments show our proposed framework significantly enhances fashion recommendation performance.", "sections": [{"title": "1 Introduction", "content": "In recent years, fashion e-commerce has garnered considerable global attentions from both consumers and investors. By 2023, the U.S. retail fashion e-commerce market is projected to generate revenues exceeding 207 billion U.S. dollars (Gelder). One of the primary objectives of e-commerce is to provide a smooth purchase experience for consumers to purchase products they are looking for. To this end, recommendation systems (RS) have become an essential part of many businesses (Zhang et al., 2019; Jin et al., 2023). While existing fashion recommendation systems (He and McAuley, 2016b; Liu et al., 2017; Kang et al., 2017; Yu et al., 2021) predominantly incorporate the visual appearance into the traditional recommendation, they often require resource-intensive processes for image collection and training. Additionally, they often struggle to capture the evolving nature of user interactions over time. In light of this, there has been a growing interest in sequential recommendation techniques (Sun et al., 2019; Kang and McAuley, 2018; Li et al., 2023). These techniques model historical user interactions as temporally ordered sequences, thereby achieving remarkable efficacy in capturing both short-term and long-term user preferences.\nWhile sequential recommendations have succeeded in general e-commerce, the fashion domain poses unique challenges. Our analysis of real-world user interactions on Amazon fashion highlights key differences: First, the rapid fashion turnover leads to a sparse user-item interaction matrix, intensifying the cold-start problem (Liu et al., 2020). Second, extensive purchase comparisons demand sophisticated approaches to capture fine-grained user preferences. Third, fashion-specific attributes like seasonality, occasion, and holiday trends require specialized modeling. Fourth, diverse search queries that reflect explicit user intentions, necessitate novel modeling techniques. Beyond these fashion-specific challenges, traditional recommendation contexts often require specialized models tailored to particular scenarios, such as the cold-start problem (Dong et al., 2020), which will result in a large number of models that are challenging to maintain and scale.\nTo tackle these challenges holistically, we present a sequential fashion recommendation system augmented by a large language model (LLM). Trained on vast and diverse datasets, LLMs have a profound understanding of various domains. Leveraging their extensive knowledge and commonsense reasoning capabilities (Zhao et al., 2023), LLMs provide a promising solution to generate meaningful recommendations. This is particularly beneficial in overcoming cold start problems and in accurately discerning fine-grained user preferences. Additionally, LLMs could offer a unified framework capable of addressing diverse recommendation tasks. Our LLM-augmented recommendation system consists of three primary stages. In the first stage, prompt engineering techniques are used to devise specialized prompts that align with recommendation-specific goals, enabling LLM to perceive fine-grained user preferences. In the second stage, we adapt Parameter-Efficient Fine-Tuning (PEFT) techniques (Hu et al., 2021; Dettmers et al., 2023) to mitigate prohibitively expensive training costs. In the final stage, we utilize predicted product titles and IDs to retrieve and rank potential candidate items. We present a mix-up-based retrieval technique that harnesses the strengths of both ID and title embeddings. Our contributions can be summarized as follows:\n\u2022 We conduct an in-depth data analysis on real-world user interaction patterns, identifying four key characteristics for fashion recommendation.\n\u2022 We propose a comprehensive recommendation framework tailored to the fashion domain. Within this framework, we propose advanced LLM enhancement techniques to address the unique challenges for fashion recommendation.\n\u2022 The comprehensive evaluations demonstrate that the proposed framework significantly enhance recommendation performance."}, {"title": "2 Related Work", "content": "Sequential Recommendation. Recommendation systems have gained significant interest from both academia and industry (Ma et al., 2022), with sequential recommendation receiving particular attention due to its exceptional capabilities of capturing the long-term and short-term dynamics of users (Li et al., 2022; Ma et al., 2023). The objective of sequential recommendation is to predict the next items that users may be interested in based on their historical interactions. There are various techniques being proposed to model user sequential patterns, from the Markov Chain (He and McAuley, 2016a; Rendle et al., 2010) in early works to recent neural network-based techniques, such as Gated Recurrent Units (GRU) (Hidasi et al., 2015), Convolutional Neural Network (CNN) (Tang and Wang, 2018), and Transformer (Sun et al., 2019; Kang and McAuley, 2018; Hou et al., 2022). Recently, Recformer (Li et al., 2023), a transformer-based framework for learning transferable language representations, has been proposed for sequential recommendations. It has shown superior performance, especially in cold-start settings.\nFashion Recommendation. Fashion recommendation systems, which target one vertical market - fashion and garment products, have gained popularity recently (Lin et al., 2019; Hou et al., 2019). Existing approaches mainly use visual signals to capture fashion characteristics by enhancing item representations (He and McAuley, 2016b; He et al., 2016; Kang et al., 2017), modeling visual compatibility (Chen et al., 2019; Yin et al., 2019), and identifying aesthetic and style information (Yu et al., 2021). For example, He and McAuley (2016b) proposed the Visual Bayesian Personalized Ranking (VBPR), which incorporates visual features extracted from product images into matrix factorization frameworks using pre-trained CNNs. Yin et al. (2019) utilized visual encodings to learn visual compatibility by training a triplet network, where an anchor item is paired with both a compatible and a non-compatible item to learn embeddings that capture visual compatibility. Additionally, Yu et al. (2021) introduced a deep aesthetic network that extracts aesthetic features from product images, incorporating them into recommendations to model users' preferences for aesthetic appeal. The methods for extracting visual signals have evolved over time. Early studies typically used pre-trained CNNs for visual encodings (He et al., 2016; He and McAuley, 2016b). However, recent works have shifted towards training visual encoders on specialized datasets (Yin et al., 2019) or jointly training visual feature extractors and recommendation modules (Kang et al., 2017; Lin et al., 2019).\nWhile incorporating visual signals is an inspiring direction, it falls outside the scope of and is furthermore orthogonal to our current study, which focuses on leveraging textual data to model user interactions. This choice is driven by the fact that learning effective product representations from images typically requires large datasets to generalize well (Deldjoo et al., 2022), which would introduce significant demands in terms of data collection and computational resources, making it challenging for industrial deployment."}, {"title": "3 Fashion Characteristics", "content": "Fashion-related shopping presents unique characteristics that must be carefully considered when developing RS. We conduct an in-depth analysis on real-user interaction patterns in Amazon Fashion, and identify the following key characteristics:\nC1: High Turnover of Products. The fashion domain is characterized by a rapid turnover of items, introducing a continuous stream of unique new products to platforms. For instance, Amazon Fashion adds approximately 3 million new purchasable products each month. Additionally, the volume of new fashion items significantly exceeds that of other categories, being 3.6 times greater than new electronics and 6.7 times more than new toy products on Amazon. This constant influx leads to a notably sparse user-item interaction matrix, gives rise to the cold-start problem (Liu et al., 2020).\nC2: Thorough Purchase Comparisons. Users involved in fashion-related purchases tend to engage in more comprehensive comparisons than those shopping in other categories. For example, the average interaction length for fashion-related purchases is 55% longer than for electronics and 81% longer than for toy-related purchases. These comprehensive comparisons can be attributed to the extensive range of options\u2014colors, styles, and sizes of fashion products.\nC3: Fashion Attribute-Driven Shopping. Fashion items often come with distinct attributes such as seasonality, occasion, and holiday-specific trends, which significantly influence user shopping intention. For instance,  shows a selection of items popular in summer, which might not receive the same attention in winter.\nC4: High Diversity of Search Queries. Search queries serve as a crucial context for understanding the evolving interests of users. We analyze the average volume of unique search queries over multiple days across three months. Our analysis shows that the number of unique search queries for fashion items is, on average, 2.63 times as much as electronics and 2.38 times as much as toys.\nWe highlight that while other industries may share some characteristics we've identified, the simultaneous presence of all four is unique to the fashion industry. Additionally, the importance of each characteristic in fashion differs from other domains. For example, attributes like seasonality, occasion, and trends have a more fine-grained influence on user choice in fashion compared to electronics or consumable products. In fashion, these factors influence not only availability but also social desirability and attractiveness at a given time."}, {"title": "4 Method", "content": "4.1 Problem Formulation and Overview\nProblem Formulation. In the realm of sequential recommendation, consider a system composed of a set of users and items. The set of users is represented by \\(U = \\{U_1, U_2, ..., U_N\\}\\), the set of items by \\(V = \\{U_1, U_2, ..., U_M\\}\\) and the set of queries as \\(Q = \\{q_1, q_2, ..., q_s\\}\\). Each user \\(u_i \\in U\\) is associated with an interaction sequence \\(S_i\\), which can be denoted as \\(S_i = [(s_{1,i}, a_{1,i}, t_{1,i}), ..., (s_{K,i}, a_{K,i}, t_{K,i})]\\), where K is the sequence length, \\(a_{k,i}\\) represents the specific action type, \\(t_{k,i}\\) represents the timestamp of the action, \\(s_{k,i}\\) can be an item or a query depending on the action type. When \\(a_{k,i}\\) represents a search action, \\(s_{k,i} \\in Q\\) represents the k-th query. For other actions, \\(s_{k,i} \\in V\\) represents the k-th interacted item. In this paper, we are interested in three action types, search, click, and purchase behavior, and we aim to predict the future item the user will be interested in purchasing after observing interaction sequences \\(S_i\\). Additionally, each item v is associated with an attribute dictionary containing various textual information, such as titles, colors, and descriptions. We formulate these as key-value attribute pairs and assign a unique ID to each item, in line with ID-based recommendation methods (Sun et al., 2019; Kang and McAuley, 2018).\nChallenges and Overview. Addressing the unique characteristics of fashion poses significant challenges for recommendation systems. For instance, the cold-start problem remains a persistent issue in recommender systems. Traditional approaches to mitigate this challenge often rely on complex and specialized architectures (Zhu et al., 2021; Dong et al., 2020). Capturing fine-grained user preferences further complicates this task, typically requiring specialized modules (Wang et al., 2019; Chen et al., 2021). Additionally, leveraging search data to enhance recommendations remains relatively unexplored (Si et al., 2023), where the primary difficulty lies in the distinct nature of user intent in search versus recommendation tasks. To address these challenges, we propose an LLM-augmented fashion sequential recommendation system, as shown in Figure 2. The process initiates with the creation of prompts. A query-product memory module assesses user-item interactions to identify top products associated with user queries. This information is synthesized into a natural language format using a recommendation-specific prompt template, incorporating fashion-related attributes. In the subsequent training stage, we utilize the prepared prompts to fine-tune the LLM through a Parameter-Efficient Fine-Tuning method. Finally, in the retrieval and ranking stage, we convert the generated titles and IDs into embeddings using two specialized models. These embeddings are integrated into a retrieval module with a mixup strategy, obtaining the final recommended items.\n4.2 Prompt Design\nRecommendation-specific Prompt Construction. Prompting offers a natural and intuitive interface for humans to interact with LLMs (Zhou et al., 2022). Given that LLMs are initially trained for general tasks, specialized prompts are essential for aligning LLMs with recommendation-specific goals.\nThe instruction segment aims to clearly define the task and consists of three core elements: task description (highlighted in purple), execution requirement (highlighted in blue), and format indicator (highlighted in brown). In the task description, we explicitly specify that the context is a recommendation task. The execution requirement emphasizes a set of strategies tailored to address the unique characteristics of fashion. A prime emphasis here is the consideration of sequential order. To address C2, we intend for the LLM to focus on varying attributes, as they offer insight into users' fine-grained preferences. To address C3, we emphasize the importance of fashion-specific attributes. To address C4, we observed that customers generally have preferences in purchasing the top exposure results on the shopping page, thus we instruct the LLM to prioritize the recommendation in the top exposure results corresponding to the search query. Finally, the format indicator strictly defines an output format for automated decoding. The input segment is a refined representation of user-item interactions, enriched with detailed item attributes. Search queries are also included to highlight their importance in the recommendation task. The response segment, employed only during the training phase, encapsulates the final item purchased by the user, including both the product ID and title.\nQuery-product Memory Module. We observe that users frequently opt to purchase items listed at the top of their search results. In response to this behavior, we propose a Query-Product Memory Module that preserves key-value pairs consisting of search queries and their corresponding product listings. To obtain these product lists, data is grouped by search queries and then sorted by organic position. Recognizing that queries can appear in various forms that convey similar meanings, we employ CLIP (Radford et al., 2021) to convert these queries into embedding vectors, which serve as the keys in our module. During the recommendation process, the current search query is transformed into its respective embedding, enabling us to compute the cosine distance, identify the nearest Q matching queries, and subsequently retrieve their associated top V products.\n4.3 Training Strategy\nLow-Rank Adaptation (LoRA) (Hu et al., 2021) has emerged as a notable Parameter-Efficient Fine-Tuning (PEFT) technique, offering performance comparable to full fine-tuning while requiring substantially fewer trainable parameters. Consequently, we have adopted this method to fine-tune our model. We further reduce memory usage by employing model quantization as implemented in QLORA (Dettmers et al., 2023). Specifically, we maintain distinct storage and computation data types. We quantize the model to a more memory-efficient storage type and, during the forward and backward passes, dequantize the data back to the computation type to avoid performance loss. During our preliminary experiments, we observed that the model exhibited a 7% likelihood of generating outputs in an inconsistent format. This inconsistency made the automated decoding of product IDs and titles challenging. One possible reason is that product titles in e-commerce often display limited sentence coherence and are more like a collection of individual words, setting them apart from typical natural language structures. To mitigate this issue, we identified the high-perplexity prompts and subjected them to additional training cycles relative to their lower-perplexity counterparts.\n4.4 Retrieval and Ranking Method\nTitle Embedding Model. To effectively capture the semantic similarity of item titles in recommendation tasks, we leveraged the insight that the items that were purchased in the same search queries should be similar in embedding space. Based on this insight, we first tokenize both the query and product title. Once tokenized, the model computes the embeddings for query and title by employing the LSTM model. We train the whole model using a triplet loss (Jiang et al., 2016), where we pair two hard negatives with one positive pair during each forward pass. The positive match means the item that was purchased from the query. We choose the title that is closest to the query but is not a positive match and the query that is closest to the title but is not a positive match as the hard negatives.\nID Embedding Model. The ID embedding model maps pre-defined item IDs into their embeddings. We leverage the item embedding table from the CORE model, which has demonstrated superior performance compared to state-of-the-art methods. Specifically, we train the CORE model using user-item interaction sequences, then keep only the item embedding table as our ID embedding model.\nRetrieval with Mixup. After obtaining both title embeddings and ID embeddings, the next step is to perform retrieval and ranking processes to get the candidate items for recommendation. Title embeddings are designed to capture the semantic content of an item's title, thus offering better generalization, even if an item hasn't been seen before (i.e. cold start). Conversely, ID embeddings are designed to uniquely represent specific items, so the embedding can capture nuances specific to that item, thus being suitable in top matches. To effectively combine the advantages of the two methods, we propose a mixup-based retrieval method. This approach begins with separate retrievals based on title and ID embeddings, resulting in two distinct lists of items. To generate our final list of top-K items, we adopt the following approach: We select the top-N items from the ID embedding-based list. Subsequently, we choose items ranging from positions N + 1 to K from the title embedding-based list. We set N = 1 for all our experiments."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDatasets. We have collected a large-scale dataset derived from customer interactions on the Amazon fashion service, containing approximately 5.9 million user shopping interactions with a total of 2.4 million products. We aggregated them into four primary categories: Luggage and Bags, Footwear, Accessories and Jewelry, and Clothing. The sequences included three action types: search, click, and purchases. We also filtered the 'click' interactions on the items that were eventually purchased. Each item in our dataset is described by an array of attributes such as item and user identifier, product title, category, brand, color, and size.\nEvaluation Settings. To assess the efficacy of our sequential recommendation approach, we employ three widely used metrics: Recall@N, NDCG@N, and MRR, where N is set to 10. During the evaluation, we rank the ground-truth item (i.e., final purchased item) of each sequence among all items in the same category and report the average values of all sequences in the test data. We employ the common leave-one-out strategy (Sun et al., 2019; Zhou et al., 2020) to split the data for evaluation."}, {"title": "5.2 Evaluation Results", "content": "We compared the performance of our method to baselines on four different datasets. Our method achieves the best overall performance on all datasets. Notably, we observed a 9.8% improvement in Recall@10 and a 14.4% improvement in NDCG@10 on the footwear dataset. On sparser datasets, the gains are more significant, with our method achieving a 30.4% improvement in Recall@10 and a 64.5% improvement in NDCG@10 on the clothing dataset. This is because item IDs cannot capture the rich semantic relationships that are readily expressed in item texts (e.g., color, brand). In comparison to Recformer, which also leveraged text information, our method has the additional advantage of incorporating general knowledge and reasoning capabilities inherent in large language models. This yielded superior performance across recommendation tasks."}, {"title": "5.3 Ablation Study", "content": "We analyzed how different components in our design influence recommendation performance by introducing various model variants and testing them on the luggage and bags dataset. Specifically, we consider the following variants: (1) w/o product attributes: Product representation includes only the product title, omitting all attributes. (2) w/o query-product memory: Removes the query-product memory module. (3) w/o text embedding: Uses only ID embeddings for item retrieval. (4) w/o ID embedding: Uses only text embeddings for item retrieval. (5) w/ CLIP text embedding: Uses CLIP models for item retrieval. (6) w/o q.p.m. & id emb.: a combination of the removals from variants (2) and (4). (7) w/o q.p.m. & id. & pro. a.: combining the removals from variants (1), (2), and (4).\nThe results show that each component improves performance. Notably, variants 3 and 4 highlight the benefits of our mixup-based retrieval method. The performance gap between variants 4 and 5 indicates that CLIP embedding models are less effective for recommendation tasks. Additionally, the slight performance drop from (4) to (6) indicates that the Query-Product Memory Module mainly influences the ID representation. Comparing (6) and (7) reveals the significance of product attributes in generating precise product titles."}, {"title": "5.4 Further Investigation", "content": "Cold-start Setting. The cold-start problem is a well-known issue in recommendation systems (Lee et al., 2019; Pan et al., 2019; Zhu et al., 2021). To assess our model's performance in a cold-start context, we have selected items from the testing sets that have not appeared in the training sets to construct the cold-start dataset for evaluation. For ID-based methods like CORE, we incorporate a \"cold\" token embedding into the item embeddings to supply prior knowledge, following the approach in (Li et al., 2023). It is evident that text-based methods significantly outperform ID-based approaches, primarily due to the limitations of randomly initialized cold-start item embeddings. Furthermore, our method surpasses Recformer, illustrating the effective incorporation of general knowledge and reasoning capabilities provided by LLMs.\nZero-shot Setting. In this setting, the models are required to learn knowledge from pre-trained datasets and directly test on downstream datasets without further fine-tuning, thus ID-based methods are not applicable here. To ensure a fair comparison with Recformer, which undergoes pre-training on large-scale, recommendation specific datasets, we used models pre-trained on the footwear dataset to evaluate performance on three other datasets. We also employed a model trained on the luggage dataset to assess its performance on the footwear dataset. The superior performance given in Figure 5 demonstrates that our method can effectively capture and transfer learned knowledge to new tasks based on language understanding.\nLow Resource Setting. In this setting, we trained models on datasets with different ratios of training data. The experiment results are given in . We can see that when the less training data is available, the text-based methods outperforms the ID-based CORE, this advantage stems from the transferable knowledge encoded in item texts. Additionally, as the amount of training data increases, our method shows a more significant performance improvement compared to Recformer, highlighting its efficiency in learning task-specific knowledge."}, {"title": "6 Conclusion", "content": "In this paper, we propose a sequential fashion recommendation system enhanced by a LLM. Our method consists of three stages: prompt creation, training and inference, and retrieval and ranking. First, we design specialized prompts that align the model with recommendation-specific goals. Second, we conduct efficient training to optimize the model. Third, we introduce a novel mix-up-based retrieval strategy that utilizes both ID and title embeddings to finalize item recommendations. Extensive experiments show our method significantly enhances fashion recommendation performance."}, {"title": "Limitations", "content": "Training and Inference Overhead. Incorporating LLMs into recommendation systems introduces additional complexities in terms of time and space. Despite these challenges, the domain of enhancing LLM efficiency is evolving swiftly, presenting strategies to alleviate these concerns. For instance, parameter-efficient fine-tuning techniques can notably reduce memory requirements and training time. In terms of inference efficiency, there is a growing body of research dedicated to developing more efficient inference frameworks.\nIncorporating Visual Signals. Visual signals play an important role in shaping users' shopping decisions in the fashion domain. Our current approach focuses on textual data to model user interaction patterns, as incorporating images would significantly increase data collection and computational demands. However, integrating visual signals into our recommendation framework remains a promising direction.\nSecurity and Privacy Risks. Like other machine learning models, LLMs are vulnerable to various security and privacy risks. Addressing these risks with effective countermeasures is an important direction for future work."}]}