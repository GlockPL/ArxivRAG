{"title": "SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact Visual Discretization", "authors": ["Zhentao Tan", "Ben Xue", "Jian Jia", "Junhao Wang", "Wencai Ye", "Shaoyun Shi", "Mingjie Sun", "Wenjin Wu", "Quan Chen", "Peng Jiang"], "abstract": "This paper presents the Semantic-aWarE spatial-tEmporal Tokenizer (SweetTokenizer), a compact yet effective discretization approach for vision data. Our goal is to boost tokenizers' compression ratio while maintaining reconstruction fidelity in the VQ-VAE paradigm. Firstly, to obtain compact latent representations, we decouple images or videos into spatial-temporal dimensions, translating visual information into learnable querying spatial and temporal tokens through a Cross-attention Query AutoEncoder (CQAE). Secondly, to complement visual information during compression, we quantize these tokens via a specialized codebook derived from off-the-shelf LLM embeddings to leverage the rich semantics from language modality. Finally, to enhance training stability and convergence, we also introduce a curriculum learning strategy, which proves critical for effective discrete visual representation learning. SweetTokenizer achieves comparable video reconstruction fidelity with only 25% of the tokens used in previous state-of-the-art video tokenizers, and boost video generation results by 32.9% w.r.t gFVD. When using the same token number, we significantly improves video and image reconstruction results by 57.1% w.r.t rFVD on UCF-101 and 37.2% w.r.t rFID on ImageNet-1K. Additionally, the compressed tokens are imbued with semantic information, enabling few-shot recognition capabilities powered by LLMs in downstream applications.", "sections": [{"title": "1. Introduction", "content": "Visual tokenizers [6, 12, 41, 43, 45, 53, 55] are emerging as essential components in the field of modern computer vision models, particularly in the generation [12, 50, 55] and understanding [18, 28, 36, 44, 47] of vision data. These tools convert visual inputs into discrete tokens, capturing essential temporal and spatial features that facilitate advanced analysis by formulating visual-related tasks as a token prediction process.\nCompression ratio and reconstruction fidelity are vital criteria for evaluating a tokenizer. However, recent visual tokenizers, especially video tokenizers [43, 45, 55] typically retain a low compression ratio. This is because visual tokens are usually derived from 2D patches [10] or 3D tubes [12, 45] which preserve location relationships (e.g., each token corresponds to a specific region of input [56]), leading to redundancy in both spatial and temporal dimensions. To reduce token count, we take inspiration from Q-Former [27], which compresses raw input into learnable \u201cquery tokens\". However, it is observed that directly flattening video tokens into sequence may lead to catastrophic temporal information loss. Therefore, we explore a spatial-temporal decoupled approach to perform tokenization.\nA higher compression ratio generally makes reconstruction more challenging. To complement visual information during compression, we follow [29, 54, 57] to introduce pretrained language embeddings as the latent codebook, leveraging the powerful semantic representation capabilities of large language models (LLM) [1, 38]. However, previous works primarily focus on image modality, overlooking the relationships between text and motion in video domain.\nTo address existing limitations, we propose SweetTokenizer - Semantic-aWare spatial-tEmporal Tokenizer as illustrated in Figure 1. First, considering the heterogeneous redundancy in static images and dynamic frames, we propose the Cross-attention Query AutoEncoder (CQAE) to compress spatial and temporal information into separate learnable queries. Different from previous works [27, 56], our findings indicate that coupling the compression of spatiotemporal information increases the difficulty for the decoder to learn the motion information of the same pixel across consecutive frames. Thus, taking the decoupled spatial and temporal queries as inputs, we devise a strategy of spatial decoding followed by temporal decoding to achieve a separate reconstruction of the spatial and temporal dimensions of visual information. Additionally, the decoupled spatiotemporal reconstruction approach allows for pre-training with a large amount of image data, enhancing the model spatial representation. Second, to integrate the semantic information inherent in large language model (LLM), we design specialized codebooks tailored for spatial and temporal compression addressing the differences in semantic representation between spatial and temporal information. Specifically, we design two language-based codebooks based on the part of speech, using nouns and adjectives for spatial static information and verbs and adverbs for temporal motion information. By incorporating language-based codebooks, the learnable compressed queries can also be easily adapted to downstream visual understanding tasks by in-context learning of LLM. Third, we introduce the curriculum learning [2] mechanism by training SweetTokenizer in three stages to achieve stable convergence. We initially pre-train the spatial CQAE with image data, then joint training of the spatial and temporal CQAE with video data. The first two stages are supervised by the commitment loss of the codebook and the cross-entropy loss of the proxy code [56] from the pretrained tokenzier. In the last stage, we train the whole network and introduce the spatiotemporal decoder to reconstruct the original video.\nExhaustive experiments demonstrate the effectiveness of SweetTokenizer. Compared with SOTA methods [45], SweetTokenizer achieves comparable rFVD on the UCF-101 [34] while utilizing only 25% of the tokens. Notably, it enhances the gFVD metric from 191 to 128. At an equivalent compression rate on UCF-101, SweetTokenizer reduces rFVD from 42 to 18 and gFVD from 191 to 154. On the ImageNet-1k [8], SweetTokenizer demonstrates a substantial improvement in rFID, decreasing it from 0.59 to 0.37.\nIn summary, our work makes the following key contributions:\n\u2022 We introduce SweetTokenizer, a cutting-edge visual tokenizer that achieves decent reconstruction fidelity with a high compression ratio via spatial-temporal decoupling and cross-attention query autoencoder, reaching a \"sweet spot\" between compression and fidelity.\n\u2022 We leverage a semantic-enhanced latent codebook to utilize the off-the-shell representation capability of LLM embeddings, which improves reconstruction quality and facilitates downstream video understanding tasks.\n\u2022 We design a progressive training schedule following the curriculum learning mechanism, which ensures better convergence of visual tokenizers.\n\u2022 We perform extensive experiments to verify the effectiveness of SweetTokenizer, which exhibits the state-of-the-art performance on video reconstruction, image reconstruction, and class-conditional video generation tasks, leading by a large margin of 57.1%, 37.2%, and 32.9%."}, {"title": "2. Background", "content": "2.1. Visual Tokenizer With Vector Quantization\nExploring visual tokenizers and their applications in generative models has led to significant advancements in image/video-related tasks. The general idea is to discretize visual data into tokens, then tasks like visual generation [6, 12, 51, 52] & understanding [4, 10, 18, 19, 28, 36, 44, 47] can be tackled in a sequence prediction style as natural language processing [9, 30, 38]. Our work belongs to the series of Vector Quantized Variational AutoEncoder (VQ-VAE) [32, 41] tokenizers, which introduce a discrete latent space for continuous VAE [22] encoder-decoder structure. It typically encodes a high-dimensional image into a low-dimensional latent representation, then queries the nearest index from a learnable codebook to quantize the latent vector, and finally decodes back reversely to reconstruct the raw input signal. Since this type of tokenizer acquires reconstruction loss, it can maintain high-level semantic and low-level details of input vision. VQGAN [11] adopted adversarial training loss to improve high-frequency details. VIT-VQGAN [51] upgraded encoder-decoder with vision-transformer (ViT) architecture [10] and further boosted results. TiTok [56] replaced 2D image structure with 1D sequence latent representation, then used a self-attention transformer [42] to compress token number.\nHowever, the above methods can only process image data. For video modality, TATS [12] used 3D-CNN to encode video patches and adopted sliding windows to deal with long-term relations. CViViT [43] used ViT [10] structure to encode spatial patches and then adopted a causal transformer to model temporal information. OmniTokenizer [45] and MAGVIT [53, 55] adopted similar trans-"}, {"title": "2.2. Language-based Latent Codebook", "content": "The codebooks learned by vanilla VQ-VAEs are not interpretable with lexical meanings. Therefore, many works attempt to utilize pretrained language models embedding codebooks to enhance semantics. LQAE [29] replaced the visual codebook with frozen word embeddings from BERT [9]. SPAE [54] quantized image latent space in a pyramid structure to preserve semantic information from low-level to high-level. It also used large language model (LLM) codebook [7] so that the encoded image token can be directly adapted to visual understanding tasks through in-context learning [3] ability of LLM. We follow this evaluation pipeline for few-shot classification in our paper. V2L-Tokenizer [59] utilized CLIP [31] pretrained encoder and injected a learnable projector to align visual-text latent space implicitly. VQCT [57] replaced the projector with graph convolution networks [24] to consider the relationship between vocabularies. Furthermore, De-Diffusion [48] directly encoded image into plain text as latent space interface and decodes back through a text-to-image (T2I) diffusion model [33]. However, none of these works dives deeply into the codebook design for video modality. Therefore, we propose splitting the codebook according to the video's spatial-temporal attribute, which uses nouns & adjectives for spatial information, and verbs & adverbs for temporal information. This design helps us better align visual-motion-text semantics."}, {"title": "3. Method", "content": "3.1. Preliminary\nA typical visual vector-quantization (VQ) model [12, 45, 53, 55] contains three parts: encoder E, decoder D and latent quantizer Q. Take video modality as an example, given a video input $x \\in R^{T\\times H\\times W\\times 3}$, where T represents the temporal length and H \u00d7 W denotes spatial resolution, encoder E(x) projects it into latent space $Z \\in R^{N\\times D}$, where D is latent dimension and N is token number. A quantizer Q is constructed in this latent space Z by querying the nearest neighbor in codebook $C\\in R^{L\\times D}$, where L is codebook size. Then D decodes latent space back to pixel space and applies self-supervised reconstruction loss:\n$\\mathcal{L}_{rec}(x, D(Q(E(x)))).$\nWe aim to learn a more compact latent space Z while maintaining the reconstruction fidelity. To this end, we design a semantic-enhanced spatial temporal decoupled tokenizer (SweetTokenizer) to address the issue of token compression (Figure 2). The main component is Cross-attenion Query AutoEncoders (CQAE), which aggregates information from disentangled spatial-temporal dimensions into learnable queries, achieving a 4\u00d7 compression ratio improvement over previous video modality tokenizers. To enhance latent space representation and reconstruction fidelity, we incorporate compact text embeddings and refine vocabularies by appearance/motion attributes for better semantic alignment. Finally, we propose a progressive training schedule to stabilize convergence.\n3.2. Spatial-Temporal Tokenization\n3.2.1 Patchify\nGiven a video frame sequence $x \\in R^{T\\times H\\times W\\times 3}$, we select the first frame $x_0$ as a reference for spatial information, the"}, {"title": "3.2.2 Cross-attention Query AutoEncoder (CQAE)", "content": "So far, the raw video has been split into patches with a fixed ratio $p_t \u00d7 P_h \u00d7 P_w$. However, many works [27, 37, 56] have demonstrated that visual information remains redundant spatially and temporally, even after tokenization. To select \"key words\" from these tokens, we adopt a strategy similar to Q-Former [27], compressing patch sequence into a fixed number of query tokens via cross-attention interactions. As an innovation, we recursively inject these cross-attention query modules into transformer-based au-"}, {"title": "3.2.3 Language-based Latent Codebook", "content": "Previous works [29, 54, 57] have shown that text representations can enhance image VQ-VAEs, as the text provides additional semantic information from pre-trained language models. However, previous works mainly focus on the relationship between static image appearance and text semantics. Our experiment in Table 5 shows this is insufficient for video data, as static and motion information are typically embedded in different subsets of vocabularies.\nTo address this, we construct two separate codebooks, the spatial quantization codebook and the temporal quantization codebook, according to vocabulary attributes, as illustrated in Figure 2. We first extract candidate vocabularies from video captions, which is obtained by Qwen-2.5B [1] instruct model. We filter out words that occur with low frequency. Afterward, we extract Qwen-2.5B [1] text embedding of these vocabularies to fill in the columns of our codebook $C\\in R^{L\\times D}$ and split it into four subsets: nouns, adjectives, verbs, and adverbs. Note that $C_{noun}, C_{adj}$ are for spatial latent tokens $\\Omega_s$ while $C_{verb}, C_{adv}$ are for temporal latent tokens $\\Omega_t$.\nGiven two encoded continuous latent vectors: $z_s \u2208 \u03a9_s$ and $z_t \u2208 \u03a9_t$, $z_s$ is passed through spatial quantization codebook, and $z_t$ is passed through temporal quantization codebook. The quantized $\\hat{z}_s$ and $\\hat{z}_t$ are obtained by nearest neighbor searching:\n$z_s, z_t = E(x),$\n$\\hat{z}_s = P(c_i), i = \\underset{c_i \\in C_{noun} \\cup C_{adj}}{arg \\min} || z_s - P(c_i)||,$\n$\\hat{z}_t = P(c_i), i = \\underset{c_i \\in C_{verb} \\cup C_{adv}}{arg \\min} ||z_t - P(c_i)||$.\nTo maintain text semantic information, we freeze the codebook and use a tiny projector network P to map from text space into visual space, following the strategy in [57]. Finally, the gradient is passed to the encoder via vector-quantization commitment loss proposed in [41], a common method to approximate differentiability:\n$\\mathcal{L}_{vq} = ||sg[z_s] \u2013 Q(z_s)||^2 + ||\\hat{z}_s - sg[Q(z_s)]||^2 $\n$+||sg[z_t] - Q(z_t)||^2 + ||\\hat{z}_t - sg[Q(z_t)]||^2$\nwhere sg[] is stop-gradient operator. Figure 4 also shows that the encoded latent words by SweetTokenizer capture semantic meanings related to both visual appearance and motion."}, {"title": "3.2.4 Progressive Training Schedule", "content": "To ensure better convergence, we follow the philosophy of curriculum learning [2], thus designing a progressive training schedule comprising three stages."}, {"title": "4. Experiments", "content": "4.1. Experiments Settings\nDataset. We evaluate the tokenization performance of SweetTokenizer on image and video datasets, including ImageNet [8], UCF-101 [34], and Kinetics-600 [5, 21]. Following [45], all images and video frames are resized to 256 \u00d7 256 resolution for experiments. The semantic capabilities of SweetTokenizer are tested through few-shot image classification on Real-Name Open-Ended miniImageNet [39] and few-shot video action recognition on UCF-101, as described in [58].\nEvaluation Metrics. For video reconstruction experiments, we evaluate using the Reconstruction Frechet Video Distance (rFVD) [40]. For video generation, we use the Generation Frechet Video Distance (gFVD) metric. For image reconstruction, we categorize recent methods by the number of compressed tokens, with each group assessed using the Frechet Inception Distance (FID) [15].\nImplementation Details. SweetTokenizer adopts a spatial-temporal architecture consisting of 8 spatial layers and 4 temporal layers, with both the encoder and decoder configured to a hidden dimension of 512. The latent space dimension is set to 256. For the LLM codebook quantizer, we exclude words with a frequency below 5, resulting in"}, {"title": "4.2. Video Reconstruction & Generation", "content": "We first evaluate the tokenization capability of SweetTokenizer on the UCF-101 and K-600 video datasets. As shown in Table 1, SweetTokenizer uses only 1,280 tokens (256 spatial tokens and 1,024 temporal tokens), which is four times fewer than OmniTokenizer's 5,120 tokens. Despite this significant reduction, it achieves comparable performance, with rFVD scores of 44.35 on UCF-101 and 27.95 on K-600, demonstrating its effectiveness in compressing tokens while maintaining high fidelity. Notably, at the same compression ratio, SweetTokenizer* significantly outperforms all baselines, achieving an rFVD of 18.74 on UCF-101, 57.1% lower than OmniTokenizer's 42.35, and an rFVD of 7.51 on K-600, 71.0% lower than OmniTokenizer's 25.97.\nThe generative capability of SweetTokenizer is evaluated on UCF-101 in a class-conditional generation task. SweetTokenizer is used to extract quantized spatial tokens ($\\hat{\\Omega}_s$) and temporal tokens ($\\hat{\\Omega}_t$) from UCF videos. These tokens"}, {"title": "4.3. Image Reconstruction", "content": "We evaluate the image tokenization performance of SweetTokenizer on ImageNet by fine-tuning the spatial branch CQAE with a pixel-level loss during Stage 3. As shown"}, {"title": "4.4. Ablation Studies", "content": "Spatial-Temporal Decoupling. We demonstrate that naively flattening video tokens into a 1D sequence is infeasible, as shown in Table 4. Training SweetTokenizer without decoupling results in significantly degraded performance. We attribute this degradation to two factors: (1) the flattening operation discards substantial consecutive temporal information, and (2) without decoupling, the model cannot leverage the crucial spatial pretraining from large-scale image datasets, as further discussed in the next paragraph."}, {"title": "4.5. Visual Semantic Comprehension", "content": "Few-Shot Visual Classification. To evaluate the semantic capabilities of SweetTokenizer, we conducted experiments on few-shot image classification and video action recognition tasks. In both experiments, we initially extracted visual tokens using SweetTokenizer and transformed them into natural language words via our LLM codebook. Subsequently, we employed CLIP to compute the similarity between the visual inputs and text embeddings. The top 21 tokens with the highest similarity were selected to form a prompt for prediction using the Qwen LLM. For the image classification task, we adhered to the V2L protocol [59], comparing SweetTokenizer against other language-based visual tokenizers SPAE [54] and V2L. In the video action recognition task, we used ARN [58] and HF-AR [25] as baselines. The results in Table 6 indicate that SweetTokenizer achieved an accuracy of 90.8% on the miniImageNet dataset, surpassing SPAE 89.9% and V2L 87.6%. On the UCF-101 dataset, SweetTokenizer attain an average accuracy of 90.1%, outperforming ARN 83.1% and HF-AR 86.4%. These findings demonstrate SweetTokenizer robust semantic understanding and superior performance in both image and video tasks."}, {"title": "5. Conclusions", "content": "We present SweetTokenizer, an efficient visual tokenization framework that compresses spatial and temporal information through the cross-attention query autoencoder. Combined with language-based latent spaces and progressive training schedule, SweetTokenizer reduces token count by 4\u00d7 for video data while maintaining high reconstruction fidelity. Our approach offers a compact representation of visual data, making it well-suited for downstream tasks such as visual generation, visual recognition, marking a significant step in efficient visual tokenization."}, {"title": "6. Experimental Settings", "content": "6.1. Model Implementation Details\nVisual Tokenizer. The tokenizer is composed of an encoder E, decoder D, and latent quantizer Q. The tokenizer takes a video clip of 17 consecutive frames with a resolution of 256 \u00d7 256 with the elements normalized to [-0.5, 0.5] as input. Then the video clip will be patchified to a resolution of 1 x 32 x 32 spatial feature and 4 \u00d7 32 \u00d7 32 temporal feature as illustrated in the main paper. The encoder & and decoder D in our tokenizer are both composed of 8 CQAES modules and 4 CQAEt modules with 512 hidden states and 8 attention heads. Each modules consists of self-attention, feed-forward and cross-attention layers. Before the attention computation, the visual features will be reshaped into [(BT) \u00d7 (HW) \u00d7 D] and [(BHW) \u00d7 T \u00d7 D] for CQAES and CQAEt modules, respectively. The encoder & generates 256 spatial and 1024 temporal continuous latent tokens. These tokens are then passed to the quantizer Q, which produces the quantized spatial and temporal latent tokens. The\n6.2. Training Datasets\nUCF-101. UCF-101 is a large-scale action recognition dataset consisting of 13,320 videos with 9537 for training and 3783 for testing across 101 action categories. The dataset includes videos with significant variations in camera motion, object appearance, scale, viewpoint, cluttered backgrounds, and lighting conditions, making it one of the most challenging datasets for action recognition.\nKinetic-600.\nKinetics-600 is a large-scale action recognition dataset containing approximately 480K videos across 600 action categories. The dataset is split into 390K training, 30K validation, and 60K test videos. Each video is a 10-second clip extracted from raw YouTube footage, focusing on key action moments.\nImageNet-1K. ImageNet-1K is a widely used subset of the larger ImageNet dataset, specifically designed for image classification tasks. It contains 1.2 million labeled images across 1,000 distinct categories, ranging from animals and plants to everyday objects and scenes. Each category in ImageNet-1K includes a set of training images, along with"}, {"title": "6.3. Notations", "content": "The meaning of our notations appeared in the main paper are explained in Table 7."}, {"title": "6.4. Training Settings", "content": "The detailed training hyper-parameter settings for SweetTokenizer are reported in Table 9."}, {"title": "7. Additional Results", "content": "7.1. Generation results for ImageNet\nFollowing the OmniTokenizer protocol [45], we train a VideoGPT-based generator on ImageNet-1K. As shown in Table 8, SweetTokenizer consistently outperforms all baselines in terms of FID. With fewer tokens, SweetTokenizer achieves 5.48 FID, outperforming OmniTokenizer by 26.4 %. These results align with findings on UCF-101, reinforcing that fewer tokens lead to better generation performance."}, {"title": "7.2. More Visualizations", "content": "Fig 8 and Fig 7 visualize the reconstruction results for the UCF-101 and K-600 datasets. The pixel-level differences between ground truth and model are shown, with brighter areas indicating greater disparity and darker areas reflecting consistency. As shown, SweetTokenizer exhibits fewer reconstruction differences compared to OmniTokenizer, demonstrating its superior performance.\nFig 8 and Fig 9 visualize the reconstruction and generation results of SweetTokenizer on ImageNet-1K. For reconstruction, differences between models are highlighted in red blocks, with details shown in green blocks. Clearly, SweetTokenizer outperforms all baselines by a significant margin.\nFinally, we visualize the words from our LLM codebook in Fig 10, based on few-shot video action recognition tasks"}, {"title": "8. Limitations", "content": "Our tokenizer is not suitable for tasks requiring precise semantic understanding, like VQA, because the LLM codebook is trained in an unsupervised manner. Without additional constraints, such as contrastive learning between image features from Qwen-VLM and text embeddings in our codebook, aligning the image and text domains is challenging. A promising direction for future work is to enhance SweetTokenizer into a semantically strong tokenizer by contrastive learning."}]}