{"title": "Memory\u00b3: Language Modeling with Explicit Memory", "authors": ["Hongkang Yang", "Zehao Lin", "Wenjin Wang", "Hao Wu", "Zhiyu Li", "Bo Tang", "Wenqiang Wei", "Jinbo Wang", "Zeyun Tang", "Shichao Song", "Chenyang Xi", "Yu Yu", "Kai Chen", "Feiyu Xiong", "Linpeng Tang", "Weinan E"], "abstract": "The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining \"abstract knowledge\". As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named Memory\u00b3, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have enjoyed unprecedented popularity in recent years thanks to their extraordinary performance. The prospect of scaling laws and emergent abilities constantly drives for substantially larger models, resulting in the rapid increase in the cost of LLM training and inference. People have been trying to reduce this cost through optimizations in various aspects, including architecture, data quality, operator, parallelization, optimizer, scaling laws, generalization theory, hardware, etc.\nWe introduce the novel approach of optimizing knowledge storage. The combined cost of LLM training and inference can be seen as the cost of encoding the knowledge from text data into various memory formats, plus the cost of reading from these memories during inference:\n$\\sum_{\\text{knowledge } k} \\min_{\\text{format } m} \\text{cost}_{\\text{write}}(k, m) + n_k \\cdot \\text{cost}_{\\text{read}}(k, m)$\nwhere $\\text{cost}_{\\text{write}}$ is the cost of encoding a piece of knowledge $k$ into memory format $m$, $\\text{cost}_{\\text{read}}$ is the cost of integrating $k$ from format $m$ into inference, and $n_k$ is the expected usage count of this knowledge during the lifespan of this LLM (e.g. a few months for each version of ChatGPT). The definitions of knowledge and memory in the context of LLMs are provided in Section 2, and this paper uses knowledge as a countable noun. Typical memory formats include model parameters and plain text for retrieval-augmented generative models (RAG); their write functions and read functions are listed in Table 3, and their $\\text{cost}_{\\text{write}}$ and $\\text{cost}_{\\text{read}}$ are provided in Figure 4.\nWe introduce a new memory format, explicit memory, characterized by moderately low write cost and read cost. As depicted in Figure 1, our model first converts a knowledge base (or any text dataset) into explicit memories, implemented as sparse attention key-values, and then during inference, recalls these memories and integrates them into the self-attention layers. Our design is simple so that most of the existing Transformer-based LLMs should be able to accommodate explicit memories with a little finetuning, and thus it is a general-purpose \u201cmodel amplifier\". Eventually, it should reduce the cost of pretraining LLMs, since there will be much less knowledge that must be stored in parameters, and thus less training data and smaller model size.\nThe new memory format enables us to define a memory hierarchy for LLMs:\nplain text (RAG) \u2192 explicit memory \u2192 model parameter\nsuch that by going up the hierarchy, $\\text{cost}_{\\text{write}}$ increases while $\\text{cost}_{\\text{read}}$ decreases. To minimize the cost (1), one should store each piece of knowledge that is very frequently/rarely used in the top/bottom of this hierarchy, and everything in between as explicit memory. As illustrated in Table 3, the memory hierarchy"}, {"title": "2 Memory Circuitry Theory", "content": "This section introduces our memory circuitry theory, which defines knowledge and memory in the context of LLM. We will see that this theory helps to determine which knowledge can be stored as explicit memory, and what kind of model architecture is suitable for reading and writing explicit memories. For readers interested primarily in the results, it may suffice to review Claim 1 and Remark 1 before proceeding to the subsequent sections. The concepts to be discussed are illustrated in Figure 5."}, {"title": "2.1 Preliminaries", "content": "The objective is to decompose the computations of a LLM into smaller, recurring parts, and analyze which parts can be separated from the LLM. These small parts will be defined as the \"knowledge\" of the LLM, and this characterization helps to identify what knowledge can be externalized as explicit memory, enabling both the memory hierarchy and a lightweight backbone.\nOne behaviorist approach is to define the smaller parts as input-output relations between small subsequences, such that if the input text contains a subsequence belonging to some pattern, then the output text of the LLM contains a subsequence that belongs to some corresponding pattern.\nOne specific input-output relation is that if the immediate context contains \"China\" and \"capital\", then output the token \"Beijing\".\nOne abstract input-output relation is that if the immediate context is some arithmetic expression (e.g. \"123 \u00d7 456 =\") then output the answer (e.g. \u201c56088\").\nOne abstract relation that will be mentioned frequently is the \u201csearch, copy and paste\u201d , such that if the context has the form \u201c. . . [a][b]. . . [a]\u201d then output \u201c[b]\", where [a] and [b] are arbitrary tokens.\nA decomposition into these relations seems natural since autoregressive LLMs can be seen as upgraded versions of n-grams, with the fixed input/output segments generalized to flexible patterns and with the plain lookup table generalized to multi-step computations.\nNevertheless, a behaviorist approach is insufficient since an input-output relation alone cannot uniquely pin down a piece of knowledge: a LLM may answer correctly to arithmetic questions based on either the actual knowledge of arithmetic or memorization (hosting a lookup table for all expressions such as\""}, {"title": "2.2 Knowledge", "content": "We begin with the definition of the knowledge of LLMs. For now, it suffices to adopt heuristic definitions instead of fully rigorous ones. Throughout this section, by LLM we mean autoregressive Transformer LLM that has at least been pretrained. Let $L$ be the number of Transformer blocks and $H$ be the number of attention heads at each attention layer, and the blocks and heads are numbered by $l = 0, ... L - 1$ and $h = 0, ... H - 1$. There are in total $2L$ layers (MLP layers and attention layers), and the input features to these layers are numbered by $0, ... 2L \u2013 1$.\nDefinition 1. Given an LLM and a text $t = (t_0, ... t_n)$, the computation graph $G$ on input $(t_0,... t_{n-1})$ and target $(t_1,... t_n)$ is a directed graph with weighted edges such that\nIts nodes consist of the hidden vectors $x_i^l$ before all attention layers, the hidden vectors $x_i^{2l+1}$ before all MLP layers, and the output vectors $x_i^{2l+2}$, for all blocks $l = 0, ... L \u2212 1$ and positions $i = 0, ... n - 1$.\nIts directed edges consist of each attention edge $e_{i,j}^{l,h}$ that goes from $x_i^l$ to $x_i^{2l+1}$ at the h-th head of the l-th attention layer for all $l,h$ and $i < j$, as well as each MLP edge $e_{i,m}^{l}$ that goes from $x_i^{2l}$ to $x_i^{2l+1}$ through the m-th neuron of the l-th MLP layer for all $l, m, i$.\nThe weight of each attention edge $e_{i,j}^{l,h}$, which measures the influence of the attention score $a_{i,j}^{l,h}$ on the LLM output, is defined by\n$\\frac{dL}{da_{i,j}^{L-Cl=0}}$ or $\\frac{\\partial L}{\\partial a_{i,j}^{1,h}}$\nwhere $L$ is the log-likelihood of the target $(t_1,...t_n)$, with $L|_{a=0}$ obtained by setting $a = 0$ (i.e. causal intervention). Similarly, the weight of each MLP edge $e_{i,m}^{l}$, which measures the influence of the neuron activation $a_m^l$ on the LLM output, is defined likewise.\nGiven any subgraph $S \u2286 G$, define the associated input of $S$ as a subsequence $t_{in}(S) \u2286 (t_0, ...t_{n-1})$ such that a token $t_i$ belongs to $t_{in}(S)$ if and only if $||x_a||$ is large for some attention edge (or MLP edge) in $S$ with attention score (or activation) $a$.\nSimilarly, define the associated output of the subgraph $S$ as a subsequence $t_{out}(S) \u2286 (t_1,...t_n)$ such that a token $t_i$ belongs to $t_{out}(S)$ if and only if\n$\\frac{L_i}{Li-Lila}$ or $\\frac{\\partial Li}{\\partial a}$\nis large for some attention edge (or MLP edge) in $S$ with attention score (or activation) $a$. Here $L_i$ is the log-likelihood of $t_i$ with respect to the LLM output.\nDefinition 2. Given two computation graphs $G_1, G_2$ of an LLM and their subgraphs $S_1, S_2$, a mapping $f$ from the nodes of $S_1$ to the nodes of $S_2$ (not necessarily injective) is a homomorphism if\nevery node at depth $l \\in {0,... 2L}$ is mapped to depth $l$,\nif two nodes are on the same position $i$, then they are mapped onto the same position,\nif two nodes share an edge on attention head $h$ or MLP neuron $m$, then their images also share an edge on head $h$ or neuron $m$.\nIf such a homomorphism exists, then we say that $S_1$ is homomorphic to $S_2$.\nIt may be more convenient to define the mapping to be between the input tokens of two sentences, but we adopt the current formulation as it is applicable to more general settings without an obvious correspondence between the tokens and the hidden features at each layer."}, {"title": "2.3 Memory", "content": "Now the question is what knowledge can be separated from the model parameters and moved to the lower levels of the memory hierarchy.\nDefinition 6. A knowledge $K$ of the reference LLM is separable if there exists another LLM $M$ such that\nM does not possess this knowledge, such that for any realization $t$ of $K$, the model $M$ cannot generate each token of the associated output $t_{out}$ with high probability, e.g. $P_M(t_i|t_0...t_{i-1}) \u2264 1/2$ for some $t_i \u2208 t_{out}$.\nThere exists a text $t^*$ such that for any realization $t$ of $K$, the model $M$ using $t^*$ as prefix can generate each token of the associated output $t_{out}$ with high probability, e.g. $P_M(t_i|t^*t_0...t_{i-1}) \u2265 0.9$ for every $t_i \u2208 t_{out}$.\nIf among the realizations of $K$, the same associated input $t_{in}$ can correspond to multiple associated outputs $t_{out}$, then the above probabilities are summed over all branches if position $i$ is a branching point.\nDefinition 7. A separable knowledge $K$ of the reference LLM is imitable if any realization $t'$ of $K$ can be used as the prefix $t^*$ in Definition 6, e.g. for any realizations $t, t'$ of $K$, we have $P_M(t_i|t't_0 ... t_{i-1}) \u2265 0.9$ for every $t_i \u2208 t_{out}$.\nBasically, imitability means that LLMs can achieve the same effect as possessing this knowledge by retrieving example texts that demonstrate this knowledge. Few-shot prompting can be seen as a special case of providing realizations.\nSeparability is a more general property than imitability. For instance, one can set the prefix $t$ to be an abstract description of $K$ instead of its realization, and this is reminiscent of instruction prompting. Nevertheless, it is not obvious whether the set of separable knowledges is strictly larger than the set of imitable knowledges.\nClaim 1. Every specific knowledge $K$ is imitable and thus is separable.\nProof (informal). Without loss of generality, we can assume that for any realization $t$ of $K$, all tokens of the associated input $t_{in}$ precede all tokens of the associated output $t_{out}$. Otherwise, we can split $t_{in}$ into two halves $t_1, t_2$ that precedes/does not precede $t_{out}$, and split the corresponding subgraph $S \u2208 K$ into two halves $S_1, S_2$ that have high weights with respect to $t_1, t_2$. Using monotonicity arguments once Definition 3 is fully formalized, one can try to show that this splitting is invariant across $S \u2208 K$ and therefore the sets of $S_1, S_2$ are two specific knowledges.\nConsider sequences of the form [a][b]. . . [a'][b'], where [a], [a'] (or [b], [b']) could be the associated inputs (or outputs) of any subgraphs $S, S' \u2208 K. By Definition 4, [a] and [a'] always share some interpretable meaning, while [b] and [b'] are approximately the same sequence. One can construct an abstract knowledge that completes [a][b]... [a'] with [b']: the first part of this circuit detects the common feature of the [a]'s (possibly overlapping with the subgraphs of $K), the second part is an induction head (analogous"}, {"title": "3 Inference Process", "content": "From now on, we refer to the realizations of separable knowledges (Definitions 5 and 6) as references. Our knowledge base (or reference dataset) consists of 1.1 \u00d7 10\u2078 text chunks with length bounded by 128 tokens. Its composition is described in Section 4.4.\nEach reference can be converted to an explicit memory, which is a tensor with shape\n(memory layers, 2, key-value heads, sparse tokens, head dimension) = (22, 2, 8, 8, 80)\nThe 2 stands for the key and value, while the other numbers are introduced later.\nBefore inference, the Memory\u00b3 model converts all references to explicit memories and save them on drives or non-volatile storage devices. Then, at inference time, whenever (the id of) a reference is retrieved, its explicit memory is loaded from drives and sent to GPU to be integrated into the computation of Memory\u00b3. By Remark 1, a reference during encoding does not need to attend to any other texts (e.g. other references or query texts), so it is fine to encode each reference independently prior to inference. Such isolation also helps to reduce the compute of attention.\nOne can also employ a \"cold start\" approach to bypass preparation time: each reference is converted to explicit memory upon its initial retrieval, rather than prior to inference. Subsequent retrievals will then access this stored memory. The aforementioned inference with precomputed explicit memories will be called \"warm start\"."}, {"title": "3.2 Writing and Reading Memory", "content": "Each explicit memory is a subset of the attention key-values from a subset of attention heads when encoding a reference. Thus, during inference, the LLM can directly read the retrieved explicit memories through its self-attention layers by concatenating them with the usual context key-values. Specially, for each attention head h at layer l, if it is chosen as a memory head, then its output $Y^{l,h}$ changes from the usual\n$Y^{l,h} = \\text{softmax}(\\frac{X^{l,h}W^{l,h}_Q \\cdot (X^{l,h}W^{l,h}_K)^T}{\\sqrt{d_h}})X_V^{[:2]}W^{l,h}_V$,\nwhere $X^{[i]}$ denotes all tokens before or at position $i$ and $d_h$ denotes the head dimension, to\n$Y^{l,h} = \\text{softmax}(\\frac{X^{l,h}W^{l,h}_Q \\cdot \\text{concat}(K_{1}^{l,h}, ..., K_{4}^{l,h}, X^{l,h}W^{l,h}_K)^T}{\\sqrt{d_h}}) \\text{concat}(V_{1}^{l,h},...,V_{4}^{l,h}, X^{l,h}W^{l,h}_V)W_V^{l,h}$\nwhere each $(K_j, V_j)$ denotes the keys and values of an explicit memory.\nWhile the context BOS token is <s> as usual, when encoding each reference we modify the BOS to \u201cReference:\u201d to help the LLM distinguish between encoding normal texts and encoding references. This modified BOS is also prepended to the context during inference, as illustrated in Figure 9, while the context BOS token now serves as a separator between the references and context. Unlike the explicit memories which only appear at a subset of attention heads, this modified BOS is placed at every head at every layer. The motivation is that since the context BOS can attend to the references, its feature is no longer constant, so the LLM needs the modified BOS to serve as the new constant for all attention heads.\nFurthermore, we adopt parallel position encoding for all explicit memories, namely the positions of all their keys lie in the same interval of length 128, as depicted in Figure 9. We use the rotary position encoding (ROPE). The token sparsification is applied after RoPE processes the attention keys, so the selected tokens retain their relative positions in the references. Besides flexibility, one motivation for parallel position is to avoid the \u201clost in the middle\u201d phenomenon , such that if the references are positioned serially, then the ones in the middle are likely to be ignored. Similarly, token sparsification also helps to alleviate this issue by making the attention more focused on the important tokens. We note that designs analogous to the parallel position have been used to improve in-context learning and long-context modeling ."}, {"title": "3.3 Memory Sparsification and Storage", "content": "One of the greatest challenges for explicit memories is that the attention key-values occupy too much space. They not only demand more disk space, which could be costly, but also occupy GPU memory during inference, which could harm the batch size and thus the throughput of LLM generation. An intense compression is needed to save space. The full attention key tensor (or value tensor) for each reference has shape (layers, key-value heads, tokens, head dimension), so we compress all four dimensions.\nRegard layers, we only set the first half of the attention layers to be memory layers, i.e. layers that produce and attend to explicit memories , while the second half remain as the usual attention layers. Note that Remark 1 suggests that it is usually the attention heads in the middle of the LLM that attend to the references. So it seems that appointing the middle attention layers (e.g. the ones within the 25% to 75% depth range) to be memory layers is a more sensible choice. This heuristic is supported by the observations in that the attention to the distant context usually takes place in the middle layers.\nRegarding heads, we set all key-value heads at each memory layer to be memory heads. We reduce their amount by grouped query attention (GQA), letting each key-value head be shared by multiple query heads, and obtain 20% sparsity (8 versus 40 heads). It is worth mentioning that, besides GQA and memory layers, another approach is to select a small subset of heads that are most helpful for reading memories, and this selection does not have to be uniform across layer. We describe several methods for selecting memory heads in Remark 4.\nRegarding tokens, we select 8 tokens out of 128 for each key-value head. We choose a high level of sparsity, since Remark 1 indicates that the attention from the context to the references are expected to be"}, {"title": "3.4 | Model Shape", "content": "As discussed in Section 2.3, the specific knowledges can be externalized to explicit memories, and thus to minimize the total cost (1), the model parameters (or implicit memory) only need to store abstract knowledges and the subset of specific knowledges that are frequently used. The shape of our model, i.e. (the number of Transformer blocks L, heads H, head dimension $d_h$, width of the MLP layers W), is chosen"}, {"title": "3.5 Training Designs", "content": "Similar to our architecture design, the design of our training scheme focuses on learning abstract knowledges. The goal is to reduce the training compute, as the LLM no longer needs to memorize many of the specific knowledges. This shift in learning objective implies that all the default settings for pretraining LLMs may need to be redesigned, as they were optimized for the classical scenario when the LLMs learn both abstract and specific knowledges.\n1. Data: Ideally, the pretraining data should have a high concentration of abstract knowledges and minimum amount of specific knowledges. It is known that LLM pretraining is very sensitive to the presence of specific knowledges. For instance, observes that a small model can master arithmetic (e.g. addition of large numbers) if trained on clean data. However, if the training data is mixed with trivial information (e.g. random numbers), then the test accuracy stays at zero unless the model size is increased by a factor of 1500. It suggests that training on specific knowledges significantly inhibits the learning of abstract knowledges, and may explain why emergent abilities are absent from small models. Notably, the Phi-3 model is pretrained with a data composition that closely matches our desired composition. Although the technical details are not revealed, it is stated that they filter data based on two criteria: the data should encourage reasoning, and should not contain information that is too specific.\n2. Initialization: observes that initializing Transformer parameters with a smaller standard deviation (de with c < -1/2 instead of the usual $d_e(-1/2)$ can encourage the model to learn compositional inference instead of memorization. Specially, an arithmetic dataset is designed with a train set and an out-of-distribution test set, which admits two possible answers. One answer relies on memorizing more rules during training, while the other requires an understanding of the compositional structure underlying these rules. The proposed mechanism is that training with smaller initialization belongs to the condensed regime that encourages sparse solutions, contrary to training with large initialization that belongs to the kernel regime or critical regime .\n3. Weight decay: observe that using a larger weight decay coefficient (i.e. greater than the usual range of 0.001 ~ 0.1) can guide LLMs to favor generalization over memorization, and accelerate the learning of generalizable solutions. They consider settings that exhibit grokking such that training would transit from perfect train accuracy and zero test accuracy to perfect test accuracy, and generalization ability is measured by how quickly this transition occurs. Moreover, theoretically speaking, it is expected that training generative models needs stronger regularization than training regression models, in order to prevent the generated distributions from collapsing onto the training data and become trivial .\nIn summary, it is recommendable to pretrain the Memory\u00b3 model with a data composition that emphasizes abstract knowledges and minimizes specific information, a smaller initialization for parameters, and a larger weight decay coefficient.\nSince this work is only a preliminary version of Memory\u00b3, we decide to stick with the conventional setting for training and have not experimented with any of these ideas. We look forward to incorporating these designs in future versions of the Memory\u00b3 model."}, {"title": "3.6 Two-stage Pretrain", "content": "The Memory\u00b3 model learns to write and read explicit memories during pretraining. The training data is prepended with retrieved references; the model encodes these references into explicit memories in real time, and integrates them into the self-attention computation of the training data.\nUnexpectedly, our pretraining consists of two stages, which we name as warmup and continual train. Only the continual train stage involves explicit memories, while the warmup stage uses the same format as ordinary pretraining. Our motivation is depicted in Figure 11. We observe that pretraining with explicit memories from the beginning would render the memories useless, as there appears to be no gain in training loss compared to ordinary pretraining. Meanwhile, given a checkpoint from ordinary pretraining, continual training with explicit memory exhibits a visible decrease in training loss. This comparison"}, {"title": "4 Pretraining Data", "content": "This section describes the procedures for collecting and filtering our pretraining dataset and knowledge base (or reference dataset)."}, {"title": "4.1 | Data Collection", "content": "The pretrain data is gathered from English and Chinese text datasets, mostly publicly available collections of webpages and books. We also include code, SFT data (supervised finetuning), and synthetic data.\nSpecially, the English data mainly consists of RedPajama V2 , SlimPajama and the Piles , in total 200TB prior to filtering. The Chinese data mainly comes from Wanjuan , Wenshu , and MNBVC , in total 500TB prior to filtering. The code data mainly comes from Github, and we take the subset with the highest repository stars. The SFT data is included since these samples generally have higher quality than the webpages. We use the same data as in SFT training (Section 6.1), except that these samples are treated as ordinary texts during pretraining, i.e. all tokens participate in the loss computation, not just the answer tokens."}, {"title": "4.2 Filtering", "content": "The raw data is filtered with three steps: deduplication, rule-based filtering, and model-based filtering. First, deduplication is performed with MinHash for most of the datasets. One exception is RedPaja-maV2, which already comes with deduplication labels.\nSecond, we devise heuristic, rule-based filters analogous to the ones from . The purpose is to eliminate texts that are ostensibly unsuitable for training, such as ones that only contain webpage source codes, random numbers, or incomprehensible shards. Our filters remove documents with less than 50 words, documents whose mean word lengths exceed 10 characters, documents with 70% of context being non-alphabetic characters, documents whose fractions of unique words are disproportionately high, documents whose entropy of unigrams is excessively low, and so on.\nFinally, we select the subset of data with highest \"quality\", a score produced by a finetuned BERT model. Specially, we sample ten thousand documents and grade them by the XinYu-70B model with prompt-guided generation. The prompt asks the model to determine whether the input text is informative and produce a score between 0 and 5. Then, these scores are used to finetune the Tiny-BERT model , which has only 14M parameters. The hyperparameters of this finetuning are optimized with respect to a held-out validation set. After that, we use this lightweight BERT to grade the entire dataset."}, {"title": "4.3 Tokenizer", "content": "Similar to our dataset, our tokenizer mainly consists of Chinese and English tokens. The English vocabulary comes from the 32000 tokens of the LLaMA2 tokenizer. We include roughly the same amount of Chinese tokens produced from byte-pair encoding (BPE). The BPE is trained on a 20GB Chinese corpus that consists of Chinese news and e-books. After deduplication, the final vocabulary has 60299 tokens."}, {"title": "4.4 Knowledge Base", "content": "The knowledge base (or reference dataset) is used during training and inference as the source of explicit memories, as depicted in Figure 1. It consists of reference texts that are split into token sequences with length 128, as described in Section 3.1.\nHeuristically, a larger knowledge base is always better, as long as it does not contain misinformation, so it is not surprising that the reference dataset of Retro contains its entire pretrain dataset . Nevertheless, the storage of explicit memories is more costly than plain texts despite our sparsification (Section 3.3), and thus to save storage space, we select a small subset of our pretrain dataset as the knowledge base.\nWith a focus on high quality data, we include for references the English Wikipedia, WikiHow, the Chinese baike dataset, the subset of English and Chinese books whose titles appear academic, Chinese news, synthetic data and high quality codes. These texts are tokenized and split into chunks of 128 tokens, resulting in 1.1 \u00d7 10\u2078 references in total.\nOne may be curious whether our knowledge base may contain some of the evaluation questions, rendering our evaluation results (Section 7.1) less credible. To prevent such leakage, we include in our evaluation code a filtering step, such that for each evaluation question, if a retrieved reference has an overlap with the question that exceeds a threshold, then it is discarded. This deduplication is analogous to the one used when preparing for continual train (Section 3.6), with the overlap measured by . The threshold 2/3 is chosen since we observe that typically a reference that contains a question would have an overlap \u2265 80%, while a relevant but distinct reference would have an overlap \u2264 40%."}, {"title": "5 Pretrain", "content": "This section describes the details of the pretraining process. The two-stage pretrain and memory-augmented data follow the designs introduced in Section 3.6. As an interpretation, the Memory\u00b3 model during the warmup stage develops its reading comprehension, which is necessary during the continual train stage for initiating memory formation."}, {"title": "5.1 Set-up", "content": "Training is conducted with the Megatron-DeepSpeed package and uses mixed-precision training with bfloat16 model parameters, bfloat16 activations, and float32 AdamW states. The batch size is around 4 million training tokens with sequence length 2048, not including the reference tokens. The weight decay is the common choice of 0.1.\nWe adopt the \"warmup-stable-decay\" learning rate schedule of MiniCPM , which is reportedly better than the usual cosine schedule in term of training loss reduction. The learning rate linearly increases to the maximum value, then stays there for the majority of training steps, and finally in the last 10% steps decays rapidly to near zero. Our short-term experiments confirm the better performance of this schedule.\nNevertheless, frequent loss spikes and loss divergences are encountered during the official pretraining, so we have to deviate from this schedule and manually decrease the learning rate to stabilize training.\nOriginally, it is planned that both the warmup and continual train stages go through the entire 4T token pretrain dataset (Section 4). Due to the irremediable loss divergences, both stages have to be terminated earlier."}, {"title": "5.2 Warmup Stage", "content": "The training loss and learning rate schedule are plotted in Figure 13. Whenever severe loss divergence occurs, we restart from the last checkpoint before the divergence with a smaller learning rate, and thus the divergences are not shown in the figure. Eventually, the training terminates at around 3.1T tokens, when reducing the learning rate can no longer avoid loss divergence."}, {"title": "5.3 Continual Train Stage", "content": "The explicit memories enter into the Memory\u00b3 model at this stage. The training steps are slower since the model needs to encode the references retrieved for the pretrain data to explicit memories in real time, and each step takes a bit more than twice the time of a warmup step. The training loss and learning rate schedule are plotted in Figure 14.\nThe loss divergence soon becomes irremediable at around 120B training tokens, much shorter than the planned 4T tokens, and training has to stop there. One possible cause is that the continual train is initialized from the latest warmup checkpoint, which is located immediately before the break down of the warmup stage, and thus is already at the brink of divergence. The smaller learning rate of continual train delays the onset of divergence but not for long."}, {"title": "6 Fine-tuning and Alignment", "content": "This section describes our model finetuning, specifically supervised finetuning (SFT) and direct preference optimization (DPO)."}, {"title": "6.1 Supervised Finetuning", "content": "Analogous to the StableLM model , our Memory\u00b3 model is finetuned on a diverse collection of SFT datasets. We use the following datasets, which are publicly accessible on the Hugging Face Hub: UltraChat , WizardLM , SlimOrca , ShareGPT , Capybara , Deita , and MetaMathQA"}, {"title": "6.2 Direct Preference Optimization", "content": "The Memory\u00b3 model is further finetuned by DPO , to align with human preference and improve its conversation skills. The DPO dataset consists of general conversations (UltraFeedback Binarized ), math questions (Distilabel Math ) and codes questions (Synth Code ). The training uses the cosine learning rate schedule with max lr 4 \u00d7 10\u207b\u2076. The inverse temperature \u03b2 of the DPO loss is set to 0.01. The improvement from DPO is displayed in Section 7.2."}, {"title": "7 Evaluation", "content": "We evaluate the general abilities (benchmark tasks), conversation skills, professional abilities (law and medicine), and facutality & hallucination of the Memory\u00b3 model. We also measure its decoding speed. Our model is compared with SOTA LLMs of similar and larger sizes, as well as RAG models."}, {"title": "7.1 | General Abilities", "content": "To evaluate the general abilities of Memory\u00b3, we adopt all tasks from the Huggingface leaderboard and also include two Chinese tasks. Most of the results are displayed in Table 16, while TruthfulQA is listed in Table 19. All results are obtained in bfloat16 format, using the lm-evaluation-harness package and the configuration of HuggingFace Open LLM leaderboard , i.e. the number of few-shot examples and grading methods.\nAs described in Section 4.4, to prevent cheating, a filtering step is included in the retrieval process so that the model cannot copy from references that resemble the evaluation questions.\nThe results of our model without using explicit memory is included, which indicates that explicit memory boosts the average score by 2.51%. In comparison, the score difference between Llama2-7B and 13B is 4.91% while the latter has twice the amount of non-embedding parameters. Thus, it"}]}