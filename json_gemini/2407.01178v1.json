{"title": "Memory\u00b3: Language Modeling with Explicit Memory", "authors": ["Hongkang Yang", "Zehao Lin", "Wenjin Wang", "Hao Wu", "Zhiyu Li", "Bo Tang", "Wenqiang Wei", "Jinbo Wang", "Zeyun Tang", "Shichao Song", "Chenyang Xi", "Yu Yu", "Kai Chen", "Feiyu Xiong", "Linpeng Tang", "Weinan E"], "abstract": "The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining \"abstract knowledge\". As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named Memory\u00b3, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have enjoyed unprecedented popularity in recent years thanks to their extraordinary performance [5, 9, 110, 11, 126, 4, 56, 54]. The prospect of scaling laws [60, 53, 99] and emergent abilities [119, 105] constantly drives for substantially larger models, resulting in the rapid increase in the cost of LLM training and inference. People have been trying to reduce this cost through optimizations in various aspects, including architecture [40, 6, 30, 75, 89, 109], data quality [104, 58, 48, 66], operator [32, 63], parallelization [95, 103, 62, 91], optimizer [71, 124, 117], scaling laws [53, 127], generalization theory [132, 55], hardware [33], etc.\nWe introduce the novel approach of optimizing knowledge storage. The combined cost of LLM training and inference can be seen as the cost of encoding the knowledge from text data into various memory formats, plus the cost of reading from these memories during inference:\n$\\sum\\limits_{\\text{knowledge } k} \\min\\limits_{\\text{format } m} \\text{ cost}_{\\text{write}}(k, m) + n_k \\cdot \\text{ cost}_{\\text{read}}(k, m)$\nwhere $\\text{cost}_{\\text{write}}$ is the cost of encoding a piece of knowledge $k$ into memory format $m$, $\\text{cost}_{\\text{read}}$ is the cost of integrating $k$ from format $m$ into inference, and $n_k$ is the expected usage count of this knowledge during the lifespan of this LLM (e.g. a few months for each version of ChatGPT [86, 102]). The definitions of knowledge and memory in the context of LLMs are provided in Section 2, and this paper uses knowledge as a countable noun. Typical memory formats include model parameters and plain text for retrieval-augmented generative models (RAG); their write functions and read functions are listed in Table 3, and their $\\text{cost}_{\\text{write}}$ and $\\text{cost}_{\\text{read}}$ are provided in Figure 4.\nWe introduce a new memory format, explicit memory, characterized by moderately low write cost and read cost. As depicted in Figure 1, our model first converts a knowledge base (or any text dataset) into explicit memories, implemented as sparse attention key-values, and then during inference, recalls these memories and integrates them into the self-attention layers. Our design is simple so that most of the existing Transformer-based LLMs should be able to accommodate explicit memories with a little finetuning, and thus it is a general-purpose \u201cmodel amplifier\". Eventually, it should reduce the cost of pretraining LLMs, since there will be much less knowledge that must be stored in parameters, and thus less training data and smaller model size.\nThe new memory format enables us to define a memory hierarchy for LLMs:\nplain text (RAG) $\\rightarrow$ explicit memory $\\rightarrow$ model parameter\nsuch that by going up the hierarchy, $\\text{cost}_{\\text{write}}$ increases while $\\text{cost}_{\\text{read}}$ decreases. To minimize the cost (1), one should store each piece of knowledge that is very frequently/rarely used in the top/bottom of this hierarchy, and everything in between as explicit memory. As illustrated in Table 3, the memory hierarchy"}, {"title": "2 Memory Circuitry Theory", "content": "This section introduces our memory circuitry theory, which defines knowledge and memory in the context of LLM. We will see that this theory helps to determine which knowledge can be stored as explicit memory, and what kind of model architecture is suitable for reading and writing explicit memories. For readers interested primarily in the results, it may suffice to review Claim 1 and Remark 1 before proceeding to the subsequent sections. The concepts to be discussed are illustrated in Figure 5."}, {"title": "2.1 Preliminaries", "content": "The objective is to decompose the computations of a LLM into smaller, recurring parts, and analyze which parts can be separated from the LLM. These small parts will be defined as the \"knowledge\" of the LLM, and this characterization helps to identify what knowledge can be externalized as explicit memory, enabling both the memory hierarchy and a lightweight backbone.\nOne behaviorist approach is to define the smaller parts as input-output relations between small subsequences, such that if the input text contains a subsequence belonging to some pattern, then the output text of the LLM contains a subsequence that belongs to some corresponding pattern.\n\u25a0 One specific input-output relation is that if the immediate context contains \"China\" and \"capital\", then output the token \"Beijing\".\n\u25a0 One abstract input-output relation is that if the immediate context is some arithmetic expression (e.g. \"123 \u00d7 456 =\") then output the answer (e.g. \u201c56088\").\n\u25a0 One abstract relation that will be mentioned frequently is the \u201csearch, copy and paste\" [85], such that if the context has the form \u201c. . . [a][b]. . . [a]\u201d then output \u201c[b]\", where [a] and [b] are arbitrary tokens.\nA decomposition into these relations seems natural since autoregressive LLMs can be seen as upgraded versions of n-grams, with the fixed input/output segments generalized to flexible patterns and with the plain lookup table generalized to multi-step computations.\nNevertheless, a behaviorist approach is insufficient since an input-output relation alone cannot uniquely pin down a piece of knowledge: a LLM may answer correctly to arithmetic questions based on either the actual knowledge of arithmetic or memorization (hosting a lookup table for all expressions such as\""}, {"title": "2.2 Knowledge", "content": "We begin with the definition of the knowledge of LLMs. For now, it suffices to adopt heuristic definitions instead of fully rigorous ones. Throughout this section, by LLM we mean autoregressive Transformer LLM that has at least been pretrained. Let $L$ be the number of Transformer blocks and $H$ be the number of attention heads at each attention layer, and the blocks and heads are numbered by $l = 0, ... L - 1$ and $h = 0, ... H - 1$. There are in total $2L$ layers (MLP layers and attention layers), and the input features to these layers are numbered by $0, ... 2L \u2013 1$.\nDefinition 1. Given an LLM and a text $t = (t_0, ... t_n)$, the computation graph $G$ on input $(t_0,... t_{n-1})$ and target $(t_1,... t_n)$ is a directed graph with weighted edges such that\n\u25a0 Its nodes consist of the hidden vectors $x_i^l$ before all attention layers, the hidden vectors $x_i^{2l+1}$ before all MLP layers, and the output vectors $x_i^{2l+2}$, for all blocks $l = 0, ... L \u2212 1$ and positions $i = 0, ... n - 1$.\n\u25a0 Its directed edges consist of each attention edge $e_{i,j}^{l,h}$ that goes from $x_i^l$ to $x_{j}^{2l+1}$ at the h-th head of the l-th attention layer for all $l, h$ and $i < j$, as well as each MLP edge $e_{i,m}^{l}$ that goes from $x_i^{2l}$ to $x_{i}^{2l+2}$ through the m-th neuron of the l-th MLP layer for all $l, m, i$.\n\u25a0 The weight of each attention edge $e_{i,j}^{l,h}$, which measures the influence of the attention score $a_{ij}^{l,h}$ on the LLM output, is defined by\n$\\dfrac{\\partial \\mathcal{L}}{\\partial a_{i,j}^{l,h}} |_{\\mathcal{L}-Cla=0}$ or $\\dfrac{\\partial \\mathcal{L}_i}{\\partial a_{i,j}^{l,h}}$\nwhere $\\mathcal{L}$ is the log-likelihood of the target $(t_1,...t_n)$, with $\\mathcal{L}|_{a=0}$ obtained by setting $a = 0$ (i.e. causal intervention). Similarly, the weight of each MLP edge $e_{i,m}^{l}$, which measures the influence of the neuron activation $a_{im}^{l}$ on the LLM output, is defined likewise.\n\u25a0 Given any subgraph $S \\subseteq G$, define the associated input of $S$ as a subsequence $t_{in}(S) \\subseteq (t_0, ...t_{n-1})$ such that a token $t_i$ belongs to $t_{in}(S)$ if and only if $||x_a||$ is large for some attention edge (or MLP edge) in $S$ with attention score (or activation) $a$.\n\u25a0 Similarly, define the associated output of the subgraph $S$ as a subsequence $t_{out}(S) \\subseteq (t_1,...t_n)$ such that a token $t_i$ belongs to $t_{out}(S)$ if and only if\n$\\dfrac{\\mathcal{L}_i - \\mathcal{L}_i|_{a=0}}{\\partial a}$ or $\\dfrac{\\partial a}{\\partial a}$\nis large for some attention edge (or MLP edge) in $S$ with attention score (or activation) $a$. Here $\\mathcal{L}_i$ is the log-likelihood of $t_i$ with respect to the LLM output.\nDefinition 2. Given two computation graphs $G_1, G_2$ of an LLM and their subgraphs $S_1, S_2$, a mapping $f$ from the nodes of $S_1$ to the nodes of $S_2$ (not necessarily injective) is a homomorphism if\n\u25a0 every node at depth $l \\in {0,... 2L}$ is mapped to depth $l$,\n\u25a0 if two nodes are on the same position $i$, then they are mapped onto the same position,\n\u25a0 if two nodes share an edge on attention head $h$ or MLP neuron $m$, then their images also share an edge on head $h$ or neuron $m$.\nIf such a homomorphism exists, then we say that $S_1$ is homomorphic to $S_2$.\nIt may be more convenient to define the mapping to be between the input tokens of two sentences, but we adopt the current formulation as it is applicable to more general settings without an obvious correspondence between the tokens and the hidden features at each layer."}, {"title": "3 Design", "content": "This section describes the architecture and training scheme of Memory\u00b3.\nRegarding architecture, the goal is to design an explicit memory mechanism for Transformer LLMS with moderately low write cost and read cost. In addition, we want to limit the modification to the Transformer architecture to be as little as possible, adding no new trainable parameters, so that most of the existing Transformer LLMs can be converted to Memory\u00b3 models with little finetuning. Thus, we arrive at a simple design:\n\u25a0 Write cost: Before inference, the LLM writes each reference to an explicit memory, saved on drives. The memory is selected from the key-value vectors of the self-attention layers, so the write process involves no training. Each reference is processed independently, avoiding the cost of long-context attention.\n\u25a0 Read cost: During inference, explicit memories are retrieved from drives and read by self-attention alongside the usual context key-values. Each memory consists of very few key-values from a small amount of attention heads, thus greatly reducing the extra compute, GPU storage, drive storage and loading time. It allows the LLM to retrieve many references frequently with limited influence on decoding speed.\nRegarding training, the goal is to reduce the cost of pretraining with a more efficient distribution of knowledge. Based on the discussion in Section 2.3, we want to encourage the LLM to learn only abstract knowledges, with the specific knowledges mostly externalized to the explicit memory bank. Ideally, the pretraining cost should be reduced to be proportional to the small amount of knowledge stored in the model parameters, thereby taking a step closer to the learning efficiency of humans."}, {"title": "3.1 Inference Process", "content": "From now on, we refer to the realizations of separable knowledges (Definitions 5 and 6) as references. Our knowledge base (or reference dataset) consists of 1.1 \u00d7 10\u2078 text chunks with length bounded by 128 tokens. Its composition is described in Section 4.4.\nEach reference can be converted to an explicit memory, which is a tensor with shape\n(memory layers, 2, key-value heads, sparse tokens, head dimension) = (22, 2, 8, 8, 80)\nThe 2 stands for the key and value, while the other numbers are introduced later.\nBefore inference, the Memory\u00b3 model converts all references to explicit memories and save them on drives or non-volatile storage devices. Then, at inference time, whenever (the id of) a reference is retrieved, its explicit memory is loaded from drives and sent to GPU to be integrated into the computation of Memory\u00b3. By Remark 1, a reference during encoding does not need to attend to any other texts (e.g. other references or query texts), so it is fine to encode each reference independently prior to inference. Such isolation also helps to reduce the compute of attention.\nOne can also employ a \"cold start\" approach to bypass preparation time: each reference is converted to explicit memory upon its initial retrieval, rather than prior to inference. Subsequent retrievals will then access this stored memory. The aforementioned inference with precomputed explicit memories will be called \u201cwarm start\u201d."}, {"title": "3.2 Writing and Reading Memory", "content": "Each explicit memory is a subset of the attention key-values from a subset of attention heads when encoding a reference. Thus, during inference, the LLM can directly read the retrieved explicit memories through its self-attention layers by concatenating them with the usual context key-values (Figure 9). Specially, for each attention head h at layer l, if it is chosen as a memory head, then its output $Y^{l,h}$ changes from the usual\n$Y^{l,h} = \\text{ softmax}(\\dfrac{X_i^{l,h}W_Q^{l,h}(X_i^{l,h}W_K^{l,h})^T}{\\sqrt{d_h}})X^{[:]}W_V^{l,h}$\nwhere $X_i^{[:]}$ denotes all tokens before or at position i and $d_h$ denotes the head dimension, to\n$Y^{l,h} = \\text{ softmax}(\\dfrac{X_i^{l,h}W_Q^{l,h} concat(K_1^{l,h}, ..., K_N^{l,h})^T}{\\sqrt{d_h}}) concat(V_1^{l,h}, ..., V_N^{l,h}, X^{[:]}W_V^{l,h})W_V^{l,h}$\nwhere each $(K_j, V_j)$ denotes the keys and values of an explicit memory.\nWhile the context BOS token is <s> as usual, when encoding each reference we modify the BOS to \u201c(s)Reference:\u201d to help the LLM distinguish between encoding normal texts and encoding references. This modified BOS is also prepended to the context during inference, as illustrated in Figure 9, while the context BOS token now serves as a separator between the references and context. Unlike the explicit memories which only appear at a subset of attention heads, this modified BOS is placed at every head at every layer. The motivation is that since the context BOS can attend to the references, its feature is no longer constant, so the LLM needs the modified BOS to serve as the new constant for all attention heads.\nFurthermore, we adopt parallel position encoding for all explicit memories, namely the positions of all their keys lie in the same interval of length 128, as depicted in Figure 9. We use the rotary position encoding (ROPE) [107]. The token sparsification is applied after RoPE processes the attention keys, so the selected tokens retain their relative positions in the references. Besides flexibility, one motivation for parallel position is to avoid the \u201clost in the middle\u201d phenomenon [72], such that if the references are positioned serially, then the ones in the middle are likely to be ignored. Similarly, token sparsification also helps to alleviate this issue by making the attention more focused on the important tokens. We note that designs analogous to the parallel position have been used to improve in-context learning [96] and long-context modeling [15]."}, {"title": "3.3 Memory Sparsification and Storage", "content": "One of the greatest challenges for explicit memories is that the attention key-values occupy too much space. They not only demand more disk space, which could be costly, but also occupy GPU memory during inference, which could harm the batch size and thus the throughput of LLM generation. An intense compression is needed to save space. The full attention key tensor (or value tensor) for each reference has shape (layers, key-value heads, tokens, head dimension), so we compress all four dimensions.\nRegard layers, we only set the first half of the attention layers to be memory layers, i.e. layers that produce and attend to explicit memories (4), while the second half remain as the usual attention layers. Note that Remark 1 suggests that it is usually the attention heads in the middle of the LLM that attend to the references. So it seems that appointing the middle attention layers (e.g. the ones within the 25% to 75% depth range) to be memory layers is a more sensible choice. This heuristic is supported by the observations in [122, 39] that the attention to the distant context usually takes place in the middle layers.\nRegarding heads, we set all key-value heads at each memory layer to be memory heads. We reduce their amount by grouped query attention (GQA) [6], letting each key-value head be shared by multiple query heads, and obtain 20% sparsity (8 versus 40 heads). It is worth mentioning that, besides GQA and memory layers, another approach is to select a small subset of heads that are most helpful for reading memories, and this selection does not have to be uniform across layer. We describe several methods for selecting memory heads in Remark 4.\nRegarding tokens, we select 8 tokens out of 128 for each key-value head. We choose a high level of sparsity, since Remark 1 indicates that the attention from the context to the references are expected to be"}, {"title": "3.4 Model Shape", "content": "As discussed in Section 2.3, the specific knowledges can be externalized to explicit memories, and thus to minimize the total cost (1), the model parameters (or implicit memory) only need to store abstract knowledges and the subset of specific knowledges that are frequently used. The shape of our model, i.e. (the number of Transformer blocks L, heads H, head dimension $d_h$, width of the MLP layers W), is chosen"}, {"title": "3.5 Training Designs", "content": "Similar to our architecture design, the design of our training scheme focuses on learning abstract knowledges. The goal is to reduce the training compute, as the LLM no longer needs to memorize many of the specific knowledges. This shift in learning objective implies that all the default settings for pretraining LLMs may need to be redesigned, as they were optimized for the classical scenario when the LLMs learn both abstract and specific knowledges.\n1. Data: Ideally, the pretraining data should have a high concentration of abstract knowledges and minimum amount of specific knowledges. It is known that LLM pretraining is very sensitive to the presence of specific knowledges. For instance, [55] observes that a small model can master arithmetic (e.g. addition of large numbers) if trained on clean data. However, if the training data is mixed with trivial information (e.g. random numbers), then the test accuracy stays at zero unless the model size is increased by a factor of 1500. It suggests that training on specific knowledges significantly inhibits the learning of abstract knowledges, and may explain why emergent abilities [119] are absent from small models. Notably, the Phi-3 model [4] is pretrained with a data composition that closely matches our desired composition. Although the technical details are not revealed, it is stated that they filter data based on two criteria: the data should encourage reasoning, and should not contain information that is too specific.\n2. Initialization: [132] observes that initializing Transformer parameters with a smaller standard deviation (de with c < -1/2 instead of the usual (d^{-1/2}) [47, 52]) can encourage the model to learn compositional inference instead of memorization. Specially, an arithmetic dataset is designed with a train set and an out-of-distribution test set, which admits two possible answers. One answer relies on memorizing more rules during training, while the other requires an understanding of the compositional structure underlying these rules. The proposed mechanism is that training with smaller initialization belongs to the condensed regime that encourages sparse solutions, contrary to training with large initialization that belongs to the kernel regime or critical regime [78, 19].\n3. Weight decay: [90, 88] observe that using a larger weight decay coefficient (i.e. greater than the usual range of 0.001 ~ 0.1) can guide LLMs to favor generalization over memorization, and accelerate the learning of generalizable solutions. They consider settings that exhibit grokking [90] such that training would transit from perfect train accuracy and zero test accuracy to perfect test accuracy, and generalization ability is measured by how quickly this transition occurs. Moreover, theoretically speaking, it is expected that training generative models needs stronger regularization than training regression models, in order to prevent the generated distributions from collapsing onto the training data and become trivial [128].\nIn summary, it is recommendable to pretrain the Memory\u00b3 model with a data composition that emphasizes abstract knowledges and minimizes specific information, a smaller initialization for parameters, and a larger weight decay coefficient.\nSince this work is only a preliminary version of Memory\u00b3, we decide to stick with the conventional setting for training and have not experimented with any of these ideas. We look forward to incorporating these designs in future versions of the Memory\u00b3 model."}, {"title": "3.6 Two-stage Pretrain", "content": "The Memory\u00b3 model learns to write and read explicit memories during pretraining. The training data is prepended with retrieved references; the model encodes these references into explicit memories in real time, and integrates them into the self-attention computation of the training data.\nUnexpectedly, our pretraining consists of two stages, which we name as warmup and continual train. Only the continual train stage involves explicit memories, while the warmup stage uses the same format as ordinary pretraining. Our motivation is depicted in Figure 11. We observe that pretraining with explicit memories from the beginning would render the memories useless, as there appears to be no gain in training loss compared to ordinary pretraining. Meanwhile, given a checkpoint from ordinary pretraining, continual training with explicit memory exhibits a visible decrease in training loss. This comparison"}, {"title": "4 Pretraining Data", "content": "This section describes the procedures for collecting and filtering our pretraining dataset and knowledge base (or reference dataset)."}, {"title": "4.1 Data Collection", "content": "The pretrain data is gathered from English and Chinese text datasets, mostly publicly available collections of webpages and books. We also include code, SFT data (supervised finetuning), and synthetic data.\nSpecially, the English data mainly consists of RedPajama V2 [23], SlimPajama [104] and the Piles [43], in total 200TB prior to filtering. The Chinese data mainly comes from Wanjuan [51], Wenshu [2], and MNBVC [81], in total 500TB prior to filtering. The code data mainly comes from Github, and we take the subset with the highest repository stars. The SFT data is included since these samples generally have higher quality than the webpages. We use the same data as in SFT training (Section 6.1), except that these samples are treated as ordinary texts during pretraining, i.e. all tokens participate in the loss computation, not just the answer tokens."}, {"title": "4.2 Filtering", "content": "The raw data is filtered with three steps: deduplication, rule-based filtering, and model-based filtering.\nFirst, deduplication is performed with MinHash for most of the datasets. One exception is RedPaja-maV2, which already comes with deduplication labels.\nSecond, we devise heuristic, rule-based filters analogous to the ones from [76, 92, 25]. The purpose is to eliminate texts that are ostensibly unsuitable for training, such as ones that only contain webpage source codes, random numbers, or incomprehensible shards. Our filters remove documents with less than 50 words, documents whose mean word lengths exceed 10 characters, documents with 70% of context being non-alphabetic characters, documents whose fractions of unique words are disproportionately high, documents whose entropy of unigrams is excessively low, and so on.\nFinally, we select the subset of data with highest \"quality\", a score produced by a finetuned BERT model. Specially, we sample ten thousand documents and grade them by the XinYu-70B model [65, 68] with prompt-guided generation. The prompt asks the model to determine whether the input text is informative and produce a score between 0 and 5. Then, these scores are used to finetune the Tiny-BERT model [57], which has only 14M parameters. The hyperparameters of this finetuning are optimized with respect to a held-out validation set. After that, we use this lightweight BERT to grade the entire dataset."}, {"title": "4.3 Tokenizer", "content": "Similar to our dataset, our tokenizer mainly consists of Chinese and English tokens. The English vocabulary comes from the 32000 tokens of the LLaMA2 tokenizer. We include roughly the same amount of Chinese tokens produced from byte-pair encoding (BPE). The BPE is trained on a 20GB Chinese corpus that consists of Chinese news and e-books. After deduplication, the final vocabulary has 60299 tokens."}, {"title": "4.4 Knowledge Base", "content": "The knowledge base (or reference dataset) is used during training and inference as the source of explicit memories, as depicted in Figure 1. It consists of reference texts that are split into token sequences with length 128, as described in Section 3.1.\nHeuristically, a larger knowledge base is always better, as long as it does not contain misinformation, so it is not surprising that the reference dataset of Retro contains its entire pretrain dataset [16]. Nevertheless, the storage of explicit memories is more costly than plain texts despite our sparsification (Section 3.3), and thus to save storage space, we select a small subset of our pretrain dataset as the knowledge base.\nWith a focus on high quality data, we include for references the English Wikipedia, WikiHow, the Chinese baike dataset, the subset of English and Chinese books whose titles appear academic, Chinese news, synthetic data and high quality codes. These texts are tokenized and split into chunks of 128 tokens, resulting in 1.1 \u00d7 10\u2078 references in total.\nOne may be curious whether our knowledge base may contain some of the evaluation questions, rendering our evaluation results (Section 7.1) less credible. To prevent such leakage, we include in our evaluation code a filtering step, such that for each evaluation question, if a retrieved reference has an overlap with the question that exceeds a threshold, then it is discarded. This deduplication is analogous to the one used when preparing for continual train (Section 3.6), with the overlap measured by (6). The threshold 2/3 is chosen since we observe that typically a reference that contains a question would have an overlap \u2265 80%, while a relevant but distinct reference would have an overlap \u2264 40%."}, {"title": "5 Pretrain", "content": "This section describes the details of the pretraining process. The two-stage pretrain and memory-augmented data follow the designs introduced in Section 3.6. As an interpretation, the Memory\u00b3 model during the warmup stage develops its reading comprehension, which is necessary during the continual train stage for initiating memory formation."}, {"title": "5.1 Set-up", "content": "Training is conducted with the Megatron-DeepSpeed package [3] and uses mixed-precision training with bfloat16 model parameters, bfloat16 activations, and float32 AdamW states. The batch size is around 4 million training tokens with sequence length 2048, not including the reference tokens. The weight decay is the common choice of 0.1.\nWe adopt the \"warmup-stable-decay\" learning rate schedule of MiniCPM [54], which is reportedly better than the usual cosine schedule in term of training loss reduction. The learning rate linearly increases to the maximum value, then stays there for the majority of training steps, and finally in the last 10% steps decays rapidly to near zero. Our short-term experiments confirm the better performance of this schedule. Nevertheless, frequent loss spikes and loss divergences are encountered during the official pretraining, so we have to deviate from this schedule and manually decrease the learning rate to stabilize training.\nOriginally, it is planned that both the warmup and continual train stages go through the entire 4T token pretrain dataset (Section 4). Due to the irremediable loss divergences, both stages have to be terminated earlier."}, {"title": "5.2 Warmup Stage", "content": "The training loss and learning rate schedule are plotted in Figure 13. Whenever severe loss divergence occurs, we restart from the last checkpoint before the divergence with a smaller learning rate, and thus the divergences are not shown in the figure. Eventually, the training terminates at around 3.1T tokens, when reducing the learning rate can no longer avoid loss divergence."}, {"title": "5.3 Continual Train Stage", "content": "The explicit memories enter into the Memory\u00b3 model at this stage. The training steps are slower since the model needs to encode the references retrieved for the pretrain data to explicit memories in real time, and each step takes a bit more than twice the time of a warmup step. The training loss and learning rate schedule are plotted in Figure 14.\nThe loss divergence soon becomes irremediable at around 120B training tokens, much shorter than the planned 4T tokens, and training has to stop there. One possible cause is that the continual train is initialized from the latest warmup checkpoint, which is located immediately before the break down of the warmup stage, and thus is already at the brink of divergence. The smaller learning rate of continual train delays the onset of divergence but not for long."}, {"title": "6 Fine-tuning and Alignment", "content": "This section describes our model finetuning, specifically supervised finetuning (SFT) and direct preference optimization (DPO)."}, {"title": "6.1 Supervised Finetuning", "content": "Analogous to the StableLM model [14], our Memory\u00b3 model is finetuned on a diverse collection of SFT datasets. We use the following datasets, which are publicly accessible on the Hugging Face Hub: UltraChat [34], WizardLM [125], SlimOrca [67], ShareGPT [114], Capybara [31], Deita [73], and MetaMathQA"}, {"title": "6.2 Direct Preference Optimization", "content": "The Memory\u00b3 model is further finetuned by DPO [93], to align with human preference and improve its conversation skills. The DPO dataset consists of general conversations (UltraFeedback Binarized [111]), math questions (Distilabel Math [10]) and codes questions (Synth Code [36]). The training uses the cosine learning rate schedule with max lr 4 \u00d7 10\u207b\u2076. The inverse temperature \u1e9e of the DPO loss is set to 0.01. The improvement from DPO is displayed in Section 7.2."}, {"title": "7 Evaluation", "content": "We evaluate the general abilities (benchmark tasks), conversation skills, professional abilities (law and medicine), and facutality & hallucination of the Memory\u00b3 model. We also measure its decoding speed. Our model is compared with SOTA LLMs of similar and larger sizes, as well as RAG models."}, {"title": "7.1 General Abilities", "content": "To evaluate the general abilities of Memory\u00b3, we adopt all tasks from the Huggingface leaderboard and also include two Chinese tasks. Most of the results are displayed in Table 16, while TruthfulQA is listed in Table 19. All results are obtained in bfloat16 format, using the lm-evaluation-harness package [44] and the configuration of HuggingFace Open LLM leaderboard [13], i.e. the number of few-shot examples and grading methods.\nAs described in Section 4.4, to prevent cheating, a filtering step is included in the retrieval process so that the model cannot copy from references that resemble the evaluation questions.\nThe results of our model without using explicit memory is included, which indicates that explicit memory boosts the average score by 2.51%. In comparison, the score difference between Llama2-7B and 13B is 4.91% while the latter has twice the amount of non-embedding parameters. Thus, it reasonable to say that explicit memory can increase the \u201ceffective model size\u201d by 2.51/4.91 \u2248 51.1%. (Also, the score difference between Qwen-1.8B and 4B is 8.48% while the latter has 167% more non-embedding parameters. With respect to this scale, explicit memory increases the \"effective model size\" by 1.2.51/8.48 \u00d7 1.67 \u2248 49.4```json\n[9])\n\nWe also include the results of Memory\u00b3 with vector compression (Section 3.3). Even though the key-value vectors of the explicit memories are compressed to 8.75% of their original sizes, the performance of our model does not show any degradation.\nOther supplementary evaluations can be found in Appendix C.\nNext, we compare with a LLM that is pretrained with text retrieval. Specially, we consider the largest version of the Retro++ model [113], Retro++ XXL with 9.5B parameters. All tasks from Table 6 of [113]"}, {"title": "7.2 Conversation Skill", "content": "Next we evaluate the conversation skills of Memory\u00b3. We use MT-Bench (the Multi-turn Benchmark) [133] that consists of multi-round and open-ended questions. The results are listed in Table 18, including the Memory\u00b3 model finetuned by DPO introduced in Section 6.2.\nWe obtain all these scores using GPT-4-0613 as grader, following the single answer grading mode of MT-Bench. Our model outperforms Vicuna-7B, Falcon-40B-Instruct, and ChatGLM2-6B with fewer parameters."}, {"title": "7.3 Hallucination and Factuality", "content": "Despite considerable progress, LLMs still face issues with hallucination, leading to outputs that often stray from factual accuracy [97]. Conceptually, Memory\u00b3 should be less vulnerable to hallucination, since its explicit memories directly correspond to reference texts, whereas compressing texts into the model parameters might incur information loss. To evaluate hallucination, we select two English datasets, TruthfulQA [70] and HaluEval, and one Chinese dataset [64], HalluQA [20]. TruthfulQA is implemented with lm-evaluation-harness [44], while HaluEval and HalluQA are implemented with UHGEval [69]. The results are shown in Table 19, with Memory\u00b3 achieving the highest scores on most tasks."}, {"title": "7.4 Professional Tasks", "content": "One benefit of using explicit memory is that the LLM can easily adapt to new fields and tasks by updating its knowledge base. One can simply import task-related references into the knowledge base of Memory\u00b3, and optionally, convert them to explicit memories in the case of warm start. Then, the model can perform inference with this new knowledge, skipping the more costly and possibly lossy process of finetuning, and running faster than RAG. This cost reduction has been demonstrated in Figure 4 and Appendix A, and could facilitate the rapid deployment of LLMs across various industries.\nBesides cost reduction, we need to demonstrate that Memory\u00b3 can perform no worse than RAG. We consider two professional tasks in law and medicine. The legal task consists of multiple-choice questions from the Chinese National Judicial Examination (JEC-QA) dataset [134]. The field-specific references are legal documents from the Chinese national laws and regulations database [1]. These references are merged with our general-purpose knowledge base (Section 4.4) for inference.\nThe medical task consists of the medicine-related questions of C-Eval, MMLU and CMMLU, specifically from the following subsets:\n\u25a0 C-Eval: clinical medicine, basic medicine\n\u25a0 MMLU: clinical knowledge, anatomy, college medicine, college biology, nutrition, virology, medical genetics, professional medicine\n\u25a0 CMMLU: anatomy, clinical knowledge, college medicine, genetics, nutrition, traditional Chinese medicine, virology\nOur knowledge base is supplemented with medical texts from the open-source medical books dataset [101]."}, {"title": "7.5 Inference Speed", "content": "Finally, we evaluate the decoding speed or throughput of Memory\u00b3, measured by generated tokens per second. The results are compared to those of RAG models, to quantify the speedup of explicit memory over text retrieval.\nA direct comparison of speeds is uninformative: The memory hierarchy (Figure 8) implies that the Memory\u00b3 model is more reliant on retrieval to supply knowledge, and naturally Memory\u00b3 performs retrieval with higher frequency (5 references per 64 tokens, possibly higher in future versions). Therefore, it is necessary to jointly compare performance and speed. The speed measured in this section is plotted against the retrieval-augmented test accuracy from Section 7.4, resulting in Figure 2 (right).\nWe measure decoding speed on a A800 GPU, and run all models with Flash Attention [32]. All models receive an input of batch size 32 and length 128 tokens, and generate an output with length 128 tokens. The throughput is computed by 32 \u00d7 128 divided by the time spent. We test each model 9 times, remove the first record, and take the average of the rest. Memory\u00b3 performs 2 \u00d7 128/64 \u2212 1 = 3 retrievals (the -1 means that the first decoded chunk inherits the explicit memories retrieved by the last input chunk). Each retrieval uses 32 queries to get 32 \u00d7 5 explicit memories. We consider the warm start scenario, with the explicit memories precomputed and saved to drives. We implement the worst case scenario, such that the reference ids are reset to be unique after vector search and the memory cache on RAM is disabled, forcing Memory\u00b3 to load 32 \u00d7 5 memories from drives. Meanwhile, each RAG model performs one retrieval with query length 64 tokens, receives 5 references for each sample, and inserts them at the beginning of the sample, similar to the setup for Table 20.\nThe results are listed in Table 21 (local server). The throughput of these models without retrieval is also provided.\nIn addition, we study the throughput of these models when they are hosted on an end-side device and retrieve from a knowledge base on a remote server. Specifically, we use Jetson AGX Orin, and the server uses the vector engine MyScale [82]. The models are run with plain attention, with batch size 1. To simulate real-world use cases, the input is a fixed text prompt, with approximately 128 tokens, while the exact length can vary among different tokenizers. The output length is fixed to be 128 tokens. The results are listed in Table 21 (end-side device), and the Memory\u00b3 model"}, {"title": "8 Conclusion", "content": "The goal of this work is to reduce the cost of LLM training and inference, or equivalently, to construct a more efficient LLM that matches the performance of larger and slower LLMs. We analyze LLMs from the new perspective of knowledge manipulation, characterizing the cost of LLMs as the transport cost of \"knowledges\" in and out of various memory formats. Two causes of inefficiency are identified, namely the suboptimal placement of knowledges and the knowledge traversal problem. We solve both problems with explicit memory, a novel memory format, along with a new training scheme and architecture. Our preliminary experiment, the Memory\u00b3-2B model, exhibits stronger abilities and higher speed than many SOTA models with greater sizes as well as RAG models.\nFor future work, we plan to explore the following directions:\n1. Efficient training with abstract knowledges: Ideally, the training cost of Memory\u00b3 model should be proportional to the small amount of non-separable knowledges, approaching the learning efficiency of humans. One approach is to filter the training data to maximize abstract knowledges and minimize specific knowledges (cf. Section 3.5 and Remark 6), and preferably the LLM should assess the quality of its own training data and ignore the unhelpful tokens.\n2. Human-like capabilities: As described in the introduction, the explicit memory allows for interesting cognitive functions such as handling infinite contexts (conversion of working memory to explicit memory), memory consolidation (conversion of explicit memory to implicit memory), and conscious reasoning (reflection on the memory recall process). These designs may further improve the efficiency and reasoning ability of Memory\u00b3.\n3. Compact representation of explicit memory: The explicit memory of humans can be subdivided into episodic memory, which involve particular experiences, and semantic memory, which involve general"}, {"title": "A Cost Estimation", "content": "This section provides the calculations for Figure 4, and we equate cost with the amount of compute measured in Tflops.\nOur 2.4B Memory\u00b3 model is adopted as the backbone. Recall from Section 3.4 that this model has shape\n\u25a0 Transformer blocks L = 44\n\u25a0 Query heads H = 40 and key-value heads Hkv = 8\n\u25a0 Head dimension $d_h$ = 80 and hidden dimension d = $H d_h$ = 3200\n\u25a0 MLP width W = d\n\u25a0 Vocabulary size as well as LM head size nvocab = 60416\n\u25a0 memory layers Lmem = 22, which is also the depth of the deepest memory layer.\nFix a separable knowledge K, and represent it by one of its realizations t (Definition 5), and assume that t has length $l_{ref}$ = 128 tokens, following the setup of our reference dataset (Section 4.4). Recall from Section 3.3 that each memory has $l_{mem}$ = 8 tokens per memory head, and it is read by a chunk of length $l_{chunk}$ = 64.\nSince we want to show that explicit memory is cheaper than implicit memory and RAG, it suffices to use coarse lower bounds on their costs."}, {"title": "A.1 Implicit Memory", "content": "The write cost of implicit memory or model parameters is the training compute with t as input. Usually the training data of Transformer LLMs have length 2048 ~ 8192, so we assume that t is a subsequence of a train sample ttrain with length Itrain = 2048. By [84], the training compute of one step with one sample is approximately\n$\\dfrac{12}{I_{train}} [L(I_{train} (2d^2 + 2dd_h H_{kv} + 3dW) + 2 \\tfrac{I_{train}}{2} d) + I_{train} n_{vocab}d]$\nwhere 3 means that the backward step costs twice as the forward step (and thus 3 times in total), the first 2 means that the compute of matrix multiplication involves same amount of additions and multiplications. The five terms in the bracket come from QO embedding, KV embedding, MLP, attention, and LM head, respectively. The lower order terms, such as layer normalizations, are omitted. The fraction of the compute attributable to t is given by\n$\\dfrac{I_{ref}}{I_{train}}  [L(I_{ref} (2d^2 + 2dd_h H_{kv} + 3dW) + 2I_{ref} d) + I_{ref} n_{vocab}d]$\nAssume that one training step is sufficient for storing knowledge K into model parameters. Then, the write cost is equal to the above term, and we obtain\n$cost_{write} \\approx 2.24  TFlops$\nMeanwhile, we lower bound the read cost by zero.\n$cost_{read} \\geq 0  TFlops$\nThis lower bound is obviously correct and suits our comparison, since it makes implicit memory appear more competitive. The difficulty in estimating the cost is that the correspondence between knowledges and parameters is not fully understood. Nevertheless, we describe a possible way to obtain a reasonable bound. Recall from Section 1 that the model parameters suffer from the issue of knowledge traversal such that each parameter (and thus each implicit memory) is invoked during each call of the LLM. So"}, {"title": "A.2 Explicit Memory", "content": "The write cost of an each explicit memory mainly comes from $L_{mem}$ self-attention layers, $L_{mem-1}$ MLP layers, and $L_{mem}$ token sparsification operations (computing the full attention matrix):\n$cost_{write} = 2* [L_{mem} (I_{ref}(2d^2 + 2dd_h H_{kv} + 2 I_{ref}d) + (L_{mem} - 1) (I_{ref} 3dW) + L_{mem} (\\dfrac{I_{ref}}{2}^2d)]$\n$\\approx 0.308 TFlops$\nThe read cost consists of the attention to the sparse tokens of an explicit memory from the chunk that retrieves this memory:\n$cost_{read} = 2L_{mem} * 2I_{chunk} I_{mem}d  \\approx 1.44 x 10^4 TFlops$"}, {"title": "A.3 External Information", "content": "The write cost of text retrieval-augmented generation (RAG) is set to be zero, since the reference is stored as plain text.\n$cost_{write} = 0  TFlops$\nThe read cost is the additional compute brought by the retrieved references that are inserted in the prompt. To make RAG appear more competitive, we assume that only a chunk of the prompt or decoded text with length $I_{chunk}$ can attend to the references, and each reference can only attend to itself, which in general is not true. Then,\n$cost_{write} \\geq 2 * [L(I_{ref}(2d^2 + 2dd_h H_{kv}) + 2 (I_{ref} + I_{chunk})d) + (L-1)(I_{ref} 3dW)]$\n$\\approx 0.624 TFlops$\nIn summary, the total cost (TFlops) of writing and reading each separable knowledge in terms of its expected usage count n is given by\n$C_{implicit} (n) \\geq 2.24$\n$C_{explicit} (n) = 0.308 + 0.000144n$\n$C_{external}(n) \\geq 0.624n$\nThese curves are plotted in Figure 4. Hence, if $n \\in (0.494, 13400)$, then it is optimal to store the knowledge as an explicit memory."}, {"title": "B Vector Compression", "content": "Regarding the vector quantizer discussed in Sections 3.3 and 7.1, we use the composite index of FAISS with index type OPQ20x80-Residual2x14-PQ8x10. It can encode a 80-dimensional bfloat16 vector into a 14-dimensional uint8 vector, and thus its compression rate is $\\frac{80 x 2}{14 x 1} \\approx 11.4$.\nTo train this quantizer, we sample references from our knowledge base, encode them into explicit memories by our Memory\u00b3-2B-SFT model, and feed these key-value vectors to the quantizer. The references are sampled uniformly and independently, so the training is not biased towards the references that are retrieved by any specific evaluation task."}, {"title": "C Supplementary Evaluation Results", "content": "First, Table 22 records the growth of the test scores (Table 16) over the three training stages: warmup, continual train, and SFT. We believe that for future versions of Memory\u00b3, fixing the loss divergence during the warmup stage can allow the continual train stage to proceed much further (cf. Section 5.3), and thus increase the performance boost of this stage."}]}