{"title": "EVALUATING AND ENHANCING LLMS AGENT BASED\nON THEORY OF MIND IN GUANDAN (\u63bc\u86cb): A MULTI-\nPLAYER COOPERATIVE GAME UNDER IMPERFECT IN-\nFORMATION", "authors": ["Yauwai Yim", "Chunkit Chan", "Tianyu Shi", "Zheye Deng", "Wei Fan", "Tianshi Zheng", "Yangqiu Song"], "abstract": "Large language models (LLMs) have shown success in handling simple games\nwith imperfect information and enabling multi-agent coordination, but their ability\nto facilitate practical collaboration against other agents in complex, imperfect\ninformation environments, especially in a non-English environment, still needs\nto be explored. This study investigates the applicability of knowledge acquired\nby open-source and API-based LLMs to sophisticated text-based games requiring\nagent collaboration under imperfect information, comparing their performance to\nestablished baselines using other types of agents. We propose a Theory of Mind\n(ToM) planning technique that allows LLM agents to adapt their strategy against\nvarious adversaries using only game rules, current state, and historical context as\ninput. An external tool was incorporated to mitigate the challenge of dynamic\nand extensive action spaces in this card game. Our results show that although a\nperformance gap exists between current LLMs and state-of-the-art reinforcement\nlearning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It\nconsistently improves their performance against opposing agents, suggesting their\nability to understand the actions of allies and adversaries and establish collaboration\nwith allies. To encourage further research and understanding, we have made our\ncodebase openly accessible.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent progress in Large Language Models (LLMs) (Touvron et al., 2023; Chowdhery et al., 2023;\nChan et al., 2024a), has demonstrated their potential in solving complex problems and the LLM-\nbased agent harnesses the capability to complete tasks in some complicated environments (Wang\net al., 2022; Raman et al., 2022; Wu et al., 2023). However, these methods have several significant\nlimitations: (1) The assumption that the agent has access to all necessary information, which is\noften different in real-world situations. Games such as diplomacy (Bakhtin et al., 2022; Gray et al.,\n2021) and poker (Moravc\u00edk et al., 2017; Brown & Sandholm, 2018) underscore the difficulties posed\nby imperfect information. (2) Do not require cooperation among agents, which is uncommon in\nmost scenarios, such as Overcooked (Carroll et al., 2019), agents must collaborate to complete their\nobjectives. (3) All tests in an English language environment.\nDespite these limitations, some researchers have made progress in addressing them. For instance, (Li\net al., 2023c; Agashe et al., 2023) have demonstrated the ability of LLM-based agents to cooperate\nand complete tasks in perfect information settings, while Guo et al. (2023) has shown that LLM agents\ncan perform well in imperfect information games but without requiring cooperation. However, the\ncapabilities of LLM agents in imperfect information games and scenarios necessitating cooperation"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 LLM REASONING AND MULTI-AGENT COLLABORATION IN IMPERFECT INFORMATION\nGAMES", "content": "Recent advancements in Large Language Models (LLMs) have showcased their impressive reasoning,\nplanning, and problem-solving capabilities across various tasks (Chiang et al., 2023; OpenAI, 2023b;\nOuyang et al., 2022; Wei et al., 2022; Yao et al., 2023). LLMs augmented with memory, belief, and\ntools have excelled in multi-step reasoning and outperformed state-of-the-art reinforcement learning\nmethods in open-domain survival games and simple two-player imperfect information games (Park\net al., 2023; Huang et al., 2022; Guo et al., 2023). Researchers have also explored multi-agent\ncoordination using unreal benchmark environments (Carroll et al., 2019; Wang et al., 2022; Agashe\net al., 2023; Li et al., 2023c) and studied human collaborative behaviors (Riedl et al., 2021). However,\nusing LLMs to investigate collaborative behaviors in imperfect information games, especially in\nthe Chinese environment, remains largely unexplored. Our research focuses on integrating theory-\nof-mind (ToM) capabilities into the planning process in Guandan, leveraging LLMs' zero-shot\ncapabilities and ability to facilitate natural language interactions, which is lacking in other studies."}, {"title": "2.2 THEORY OF MIND", "content": "Theory of Mind (ToM), a crucial cognitive ability that enables individuals to understand and predict\nthe mental states of themselves and others (Frith & Frith, 2005; Premack, 1978; Hurley, 2008;\nChan et al., 2024b), has been used in games with incomplete information to anticipate opponents'\nactions and improve decision-making (de Weerd et al., 2013). Recent studies have investigated the\nToM capabilities of large language models (LLMs) using text-based evaluations (Kosinski, 2023;\nMoghaddam & Honey, 2023). However, LLMs struggle with complex ToM inferences involving\ncommunication or second-order beliefs (Ullman, 2023). The current study examines ToM within\nan interactive team task, where agents' mental states evolve dynamically, making the reasoning\nprocess more demanding. ToM has been applied to improve artificial agents' performance in various\nsettings, such as integrating Bayesian Theory of Mind with optimal-planning agents (Lim et al.,\n2020), using SymbolicToM for reading comprehension tasks (Sclar et al., 2023), and enhancing\ncollaboration in multi-agent reinforcement learning (Oguntola et al., 2023; Yuan et al., 2021). Inspired\nby these studies, the present research aims to improve LLM-based agents' collaborative behaviors by\nincorporating explicit belief representations."}, {"title": "3 LLM-BASED EMBODDIED AGENT FOR COLLABORATION", "content": "This study proposes a modular approach that enables LLM-based embodied agents to collaborate\nand strategically interact with adversarial agents in imperfect information games, Guandan, without\nrequiring specialized training in the Chinese textual environment. The task is decomposed into\nessential components (Figure 2), which include a State Interpreter, ToM Planner, and Plan Evaluator."}, {"title": "3.1 GAME RULE, OBSERVATION RULE, AND GAME HISTORY", "content": "This part refers to step 1 in Figure 2. Inspired by Guo et al. (2023), we designed structured prompts\nto help LLMs understand the game rules and current state information, including card types, scoring\nrules, and win/loss conditions. The game rule template is shown as follows:\n\u2022 General Rules: A brief game introduction, team position rules, all single cards, and cards\nranking.\n\u2022 Card Type that cannot beat other types: {Description of Card Type 1, Example of Card\nType 1}, {Description of Card Type 2, Example of Card Type 2}, ...;\n\u2022 Card Type that can beat other types: {Description of Card Type 1, Example of Card Type\n1}, {Description of Card Type 2, Example of Card Type 2}, ...;\n\u2022 Single Win/Loss Rule: The scoring rules for four players in a single game, with different\ncombinations of cards being played out in different order.\n\u2022 Whole Win/Loss Rule: The overall win/loss conditions.\nA sample template can be found in A.2.\nSeveral works (Zha et al., 2019; Guo et al., 2023) note that game states are represented using low-level\nnumerical values in most imperfect information game environments. LLMs can convert these atomic\ngame states into human-readable text (Wu et al., 2023; Wang et al., 2023; Guo et al., 2022; 2023),\nenhancing the model's understanding. Therefore, we structure the observation rule and game history\nconversion rules similarly to game rules to help LLM convert low-level current state and history\ninformation to readable text. The template includes input explanations, element descriptions, and\nconversion tips is shown as follows:\n\u2022 Input Explanation: The input types, like dictionaries and lists, are clearly specified. A\ndescription of every component in the input is also provided.\n\u2022 Conversion Tips: Additional instructions guide converting the low-level game state repre-\nsentations into natural language descriptions.\nAn example of converted results can be found in C.1 and C.2.\nWe can effectively convert low-level game states and history records into human-readable\ntext, denoted as $Obs_r$ and $His_r$, respectively, by employing the game rule, observation con-\nversion rule, and history conversion rule. The conditional distribution for each element\n$Obs_r[j]$ within the generated text can be modeled using the prompts $Prompt_{obs}$ as: $Obs_r \\sim$\n$\\prod_{i=1}^{N} LM_{\\theta}(Obs_r[j] | Prompt_{obs}, Rule, Rule_{obs}, Obs_r[1, ..., j \u2013 1])$, where $LM_{\\theta}$ represents the lan-\nguage model parameterized by $\\theta$, and $N$ is the length of the generated text $Obs_r$. This Current State\nInterpreter module facilitates more intuitive interaction with the model in games with incomplete\ninformation."}, {"title": "3.2 TOOL USE AND VANILLA PLANNING MODULE", "content": "In Guandan, the number of valid actions per turn can vary significantly, making it challenging for\nthe model to accurately evaluate each action's merits. To address this, we incorporated an action\nrecommender, a RL-based model. The model scores all valid actions available to the player in\neach round, allowing the LLM agent to focus on the top k highest score actions. We use the learned\nembedding from Danzero (Lu et al., 2023) to score every valid action.\nThis part refers to step 3 in Figure 2. After transforming game states and history into a readable\nformat and getting the top k actions selected by the action recommender, we design prompts to guide"}, {"title": "3.3 PLANNING WITH THEORY OF MIND (TOM) AND COLLABORATION", "content": "The part refers to the step 2 in Figure 2. Vanilla planning techniques often struggle with ambiguities\nin games with imperfect information, especially when playing against opponents skilled at exploiting\nothers' strategies. To address this, we developed a planning approach that employs the Theory\nof Mind (ToM) capabilities of LLMs (Frith & Frith, 2005; Kosinski, 2023; Guo et al., 2023). By\nunderstanding the actions of all opponents and teammates, this method enables the adaptation of\nstrategies, enhancing success in complex, multi-agent game environments. We employ LLMs to\nanalyze other agents' behavior and predict their actions using different ToM orders.\nPlanning with First Order ToM: In the first-order ToM modeling approach, LLM-Agent infers the\ncard types that both opponents and teammates may possess based on their past actions and analyzes\ntheir strategies. This information is used to formulate confrontation or cooperative strategies. By\ninputting the game history $D_1 = (h_1, h_2, ..., h_8)$, current observation $Obs_r$, and game rules into\nLLMs, we prompt the model to examine the behavioral patterns exhibited by all agents:\n$Belief \\sim \\prod_{i=1}^{M} L_{\\theta}(Belief[i] | Prompt_{pattern}, Rule, D^i, Obs_r, Belief[1, . . ., i \u2212 1]).$\nThe predictions from the previous step can be used to enhance the Planning and Evaluator modules\nby incorporating $Belief$ as an additional input. To optimize costs, we merge the Planning and\nEvaluator modules. A sample prompt template for the new Planning and Evaluator modules can be\nfound in B.2, and the sample outputs of this part can be found in C.\nPlanning with Second Order ToM: Skilled card game players excel at dynamically adapting their\ntactics, potentially tricking opponents into making suboptimal plays. Relying solely on first-order\ntheoretical models can lead to misjudgments and gaps in the agent's cooperative approach. To address\nthis, we introduce a planning method incorporating a second-order ToM. This enables the LLM agent\nto engage in more sophisticated reasoning by considering what the adversary believes about the LLM\nagents' intentions. This allows the agent to analyze the situation more comprehensively, mirroring\nthe thought processes of high-level players. We include a simple prompt shown in B.3. This method\nallows us to leverage Planning with a second-order Theory of Mind, enabling well-informed decisions\nand adaptive tactics based on the changing dynamics of the game."}, {"title": "4 EXPERIMENT AND RESULTS", "content": "Environments To investigate the performance of LLM-based agents in a more realistic setting\ncharacterized by imperfect information, absence of communication, and collaborative dynamics,\nwe selected Guandan as our experimental environment. We conducted tests on both open-source\nand close-source language models, including OpenAI's commercial model GPT-4-Turbo and GPT-\n3.5-Turbo (OpenAI, 2023b), and Chinese-language open-source models like Baichuan2-7B-Chat,\nBaichuan2-13B-Chat, chatglm3-6b-32k, Qwen1.5-7B-Chat, and Qwen1.5-14B-Chat (Yang et al.,\n2023; Zeng et al., 2023; Bai et al., 2023). In the test, each type of LLM agent will team up to compete\nagainst enemy teams consisting of other agent types. All experiments were conducted in a Chinese\nenvironment. The detailed rules and mechanics of Guandan are elaborated upon in Appendix A.\nCompeting Methods Danzero+ (Lu et al., 2023): it is a state-of-the-art model and serves as the\nupper bound compared with other LLMs. Danzero+ uses the distributed framework to train the\nreinforcement learning model with the deep Monte Carlo method. They demonstrated their ability to\nperform at a human level. Rule-based Agent: selecting optimal card combinations under two distinct\nstrategies: 'bestP_lowV' and 'random'. The 'bestP_lowV' strategy selects the best combination\nbased on non-value criteria and calculated priority, maximizing strategic advantage while minimizing\npotential loss. It chooses the combination with the highest strategic priority and the lowest value\namong those with the optimal pattern. The 'random' strategy will execute a random action from the\navailable options.\nEvaluation Methods We follow Guo et al. (2023) to design a dual-method evaluation framework\nto ensure a more robust assessment of the LLM-agent's performance, addressing potential factors\nthat may provide inherent advantages in this type of game. Variable random seeding: The method\ninvolves the investigated agents playing 40 games against different baselines, each utilizing a unique\nrandom seed to mitigate variability arising from random seed setting; results are shown in Table 1.\nPosition swapping: This method focuses on the potential influence of a player's position on the\ngame's outcome, as it often determines the situation in the Guandan game. The investigated agent\nwas initially assigned positions 0 and 2 for the first 20 games. The positions of the investigated agent\nand the baseline model were then exchanged for a repeat of the 20 games. This approach allows for\nassessing the impact of position on the game's winning rate, with the results shown in Table 2.\nAction distribution Since Guandan offers a dynamic length of valid actions list for each agent to\nchoose from in every round. We analyzed 10 games, recording the number of valid actions available\nto LLM-Agent whenever it was their turn to play a card, along with the action index they ultimately\nselected (totaling approximately 1900 actions). Notably, although LLM-Agent often had more than\n20 possible actions to select from during their turn, their chosen actions were consistently restricted\nto a maximum of 7. Moreover, they strongly preferred the first five actions, as illustrated in Figure 3.\nWe believe that when an LLM faces too many legal actions, it often has trouble effectively processing\nthe extensive text input. This leads to a reduced understanding of each action, especially those\naction elements at the end of the valid action list. Moreover, when there are many valid actions,\nit becomes impractical for the model to generate and analyze a complete set of solutions for each"}, {"title": "4.1 ABLATION STUDY AND COMPONENT ANALYSIS", "content": "Ablation Study on Different Languages: Guandan is widely popular in China. Due to its cultural\nspecificity, many game-related terms take time to translate accurately into other languages. To\ninvestigate the impact of different language environments on the performance of LLM agents, we\nconducted GPT-3.5 as a testbed model. Table 3 presents the performance of GPT-3.5 with various\nToM planning methods in both Chinese and English game environments. The results demonstrate\nthat the LLM-Agent's performance in the Chinese language environment significantly surpasses its\nperformance in the English language environment. This phenomenon can be attributed to the fact that\nGPT-3.5 is pre-trained on an enormous amount of data, and most Guandan-related data is primarily\navailable in Chinese. Consequently, we opted to use Chinese as the language environment for our\nexperiments to leverage the language model's inherent knowledge and maximize its performance in\nGuandan.\nAblation Study on the Action Recommender: As illustrated in Figure 3, we observed that the\nactions selected by the LLM-Agent are primarily concentrated within the first five actions, even\nwhen presented with more than 50 options. To address this limitation, we introduced an action\nrecommender component, an external tool, specifically an RL-based model, to assign scores to\neach available action and enable the LLM-Agent to invoke it. This tool returns the top 5 actions\nwith the highest scores by default. In this section, we primarily investigate the impact of the action\nrecommender component on the LLM-Agent's performance. We compare three scenarios: (1) the\nabsence of an action recommender, (2) the inclusion of an action recommender returning the top 5\nactions, and (3) the inclusion of an action recommender returning the top 3 actions. Furthermore, we\nconduct these comparisons using GPT-3.5 with various planning methods to assess the effectiveness\nof the action recommender across different configurations.\nThe results shown in Table 4 allow for the following observation: (1) LLM capabilities can be\nimproved with the help of external tools: The utilization of this external tool significantly improves\nthe performance of the LLM-Agent in this game, which demonstrates that LLM-Agents struggle\nwith long reasoning problems. Even when the game environment provides the agent with a relatively\nextensive list of valid actions, the agent cannot effectively process such lengthy text and consequently\nfocuses only on the first few actions. The introduction of the action recommender, which only returns\nthe top k operations, alleviates this problem and enhances the agent's capabilities. (2) The gap\nbetween different numbers of top k action: The results demonstrate that setting K to 5 leads\nto superior model performance compared to K=3. This finding indicates that the RL-based tool\nmay not always produce optimal results. When the LLM-Agent is restricted to choosing from only\nthree actions, it may not be able to fully leverage its capabilities, especially when significantly more\nthan three valid actions are available in a given scenario. In such cases, the model is constrained\nto analyzing and selecting from a limited subset of possibilities. Additionally, the improvement in"}, {"title": "5 CONCLUSION", "content": "This study compares open-source and closed-source LLMs with SOTA-RL-based models in emu-\nlating opponent and teammate behavior in the complex card game Guandan. LLMs exhibit subpar\nperformance compared to RL-based approaches in intricate and realistic scenarios. We propose a\nToM planning method to optimize LLM-agent's decision-making and collaboration in multi-agent\ngaming environments. We also develop an RL-based model to address LLMs' challenges in adapting\nto dynamic changes and extensive legal action lists. This research deepens our understanding of\nLLMs' potential in navigating complex social interactions and advances cognitive modeling in AI."}, {"title": "ETHICS STATEMENT", "content": "In this study, we conducted experiments and evaluations on large language models (LLMs) for\ntheir applicability and performance in the Guandan card game, imperfect information, and multi-\nagent collaboration tasks. We compared open-source and closed-source LLMs to ensure a fair and\ntransparent evaluation, aiming to contribute to the broader understanding of LLMs and their potential\nto address complex tasks without bias. Our research adheres to the highest ethical standards, ensuring\nthat the data used does not contain personally identifiable information and focuses solely on the\nperformance of LLMs in the context of the Guandan card game. While employing the Theory\nof Mind (ToM) concept, we do not attribute human-like consciousness to these AI models. Our\nobjective is to investigate the potential of LLMs in complex social interactions and strategic thinking,\nacknowledging the potential risks and emphasizing the importance of responsible AI development,\ndeployment, and use."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of our research, we have taken several steps. We have comprehensively\ndescribed our methods, including the planning approaches, the action recommender, and the ToM-\naware LLMs. This detailed account of our methodology enables other researchers to replicate our\nexperiments and build upon our findings. We have evaluated open-source and closed-source LLMs,\nensuring that researchers with access to either model type can reproduce our results. Furthermore,\nto facilitate further research and promote transparency, we have committed to making all prompts\nand codes publicly available in our codebase. This will enable other researchers to reproduce our\nexperiments and verify our results. By taking these steps, we aim to ensure that our research is\ntransparent and reproducible and contributes to the broader understanding of LLMs and their potential\nin addressing complex, real-world tasks."}]}