{"title": "MapFusion: A Novel BEV Feature Fusion Network for Multi-modal Map Construction", "authors": ["Xiaoshuai Hao", "Yunfeng Diao", "Mengchuan Wei", "Yifan Yang", "Peng Hao", "Rong Yin", "Hui Zhang", "Weiming Li", "Shu Zhao", "Yu Liu"], "abstract": "Map construction task plays a vital role in providing precise and comprehensive static environmental information essential for autonomous driving systems. Primary sensors include cameras and LiDAR, with configurations varying between camera-only, LiDAR-only, or camera-LiDAR fusion, based on cost-performance considerations. While fusion-based methods typically perform best, existing approaches often neglect modality interaction and rely on simple fusion strategies, which suffer from the problems of misalignment and information loss. To address these issues, we propose MapFusion, a novel multi-modal Bird's-Eye View (BEV) feature fusion method for map construction. Specifically, to solve the semantic misalignment problem between camera and LiDAR BEV features, we introduce the Cross-modal Interaction Transform (CIT) module, enabling interaction between two BEV feature spaces and enhancing feature representation through a self-attention mechanism. Additionally, we propose an effective Dual Dynamic Fusion (DDF) module to adaptively select valuable information from different modalities, which can take full advantage of the inherent information between different modalities. Moreover, MapFusion is designed to be simple and plug-and-play, easily integrated into existing pipelines. We evaluate MapFusion on two map construction tasks, including High-definition (HD) map and BEV map segmentation, to show its versatility and effectiveness. Compared with the state-of-the-art methods, MapFusion achieves 3.6% and 6.2% absolute improvements on the HD map construction and BEV map segmentation tasks on the nuScenes dataset, respectively, demonstrating the superiority of our approach.", "sections": [{"title": "1. Introduction", "content": "Map construction task provides abundant and precise static environmental information of the driving scene, which is vital yet challenging for planning in autonomous driving systems. Recently, researchers have focused on two crucial tasks: High-definition (HD) map construction and semantic map construction. Both tasks increasingly utilize the Bird's Eye View (BEV) representation as an ideal feature space for multi-view perception, thanks to its effectiveness in addressing scale ambiguity and occlusion challenges. Specifically, HD map construction methods consider this task as the problem of predicting a collection of vectorized static map elements in bird's-eye view (BEV), such as pedestrian crossing, lane divider, road boundaries, etc. On the other hand, semantic map construction methods treat map construction as a BEV semantic segmentation task, where each pixel in the BEV plane is assigned a semantic label.\nBased on the input sensor modality, map construction methods can be categorized into camera based [58, 48, 12], LiDAR based [20, 53] and camera-LiDAR fusion [21, 28, 25, 44, 54, 31] methods. Camera sensors capture rich"}, {"title": "2. Related Work", "content": "Our work is highly related to map construction task (See Fig. 2) and multi-sensor fusion methods, which will be discussed thoroughly in the following."}, {"title": "2.1. Map Construction Task", "content": "HD map construction. HD map construction is a critical and extensively researched area in autonomous driving. Based on input sensor modalities, HD map construction models can be categorized into camera-based [56, 9, 39, 26, 55], LiDAR-based [20, 53] and camera-LiDAR fusion [21, 28, 25, 13, 14] models. Camera-only methods [56, 9, 39, 26, 55] have increasingly adopted the Bird's-eye view (BEV) representation as an ideal feature space for multi-view perception, owing to its remarkable ability to mitigate scale ambiguity and occlusion challenges. Various techniques have been proposed to project perspective view (PV) features onto the BEV space by leveraging geometric priors, such as LSS [37], Deformable Attention [22], and GKT [4]. Nevertheless, camera-only methods lack explicit depth information, which forces them to rely on higher resolution images or larger backbone models to achieve enhanced accuracy [30, 29, 16, 46, 22, 52, 49]. In contrast, LiDAR-only approaches [20, 53] benefit from the accurate 3D geometric information provided by LiDAR input. However, they face challenges related to data sparsity and sensing noise.\nRecently, camera-LiDAR fusion methods [21, 28, 25] leverage the semantic richness of camera data and the geometric information from LiDAR in a collaborative manner. BEV-level fusion, which uses two independent streams to encode raw inputs from camera and LiDAR sensors into features within the same BEV space, has gained significant attention [31, 24]. This approach incorporates complementary modality features, outperforming uni-modal input approaches. Existing HD map construction multi-sensor fusion methods-HDMapNet [21], VectorMapNet [28], and MapTR [25]- utilize straightforward channel concatenation and convolution for multi-modal feature fusion. However, these methods overlook modality interaction and employ very simple fusion strategies, leading to issues of misalignment and information loss.\nBEV map segmentation. Semantic map construction methods [41, 35, 11, 31] take map construction as a BEV semantic segmentation task, assigning semantic labels to each pixel in the BEV plane. Building on Perspective View (PV) segmentation [42, 36], early approaches utilize homography transformations to convert camera images into bird's-eye view (BEV) representations, followed by the estimation of segmentation maps [1, 57, 10, 59]. However, homography transformation introduces strong artifacts, and BEV-based methods [58, 48, 31], i.e. performing segmentation directly on BEV plane, have received extensive attention. CVT [58] employs a learned map embedding and an attention mechanism between map queries and camera features. Furthermore, BEVFusion [31], BEVerse [57] and M\u00b2BEV [48] explore multi-task learning with 3D object detection. However, these approaches lack explicit utilization of depth information, resulting in unsatisfactory performance.\nExisting fusion methods [44, 54] primarily focus on object-centric and geometry-oriented approaches. For instance, PointPainting [44] enhances only the foreground LiDAR points, while MVP [54] concentrates solely on densifying foreground 3D objects. Both methods also assume that LiDAR is the more effective modality for sensor fusion, which may not be valid for map construction tasks [31]. Additionally, X-Align [2] employs an integration method that combines the features of the two modalities before applying attention, neglecting modality interactions and relying on"}, {"title": "2.2. Multi-sensor Fusion", "content": "Multi-sensor fusion has garnered significant attention in the field of autonomous driving. Existing approaches can be broadly categorized into three types: point-level fusion, feature-level fusion, and BEV-level fusion. Point-level fusion methods [45, 44, 50, 6, 54] typically project image semantic features onto foreground LiDAR points, enabling LiDAR-based detection on the enhanced point cloud. While effective for 3D object detection tasks, these methods are less suitable for semantically driven tasks such as BEV map segmentation [31, 24, 58, 48] and HD map construction [21, 28, 25]. This limitation stems from the lossy projection of camera features to LiDAR, where only about 5% of camera features align with points from a typical 32-beam LiDAR scanner, resulting in significant information loss. Feature-level fusion methods [5, 23] first project LiDAR points into a feature space or generate proposals, query the corresponding camera features, and then concatenate them back into the feature space. However, both point-level and feature-level fusion approaches encounter generalization challenges. Specifically, point-level fusion is not easily extendable to other sensor modalities, while feature-level fusion struggles with generalization across different tasks.\nRecently, multi-modal feature fusion in a unified BEV space has gained considerable attention [25, 28, 21, 31, 24]. BEV-level fusion employs two independent streams to encode raw inputs from camera and LiDAR sensors into features within the same BEV space. This approach offers a straightforward yet effective means to integrate BEV-level features from both streams, facilitating their use in various downstream tasks. However, existing BEV-level fusion methods often overlook modality interactions, relying on element-wise operations (such as summation or mean) or simple concatenation. This can lead to issues of misalignment and information loss. In this paper, we propose a simple and effective camera-LiDAR BEV feature fusion method that simultaneously integrates complementary information from different modalities, specifically targeting multi-modal map construction tasks.\nComparison with Existing Works. This work differs from prior literature in three key aspects. Firstly, we focus on the BEV-based multi-modal map construction task, distinct from other BEV perception tasks [7, 45, 50], as it aims at predicting map elements, such as pedestrian crossing, lane divider, road boundaries, etc. In fact, the map construction task is a semantic-oriented task, which pays more attention to the semantic information in the image. Therefore, the performance of directly using the fusion method on the 3D object detection task to the map task is not satisfactory. Secondly, to solve the semantic misalignment problem between Camera and LiDAR BEV features, we propose Cross-modal Interaction Transform (CIT) module to enable the two BEV feature spaces to interact with each other and enhance feature representation through a self-attention mechanism. Additionally, to further fuse features from different modalities, we propose an effective Dual Dynamic Fusion (DDF) module to adaptively select valuable information from different modalities. To the best of our knowledge, MapFusion is the first to explore the effectiveness of interactive modules on multi-modal map construction tasks. Last but not least, the core components of MapFusion, i.e., CIT module and DDF module, are simple yet effective plug-and-play techniques compatible with existing pipelines for various map tasks, such as HD map and semantic map construction."}, {"title": "3. Methodology", "content": "We propose a novel multi-modal BEV map construction approach called MapFusion, which is a simple yet effective plug-and-play technique compatible with existing pipelines for various map construction tasks. The overview framework of MapFusion is shown in Fig. 3. Given different sensory inputs, we first apply modality-specific encoders to extract their features. These multi-modal features are then transformed into a unified BEV representation that preserves both geometric and semantic information. Then, we propose Cross-modal Interaction Transform (CIT) module to make these two BEV feature spaces exchange knowledge with each other to enhance the feature representation by the self-attention mechanism. Additionally, we introduce a novel Dual Dynamic Fusion (DDF) module to automatically select valuable information from different modalities, which can take full advantage of the inherent complementary information between different modalities. Finally, the fused multi-modal BEV features are fed into decoder and prediction heads for map construction tasks."}, {"title": "3.1. Preliminaries", "content": "For notation clarity, we first introduce some symbols and definitions used throughout this paper. Our goal is to design a novel framework taking multi-modal sensor data x as input and predicting map elements in BEV space, and"}, {"title": "3.2. Map Encoder", "content": "We apply modality-specific encoders to extract their features and transform multi-modal features into a unified BEV representation that preserves both geometric and semantic information. Note that our approach is compatible with other Map Encoders that can also be employed to generate camera-only and LiDAR-only BEV features.\nCamera to BEV. We extract BEV features from multi-view RGB images with the BEV feature extractor. It consists of a backbone [17, 30] to extract multi-scale 2D features from each perspective view, an FPN [27] to refine and fuse multi-scale features into single-scale features, and a 2D-to-BEV feature transformation module [36, 4] to map 2D features into BEV features. The camera BEV features can be denoted as $F_{Camera}^{BEV} \\in R^{H \\times W \\times C}$, where H, W, C refer to the spatial height, spatial width, and the number of channels of BEV feature maps, respectively.\nLiDAR to BEV. For the LiDAR points, we follow SECOND [51] in using voxelization and a sparse LiDAR encoder. The LiDAR features are projected to BEV space using a flattening operation as in [31], to obtain the unified LiDAR BEV representation $F_{LiDAR}^{BEV} \\in R^{H \\times W \\times C}$"}, {"title": "3.3. Cross-modal Interaction Transform (CIT)", "content": "Existing methods directly convert all sensory features to the shared BEV representation, and then fuse them via arithmetic or splicing operations to obtain multi-modal BEV features. However, despite being in the same BEV space, LiDAR BEV features and camera BEV features can still be semantically misaligned due to the significant modality gap, leading to a misalignment problem. To address this issue, we propose a new and powerful Cross-Modal Interaction Transformer (CIT) module to enhance one modality from another modality by the self-attention mechanism. Next, we describe in detail our proposed CIT module.\nConcatenation Interaction Transformer. First, given the BEV features from both camera ($F_{Camera}^{BEV} \\in R^{H \\times W \\times C}$) and LiDAR ($F_{LiDAR}^{BEV} \\in R^{H \\times W \\times C}$) sensors, the BEV tokens $T_{Camera}^{BEV} \\in R^{HW \\times C}$ and $T_{LiDAR}^{BEV} \\in R^{HW \\times C}$ are obtained by flattening each BEV feature and permuting the order of the matrices. Second, we concatenate the tokens of each modality and add a learnable positional embedding, which is a trainable parameter of dimension 2HW \u00d7 C,"}, {"title": "3.4. Dual Dynamic Fusion (DDF)", "content": "Despite the effectiveness of the cross-modal interaction transform module, we argue that how to design an effective cross-modal fusion strategy to adaptively select valuable information from different modalities for better feature fusion is still very important. Recently, multi-modal BEV feature fusion methods [24, 31] have received much attention. It is a common approach to utilize concatenation followed by convolution to combine features from multi-modal BEV feature inputs, $BEV_{Camera}$ and $BEV_{LiDAR}$, resulting in the aggregated features $F_{fused}$, as shown in Fig. 5 (a) Conv Fusion. Another common method is to use CNN to convolve the BEV features of different modalities separately, and then add the convolutional features, as shown in Fig. 5 (b) Add Fusion. As Fig. 5 (c) illustrates, the input of the Dynamic Fusion (DF) module is the Conv Fusion output features, and then they are fused with learnable static weights, inspired by Squeeze-and-Excitation mechanism [19]. To effectively select valuable information from different modalities, we propose a Dual Dynamic Fusion (DDF) module for better feature fusion and maximum performance gain. Next, we describe in detail our proposed fusion designs.\nDual Dynamic Fusion. As shown in Fig. 5(d), our Dual Dynamic Fusion (DDF) module takes two sets of features from the camera BEV features and LiDAR BEV features as input. In order to generate meaningful attention weights that can effectively select informative features from both inputs, we first sum the features from both branches before performing the squeeze and excitation operations that generate the attention weights. We can formulate this process as:\n$w = \\sigma \\left(\\gamma\\left(AvgPool\\left(F_{\\text {Camera }}^{B E V}+F_{\\text {LiDAR }}^{B E V}\\right)\\right)\\right),$ (6)\nwhere \u03c3 and \u03b3 represent the sigmoid function and linear layers respectively, AvgPool is the global average pooling operation, and w denotes the attention weights. We then multiply w and 1 \u2212 w to both input features before the summation so that the fusion process essentially acts as a self-gating mechanism to adaptively select useful information from different BEV features:\n$F_{\\text {fused }}=Adaptive\\left(Conv3 \\times 3\\left(\\left[w \\cdot F_{\\text {Camera }}^{B E V},(1-w) \\cdot F_{\\text {LiDAR }}^{B E V}\\right]\\right)\\right),$ (7)\nwhere [, ] denotes the concatenation operation along the channel dimension. \u00b7 is element-wise multiplication. Conv3x3 fuses the channel and spatial information with a 3\u00d73 convolution layer to reduce the channel dimension of concatenated feature to C. With input feature $\\hat{F} \\in R^{H \\times W \\times C}$, the Adaptive operation is formulated as:\n$Adaptive(\\hat{F})=\\sigma(W AvgPool(\\hat{F})) \\cdot \\hat{F},$ (8)\nwhere W denotes linear transform matrix (e.g., 1\u00d71 convolution) and \u03c3 denotes sigmoid function. Therefore, the DDF module can adaptively select valuable information from two modalities for better feature fusion. The output fused feature Ffused will be used for map construction task, with the decoder and prediction heads.\nRemarks: DF only performs channel-wise fusion, while DDF first conducts spatial fusion and then channel-wise fusion. DDF enhances DF by incorporating global average pooling, utilizing global weights to reduce information loss. In DDF module, the AvgPool in Eq 6 is performed in the spatial domain with an input dimension of W \u00d7 H \u00d7 C and an output of C; The AvgPool in Eq. 8 is performed in the channel domain with an input of W \u00d7 H \u00d7 C and an output of W x H."}, {"title": "3.5. Map-Task Heads", "content": "We apply specific heads for different map tasks to the fused BEV features. We show two examples: HD map construction and BEV map segmentation.\nHD map construction head. HD map constructors formulate this task as predicting a collection of vectorized static map elements in bird's eye view (BEV), i.e., pedestrian crossings, lane dividers, road boundaries. We follow MapTR [25] to train the map head with the classification loss [34], the point2point loss [33], and the edge direction loss [25].\nBEV map segmentation head. Different map categories may overlap (e.g., crosswalk is a subset of drivable space). Therefore, we formulate this problem as multiple binary semantic segmentation, one for each class. We follow BEVFusion [31] to train the segmentation head with the standard focal loss [34]."}, {"title": "4. Experiments", "content": "4.1. Dataset\nNuScenes Datasets. We evaluate our method on the widely-used challenging nuScenes [3] dataset following the standard settings of previous methods [31, 25]. The nuScenes dataset contains 1,000 sequences of recordings collected by autonomous driving cars. Each sample is annotated at 2Hz and contains 6 camera images covering 360\u00b0 horizontal FOV of the ego-vehicle. For the HD map construction task, we following MapTR [25] and three kinds of map elements are chosen for fair evaluation \u2013 pedestrian crossing, lane divider, and road boundary. Moreover, for the BEV map segmentation task, we following BEVFusion [31], we predict six semantic classes: drivable lanes, pedestrian crossings, walkways, stop lines, carparks, and lane dividers.\nArgoverse2 Dataset. There are 1,000 logs in the Argoverse2 dataset [47]. Each log contains 15s of 20Hz RGB images from 7 cameras, 10Hz LiDAR sweeps, and a 3D vectorized map. The train, validation, and test sets contain 700, 150, and 150 logs, respectively. For both HD map construction and BEV map segmentation tasks, we select three map elements for fair evaluation: pedestrian crossing, lane divider, and road boundary."}, {"title": "4.2. Evaluation Metrics", "content": "HD map construction task. We adopt the evaluation metrics consistent with previous works [21, 28, 25], where average precision (AP) is used to evaluate the map construction quality and Chamfer distance DChamfer determines the matching between predictions and ground truth. We calculate the AP, under several DChamfer thresholds (\u03c4 \u2208 T,T = {0.5, 1.0, 1.5}, unit is meter), and then average across all thresholds as the final mean AP (mAP) metric,\n$m A P=\\frac{\\sum_{\\tau \\in T} A P_{\\tau}}{|T|}$. (9)\nThe perception ranges are [-15.0m, 15.0m]/[-30.0m, 30.0m] for X/Y-axes.\nBEV map segmentation task. For the BEV map segmentation task, our primary evaluation metric is the mean Intersection over Union (mIoU). Due to potential overlaps between classes, we apply binary segmentation separately to"}, {"title": "4.3. Experimental Setting", "content": "MapFusion is trained with 4 NVIDIA RTX A6000 GPUs. For the HD map construction task, we build upon MapTR [25] as the baseline. Specifically, we adopt ResNet50 [18] and SECOND [51] as the backbone and employ GKT [4] as the default 2D-to-BEV module. Training losses include classification loss, point2point loss, and edge direction loss. with weights of 2.0, 5.0, and 0.005, respectively. The model is trained for 24 and 6 epochs on the nuScenes and Argoverse2 datasets respectively. All the data pre-processing steps for both datasets follow MapTR [25]. We set the mini-batch size to 16, and use a step-decayed learning rate with an initial value of 4e-3. For the BEV map segmentation task, we use BEVFusion [31] as our baseline and train our networks within the mmdetection3d framework [8]. Specifically, we adopt Swin-T [30] and VoxelNet [51] as the backbone, and utilize LSS [36] as the default 2D-to-BEV module. The model is trained for 20 and 6 epochs on the nuScenes and Argoverse2 datasets, respectively. The baseline is trained using the hyperparameters reported in [31], following a learning schedule of 20 epochs with a cyclic learning rate, starting for 1e-4 and performing a single cycle with target ratios 10, 1e-4 and a step of 0.4 For the CIT module described in Section 3.3 of the paper, we added this module before the fuser operation in the baseline model. To implement the cross-modal interaction, we first obtain BEV tokens by flattening each BEV feature and permuting the order of the matrix. Then, we concatenate the tokens of each modality and add a learnable positional embedding. This step is followed by a multi-head self-attention block as described in [43], containing 8 heads and an embedding dimension of 256. For the DDF module described in Section 3.4 of the paper, we replace the the naive convolutional fuser with the DDF module in the baseline model."}, {"title": "4.4. Comparison with the State-of-the-Arts", "content": "4.4.1. HD map construction task\nWe compare MapFusion with state-of-the-art HD map construction methods on nuScenes and Argoverse2 datasets. Our proposed MapFusion outperforms all existing approaches in both single-class APs and the overall mAP by a significant margin.\nExperimental Settings. We adopt average precision (AP) to evaluate the map construction quality. Chamfer distance DChamfer is used to determine whether the prediction and GT are matched or not. We calculate the AP under several DChamfer thresholds (\u03c4 \u2208 T,T = {0.5, 1.0, 1.5}, unit is meter), and then average across all thresholds as the final AP metric. The resolution of source images is 1,600 \u00d7 900. During the training phase, we resize the source images using a ratio of 0.5. Moreover, we set the maximum number of map elements in one frame, the number of points in one map element, the size of each BEV grid, and the number of transformer decoder layers to 100, 20, 0.75m, and 2, respectively. We follow the experimental settings of existing methods from MapTR [25].\nExperimental Results. With the same settings and data partition, we compare the proposed MapFusion method with several state-of-the-art methods, i.e., HDMapNet [21], VectorMapNet [28] and MapTR [25]. Tab. 1 and Tab. 2 show the overall performance of MapFusion and all the baselines on nuScenes and Argoverse2 datasets, respectively. Note that re-implementation is needed because the reference methods do not report results on Argoverse2 data set, which has different input data format from nuScenes. The experimental results reveal a number of interesting points: (1) The performance of multi-modal methods are obviously better than that of single-modal methods, which proves the significance of utilizing complementary cues from camera and LiDAR to improve the HD map construction performance. (2) In the multi-modality setting, the proposed MapFusion approach achieves a 3.6% absolute improvement in mAP over the previous state-of-the-art MapTR [25] on the nuScenes dataset. Similarly, it shows a 4.1% absolute improvement in mAP compared to MapTR [25] on the Argoverse2 dataset. This advantage arises from the limitations of the three compared HD map construction methods\u2014HDMapNet [21], VectorMapNet [28], and MapTR [25]\u2014which rely on straightforward channel concatenation and convolution for multi-modal feature fusion, as shown in Fig. 5 (a) (Conv Fusion). These methods neglect modality interaction and employ overly simplistic fusion strategies, resulting in misalignment and information loss.\nIn a nutshell, MapFusion shows significant superiority over other multi-modal methods, indicating the benefit of cross-modal interaction transform (CIT) module and dual dynamic fusion (DDF) module. This is due to the fact that the CIT module enables the two feature spaces to interact with each other and enhances feature representation through a self-attention mechanism, while the DDF module automatically selects valuable information from different modalities and can make full use of the inherent complementary information between different modalities."}, {"title": "4.4.2. BEV map segmentation task", "content": "We further compare MapFusion with state-of-the-art BEV map segmentation models, where MapFusion outperforms the state-of-the-art multi-sensor fusion methods and achieves consistent improvements across different categories.\nExperimental Settings. We report the Intersection-over-Union (IoU) on 6 background classes (drivable space, pedestrian crossing, walkway, stop line, car-parking area, and lane divider) on nuScenes dataset and 3 background classes (drivable space, pedestrian crossing and lane divider) on Argoverse2 dataset. The class-averaged mean IoU as our evaluation metric. For each frame, we only perform the evaluation in the [-50m, 50m] \u00d7 [-50m, 50m] region around the ego car following [44, 54, 31, 24]. In MapFusion model, we use a single model that jointly performs binary segmentation for all classes instead of following the conventional approach to train a separate model for each class. We follow the experimental results of existing methods from BEVFusion [31].\nExperimental Results. With the same settings and data partition, we compare the proposed MapFusion method with several state-of-the-art methods, i.e., PointPainting [44], MVP [54], and BEVFusion [31]. Tab. 3 and Tab. 4 show the overall performance of MapFusion and all the baselines on nuScenes and Argoverse2 datasets, respectively. Similar to HD map construction, we also re-implemented the experimental results for the Argoverse2 dataset.\nThe experimental results reveal several interesting points: (1) In the single-modality setting, camera-based models perform significantly better than LiDAR-based models. This observation is the exact opposite of results in 3D object detection task [24, 31]. The main reason is that the map construction task is a semantic-oriented task, which pays more attention to the semantic information in the image. Therefore, the performance of directly using the fusion method on the 3D object detection task for the map task is not satisfactory. (2) In the multi-modality setting, MapFusion outperforms existing state-of-the-art multi-sensor fusion methods, consistently across various categories. This advantage arises from the limitations of these methods: PointPainting [44] is object-centric, focusing solely on enhancing foreground LiDAR points, while MVP [54] is geometry-oriented, concentrating exclusively on densifying foreground 3D objects-neither effectively segments map components. Furthermore, BEVFusion [31] and X-Align [2] neglect modality interactions and rely on overly simplistic fusion strategies (see Fig. 5(a) Conv Fusion and Fig. 5(c) Dynamic Fusion), resulting in misalignment and information loss. Our proposed MapFusion approach achieves a 6.2% absolute improvement in mean Intersection over Union (mIoU) compared to the previous state-of-the-art BEVFusion [31] on the nuScenes dataset, and a 6.5% absolute improvement on the Argoverse2 dataset. Notably, we re-implemented the BEVFusion method following the original settings outlined in their paper. Overall, MapFusion consistently enhances the performance of existing fusion methods on both the nuScenes and Argoverse2 datasets, demonstrating the effectiveness of our proposed CIT and DDF components."}, {"title": "4.5. Ablation Studies", "content": "4.5.1. Contribution of each component\nTo systematically evaluate the effectiveness of each module of our proposed MapFusion, we train the model using different components and show the experimental results of the HD map construction task and BEV map segmentation task in Tab. 5 and Tab. 6 respectively. In the main ablation study, we design the following model variants: (1) MapFusion (Baseline): we train the model without the cross-modal interaction transform module and dual dynamic fusion module. (2) MapFusion (w/ DDF) : we train the model with the dual dynamic fusion module. (3) MapFusion (w/ CIT) : we train the model with the cross-modal interaction transform module. (4) MapFusion (full) : we train the model with the cross-modal interaction transform module and dual dynamic fusion module.\nThe experimental results reveal some interesting findings: (1) The results of both MapFusion (w/ DDF) and MapFusion (w/ CIT) are significantly better than the MapFusion (Baseline), verifying the effectiveness of CIT and DDF components for improving multi-modal BEV map construction. Compared with the baseline model, DDF and CIT modules achieve 2.5% and 3.0% absolute improvements respectively on HD map construction task, demonstrating the superiority of our approach. Similarly, compared with the baseline model, DDF and CIT modules achieve 1.4% and 5.6% absolute improvements respectively on BEV map segmentation task. (2) The results of MapFusion (w/DDF) and MapFusion (w/ CIT) are inferior to the MapFusion (full), verifying the effectiveness of using both CIT and DDF simultaneously. MapFusion (full) achieves 3.6% and 6.2% absolute improvements on the HD map construction and semantic map construction tasks, respectively, demonstrating the superiority of our method.\nThese experimental results demonstrate that the CIT module enables the camera and LiDAR BEV space to interact with each other to enhance feature representation through the cross-attention mechanism. Moreover, it is verified that the DDF module can automatically select valuable information from different modalities, thereby making full use of the inherent complementary information between different modalities."}, {"title": "4.5.2. Analysis on different Fusion methods", "content": "To systematically evaluate the effectiveness of the dual dynamic fusion (DDF) method, we train the model using different fusion methods detailed in Section 3.4. Tab. 7 and Tab. 8 show the experimental results on the HD map construction task and BEV map segmentation task using different fusion methods, respectively. For instance, the proposed DDF method achieves 2.5% absolute improvements compared with Baseline model (Conv. Fusion) method on HD map construction task. Similarly, DDF method achieves 1.4% absolute improvement compared with Baseline model (Conv. Fusion) on BEV map segmentation task. Experimental results show that the DDF module plays a vital role in multi-modal BEV feature fusion and can automatically select valuable information from different modalities for better feature fusion."}, {"title": "4.5.3. Compatibility with other HD Map Construction methods", "content": "We show MapFusion is compatibility with other HD Map Construction methods, i.e., HDMapNet [21], VectorMapNet [28], and MapTR [25]. Besides adding MapFusion, we do not modify their original training settings. For all experiments, we report the result of the nuScenes val set. As shown in Tab. 9, simply adding MapFusion on top of these strong baselines consistently improves state-of-the-art performance. MapFusion demonstrates a significant accuracy boost (absolute): HDMapNet(+7.6%), VectorMapNet (+5.5%), and MapTR (+3.6%). This shows the versatility of MapFusion as a multi-modal BEV feature fusion method."}, {"title": "4.5.4. Accuracy-Computation Analysis", "content": "In Fig. 6, we report the accuracy-computation trade-off by utilizing our proposed CIT module (See section 3.3) and different fusion strategies (See section 3.4). It can be seen that when using the CIT module, we achieve the highest accuracy improvement at a higher computational cost, while the DDF module introduces less additional cost but provides less performance gain. It can be seen that all our proposed fusion modules achieve better trade-offs compared with the baselines. Furthermore, we find that the CIT module significantly outperforms existing BEV fusion strategies, which again verifies that the baseline fusion using simple concatenation and convolutions does not provide the suitable capacity for the model to align and aggregate multi-modal features."}, {"title": "4.6. Visualization", "content": "t-SNE. We randomly choose 500 samples from the nuScenes validation dataset and show the t-SNE [32", "that": 1}]}