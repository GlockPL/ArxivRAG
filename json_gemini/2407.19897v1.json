{"title": "BEEXAI: Benchmark to Evaluate Explainable AI", "authors": ["Samuel Sithakoul", "Sara Meftah", "Cl\u00e9ment Feutry"], "abstract": "Recent research in explainability has given rise to numerous post-hoc attribution methods aimed at enhancing our comprehension of the outputs of black-box machine learning models. However, evaluating the quality of explanations lacks a cohesive approach and a consensus on the methodology for deriving quantitative metrics that gauge the efficacy of explainability post-hoc attribution methods. Furthermore, with the development of increasingly complex deep learning models for diverse data applications, the need for a reliable way of measuring the quality and correctness of explanations is becoming critical. We address this by proposing BEExAI, a benchmark tool that allows large-scale comparison of different post-hoc XAI methods, employing a set of selected evaluation metrics. The developed Python library is open-source at https://github.com/SquareResearchCenter-AI/BEEXAI.", "sections": [{"title": "1 Introduction", "content": "Advanced Machine Learning (ML) models, particularly Neural Networks (NNs), are increasingly adopted in a variety of sectors. This is because they outperform simpler models, such as linear regression or ruled-based models, in complex tasks. However, these models are harder to understand, making them less used in high-risk and heavily regulated fields like finance, cybersecurity and healthcare. Hence, there is a growing demand for eXplainable AI methods (XAI methods) to enhance these black-box models' transparency. In addition, explainability could play an important role in compliance with the AI Act [30].\nOver the past years, different methods have been developed to explain the predictions made by ML models, whose decision making process is not directly explainable. One category, known as ad-hoc methods, aims to train models to generate explanations in parallel with their predictions [22]. In contrast, post-hoc methods form another family aim to explain models that have already"}, {"title": "2 BEExAI library", "content": "BEEXAI offers an end-to-end pipeline for practitioners and researchers to generate benchmarks and compare XAI methods using a set of selected metrics. The proposed pipeline consists of 5 main components:\nData Preprocessing: Processing of tabular datasets.\nModels Training: Training of ML models.\nXAI Methods: Generating explanations (attribution scores) with various XAI methods.\nEvaluation: Computing evaluation metrics for XAI methods.\nBenchmark: Producing automatic benchmarks to compare multiple datasets, models, and XAI methods.\nIn the following section, we will describe mainly the evaluation component as it is the main focus of this work.\nOther details about the pipeline used for processing, ML models and XAI methods choice can be found in Appendix.A. Benchmarking procedure will be discussed in section.3.3."}, {"title": "2.1 \u03a7\u0391\u0399 evaluation", "content": "The literature has explored various metrics and desired properties. However, our purpose is not to include all available metrics, which could introduce unnecessary noise, but to deliver users with a tool that integrates the most commonly used and practical metrics, helping them decide the most suitable XAI method for their use case. Additionally, given the plurality of denominations and formulations attributed to the same metric in the literature, we opt for the most commonly used formulas with low computational complexity to facilitate the understanding of the quantified property and its results.\nThe evaluation component of BEExAI v0.0.6 provides 9 metrics to quantify 3 main desired properties of explainability: Faithfulness [7], Robustness [2] and Complexity [7]:"}, {"title": "2.2 Choice of baseline", "content": "Some evaluation metrics require a features' ablation process, which involves replacing (masking) specific features with a baseline value. Choosing the right baseline is crucial as it needs to represent a neutral reference point for attributions [15]. One group of baselines consists in sampling feature values from the other instances of the dataset as baselines. Possible methods are:\nUniform or normal distribution,\nOpposite: take a sample with a different ground-truth label,\nMax distance: select an instance with the maximum L1 distance from the instance of interest while remaining within the features' distribution.\nAnother alternative is using a constant baseline, which is the most common choice. It consists in the replacement of a feature by a constant value, often zero, the mean or the median of the concerned feature.\nThe benchmarks that follow are based on the zero baseline. This choice is considered as the most appropriate for NNs [3] because it leads empirically to a prediction near zero resulting from the multiplication of weights.\nFurther research is needed to accurately measure the baselines' choice impact on evaluation metrics' results and how they align with the intended interpretation of these metrics. This is particularly important when dealing with out-of-distribution instances, which can hinder the production of explanations that are representative of the dataset and the task under consideration [17].\nIt is possible to compute all the mentioned explainability metrics with just one line of code. This requires a curated dataset, a trained model and an explainer as inputs. More parameters can be selected to customize the computation of metrics and get more details about the desired output format."}, {"title": "3 Experiments", "content": "Main External Libraries Utilized for BEExAI Implementation For the implementation of ML models, we use the scikit-learn library [32], encompassing Gradient Boosting, Histogram-based Gradient Boosting, XGBoost, Random Forest, Linear Regression and Logistic Regression. Default hyperparameters are retained for these models to ensure easier reproducibility with satisfactory results. The proposed implementation of NNs utilizes PyTorch [31]. Hyperparameters' selection is discussed in Section.3.3.\nThrough the BEExAI library, different models can be used to generate benchmarks. This paper provides evaluation results only on XGBoost and NNs"}, {"title": "3.2 Datasets", "content": "In this section, we present an overview of the datasets employed in our benchmark evaluation. To ensure robustness in handling real-world project tasks, we utilize datasets that are derived from two benchmarks, specifically designed for tabular data.\ninria-soda/tabular-benchmark [14] is a selection of datasets from OpenML for regression and classification tasks with a specific curation process. This benchmark includes datasets with only numerical features and data with both categorical and numerical features. These datasets are diverse in size and dimensionality, allowing for the generation of explanations and evaluation metrics at different scales.\nWe utilized all the datasets present in the benchmark for binary classification. However, for regression, we have excluded ten datasets having an R2 score below 0.3. We choose to restrict our analysis to models with similar performances to limit potential biases. We keep the study of the influence of models' predictive capability on XAI metric to future work.\nOpenML-CC18 Curated Classification [8] The CC18 benchmark is a suite of 72 classification tasks extracted from other datasets from OpenML with a different curation process than the previous benchmark.\nWithin these 72 datasets, we selected a subset of 8 datasets with strictly more than two classes to predict, more than 500 training samples and less than 300 features after one-hot encoding. This selection maintains a satisfying variety in size and feature dimensionality, which can lead to more accurate results in multi-classification problems and thus completing the precedent benchmark as we can compare in Table 4."}, {"title": "3.3 Benchmarking procedure", "content": "Data: The benchmarks were produced on OpenML-CC18 Curated Classification and inria-soda/tabular-benchmark to cover a wide range of tasks with tabular data. We used the provided curated datasets from the two benchmarks without making any additional processing. To reduce the influence of outliers on metrics like Infidelity, which can introduce noise in the calculations, we used Quantile Transformer scaling on the input features. Additionally, we chose to use"}, {"title": "4 Task specificity", "content": "During our experiments, we realized that to quantify evaluation measures effectively, we need to apply distinct treatments to the generated explainability attributions and evaluations metrics based on the type of task of interest (binary classification, multi-label classification, or regression). To the best of our knowledge, there is a lack in previous research addressing this matter."}, {"title": "4.1 Classification", "content": "To compute the attributions, we select, for each instance, the label with the highest predicted probability as the target. We believe that this approach provides a more accurate portrayal of the model's predictive capability. Afterwards, we calculate the evaluation metrics using the same chosen targets.\nAn alternate solution that we discarded is to choose the ground-truth label. Indeed, in real-world scenarios, we do not have access to ground-truth labels, making this method impractical.\nWe have also experimented with other alternatives. One approach we tried was to compute attributions and evaluation measures for each label, then take the mean of these values across all labels to obtain a global score for each instance. However, this approach was computationally expensive, especially for multi-classification tasks, and it did not yield accurate results as the predicted probabilities for classes that were not predicted might not accurately reflect the model's predictive capability. We also considered the approach of summing on the absolute values of the attributions for each label before calculating the explainability metrics. The assumption behind this approach was that we were only concerned with the relative importance of each feature. However, this possibility was discarded as it was not accounting for the fact that the attributions' sign has a significant role in classification tasks.\nWe made our choice of treatment according to our results, but we emphasize the need for in-depth research to analyze the best method that ensures consistent comparisons between XAI methods."}, {"title": "4.2 Regression", "content": "Attributions We consider the absolute values of the generated attributions instead of raw values. This choice is motivated by the specificity of regression tasks compared to classification ones. Indeed, in regression tasks, the goal is to predict a scalar value, not a probability as in classification tasks. Hence, the target value can be approached by positive and negative contributions from the input features. Negative contributions can offset positive ones, leading to faster convergence toward the target. Therefore, we prioritize the features with the highest contributions, regardless of whether they are positive or negative. We found that retaining the signs of attributions led to less satisfactory results for metrics like Comprehensiveness and Sufficiency.\nMetrics We choose to adjust specific metrics (Faithfulness Correlation, Comprehensiveness, Sufficiency, and Monotonicity) that consider the difference between the model's prediction on a sample and the same model's prediction of a perturbed version by taking the absolute value of this difference instead of the signed difference. Indeed, the interpretation of the difference's sign is different if we are on a regression or a classification scenario. In the case of classification, the prediction is represented as a probability for the label of interest. Hence, when masking the first most important features, we expect a positive difference that represents a probability's degradation of the label of interest (the predicted label). However, in the case of regression, when masking the first most important features, this degradation can be either positive or negative because the prediction"}, {"title": "5 Results", "content": "In this section, we present the XAI methods' evaluation results of two ML models: NNs and XGBoost. Indeed, we found that the two methods perform similarly in terms of accuracy for classification and R2 score for regression tasks. This similarity of performance is an excellent point to focus on comparison between XAI methods."}, {"title": "6 Conclusion", "content": "We have presented BEEXAI, an open-source library that can be used for benchmarking and evaluating post-hoc explainability attribution methods. This library aims to establish a standard evaluation of XAI methods through quantitative analysis. It provides a simplified and reproducible end-to-end pipeline that allows users to evaluate, compare, and benchmark XAI methods on any tabular dataset, covering a wide range of ML models, \u03a7\u0391\u0399 methods, and metrics. Furthermore, we provide the largest benchmark available to date for three fundamental tasks in tabular-data problems: regression, binary classification, and multi-label classification. We hope that BEExAI will contribute to the development of XAI in the future and serve as a foundation for the research community.\nThroughout this paper, we have shared insightful reflections on the challenges that arise when studying XAI evaluation. First, we have discussed the challenges raised for each task type and their impact at different stages of the evaluation process. Second, we have delved into the baseline choice for evaluation metrics that involve a feature ablation process. Third, we have proposed a sanity check that compares random explanations with explanations of XAI methods to assess the quality of generated explanations. However, although we have discussed and proposed solutions to these challenges, we believe that in-depth research is needed to gain a thorough understanding.\nTo complete BEExAI, we propose a set of perspectives that could significantly enhance its capabilities. Regarding the data, expanding our library to include NLP and Computer Vision pipelines would be a welcome extension. In addition, it would be interesting to consider a more extensive evaluation of XAI methods (1000 data-points are used for this paper's results). Regarding the models, we hope that future work addresses the influence of models' predictive capability on explainers' performance, especially when two ML models have significant discrepancies in performance. For explainers, future work could focus on integrating other types of explanations, such as rule-based methods, into BEE\u03a7\u0391\u0399.\nFinally, we believe that quantitative analysis should be associated with a human assessment of the results, i.e., by studying the Plausibility of explanations."}, {"title": "A BEEXAI other components", "content": "Data processing BEEXAI provides a pre-built preprocessing component for tabular data. Its main purpose is to simplify the production of large-scale benchmarks. Different options are proposed to prepare datasets for models' training:\nAutomated DateTime features processing: Automatically convert features with DateTime format by extracting key time information such as year, month, day, and hour.\nCreation of new features: Adding new features based on elementary operations on other features. This can help to provide more context and insights for the model and improve its accuracy.\nRemoving non-relevant samples: Delete rows having a feature with a specific value.\nCorrelated features: To avoid redundancy and overfitting, automatically keep only features with a cross-correlation lower than a given threshold.\nStandardization: Numerical input features and regression target are automatically standardized to warrant an accurate comparison. This includes Standard, Min-Max, Max-Abs, Robust scaler and Quantile transformation with uniform or Gaussian distribution.\nCategorical features encoding: Ordinal encoding or One-Hot encoding.\nThese steps allow processing a custom tabular dataset associated with a configuration file into a curated dataset, and splits for training and evaluation.\nIt should be noted that using this component is optional. We invite users to provide their own processed datasets, if available. Indeed, the main objective of this library is not to maximize models' performance. Hence, this component provides a good starting point for simple datasets to focus more on the XAI, but there is room for further improvement with more preprocessing options."}, {"title": "Model Training", "content": "BEExAI provides an easy and automatic training component for multi-label classification, binary classification and regression. It allows using the most commonly used ML models, such as XGBoost, Random Forest, Decision Tree, Gradient Boosting, and Logistic and Linear Regression.\nWe also provide an implementation of fully-connected NNs with linear layers and ReLU activations. Users can adjust hyperparameters, such as hidden layers' number and regularization. We leave the exploration of more intricate NNs architectures to future studies, e.g., data-types like Natural Language Processing (NLP), where the use of Large Language Models add intricacy, or in Computer Vision, where Convolutional NNs are commonly used.\nA model can easily be instantiated and trained with a Model class and a preprocessed training set. Custom hyperparameters can be provided to enhance the model beyond the default settings to make it more suitable for the specific dataset at hand. We also provide training options with K-fold cross-validation and GridSearch to tune previously mentioned scikit-learn models' hyperparameters."}, {"title": "Explainers", "content": "The explainers' component provides access to the most popular XAI methods. Note that, as aforementioned, our focus lies exclusively on local post-hoc feature attribution methods. Rule-based approaches such as Anchors and example-based methods like Counterfactuals are not within the scope of this work. These are reserved for future research and independent benchmarking tools, as we believe that distinct evaluation frameworks are necessary for each type or format of explanation.\nThe first category of XAI methods available in BEEXAI v0.0.6 are perturbation-based methods that modify the input data and measure the changes in the model's predictive ability. These XAI methods estimate the importance of features for a single instance by comparing the difference between the model's output when using the original instance and the model's output when a perturbed version of the instance of interest is used. To obtain a good approximation for the computed attributions, these methods often repeat this process with several perturbations.\nMost perturbation-based methods are, by definition, model-agnostic, although some combinations with gradient-based methods such as TreeSHAP [26] or DeepSHAP [27] have emerged in recent years. This allows attribution computation for a wide range of ML models. However, this type of method has a higher computational cost due to the iterative process on multiple forwards passes with several perturbed samples. Additionally, these methods can be sensitive to the choice of the perturbation function.\nWe consider the following XAI methods from different public repositories:\nLIME (Local Interpretable Model-Agnostic Explanations) [33]\nShapley Value Sampling [37]\nKernel SHAP [27]\nFeature Ablation, a particular case of [42]\nThe second category of XAI methods considered in BEEXAI v0.0.6 are gradient-based methods specific to models with gradient descent optimization,"}, {"title": "\u0392 \u03a7\u0391\u0399 metrics", "content": "Complexity [7] computes the Shannon entropy of features' fractional contributions. The fractional contribution of a feature i, denoted $P_{g(i)}$, is defined as:\n$P_{g(i)} = \\frac{g(f,x)_i}{\\sum_{s \\in S} g(f,x)}$, where S represents the features set, g is the explainability function, f is the predictor and x a specific instance. Complexity, denoted $\\mu_c(f, g; x)$, is defined as:\n$\\mu_c(f, g; x) = E_i[-\\ln(P_g)] = -\\sum P_g(i) \\ln(P_g(i)).$\n(1)\nThe objective is to minimize complexity, with a set of features ideally having high contributions and the others approaching 0. On the contrary, uniformly distributed attributions among features result in high complexity. In our implementation, we standardize complexity to get values ranging from 0 to 1.\nSparseness [9] is another way of quantifying the concentration of explanations on specific features. Given an attribution vector $v \\triangleq [v_1, v_2, ..., v_d]$ with non-negative values sorted in non-decreasing order, Sparseness is defined with the Gini Index:\n$G(v) = 1 - 2\\sum_{k=1}^{d} (\\frac{v_k}{||v||_1})(\\frac{d-k+0.5}{d})$.\n(2)\nFaithfulness correlation [7] quantifies the correlation between the sum of attributions of $x_s$ (where $x_s$ denotes x with a subset S of features replaced by a baseline value) and the difference in model predictions between $x_s$ and the original input x. Given a predictor function f, an explanation function g, an input instance x, and a feature subset S, the faithfulness correlation is defined as:\n$\\mu_F(f, g; x) = corr_{S \\in (\\frac{\\d}{|S|}) i \\in S} (\\sum g(f,x)_i, f(x) - f(x_s)).$\n(3)\nNote that not all subsets within $(\\frac{\\d}{|S|})$ are explored and the size of subsets |S| is fixed. By definition, the correlation values range from -1 to 1. Ideally, $\\mu_F = 1$. In our experiments, we set the subset size |S| as a percentage of the total number of features, with a default value of 20%, which we found yields sufficiently discriminating results between different explainability methods. We iterate this process 20 times, which we consider a good trade-off between computation time and approximation quality.\nArea Under the Threshold Performance Curve [6] computes the AUC of the curve (q, P(f(x) \u2212 f(x_{q\\%})), where P represents the performance measure of the task of interest and $x_{q\\%}$ denotes the input with the q% most important features (according to the explainability method under consideration) replaced by a baseline, with q \u2208 [0, 10, ..., 100]. We expect the performance curve will notably decrease after removing the most important features, hence the objective is to minimize the AUC. We standardize the final results by the number of features in the dataset to ensure comparability across different tasks.\nComprehensiveness [12] quantifies the impact of replacing the q% most important features by a baseline value. It is defined as:\n$C(x,q) = f(x) - f(x_{q\\%})$\n(4)\nwhere $x_{q\\%}$ represents the input x with the q% most important features replaced by a baseline. We expect that as we replace features in a non-ascending order of their attributions, the metric value will increase with the augmentation of q%, the percentage of replaced features. The most important features that are ablated first have the greatest impact on the metric value. As we remove the less important features, the impact decreases.\nSufficiency [12] measures the impact of including the most important features to a baseline on the model's output. It is complementary to comprehensiveness and is defined as:\n$S(x,r) = f(x) - f(x_{r\\%})$\n(5)"}, {"title": "", "content": "where $x_{r\\%}$ represents the input x with only r% of the most important features added starting from a baseline. Ideally, as we incorporate the most important features in a non-ascending order of their attributions, we expect the prediction to converge towards the prediction when the original input is used.\nFor both Comprehensiveness and Sufficiency, we use a ratio of 30% for q and r.\nMonotonicity [24] compares the marginal impact of a set of features to their corresponding weights. Given an ensemble $S_i$ comprising the i most important attributions, an instance x and a predictive function f, Monotonicity is defined as:\n$m = \\frac{1}{D-1} \\sum_{i=0}^{D-1} | \\delta_i \\leq \\delta_{i+1} |$\n(6)\nwith $d_i = f(x_{S_{i+1}}) \u2212 f(x_{S_i})$. An ideal Monotonicity value is 1, indicating that each subsequent feature provides better marginal improvement than a less important feature.\nInfidelity [41] quantifies Faithfulness by measuring the impact of significant perturbations on the predictive function. It is computed as the MSE between the attributions multiplied by a perturbation value and the difference between the predictive function evaluated on the original input and the perturbed input.\n$Inf(g, f,x) = [E_{I \\sim \\mu_I} [(I \\cdot g(f,x) - (f(x) \u2212 f(x - I)))^2]].$\n(7)\nHere, I represents a significant perturbation around x. The choice of perturbation should align with the task at hand. A common choice for perturbation is Gaussian-centered noise with a specific std. We adopt this option, with an std being the average distance between dataset's points [7].\nSensitivity [41] measures the impact of small perturbations on the predictive function. It is computed as the gradient of the explainability function with respect to the input: $[\\nabla_x g(f(x))]_i = \\lim_{e \\rightarrow 0} \\frac{g(f(x+ee_j)) - g(f(x))}{e}$. Here, $e_j$ is the basis vector of coordinate j. However, Sensitivity is often derived as the Max-Sensitivity within a sphere around the input of radius r:\n$SENS_{MAX}(g, f,x,r) = \\underset{||y-x||<r}{max} ||g(f,x) - g(f,y)||.$\n(8)\nMax-Sensitivity proves to be a more robust metric compared to local Lipschitz continuity measure, as the later can be unbounded for NNs. In our experiments, we sample perturbed inputs within a sphere centered around the original sample, with a radius equal to that chosen for Infidelity; the average distance between dataset's points."}]}