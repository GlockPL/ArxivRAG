{"title": "BEEXAI: Benchmark to Evaluate Explainable AI", "authors": ["Samuel Sithakoul", "Sara Meftah", "Cl\u00e9ment Feutry"], "abstract": "Recent research in explainability has given rise to numerous post-hoc attribution methods aimed at enhancing our comprehension of the outputs of black-box machine learning models.\u00b9 However, evaluating the quality of explanations lacks a cohesive approach and a consensus on the methodology for deriving quantitative metrics that gauge the efficacy of explainability post-hoc attribution methods. Furthermore, with the development of increasingly complex deep learning models for diverse data applications, the need for a reliable way of measuring the quality and correctness of explanations is becoming critical. We address this by proposing BEExAI, a benchmark tool that allows large-scale comparison of different post-hoc XAI methods, employing a set of selected evaluation metrics. The developed Python library is open-source at https://github.com/SquareResearchCenter-AI/BEEXAI.", "sections": [{"title": "1 Introduction", "content": "Advanced Machine Learning (ML) models, particularly Neural Networks (NNs), are increasingly adopted in a variety of sectors. This is because they outperform simpler models, such as linear regression or ruled-based models, in complex tasks. However, these models are harder to understand, making them less used in high-risk and heavily regulated fields like finance, cybersecurity and healthcare. Hence, there is a growing demand for eXplainable AI methods (XAI methods) to enhance these black-box models' transparency. In addition, explainability could play an important role in compliance with the AI Act [30].\nOver the past years, different methods have been developed to explain the predictions made by ML models, whose decision making process is not directly explainable. One category, known as ad-hoc methods, aims to train models to generate explanations in parallel with their predictions [22]. In contrast, post-hoc methods form another family aim to explain models that have already\n\u00b9 There is little consensus regarding the differences between \"interpretability\" and \"explainability\" [23]. Within the scope of this paper, we adopt the term \"explainability\" to denote methods specifically designed to provide interpretable justifications for the predictions made by a machine learning model."}, {"title": "2 BEExAI library", "content": "BEEXAI offers an end-to-end pipeline for practitioners and researchers to generate benchmarks and compare XAI methods using a set of selected metrics. The proposed pipeline consists of 5 main components:\nData Preprocessing: Processing of tabular datasets.\nModels Training: Training of ML models.\nXAI Methods: Generating explanations (attribution scores) with various \u03a7\u0391\u0399 methods.\nEvaluation: Computing evaluation metrics for XAI methods.\nBenchmark: Producing automatic benchmarks to compare multiple datasets, models, and XAI methods.\nIn the following section, we will describe mainly the evaluation component as it is the main focus of this work.\nOther details about the pipeline used for processing, ML models and XAI methods choice can be found in Appendix.A. Benchmarking procedure will be discussed in section.3.3."}, {"title": "2.1 \u03a7\u0391\u0399 evaluation", "content": "The literature has explored various metrics and desired properties. However, our purpose is not to include all available metrics, which could introduce unnecessary noise, but to deliver users with a tool that integrates the most commonly used and practical metrics, helping them decide the most suitable XAI method for their use case. Additionally, given the plurality of denominations and formulations attributed to the same metric in the literature, we opt for the most commonly used formulas with low computational complexity to facilitate the understanding of the quantified property and its results.\nThe evaluation component of BEExAI v0.0.6 provides 9 metrics to quantify 3 main desired properties of explainability: Faithfulness [7], Robustness [2] and Complexity [7]:"}, {"title": "2.2 Choice of baseline", "content": "Some evaluation metrics require a features' ablation process, which involves replacing (masking) specific features with a baseline value. Choosing the right baseline is crucial as it needs to represent a neutral reference point for attributions [15]. One group of baselines consists in sampling feature values from the other instances of the dataset as baselines. Possible methods are:\nUniform or normal distribution,\nOpposite: take a sample with a different ground-truth label,\nMax distance: select an instance with the maximum L1 distance from the instance of interest while remaining within the features' distribution.\nAnother alternative is using a constant baseline, which is the most common choice. It consists in the replacement of a feature by a constant value, often zero, the mean or the median of the concerned feature.\nThe benchmarks that follow are based on the zero baseline. This choice is considered as the most appropriate for NNs [3] because it leads empirically to a prediction near zero resulting from the multiplication of weights.\nFurther research is needed to accurately measure the baselines' choice impact on evaluation metrics' results and how they align with the intended interpreta- tion of these metrics. This is particularly important when dealing with out-of- distribution instances, which can hinder the production of explanations that are representative of the dataset and the task under consideration [17].\nIt is possible to compute all the mentioned explainability metrics with just one line of code. This requires a curated dataset, a trained model and an explainer as inputs. More parameters can be selected to customize the computation of metrics and get more details about the desired output format.\nfrom beexai.evaluate.metrics.get_results import\nget_all_metrics\nget_all_metrics(X_test, model, explainer)"}, {"title": "3 Experiments", "content": "3.1 Implementation details\nMain External Libraries Utilized for BEExAI Implementation For the implementation of ML models, we use the scikit-learn library [32], encompassing Gradient Boosting, Histogram-based Gradient Boosting, XGBoost, Random Forest, Linear Regression and Logistic Regression. Default hyperparameters are retained for these models to ensure easier reproducibility with satisfactory results. The proposed implementation of NNs utilizes PyTorch [31]. Hyperparameters' selection is discussed in Section.3.3.\nThrough the BEExAI library, different models can be used to generate benchmarks. This paper provides evaluation results only on XGBoost and NNs"}, {"title": "4 Task specificity", "content": "During our experiments, we realized that to quantify evaluation measures ef- fectively, we need to apply distinct treatments to the generated explainability attributions and evaluations metrics based on the type of task of interest (bi- nary classification, multi-label classification, or regression). To the best of our knowledge, there is a lack in previous research addressing this matter."}, {"title": "4.1 Classification", "content": "To compute the attributions, we select, for each instance, the label with the highest predicted probability as the target. We believe that this approach provides a more accurate portrayal of the model's predictive capability. Afterwards, we calculate the evaluation metrics using the same chosen targets.\nAn alternate solution that we discarded is to choose the ground-truth label. Indeed, in real-world scenarios, we do not have access to ground-truth labels, making this method impractical.\nWe have also experimented with other alternatives. One approach we tried was to compute attributions and evaluation measures for each label, then take the mean of these values across all labels to obtain a global score for each instance. However, this approach was computationally expensive, especially for multi-classification tasks, and it did not yield accurate results as the predicted probabilities for classes that were not predicted might not accurately reflect the model's predictive capability. We also considered the approach of summing on the absolute values of the attributions for each label before calculating the explainability metrics. The assumption behind this approach was that we were only concerned with the relative importance of each feature. However, this possibility was discarded as it was not accounting for the fact that the attributions' sign has a significant role in classification tasks.\nWe made our choice of treatment according to our results, but we emphasize the need for in-depth research to analyze the best method that ensures consistent comparisons between XAI methods."}, {"title": "4.2 Regression", "content": "Attributions We consider the absolute values of the generated attributions instead of raw values. This choice is motivated by the specificity of regression tasks compared to classification ones. Indeed, in regression tasks, the goal is to predict a scalar value, not a probability as in classification tasks. Hence, the target value can be approached by positive and negative contributions from the input features. Negative contributions can offset positive ones, leading to faster convergence toward the target. Therefore, we prioritize the features with the highest contributions, regardless of whether they are positive or negative. We found that retaining the signs of attributions led to less satisfactory results for metrics like Comprehensiveness and Sufficiency.\nMetrics We choose to adjust specific metrics (Faithfulness Correlation, Compre- hensiveness, Sufficiency, and Monotonicity) that consider the difference between the model's prediction on a sample and the same model's prediction of a per- turbed version by taking the absolute value of this difference instead of the signed difference. Indeed, the interpretation of the difference's sign is different if we are on a regression or a classification scenario. In the case of classification, the prediction is represented as a probability for the label of interest. Hence, when masking the first most important features, we expect a positive difference that represents a probability's degradation of the label of interest (the predicted label). However, in the case of regression, when masking the first most important features, this degradation can be either positive or negative because the prediction"}, {"title": "5 Results", "content": "In this section, we present the XAI methods' evaluation results of two ML models: NNs and XGBoost. Indeed, we found that the two methods perform similarly in terms of accuracy for classification and R2 score for regression tasks.6 This similarity of performance is an excellent point to focus on comparison between \u03a7\u0391\u0399 methods.\nTo facilitate the interpretation of our results, we distinguish between two categories. Metrics where high values are desired are marked with, signifying\n6 It is worth noting that we did not explore hyperparameters for XGBoost models as they were already performing well, and specific model tuning was not the main focus of this work."}, {"title": "6 Conclusion", "content": "We have presented BEEXAI, an open-source library that can be used for bench- marking and evaluating post-hoc explainability attribution methods. This library aims to establish a standard evaluation of XAI methods through quantitative analysis. It provides a simplified and reproducible end-to-end pipeline that allows users to evaluate, compare, and benchmark XAI methods on any tabular dataset, covering a wide range of ML models, \u03a7\u0391\u0399 methods, and metrics. Furthermore, we provide the largest benchmark available to date for three fundamental tasks in tabular-data problems: regression, binary classification, and multi-label classi- fication. We hope that BEExAI will contribute to the development of XAI in the future and serve as a foundation for the research community.\nThroughout this paper, we have shared insightful reflections on the challenges that arise when studying XAI evaluation. First, we have discussed the challenges raised for each task type and their impact at different stages of the evaluation process. Second, we have delved into the baseline choice for evaluation metrics that involve a feature ablation process. Third, we have proposed a sanity check that compares random explanations with explanations of XAI methods to assess the quality of generated explanations. However, although we have discussed and proposed solutions to these challenges, we believe that in-depth research is needed to gain a thorough understanding.\nTo complete BEExAI, we propose a set of perspectives that could significantly enhance its capabilities. Regarding the data, expanding our library to include NLP and Computer Vision pipelines would be a welcome extension. In addition, it would be interesting to consider a more extensive evaluation of XAI methods (1000 data-points are used for this paper's results). Regarding the models, we hope that future work addresses the influence of models' predictive capability on explainers' performance, especially when two ML models have significant discrepancies in performance. For explainers, future work could focus on integrating other types of explanations, such as rule-based methods, into BEE\u03a7\u0391\u0399.\nFinally, we believe that quantitative analysis should be associated with a human assessment of the results, i.e., by studying the Plausibility of explanations."}, {"title": "\u0392 \u03a7\u0391\u0399 metrics", "content": "Complexity [7] computes the Shannon entropy of features' fractional contri- butions. The fractional contribution of a feature i, denoted Pg(i), is defined as:\nPg(i) =  \\frac{g(f,x)i}{\\Sigma_{s \\in S} g(f,x)}, where S represents the features set, g is the explainabil- ity function, f is the predictor and x a specific instance. Complexity, denoted \u03bcc(f, g; x), is defined as:\n\u03bcc(f, g; x) = E\u00bf[\u2212ln(Pg)] = \u2212 \u2211Pg(i)ln(Pg(i)).\n(1)\nSES\nThe objective is to minimize complexity, with a set of features ideally having high contributions and the others approaching 0. On the contrary, uniformly distributed attributions among features result in high complexity. In our imple- mentation, we standardize complexity to get values ranging from 0 to 1.\nSparseness [9] is another way of quantifying the concentration of explanations on specific features. Given an attribution vector v [U1, U2, ..., va] with non-negative values sorted in non-decreasing order, Sparseness is defined with the Gini Index:\nG(v) = 1 - 2 \\sum_{k=1}^{d} ( \\frac{U_k}{\\| v \\|_1 } (\\frac{d-k +0.5}{d})).\n(2)"}]}