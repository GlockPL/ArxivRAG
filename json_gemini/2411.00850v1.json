{"title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models", "authors": ["Yihua Shao", "Siyu Liang", "Xiaolin Lin", "Zijian Ling", "Zixian Zhu", "Minxi Yan", "Haiyang Liu", "Siyu Chen", "Ziyang Yan", "Yilan Meng", "Chenyu Zhang", "Haotong Qin", "Michele Magno", "Yang Yang", "Zhen Lei", "Yan Wang", "Jingcai Guo", "Ling Shao", "Hao Tang"], "abstract": "Large language models (LLMs) show impressive performance in solving complex language tasks. However, its large number of parameters present significant challenges for the deployment and application of the model on edge devices. Compressing large language models to low bits can enable them to run on resource-constrained devices, often leading to performance degradation. To address this problem, we propose gradient-aware weight quantization (GWQ), the first quantization approach for low-bit weight quantization that leverages gradients to localize outliers, requiring only a minimal amount of calibration data for outlier detection. GWQ retains the weights corresponding to the top 1% outliers preferentially at FP16 precision, while the remaining non-outlier weights are stored in a low-bit format. GWQ found experimentally that utilizing the sensitive weights in the gradient localization model is more scientific compared to utilizing the sensitive weights in the Hessian matrix localization model. Compared to current quantization methods, GWQ can be applied to multiple language models and achieves lower PPL on the WikiText2 and C4 dataset. In the zero-shot task, GWQ quantized models have higher accuracy compared to other quantization methods.GWQ is also suitable for multimodal model quantization, and the quantized Qwen-VL family model is more accurate than other methods. zero-shot target detection task dataset RefCOCO outperforms the current stat-of-the-arts method SPQR. GWQ achieves 1.2\u00d7 inference speedup in comparison to the original model, and effectively reduces the inference memory.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Touvron et al., 2023a; Achiam et al., 2023; Almazrouei et al., 2023; Touvron et al., 2023b) based on Transformer (Vaswani, 2017) have demonstrated their outstanding capabilities in scenarios such as complex question and answer, language modeling (Brown, 2020), and so on. It can often generate more accurate answers with different inputs. Its ability to handle complex linguistic tasks is due to its huge pre-trained corpus (Kaplan et al., 2020) and an astronomical number of parameters (Chowdhery et al., 2023). Its outstanding performance is rapidly generating huge applications in people's lives. However, the huge memory consumption generated by its pre-training and inference phases leads to great difficulties in its application and deployment (Li et al., 2024a). Therefore, when deploying large language models, there are often huge GPU clusters to support the inference of the models (Xu et al., 2024).\nIn order to make large language models adaptable to resource-constrained edge devices (Li et al., 2024b; Huang et al., 2024), model compression (Ma et al., 2023; Gu et al., 2023) has become a common means to reduce the computational memory of models. Among them, post-training large language model quantization (Dettmers and Zettlemoyer, 2023) is a commonly used method to reduce the model inference overhead. Most current quantization methods compress the model to 3 or 4 bits (Dettmers et al., 2023; Frantar et al., 2022a), while trying to ensure that the model has as little loss of accuracy and performance as possible. Post-training quantization of pre-trained models reduces model inference overhead by allowing the number of parameter bits to decrease. This approach produces a loss of accuracy that affects the performance of the model in use. Therefore, to reduce the size of the model while ensuring its performance, it is necessary to design effective quantization algorithms (Lin et al., 2024). With effective quantization algorithms, researchers can make the performance of low-bit models infinitely close to that of 16-bit models."}, {"title": "2 Related Work", "content": "Post-training Quantization. Post-training quantization (PTQ) applies mainly to visual models (Gholami et al., 2022). For example, AdaRound (Nagel et al., 2020), AdaQuant (Hubara et al., 2020), Bit-Split (Wang et al., 2024b) can be used for models with a small number of parameters. When models have a large number of parameters, they cause a significant loss of accuracy, and the reduction in memory overhead of the quantized model compared to the original model is not particularly significant. Most current quantization methods for models are improved by Optimal Brain Quantization (OBQ) (Frantar and Alistarh, 2022). OBQ is improved by the Optimal Brain Surgeon (OBS) (Hassibi et al., 1993; Singh and Alistarh, 2020; Frantar et al., 2021), both OBS and OBQ default to a model whose response to input is 0 at the end of training, but the OBQ method applies the idea of weight pruning to model quantization by quantifying the weights that have the least impact on the network and compensating for them with the remaining weights. Some current PTQ methods perform outlier search by reinforcement learning (Wang et al., 2024b) to retrieve the best outlier by an objective function on a calibration set with a very large sample size. However, the training process of reinforcement learning is not robust and is difficult to converge compared to other PTQ methods, making the calibration process very inefficient.\nLarge Language Model Quantization. Currently, large language model quantization is mainly divided into quantization-based training (Liu et al., 2023; Bondarenko et al., 2024; Zhu et al., 2023) and post-training quantization. Quantization during training is very difficult to apply due to the large amount of computational resources and overhead required, as well as the large amount of training data. The remaining methods such as ZeroQuant (Yao et al., 2022) utilize knowledge distillation to train one transformer layer at a time instead of training all transformer layers directly. However, poor quality of certain data can also affect the performance of the model after quantization (Wang et al., 2024a). Post-training quantization requires less data compared to quantization-aware training, and only a few thousand or even a few hundred calibration sets are needed to complete the retrieval of sensitive weights for the model. This greatly reduces the data cost and the computational cost of model quantization. Both GPTQ (Frantar et al., 2022a) and SPQR (Dettmers et al., 2023), quantization methods derived from the OBQ method, have shown superior performance in large language models. There are also methods such as AWQ (Lin et al., 2024), LLM.int8() (Dettmers et al., 2022) that determine the outlier of the model by searching for the activation of the model, and then quantify it with the corresponding method. However, the method of searching for outliers by activation has poor interpretability."}, {"title": "3 The Proposed Method", "content": "The outlier search method based on Hessian matrix (Frantar et al., 2022a; Dettmers et al., 2023) is built upon the OBQ approach (Frantar et al., 2022b). The OBQ method retrieves the parameters that have the least impact on the model parameters by using the following equation:\n$\\Delta F = \\sum_i g_i w_i + \\frac{1}{2} \\sum_i h_{ii} w_i \\Delta w_i + \\frac{1}{2} \\sum_{i \\ne j} h_{ij} w_i \\Delta w_j + O(\\Delta w^3).$\nwhere $g_i = \\frac{\\partial E}{\\partial w_i}$ is the first order gradient. In the OBQ framework, it is suggested that once a pre-trained model has fully converged, its gradients $\\frac{\\partial E}{\\partial w_i}$ should ideally approach zero. However, our experiments demonstrate that LLMs continue to generate gradients in response to various text inputs. Bondarenko et al. (2023) denotes that the occurrence of outliers in large models arises from attention heads attempting to avoid updating hidden states. During this process, the softmax function magnifies the formation of strong outliers. Building on this observation, we hypothesize that when a well-trained LLM computes gradients for text input, it often focuses these gradients on irrelevant outliers as a mechanism to prevent hidden state updates. As shown in Fig. 1, compared to SPQR, GWQ demonstrates a sparser allocation of outliers. Specifically, the outliers identified are distributed either per-channel or per-row, which contrasts with the more dense and uniform distribution of outliers observed in SPQR. This sparsity in GWQ results in a more reasonable allocation, as it does not require to concentrate on the compensation of sensitive weight errors across every layer output. As shown in Tab. 1, with different proportions of outliers, the model using Hessian matrix search and quantized using RTN (Nagel et al., 2020) performs worse than the model using gradient search and quantized"}, {"title": "3.1 Sensitive Weight Location", "content": "GWQ captures the absolute gradients with respect to LLM, acquired through back-propagation, the detail is shown in Alg. 1. We disabled bias in all selected linear layers, since the bias term does not get multiplied by inputs in the same way as weights do. This can cause the weight gradients to be unrepresentative of the actual importance of the weights, leading to a disproportionate influence on the MSE loss calculation. As it shown in Fig. 2, the calibration dataset is represented as Dc, the weights of LLM as W, the loss function of LLM as $L = (W; D_c)$. The gradients g to the LLM is:\n$g = \\nabla_w \\pounds = \\nabla_w L(W; D_c).$\nIn order to locate the sensitivities as outliers, we calculate loss $L(W_Q)$ after model quantization based on Eq. (3).\n$L(W_Q) \\sim L(W) - g(W - W_Q).$\nIn order to search for quantitatively optimal outliers, we define the quantization process as an opti-"}, {"title": "3.2 Gradient Aware Quantization", "content": "To quantize the LLM weight W, we first compute the scale s and the zero point z, which are calculated as depicted in Eq. (6) and (7), where B represents the group size (set to 16 in this case) and b is the bit-width for quantization. The detail is shown in Alg. 2.\n$s = \\frac{max(W) \u2013 min(W)}{2^{b-1}}.$\n$z = \\frac{- min W}{s}.$\nWe select the weight by absolute values of the weight gradients $|W_{grad}|$, the largest 1% of them are filtered as outliers, and the rest are used as quantization weights. This process can be expressed as:\n$O = I (|W_{grad}| \\geq quantile(\\alpha_1)),$\nwhere $\\alpha_1$ is the specific quantile value of the magnitude of weight gradients, which are used to identify outliers matrix O.\nThe quantization and dequantization operations are carried out per channel within each group to assess the quantization error. The formulas for quantization and dequantization are provided below (Eq. (9) and (10)), and any identified outliers are excluded from the error computation (Eq. (8)):\n$q = round(\\frac{W}{s} + z).$\nThe quantized weight $Q(W)$ can be denoted as:\n$Q(W) = s \\times (q - z).$"}, {"title": "4 Experiments", "content": "Overview. First, we validate the superiority of our method over other quantization approaches on the Llama-2 family (Touvron et al., 2023b), providing detailed compression metrics for comparison. Additionally, we present the results of our method compressed to 2 bits versus 3 bits, confirming its reliability at lower bit precision. Following this, we demonstrate the quantization performance of our method on other models, showcasing its generalizability. Finally, we present zero-shot quantization results on multimodal models to verify the multimodal context generalizability of our approach."}, {"title": "4.1 Experiments Setting", "content": "Setups. The language-only models we selected include Llama2-7B and 13B (Touvron et al., 2023b), Falcon-7B (Penedo et al., 2023), and Mistral-7B-v0.1 (Jiang et al., 2023). These models were quantized using the first sample of the RedPajama dataset. To evaluate Perplexity (PPL) and Accuracy (Acc.), we utilize the WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2020) validation sets. All experiments were conducted on an NVIDIA A100 (80G) GPU.\nBaselines. Our primary baselines include AWQ (Lin et al., 2024) and GPTQ (Frantar et al., 2022a), while we benchmark our approach against the state-of-the-arts SPQR (Dettmers et al., 2023). To ensure that the average bit-width of the model remains below 4 bits-surpassing GPTQ and AWQ in compression-we quantize the non-outlier weights at 4-bit and 3-bit precision. These configurations are denoted GWQ-O for 4-bit quantization, GWQ-R for 3-bit quantization."}, {"title": "4.2 Main Result", "content": "We conduct separate evaluations for both pure large language models and large multimodal models. For the large language models, we assess the accuracy of the quantized model in terms of perplexity, zero-shot tasks, and quantized inference memory overhead and latency. In addition to evaluating performance on linguistic tasks for the large multimodal models, we also test the zero-shot target detection task across multiple datasets.\nLanguage Model. As shown in Tab. 2 and Tab. 3, we first evaluate the perplexity of the quantized model on the WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2020) datasets on the Llama2-7B and 13B models. Additionally, we assess the"}, {"title": "4.3 Ablation Study", "content": "In this section, we examine the impact of varying the number of calibration samples on the performance of our method. Tab. 8 presents the results for the calibration sample sizes 1, 4, 8 and 12, evaluated on the WikiText2 and C4 datasets. Tab. 8 and Fig. 4 indicate that increasing the number of calibration samples has minimal impact on the PPL of the quantized model. This suggests that the size of the calibration set has little influence on the quantization performance of GWQ. Compared to most of the current quantization methods, which require large datasets, GWQ has a lower calibration cost and can achieve calibration with fewer samples.\nTo further investigate the impact of different cal-"}, {"title": "5 Conclusion", "content": "This paper introduces GWQ, a weight-only post-training mixed-precision quantization approach based on first-order gradients. GWQ preserves the weights corresponding to the top 1% of outliers at FP16 precision, while storing the remaining non-outlier weights in a lower-bit format. GWQ found that it is easier to obtain more accurate outliers using the first-order gradient to locate the sensitive weights of the pre-trained model than the current search for model outliers via the Hessian matrix. GWQ achieves the lowest quantization loss compared to current state-of-the-arts methods by utilizing only a single calibration sample. Considerable results are also obtained by GWQ on multimodal models, which can achieve better results on most of the zero-shot target detection tasks compared to the current stat-of-the-art methods. GWQ achieves 1.2x inference acceleration compared to the original model internship and uses less memory in the inference process."}, {"title": "6 Limitation", "content": "GWQ gets gradients via back propagation and ranks them accordingly. However, in models such as OPT, the activation function is ReLU, and the correct gradients cannot be computed because of gradient vanishing. Additionally, backpropagation demands substantial memory resources, a single A100 80G GPU is insufficient to handle the large computational requirements. Furthermore, since GWQ employs mixed-precision quantization, it is less hardware-friendly compared to methods like AWQ, resulting in higher inference latency after quantization. We will continue to refine and optimize the GWQ methodology."}]}