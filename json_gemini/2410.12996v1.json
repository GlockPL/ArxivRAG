{"title": "SSET: Swapping-Sliding Explanation for Time Series Classifiers in Affect Detection", "authors": ["Nazanin Fouladgar", "Marjan Alirezaie", "Kary Fr\u00e4mling"], "abstract": "Local explanation of machine learning (ML) models has recently received significant attention due to its ability to reduce ambiguities about why the models make specific decisions. Extensive efforts have been invested to address explainability for different data types, particularly images. However, the work on multivariate time series data is limited. A possible reason is that the conflation of time and other variables in time series data can cause the generated explanations to be incomprehensible to humans. In addition, some efforts on time series fall short of providing accurate explanations as they either ignore a context in the time domain or impose differentiability requirements on the ML models. Such restrictions impede their ability to provide valid explanations in real-world applications and non-differentiable ML settings. In this paper, we propose a swapping-sliding decision explanation for multivariate time series classifiers, called SSET. The proposal consists of swapping and sliding stages, by which salient sub-sequences causing significant drops in the prediction score are presented as explanations. In the former stage, the important variables are detected by swapping the series of interest with close train data from target classes. In the latter stage, the salient observations of these variables are explored by sliding a window over each time step. Additionally, the model measures the importance of different variables over time in a novel way characterized by multiple factors. We leverage SSET on affect detection domain where evaluations are performed on two real-world physiological time series datasets, WESAD and MAHNOB-HCI, and a deep convolutional classifier, CN-Waterfall. This classifier has shown superior performance to prior models to detect human affective states. Comparing SSET with several", "sections": [{"title": "1 Introduction", "content": "Due to the ubiquitous use of machine learning (ML) models, explainable artificial intelligence (XAI) has become a hot topic in various practical domains [1, 2]. In essence, some ML models, known as \"black-box\", have a complex nature which result in reducing transparency, user trust, and debugging capacity. To overcome these chal-lenges, practitioners have paved the way through local (decision) explanations by which feature attributions are provided at a particular instance [3-5]. As an example, let us consider multiple sensory data containing electrocardiogram (ECG), tempera-ture (Temp), and respiration (RESP) measurements collected from an individual in the context of affect recognition. A local explanation method may provide the ECG and RESP measurements contributing to the ML decision of individual's surprised state [6]. This example captures explanability for multivariate time series data type. Although our world is surrounded by such data, the literature contains numerous XA\u0399 models that focus on images [7] and tabular data [8]. One possible reason for this deficiency is that the evaluation of generated explanations on time series is not as straightforward as the former data for humans. For example, an XAI model makes sense to humans when it gives yellow petals as the prediction explanation of a sun-flower [7] or when it gives an age of under 25 as the explanation for rejecting a loan application [8]. However, due to the inherent conflation of time and variables, it is difficult for humans to process the significance of each time step in the case of time series data. In the literature, two approaches, the intrinsic [9-11] and post-hoc [6, 12] mech-anisms, are usually introduced as local explanations on multivariate time series data. While the former creates built-in modules in the ML design process, focusing on rele-vant features in terms of attention weights, the latter applies surrogate techniques on top of ML models, assigning attribution scores to the latter features. The study in [13] shows that providing reliable explanations in terms of attention weights depends on the complexity of datasets, requiring some measures to assess complexity on the lower level of explainability. On the other hand, existing post-hoc approaches fall short of intervening sequences of time steps (context) in the design process [14]. Instead, a step-wise explanation is followed in the time domain. Taking the context into account is crucial in time series applications, specifically affect detection, in which no single time step plays an influential role in the detection of human affective states. Another drawback of post-hoc explanations refers to the inherent differentiability constraint on ML models [12]. Such requirement restricts the applicability of explanations to non-differentiable settings."}, {"title": "2 Related Works", "content": "To address these challenges, we propose a post-hoc swapping-sliding decision explanation for multivariate time series classifiers, called SSET. The model includes swapping and sliding stages to detect salient variables and time intervals, respectively. We assign importance scores to the observations in a novel fashion to present expla-nations. Specifically, in the first stage, we sample train data from target classes in the neighborhood of each instance to be explained. The salient variables are extracted by swapping the instance variables with their counterparts in the train neighbors. In the second stage, we slide a window of selected train data over each time step of the salient variables to thoroughly examine the important sub-sequences. The degree of significance is eventually determined by the drop in the black-box prediction score, the window size, and the role of each observation in the sub-sequence construction. Focusing on affect detection, specifically the CN-Waterfall classifier presented in [15], we show that SSET outperforms three baselines, including Dynamask [12], integrated gradients (IG) [16], and LIME [17], on two real-world datasets, WESAD [18] and MAHNOB-HCI [19], in terms of precision and informativeness. We organize the rest of the paper as follows. In Section 2, we review related works on XAI models tailored to time series data. We present our novel XAI model, SSET, in Section 3, followed by experiments conducted to demonstrate its performance in Section 4. Finally, we conclude our study and discuss future research directions in Section 5. In this section, we review the literature concerning post-hoc local explanation mod-els tailored to multivariate time series classifiers. As mentioned in Section 1, in the post-hoc explanations, external models are designed out of the trained ML models to justify the decisions. Studies show the employment of gradient-based approaches in this explanation category [20-22]. In [20], two GRAD-CAMs [23] were proposed to measure the output gradients of 2D and 1D convolution neural networks (CNNs) with respect to the networks' feature maps. The goal was to provide spatiotemporal explanations for prediction of the average energy production of photovoltaic power plants and rare server outages. A framework called TSXplain [21] leveraged salient regions of anomalous sequences in an anomaly detection application, using the gra-dients of output with respect to input components. Similarly, TSInsight [22] employs the influence of inputs on the activation of the last layer in a fine-tuned classifier. The model aimed at estimating features importance applicable in the optimization process of the classifier. One of the main drawbacks of the gradient-based approaches is the requirement for the black-box to be differentiable, which limits their applicability in non-differentiable settings. This limitation also applies to the work in [12], where the proposed model, Dynamask, supports mask perturbations. More specifically, Dyna-mask produces importance scores for each feature and time step by perturbing a mask on the inputs in a gradient-based optimization procedure. Dynamask's strength is that it captures a context for each observation, which was not explicitly addressed in the remaining works discussed here."}, {"title": "3 Method", "content": "In this section, we begin with a formulation and description of the proposed local explanation for multivariate time series classifiers, namely SSET."}, {"title": "3.1 Preliminaries", "content": "Let $Xtr = {x' \u2208 RT\u00d7V}$ and $Xte = {x \u2208 RT\u00d7V}$ be sets of train and test data, respectively, where R indicates real numbers in the range [0, 1]. Moreover, T and V are the number of time steps and variables (signals), respectively. We assume the trained black-box f produces probability scores y = f(x) for each test instance x over a set of classes $Cl = {1, 2, ..., C}$, where C denotes the total number of classes. This set includes a winner class c. Throughout the remainder of the paper, the following notation will be used: \u2022 Cl: a set of all classes excluding c (or target classes) \u2022 s \u2208 S = {1, . . ., V}: a signal from a set of signals S \u2022 t \u2208 {1, ..., T}: a time step \u2022xi: the test instance of interest \u2022 y: the prediction score of xi over the class c \u2022 x1:T,s: values of x in the signal s \u2022 xt-jt+k,s: values of a sub-sequence between [t - j,t + k] time steps in the signal s \u2022 Xi of the instance xi, where j, k \u2208 {0, . . ., T \u2013 1}"}, {"title": "3.2 Proposed XAI Model: SSET", "content": "We present our proposed local explanation model, SSET, with a focus on the affect detection domain. In this field, the data are usually collected from physiological sen-sors [18, 19], which are used to record a cascade of physiological processes occurring when humans are subject to different stimuli [27]. Taylor [28] argues that these pro-cesses have a duration of less than 90 seconds. The argument could imply that for series sampled at more than 1 Hz, manipulating only one time step may have no effect on detecting a human's mental state. In other words, to drive the decision explanation of the detector, we need to consider the manipulation of several steps and then track the outcome variations. In this paper, we define a context or neighboring steps for each observation to scrutinize the variations and detect salient sub-sequences. However, to alleviate the cognitive burden on end users when multiple sensory data are employed, we focus on salient sub-sequences of important physiological sensors. To specify the degree of salience, we further present a novel formulation considering the impact of the context size, among other criteria. The procedure yields to meaningful explanations for the detection of human affective states. Algorithm 1 shows the SSET process. In the following, we describe the three main components of the model (Fig. 1), including swapping, sliding, and calculating importance scores in detail."}, {"title": "3.2.1 Swapping", "content": "In this stage, we wish to distinguish the salient physiological signals (Vimp) contribut-ing to the detection of a human's mental state. Focusing on these signals helps the next stage to produce informative explanations. As mentioned in Algorithm 1 and shown in Fig. 2, first, a set of random instances is selected from the test data. For each instance $xi \u2208 Xte$ (the red star in Fig. 2), the black-box model predicts y. The train data with target classes in $Cl$ (the colored circles in Fig. 2) are then extracted from the dataset, denoted as $Xtc C Xtr$. We replace each signal in xi with its coun-terparts in $Xtc$, while keeping other signals unchanged. In other words, the values of xs are manipulated by those in $Xtc$. Using the train data in this process guarantees that the generated instances fall in an in-distribution space. However, as there can be large numbers of $Xte$ instances in large datasets, providing the explanation will be computationally expensive. Therefore, we randomly sample a user-defined number of neighbors Xneighbors C Xte by which the replacement of signal values is accomplished. Fig. 3 illustrates how the neighborhood scope is defined, and we will also explain it in more detail. 1:T,s We denote the swapped instances corresponding to the signals by $Xswps$. Evaluating the instances and using in the next stage, we select those which cause maximum drop in the black-box performance below a certain threshold, thre. The goal is to"}, {"title": null, "content": "$Simp = {s \u2208 S| max{f(Xswps) \u2264 thrc}}$\n(1)"}, {"title": null, "content": "In the above procedure, one can infer that the train neighbors play a vital role in identifying the salient signals. Here, we explain how the neighborhood scope is defined and what strategies SSET takes to extract the neighbors. We first consider the region within a distance l of xi (the white circle in Fig. 3), where a number of train instances is sampled randomly from the target classes. In case the instances are distributed outside the region or no promising observations are detected in the region, the sampling is repeated until a certain number of attempts (thra) is met. By promising observations, we mean the neighbors which cause the prediction drops below the threshold thre. This strategy improves the chance of finding promising neighbors. We further update the neighborhood region and reset the attempts to extend the explorations, provided that the sampling failures. The region is updated by shifting the exploration space to \u03b4. Formally, we are interested on any promising neighbor In \u2208 Xneighbors whose Euclidean distance to the instance of interest falls in this region:"}, {"title": null, "content": "start + d \u2264 ||xi - Xn || \u2264 start + d + l\n(2)"}, {"title": null, "content": "where:"}, {"title": null, "content": "start + d + l < thrn"}, {"title": "3.2.2 Sliding", "content": "In the Equation above, we restrict the neighborhood scope and the updates by a threshold thrn. Given the swapping stage and mentioned strategies, according to our observations (see Section 4 for details), only one signal contributes to the black-box decision in most test data. For the data without any individual salient signals, a dual-signals swapping mechanism (shown as DualSignals in Fig. 2 and Algorithm 1) is performed. In this mechanism, we take inspiration from the CN-Waterfall architecture [15], in which correlated and non-correlated signals are distinguished. We extract the respective sets of signals, and perform a similar swapping procedure as before, yet on each set and pair of signals. The goal of this stage is to provide important sub-sequences of the signals selected by the swapping stage and present them as the contributors to the black-box decision. To achieve the goal, we consider a context (ctx) with an adaptive size for each observation of the salient signals (Fig. 4). The context stands for the neighbors of each time step constructing sub-sequences in the signals (see Section 3.1). Sliding a window w of the same size as the sub-sequences, the corresponding values in xi are replaced by their counterparts in Xswps, while the remaining observations are unchanged. This way, T manipulated instances (Xsld), are generated in each salient signal and for a specific window size. To find the salient sub-sequences, we evaluate the performance loss of the manipulated instances at the winner class. If the performance drops below thre, the respective context and time step are counted important. We show these sub-sequences as Seqimp and formulate in the following:"}, {"title": null, "content": "$Seqimp = {x-jt+k,s \u2208 Xsld\u2081|f(xsld\u2081) \u2264 thrc}$\n(3)"}, {"title": null, "content": "where:\nXsld\u2081 \u2208 Xsld, S \u2208 Simp\nHere, Xsldt refers to the respective manipulated instance at time step t.\nWe further enrich the context by increasing the number of neighbors and corre-spondingly increase the window size, in case the former context does not play an"}, {"title": "3.2.3 Importance Score", "content": "influential role in the output. The procedure continues until at least one sub-sequence satisfies the dropping constraint and is determined to be salient. In this stage, we quantify the importance score of salient sub-sequences and present the explanation of the instance to be explained. In Equation. 4, we formulate the scores in a novel fashion, incorporating multiple factors: the drop in the black-box output, the window size, and the role of each time step i.e., either \"current\" or \"neighbor\". As mentioned before, the salient sub-sequences are taken to be the ones by which the black-box output drops. The larger the drop is, the higher the importance of the sub-sequence will be. However, relying solely on the amount of performance loss may not accurately reflect the importance of sub-sequences with different sizes, yet the same amount of loss. To address this limitation, we take the impact of the window size into account in each time step. Recalling the informativeness of the explanation from the sliding stage and to reduce the cognitive burden on users, smaller salient sub-sequences are expected to be more informative than larger ones. Therefore, we incorporate an exponential effect of window size w, with respect to the total num-ber of time steps. To avoid producing scores greater than 1, we further regulate the contribution of the window size by a coefficient A and impose a minimum operator on the incorporated factors. As any salient sub-sequence is the result of sliding w over each time step, we distinguish the role of steps. The time step t' over which the win-dow is constructed takes the role of \"current\", while other steps in the window are considered as \"neighbor\". It is assumed that the current step is the source of output drop, thereby a higher importance score is assigned to this step than its neighbors. We resort to an a proportion of the score in the current step, as the importance of its neighbors. In the following, we address the process formally:"}, {"title": null, "content": "It\nteSeqimp = $\\begin{cases}\nmin (y-y+xx exp(-\\frac{w}{\\lambda T}),1) & \\text{if t = t'}\n\\text{a x }I_{t! & \\text{Otherwise}\n\\end{cases}$\n(4)"}, {"title": null, "content": "where It is the important score at time t of the salient sub-sequence. The prediction score of the manipulated instance at the winner class c is also denoted as ym. Although the Equation (4) can capture the importance score of each time step in the salient sub-sequence, it doesn't hold under overlapping salient sub-sequences. In this case, the observations could take both the \"current\" and \"neighbor\" roles resulting in multiple importance scores. To tackle the problem, we accept the highest score in each time step. Note that the steps and signals other than those in Seqimp, respectively, are assigned zero importance. We formulate the final scores in a matrix, namely IMP:"}, {"title": null, "content": "IMP = $\\begin{cases}\n\\text{max }\\left(I_{t}\\right) & t\\epsilon\\text{ Seqimp}\n0 & \\text{Otherwise}\n\\end{cases}$\n(5)"}, {"title": null, "content": "It is worth mentioning that a similar regime of measuring the importance scores is imposed for pairs of salient sub-sequences when the dual-signal functionality is activated."}, {"title": "4 Experimental Results", "content": "In this section, we evaluate the quality of SSET and compare it with benchmarks."}, {"title": "4.1 Datasets", "content": "We employed a public and an academically available dataset, WESAD and MAHNOB-HCI, respectively. The datasets were preprocessed as discussed in [15]. In short, the signals of eight sensors were extracted from the WESAD dataset, including three-axis accelerometer (ACC), ACC1, ACC2), electrodermal activity (EDA), electromyogram (EMG), RESP, ECG, and TEMP data. We selected four affective states\u2014neutral, amusement, stress, and meditation from the data of all participants, resulting in 433 350 samples. The data of the participants were then unified in terms of length, downsampled to 10 Hz, normalized to the range [0,1], and segmented into series of 3-second windows (30 time steps) with 1-second overlaps (10 time steps). A similar procedure was applied to the MAHNOB-HCI dataset but on the signal data from seven"}, {"title": "4.2 Black-box", "content": "We used CN-Waterfall, a deep convolutional affect recognizer, which has been shown to be superior to other traditional and deep learning models [15]. Briefly, this black-box consists of Base and General components, providing different levels of data repre-sentation. It is worth mentioning that in the General component, correlated and non-correlated data representations are fused independently to extract signals inter-relation information. Moreover, CN-Waterfall was trained on data randomly split into train and test sets in an 80: 20 ratio."}, {"title": "4.3 Benchmarks", "content": "To evaluate the explanations produced by SSET, we included local explainers from different categories: IG and LIME as gradient- and perturbation-based approaches, respectively, which have been widely applied to non-time-series data types, and Dyna-mask as a perturbation-based approach that has mainly been applied to time series datasets."}, {"title": "4.4 Settings", "content": "In SSET, different user-defined hyperparameters has been introduced. We select 200 test and 10 train data in the neighborhood of the instance to be explained from each dataset. The neighborhood scope is initialized as the space between 0 and l = 1 and shifted by d = 0.1 when needed. Therefore, we set start = -1 to meet the space constraint. We also adjust the maximum neighborhood after shifting the scope to thrn = 8, empirically. In addition, finding the neighboring samples is attempted for a maximum of thra = 10 times. An intuitive choice of threshold, thrc = 0.5, is considered as the measure of prediction drop. However, any desired value could be initialized. We consider the context ctx = 1 to produce as informative explanation as possible. Moreover, \u03bb = 0.1 is initialized empirically. To avoid a significant suppression in the neighbors contribution, we select a = 0.9. In IG, the baseline was taken to be the average of the train data. The number of steps along a path from the baseline was 10, at which the gradients were calcu-lated. Regarding LIME, we limited the number of generated samples to 50, by which LIME approximates a linear model. In Dynamask, the same configurations as those in the original paper [12] were examined using the deletion variant and fade-to-moving average perturbation \u03c0m."}, {"title": "4.5 Explanations Evaluation", "content": "In this subsection, we investigate how SSET generates explanations compared to the benchmarks on the WESAD and MAHNOB-HCI datasets."}, {"title": "4.6 Salient Signals Evaluation", "content": "In this subsection, we look at how the importance is distributed across the signals in the 200 test data. The goal is to understand which sensory data are mostly of the focus of CN-Waterfall to make decisions. The results can shed light on the solution to the problem of resource-limited laboratories so that the most efficient sensors can be employed on experiments."}, {"title": "4.7 Window Size Evaluation", "content": "Here, we evaluate in what context the decided affective states are suppressed. In other words, the distribution of window sizes is measured over the 200 test data to give us insight into the granularity of data sampling in terms of informativeness. Fig. 9(a) shows that on WESAD, the salient sub-sequences with a size of 11 (more than 1 second of data) reduce the prediction score at class c in the majority of cases. On MAHNOB-HCI (Fig. 9(b)), the states of amusement, happiness, and surprised could be altered within less than 1 second. More precisely, the dynamic of most instances can be changed by sub-sequences of size 8. We also confirm that there are highly informative signals in 5% of the selected test data on both datasets with the size of"}, {"title": "4.8 Window Size and Neighboring Distance Relation", "content": "We investigate whether there is any relation between the distance of the sampled neighbors from the instance to be explained and the detected window size. To this end, we examine the scatter plot between these two factors and infer their correlation."}, {"title": "4.9 Explanation Quality of Explainers", "content": "To gain an overall view, the quality of the explanations generated by the state-of-the-art XAI models and SSET are compared on both datasets. We explore the explanation quality measured in terms of precision, informativeness, similarity. We measure the precision as the amount of suppression caused by the manipulated instances at the class of interest [12]. A larger drop results in a higher precision of the explainer. We also postulate informativeness as the smallest sub-sequences that maximally drop the prediction score, similar to Fong's suggestion on images [30]. As pointed out by Miller [31], the goal is to reduce the cognitive load of generated"}, {"title": null, "content": "explanations for end users. Regarding similarity [25], close manipulated data to the instance to be explained are of interest to examine the black-box variations locally. Here, the Euclidean distance between the manipulated data and the instance of interest is calculated. Note that for the quality metrics, we take the average across the 200 instances. As Tables 1 and 2 show, SSET outperforms the benchmarks in precision and infor-mativeness on both datasets, at the expense of lower similarity. In essence, SSET provides the explanation with the precision of 0.1 and 0.07 on WESAD and MAHNOB-HCI, respectively, thanks to the neighborhood strategy and SSET components. It is worth mentioning that the model could perform even more precisely if the threshold thre was set to a value lower than 0.5. In contrast, LIME produces \"blind\" pertur-bations around the instance of interest, which result in insignificant prediction drops (0.03 on WESAD and 0.02 on MAHNOB-HCI). In IG, the inherent dependency of explanations on the baseline and gradient-based characteristic impede crucial suppres-sions on the CN-Waterfall decision (0.04 and 0.03 on WESAD and MAHNOB-HCI, respectively). Dynamask also strongly depends on the mask initialization to eventually build a perturbed instance that could reduce the black-box performance. However, a naive choice of mask ends up with unpromising perturbations, rejecting the perfor-mance drop. In our work, we employ mask values equal to 0.5 as suggested by the original paper [12], resulting in no drop in the black-box output and zero precision. On average, SSET discerns 9 time steps as contributing to the detection of the cur-rent state in CN-Waterfall, while LIME approximates a linear model with respect to all time steps (30) of the instance variable. Similarly, IG and Dynamask take the gradi-ents over 30 steps of each variable. One could infer that more informative explanations are presented by SSET, even if the benchmarks generate valid explanations. Regarding the similarity measurement, SSET performs better than LIME by 2.02 and 31.8 on the first and second datasets, respectively. In LIME, undue distances between the perturbed data and the instance of interest make this model ineffective (15.49 on WESAD and 16.51 on MAHNOB-HCI). Although the corresponding dis-tances are lower in IG (1.46 on WESAD and 2.23 on MAHNOB-HCI) and Dynamask (0.39 on WESAD and 0.57 on MAHNOB-HCI), the latter models cannot provide valid explanations."}, {"title": "5 Conclusion and Future Works", "content": "In this paper we introduced SSET, a post-hoc local XAI model applicable to time series in the affect detection domain. With SSET, we aimed to provide contextual explanations for the outcomes of non-differentiable detectors. To this end, two stages of swapping and sliding explored salient signals and intervals, respectively. Moreover, the degree of salience was measured in a novel fashion by incorporating multiple factors. We performed comprehensive experiments on CN-Waterfall, a highly accurate deep convolutional affect detector, and two datasets, WESAD and MAHNOB-HCI. Comparing SSET with LIME, IG, and Dynamask, we demonstrated the superiority of our proposed model over the benchmarks in terms of precision and informativeness. Furthermore, it was shown that the correlated signals contribute significantly to the decisions of CN-Waterfall. These results improve the transparency of ML models and assist experts in choosing the most appropriate sensors in resource-limited laboratory settings. SSET was the only model that produced promising explanations. However, this model requires more computational time than the others, especially for the WESAD dataset. It took about 3 hours to run for this dataset, while LIME and IG took about 1 minute and Dynamask took about 1 hour. With respect to the MAHNOB-HCI dataset, our proposed model and Dynamask ran for about 1 hour to produce explanations, whereas LIME and IG ran for about 1 minute. In the future, it should be possible to apply heuristics and efficient search algo-rithms targeting the neighborhood scope and context size to reduce the computation time. Another avenue would be to investigate SSET's quality on other time series and applications. It would be useful to examine different values of the dropping thresh-old to gain a better assessment of the precision. Finally, SSET could be studied in privacy-aware settings where time series should not be disclosed. Such study allows practitioners to understand deeper whether/to what extent SSET meet data protection measures. As scholars claim that explanability distorts privacy [32], it is recommended to examine the proposed model under such settings before employing in the wild."}, {"title": "Declarations", "content": "\u2022 Fund: This paper is funded by Ume\u00e5 University \u2022 Conflict of interest: The authors declare that they have no conflict of interest. \u2022 Competing Interests: No competing interests."}]}