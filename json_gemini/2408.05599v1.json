{"title": "Sequential Representation Learning via Static-Dynamic Conditional Disentanglement", "authors": ["Mathieu Cyrille Simon", "Pascal Frossard", "Christophe De Vleeschouwer"], "abstract": "This paper explores self-supervised disentangled representation learning within sequential data, focusing on separating time-independent and time-varying factors in videos. We propose a new model that breaks the usual independence assumption between those factors by explicitly accounting for the causal relationship between the static/dynamic variables and that improves the model expressivity through additional Normalizing Flows. A formal definition of the factors is proposed. This formalism leads to the derivation of sufficient conditions for the ground truth factors to be identifiable, and to the introduction of a novel theoretically grounded disentanglement constraint that can be directly and efficiently incorporated into our new framework. The experiments show that the proposed approach outperforms previous complex state-of-the-art techniques in scenarios where the dynamics of a scene are influenced by its content.", "sections": [{"title": "1 Introduction", "content": "Disentangled Representation Learning (DRL) focuses on embedding high dimensional complex data into a low dimensional space which factorizes the hidden underlying factors of variations. This disentanglement is expected to facilitate numerous downstream generation or classification tasks and enhance model explainability [5, 14, 31, 34, 51]. The present work in particular is concerned with unsupervised sequential data DRL [44, 46, 48] which, compared to disentanglement on static data [7,20,27,33, 39, 51, 59], exhibits specific temporal structure that could be leveraged in the learning process. More precisely, the objective is to learn a representation of video data that factorizes the time independent factors (i.e., what is static; the identities, the backgrounds) and time varying factors (i.e., what is dynamic; the poses, the motions) in two separate feature vectors.\nAs the above description might leave room to some ambiguity into what constitutes a static/dynamic variable, we introduce in this work an explicit"}, {"title": "2 Background", "content": "Let G = {x:T}=1 denote a dataset of N video sequences of length T; where each x corresponds to a video frame at time step t in sequence j. The main hypothesis underlying this work is that the frames xt can be uniquely represented into two parts, s and dt with:\n\u2022s: the static factors, common and shared by all frames, representing the time invariant information;\n\u2022 dt: the dynamic factors, changing between frames, corresponding to the time varying information.\nDue to temporal coherence, consecutive frames have common factors that are shared between them. Therefore, inspired by Pattern Transformation Manifolds (PTM) representation [50], we consider that each frame resides on a common manifold that corresponds to that shared content. The code s represents the manifold while d1:7 captures the varying parts specific to each frame. For example, for a video of a person walking, all frames share the same background and person identity thus s represents those static factors, while d\u2081:T encodes the person specific pose at each frame i.e., the motion."}, {"title": "2.1 Disentangled Sequential VAE", "content": "With the above representation, the work in [29] proposed the following probabilistic model where each frame is generated from its corresponding static and dynamic codes and the dynamic codes are obtained given previous time steps:\np(x_{1:T}, s, d_{1:T}) = p(s) \\prod_{t=1}^{T}p(d_t|d_{<t})p(x_t|s, d_t).\n(1)\nThe objective is to extract the codes s and d1:7 given the observed data i.e., to learn the factorized posterior distribution p(s, d1:T|X1:T). Variational inference can be used to learn an approximation of that posterior:\nq(s, d_{1:T}|X_{1:T}) = q(s|x_{1:T}) \\prod_{t=1}^{T}q(d_t|s, d_{<t}, x_{<t})\n(2)\nwith the two terms obtained through separate sequential networks [29]. The model is trained via the VAE algorithm. This encoder network, introduced in [29], is named 'full q' and is the building block of all following sequential disentanglement works. Unfortunately, despite the disentanglement bias imposed by the factorized latent representation, this model might not be sufficient to achieve a well-disentangled representation. Indeed, as formalized by [32], unsupervised"}, {"title": "2.2 Other Related Works", "content": "Differently than the VAE-based methods derived from the DSVAE model [3,4,18, 19,29,35,41,45,60], Berman et al. [6] recently developed structured Koopman autoencoders that try to achieve multifactor disentanglement of the sequential data with more than two disentangled components. However, their results still lie behind recent DSVAE-based models [41]. In parallel, the work FHVAE [21,22] also proposed unsupervised disentanglement through an LSTM-based model but is limited to audio data. Then, within the works that try to tackle disentanglement learning for static data [10,20,26], we can mention the field of style/content disentanglement [7,15,49,57], with a definition of the static variables related to ours (Sec.3). Interestingly, the effectiveness of those methods can thus be explained"}, {"title": "3 Problem Formulation", "content": "Before trying to disentangle the static and dynamic factors, we propose a new definition of those concepts. This is done through formalizing the video generative process as a latent variable model that comprises two disjoint codes representing the time varying and time invariant factors. We further provide the assumptions underlying this partition.\nDisentangled video generative model: We propose a model where the generative process of each frame consists in first drawing a latent code zt from its associated probability density p(zt) and then obtaining the observations Xt by sampling from p(xt|zt). More formally, the frame generative process is:\nZ_t \\sim P(z_t),\n x_t = g(z_t)\n(3)"}, {"title": "4 Method", "content": "We seek to learn from the video sequences X1:T, two latent codes, a static code f and dynamic codes A1:7 such that these codes identify the ground-truth factors as presented in Def.1. We start by deriving from our formalism sufficient conditions to provably extract the factors. We then propose a practical model and show how, strictly by leveraging the data structure, this method does achieve the disentanglement conditions. The complete model is summarized in Fig.3."}, {"title": "4.1 Sufficient Conditions for Disentanglement", "content": "Static disentanglement : The objective is to learn an encoder posterior distribution q(f|x1:7) mapping the sequences to their static code such that the static codes f sampled from q(f|x1:T) solely capture the static factors s.\nProposition 1. Consider the generative process and conditions presented in Sec.3. Further assume that dim(f) = nf \u2265 ns. Let x and x* denote two frames (or disjoint sets of frames) belonging to the same sequence x1:T and D be a divergence between two distributions. Given unlimited data from p(x1:T), any candidate posterior distribution q(f|x1:T) that minimizes\nL_{prop1} = E_{p(x_{1:T})}D(q(f|x), q(f|x^*)) \u2013 I(f, s) + I(s, s),\n(7)\nis disentangled in the sense that q(f) = \u222b q(f|x1:T)p(x1:T)dx1:T, the aggregated posterior, is a reparametrization of p(s).\nThe proof is provided in App.A.2. Intuitively, by imposing f to be both invariant to the dynamic factors and informative of the static ones, the code f will capture all and only s. Prop.1 extends Thm.4.4 of [49] to distributions and cope with an unknown number of static variables ns. Prop.1 shows that simply maximizing I(x1:T, f) as proposed in Sec.2 is not sufficient to extract s. Indeed, as shown in [16,56], a global aggregated code f could encode the dynamic information and should therefore additionally be made invariant.\nDynamic disentanglement: As explained in Sec.3, because the static and dynamic factors might not be independent, the goal for the dynamic codes A1:T is to capture ed,1:T. Unfortunately, following an approach similar to Prop.1 is not possible in this case. Indeed, this approach would require to have access to two samples x and x* that share the same motion but have different static factors. However, by the definition of the dynamic factors (Sec.3) there are no easily"}, {"title": "4.2 Practical Implementation", "content": "Compared to previous methods that solely rely on VAEs, we propose instead to improve sequence modeling based on additional Normalizing Flows (NF) [43]. A NF is a transformation of a random variable through a sequence of differentiable invertible mappings. As a result, it is a natural candidate to learn the invertible function h. More specifically, assuming the common static code f has been extracted from the frames, we model the frames likelihood p(x1:7|f) using a CNF [53] as\np(x_{1:\u03c4}|f) = p(\u03bb_{1:7}|f) det\\frac{\u03b4h^{-1} (x_{1:7}, f)}{\u03b4\u03a7_{1:\u03a4}}|\n(9)\nwith A1:T = h\u00af\u00b9(x1:T, f) the dynamic codes. This term can be evaluated through exact likelihood evaluation. Because a NF is a bijective transformation and f is common to all frames, the transformed latents A1:T will retain and encode the dynamics of the sequence. Since the NF is conditioned on the static code f, the dynamics are encoded conditionally to the static part. The NF is based on affine coupling [13] and LSTM layers that model the temporal behavior of the sequence, similarly to [37], and provide a bias towards encoding the dynamics. Because, for a disentangled model, the transformation h needs to fully model the conditional dependence and causal mechanism, the final distribution p(1:7|f) is replaced with p(1:1). Details on the architecture are provided in App.B.3."}, {"title": "5 Experiments", "content": "In order to validate the effectiveness of the proposed model, it is compared qualitatively and quantitatively to state-of-the-art sequence disentanglement learning methods including MoCoGAN [46], DSVAE [29], R-WAE [19], S3VAE [60], SKD [6], CDSVAE [4] and SPYL [41].\nDatasets: Similarly to the baseline works, the model is evaluated using the standard LPCSprites [42] and MUG [2] databases, which contain videos of animated cartoon characters and facial expressions respectively. As all methods manage to provide high quality results on the original LPCSprites dataset [41],"}, {"title": "6 Conclusion", "content": "In this work, we introduce a novel formal approach toward sequential data disentanglement. Diverging from previous works, it extends the VAE framework with a conditional Normalizing Flow that directly models the relationships between the static and dynamic factors, making it closer to the data generative process. This constitutes a crucial change compared to the baseline methods by lifting their most detrimental assumption. It translates into large improvements over the state-of-the-art models, which fail to disentangle dependent factors even in simple cases. To further enforce disentanglement, a simple shuffle constraint is proposed which leads to a novel ELBO formulation and our model to be provably disentangled. Hence, compared to the baselines, our method is simple, non dataset/domain specific and theoretically justified. It proved its ability to properly disentangle the static/dynamic factors on multiple datasets and to generalize to less restrictive cases with dependent variables. Interestingly, while our method has been only evaluated for videos, the presented approach directly extend to other domains such as audio or biology which could be explored in future works."}]}