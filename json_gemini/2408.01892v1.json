{"title": "Re-ENACT: Reinforcement Learning for Emotional Speech Generation using Actor-Critic Strategy", "authors": ["Ravi Shankar", "Archana Venkataraman"], "abstract": "In this paper, we propose the first method to modify the prosodic features of a given speech signal using actor-critic reinforcement learning strategy. Our approach uses a Bayesian framework to identify contiguous segments of importance that links segments of the given utterances to perception of emotions in humans. We train a neural network to produce the variational posterior of a collection of Bernoulli random variables; our model applies a Markov prior on it to ensure continuity. A sample from this distribution is used for downstream emotion prediction. Further, we train the neural network to predict a soft assignment over emotion categories as the target variable. In the next step, we modify the prosodic features (pitch, intensity, and rhythm) of the masked segment to increase the score of target emotion. We employ an actor-critic reinforcement learning to train the prosody modifier by discretizing the space of modifications. Further, it provides a simple solution to the problem of gradient computation through WSOLA operation for rhythm manipulation. Our experiments demonstrate that this framework changes the perceived emotion of a given speech utterance to the target. Further, we show that our unified technique is on par with state-of-the-art emotion conversion models from supervised and unsupervised domains that require pairwise training.", "sections": [{"title": "1. Introduction", "content": "Human speech carries a plethora of information beyond language and grammar. It conveys details about the speaker, their mood and intent. Reproducing or injecting these para-linguistic characteristics in machine generated speech is currently an active area of research. Emotional speech synthesis has many applications such as providing personalized customer-support, developing voice-assisted therapy for elderly, and designing human-computer interface systems [1, 2]. Modifications in pitch and energy contour can inject emotional cues into the neutral speech or change the overall speaking style [3, 4, 5, 6, 7]. These prosodic features are also used to evaluate the quality of human machine dialog systems [8], and they play a significant role in speaker identification and recognition [2]. A central idea connecting most of these recent published works is learning a transformation or mapping function for the prosodic features from one emotion to another [9] for synthesizing speech. Rhythm or cadence, in particular, plays a crucial role in conveying emotions [10] and in diagnosing human speech pathologies [11]. Despite this, very few works on emotional speech generation/conversion target rhythm modification [12]. This can be attributed to the difficulty in modeling rhythm as an intrinsic parameter of speech like pitch and intensity. Further, sequence-to-sequence algorithms allows learning of the rhythm aspect without explicit modeling, but they require huge amount of good quality parallel utterances which can be expensive.\nAuthors in [12] adopted a dynamic time warping procedure baked directly into a neural network to perform local and global rhythm modification. However, its main limitation is the supervised paradigm of learning that again requires parallel data to train the underlying generative model. In order to solve this problem in an unsupervised manner, we will make a few simplifications in the problem statement and adopt an actor-critic reinforcement learning strategy [13, 14, 15]. We will further expand the scope of our modeling technique to the other critical prosodic features for emotion conversion, namely: pitch and intensity. To be more specific, the rhythm modification for emotion/voice conversion has three sub-problems:\n\u2022 Identifying informative segments of emotion in speech\n\u2022 Predicting a factor of modification for each segment\n\u2022 Modifying the rhythm of these segments using WSOLA\nIn the proposed approach, we will not modify the prosody of every single phoneme/syllable in an utterance. Instead, we modify only a subset of these segments (most important ones) which are identified by a Markov temporal mask for the task of emotion recognition. We adjust the prior on this temporal mask to identify segments spanning a complete syllable or a word. However, they are allowed to be a collection or mix of any of these components including short pause and silences which is a limitation in [12]. After identifying such segments (countable in practice), we will process them sequentially to predict a modification factor for length, pitch and intensity. These factors are uniform for the entire length of the segment. Finally, after manipulating the features by their corresponding predicted factor, we re-synthesize the utterance with a different emotional intent than before. In our knowledge, this is the first fully-unsupervised model for emotion conversion targeting pitch, intensity and rhythm in a single unified framework. Our experiments on a multispeaker corpus will demonstrate the efficacy of the proposed approach in comparison to prior techniques.\nIn the rest of this paper, we will solve each of the three"}, {"title": "2. Method", "content": "sub-problems sequentially. First, we will discuss the mechanism of length modification in speech (owing to its difficulty in modeling), followed by our Markov masking strategy for discovery of important segments in a weakly supervised setting. Finally, we will predict the factor of modification using actor-critic reinforcement learning strategy. Segment discovery will rely on prediction of human perception of emotional saliency using VESUS [16] and CREMA-D [17] corpora."}, {"title": "2.1. Mechanism of Modification", "content": "There are many algorithms which allow speech length modification such as overlap-add (OLA) [18, 19], wave similarity overlap-add (WSOLA) and phase vocoder, to name a few. The underlying principle in these methods is to split the input speech signal into chunks of a fixed length, and use overlap add to expand or shorten the duration of signal by local replication or truncation. In [12], the authors use the same local replication and deletion of frames from the input signal to modify its rhythm. While this approach works in practice, it also introduces discontinuities in the phase component. As a result of this discontinuity, the modified signal has a noticeable choppy effect for the listeners. WSOLA reduces this artifact via a correlation based search strategy to locally find the best segment for reconstruction in the neighborhood at any given frame.\nTo modify duration of a signal using overlap-add, the first step is to specify the type of window function $w(n)$, its width $I$ and overlap factor $\u03b7$. Hanning window of width more than the lowest expected fundamental frequency is a common choice for this operation. Then, the length of output signal $z(n)$, and the overlap factor decides the time-stamps where the window's center should appear in the input signal $y(n)$. Specifically, let $T(n)$ be the time-stretching function, the position of window on output signal can be easily derived via:\n$\\gamma(1) = 1 \\space and \\space \\gamma(k) = \\gamma(k - 1) + \\eta$ (1)\nHere the total number of $\u03b3$ is $[|z|/\u03b7]$ where $|z|$ denotes the length of input signal. Knowing the $\u03b3(k)$, we can figure out the position of windows on input signal $y$ by $\u03c3(k) = \u03c4^{-1}(\u03b3(k))$. It is very important for the time-stretching function $\u03c4$ to be monotonic in nature for invertibility. Finally, the reconstruction by overlap-add algorithm is given by:\n$z(n) = \\frac{\\sum_{k=1}^{len(\u03c3)}w(n - \u03b3(k)) \\cdot y(n - \u03b3(k) + \u03c3(k))}{\\sum_{k=1}^{len(\u03c3)}w(n - \u03b3(k))}$ (2)\nThe choice of Hanning window with an overlap factor of $\u22650.5$ ensures that the denominator in Equation 2 always adds up to a constant (1 when equal to 0.5). Fig. 1 represents the schematic diagram of overlap-add operation at a high level.\nThe main drawback of incorporating WSOLA algorithm in any data-driven rhythm modification algorithm is its non-differentiable nature. Therefore, the loss function for optimization is infeasible due to the lack of a functional form as back-propagation is undefined. However, we can use reinforcement learning strategy where the WSOLA operation can be declared part of the agent's interaction environment to skip backpropagating through it. This is the solution we focus on in our model-free reinforcement learning for predicting modification factors."}, {"title": "2.2. Salience Prediction", "content": "As mentioned before, estimating the salient regions for emotion perception is the second piece of the puzzle. We employ a simple masking strategy (similar to the attention maps) in order to recover a continuous segment of speech responsible for human perception of emotion [20]. Corpora like VESUS [16] consists of utterances in 5 emotion categories, namely: neutral, angry, happy, sad and fearful. In addition to these audio files, each utterance in VESUS has annotations obtained from 10 listeners on Amazon Mechanical Turk (AMT) asking them to identify emotions in the corresponding utterance. The ratings provided by these listeners therefore, form a categorical distribution over the emotion classes. This is important for multiple reasons: (a) it gives us an idea of how strongly a specific emotion is portrayed, (b) the soft assignment can be used to inject confidence in the model for emotion prediction task. Therefore, our task boils down to predicting the perception score for each emotion using the content from the masked portion of input speech.\nTo think simplistically, attention mechanism is a fairly straightforward approach to solve this problem. However, without any additional constraints, it may not discover any contiguous segments of speech (e.g. single frames) which are non-informative for downstream task of emotion conversion. Therefore, our objective is to find continuous segments (syllable/word level). This facilitates easy manipulation of prosodic features, i.e., pitch, intensity and rhythm by WSOLA. We devise a clever masking strategy that allows us to discover such segments. Specifically, we design a neural network with three components: (a) feature extractor module that is made entirely of downsampling convolution layers, (b) a mask generator module that estimates a collection of Bernoulli random variables over the features and (c) a salience predictor module to predict emotional saliency using information contained in the masked region. Fig. 4 shows the neural network architecture in detail."}, {"title": "2.2.1. Masking Variable", "content": "We generate mask via sampling from the variational posterior learned by a combination of recurrent and linear projection layers (Fig. 4). Given a sequence of frames $X_t \\space V_t\u2208 [1,....T]$, the binary mask sequence $[M_1, M_2, M_3....M_T]$ is a collection of T Bernoulli random variable with the following prior:\n$P(M_t | M_{t-1}) = \\begin{cases}Ber(p), & if \\space M_t = M_{t-1}\\\\Ber(1-p), & otherwise\\end{cases}$\n$\\space V t = 2,3,...T$ and $p\u2208 (0, 1)$. Further, we specify $P(M_1) = Ber(0.01)$ so that the masking follows a first-order Markov property, i.e. the future is independent of past given present and has low activation for the first frame of signal. The mask distribution as time t is dependent on the mask at time t-1. It is by-design similar (0 or 1) to the previous time-step, ensuring a continuity in the segments. Fig. 3(b) shows the state transition diagram of the corresponding Markov chain. While this prior constraint on the mask helps identify continuous segments, it can happen that the mask takes the value 1 for the entire duration of the speech utterance. Hence, we define the mean-field distribution over the mask estimated by the neural network as:\n$\u0434_\u03b8 (M_1, ...., M_T|X) = q_\u03b8 (M_1|X)q_\u03b8 (M_2|X)...\u0434_\u03b8 (M_T |X)$ (3)\nwhere, we have used the mean-field approximation [21, 22] for the variational posterior learned by the neural network (parameterized by \u03b8). We add a sparsity penalty at each time step via"}, {"title": "2.2.2. Neural Network for Salience Prediction", "content": "We parameterize the distribution $q_\u03b8 (M_t |X)$ by Bernoulli parameters estimated using sigmoid activations in the mask generator to get their posteriors (Fig 4). Then, we sample from approximate posterior to generate the mask which gets element-wise multiplied with the extracted features (from convolutional stack) and fed into the salience prediction module. The saliency loss is a simple L-1 penalty over the predicted softmax (5 classes: neutral, angry, happy, sad and fear) and the ground truth. Finally, since we sample from the variational posterior to generate the mask for salience prediction, we use Gumbel softmax [23, 24] for backpropagation through the sampler module. Therefore, the training loss for saliency prediction is given by:\n$L_{saliency} = ||Y - \\hat{Y} || + A_{prior}L_{prior} + A_{sparse}L_{sparse}$ (5)\nwhere, Y and \u0176 are the ground-truth saliency (obtained from AMT) and saliency predicted by model in Fig. 4."}, {"title": "2.3. Factor of Modification: Policy Gradient", "content": "From the last subsection, we have a strategy for obtaining portions of speech that affects our emotion perception. Knowing these allow us to use WSOLA algorithm to modify their length. Here, we will discuss our approach to get a distribution over the factors of modification using actor-critic reinforcement learning strategy. Our first step is to discretize the space of possible duration factors for simplification. We choose a range of 0.25-1.9 in steps of 0.15. Note that, this covers a very wide range for WSOLA operation. Extreme modifications in speaking rate can lead to distortions in the signal. However, we found this interval to be within the operating range. Furthermore, by creating a finite number of classes we can learn a categorical distribution.\nWe employ the offline actor-critic policy gradient method to estimate the factor of modification given the speech signal and information about the salient segments [25, 26, 27]. Denoting the speech utterance by $y(t)$ and the mask variable by $M_t\u2208 {0, 1}$, the state of the system can be characterized by the tuple $S = (y_t, M_t)$. Mask variable $M_t$ is an indicator function (same length as $y$) denoting the important segments of the signal. The learning agent takes this state tuple and the target emotion description (as one-hot vector representation) to predict a distribution over the discrete set of factors describing the action space A, and the advantage of current state $S_T$.\nAfter sampling from the action space distribution, we modify the length of corresponding segment using WSOLA and obtain a reward signal $r$. This reward signal measures the goodness of predictive distribution over the actions for emotion modification, i.e., the increase in target emotion category score measured by salience predictor. To train the model efficiently, we uniformly sample one salient region at a time during training. Therefore, an episode is a single time-step. Fig. 2 shows the complete RL framework at a coarser level. We will now dive into the details of the reinforcement learning agent used here."}, {"title": "2.3.1. RL Agent", "content": "The reinforcement learning (RL) agent is a deep neural network [28] consisting a stack of convolution and transformer layers to learn appropriate distribution over the actions A. It is conditioned on three quantities: (a) the input speech signal (in time domain), (b) segment mask through indicator variables, and (c) the target emotion code corresponding to which a prediction has to be made. Fig. 5 shows the neural network architecture used for estimating a probability distribution over the allowable set of factors. Since, the distribution is over an entire masked region, we use max-pooling on the output of transformer layer to feed into the final softmax layer. Therefore, the policy function is a neural network parameterized by \u03b8. The objective of this neural network is to maximize the expected reward which can be written as [25]:\n$L(\u03b8) = E_n [r(s)] \u21d2 L(\u03b8) = \\sum_{\u03b1\u2208A} \u03c0(a|s)r(s)$\n$\\sqrt{L(\u03b8)} = \\sum_{\u03b1\u2208A}\\sqrt{\u03c0(a|s)}r(s)$\n$= \\sum_{\u03b1\u2208A} \u03c0(a|s)\u2207log \u03c0(a|s)r(s)$\n$= E [r(s) log (a|s)]$\nThe objective function suggests that, we do not need estimation of gradients through the reward model for training the agent network. It is specially helpful in our case because the reward framework uses WSOLA operation which is not differentiable."}, {"title": "3. Experiments and Results", "content": "In this section, we will discuss the results of saliency prediction using proposed technique. Our evaluation covers both objective and subjective metrics in terms of preference scores. We compare our RL-based technique to existing state-of-the-art models for supervised/unsupervised emotion conversion published recently. We start with a description of data used for experiments. We use the VESUS [16] corpus to carry out our evaluations. VESUS provides a crowd-sourced annotation obtained from 10 listeners on Amazon Mechanical Turk (AMT) for each sample. This allows us to create a soft assignment over the mixture of emotion (neutral/angry/happy/sad/fearful) rather than a single emotion category for prediction. As a result, our proposed salience predictor predicts the emotion perceived by AMT listeners during training and inference stage. Further, we split the VESUS according to the following scheme:\n\u2022 11.5k samples are randomly selected for training\n\u2022 150 samples are randomly selected for validation\n\u2022 400 samples are randomly chosen for evaluation/testing\nAdditionally, we also show the performance of proposed salience predictor on another standard dataset called CREMA-D([17]). CREMA-D also provides a crowd-sourced emotion"}, {"title": "3.1. Emotion Recognition", "content": "As mentioned above, we evaluate the human emotion perception prediction on the VESUS and CREMA-D corpora. The results are summarized in Table 1. We can see that the top-1 results (weighted F1 and accuracy) are above 75% for VESUS and 65% for CREMA-D. It shows that the salience prediction network (Fig. 4) is reliable for predicting soft score over the emotion classes. We further evaluate top-2 accuracy of the proposed model by checking the presence of the mode of target distribution in the top-2 prediction scores. The accuracy score corresponding to this evaluation is > 90% on VESUS and > 80% on CREMA-D. This analysis is particularly important because, in many cases the ground-truth saliency score is a tie among two emotion categories. The top-1 prediction completely ignores this scenario whereas top-2 compensates for it.\nFig. 6 shows the confusion matrices on the test set obtained from proposed model. We can see that on VESUS dataset, the diagonal elements are higher for most emotions. Fear and sad categories have relatively higher confusion among themselves than any other emotion pairs. This is expected because fear and sadness are usually expressed with a shaky high-pitched articulation which can be confusing for the listeners. Further, fear has the lowest voter confidence among any category as shown in [16]. CREMA-D results are more muddled than VESUS due to inconsistency in number of reviewer's ratings. Here, we see that happy, sad and fear are most confused emotions with no clear pattern among them. Due to these reasons, we will focus only on VESUS corpus for the next task which is RL-based prosody manipulation using the salience predictor as feedback.\nFigure 7 shows an example of the discovered segment and the corresponding prediction on a test utterances from VESUS test set. The top plot (Fig.7(a)) is the audio signal while the second plot (Fig.7(b)) is the extracted features from the convolutional encoder portion of salience predictor (Fig. 4). The third plot is the variational posterior predicted (Fig.7(c)) for the masking random variable and the corresponding mask sample"}, {"title": "3.2. Emotion Conversion", "content": "In order to change the prosody of segments estimated by salience predictor, we train the RL agent by sampling one of the contiguous chunks during training. During inference, the individual chunks are separately processed which provides more flexibility in terms of rhythm, loudness and pitch manipulation as different segments can undergo varying degrees of modification. We evaluate the emotion conversion module on VESUS dataset for three primary categories of emotion: angry, happy and sad. Our first two evaluations use the salience prediction mode as the evaluator of emotion conversion. Specifically, we observe the positive change in score of the target emotion (Fig. 8) of modified samples against state-of-the-art baselines."}, {"title": "3.2.1. Baselines", "content": "Our baseline models are from supervised and unsupervised domain. From supervised domain, we select three models: a Gaussian mixture model (GMM) with global variance technique proposed in [29], an Bi-LSTM model using wavelet parameterization of prosodic features proposed in [30] and a regularized convolutional neural network model (EDP) proposed in [31]. The unsupervised baselines consist of cycle-GAN (CGAN) formulation using wavelet parameterization of pitch suggested in [32], and the variational cycle-GAN (VCGAN) using diffeomorphic flow regularization proposed in [9]."}, {"title": "3.2.2. Emotion Recognition of Modified Samples", "content": "Table 2 shows the accuracy of modified samples estimated using the salience prediction network trained via Markov masking strategy. We can observe that supervised model (EDP) has the best overall performance across all emotion categories. Our proposed technique comes very close to the supervised method showing that by carefully modeling the prosody modification, we can achieve on-par performance even in unsupervised setting. Further, our proposed model is the only technique that uses a single model for generating all target emotions unlike baselines, which train one model for each target emotion.\nFig. 8 shows the change in score of the target emotions on the test set. We can see that the pattern of score difference is"}, {"title": "3.2.3. Subjective Evaluation via A/B Testing", "content": "Next, we use Amazon Mechanical Turk (AMT) platform to conduct an A/B listening test where raters are asked to pick between unmodified and modified sample for a target emotion. We also provide a no preference as third option for difficult cases. The results of this subjective listening are presented in Fig. 9. We notice that in more than 50% of the cases, the listeners selected the modified samples as representing target emotion. It confirms the ability of the reinforcement model to transfer or modify emotions by segment selection strategy. Additionally, we modify the emotional characteristics of the speech signal with a unified model which is advantageous in low-resource scenario."}, {"title": "3.2.4. Intelligibility Assessment", "content": "We objectively evaluate the intelligibility of modified utterances by passing them through OpenAI's Whisper [33] model. We compare the word error and character error rate of modified signal using ground truth as baseline. Fig. 10 shows the result of this comparison. We can see that the ground-truth samples have a word error rate of 0.13 which is comparatively lower than the modified samples in any emotion category. This suggests that prosody modification impacts the recognition performance significantly. However, this is only one part of the puzzle, the moderate increase in character error rate (compared to ground-truth) shows that certain phonemes become difficult to identify post-modification which results in poor word error. The main reason is due to extreme modifications allowed in the prosody by reinforcement agent. We can control this in a desirable manner by reducing the scales of modification. Our technique facilitates convenient tuning or control of artifacts in generated samples."}, {"title": "4. Conclusion", "content": "In conclusion, the reinforcement learning model developed for rhythm modification in speech for emotional speech synthesis represents a significant advancement in the field of emotional speech generation. By effectively identifying contiguous segments crucial for emotion perception through the innovative Markov masking strategy and implementing KL divergence-based sparsity loss, the model not only excels in emotion recognition on the VESUS corpus but also provides valuable insights into the identification of speech segments for duration modification. We show that by training the reinforcement learning agent to estimate a distribution over discrete modification factors in conjunction with WSOLA allows us to enhancement or inject emotional attributes of a speech signal. Our modeling approach is completely designed in a bottom-up generative approach with a Markov prior. We conduct both objective and subjective evaluations to further establish the effectiveness of proposed technique for emotional speech generation. We also discuss that prosody modification can lead to loss of intelligibility in the output which requires more analysis and research. Code will be made publicly available here: https://github.com/ravi-0841/fac-ppg."}]}