{"title": "Distributed AI Platform for the 6G RAN", "authors": ["Ganesh Ananthanarayanan", "Xenofon Foukas", "Bo\u017eidar Radunovi\u0107", "Yongguang Zhang"], "abstract": "Cellular Radio Access Networks (RANs) are rapidly evolving towards 6G, driven by the need to reduce costs and introduce new revenue streams for operators and enterprises. In this context, Al emerges as a key enabler in solving complex RAN problems spanning both the management and application domains. Unfortunately, and despite the undeniable promise of AI, several practical challenges still remain, hindering the widespread adoption of AI applications in the RAN space. This article attempts to shed light to these challenges and argues that existing approaches in addressing them are inadequate for realizing the vision of a truly AI-native 6G network. Motivated by this lack of solutions, it proposes a generic distributed AI platform architecture, tailored to the needs of an AI-native RAN and discusses its alignment with ongoing standardization efforts.", "sections": [{"title": "I. INTRODUCTION", "content": "Cellular Radio Access Networks (RANs) are undergoing a paradigm shift. The main driver behind this evolution is the reduction of the high CapEx and OpEx that telco operators are faced with and the introduction of new revenue streams. This transformation started with the launch of 5G, which introduced several key changes in the way that the network is being deployed and managed. As illustrated in Fig. 1, it turned the monolithic base stations into disaggregated and virtualized components \u2013 Central Unit (CU), Distributed Unit (DU) and Radio Unit (RU) that can be deployed on top of commodity hardware across several locations (far edge, near edge, cloud), simplifying their lifecycle management and release of new features. Furthermore, it introduced open and programmable interfaces, allowing the deployment of applications on top of RAN Intelligent Controllers (RICs), to further accelerate innovation. Finally, it introduced new radio access technologies, like Massive MIMO and millimeter waves, and pushed the densification of the network to new limits, with the goal of expanding the network capacity and enabling new types of applications (e.g., IoT).While the shift to 5G has so far served as a transitional phase, the emerging 5G Advanced and 6G networks are expected to truly unlock the networks' potential in two ways. First, by developing sophisticated control and management solutions that can tackle a plethora of long-withstanding network problems (e.g., in the context of radio resource management, mobility, energy savings, etc.), which have been further amplified by the added complexity of 5G. Second, by introducing new transformative applications (e.g., joint communication-sensing, security, slicing), that can add value and provide differentiation for operators and verticals.In this context, AI is expected to play a pivotal role. The recent advances in the domain of generative AI have acted as a catalyst in drawing the attention of the telecommunications industry to the benefits of AI in solving the aforementioned problems. Al is an ideal fit for many long standing RAN problems, which involve pattern recognition of signals for classification, predicting traffic and devising solutions for com-putationally intractable problems, such as scheduling. In fact, the influence of AI is becoming so big in the telco space, that the general consensus is that next generation mobile networks should be AI-native, with both the industry and academia rallying behind initiatives like the AI-RAN Alliance [1].Unfortunately, while the promise of AI in the RAN is undoubtedly high, its potential still remains untapped, due to several practical roadblocks. First, the high accuracy of AI models is contingent to the existence of rich data sources. Given that RAN base stations are distributed across tens of thousands of locations, one would have to deal with sev-eral problems related to the collection and transmission of vast amounts of data. Second, RAN applications leveraging Al models present heterogeneous characteristics in terms of their compute requirements, response latency and privacy constraints. When operating over a distributed hierarchy of edges and the cloud, the varied compute availabilities and net-work bandwidths pose a significant challenge for operational deployments and handling AI RAN application constraints is far from straightforward.Motivated by these observations, in this article, we attempt to shed light to the challenges of designing and deploying AI models for the RAN. We argue that existing approaches, which rely on the static data collection and orchestration of Al models are not suitable to meet the demands of AI-native 6G networks. Building on these observations, we present our vision of a distributed AI architecture that is tailored to the needs and requirements of an AI-native RAN, and we outline our thoughts of how our proposed architecture can be aligned with the ongoing standardization efforts."}, {"title": "II. THE NEED FOR AI IN RAN", "content": "Here, we discuss why the need for AI in the RAN space is becoming increasingly important. The AI-RAN Alliance has succinctly grouped AI use cases in three key domains [1]:\n1) AI-for-RAN \u2013 This refers to utilizing AI to optimize and enhance the RAN performance, motivated by the need to use the limited spectrum efficiently. The demand for more traffic has led to the introduction of new, higher frequency bands. These bands offer more capacity but lower coverage, leading to an increase in the overall tower density. Furthermore, new radio resource allocation technologies, like Massive MIMO, see the use of large antenna arrays as a way to further increase the cell capacity. Assigning users to different bands and managing interference across cells by adjusting scheduling, power control and antenna assignments leads to a search space explosion, which is computationally intractable to manage with conventional optimization methods, making the use of AI a compelling alternative [2].\nSimilar to radio resource optimization, there exist a whole set of infrastructure optimization problems, caused by the increased 5G complexity, which AI promises to solve. Telco operators often manage close to 100k RAN sites. Predictive maintenance for an infrastructure at that scale (e.g., dealing with hardware issues, software bugs, etc.) is a challenge on its own. The virtualization of the RAN adds an extra layer of complexity, making the identification and root cause analysis of performance issues far from obvious [3]. Finally, in the context of energy efficiency, a lot of effort has been on how to reduce the power consumption of various elements, while maintaining the network coverage and quality, with several AI-based research works showing promising results [4].\n2) AI-and-RAN \u2013 This domain focuses on the compute shar-ing (e.g., CPU, GPU) between RAN network functions and AI applications, for efficient utilization of the infrastructure. Typically, virtualized RAN functions utilize the infrastructure lightly (typically < 50% utilization) [5], for reasons inherent to the characteristics of the RAN workload. vRAN servers have to be provisioned for peak capacity, but they typically serve significantly less traffic (e.g., due to the diurnal traffic patterns). This inefficiency persists even at times of peak usage due to imbalances in the computational demand when processing uplink and downlink traffic [6].\nIn contrast to cloud computing, where the sharing of compute resources is commonplace, sharing resources with RAN workloads introduces significant challenges. The real-time nature of the RAN means that sharing can lead to deadline violations and adversely affect network performance. Several recent proposals have demonstrated how AI can be leveraged to share the infrastructure in both CPUs [6] and GPUs [7].\n3) AI-on-RAN This domain encompasses use cases that leverage the RAN infrastructure to support AI applications. A key enabler of such use cases are the open interfaces exposed by the RAN functions, which can allow third-parties to tap into RAN data and/or affect RAN traffic-related decisions, in order to enhance the application capabilities. One example that falls under this category is localization via sensing. By leveraging channel state information exposed by the physical"}, {"title": "III. BACKGROUND", "content": "To help with highlighting the need for a distributed AI RAN platform, we define two AI application examples (Fig. 2), which we use as a reference for the remainder of this article.\nRAN slicing scheduler A scheduler allocates radio re-sources across network slices (inter-slice scheduling) and across users of the same slice (intra-slice scheduling). Both schedulers use the traffic demand information provided by the phones through buffer status reports, to try and predict the traffic load for the upcoming period [11]. They also use physical layer sounding reference signals to predict the users' signal quality. The inter-slice scheduler makes scheduling decisions at coarse granularities (e.g., seconds) and feeds its decisions to the real-time intra-slice scheduler. This example illustrates both AI-on-RAN (predicting load using buffer status reports) as well as AI-for-RAN (scheduling decisions).\nAnomaly detection and root cause analysis A service management and orchestration framework tries to detect RAN-related anomalies and identify their root cause for potential mitigations towards AI-for-RAN. Since the anomalies can originate both from the RAN and the platform, the application collects data from both sources [3]. It also applies imputation and aggregation techniques, to deal with noisy/missing data and to reduce their volume. It then processes the collected statistics, to detect if an anomaly is present, and if so, it attempts to localize it and detect its root cause.\nFrom these examples, we see that a typical AI RAN appli-cation, rather than being a monolith, may consist of a sequence of blocks. Each block can do its own data pre-processing and can include an ML model in itself, with substantial research and development driving its release. Those blocks are chained together into a graph (e.g., by solution providers), where the"}, {"title": "IV. CHALLENGES TO BUILD AI RAN APPLICATIONS", "content": "We now discuss the challenges of deploying AI RAN appli-cations, that make the case for a distributed AI RAN platform.\nDifferent applications require different feature sets, that might have heterogeneous characteristics in terms of type, time granularity, etc. For example, the radio resource scheduling application might require real-time data from the RAN net-work functions, while the anomaly detection might need to combine aggregate data from both the RAN and the platform in time windows of seconds. Similarly, the anomaly detection application might rely on the inter-arrival time between IQ samples, while the scheduler might require the actual raw IQ samples carrying the sounding reference signals.\nThe heterogeneity in the input features of AI applications is what makes the data collection process challenging. Exposing raw data from all possible data sources is not a viable option, as it would lead to a huge volume of data that one would have to process, store and transmit. For example, capturing all the raw IQ samples from the physical layer of the RAN translates into several gigabits of traffic per second, even for a single base station with four antennas. Similarly, capturing all the CPU scheduling events of a server, in order to detect if a platform anomaly due to CPU interference is present, would result in the collection of millions of events per second.\nThe current standard practice to bypass this roadblock is to expose a set of coarse-grained data sources, that are applicable to a wide range of use cases. These data sources are exposed through static APIs specified by standardization bodies like 3GPP and O-RAN. For instance, O-RAN defines the E2 interface for the collection of pre-defined RAN KPIs in the form of so-called static \"service models\", and the O2 interface for the collection of monitoring data from the platform.Any change to a data source (e.g. reporting the 90th percentile of packet latency instead of max), requires the standardization body consensus.This can be a long process that is typically met with skepticism, due to performance concerns that might arise when modifying mission critical RAN and platform code.In conclusion, data collection challenges force AI RAN applications to be developed and deployed as an afterthought, subject to the available data sources, rather than in a truly AI-native way, where the data collection is application-driven.\nThe disaggregated nature of the RAN means that its network functions might be deployed across the edges and the cloud. This raises a fundamental question as to what should be the location in which the blocks of AI applications should reside. Answering this question is far from straightforward. It involves matching the applications' requirements to the capabilities of the infrastructure, which can vary in terms of compute resources, network bandwidth, latency, etc, depend-ing on the location (Fig. 1). The placement decision could also be affected by other factors, such as privacy concerns, since operators might have a preference or legal obligation to process certain data streams in their own premises.\nThe many constraints and trade-offs make the deployment of AI RAN applications a major challenge. First, a developer needs to understand the underlying constraints which may differ from one deployment to another. Second, they need to carefully choose what data to collect and where to place the various application blocks, to maintain high accuracy, while respecting the constraints. Third, deployed applications will naturally have competing requests for the same resources, meaning that it falls on the developer to design their appli-cation with placement flexibility in mind. Finally, as alluded to in the case of AI-and-RAN, AI RAN applications have to coexist with other AI applications (non-RAN) that also execute on the edge such as video analytics and self-driving cars [10].In the absence of a structured framework, AI RAN applications have to be developed with ad hoc programming interfaces and manually distributed by developers. As a result, they are rarely deployed at scale in production settings and have led to their potential remaining largely untapped."}, {"title": "V. DISTRIBUTED AI PLATFORM FOR RAN", "content": "We present our vision for distributed AI-native RAN plat-form, with the architecture illustrated in Fig. 3. It builds on top of three components; i) programmable probes for the flexible collection of data, ii) AI processor runtimes for the deployment of AI applications across the distributed compute fabric, and iii) an orchestrator for the coordination of the platform.\nProgrammable probes for dynamic data collection To allow developers to define optimal feature sets for their Al applications, we propose the use of dynamic probes for the platform (i.e., OS kernel) and the userspace (i.e., RAN network functions) to programmatically collect data. Through the probes, a developer could write small pieces of code to access raw events and data structures and summarize them in a custom way. This would expose the right features for training and inference, while minimizing the volume of generated data. For example, a developer could leverage a probe to access the raw IQ samples of the base station to feed them directly to the"}, {"title": "VI. AN EFFICIENT AI PROCESSOR RUNTIME FOR THE FAR EDGE", "content": "We advocate that a highly optimized far edge runtime is required, as it is expected to host real-time AI applications (e.g. the RAN slicing scheduler), as envisaged with a concept of dApps [14]. As such, it needs to have sub-millisecond reaction times. Furthermore, the far edge is resource constrained, with only a small fraction of CPUs available for AI applications, and with a small or no GPU present. Our proposed design is illustrated in Fig. 5 and has the following characteristics:\nTight integration with the probes The AI processor run-time communicates with the probes through fast IO channels using shared memory and a zero-copy mechanism. This allows the passing of data between the probes and the AI applications with very low compute and latency overhead.\nEfficient, flexible and secure execution environment We have experimented with the use of WASM as an execution en-vironment and observed its benefits. It enables the sandboxing of applications with a very small overhead, while maintaining near-native runtime performance. The interaction with the rest of the system for the acceleration of inference or the data"}, {"title": "VII. OPEN VS CLOSED ARCHITECTURE AND INTERFACES FOR INTEGRATION WITH RAN", "content": "Despite the progress within the O-RAN community, many fundamental tensions remain around control interfaces. For example, several major vendors do not support the idea of a near real-time RIC [15], expressing concerns regarding the network stability and security. If RAN vendors allow developers to exert fine-grained control on the RAN behavior, these may clash with the proprietary algorithms vendors have implemented. Also, some vendors argue that exposing sensitive RAN data may affect the security of the network and the data privacy. Vendors are also reluctant to open up some of their interfaces, as this may reduce their competitive advantage. All these concerns are important, and different operators take different stances on them. Our view is that the distributed AI platform proposed in this article should be a building block that can be customized appropriately for different use cases. Next, we briefly discuss some examples of deployment options (also illustrated in Fig. 4).\nO-RAN based design One extreme is an O-RAN based design (Fig. 6, left), allowing any first or third-party vendor to deploy AI applications across the RAN infrastructure. One could leverage the O-RAN RICs and integrate the AI processor runtimes in them as apps (i.e., rApps, xApps, dApps). For the local data collection and control operations, the AI processor runtimes could leverage the open RIC interfaces (e.g., E2, 01), augmented with programmable probes and exposed to the AI runtimes through appropriate adapters. The communication between the AI processor runtimes could be facilitated by a message bus, which could be standardized and realized as an overlay network on top of the RIC fabric. For the far-edge, and considering that there is currently no real-time RIC"}, {"title": "VIII. CONCLUSIONS", "content": "In this paper we focused on the topic of AI as a key enabler behind realizing the 6G vision. We argued about the benefits of introducing Al across several dimensions of the RAN, from the management and infrastructure up to the application layer. Based on these observations, we outlined the challenges for deploying AI solutions, stemming from the requirements of the applications and the characteristics of the RAN in-frastructure. Motivated by these challenges, we proposed a distributed AI platform architecture. Its goal is to alleviate the common painpoints of deploying AI applications in the RAN without being prescriptive about the implementation details. This allows the platform to be tailored to the needs of the infrastructure provider or a standardization body. We believe that by identifying the future requirements and by initiating a discussion on the architecture of a 6G AI platform we can help both the standards and the vendors to create new opportunities for introducing AI solutions in this space."}]}