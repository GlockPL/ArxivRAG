{"title": "MRNet: Multifaceted Resilient Networks for Medical Image-to-Image Translation", "authors": ["Hyojeong Lee", "Youngwan Jo", "Inpyo Hong", "Sanghyun Park"], "abstract": "We propose a Multifaceted Resilient Network(MRNet), a novel architecture developed for medical image-to-image translation that outperforms state-of-the-art methods in MRI-to-CT and MRI-to-MRI conversion. MRNet leverages the Segment Anything Model (SAM) to exploit frequency-based features to build a powerful method for advanced medical image transformation. The architecture extracts comprehensive multiscale features from diverse datasets using a powerful SAM image encoder and performs resolution-aware feature fusion that consistently integrates U-Net encoder outputs with SAM-derived features. This fusion optimizes the traditional U-Net skip connection while leveraging transformer-based contextual analysis. The translation is complemented by an innovative dual-mask configuration incorporating dynamic attention patterns and a specialized loss function designed to address regional mapping mismatches, preserving both the gross anatomy and tissue details. Extensive validation studies have shown that MRNet outperforms state-of-the-art architectures, particularly in maintaining anatomical fidelity and minimizing translation artifacts.", "sections": [{"title": "I. INTRODUCTION", "content": "MEDICAL image-to-image translation aims to map relationships between different modalities. The main challenge is to transforming the distribution of a source image into that of a target image. As a result, generative adversarial network (GAN)-based pixel mapping has become widely used in the medical field for tasks such as image registration [1], \u0421\u0422 noise denoising [2], and MRI-CT/MRI translation. Advances in deep learning have made these applications feasible. Among the various tasks that deep learning supports, bidirectional conversion between modalities, such as MRI and CT is crucial due to challenges in acquiring single-modality due to the distinct image acquisition characteristics of medical images, such as the need to protect pregnant women from radiation or accommodate patients with implanted medical devices. Missing modalities may also occur due to clinical situations. and in such cases, modality conversion can be clinically beneficial. In GAN-based image translation, the focus is on the generator's image generation process, with improvements to both the model and loss function for better results. A model designed to better capture the distribution of the source and target domains has been proposed [3], along with a specialized loss function to enhance performance [4]. However, in the medical domain, the goal is not just to generate an image resembling the target but to preserve the original underlying features during translation. It is important to recognize that images demonstrating stylistic similarity through latent vector representation may not always ideal for translation. Therefore, success translation requires preserving semantic content while identifying differences between source and target images To address these challenges, we introduce the Multifaceted Resilient Network(MRNet), a novel medical image translation network.\nThe novelties and contributions of this paper are summa-rized as follows :\n\u2022 To the best of our knowledge, MRNet is the first model to integrate Segment Anything Model(SAM)-based fre-quency interpretation. The proposed framework leverages image encoding capabilities for medical image transla-tion, focusing on preserving anatomical structures during MRI-to-CT conversion.\n\u2022 Resolution specific feature fusion: We introduce a feature fusion mechanism that combines encoder features and SAM-encoded features across multiple resolutions, en-abling the effective preservation of both local and global image characteristics.\n\u2022 Dual-mask Framework: We introduce a multimask approach enhances feature selection during translation through dynamic mask generation and specialized loss terms. This method significantly improves the preservation of anatomical structures compared to conventional tech-niques. The proposed MRNet consistently outperforms state-of-the-art methods in MRI-to-CT and MRI-to-MRI image conversion."}, {"title": "II. RELATED WORK", "content": "Image-to-image translation has evolved dramatically in recent years. The breakthrough came through conditional generative adversarial networks (cGANs) [5], which have demonstrated remarkable capabilities. cGANs improve existing GAN architectures by incorporating class-specific constraints. By directly passing the constraints as conditions, the generator can continuously challenge the discriminator to produce incredibly realistic cross-modality images. Pix2Pix [6] took this concept further by directly conditioning the input images. If the latent vector-based generator's mapping function takes both a source image x and a random noise vector z to produce y, expressed mathematically as G : {x, z} \u2192 {y}. More recently, PPT [7] introduced sophisticated patch-based contrastive learning techniques, achieving exceptional input-output consistency when used to analyze pathological image datasets. When researchers cannot access paired domain data, they can define informative loss functions that minimize discrepancies between the target and generated images, leading to more precise results [8].\nCycleGAN [9] proposed another architecture by enabling bidirectional image transformations with unpaired datasets. This innovative approach to transforming style Y back to style X facilitates bidirectional image transformation using nonpaired datasets. To preserve the characteristics of the original image, cycle consistency loss was proposed during the conversion process.\nCycleGAN-based approaches have been widely applied in medical image transformation tasks. However, cycle consistency loss makes it difficult to preserve diagnostic features, especially in medical applications. Although this method preserves global structures relatively well, it often struggles with fine anatomical details that are important for accurate diagnosis [10]. The most concerning aspect is that the generated artifacts may be mistaken for genuine pathological findings.\nAdditionally, recent studies on aspects of cycle consistency models have shown that they are adept at encoding hidden information via steganographic techniques [11], [12]. The use of this feature should be used cautiously in clinical settings where image integrity must be absolutely assured.\nThese limitations are important in medical imaging modalities where precise structural preservation of the image affects diagnostic accuracy. For example, tasks such as cancer detection and disease progression monitoring, where minor artifacts or distortions must be avoided, are involved. Given these considerations, it is best to use paired training data whenever possible in medical imaging applications.\nPaired training offers distinct advantages. First, direct supervision allows for more accurate preservation of anatomical details and significantly reduces the risk of artifacts and structural distortions. In terms of validation, it also allows for comparison of the ground truth-to-ground transformation accuracy. Unpaired methods are still valuable when paired data are scarce or unavailable in medical imaging. However, developing high-quality paired datasets should be a priority to ensure optimal accuracy and reliability in clinical applications. Therefore, the focus should be on utilizing paired datasets whenever possible, while reserving unpaired methods for situations where paired data acquisition is impractical or impossible."}, {"title": "B. SAM", "content": "SAM(Segment Anything Model) [13] is a recently developed segmentation foundation model with strong generalization performance and applicability to diverse vision tasks. It comprises three main components: an image encoder, a prompt encoder, and a mask decoder. The image encoder, built on the Vision Transformer (ViT) architecture, divides images into patches and encodes them to extract features. The prompt encoder processes various prompt types, such as points, boxes, and text, resolving ambiguities caused by insufficient prompt data. This innovative use of prompts has enabled the development of multimodal architectures. The mask decoder combines encoded image and prompt embeddings with output tokens to generate segmentation masks.\nIn this study, we utilized a pretrained base-size ViT SAM model trained on an 11M image dataset, enabling powerful feature selection through generalized representations. This approach reduces training time and enhances performance. We extract features using the ViT-based SAM image encoder and integrate them into the decoder branch."}, {"title": "C. Unlocking Hybrid Synergy: Strengths of Vision Transform-ers and Convolutional Neural Networks", "content": "Recent research has highlighted the distinct advantages of ViT in low-frequency signal processing, in contrast to convolutional neural networks (CNNs) that excel at high-frequency signal analysis [14], [15]. These differences suggest that developing a hybrid approach that combines the capabilities of ViT for SAM encoding with the analytical strengths of CNNs could ultimately improve overall processing efficiency. To leverage this synergy effectively, we engineered a novel in-tegration of SAM encoding within the decoder branches of the network, addressing the low-frequency processing limitations inherent to CNNs. Careful alignment of the feature layouts with the original network architecture was essential to ensure compatibility and maintain output integrity [16]. The encoder responsible for extracting the SAM-based features is built upon a stable pretrained model, facilitating coherent incorporation into the existing framework. By allowing residual connections across varying resolutions of the feature maps, we preserve the integrity of the input image structure. This separate learning of the SAM based feature flow provides a comprehensive examination of the image and addresses the steganography challenges associated with GANs. In steganography, critical information is concealed within subtle patterns; however, this challenge can be managed effectively through noise introduction or blurring techniques [17].\nThe importance of synthetic-based decoders in image trans-formation highlights the rationale for adopting hybrid models. The different frequency components of an image have a significant impact on its image characteristics. Low-frequency components are usually related to the overall style of the"}, {"title": "III. MRNET", "content": "Our generator is structured with a U-Net-like encoder-decoder architecture, which is depicted in Fig. 1. Our study in-tegrates pre-trained SAM image encoders to demonstrate their effectiveness in medical image representation and segmenta-tion. The MRNet encoder architecture merges SAM with a UNet-like structure and passes through a hierarchical encoder-decoder framework tailored for medical image transformation, focusing on capturing low-frequency information. The feature integration process can be formally expressed as follows:\n$E_i = T_i(e_i \\bigoplus A_i(feature_{SAM}(x)))$                                                                      (1)\nwhere Ei represents the original encoder features, Ai denotes the SAM adaptor function at stage i, and indicates the channel wise concatenation. Transformation function Ti encompasses stage-specific operations including normalization and nonlinear activation.\nSAM adaptor Ai performs two crucial operations:\n$A_i(feature_{SAM}(x)) = C_i(B(F, s_i))$                                                                       (2)\nWhere Ci is the convolutional adaptation layer, B is bilinear interpolation, F is the SAM encoder feature and si is the target spatial dimension at stage i. This formulation ensures both channel compatibility and spatial alignment between the SAM encoder features and the encoder representations."}, {"title": "", "content": "The integration process starts by extracting features from the SAM model, which are passed through an adaptive con-volution layer that acts as a SAM adapter. These adapted features are then bilinearly interpolated to fit the dimensions of the corresponding encoder stage. The interpolated features are then concatenated with the original encoder features along the channel dimensions. Finally, these combined features are then processed step-by-step to produce refined outputs for each encoder stage.\nThe integration mechanism is systematically applied to all encoder stages from El to E8, ensuring that the features are connected to the decoder branches at their respective resolutions via residual connections. This architectural de-sign effectively captures the hierarchical features at various frequency levels while maintaining structural alignment with the input image. In particular, it prioritizes the extraction of incremental information at low frequencies, which enhances the model's ability to transform detailed and accurate medical images while preserving essential structural information. By integrating SAM features, the model achieves improved perfor-mance in medical image transformation tasks while preserving important anatomical details and structural integrity. This design facilitates the integration of fine (local) and coarse (global) information even within deep structures. As a result, it reduces information loss and effectively resolves the tradeoff between details and semantic relationships.\nThe intermediate steps after feature extraction via SAM are visualized as shown in Fig. 1. The power spectrum clearly exhibits the frequencies on which the model is trained. The bright spot in the center represents low-frequency com-ponents, whereas the outer part represents high-frequency components with detailed information. This suggests that the SAM is particularly sensitive to low-frequency components. The concentration of brightness in the central region and the darkness of the outer high-frequency area indicate a potential bias towards capturing broader, larger-scale features, while potentially neglecting detailed texture, which are then passed to the decoder. As shown in the middle column of Fig. 2, the output from the first encoder layer displays a balanced distribution of the high- and low-frequency components. When combined with the SAM and passed on to the decoder, the low-frequency components are filtered out and passed on, as depicted in the right column of Fig. 2.\nThe final stage of our encoder pipeline incorporates a soft clamping mechanism [17], [24] to maintain data stability before decoder processing. This SoftClamp function is math-ematically expressed as follows:\n$v = min(1, max(0, z))$\n$S(z) = v + \\alpha(min(0, z) + max(0, z - 1))$                                                                      (3)\nThe SoftClamp function, denoted as S(x), utilizes a scaling factor \u03b1 with a softness parameter of 0.001 to clamp the feature values within the desired range, ensuring a smooth and stable transition to the decoder stage.\nOur decoder architecture is intended to avoid complex attention mechanisms by implementing a lightweight design based on convolutional operations. This design offers several"}, {"title": "", "content": "advantages in terms of resource efficiency and functional preservation. Given that the encoder already incorporates a computationally intensive SAM model, we intentionally balance the computational requirements of the network. Therefore, we use a lightweight decoder that relies on the underlying convolutional operations to maintain the overall performance of the network. This strategy is consistent with the work of [25], which showed that complex decoder architectures do not necessarily lead to proportional improvements in performance.\nWe achieve commendable performance through a robust hierarchical transformer encoder and a decoder composed solely of MLPs, especially when paired with a powerful encoder. Such as SegFormer [26]. This approach highlights that a strong encoder can deliver accurate and high-resolution segmentation results without requiring complicated decoder designs. Thus, we focus on feature extraction using a vigorous hierarchical encoder that effectively captures both coarse and fine features across various resolutions while maintaining simplicity throughout the rest of the network.\nThe decoder architecture can be formally described as follows: At each decoder stage i (i \u2208 1,...,8), the output Di is computed through a series of operations involving skip connections from the encoder pathway. The feature maps from the encoder stages are concatenated with the upsampled decoder features along their channel dimensions. This concatenated output undergoes a transposed convolution followed by instance normalization and is subsequently processed through a ReLU activation function. The final decoder output is transformed through a hyperbolic tangent activation function before the mask correction stage. This step captures the hierarchical feature processing and skip connection ar-chitecture characteristic of modern encoder-decoder networks, where information from the corresponding encoder levels is systematically integrated into the decoder pathway to preserve the fine-grained spatial details.\nThe discriminator and its associated loss function were implemented using the same version of the patch discriminator, as outlined in Pix2Pix [6]. The patch size was set to 30, which was tailored to the dimensions of the training images employed in the present research."}, {"title": "B. Dual Mask-Based Correction", "content": "We adopted the mask generation mechanism from Ste-goGAN [17] and extended it into a multimask framework to enhance the selective feature extraction process during image"}, {"title": "", "content": "translation. The generator employs three parallel masks, each generated through a dedicated convolutional neural network pathway that learns to identify and segment relevant features from the intermediate feature representations.\nThe mask generation process can be formally expressed as the following:\n$\u039c_i = \\sigma(\\psi_i(\\hat{y}_{i-1})), i \\in {1,...,n}$                                                                 (4)\nWhere Mi represents the i-th mask, \u03a8i denotes a sequence of convolutional layers with instance normalization and ReLU activation, and \u03c3 is the sigmoid activation function that nor-malizes the mask values to the range of [0, 1]. When i = 1, yo is the output of the generator's decoder pathway of the generator after tanh activation.\nThese generated masks are then applied sequentially to modulate the output features:\n$\\hat{y}_k = \\hat{y}_{k-1} \\bigodot \\prod_{i=1}^{k} \\sigma(\\psi_i(\\hat{y}_{k-1})), k \\in {1,...,n}$                                             (5)\nThe process involves taking the translated values of y2 and each mask Mi, which are sequentially applied. This method allows for iterative refinement and enhances the accuracy of the model in achieving the desired visual outcomes. More importantly, both the final translated output (y2) and the mask values (Mi) are incorporated into the total loss computation, with the masks being compared against their respective ground truth masks for optimal feature selection.\nThis design enables each mask to automatically learn to identify and focus on different relevant features during the translation process. The effectiveness of the multimask mechanism was validated through quantitative comparisons with ground truth images, demonstrating improved preservation of anatomical structures and enhanced feature selection compared with single mask approaches. The masks adaptively adjust their attention patterns based on the input context, allowing for dynamic feature emphasis depending on the specific char-acteristics of each medical image."}, {"title": "C. Overall Losses", "content": "The total generator loss comprises multiple components designed to ensure high-quality image translation while maintaining structural consistency. Each component serves a spe-cific purpose in the training process.\nTo ensure that the generator produces realistic outputs that can fool the discriminator, we employ an MSE-based adversarial loss:\n$L_{GAN}(G, D) = E_{x,y}[(D(G(x), x) \u2013 1)^2]$                                                                                              (6)\nTo maintain the structural consistency between the gener-ated output and the ground truth, we utilize an L1 distance-based pixel-wise loss:\n$L_{pixel} (G) = E_{x,y} [||y \u2013 G(x)||_1]$                                                                                                (7)\nFor each mask application step, we also compute the inverse masks and the corresponding discarded features:\n$M_i = 1 - M_i$                                                                                              (8)\nThe feature discarding loss penalizes activation in nonrele-vant regions identified by the inverse masks:\n$L_{feature} = \\sum_{i=1}^{2} || y_i \\bigodot \\bar{M_i}||_1$                                                                              (9)\n$L_{mask} = \\sum_{i=1}^{2} ||\\bar{M_i}||_1$                                                                                               (10)\nwhere \u0177i represents the features in each masked region. The feature discarding loss Lfeature penalizes activation in non-relevant regions identified by the inverse masks (1 - Mi), whereas the mask sparsity loss Lmask promotes efficient attention distribution by encouraging masks to be selective in their activation to promote efficient attention distribution. The final generator loss combines all components with the appropriate weighting factors:\n$L = L_{GAN} (G, D) + \\lambda_{pixel}L_{pixel} (G)$\n$+ \\lambda_{feature}L_{feature}(G) + \\lambda_{mask}L_{mask}(G)$                                               (11)\nwhere \u03bbpixel = 100 controls the importance of the pixel wise reconstruction, \u03bbfeature = 0.1 weights the feature preser-vation loss, and \u03bbmask = 0.05 balances the mask sparsity constraint.\nThis comprehensive loss function guides the network to achieve high-quality image translation while maintaining structural consistency and efficient feature selection through the multimask mechanism."}, {"title": "IV. EXPERIMENTS", "content": "The current study employed the MRI-CT paired dataset presented in the SynthRAD2023 challenge [27] for image-to-image translation. The dataset consists of 3D brain MRI-CT pairs, and we complied the dataset with the official protocols provided by SynthRAD2023 regard-ing image registration and preprocessing. The dataset, which included 180 subjects, was randomly divided into training, validation, and testing sets in an 8:1:1 ratio, yielding 144, 18, and 18 subjects for each set, respectively. Following the parsing of each slice along the x, y, and z axes, the dimensions of the 3D voxel-shaped images were resized to 256x256 pixels.\nIn this learning approach, as opposed to the conventional method that primarily focuses on the z-axis slice, we gain the advantage of comprehensively examining the coronal, sagittal, and axial perspectives of the human body in a three-dimensional context. This methodology provides benefits such as data augmentation. Therefore, a total of 119,317 pairs of 2D image datasets were used in the study, the details of which are shown in Table I."}, {"title": "B. Experimental Settings", "content": "In our experimental settings, the input images were resized to a tensor configuration of 3x256x256 and processed indi-vidually. To prevent overfitting, the dataset was meticulously divided into training, validation, and testing subsets following the established 8:1:1 ratio. The random seed was fixed in 2024 to ensure the reproducibility of the experiment. The Adam optimizer was initially configured with a learning rate of 0.0001 and momentum parameters of 0.5 and 0.999, which enhanced the gradient stability during training. The training duration encompassed a total of 40 epochs with a consistent batch size of 4. All experiments were conducted using the PyTorch framework, leveraging either four GPUs or a single A100 GPU.\nCompetitive models, except for CycleGAN, were utilized as architecture generators within the c-GAN framework. All variables were held constant across MRNet and the discrimina-tor, except for specific parameters tailored to each model. The models compared included Pix2Pix [6], CycleGAN [9], TC-GAN [22], TransUNet [19], SwinTransformer(Swin-T) [31], ResViT [20] and PPT [7]. All architectures, apart from Cy-cleGAN, were trained using paired datasets, with loss up-dated through direct comparisons between ground truth and translated values. In similar studies, TransUNet and Swin-T replaced the segmentation head with a convolutional layer, following the methodology applied in [20]. To ensure a fair comparison with competitive methods, all models utilized the officially distributed code. Based on the recommended configurations, the parameters were adjusted to optimize the performance according to the specific dataset. Early stopping techniques were also implemented to mitigate the overfitting."}, {"title": "C. Result Analysis", "content": "We evaluated the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) for an objective and quan-titative assessment. Using paired samples, we tested the trans-lated B modal generated from the A modal learned during training, against the corresponding ground truth B modal image.\nTable II proviedes a comprehensive comparison of various image enhancement models, based on PSNR and SSIM. These standard metrics, sidely used in image processing, indicate better image quality and structural fidelity with higher values."}, {"title": "", "content": "Among the existing methodologies, Pix2Pix performs well, with a PSNR of 28.2379 and an SSIM of 0.7277. It signif-icantly outperforms the CycleGAN architecture, which has a PSNR of 15.6958 and an SSIM of 0.4311. This performance gap suggests that the direct image-to-image translation strategy used by Pix2Pix is more effective for the task compared to the cycle-consistent learning approach. Hybrid models that integrate the transformer architecture (TCGAN, TransUNet, and ResVIT) outperform CycleGAN, with a PSNR value of 23.3101-24.8740 and a SSIM value of 0.7558-0.7896. These trends generally suggest that incorporating the transformer component improves the model's ability to capture long-range dependencies and complex image features. A pure Transformer-based model, Swin-T, shows an intermediate re-sult, with a PSNR of 24.3163 and an SSIM of 0.7717. This suggests that while the transformer architecture can adeptly manage image enhancement tasks, a hybrid strategy may provide more benefits in balancing local and global feature processing.\nMost importantly, the proposed model outperforms all the baseline models by achieving a PSNR of 30.7189 and an SSIM of 0.9111. Compared to the second best performing ResVIT, our approach achieves a PSNR of 5.8449 and SSIM of 0.1215. This significant improvement is attributed to the innovative design of our model architecture. The consistently superior performance in both metrics strongly validates our architectural design choices and demonstrates the effectiveness of our approach in preserving both pixel-level accuracy (de-noted by PSNR) and structural consistency (denoted by SSIM). These quantitative results confirm that the proposed model achieves state-of-the-art performance, especially in scenarios where detailed preservation and structural integrity are most important."}, {"title": "", "content": "and ResVIT, show various performances. TCGAN and Tran-sUNet show notable improvements, achieving PSNR values of 29.5175 and 29.5681 and SSIM values of 0.9388 and 0.9396, respectively. This highlights that the hybrid models are effec-tive in preserving soft tissue structural information of MRI during the conversion process. The pure transformer-based model Swin Transformer outperforms most hybrid approaches with PSNR of 30.2356 and SSIM of 0.9337. This suggests that the transformer architecture is particularly suitable for capturing the complex relationship between the FLAIR and T2 modalities. Interestingly, the performance of ResVIT is moderate with a PSNR of 26.7941 and an SSIM of 0.9174.\nMost importantly, the proposed model outperforms all the compared models with PSNR of 31.2852 and SSIM of 0.9411. Although these improvements are smaller compared to the MRI-to-CT conversion task, they still demonstrate the robust-ness and effectiveness of our architectural design. The patch-based PPT model showed significantly lower performance indices and was not suitable for this particular conversion task.\nThese quantitative results comprehensively validate the effec-tiveness of our model in preserving both pixel-level accuracy and structural consistency in the FLAIR-to-T2 conversion task, establishing a new state-of-the-art benchmark in this domain."}, {"title": "D. Ablation Study", "content": "We conducted different ablation studies to verify the effects of the multimask frameworks and specialized loss functions on the MRI-to-CT dataset. The first ablation study was conducted by varying the number of masks. Then, we tested the effect with and without mask loss."}, {"title": "", "content": "1) Amount of Mask Ablation: The proposed algorithm utilizes multiple masks to minimize the disparity between the MR-based generated CT image and the ground truth CT. We conducted an ablation study to confirm the effectiveness of the mask algorithm. Our findings confirm that employing multiple masks leads to better performance than using no masks or just one mask. Table IV contains information about the number of masks employed to achieve the most significant performance enhancement.\nThe use of multimasks was determined through extensive experimentation, demonstrating that this configuration pro-vides an optimal balance between model complexity and translation quality. One mask resulted in insufficient feature capture, whereas additional masks led to redundant attention patterns without meaningful performance gains."}, {"title": "", "content": "2) Incorporation of Mask-Based Loss: The integration of the dual-mask-based loss into our training framework has enhanced both the stability and performance of the model. Mask loss, derived from segmentation or object masking processes, was introduced to refine the image quality by directing the focus of the model toward critical regions. The goal was to ensure that the model not only captures global features through adversarial loss but also incorporates local contextual and structural information through mask guidance."}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "The current research introduces MRNet, an innovative ar-chitecture for medical image-to-image translation, addressing longstanding challenges in the field. By integrating the fre-quency analysis capabilities of SAM with a dual-mask frame-work, MRNet achieves state-of-the-art performance in MRI-to-CT and MRI-to-MRI translation. The model was trained using a paired dataset that provides a robust baseline performance. However, given the limited availability of well-structured paired datasets in the medical domain, future research should focus on developing models capable of achieving strong per-formance with minimal paired datasets.\nA key strength of MRNet is that it is based on a well-established UNet-like encoder-decoder architecture, enabling easy module-specific modifications for enhanced performance. This study aims to inspire the development of lightweight models capable of delivering accurate results. While MRNet performs reliably, opportunities remain for optimization, par-ticularly in computational efficiency and edge case handling in clinical settings. The success of the network opens up several promising research avenues, especially within a clinical context. For instance, plane-aware translation mechanisms could address performance variations across specific planes. Additionally, incorporating clinical validation metrics should be incorporated to ensure alignment with real-world medical applications."}]}