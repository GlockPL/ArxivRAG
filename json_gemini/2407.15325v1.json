{"title": "ODYSSEY: Empowering Agents with Open-World Skills", "authors": ["Shunyu Liu", "Yaoru Li", "Kongcheng Zhang", "Zhenyu Cui", "Wenkai Fang", "Yuxuan Zheng", "Tongya Zheng", "Mingli Song"], "abstract": "Recent studies have delved into constructing generalist agents for open-world embodied environments like Minecraft. Despite the encouraging results, existing efforts mainly focus on solving basic programmatic tasks, e.g., material collection and tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond task as the ultimate goal. This limitation stems from the narrowly defined set of actions available to agents, requiring them to learn effective long-horizon strategies from scratch. Consequently, discovering diverse gameplay opportunities in the open world becomes challenging. In this work, we introduce ODYSSEY, a new framework that empowers Large Language Model (LLM)-based agents with open-world skills to explore the vast Minecraft world. ODYSSEY comprises three key parts: (1) An interactive agent with an open-world skill library that consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned LLaMA-3 model trained on a large question-answering dataset with 390k+ instruction entries derived from the Minecraft Wiki. (3) A new open-world benchmark includes thousands of long-term planning tasks, tens of dynamic-immediate planning tasks, and one autonomous exploration task. Extensive experiments demonstrate that the proposed ODYSSEY framework can effectively evaluate the planning and exploration capabilities of agents. All datasets, model weights, and code are publicly available to motivate future research on more advanced autonomous agent solutions.", "sections": [{"title": "1 Introduction", "content": "Developing autonomous embodied agents capable of performing open-world tasks represents a significant milestone towards achieving artificial general intelligence [9, 31, 33]. These open-world tasks necessitate that agents interact with complex and dynamic environments, make decisions based on incomplete information, and adapt to unexpected events. Early reinforcement learning agents [12, 26, 36] have demonstrated limited knowledge in such open-world setting. Furthermore, these agents often struggle with long-term planning, which is crucial for the fulfillment of intricate goals. Recent breakthrough of Large Language Models (LLMs) [1, 15, 37] have shown the potential to revolutionize various fields such as healthcare [46, 50], robotics [2, 16], and web services [7, 25], attributed to its capability on endowing agents with expansive knowledge and sophisticated planning akin to human reasoning [19, 41, 44, 47]. However, the development of LLMs in open-world tasks remains challenging due to the need for well-defined environments and measurable benchmarks [30, 40, 42, 43, 53]."}, {"title": "2 Open-World Skill-based Interactive Agent", "content": "ODYSSEY develops an LLM-based interactive agent with an open-world skill library, aiming to enhance the efficiency and adaptability of agents in complex Minecraft environments. The skill library comprises 40 primitive skills and 183 compositional skills, while the LLM-based agent employs a planner-actor-critic architecture to facilitate task decomposition, skill execution, and performance feedback. The architecture of the interactive agent is depicted in Fig. 2. Full skill and prompt details used in the LLM-based interactive agent are given in Appendix A."}, {"title": "2.1 Open-World Skill Library", "content": "Primitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript APIs [29], divided into two main categories: 32 operational skills and 8 spatial skills. This suite of skills exceeds the 18 primitive skills (all are operational skills) delineated in Voyager [40]. Operational skills serve as foundational interfaces with parameterized input, such as mine() for material collection and craft() for tool crafting. Additionally, we pioneer 8 spatial skills that Voyager [40] lacks, allowing for environmental interactions based on the agent coordinates. Given that our work is conducted within a text-based Minecraft environment [10, 40], spatial skills are crucial for handling tasks that require precise positioning and orientation, especially in the absence of visual input.\nCompositional skills represent an advanced encapsulation of primitive skills into more high-level skills, functioning to address a variety of basic programmatic tasks, such as mineDiamond and craftIronPickaxe. ODYSSEY classifies 183 compositional skills into main types like mineX and craftX, and extends to plantX, breedX, cookX, etc. We introduce a recursive method to construct the skill library, which simplifies the decomposition of complex tasks by ensuring prerequisites are satisfied prior to skill execution. Taking mineDiamond as an example, if the agent lacks an iron pickaxe, it will recursively execute craftIronPickaxe. This indicates that our program internally manages the construction and execution order of skills through its recursive method, thereby obviating the need for the agent to engage in additional planning. Consequently, the agent can focus more on the exploration without concern for orchestrating the internal details for basic programmatic tasks.\nTo facilitate efficient retrieval of skills in the skill library, we first generate a description for each skill by calling the LLM and using the complete program code as a prompt. We then employ Sentence Transformer [32] to encode the skill description. This method transforms text information into vector representations, facilitating semantic retrieval and enabling the agent to find the most relevant skill description based on the context provided."}, {"title": "2.2 LLM Planner", "content": "The LLM Planner is responsible for developing a comprehensive plan, facilitating efficient exploration through long-term goal decomposition. The LLM Planner breaks down high-level goals into specific low-level subgoals, each corresponding to a particular skill outlined in Sec. 2.1. By addressing each subgoal in the plan, the ultimate goal can be progressively achieved. The input prompt to the planner consists of several components:"}, {"title": "2.3 LLM Actor", "content": "In the execution phase, the LLM actor is invoked to sequentially execute the subgoals generated by the LLM planner within the Minecraft environment. This process utilizes the open-world skill library to achieve these subgoals. The mapping from high-level subgoals to executable skill code is accomplished through query context encoding and skill similarity retrieval. This process includes:\n\u2022 Query context. The text-based subgoals generated by the LLM planner are encoded by Sentence Transformer [32] to vector representations as the query context.\n\u2022 Similarity matching. The vector similarity between the query context and the skill descriptions in the skill library is computed to determine semantic closeness.\n\u2022 Skill selection. The top-5 relevant skills with the highest scores are identified, and the actor agent selects the most appropriate code for execution within the environment based on their descriptions."}, {"title": "2.4 LLM Critic", "content": "During action execution, it is critical for an agent to document its experiences, particularly noting the successful outcomes and the points of failure for each task undertaken. Given the unpredictable nature of open-world planning, such documentation is essential to establish a feedback-informed system. This allows for the correction of any discrepancies that may have been present in the initial plan, which could lead to execution errors. For example, the goal of animal breeding cannot be achieved without the prerequisite of crops for feed. The LLM critic can evaluate the effectiveness of the executed actions by comparing expected outcomes with actual results, thereby providing valuable insights for refining strategies in subsequent iterations. We categorize feedback into three types:\n\u2022 Execution feedback. This captures the progress of skill execution. For example, \"No hoe in inventory. Craft a hoe first!\" not only highlights the reason for failure in hoeing farmland but also provides a guideline to address this problem.\n\u2022 Self-validation. By presenting inventory changes post-action to the LLM critic, we empower it to validate whether the skill has achieved its subgoal, eliminating the need for manual checks.\n\u2022 Self-reflection. Simply confirming the completion of a subgoal is often inadequate for correcting planning errors. The LLM critic also serves as an analyst, deducing the cause of task failure by evaluating the current state of the agent and its environment. It then offers a critique, suggesting a more efficient strategy for task completion."}, {"title": "3 Fine-tune Minecraft LLM", "content": "To improve the performance of LLM-based agents in the Minecraft environment, we fine-tune the LLaMA-3 model [37] using a large-scale Question-Answering (Q&A) dataset with 390k+ instruction entries sourced from the Minecraft Wiki. ODYSSEY presents an effective procedure for converting a foundation model into a domain-specific model in Minecraft, which involves dataset generation, model fine-tuning, and model evaluation. The detailed descriptions can be found in Appendix B.\nDataset Generation. We develop an LLM-assisted method to generate an instruction dataset for Minecraft. We crawl and organize relevant content from the Minecraft Wiki, excluding non-essential sections like history. The data is then formatted uniformly. Based on this, we employ GPT-3.5-Turbo [27] with customized prompts to automatically generate diverse Q&A pairs, categorized into short, normal, long, and boolean answers, resulting in 390k+ instruction entries. In contrast, the Wiki dataset released by MineDojo [10] only collects Minecraft Wiki pages, without refining the content and generating Q&A pairs for model training. STEVE [51] introduces a non-public dataset with 20k+ Q&A pairs, which is smaller than our dataset in terms of scale and diversity.\nModel Fine-tuning. We use the LoRA method [15] for model fine-tuning. LoRA is a parameter-efficient training method that introduces small, trainable low-rank matrices to adapt a pre-trained neural network, enabling targeted updates without retraining the entire model. We employed LoRA to fine-tune the LLaMA-3-8B-Instruct and LLaMA-3-70B-Instruct models using the specially generated Minecraft dataset, resulting in the new models termed MineMA-8B and MineMA-70B, respectively.\nModel Evaluation. In the context of Minecraft, questions are often open-ended and can yield a variety of answers; therefore, conventional evaluation metrics [21, 21, 28] may fall short. Meanwhile, common benchmarks [14, 38, 39] are not suitable for assessing the capabilities of expert models. Thus, we employed GPT-4 [1] to generate two Multiple-Choice Question (MCQ) datasets based on different themes and keywords related to Minecraft. These datasets can quantitatively evaluate the domain-specific expertise of models."}, {"title": "4 Open-World Benchmark", "content": "ODYSSEY introduces a new open-world benchmark to evaluate the planning and exploration capabilities of agents in Minecraft. This benchmark comprises three types of innovative evaluation tasks, including thousands of long-term planning tasks, tens of dynamic-immediate planning tasks, and one autonomous exploration task. The ODYSSEY benchmark offers a standardized framework to evaluate autonomous agents across a range of open-world challenges, ensuring consistent and comparative assessments of the agent performance. Please refer to Appendix C for more details."}, {"title": "4.1 Long-term Planning Task", "content": "We design a suite of combat tasks to assess the long-term planning capabilities of agents, where agents should plan to craft appropriate weapons and equipment to defeat different monsters. These combat tasks can be divided into single-type and multi-type monster tasks. For the single-type tasks, we choose 5 unique monsters, each with its own attack styles, movement patterns, and hostility levels. For the multi-type tasks, we focus on typical monster groupings encountered in the game. In these tasks, agents must directly generate a long-term plan in one go, detailing the sequence of crafting different weapons and equipment in response to the assigned combat task. The agent's performance is evaluated based on the remaining health and the consumed time. After combat, agents can iteratively optimize the previous plan using battle outcomes to improve performance in subsequent rounds."}, {"title": "4.2 Dynamic-immediate Planning Task", "content": "The dynamic-immediate planning tasks require agents to dynamically generate and execute plans based on immediate environmental feedback. Thus, we design a suite of farming tasks, where agents engage in activities like planting, harvesting, and animal husbandry. In these tasks, agents are expected to adapt their strategies based on real-time feedback like nearby resources and animals. Task completion time and success rate are adopted for agent performance evaluation."}, {"title": "4.3 Autonomous Exploration Task", "content": "To test the exploratory capability of agents within open-world settings, we design an autonomous exploration task in Minecraft. In this task, agents are required to determine their subsequent objectives and execute the appropriate skills based on the game context. The exploration task involves discovering and utilizing resources, while adapting to unexpected events such as encounters with hostile monsters. Agents must adapt to these challenges by developing strategies for resource management and task prioritization. The performance metrics include the number of distinct items obtained, the total items crafted, the recipes and advancements (R&A) unlocked, and the distance traveled."}, {"title": "5 Experiments", "content": "To demonstrate the effectiveness of the proposed ODYSSEY framework, we conduct experiments on several basic programmatic tasks and the open-world benchmark. We aim to answer the following questions: (1) Can the open-world skill library improve the efficiency of agents in Minecraft? (Sec. 5.1). (2) How well do agents with different LLMs perform on the open-world benchmark tasks? (Sec. 5.2). (3) What is the contribution of different components of the ODYSSEY agent to its overall performance? (Sec. 5.3).\nOur simulation environment is built on top of MineDojo [10] and Voyager [40], which provides a text-based interface for agents to interact with Minecraft. In addition to using GPT-3.5 and GPT-4 for one-time generation of fine-tuning data, we conduct all experiments based on the open-source LLaMA-3 model. The experimental costs are considerably reduced compared to methods that rely on GPT-4 for skill generation [40, 42]. The detailed experimental settings are provided in Appendix D."}, {"title": "5.1 Open-World Skill Library", "content": "To demonstrate the superior capability of our open-world skill library in Minecraft, we first tested it on 5 basic programmatic tasks from previous studies [53]. We conducted 120 repeated experiments on each task and recorded the average completion time for each task as well as the success rates at different time points. We report the performance of baselines using the results reported from their own paper. The experimental results in Tab. 1 and Tab. 2 show that our skill library significantly improved the success rate and efficiency of the above tasks, surpassing previous studies."}, {"title": "5.2 Open-World Benchmark", "content": "We evaluate the LLM-based agent on 8 long-term planning tasks, 4 dynamic-immediate tasks, and the autonomous exploration task from the ODYSSEY benchmark. These tasks cover a variety of complex gaming scenarios and require diverse solutions."}, {"title": "5.2.1 Long-term Planning Task", "content": "For the long-term planning tasks, the agent is required to plan a list of weapons and equipment to craft based on the strength of different monsters, with the goal of defeating the monster in as short a time as possible. We compare the original LLaMA-3-8B model with the fine-tuned MineMA-8B model on these tasks. Moreover, we also evaluate the performance of single-round and multi-round planning. The single-round test results in Tab. 3 show that the fine-tuned MineMA-8B model outperforms the original LLaMA-3-8B model in terms of success rate and time efficiency, albeit at the cost of more LLM iterations. The multi-round test results in Fig. 4 show that the multi-round planning strategy significantly improves the time efficiency of the agent, indicating that the agent can iteratively optimize its plan based on the outcomes of previous battles to enhance its performance in subsequent rounds."}, {"title": "5.2.2 Dynamic-immediate Planning Task", "content": "For the dynamic-immediate planning tasks, the agent is required to dynamically generate and execute plans based on immediate environmental feedback. We evaluate the performance of the MineMA-8B and the MineMA-70B model to investigate the impact of model size on task performance. As shown in Tab. 4, the MineMA-70B model shows superior performance compared with the MineMA-8B model. Across all tasks, MineMA-70B demonstrates higher success rates and generally lower average execution times and LLM iterations."}, {"title": "5.2.3 Autonomous Exploration Task", "content": "In the autonomous exploration task, the agent is required to explore the Minecraft world freely without any specific goals. We evaluate the performance of the LLaMA3-8B and the MineMA-8B model on this task. As shown in Fig. 5, both models achieve promising results, indicating that the agent can autonomously explore the Minecraft world without specific goals. Our agent with the MineMA-8B model can achieve the consistent performance compared with Voyager [40] with GPT-4 and even surpasses Voyager with GPT-3."}, {"title": "5.3 Ablation Study", "content": "We conduct ablation studies on two core components of the ODYSSEY agent, including the LLM planner and the open-world skill library. The results are shown in Fig. 5. In the autonomous exploration task, the LLM planner is responsible for generating a comprehensive plan based on the open-world skill library. The ablation study demonstrates that the planner is indispensable for the agent to effectively navigate the complex Minecraft environment. Additionally, our experimental results indicates that the absence of the open-world skill library significantly degrades performance. Without the open-world skill library, the 8B LLM model alone is largely incapable of generating executable codes for the agent. This underscores the critical role of the open-world skill library in enabling the agent to perform complex tasks within the open-world setting of Minecraft."}, {"title": "6 Related Works", "content": "Minecraft agents have been widely studied in recent years to test the capabilities of autonomous agents in open-world environments. Previous works focused on training Minecraft agents with hierarchical Reinforcement Learning (RL) [13, 23, 24, 26, 36] or imitation learning [3, 4, 20], which are extensively used in the MineRL [12] competition to solve the ObtainDiamond task. With the rapid development of LLMs, numerous studies leverage LLMs to enhance agent capabilities [11, 40, 42, 49, 51, 52, 53]. Among these, several works [8, 18, 30, 43, 48] employ LLMs to guide skill learning in Minecraft, enabling agents to act in a human-like way. However, these methods mainly focus on learning primitive skills from scratch, lacking a reusable skill library. Voyager [40] builds a skill library by allowing the LLM to write its own skills. These skills are also learned from a narrowly defined set of primitive skills, resulting in unstable skill generation and high learning costs. In contrast, ODYSSEY provides an open-world skill library with both primitive skills and compositional skills, enabling LLM-based agents to efficiently and stably generate complex policies for broader exploration and more complex tasks.\nOpen-world benchmarks have gained considerable attention from research communities [5, 6, 17, 34, 35]. Minecraft, with its diverse tasks and mature game mechanics, has emerged as an ideal test-bed for open-world tasks. Built on Minecraft, MineRL [12] implements a simulation environment for agent learning. MineDojo [10] further extends MineRL with thousands of diverse tasks. MCU [22] collects a variety of atom tasks, offering a method to generate infinite tasks by combining the atom tasks. However, existing benchmarks mainly focus on providing basic programmatic tasks to evaluate agents learned from scratch. Our ODYSSEY benchmark is built on top of the skill library, enabling the agents to bypass basic programmatic tasks and focus on complex open-world challenges."}, {"title": "7 Conclusion", "content": "This work proposes ODYSSEY to empower agents with open-world skills in the Minecraft environment. We introduce (1) an interactive agent endowed with an extensive open-world skill library comprising various primitive skills and compositional skills; (2) a fine-tuned LLaMA-3 model, trained on a large-scale question-answering dataset sourced from the Minecraft Wiki; (3) a new open-world benchmark that encompasses tasks requiring long-term planning, dynamic-immediate planning, and autonomous exploration. The public availability of all datasets, model weights, and code will facilitate future research in the development of autonomous embodied agents. We hope that ODYSSEY will inspire further innovation and progress in the field of autonomous agent development."}, {"title": "Limitations and Future Works", "content": "The proposed open-world skill library enables the use of open-source LLMs as the foundation for agents to call upon skills, avoiding the high costs associated with previous work using GPT-4 [18, 30, 40]. However, the open-source LLMs are prone to generating hallucinations, leading to a decrease in agent performance. Thus, our future research will focus on employing retrieval-augmented generation to improve LLMs in Minecraft. Additionally, the skill library is still text-based, which limits its functionality in tasks requiring visual information. We plan to integrate visual processing capabilities into the skill library to expand its capabilities."}, {"title": "A.1 Open-World Skill Library", "content": "A.1.1 Primitive skills\nPrimitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript APIs [29], divided into two main categories: 32 operational skills and 8 spatial skills. In addition to Voyager's 18 operational skills [40], 14 operational skills implemented by us are presented as follows:\n\u2022 plantSeeds(bot, type): Let the agent find the nearest farmland and plant a particular kind of seed.\n\u2022 feedAnimals (bot, type, count=1): Let the agent find the nearest animals of a particular species and numbers and feed them with the appropriate food.\n\u2022 killAnimal(bot, type): Let the agent kill a particular kind of animal using the best sword in its inventory.\n\u2022 killMonsters(bot, type, count=1): Let the agent kill monsters nearby of a particular species and numbers using the best sword in its inventory.\n\u2022 cookFood (bot, type, count=1): Let the agent cook food of a particular kind and numbers using coal and furnace.\n\u2022 eatFood (bot, type): Let the agent eat a particular kind of food.\n\u2022 equipArmor (bot): Let the agent equip the best armor(helmet, chestplate, leggings and boots) in its inventory.\n\u2022 equipSword/Pickaxe/Axe/Hoe/Shovel (bot): Let the agent equip the best corresponding tool in its inventory.\n\u2022 getLogs/PlanksCount(bot): Return the number of logs/planks (counted in seven different categories) in the inventory.\nAdditionally, we pioneer 8 spatial skills that Voyager [40] lacks, allowing for environmental interactions based on the agent coordinates. The spatial skills implemented by us are presented as follows:\n\u2022 findSuitablePosition(bot): Let the agent find the best nearby location for placing devices such as a crafting table or furnace. The block must be minecraft: air and at least one adjacent reference block exists.\n\u2022 checkAdjacentBlock(bot, types, x, y, z): Check blocks adjacent to the block at position (x,y,z). Return true if any of the adjacent blocks match the specified types.\n\u2022 checkBlockAbove(bot, type, x, y, z): Check block above the block at position (x,y,z). Return true if the above block matches the specified type.\n\u2022 checkBlocksAround(bot, type, x, y, z): Check blocks around the block at position (x,y,z).Return true if any of the around blocks match the specified type.\n\u2022 checkNearbyBlock(bot, types, x, y, z, r): Check blocks in a radius around the block at position (x, y, z). Return true if any block within the radius matches the specified types.\n\u2022 checkNoAdjacentBlock(bot, types, x, y, z): Check adjacent blocks of block at position (x,y,z). Return true if not all adjacent blocks are within the specified types.\n\u2022 goto(bot, x, y, z): Let the agent go to the corresponding position (x,y,z) until it reaches the destination.\n\u2022 getAnimal (bot, type, x, y, z): Let the agent attract a particular kind of animal to a particular position (x,y,z) with the appropriate food."}, {"title": "A.1.2 Compositional skills", "content": "All compositional skills are encapsulated by the Mineflayer APIs and the aforementioned primitive skills, while higher-level compositional skills recursively call lower-level ones. Fig. 6 illustrates the nested relationships among the 13 skills required to complete the mineDiamond task. We classify all compositional skills into main types as follows:\n\u2022 mineX(bot): Equip the agent with the appropriate tools and find the nearest specific block to mine it.\n\u2022 craftX(bot): Let the agent collect the necessary materials and check if the crafting table exists in the inventory (if needed), to craft a specific tool or something.\n\u2022 smeltX(bot): Let the agent check the furnace and fuel, and smelt the specified materials.\n\u2022 collectX(bot): Similar to mineX, used to collect multiple quantities of a certain item.\n\u2022 makeX (bot): Similar to craftX, used to make food.\n\u2022 cookX (bot): Similar to smeltX, used to cook food.\n\u2022 plantX(bot): Let the agent check the inventory for seeds, collect them if not present, and plant them in nearby farmland.\n\u2022 breedX(bot): Let the agent check the inventory for the required corresponding feed, find the nearest two animals, feed them, and facilitate their breeding.\n\u2022 killX(bot): Let the agent equip the best sword in the inventory, find the nearest specific animal or monster, kill it, and collect the dropped items.\n\u2022 placeX (bot): Let the agent place an item at its current or a nearby suitable location, and if the item is not in inventory, craft it first.\nAdditionally, there are several other compositional skills aimed at executing specific behaviors, such as catchFish, hoeFarmland, shearSheep, takeAndMoveMinecart."}, {"title": "A.2 LLM Planner", "content": "ODYSSEY relies on LLMs to generate language-based plans. In our Minecraft experiment, we propose three novel tasks (long-term planning task, dynamic-immediate planning task and autonomous exploration task) for agents to explore. Therefore we designate three types of prompt messages for them respectively, offering LLM Planner the ability to generate different routines on different tasks. The format of the prompt is presented thus:\n\u2022 \"SYSTEM\" role: A high-level instruction that gives directions to the model behavior. It sets an overall goal for the interaction and provides external information.\n\u2022 \"USER\" role: Detailed information like environment, states and achievements of the agent will be provided to the planner for the next immediate subgoals.\n\u2022 \"ASSISTANT\" role: A guideline generated by the planner."}, {"title": "A.2.1 Long-term Planning", "content": "We design a suite of combat tasks to assess the long-term planning capabilities of agents, where the LLM Planner should plan to craft appropriate weapons and equipment to defeat monsters.\nThe input prompt to LLM consists of several components:\n\u2022 Ultimate goals: The monsters that need to be defeated.\n\u2022 Directives and behavior constraints that guarantee the proposed task is achievable and verifiable.\n\u2022 Information of last combat: This ensures that the prompt is exposed to increasing amounts of information over the combat and thus progressively advances towards more efficient plans.\nLong-term Planning System Prompt\n-Overall goals\u2014\nYour goal is to generate the plan that can defeat all monsters while using the shortest time. So, more is not always better when proposing your plan list.\n-External information-\nIn Minecraft, combating with monsters requires weapons and armor. The weapon options are limited to \"sword\", while the armor includes \"helmet\", \"chestplate\", \"leggings\", and \"boots\". The materials for swords range from low to high level: wooden swords, stone swords, iron swords, and diamond swords; The materials for armor range from low to high level: iron, diamond. The higher the material level, the greater the attack damage of the weapon and the better the protection effect of the armor. However, the higher the material level, the more time it costs to collect.\nTips: Wooden, stone, iron and diamond are the only levels of sword; iron and diamond are the only levels of armors; helmet, chestplate, leggings and boots are the only types of armors; do not generate information that doesn't relate to them.\nAfter each round of combat, I will give you:\nEquipment obtained from last round:\nHealth after last combat:\nCritique:\nMonster: The monsters you need to defeat.\n-Directions-\nThe critique (if any) will tell you the subgoal list from the previous round and whether you should trim or add to it. Remember to refer to the critique to adjust your task list. Next, you will start a new combat task, the last round of equipment and health is only for planning reference, not related to the current round. Plan from scratch!\n-Behaviour constraints-\nYou must follow the following criteria:\n1) Return a Python list of subgoals that can be completed in order to complete the specified task.\n2) Each subgoal should only start with \"craft\"! do not propose any other type of skills!\n3) Each subgoal should follow a concise format \"craft [material type] [equipment type]\".\nYou should only respond in JSON format as described below:\n[\"subgoall\", \"subgoal2\", \"subgoal3\", ...]\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no single quotes, etc.\nAfter finish collecting weapons and equipment, we also plan an efficient routine to combat with monsters for higher survival rates. For example, monsters that are more harmful and aggressive should be placed in a higher priority. The full prompt for re-ranking the combat order of monsters is shown below.\nComabt Order System Prompt\nYou are a helpful assistant that generates the order of fighting monsters to defeat all monsters specified by me.\nI'll give you a list of monsters, and you need to rearrange the order of monsters according to how hard it is to beat them.\nYou should give priority to monsters that attack the player and do more damage, while monsters that don't actively attack the player or do less damage should be left behind. Make sure your list includes all the monsters in your task. The output format must be exactly same as the input, including the underline.\nIf your task is to combat a single type of monsters, return a list containing only that monster as well.\nYou should only respond in JSON format as described below: [\"quantity monster1\", \"quantity monster2\", \"quantity monster3\", ...]\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no single quotes, etc."}, {"title": "A.2.2 Dynamic-immediate Planning", "content": "In this kind of task, agents are expected to adapt their plans based on the real-time feedback like nearby resources and animals.\nThe input prompt to LLM consists of the following components:\n\u2022 Ultimate goals: A suite of farming tasks, such as planting, harvesting, and animal husbandry.\n\u2022 The current states of agent: hunger and health values, position and nearby entities, etc.\n\u2022 Achievements of the agent: the current inventory and unlocked equipment, as well as previously successful and failed tasks.\nDynamic-immediate Planning System Prompt\n-Overall goals-\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft. My ultimate goal is to \"goals\".\nMake sure that the proposed task is related to the ultimate goal, and do not propose unrelated tasks!\n-Directions-\nYou need to plan step by step towards your ultimate goal, so propose necessary pre-requisite tasks first.\nFor example, \"craft hoe\" before \"hoe farmland\", \"collect [type] seeds\" and \"hoe farmland\" before \"plant seed\", \"kill [animalType]\" before \"cook meat\", \"craft shears\" before \"shear sheep\", \"craft bucket\" before \"collect milk\".\nPropose the current task only when you ensure that you have all the necessary dependent items in inventory.\nDon't ask for repetitive tasks. If you already have an item in your inventory, try not to collect it repeatedly.\nFor example, when you already have a hoe in your inventory, propose \"hoe farmland\" instead of \"craft hoe\" again.\n-External information\u2014\nI will give you the following information:\nUltimate goal:\nReference:\nBiome:\nNearby blocks:\nOther blocks that are recently seen:\nNearby entities (nearest to farthest):\nHealth: Higher than 15 means I'm healthy.\nHunger: Higher than 15 means I'm not hungry.\nInventory (xx/36):\nLogs: The execution logs in last task, you can refer to it to propose next task. Completed tasks so far:\nFailed tasks that are too hard:\n-Behaviour constraints-\nYou must follow the following criteria:\n1) Please be very specific about what resources I need to collect, what I need to farm, or what animals I need to breed/kill.\n2) The next task should follow a concise format, such as \"craft [item]\", \"breed/kill [animal type]\", \"cook/eat [food type]\", \"plant [seed type] seed\" or some specific action \"shear sheep\", \"collect milk\". Do not propose multiple tasks at the same time. Do not mention anything else.\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"Based on the information I listed above, do reasoning about what the next task should be\",\n\"task\": \"The next task\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no single quotes, etc."}, {"title": "A.2.3 Autonomous Exploration", "content": "In this task, the agent is required to explore the Minecraft world freely without any specific goals. This poses a great challenge to the planner for maximal exploration. It should propose suitable tasks based on the current state and environment, e.g., plan to obtain sand or cactus before wood if it finds itself in a desert rather than a forest. The input prompt to LLM consists of several components:\n\u2022 Guidelines encouraging diverse tasks.\n\u2022 The current states of agent: hunger and health values, position and nearby entities, etc.\n\u2022 Achievements of the agent: the current inventory and unlocked equipment, as well as previously successful and failed tasks.\nAutonomous Exploration System Prompt\n-Overall goals-\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft.\nMy ultimate goal is to discover as many diverse things as possible, accomplish as many diverse tasks as possible and become the best Minecraft player in the world.\n-External information-\nI will give you the following information:\nBiome:\nTime:\nNearby blocks:\nOther blocks that are recently seen:\nNearby entities (nearest to farthest):\nHealth: Higher than 15 means I'm healthy.\nHunger: Higher than 15 means I'm not hungry.\nPosition:\nEquipment: If I have better armor in my inventory, you should ask me to equip it.\nInventory (xx/36):\nChests:\nCompleted tasks so far:\nFailed tasks that are too hard:\n-Directions-\nYou must follow the following criteria:\n1) You should act as a mentor and guide me to the next task based on my current learning progress.\n2) Please be very specific about what resources I need to collect, what I need to craft, or what mobs I need to kill.\n3) The next task should follow a concise format, such as \"Mine [block]\", \"Craft [item]\", \"Smelt [item]\", \"Kill [mob]\", \"Cook [food]\", \"Equip\" etc. It should be a single phrase. Do not propose multiple tasks at the same time. Do not mention anything else.\n4) The next4) The next task should be novel and interesting. I should look for rare resources, upgrade my equipment and tools using better materials, and discover new things. I should not be doing the same thing over and over again.\n5) Don't propose tasks that have already completed once or failed more than three times!\n6) Do not ask me to build or dig shelter even if it's at night. I want to explore the world and discover new things. I don't want to stay in one place.\n7) Tasks that require information beyond the player's status to verify should be avoided. For instance, \"Placing 4 torches\" and \"Dig a 2x1x2 hole\" are not ideal since they require visual confirmation from the screen. All the placing, building and trading tasks should be avoided. Do not propose task starting with these keywords.\n8) For wood-related tasks, you don't need to emphasize the type of wood, just propose \"mine log\" or \"craft planks\".\n-Behaviour constraints-\nYou should only respond in JSON format as described below: {\n\"reasoning\": \"Based on the information I listed above, do reasoning about what the next task should be.\",\n\"task\": \"The next task.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no single quotes, etc."}, {"title": "A.3 LLM Actor", "content": "In actor, the mapping from higher language subgoals S to lower executable codes is implemented through query context encoding and similarity retrieval. We employ the following prompt during the generation of query context (Question-Answer pairs).\nQuery Context Prompt\nSYSTEM:\nYou are a helpful assistant that answer my question about Minecraft.\nI will give you the following information:\nQuestion:\nYou will answer the question based on the context (only if available and helpful) and your own knowledge of Minecraft.\n1) Start your answer with \"Answer: \".\n2) Answer \"Answer: Unknown\" if you don't know the answer.\nUSER:\nHow to complete S in Minecraft?\nAfter recalling the top-10 relevant skills with the highest scores, we require LLM to determine the most appropriate code for execution within the environment based on their description. The full prompt of code selection is shown in the following.\nSkill Selection System Prompt\nYou are a helpful assistant that decides Mineflayer javascript code to complete any Minecraft task specified by me.\nI will give you\nTask: The task I need to complete in this stage.\nPrograms: The description of relevant programs that may use to complete the task. Program used in the last round:\nCritique:\nYou will choose only one program based on the program description and critique. You should only respond in the format as described below: {\n\"program\": \"your selected program name\",\n\"reason\": \"Reason you choose the program.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no single quotes, etc.\nPlease ensure that the program name you output should be exactly the same (case-inclusive) as the information provided!"}, {"title": "A.4 LLM Critic", "content": "The LLM critic should evaluate the success of the executed actions by comparing expected outcomes with actual results, thereby providing valuable critiques for refining strategies in subsequent iterations. We design a chain-of-thought [45] prompting mechanism: We first require LLM to reason about the task's success or failure, then output a boolean variable representing the execution result, and finally provide a critique to the agent if the task fails."}, {"title": "B Fine-tune Minecraft LLM", "content": "For detailed code, datasets, and models used in this section, please visit ODYSSEY for more information. The overall fine-tuning framework is shown in Fig. 7."}, {"title": "B.1 Dataset Generation", "content": "The code used in this section can be found on ODYSSEY. The dataset produced in this section has been publicly available at: https://huggingface.co/datasets/Aiwensile2/Minecraft_QA-pairs_Instruction_Dataset."}, {"title": "B.1.1 Data Cleaning", "content": "For this study, we select two primary sources of information, the Minecraft Fandom Wiki (https://minecraft.fandom.com/wiki/Minecraft_Wiki) and the Minecraft Wiki (https://minecraft.wiki/).\nFor the Minecraft Fandom Wiki, we first crawl the content of all its pages and perform a preliminary filtering on the resulting pages, removing pages that were obviously useless for our fine-tuning task, such as game version information, and obtaining a series of JSON files. These files still contain a significant amount of information that we do not need to create the dataset, so we carry out a data cleaning process, extracting the text and table content of the original pages, storing them in a series of TXT files, with each page corresponding to a TXT file. Through the above method, we obtain the cleaned TXT format page information.\nFor Minecraft Wiki, we exclude a few categories that are useless for our fine-tuning task, such as History, and crawl the content of all other categories' pages. Similar to the process for Minecraft Fandom Wiki, these pages also contain a lot of information that we didn't need. We remove some irrelevant sections such as Achievements, Advancements, and History, and only retained the text and table content of other useful sections. After that, we store the processed data in markdown files and perform segmentation operations based on the number of words in the file. We ensure that the word count of the segmented file did not exceed the set limit without cutting the section block, thus obtaining clean MD format page information."}, {"title": "B.1.2 Q&A Pairs", "content": "In this study, we design four types of Q&A pairs for our training dataset as follows:\n\u2022 Short Answer Q&A: Questions that can be directly answered with a word or phrase, such as names, statistics, etc.\n\u2022 Long Answer Q&A: Questions that require one or more complete sentences as answers. These questions are suitable for explanatory or definitional responses, where a detailed explanation or a clear definition is needed to fully address the question.\n\u2022 Bool Answer Q&A: Questions that can be answered with a simple True or False.\n\u2022 Normal Answer Q&A: QA pairs with no specific form restrictions, used to supplement the dataset and ensure its comprehensiveness.\nSome Q&A pair examples are shown as follows:\nShort Answer Q&A\nQ: What mobs spawn naturally in the desert biome during the day in Minecraft?\nA: During the day in Minecraft, zombies in desert biomes are replaced by husks, which inflict the Hunger effect and do not burn in sunlight.\nLong Answer Q&A\nQ: How does the use of light sources like torches and lava help in preventing mob spawning in Minecraft?\nA: In Minecraft, placing light sources such as torches, lava, glowstone, jack o'lanterns, sea lanterns, and lanterns helps prevent hostile mob spawning. Mobs generally spawn in low light conditions, and these light sources provide enough illumination to raise the light level above the threshold needed for mob spawning. Torches are the most commonly used due to their easy availability and simplicity in creating a well-lit perimeter around an area. Lava is also effective but must be used cautiously as it can ignite nearby flammable materials.\nBool Answer Q&A\nQ: Can skeletons shoot through bamboo?\nA: F\nNormal Answer Q&A\nQ: What are piglins in Minecraft?\nA: Piglins are neutral mobs found in the Nether. They become hostile towards players unless the player is wearing at least one piece of golden armor. They can be bartered with using gold ingots for various items, and they wield either a golden sword or a crossbow for melee or ranged attacks, respectively."}, {"title": "B.1.3 Instruction Datasets Generation Prompts", "content": "We design four types of prompts for four types of Q&A pairs to generate instruction datasets by using GPT-3.5-Turbo. Here are our prompts.\nShort Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be used to train a large language model designed to address questions posed by users regarding the game Minecraft, and from that, you will generate question-and-answer data samples, each with a prompt/response pair.\nYou will do so in this format:\nprompt\nprompt_goes_here\nresponse\nresponse_goes_here\nYour task is to generate at least 30 examples. Make sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 30 question-and-answer examples, and you should generate questions within the provided user text that can be directly answered with a word or phrase, such as dates, names, statistics, etc. This involves identifying specific, concise information within the text that can be succinctly responded to, ensuring that the answers are clear and directly related to the questions asked. And you will do so in this format:\nprompt\nprompt_goes_here\nresponse\nresponse_goes_here\nPlease generate as many question and answer pairs as possible. Here is the user content: {user_content}\nLong Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be used to train a large language model designed to address questions posed by users regarding the game Minecraft, and from that, you will generate question-and-answer data samples, each with a prompt/response pair.\nYou will do so in this format:\nprompt\nprompt_goes_here\nresponse\nresponse_goes_here\nYour task is to generate at least 20 examples. Make sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 20 question-and-answer examples. Identify questions within the provided user text that require one or more complete sentences as answers. These questions should be suitable for explanatory or definitional responses, where a detailed explanation or a clear definition is needed to fully address the question. This involves crafting answers that are comprehensive and informative, ensuring they adequately explain or define the subject matter in question. And you will do so in this format:\nprompt\nprompt_goes_here\nresponse\nresponse_goes_here\nPlease generate as many question and answer pairs as possible. Here is the user content: {user_content}\nBool Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be used to train a large language model designed to address questions posed by users regarding the game Minecraft, and from that, you will generate question-and-answer data samples, each with a prompt/response pair.\nYou will do so in this format:\nprompt\nprompt_goes_here\nresponse\nresponse_goes_here\nYour task is to generate at least 10 examples. Make sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 10 question-and-answer examples. Look for questions within the provided user text that can be answered with a simple True or False. This task involves pinpointing statements or queries within the text that lend themselves to binary responses, ensuring that the answers are straightforward and unambiguous, clearly indicating whether the statement is true or false based on the information available. And you will do so in this format:\nprompt\nprompt_goes_here\nresponse"}, {"title": "B.2 Model Fine-tuning", "content": "The code used in this section can be found on \u2611 ODYSSEY. MineMA-8B and MineMA-70B series of models have been publicly available at: https://huggingface.co/Aiwensile2/MineMA-8B.\nIn this study, we use the instruction dataset with 390,317 instruction entries mentioned above to fine-tune the Minecraft Q&A expert models, using the LoRA fine-tuning method. We name the series of fine-tuned models MineMA. The resulting models include MineMA-8B-v1, MineMA-8B-v2, MineMA-8B-v3, derived from the base model LLama-3-8B-Instrument, and MineMA-70B-v1, MineMA-70B-v2, derived from the base model LLama-3-70B-Instrument. MineMA-70B series of models are fine-tuned on four A6000 GPUs, while the remaining models are fine-tuned on a single A6000 GPU each. Among the models, MineMA-8B-v1 and MineMA-70B-v1 only undergo one round of training without an evaluation process, while the other models are trained with multiple rounds that incorporate an evaluation procedure. We use the EarlyStopping method to halt the training process when there is no reduction in the evaluation loss over a series of evaluations, and finally save the model which has the best performance. Some training parameters are shown in Tab. 5."}, {"title": "B.3 Model Evaluation", "content": "The code used in this section can be found on ODYSSEY. The datasets used in this section have been publicly available at: https://huggingface.co/datasets/Aiwensile2/Minecraft_MCQ_Datasets."}, {"title": "B.3.1 Evaluation datasets creating process", "content": "In this study, we utilize GPT-4 to create two evaluation MCQ datasets: a multi-theme MCQ dataset and a Wiki-based MCQ dataset. For the multi-theme MCQ dataset, we first summarize the following Minecraft content themes:\nGame Basics\nBlocks and Items: Basic blocks, special blocks, tools, weapons, armor, etc.\nSurvival Mechanics: Health, hunger, experience levels, death and respawn, etc.\nWorld Exploration\nBiomes: Characteristics of different biomes, generated structures, unique resources, etc.\nTerrain and Landforms: Features and resource distribution of different terrains.\nMobs and Interactions\nMobs: Characteristics and behaviors of passive, neutral, and hostile mobs.\nCombat System: Monster types, combat tactics, weapons and equipment, enchantments, potions, etc.\nTrading and Villagers: Villager professions, trading mechanics, village structures, etc.\nSurvival Skills\nResource Gathering: Methods of obtaining various resources and their uses.\nCrafting and Production: Usage of crafting tables, furnaces, etc., equipment crafting and upgrading.\nFarming and Animal Husbandry: Crop planting, animal breeding, automated farms, etc.\nBuilding and Creativity\nBuilding Styles: Various building styles and key points.\nBuilding Techniques: Symmetry, proportion, detail handling in construction, etc.\nInterior Decoration: Interior design, lighting, item placement, etc.\nRedstone Mechanics: Redstone components, circuit design, automated devices, etc.\nSpecial Dimensions\nThe Nether: Peculiarities of the Nether, unique blocks and mobs, special structures, etc.\nThe End: Characteristics of the End, Ender Dragon, cities, ships, etc.\nAdventure and Exploration: Special generated structures like ocean monuments, woodland mansions, ruins, fortresses, etc.\nThen, we list different numbers of keywords for each theme based on the amount of relevant knowledge content. According to the amount of information related to each keyword, we match a number for each keyword, representing the number of multiple-choice questions to be generated based on that keyword. After preparing the groundwork, we use GPT-4 to generate the multi-theme MCQ dataset, totaling 1,050 multiple-choice questions. The relevant prompts are shown below, taking the generation of multiple-choice questions in the Special Dimensions theme as an example:\nSystem Message\nYou are an expert in generating Minecraft quiz questions. Your task is to create multiple-choice questions about the game Minecraft based on the theme of \"Special Dimensions\" and the provided keywords. The introduction of the theme of \"Special Dimensions\" is as follows: Special Dimensions:\nThe Nether: Peculiarities of the Nether, unique blocks and mobs, special structures, etc. The End: Characteristics of the End, Ender Dragon, cities, ships, etc. Adventure and Exploration: Special generated structures like ocean monuments, woodland mansions, ruins, fortresses, etc. Provide four answer options labeled A, B, C, and D. Only one option should be correct. After the question and options, state the correct answer. Please format the output as follows: Difficulty: Easy/Medium/Hard\nTopic: Special Dimensions\nKey Word: text\nQuestion: Question text\nOptions: A.text B.text C.text D.text\nCorrect Answer: A/B/C/D\nEnsure that the difficulty distribution of the questions and options is reasonable, and the answers should be detailed and informative.\nUser Message\nPlease generate some Minecraft multiple-choice questions based on the following 5 keywords, covering three difficulty levels: simple, moderate, and difficult. The number after the keyword represents how many multiple-choice questions to generate based on this keyword. Keywords: {keywords_go_here}\nEnsure that the Q&A content is rich and accurate, and test the player's understanding of the game. Provide a balanced combination of simple, medium, and difficult questions. Generate each question and answer in the given format. Here is an example:\nExample:\nDifficulty: Hard\nTopic: Special Dimensions\nKey Word: End Ship\nQuestion: What exclusive item can be found in the End Ship in Minecraft?\nOptions: A. Netherite B. Dragon Egg C. Elytra D. Beacon\nCorrect Answer: C"}, {"title": "Open-World Benchmark", "content": "In Minecraft, there are a total of 35 hostile creatures. We conducted experiments on both single-monster combat tasks and combined combat tasks (up to three types of monsters), resulting in thousands of different tasks that can all be implemented through the interfaces we provided.\n\u2022 combatEnv (bot, h, r, y): Generates a hollow rectangular arena with a height of h and a square base with side length 2r at altitude y, positioning the agent at the exact center of this enclosed space. This configuration ensures controlled conditions for evaluating combat scenarios, especially considering not being influenced by naturally spawning monsters.\n\u2022 summonMob (bot, n = 1, r, type): Facilitates the spawning of hostile creatures around the bot. It randomly positions n monsters within a designated range (r to 2r along the x and z axes) from the bot, allowing for the creation of varied combat tasks and enabling comprehensive testing of bot performance under different tactical challenges."}, {"title": "C.2 Dynamic-immediate Planning Task", "content": "In Minecraft, many farming tasks require interaction with the environment and dynamic planning. We propose a series of tasks that can be accomplished through our skill library, including hoeing farmland, planting seeds, harvesting crops, making food, slaughtering animals, cooking meat, feeding and breeding animals, among others. For example, in the task cook meat, if the agent is informed that there is a chicken nearby, it should plan to \"kill one chicken\" rather than anything else. Additionally, in the task milk cow, the agent must simultaneously monitor the appearance of cows in the vicinity and gather materials to craft a bucket to collect the milk."}, {"title": "C.3 Autonomous Exploration Task", "content": "In Minecraft, autonomous exploration is the gameplay approach that most closely mimics how human players engage with the game. To evaluate the diversity of discoveries made by the agent during autonomous exploration, we used \"Distinct Items Obtained\" as the primary evaluation metric. The acquisition of a greater variety of items demonstrates more diverse exploratory behavior by the agent. Additionally, based on statistical information and progress in-game achievements, we calculated supplementary evaluation metrics including the \"Distance Traveled\" by the agent (summing walking, sprinting, climbing, swimming, and other forms of movement), the total number of \"Items Crafted\" (the sum of all types of items obtained by crafting), and \"Recipes and Achievements Unlocked\" (the sum of crafting recipes and game achievements unlocked)."}, {"title": "D Experiments", "content": "D.1 Experimental Details\nWe select the 1.19.4 version of Minecraft as the experimental environment. Within this virtual game world, spatial measurements are determined by blocks, while temporal measurements are dictated by ticks, each lasting 0.05 seconds. A single day-night cycle in the game is 24,000 ticks, equivalent to 20 minutes in the real world, with 10 minutes of daytime, 7 minutes of nighttime, and a 3-minute dawn/dusk transition (when both the sun and moon are visible in the sky). Additionally, the game's weather system randomly transitions between clear, rainy, thunderstorm, and snowy conditions, adding dynamic changes to the environment. Players are born into a randomly generated massive world, covering an area of 30,000,000 blocks \u00d7 30,000,000 blocks, which can be approximately considered an infinite world without boundaries. Players start with no resources and must gather everything from scratch that is beneficial for survival and completing the ultimate goal. When a player character dies, it will respawn randomly within a 32-block radius of the death location on the ground, and any collected items will not be dropped. Agents can connect to the game through local networks or multiplayer servers. We have tested on Ubuntu 20.04, Windows 10, and macOS. In all experiments of the open-world benchmark, the \"MineMA-8B\" refers to \"MineMA-8B-v3\", and the \"MineMA-70B\" refers to \"MineMA-70B-v1\".\nWe use the following Minecraft mods in our experiment. It is important to note that the version of mods must be consistent with the game version, specifically 1.19.4.\n\u2022 Fabric API: Basic Fabric APIs.\n\u2022 Mod Menu: Used to manage all the mods that you download.\n\u2022 Complete Config: Dependency of server pause.\n\u2022 Multi Server Pause: Used to pause the server when waiting for LLM to reply.\n\u2022 Better Respawn: Used for random respawning of player characters.\nConsidering the randomness of resource distribution in the Minecraft world, we ensure that the agent starts from different locations in the game world before each round of experiments. We implemented the respawnAndClear interface to perform some initialization settings.\n\u2022 respawnAndClear(bot): Transport the agent to a new location and clear its inventory, ensuring that the game mode is switched to survival and the game difficulty is switched to peaceful."}, {"title": "D.2 Open-World Benchmark", "content": "In our multi-round Long-term Planning Task, the agent is required to iteratively improve planning based on combat outcomes, aiming for victory with the highest efficiency, take as little time as possible. Specifically, if the agent wins in the previous round, it should streamline its planning in the next round, gathering materials and crafting equipment in less time to enhance time efficiency (reflected in the experimental results as a decrease in time and LLM iterations); conversely, if it loses, it must refine its planning to upgrade the quality of weapons and equipment in the planning list to ensure ultimate success (reflected in the experimental results as an increase in health, or go from losing to winning). Additionally, when calculating experimental results, we compute the average and standard deviation for time, LLM iters (LLM iterations) and the health metric only for victorious outcomes, since a defeat, indicated by health being zero, is not meaningful.\nExample of multi-round combat task\nCombat Task: 1 Skeleton\nPlan list of 1st round: [craft iron sword, craft iron helmet, craft iron chestplate, craft iron leggings, craft iron boots]\nEquipment obtained of 1st round: [iron_helmet, iron_chestplate, iron_leggings, iron_boots, crafting_table, None]\nTime spent on crafting equipment: 15,953 ticks; 8 LLM iters\nRemaining Health after the combat: 14.0 / 20 (victory)\n-streamlining-\nPlan list of 2nd round: [craft iron sword]\nEquipment obtained of 2nd round: [None, None, None, None, iron_sword, None]\nTime spent on crafting equipment: 3,614 ticks; 4 LLM iters\nRemaining Health after the combat: 9.2 / 20 (victory and more efficiently)\n-streamlining\u2014\nPlan list of 3rd round: [craft wooden sword]\nEquipment obtained of 3rd round: [None, None, None, None, wooden_sword, None]\nTime spent on crafting equipment: 416 ticks; 1 LLM iter\nRemaining Health after the combat: 9.0 / 20 (victory and even more efficiently)\nIn our Dynamic-immediate Planning Task, the agent is asked to plan step by step based on environmental information. We calculate the success rate across various tasks, the average execution time and LLM iters as well as their standard deviation (only if successful) as evaluation metrics. It is important to note that skills used in these tasks do not utilize the recursive decomposition mechanism we propose but require the agent to plan the necessary preparatory steps by itself. The following outlines the specific skill execution pathways for the five tasks in our experiments:\nSkill execution path of the Dynamic-immediate Planning Tasks\nCollect Seeds: Collect Wheat Seeds / Collect Melon Seeds / Collect Pumpkin Seeds\nHoe Farmland: Craft Hoe \u2192 Hoe Farmland\nShear Sheep: Craft Shears\u2192Shear Sheep Using Shears\nMilk Cow: Craft Bucket Milk Cow Using Bucket\nCook Meat: Kill Pig\u2192Cook Porkchop / Kill Chicken Cook Chicken / Kill Sheep\u2192Cook Mutton / Kill Cow Cook Beef\nIn our Autonomous Exploration Task, the agent also needs to plan step by step without a given goal. Every time a new plan is proposed, the agent retrieves the ten most semantically similar skills from our skill library and selects one to execute. We tally the number of distinct item types obtained by the agent in each round, as well as the cumulative number of item types. Here are the distinct items obtained by the agent from one round of the experiment:\nDistinct items obtained within 80 LLM iters ['oak_log', 'stick', 'wooden_sword', 'crafting_table', 'wooden_pickaxe', 'stone_pickaxe', 'oak_planks', 'wheat_seeds', 'dirt', 'cobblestone', 'raw_iron', 'granite', 'andesite', 'cob-bled_deepslate', 'diorite', 'diamond', 'iron_pickaxe', 'furnace', 'cobblestone_wall', 'coal', 'iron_ingot', 'iron_trapdoor', 'dandelion', 'azure_bluet', 'poppy', 'oxeye_daisy', 'chest', 'cob-blestone_stairs', 'raw_copper', 'copper_ingot', 'calcite', 'copper_block', 'birch_planks', 'jun-gle_log', 'arrow', 'bone', 'rotten_flesh'], Num: 37\nThis result is comparable to the Voyager [40] framework that employs GPT-4 for skill code generation and significantly outperforms Voyager using GPT-3.5."}, {"title": "D.3 Ablation Study", "content": "We conduct ablation studies on two core components of the ODYSSEY agent, including the LLM planner and the open-world skill library.\nFor the LLM planner ablation, we remove the current environmental information in the planner system prompt as follows. Moreover, in each task proposed during each round, if the retrieved skills were not relevant to the current task (i.e., if the semantic retrieval score was below a certain threshold), the execution of those skills was not carried out.\nPlanner System Prompt in Ablation\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft. My ultimate goal is to discover as many diverse things as possible, accomplish as many diverse tasks as possible and become the best Minecraft player in the world. You can propose next suitable tasks for me, such as \"Mine [block]\", \"Craft [item]\", \"Smelt [item]\", \"Kill [mob]\", \"Cook [food]\", \"Equip\" etc. It's better to be a single phrase.\nYou should only respond in JSON format as described below: {\n\"reasoning\": \"Do reasoning about what the next task should be.\",\n\"task\": \"The next task.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no single quotes, etc.\nFor the open-world skill library ablation, we removed the entire skill library and provided the LLM only with the necessary interfaces required for composing new skills. Each round's skill retrieval and execution were changed to code writing and execution, similar to the approach used in Voyager [40]. The actor system prompt is shown as follows:\nActor System Prompt in Ablation\nYou are a helpful assistant that writes Mineflayer javascript code to complete any Minecraft task specified by me.\n-External information-\nAt each round of conversation, I will give you\nCode from the last round:\nExecution error:\nChat log:\nBiome:\nNearby blocks:\nNearby entities (nearest to farthest):\nHealth:\nHunger:\nPosition:\nEquipment:\nInventory (xx/36):\nChests:\nTask:\nContext:\nCritique:\n-Directions-\nYou should then respond to me with\nExplain (if applicable): Are there any steps missing in your plan? Why does the code not complete the task? What does the chat log and execution error imply?\nPlan: How to complete the task step by step. You should pay attention to Inventory since it tells what you have. The task completeness check is also based on your final inventory.\nCode:\n1) Write an async function taking the bot as the only argument.\n2) Reuse the above useful programs as much as possible.\n3) ...\n-Behaviour constraints-\nYou should only respond in the format as described below:\nExplain:\nPlan:\n1) ...\n2) ...\n3) ...\nCode:\njavascript\n// helper functions (only if needed, try to avoid them)\n// main function after the helper functions\nasync function yourMainFunctionName(bot) {\n// ..."}]}