{"title": "ODYSSEY: Empowering Agents with Open-World Skills", "authors": ["Shunyu Liu", "Yaoru Li", "Kongcheng Zhang", "Zhenyu Cui", "Wenkai Fang", "Yuxuan Zheng", "Tongya Zheng", "Mingli Song"], "abstract": "Recent studies have delved into constructing generalist agents for open-world\nembodied environments like Minecraft. Despite the encouraging results, existing\nefforts mainly focus on solving basic programmatic tasks, e.g., material collection\nand tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set of\nactions available to agents, requiring them to learn effective long-horizon strate-\ngies from scratch. Consequently, discovering diverse gameplay opportunities in\nthe open world becomes challenging. In this work, we introduce ODYSSEY, a\nnew framework that empowers Large Language Model (LLM)-based agents with\nopen-world skills to explore the vast Minecraft world. ODYSSEY comprises three\nkey parts: (1) An interactive agent with an open-world skill library that consists\nof 40 primitive skills and 183 compositional skills. (2) A fine-tuned LLaMA-3\nmodel trained on a large question-answering dataset with 390k+ instruction entries\nderived from the Minecraft Wiki. (3) A new open-world benchmark includes thou-\nsands of long-term planning tasks, tens of dynamic-immediate planning tasks, and\none autonomous exploration task. Extensive experiments demonstrate that the pro-\nposed ODYSSEY framework can effectively evaluate the planning and exploration\ncapabilities of agents. All datasets, model weights, and code are publicly available\nto motivate future research on more advanced autonomous agent solutions.", "sections": [{"title": "Introduction", "content": "Developing autonomous embodied agents capable of performing open-world tasks represents a\nsignificant milestone towards achieving artificial general intelligence [9, 31, 33]. These open-world\ntasks necessitate that agents interact with complex and dynamic environments, make decisions\nbased on incomplete information, and adapt to unexpected events. Early reinforcement learning\nagents [12, 26, 36] have demonstrated limited knowledge in such open-world setting. Furthermore,\nthese agents often struggle with long-term planning, which is crucial for the fulfillment of intricate\ngoals. Recent breakthrough of Large Language Models (LLMs) [1, 15, 37] have shown the potential\nto revolutionize various fields such as healthcare [46, 50], robotics [2, 16], and web services [7, 25],\nattributed to its capability on endowing agents with expansive knowledge and sophisticated planning\nakin to human reasoning [19, 41, 44, 47]. However, the development of LLMs in open-world tasks\nremains challenging due to the need for well-defined environments and measurable benchmarks [30,\n40, 42, 43, 53]."}, {"title": "Open-World Skill-based Interactive Agent", "content": "ODYSSEY develops an LLM-based interactive agent with an open-world skill library, aiming to\nenhance the efficiency and adaptability of agents in complex Minecraft environments. The skill\nlibrary comprises 40 primitive skills and 183 compositional skills, while the LLM-based agent\nemploys a planner-actor-critic architecture to facilitate task decomposition, skill execution, and\nperformance feedback. The architecture of the interactive agent is depicted in Fig. 2. Full skill and\nprompt details used in the LLM-based interactive agent are given in Appendix A."}, {"title": "Open-World Skill Library", "content": "Primitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript APIs [29],\ndivided into two main categories: 32 operational skills and 8 spatial skills. In addition to Voyager's\n18 operational skills [40], 14 operational skills implemented by us are presented as follows:\n\u2022 plantSeeds(bot, type): Let the agent find the nearest farmland and plant a particular\nkind of seed.\n\u2022 feedAnimals (bot, type, count=1): Let the agent find the nearest animals of a partic-\nular species and numbers and feed them with the appropriate food.\n\u2022 killAnimal(bot, type): Let the agent kill a particular kind of animal using the best\nsword in its inventory.\n\u2022 killMonsters(bot, type, count=1): Let the agent kill monsters nearby of a particu-\nlar species and numbers using the best sword in its inventory.\n\u2022 cookFood (bot, type, count=1): Let the agent cook food of a particular kind and\nnumbers using coal and furnace.\n\u2022 eatFood (bot, type): Let the agent eat a particular kind of food.\n\u2022 equipArmor (bot): Let the agent equip the best armor(helmet, chestplate, leggings and\nboots) in its inventory.\n\u2022 equipSword/Pickaxe/Axe/Hoe/Shovel (bot): Let the agent equip the best correspond-\ning tool in its inventory.\n\u2022 getLogs/PlanksCount(bot): Return the number of logs/planks (counted in seven dif-\nferent categories) in the inventory.\nAdditionally, we pioneer 8 spatial skills that Voyager [40] lacks, allowing for environmental in-\nteractions based on the agent coordinates. The spatial skills implemented by us are presented as\nfollows:\n\u2022 findSuitablePosition(bot): Let the agent find the best nearby location for placing\ndevices such as a crafting table or furnace. The block must be minecraft: air and at least\none adjacent reference block exists.\n\u2022 checkAdjacentBlock(bot, types, x, y, z): Check blocks adjacent to the block at\nposition (x,y,z). Return true if any of the adjacent blocks match the specified types.\n\u2022 checkBlockAbove(bot, type, x, y, z): Check block above the block at position\n(x,y,z). Return true if the above block matches the specified type.\n\u2022 checkBlocksAround(bot, type, x, y, z): Check blocks around the block at posi-\ntion (x,y,z).Return true if any of the around blocks match the specified type.\n\u2022 checkNearbyBlock(bot, types, x, y, z, r): Check blocks in a radius around the\nblock at position (x, y, z). Return true if any block within the radius matches the specified\ntypes.\n\u2022 checkNoAdjacentBlock(bot, types, x, y, z): Check adjacent blocks of block at\nposition (x,y,z). Return true if not all adjacent blocks are within the specified types.\n\u2022 goto(bot, x, y, z): Let the agent go to the corresponding position (x,y,z) until it\nreaches the destination.\n\u2022 getAnimal (bot, type, x, y, z): Let the agent attract a particular kind of animal to a\nparticular position (x,y,z) with the appropriate food."}, {"title": "LLM Planner", "content": "ODYSSEY relies on LLMs to generate language-based plans. In our Minecraft experiment, we propose\nthree novel tasks (long-term planning task, dynamic-immediate planning task and autonomous\nexploration task) for agents to explore. Therefore we designate three types of prompt messages for\nthem respectively, offering LLM Planner the ability to generate different routines on different tasks.\nThe format of the prompt is presented thus:\n\u2022 \"SYSTEM\" role: A high-level instruction that gives directions to the model behavior. It sets\nan overall goal for the interaction and provides external information."}, {"title": "Long-term Planning", "content": "We design a suite of combat tasks to assess the long-term planning capabilities of agents, where the\nLLM Planner should plan to craft appropriate weapons and equipment to defeat monsters.\nThe input prompt to LLM consists of several components:\n\u2022 Ultimate goals: The monsters that need to be defeated.\n\u2022 Directives and behavior constraints that guarantee the proposed task is achievable and\nverifiable.\n\u2022 Information of last combat: This ensures that the prompt is exposed to increasing amounts of\ninformation over the combat and thus progressively advances towards more efficient plans."}, {"title": "Dynamic-immediate Planning", "content": "In this kind of task, agents are expected to adapt their plans based on the real-time feedback like\nnearby resources and animals.\nThe input prompt to LLM consists of the following components:\n\u2022 Ultimate goals: A suite of farming tasks, such as planting, harvesting, and animal husbandry.\n\u2022 The current states of agent: hunger and health values, position and nearby entities, etc.\n\u2022 Achievements of the agent: the current inventory and unlocked equipment, as well as\npreviously successful and failed tasks."}, {"title": "Autonomous Exploration", "content": "In this task, the agent is required to explore the Minecraft world freely without any specific goals.\nThis poses a great challenge to the planner for maximal exploration. It should propose suitable tasks\nbased on the current state and environment, e.g., plan to obtain sand or cactus before wood if it finds\nitself in a desert rather than a forest. The input prompt to LLM consists of several components:\n\u2022 Guidelines encouraging diverse tasks.\n\u2022 The current states of agent: hunger and health values, position and nearby entities, etc.\n\u2022 Achievements of the agent: the current inventory and unlocked equipment, as well as\npreviously successful and failed tasks."}, {"title": "LLM Actor", "content": "In actor, the mapping from higher language subgoals S to lower executable codes is implemented\nthrough query context encoding and similarity retrieval. We employ the following prompt during the\ngeneration of query context (Question-Answer pairs)."}, {"title": "LLM Critic", "content": "The LLM critic should evaluate the success of the executed actions by comparing expected outcomes\nwith actual results, thereby providing valuable critiques for refining strategies in subsequent iterations.\nWe design a chain-of-thought [45] prompting mechanism: We first require LLM to reason about the\ntask's success or failure, then output a boolean variable representing the execution result, and finally\nprovide a critique to the agent if the task fails."}, {"title": "Fine-tune Minecraft LLM", "content": "To improve the performance of LLM-based agents in the Minecraft environment, we fine-tune the\nLLaMA-3 model [37] using a large-scale Question-Answering (Q&A) dataset with 390k+ instruction\nentries sourced from the Minecraft Wiki. ODYSSEY presents an effective procedure for converting\na foundation model into a domain-specific model in Minecraft, which involves dataset generation,\nmodel fine-tuning, and model evaluation. The detailed descriptions can be found in Appendix B."}, {"title": "Dataset Generation", "content": "We develop an LLM-assisted method to generate an instruction dataset for\nMinecraft. We crawl and organize relevant content from the Minecraft Wiki, excluding non-essential\nsections like history. The data is then formatted uniformly. Based on this, we employ GPT-3.5-\nTurbo [27] with customized prompts to automatically generate diverse Q&A pairs, categorized into\nshort, normal, long, and boolean answers, resulting in 390k+ instruction entries. In contrast, the Wiki\ndataset released by MineDojo [10] only collects Minecraft Wiki pages, without refining the content\nand generating Q&A pairs for model training. STEVE [51] introduces a non-public dataset with 20k+\nQ&A pairs, which is smaller than our dataset in terms of scale and diversity."}, {"title": "Model Fine-tuning", "content": "We use the LoRA method [15] for model fine-tuning. LoRA is a parameter-\nefficient training method that introduces small, trainable low-rank matrices to adapt a pre-trained\nneural network, enabling targeted updates without retraining the entire model. We employed LoRA to\nfine-tune the LLaMA-3-8B-Instruct and LLaMA-3-70B-Instruct models using the specially generated\nMinecraft dataset, resulting in the new models termed MineMA-8B and MineMA-70B, respectively."}, {"title": "Model Evaluation", "content": "In the context of Minecraft, questions are often open-ended and can yield a\nvariety of answers; therefore, conventional evaluation metrics [21, 21, 28] may fall short. Meanwhile,\ncommon benchmarks [14, 38, 39] are not suitable for assessing the capabilities of expert models.\nThus, we employed GPT-4 [1] to generate two Multiple-Choice Question (MCQ) datasets based on\ndifferent themes and keywords related to Minecraft. These datasets can quantitatively evaluate the\ndomain-specific expertise of models."}, {"title": "Open-World Benchmark", "content": "ODYSSEY introduces a new open-world benchmark to\nevaluate the planning and exploration capabilities of\nagents in Minecraft. This benchmark comprises three\ntypes of innovative evaluation tasks, including thou-\nsands of long-term planning tasks, tens of dynamic-\nimmediate planning tasks, and one autonomous explo-\nration task. The ODYSSEY benchmark offers a stan-\ndardized framework to evaluate autonomous agents\nacross a range of open-world challenges, ensuring con-\nsistent and comparative assessments of the agent per-\nformance. Please refer to Appendix C for more details."}, {"title": "Long-term Planning Task", "content": "We design a suite of combat tasks to assess the long-\nterm planning capabilities of agents, where agents\nshould plan to craft appropriate weapons and equip-\nment to defeat different monsters. These combat tasks"}, {"title": "Dynamic-immediate Planning Task", "content": "The dynamic-immediate planning tasks require agents to dynamically generate and execute plans\nbased on immediate environmental feedback. Thus, we design a suite of farming tasks, where\nagents engage in activities like planting, harvesting, and animal husbandry. In these tasks, agents\nare expected to adapt their strategies based on real-time feedback like nearby resources and animals.\nTask completion time and success rate are adopted for agent performance evaluation."}, {"title": "Autonomous Exploration Task", "content": "To test the exploratory capability of agents within open-world settings, we design an autonomous\nexploration task in Minecraft. In this task, agents are required to determine their subsequent objectives\nand execute the appropriate skills based on the game context. The exploration task involves discov-\nering and utilizing resources, while adapting to unexpected events such as encounters with hostile\nmonsters. Agents must adapt to these challenges by developing strategies for resource management\nand task prioritization. The performance metrics include the number of distinct items obtained, the\ntotal items crafted, the recipes and advancements (R&A) unlocked, and the distance traveled."}, {"title": "Experiments", "content": "To demonstrate the effectiveness of the proposed ODYSSEY framework, we conduct experiments\non several basic programmatic tasks and the open-world benchmark. We aim to answer the\nfollowing questions: (1) Can the open-world skill library improve the efficiency of agents in\nMinecraft? (Sec. 5.1). (2) How well do agents with different LLMs perform on the open-world\nbenchmark tasks? (Sec. 5.2). (3) What is the contribution of different components of the ODYSSEY\nagent to its overall performance? (Sec. 5.3).\nOur simulation environment is built on top of MineDojo [10] and Voyager [40], which provides a\ntext-based interface for agents to interact with Minecraft. In addition to using GPT-3.5 and GPT-4\nfor one-time generation of fine-tuning data, we conduct all experiments based on the open-source\nLLaMA-3 model. The experimental costs are considerably reduced compared to methods that rely on\nGPT-4 for skill generation [40, 42]. The detailed experimental settings are provided in Appendix D."}, {"title": "Open-World Skill Library", "content": "To demonstrate the superior capability of our open-world skill library in Minecraft, we first tested it\non 5 basic programmatic tasks from previous studies [53]. We conducted 120 repeated experiments\non each task and recorded the average completion time for each task as well as the success rates at\ndifferent time points. We report the performance of baselines using the results reported from their\nown paper. The experimental results in Tab. 1 and Tab. 2 show that our skill library significantly\nimproved the success rate and efficiency of the above tasks, surpassing previous studies."}, {"title": "Open-World Benchmark", "content": "We evaluate the LLM-based agent on 8 long-term planning tasks, 4 dynamic-immediate tasks, and the\nautonomous exploration task from the ODYSSEY benchmark. These tasks cover a variety of complex\ngaming scenarios and require diverse solutions."}, {"title": "Long-term Planning Task", "content": "For the long-term planning tasks, the agent is required to plan a list of weapons and equipment to craft\nbased on the strength of different monsters, with the goal of defeating the monster in as short a time\nas possible. We compare the original LLaMA-3-8B model with the fine-tuned MineMA-8B model on\nthese tasks. Moreover, we also evaluate the performance of single-round and multi-round planning.\nThe single-round test results in Tab. 3 show that the fine-tuned MineMA-8B model outperforms\nthe original LLaMA-3-8B model in terms of success rate and time efficiency, albeit at the cost of"}, {"title": "Dynamic-immediate Planning Task", "content": "For the dynamic-immediate planning tasks, the\nagent is required to dynamically generate and\nexecute plans based on immediate environmen-\ntal feedback. We evaluate the performance of\nthe MineMA-8B and the MineMA-70B model\nto investigate the impact of model size on task\nperformance. As shown in Tab. 4, the MineMA-70B model shows superior performance compared\nwith the MineMA-8B model. Across all tasks, MineMA-70B demonstrates higher success rates and\ngenerally lower average execution times and LLM iterations."}, {"title": "Autonomous Exploration Task", "content": "In the autonomous exploration task, the agent is required to explore the Minecraft world freely\nwithout any specific goals. We evaluate the performance of the LLaMA3-8B and the MineMA-8B\nmodel on this task. As shown in Fig. 5, both models achieve promising results, indicating that the\nagent can autonomously explore the Minecraft world without specific goals. Our agent with the\nMineMA-8B model can achieve the consistent performance compared with Voyager [40] with GPT-4\nand even surpasses Voyager with GPT-3."}, {"title": "Ablation Study", "content": "We conduct ablation studies on two core components of the ODYSSEY agent, including the LLM\nplanner and the open-world skill library. The results are shown in Fig. 5. In the autonomous\nexploration task, the LLM planner is responsible for generating a comprehensive plan based on the\nopen-world skill library. The ablation study demonstrates that the planner is indispensable for the\nagent to effectively navigate the complex Minecraft environment. Additionally, our experimental\nresults indicates that the absence of the open-world skill library significantly degrades performance.\nWithout the open-world skill library, the 8B LLM model alone is largely incapable of generating"}, {"title": "Related Works", "content": "Minecraft agents have been widely studied in recent years to test the capabilities of autonomous\nagents in open-world environments. Previous works focused on training Minecraft agents with\nhierarchical Reinforcement Learning (RL) [13, 23, 24, 26, 36] or imitation learning [3, 4, 20], which\nare extensively used in the MineRL [12] competition to solve the ObtainDiamond task. With the\nrapid development of LLMs, numerous studies leverage LLMs to enhance agent capabilities [11,\n40, 42, 49, 51, 52, 53]. Among these, several works [8, 18, 30, 43, 48] employ LLMs to guide skill\nlearning in Minecraft, enabling agents to act in a human-like way. However, these methods mainly\nfocus on learning primitive skills from scratch, lacking a reusable skill library. Voyager [40] builds a\nskill library by allowing the LLM to write its own skills. These skills are also learned from a narrowly\ndefined set of primitive skills, resulting in unstable skill generation and high learning costs. In\ncontrast, ODYSSEY provides an open-world skill library with both primitive skills and compositional\nskills, enabling LLM-based agents to efficiently and stably generate complex policies for broader\nexploration and more complex tasks.\nOpen-world benchmarks have gained considerable attention from research communities [5, 6, 17,\n34, 35]. Minecraft, with its diverse tasks and mature game mechanics, has emerged as an ideal test-\nbed for open-world tasks. Built on Minecraft, MineRL [12] implements a simulation environment for\nagent learning. MineDojo [10] further extends MineRL with thousands of diverse tasks. MCU [22]\ncollects a variety of atom tasks, offering a method to generate infinite tasks by combining the atom\ntasks. However, existing benchmarks mainly focus on providing basic programmatic tasks to evaluate\nagents learned from scratch. Our ODYSSEY benchmark is built on top of the skill library, enabling\nthe agents to bypass basic programmatic tasks and focus on complex open-world challenges."}, {"title": "Conclusion", "content": "This work proposes ODYSSEY to empower agents with open-world skills in the Minecraft environ-\nment. We introduce (1) an interactive agent endowed with an extensive open-world skill library\ncomprising various primitive skills and compositional skills; (2) a fine-tuned LLaMA-3 model,\ntrained on a large-scale question-answering dataset sourced from the Minecraft Wiki; (3) a new\nopen-world benchmark that encompasses tasks requiring long-term planning, dynamic-immediate\nplanning, and autonomous exploration. The public availability of all datasets, model weights, and\ncode will facilitate future research in the development of autonomous embodied agents. We hope that\nODYSSEY will inspire further innovation and progress in the field of autonomous agent development."}, {"title": "Limitations and Future Works", "content": "The proposed open-world skill library enables the use of open-\nsource LLMs as the foundation for agents to call upon skills, avoiding the high costs associated with\nprevious work using GPT-4 [18, 30, 40]. However, the open-source LLMs are prone to generating\nhallucinations, leading to a decrease in agent performance. Thus, our future research will focus on\nemploying retrieval-augmented generation to improve LLMs in Minecraft. Additionally, the skill\nlibrary is still text-based, which limits its functionality in tasks requiring visual information. We plan\nto integrate visual processing capabilities into the skill library to expand its capabilities."}]}