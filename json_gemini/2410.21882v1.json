{"title": "Building Altruistic and Moral AI Agent with\nBrain-inspired Affective Empathy Mechanisms", "authors": ["Feifei Zhao", "Hui Feng", "Haibo Tong", "Zhengqiang Han", "Enmeng Lu", "Yinqian Sun", "Yi Zeng"], "abstract": "Abstract-As AI closely interacts with human society, it is\ncrucial to ensure that its decision-making is safe, altruistic, and\naligned with human ethical and moral values. However, existing\nresearch on embedding ethical and moral considerations into AI\nremains insufficient, and previous external constraints based on\nprinciples and rules are inadequate to provide AI with long-\nterm stability and generalization capabilities. In contrast, the\nintrinsic altruistic motivation based on empathy is more willing,\nspontaneous, and robust. Therefore, this paper is dedicated\nto autonomously driving intelligent agents to acquire morally\nbehaviors through human-like affective empathy mechanisms.\nWe draw inspiration from the neural mechanism of human\nbrain's moral intuitive decision-making, and simulate the mirror\nneuron system to construct a brain-inspired affective empathy-\ndriven altruistic decision-making model. Here, empathy directly\nimpacts dopamine release to form intrinsic altruistic motivation.\nBased on the principle of moral utilitarianism, we design the\nmoral reward function that integrates intrinsic empathy and\nextrinsic self-task goals. A comprehensive experimental scenario\nincorporating empathetic processes, personal objectives, and\naltruistic goals is developed. The proposed model enables the\nagent to make consistent moral decisions (prioritizing altruism)\nby balancing self-interest with the well-being of others. We\nfurther introduce inhibitory neurons to regulate different levels\nof empathy and verify the positive correlation between empathy\nlevels and altruistic preferences, yielding conclusions consistent\nwith findings from psychological behavioral experiments. This\nwork provides a feasible solution for the development of ethical\nAI by leveraging the intrinsic human-like empathy mechanisms,\nand contributes to the harmonious coexistence between humans\nand AI.", "sections": [{"title": "I. INTRODUCTION", "content": "As AI rapidly evolves, it is vital to explore its safety and\nethical implications. We hope to develop autonomous\nagents that make human-like decisions, act altruistically, safely\nand morally, so that such Als are credible and can be sustain-\nable and beneficial. Enabling AI to make ethical decisions is\na complex process that requires a trade-off between personal\nand other interests. Altruistic behavior is acknowledged as\na crucial moral value, i.e., sacrificing one's self-interest for\nthe greater well-being of others [1]\u2013[4], and serves as the\nfoundation for natural reproduction and a harmonious society.\nThe motivations for altruism can be the desire for higher\nsocial recognition [5], future collaborative opportunities [6],\nand enhancement of personal satisfaction and pleasure [7], etc.\nThese external pressures, rational judgments are not stable and\nwill lose effectiveness as the environments change. Empathy\nas an intrinsic altruistic motivation, especially direct and rapid\nempathy for the emotions of others, is the most robust and\nsolid altruistic motivation [8].\nEmpathy can be triggered either by rapid affective empathy\nthrough direct observation of outward information such as\nother's expression, behavior (i.e., mirror neuron system) or\nby prediction based on episodic memory without outward\ninformation (i.e., theory of mind). Obviously, direct empathy\nfor outward information is a more rapid and instant empathic\nresponse and is more likely to drive moral intuition [8],\n[9]. There has been extensive mature research focusing on\nfacia [10]\u2013[12], auditory [13]\u2013[15], textual [16] and physi-\nological signals [17]-based emotion recognition, as well as\nrobot facial expression and verbal feedback based on multi-\nmodal emotion recognition [18]. However, understanding and\nempathizing with others' emotions, modeling the human af-\nfective empathy process, and exploring how this empathy\ndirectly influence one's own behavior to alleviate others'\nnegative emotions are all critical research fields. Investigating\nthese aspects will significantly advance the development of\nempathy-driven ethical AI, especially to empower the meaning\nderived from emotion recognition.\nExisting AI ethics research has explored encoding ethical\nknowledge as external rewards within specific ethical envi-\nronments, such as \"Cake or Death\" and \"Burning Room\". In\nthese frameworks, designed rewards are linearly weighted to\nprioritize ethical behaviors, allowing Reinforcement Learning\n(RL) algorithms to acquire ethical decision-making skills [19].\nAdditionally, some studies combine constrained RL [20],\n[21] and multi-objective optimization methods [22]\u2013[24] to\ntackle various rewards as multiple objectives. Similar ideas of"}, {"title": "II. RELATED WORKS", "content": "Previous AI Ethical Model can be broadly categorized as\nrule-based [19], reward learning from human [43], [44], and\nmulti-objective constraint-based [20]\u2013[24]. [19] characterizes\nethical rules as multiple rewards with the linear weighting\nfactor determining the priority of norm compliance. [43]\nlearns human ethical strategies from human data and allows\nagent to align human values through reward shaping. [44]\nlearns standard behaviors from human behavioral data, uses\nInverse Reinforcement Learning (IRL) to infer human inten-\ntions and goals, and avoids unsafe behaviors with human\nsupervision and intervention. [21] follows behavioral norms\nthrough constraint-reinforcement learning. [20] captures eth-\nical constraints (e.g., not allowed to eat something) through\nIRL, in combination with policy orchestration to optimize\nbehaviors. [22] learns individual and ethical goals through\nmulti-objective reinforcement learning to achieve alignment\nof moral values. [23] designs ethical environments and em-\npowers agents to behave ethically by using a multi-objective\nreinforcement learning approach. [27] defines moral norms\nbased on the moral philosophical theories of Consequentialism\n(Utilitarianism), Deontology and virtue ethics respectively,\ncomparing and distinguishing the effects of different moral\ntheories.\nExternal ethical rule constraints in specific scenarios are\nlimited by the environment itself, and multi-objective learning\nmethods cannot address situations where multiple objectives\nare clearly in conflict, i.e., where one must choose between the\ninterests of the self and others, which is at the core of moral\ndecision-making. Learning from human data runs the risk of\nlearning about human misguided morality. More importantly,\nthe altruistic moral behaviors exhibited by these methods are\nnot driven by intrinsic empathy. External constraints in specific\nscenarios are difficult to ensure absolute compliance, leading\nto limited generalization.\nEmpathy can be divided into cognitive empathy (which\ninvolves understanding others' mental states) and affective\nempathy (which directly empathizes with others' emotional\nstates) [45], [46]. The vast majority of existing research has\nfocused on the computational modeling of cognitive empathy,\nas well as its integration with reinforcement learning and\nmulti-agent systems. Rabinowitz et al. [30] designed a ToM-\nnet neural network model to predict the future behavior of\nother agent through meta-learning. Akula et al. [31] proposed\nan interpretable AI framework, CX-ToM, designed to interpret\ndecisions made by deep convolutional neural networks. This\nmodel explicitly captures human users' intentions, enhancing\ninterpretability through multiple rounds of interaction between\nthe user and the machine. Yang et al. [47] proposed the\nBayes-ToMoP method to detect the reasoning strategies used\nby opponents and learn the optimal response strategies ac-\ncordingly. ToM2C [32] uses historical information as a kind\nof supervised signal and predicts the observations and goals\nof others to help agent make more appropriate decisions.\nMIRLTOM [33] uses ToM to estimate the posterior distribution\nof the reward curves based on observed agent's behaviors.\nZhao et al. [34], [35] proposed to realize the inference of\nother agents' behaviors and goals based on self-experience\nand modeling of others, which in turn helps to improve the\nefficiency of multi-intelligence collaboration.\nBased on cognitive empathy, some studies implement pre-\ndictions of others' strategies and rewards, in order to help\nagents avoid negative effects on others [36]\u2013[38], as well\nas helping others to avoid safety risks [39]. [36] combines\nown rewards with the estimated values of other agents, by\nimagining the value of being in the situation of the other\nagent. [37] first infers the agent's reward function through\nIRL, and then learn a strategy based on a convex combination\nof the inferred reward and the agent's own reward to achieve\navoidance of negatively effective behavior. [38] empowers RL\nagents to increase their gains based on the expected returns of\nothers in their environment, and to exhibit self-less behaviors.\nThe above methods utilize the RL techniques to predict\nothers' rewards or strategies and integrate them into their own\nbehavioral objectives to minimize harm to others. While this\nis a feasible approach, it does not involve the agent genuinely\nempathizing with others' emotions. Direct affective empathy\ndrives the agent to alleviate its empathetic negative emotions\nonly through altruism, embodying the principle that \"if others\nare well, then I am well.\" This is the most robust motiva-\ntion behind human altruistic and ethical behaviors. However,\nexisting research has primarily focused on partial aspects of\naffective computing, such as recognizing human emotions\nthrough various external cues such as facial expressions and\nspeeches [10], [12], [14], [15], [48]. Building on this external\nrecognition, we need to further model the internal process\nof human affective empathy, mapping the external emotional\nexpression of others to our own empathic experience and\nestablishing a direct connection with our own decision-making\nto spontaneously drive altruistic behavior."}, {"title": "III. BRAIN-INSPIRED AFFECTIVE EMPATHY-DRIVEN\nMORAL DECISION-MAKING ALGORITHM", "content": "In this section, we present the proposed affective empathy-\ndriven moral decision-making algorithm, as shown in Fig.\n1. We first describe the overall framework of the proposed\nalgorithm. Then, we provide computational details of the\naffective empathy module and the altruistic decision-making\nmodule, respectively.\nTo closely align with the specific processes of affective\nempathy guided moral behavior in the human brain, we\nfirst conduct a detailed investigation of the relevant neural\nmechanisms. Based on this, we construct a multi-brain areas\ncoordinated framework for affective empathy-driven moral\ndecision-making. As shown in Fig. 1, our proposed model\nincludes the interaction and collaboration between the affective\nempathy module and the moral decision-making module."}, {"title": "B. Detailed Implementation of the Proposed Model", "content": "The affective empathy module consists of a recurrent interac-\ntive loop formed by the excitatory connection of mirror neuron\nclusters linking the perceptual and emotional regions. Due to\nthe strict temporal correlation between emotions and external\naction and perception, the connections between the three\nclusters of neurons are strengthened. Since the connections be-\ntween the modules are bidirectional, it will be interactively and\nrepeatedly facilitated to enhance the bidirectional connection\nweights. Therefore, we utilize spiking neural networks [56]\nto model the connections among the emotional brain region,\nmirror neuron system, and perceptual brain region, with Spike-\nTiming-Dependent Plasticity (STDP) [57] employed to facili-\ntate learning of temporal sequence-dependent associations.\nDuring the self-experience learning phase, the firing of\nspecific self-emotional neurons triggers corresponding external\nactions and perceptions (with first emotiaonal neurons firing\nmirror neurons firing 100ms later, followed by perceptual\nneurons firing 200ms later). Due to the temporal correlation,\nthe connection weights among the three brain regions are\nreinforced through STDP. Here, we use the Leaky Integrate-\nand-Fire (LIF) spiking neuron [58] and long-term potentiation\n(LTP) in STDP as shown in Eq. 1. In the testing phase, when\npresented with the external information of others, the network\nis able to automatically trigger the firing of the same self-\nemotional neurons.\n$\\triangle w_{emp} = LTP (S_i, S_j) = A^+ exp \\left( \\frac{t_i - t_j}{\\tau^+} \\right), t_i - t_j < 0$ (1)\nwhere $S_i, S_j$ denote the Spike train of neurons in two\nregions, $t_i, t_j$ denote the specific firing time of the two types\nof neurons. $A^+ = 0.5$ denotes the learning rate, $\\tau^+ = 20ms$\nis a time constant.\nIn our\nmodel, emotional neurons directly provide inhibitory connec-\ntions to dopamine neurons that represent intrinsic emotions.\nThe stronger the negative emotions, the lower the dopamine\nlevels will be. Since the model aims for high dopamine levels,\nit drives the alleviation of negative emotions. The negative\nemotions generated from empathizing with others also affect\ndopamine levels, creating an intrinsic motivation for altruism.\nDopamine represents the reward prediction error [59], which\nis the difference between the predicted reward and the actual\nreward received. We statistically analyze the firing rate $S (t)$\nof dopamine neurons representing empathy under the inhibi-\ntion of empathic neurons as the actual feedback, while the\npredicted values $P (t)$ are initialized at zero and iteratively\nupdated based on the prediction error $\\delta (t)$. Thus, empathy-\ndriven dopamine level is calculated as follows:\n$DA_{in-emp} = \\alpha * \\delta (t)$ (2)\n$\\delta (t) = S (t) - P (t)$ (3)\n$P (t + 1) = P (t) + \\beta * \\delta (t)$ (4)\nwhere $\\alpha = 30, \\beta = 0.2$ are the constant. When the agent's\nempathized emotion changes from negative to normal, the\nvalue of the change in the firing rate of the negative emotion\nneurons is negative and $DA_{in-emp}$ is positive. Only when\nthe emotional outward expressions corresponding to others'\nnegative emotions are adjusted,meaning altruistic behavior is\nperformed, will the own negative emotion neurons not fire,\nleading to an increase in dopamine levels. Consequently, the\nagent learns altruistic behavior under dopamine regulation.\nIn\naddition to influencing internal dopamine levels, affective\nempathy also affects the observation of decision making. The\nagent's observations include not only the observed state-\nhorizontal and vertical coordinate information ($x, y$) of the\nenvironment when performing its own task, but also the\nempathized emotional state $O_{emp}$ from the peer. Empathizing\nwith others' emotional states provides a cue that helps the\nagent learn altruistic behavior. Thus, the input state of the\nmoral decision-making SNN is:\n$state: (x, y, O_{emp})$ (5)\nwhere $O_{emp}$ characterizes the emotional state of an agent.\nWhen the agent is in a negative emotional state (negative\nemotional neurons firing), $O_{emp} = -1$; otherwise, $O_{emp} = 0$.\nThe decision module consists of fully connected state neu-\nrons that represent the environment and action neurons. The\naction neurons employ population coding, with each action\nrepresented by a group of 50 neurons, and the behavior with\nthe highest number of neuron population fires will be executed.\nThe agent interacts autonomously with the moral decision-\nmaking environment, which includes the agent's own tasks\n$R_{self-task}$ as well as the explicit information of others. The\nexplicit information from others as the emotional outward\ninformation is processed through the affective empathy module\nto yield an empathy reward $DA_{in-emp}$. Here, we draw on\nnormative ethics from moral theory [60], using consequen-\ntialism/utilitarianism principle to guide the agent's behavior.\nUtilitarianism emphasizes that the assessment of moral be-\nhavior is based on the consequences of actions, meaning that\nthe correct behavior is that which produces the best outcomes,\nmaximizing the interests of both oneself and others [61]. Based"}, {"title": "IV. EXPERIMENTS", "content": "We designed a\nmoral decision-making experimental scenario that includes\nexperiencing one's own emotions and explicit information,\nempathizing with other agent, and conflicts between self-\ngoal and altruistic goal. As shown in Fig. 2, Agent A first\nrandomly explores the environment, experiencing its own\nnegative emotions and perceiving changes in its emotional\noutward expressions (the color changes from green to red).\nThis process establishes a connection between the change in"}, {"title": "V. CONCLUSION", "content": "This paper presents an altruistic moral AI agent inspired\nby the affective empathy mechanisms in the human brain,\nenabling the agent to empathize with others based on its own\nexperiences and develop intrinsic motivation for altruism, par-\nticularly in moral dilemmas involving conflicts between self-\ninterest and the interests of others. Specifically, we proposed\na multi-brain area coordinated spiking neural network model\nthat integrates the mirror neuron system for spontaneous empa-\nthy and regulates dopamine levels to drive altruistic decision-\nmaking. Additionally, a moral reward system is designed based\non moral utilitarianism, combining intrinsic empathy-related\ndopamine levels with external self-task goals, facilitating con-\nsistent moral behavior that balances self-interest with altruism.\nIn the designed moral decision-making experimental scenarios,\naffective empathy spontaneously drives altruistic motivation,\nleading the agent to prioritize altruistic behavior even at the\ncost of sacrificing its own interests. The introduction of brain-\ninspired inhibitory neural populations allows for the regulation\nof different empathy levels, demonstrating that agents with\nhigher empathy are more willing to sacrifice their interests to\nalleviate others' negative emotion, which aligns with psycho-\nlogical behavioral experiments.\nThe ultimate goal of our research is to endow intelligent\nrobots with the ability for human-like empathy, driving them\nto consistently prioritize human interests and perform ethical\nbehaviors in human-robot interactions. This paper has pre-\nliminarily achieved empathy for emotional expressions and\naltruistic moral behaviors empowered by affective empathy.\nThe significance of this work lies more in the modeling\nof the empathy and moral decision-making mechanisms of\nbiological brains, ensuring that the model possesses biolog-\nical plausibility and effectiveness. In the future, we hope to\nintegrate more models of affective computing, using robots\nas vehicles to achieve computational modeling that spans\nfrom the recognition of others' emotions to affective and\ncognitive empathy. Based on the robots' empathy ability, we\naim for them to autonomously learn altruistic, moral, and safe\nbehaviors in more complex social interaction scenarios."}]}