{"title": "Quantum-Classical Sentiment Analysis", "authors": ["Mario Bifulco", "Luca Roversi"], "abstract": "In this study, we initially investigate the application of a hybrid classical-quantum classifier (HCQC) for sentiment\nanalysis, comparing its performance against the classical CPLEX classifier and the Transformer architecture.\nOur findings indicate that while the HCQC underperforms relative to the Transformer in terms of classification\naccuracy, but it requires significantly less time to converge to a reasonably good approximate solution. This\nexperiment also reveals a critical bottleneck in the HCQC, whose architecture is partially undisclosed by the\nD-Wave property. To address this limitation, we propose a novel algorithm based on the algebraic decomposition\nof QUBO models, which enhances the time the quantum processing unit can allocate to problem-solving tasks.", "sections": [{"title": "1. Introduction", "content": "In natural language processing, the two main challenges in developing new models are the difficulty in\nacquiring high-quality data and the extensive training times required to make models more expressive[1].\nThis work focuses on the latter issue. We experiment with unconventional computing architectures,\nthe goal being to assess if and how they can help accelerate training time to obtain more expressive\nmodels. To this purpose, our choice is architectures that develop Adiabatic Quantum Computing (AQC),\nwhere the technology proposed by D-Wave is considered a standard. The reason is twofold. On one\nside AQC by its very nature solves minimization problems in the QUBO form (Quadratic Unconstrained\nBinary Optimization). On the other, the core of many AI problems is minimizing some functions by\nlooking for the values of specific parameters.\nWe choose SVM[2] over more standard Transformers models because preliminary investigations on\nSVMs that leverage AQC already exist[3] and SVM share some similarities with the attention mechanism\nof Transformers[4].\nPrecalling that, among classification tasks in natural language processing, the binary version of\nSentiment Analysis (BSA) aims to separate sentences that convey \u201cpositive\u201d emotions from those that\nconvey \"negative\u201d emotions. We reduced the BSA to QUBO and evaluated the following: 1) performance\nduring classification; 2) the time required to train the model; 3) the time required to classify new examples,\ncompared to more standard techniques implemented with heuristics and classical architectures. To\novercome the limited use of the quantum process unit (QPU) by the D-Wave hybrid solver, we also\nstarted to investigate algebraic-based alternatives to the proprietary mechanisms that split QUBO\nproblems between QPU and CPU."}, {"title": "2. Quantum Support Vector Machine for Sentiment Analysis", "content": "We choose TweetEval[5] to verify the effectiveness of SVM for BSA. TweetEval is considered a standard\nfor comparing different models and contains a sufficiently large and representative number of examples,\ni.e. Tweets extracted from https://x.com/ and labelled automatically. The \u201csentiment\" split of TweetEval\nincludes three classes: positive, negative and neutral. We choose to discard all \u201cneutral\u201d samples to avoid\nintroducing errors during learning due to examples belonging to non-expressive classes. Additionally,\nwe normalize the quantity of elements in the positive and negative classes to ensure a balanced dataset.\nSince SVMs do not natively support text processing, it is necessary to compute embeddings. Among\nthe various possibilities, SentenceBert[6] allows for capturing the contextual information of the entire\nsentence by producing a single embedding.\nFor comparison with the classical counterpart, we choose: 1) the CPLEX[7] solver, a widely used\noptimizer for solving both linear and non-linear programming problems; 2) RoBERTa[8], a deep-learning\nmodel based on BERT[9] and the attention mechanism [10]. RoBERTa allows a fair comparison as the\nmodel we use [11] is fine-tuned on TweetEval.\nBelow are the results obtained, D-Wave represents our solution.\nThe classification conducted by RoBERTa is significantly better (94.3%), although the results\nof both CPLEX and D-Wave are also well above random guesses, respectively with 76.9% and 76.1%. The\nslight difference between the solution obtained with CPLEX and that with D-Wave may be due to some\nlimitations of the current hybrid solvers, which restricted the domain of the optimisation variables from\nreal numbers to integers.\nThe time required by D-Wave to find an optimal assignment is 60% less than that of\nthe classical counterpart, with 39.2 seconds against 101.9. Although not available, it is reasonable to\nexpect an even better result if compared to the time required by RoBERTa, which we can fairly expect\nto amount to several hours of training on high-performance machines.\nThe complexity of RoBERTa architecture also affects the time required for prediction\n(136.8 seconds) by requiring more computing time than CPLEX or D-Wave, 2.2 and 33.9 seconds\nrespectively. The higher time required is due to D-Wave returning a set of optimal solutions. For each of\nthem, a model is created to apply a majority vote to establish the class in inference. Since no advantages\nemerge from using several models, it is possible to use only one of the optimal assignments, reducing\nthe time to values comparable to those of CPLEX."}, {"title": "3. Maximizing quantum boost", "content": "Using the default hybrid solver by D-Wave entails an opaque workflow due to the underlying proprietary\ntechnology. Examination of the available data, however, shows that to obtain the QPU contributes very\nmarginally, averaging 0.08%. Since the performance boost should derive from the quantum component,\nit is worth considering whether it is possible to use a greater amount of it, ideally shifting the entire\nproblem resolution onto the QPU and delegating only pre- and post-processing operations to some\nCPUs or GPUs.\nDirect access to the QPU is possible provided you manually perform certain pre-processing operations\nof the problem such as:\n1. Convert the problem into QUBO form i.e: a) to incorporate the constraints into the objective\nas penalty functions via appropriately parameterised Lagrangian relaxation[12]; b) to convert\nthe optimisation variables into binary variables; c) to transform the objective function into a\nminimisation problem.\n2. Search for the minor [13] [14] associated with the QUBO problem that can be mapped onto the\nD-Wave QPU's physical graph architecture, Figure 1a.\nBy performing these steps independently, it is possible to bypass D-Wave's proprietary technology to\ntry to maximise QPU usage. The only operation with high computational cost among those described\nis the search for the minor embedding, which is NP-complete. For this reason, Table 1 focuses on the"}, {"title": "4. Homebrewing a Hybrid Solver", "content": "Given the practical impossibility of using the QPU directly, it is unavoidable to rely on hybrid solvers.\nTherefore, we investigate how to design a hybrid solver, called QSplit, to increase the use of the QPU,\nbased on a quite simple algebraic technique that decomposes QUBO problems into smaller chunks.\nSufficiently small problems can be solved by mapping them on the QPU directly, eventually aggregating\nthe results. This technique is of general application, not limited to QUBO instances we get from SVM.\nGiven any QUBO matrix we recursively divide it into four parts as in Figure 1b: 1) $U_L$s and $B_R$s\nare themselves QUBO matrices operating on a partition of the optimization variables; 2) $U_R$ is not\nguaranteed to be upper triangular, as required by a QUBO instance, but it retains the information linking\nthe partitions $U_{L0}, B_{L0}$ and $U_{R0}, B_{R0}$ of the variables and we can safely transform it into an upper\ntriangular matrix, namely a QUBO instance; 3) (0) is a matrix composed entirely of zeros, so we ignore it.\nThe recursive subdivision can continue until the matrices reach a predetermined size. Compatible\nwith the results of Table 1, directly solving 32 \u00d7 32 matrices via QPU might be a good compromise for\nusing QSplit in real-world contexts.\nLet us see how an inductive step of the process works. Let us assume that we have classifications\noffered by $U_{L1}, U_{R1}, B_{R1}$. The classification offered by: 1) $U_{L1}$ and $B_{R1}$ can be combined to generate\nconflict-free initial assignments called $S_1$; 2) $U_{R1}$ offers classifications based on two partitions of\nvariables. In practice, it is solved by incorporating the coefficients of a matrix of zeros of size $U_{L0}$.\nAlthough this could lead to matrices larger than the bounds tested in Table 1, its graph structure is\nsmall enough to allow for manageable computation times directly on the QPU. The solutions from $S_1$\nand those obtained from $U_{R1}$ are combined by searching for compatible assignments and marking\nconflicting variables. From the conflicting values, a QUBO problem is extracted, considering only\nthe rows and columns associated with these variables, which is again solved via QPU. Of course, the\nproposed method of resolving conflicting assignments could, in the worst case, require reconsideration\nof the entire problem $U_{L0}$, nullifying the decomposition process and leading to intractable cases.\nHowever, the problem did not show up in our tests. From the set of possible assignments obtained by\nsolving conflicting classifications, duplicates are removed and only the k most promising assignments\nare retained. This is a heuristic that should help avoid an exponential increase in the number of solutions\nto be aggregated.\nTo test QSplit we ran the algorithm on some random problems with 128 variables, collecting:\n\u2022 QSplit information: 1) Cut Dim is the size of the problem solved directly via QPU; 2) CPU time\nis the total time spent on the CPU; 3) QPU time is the total time spent on the QPU; 4) Solution is\nthe best result obtained and normalised with range [0, 1].\n\u2022 QPUSampler information: 1) Total time is the sum of QPU and CPU time; 2) Solution is the best\nresult obtained and normalised with range [0, 1].\nWe can see that as the Cut Dim decreases, from 32 to 2, the time spent on the CPU increases\nsignificantly. Still, simultaneously, the time spent on the QPU systematically increases, aligning with\nthe objectives of QSplit. Up to a Cut Dim of 8, the total time required by QSplit is less than that of\nthe direct solution, significantly reducing the time needed for minor embedding computation.\nWhile QPUSampler consistently achieves the optimal value, the solutions proposed by QSplit\ndeteriorate by 35% to 50%. It is noteworthy that this performance gap increases as the Cut Dim\ndecreases. This behaviour is reasonable since, by not allowing a holistic view of the problem, QSplit\nis more likely to encounter local minima that are difficult to bypass."}, {"title": "5. Conclusion", "content": "Our investigation contributes to giving evidence that quantum computing can bring tangible benefits\nover traditional methods to solve optimization problems. Our work also confirms that a trade-off\nbetween the speed of finding a solution and the quality of the solution is an aspect that must be\nevaluated on a case-by-case basis.\nIn many application scenarios, large computational resources are not available, such as in personal\ncomputers or embedded systems. In these situations, hybrid solvers such as the one we have presented\nare good candidates to allow for a good approximation of results while significantly reducing complexity\nand computational power.\nQSplit presents a method for handling large QUBO problems that is alternative to those found\nin the literature[16] [17]. It focuses on maximising the use of the QPU. Although this approach is\nreasonable, it is not guaranteed to lead to an optimal result. The assignment produced can be improved\nby implementing more refined problem partitioning strategies[18], or by creating work pipelines capable\nof using a set of methods for finding the optimal assignment[19]."}]}