{"title": "FALCON: FEEDBACK-DRIVEN ADAPTIVE LONG/SHORT-TERM MEMORY REINFORCED CODING OPTIMIZATION", "authors": ["Zeyuan Li", "Yangfan He", "Lewei He", "Jianhui Wang", "Tianyu Shi", "Bin Lei", "Yuchen Li", "Qiuwu Chen"], "abstract": "Recently, large language models (LLMs) have achieved significant progress in automated code generation. Despite their strong instruction-following capabilities, these models frequently struggled to align with user intent in the coding scenario. In particular, they were hampered by datasets that lacked diversity and failed to address specialized tasks or edge cases. Furthermore, challenges in supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) led to failures in generating precise, human-intent-aligned code. To tackle these challenges and improve the code generation performance for automated programming systems, we propose Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization (i.e., FALCON). FALCON is structured into two hierarchical levels, from the global level, long-term memory improves code quality by retaining and applying learned knowledge, while from the local level, short-term memory allows for the incorporation of immediate feedback from compilers and AI systems. Additionally, we introduce meta-reinforcement learning with feedback rewards to solve the global-local bi-level optimization problem and enhance the model's adaptability across diverse code generation tasks. Extensive experiments are conducted and it is found that our technique achieves state-of-the-art performance, leading other reinforcement learning methods by more than 4.5 percentage points on the MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The open-sourced code is publicly available at https://anonymous.4open.science/r/FALCON-BFE0/README.md.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of Large Language Models (LLMs) has significantly advanced automated code generation (Zan et al., 2023). Models like CodeLLaMA (Roziere et al., 2023) and DeepSeek-Coder (Guo et al., 2024), tailored for code-centric tasks, have demonstrated outstanding performance across programming challenges. While LLMs excel in instruction-following through tuning (Jiang et al., 2024), they often misalign with user intent, making feedback-based adjustments critical. For example, InstructGPT (Ouyang et al., 2022) leverages reinforcement learning with human feedback (RLHF), and CodeRL (Le et al., 2022) uses compilation feedback to refine model performance. Similarly, CompCoder (Wang et al., 2022) enhances code compilability with compiler feedback, and RLTF (Liu et al., 2023a) offers fine-grained feedback on compiler errors. However, current RL frameworks generate compilation errors and overlook non-differentiable features (e.g. coding style) that affect the performance significantly (Jiang et al., 2024). To address these challenges, we propose a reinforcement learning system combining long-term and short-term memory feedback. From the global level, long-term memory tracks trends over time for higher-quality code retrieval, while from the local level, short-term memory captures recent errors and immediate feedback. The main contributions of this paper are as follows:\n\u2022 Short-Term and Long-Term Memory for Reinforcement Learning: We propose a dual-memory approach for reinforcement learning in code generation, where short-term memory enables real-time corrections and long-term memory accumulates knowledge from past runs to improve code quality and reduce repetitive mistakes.\n\u2022 Non-Differentiable Code Features into Feedback Loops: Our approach addresses the limitation of current RL frameworks by integrating non-differentiable code features like style, readability, and best practices into the feedback loop, ensuring the generated code is both functionally sound and aligned with real-world programming standards.\n\u2022 Meta-Reinforcement Learning for Generalization Across Tasks: We enhance the model's versatility by incorporating meta-reinforcement learning, allowing it to efficiently generalize across diverse programming tasks, adapt quickly to new environments, and handle a wide range of coding challenges with fewer training iterations."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 PRE-TRAINED MODELS FOR CODE GENERATION", "content": "In recent years, pre-trained language models have made significant progress in the field of code generation. Trained on large-scale code corpora, these models have demonstrated powerful code generation capabilities. For example, CodeBERT Feng et al. (2020), a model based on an encoder-only architecture, has shown impressive results. With the advent of in-context learning methods, decoder-only Transformer models have become the dominant technology for language modeling Vaswani et al. (2017). Several models, such as CodeGPT Lu et al. (2021), CodeGeeX Zheng et al. (2024), and DeepSeek-Coder Guo et al. (2024), use Causal Language Modelling (CLM) pretraining, while others like CodeT5 Wang et al. (2021) and AlphaCode Li et al. (2022) utilize an encoder-decoder architecture. Additionally, models like CodeFusion Singh et al. (2023) leverage diffusion-based techniques for code generation. These pre-trained models exhibit great potential in code generation tasks, achieving notable improvements in accuracy through various architectures and training strategies. However, they still face challenges in ensuring the syntactical and functional correctness of the generated code."}, {"title": "2.2 REINFORCEMENT LEARNING ON CODE", "content": "Reinforcement learning (RL) is a method that learns optimal strategies through reward signals from interacting with the environment Fujimoto et al. (2019). It excels in sequence generation tasks, such as enhancing performance in translation and summarization models by improving BLEU and ROUGE scores Ahn et al. (2019). Unlike traditional natural language processing tasks, code generation is more complex. Beyond syntactical correctness, the generated code must be functionally accurate, meaning it should compile and perform as expected in various scenarios. Passing unit tests is important for verifying correctness, safety, and precision, but not sufficient unless the tests are comprehensive and well-designed. Unit tests must cover diverse use cases and edge cases to ensure the code adheres to standards and meets requirements. For instance, the RL-based code generation fine-tuning framework, CodeRL Le et al. (2022), guides fine-tuning by integrating unit test signals with reinforcement learning. PPOCoder Shojaee et al. (2023) improves on CodeRL by employing Proximal Policy Optimization to refine the approach, while RLTF Liu et al. (2023a) incorporates specific error information from the code and multi-granularity feedback alongside an online framework for model training. StepCoder Dou et al. (2024) enhances the code generation process through compiler feedback and segmental optimization. However, current RL-based methods still face some limitations. Primarily, these methods lack detailed feedback on specific types and distributions of programming errors, which is crucial for identifying patterns, understanding root causes, and creating targeted interventions. This makes addressing recurring issues difficult. Additionally, the corrective mechanisms are not robust enough, often failing to provide timely, specific guidance, hindering effective learning. Additionally, the diversity of input-output examples in existing benchmarks is limited, restricting the model's ability to adapt and generalize to different or unseen problems Liu et al. (2023a)."}, {"title": "3 PROBLEM SETTING", "content": "We aim to enhance the automated code generation capabilities of Large Language Models (LLMs) by addressing key challenges in accuracy, diversity, error correction, and code quality. Formally, given a high-level specification D, the task is to generate a sequence of code tokens W = {W1,W2,...,W\u0442} that maximizes the conditional probability P(W|D, 0), where 0 represents the model parameters. The optimization objective is defined as \u03b8* = arg max\u03b8 ED\u223cD [log P(W|D, \u03b8)].\nChallenges. Effective code generation is impeded by several factors. Accuracy requires minimizing syntactical or logical errors in W to ensure correct functionality. Diversity of input-output examples X is often limited, restricting the model's ability to generalize across varied programming tasks. Efficient error correction mechanisms are necessary to identify and rectify errors in W, ensuring robust performance. Maintaining high code quality, which encompasses adherence to coding standards, style guidelines, and managing code complexity C(W), remains a persistent challenge. Additionally, adaptability refers to the model's capacity to adapt to new tasks and incorporate feedback for continuous improvement, which is constrained without robust memory mechanisms.\nFALCON Framework. To address these challenges, we propose the FALCON framework. The framework leverages both a long-term memory buffer Mlong = {(Di, Wi, Ti, Fi)}M i=1 and a short-term memory buffer Mshort = {(Dj, Wj, Tj, Fj)}N j=1, to utilize diverse feedback, enabling fine-tuning of \u03b8. Formally, FALCON seeks to optimize \u03b8 by maximizing a composite reward function R(W, F) = \u03b1T(W) + \u03b2S(W) + \u03b3C(W) + \u03b4E(W), where T(W), S(W), C(W), and E(W) represent unit test, code style, complexity, and error feedback, respectively. The optimization objective is defined as \u03b8* = arg max\u03b8 ED\u223cD [EW\u223cP(W|D,\u03b8) [R(W, F)]].\nAssumptions. The effectiveness of FALCON is based on the following assumptions: 1. Exchange-ability of the dataset D = {Di}N i=1, implying that the order of tasks does not influence model performance; 2. Independence of feedback signals F given the generated code W; 3. Sufficient memory capacity in both Mlong and Mshort to store relevant interactions without significant data loss; and 4. Feedback efficacy, ensuring that integrated feedback mechanisms provide meaningful and actionable information to guide the optimization of \u03b8.\nOptimization Objective. Given a set of tasks T = {Di}N i=1, the goal is to learn optimal parameters \u03b8* that maximize the expected composite reward across all tasks, formulated as \u03b8* = arg max\u03b8 1N \u2211N i=1 R(Wi, Fi), where Wi \u223c P(W|Di, \u03b8) is the generated code for task Di, and Fi represents the aggregated feedback. The formulation ensures continuous improvement by leveraging both historical and recent feedback to enhance code generation quality."}, {"title": "4 METHODOLOGY", "content": "In this section, we explore the FALCON framework, which integrates comprehensive unit testing with reinforcement learning, supported by both long-term and short-term memory buffers. During the code generation process, the system stores task descriptions, generated code, and various feedback (e.g., compilation results, code style, and complexity) in the long-term memory buffer. By retrieving this information, the model references high-quality code, avoids past mistakes, and ensures adherence to required standards. After generating the code, a judge model evaluates it and calculates rewards based on the feedback, which are then used to update the model's parameters through reinforcement learning. All generated code and feedback are stored for future reference and optimization. The combination of long-term and short-term memory feedback in the FALCON framework allows the model to not only learn from a wide range of historical data but also adapt quickly to new tasks based on recent performance. The overall framework is illustrated in Figure 1."}, {"title": "4.1 TASK DEFINITION: CODE GENERATION BY LLMS", "content": "The code generation task involves the automated creation of computer code W from a high-level specification of the desired behavior D. The goal of this process is to enhance the likelihood of generating code that effectively solves a given problem, which is represented as a conditional probability P(W|D). This task can be modeled as a sequence-to-sequence problem, aiming to maximize the conditional probability of generating the correct output given the input and the parameters of the LLM model, mathematically represented as:\n$\\max P(W|D, \\theta) = \\max \\prod_{t=1}^{T} p(w_t|D, \\theta, W_{1:t-1})$ (1)\nwhere \u03b8 denotes the parameters of the LLM model. The LLM model is employed to learn the conditional distribution and is trained on a set of input-output pairs. During the training phase, the parameters \u03b8 are optimized to increase the likelihood of generating accurate outputs for a given input."}, {"title": "4.2 MRLF: META-REINFORCEMENT LEARNING WITH DUAL MEMORY BUFFERS", "content": "We propose the MRLF algorithm 1, which involves random task sampling from the task distribution during training. Previous works have indicated that different data sampling strategies have varying impacts on model information extraction Zhang et al. (2024). Consequently, we implement both long and short memory sequences. The long memory strategy stores the solutions generated for each problem and the compiler feedback results, whereas the short memory sequence selects the latest samples and unit test feedback from each current iteration. To address repetitive runtime and compilation errors, the long memory strategy categorizes and stores various errors by their types and records the corresponding error lines, enabling fine-grained reward allocation."}, {"title": "4.3 LONG-TERM MEMORY FEEDBACK", "content": "Retrieving information from long-term memory helps improve code quality. We use the FAISS framework (Douze et al., 2024) to retrieve relevant historical code, feedback, and evaluation scores. Task descriptions and feedback are transformed into embedding vectors and then indexed. During code generation, a query vector from the current task retrieves the top-k most similar historical data to guide the process and avoid past errors. The prompt template is provided in the appendix. Consider a set of historical data D = {(ti, fi, ei)}n i=1, where ti represents the task description, fi is the corresponding feedback, and ei is the evaluation score. We use an embedding function \u03c6(\u00b7) to transform these tasks and feedback into embedding vectors vi = \u03c6(ti, fi) and index them with FAISS. During the code generation phase, the current task description tcurrent and feedback f are transformed into a query vector q = \u03c6(tcurrent). We compute the similarity between the query vector q and the historical vectors vi using cosine similarity cos(q, vi), and retrieve the top-k most similar historical tasks. The retrieval process can be represented as:\n{(ti1, fi1, Ci1), ..., (tik, fik, Cik)} = Top-k (vi | i = 1, 2, . . ., n) (7)\nBy referencing these most relevant historical tasks and feedbacks, the system can guide the current code generation process with past mistakes avoided and ultimate code quality improved."}, {"title": "4.4 SHORT-TERM MEMORY FEEDBACK", "content": "During the reinforcement learning phase, we utilize the generated code w to construct the reinforcement learning loss function as follows:\n$L_{r1} = \\sum_{t=S_{fine}}^{E_{fine}} R_{fine}(w_t) \\log p(w_t|D, \\theta, W_{1:t-1})$ (8)\nwhere $R_{fine}(*)$ represents the reward coefficient, and $S_{fine}$ and $E_{fine}$ denote the start and end positions of the code snippet, respectively. These values are determined based on different types of feedback."}, {"title": "Compiler Feedback.", "content": "For compiler feedback, we adopt the same settings as CodeRL:\n$R_{coarse}(W) = \\begin{cases}1.0, & \\text{if } F_B(W) \\text{ is pass} \\\\-0.3, & \\text{if } F_B(W) \\text{ is failure} \\\\-0.6, & \\text{if } F_B(W) \\text{ is runtime error} \\\\-1.0, & \\text{if } F_B(W) \\text{ is syntax error}\\end{cases}$ (9)\n$S_{coarse} = 0, E_{coarse} = T$\nwhere Rcoarse is based on compiler feedback with the start and end positions set to 0 and T.\nAdaptive Feedback. To enhance the model's efficiency in handling various programming tasks, we devise a mechanism that dynamically adjusts rewards based on the proportion of passes to failures in unit tests. This strategy encourages the model not only to pass unit tests but also to learn from failures, thereby improving its problem-solving capabilities. The reward is calculated as:\n$R_{error}(W) = -0.3 + 1.3 \\times \\frac{N_{pass}}{N_{pass} + N_{fail}}$ (10)\nCoding Style Feedback. To further enhance the quality of the generated code, we employ AI Feedback to optimize coding style. An evaluation model scores the generated code based on adherence to the expected coding style standards. The scoring system ranges from -1 to 2, and these evaluation scores are directly used as reward signals in the reinforcement learning process to guide the model toward producing higher-quality code. The coding style assessment template is provided in Table 8.\nComplexity Feedback. Just like with coding style, we use AI Feedback to evaluate complexity and calculate rewards based on the scores. The complexity assessment template is provided in Table 9.\nLong-term Error Type Feedback. Introducing a reward mechanism that combines short-term memory recall error rate, current test error, and long-term memory recall of past task performance enables the model to dynamically adjust its rewards based on past error patterns, adapt to various error types and feedback, and ultimately enhance its generalization ability:\n$R_{negative} = \\sum_{error} N_{error} \\times P_{error}$ (11)\nwhere Nerror represents the short-term memory recall, the frequency of each error type in the generated code by the model. This is the immediate feedback for the current task. Perror represents the long-term memory recall, the proportion of each error type. This is the model's performance statistics on long-term tasks, providing a cumulative history of various error types. By correct rewarding, the model can reduce the occurrence of these errors and thereby enhance the accuracy and quality of the generated code."}, {"title": "5 EXPERIMENT", "content": ""}, {"title": "5.1 QUANTITATIVE EVALUATION ON APPS", "content": "To ensure a fair comparison, we use the CodeT5 770M model as our baseline. Our benchmarks include the latest advancements that integrate reinforcement learning (RL) with large language models (LLMs), particularly CodeRL, PPOCoder, and RLTF. For evaluation, we apply the same benchmarks and settings used in these previous works. As shown in Table 1 for the experimental results, our FALCON approach delivers additional performance improvements and surpasses other RL-based methods, indicating that RL with appropriate feedback can effectively improve the model output space and thereby enhance the quality of code generation. In particular, our method achieves the highest pass@1 rates of 8.60%, 2.56%, and 1.25% in the Introductory, Interview, and Competition categories, respectively."}, {"title": "5.2 QUANTITATIVE EVALUATION ON HUMANEVAL AND MBPP", "content": "To further validate the effectiveness of our method, we evaluate the zero-shot performance of the DeepSeek-Coder-Instruct model, trained with our method on our custom dataset, using the well-established MBPP and HumanEval benchmarks. We also compare these results against other reinforcement learning methods, such as PPOCoder and RLTF. The experimental results are illustrated in Table 2.\nCompared to other reinforcement learning methods, our method consistently achieves the best performance on both the HumanEval and MBPP benchmarks. The significant advantage of our method can be attributed to its diversified feedback mechanism. Unlike other methods that may focus on a single metric, our method continuously optimizes the model's generation capability through multi-dimensional feedback. This approach demonstrates a strong ability to enhance the generation of correct code and proves particularly effective in complex tasks."}, {"title": "5.3 QUANTITATIVE EVALUATION ON CODAL-BENCH", "content": "In addition to evaluating the functional correctness of the code, we adopt CODAL-Bench, a rigorous and comprehensive benchmark for LLM consistency in coding preferences to validate the effectiveness of short-term memory feedback. DeepSeek-Coder-Instruct-6.7B model is used and the results are illustrated in Figure 3. It is found that there is a noticeable improvement in various coding preferences, particularly in Code Complexity and Coding Style after implementing the FALCON framework. This observation is attributed to the inclusion of feedback on these aspects in short-term memory. However, the improvement in Instruction Following is not as significant."}, {"title": "5.4 QUANTITATIVE EVALUATION ON SCICODE", "content": "To validate the general-purpose task capabilities of our framework, we also select the SciCode benchmark, which covers challenging research-level coding problems across natural sciences, including mathematics, physics, chemistry, and biology. SciCode decomposes the main problems into several subproblems, making it a particularly rigorous benchmark of a model's coding capabilities. Even the most advanced models nowadays, such as Claude 3.5-Sonnet and ChatGPT-4.0, can only solve 1.5% and 4.6% of the main problems, respectively. Although Deepseek-Coder-6.7B-instruct initially demonstrates a low task pass rate on this benchmark, we observe improvements on the subproblems after applying our framework due to the utilization of long-term memory mechanisms."}, {"title": "5.5 QUANTITATIVE EVALUATION ON AGENTBENCH", "content": "To further evaluate the performance of our framework, we conduct a systematic assessment on AgentBench Liu et al. (2023b), focusing specifically on long-term memory capabilities. Since our primary focus is on code generation tasks, we select three environments within AgentBench: Operating System (OS), Database (DB), and Knowledge Graph (KG). In this evaluation, we compare proprietary models (such as GPT-4 and GPT-3.5) with open-source models (such as Codellama and Qwen 2.5) Liang et al. (2024). The results reveal that the models optimized through our framework exhibit significant improvements, particularly in the OS environment with an increase of 4.8 percentage points. The experimental results are illustrated in Table 4."}, {"title": "5.6 ABLATION STUDIES", "content": "The Influence of Models. To validate the scalability and robustness of our framework, we conduct experiments with the larger model, DeepSeek-Coder-Instruct-6.7B, to further evaluate its performance. Notably, the improvements in introductory-level tasks are significant, which can be attributed to the use of long-term memory that enhances the quality of generated data and further unlocks the model's potential. The results are illustrated in Table 5.\nThe Influence of Different Feedbacks on Coding Preferences. As shown in Figure 4, ablation experiments are also conducted to validate the effectiveness of the feedback that we introduce for coding preferences. It is found that incorporating targeted feedback enhances the model's performance concerning its respective coding preferences. Notably, the optimization aiming at increasing code complexity achieves the best results. Although there are some improvements in coding style and instruction following, it is worth noting that the enhancement in code instruction following is not significant. We leave it as a topic for future investigation.\nThe Influence of Memory. To validate the impact of long-term and short-term memories on code generation capabilities, we conduct ablation experiments using CodeT5 as the base model and test it on the APPS dataset. As shown in Table 6, the experimental results indicate that both long- and short-term memory feedback enhances the model's code generation performance effectively, while the short-term memory feedback demonstrates a more significant improvement. This improvement can be attributed to the effective reward design which plays a positive role in fine-tuning the model."}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "In this work, we propose FALCON, a novel framework that enhances automated code generation by integrating long-term and short-term memory feedbacks within a meta-reinforcement learning strategy. Long-term memory retains past interactions to improve code quality and reduce repetitive mistakes, while short-term memory enables immediate adjustments based on recent feedback from compilers and AI systems. This dual-memory approach addresses limitations in existing models that struggle to align code generation with user intent, especially in specialized tasks or edge cases. By incorporating non-differentiable code features like style and complexity into the feedback loop, FALCON ensures that the generated code is not only functionally correct but also adheres to real-"}, {"title": "A EXAMPLE", "content": "We provided an example of code generation with long-term memory. When generating code without long-term memory, it often results in repetitive ValueError issues. By incorporating long-term memory to retrieve the most relevant code blocks and embedding them as context during generation, the quality of the generated code can be significantly improved. We also provided an instance of using long-term memory in SCICode. In direct generation, there were certain logical issues within the code, and the coding style lumped all formulas together. However, after employing long-term memory retrieval for assistance, the code was segmented appropriately and the logic was correctly implemented."}, {"title": "B FEEDBACK ERROR CORRELATION ANALYSIS", "content": "We performed an analysis of different feedback types and their associated error categories. We conducted experiments using CodeT5 as the base model on APPS with individual feedback and collected the occurrence frequency of various sub-errors. The results are shown in the Figure 7. The experimental results indicate that compiler feedback significantly reduces Syntax Errors and Index Errors. However, it also slightly increases the occurrence of Value Errors. This can be attributed to the corrective nature of compiler feedback on errors. Other feedback types, such as Coding Style, Instruction Following, and Code Complexity Feedback, can reduce Syntax Errors compared to having no feedback. However, their reduction is not as significant as that achieved by compiler feedback. Instruction Following Feedback specifically shows some improvement in reducing Value Errors, indicating an enhancement in instruction adherence."}, {"title": "C PROMPTS", "content": "We have compiled relevant prompt templates for code generation based on long-term memory retrieval and AI feedback."}, {"title": "D ERROR CATEGORY", "content": "Due to the differences in languages accepted by Compiler Feedback during unit tests for various language tasks, we have standardized the definition of sub-errors in Compiler Feedback. The table 11 12 13 below outlines our specifications for Python, C, and Java."}, {"title": "E SUPPLEMENTARY EXPERIMENTS", "content": ""}, {"title": "E.1 QUANTITATIVE EVALUATION ON APPS", "content": "We utilized a larger model, DeepSeek-Coder-Instruct-6.7B, as the backbone to compare popular and widely used advanced reinforcement learning methods, further validating the effectiveness and necessity of our approach. The specific results indicate that our method achieved scores of 44.7, 14.4, and 3.2 in the categories of \"Introductory\" \"Interview\" and \"Competition\" respectively. This"}, {"title": "E.2 QUANTITATIVE EVALUATION ON LEETCODE", "content": "To further validate our approach in solving real-world programming problems, we utilized the LeetCode Contest benchmark. This benchmark comprises 180 of the latest LeetCode contest problems collected from July 2023 to January 2024. Each problem is accompanied by 100 test cases, and the data format is consistent with human-eval. The results indicate that our approach can enhance the code generation capabilities of models in solving real-world programming problems. Compared to other RL-based methods, our approach achieved greater improvements and can rival models with significantly larger parameters."}]}