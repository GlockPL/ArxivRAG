{"title": "A Grounded Observer Framework for Establishing Guardrails for Foundation Models in Socially Sensitive Domains", "authors": ["Rebecca Ramnauth", "Dra\u017een Br\u0161\u010di\u0107", "Brian Scassellati"], "abstract": "As foundation models increasingly permeate sensitive domains such as healthcare, finance, and mental health, ensuring their behavior meets desired outcomes and social expectations becomes critical. Given the complexities of these high-dimensional models, traditional techniques for constraining agent behavior, which typically rely on low-dimensional, discrete state and action spaces, cannot be directly applied. Drawing inspiration from robotic action selection techniques, we propose the grounded observer framework for constraining foundation model behavior that offers both behavioral guarantees and real-time variability. This method leverages real-time assessment of low-level behavioral characteristics to dynamically adjust model actions and provide contextual feedback. To demonstrate this, we develop a system capable of sustaining contextually appropriate, casual conversations (\"small talk\"), which we then apply to a robot for novel, unscripted interactions with humans. Finally, we discuss potential applications of the framework for other social contexts and areas for further research.", "sections": [{"title": "Introduction", "content": "Foundation models are rapidly being integrated into various fields, from medical diagnostics and financial predictions to socially sensitive areas such as education, mental healthcare, and support for individuals with disabilities. Despite being aware of the inherent risks of AI hallucinations, misinformation, and bias, a recent large-scale global study revealed that 66% of respondents are still willing to use this nascent technology in sensitive areas such as personal advice and relationship counseling (Capgemini 2023). This paradox highlights the immense potential benefits of these models in addressing societal challenges while also underscoring the current concerns. A significant issue tempers the widespread adoption of these tools: the lack of comprehensive guardrails to prevent undesired behavior and ensure reliable outcomes.\nIn fields where accuracy and reliability are paramount, such as healthcare and finance, the consequences of errors can be severe. Yet, in socially sensitive domains, where the parameters of success are less tangible, the impact of missteps can be as profound. For example, a system intended to provide calming techniques in a clinic waiting room could exacerbate anxiety if it delivers generic or poorly timed suggestions. If it fails to recognize the urgency or context of a patient's distress, it may offer advice that feels dismissive or irrelevant, potentially increasing the patient's anxiety. In light of such effects, foundation models should have robust guardrails to protect users and the system's integrity.\nDesigning usable systems that impose limits on foundation models involves two key challenges. First, foundation models are based on statistical learning from vast datasets, making their internal mechanisms complex and opaque. Traditional rule-based systems use symbolic representations, which are formal and interpretable but not directly compatible with the statistical nature of foundation models. This difficulty is compounded when integrating symbolic rule-based systems that map human concepts into precise rules, a challenge akin to reconciling statistical learning mechanisms with symbolic representation systems. While neurosymbolic approaches that aim to blend statistical and symbolic methods are being explored (e.g., Garcez and Lamb 2023), effective integration remains an open area of research.\nSecond, foundation models must be able to adapt their behavior in real-time to the unique needs and contexts of individual users (Wang et al. 2023; Chen et al. 2024). Static, predefined rules often do not address the dynamic and nuanced nature of personal interactions (Raman et al. 2022). For instance, a large language model (LLM) for mental health support must respond appropriately to a user's current emotional state and context. A static rule-based approach may fail to provide suitable support during a crisis or tailor interactions based on ongoing conversations, highlighting the need for real-time adaptability to meet individual user needs.\nThese two challenges are not unique to foundation models but manifest in other areas, such as robotics. In action selection for robot systems, an agent must decide on actions to take, often using large-scale statistical models, while adhering to user-specified rules, such as \u201cdon't touch the stove.\u201d Addressing this involves techniques known as shielding (Alshiekh et al. 2018) and interactive policy shaping (Griffith et al. 2013). Shielding techniques prevent particular actions from being executed, effectively restricting the robot's behavior, while interactive policy shaping modifies the action selection policy in real time based on user input or situational changes. These approaches aim to reconcile the flexibility of statistical models with the necessity of adhering to predefined constraints (Biza et al. 2021), reflecting similar challenges faced in the context of foundation models.\nDrawing inspiration from robotic action selection tech-"}, {"title": "Related Work", "content": "Given their complexity and the vast datasets they are trained on, ensuring that foundation models behave in predictable and socially acceptable ways is a significant challenge. Researchers have explored approaches to impose constraints on these models, each with strengths and limitations."}, {"title": "Prompt Engineering", "content": "The current standard for constraining model behavior is having a good prompt. While crafting specific input prompts has shown promise in many applications (Giray 2023; Mesk\u00f3 2023; White et al. 2023), it has significant limitations when it comes to robustly constraining agent behaviors, especially in complex, dynamic, and sensitive contexts.\nLack of Robustness. One of the primary limitations of prompt engineering is its lack of robustness. While specific prompts can guide the model in controlled scenarios, they often fail to generalize across different contexts and variations. A prompt that works well in one situation might produce unexpected or undesirable results in another, leading to inconsistent behavior (Zhou et al. 2022; Huang et al. 2024).\nContext Sensitivity. Foundation models are highly sensitive to the context provided by prompts. Small changes in phrasing can lead to significantly different outputs, making it challenging to predict and control the model's behavior reliably (Denny, Kumar, and Giacaman 2023; Dong et al. 2024). This sensitivity can be particularly problematic in dynamic environments where the context is continuously changing.\nInability to Enforce Hard Constraints. Prompt engineering cannot enforce hard constraints on model behavior. While prompts can suggest or guide the model toward certain behaviors, they cannot guarantee that it will always comply with these suggestions (Niknazar et al. 2024). This limitation is critical in applications where strict adherence to ethical guidelines or safety protocols is necessary.\nTranslating to Real-World Behavior. Many real-world scenarios involve ambiguous and complex situations that are difficult to capture with prompts (Leite, Martinho, and Paiva 2013). For instance, ensuring that an LLM provides appropriate mental health support requires understanding and responding to nuanced emotional cues, which cannot be fully encapsulated in a prompt. In such cases, prompt engineering alone cannot ensure reliable and sensitive behavior.\nTemporal Constraints. Prompt engineering does not inherently support temporal constraints, where the desired behavior depends on the sequence and timing of interactions (Lyu et al. 2024; Chen and Huang 2023). For example, maintaining consistent behavior over multiple exchanges with a user is challenging to achieve through prompt design alone."}, {"title": "Constrained Reinforcement Learning", "content": "Constrained reinforcement learning (CRL) enhances traditional RL by integrating predefined constraints to ensure agents operate within specific safety, ethical, or operational boundaries. While traditional RL focuses solely on maximizing cumulative rewards, CRL incorporates additional constraints as hard limits (e.g., avoiding unsafe actions) or soft constraints (e.g., minimizing deviation from desired behaviors). CRL incorporates inductive biases through logical rules that govern the agent's behavior, applying these constraints directly to states and actions or modifying the reward function to align with the defined limits (Gu et al. 2022).\nA notable approach within CRL is shielded RL, which employs user-defined policy overrides, or \u201cshields,\" to restrict certain actions based on specific conditions, thereby minimally disrupting the RL model while enforcing desired behaviors (Garcia and Fern\u00e1ndez 2015). However, shielded RL typically relies on a dynamic model and repairing existing policies rather than adapting to evolving preferences. In contexts such as personalized healthcare or companionship, a flexible approach to adapt policies to meet context-specific needs in real-time is more suitable.\""}, {"title": "Transparent Matrix Overlays", "content": "Transparent Matrix Overlays (TMOs) is a promising technique for real-time modification of agent behavior by integrating user directives as symbolic constraints on a robot's policy (Brawer et al. 2023). This approach merges concepts from CRL and shielded RL, leveraging symbolic reasoning to enhance flexibility in behavioral adaptation.\nDemonstrated through a simulated collaborative cooking task (Brawer et al. 2023), TMOs allowed adjustments to a robot's policy without requiring extensive retraining. By applying logical rules and user-specific directives as temporary constraints, TMOs facilitated immediate changes in behavior to align with evolving user preferences. This method contrasts with traditional CRL techniques, which often require substantial retraining to incorporate new constraints, and shielded RL methods that focus on policy repairs rather than accommodating real-time preference changes. This approach balances the stability of learned behaviors with the flexibility required to meet new and evolving preferences, making it a valuable tool for interactive systems."}, {"title": "State and Action Space Abstraction", "content": "Most action selection mechanisms, like TMOs, assume a known, discrete, or discretized state space with well-defined actions. However, for foundation models, an action selection mechanism must handle continuous and possibly infinite state spaces where iterating through all possible actions or states may be impractical (Paul 2024). This requires rules that can overlay abstracted state representations or symbolic predicates to approximate the agent's internal state and action space. Instead of exhaustively evaluating every action, the agent can use these overlays to focus on a manageable subset of candidate actions or employ probabilistic sampling techniques within the space emphasized by the overlays. Furthermore, such abstraction must supersede differences in how proprietary architectures handle context, manage memory, and generate responses (Naveed et al. 2023)."}, {"title": "The Grounded Observer Framework", "content": "Social behavior is inherently emergent and complex. However, in many cases, appropriate behavior can be guided by simple rules. Just as TMOs embed rules to control behavior, we can apply similar principles to ensure that foundation models exhibit appropriate social behavior. Foundation models are analogous to the action policies generated-they are statistical models that are expensive to generate, difficult to dissect, and opaque to inspection. By imposing transparent and adaptive constraints, we can manage and direct these models to align with desired outcomes in socially sensitive domains. This can be achieved by evaluating a model's output through context-based rules and providing feedback to guide the model toward more appropriate behaviors."}, {"title": "Overview of the Framework", "content": "We begin with a foundation model, referred to as the base model in Fig 1, which generates actions in response to environmental or user inputs. Depending on the type of model, these actions can take the form of text, images, or other outputs. To provide a clear overview in this section, we will focus on LLMs, assuming that both the model's inputs and outputs are in text form, though other modalities are also applicable. To evaluate the base model's actions, feature extractors convert these actions and the surrounding context into numerical features. These features can then be analyzed as scores based on the characteristics we want to evaluate. Depending on the scenario, these extractors may also incorporate inputs from high-level planners or context observers."}, {"title": "Action Filtering", "content": "A buffer acts as a gatekeeper, as shown in Fig. 1, determining whether a proposed action should be accepted. Each overlay can be assigned a rigidity parameter (depicted as e) that defines how strictly the model must adhere to the rule. Essentially, in reference to the overhead projector analogy, this parameter controls the translucency of an overlay. Instead of enforcing a strict binary compliance where actions either fully meet the overlays or not-rigidity offers a gradient of compliance or a buffer around proposed actions.\nFor highly rigid overlays, compliance is strictly enforced. If an action or response deviates from the specified rules, the base model is required to regenerate new candidate actions. This ensures that only actions meeting the strict criteria are considered. For instance, if an overlay rule demands that responses must be empathetic, any response lacking em-"}, {"title": "Feedback Directives", "content": "The observer utilizes the overlay descriptors and rigidity to create targeted feedback prompts to the base model. We incorporate two types of feedback:\nImplicit feedback notes that the action is acceptable but offers constructive advice for improving subsequent actions. For example, if the actions are near compliance but not perfect, implicit feedback may recommend minor adjustments, such as modifying tone or phrasing. Suppose the base model generates a response that is mostly empathetic but could be softer in tone. The implicit feedback might suggest: \"Consider using a gentler tone in your responses.\u201d This allows the base model to refine its output in future iterations.\nForced feedback is employed when the base model's actions significantly deviate from the overlay constraints. When the descriptors reveal substantial misalignment with the overlay rules, the observer generates a more directive prompt, instructing the base model to focus on specific improvements until it fully complies with the constraints. The observer may issue several rounds of feedback if needed until proposed actions meet the overlay requirements.\nOverall, this feedback loop ensures that the base model continually aligns with the overlays by translating its performance on specific rules into clear instructions. In the next section, we apply this framework and demonstrate the role of each component within a social context."}, {"title": "Creating Agents Capable of Good Small Talk", "content": "Imagine a modern care home for the elderly where a state-of-the-art robotic assistant, designed to enhance residents' well-being, manages routine healthcare tasks. Alex, a resident, seeks a connection beyond the daily routine and attempts to chat with the robot:\nALEX: Hi CareBot, how's it going?\nBOT: Hello. How may I help you?\nALEX: Oh, just making conversation. Anything interesting happen in your world?\nBOT: I have access to a vast database of news articles. Would you like information on a specific topic?\nALEX: No, never mind that. The weather will be nice this weekend. How would you spend it?\nBOT: The weather forecast expects daytime highs around 75\u00b0F and comfortable evening lows of 60\u00b0F...\nToday, an essential component of designing intelligent systems is to imbue some level of speech, language understanding, and conversational behavior (Shieber 2004; Fu et al. 2022). Despite the potential for these intelligent agents to elicit meaningful interactions, the dialogue between Alex and the robot exemplifies a common shortcoming. Alex initiates a friendly exchange, expressing a desire for casual conversation with the robotic assistant. However, the robot, proficient in providing information, struggles to reciprocate the informal nature of the dialogue. Instead, the robot redirects the conversation towards its programmed functionalities, offering information and task-oriented assistance.\nAlthough the boundaries of types of conversation are always uncertain, \u201csmall talk\u201d has a recognized currency in several traditions of sociolinguistics and communication studies (Coupland 2014). It can be defined as a generally informal and light-hearted conversation with a social purpose aimed at building or sustaining interpersonal connections rather than conveying substantial information. Yet, small talk does not have a strict formula, as it is inherently flexible and context-dependent. This fluid nature presents a significant challenge for current-day LLMs, which often rely on structured and well-defined question-answer patterns.\nYet, the literature emphasizes distinct characteristics of small talk (Laver 1981; Eggins and Slade 2004). One key aspect is brevity, where responses are typically concise, avoiding unnecessary elaboration or verbosity. Another essential characteristic is tone; responses maintain a light and informal tone, steering clear of negativity, complaints, or contentious topics. Non-specificity is a hallmark of small talk, as it revolves around broad, accessible topics, deliberately avoiding highly specific details. Finally, despite its non-specific nature, small talk maintains thematic coherence, staying contextually relevant and focusing on related topics or themes to avoid disjointed elements. The delicate balance among these characteristics highlights both the nuances and the fundamental principles of effective small talk.\nA skilled conversationalist not only learns their partner's preferences over time but also adapts to them in real-time, using naturalistic cues that may be linguistic, implicit, and contextual. For intelligent agents, this means they must swiftly adjust their policies in response to high-level, imprecise, or evolving directives conveyed through natural language. Therefore, we present a proof-of-concept case study of how a grounded observer can dynamically shape an agent's behavior while adhering to high-level directives in a highly subjective social context."}, {"title": "Current Landscape of LLM Small Talk", "content": "To establish the baseline of which small talk remains a challenge, we conducted an initial study.\u00b9 Three volunteers engaged in 50 conversations each with three distinct state-of-the-art LLMs. Each model had the initial system prompt describing the role as a \"friendly companion who engages in"}, {"title": "Observer-Enabled Small Talk", "content": "It is evident from the initial study that there is a disparity in how LLMs maintain conversational momentum versus what is expected or exhibited by human speakers. Building on these insights, we apply the grounded observer framework to develop agents adept at sustaining small talk. We employ two instances of GPT-3.5, one as the base model and the other as the observer, because it performed relatively well (Fig. 2). By using the same base model prompt, we can compare the performance of an observer-enabled system against the baseline results, assessing how improvements can be achieved despite the same base model configuration.\nTo design the overlay rules, we extract specific features based on response criteria emphasized in the literature: brevity, tone, specificity, and coherence. We estimate the"}, {"title": "Chatbot Interactions", "content": "The participants in the baseline study engaged in 50 small talk conversations with our observer model, under the same experimental protocol. A total of 50 conversations with the observer model were transcribed, yielding 499 responses with an average of 9.98 responses per conversation (SD = 0.14). Of the 250 generated responses, 106 (42.4%) responses were flagged by the observer with implied feedback, and 14 (5.6%) responses triggered forced feedback for a total of 23 regeneration attempts.\nWe explored whether the observer's redirection was effective at improving the LLM's small talk behavior. To compare the responses of ChatGPT-3.5 (base model) in the baseline study to that with the observer-enabled system, we calculated the \"human-likeness\" of generated responses as described in the baseline along the four small talk criteria.\nThe Wilcoxon method with Holm-corrected significances indicates that the observer responses were significantly more human-like in that they were more concise (Z = -8.17, p < 0.0001), positive (Z = 4.53, p < 0.0001), less specific (Z = -6.76, p < 0.0001), and more thematically coherent (Z = 4.53, p < 0.0001) than the responses of the base system. Furthermore, a Brown-Forsythe test on the sum of differences across small talk criteria indicates significantly less variability in human-likeness for the observer model than the base model (F' = 15.47, p < 0.0001). As summarized in Fig. 3, the observer responses were more human-like across the criteria than the responses of the base model."}, {"title": "Robot Interactions", "content": "Agents should have the ability to engage effectively not only in virtual, text-based interactions but also in real-world, dynamic scenarios with real users. Hence, we developed an observer-enabled robot to explore how well the system navigates the nuances of novel, face-to-face interactions.\nWe used the Jibo robot which stands 11 inches tall and has 3 full-revolute axes designed for 360-degree movement. We coordinated personified behaviors such as naturalistic gaze and body movement with Jibo's onboard capabilities. Additionally, we implement a modular software architecture within the ROS framework to allow for components of the small talk system to be fully autonomous (Fig. 4).\nA within-subjects case study was conducted where 25 volunteer participants, 15 men and 10 women, ages 19 to 45 (M = 25.2, SD = 7.4), interacted with the base-only and observer-enabled system for three conversations each. Each conversation spanned a minimum of eight turns, and the order in which participants interacted with the two models was randomized. This protocol yielded 150 conversations of 1725 responses in \u2248 16.8 hours of interaction, 40.5 minutes (SD = 10.2) per participant. Following interactions with each model, participants provided open-ended feedback. We then conducted an informal thematic analysis and participant feedback was ultimately grouped into two primary themes.\nResponse Content. 21 participants expressed dissatisfaction with the base model's responses, noting its overly assistive and verbose tendencies, which led to conversations described as \"rambling\u201d, \u201cdry\u201d, and \u201clike speaking to a wall.", "Even when I spoke about my own interests, it only cared about giving me help like I was a child always in need of help...\" On the other hand, in the observer condition, 23 participants remarked on how \\\"relevant,": "human-like,", "natural\" were the robot's responses. For example, P2 stated that the robot, \u201cengaged in small talk better than most of my friends would.\u201d\nEmbodied Form. 13 participants described the impact of the physical robot form on the quality of conversation. The feedback was mostly positive, highlighting that Jibo's \u201canimated": "nd", "more than a toy\"\n    },\n    {\n      \"title\"": "Discussion"}, {"content": "Building on robotic action selection techniques, we introduced the grounded observer as a framework for aligning foundation models with desired outcomes in socially sensitive domains. Our research demonstrates this approach's usefulness by developing agents capable of seamless, contextually relevant casual conversation. In our exploratory studies, we identified gaps in existing LLMs' small talk capabilities and then enhanced a base LLM with an observer. This enhancement significantly improved the LLM's ability to follow small talk conventions, leading to more engaging and socially appropriate interactions in both virtual text-based chats and spontaneous face-to-face conversations.\nWhile the design and internal representation of different models and robotic platforms may vary, the concept of enabling an agent to observe its own compliance goes beyond specific implementations like GPT-3.5 or Jibo. Future research should explore how the grounded observer can generalize across various platforms and behavioral contexts.\nFor example, the increasing use of academic tutoring systems (Lin, Huang, and Lu 2023) introduces unique social risks (Fischer et al. 2013), such as the potential for an agent to provide feedback that is overly harsh, too lenient, or even misleading, which could negatively impact students' learning and self-esteem. To mitigate these risks, overlay rules grounded in pedagogical principles can be developed (Price et al. 2010), ensuring feedback remains supportive, specific, and tailored to the student's progress. These rules are analogous to the small talk criteria established in our study. An observer-enabled tutoring agent could dynamically adjust its feedback to foster a positive and effective learning environment, while minimizing socially inappropriate responses.\nThe grounded observer framework offers significant advantages in scalability and structure, but challenges remain in the design and implementation of overlay rules. Accurately capturing nuanced behaviors is critical, as misaligned rules can lead to ineffective or inappropriate responses. Developing systematic methods for refining these rules\u2014such as inferring rules from datasets, red-team testing (Hong et al. 2024), or other methodologies (Bommasani et al. 2021)\u2014is essential. Additionally, synthesizing effective overlay directives remains more art than science, underscoring the need for quantitative methods to evaluate the quality of generated prompts and to create reliable templates for observer-generated behavior. This could manifest, for example, as designing overlays for the observer's own behavior, essentially embedding quality evaluations into the agent itself.\nIn all, the grounded observer framework represents a step toward establishing robust guardrails for foundation models in dynamic, unstructured, and socially sensitive contexts."}, {"title": null, "content": "C = H \u00d7 W_{H} + \\frac{1}{n} \\sum_{i=1}^{n} S_{i} \u00d7 W_{i}"}]}