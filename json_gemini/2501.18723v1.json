{"title": "Scaling Policy Gradient Quality-Diversity with Massive Parallelization via Behavioral Variations", "authors": ["Konstantinos Mitsides", "Maxence Faldor", "Antoine Cully"], "abstract": "Quality-Diversity optimization comprises a family of evolutionary algorithms aimed at generating a collection of diverse and high-performing solutions. MAP-Elites (ME), a notable example, is used effectively in fields like evolutionary robotics. However, the reliance of ME on random mutations from Genetic Algorithms limits its ability to evolve high-dimensional solutions. Methods proposed to overcome this include using gradient-based operators like policy gradients or natural evolution strategies. While successful at scaling ME for neuroevolution, these methods often suffer from slow training speeds, or difficulties in scaling with massive parallelization due to high computational demands or reliance on centralized actor-critic training. In this work, we introduce a fast, sample-efficient ME based algorithm capable of scaling up with massive parallelization, significantly reducing runtimes without compromising performance. Our method, ASCII-ME, unlike existing policy gradient quality-diversity methods, does not rely on centralized actor-critic training. It performs behavioral variations based on time step performance metrics and maps these variations to solutions using policy gradients. Our experiments show that ASCII-ME can generate a diverse collection of high-performing deep neural network policies in less than 250 seconds on a single GPU. Additionally, it operates on average, five times faster than state-of-the-art algorithms while still maintaining competitive sample efficiency.", "sections": [{"title": "INTRODUCTION", "content": "Quality-Diversity (QD) optimization is a family of evolutionary algorithms designed to generate a collection of solutions that are diverse and high-performing [5, 8, 33]. Unlike traditional optimization methods that focus on identifying a single optimal solution, QD optimization encourages local competition among groups of solutions with similar characteristics to identify the fittest solution within each group. By maintaining a diverse collection of solutions - acting as \u201cstepping stones\u201d \u2013 QD avoids local optima and explores multiple search space regions. This approach reduces the risk of premature convergence, facilitates the discovery of innovative and globally optimal solutions that traditional methods often miss [4, 9, 22, 30], and enables rapid adaptation to unforeseen downstream tasks [7]. In general, QD has shown benefits across a variety of applications, from robotics [7] to design [21] and content generation [23].\nA prominent algorithm in QD is MAP-Elites (ME) [30], which is a straightforward yet potent and versatile method [1, 20]. Recently, it has been employed to generate diverse adversarial prompts [35], significantly enhancing the adversarial resilience of large language models, such as Llama 3.1 [10]. ME utilizes Genetic Algorithms (GAs) and typically employs the Directional Variation (Iso+LineDD) operator to apply random mutations to solutions, facilitating navigation within the search space. Notably, although conducting more solution evaluations in parallel reduces the total number of iterations when the evaluation budget is fixed, ME's performance remains stable and even drastically reduces its runtime, thereby showcasing strong scalability [26]. Nevertheless, its reliance on genetic variation operators lead to slow convergence in expansive search spaces, making it less effective for evolving large deep neural networks (DNNs) [7, 8, 30].\nMethods proposed to overcome this limitation include replacing or augmenting the mutation operators of ME with other gradient-based variation operators, such as policy gradients (PG) [37, 43] or natural gradients in the form of evolution strategies (ES) [34, 42]. ES-based ME algorithms have successfully scaled ME to neuroevolution. However, their reliance on numerous samples to estimate gradients renders them sample-inefficient and computationally demanding, limiting their practicality compared to PG methods [6, 13, 34]. Conversely, PG-based ME algorithms integrate ME with Deep Reinforcement Learning (DRL) [28, 29] by using PGs from a centralized actor-critic (AC) method [25, 37] to mutate solutions. While this AC training has helped ME achieve state-of-the-art performance in QD-RL tasks [14] in terms of sample efficiency [11, 12, 31], it undermines ME's potential for speed and scalability. AC training is constrained by its reliance on numerous sequential iterations necessary for convergence, which inherently slows down performance enhancement. Moreover, within a fixed evaluation budget, the necessity to increase parallel evaluations consequently reduces the total number of iterations possible, thus limiting the training's overall effectiveness and scalability.\nIn this work, we aim to efficiently evolve a collection of diverse high-performing large DNN policies, while ensuring rapid execution and scalability with massive parallelization. Specifically, we make the following contributions: (1) We introduce a PG-based variation operator, ASCII, unique among PG-based QD methods as it does not rely on centralized AC training. Utilizing the Markov Decision Process (MDP) framework, it interpolates between solution behaviors based on time step performance metrics to generate new desired behaviors. This mirrors the Iso+LineDD approach but operates in the behavior space, allowing efficient biasing of variations. Employing PGs, it then maps these behavioral changes into the solution space to mutate the solutions. (2) We integrate ASCII with ME to develop ASCII-ME, which uses both the PG-based operator, ASCII, and the genetic operator, Iso+LineDD, to mutate solutions (see Figure 1). (3) We compare our method's sample and runtime efficiency across five continuous control locomotion tasks, demonstrating that it consistently outperforms established baselines in balancing these efficiencies. (4) We conduct an extensive analysis of how parallel evaluations affect each baseline's performance and runtime under a fixed evaluation budget. Our experimental results show that ASCII-ME robustly scales up with an increasing number of parallel evaluations, significantly decreasing runtimes without compromising performance. (5) We evaluate the synergy between our operator, ASCII, and Iso+LineDD, as well as their contribution to enhancing the collection of solutions in terms of both diversity and performance.\nTo the best of our knowledge, ASCII-ME is the first PG-based QD algorithm that does not rely on AC methods, while still capable of evolving DNN policies with thousands of parameters at competitive sample and runtime efficiency. This, combined with its strong scalability on a single GPU, underscores the potential of this promising new framework for non-AC PG-based QD methods."}, {"title": "BACKGROUND & RELATED WORK", "content": ""}, {"title": "Problem Setting", "content": "We consider an agent interacting with an environment at discrete time steps t for an episode of length H. At each time step t, the agent observes a state st, executes an action at and receives a scalar reward rt+1. We model this task as a Markov Decision Process (MDP) that includes a state space S, a continuous action space A, a stationary transition dynamics distribution p(st+1|st, at) and a reward functionr: S\u00d7A \u2192 R. In this work, the policy is deterministic and represented by a neural network parameterized by a genotype x \u2208 X and denoted \u00b5x : S \u2192 A. Using its policy, the agent interacts with the environment for an episode to generate a trajectory consisting of states, actions, and rewards, denoted as {st, at, rt+1, St+1}H01. The fitness of a genotype (solution) is determined by the function F: X \u2192 R, defined as the expected return \u0395 [\u03a3\u2081rt]. The objective of quality-diversity algorithms is to populate the feature space D with high-fitness genotypes, using a user-defined feature function D: X \u2192 D that categorizes solutions by the desired diversity type."}, {"title": "Quality-Diversity Optimization", "content": "Quality-Diversity (QD) optimization [5, 8, 33] aims to generate a diverse collection of high-performing solutions to a problem by defining diversity according to a feature vector, also referred to as a set of descriptors or measures in the literature. Thus, each solution has an attributed fitness f, quantifying its quality; and a feature vector d, describing its novelty with respect to other solutions."}, {"title": "MAP-Elites", "content": "MAP-Elites (ME) is a QD algorithm that discretizes the feature space, D, into a multi-dimensional grid of cells, known as archive, and searches for the highest performing solution in each cell. ME starts by generating a set of k random solutions that are added to the archive. The process then continues iteratively until I solutions have been evaluated, following these steps: (1) a batch of k solutions from the archive are uniformly selected and modified through a genetic variation operator such as mutations and/or crossovers to produce offspring, (2) evaluate the fitness and feature vector of each offspring, placing each in the cell corresponding to its feature vector if it either improves upon the current occupant's fitness or is the first to occupy that cell."}, {"title": "Directional Variation", "content": "A common and effective variation operator used in ME to modify the solutions is the directional variation (Iso+LineDD) operator [41]. A parent, xi is selected uniformly among the elites in the archive and the offspring x is then produced using the following operator:\nx' = xi + \u03c3\u2081N(0, I) + 02 (xj \u2212 xi)N(0, 1)\nIn other words, x' is the resultant vector of xi, and another two vectors. The first vector is a randomly generated vector, while the second vector aligns with the direction of correlation between xi and a different randomly chosen elite xj. The direction of the second vector towards or away from xj is determined by a randomly generated scalar constant, and the magnitude of the vector is proportional to the Euclidean distance between xi and xj, ||xj - xi||2"}, {"title": "Accelerating MAP-Elites with Parallelization", "content": "Recent advancements in hardware acceleration, such as GPUs and TPUs, have introduced tools that parallelize task processing, significantly reducing the runtime of many optimization algorithms. Highly-parallel simulators like Brax [18] and Isaac [27] have enhanced robotic simulations, leading to the development of new QD libraries like QDax [3]. Using Brax, QDax can perform 10 to 100 times more evaluations per iteration in parallel within the same timeframe compared to traditional QD algorithms that rely on CPU-based simulations. Lim et al. [26] demonstrated that, although conducting more evaluations in parallel reduces the number of iterations when the evaluation budget is fixed, ME's performance remains stable while its runtime is reduced by up to approximately 100 times."}, {"title": "Deep Reinforcement Learning", "content": "Deep Reinforcement Learning (DRL) [28, 29] combines reinforcement learning (RL) with deep neural networks (DNNs) to approximate functions, representing policies and value functions in high-dimensional spaces. Unlike black-box optimization methods like evolutionary algorithms, DRL leverages the Markov Decision Process (MDP) framework to enhance sample efficiency by utilizing time step information.\nPolicy gradient (PG) methods is a family of DRL algorithms that aim to optimize the fitness function F(x) - find the genotype x* which maximizes F - by adjusting the genotype in the direction of the estimated fitness gradient:\nxk+1 = xk + \u03b1\u2207F(xk)\nwhere traditionally,\nF(x) := \u0395\u03c0\u03c7 [Go]\nrepresents the expected reward-to-go at the initial time step of policy \u03c0\u03c7. The reward-to-go at time step t of an episode of length H is defined as:\nGt = \u03a3 yh-trh+1.\nh=t\nBuilding on PG methods, actor-critic (AC) [25, 37] algorithms divide the optimization process into two interconnected components: the actor, which updates the policy parameters (genotype), and the critic, which estimates the value functions to guide the actor's updates. As the learning progresses through sequential steps, the critic's value estimates become increasingly accurate, providing the actor with more reliable information for gradient-based updates."}, {"title": "Related Work", "content": "The challenge of evolving diverse solutions in a high-dimensional search space has recently been prominent. Recent QD approaches, based on the ME method, emulate its strategy for maintaining solutions and using a discretized multidimensional archive. However, they diverge in their design of more efficient variation operators, and some also differ in how solutions are sampled from the archive. We categorize these algorithms into three groups: those using the TD3 [19], the OpenAI-ES [34], and the CMA-ES [24] algorithms for sampling and variation. Note that TD3 and OpenAI-ES have been applied in recent third-group variants to scale QD to neuroevolution."}, {"title": "TD3 based", "content": "PGA-ME [31] extends ME by incorporating an additional PG-based operator. It utilizes an AC method (TD3), using transitions from evaluations saved in a replay buffer to maximize an objective that is solely based on fitness. Using the trained critic, a portion of the selected solutions from the archive are mutated, while the rest are mutated using the Iso+LineDD variation operator. Additionally, the greedily trained actor from the AC training is also considered for inclusion in the archive (Actor Injection). DCRL-ME [12] builds on PGA-ME by adapting its AC training to be conditioned on the feature vectors (descriptors) of the solutions. Consequently, a descriptor-conditioned critic is employed to mutate the solutions. In addition to the mutated solutions, multiple trained actors, each conditioned on randomly selected descriptors, are considered for inclusion in the archive. QD-PG [32] replaces the GA variation operator in ME with one based on PGs. It collects transitions from evaluations, employing an AC training method (TD3) that maximizes objective functions based on diversity and fitness. Using the trained diversity and fitness critics, it then mutates the solutions for addition to the archive. The PG-based ME methods significantly improve the sample efficiency of QD optimization, evolving neural networks with thousands of parameters.\nHowever, they require extensive time for AC training, which considerably slows down the process. Additionally, the sequential nature of AC training, necessary for convergence, limits the benefits from parallelization, thereby constraining their scalability."}, {"title": "OpenAl-ES based", "content": "ME-ES [6] replaces the GA variation operator in ME with ES. It sequentially samples one solution randomly from the archive and uses the OpenAI-ES algorithm to optimize it by alternating between maximizing a novelty function and a fitness function. The gradients for either fitness or novelty are locally estimated from multiple perturbed instances of a parent solution, generating a new solution. MEMES [13] scales up ME-ES by eliminating the sequential component, thereby enabling independent ES processes to optimize multiple solutions in parallel. A portion of these processes is dedicated to optimizing for diversity, while the rest focuses on fitness. Although MEMES employs black-box optimization, it demonstrates competitive performance against PG-based QD algorithms in QD-RL tasks, when a sufficient evaluation budget is available.\nWhile MEMES theoretically offers robust scalability since optimizing additional solutions in parallel can only enhance its performance it requires significant computational resources, which may not be feasible on standard consumer hardware."}, {"title": "CMA-ES based", "content": "CMA-ES [24] has been integrated with ME to form CMA-ME [17]. Unlike ME, which applies mutations to randomly selected solutions from the archive, CMA-ME strategically uses CMA-ES to mutate solutions, focusing on intrinsic objectives to enhance archive quality. Since its inception, many new algorithms that build on and improve CMA-ME have been developed [16, 38, 39]. Among these, PPGA [2] stands out as the state-of-the-art algorithm operating under the Differential Quality Diversity (DQD) [15] framework. It integrates Proximal Policy Optimization (PPO) [36] with CMA-MAEGA [16] and alternates between estimating fitness and feature gradients using PPO and maintaining a population of coefficients. These coefficients are optimized to maximize archive improvement by linearly combining the gradients.\nPPGA achieves competitive results compared to state-of-the-art QD algorithms for QD-RL tasks; however, its requirement for a significant number of evaluations makes it sample inefficient."}, {"title": "METHODS", "content": "In this work, we introduce the Action Sequence Crossover with performance-Informed Interpolation for MAP-Elites (ASCII-ME) algorithm (see Appendix A.1). It is an extension of ME that targets evolving DNN policies while unlike other extensions retaining the speed and scalability of ME. Following the standard ME loop, a portion of the solutions uniformly sampled from the archive is processed using the Iso+LineDD operator, while the remainder are processed by our new Action Sequence Crossover with performance-Informed Interpolation (ASCII) operator (see Figure 1). This operator creates action-based variations by interpolating between two action sequences based on time step performance metrics and applies PGs to modify the solutions. Subsequently, all processed solutions from both operators are evaluated for one episode to determine their eligibility for inclusion in the archive, while the evaluation information is stored in a buffer for use in the next iteration. One action sequence is associated with a sampled solution from the archive, and the other is derived from information in the buffer, which is linked to the evaluation of a previously mutated solution. A key distinction between ASCII-ME and other PG-based QD algorithms is that ASCII-ME does not utilize a centralized AC training, making the algorithm faster and more scalable with massive parallelization."}, {"title": "ASCII Operator", "content": "Inspired by the Iso+LineDD variation operator, as detailed in Section 2.2.2, ASCII samples two action sequences and, based on their time step performance, interpolates between them to generate a new sequence. This sequence is then used to define the gradient step for policy updates through back-propagation. Specifically, ASCII iteratively applies the following variations to the sampled genotype xi over e iterations to produce its offspring:\nAx\u2081 = [\u00e3o, ..., aH-1], Axj = [ao, ..., aH-1].\nAx\u2081 = Ax\u2081 + \u03bb\u2081N(0, I) + \u03bb2Zxi,xj [Axj - Axi]\nxi = xi + J\u0394Axi\nwhere \u2206Ax; denotes the difference between the new and the old action sequence, reflecting the desired changes to the policy outputs. Here, Ax, and Axj represent the action sequences consisting of time step actions for each time step in an episode, associated with the policies of genotypes xi and xj, respectively (more details in Section 3.2.1). The term Zx\u2081,x; is a performance-based weight matrix derived from the evaluation of corresponding genotypes (more details in Section 3.3). Jx; represents the Jacobian matrix of the policy associated with xi, mapping changes in the action space back to the genotypic space (more details in Section 3.2.2).\nThe variation presented by Equation 5 adopts the concept of Iso+LineDD operator, however, it differs in two significant ways: 1) the variation is applied to an action sequence rather than directly to the genotype; 2) a different weight is applied to each time step action difference based on the performance of the corresponding time step. These weights are encapsulated in the matrix Zxi,xj, contrasting with the uniform application of 02N(0, 1) to all parameter differences in Iso+LineDD. Furthermore, we set \u03bb\u2081 = 0 since random exploration is already provided by ASCII-ME from the Iso+LineDD operator in the genotypic space. The analysis of synergies between these two potential sources of random exploration - parameter-based through Iso+LineDD and action-based through ASCII - is deferred to future work."}, {"title": "Action Sequences and Jacobian", "content": "Let xi represent the genotype uniformly sampled from the archive for mutation, and \u00b5x; its corresponding deterministic policy, referred to as the \"mutated policy\". The states visited by \u00b5x\u2081, along with their associated rewards-to-go computed from the rewards collected during a single evaluation episode, are denoted by {s, Gi}H-1. These states and rewards-to-go are also taken from the archive along with the genotype xi. Moreover, let {sj, aj, Gj}H-1represent the information sampled from the buffer. This collection comprises the states and actions taken by \u00b5x; during the evaluation phase of the previous iteration, where xj represents a mutated genotype from that iteration. We refer to the deterministic policy associated with the genotype xj, \u03bcx;, as the \u201ctarget policy\u201d.\nThe buffer often includes data from genotypes not included in the archive, providing no guarantee of data quality. Despite this, it has proven more effective than using only data from the archive, likely because it introduces more diversity and promotes greater exploration of the search space (see Appendix A.2)."}, {"title": "Action Sequences", "content": "To enable a fair comparison, we need to compare how the action sequences from the two policies differ when faced with the same states. For this, we take the state sequence of the target policy as reference, {s}H-1. Consequently, we introduce an \"imaginary\" action sequence for xi. This sequence represents the actions that the mutated policy \u03bcx\u2081, would have taken if it had followed the state sequence of the target policy px\u2081. This imaginary sequence, represented by Ax\u2081, and the actual action sequence of the target policy \u03bcx;, represented by Axj, are both depicted as block vectors in RH|A|. Specifically:\nAx\u2081 = [\u00e3o, ..., aH-1], Axj = [ao, ..., aH-1].\nwhere at = px\u2081 (s) and at = px, (s) are both in R|A|."}, {"title": "Jacobian", "content": "The Jacobian matrix Jx; \u2208 RH|A|\u00d7|X| is defined as a block matrix:\nJx\u2081 = [Bo(xi) B1(xi) ... BH-1(xi)],\nwhere\nBt (xi) = \u2202\u03bcx(s)/\n\u2202x|x=xi\nand each Bt is defined in R|A|\u00d7|X|."}, {"title": "Performance based Weight Matrix", "content": "The performance based weight matrix is defined as a block matrix:\nZxi,x; = diag(zo (Xi, Xj), ..., ZH\u22121 (Xi, xj)),\nwhere zt (xi, xj) = zt(xi, xj)I, with I \u2208 R|A|\u00d7|A|. The weight zt (xi, xj) influences the interpolation between two actions at time step t based on four factors: 1) the performance gain of each action, measured by comparing the rewards-to-go, as defined in Section 2.3; 2) the similarity of the states in which each action is taken, determined by the cosine similarity function; 3) the similarity between the actions, using a Squared Exponential Kernel; and 4) a clipping mechanism to prevent large updates."}, {"title": "Rewards-to-go Difference", "content": "To compare the performance of the two action sequences at each time step, we compute the difference between the reward-to-go of the target policy, \u00b5x;, and the reward-to-go of the mutated policy, ux\u2081, at each time-step t:\n\u2206G\u2081 = G\u00a6 \u2013 G\u00a6."}, {"title": "State Cosine Similarity", "content": "The rewards-to-go for the mutated policy, \u03bc\u03c7\u2081, are computed based on a different state sequence, {s}}, during which it took different actions, not corresponding to those in Ax\u2081. To address this discrepancy, we adjust AGt by multiplying it by the cosine similarity between the states at each time step, which quantifies how similar the states are:\nmax b,  .\nThe predefined constant b, a non-negative threshold not exceeding 1, is usually set to b = 0.25. By using the max function in conjunction with b, we ensure that all state pairs have a minimum positive weight, thus preventing weights from reaching zero or becoming negative. Zero weights would make the update matrix more sparse, whereas negative weights would reverse the sign of AGt, potentially providing misleading information about the performance of actions at each time step."}, {"title": "Action Kernel", "content": "To mitigate the inaccuracy of using the action sequence of the mutated policy, \u00b5x\u2081, based on a state sequence it did not follow, we incorporate a Squared Exponential Kernel:\nk(at, at) = exp.\nThis kernel penalizes significant deviations between actions taken in response to the same state, thereby assigning less weight to mutations involving two policies that significantly diverge from each other.\nCombining these three mechanisms, we get:\n\u03b2t (xi, xj) = k(at, at) max b,.\nRecall that at, \u0101t, s, s\u012f, and AG\u2081 are all time step information collected from the mutated policy, \u03bcx\u2081, and the target policy, \u00b5x;, thus making \u1e9et dependent on genotypes xi and xj."}, {"title": "Proximal Clipping like in PPO", "content": "We further include a predefined threshold, e, usually set e = 0.8, such that if the value of the kernel falls below this threshold and AGt is negative, then that time step is not considered in updates:\n0 if k(at, at) <\u20ac and AGt < 0,\nzt (xi, xj) = f(xi, xj) otherwise.\nThis ensures that we do not degrade the performance of the mutated policy, \u03bcx\u2081, by applying large updates where it is already outperforming the target policy, px\u2081, and actions deviate from each other significantly. This approach, akin to the clipping mechanism in PPO [36], helps avoid excessively large mutations."}, {"title": "EXPERIMENTS & RESULTS", "content": "The goal of our experimental evaluation is to answer the following three questions: (1) How does ASCII-ME compare to AC-based QD methods in terms of sample and runtime efficiency? Moving to considerations of operational scalability, (2) for a fixed evaluation budget, does ASCII-ME benefit from employing a larger batch size - evaluating more solutions in parallel - and thus fewer iterations? This question pertains to both final performance and total runtime. Further exploring the method's components, (3) how well does the ASCII operator synergize with the Iso+LineDD operator, and what is the contribution of each to improving the archive?"}, {"title": "Evaluation Tasks", "content": ""}, {"title": "Baselines", "content": "In our evaluation, we compare ASCII-ME with five baseline methods, as implemented in QDAX [3]: ME [30], MEMES [13], PGAME [31], DCRL-ME [12], and PPGA [2]. ME lays the groundwork for many advanced QD algorithms, while MEMES is a leading ES-based ME method. PPGA is state-of-the-art among methods utilizing CMA-ES, and for its implementation, we adapted the authors' code to suit our specific tasks. DCRL-ME, recognized as the leading QD algorithm, surpasses all others in sample efficiency for QD-RL tasks. We further include PGA-ME in our comparison because, unlike DCRL-ME, it is a state-of-the-art algorithm that does not utilize descriptor information, similar to our method. Additionally, to tailor ASCII-ME to our experimental needs, we conducted hyperparameter tuning, resulting in variations from ME's hyperparameters, such as in the initialization of the archive."}, {"title": "Metrics", "content": "We consider three main metrics for evaluation: 1) The QD-Score which is the total fitness of all solutions in the archive. In the task at hand, all fitness values are positive, preventing the penalization of algorithms for identifying more solutions. This score reflects both the quality and diversity of the population. 2) The Coverage which is the percentage of occupied cells in the archive, demonstrating coverage of the BD space. 3) The Maximum Fitness which is the fitness of the highest-performing solution in the archive.\nUsing the L40S (48GB) GPU, we replicate each experiment 20 times using random seeds and we report p-values based on the Wilcoxon-Mann-Whitney U test with Holm-Bonferroni correction for the quantitative results. The source code for ASCII-ME will be made available upon acceptance."}, {"title": "Evaluation Procedure", "content": "Increasing the batch size in an algorithm while maintaining a constant evaluation budget can significantly reduce runtime. However, this increase results in fewer iterations, potentially deteriorating the final performance. To fairly evaluate methods in terms of sample and runtime efficiency, we identify the optimal batch size for each algorithm after one million evaluations. This optimal size aims to achieve a balance between low runtime and high final performance, with all other hyperparameters set as recommended by the original authors (see Appendix A.4). The results presented in the main experiments (see Section 4.5.1), highlight the configurations that best balance performance and runtime. We also detail the methodology used to select these configurations and analyze the impact of batch size variations on each algorithm (see Section 4.5.2).\nNote that, in addition to the common ME evaluation cycle, PPGA incorporates a PPO evaluation cycle utilizing parallel environments for efficient training. To find the optimal configuration for PPGA, we tune the number of environments used in parallel to maintain a constant update-to-data ratio, similar to the original setup. Due to structural differences, PPGA is omitted from the scalability analysis in Section 4.5.2 to prevent unfair comparisons."}, {"title": "Results & Analysis", "content": "Figure 2 compares the results of ASCII-ME against all baselines on all tasks in terms of both sample and runtime efficiency. The first observation is that ASCII-ME achieves the highest QD scores among all baselines in the unidirectional tasks both under conditions of equal runtime and equal evaluation usage, with the exception of the Ant Uni task. Specifically, with a budget of one million evaluations, ASCII-ME achieves on average 25% higher QD score than the state-of-the-art algorithm, DCRL-ME, while being on average five times faster in the Hopper Uni and Walker Uni tasks (p < 1 \u00d7 10-7). A similar trend persists throughout the algorithms' runtime. In the Ant Uni task, ASCII-ME outperforms all baselines except for DCRL-ME in achieving higher QD scores when all algorithms are run for the same duration, until at least the first quarter of the run (p < 5 \u00d7 10-6). In terms of sample efficiency, with a budget of one million evaluations, it outperforms all baselines except the PGA-ME and DCRL-ME (p < 1 \u00d7 10\u22127).\nRemarkably, ASCII-ME does not use any descriptor information, unlike DCRL-ME, yet it achieves competitive results in omnidirectional tasks. In these tasks, where the fitness function discourages solution diversity, ASCII-ME still demonstrates a coverage of 100%. Throughout at least the first third of the omnidirectional tasks runtime, ASCII-ME achieves the highest QD scores among all baselines (p < 1 \u00d7 10-6), performing up to an average of 358% higher QD score than DCRL-ME (p < 5 \u00d7 10-8). It is only towards the end of training that DCRL-ME surpasses ASCII-ME (p < 5 \u00d7 10-4), with an average of 4%. In terms of sample efficiency, ASCII-ME achieves higher QD scores than all baselines except DCRL-ME, when using one million evaluations (p < 5 \u00d7 10-7).\nRegarding coverage, ASCII-ME achieves the highest scores with fewer evaluations and shorter runtime than all baselines in all tasks. This robust coverage underscores the efficacy of ASCII-ME in exploring the solution space. However, the strong coverage of ASCII-ME, along with its relatively lower maximum fitness score in Ant Uni task, highlights its inability to identify exceptionally high-performing solutions. This limitation likely stems from the fact that critics make more accurate performance estimations than our method, impairing its search efficiency in tasks with larger state and action spaces, like Ant Uni."}, {"title": "Increasing Parallelization", "content": "To determine the optimal batch size for each algorithm for use in the experiments in Section 4.5.1, we establish an \"efficiency score\" for each batch size by following these steps: 1) Perform min-max normalization on QD scores and runtimes for the varying batch sizes used by each algorithm in each task. 2) Adjust runtime scores by subtracting them from 1, ensuring higher values of both QD score and adjusted runtime score indicate better performance. 3) Compute the efficiency score for the varying batch sizes used by each algorithm in each task by multiplying the normalized QD score with the adjusted normalized runtime score. 4) Calculate the mean efficiency score for the varying batch sizes used by each algorithm across all tasks. 5) Select the batch size that yields the highest final efficiency score for each algorithm (see Appendix B).\nFigure 3 illustrates the impact of varying batch sizes on the final performance and runtime of each algorithm across all tasks after one million evaluations. Increasing the batch size generally reduces runtime for all algorithms; however, the rate of reduction diminishes with larger sizes due to the constraints of parallel evaluations on the considered GPU. Despite these changes in batch size, the performance of ASCII-ME and ME remains remarkably stable, with a mean coefficient of variation (CV) of 2% and 3% across all tasks, respectively. In contrast, the AC-based QD algorithms, DCRL-ME and PGA-ME, show an average decline in performance as batch sizes increase, with mean CVs of 13% and 8% across all tasks, respectively. MEMES is the only algorithm that demonstrates an overall positive trend in performance with increasing batch sizes, though it achieves relatively smaller reductions in runtime, primarily due to its higher computational demands.\nFocusing on the PG-based algorithms, we observe significant differences in how batch sizes influence QD scores between methods using centralized AC training and ASCII-ME, which does not. As outlined in Section 2.4.1, in DCRL-ME and PGA-ME, AC training utilizes data from evaluations stored in a buffer during each iteration. Although data availability increases in each iteration, the total number of iterations decreases as batch sizes grow. This dynamic raises three strategic questions regarding the AC training: 1) Should the mini-batch size per gradient step be increased to accommodate larger batch sizes? 2) Should the number of critic gradient steps be adjusted to match the total number done with smaller batch sizes? 3) Should the training parameters remain unchanged? These strategies were implemented and tested across three of the five tasks. Based on the \"efficiency score\", strategy 3 proved to be the most efficient for both DCRL-ME and PGA-ME (see Appendix A.3). While strategies 1 and 2 achieved higher QD scores, they did not reduce runtime; in fact, they often resulted in longer runtimes, leading to lower efficiency scores. Using smaller batch sizes across more iterations, with fewer critic gradient updates per iteration, enhances AC training by promoting incremental learning and iterative policy refinement. This strategy mitigates overfitting in individual iterations and improves the critic's accuracy in policy evaluation, leading to significantly better overall performance of AC-based QD algorithms. Although extensive task-specific hyperparameter tuning could potentially enhance scalability for these algorithms, ASCII-ME offers straightforward scalability. With ASCII-ME, users can scale up through parallelization without needing to adjust parameters, thus benefiting from reduced runtime and consistently high performance."}, {"title": "Synergizing ASCII with Iso+LineDD", "content": "To test the synergy between the operators ASCII and Iso+LineDD, we include the QD score of variants of ASCII-ME after one million evaluations, varying the proportions of solutions mutated by Iso+LineDD across all tasks (see Figure 5). In this setup, n% Iso+LineDD corresponds to (100-n)% ASCII. Our findings indicate that on average, neither 0% nor 100% Iso+LineDD provides superior performance compared to the mixed variants, indicating the necessity of using both operators. Interestingly, in the omnidirectional tasks, no solutions can be found when our operator ASCII is not used. ASCII-ME, which includes 50% of Iso+LineDD, achieves performance similar to both 25% and 75% Iso+LineDD and is highly robust across all tasks.\nMoreover, we track the number of solutions added to the archive when mutated by each operator for all algorithms that use a PGbased operator and the Iso+LineDD operator, aiming to understand each operator's contribution to archive improvement (see Figure 4). We observe that, except in Ant Uni task, the contribution of ASCII to the archive is consistently higher than that of other PG-based operators. This indicates the robustness of ASCII alone at diversifying and improving the quality of the archive. Additionally, the contribution of Iso+LineDD in ASCII-ME is consistently higher than in ME for omnidirectional tasks. This suggests that ASCII effectively navigates the search space to regions where"}]}