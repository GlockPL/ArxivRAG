{"title": "StatLLM: A Dataset for Evaluating the Performance of Large Language Models in Statistical Analysis", "authors": ["Xinyi Song", "Lina Lee", "Kexin Xie", "Xueying Liu", "Xinwei Deng", "Yili Hong"], "abstract": "The coding capabilities of large language models (LLMs) have opened up new opportunities for automatic statistical analysis in machine learning and data science. However, before their widespread adoption, it is crucial to assess the accuracy of code generated by LLMs. A major challenge in this evaluation lies in the absence of a benchmark dataset for statistical code (e.g., SAS and R). \u03a4o fill in this gap, this paper introduces StatLLM, an open-source dataset for evaluating the performance of LLMs in statistical analysis. The StatLLM dataset comprises three key components: statistical analysis tasks, LLM-generated SAS code, and human evaluation scores. The first component includes statistical analysis tasks spanning a variety of analyses and datasets, providing problem descriptions, dataset details, and human-verified SAS code. The second component features SAS code generated by ChatGPT 3.5, ChatGPT 4.0, and Llama 3.1 for those tasks. The third component contains evaluation scores from human experts in assessing the correctness, effectiveness, readability, executability, and output accuracy of the LLM-generated code. We also illustrate the unique potential of the established benchmark dataset for (1) evaluating and enhancing natural language processing metrics, (2) assessing and improving LLM performance in statistical coding, and (3) developing and testing of next-generation statistical software \u2013 advancements that are crucial for data science and machine learning research.", "sections": [{"title": "1 Introduction", "content": null}, {"title": "1.1 Background and Motivation", "content": "Statistical analysis plays an important role in machine learning and data science. The programming capabilities of large language models (LLMs) have unlocked new possibilities for automatic statistical analysis. Current versions of LLMs (e.g., ChatGPT, OpenAI et al. 2024, and Llama, Touvron et al. 2023) can generate statistical code in SAS (SAS Institute Inc. 2025) and R (R Development Core Team 2025), two widely used languages for statistical analysis in academia and industry. However, before these models see widespread adoption, it is essential to evaluate the accuracy and reliability of the code they generate.\nOne of the key challenges in evaluating the performance of LLMs for statistical program- ming is the lack of a systematically designed benchmark dataset, particularly for SAS and R. Unlike fields such as natural language processing (NLP) and computer vision, where stan- dardized datasets are widely available for model evaluation, statistical programming lacks such resources. This gap hinders the systematic assessment of LLM-generated statistical code, making it difficult to quantify performance in terms of correctness, executability, and output quality. Thus, there is a pressing need to develop a comprehensive benchmark dataset tailored for statistical code evaluation.\nTo bridge this data gap, we introduce StatLLM, an open-source dataset specifically de- signed to evaluate the performance of LLMs in statistical analysis. The StatLLM dataset is structured into three key components: (i) statistical analysis tasks, (ii) LLM-generated SAS code, and (iii) expert human evaluation scores, which are detailed in Section 2. Below, we provide an overview of each component and its contribution.\nThe first component, statistical analysis tasks, comprises a diverse collection of prob- lem descriptions covering a broad range of statistical topics, including data visualization, descriptive statistics, hypothesis testing, regression and ANOVA, generalized linear models, survival analysis, model selection, and nonparametric statistics. These topics are commonly taught at the upper undergraduate and master's levels in statistics programs. Each task is accompanied by detailed dataset descriptions and human-verified SAS code, ensuring a rig- orous evaluation of LLMs' ability to interpret statistical requirements and generate accurate statistical code.\nThe second component, LLM-generated SAS code, consists of code produced by three LLMS: ChatGPT 3.5, ChatGPT 4.0, and Llama 3.1 70B. For each model, we provide a problem description and dataset details, prompting it to generate SAS code for the statis- tical analysis. SAS is selected due to its structured syntax, which allows most statistical analyses to be performed within ten lines of code, making human evaluation more efficient"}, {"title": "1.2 Related Work", "content": "For AI technology to gain widespread acceptance, its reliability and performance must be rigorously tested (Hong et al. 2023). Several studies have evaluated LLMs' programming capabilities. Atkinson (2023) analyzed GPT 3.5's performance across multiple programming languages. Bubeck et al. (2023) examined GPT 4.0's performance on 40 coding problems, showing it outperforms GPT 3.5 in code generation. Similarly, Yeadon et al. (2024) com- pared the coding performance of human students, GPT 3.5, and GPT 4.0 on university-level programming tasks. Song et al. (2025) conducted human evaluations of LLMs' performance in SAS programming across various statistical tasks.\nBenchmarks have been developed to assess LLMs' programming abilities, primarily for"}, {"title": "1.3 Overview", "content": "The remainder of the paper is organized as follows. Section 2 introduces the StatLLM datasets. Section 3 illustrates their use and impact on statistical analysis and LLM research. Section 4 discusses ethics and fairness. Finally, Section 5 presents the conclusions."}, {"title": "2 The StatLLM Dataset", "content": null}, {"title": "2.1 The Structure of StatLLM", "content": "This section provides an overview of the structure of StatLLM, as illustrated in Figure 1. StatLLM consists of three key components: statistical analysis tasks, LLM-generated SAS code, and human-verified SAS code, which are detailed in Sections 2.2, 2.3, and 2.4, respec- tively. The dataset is available in the GitHub repository: https://github.com/yili-hong/ StatLLM, with a description of its folder structure provided in Appendix A."}, {"title": "2.2 Statistical Analysis Tasks", "content": "A key objective of the StatLLM is to develop a comprehensive set of statistical analysis tasks to evaluate LLM capabilities. During the 2022-2023 academic year, researchers assembled a diverse collection of datasets from various public online resources. The data collection spanned multiple disciplines, including biological sciences, medical research, engineering, and social sciences. The final compilation consists of 65 CSV datasets with detailed descriptions, which are used to create 207 distinct statistical analysis tasks. Each problem includes both a problem description and its implementation in SAS programming language. This material is systematically organized for analysis purposes.\nEach data description is documented with key aspects like dataset names, contextual background, variable specifications, and comprehensive variable descriptions."}, {"title": "2.3 LLM-Generated SAS Code", "content": "We first give a brief description of the three LLMs chosen for the study."}, {"title": "2.3.1 GPT 3.5 and GPT 4.0", "content": "ChatGPT (OpenAI et al. 2024) is an advanced AI model by OpenAI, built on a generative pre-trained transformer. It processes large-scale data to enable natural-language interactions, assisting with queries, problem-solving, research, and programming tasks.\nChatGPT has evolved across versions, improving problem-solving, efficiency, and adapt- ability in coding. GPT 3.5, released in March 2022, features a transformer-based architecture with dense attention layers, supporting code generation and debugging, particularly for be- ginners (Shirafuji et al. 2023). However, its performance declines in complex, multi-step programming tasks. Sherman (2024) showed that GPT 3.5 struggled with non-compliant C code, frequently missing obvious errors while focusing on minor issues, highlighting its limitations in complex code analysis.\nReleased on March 14, 2023, GPT 4.0 significantly improves comprehension, reasoning, and accuracy over previous versions (OpenAI et al. 2024). Its refined architecture enhances complex problem-solving, code generation, and debugging across languages. In this study, we use both GPT 3.5 and GPT 4.0 to generate SAS code."}, {"title": "2.3.2 Llama", "content": "The Llama series, developed by Meta AI, includes open-source LLMs for NLP tasks like code generation. Llama 1 debuted in February 2023, followed by the fully open-source Llama 2 in July 2023. Code Llama (Rozi\u00e8re et al. 2023), built on Llama 2, rivals proprietary models like OpenAI's Codex on benchmarks such as HumanEval and MBPP. However, it struggles with complex contexts, prompt dependency (Chen et al. 2021), and security risks (Wong et al. 2023). Llama 3 enhances multilingual understanding, coding, reasoning, and tool use (Dubey et al. 2024). Available in configurations like Llama 3.1 8B, 70B, and 405B, it scales from 8 to 405 billion parameters, impacting code generation quality. The 8B model suits moderate tasks, the 70B model excels in complex analysis, and the 405B model handles advanced, multi-task programming. In this study, we used Llama 3.1 70B to generate SAS code."}, {"title": "2.3.3 SAS Code Generation", "content": "For the three selected LLMs, we utilize their APIs to generate SAS code for statistical analysis tasks. Each model receives a data description and problem statement, prompting it to produce SAS code as a solution. The generated code is then extracted from the LLM output, filtered to remove irrelevant content, and stored for future evaluation and assessment."}, {"title": "2.4 Human Evaluation Scores", "content": "Song et al. (2025) conducted a comprehensive evaluation of SAS code generated by LLMs. Here, we provide a brief overview of the rating process, while the detailed 10 rating criteria can be found in Song et al. (2025). Figure 3 illustrates the flowchart for human evaluation."}, {"title": "3 Illustrations and Impact", "content": "In this section, we discuss and illustrate how the StatLLM dataset can be used to advance research in statistics and data science."}, {"title": "3.1 NLP Metrics and Benchmarking", "content": "As discussed earlier, human evaluation is labor-intensive and does not scale efficiently. With both LLM-generated and human-verified SAS code available, we can leverage automatic"}, {"title": "3.2 Improving NLP Metrics for Assessing Statistical Code", "content": "Here, we illustrate one approach to leveraging the StatLLM dataset to improve NLP metrics for assessing statistical code. Specifically, we use the eight automatic NLP metric scores from Section 3.1 as inputs to predict the total human score. To increase sample size, we aggregate all rating scores from three LLM models, resulting in a total of 621 observations. The dataset is then randomly split, with 75% for training and 25% for testing.\nWe consider four statistical and machine learning (ML) methods for prediction: linear regression model, random forest (Breiman 2001), XGBoost (Chen et al. 2015), and a deep learning model with a simple structure (Goodfellow et al. 2016). Figure 6 illustrates the model training and correlation computation process using machine learning methods to pre- dict human rating scores based on NLP metric scores. As before, Pearson correlation is used for evaluation.\nTable 2 presents the correlations between the predicted and true total human scores using various new metrics built with machine learning methods on the test set, as well as the correlations between true total human score and existing NLP metrics. The results show that all ML-based metrics outperform existing NLP metrics in correlation. Among them, XGBoost achieves the highest correlation of 0.434, while the best-performing existing metric, Rouge-2, has a correlation of 0.367, representing an improvement of approximately 18%. This highlights the potential of leveraging the StatLLM dataset to enhance existing NLP metrics."}, {"title": "3.3 Assessing and Improving LLM Performance in Statistical Programming", "content": "Established datasets can be highly valuable for advancing research aimed at improving the robustness of code generated by LLMs. Song et al. (2025) highlighted the weaknesses in LLM-generated statistical code, noting that it often lacks executability and produces inaccurate outputs. Thus, significant room for improvement remains. In this section, we explore how StatLLM can be leveraged to enhance the performance of LLMs in statistical programming."}, {"title": "3.4 Developing and Testing of Next-generation Statistical Software", "content": "StatLLM can facilitate the development and testing of next-generation statistical software. The way humans communicate with statistical software has evolved (Min et al. 2024), tran- sitioning from command-line interfaces and compiled languages to graphical user interfaces (GUIs) with menus and clicks. These advancements have made statistical tools more acces- sible to non-statisticians for data analysis. By leveraging LLMs, next-generation statistical software will enable users to interact with the software using natural language.\nAs an example, we developed an R Shiny app to demonstrate the potential of StatLLM in enabling automatic statistical analysis in R through R Shiny (Chang et al. 2024). Figure 7 showcases an AI-powered automatic statistical analysis. As a preliminary step, this app leverages LLMs for automatic statistical analysis. When users upload their dataset along with a corresponding problem statement, the app forwards this information to ChatGPT 4.0, which generates R code for tasks ranging from data preprocessing and model fitting to visualization. The app then executes the generated code in R and displays the results through an interactive user interface. Additionally, the system supports iterative code refine- ment, allowing users to specify modifications for further customization. By streamlining this process, the app eliminates the need for manual R programming and running, significantly simplifying statistical analysis.\nTo evaluate the app's performance, we tested it using the auto dataset and a problem description from StatLLM. The dataset includes automotive specifications and pricing for various car models. Specifically, we instructed the app to perform a regression analysis, modeling car price as a function of miles per gallon (mpg) and weight. The app automatically generated and executed R code to fit a linear regression model, displaying both the model fitting results and diagnostic plots."}, {"title": "4 Ethics and Fairness", "content": "The StatLLM data collection process in this work does not involve individual privacy con- cerns. To the best of our knowledge, we have ensured that the data remains transparent and accountable. To enhance the fairness and reliability of rating score collection, we have fully incorporated design of experiments strategies, including randomization, blocking, and replication (Wu and Hamada 2011). The questions and datasets used for analysis are sourced from publicly available materials, with a focus on statistical analysis tasks at the undergrad- uate and master's levels.\nOur data collection procedure carefully applies randomization and ensures that rater identities remain anonymous during the scoring of LLM-generated code. To minimize po- tential biases, we employ both treatment randomization and randomized assignment when distributing code for evaluation (i.e., obtain the rating score). Additionally, LLM identities are concealed during grading to prevent biases related to model attribution.\nTo further control for bias, the sequence of problem descriptions presented to graders is randomized, reducing potential order and time effects. Moreover, each problem description and its corresponding LLM-generated code are evaluated by three independent graders in- dependently, helping to reduce individual scoring variability and enhance the reliability of the ratings."}, {"title": "5 Conclusions", "content": "In this work, we establish an open-source dataset to evaluate the performance of LLMs in statistical analysis. The dataset, named StatLLM, serves as a benchmark for AI assurance, particularly in AI-assisted statistical coding and AI-automated coding. StatLLM consists of three key components: statistical analysis tasks, LLM-generated SAS code, and human evaluation scores. These components provide valuable resources for addressing research gaps and enabling new studies, as illustrated in Section 3. Additionally, the dataset enhances the reproducibility of AI-driven statistical methods across various applications.\nFurthermore, StatLLM is highly extendable, allowing for the inclusion of additional com- ponents along multiple dimensions. Future expansions may incorporate more advanced sta- tistical analysis tasks, LLM-generated code in other statistical programming languages, such as R, and alternative evaluation metrics. By continuously evolving, StatLLM can support broader research efforts and further advance the integration of AI in statistical analysis."}, {"title": "A Online Data Repository", "content": "The dataset is available at https://github.com/yili-hong/StatLLM. The repository is systematically structured into three main directories: Statistical_Analysis_Tasks, LLM_ Generated_SAS_Code, and Human_Evaluation_Scores, each containing well-organized sub- directories to facilitate efficient access to statistical problems, datasets, and SAS code."}, {"title": "1. Statistical Analysis Tasks", "content": "This directory contains all essential resources required for statistical task solving. The file Summary.csv serves as a metadata file mapping each statistical task to corresponding DataDescription, DataSet, SAS code solutions and SAS code for data reading.\n\u2022 ProblemDescription: Provides detailed statistical task statements.\n\u2022 DataDescription: Stores detailed dataset descriptions, including dataset name and variable definitions.\n\u2022 TaskDatasets: Includes raw dataset files in CSV or XLSX format.\n\u2022 HumanVerified_SAScode: Contains manually verified SAS solutions for each task.\n\u2022 DataReading_SASCode: Contains SAS scripts designed to load and preprocess datasets."}, {"title": "2. LLM-Generated SAS Code", "content": "This directory stores SAS code generated by different large language models (LLMs): GPT35, GPT4, and Llama. Each folder contains SAS scripts generated by the respective model under folder SAS_code_only."}, {"title": "3. Human Evaluation Scores", "content": "This directory contains the three group scores, which are CodeQuality_Score, CodeExecuta bility_Score, and CodeOutput_Score, along with the Total_score for the three LLMs, evaluated across 207 statistical tasks. These scores are derived based on the SAS code found in the LLM-generatedSASCode/SAS_code_only folder. The corresponding task IDs are documented in Summary.csv.\nTo obtain the SAS results, users must run the corresponding DataReading_SASCode file along with either the Humanverified_SAScode or the SAS_code_only file from the LLM- generated outputs (refer to Summary.csv). For example, if solving the problem PD0007 with dataset DS0003, the user should first run DataReading_SASCode/DRO003.txt to properly import the dataset into SAS. After successfully loading the data, the user can then exe- cute either Humanverified_SAScode/SC0007.txt (if using the manually verified solution) or LLM_Generated_SAS_Code/GPT4/SAS_code_only/sas_query7.txt (if testing the LLM- generated code)."}, {"title": "B Calculation of NLP Metrics", "content": "To make the paper self-contain, in this appendix, we provide a brief introduction on the calculation of NLP metrics used in this paper."}, {"title": "B.1 BLEU", "content": "BLEU (Bilingual Evaluation Understudy) is the most commonly used metric in machine translation (Papineni et al. 2002). It measures the overlap of contiguous n-token sequences between machine-generated and human-referenced texts and ranges from 0 to 1, with higher scores indicating greater similarity. In this paper, BLEU has been adapted to assess the similarity between LLM-generated and human-verified SAS code. Before computing n-gram overlaps, the SAS code is tokenized into elements such as PROC procedures, variables, and models. In particular, BLEU is computed as,\nBLEU = BP x exp (\\sum_{n=1}^{N} w_{n} log(p_{n}));\nwhere the brevity penalty (BP) is\nBP = \\begin{cases}\n1, & \\text{if } c > r \\\\\nexp(1-\\frac{r}{c}), & \\text{if } c \\leq r\n\\end{cases}\nwhere c is length of the LLM-generated SAS code, r is length of human-verified SAS code, N is the maximum n-gram order (e.g., 4), $w_{n}$ is the weight for n-gram of order n (e.g., $\\frac{1}{N}$),\nand $p_{n}$ is modified precision for n-grams as in,\np_{n} = \\frac{\\sum_{g \\in G} min\\{Count_{c}(g), MaxRefCount(g)\\} }{\\sum_{g \\in G} Count_{c}(g)} ,"}, {"title": "B.2 ROUGE", "content": "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a series of metrics (Lin 2004) that can be used to measure the quality of LLM-generated SAS code, denoted by L, by comparing it against the human-verified SAS code, denoted by H. In this paper, we consider ROUGE-1, ROUGE-2, and ROUGE-L.\nROUGE-1 measures the overlap of unigrams (single words or tokens) between H and L. we define,\nR1 Recall = \\frac{\\sum_{w\\in H}min\\{Count(w, L), Count(w, H)\\} }{\\sum_{w\\in H} Count(w, H)} ,\nR1 Precision = \\frac{\\sum_{w\\in H}min\\{Count(w, L), Count(w, H)\\} }{\\sum_{w\\in L} Count(w, L)}\nwhere Count(w, X) represents the number of times uni-gram w happens at SAS code X. The the ROUGE-1 F1 score is computed as,\nROUGE-1 F1 = \\frac{2 \\times R1 Precision \\times R1 Recall}{R1 Precision + R1 Recall}\nThe computing of ROUGE-2 is similar, which measures the overlap of bi-grams (consecutive word pairs) between L and H.\nROUGE-L measures the longest common subsequence (LCS) between H and L, which is computed as,\nLCS(H, L) = max |LCS(H, L)|,\nwhere S denotes all common subsequences. We compute,\nRL Recall = \\frac{LCS(H, L)}{|H|}\nRL Precision = \\frac{LCS(H, L)}{|L|},\nwhere |H| and |L| represent the number of tokens in H and L. The ROUGE-L F1 score is computed as,\nROUGE-L F1 = \\frac{(1 + \\beta^{2}) \\times RL Precision \\times RL Recall}{\\beta^{2} \\times RL Precision + RL Recall} ,"}, {"title": "B.3 METEOR", "content": "Meteor (Metric for Evaluation of Translation with Explicit ORdering) (Banerjee and Lavie 2005) incorporates both precision and recall of uni-gram matches between the LLM-generated SAS code and the human-verified SAS code, and it is then adjusted by a fragmentation penalty. The Meteor is computed as,\nMETEOR = F_{mean} \\times (1 \u2212 p),\np=(\\frac{c}{m})^{\\beta}\nwhere p is fragmentation penalty, $F_{mean}$ is harmonic mean of recall and precision, cis the number of chunks, m is the umber of matched uni-grams between the LLM-generated SAS code and human-verified SAS code, and \u03b3 and \u03b2 controls the magnitude of penalty."}, {"title": "B.4 CodeBERTScore", "content": "BERTScore leverages embeddings from pre-trained models like BERT to assess the semantic similarity of generated text (Zhang et al. 2020). CodeBERTScore is an advanced evaluation metric derived from BERTScore, specifically designed for code evaluation (Zhou et al. 2023). Given the encoded tokens for human-verified SAS code with mask, and the encoded tokens for LLM-generated SAS code with mask, we compute $CB_{Precision}$ and and $CB_{Recall}$ as the precision and the recall of CodeBERTscore, respectively. Then, we compute the CodeBERT score as follow,\nCB = \\frac{10 \\times CB_{Precision} \\times CB_{Recall}}{9 \\times CB_{Precision} + CB_{Recall}}"}, {"title": "B.5 Character F-score (chrF)", "content": "The Character n-gram F-score (chrF) is an evaluation metric (Popovic 2015) that can be used to measure the similarity between LLM-generated SAS code, denoted by L, and human- verified SAS code, denoted by H. The common n-gram count $C_{n}$ between H and L is:\nC_{n} = \\sum_{\\gamma\\EN_{n} (L)} min\\{f(\\gamma, L), f(\\gamma, H)\\}\nwhere f(y, L) indicates the frequency of an n-gram y in text L and same for f(y, H), and $N_{n}(L)$ denotes the set of character n-grams extracted from text L. The total candidate n-grams in the LLM-generated SAS code denoted as $T_{L}$ is defined as follows:\nT_{L} = \\sum_{\\gamma\\EN_{n} (L)} f(\\gamma, L)."}]}