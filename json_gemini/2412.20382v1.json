{"title": "Natural Language Fine-Tuning", "authors": ["Jia Liu", "Yue Wang", "Zhiqi Lin", "Min Chen", "Yixue Hao", "Long Hu"], "abstract": "Large language model fine-tuning techniques typically depend on extensive labeled data, external guidance, and feedback, such as human alignment, scalar rewards, and demonstration. However, in practical application, the scarcity of specific knowledge poses unprecedented challenges to existing fine-tuning techniques. In this paper, focusing on fine-tuning tasks in specific domains with limited data, we introduce Natural Language Fine-Tuning (NLFT), which utilizes natural language for fine-tuning for the first time. By leveraging the strong language comprehension capability of the target LM, NLFT attaches the guidance of natural language to the token-level outputs. Then, saliency tokens are identified with calculated probabilities. Since linguistic information is effectively utilized in NLFT, our proposed method significantly reduces training costs. It markedly enhances training efficiency, comprehensively outperforming reinforcement fine-tuning algorithms in accuracy, time-saving, and resource conservation. Additionally, on the macro level, NLFT can be viewed as a token-level fine-grained optimization of SFT, thereby efficiently replacing the SFT process without the need for warm-up (as opposed to ReFT requiring multiple rounds of warm-up with SFT). Compared to SFT, NLFT does not increase the algorithmic complexity, maintaining O(n). Extensive experiments on the GSM8K dataset demonstrate that NLFT, with only 50 data instances, achieves an accuracy increase that exceeds SFT by 219%. Compared to ReFT, the time complexity and space complexity of NLFT are reduced by 78.27% and 92.24%, respectively. The superior technique of NLFT is paving the way for the deployment of various innovative LLM fine-tuning applications when resources are limited at network edges.", "sections": [{"title": "1 Introduction", "content": "Supervised Fine-Tuning (SFT) is the most commonly employed method for fine-tuning Large Language Models (LLMs). As a foundational step in model fine-tuning, its application enables LLMs to better adapt to tasks across various domains and more effectively address specialized issues. For instance, in the alignment tasks, both Reinforcement Learning with Human Feedback (RLHF) [Ouyang et al., 2022] and Direct Preference Optimization (DPO) [Rafailov et al., 2024b] utilize SFT to provide a solid initial condition for the LLM, thereby leading the learning process to be more efficient and stable. Typically, for mathematical problem solving, researchers employ Chain-of-Thought (CoT) [Wei et al., 2022] to annotate problems and answers, then use SFT to fine-tune the model for better tackling mathematical questions [Feng et al., 2024a] [Chu et al., 2023] [Wang et al., 2022].\nAs the application field of LLMs continues to expand, fine-tuning with small-scale, domain-specific data remains challenging due to the inefficiency of existing methods in utilizing limited samples. Consequently, researchers are progressively exploring renovated fine-tuning methods to optimize models. Recently, Reinforced Fine-Tuning (ReFT [Luong et al., 2024] or RFT [OpenAI, 2024]) has garnered widespread attention in the academic community for its superior performance in terms of accuracy increment. It employs Reinforcement Learning (RL) for the fine-tuning of model parameters, and thus achieving more efficient model optimization within the same data scale. However, introducing RL into LLMS results in a significant increase in time and space complexity, thereby raising the barriers to the deployment and utilization of this technology, especially in a mobile and/or dynamic network environment. Meanwhile, ReFT does not entirely replace SFT. In practical applications, it still requires the SFT to warming-up to ensure that the model can more effectively adapt to specific tasks or datasets [Luong et al., 2024]\nTo address these issues, in this paper, we propose a novel minimal data fine-tuning method named Natural Language Fine-Tuning (NLFT). Compared to previous methods, NLFT utilizes natural language as the supervising signal and employs a simple minimal data fine-tuning algorithm to enhance the efficiency of fine-tuning. To better illustrate the differences among SFT, ReFT, and NLFT, we give the following analogy.\nLLM is analogous to a student, and LLM's fine-tuning process is similar to the learning process of the student. Then, SFT, ReFT, and NLFT represent three individual learning processes of the student. Given learning math reasoning as an example, in SFT, the student studies in parrot-fashion, where the student is expected to write down a predetermined answer when seeing some particular question after screening numerous pairs of questions and standard answers. In ReFT, the student first obtains the basic technique of solving math reasoning problems by several epochs of SFT. Then, in order to further improve the technique, ReFT requires the student to submit answer sheets which include the detailed analysis for leading the math problem solution. In ReFT, a score is given for each answer sheet by comparing it to the standard answer. By the score, the student adjusts the strategy of math reasoning, which is similar to reinforcement learn-ing. Since the target of the student is to achieve a high score as much as possible, lots of rounds of submitting the answer sheet and obtaining feedback from the evaluating system are needed. However, in NLFT, the student's learning process more closely resembles a self-study method. When the answer sheet hits majority scoring points, which represents the student is on a good studying track at the beginning, the student will re-answer the exam based on the standard answer and compare it with the previous one to identify the scoring points (see the upper part in Fig. 2c). When the answer sheet contains lots of incorrect points, the student will re-answer the exam based on both the standard answer and the judgment from external evaluating system (see the bottom part in Fig. 2c). After that, the losing points is identified by similar comparison. Repeating these steps, the student's ability improves as the score hitting and point losing become clear. It should be noted that the initial \"answer sheet\" may not necessarily be completed by the efforts of the student. That is if the student's ability is not enough, learning from others' answers is encouraged, especially from \"good students\". If the student is capable, self-studying is a better strategy than the use of the student's answers.\nSpecifically, for the reasoning output, NLFT evaluates the conditional probability variations of each token under different prompt conditions to allocate the saliency token. On this basis, the model refines the loss function based on the saliency level of each token, thereby enabling more efficient fine-tuning of the model. An overview of our algorithm is shown in fig.2.\nIn summary, the contributions of this paper are as follows:\n1.  This paper introduces NLFT, a token-level natural language fine-tuning algorithm. In contrast to previous response-level fine-tuning methods that convert natural language into scalar rewards, our approach directly leverages natural language and conducts token-level annotation. This method significantly improves the information utilization efficiency within datasets, thereby reducing the data requirements for NLFT. As a result, by the use of only tens to thousands of data entries, NLFT can achieve domain-specific fine-tuning of LLMs.\n2.  In this paper, we propose a novel Minimal data fine-tuning that eliminates the need for a warm-up phase, which is required by ReFT process. Furthermore, NLFT achieves an accuracy rate that significantly surpasses SFT with a small number of data entries. Besides, due to the efficient algorithm design in both GPU memory usage and training time, our method offers a substantial advantage in time and memory utilization compared to other methods (such as ReFT).\n3.  NLFT exhibits the extraordinary ability to solve the intrinsic pitfall of overfitting phenomenon associated with small-sample data [Li et al., 2019]. Moreover, as a token-level fine-tuning approach, NLFT possesses an outstanding interpretability compared to response-level fine-tuning. NLFT achieves a 64.29% accuracy with only 50 training data samples, exhibiting a faster improvement rate compared to SFT and outperforming SFT by 219%, as shown in Fig. 1."}, {"title": "2 Related Work", "content": "In the past few years, much of the research in the area of LLM fine-tuning has focused on scalar rewards, which are less efficient than directly using explicit natural language feedback. This is because scalar reward-based methods provide only an indirect understanding of semantic information, which can be suboptimal. In contrast, natural language feedback enables the expression with more delicate and complex preferences. For example, Contrastive Unlikelihood Training (CUT) [Xu et al., 2023] uses negative judgments to align values at the token level. Building on this idea, Natural Language Reinforcement Learning (NLRL) [Feng et al., 2024b] redefines RL principles within a natural language representation space, further demonstrating how natural language can facilitate both efficient policy optimization and improved interpretability."}, {"title": "2.2 Token-level LLMs Fine-Tuning", "content": "Response-level fine-tuning has played a significant role in pretraining LLMs, but it often faces challenges in terms of training difficulty and stability. In contrast, token-level fine-tuning has emerged as a promising alternative. Recently, several notable works have been proposed to improve token-level fine-tuning. For instance, [Rafailov et al., 2024a] extends DPO to a token-level Markov Decision Process (MDP), which enhances alignment with the autoregressive structure of LLMs and optimizes credit assignment. Additionally, [Zhong et al., 2024] introduces Reinforced Token Optimization (RTO), which combines token-level rewards with DPO and Proximal Policy Optimization (PPO) to improve policy learning efficiency significantly. These methods demonstrate the substantial potential of token-level fine-tuning in improving model performance, particularly in complex tasks where precision and consistency are crucial."}, {"title": "3 Method", "content": "In this section, we will provide a comprehensive overview of the token-based fine-tuning algorithm NLFT, a novel fine-tuning algorithm that concentrates on natural language CoT and its result. Assuming the input of LLM is X and the reasoning outcomes from CoT is Y {Y1, Y2, ..., Yn}, by conducting different input prompt X, we can obtain the conditional probabilities of each token within the same CoT output under different input conditions. After that, by comparing these conditional probabilities, the saliency tokens are allocated and we perform token-level loss calculation, thereby achieving fine-grained tuning of the LLM. In short, the NLFT algorithm yields significantly superior results with time and space complexity compared to SFT. The detailed process of NLFT is shown in Algorithm 1: Natural Language Fine-Tuning. A more intuitive illustration can be observed in Fig. 3."}, {"title": "3.1 Preliminary Considerations", "content": "Recently, an interesting work on AI alignment via linguistic feedback, Contrastive Unlikelihood Training(CUT) [Xu et al., 2023] employs contrastive learning to fine-tune LLMs to modify erroneous output based on human negative judgments. CUT reveals that, in contrast to the error-free tokens, erroneous tokens experience significant variations in conditional probabilities under the two distinct input conditions of presence and absence of judgment. Building upon this insight, we formulate the following hypotheses through extensive experimental observations: Denoted different prompt inputs as X, the CoT reasoning results as Y = {Y1, Y2, \u2026, Yn }, and the conditional probability of each token under different input as P(yt X, Yt\u2212), we assume that:\n1.  When the output Y aligns with expectations (that is, the output is correct), the conditional probability P(yt X, yt-) of key scoring token (Saliency Token) Yt is significantly higher with prompt input Xstandard =< question, standard answer > compared to Xbase = <question>.\n2.  When the output Y falls to meet expectation (that is, the output is incorrect), the conditional probability P(yt X, yt-) of key scoring token (Saliency Token) yt exhibit substantial variations under three prompt inputs: Xbase =< question >, Xjudge =< question, judgment >, and Xstandard =< question, standard answer >.\nBased on these assumptions, we introduce the NLFT algorithm. Firstly, depending on whether the output meets expectations, the conditional probabilities of each token are obtained under different prompt inputs. After that, Saliency Tokens are located through contrastive learning, and the phrases containing these tokens are located using a semantic similarity cluster. Finally, a token-level loss function is constructed to achieve fine-grained fine-tuning."}, {"title": "3.2 Token-level Conditional Probability Analysis", "content": "Formally, the process of generating results through the natural language CoT can be decomposed into a sequence of next-token prediction actions. To be specific, take the CoT reasoning result as Y = {Y1, Y2, \u2026, Yn }, where each< yt > is a token inferred based on the given input X and the previous output Yt = {Y1, Y2, \u2026, Yt\u22121}. Besides, each yt has its own conditional probability function P(yt|X, yt-). As previously stated, the conditional probabilities of salient tokens undergo significant changes with varying inputs X. Building on this observation, we categorize the reasoning results Y into correct and incorrect outcomes, then selectively perform conditional probability lookup and collection for each category.\nCorrect Output. When the CoT reasoning output Y is correct, the input X is divided into two categories: Xbase =< question > and Xstandard =< question, standard answer >. Then, for each token Yt of output Y, we will have two conditional probability P(yt Xbase, Yt-) and P(yt Xstandard, Yt-). In following section, we will allocate the saliency token based on these conditional probabilities.\nIncorrect Output. When the reasoning output Y is incorrect, the input X is divided into three categories: Xbase =< question >, Xjudge =< question, judgement >, and Xstandard =< question, standard answer >. Then, for each token yt of output Y, we will have three conditional probabilityP(yt Xbase, Yt-), P(yt|Xjudge, Yt-), and P(yt Xstandard, Yt-). In the following section, we will allocate the saliency token based on these conditional probabilities."}, {"title": "3.3 Probability-driven Saliency Token Allocation", "content": "After obtaining the conditional probabilities, we proceed to classify them and allocate the saliency tokens. Similarly, the allocation strategies are divided into two categories: correct outcomes and incorrect outcomes.\nCorrect Output. When the CoT reasoning output Y is correct, we set the threshold conditional probability pcorrect. When the conditional probability P(yt|Xstandard,Yt-) > pcorrect, it is considered that the token is more likely to be adopted under Xstandard condition than the threshold conditional probability. This implies a greater correlation between the token and the Xstandard condition, hence it is allocated as a saliency token. After that, we perform semantic clustering around these saliency tokens to identify their associated phrase and designate them as sub-saliency tokens. Finanlly, the remaining token are categorized as irrelevant tokens.\nIncorrect Output. For the incorrect CoT reasoning output Y, we cannot simply set a threshold conditional probability, as the conditional probabilities for all tokens are generally lower in such instances. Therefore, we calculate the following two ratios,\nr1 = P(Yt|Xjudge, Yt\u2212)/ P(yt Xbase, Yt\u2212)\nr2 = P(Yt|Xjudge, Yt-\u2212)/ P(yt Xstandard, Yt-)\nIf a token is a saliency token, then its conditional probability under Xjudge condition should be much higher than that under Xbase and Xstandard. Consequently, its corresponding ratios r\u2081 and r2 will also be higher. Therefore, if a token has both its corresponding ratios r\u2081 and r2 exceeding a preset value ro, and its own conditional probability surpasses the threshold pincorrect, the token will be allocated as a saliency token. After that, we also perform semantic clustering around these saliency tokens to identify the associated phrase. Given that these tokens are close to the saliency token under incorrect situations, we directly categorize them as irrelevant tokens and assign a scale of zero to them in subsequent contrastive learning processes."}, {"title": "3.4 Token-level Loss Calculation", "content": "In this section, we will assign scale values based on the previously allocated saliency tokens and proceed with contrastive learning. Similarly, the scale strategies are divided into two categories: correct outcomes and incorrect outcomes.\nCorrect Output. As we shown before, in the correct CoT reasoning output, the tokens are classified into three kinds: saliency tokens, sub-saliency tokens, and irrelevant tokens. For each token, we have scales,\nS(yt) =  1+ (P(Yt | Xstandard-Y-) - poorrect)/(1 - pcorrect), if yt \u2208 Ysaliency,\n P(yt | Xstandard, Yt\u2212)/ pcorrect,  if yt \u2208 Ysub-saliency,\nC2/C3, if yt \u2208 Yirrelevant.\nwhere C1, C2, and c3 are hyper-parameters and C2 < C3. It can be observed that under such configuration, all three scales increase as the conditional probability P(yt|Xstandard) grows. Meanwhile, the scales of the saliency tokens are the largest and always exceed 1, while the scales of the sub-saliency tokens are consistently greater than that of the irrelevant tokens.\nIncorrect Output. In the incorrect CoT reasoning output, the tokens are classified into two kinds: saliency tokens and irrelevant tokens. For the irrelevant tokens, we will set the scales to 0, as within incorrect Output, we only wish to consider the saliency tokens. For the saliency tokens, we have scales,\nS(yt) = 2/(1+e-(r1-ro))\nThe scales are larger than 0 and smaller than 1, and these scales also increase as the conditional probability P(yt Xjudge) grows.\nAfter obtaining the scales of each token, we get our final loss function,\nL = 1/N( \u2211 S(yt) x log P(yt|Xbase, Yt-) +  \u2211 S(yt) \u00d7 (1 - log P(yt|Xbase, Yt-))\nYt EYcorrect\nYtEYincorrect"}, {"title": "4 Experiments", "content": "We conduct experiments on the Mathematics problem dataset GSM8K [Cobbe et al., 2021]. It offers mathematics problems in natural language form, standard solution processes, and numerical standard answers. The training set of GSM8K contains 7473 entries, while the test set contains 1319 entries. We employed data prompting and CoT prompting to obtain the analysis and result (that is, the reasoning output).\nLearning from teaching: When the accuracy of the base model to be fine-tuned is low, we choose to use other models with better performance to generate the analysis and result (that is, the reasoning output). We refer to this process as \"teaching\". Specifically, We utilized LLAMA3-8B-Instruct [Roziere et al., 2023] to obtain the analysis and result of the case.\nLearning from self-study: When the model has the capacity to generate a certain percentage of correct responses, we proceed to let the trained model learn from its own answers and produce results. We refer to this process as self-study. In this scenario, we directly utilize the trained model to produce the reasoning output.\nWhen the reasoning output is incorrect, we leverage the GPT-40 [OpenAI, 2023] to acquire the annotations for judgment. The instruction prompt for judgment is provided in Appendix. A.1."}, {"title": "4.2 Experimental Setup", "content": "We conduct experiments on the LLAMA3-8B base model [Roziere et al., 2023].\nTraining Subset Setup: To investigate the learning capacity of NLFT with a small dataset, we randomly shuffle the data in the training set. According to the shuffled index order, we take the first 400, first 800, first 25%, first 50%, and 100% respectively to construct training sets for experimental preparations.\nHyper-parameter Settings: All experiments are carried out on two A800 GPUs, which is four times lower than the requirement demanded by reinforcement learning-based fine-tuning methods such as ReFT. Besides, we select the AdamW optimizer [Loshchilov and Hutter, 2019] and the cosine learning rate scheduler. The batch size is set to be 4 and the learning rate is set to be 5 \u00d7 10-5. If a small dataset is utilized, the model is trained for 10 epochs, while with an extensive dataset, training is trained for 3 epochs. For the parameters in NLFT, pcorrect is set to be 0.95, pincorrect is set to be 0.01, and ro is set to be 1.5. For the scale hyper-parameters, C1, C2, and c3 are set to be 5, 0.3, and 0.6. Detailed hyper-parameter configurations are shown in Appendix B.\nEvaluation: We utilize the full dataset to assess the accuracy of natural language CoT reasoning Output from fine-tuned LLM. The evaluation process employs the same prompt templates as those used during the Training phase. In our settings, the temperature is set to 0.6 during text generation, and maximum generation length is 512 tokens."}, {"title": "4.3 Baseline", "content": "We compare our model NLFT with SFT [Ouyang et al., 2022] and ReFT [Luong et al., 2024] baselines. To ensure a fair comparison, we make sure that the hyperparameter settings for the SFT baseline match those of the NLFT experiments. The details on the hyperparameter settings is shown in Appendix B. Besides, our study concentrates on natural language fine-tuning, hence we only select the corresponding component of ReFT, that is, the CoT-N portion. Although the procedural language (CoT-P) component of ReFT shows better performance on the GSM8K dataset for mathematical reasoning tasks, it significantly deviates from our experimental setup, leading to its omission."}, {"title": "4.4 Results", "content": "Fig. 4 compares the performance of NLFT and SFT baseline on GSM8K dataset. Starting from the same initial base model at proportion of 0 1. 1, we can observe that NLFT consistently achieves higher accuracy over SFT, with NLFT achieving an accuracy rate above 70% across all four percentage datasets, whereas SFT is between 44% and 46%. NLFT outperformed SFT with an accuracy improvement of over 25%. Furthermore, the change in the proportion of the training dataset has little impact on the accuracy of both NLFT and SFT, which indicates the boundary effect on accuracy improvement by expanding data size will gradually decrease. Therefore, it is reasonable to shift our focus to a smaller size of training dataset especially lower than 25%, to fill the gaps of accuracy details from 0 to 25%.\nTo investigate the per-In our experiments, LLAMA3-8B base model failed to provide any reasoning for getting answer, and its generation mostly just repeats the instruction prompt. The accuracy claimed in LLAMA3 official website is achieved by LLAMA3-8B-Instruct under our reproduction."}, {"title": "Time Cost Analysis", "content": "Under 800 data samples, we conducted a comparative analysis of the training time required for NLFT, SFT, and ReFT across 10 epochs. Since ReFT algorithm cannot be executed in two-GPU configuration, we recorded time consumption of each algorithm under an eight-GPU configuration for fair comparison. As shown in Fig. 8, ReFT required an average of 30 minutes per epoch, whereas NLFT, due to the use of more GPUs, saw a significant reduction in training time compared to 26.1 minutes the two-GPU setup, averaging 6.5 minutes.\nIt is worth noticing that, the time consumption ratio between NLFT and SFT is around 3. NLFT involves three times the forward inference processes compared to SFT, hence its time complexity constant is at least 3. Despite the increased constant term, NLFT still qualifies as a lightweight fine-tuning algorithm with linear time complexity."}, {"title": "Memory Use Analysis", "content": "Fig. 7 illustrates the runtime GPU memory usage of each fine-tuning algorithm. With a two-GPU configuration, SFT averages a total memory usage of 44.55 GB, while NLFT averages 46.87 GB. NLFT's memory usage is only 5.2% higher than SFT's, which still falls within the category of lightweight fine-tuning algorithms. In contrast, ReFT requires an average of 599.57 GB of total memory, which is not in the same order of magnitude as NLFT. Regarding hardware configuration and memory usage, NLFT not only matches SFT's requirements but also significantly outperforms reinforcement learning-based fine-tuning algorithms like ReFT, offering a unique advantage in terms of hardware resource demands."}, {"title": "5 Analysis", "content": "Compared with SFT and ReFT, the NLFT shows a robust capacity to mitigate overfitting in minimal data scenarios. Fig. 6 shows the performance of SFT and NLFT with a minimal dataset of 50 samples across 1-16 epochs. It can be observed that SFT exhibits pronounced overfitting, while NLFT maintains a consistent level of accuracy. Fig. 9 shows the performance of ReFT, SFT, and NLFT with a dataset of 800 samples. It can be observed that SFT exhibits slow improvement in the initial epochs and reaches near-optimal performance after 5 epochs. However, as the number of epochs increases, the risk of overfitting grows, leading to a significant overfitting and a sharp decline in accuracy. In contrast, NLFT exhibits a stable accuracy of approximately 70%, reflecting its robust stability. We suppose this is because the algorithm shows more attention to the saliency token, thereby focusing on the most critical problem-solving pathway and improving training stability. This approach is similar to causal-inspired stable learning in computer vision [Zhang et al., 2021], which effectively filters out irrelevant features and uses only truly relevant ones for prediction, resulting in more stable performance in wild, non-stationary environments."}, {"title": "5.2 Algorithm Complexity Analysis", "content": "The time complexity and space complexity of our algorithm are both O(n), which means that as the input size n grows, the required time and space resources grow linearly. This linear growth indicates that our algorithm is efficient and resource consumption is controllable when dealing with large-scale data. In contrast, the ReFT algorithm uses the PPO algorithm [Schulman et al., 2017] for optimization, whose time complexity is O(TC + NBP), which is proportional to the product of the number of samples, the neural network's forward and backward propagation complexity, and the number of update iterations [Luong et al., 2024]. Due to its higher time and space complexity, ReFT may suffer from decreasing efficiency and increasing resource consumption when processing large data. Therefore, in applications involving large-scale data processing, our algorithm demonstrates a significant performance advantage over the ReFT."}, {"title": "6 Discussion", "content": "Our experiments indicate that increasing the proportion of incorrect samples in the training data leads to a decrease in learning efficiency. To fine-grained investigate the contribution of incorrect samples to model fine-tuning, we attempt to enhance the learning efficiency from incorrect samples, we will continue to explore this in our subsequent research."}, {"title": "6.2 Model Generalization", "content": "In our previous research, we applied the simplified version of the NLFT to human-computer collaborative tasks and achieved significant performance improvement. Theoretically, NLFT is applicable to scenarios where outputs can be generated through CoT and labeled data is available, such as coding, medical diagnosis, natural language inference, and complex question-answering systems. By comparing the generated output with the labels, it is possible to annotate the saliency tokens, thereby applying NLFT for token-level fine-tuning. In our future work, we will explore the application of NLFT to broader fields and refine the NLFT algorithm based on the characteristics of each task."}, {"title": "7 Conclusion", "content": "In this paper, we propose a novel natural language minimal data fine-tuning algorithm NLFT. The algorithm compares the conditional probabilities of various natural language tokens under different prompts, utilizing natural language as a supervisory signal to identify saliency tokens and assign them scaling values. Experimental results demonstrate that our algorithm, compared to previous ones, has lower time complexity and better performance. Under the GSM8K dataset evaluation, only random 50 training data allows NLFT to achieve over 60% accuracy, and performance of NLFT is stably increased by 25% compared to SFT. In contrast to RL-based fine-tuning algorithm like ReFT, NLFT saves huge time and space complexity, enabling broader imagination for lightweight fine-tuning and applications."}, {"title": "A.1 Prompt Strategy of Judgment on the GSM8K Dataset", "content": "In this section, we present a prompt engineering strategy for generating the judgment of incorrect samples."}, {"title": "A.2 Prompt Strategy of Math Reasoning on the GSM8K Dataset", "content": "In this section we present a prompt engineering strategy for testing model performance."}, {"title": "B Detailed Hyperparameter Settings", "content": "NLFT: NLFT is trained using LoRA [Hu et al., 2021], where the parameters r, a and dropout are set to 16, 16, and 0.05, respectively. The learning rate is set to 5 \u00d7 10\u22125. For most NLFT experiments, the maximum number of epochs is set to 10, unless the setting in Fig. 9 specifies 30.\nSFT: Our SFT implementation employs SFTTrainer in trl [von Werra et al., 2020]. To ensure that the SFT code configuration is largely consistent with NLFT configuration, we have essentially adopted most of the parameter settings of NLFT.\nReFT: Following [Luong et al., 2024], before ReFT algorithm, we perform SFT warmup for 2 epochs with the learning rate of 1 \u00d7 10-5 on GSM8K dataset. When performing SFT warmup, the batch size is set to 48, and the maximum input length is set to 512. After warmup phase is finished, we perform ReFT algorithm. The maximum input length is set to 300, and the maximum length of model generation is set to 700. The batch size is set to 16 to avoid crash during training. The number of updates per RL step is set to 2. The learning rate is set to 3 x 10-7."}]}