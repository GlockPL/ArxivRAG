{"title": "Revisiting VerilogEval: Newer LLMs, In-Context Learning, and Specification-to-RTL Tasks", "authors": ["Nathaniel Pinckney", "Christopher Batten", "Mingjie Liu", "Haoxing Ren", "Brucek Khailany"], "abstract": "The application of large-language models (LLMs) to digital hardware code generation is an emerging field. Most LLMs are primarily trained on natural language and software code. Hardware code, such as Verilog, represents only a small portion of the training data and few hardware benchmarks exist. To address this gap, the open-source VerilogEval benchmark was released in 2023, providing a consistent evaluation framework for LLMs on code completion tasks. It was tested on state-of-the-art models at the time including GPT-3.5, GPT-4, and codegen-16b-verilog-sft. However, VerilogEval and other Verilog generation benchmarks lack failure analysis and, in present form, are not conducive to exploring prompting techniques. Also, since VerilogEval's release, both commercial and open-source models have seen continued development.\nIn this work, we evaluate new commercial and open-source models of varying sizes (GPT-4 Turbo, Llama 3.1 8B/70B/405B, Llama 3 70B, Mistral Large, Deepseek Coder 33B and 6.7B, CodeGemma 7B, and RTL-Coder) against an improved VerilogEval benchmark suite. We enhance VerilogEval's infrastructure and dataset by automatically classifying failures, introduce new prompts for supporting in-context learning (ICL) examples, and extend the supported tasks to specification-to-RTL translation. We find a measurable improvement in commercial state-of-the-art models, with GPT-4 Turbo achieving a 59% pass rate on specification-to-RTL tasks. We also study the performance of open-source and domain-specific models that have emerged since the original release of VerilogEval, and demonstrate that models can benefit substantially from ICL. We find that recently-released Llama 3.1 405B achieves a pass rate of 58%, effectively matching that of GPT-4 Turbo, and that the much smaller domain-specific RTL-Coder 6.7B models achieve an impressive 37% pass rate. However, prompt engineering is key to achieving good pass rates, and varies widely with model and task. A benchmark infrastructure that allows for prompt engineering and failure analysis is key to continued model development and deployment.", "sections": [{"title": "I. INTRODUCTION", "content": "Applications of large-language models (LLMs) to software coding have reached wide deployment, with examples such as Github CoPilot [1]. Yet, applications of LLMs to hardware design are still in their infancy [2], [3]. Only a small handful of Verilog code design benchmarks exist in the literature, including RTLLM [4], VerilogEval [5], VeriGen [6], [7], and most recently RTL-Repo [8]. While RTLLM bench- marked conversational specification-to-RTL generation perfor- mance, VerilogEval, VeriGen, RTL-Repo are code completion benchmarks. Additionally, none of the benchmarks explore a model's generation performance using in-context learning [9] examples nor do they provide a detailed way to inspect the reasons for a model's failure.\nThis work aims to address these limitations by extending VerilogEval [5] to support specification-to-RTL tasks in addi- tion to the original code completion task. We also incorporate a variable number of in-context learning prompts, and pro- vide a robust failure classification mechanism, to provide a more comprehensive evaluation framework for Verilog code generation tasks. The significance of this work lies in its potential to push LLM development forward for hardware design, through offering insights into model performance and the efficacy of prompt tuning, and to point out differences in generation quality across tasks. Even with similar problem statements and in-context learning examples, we find divergent responses by large-language models. This variability highlights the importance of understanding how different models respond to various prompts and contexts through the use of the benchmarks providing granular failure feedback.\nMoreover, we evaluate newer large-language models than those tested in the original VerilogEval paper, including GPT- 4 Turbo [10], open-source models like Llama 3.1 [11], and domain-specific models such as RTL-Coder [12]. In short, we assess the latest state-of-the-art language models to determine the current frontier of LLM-based Verilog code generation while also evaluating the impact of prompt tuning. We find that recent open-source models are becoming competitive with last year's closed models, and that prompt tuning varies considerably across models.\nThe following new features are part of the proposed bench- mark infrastructure:\n1) Specification-to-RTL task support: VerilogEval only sup- ported code completion tasks, such as used in CoPilot [1], while many models are tuned and deployed as instruction-tuned models [13], with question and answer prompting.\n2) In-context learning examples: No in-context learning (ICL) [9] examples were supported as part of the prompt in VerilogEval.\n3) Failure classification: VerilogEval only reported pass/- fail results of a benchmark problem, and did not give fine-grained feedback on failures.\n4) Makefile-based evaluation environment: The original VerilogEval benchmark [5] used a monolithic dataset,"}, {"title": "II. BENCHMARK IMPROVEMENTS", "content": "The improved VerilogEval benchmark is available publicly at https://github.com/NVlabs/verilog-eval.\nA. Specification-to-RTL Task Support\nThe proposed benchmark supports both code comple- tion and specification-to-RTL tasks to better match the instruction-tuning [13] of recent models. The full 156-problem dataset from VerilogEval is converted into specification-to- RTL prompting in this work. Code completion has the prob- lem description in Verilog-compatible comments and always appends the module interface declaration to the end of the prompt. On the other hand, specification-to-RTL's prompt style is as a chat bot, with well-defined \"Question\" and \"Answer\" sections. The specification-to-RTL prompting is implemented in a manner similar to the Mostly Basic Python Problems (MBPP) benchmark [14] with [BEGIN] and [DONE] tags surrounding code blocks. Examples of these two styles can be found in listings 1 and 2 with only the highlighted code indicating the prompt styles.\nB. Support for In-Context Learning Examples\nIn-context learning (ICL) was proposed by [9] to add examples of task questions and desired responses into the prompt context, so that an LLM can better respond to a given task. ICL is implemented through simple Verilog code examples, tailored for both code completion (Listing 1) and specification-to-RTL tasks (Listing 2). The listings contain the 1-shot examples used for both tasks, except line width and whitespace was adjusted for printing. The examples were selected to be short and simple, while including a full module (from declaration to endmodule). Two additional examples for each task are added: a sequential incrementer similar to the first 1-shot example, and a basic finite-state machine for the third example. The number of shots is parameterized and can easily be swept to determine sensitivity of a model's pass rate as ICL examples are added to the prompt. 1-shot includes only the combinational incrementer, 2-shot adds the sequential incrementer, and 3-shot includes all three examples in the context prompt.\nC. Support for Failure Classification\nFailures of LLM-generated responses are automatically clas- sified by broad reasons for failure, both Verilog compile time errors and simulation runtime errors, such as incorrectly using a wire as a register, incorrect bit widths, and missing mod- ule interface definitions. This classification feature provides insight into the most common reasons for failures and how to mitigate poor code generation through prompt tuning. The classification is dependent on specific warnings and errors given by Icarus Verilog or the test harness. The failures are classified in Table I.\nClassifications were developed by human inspection of common failure modes across the code completion benchmark. For example, LLMs were observed frequently mixing up the use of registers and wires. Solutions in prompt tuning could vary: from adding prompt rules to only use wires on ports to suggesting the use of SystemVerilog logic port types,"}, {"title": "III. BENCHMARK EVALUATION", "content": "We evaluate eight publicly available large-language models on the proposed benchmark:\n\u2022 OpenAI GPT-4 Turbo [10] (gpt-4-1106-preview)\n\u2022 OpenAI GPT-4 [17] (gpt-4-0613)\n\u2022 Mistral AI Mistral Large [18]\n\u2022 Meta Llama 3.1 405B [11]\n\u2022 Meta Llama 3 70B [11]\n\u2022 Meta CodeLlama 70B [19]\n\u2022 Google CodeGemma 7B [20]\n\u2022 DeepSeek Coder 33B and 6.7B [21]\n\u2022 Meta Llama 3.1 8B [11]\n\u2022 RTL-Coder DeepSeek v1.1 6.7B [12].\nThe models are comprised of a range of closed and open source, parameter sizes, and general-purpose to specialized. Model results were captured as both a 20-sample (n=20) high temperature (T=0.85, top_p=0.95) set and 1-sample (n=1) low temperature (T=0.0, top_p=0.01) set. The 20-sample set is similar to the model parameters from VerilogEval [5] which had a 20-sample set with temperature T=0.80.\nThe graph in Figure 1 illustrates the performance of vari- ous large-language models (LLMs) on code completion and specification-to-RTL translation tasks, as measured by the benchmark pass rate (pass@1 in [5]). Models are arranged along the x-axis by model size, with undisclosed model sizes on the right. The evaluation compares models with and without 1-shot in-context learning (ICL) examples, represented by ar- rows indicating the change in performance as 1-shot examples are added. For code completion tasks, GPT-4 Turbo achieves the the highest pass rate at approximately 48%, surpassing the previously established state-of-the-art frontier of 43% for 0-shot by GPT-4 [5]. Further adding an ICL example in the 1-shot result leads to to the highest performance yet at 58%. This highlights GPT-4 Turbo's robust improvement over GPT- 4 for RTL generation tasks despite being a general-purpose model.\nLlama 3.1 405B demonstrates that open models have matched closed models by scoring 57% in the 0-shot code completion task, exceeding both GPT-4 and GPT-4 Turbo, while Llama 3.1 70B nearly matching GPT-4 despite being much smaller in size. While Llama 3.1 generally improves with in-context learning examples, Llama 3 70B declines in pass rate when the 1-shot ICL example is added to the prompt, which will be discussed in detail in the next section. Among the smaller specialized models, RTL-Coder showed significant improvements with 1-shot ICL examples, reaching pass rates of around 35%, while being much smaller than general- purpose models. RTL-Coder when originally sampled did not properly insert whitespace after endmodule statements and would often repeat code blocks. We modified our post-process script that extract the Verilog code form the response to match the post-processing in RTL-Coder's evaluation scripts [22], and Figure 1's RTL-Coder results are shown using their corrected extraction.\nSpecification-to-RTL task results showed generally similar pass rate performance compared to code completion. GPT-4 Turbo showed noticeable pass rate improvement in spec-to- RTL 0-shot tasks, but similar pass rates for 1-shot. Mistral Large showed the opposite trend, with measurable improve- ment in 1-shot results and Llama 3 and Llama 3.1 70B saw improvement in both, as did Llama 3.1 8B. In Llama 3.1 405B across both tasks, adding an ICL example made little differ-"}, {"title": "IV. IMPACT OF ICL ON PASS RATES AND FAILURES", "content": "ence in pass rate. Interestingly, RTL-Coder initially fails at the specification-to-RTL task with 0-shot but recovers with 1- shot examples. This variability underscores the importance of tailored prompt tuning and the potential of ICL to enhance code generation performance in certain models.\nThe full results are shown in Table II and include both n=20 (20 samples, temperature=0.85, top_p=0.95) from Figure 1 along with deterministic n=1 (1 sample, temperature=0.0, top_p=0.01). RTL-Coder results are shown for both the cor- rected and original extraction methods, with the original method also applied to the other models. As mentioned above, RTL-Coder at high temperatures (temperature=0.85, n=20) has a near-zero (1.6%) pass rate in 0-shot specification-to-RTL, but does have a respectable pass rate (37%) when temperature=0 (n=1). Inspection of the RTL-Coder responses with high temperature show that it tries to do code completion instead of specification-to-RTL in 0-shot and often omits the modular declaration. Adding an ICL example in 1-shot corrects this behavior.\nOverall, larger models generally achieve higher pass rates, though resource costs and model-specific responses to ICL examples vary significantly. Within the context of VerilogEval, GPT-4 Turbo and Llama 3.1 405B have become clear leaders for the highest achieved pass rates, demonstrating that open\nA. Higher-Shot ICL Results\nAs demonstrated from the previous section, in-context learn- ing examples improve model generation accuracy in some conditions but degrade accuracy in others. ICL impact bears further investigation. Higher-shot ICL runs were conducted for four models across parameter size classes: GPT-4 Turbo, Llama 3 70B, Llama 3.1 70B, and RTL-Coder. The second ICL example was similar to the first example but requested a sequential (flopped) incrementer instead of a combinational incrementer. The third example involved designing a finite- state machine.\nPass rates across three models for the two tasks across 0-shot to 3-shots is shown in Figure 2. Notably, GPT-4 Turbo exhibits stable and high performance across all ICL example counts of at least 1-shot, maintaining a pass rate of 55% to 60%. In contrast, Llama 3 70B demonstrates divergent trends; its spec-to-RTL performance improves from 40% to nearly 50% with more ICL examples, whereas its code completion performance declines from 35% to just above 20%. Llama 3.1 70B is similar to Llama 3 for spec-to- RTL, and doesn't demonstrate degradation in code completion. RTL-Coder shows significant variability, with its spec-to-RTL performance improving dramatically from around 5% at 0-shot to almost 35% at 3-shot. As mentioned in the previous section, RTL-Coder drops to a very lower pass rate at high temper- ature with 0-shot spec-to-RTL because it omits the module declaration, but recovers to a nominal pass rate once ICL examples are added. This graph highlights the varying impact of ICL examples on different models and tasks, emphasizing the potential benefits of task-specific tuning and the necessity of providing contextual examples to enhance model outputs.\nB. Failure Analysis\nFigure 3 employs the new failure classification feature of the improved benchmark infrastructure to illustrate the number and types of failures encountered by different models across various numbers of in-context learning (ICL) examples. The y-axis represents the number of failures, with lower values indicating better pass rates. Each bar is segmented to show different categories of errors, with orange shades representing compiler errors and blue shades representing runtime errors. The figure is divided into three sections for the three models from Figure 2, highlighting the numbers and types of failures across 0-shot to 3-shot ICL examples. As compiler errors will be flagged and mask runtime errors, the bars on the graph are best read from bottom to top. A reduction in runtime errors for the same total bar height indicates that compiler errors have displaced runtime errors. This layering effect should be considered when interpreting the improvements or degrada- tions in model performance as additional ICL examples are introduced.\nFor RTL-Coder, the model shows notable improvement"}, {"title": "V. CONCLUSIONS", "content": "in both tasks up to 2-shot ICL examples, after which the performance stabilizes. The primary source of failure in 0- shot examples are compile-time errors, but with the addi- tion of ICL examples, these errors decrease significantly. As mentioned previously, in the specification-to-RTL task, the model attempts code completion when the temperature is high, leading to a high number of \u201cmodule missing\" errors, which are reduced with the introduction of ICL examples.\nLlama 3 exhibits a different pattern, where code completion performance degrades with the addition of ICL examples due to frequent endmodule errors. In contrast, for the specification-to-RTL task, adding ICL examples mitigates er- rors related to wires being declared as registers but introduces other compiler errors.\nGPT-4 Turbo shows a mixed response to ICL examples. For code completion, the model benefits from ICL examples, as indicated by a reduction in compiler errors across the board. However, in the specification-to-RTL task, the performance slightly degrades with more ICL examples, resulting in an increase in compiler errors.\nThe results emphasize the need for careful tuning of ICL examples to optimize results. While ICL can help correct certain types of mistakes, it can also introduce new issues leading to similar or even worse performance. In addition to the failure classification feature capturing high-level counts of types of failures across different models and prompting settings, it also allows for detailed inspection on a problem- by-problem basis within a run. This granular analysis helps identify whether specific problems or categories of problems have systematic types of failures. Such insights can guide more careful tuning of prompts across the benchmark, leading to more effective and targeted improvements in model perfor- mance. A careful analysis of the problem categories within VerilogEval and comparative failure counts could help find the best ICL examples to use for a given model.\nThe enhanced VerilogEval benchmark provides a more robust framework for evaluating the performance of large- language models (LLMs) on digital hardware code generation tasks. Our findings demonstrate that both Llama 3.1 405B and GPT-4 Turbo push the frontier of performance with a 60% and 48% 0-shot pass rate on code completion tasks, respectively, surpassing the previously established 43% pass rate by GPT-4 (non-Turbo) [5]. Open-source general-purpose models, namely Llama 3 70B, and domain-specific models, RTL-Coder, show favorable pass rates compared to last year's closed models, 37% and 32%, respectively. The addition of specification- to-RTL task support in the improved VerilogEval benchmark reveals even better model capabilities. GPT-4 Turbo achieves an impressive 59% pass rate in specification-to-RTL tasks, exceeding Llama 3.1 405B at 56%, while Llama 3 70B and RTL-Coder 6.7B also demonstrate strong competitiveness with pass rates of 42% and 37%, respectively. Adding in-context"}]}