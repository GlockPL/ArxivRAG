{"title": "Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care", "authors": ["Yuxiao Cheng", "Xinxin Song", "Ziqian Wang", "Qin Zhong", "Qionghai Dai", "Kunlun He", "Jinli Suo"], "abstract": "Recent advances in deep learning (DL) have prompted the development of high-performing early warning score (EWS) systems, predicting clinical deteriorations such as acute kidney injury, acute myocardial infarction, or circulatory failure. DL models have proven to be powerful tools for various tasks but come with the cost of lacking interpretability and limited generalizability, hindering their clinical applications. To develop a practical EWS system applicable to various outcomes, we propose causally-informed explainable early prediction model, which leverages causal discovery to identify the underlying causal relationships of prediction and thus owns two unique advantages: demonstrating the explicit interpretation of the prediction while exhibiting decent performance when applied to unfamiliar environments. Benefiting from these features, our approach achieves superior accuracy for 6 different critical deteriorations and achieves better generalizability across different patient groups, compared to various baseline algorithms. Besides, we provide explicit causal pathways to serve as references for assistant clinical diagnosis and potential interventions. The proposed approach enhances the practical application of deep learning in various medical scenarios.", "sections": [{"title": "Introduction", "content": "The rapid development of deep learning technologies has yielded some powerful early warning score (EWS) tools for predicting critical clinical deterioration events, e.g., acute kidney injury, acute myocardial infarction, and circulatory failure. Although achieving promising performance in various tasks, deep learning models are faced with multiple limitations that significantly hinder their practical applications, including low interpretability and limited generalizability1\u20134.\nInterpretability and generalizability are especially impor- tant for a clinically practical EWS system because physicians intend to trust a prediction with explicit inference mecha- nisms as well as consistent accuracy in different environ- ments. Researchers have made a lot of attempts to build high-performance deep neural networks with high interpretability and generalizability. The techniques enhancing interpretabil- ity involve post hoc methods such as GradCAM, LIME, or SHAP5-7, attention mechanism, decision trees and knowl- edge distillation4. For better generalizability, various strate- gies have been proposed, including dropout10, weight decay\u00b9\u00b9, or more complex approaches such as invariant risk minimiza- tion (IRM), GroupDRO, or VREx12\u201314, etc. Despite the efforts and progress in the above two respective directions, in the field of medical AI, the interpretability is often constrained to variable-to-outcome explanations through post-hoc methods like SHAP5,15. These methods fail to show underlying causal structures, and thus only provide limited information for po- tential clinical intervention. Additionally, most algorithms are not specially designed to boost generalizability by consider- ing the intrinsic relationships among the clinical variables and outcomes 16-19.\nCausal models are exactly what can address the above issues since causality is stable and interpretable20-24. For example, it is well-established among clinicians that elevated insulin levels lead to a reduction in blood glucose levels, i.e., insulin causes decreased glucose. Consequently, predicting a decrease in blood glucose based on an observation of high insulin levels is both interpretable and consistent across nearly all individuals and clinical settings. Manually building the complex and entangled causal relationship behind numerous diseases is extremely difficult and sometimes even impossible. However, recent advancements in DL-based causal discovery algorithms have made it possible to construct intricate causal models by analyzing vast datasets25\u201328, providing a promising approach to developing reliable and practical EWS.\nThis study presents the causally-informed Dynamic Ex- plainable Early Prediction (cDEEP), a novel method that em- ploys causal discovery to identify stable associations between the clinical variables of ICU patients and their outcomes. Specifically, the causal graph captures causal variables that are important to each outcome, boosting the generalizability of the model by only including causal variables. Moreover, it explores causal associations among dynamic variables, which facilitates identifying the inference path of the prediction ex- plicitly and enhances the interpretability further. The graph would empower clinicians to identify actionable variables throughout the chain, thereby facilitating more effective inter-"}, {"title": "Results", "content": "Data. We used two large electronic health record (EHR) databases-the Medical Information Mart for Intensive Care (MIMIC) IV29 and the eICU Collaborative Research Database30. Six medical outcomes were identified: acute kidney injury (AKI), acute respiratory distress syndrome (ARDS), circulatory failure, death, delirium, and sepsis. As shown in Extended Fig. 1a, all patients under or at the age of 75 were randomly split into training, validation, and in-distribution testing sets (80%, 10%, and 10%), while the out-of-distribution testing sets comprised patients aged 76 and above. Additionally, we partitioned the out-of-distribution testing sets according to admission time for additional experi- ments aimed at assessing generalizability. Shown in Extended Data Fig. 1b, dynamic data from the databases are converted to temporally structured sequences, and the predictions are made dynamically at each time point. Patient population statis- tics are shown in Tab. 1, and all included variables are listed in Tab. 2. For further information regarding data preprocessing,"}, {"title": "Overview of CDEEP", "content": "Current deep-learning-based ap- proaches, though excel at making accurate decisions, primar- ily focus on establishing correlations rather than causations, leading to unreliable decisions due to a lack of fundamental interpretability and generalizability (refer to Supplements A for more detailed literature reviews). As a result, some recent literature delves into causal models, among which the most ex- amined framework is Pearl's structural causal model (SCM), with variables depicted as nodes and causal influences indi- cated by directed edges. Recently, various causal discovery approaches have been proposed to build SCMs with observa- tional data25\u201328, helping to develop reliable and practical deep learning models.\nThe proposed cDEEP, demonstrates a novel early warning system scheme that integrates causal discovery with deep learning to predict six critical care outcomes. As illustrated in Fig. 1a, after collecting dynamic and static data from the EHR databases, we trained the deep learning model to assess the risk of a patient experiencing a specific outcome within the next 24 hours. The model combines an encoder- decoder neural network architecture with a causal discovery process to identify and utilize the causal relationships among the input variables and outcomes. During clinical applications illustrated in Fig. 1b, cDEEP takes advantage of the learned causal graph and the prediction model to estimate the risks of six outcomes and offers explicit causal pathways for high interpretability.\nWe train the model by iterating two key steps: i) opti-"}, {"title": "High prediction accuracy", "content": "CDEEP can predict multiple outcomes with a single neural network and achieves high pre- dictive accuracy in the testing set, with the results illustrated in Fig. 3a.\ncDEEP performs quite well in the prediction of AKI, Cir- culation failure, Delirium, and Sepsis. Taking AKI as an example, we achieve an area under the receiver operating characteristic curve (AUROC) of 0.915, with 95% confidence interval (CI) over 100 cross-validation folds being [0.915, 0.916]; area under the precision-recall curve (AUPRC) is 0.823 (CI [0.822, 0.824])."}, {"title": "Qualitative interpretation", "content": "Causal models possess inher- ent interpretability that allows for the elucidation of neural network decisions by analyzing the parent nodes within a causal graph. It follows logically that the parents in the causal graph are responsible for the outcomes of interest, as illus- trated in Fig. 2a. By integrating V2V and V2O graphs, CDEEP obtains a comprehensive causal graph encompassing both vari- ables and clinical outcomes. For a visual representation of the constructed causal graph, please refer to Extended Data Fig. 3. This combined causal graph enables the identification of the causal pathways that illustrate the influence of each variable. Taking the path chloride \u2192 BUN \u2192 circulatory failure as an example, we explicitly show the inference philosophy behind"}, {"title": "Acceleration for quantitative interpretation", "content": "Specifically, as shown in Fig. 4c, after building a variable-to-outcome (V2O) graph, 18, 17, 23, 22, 27, and 25 direct causes are identified as critical inputs for AKI, ARDS, circulatory failure, death, delirium, and sepsis, respectively. Moreover, the distillation of input variables occurs not only across the variable axis but also along the time axis. We incor- porate a cumulative window graph (see Methods) to minimize the time lags of causal dependencies, thereby further reducing data intensity. As a result, while maintaining a high level of prediction accuracy, the total number of input features to the neural network has been decreased by 80.1%, as shown in Fig. 4a-b.\nAdditionally, we propose an accelerated inference tech- nique, which allows for updating only a subset of the neu- ral network's hidden layers following the perturbation of a variable, rather than conducting a complete inference. This methodology is elaborated in the Methods section. Experi- ments in Fig. 4c show that this acceleration technique de- creases the computation time for constructing the V2O and V2V graphs by 63.1% and 90.5%, respectively.\nTo better demonstrate the decision rationale behind cDEEP's outcome prediction, we designed a web-based tool to visualize the causal pathways alongside the corresponding CDE values. The tool features a user-friendly interface, al-"}, {"title": "Generalizability", "content": "The theoretical foundation for the gen- eralizability of cDEEP is established through its causal dis- covery process. As shown in Theorem 1, by assuming that distribution shifts arise from interventions on various vari- ables, CDEEP exclusively utilizes causal variables as input and thus can sustain consistent predictions despite such inter- ventions, since the direct causal influences on the outcomes remain unaffected.\nTo assess the generalizability of our approach, we created out-of-distribution testing data by splitting the datasets by pa- tients' ages, i.e., training on patients with age \u2264 75 and testing on patients with age \u2265 76. We show that cDEEP achieves high prediction accuracy in out-of-distribution testing data, shown in Fig. 3a.\nExisting generalizable AI approaches, e.g., IRM12, Group- DRO13, and VREx14, base the predictions on all available input variables while cDEEP only uses the direct causal vari- ables. Theoretically, Theorem 1 shows that by inputting only causal variables, cDEEP is the optimal worst-case generaliz- able model. Yet this is not to say that the non-causal variables serve no purpose in improving the prediction accuracy. Actu- ally, the non-causal variables may contain some information that is not fully captured by the causal variables. As a result, direct comparisons between cDEEP and these methods may not be entirely fair. To further experimentally substantiate the generalizability of cDEEP, we finetuned cDEEP on all avail- able input variables, which we refer to as \u201ccDEEP-full\u201d. The comparisons of \u201ccDEEP-full\u201d (which inputs all variables) as well as \"CDEEP\" (which inputs causal variables) with the ex- isting generalizable AI methodologies-IRM, VREx, Group- DRO, and Dropout\u2014are shown in Fig. 3b-c and Supplements Tab. S3-4. It is observed that cDEEP achieved competing or superior performance in most cases in terms of AUROC and AUPRC already, even with significantly fewer input variables. On the other hand, the performance of CDEEP-full is even better than cDEEP, beating baseline methods in most cases, which further validates the superiority of CDEEP in terms of generalizability. Furthermore, experiments on robustness to noise also indicate that cDEEP and cDEEP-full outperform baseline methods in most cases across various noise levels, as presented in Supplements Fig. S5.\nWe also created out-of-distribution testing data by split- ting the datasets by admitting time, i.e., training on patients admitted before 2014 and testing on patients admitted after. The results show similar conclusions as the age-based split, which is shown in Supplements Tab. S5-6."}, {"title": "Discussions", "content": "This study underscores the integration of deep learning with causal discovery for predicting clinical outcomes. Our methodology, termed cDEEP, focuses on causal relationships rather than mere correlations to uncover the small number of direct causal variables for outcomes and the inferring path for outcome prediction. Such a shift from association-based to causality-based models fills a significant void in existing deep learning-based EWS systems, which often struggle with low generalizability and limited interpretability, and lay a foundation for practical AI-based EWS tools.\nThe generalizability of CDEEP represents a significant ad- vancement in the realm of medical artificial intelligence16\u201319. By constructing stable causal graphs across diverse environ- ments and populations, cDEEP showcases robust performance even in out-of-distribution testing scenarios. This feature is essential for deploying Early Warning Systems (EWS) in vari- ous clinical contexts, where patient demographics and condi- tions may vary considerably from the training data. cDEEP's ability to sustain high accuracy under such circumstances high- lights its potential as a dependable tool for early prediction in critical care settings. Our advantageous generalizability is fundamentally different from the existing AI-driven medical tools, which adjust the model structure or parameters to bal- ance the capacity and generalizability and conduct external experiments to validate their design, instead of offering a more thorough methodology to enhance generalizability.\nThe second distinguishing feature of CDEEP is its inter- pretability 15,47. By elucidating explicit causal relationships between variables and outcomes, CDEEP enables clinicians to gain a clear understanding of how each variable impacts the prediction. The visualization tool developed in this study further improves the interpretability of CDEEP by allowing clinicians to investigate the causal pathways and the controlled direct effects of each variable on the outcomes. Moreover, CDEEP accelerates the calculation of causal pathways by re- ducing input variables to essential causal variables, signifi- cantly decreasing the computation time required for interpre- tation. This level of transparency is vital for fostering trust in AI-based clinical decision support systems and facilitating their practical use.\nOur methodology of achieving high interpretability via causal discovery substantially advances the previous stud- ies in explainable early warning systems in critical care set- tings5. For example, Meyer et al.15 have developed a dynamic explainable AI model for predicting 90-day mortality rates among ICU patients and conducted post-hoc interpretability analysis by calculating the SHAP values. However, they only offer a localized attribution between variables and outcomes, and the SHAP-based interpretation method only focuses solely on the impact of input variables on the outcomes and leaves the variable-to-variable impact unexplored. As discussed in their paper, clinicians may struggle to pinpoint effective risk- reduction strategies without a comprehensive view of how variables interact and affect one another. Conversely, the proposed model encompasses the entire causal pathway (as illustrated in Fig. 2b) facilitating a more extensive analysis, re- vealing multiple potential intervention points. Addressing an arbitrary variable within this chain could enhance clinicians' decision-making processes and improve patient outcomes.\nCurrently, CDEEP focuses primarily on structured clinical data. Expanding to incorporate other data modalities, such as medical imaging, could further enhance the model's diag- nostic capability and generalizability across diverse clinical scenarios. Additionally, we aim to develop an extended ver- sion of CDEEP that factors in medical interventions, enabling the system to suggest potentially effective treatments and thus broaden its role from predictive assessment to actionable in- sights in patient management."}, {"title": "Methods", "content": "Data description\nDatabases. We used two large electronic health record (EHR) datasets: the Medical Information Mart for Intensive Care (MIMIC) IV and the eICU Collaborative Research Database. MIMIC-IV is a publicly available collection containing de-identified health-related data associated with over 70,000 patients staying in critical care units, and eICU consists of health data of over 200,000 patients from multiple centers in the United States.\nMedical outcomes identification. We focused on six critical care outcomes: acute kidney injury (AKI), acute respiratory distress syndrome (ARDS), circulatory failure, death, delirium, and sepsis, which are mostly frequently occurring cases in critical care and sig- nificantly impact patient prognosis. In both databases, we identified these outcomes using the following criteria: AKI was identified using the KDIGO criteria\u00b3\u00b9, ARDS using the Berlin criteria48, cir- culatory failure by looking at the lactic acid level and MAP16, death using the hospital mortality flag, delirium using the CAM-ICU- related score, and sepsis using the SOFA score calculated in the MIMIC-IV-Derived dataset29. For details of our implementations of these criteria, please refer to the Supplements D.1.\nTime-series construction. Before learning the prediction model, we organized the data of each patient into temporally structured series. Patient hospitalization data were chronologically arranged to create time-series, which are partitioned into non-overlapping 2- hour periods. The data from 14 days, i.e., a time window including 168 time points, can be used to predict the outcome in the next 24 hours. We slide the 14-day time window at an interval of 2 hours to form sufficient prediction points and use all the available time win- dows during the whole hospitalization period to learn the outcomes prediction model and obtain an average of 74.42 prediction points for each patient. To address the missing observations, rather than using imputation methods, we preserved the integrity of the original observations by introducing a binary indicator for each variable, signaling whether data was missing. It is shown in Supplements Fig. S1 that the Missing rates of variables do not significantly bias causal discovery results, supporting our missing value preprocess- ing method. For variables measured multiple times within the same time window, we take the most recent measurement.\nWe also include three static variables, i.e., age, gender, and eth- nicity, which are assumed to be influential factors for the outcomes and share the same causal discovery process as the time-series"}, {"title": "Definition 1", "content": "Variable $x_i$ Granger cause $y$ if and only if there exists $x_i \\neq x_i$,\n$\\qquad f_{\\theta} (x_1,...,x_i,..., x_N) \\neq  f_{\\theta} (x_1,...,x_i,..., x_N)$                                        (1)\ni.e., the input variable $x_i$ influences the prediction of $y$."}, {"title": "Outcomes prediction with causal discovery", "content": "Let $x_i$ denote the input variables, $y_{j,p}$ the outcome, and $f_{\\theta}$ the neural network, we aim to learn a prediction model that can infer $y_{j,p}$ from $x_i$, where $t \\in {1, ..., T}, i \\in {1, ...,N}, j\\in {1, ...,M},p\\in {1,..., P}$ and $T$ is the time-window size of the input, $N$, $M$ and $P$ are respectively the number of input variables, outcomes, and patients. After representing the input variables as a matrix $X_p = {x_{t,i}}_{t=1,i=1}^{T,N}$ and the outcome as a vector $y_p = {y_{j,p}}_{j=1}^M$, we can formulate the learning problem as\n$\\qquad \\min_{\\Theta} \\sum_{p=1}^P L (f_{\\theta} (X_p), y_p),$            (2)\nwhere $L$ is the loss function, e.g., cross-entropy loss for binary classification, and $\\Theta$ is the parameters of the neural network. To simplify the notation, we denote the network for predicting a single out- come $y_j$ as $f_{\\theta^j}$, and  $f_{\\theta}(X_p) = {f_{\\theta^j} (x_p)}_{j=0}^{M-1}, L (f_{\\theta^j} (x_p):y_p) = \\sum_{j=0}^{M-1} C (f_{\\theta^j} (x_p), y_{j,p}).$\nNetwork structure. For the prediction neural network, we implemented $f_{\\theta^j}$ as an encoder-decoder structure, i.e.,\n$f_{\\theta^j} (X_p) \\triangleq Dec_j(Enc(X_p|Pa(y_j; G^{v2o}))) ,  (3)$"}, {"title": "Causal discovery", "content": "To achieve interpretable and generalizable prediction, we incorporate causal discovery into the learning process. Specifically, we aim to discover the structural causal model (SCM) from the input variables to outcomes, i.e., the window causal graph $G$, and use the discovered causal graph to guide the learning process. In the medical outcomes prediction, it is reasonable to assume that the outcome is caused by a subset of input variables but the outcome cannot be the cause of either the input variables or future outcomes. As a result, the causal graph $G$ can be split into two parts: the variable-to-outcome (V2O) graph $G^{v2o}$ and the variable-to-variable (V2V) $G^{v2v}$ graph, while the outcome-to-outcome and outcome-to- variable graph is assumed to be empty.\nFor the V2O graph, we denote the causal parents of out- come $y_j$ as $Pa(y_j; G^{v2o})$ and the input data matrix as $X_p|Pa(y_j; G^{v2o})$"}, {"title": "Causal probability matrix", "content": "We implemented the $R(G^{v2o})$ and $R(G^{v2v})$ in Eq. 6 with causal probability graph. Since the regular- ization term contains binary variables, the optimization problem is extremely hard to solve. To address this issue, we propose to relax the binary variables to continuous ones, i.e., the causal probabil- ity graph. Specifically, we define the causal probability matrix as $\\mathcal{M}^{v2o} = {p_{t,i,j}}_{i=0,j=0,t=0}^{N-1, M-1, \\tau-1}$ with $p_{t,i,j}$ denoting the probability of $x_{t,i}$ being $y_j$'s causal parent, then the regularization term is defined as\n$\\qquad R(G^{v2o}) = \\sum_{i=0}^{N-1}\\sum_{j=0}^{M-1}\\sum_{t=0}^{\\tau -1} p_{t,i,j}.$          (7)\nConsequently, the causal parents of outcome $y_j$ are determined by sampling from the causal probability matrix $\\mathcal{M}^{v2o}$, $t=0,...,\\tau-1$, i.e., $Pa(y_j; G^{v2o}) = {i; s_{t,i} = 1}_{t=0}^{\\tau-1}$, where $s_{t,i}$ is sampled from the distribution $Ber(p_{t,i,j})$.\nBecause the sampling process is non-differentiable, we use the Gumbel-Softmax trick59 to relax the sampling process, i.e.,\n$\\qquad s_{t,i} = \\frac{exp\\left((log(p_{t,i,j}) + g_{t,i}) / \\tau \\right)}{\\sum_{i=0}^{N-1} exp\\left((log(p_{t,i,j}) + g_{t,i}) / \\tau \\right)},$     (8)\nwhere $g_{t,i}$ is the Gumbel noise and $\\tau$ is the temperature parameter. In practice, the learning process consists of two alternating steps, one for optimizing the prediction neural network and the other for optimizing the causal probability matrix. We only use the Gumbel- Softmax trick in the latter step, and please refer to Supplements D.3 for details."}, {"title": "Cumulative window graph", "content": "In medical outcomes prediction, it is reasonable to assume that the causal effect of near time points is stronger than far ones, so we propose to incorporate a cumula- tive window graph into the learning process to penalize the causal probability with longer time lags. Specifically, we define\n$\\qquad p_{i,j,t}^{v2o} = \\sigma (\\prod_{w=1}^{t} q_{i,j,w}^{v2o} ) \\qquad p_{i,j,t}^{v2v} = \\sigma (\\prod_{w=1}^{t} q_{i,j,w}^{v2v} )$             (9)\nwhere $\\sigma(\\cdot)$ is the sigmoid function that maps the input to the range of [0,1]. In practice, the input time window (168 time points) is"}, {"title": "Controlled direct effect", "content": "Based on the discovered V2V and V2O graphs, we can provide causal pathways of the outcomes, demonstrating how each variable influences the subsequent one, and finally the outcome. Moreover, we can calculate the controlled direct effect (CDE) of each variable on the outcomes, i.e.,\n$\\qquad CDE(x; i) = f_{\\theta} (x_1, ..., x_i, ..., x_N) - f_{\\theta} (x_1,...,\\bar x_i, ..., x_N)$                                         (12)\nwith $\\bar x$ being the perturbance value of $x_i$, to serve as an explainable causal reasoning tool by providing a quantitative contribution of causal variables56,60. To better serve clinical practice, we developed a visualization tool to show the causal graphs and CDE values. The tool is built with Vue.js\u00b9 and Chart.js\u00b2, as exemplified in Fig. 2 and more examples are shown in Supplements C.3."}, {"title": "Efficient calculation", "content": "To calculate each CDE value, we need first to perform a full inference of the neural network, then perturb the variable of interest, and finally perform another full inference. This process includes redundant calculations and is too time-consuming for inferring thousands of CDE values. For acceleration, we propose to utilize the causally-decoupled inference techniques, which only update a subset of the hidden layers of the neural network after perturbing a variable. Specifically, the calculation process is as\n$\\qquad Inference: f_{\\theta} (X_p) = {Dec_j(Enc(X_p|Pa(y_j; G^{v2o})}))}_{j=0}^{M-1};$                                                               (13)\n$\\qquad Perturbations: f_{\\theta}(X'_p) = {Dec_j(Enc(X_p|Pa(y_j; G^{v2o})}))}_{j=0}^{M-1}.$\nHere $X'$ denotes perturbing variable $x_{t,i}$ to $\\bar x_{t,i}$, which means $(N+1)M$ inferences are required for calculating the CDE values of $N$ variables on $M$ outcomes. By contrast, when referring to the causal graph, we only need to calculate the sub-network corresponding to the causal parents of the variable of interest, i.e.,\n$\\qquad Perturbations : f_{\\theta}(X') = {Dec_j(Enc(...))}_{j\\in Desc(x_{t,i}; G^{v2o})},$                                                        (14)\nwhere $Desc(x_{t,i}; G^{v2v})$ is $x_{t,i}$'s descendants in the V2V graph. As a result, only $M + ||G^{v2o}||$ inferences are needed for calculating the CDE values, where $||G^{v2o}||$ is the total number of the causal parents for each outcome."}, {"title": "Theorem 1", "content": "Assuming that the causal graph $G^{v2o}$ is the true causal graph, and the causal parents of the outcome $y_j$ is $Pa(y_j; G^{v2o})$, $\\qquad f^* \\in \\underset{F\\in C^0}{arg\\underset{p \\in P}{min} } sup_{E_{(X,Y)\\sim p}} E(y - f(X))^2,$                       (16)\nwhere $C^0$ is the set of continuous functions $\\mathbb{R}^{N \\times t} \\to \\mathbb{R}$, and $P$ is the set of all possible distributions of $(X^T, Y^T)$."}]}