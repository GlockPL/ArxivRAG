{"title": "Transferable and Forecastable User Targeting Foundation Model", "authors": ["Bin Dou", "Baokun Wang", "Yun Zhu", "Xiaotong Lin", "Yike Xu", "Xiaorui Huang", "Yang Chen", "Yun Liu", "Shaoshuai Han", "Yongchao Liu", "Tianyi Zhang", "Yu Cheng", "Weiqiang Wang", "Chuntao Hong"], "abstract": "User targeting, the process of selecting targeted users from a pool of candidates for non-expert marketers, has garnered substantial attention with the advancements in digital marketing. However, existing user targeting methods encounter two significant challenges: (i) Poor cross-domain and cross-scenario transferability and generalization, and (ii) Insufficient forecastability in real-world applications. These limitations hinder their applicability across diverse industrial scenarios. In this work, we propose FIND, an industrial-grade, transferable, and forecastable user targeting foundation model. To enhance cross-domain transferability, our framework integrates heterogeneous multi-scenario user data, aligning them with one-sentence targeting demand inputs through contrastive pre-training. For improved forecastability, the text description of each user is derived based on anticipated future behaviors, while user representations are constructed from historical information. Experimental results demonstrate that our approach significantly outperforms existing baselines in cross-domain, real-world user targeting scenarios, showcasing the superior capabilities of FIND. Moreover, our method has been successfully deployed on the Alipay platform and is widely utilized across various scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, user targeting [25, 33], the strategic practice of concentrating on specific customers or users to deliver tailored content, messaging, and product recommendations [17], has garnered significant attention with the advancement of digital marketing [5]. Precise customer targeting forms the cornerstone of any effective marketing strategy, enabling the identification of optimal target audiences for each message and fostering consistent, personalized experiences across channels for individual customers.\nTo achieve precise user targeting, some previous studies [27] have developed domain-specific experts tailored to individual scenarios or learning common user embeddings [32] through user behavioral sequence data for e-commerce domain. However, expert methods are constrained by their exclusive focus on single scenarios, which limits their transferability and generalization. Additionally, most of them rely on labeled data to train supplementary classification heads for downstream tasks, posing challenges for application in low-resource settings, such as zero-shot learning. More recently, LLM-based methods, such as ARALLM[35], aim to enhance generalization across certain scenarios by leveraging vast repositories of common knowledge. Nevertheless, LLMs face significant challenges in utilizing industrial multi-source data, such as payment records and user behavior patterns, due to their insufficient understanding of long-tail knowledge [1, 34]. This limitation undermines the effectiveness of LLM-based methods in complex scenarios, particularly those involving multi-modal temporal and personal data. In conclusion, current methods primarily face two challenges: (i) Poor transferability and generalization, and (ii) insufficient forecastability in complex real-world scenarios.\nTherefore, we propose FIND, a transferable and forecastable user targeting foundation model designed to address the aforementioned challenges. To enhance transferability and generalization, we develop a user modeling framework that integrates heterogeneous multi-source data, such as billing records, super position model , mini-program descriptions, user information tables (containing user consumption records, financial irregularities, etc), and search texts within the Alipay application. To improve the forecastability, we collect and curate forecastable user-text pair data to align diverse modalities using contrastive learning. Considering the complexity of Alipay's data, this process is executed in two stages to ensure training stability. First, we design two novel self-supervised tasks to pretrain user models, facilitating the extraction of universal embeddings from heterogeneous, multi-scenario user data. Subsequently, we perform user-text alignment, where features from multi-modal data are fused and aligned with representations generated by a fine-tuned LLM through contrastive learning. Building on this foundation model, we propose two strategies for utilizing the pretrained models in user targeting: (1) Zero-shot Transfer, which directly inputting requirements in text format to identify targeted users based on user-text similarity. (2) Few-shot Targeting via Prompt-tuning, which Leverages a small set of seed users to enhance the text prompt's descriptive capabilities, thereby improving the performance of text-based user targeting. Extensive experiments demonstrate that FIND consistently outperforms baseline methods in user targeting, particularly in real-world Alipay applications, where it showcases strong predictive capabilities in complex scenarios. Furthermore, FIND achieves superior performance across diverse scenarios and domains, including security, marketing, and recommendation, highlighting its exceptional transferability and generalization.\nIn summary, our work makes the following contributions:\nWe propose FIND, an industrial-grade, transferable, and forecastable user-targeting foundation model capable of understanding users through large-scale heterogeneous multi-source data to accurately identify targeted users.\nWe design novel self-supervised tasks for pretraining user models and align different modalities through constructed user-text pair data in subsequent phases, ensuring training stability and forecastability within complex real-world scenarios while enabling the selection of targeted users based on one-sentence demands.\nExtensive experiments reveal that our model surpasses baseline methods across diverse scenarios and multiple domains, demonstrating its superior transferability and forecastability in real-world Alipay applications."}, {"title": "2 RELATED WORK", "content": "2.1 Multi-modal Pre-training\nMultimodal pretraining [36] has become essential for integrating diverse data types, such as text, images, audio, and video, to enhance model performance across various tasks. For instance, CLIP [30] employs contrastive learning to align image and text embeddings in a shared latent space, enabling zero-shot transfer without task-specific fine-tuning. ALIGN [14] expands this approach by utilizing larger datasets and more complex architectures, achieving state-of-the-art results in image-text retrieval and other benchmarks. BLIP [20] advances the field by combining contrastive and generative objectives within a unified transformer framework, enriching multimodal representations and improving tasks like image caption-ing. Additionally, BLIP-2 [19] and LLaVA [22] integrate multimodal encoders with large language models, creating robust multimodal large language models. Collectively, these methodologies facilitate the seamless integration of diverse modalities and leverage intermodal connections to boost model performance.\nBuilding upon these advancements, applications in other domains have garnered significant attention, particularly in recommendation systems, by incorporating user-item interactions with multimodal features. This synthesis enhances recommendation accuracy by capturing intricate relationships, as exemplified by models such as MGCL [23], MMSSL [37], and MMCPR [24]. In the graph domain, models such as ConGrat [4], G2P2 [39], and Graph-CLIP [46] extend CLIP into graph tasks, with the goal of enhancing zero-shot learning capabilities for graph-related applications.\nUnlike existing approaches, we integrate multi-source data of greater complexity. The distinct information across modalities complicates direct model alignment. To overcome these challenges, we propose a two-stage pretraining process: first, pretrain multimodal models using proposed novel auxiliary tasks, and second, align the pretrained models through meticulously constructed data pairs.\n2.2 User Modeling and Targeting\nUser modeling focuses on learning transferable user patterns from user behavioral sequence data [8, 32], while user targeting involves selecting targeted users based on the trained user models. In the context of user modeling, deep representation learning frameworks [26, 41] aimed at obtaining universal user representations are still in their early stages. For instance, Ni et al. [26] perform multitask representation learning using an attention-based RNN architecture to capture in-depth representations of portal users. Yuan et al. [41] propose parameter-efficient transfer learning architecture named PeterRec to relieve the computational cost burden of fine-tuning. PeterRec covers five downstream tasks that include predicting user profiles like gender, age, and life status, as well as a"}, {"title": "3 PRELIMINARIES", "content": "3.1 Notations\nIn the Alipay application, extensive non-sensitive user information and interactions are accessible. For instance, user behavioral sequence data is denoted as $V = \\{\\{B_n\\}_{n=1}^{N}, \\{S_n\\}_{n=1}^{N}, \\{M_n\\}_{n=1}^{N}\\}$, comprising PayBill B, Super Position Model S, and MiniProgram M. Here, $B_n \\in T^{L(B)}$, $S_n \\in T^{L(S)}$, and $M_n \\in T^{L(M)}$ represent the raw text for PayBill information, Super Position Model details, and MiniProgram descriptions, respectively. Let N denotes the number of users and $n \\in [1, 2, ..., N]$, T is the token dictionary, and $L(\\cdot)$ indicates the sequence length. Tabular data is denoted as $T \\in \\mathbb{R}^{N \\times F \\times D}$, where F represents the number of features and D denotes the dimensionality of each feature. Search text data is represented as $R \\in T^{L(R)}$. This work focuses on the user targeting task, utilizing the aforementioned data to pretrain a user model capable of selecting targeted users based on a single-sentence demand $Q \\in T^{L(Q)}$.\n3.2 User Targeting\nTo develop a user targeting foundation model, extensive user data can be utilized to pre-train a general model endowed with transferable knowledge:\n$\\theta_0^* = \\arg \\min_{\\theta} \\mathbb{E}_{V \\in V_S} L_{pretrain} (\\theta_0; V),$   (1)\nwhere $V_S$ represents the source data of user behavioral sequences, $\\theta_0$ denotes the pretrained user targeting foundation model, and $L_{pretrain}$ refers to our proposed pretraining tasks, which will be detailed in Sec. 4.\nWith the user targeting foundation model trained, the zero-shot transfer for user targeting and few-shot user targeting via prompt-tuning are proposed. For zero-shot setting, the pre-trained user model can be directly deployed on target data:\n$P_i = \\arg \\max_{U \\in U} P_{\\theta_0^*} (U_i | Q_i)$   (2)\nwhere $Q_i$ denotes a single-sentence demand and $U_i$ are the targeted users which require the demand which are sampled from user candidates U.\nFor the few-shot setting, a limited number of training samples for each class are used for fine-tuning:\n$\\theta_0' \\in \\arg \\max_{\\theta} \\mathbb{E}_{(Q_i,U_i) \\in \\mathcal{I}_{tr}} P_{\\theta} (\\hat{U}_i = U_i | Q_i)$,  (3)\nwhere $\\mathcal{I}_{tr}$ represents the supervised training paired data for downstream user targeting, comprising user demand and the corresponding targeted user group. $\\hat{U}_i$ is the predicted user group based"}, {"title": "4 METHOD", "content": "In this section, we first present the training corpus we collected and curated in Sec. 4.1. Next, we detail the proposed self-supervised tasks for user modeling in Sec. 4.2. Subsequently, we explain the user-text alignment process in Sec. 4.3. Finally, we demonstrate the application of our pre-trained model to downstream tasks in Sec. 4.4. The overall pipeline of FIND is illustrated in Fig. 1.\n4.1 Forecastable User-Text Pair Preparation\nBased on the full amount of user bill, page clicking behavior, miniprogram browsing and ordering, homepage search and other data within the Alipay system, a natural language description of user behavior is constructed. It should be noticed that the each user's language description is built based on the future behaviors mentioned before, which will enhance the forecastability of the model pre-trained by the data. Specifically, each user-text pair can be denoted as:\n$A = \\{V_{i,t_1}, T_{i,t_1}, R_{i,t_1}; Q_{i,t_2}\\}_{i=1}^{I}$   (4)\nwhere $t_1$ and $t_2$ denote the previous and future period.\nAdditionally, different templates are designed according to different data sources, and there are five types in total, divided into purchase category, browsing category, search category, click category and payment channel category. For example, the billing information occurred by a user will be strung together into a complete description as a user behavioral alignment label based on being at a certain merchant, purchasing a certain product, belonging to a certain category, transaction amount and payment channel. The template takes the form as:\n4.2 Self-Supervised User Modeling\n4.2.1 User Behavioral Sequence Encoding. As user behavior contains abundant information, which greatly improves the generalization ability of user representation and empowers user modeling in many downstream tasks, user behavioral sequence is selected as one of the user representational modal. At AliPay platform, PayBill P, MiniProgram M and SPM S are the main data sources for user representation, thus they are utilized as the input data for user understanding. Therefore, representational embedding for user i can formulated as:"}, {"title": "4.2.1 User Behavioral Sequence Encoding", "content": "$\\mathbf{e}(V) = P(\\theta_0(B_i, M_i, S_i))$  (5)\nwhere $P$ denotes the average pooling function, $\\theta_e$ is a composite function $\\theta_e = g_{\\mu} \\circ g_{\\nu}$, with $g_{\\mu}$ representing a time-aware function such as GRU [6], and $g_{\\nu}$ serving as a text encoding function, such as ALBERT [18]. Here, $z = g_{\\nu}(B_i, M_i, S_i)$ and $c = g_{\\mu}(z)$.\nAs the modals of user behavioral sequence are represented with text sequences, for User Behavioral Sequence Encoder $\\theta_e$, we utilize ALBERT [18], a light BERT [15] for self-supervised learning of language representations, to first encode text sequences to embeddings. Then, as illustrated in Fig. 2 (a), the forecasting prediction is utilized as the main proxy self-supervised pre-training task inspired by Contrastive Predictive Coding [28], as contrastive learning is employed to train the model\n$\\mathcal{L}_{CL} = - \\frac{1}{K} \\sum_{t=1}^{K} \\sum_{i=1}^{k} \\log \\frac{\\exp(s(c_t, z_{t+i}))}{\\sum_{j=1}^{K} \\exp(s(c_t, z_{t+j}))}$,    (6)\nwhere $z_t$ represents the user behavioral embedding at timestamp $t$ derived from the three data modalities, $c_t$ denotes the embedding aggregated with temporal information by $\\mu$, $s$ is the cosine similarity function, K is the total time length, and $k < K$ controls the window size for positive samples.\nAdditionally, as the paybill, miniprogram browsing records are commonly presented cyclically, we add cyclic regularization in user behavioral sequence encoding (shown in Fig. 2 (a)), enforcing each user's representation to be close across several time windows measured by KL-divergence. Therefore, the total self-supervised pre-training loss for user behavioral sequence is:\n$\\mathcal{L}_{UB} = \\mathcal{L}_{CL} + \\lambda KL(c_t||C_{t+T})$    (7)"}, {"title": "4.2.2 Tabular Encoding", "content": "In Alipay application scenarios, structural user data can be collected primarily in the form of tabular data. The key features of this tabular data pertain to account security and fund flows, facilitating transferability across multiple domains, such as risk management or marketing, for user targeting.\nTo obtain representation from user tabular, we employ transformer-based table encoder (using similar architecture in [13], denoted as $h_p$), pre-trained with two distinct strategies:"}, {"title": "4.2.3 Text Encoding", "content": "At Alipay platform, user searching text can also assist to achieve user understanding. Therefore, we utilize a light BERT model, i.e., AIBERT [18] as the encoder $f$, to encode the user i's searching text $x_{st}$, as the searching text encoding can be expressed as:\n$\\mathbf{e}(x_{st}) = f(R_i)$   (10)"}, {"title": "4.3 User-Text Alignment", "content": "4.3.1 Attention-based User Feature Fusion. For the multi-modal user representation, the embeddings from different sources are fused using cross-attention, which is a common feature aggregation for multi-modal features. The fused embedding of the user i can be expressed as:\n$\\mathbf{e}_f = CA(\\mathbf{e}(V_i), \\mathbf{e}(tab_i), \\mathbf{e}(r_i))$   (11)\nwhere CA represents the cross-attention among all data modalities.\n4.3.2 Contrastive Text-user Pre-training. To achieve user targeting task with only one sentence input, we develop the user targeting foundation model inspired by CLIP [30], which aligns multi-modal features with contrastive learning. Specifically, each user's description text is developed according to Sec. 4.1, which are fed into the LLM encoder fine-tuned with LoRA [12] to generate the user text embedding:\n$\\mathbf{e}_{u} = LLM(F(B_{i,t2}, M_{i,t2}, S_{i,t2}))$   (12)"}, {"title": "4.3.2 Contrastive Text-user Pre-training", "content": "$\\mathcal{L}_{CP} = - \\sum_{i=1}^{B} \\log \\frac{\\exp(s(\\mathbf{e}_f, \\mathbf{e}_{u}^{i,t2}))}{\\sum_{j=1}^{B} \\exp(s(\\mathbf{e}_{f}, \\mathbf{e}_{u}^{j,t2}))}$  (13)\nwhere B means the batch size and $s$ denotes the cosine similarity function."}, {"title": "4.4 User Targeting", "content": "In this section, we introduce the techniques employed to adapt models for user targeting. Figure 1 (b, I) illustrates the main user targeting pipeline of our model. First, we illustrate the adaptation of our model on target data for zero-shot transfer. Then, we propose a novel prompt tuning method for few-shot user targeting.\n4.4.1 Zero-shot Transfer. Once pre-trained, our model can be directly deployed on target datasets without any additional training, i.e., enabling zero-shot inference according to the one-sentence input as depicted in Figure 1 (b, I).\nAdditionally, to enable our model accessible to language demand from non-experts, we add a module to rewrite the demand, matching it with the template designed during pre-training and enhancing its cognitive ability for user data inside Alipay application. Therefore, the module performs as the Query Rewriting. We design a simple two-stage fine-tuning scheme: the first stage is designed to refer to scaling-law to synthesize a multi-round conversation set using multi-source data from Alipay to inject domain knowledge to one LLM (QWEN2-14B); then the seconda stage is to mine different people's circling intentions under  for fine-tuning.\n4.4.2 Few-shot User Targeting via Prompt Tuning. In low-resource scenarios, where only a few user samples (named seed users) exist for the targeting task, we also design an approach for few-shot learning via prompt-tuning, as illustrated in Figure 1 (b, II).\nInspired by previous studies [43\u201345], the seed users can be utilized as labels to learn contexts which improves the descriptive ability of the prompt, by adding learnable tokens to the input text. In practical user targeting scenarios, some negative samples share the similar behaviors with the ground-truth samples. For example, 3C digital repairers may have the similar behaviors compared to 3C digital enthusiasts. They can be defined as the hard-negative samples in user targeting task and the mistargeting of them may cause risk in real-world scenarios. To handle the hard-negative samples in our few-shot learning process, we introduce the negative samples $U^-$, which can obtained from business feedback, into prompt-tuning together with positive seed users $U^+$. Through the contrastive loss, i.e., triplet loss [11], the learned prompt embedding.\n$\\mathcal{L}_{Tri}(\\mathbf{e}_{f^+}, \\mathbf{e}_{f^-}, \\mathbf{e}_{q}) = \\sum_i max(0, || \\mathbf{e}_{q}-\\mathbf{e}_{f^+}||^2 - || \\mathbf{e}_{q}-\\mathbf{e}_{f^-}||^2 + \\alpha)$   (14)"}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct extensive experiments on our proposed model, to demonstrate its effectiveness on user targeting task as well as the user representation performance.\n5.1 Experimental Setups\n5.1.1 Datasets. For pre-training the industrial framework of user targeting, we employ the real-world industrial dataset, which collects heterogeneous multi-scenario user data on Alipay. Specifically, the training dataset $D_{train}$ contains around 500 million pieces of user information (user-text pairs), with each piece organized according to Section 4.1. The user targeting test benchmark contains 13 scenarios, covering security risk control, recommendation and marketing domain.\nThe dataset of each scenario contains 0.3-30 million pieces of user information, which are in the format $D_{test} = \\{(u_i, l_i)\\}_{i=1}^{N}$, as $u_i$ represents the same user data modal as in training while label $l_i$ ranges from $\\{0, 1\\}$, denoting whether the user conducts the operations in the domain, which is annotated according to real-world business scenario. For linear probe analysis, each test dataset is divided 1:4, of which the first part is used for linear layer fitting (named fitting part) while the second (named evaluation part) is used for evaluation.\n5.1.2 Baselines. The following state-of-the-art user behavioral understanding and targeting models are utilized in our comparison experiments.\nARALLM[35] Analogical Reasoning Augmented Large Language Models, which is the LLM-based method and achieves user targeting from one-sentence demand by structurally understanding the input sentence and matching the users representation with the labels. Pre-trained as well as Question-Answering fine-tuned models can be used for user targeting. Here we employ GPT-3.5[29] as the pre-trained model, open-source LLMs as the base model for fine-tuning, such as ChatGLM[9] and QWEN2[40].\nU-MLP One4all [32]. One4all is a general-purpose representation learning through large-scale pre-training, while U-MLP is one of its extended user targeting model which adds MLP user decoder to generate targeted users.\nMSDP[8] Multi-scale Stochastic Distribution Prediction model for learning user behavioral sequence representation, which takes the prediction on user's behaviors distribution over a period of time as the self-supervision signal.\nIn addition, TabTransformer trained with RTD & MLM tasks is employed as the tabular encoder, QWEN2-7B is employed as the searching text encoder for user representation evaluation.\n5.1.3 Evaluation Metrics. For User Targeting Task, as in each scenario the task can be treated as binary classification, we employ the common metrics Accuracy / Precision / Recall for evaluation. For User Representation, linear probe representation analysis is conducted on all annotated datasets, which is evaluated by AUC (Area Under the ROC Curve [3]) and KS (Kolmogorov-Smirnov [2]) metrics.\n5.1.4 Implementation Details. The embedding sizes of user representation and text features are set to 1024-dim. In Self-Supervised User Pre-training stage, the user behavioral sequences, i.e., pay-bill, SPM and miniprogram, which are in the form of text, are first encoded with AlBERT[18]. Then 6-layer Transformer is used to generate encoding of the series for further pre-training. For user tabular information, the similar configuration of TabTransformer is utilized in the tabular encoder. In the following User-Text Alignment stage, embeddings from multi-modal user features are fused using 5 cross attention layers. To generate the encoding of user text description, QWEN2-1.8B [40] is utilized as the LLM encoder, which are tuned with LoRA[12] together with user encoding model. For both of stages, the optimizer is AdamW, with cosine decay learning rate initialized at 4e-4. The model is pre-trained on 16 A100 GPUs (80G), and a single A100 is used for testing (linear probe for user embedding and zero-shot, few-shot user targeting)."}, {"title": "5.2 Comparisons", "content": "5.2.1 Zero-shot User Targeting. We take user targeting via zero-shot transfer based on one sentence input on the benchmarks.\nExperimental Setup. Both ARALLM and fine-tuned ARALLM are also employed as the baseline, taking the same sentence prompt as input.\nAnalysis. The quantitative results demonstrate the superiority of our model, as our model outperforms the baselines 10%/15%/10% at Accuracy/Precision/Recall metrics crossing all domains.\n5.2.2 Few-shot User Targeting. We also take few-shot user targeting on the benchmarks.\nExperimental Setup. Both ARALLM and fine-tuned ARALLM are also employed as the baseline, together with U-MLP One4all model trained with the seed users. For all few-shot learning baselines, the number of user samples is kept at 10. To ensure the comparison fairness upon seed user number, our model is trained with 5 positive and 5 negative users (10 total).\nAnalysis. The quantitative results demonstrate the superiority of our model, as our model outperforms the baselines 5%/8%/5% at Accuracy/Precision/Recall metrics crossing all domains."}, {"title": "5.2.3 User Representation Evaluation", "content": "We also evaluate the performance of the pre-trained user representation from both our model and other user modeling baselines.\nExperimental Setup For representation evaluation, we utilize linear probe analysis, which means employing linear network layers to the user embeddings to fit the fitting part of the dataset and evaluate on the evaluation part.\nAnalysis As shown in Table 4, the representation from our user model outperforms other baselines trained by single-modal data (+"}, {"title": "5.3 Ablation Studies", "content": "We conduct ablation studies on user data modal, feature fusion module and the user-text alignment method.\n5.3.1 Pre-training Data Modal. To verify the effect of the multi-source pre-training data, we conduct the comparison between the"}, {"title": "5.3.2 Attention-based Feature Fusion", "content": "The effect of feature fusion module in our user-text alignment stage is also evaluated.\nExperimental Setup. We employ the feature concatenation instead of the attention-based fusion module to fuse multi-modal user features as comparison.\nAnalysis. As shown in 4-th row of Table 5, the absence of attention-based multi-modal user feature fusion will lead to a performance decrease of 1%/2%/1% Accuracy/Precison/Recall approximately."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose an industrial framework FIND for transferable and predictive user targeting. Our model takes one-sentence form demand as input and achieve user understanding with heterogeneous multi-scenario data, enabling it to select targeted user among candidates. The framework is pre-trained in a two-stage strategy, where the first Self-Supervised User Modeling Stage is built to generate user representation from multi-modal user data, and the second User-Text Alignment Stage is built for fusing multi-modal user features and align user embedding with corresponding language representation for user targeting. Additionally, to enhance the model predictive ability, the user-text pair for our framework pre-training is also designed, as the text is obtained via from future use behaviors while user representation is generated from pervious information. With the pre-trained framework, zero-shot transfer and few-shot user targeting via prompt-tuning are designed to achieve user targeting. On real-world benchmarks, our model outperforms baselines in cross-domain scenarios, which demonstrates the superiority of our model. Additionally, in real-world application, our model exhibits superior performance compared with other industrial frameworks. Since 2024, FIND is deployed in the user selection platform of Alipay for numerous domains, obtaining an improvement for different user understanding scenarios. For example, improving 30% CTR for \"Used Car Selling Preferences\" users and 13.2% CTR for \"Video Watching in Alipay\" users."}]}