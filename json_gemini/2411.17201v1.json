{"title": "Learning Hierarchical Polynomials of Multiple Nonlinear Features with Three-Layer Networks", "authors": ["Hengyu Fu", "Zihao Wang", "Eshaan Nichani", "Jason D. Lee"], "abstract": "In deep learning theory, a critical question is to understand how neural networks learn hierarchical features. In this work, we study the learning of hierarchical polynomials of multiple nonlinear features using three-layer neural networks. We examine a broad class of functions of the form $f^* = g^* \\circ p$, where $p : \\mathbb{R}^d \\rightarrow \\mathbb{R}^r$ represents multiple quadratic features with $r < d$ and $g^* : \\mathbb{R}^r \\rightarrow \\mathbb{R}$ is a polynomial of degree $p$. This can be viewed as a nonlinear generalization of the multi-index model [Damian et al., 2022], and also an expansion upon previous work that focused only on a single nonlinear feature, i.e. $r = 1$ [Nichani et al., 2023; Wang et al., 2023].\nOur primary contribution shows that a three-layer neural network trained via layerwise gradient descent suffices for\n\\begin{itemize}\n    \\item complete recovery of the space spanned by the nonlinear features\n    \\item efficient learning of the target function $f^* = g^* \\circ p$ or transfer learning of $f = g \\circ p$ with a different link function\n\\end{itemize}\nwithin $O(d^4)$ samples and polynomial time. For such hierarchical targets, our result substantially improves the sample complexity $\\Theta(d^{2p})$ of the kernel methods, demonstrating the power of efficient feature learning. It is important to highlight that our results leverage novel techniques and thus manage to go beyond all prior settings such as single-index and multi-index models as well as models depending just on one nonlinear feature, contributing to a more comprehensive understanding of feature learning in deep learning.", "sections": [{"title": "Introduction", "content": "Deep neural networks have achieved remarkable empirical success across numerous domains of artificial intelligence [Krizhevsky et al., 2012; He et al., 2016]. This success can be largely attributed to their ability to extract latent features from real-world data and decompose complex targets into hierarchical representations, which improves test accuracy [He et al., 2016] and allows efficient transfer learning [Devlin, 2018]. These feature learning capabilities are widely regarded as a core strength of neural networks over non-adaptive approaches such as kernel methods [Wei et al., 2020; Bai and Lee, 2020].\nDespite these empirical achievements, the feature learning capabilities of neural networks are less well understood from a theoretical point of view. Previous work on feature learning has shown that two-layer neural networks can learn multiple linear features of the input [Damian et al., 2022], that is, multi-index models. However, the two-layer architecture inherently limits the network's ability to represent and learn nonlinear features [Daniely, 2017]. Given that many real-world scenarios involve diverse and nonlinear features, recent studies have shifted focus to investigating the learning of nonlinear features using deeper neural networks. Safran and Lee [2022]; Ren et al. [2023]; Nichani et al. [2023]; Wang et al. [2023] have demonstrated that three-layer networks, when trained via gradient descent, can efficiently learn hierarchical targets of the form $h = g \\circ p$, where $p$ represents certain types of features such as the norm $|x|$ or a quadratic form $x^\\top A x$. However, these studies are limited to relatively simple hierarchical functions and mainly focus on targets of a single feature. It remains unclear whether neural networks can efficiently learn a wider range of hierarchical functions, particularly those that depend on multiple nonlinear features. This leads us to the following central question:\nCan neural networks adaptively identify multiple nonlinear features from the hierarchical targets by gradient descent, thereby allowing an efficient learning for such targets?"}, {"title": "Main Contributions", "content": "In this paper, we provide strong theoretical evidence that three-layer neural networks have the ability to learn multiple hidden nonlinear features. Specifically, we study the problem of learning any hierarchical polynomial with multiple quadratic features using a three-layer network trained via layer-wise gradient descent. Our main contributions are summarized as follows:\n\\begin{itemize}\n    \\item A Novel Analytic Framework for Multi-Nonlinear Feature Learning. We demonstrate that when the target function belongs to a broad class of the form $f^* = g^* \\circ p$, where $p : \\mathbb{R}^d \\rightarrow \\mathbb{R}^r$ represents $r$ quadratic (nonlinear) features and $g^*$ is a link function, the first step of gradient descent efficiently learns and recovers the space spanned by these nonlinear features $p$ within only $O(d^4)$ samples. We remark that our proof techniques are also applicable to general nonlinear features. The core technical novelty is that we develop a novel and general universality argument (Lemma\n    \\item Improved Sample Complexity and Efficient Transfer Learning. Leveraging the learned features in the first GD step, we prove that when the link function $g^*$ is a polynomial of degree $p$, the gradient descent on the outer layer can achieves a vanishing generalization error with a small outer width and at most $O(r^{(p)})$ additional training samples, removing the dependence on $d$ (Theorem 1). This significantly improves upon the sample complexity of kernel methods, which require $\\Theta(d^{2p})$ samples. Moreover, our analysis enables efficient transfer learning for any other target function of the form $f = g \\circ p$ with a different link function $g$, which also only requires $O(r^{(p)})$ additional samples.\n\\end{itemize}"}, {"title": "Related Works", "content": "Kernel Methods. Earlier research links the behavior of gradient descent (GD) on the entire network to its linear approximation near the initialization. In this scenario, neural networks act as kernels, known as the Neural Tangent Kernel (NTK). This connection bridges neural network analysis with established kernel theory and offers initial learning guarantees for neural networks [Jacot et al., 2018; Soltanolkotabi et al., 2018; Du et al., 2018; Chizat et al., 2019; Arora et al., 2019]. However, kernel theory fails to explain the superior empirical achievements of neural networks over kernel methods [Arora et al., 2019; Lee et al., 2020; E et al., 2020]. Networks in the kernel regime fail to learn features [Yang and Hu, 2021], not adaptable to hierarchical structures of real world targets. Ghorbani et al. [2021] proves that for uniformly distributed data on the sphere, the NTK method requires $\\Omega(d^k)$ samples to learn any polynomials of degree $k$ in $d$ dimensions, which is impractical when $k$ is large. Thus, a central question is how neural networks can detect and capture the underlying hierarchies in the target functions, which allows for a better generalization behavior versus kernel methods.\nLearning Linear Features. Recent studies have demonstrated neural networks' capability to learn hierarchical functions of linear features more efficiently than kernel methods. Specifically, Bietti et al. [2022]; Ba et al. [2022] establish the efficient learning of singleindex models, i.e., $f^*(x) = g(\\langle u, x \\rangle)$. Furthermore, recent works Damian et al. [2022]; Abbe et al. [2023]; Dandi et al. [2023a]; Bietti et al. [2023] further demonstrate that for isotropic data, two-layer or three-layer neural networks can effectively learn multi-index models of the form $f^*(x) = g(Ux)$. These studies adopt certain modified training algorithms, such as layer-wise training. With sufficient feature learning, these networks can learn low-rank polynomials with a benign sample complexity of $O(d^{o(1)})$, which does not scale with the degree of the polynomial $g$. Empirically, fully connected networks trained via gradient descent on image classification tasks also capture low-rank features [Lee et al.,"}, {"title": "Preliminaries", "content": "We use bold letters to denote vectors and matrices. For a vector $v$, we denote its Euclidean norm by $||v||_2$. For a matrix $A$, we denote its operator and Frobenius norm as $||A||_2$ and $||A||_F$, respectively. For any positive integer $n$, we denote $[n] = \\{1, 2, ..., n\\}$. Moreover, for any indexes $i$ and $j$, we denote $\\delta_{ij} = 1$ if $i = j$ and $0$ otherwise. We use $O$, $\\Theta$ and $\\Omega$"}, {"title": "Problem Setup", "content": "Data distribution Our aim is to learn the target function $f^* : \\mathcal{X} \\rightarrow \\mathbb{R}$, with $\\mathcal{X} \\subseteq \\mathbb{R}^d$ being the input space. Throughout the paper, we assume $\\mathcal{X} = \\mathbb{S}^{d-1}(\\sqrt{d})$, that is, the sphere with radius $\\sqrt{d}$ in $d$ dimensions. Also, we consider the data distribution to be the uniform distribution on the sphere, i.e., $x \\sim \\text{Unif}(\\mathcal{X})$, and we draw two independent datasets $\\mathcal{D}_1, \\mathcal{D}_2$, each with $n_1$ and $n_2$ i.i.d. samples, respectively. Thus, we draw $n_1 + n_2$ samples in total.\nTarget function For the target function $f^* : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, we assume they are hierarchical functions of $r$ quadratic features\n$$f^*(x) = g^*(p(x)) = g^* (x^\\top A_1 x, x^\\top A_2 x, ..., x^\\top A_r x).$$ \nThis structure represents a broad class of functions where $p(x) = [x^\\top A_1 x, x^\\top A_2 x, ..., x^\\top A_r x]$ represents $r$ quadratic features, and $g^* : \\mathbb{R}^r \\rightarrow \\mathbb{R}$ is a link function. Here we consider the case $r < d$. To simplify our analysis while maintaining generality, we make the following assumptions:\nAssumption 1 (Orthogonal quadratic features). For any $i, j \\in [r]$, we suppose\n$$\\mathbb{E}_x [x^\\top A_i x] = 0, \\quad \\mathbb{E}_x [(x^\\top A_i x)(x^\\top A_j x)] = \\frac{\\kappa_1}{d} \\delta_{ij} \\text{ and } ||A_i||_{op} < \\frac{\\sqrt{\\kappa_1}}{\\sqrt{d}}.$$\nHere we assum\u0435 $\\kappa_1 = \\text{poly}(\\log d)$.\nThe first assumption is equivalent to $\\text{tr}(A_i) = 0$ for any $i \\in [r]$. For $A_i$ such that $\\text{tr}(A_i) \\neq 0$, we could simply subtract the mean of the feature to $A_i = A_i - (\\text{tr}(A_i)/d) \\cdot I$ so\n$$x^\\top A_i' x = x^\\top (A_i - (\\text{tr}(A_i)/d) \\cdot I)x = x^\\top A_i x - \\text{tr}(A_i).$$ The second assumption on the feature orthonormality can be attained via linear transformation on the features, preserving the overall function class. The third assumption on the operator norm bound ensures that the features are balanced, which is common in the nonlinear feature learning literature [Nichani et al., 2023; Wang et al., 2023]. Moreover, we note that when the entries of $A_i$ are sampled i.i.d., the assumption is satisfied with high probability by standard random matrix arguments."}, {"title": "Training Algorithm", "content": "Following Nichani et al. [2023], our network is trained via layerwise gradient descent with sample splitting. Throughout the training process, we freeze the innermost layer weights $V$. In the first stage, the second layer weights $W$ are trained for one step with a specified learning rate $\\eta_1$ and weight decay $\\lambda_1$. In the second stage, we reinitialize the bias $b$ and train the outer layer weights $a$ for $T \\ge 1$ steps.\nTransfer Learning We remark that our algorithm allows transfer learning of a different target function $f$ that shares the same features of the original target:\n$$f^*(x) \\rightarrow f(x) = g (x^\\top A_1 x, x^\\top A_2 x, ..., x^\\top A_r x) \\quad \\text{(transferred target)}$$ \nIn this case, we switch the target function from $f^* = g^*(p)$ to $f = g(p)$ in the second training stage. For the loss function, we use the standard squared loss:\n$\\mathcal{L}^{(1)}(\\theta) = \\frac{1}{n_1} \\sum_{x \\in \\mathcal{D}_1} (f(x; \\theta) - f^*(x))^2$ (original),\n$\\mathcal{L}^{(2)}(\\theta) = \\frac{1}{n_2} \\sum_{x \\in \\mathcal{D}_2} (f(x; \\theta) - f^*(x))^2$ (transferred). \nThis layer-wise training approach, combined with the ability to perform transfer learning, provides a powerful framework for learning and adapting to hierarchical functions with hidden features [Kulkarni and Karande, 2017; Damian et al., 2022; Nichani et al., 2023]."}, {"title": "Technical Background: Analysis Over the Sphere", "content": "We briefly introduce spherical harmonics and Gegenbauer polynomials, which forms the foundation of our analysis over the sphere $\\mathbb{S}^{d-1}(\\sqrt{d})$. For more details, see Appendix A.5.\n\nLet $\\tau_{d-1}$ be the uniform distribution on $\\mathbb{S}^{d-1}(\\sqrt{d})$. Consider functions in $L^2(\\mathbb{S}^{d-1}(\\sqrt{d}), \\tau_{d-1})$, with scalar product and norm denoted as $\\langle \\cdot, \\cdot \\rangle_{L^2}$ and $|| \\cdot ||_{L^2}$. For $l \\in \\mathbb{Z}_{\\ge 0}$, let $V_{d,l}$ be the linear space of homogeneous harmonic polynomials of degree $l$ restricted on $\\mathbb{S}^{d-1}(\\sqrt{d})$. The set $\\{V_{d,l}\\}_{l \\ge 0}$ forms an orthogonal basis of the $L^2$ space, with dimension $\\text{dim}(V_{d,l}) = \\Theta(d^l)$. For each $l \\in \\mathbb{Z}_{\\ge 0}$, the spherical harmonics $\\{Y_{l,j}\\}_{j \\in [B(d,l)]}$ form an orthonormal basis of $V_{d,l}$. Moreover, we denote by $P_k$ the orthogonal projections to $V_{d,k}$, which can be written as\n$$P_k(f)(x) = \\sum_{l=1}^{B(d,k)} \\langle f, Y_{k,l} \\rangle_{L^2} Y_{k,l}(x).$$ We also define $P_{\\le l} = \\sum_{k=0}^l P_k$, $P_{>l} = I - P_{\\le l}$, $P_{<l} = P_{\\le l-1}$, and $P_{>l} = P_{>l-1}$.\nCorresponding to the degree $l$ spherical harmonics in the $d$-dimension space, the $l$-th Gegenbauer polynomial $Q_l : [-d, d] \\rightarrow \\mathbb{R}$ is a polynomial of degree $l$. The set $\\{Q_l\\}_{l \\ge 0}$ forms an orthogonal basis on $L^2([-d, d], \\tau_{d-1})$, where $\\tau_{d-1}$ is the distribution of $\\sqrt{d}(x, e_1)$ when $x \\sim \\tau_{d-1}$. In particular, these polynomials are normalized so that $Q_l(d) = 1$. We present the explicit forms of Gegenbauer polynomials of degree no more than 2:\n$Q_0(t) = 1$,\n$Q_1(t) = \\frac{t}{d}$,\nand $Q_2(t) = \\frac{t^2 - d}{d(d-1)}$.\nGegenbauer polynomials are directly related to spherical harmonics, leading to a number of elegant properties. We provide further details on these properties in Appendix A.5."}, {"title": "Main results", "content": "The following is our main theorem, which bounds the population absolute loss of Algorithm 1:\nTheorem 1. Suppose $n_1, n_2 = \\tilde{O}(d^4)$. Let $\\theta$ be the output of Algorithm 1 after $T = \\text{poly}(n_1, n_2, m_1, m_2, d)$ steps. Then, there exists a set of hyper-parameters $(\\epsilon, \\eta_1, \\eta_2, \\lambda_1, \\lambda_2)$ such that, with high probability over the initialization of parameters and draws of $\\mathcal{D}_1, \\mathcal{D}_2$, we have\n$$\\mathbb{E}_x [|f(x; \\theta) - f^*(x)|] = \\tilde{O} \\left( \\frac{r^p \\kappa_2}{\\sqrt{\\min(m_1, n_2)}} + \\frac{d^6 r^{p+1}}{m_2} + \\frac{d^2 r^{p+1}}{\\sqrt{n_1}} + \\frac{r^{p+2} \\kappa_1}{d^{1/6}} \\right).$$\nMoreover, for any other degree $p$ polynomial $g : \\mathbb{R}^r \\rightarrow \\mathbb{R}$ with $||g||_{L^2} \\le 1$, by substituting the target function $f^* = g^* \\circ p$ by $f = g \\circ p$ in the second training stage, we can achieve the same result for learning the new target function."}, {"title": "Stage 1: Learning the Features", "content": "We provide a brief analysis on the learned representations after the first training stage. Denote $w_j = \\epsilon^{-1} w^{(0)}_j \\sim \\mathcal{N}(0, I_{m_2})$. According to Algorithm 1, by setting $\\epsilon$ sufficiently small, after one-step gradient descent on $W$, we know for each $j \\in [m_1]$,\n$$\\eta_1 \\nabla_{w^{(0)}_j} \\mathcal{L}^{(\\theta^{(0)})} = - \\frac{\\eta_1}{m_1} \\frac{a_j^{(0)}}{n_1} \\sum_{x_i \\in \\mathcal{D}_1} f^*(x_i) h^{(0)}(x_i) \\sigma_1'(\\langle \\epsilon w_j, h^{(0)}(x_i) \\rangle)$$\n$$= - \\frac{2 \\epsilon \\eta_1 a_j^{(0)}}{m_1 n_1} \\sum_{x_i \\in \\mathcal{D}_1} f^*(x_i) h^{(0)}(x_i) h^{(0)}(x_i)^\\top w_j.$$\nBy taking $\\frac{2 \\epsilon \\eta_1 \\sqrt{2l}}{m_1} \\cdot \\eta$ for some $\\eta > 0$ to be chosen later and $\\lambda_1 = \\eta_1^{-1}$, we have\n$$w_j^{(1)} = w_j^{(0)} - \\eta_1 [\\nabla_{w_j^{(0)}} \\mathcal{L}^{(\\theta^{(0)})} + \\lambda_1 w_j^{(0)}]$$\n$$= - \\eta_1 \\nabla_{w_j^{(0)}} \\mathcal{L}^{(\\theta^{(0)})}$$\n$$= \\frac{2 \\epsilon \\eta_1}{m_1 n_1} a_j^{(0)} \\sum_{x_i \\in \\mathcal{D}_1} f^*(x_i) h^{(0)}(x_i) h^{(0)}(x_i)^\\top w_j.$$\nThen for any second-stage training sample $x' \\in \\mathcal{D}_2$, the inner-layer representation becomes\n$$\\langle w_j^{(1)}, \\sigma_2(Vx') \\rangle = \\frac{\\eta a_j^{(0)}}{m_2 n_1} \\sum_{x_i \\in \\mathcal{D}_1} f^*(x_i) h^{(0)}(x_i) h^{(0)}(x_i)^\\top w_j, \\sigma_2(Vx')$$\n$$= \\eta \\frac{a_j^{(0)}}{m_2 n_1} \\sum_{x_i \\in \\mathcal{D}_1} f^*(x_i) \\langle h^{(0)}(x_i), h^{(0)}(x') \\rangle h^{(0)}(x_i).$$\nOur main contribution in this part is that the first-step trained presentations representations $h^{(1)}(x)$ approximately spans the space of the target features $(x^\\top A_1 x, x^\\top A_2 x, ..., x^\\top A_r x)$. Thus, the target features $p(x)$ can be reconstructed through a linear transformation from the learned representations $h^{(1)}(x)$, which is formalized in the following proposition."}, {"title": "Stage 2: Learning the Link Function", "content": "By the deduction above, after the first training stage, the model becomes a random-feature model [Rahimi and Recht, 2007]:\n$$f(x'; \\theta) = \\frac{1}{m_1} \\sum_{j=1}^{m_1} a_j \\sigma_1 (\\langle \\eta_2 (w_j, h^{(1)}(x')) + b^{(1)} \\rangle).$$\nHere $\\theta = (a, W^{(1)}, b^{(1)}, V)$, with $a = [a_1, a_2, ..., a_{m_1}] \\in \\mathbb{R}^{m_1}$ being the trainable parameters in the second stage. Leveraging the construction in Proposition 1, we can construct a corresponding weight vector $a$ in the outer layer to express the polynomial $g(B^*h^{(1)}(x)) \\approx g(p(x))$"}, {"title": "Numerical Experiments", "content": "We empirically verify Theorem 1 and Proposition 1. We consider learning functions with $r = 3$ quadratic features. Regarding the target function, we choose the target functions to be of the form\n$$f_{a,p}^*(x) = \\frac{f_{d,p}(x) - \\mathbb{E} [f_{d,p}(x)]}{\\sqrt{\\text{Var}[f_{d,p}(x)]}}, \\quad \\text{with } f_{d,p}(x) = \\sum_{i=1}^{r} (x^\\top A_i x)^p, p \\in \\mathbb{N}.$$ \nFor the underlying features, we take $p(x) = [x^\\top A_1 x, x^\\top A_2 x, x^\\top A_3 x]^\\top$ with $A_k = \\text{diag}(c a_k)$, and $c > 0$ is a normalizing constant. To ensure the orthogonality of the features and $\\text{tr}(A_k) = 0$, we choose the ambient dimension $d$ to be divisible by 4 and take $a_k$ to be\n$a_1 = \\text{Vec}([1, 1, -1, -1])$, $a_2 = \\text{Vec}([1, -1, 1, -1])$, and $a_3 = \\text{Vec}([1, -1, -1, 1])$.\nHere 1 is a vector of ones in $d/4$ dimensions, and $c = \\frac{d + 2}{\\sqrt{2d^2}}$ to ensure that $\\mathbb{E}_x [(x^\\top A_k x)^2] = 1$ for each $k = 1, 2, 3$.\nFor the network architecture, we choose $\\sigma_1$ as per (2) and $\\sigma_2 = Q_2$, with network sizes set to $m_1 = 10000$ and $m_2 = 20000$. We compare our proposed model (4) (given by Algorithm 1) against the naive random-feature model defined as\n$$f^{\\text{RF}}(x'; \\theta) = \\frac{1}{m_1} \\sum_{j=1}^{m_1} a_j \\sigma_1 (\\langle \\eta a_j^{(0)} (w_j, h^{(0)} (x')) + b_j^{(1)} \\rangle),$$ where $a$ is the only trainable parameter throughout the training process. Our experiments involve learning $f_{a,p}^*$ with $p = 4$ and $d \\in \\{8, 16, 32\\}$. To examine our model's transfer learning capabilities, we also train the model on an initial target function $f_{16,2}^*$ with $d = 16$ and $n_1 = 2^{16}$ in the first stage, then transfer to targets $f_{a,p}^*$ with $p = 4, 6, 8$. For each task, we explore a range of sample sizes from $2^8$ to $2^{16}$."}, {"title": "Conclusions and Discussions", "content": "As discussed under assumptions 3 and the initialization of our neural networks, our work differs significantly in the targets of interests, the parametrization of neural networks, the mathematical strategies, and the intuitions behind the results. Our assumptions ensure a nearly zero linear component and a non-degenerate second order term of the link function $g$ which significantly contrasts the assumptions posed in Nichani et al. [2023]; Wang et al. [2023] that emphasize the linear component. Our random initialization (rather than a deterministic initialization used in the aforementioned two works) in the weights of the three-layer neural networks allows the learned weights to capture multiple features in all directions simultaneously after training rather than converge to a single direction. We develop a novel universality result"}]}