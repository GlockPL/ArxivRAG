{"title": "UniForm: A Unified Diffusion Transformer for Audio-Video Generation", "authors": ["Lei Zhao", "Linfeng Feng", "Dongxu Ge", "Fangqiu Yi", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "abstract": "As a natural multimodal content, audible video delivers an immersive sensory experience. Consequently, audio-video generation systems have substantial potential. However, existing diffusion-based studies mainly employ relatively independent modules for generating each modality, which lack exploration of shared-weight generative modules. This approach may under-use the intrinsic correlations between audio and visual modalities, potentially resulting in sub-optimal generation quality. To address this, we propose UniForm, a unified diffusion transformer designed to enhance cross-modal consistency. By concatenating auditory and visual information, UniForm learns to generate audio and video simultaneously within a unified latent space, facilitating the creation of high-quality and well-aligned audio-visual pairs. Extensive experiments demonstrate the superior performance of our method in joint audio-video generation, audio-guided video generation, and video-guided audio generation tasks.", "sections": [{"title": "1. Introduction", "content": "With the flourishing of deep learning, artificial intelligence generated content (AIGC) has enabled vivid creation across text (Chung et al., 2024; Li et al., 2024), images (Rombach et al., 2022; Zhu et al., 2023), audio (Liu et al., 2024a; Wang et al., 2024b), and video (Song et al., 2024; Zheng et al., 2024). It unlocks new applications in writing, design, audio, and video production, and broadens possibilities in digital media. However, AI-based content generation is typically conducted within isolated subdomains, such as text to sound effect generation (Tan et al., 2024) or text to silent video generation (Wang et al., 2024c), neglecting the integration of auditory and visual elements that are crucial for a complete immersive experience. Therefore, the aim of our current work is to generate high-quality audible video by integrating text, audio, or video inputs.\n\nA common and straightforward approach for generating audible video content is a two-stage process: first, silent video is generated, followed by the corresponding audio based on the video. For example, in Movie Gen (Polyak et al., 2024), silent video can be generated using a single video prompt or supplemented with images as key frames. The silent video is then used as a condition in conjunction with an audio prompt to customize the audio. This approach completely decouples the generation of the two modalities. The advantage is its ease of implementation, while the drawback is that the association between audio and video may not be enough. The alignment between audio and video content faces challenges.\n\nIn contrast, recent works have focused on jointly generating audio and video. MM-Diffusion (Ruan et al., 2023) utilizes two subnets to generate audio-video pairs. In (Xing et al., 2024), a multimodal latent aligner is introduced to link powerful pre-trained single-modal generation models. They propose jointly generating video and audio using U-Net based diffusion models. In contrast, another emerging diffusion backbone is the Diffusion Transformer (DiT) (Peebles & Xie, 2023), which has demonstrated remarkable"}, {"title": "2. Related Work", "content": "In this paper, we focus on \u201cFoley\u201d audio 1, which refers to sound effects created and added during post-production to enhance the auditory experience of multimedia (Choi et al., 2023), such as simulating the crunch of leaves underfoot or the clinking of glass bottles. AI-based Foley generation in previous years was based on class labels (Liu et al., 2021) or text prompts (Liu et al., 2023). Building on this, recent advancements have expanded the scope of video-to-audio generation. (Du et al., 2023) requires conditional audio alongside silent video to generate multiple audio tracks, which are subsequently selected using an audio-visual synchronization model. DIFF-FOLEY (Luo et al., 2024) only requires silent video as input. It first learns aligned features through contrastive audio-visual pretraining (CAVP), followed by training a diffusion model conditioned on CAVP visual features within the spectrogram latent space. Meanwhile, FoleyCrafter (Zhang et al., 2024) introduces an optional text prompt to further enhance the granularity of audio generation. Additionally, it employs a semantic adapter and a temporal controller to improve alignment. LoVA (Cheng et al., 2024) use a DiT for V2A tasks, specifically targeting 10-second long videos. Unlike previous works that primarily rely on GANs or diffusion models, FRIEREN (Wang et al., 2024d) adopts a flow matching generative model as its backbone. To enhance temporal alignment, it leverages a non-autoregressive vector field estimator network that avoids downsampling along the temporal dimension.\n\nGiven the substantial information density of video, video-to-audio generation tasks can prioritize video as the primary input, with text serving a supplementary role. In contrast, audio-to-video generation tasks rely on audio mainly as a reference for alignment. This is because audio alone carries limited information (e.g., neither machines nor humans can easily determine whether a given audio clip corresponds to climbing stairs or tap dancing). As a result, audio to video generation typically requires texts or images to provide additional context for the video content. AADiff (Lee et al., 2023) is an early example of an audio-aligned diffusion framework that uses both text and audio as input. TPOS (Jeong et al., 2023) employs a stage-wise generation strategy: first, it generates an initial frame based on a text prompt, then adaptively modifies the generated images in response to the audio inputs. (Yariv et al., 2024) introduces a lightweight adaptor network that learns to map audio-based representations to the input format expected by an existing text-to-video generation model. Meanwhile, AVSyncD"}, {"title": "3. Method", "content": "In this section, we introduce Unified Diffusion Transformer (UniForm) model to achieve not only text-guided audio and video mutual generation but also joint generation in a single model. We first review the preliminary knowledge of the process of generation based on diffusion in section 3.1.1 as well as Diffusion Transformer (DiT) model in section 3.1.2. Then in 3.1.3 we give the problem definition of our three basic generation tasks. Finally, in Section 3.2, we provide a detailed introduction to our proposed UniForm. UniForm leverages the cross-modal representation capability of DiT to successfully tackle three tasks simultaneously."}, {"title": "3.1. Preliminaries", "content": ""}, {"title": "3.1.1. DIFFUSION-BASED GENERATION", "content": "As a probabilistic generative model, diffusion model has garnered significant attention in recent years, owing to its exceptional performance in tasks like image generation (Ramesh et al., 2022) and audio generation (Liu et al., 2024a). The core idea of the diffusion model is that it first defines a process of gradually transforming a data distribution into a Gaussian noise distribution (forward process), which can be seen as a series of steps that gradually add noise. Subsequently, the model learns how to perform the inverse operation of this process, starting from pure noise and gradually denoising through a series of inverse steps, ultimately generating samples that are close to the original data distribution (reverse process).\n\nMost existing diffusion models are built upon Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020). DDPMs define the forward process as a transition from the data distribution $X$ to the standard Gaussian distribution $N(0,I)$ by gradually adding noise to the original data sample $x_0$ within discrete time step $t$ in forward process. The forward process is defined as a Markovian process, which can be formulated as $q(x_t | x_{t-1}) = N(x_t; (1 - \\beta_t)x_{t-1}, \\beta_tI)$, where $x_0 \\sim q(x_0)$ represents the initial true data distribution, noising schedules {$\\beta_1, \\beta_2, ..., \\beta_t, ..., \\beta_T$} are added over a total diffusion steps $T$. Re-parameterization of the forward process can be denoted as $q(x_t | x_0) = N(x_t;\\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$ where $x_t$ represents as a direct sample from $x_0$, $\\bar{\\alpha}_t = \\prod_{i=1}^{t}(1 - \\beta_{\\epsilon})$. This formula indicates that $x_t$ at any timestep can be generated directly from $x_0$ and standard Gaussian noise, $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{(1 -\\bar{\\alpha}_t)}\\epsilon$, $\\epsilon \\sim N(0, 1)$.\n\nThe goal of the reverse process is to progressively generate $x_0$ from pure noise $x_T \\sim N(0, I)$. Similar to for-"}, {"title": "3.1.2. DIFFUSION TRANSFORMER (DIT)", "content": "Latent Diffusion Model (LDM) (Rombach et al., 2022) transfers the diffusion process from a high-dimensional data space to a low-dimensional latent space, significantly decreasing computational overhead. LDM utilizes pre-trained autoencoders to project data into the latent space, where it subsequently performs the diffusion process using diffusers, typically a U-Net architecture (Ronneberger et al., 2015). Diffusion Transformer (DiT) (Peebles & Xie, 2023) builds upon Vision Transformer (ViT) (Dosovitskiy, 2020), serving as an enhancement to the traditional U-Net backbone for class-conditioned image generation. The self-attention mechanism of the Transformer excels at modeling global dependencies and spatial context information, making it particularly suitable for high-resolution images. In this work, we utilize DiT as the backbone of our multi-task model."}, {"title": "3.1.3. PROBLEM DEFINITION", "content": "Our goal is to showcase that both video and audio modalities can be produced using a completely same model. This contrasts with current approaches to audio and video generation, which typically involve separate models or designs for generating video and audio. Here, we define three multimodal generation tasks, including text to audio and video (T2AV), audio to video (A2V) and video to audio (V2A). In T2AV inference stage, given vision and audio Gaussian noise $z_v^T$ and $z_a^T$, denoising network $\\theta_{av}$ aims to gradually generate two modalities simultaneously conditioned on given text. In single modality tasks A2V and V2A, diffusion model tries to generate video and audio respectively based on known modality and text description. We further employ the classifier-free guidance (CFG) scheme to enable text-conditioned generation. We train our diffusion model to predict conditional noise $\\epsilon_{\\theta}(x_t, t, c)$ using conditional information $c$ while predict unconditional noise $\\epsilon_{\\theta}(x_t, t, \\phi)$ when condition information is randomly drop."}, {"title": "3.2. Text-guided UniForm for Multitask Generation", "content": "To achieve free multimodal generation of audio and video within a fully unified framework. We propose a diffusion based generation framework UniForm that processes vision and audio latent in a unified space. Detailed descriptions of the key components design of our model are provided in the following part."}, {"title": "3.2.1. VIDEO & AUDIO LATENT ENCODING", "content": "As mentioned in Section 3.1.2, DiT maps the input to the latent space in order to decrease computational costs. Our UniForm similarly follows this paradigm. More specifically, in training stage, given a batch size $B$ input videos $V \\in R^{B\\times F\\times C \\times H \\times W}$, where $F$ denotes the number of frames in the videos, each frame has $C$ channels, with a height of $H$ and a width of $W$. A 3D-VAE encoder (Zheng et al., 2024) $E_v$ is adopted to extract vision latent $z^v \\in R^{B \\times \\hat{C}^v \\times \\hat{F} \\times \\hat{H} \\times \\hat{W}}$ from videos, where $\\hat{C}^v, \\hat{F}, \\hat{H}, \\hat{W}$ are hidden dims of vision tokens. For the audio inputs, we initially utilize the Short-Time Fourier Transform (STFT)"}, {"title": "3.2.2. MULTITASK EMBEDDING", "content": "As shown in Figure 2, we incorporate additional task tokens into the input to assist the model in better understanding the task. Specifically, the task ID among three tasks is passed through an Embedding layer to obtain its latent representation, which is called task token. All latent tokens remain consistent across all dimensions except for the last, which is adjusted through reshaping, and they are then connected together using concatenation. Subsequently, the input passes through a Multimodal Patch Embedder and is augmented with 2D positional encoding. Additionally, a Time Embedder is utilized to integrate time information into the input. After obtaining the joint representation of vision as well as audio as input, we adopt spatial and temporal STDiT3"}, {"title": "3.2.3. VIDEO & AUDIO LATENT DECODING", "content": "Once we obtain the final output from the DiT blocks, we utilize Multimodal unpatchify to derive the predicted noise and variance for both the video and audio, which serves the diffusion objective. Specifically, after the diffusion process is completed, the two sampled Gaussian noises are concatenated and fed into the trained model. This process gradually reduces the noise, ultimately generating latent information with minimal noise in the final diffusion time step. The"}, {"title": "3.2.4. VIDEO & AUDIO GENERATION LOSS", "content": "Here, we outline the objective of our model in the denoising process for the three tasks that we previously proposed. During V2A task, the audio generation loss $L_a$ can be formulated as:\n\n$L_a = MSE(Mask_a(\\epsilon) - Mask_a(\\epsilon_{\\theta}(z_v^t, z_a^t, t,c)))$, (1)\n\nwhere $Mask_a$ represents the masking operation to all latent representations, with the exception of audio tokens, i.e., task tokens, and vision tokens. For the video generation loss $L_v$"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Datasets: The training datasets used in this work include AudioSet-balanced (Gemmeke et al., 2017) AudioSet-strong (Hershey et al., 2021), VGGSound (Chen et al., 2020) and Landscape (Lee et al., 2022). Audioset is a large-scale Audio-visual dataset released by Google, which contains more than a million labeled video clips extracted from YouTube videos. Each clip has been manually annotated, marking one or more sound events present within it. AudioSet-balanced consists of 22,176 segments selected"}, {"title": "4.2. Results on Video to Audio Generation", "content": "Table 1 lists the comparison results of our multi-task model with some recent approaches on video to audio generation. From the table, we can see that our method outperforms the majority of baselines across most metrics. Specifically, it achieves first place in both the FAD (1.3) and FD (6.21) metrics, leading over FoleyCrafter (Zhang et al., 2024) and VATT (Liu et al., 2024b), which are ranked second in those respective categories with FAD scores of 2.51 and 2.77, and FD scores of 16.24 and 10.63, respectively. Additionally, our model secures second place in the IS metric with a score of 14.68, closely following FoleyCrafter (Zhang et al., 2024), which ranks first with an IS score of 15.68. Notably, with the exception of (Xing et al., 2024), the other baseline methods are restricted to single video-to-audio tasks. This highlights that our multi-task model can generate audio that is highly relevant to the video content, with a quality that rivals the best current V2A approaches."}, {"title": "4.3. Results on Audio to Video Generation", "content": "Table 2 highlights the exceptional performance of our method in audio-to-video generation. Specifically, our approach achieves the lowest FVD score (319), demonstrating its ability to generate videos with the highest quality. Meanwhile, in terms of IS, which measures content diversity, our approach also leads (4.61), showcasing its strength in generating varied content. Although our method's AV-Align score (0.51) is slightly lower than that of TempoToken (0.54), overall, our method excels in the field of audio-to-video generation, particularly in video quality and diversity. This highlights our method's ability to enhance synchronization between audio and visual elements while significantly improving the overall quality of generated videos."}, {"title": "4.4. Results on Joint Audio-Video Generation", "content": "Table 3 lists the comparison results of different methods for joint video and audio generation on the Landscape dataset. As shown in the table, our method performs excellently on the FVD (326), KVD (22.8), and AV-Align (0.30) metrics, indicating that the generated video content is not only more realistic but also better aligned between audio and video."}, {"title": "4.5. Ablations on text prompts", "content": "Table 4 demonstrates the influence of text on both V2A and A2V tasks. Clearly, employing text as a conditional guide markedly improves the quality of generated videos"}, {"title": "5. Conclusions", "content": "We have introduced a novel unified audio-video generation model, UniForm, which achieves simultaneous audio and video synthesis using a single diffusion framework. Built on a diffusion transformer (DiT) backbone, it employs distinct task tokens to enable audio-video synthesis under"}]}