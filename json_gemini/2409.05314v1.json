{"title": "Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications", "authors": ["Ali Maatouk", "Kenny Chirino Ampudia", "Rex Ying", "Leandros Tassiulas"], "abstract": "The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in telecommunications remain limited, often relying on general-purpose models that lack domain-specific specialization. This lack of specialization results in underperformance, particularly when dealing with telecommunications-specific technical terminology and their associated mathematical representations. This paper addresses this gap by first creating and disseminating Tele-Data, a comprehensive dataset of telecommunications material curated from relevant sources, and Tele-Eval, a large-scale question-and-answer dataset tailored to the domain. Through extensive experiments, we explore the most effective training techniques for adapting LLMs to the telecommunications domain, ranging from examining the division of expertise across various telecommunications aspects to employing parameter-efficient techniques. We also investigate how models of different sizes behave during adaptation and analyze the impact of their training data on this behavior. Leveraging these findings, we develop and open-source Tele-LLMs\u00b9, the first series of language models ranging from 1B to 8B parameters, specifically tailored for telecommunications. Our evaluations demonstrate that these models outperform their general-purpose counterparts on Tele-Eval while retaining their previously acquired capabilities, thus avoiding the catastrophic forgetting phenomenon.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have recently marked a major breakthrough in natural language processing. Since 2022, these deep learning systems have proliferated rapidly, with numerous models released by tech giants, research institutions, and open-source communities [7, 8, 24]. Trained on vast text corpora, LLMs have demonstrated unprecedented capabilities in understanding context, generating human-like text, and performing reasoning tasks across diverse domains [27]. These abilities have sparked interest from researchers and industry professionals to adopt and explore their potential applications across a wide variety of fields.\nResearchers in the telecommunications and networking domain are no exception, aiming to leverage LLMs' potential across various tasks within the field. These applications include chatbots for engineers [16], network document analysis [3], network modeling and development [23], and wireless systems design [6, 31]. The list of tasks being explored for potential LLM support continues to grow [2], with new research emerging rapidly to assess the limits and utility of these models in the telecommunications context.\nThus far, applications of LLMs in the telecommunications domain have primarily involved prompting [6, 23], in-context learning [16], and task-specific fine-tuning [3, 35], utilizing either proprietary LLMs like OpenAI's GPT [24] or local open-source generic LLMs like Meta's LLaMA series [7]. However, proprietary LLMs raise privacy concerns, as they require sharing prompts and related data with the LLM owners. Additionally, they offer limited maneuverability and adaptability, as users do not have access to the model weights. Furthermore, despite their adaptability, general-purpose open-source LLMs lack specialization in telecommunications, as they encompass knowledge from diverse domains such as medicine, history, and law. This lack of specialization hinders their performance; in fact, it is well established that LLM applications in specific domains, from prompting to task-specific fine-tuning, perform better when using domain-specific LLMs rather than general ones [11].\nGiven the above, in a multitude of domains, efforts have been made to create specialized open-source LLMs to push the performance limits of LLMs applications in those fields, such as medicine [18], law [5], and finance [30]. However, to date, there are no domain-specific open-source models"}, {"title": "2 Domain Adaptation", "content": "LLMs are trained using a next-token prediction objective on corpora that encompass a wide variety of domains, such as medicine, history, and more. In this context, a token refers to a subword that represents the most basic unit in the text. This training process allows the LLM to become accustomed to the distribution of tokens in these datasets and develop an understanding of the statistical relationships between them. Consequently, the LLM develops a well-rounded understanding of the multiple domains it has been trained on.\nAs demonstrated in [36], if one intends to use an LLM solely for a specific domain, specializing the LLM in that domain is advantageous for any subsequent in-context learning or task-specific fine-tuning. This is because the probability distribution of tokens in the domain of interest can differ significantly from those in other domains internalized by the LLM. By adapting to this distribution, the LLM's knowledge becomes more targeted to the domain, leading to transfer learning for subsequent usage stages.\nThe primary approach for domain adaptation is continual pretraining. This method involves further training the LLM on domain-specific corpora, allowing it to adapt its parameters to the field of interest [11]. Continual pretraining has become the standard for LLM domain adaptation, as evidenced by its application in various fields such as medicine [18], law [5], and finance [30]. In the following, we will explore the details of continual pretraining, with a particular emphasis on its application in the telecommunications domain."}, {"title": "2.1 Background", "content": "LLMs are trained using a next-token prediction objective on corpora that encompass a wide variety of domains, such as medicine, history, and more. In this context, a token refers to a subword that represents the most basic unit in the text. This training process allows the LLM to become accustomed to the distribution of tokens in these datasets and develop an understanding of the statistical relationships between them. Consequently, the LLM develops a well-rounded understanding of the multiple domains it has been trained on.\nAs demonstrated in [36], if one intends to use an LLM solely for a specific domain, specializing the LLM in that domain is advantageous for any subsequent in-context learning or task-specific fine-tuning. This is because the probability distribution of tokens in the domain of interest can differ significantly from those in other domains internalized by the LLM. By adapting to this distribution, the LLM's knowledge becomes more targeted to the domain, leading to transfer learning for subsequent usage stages.\nThe primary approach for domain adaptation is continual pretraining. This method involves further training the LLM on domain-specific corpora, allowing it to adapt its parameters to the field of interest [11]. Continual pretraining has become the standard for LLM domain adaptation, as evidenced by its application in various fields such as medicine [18], law [5], and finance [30]. In the following, we will explore the details of continual pretraining, with a particular emphasis on its application in the telecommunications domain."}, {"title": "2.2 Continual Pretraining", "content": "Given the scale of the training datasets of current LLMs (e.g., 15T tokens for LLama-3 [7]), it is fair to assume that the language models available in the literature have been trained on all available public-source data. Therefore, these models have likely already encountered most of the publicly available telecommunications-related data. Nevertheless, the goal of continual pretraining is to adapt the LLM's knowledge to this specific domain by re-exposing the model to these data. Specifically, consider an LLM trained on a corpus of tokens D drawn from a distribution \u03bc. This LLM is trained over the entire corpus D to minimize the cross-entropy loss\nmin Ex~\u00b5 [LCE (x; W)] = \u2212Ex~\u00b5 \u2211 log P(xt | X1:t-1; W) \nWeRn\nt=1\nwhere T is the context length, x1:t\u22121 is equal to (x1, ..., Xt\u22121), W is the LLM's parameters, and n is the total number of parameters. On the other hand, let us consider a set of telecom-related data DTele-Data. The samples of tokens x = (x1, ..., XT) drawn from DTele-Data originate from a different distribution, denoted by \u03c3. Continual pretraining involves initializing the LLM to its existing parameters, denoted by Wo, and further reducing the cross-entropy loss, this time using samples of tokens x drawn from DTele-Data:\nmin - Ex- log P(xt | X1:t-1; W, Wo)\nWeRn\nt=1\nBy doing so, the model parameters shift closer to the new distribution, allowing the LLM to be better calibrated to telecommunications-specific data."}, {"title": "2.3 Catastrophic Forgetting", "content": "Ideally, one would hope that such domain adaptation comes at no penalty. However, generally, a penalty is incurred. By shifting the focus from the general token distribution \u03bc of the training dataset D to a new distribution \u03c3, the model risks forgetting previously acquired knowledge. This risk depends on the size of the model, the size of the domain-specific dataset, and how different \u03c3 is from \u00b5 [21]. Addressing this issue is crucial; otherwise, one may end up with a telecom-knowledge proficient LLM but lose reasoning capabilities, coding abilities, and general English language understanding in the process. Later in this paper, specifically in Section 7, we will detail how we addressed this issue in our continual pretraining framework. With this in mind, the next step in our framework involves curating a comprehensive telecommunications corpus, which we will refer to as Tele-Data."}, {"title": "3 Tele-Data", "content": "Our collection of telecommunications material comprises four main sources: (1) scientific papers from arXiv, (2) 3GPP standards, (3) Wikipedia articles related to telecommunications, and (4) telecommunications-related websites extracted"}, {"title": "3.1 Arxiv", "content": "Curation. One of the largest sources of open-access research on telecommunications consists of preprints submitted by authors to the arXiv repository. As of March 2024, the combined arXiv snapshot for both computer science and electrical engineering categories contains approximately 610k papers. However, given the overlap between these categories and the inclusion of topics beyond telecommunications, targeted filtering is necessary to identify the relevant material. To achieve this, we use a language model-based filtering approach. Specifically, we leverage the Mixtral 8x-7B-Instruct\u00b2 model, providing it with the abstract of each paper to determine if it is related to the telecommunications and networking domain. The model is prompted\u00b3 to provide a Yes or No answer regarding whether the paper is relevant. We then utilize the logits of the 'Yes' and 'No' tokens to classify whether or not a paper is related to telecommunications.\nTo assess the quality of our filtering process, we randomly sampled 500 arXiv papers and manually annotated them to determine their relevance to the telecommunications and networking domain. To ensure a balance between relevant and irrelevant examples, we drew an equal number of samples from both the computer science and electrical engineering categories given that the latter is more likely to contain telecommunications material, while the former is more likely to include unrelated papers. We then evaluated both the precision and recall of the filtering process. As shown in Table 2, the process demonstrates high recall and moderate precision. High recall is particularly important, as it indicates that our"}, {"title": "Cleaning", "content": "Following curation, we implement a comprehensive cleaning process for the papers, which includes: (1) removing comments, (2) flattening LaTeX sources, (3) substituting user-defined macros with standard LaTeX commands, (4) eliminating LaTeX native commands, and (5) standardizing citation formats and removing markup changes. We also remove figures and tables to focus on inline text and equations. Details on this procedure are provided in Appendix A. To assess the effectiveness of our cleaning process, we randomly selected 500 arXiv papers and derived a cross-entropy-based approach to evaluate cleanliness as detailed below. Let X = (x1, x2, . . ., xN) and Y = (y\u00b9, y\u00b2, . . ., yM) denote the non-overlapping chunks of T tokens from the raw and cleaned datasets, respectively, where N and M are the total number of chunks. Next, let us define the cross-entropy loss for any token index t \u2208 {1, ..., T} in any chunk of either the raw or cleaned data using GPT-2 [27] as follows\nL(x|xi:t-1) = \u2212log P(x|xi:t\u22121; WGPT-2), i = 1, . . ., N,\nCE\nLclean (y|yi:t-1) = \u2212 log P(y|yi:t\u22121; WGPT-2), i = 1, . . ., M.\nGPT-2 was chosen because it was primarily trained on web content, excluding arXiv scientific papers. Therefore, the cross-entropy here serves as a proxy for how closely the text resembles generic online content rather than the more complex arXiv papers, thus providing an indication of the dataset's cleanliness. With this in mind, we set T to 1024, in line with GPT-2's training data [27], and define the total set"}, {"title": "3.2 Standards", "content": "Curation. Standards play a pivotal role in telecommunications as they ensure interoperability among technologies from various vendors. These standards are established and maintained by recognized bodies such as 3GPP, IEEE, and ITU. Due to their open-source nature, we focus on incorporating 3GPP documents into our dataset. To do so, using the 3GPP FTP portal\u2074, we downloaded the latest specifications for each standard of each series, resulting in a dataset of approximately 2.8k documents.\nCleaning. Following the curation process, we clean and process the standards files through several steps. We begin by removing non-essential sections such as related works and appendices, and eliminating figures and tables to focus on inline text and equations, similar to our approach with arXiv papers. A key caveat is that equations in .doc files are formatted in XML, unlike the LaTeX format used in arXiv papers. To address this, we first convert all .doc files to .docx format and then utilize docx2tex5 to transform the standards into LaTeX format. This standardization improves the training process by ensuring consistency of equations across all document types. Finally, we apply the same cleaning pipeline used for arXiv papers to the converted standards LaTex files to ensure a uniform level of cleanliness and coherence across our entire dataset."}, {"title": "3.3 Wikipedia", "content": "Another source of telecommunications material is the Wikipedia corpus, specifically articles related to the telecommunications domain and its associated technical content. To curate this dataset, we utilize the English subset of the Wikipedia dataset\u2076, which contains 6.4 million samples. Given the size of this dataset, applying a pure LLM-based classification would be computationally expensive. Instead, we employ a two-step process:\n(1) Keyword Filtering: We define a set of 100 telecom-related keywords, including terms such as telecommunications, base station, Wi-Fi, and 5G. Articles containing any of these keywords are flagged for the next step. This process significantly reduces the number of articles from 6.4M to approximately 70k.\n(2) LLM-based Content Evaluation: In this second step, we apply an LLM-based filtering process to the flagged articles. Particularly, the first 10,000 characters of each article are provided to the Mixtral 8x-7B-Instruct model, and the LLM is prompted to provide a Yes or No answer regarding the article's relevance and the presence of technical content related to telecommunications. The reason behind this is to exclude articles that discuss non-technical aspects, such as a telecom operator's history.\nThrough this two-step process, we curate a dataset of 19.5k technically relevant telecommunications articles from Wikipedia. Similarly, we evaluated our LLM-based filtering by sampling 500 flagged articles and manually annotating them. We then assessed the precision and recall of our filtering process. The results in Table 2 showcase the high recall of the process, showcasing its ability to identify relevant technical content in telecommunications. The moderate precision, on the other hand, stems from the challenge of determining the appropriate level of technicality required for an article, resulting in false positives within the dataset."}, {"title": "3.4 Websites", "content": "The last source of telecommunications material we consider is the Common Crawl dataset, which comprises web archives from across the internet. To avoid issues with duplicates, non-English content, and potential profanity found in raw dumps, we utilized the refined web dataset [26]. This curated version of Common Crawl contains approximately 1 billion rows across 2.8 terabytes of data. To further eliminate duplicates, we filtered out Wikipedia articles from the dataset. Next, to extract telecommunications-related content from this refined dataset, we employed the same two-step process used for Wikipedia articles. Additionally, we incorporated content from well-known telecommunications blogs, such as ShareTechNote, to enhance the dataset's relevance. The resulting collection consists of content from 740k website links, providing a comprehensive representation of telecommunications information available on the web."}, {"title": "3.5 Dataset Format", "content": "Tele-Data is structured as a JSONL (JSON Lines) file, where each line represents a JSON object with five distinct fields:\n\u2022 ID: A unique identifier for each data entry, combining the data category and a number. For example, 'wiki_132' refers to the 132th item of the Wikipedia data points.\n\u2022 Category: A string indicating the source of the data: wiki, standard, arxiv, or web.\n\u2022 Content: A string containing the main text of the material.\n\u2022 Metadata: A JSON object containing various information relevant to each specific element, with the structure varying depending on the category. For example, for standards, the JSON object includes the 3GPP series number, release, and standard file name, while for arXiv papers, it contains the arXiv ID, title, and abstract."}, {"title": "4 Evaluation Dataset", "content": "After preparing Tele-Data, the next step involves creating an evaluation dataset to test the resulting domain-adapted models' telecommunications knowledge. Currently, only one such dataset exists: TeleQnA [22]. TeleQnA is a multiple-choice question (MCQ) dataset drawn from standards and research papers, generated with human involvement. Although an MCQ dataset simplifies evaluation in terms of accuracy, it remains limiting because LLMs are not robust MCQ selectors due to their inherent \u2018selection bias\u2019, a bias prevalent in virtually all LLMs [9, 33].\nGiven the above, to examine the telecommunications proficiency of LLMs, we take another approach by creating a dataset of open-ended telecommunications questions. Open-ended questions assess a model's ability to elaborate on its own to answer questions about specific telecommunications concepts. Given the large number of concepts to be covered and the specialized nature of telecommunications knowledge, large-scale open-ended questions are needed, making a purely human-based approach infeasible.\nTo overcome this challenge, we adopt an LLM-based approach to create our evaluation dataset, which we refer to"}, {"title": "5 Evaluation Metrics", "content": "One of the key challenges after continual pretraining is effectively evaluating the resulting domain-adapted model. As seen in the previous section, Tele-Eval consists of open-ended question-answer pairs. Evaluating open-ended responses is more complex compared to MCQ datasets. For example, traditional evaluation metrics such as ROUGE [20] and BLEU [25], commonly used for summarization and translation tasks, measure lexical overlap between the model's output and the reference answers. However, they fail to capture the semantic similarity and correctness of responses when the model uses alternative wording or lexicon to convey the same meaning as the reference answers. This challenge is especially pronounced when evaluating equations generated by the models, as is often required in the telecommunications domain, since these metrics struggle to capture such nuances. To address this, we adopted three evaluation metrics: Answer perplexity, SemScore [1], and LLM-Eval [34]. In Section 7, we will showcase the pros and cons of each evaluation metric and conclude that LLM-Eval is the most robust comparative tool for the telecommunications domain. Below, we provide details on each of these metrics."}, {"title": "5.1 Answer Perplexity", "content": "We define answer perplexity as the perplexity of the model with respect to the ground truth answer, conditioned on the question. Specifically, let us consider N samples of Tele-Eval, where x\u00b2 = (x1,...,x) represents a concatenation of both the question and the ground truth answer of the i-th pair. Assume that at index ki, the ground truth answer begins. With that in mind, we define the answer perplexity as:\nAns-PPL(x) = exp (\u03a3\u03a3 log P(x)} | x1:1-1;W));\nwhere Ti is the number of tokens in the i-th pair, and W represents the model weights. This metric can be seen as how surprised the model is by the answer, given the question. Intuitively, a model that is well-versed in telecommunications should be less surprised by the answer (hence, have a lower perplexity) compared to a non-specialized model."}, {"title": "5.2 SemScore", "content": "Another metric we use to evaluate the correctness of a model's output relative to the ground truth answer is semantic similarity. This is achieved by leveraging sentence transformer models [28], such as the all-mpnet-base model7 from the Sentence Transformers family. These encoder-only BERT models are trained using contrastive loss to encode pairs of sequences into a high-dimensional space, such that the cosine similarity between embeddings of similar pairs is high (closer to 1), while it is lower for dissimilar ones (closer to -1). With this in mind, we define the SemScore [1] as\nSemScore(x, y) = cos(embed(x), embed(y)),\nwhere embed(\u00b7) refers to the BERT embedding model, and x and y refer to the ground-truth answer and the model's output, respectively. Thus, this metric allows us to judge how closely the model's output aligns with the correct answer."}, {"title": "5.3 LLM-Eval", "content": "The last metric in our framework involves using an LLM as a judge to assess the correctness of a model's output compared to the ground truth answer [34]. Given the extensive knowledge LLMs have acquired through their training, we can leverage them as assistants to evaluate the quality and accuracy of model outputs. Previous studies have demonstrated that LLM-based judges can effectively match both controlled and crowdsourced human preferences in various evaluation tasks [34]. With this approach, when evaluating any model outputs for questions in Tele-Eval, we prompt Mixtral 8x-7B-Instruct to compare the model's output to the ground truth and provide a Yes or No answer regarding its correctness. We define the LLM-Eval metric as the average score of these boolean responses, such that\nLLM-Eval =\n\u2211 1 {LLM judges y as correct},\nwhere y' is the model's answer to question i of Tele-Eval, N is the total number of evaluated outputs, and 1{\u00b7} is the indicator function."}, {"title": "6 Initial Experiments", "content": "With both the training and evaluation datasets prepared, the next step is to train our models. However, before proceeding, several key questions regarding training techniques and parameters need to be addressed to lay the foundation for the training procedure. While general guidelines on training parameters, such as learning rate and batch size, can be drawn from previous research [5, 18, 30], the characteristics of telecommunications knowledge-such as the prevalence of equations-introduce specific challenges. Key questions"}, {"title": "6.1 Training Settings", "content": "For these experiments, we consider two models: Gemma-2B and LLaMA-3-8B. Throughout these studies, we use 20% of the Tele-Data dataset as the training data, with 10% of this subset serving as the validation set. To solve eq. (2), we set the batch size to 4M tokens (with a sequence length of 8192 tokens) and use the AdamW optimizer with a weight decay of 0.1, keeping the default Hugging Face \u03b2 parameters unchanged. The maximum gradient norm is set to 1. The learning rate is decreased according to a cosine learning rate schedule, down to 10% of the maximum learning rate, and a linear warmup of 10% of an epoch is applied [10]. The maximum learning rate is set to 1e-5 for LLaMA and 2e-5 for Gemma. These values are aligned with the pretraining stages of these LLMs [7, 29]. To improve efficiency, we employ mixed-precision training alongside sample packing and block attention techniques to avoid cross-contamination [17]."}, {"title": "6.2 PEFT vs. FFT", "content": "PEFT techniques aim to reduce computational costs and memory requirements during domain adaptation by training only a minimal number of parameters, in contrast to FFT methods that update all the model's parameters. Among these techniques, LoRa [14] stands out as a prominent PEFT method. It adds small, trainable low-rank matrices to the existing weights of the pretrained model. To investigate whether PEFT methods like LoRa are sufficient for adapting language models to the telecommunications domain, we conducted a comparative study between LoRa and FFT using both Gemma-2B and LLAMA-3-8B models. For our LoRa implementations, we fixed the LoRa rank to r=64, set the Lora alpha to 32, and used a LoRa dropout rate of 0.1.\nThe results, reported in Fig. 4, demonstrate that for smaller models, LoRa can initially inject telecom knowledge but quickly saturates due to its limited capacity. In contrast, for the LLaMA-3-8B model, which is more knowledgeable about telecommunications, the gradient norm of the LoRa method remained extremely low. This hindered parameter updates, causing the training loss to barely change and limiting LoRa's ability to inject additional knowledge. These findings indicate that, despite its computational efficiency, LoRa may not be sufficient to adapt the model to the telecommunications field. Consequently, in the next sections, we will rely on full parameter fine-tuning for all our training."}, {"title": "6.3 One or Multiple Epochs?", "content": "A crucial question when adapting an LLM to a target domain, in our case being telecommunications, is determining the most effective duration for model training. Is a single epoch enough to adapt the model, or do multiple exposures to the data with shuffling at each epoch yield better results? To address this, we conducted experiments using two different model scales: Gemma-2B and LLama-3-8B. As we will demonstrate, this distinction arises from the distinct behaviors of these models due to their different sizes. The results are reported in Fig. 5 and discussed below."}, {"title": "7 Tele-LLMS", "content": "Having completed the initial experiments, we proceed with the full-scale training of the LLMs to create the telecom-adapted series of models, starting with the following three models: Tinyllama-1.1B-Tele, Gemma-2B-Tele, and LLaMA-3-8B-Tele. Our selection of these base models was guided by a combination of factors, including their performance on the Hugging Face Open LLM leaderboard and a consideration of their licenses. We also accounted for the diversity of the releasing companies to mitigate potential risks associated with changes in these licenses.\nTraining Settings. We maintain the settings described in Section 6.1, with Tinyllama-1.1B following the same approach as Gemma-2B. One caveat is the use of a sequence length of 2048 for Tinyllama-1.1B, aligning with its initial pre-training specifications [8]. For the training dataset across the entire series, we utilize the entire Tele-Data dataset for two"}, {"title": "8 Further Exploration", "content": "In the final phase of our work, we investigate various adaptation strategies and specific dynamics that emerge when tailoring LLMs to the telecommunications domain. We particularly focus on two key aspects: the potential for expertise division in the adaptation process, and the unique adaptation dynamics that arise based on each model's characteristics and training data. We also adapt our models to follow instructions, creating chatbot-like models that users can interact with. We show that these adapted models demonstrate"}, {"title": "8.1 Division of Expertise", "content": "When adapting an LLM to the telecommunications domain, a key question arises: should we adapt a single LLM to cover the entire domain, or should we adapt multiple LLMs, each specialized in a specific aspect of telecommunications? For example, one LLM could focus on scholarly material while another concentrates on standards. To explore this, we compare two versions of the Gemma-2B model: one trained exclusively on the standards portion of Tele-Data, referred to as Gemma-2B-Standards, and Gemma-2B-Tele. The training parameters align with those previously reported for Gemma-2B-Tele. Evaluation is based on both the entirety of Tele-Eval and questions related to standards using the LLM-eval metric, along with general knowledge evaluation datasets to assess the retention of the model's capabilities. The results are reported in Table 4.\nAs shown, Gemma-2B-Tele outperforms Gemma-2B-Standards on the standards-related questions while also demonstrating stronger general knowledge capabilities. Additionally, Gemma-2B-Standards performs worse than the base model Gemma-2B on the overall Tele-Eval dataset. The reason behind this is that the Gemma-2B-Standards model, having been trained solely on standards, becomes more attuned to that specific type of content. Standards tend to be highly technical with a unique token distribution that differs from the broader telecommunications knowledge found in scholarly and Wikipedia articles. This narrow focus makes the model less capable of handling scholarly material compared to the base model, leading to underperformance on Tele-Eval. At the same time, this narrow focus also negatively impacts the LLM's general abilities. All in all, this highlights that the most effective strategy is to adapt the LLM to the entire telecommunications dataset, thereby benefiting from the transfer learning that occurs across these diverse materials rather than narrowly focusing on a single aspect."}, {"title": "8.2 Pretraining Data Impact", "content": "Perhaps the most impactful element that shapes an LLM's behavior is its pretraining data. This data is generally not publicly available, so one can only infer its type through interactions with the model. Understanding this behavior"}, {"title": "8.3 Instructions Fine-tuning", "content": "Although base models are essential, as they contain the raw LLM's knowledge and are best suited for fine-tuning for specific telecommunications applications, it is common for users to interact with these models as chatbots. Therefore, in this section, we proceed to fine-tune our telecom-adapted models to follow instructions through instructions fine-tuning.\nTraining Settings. We maintain the training settings from Section 7 with minor adjustments. Specifically, we decrease the batch size to 128k tokens, set the context length to 2048, and limit the number of epochs to 1. For this stage, we use two datasets: Alpaca and Open-Instruct\u00b9\u00b9. The combined datasets provide approximately 200k samples for training.\nResults. As seen in Fig. 8, the training loss consistently decreases as the model becomes more adept at following instructions. Next, in Table 6, we compare the Tele-Instruct versions of our models to their general instruct counterparts."}, {"title": "9 Conclusions", "content": "In this paper, we addressed the challenge of adapting LLMs for specialized use in telecommunications. In our endeavor, we created and released Tele-Data and Tele-Eval, a comprehensive telecommunications training and evaluation datasets. Through extensive experimentation, we identified the most suitable training strategies for adapting LLMs to this domain. The culmination of our work is Tele-LLMs, a series of open-source models ranging from 1B to 8B parameters, specifically designed for the telecommunications domain. These models outperform their general-purpose counterparts on Tele-Eval while retaining their broader capabilities.\nBeyond this culmination, our work also investigated various adaptation strategies, such as the division of expertise, and explored the dynamics that arise during the adaptation process depending on the models involved. As a future direction, our work will aim to leverage Tele-LLMs and augment them with multi-modal capabilities to understand and reason about wireless measurements and signals."}, {"title": "A Manuscripts Cleaning Procedure", "content": "Given the importance of ensuring that the arXiv papers are clean for training, we employ a rigorous cleaning process on these sources. Our process begins with the removal of all comments written in the LaTeX files. For this, we leverage Google's arXiv LaTeX Cleaner12.\nNext, because LaTeX sources can include multiple LaTeX files, we start by unifying the LaTeX commands used for importing these files. Particularly, we ensure that imports use the \\input{\u00b7} command. By creating a directed graph that maps these relationships, we identify the main LaTeX file of the paper. We then use the Latexpand Perl script13 to flatten the document, ensuring the main file contains all material.\nIn the subsequent step, we address the author's custom commands by 'de-macoring' them. To do so, we unify all custom commands-from \\def{\u00b7} and \\DeclareMathOperator{\u00b7} to \\newcommand{\u00b7}-before using the Python library de-macro (https://ctan.org/pkg/de-macro) to replace these macros with their native LaTeX equivalents. Afterwards, we target the removal of figures and tables, as our focus is on the in-line"}, {"title": "B Samples of Tele-Data", "content": "We provide below an example of each category of Tele-Data. The [...] symbol is inserted below to reduce the size of the strings.\nID: arxiv_14326\nCategory: arxiv\nContent: Flexible-Position MIMO for Wireless Communications: Fundamentals, Challenges, and Future Directions\\n\\n Abstract\\n \\n_The_flexible-position multiple-input multiple-output (FLP-MIMO), such as fluid antennas and movable antennas, is a promising technology for future wireless communications [...]\nMetadata:\nArxiv_id: 2308.14578\nTitle: Flexible-Position MIMO for Wireless Communications: Fundamentals, Challenges, and Future Directions\nAbstract: The flexible-position multiple-input multiple-output (FLP-MIMO), such as [...]\nID: standard_2413\nCategory: standards\nContent: 3rd Generation Partnership Project; \\n Technical Specification Group Core Network and Terminals;\\nInterworking between the Public Land Mobile Network (PLMN)\\n supporting packet based services with\\n Wireless Local Area Network (WLAN) Access and\\n Packet Data Networks (PDN)\\n (Release 12)\\n Foreword\\n This Technical Specification (TS) has been produced [...]\nMetadata:\nSeries: 29\nRelease: 12\nFile_name: 29161-c00\nID: wiki_5438\nCategory: wiki\nContent:A backbone or core network is a part of a computer"}, {"title": "C LLM Prompts", "content": "The following prompts were utilized throughout our framework to develop and evaluate various areas. In summary", "tasks": "n(1) Prompt 1: Used to find arXiv papers related to the telecommunications and networking domains.\n(2) Prompt 2: Used to find websites and Wikipedia pages related to the telecommunications and networking domains.\n(3) Prompt 3: Leveraged to generate the QnAs that form the initial Tele-eval dataset.\n(4) Prompt 4: Leveraged to filter out locally relevant QnAs", "5": "Used to instruct the base model to complete the answer based on the provided question.\n(6) Prompt 6: Utilized as a prompt for LLM-Eval.\nPrompt 1: arXiv filtering\nGiven the following scientific paper abstract: {Abstract}, Answer by Yes or"}]}