{"title": "LAWGPT: KNOWLEDGE-GUIDED DATA GENERATION\nAND ITS APPLICATION TO LEGAL LLM", "authors": ["Zhi Zhou", "Kun-Yang Yu", "Shi-Yu Tian", "Xiao-Wen Yang", "Jiang-Xin Shi", "Peng-Xiao Song", "Yi-Xuan Jin", "Lan-Zhe Guo", "Yu-Feng Li"], "abstract": "Large language models (LLMs), both proprietary and open-source, have demonstrated remarkable capabilities across various natural language processing tasks. However, they face significant limitations in legal reasoning tasks. Proprietary models introduce data privacy risks and high inference costs, while open-source models underperform due to insufficient legal domain training data. To address these limitations, we study data generation for legal reasoning to improve the legal reasoning performance of open-source LLMs with the help of proprietary LLMs. This is challenging due to the lack of legal knowledge in proprietary LLMs and the difficulty in verifying the generated data. We propose KGDG, a knowledge-guided data generation framework for legal reasoning. Our framework enables leveraging legal knowledge to enhance generation diversity and introduces a refinement and verification process to ensure the quality of generated data. Moreover, we expand the generated dataset to further enhance the LLM reasoning capabilities. Using KGDG, we create a synthetic legal reasoning dataset containing 50K high-quality examples. Our trained model LAWGPT outperforms existing legal-specific LLMs and achieves performance comparable to proprietary LLMs, demonstrating the effectiveness of KGDG and LAWGPT. Our code and resources is publicly available at https://github.com/LAMDASZ-ML/\nKnowledge-Guide-Data-Generation.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) (OpenAI, 2023b; Touvron et al., 2023) have achieved remarkable success in various natural language processing tasks, including natural language understanding (Dong et al., 2019), reasoning (Huang & Chang, 2023), and generation (Yu et al., 2022). Both proprietary and open-source LLMs exhibit strong generalization capabilities, enabling their application in diverse downstream scenarios, such as medicine (Thirunavukarasu et al., 2023), finance (Yang et al., 2023), education (Gan et al., 2023). Recent studies (Fei et al., 2023; Nguyen, 2023) have demonstrated the preliminary effectiveness of existing general LLMs in legal reasoning tasks, including legal documents retrieval (Chen et al., 2013), legal judgment prediction (Luo et al., 2017), and legal question answering (Zhong et al., 2020).\nDespite their preliminary success in legal reasoning applications, LLMs face significant practical limitations. Proprietary LLMs such as GPT-4 (OpenAI, 2023b) and GPT-3.5 Turbo (OpenAI, 2023a), as well as extremely large open-source models like DeepSeek V3 (DeepSeek-AI et al., 2024), require API access, introducing substantial data privacy risks and high inference costs. Open-source LLMs like Qwen (Yang et al., 2024) and ChatGLM (Du et al., 2022) show suboptimal performance due to training with insufficient legal data. These limitations create an opportunity to leverage proprietary LLMs for generating legal reasoning data to build open-source legal LLMs.\nPrevious studies have developed various data generation methods using proprietary LLMs for downstream reasoning tasks, which have been effective for mathematical reasoning (Luo et al., 2025)."}, {"title": "METHODOLOGY", "content": "In this section, we introduce KGDG, an LLM-based data generation framework, building data to improve the legal reasoning performance of open-source LLMs. However, the following two challenges make it difficult for exising LLMs to generate data for legal reasoning:\n(a) LLMs for data generationlack legal knowledge, which limits the diversity of synthetic data.\n(b) Legal synthetic data is difficult to formalize and verify, making it challenging to detect and eliminate hallucinations in the generation process.\nWe design Knowledge-Guided Generation (KGGEN) to address the first challenge by introducing legal documents as knowledge base. Then, Knowledge-Guided Fixer (KGFIX) and Data Verifier (DAVER) addressing the second challenge by refining correctable errors and removing uncorrectable data. To further improve model reasoning performance, we implement a Mixture Training (MITRA) to teach open-source LLMs to reason step-by-step while keeping the capability to directly generate answers efficiently. Overall illustration is shown in Figure 1 and each module is detailed below."}, {"title": "KNOWLEDGE-GUIDED GENERATION (KGGEN)", "content": "Existing studies (Li et al., 2024b) demonstrate that data generation methods based on LLMs have strong potential for building high-quality training data. However, for tasks that require specific domain knowledge, such as legal reasoning, LLMs may fail to build high-quality data due to their lack of domain knowledge, leading to insufficient diversity in synthetic data. To address this challenge, we design KGGEN by introducing a knowledge base K to compensate for the lack of legal knowledge inherent in LLMs. This enables us to generate diverse synthetic data by leveraging legal knowledge sampling on the knowledge base K. Specifically, for legal reasoning task, KGGEN consists of two components: Knowledge-Aware Sampler and Knowledge-Guided Writer. The Knowledge-Aware Sampler employs sampling strategies to enhance the diversity of synthetic data, while the Knowledge-Guided Writer leverages LLMs to extract core information and generate question-answer pairs.\nThe Knowledge-Aware Sampler takes two inputs: a knowledge base K containing legal documents and a seed problem set & providing format examples for legal reasoning tasks. The sampling process is controlled by a strategy \u03c0(k, e DGen) that samples from K and E conditioned on the current generated dataset DGen, where k \u2208 K represents a sampled legal document and e \u2208 E represents a sampled seed problem. We implement \u03c0as a two-step sampling strategy: (1) LLM selects specific types of legal knowledge from K based on the sampled example problem e to ensure consistency between the example and knowledge; (2) Monte Carlo sampling ensures diverse and balanced synthetic data across all problem types and their corresponding legal knowledge domains.\nThe Knowledge-Guided Writer LLMw takes the sampled legal document k and example problem e as inputs and generates the unverified draft data x containing question q, answer \u00e3, reasoning path p, and references \u0159:\nx = (q, \u00e3, \u0159, p) = LLMw (k, e)"}, {"title": "KNOWLEDGE-GUIDE FIXER (KGFIX) AND DATA VERIFIER (DAVER)", "content": "The unverified draft data x = (\u1fb7, \u00e3, \u0159, p) contains potential errors in all components due to the hallucination problems of LLMs. To address this issue, we introduce KGFIX to fix correctable errors in the reasoning path p and references \u0159, and DAVER to filter out uncorrectable data.\nKGFIX consists of two components: Reference Modifier and Reasoning Corrector. The Reference Modifier validates and corrects legal references using LLMs or the knowledge base, generating a corrected reference r = Fixer\u2122 (r). The Reasoning Corrector examines the reasoning path for logical and computational errors using LLMs or tools, producing a corrected path p = Fixerc(p).\nWhile KGFIX ensures the correctness of reference \u00ee and reasoning path \u00ee\u00ee, it cannot guarantee their relevance to the generated question-answer pair (\u1fb7, \u00e3). Therefore, we implement DAVER to validate whether the answer a can be derived from the question \u1fb7 using the corrected references \u00ee and reasoning path p. If the validation succeeds, we mark the question-answer pair as valid (denoted as \u011d and a). The verified data \u00ee = (\u011d, \u00e2, r, p) is then added to the synthetic dataset DGen. This process continues until DGen meets the required data volume."}, {"title": "MIXTURE TRAINING (MITRA)", "content": "To further enhance the reasoning performance of the trained LLM, we implement MITRA to generate two types of training data using DGen: (1) standard question-answer pairs and (2) question-answer pairs with explicit reasoning paths. The standard pairs enable efficient direct responses, while the pairs with reasoning paths teach the model step-by-step reasoning.\nSpecifically, we design two prompt templates: T\u300f(\u00e2, \u00e2) for standard pairs and T(\u00f4q, \u00e2, r, \u00f4p) for pairs with reasoning paths. Here, Ts generates training instances using only questions and answers, while Tr incorporates additional reasoning steps seperated by a thinking tag in the responses. Example problems for both types of training data are provided in Appendix B. The final training dataset is constructed by combining both types:\nDTrain = {Ts(q, \u00e2)} \u222a {Tr(\u00f4q, \u00e2, r, p)}, (q, \u00e2, r, p) ~ DGen"}, {"title": "EXPERIMENTS", "content": "In this section, we compare the performance of LAWGPT against base models, legal-specific LLMs, and general LLMs to demonstrate the effectiveness of KGDG framework and the trained LAWGPT."}, {"title": "EXPERIMENTAL SETTINGS", "content": "Evaluation Protocol. To evaluate the legal reasoning performance of each model, we adopt four legal reasoning tasks: Scene-based Article Prediction (Task #1) (Liu et al., 2023), Prison Term Prediction without Article (Task #2), Prison Term Prediction with Article (Task #3) (Xiao et al., 2018), and Criminal Damages Calculation (Task #4) 1. Task #1 is evaluated using the ROUGE-L score to compare the legal article prediction with the ground truth. Tasks #2 and #3 are evaluated using Normalized log-distance to compare the predicted prison term. Task #4 is evaluated using accuracy to determine whether the predicted damages match the ground truth. The implementation of our evluation is based on the LawBench (Fei et al., 2023).\nComparison Models. We compare two types of models: (1) General proprietary LLMs, including GPT-4 (OpenAI, 2023b), GPT-3.5 Turbo (OpenAI, 2023a), and DeepSeek V3 (DeepSeek-AI et al., 2024); (2) Legal-specific LLMs, including Lexilaw (Li et al., 2024a), LaywerLLaMA (Huang et al., 2023), HanFei (He et al., 2023), ChatLaw (Cui et al., 2023), FuziMingcha (Deng et al., 2023), and WisdomInterrogatory (Wu et al., 2024).\nDataset Construction. We implement the KGDG framework using the DeepSeek V3 model (DeepSeek-AI et al., 2024), based on a legal knowledge base and a constructed seed problem set. Specifically, to construct the legal knowledge base, we manually collect 186,197 high-quality criminal legal documents and 152,452 civil legal documents. Each document includes judgment facts, reasons, results, and relevant laws. This knowledge base supports the generation of diverse and synthetic problems for legal reasoning, as well as the verification and correction of generated reasoning paths and answers. For seed problems, we manually construct ten problems for each task as examples to guide the KGDG to generate legal problems in the desired format. These seed problems are solely for demonstration and are not used for training. KGDG generates 25K legal problems with verified answers. MITRA expands each problem into two: one with direct answers and one with answers accompanied by detailed reasoning steps, resulting in a total of 50K training examples. The detailed implementation of KGDG and generation process is provided in Appendix A.\nModel Training. We adopts the LLaMA-Factory (Zheng et al., 2024) to fine-tune the series of Qwen-2.5 models (Yang et al., 2024), including 0.5B, 1.5B, and 3B versions. The training epochs are set to 3 and learning rate is set to le-5 with a cosine learning rate scheduler. Our training process is conducted on a Linux server with 4 NVIDIA A800 GPUs."}, {"title": "EMPIRICAL RESULTS", "content": "In this section, we conduct experiments to compare the performance of LAWGPT with base models, general LLMs, and legal-specific LLMs to demonstrate the effectiveness of our KGDG framework as well as the trained legal LLM LAWGPT.\nEffectiveness of KGDG. To evaluate the effectiveness of our proposed KGDG data generation framework, we fine-tune Qwen-2.5 models of different scales using our generated 50K data. The results in Table 1 demonstrate that out fine-tuned model consistently outperforms the base models across all scales. This indicates that KGDG generates high-quality legal data that effectively improves the reasoning capabilities of base models regardless of their size. Moreover, we analyze the scalability of KGDG in Figure 2. The experimental results demonstrate that the performance of trained LLMs consistently improves across all tasks as the volume of generated training data increases, indicating the strong potential of KGDG for developing more capable legal LLMs.\nEffectiveness of LAWGPT. We evaluate LAWGPT against both general and legal-specific LLMs. For general LLMs, we include two proprietary models (GPT-4 and GPT-3.5 Turbo) and one large-scale open-source model (DeepSeek V3). We also compare against seven legal-specific LLMs of various sizes. As shown in Table 2, LAWGPT outperforms all existing legal-specific LLMs despite its smaller scale. Furthermore, LAWGPT surpasses GPT-4 and GPT-3.5 Turbo while achieving performance comparable to DeepSeek V3 on multiple tasks. These results demonstrate both the value of specialized legal LLMs and the effectiveness of our KGDG framework."}, {"title": "ABLATION STUDY", "content": "We conduct an ablation study using a 4K subset of the training data to evaluate the effectiveness of each component in our KGDG framework. The results are shown in Table 3. The model achieves its best average performance only when all four modules are integrated. For Task #2 and #3, we observe that the DAVER module introduces a slight performance degradation when handling complex prison"}, {"title": "CONCLUSION", "content": "In this paper, we study data generation for legal reasoning to improve the performance of open-source legal LLMs with the help of proprietary LLMs. To address the challenges of limited diversity in synthetic legal data and the difficulty of data verification, we propose KGDG, a knowledge-guided data generation framework. Our framework consists of three key components that leverage legal knowledge to enhance generation diversity and ensure data quality through refinement and verification processes. Additionally, we develop MITRA to expand the generated dataset and further enhance LLM reasoning capabilities. Both KGDG and LAWGPT are validated by extensive experiments on multiple legal reasoning tasks. LAWGPT achieves comparable performance to proprietary LLMs while being significantly smaller in scale.\nLimitations and Future Work. This paper gives a preliminary study on the data generation for legal LLMs and we only make a simple attempt to build each component in the KGDG framework,"}, {"title": "IMPLEMENTATION DETAILS FOR KGDG", "content": "We implement the KGDG framework based on DeepSeek V3 model (DeepSeek-AI et al., 2024) and a knowledge based with 186,197 high-quality criminal legal documents and 152,452 civil legal documents. In our implementation, we call API of DeepSeek V3 model in parallel with a batch size of 16 and the generation process repeats until the number of generated data reaches 25K. The specific implementation details are as follows.\nKGGEN. We first use the Prompt for Generation of KGGEN to select which type of legal document should be sampled to generate similar types of reasoning problems based on the example."}, {"title": "Prompt for Sampling of KGGEN", "content": "\u7ed9\u4f60\u4e00\u4e2aJSON \u683c\u5f0f\u7684\u6cd5\u5f8b\u9886\u57df\u7684\u95ee\u9898\u53ca\u5176\u7b54\u6848\u3002\u5176\u4e2d,instruction \u5b57\u6bb5\u6307\u5bfc\u5982\u4f55\u56de\u7b54\n\u95ee\u9898,question \u5b57\u6bb5\u4e2d\u5305\u542b\u95ee\u9898,answer \u5b57\u6bb5\u4e2d\u5305\u542b\u7b54\u6848\u3002\n{JSON}\n\u73b0\u5728\u8bf7\u4f60\u6839\u636e\u6cd5\u5f8b\u6587\u4e66\u6570\u636e\u751f\u6210\u7c7b\u4f3c\u7684\u95ee\u9898,\u8bf7\u95ee\u4f60\u9700\u8981\u4ec0\u4e48\u7c7b\u578b\u7684\u6587\u4e66\u6570\u636e\u3002\u53ef\u4ee5\u9009\n\u62e9\u7684\u7c7b\u578b\u6709:\u5211\u4e8b\u6cd5\u5f8b\u6587\u4e66\u3001\u6c11\u4e8b\u6cd5\u5f8b\u6587\u4e66\u3002\u8bf7\u4f60\u9009\u62e9\u4e00\u9879\u5e76\u4ee5 JSON \u683c\u5f0f\u5728 type \u5b57\u6bb5\n\u4e2d\u8fd4\u56de\u3002\nHere, the example problem is provided in JSON format in \u2018{JSON}'. The Knowledge-Aware Sam-\npler first determines the appropriate legal document type based on the example problem. Then, it\nrandomly samples a document from the knowledge base of that type and generates a new problem-\nanswer pair, complete with extracted references and reasoning paths."}, {"title": "Prompt for Generation of KGGEN", "content": "\u7ed9\u4f60\u4e00\u4e2aJSON \u683c\u5f0f\u7684\u6cd5\u5f8b\u9886\u57df\u7684\u95ee\u9898\u53ca\u5176\u7b54\u6848\u3002\u5176\u4e2d,instruction \u5b57\u6bb5\u6307\u5bfc\u5982\u4f55\u56de\u7b54\n\u95ee\u9898,question \u5b57\u6bb5\u4e2d\u5305\u542b\u95ee\u9898,answer \u5b57\u6bb5\u4e2d\u5305\u542b\u7b54\u6848\u3002\n{JSON}\n\u8bf7\u4f60\u4ee5\u5982\u4e0b\u6cd5\u5f8b\u6587\u4e66\u7684\u5185\u5bb9\u4e3a\u539f\u578b,\u6309\u7167\u76f8\u540c\u7684JSON\u683c\u5f0f\u548c\u95ee\u9898\u5f62\u5f0f,\u5728 instruction\n\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b,\u7f16\u9020\u4e00\u4e2a\u65b0\u95ee\u9898\u4e0e\u5bf9\u5e94\u7684\u7b54\u6848\u3002\n\u8bf7\u589e\u52a0\u4e00\u4e2a reasoning \u5b57\u6bb5,\u6b64\u5b57\u6bb5\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32,\u8868\u793a\u5f97\u51fa\u7b54\u6848\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\n\u8bf7\u589e\u52a0\u4e00\u4e2a reference \u5b57\u6bb5,\u6b64\u5b57\u6bb5\u662f\u4e00\u4e2a\u5b57\u5178,Key \u4e3a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6d89\u53ca\u7684\u6cd5\u5f8b\u6cd5\n\u6761,Value \u8868\u793a\u6cd5\u5f8b\u6cd5\u6761\u7684\u5177\u4f53\u5185\u5bb9\u3002\n\u8bf7\u9002\u5f53\u6539\u5199\u6cd5\u5f8b\u6587\u4e66\u7684\u5185\u5bb9,\u4e0d\u8981\u5305\u542b\u4e0e\u7b54\u6848\u65e0\u5173\u7684\u5185\u5bb9,\u4e0d\u8981\u76f4\u63a5\u590d\u8ff0\u6cd5\u5f8b\u6587\u4e66\u7684\u5185\n\u5bb9\u3002\n\u8bf7\u4fee\u6539\u95ee\u9898\u4e0e\u7b54\u6848\u4e2d\u7684\u59d3\u540d\u3001\u4f01\u4e1a\u540d\u79f0\u3001\u5730\u70b9\u7b49\u6d89\u53ca\u9690\u79c1\u7684\u5185\u5bb9\u3002\nanswer \u5b57\u6bb5\u7684\u5185\u5bb9\u5e94\u8be5\u5b8c\u5168\u6309\u7167 instruction\u4e2d\u7684\u5bf9\u7b54\u6848\u7684\u683c\u5f0f\u8981\u6c42\u7ed9\u51fa\u3002\n{DOCS}\nHere, the example problem is provided in JSON format in \u2018{JSON}' and the sampled legal document\nis provided in '{DOCS}'."}, {"title": "KGFIX", "content": "We first use the Prompt for Reference Modifier and Reasoning Corrector to correct the\nreferences and reasoning paths for each draft data."}, {"title": "Prompt for Reference Modifier", "content": "\u7ed9\u4f60\u4e00\u4e2a\u5305\u542b\u82e5\u5e72\u6cd5\u6761\u7684JSON\u5b57\u5178,\u6b64\u5b57\u6bb5\u662f\u4e00\u4e2a\u5b57\u5178,Key \u4e3a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6d89\u53ca\u7684\u6cd5\n\u5f8b\u6cd5\u6761,Value \u8868\u793a\u6cd5\u5f8b\u6cd5\u6761\u7684\u5177\u4f53\u5185\u5bb9\u3002\n{JSON}\n\u6cd5\u6761\u7684\u5185\u5bb9\u53ef\u80fd\u5b58\u5728\u95ee\u9898,\u8bf7\u4f60\u5c06 Value \u4fee\u6b63\u4e3a Key \u5bf9\u5e94\u7684\u6b63\u786e\u6cd5\u6761\u5185\u5bb9,\u5e76\u4ee5 JSON\n\u683c\u5f0f\u8fd4\u56de,\u4e0d\u8981\u9644\u52a0\u5176\u4ed6\u5185\u5bb9\u6216\u8bf4\u660e\u3002"}, {"title": "Prompt for Reasoning Corrector", "content": "\u7ed9\u4f60\u4e00\u4e2aJSON \u683c\u5f0f\u7684\u6cd5\u5f8b\u9886\u57df\u7684\u95ee\u9898\u53ca\u5176\u7b54\u6848\u3002\u5176\u4e2d,instruction \u5b57\u6bb5\u6307\u5bfc\u5982\u4f55\u56de\u7b54\n\u95ee\u9898,question \u5b57\u6bb5\u4e2d\u5305\u542b\u95ee\u9898,answer \u5b57\u6bb5\u4e2d\u5305\u542b\u7b54\u6848,reference \u5b57\u6bb5\u4e2d\u5305\u542b\u6cd5\u5f8b\u6cd5\n\u6761\u7684\u5185\u5bb9,reasoning \u5305\u542b\u63a8\u7406\u8fc7\u7a0b\u3002\n{JSON}\n\u5f53\u524d\u95ee\u9898\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0e\u7b54\u6848\u53ef\u80fd\u5b58\u5728\u95ee\u9898,\u8bf7\u6839\u636e\u95ee\u9898\u5185\u5bb9\u3001\u6cd5\u5f8b\u6cd5\u6761\u5185\u5bb9,\u6539\u8fdb\u5f53\u524d\n\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0e\u7b54\u6848\u3002\n\u5982\u679c\u6b64\u95ee\u9898\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0e\u7b54\u6848\u65e0\u9700\u6539\u8fdb,\u8bf7\u76f4\u63a5\u8f93\u51fa\u539f\u59cb JSON\u683c\u5f0f\u5185\u5bb9,\u5426\u5219\u8bf7\u4fee\u6539\nreasoning \u5b57\u6bb5\u548c answer \u5b57\u6bb5\u7684\u5185\u5bb9\u540e,\u76f4\u63a5\u8f93\u51fa JSON \u683c\u5f0f\u5185\u5bb9\u3002\u4e0d\u8981\u9644\u52a0\u5176\u4ed6\u5185\u5bb9\n\u6216\u8bf4\u660e\u3002\nHere, the draft data is provided in JSON format in \u2018{JSON}'."}, {"title": "DAVER", "content": "We first use the Prompt for Verification to verify the correctness of the generated\nquestion-answer pair as well as the consistency between the reasoning, reference and the answer."}, {"title": "Prompt for Verification", "content": "\u7ed9\u4f60\u4e00\u4e2aJSON \u683c\u5f0f\u7684\u6cd5\u5f8b\u9886\u57df\u7684\u95ee\u9898\u53ca\u5176\u7b54\u6848\u3002\u5176\u4e2d,instruction \u5b57\u6bb5\u6307\u5bfc\u5982\u4f55\u56de\u7b54\n\u95ee\u9898,question \u5b57\u6bb5\u4e2d\u5305\u542b\u95ee\u9898,answer \u5b57\u6bb5\u4e2d\u5305\u542b\u7b54\u6848,reference \u5b57\u6bb5\u4e2d\u5305\u542b\u6cd5\u5f8b\u6cd5\n\u6761\u7684\u5185\u5bb9,reasoning \u5305\u542b\u63a8\u7406\u8fc7\u7a0b\u3002{JSON}\n\u8bf7\u4f60\u5224\u65ad\u6570\u636e\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0e\u7b54\u6848\u662f\u5426\u6b63\u786e,\u8bf7\u4ee5 JSON \u683c\u5f0f\u8fd4\u56de\u4f60\u7684\u5224\u65ad\u7ed3\n\u679c\u3002JSON\u683c\u5f0f\u6570\u636e\u4e2d\u5305\u542b\u4e00\u4e2a verify \u5b57\u6bb5,\u53d6\u503c\u4e3a\u6b63\u786e\u6216\u9519\u8bef,\u4e5f\u5305\u542b\u4e00\u4e2a message \u5b57\n\u6bb5,\u8868\u793a\u4f60\u5224\u65ad\u7684\u7406\u7531\u3002\nHere, the draft data to be verified is provided in JSON format in \u2018{JSON}'."}, {"title": "EXAMPLES OF SYNTHETIC DATA", "content": ""}, {"title": "Standard Question-Answer Pair", "content": "Problem:\n\u8bf7\u4f60\u4ed4\u7ec6\u8ba1\u7b97\u6587\u4e66\u4e2d\u6d89\u53ca\u7684\u72af\u7f6a\u603b\u91d1\u989d\u3002\u65e0\u9700\u7ed9\u51fa\u8ba1\u7b97\u8fc7\u7a0b,\u53ea\u9700\u8981\u7ed9\u51fa\u6700\u7ec8\n\u91d1\u989d,\u5c06\u7b54\u6848\u5199\u5728[\u91d1\u989d]\u4e0e<eoa>\u4e4b\u95f4,\u4f8b\u5982[\u91d1\u989d]2000\u5143<eoa>\u3002\u6587\u4e66:\u7ecf\u5ba1\u7406\u67e5\n\u660e,2018\u5e745\u670812\u65e5\u665a20\u65f6\u8bb8,\u88ab\u544a\u4eba\u5f20\u67d0\u67d0\u9152\u540e\u9a7e\u9a76\u6469\u6258\u8f66\u5728\u5e02\u533aXX\u8def\u884c\u9a76\u65f6,\u4e0e\n\u884c\u4eba\u674e\u67d0\u53d1\u751f\u78b0\u649e,\u9020\u6210\u674e\u67d0\u91cd\u4f24\u3002\u4e8b\u6545\u53d1\u751f\u540e,\u5f20\u67d0\u67d0\u9003\u79bb\u73b0\u573a\u3002\u7ecf\u9274\u5b9a,\u674e\u67d0\u7684\u533b\n\u7597\u8d39\u7528\u4e3a15000\u5143,\u540e\u7eed\u6cbb\u7597\u8d39\u7528\u9884\u8ba1\u4e3a5000\u5143,\u8bef\u5de5\u8d39\u4e3a3000\u5143,\u62a4\u7406\u8d39\u4e3a2000\u5143\u3002\n\u5f20\u67d0\u67d0\u5728\u6848\u53d1\u540e\u652f\u4ed8\u4e86\u674e\u67d0\u7684\u533b\u7597\u8d39\u752815000\u5143\u3002\nAnswer:\n[\u91d1\u989d]25000\u5143<eoa>"}, {"title": "Question-Answer Pair with Reasoning Path", "content": "Problem:\n\u8bf7\u4f60\u7ed9\u51fa\u56de\u590d\u7684\u65f6\u5019,\u5728<DTK>\u6807\u7b7e\u524d\u7ed9\u51fa\u4f60\u7684\u601d\u8003\u8fc7\u7a0b\u540e\u518d\u4f5c\u7b54\u3002\u8bf7\u4f60\u4ed4\u7ec6\u8ba1\u7b97\n\u6587\u4e66\u4e2d\u6d89\u53ca\u7684\u72af\u7f6a\u603b\u91d1\u989d\u3002\u65e0\u9700\u7ed9\u51fa\u8ba1\u7b97\u8fc7\u7a0b,\u53ea\u9700\u8981\u7ed9\u51fa\u6700\u7ec8\u91d1\u989d,\u5c06\u7b54\u6848\u5199\u5728[\u91d1\n\u989d]\u4e0e<eoa>\u4e4b\u95f4,\u4f8b\u5982[\u91d1\u989d]2000\u5143<eoa>\u3002\u6587\u4e66:\u7ecf\u5ba1\u7406\u67e5\u660e,2018\u5e745\u670812\u65e5\u665a20\u65f6\n\u8bb8,\u88ab\u544a\u4eba\u5f20\u67d0\u67d0\u9152\u540e\u9a7e\u9a76\u6469\u6258\u8f66\u5728\u5e02\u533aXX\u8def\u884c\u9a76\u65f6,\u4e0e\u884c\u4eba\u674e\u67d0\u53d1\u751f\u78b0\u649e,\u9020\u6210\u674e\n\u67d0\u91cd\u4f24\u3002\u4e8b\u6545\u53d1\u751f\u540e,\u5f20\u67d0\u67d0\u9003\u79bb\u73b0\u573a\u3002\u7ecf\u9274\u5b9a,\u674e\u67d0\u7684\u533b\u7597\u8d39\u7528\u4e3a15000\u5143,\u540e\u7eed\u6cbb\n\u7597\u8d39\u7528\u9884\u8ba1\u4e3a5000\u5143,\u8bef\u5de5\u8d39\u4e3a3000\u5143,\u62a4\u7406\u8d39\u4e3a2000\u5143\u3002\u5f20\u67d0\u67d0\u5728\u6848\u53d1\u540e\u652f\u4ed8\u4e86\u674e\u67d0\n\u7684\u533b\u7597\u8d39\u752815000\u5143\u3002\nAnswer:\n\u6839\u636e\u6587\u4e66\u5185\u5bb9,\u674e\u67d0\u7684\u533b\u7597\u8d39\u7528\u4e3a15000\u5143,\u540e\u7eed\u6cbb\u7597\u8d39\u7528\u9884\u8ba1\u4e3a5000\u5143,\u8bef\u5de5\u8d39\n\u4e3a3000\u5143,\u62a4\u7406\u8d39\u4e3a2000\u5143\u3002\u8fd9\u4e9b\u8d39\u7528\u603b\u548c\u4e3a25000\u5143\u3002\u867d\u7136\u5f20\u67d0\u67d0\u5df2\u7ecf\u652f\u4ed8\u4e8615000\u5143\n\u7684\u533b\u7597\u8d39\u7528,\u4f46\u603b\u72af\u7f6a\u91d1\u989d\u4ecd\u4e3a25000\u5143,\u56e0\u4e3a\u72af\u7f6a\u91d1\u989d\u7684\u8ba1\u7b97\u662f\u57fa\u4e8e\u53d7\u5bb3\u4eba\u5b9e\u9645\u906d\u53d7\n\u7684\u635f\u5931,\u800c\u975e\u88ab\u544a\u4eba\u5df2\u7ecf\u652f\u4ed8\u7684\u91d1\u989d\u3002<DTK>[\u91d1\u989d]25000\u5143<eoa>"}]}