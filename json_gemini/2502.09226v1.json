{"title": "Generating Causally Compliant Counterfactual Explanations using ASP*", "authors": ["Sopam Dasgupta"], "abstract": "This research is focused on generating achievable counterfactual explanations. Given a negative outcome computed by a machine learning model or a decision system, the novel CoGS approach generates (i) a counterfactual solution that represents a positive outcome and (ii) a path that will take us from the negative outcome to the positive one, where each node in the path represents a change in an attribute (feature) value. CoGS computes paths that respect the causal constraints among features. Thus, the counterfactuals computed by CoGS are realistic. CoGS utilizes rule-based machine learning algorithms to model causal dependencies between features. The paper discusses the current status of the research and the preliminary results obtained.", "sections": [{"title": "1 Introduction", "content": "Predictive models used in automated decision-making processes (job-candidate filtering, loan approvals) often function as black boxes, making it difficult to understand their internal reasoning for decision-making. The decisions can have significant consequences, leading individuals to seek satisfactory explanations, especially for an unfavourable (negative) decision. Explaining these decisions presents a significant challenge. Additionally, users want to understand the changes necessary to flip a negative decision into a positive one.\nFollowing Wachter et al.'s [15] approach in this research, counterfactuals are employed to explain a machine learning model's reasoning behind a prediction. Counterfactuals help answer the question: \"What changes should be made to input attributes or features to flip a negative outcome to a positive one?\" Counterfactuals also serve as a good explanation for a prediction. Wachter et al. [15] use statistical techniques by examining the proximity of points in the N-dimensional feature space to find counterfactuals.\nThis paper presents the Counterfactual Generation with s(CASP) (CoGS) framework, which generates counterfactual explanations from rule-based machine learning (RBML) algorithms such as FOLD-SE [16]. CoGS makes two advances compared to Wachter et al.'s work: (i) It computes counterfactuals using RBML algorithms and ASP [8] rather than statistical techniques, and (ii) It considers causal dependencies among features when computing these counterfactuals. Another novelty of the CoGS framework is that it further leverages the FOLD-SE algorithm [16] to automatically discover potential dependencies between features that a user subsequently approves.\nCoGS models various scenarios (or worlds): the current initial state i represents a negative outcome, and the goal state g represents a positive outcome. A state is represented as a set of feature-value pairs. CoGS finds a path from the initial state i to the goal state g by performing interventions (or transitions), where each intervention corresponds to changing a feature value while considering causal"}, {"title": "2 Background", "content": "Counterfactual Reasoning: Counterfactual reasoning is critical for explaining decisions in machine learning, offering insights on achieving desired outcomes by imagining plausible alternate scenarios. Wachter et al. [15] advocated using counterfactual explanations to explain individual decisions, suggesting what changes could flip a negative outcome to a positive one. However, this approach often ignored causal dependencies, leading to unrealistic suggestions. For a binary classifier given by f : X \u2192 {0,1}, we define a set of counterfactual explanations x\u0302 for a factual input x \u2208 X as CFf(x) = {x\u0302 \u2208 X|f(x\u0302) \u2260 f(x)}. This set includes all inputs x\u0302 leading to different predictions than the original input x under f.\nCausality Considerations: Causality relates to cause-effect relationship among predicates. P is the cause of Q, if (P \u21d2 Q) > (\u00acP \u21d2 \u00acQ) [13]. We say that Q is causally dependent on P. Causality is crucial for generating realistic counterfactuals. For example, increasing the credit score to be 'high' while still being under increasing debt obligations is unrealistic due to their causal link. Realistic counterfactuals must model these dependencies to ensure achievable changes.\nASP, s(CASP) Answer Set Programming (ASP) is a paradigm for knowledge representation and reasoning [6, 2, 8]. ASP encodes feature knowledge, decision-making rules and causal rules, enabling the automatic generation of counterfactual explanations using this symbolic knowledge. s(CASP) is a goal-directed ASP system that executes answer set programs in a top-down manner without grounding [1]. s(CASP) adopts program completion, turning \u201cif\u201d rules (P \u21d2 Q) into \u201cif and only if\u201d rules ((P \u21d2 Q) > (\u00acP \u21d2 \u00acQ)) which models causality.\nFOLD-SE: FOLD-SE [16], is an efficient rule-based machine learning (RBML) algorithm for classification tasks. It generates explainable models and learns causal rules from data. It maintains scalability and accuracy, making it a reliable component for the CoGS framework, which leverages these rules for generating counterfactuals.\nThe Planning Problem: Planning involves finding a sequence of transitions from an initial state to a goal state while adhering to constraints. In ASP, this problem is encoded in a logic program with rules defining transitions and constraints restricting the allowed transitions [8]. Solutions are represented as a series of transitions through intermediate states. Each state is represented as a set of facts or logical predicates. Solving the planning problem involves searching for a path of transitions that meets the goal conditions within the constraints. CoGS can be thought of as a framework to find a plan\u2014a series of interventions that change feature values\u2014that will take us from the initial state to the final goal state. However, unlike the planning domain, the interventions (moves) are not independent of each other due to causal dependencies among features."}, {"title": "3 Research Goal", "content": "This research aims to develop a framework that can encode feature knowledge, decision-making rules, and causal rules, enabling the automatic generation of counterfactual explanations using symbolic knowl-"}, {"title": "4 Preliminary Results", "content": "We applied the CoGS methodology to rules generated by the FOLD-SE algorithm (code on GitHub [7]). Our experiments use the German dataset [9], the Adult dataset [3], and the Car Evaluation dataset [5]. These are popular datasets in the UCI Machine Learning repository [12]. The German dataset contains demographic data with labels for credit risk (\u2018good' or 'bad'), with records with the label 'good' vastly outnumbering those labelled 'bad'. The Adult dataset includes demographic information with labels indicating income (\u2018=< $50k/year' or \u2018> $50k/year'). The Car Evaluation dataset provides information on the acceptability of a used car being purchased. We relabelled the Car Evaluation dataset to 'acceptable' and 'unacceptable' to generate the counterfactuals.\nFor the (imbalanced) German dataset, the learned FOLD-SE rules determine a 'good' credit rating, with the undesired outcome being a 'good' rating since the aim is to identify criteria making someone a credit risk ('bad' rating). Additionally, causal rules are also learnt using FOLD-SE and verified (for example, if the feature 'Job' has the value 'unemployed', then the feature 'Present employment since'\nshould have the value \u2018unemployed/unskilled-non-resident'). We learn the rules to verify these assumptions on cause-effect dependencies.\nPath to the Counterfactual: By using these rules that identify individuals with a 'good' rating, we found a path to the counterfactuals, thereby depicting steps to fall from a \u2018good' to a \u2018bad' rating in Table 1. Similarly, we learn the causal rules and the rules for the undesired outcome for the Adult dataset (undesired outcome: \u2018=< $50k/year') as shown in Table 2. For the Car Evaluation dataset (undesired outcome: 'unacceptable') shown in Table 3, we only learn the rules for the undesired outcome as there are no causal dependencies (FOLD-SE did not generate any either). Tables 1, 2 and 3 show a path to each dataset's counterfactual goal state for a specific instance. Note that the execution time for finding the counterfactuals is also reported. While we have only shown specific paths in Tables 1, 2 and 3, our CoGS methodology can generate all possible paths from an original instance to a counterfactual.\nNumber of Counterfactual Sets: Note that each path may represent a set of counterfactuals. This is because numerical features may range over an interval. Thus, CoGS generates 240 sets of counterfactuals for the German dataset, 112 for the Adult dataset, and 78 for the Car Evaluation dataset (Table 4)."}, {"title": "5 Related Work", "content": "Various methods for generating counterfactual explanations in machine learning have been proposed. Wachter et al. [15] aimed to provide transparency in automated decision-making by suggesting changes individuals could make to achieve desired outcomes. However, they ignored causal dependencies, resulting in unrealistic suggestions. Utsun et al. [14] introduced algorithmic recourse, offering actionable paths to desired outcomes but assuming feature independence, which is often unrealistic. CoGS rectifies this by incorporating causal dependencies. Karimi et al. [10] focused on feature immutability and diverse counterfactuals, ensuring features like gender or age are not altered and maintained model-agnosticism. However, this method also assumes feature independence, limiting realism. White et al. [17] showed how counterfactuals can enhance model performance and explanation accuracy. Karimi et al. [11] further emphasized incorporating causal rules in counterfactual generation for realistic and achievable interventions. However, their method did not use the \u2018if and only' property, which is vital in incorporating the effects of causal dependence. CoGS rectified this by utilizing Answer Set Programming (ASP), which does not require grounding as it leverages s(CASP) to generate counterfactual explanations, providing a"}, {"title": "6 Limitations and Planned Work", "content": "One of the limitations of of CoGS is its high computational time, which may lead to scalability issues. We are currently looking for ways to address this problem by replacing the multiple feature-independent values of a given feature with a single placeholder value. The plans for expanding on the work of CoGS include:\n\u2022 Improving the execution time taken to generate counterfactual solutions as well as paths from the current outcome to the counterfactual instance.\n\u2022 Expanding CoGS to generate counterfactuals for statistical machine learning methods: By running an RBML algorithm on the predictions of the statistical model, a rule-based model approximation is generated. This approximation can then be used as the decision rules D corresponding to the statistical model that is required by the CoGS method.\n\u2022 Improve the performance of machine learning systems: When machine learning models are trained on imbalanced datasets, the learned model often optimizes its performance on accurately predicting the majority class compared to the minority class. The plan is to generate counterfactual instances of the majority class, which will help us generate instances that belong to the minority class. The expectation is that the machine learning model trained on the modified training data will perform better with respect to both the majority and minority classes versus the original model trained on the original dataset (imbalanced)."}, {"title": "7 Conclusion", "content": "To conclude, this research is focused on automatically generating counterfactual solutions. This is accomplished by modelling causality and providing a path depicting the series of steps to be taken to"}]}