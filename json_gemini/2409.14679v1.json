{"title": "Quantifying Context Bias in Domain Adaptation for Object Detection", "authors": ["Hojun Son", "Arpan Kusari"], "abstract": "Domain adaptation for object detection (DAOD) aims to transfer a trained model from a source to a target domain. Various DAOD methods exist, some of which minimize context bias between foreground-background associations in various domains. However, no prior work has studied context bias in DAOD by analyzing changes in background features during adaptation and how context bias is represented in different domains. Our research experiment highlights the potential usability of context bias in DAOD. We address the problem by varying activation values over different layers of trained models and by masking the background, both of which impact the number and quality of detections. We then use one synthetic dataset from CARLA and two different versions of real open-source data, Cityscapes and Cityscapes foggy, as separate domains to represent and quantify context bias. We utilize different metrics such as Maximum Mean Discrepancy (MMD) and Maximum Variance Discrepancy (MVD) to find the layer-specific conditional probability estimates of foreground given manipulated background regions for separate domains. We demonstrate through detailed analysis that understanding of the context bias can affect DAOD approach and focusing solely on aligning foreground features is insufficient for effective DAOD.", "sections": [{"title": "I. INTRODUCTION", "content": "Domain adaptation for object detection (DAOD) has been studied extensively [1], [2], [3], [4], [5] to enable object detectors to perform well on datasets with distribution shifts from the training data [6], [7]. It is well known that there's an entanglement between background and foreground features in object detection, leading to a phenomenon called context bias in DAOD. Here, significant differences in background features between the source and target domains can cause a notable decline in the quality and number of detections, even when the foreground features remain unchanged. Recent studies in image classification [8] and segmentation [9], [1], [10] have attempted to mitigate context bias by minimizing this association. Oliva & Torralba [11] demonstrated that context bias could result in the corruption of foreground objects by contextually correlated backgrounds, substantially degrading detection quality. However, there has been no prior work specifically analyzing the impact of context bias in DAOD. This work aims to address this gap.\nIn the realm of human cognition, the brain can accurately and instantly recognize foreground-background associations without extensive training [12]. Several studies, including [13], [14], [12], [15], have investigated the processes of background suppression and foreground representation to understand the scene and temporal dynamics of foreground and background modulation in the brain. These insights can be applied to the field of computer vision for DAOD through comprehensive analysis of the representation of foreground-background associations.\nTo motivate our problem, we first look at the proportion of background features in autonomous driving datasets, as an example. For Cityscapes dataset [16], the number of pixels from built-up features (such as road and sidewalk) are much greater than the foreground object pixels (see Fig. 1). As an image feature, roads are very simple and constant which leads them to be trained more rapidly than other objects such as vehicles. By rapidly learning the simple and constant features of roads, the model can establish a foundational understanding that supports more complex learning tasks, such as detecting vehicles. Figure 2 shows the performance drops when activated features on road regions are zeroed out using semantic labels at the second layer of backbone in the Detectron2 [17], trained on the Cityscapes dataset for object detection. Compared to performance drop ($\\Delta D$ where 0 < $\\Delta D$ < 1) with the sky label, the amount of loss information defined as the negative log of the complement of $\\Delta D$ (-log(1 \u2013 $\\Delta D$)), tends to be larger. It means road context has more contextual association with vehicles, particularly when the vehicle size is small.\nAdditionally we train a YOLOv4 detection model [18] on"}, {"title": "II. RELATED WORK", "content": "A. Foreground-background associations and context bias\nBackground influence [20], [21], [22], [23], [24], [25], [26], [27] and context bias [28], [29], [30], [31] aim at improving performance in tasks such as classification, object recognition, and object localization. Xiao et al. [20] and Zhu et al. [24] studied background effect on accuracy of classification by modifying images with different combinations of foreground and background. A paper proposed a graphical model [30] which modeled foreground-background associations in conditional probability which serves as a methodological inspiration for us. Various studies have addressed context bias using several techniques, such as data augmentation to generate out-of-distributions samples into the background, combination of naturally unmatched background and foreground (e.g., an elephant in room), and applying background removal during training. Torralba [32] demonstrates background effect can be factorized into object priming, focus of attention, and scale selection by modeling the foreground-background associations in a probabilistic model. Liang et al. [21] studied background influence using fashion dataset [33], [34]. These studies [35], [36] can localize foreground objects better than CAM-based algorithms without using bounding box information and with only classification labels. These prior works focus on context bias in the same domain and uses datasets with smaller variations like centered objects or single objects. It lacks to provide strong insight for DAOD.\nB. Domain Adaptation for Object Detection\nDifferent variations of DAOD methods have been pro- posed using feature alignment, synthetic images, and self- training or self-distillation. Feature alignment is to find transformations between source and target domain to reduce distribution shift with adversarial training [37], [1], [38], [39]. It can be helpful to extract common latent features from different domains. Progressive Domain Adaptation for Object Detection [40] synthesized new dataset by using cycleGAN [41] which enables to bridge domain gaps and Self- Adversarial Disentangling for Specific Domain Adaptation [42] achieved 45.2 mAP on Cityscapes to Cityscapes foggy dataset using synthetic images. Gong et al. [43] utilized transformers to focus on aligning features across backbone and decoder networks. However, combining multiple sources into a single dataset and performing single-source domain adaptation for feature alignment does not guarantee better performance compared to using the best individual source domain [44].\nSelf-training uses a teacher model to predict pseudo labels on target domains to gradually understand domain shifti- ness [45], [46], [47], [2], [48]. MIC [4] employed masked"}, {"title": "III. METHOD", "content": "A. Why does the context bias occur during training?\nPrior studies [30], [32] researched context bias for object classification. The studies pointed out that relying only on local features (foreground features in our case) has limitations, including degraded quality due to noise and ambiguity in the target search space. They extended the likelihood to incorporate context information surrounding the foreground, which enhances object classification by providing a stronger conditional probability. The conditional probability of the object (O) given the features (f) was given as:\n$P(O|f) = P(O|F, B) = \\frac{P(F|O, B)P(O|B)}{P(F|B)}$                                                        (1)\nwhere F and B are the foreground and background features The modeling can also be interpreted as:\n$P(O|B) = P(c|\u03c3, \u03a5, \u0392)P(\u03c3|Y, B)P(Y|B)$\n$= P(\u03c3|Y, c, B)P(x|c, B)P(c|B)$                                      (2)\nwhere the object is represented by scale (\u03c3), location (Y), and category (c).\nThe challenge with using a convolutional neural network (CNN) to estimate likelihood is the inability to explicitly teach the model to learn each factor in a specific order. In other words, it means that parameters of CNN can be different depending on how it can be trained. In CNNs, likelihood estimation is a process to find the mean of a distribution, which necessitates more samples to accurately estimate the true mean. This aligns with the principle that a more extensive and refined dataset, achieved through data augmentation, is crucial for better performance [53].\nIn the context of a graphical casual model (F\u2192Y \u2190B), it represents the joint distribution P(Y, F, B), which can be decomposed as either P(Y|F, B)P(F|B)P(B) or P(Y|F, B)P(B|F)P(F). In the SUN 09 training set used by [30], the association between roads and cars is strong, and the number of road pixels surpasses object pixels (similar to Cityscapes: Fig. 1). Consequently, the CNN is more likely to learn P(F = car B = road) than"}, {"title": "IV. EXPERIMENTS", "content": "A. UMAP visualization\nWe start our analysis by plotting the foreground and background feature distributions of different domains using UMAP. Figures 6 and 7 present the visualization of the foreground and background features from different domains in 2D and 3D. In Figure 6, we use the features at the 4th ResNet layer from three different data distributions (a) Cityscapes training dataset as source and validation dataset as target; (b) Cityscapes train dataset as source and Cityscapes foggy dataset as target; and (c) Cityscapes train dataset as source and CARLA as target. The interesting finding is the differences of background alignment between the three comparisons. It is immediately apparent that as the target domain shifts away from the source domain, the background becomes more separable than the foreground. For the Cityscapes training and validation dataset, the fore- ground and background are separable from each other but are mingled up between source and target. For the second panel, the foreground features are together while the back- ground features are separable but overlapping. We can see an extreme case in the third panel where the foreground features between Cityscape and CARLA are next to each other but are non-overlapping while the background features are very distant from each other. The same patterns of Cityscapes-CARLA are demonstrated in Figure 7 when it uses 3d UMAP embedding. We visualized corresponding image patches for qualitative analysis. Noise and ambiguous foreground features are aligned with the background. Beside, background features contains foreground features because of objects adjacent each other. It can lead to alignment with foreground features.\nB. MMD and MVD comparison\nGiven the qualitative analysis, we derive quantitative es- timates of difference in foreground and background fea- ture distributions using MMD and MVD. Figures 8 and 9 show the MMD and MVD in violin plots across dif- ferent layers and bins respectively for different domains. We first split it into small objects in image frame (400"}, {"title": "C. Observations", "content": "The Detectron2 model trained on the Cityscapes dataset achieves 53.72 mAP, with performance dropping to 41.06 mAP on the CARLA validation set and 37.77 mAP on the Cityscapes foggy beta 0.02 set. Despite the CARLA"}, {"title": "D. Conclusion and limitations", "content": "Even though we demonstrate how the background and foreground features are aligned and separable, extracting the"}]}