{"title": "Simply Trainable Nearest Neighbour Machine Translation with GPU Inference", "authors": ["Hossam Amer", "Abdelrahman Abouelenin", "Mohamed Maher", "Evram Nairouz", "Mohamed Afify", "Hany Awadallah"], "abstract": "Nearest neighbor machine translation is a successful approach for fast domain adaption, which interpolates\nthe pre-trained transformers with domain-specific token-level k-nearest-neighbor (kNN) retrieval without re-\ntraining. Despite KNN MT's success, searching large reference corpus and fixed interpolation between the\nkNN and pre-trained model led to computational complexity and translation quality challenges. Among other\npapers, Dai et al. (2023) proposed methods to obtain a small number of reference samples dynamically for\nwhich they introduced a distance-aware interpolation method using an equation that includes free parameters.\nThis paper proposes a simply trainable nearest neighbor machine translation and carry out inference exper-\niments on GPU. Similar to Dai et al. (2023), we first adaptively construct a small datastore for each input\nsentence. Second, we train a single-layer network for the interpolation coefficient between the knnMT and\npre-trained result to automatically interpolate in different domains. Experimental results on different domains\nshow that our proposed method either improves or sometimes maintain the translation quality of methods in\nDai et al. (2023) while being automatic. In addition, our GPU inference results demonstrate that knnMT can\nbe integrated into GPUs with a drop of only 5% in terms of speed.", "sections": [{"title": "1 Introduction", "content": "Neural Machine Translation (NMT) has been show-\ning an increasing trend of translation quality ow-\ning to the ongoing development of deep neural\nnetwork models Vaswani et al. (2017); Kim et al.\n(2021). However, the quality of these models is lim-\nited as soon as the domain of the input test sentences\nis different than the training data. To handle this\nout-of-domain problem, k-nearest neighbor machine\ntranslation (kNN-MT) has proven to be successful in\nmany studies Khandelwal et al. (2021); Zheng et al.\n(021a,b); Jiang et al. (2021); Wang et al. (2022);\nMeng et al. (2022), and thus piqued much attention\nin the community of machine translation. At the\ncore of KNN-MT, a kNN classifier over an external\ndatastore is built based on cached decoder represen-\ntations and corresponding target tokens. This clas-\nsifier is utilized to augment the given NMT model\nwithout finetuning leading to improved predictions,\nespecially for domain adaption. Augmenting the\nNMT model is done via interpolating between the\noutput probability distribution of the NMT model\nand the KNN classifier output probability distribu-\ntion.\nDespite kNN-MT's noticeable success in alle-\nviating the domain adaption problem, vanilla kNN-\nMT proposed in Khandelwal et al. (2021) mainly"}, {"title": "2 Background: KNN-MT", "content": "2.1\nVanilla-kNN\nIn vanilla-kNN, a datastore is created to convert\na bilingual sentence into a set of key-value pairs.\nThese keys and values are defined in Equation 1.\n\\(K,V = F(x, y<t), Yt\\) (1)\nwhere (x, y) \u2208 (X, Y) define the reference cor-\npus for which the pretrained NMT model generates\nthe context representation F(x, y<t at each time step\nt. Then we collect the output hidden state F(x, y<t)\nas key and yt as value to construct the whole datas-\ntore (K, V).\nAt inference time, the current context represen-\ntation F(x, \u0177<t) at decoding step t, as well as the\nalready generated words, are leveraged to generate a\nretrieval distribution Pknn(Yt|Y<t, x) over the entire\nvocabulary:\n\\(P_{knn} (Y_t | x, \\hat{y}_{<t})\n=\n\\sum_{(h_i, V_i) \\in N_t}\nI_{y_t = v_i} exp(-\\frac{-L_2(h_i, F(x, \\hat{y}_{<t}))}{T})\\)\n(2)"}, {"title": "2.2 SK-MT", "content": "In SK-MT Dai et al. (2023), Elasticsearch is used\nfor semantic retrieval components to create a sen-\ntence adaptive datastore instead of a static and ex-\ntensive datastore used in Vanilla kNN-MT. In spe-\ncific, Elasticsearch does two main operations: In-\ndex & Search; storing parallel sentences in indexes\nformat, and then retrieving 32 sentences per input\nsentence with the highest relevance score from the\ntraining corpus.\nAlso, SK-MT provided a successful way of set-\nting the interpolation coefficient in Equation 4.\n\\(\\lambda = Relu(1 - \\frac{d_0}{T})\\) (4)\nwhere do is the top-1 L2 distance during the nearest\nneighbor search, T is the temperature parameter and\nis typically fixed."}, {"title": "3 Single-Layer Trainable Interpolation", "content": "Even though SK-MT introduced a simple solution\nthat derives the interpolation weight from the dis-\ntance, a fixed parameter T for all datasets is tuned\nto produce the best results and T = 100 is recom-\nmended. A fixed temperature may not be optimal\nfor all domains and datasets. For example, Table 1\nshows the BLEU score from the Koran dataset when\nvarying T from 100 to 500 with a step size of 100.\nAs seen in the table, T = 300 increases the BLEU\nscore providing evidence that the temperature value\ncan vary with the dataset and does not have to be\nthe recommended SK-MT T = 100. This obser-\nvation motivates a simple and trainable method to\nadaptively find the temperature parameter for each\ndataset.\nThe proposed simple neural network consists\nof a single layer trained to predict the interpolation\nweight given the distance of the retrieved kNN can-\ndidates. This is in contrast to other adaptive interpo-\nlation methods e.g. Jiang et al. (2022) that use more\nlayers and learnable parameters. We use the devel-\nopment set of each domain to optimize our single-\nlayer network.\nOur training objective is designed to provide\nbetter translation quality. Knowing the ground truth\ntoken, we can choose the best interpolation weight\nthat produces the best probability distribution that\nwe can get from the interpolation between Pmt and\nPknn. Thus, our final objective is to create a sharper\nfinal probability distribution toward our ground truth\ntoken."}, {"title": "4 Experimental Results", "content": "4.1 Experimental Setup\nInput stimuli and Datasets: We test our method-\nology in 2 language directions: German-English\n(deen), and English-Czech (encs). For deen,\nwe employ the multi-domain dataset as the base-\nline Khandelwal et al. (2021) in addition to an e-\ncommerce domain. For encs, we utilize two other\ndomains: finance and medpharma. Our evalua-\ntion metrics are the SacreBLEU Post (2018) and\nCOMET-22 (wmt22-COMET-da) Rei et al. (2022),\na reference-based metric that combines direct as-\nsessments (DA), sentence-level scores, and word-\nlevel tags from Multidimensional Quality Metrics\n(MQM) error annotations.\nModels: Three transformer models are used in our\nexperiments. The first two of the three are used to\nmeasure the translation quality; these two are con-\nstructed from 12 encoder layers and 12 decoder lay-\ners with 512 hidden dimensions and 2048 feedfor-"}, {"title": "4.2 Trainable kNN Retrieval Results", "content": "Table 2 shows the translation quality performance\ncomparison between the proposed trainable method\nand other baselines. As shown in the table, our pro-\nposed trainable method improves the NMT baseline\ntranslation quality by a large margin. In addition, the\nproposed method improves or sometimes maintains\nthe overall translation quality relative to SK-MT on\naverage in terms of the BLEU and COMET scores.\nIn some domains like IT and Koran, the proposed\nmethod improves the SK-MT performance. This re-\nsult demonstrates the ability to at least maintain the\nperformance of SK-MT while using a single-layer\nneural network. Also, these results overall show\nthe adaptability of the proposed method to differ-\nent datasets. For Medical and medpharma, SK-MT\noutperforms our proposed method because the data-\nstore built by the dev set does not have any seman-\ntic similarity to the training set leading to imbal-\nanced binary labeling, whereas the test does not have\nthis imbalanced binary labeling. To overcome this\nchallenge, we suggest that we add weights to the\nbinary cross-entropy training loss function. With\nthis weighted loss function, our trainable method\nachieves 57.2 BLEU, 85 COMET in Medical, and\n48.1 BLEU, 92.5 COMET in medphrama. These re-\nsults increase our average results to 48.9 BLEU, and\n86.6 COMET, respectively.\nTurning to the translation quality in terms of\nCOMET, we observe that SK-MT still either main-\ntain or improve the quality in the majority of the\ndomains. We also notice that the improvements\nin BLEU scores do not fully transfer to COMET.\nWe believe that this is due to the fact that COMET\nis trained on general domain data, therefore it's\nless sensitive to domain terminology and more fo-\ncused on coherence and fluency. For example, e-\ncommerce has an improvement of roughly 6 BLEU\npoints relative to NMT, while the improvement is\n0.3 COMET score points."}, {"title": "4.3 GPU Inference Results", "content": "Table 3 depicts the speed results of ZCode M3 infer-\nence and corresponding BLEU scores in three do-\nmains under test namely, IT, Medical, and Law. The\nKNN-MT results for beam=1, batch=1 setting on the\nlarge scale MoE improves the NMT baseline with\na large margin while dropping the speed by only\n5.2% on average. Similarly, kNN-MT has an im-"}, {"title": "5 Conclusion", "content": "This paper proposes a simply single-layer trainable\nnearest-neighbor machine translation and carries out\nexperiments on large-scale models to demonstrate\nkNN feasibility with GPU Inference. Experimental\nresults show the translation quality effectiveness of\nour adaptive and automatic interpolation technique\nrelative to other methods in literature, the training\nsimplicity in 40 mins on single-GPU, and insignifi-\ncant speed drop of knnMT on GPU inference."}]}