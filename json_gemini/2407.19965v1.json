{"title": "Simply Trainable Nearest Neighbour Machine Translation with GPU Inference", "authors": ["Hossam Amer", "Abdelrahman Abouelenin", "Mohamed Maher", "Evram Nairouz", "Mohamed Afify", "Hany Awadallah"], "abstract": "Nearest neighbor machine translation is a successful approach for fast domain adaption, which interpolates the pre-trained transformers with domain-specific token-level k-nearest-neighbor (kNN) retrieval without re-training. Despite KNN MT's success, searching large reference corpus and fixed interpolation between the KNN and pre-trained model led to computational complexity and translation quality challenges. Among other papers, Dai et al. (2023) proposed methods to obtain a small number of reference samples dynamically for which they introduced a distance-aware interpolation method using an equation that includes free parameters. This paper proposes a simply trainable nearest neighbor machine translation and carry out inference experiments on GPU. Similar to Dai et al. (2023), we first adaptively construct a small datastore for each input sentence. Second, we train a single-layer network for the interpolation coefficient between the knnMT and pre-trained result to automatically interpolate in different domains. Experimental results on different domains show that our proposed method either improves or sometimes maintain the translation quality of methods in Dai et al. (2023) while being automatic. In addition, our GPU inference results demonstrate that knnMT can be integrated into GPUs with a drop of only 5% in terms of speed.", "sections": [{"title": "1 Introduction", "content": "Neural Machine Translation (NMT) has been show-ing an increasing trend of translation quality ow-ing to the ongoing development of deep neural network models Vaswani et al. (2017); Kim et al.(2021). However, the quality of these models is lim-ited as soon as the domain of the input test sentencesis different than the training data. To handle thisout-of-domain problem, k-nearest neighbor machinetranslation (kNN-MT) has proven to be successful inmany studies Khandelwal et al. (2021); Zheng et al.(021a,b); Jiang et al. (2021); Wang et al. (2022);Meng et al. (2022), and thus piqued much attentionin the community of machine translation. At thecore of KNN-MT, a kNN classifier over an externaldatastore is built based on cached decoder represen-tations and corresponding target tokens. This clas-sifier is utilized to augment the given NMT modelwithout finetuning leading to improved predictions,especially for domain adaption. Augmenting theNMT model is done via interpolating between theoutput probability distribution of the NMT modeland the KNN classifier output probability distribu-tion.\nDespite kNN-MT's noticeable success in alle-viating the domain adaption problem, vanilla kNN-MT proposed in Khandelwal et al. (2021) mainly"}, {"title": "2 Background: KNN-MT", "content": "2.1 Vanilla-kNN\nIn vanilla-kNN, a datastore is created to converta bilingual sentence into a set of key-value pairs.These keys and values are defined in Equation 1.\n$K,V = F(x, y_{<t}), Yt$ (1)\nwhere (x, y) \u2208 (X, Y) define the reference cor-pus for which the pretrained NMT model generatesthe context representation F(x, y<t at each time step t. Then we collect the output hidden state F(x, y<t)as key and yt as value to construct the whole datas-tore (K, V).\nAt inference time, the current context represen-tation F(x, \u0177<t) at decoding step t, as well as thealready generated words, are leveraged to generate aretrieval distribution Pknn(Yt|Y<t, x) over the entirevocabulary:\n$P_{knn} (Y_t | x, \\hat{y}_{<t}) = \\frac{\\alpha}{\\sum_{(h_i,V_i)\\in N_t} I_{y_t=v_i} exp(-\\frac{L2(h_i, F(x, \\hat{y}_{<t}))}{T})}$ (2)"}, {"title": "2.2 SK-MT", "content": "In SK-MT Dai et al. (2023), Elasticsearch is usedfor semantic retrieval components to create a sen-tence adaptive datastore instead of a static and ex-tensive datastore used in Vanilla kNN-MT. In spe-cific, Elasticsearch does two main operations: In-dex & Search; storing parallel sentences in indexesformat, and then retrieving 32 sentences per inputsentence with the highest relevance score from thetraining corpus.\nAlso, SK-MT provided a successful way of set-ting the interpolation coefficient in Equation 4.\n$\\lambda = Relu(\\frac{1 - \\frac{d_0}{T}}{T})$ (4)\nwhere do is the top-1 L2 distance during the nearestneighbor search, T is the temperature parameter andis typically fixed."}, {"title": "3 Single-Layer Trainable Interpolation", "content": "Even though SK-MT introduced a simple solutionthat derives the interpolation weight from the dis-tance, a fixed parameter T for all datasets is tunedto produce the best results and T = 100 is recom-mended. A fixed temperature may not be optimalfor all domains and datasets.\nThe proposed simple neural network consistsof a single layer trained to predict the interpolationweight given the distance of the retrieved kNN can-didates. This is in contrast to other adaptive interpo-lation methods e.g. Jiang et al. (2022) that use morelayers and learnable parameters. We use the devel-opment set of each domain to optimize our single-layer network.\nOur training objective is designed to providebetter translation quality. Knowing the ground truthtoken, we can choose the best interpolation weightthat produces the best probability distribution thatwe can get from the interpolation between Pmt andPknn. Thus, our final objective is to create a sharperfinal probability distribution toward our ground truthtoken.\nAs shown in Algorithm 1, our training proce-dure is divided into two stages at each decodingstep. The first stage examines the probability of theground truth token in both distributions Pknn andPmt. If the probability of the ground truth token"}, {"title": "4 Experimental Results", "content": "4.1 Experimental Setup\nInput stimuli and Datasets: We test our method-ology in 2 language directions: German-English(deen), and English-Czech (encs). For deen,we employ the multi-domain dataset as the base-line Khandelwal et al. (2021) in addition to an e-commerce domain. For encs, we utilize two otherdomains: finance and medpharma. Our evalua-tion metrics are the SacreBLEU Post (2018) andCOMET-22 (wmt22-COMET-da) Rei et al. (2022),a reference-based metric that combines direct as-sessments (DA), sentence-level scores, and word-level tags from Multidimensional Quality Metrics(MQM) error annotations.\nModels: Three transformer models are used in ourexperiments. The first two of the three are used tomeasure the translation quality; these two are con-structed from 12 encoder layers and 12 decoder lay-ers with 512 hidden dimensions and 2048 feedfor-ward layer hidden dimensions with 8 multi-head at-tention heads. The third transformer is the ZCodeM3 model reviewed and presented in Kim et al.(2021). ZCode M3 is constructed from 24 en-coder layers and 12 decoder layers with 1024 hid-den dimensions and 4096 feedforward layer hid-den dimensions with 16 multi-head attention heads.The ZCode M3 has 32 experts, 5B parameters, and128,000 vocab size.\nBaselines: The model without knnMT is one base-line. To compare to other methods on our inhousetransformers, we utilize the SK-MT method thatuses a distance-aware adapter Dai et al. (2023). InDai et al. (2023), the authors compared with othermethods and showed success so we use Dai et al.(2023) as a transitive proxy to compare with othermethods.\nGPU Inference Hardware and Environment: In-ference and speed evaluation experiments are car-ried out on a single NVIDIA Tesla V100 GPU.Our inference environment is the highly optimizedFasterTransformer from NVIDIA. Without loss ofgenerality, we fix interpolation to $\\lambda = Relu(\\frac{1 - \\frac{d_0}{T}}{100})$ and measure the speed."}, {"title": "4.2 Trainable kNN Retrieval Results", "content": "Table 2 shows the translation quality performancecomparison between the proposed trainable methodand other baselines. As shown in the table, our pro-posed trainable method improves the NMT baselinetranslation quality by a large margin. In addition, theproposed method improves or sometimes maintainsthe overall translation quality relative to SK-MT onaverage in terms of the BLEU and COMET scores.In some domains like IT and Koran, the proposedmethod improves the SK-MT performance. Thisresult demonstrates the ability to at least maintain theperformance of SK-MT while using a single-layermeural network. Also, these results overall showthe adaptability of the proposed method to differ-ent datasets. For Medical and medpharma, SK-MToutperforms our proposed method because the data-store built by the dev set does not have any seman-tic similarity to the training set leading to imbal-anced binary labeling, whereas the test does not havethis imbalanced binary labeling. To overcome thischallenge, we suggest that we add weights to thebinary cross-entropy training loss function. Withthis weighted loss function, our trainable methodachieves 57.2 BLEU, 85 COMET in Medical, and48.1 BLEU, 92.5 COMET in medphrama. These re-sults increase our average results to 48.9 BLEU, and86.6 COMET, respectively.\nTurning to the translation quality in terms ofCOMET, we observe that SK-MT still either main-tain or improve the quality in the majority of thedomains. We also notice that the improvementsin BLEU scores do not fully transfer to COMET.We believe that this is due to the fact that COMETis trained on general domain data, therefore it'sless sensitive to domain terminology and more fo-cused on coherence and fluency. For example, e-commerce has an improvement of roughly 6 BLEUpoints relative to NMT, while the improvement is0.3 COMET score points."}, {"title": "4.3 GPU Inference Results", "content": "Table 3 depicts the speed results of ZCode M3 infer-ence and corresponding BLEU scores in three do-mains under test namely, IT, Medical, and Law. TheKNN-MT results for beam=1, batch=1 setting on thelarge scale MoE improves the NMT baseline witha large margin while dropping the speed by only5.2% on average. Similarly, kNN-MT has an im-proved translation quality with only a drop of 7%relative to NMT as beam and batch increase to 2 and20, respectively. These results show the potential ofdeploying the knnMT domain adaption approach insuch a large-scale model as ZCode M3."}, {"title": "5 Conclusion", "content": "This paper proposes a simply single-layer trainablenearest-neighbor machine translation and carries outexperiments on large-scale models to demonstratekNN feasibility with GPU Inference. Experimentalresults show the translation quality effectiveness ofour adaptive and automatic interpolation techniquerelative to other methods in literature, the trainingsimplicity in 40 mins on single-GPU, and insignifi-cant speed drop of knnMT on GPU inference."}]}