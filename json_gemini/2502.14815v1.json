{"title": "Optimizing Model Selection for Compound AI Systems", "authors": ["Lingjiao Chen", "Jared Quincy Davis", "Boris Hanin", "Peter Bailis", "Matei Zaharia", "James Zou", "Ion Stoica"], "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSELECTOR, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSELECTOR iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSELECTOR is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-40, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSELECTOR confers 5%-70% accuracy gains compared to using the same LLM for all modules.", "sections": [{"title": "1 Introduction", "content": "Researchers and developers are increasingly leveraging large language models (LLMs) by composing multiple LLM calls in a compound AI system to tackle complex tasks [Du et al., 2024, Zhang et al., 2024, Madaan et al., 2023, DeepMind, 2023, Shinn et al., 2023, Renze and Guven, 2024, Zaharia et al., 2024]. For example, a common practice is to use one LLM call to generate one initial answer, one LLM call to give feedback, and one more call to refine the answer based on the feedback, known as self-refine [Renze and Guven, 2024, Madaan et al., 2023, Ji et al., 2023]. Another example is multi-agent debate [Du et al., 2024, Liang et al., 2024, Khan et al., 2024], where multiple LLM calls are made to propose initial answers and then debate which ones are correct. Compared to monolithic models, significant improvements are possible because the compound systems decompose challenging tasks into simpler sub-tasks, and perform one LLM call for each sub-task.\nMost existing work on improving compound systems focuses on optimizing prompts used in individual modules and/or module interactions, while using the same LLM for all modules [Khattab et al., 2024, Yuksekgonul et al., 2024, Wu et al., 2023]. While this simplifies compound system design, it also leaves several important questions unaddressed. Does using different models across modules improve a compound system's performance? If so, why and by how much? Given a pool of LLMs, can we find the best model each module should use without exhaustive search?\nAs a first step towards answering such questions, we systematically study model selection in static compound AI systems, i.e., those where the number of modules, the sequencing of module calls, and the mapping between modules and models are fixed. In this context, we indeed find that allocating different LLMs to different modules leads to substantially higher performance than allocating the same LLM to all modules. As an example, consider again the self-refine system [Madaan et al., 2023] consisting of three modules: a generator, a critic, and a refiner. LLM A may be better at providing feedback but worse at generating and refining answers than LLM B. In this case, allocating LLM A for the critic and LLM B for the generator and refiner is better than allocating either one to all modules.\nThen we formulate the model selection problem (MSP), i.e., identifying the best model each module should use to maximize the overall performance. MSP is challenging in principle, as it is infeasible to exhaustively search the exponentially large space of all model choices. Our insights are that, in many cases, (i) the end-to-end performance can be monotonic in per-module performance, and (ii) per-module performance can be estimated accurately by an LLM diagnoser. This motivates us to design LLMSELECTOR, a principled framework that optimizes MSP for any static compound AI systems given a training budget. LLMSELECTOR iteratively nominates one module and allocates to it the model with the best module-wise performance, as estimated by an LLM diagnoser. One benefit is that LLMSELECTOR is applicable to any compound AI system whose number of modules is fixed. Furthermore, LLMSELECTOR only incurs a manageable amount of LLM calls. In fact, we provide mathematical conditions under which LLMSELECTOR finds the optimal solution to MSP with the number of LLM calls linear to the number of modules (Section 4).\nWe conduct systematic experiments on a diverse set of compound AI systems using real-world LLM APIs including GPT-40, Claude 3.5 Sonnet, and Gemini 1.5 Pro. Perhaps surprisingly, we have found that different model choices have a significant effect on compound systems' performance. In fact, LLMSELECTOR offers 5%-70% performance gains compared to allocating the same LLM to all modules (Figure 1). While not optimizing prompts, LLMSELECTOR also outperforms advanced techniques specializing in prompt optimization (Table 2 in Section 5). This further highlights the importance of model selection for compound AI systems.\nIn short, our main contributions are:\n\u2022 Model selection problem. We formulate the model selection problem (MSP) for compound AI systems, an increasingly important but under-explored problem.\n\u2022 The LLMSelector framework. To optimize MSP, we propose LLMSELECTOR, a principled framework that iteratively chooses one module and allocates to it the model with the highest module-wise performance estimated by an LLM.\n\u2022 Model choices matter. Through extensive experiments on practical compound systems using real-world LLM APIs including GPT-40, Claude 3.5 Sonnet, and Gemini 1.5 Pro, we have found that choosing different models can substantially affect (up to 100%) a compound AI system's performance.\n\u2022 LLMSelector finds excellent choices. Systematical experiments have shown that LLMSELECTOR identifies model choices that outperform allocating the same LLM to all modules by 5%-70%.\n\u2022 Open-source artifacts. We release\u00b9 our code and data, including compound systems' intermediate outputs generated by commercial LLM APIs."}, {"title": "2 Related Work", "content": "Compound AI system optimization. Prompt engineering and module interaction design is a central topic of compound AI system optimization. While existing work often relies on manually tuning them [DeepMind, 2023, Shinn et al., 2023, Zhou et al., 2024b, Pryzant et al., 2023, Fourney et al., 2024, Zhao et al., 2024, Lu et al., 2023, Zhao et al., 2024], recent work studies how to automate this process, such as DSPy [Khattab et al., 2024], Textgrad [Yuksekgonul et al., 2024], and Autogen [Wu et al., 2023]. For example, DSPy uses Bayesian optimization to adjust prompts for all modules, while Textgrad uses textual feedback to optimize prompts for individual modules. On the other hand, our work focuses on model selection, a third axis for compound system optimization, complementary to prompt optimization and module interaction design.\nModel market utilization. Model market utilization studies how to use all available (proprietary and open-source) models for downstream tasks [Lu et al., 2024, Ram\u00edrez et al., 2024, Miao et al., 2023]. Extensive work has built various techniques to utilize different models, such as model cascade [Chen et al., 2024b], model routing [Hu et al., 2024, Stripelis et al., 2024], and mixture-of-experts [Wang et al., 2024]. While they mainly focus on single-stage tasks such as classification [Chen et al., 2020, Huang et al., 2025] and question answering [Chen et al., 2024b, Shekhar et al., 2024], we study model utilization for compound AI systems requiring multiple stages. This is a much more challenging problem as the search space is much larger.\nModel selection. Model selection is a critical part of classic ML and has been extensively studied in the literature [Kohavi, 1995, Akaike, 1974, Elsken et al., 2019]. While classic techniques focus on model selection for one ML task, compound systems involve multiple ML tasks. Thus, model selection becomes more challenging as the search space is exponentially large in the number of tasks.\nLLM-as-a-judge. LLMs have been increasingly used for evaluating and judging complex generations, a phenomenon termed LLM-as-a-judge. Researchers have extensively studied how LLM judges align with human preferences in real-world scenarios [Zheng et al., 2023, Shankar et al., 2024], how to improve its quality [Kim et al., 2023], how to evaluate it [Chiang et al., 2024, Chen et al., 2024a, Zeng et al., 2023], as well as many other applications [Johri et al., 2025, Dhole et al., 2024, Gu et al., 2024, Zhou et al., 2024a]. In this paper, we find a novel use case of LLM-as-a-judge: diagnosing module-wise performance to accelerate the model allocation search process."}, {"title": "3 Compound AI Systems", "content": "Static Compound AI systems. As defined by [Zaharia et al., 2024], compound AI systems address AI tasks by synthesizing multiple components that interact with each other. Here, we denote a static compound AI system by a directed acyclic graph $G \\equiv (V, E)$, where each node $v \\in V$ denotes one module, and each directed edge $e\\equiv (u,v) \\in E$ indicates that the output from module u is sent to module v as input. Without loss of generality, we assume a final output module that generates the final output without any output edges, and an input module representing the input query which receives no input edges.\nLLM modules. An LLM module is a module that utilizes an LLM to process the inputs. It typically concatenates all inputs as a text snippet (via some prompt template), obtain an LLM's response to this snippet, and send the response as output (potentially after some postprocessing). Throughout this paper, all modules are LLM modules to simplify notations. In practice, if a module is not an"}, {"title": "4 Modeling and Optimizing Model Selection", "content": "This section presents how to model and optimize model selection for static compound AI systems."}, {"title": "4.1 Problem Statement", "content": "Consider a static compound AI system $G = (V, E)$ and a set of LLMs $M \\subseteq {1,2,\\ldots,|M|}$ to use. Let $F: V \\rightarrow M$ denote all possible model allocations, each of which allocates an LLM $k \\in M$ to a module $v \\in V$. Given a task distribution $D$, the performance of the compound AI system using the model allocation $f \\in F$ is $P(f) \\equiv \\mathbb{E}_{z \\in D}[p(f, z)]$. Here, $z$ denotes a task sampled from the data distribution, and $p(f, z)$ is the performance of the compound AI system on the given task $z$ using the allocation $f$. Our goal is to find one model allocation that maximizes the overall performance, i.e.,\n$\\max _{f \\in F} P(f)$   (1)"}, {"title": "4.2 The assumptions", "content": "Problem 1 is challenging without any assumptions, as it is impossible to exhaustively search all possible model allocations, the size of which grows exponentially in the number of modules $|V|$. Here we list our assumptions to enable tractable analysis.\nBinary performance. For simplicity, we only consider binary performance, i.e., $p(f, z) \\in {0,1}$.\nDecomposition to per-module performance. In classic computing systems such as a hardware stack, optimizing individual components (such as CPU, GPU, and memory) often leads to better overall performance. Similarly, improving individual modules' quality should also lead to better overall quality of a compound AI system. For the sake of analysis, we also assume that we can decompose a compound system's performance as a monotone function of individual modules' performance. Formally, let $p_{i}(f, z)$ denote module $v_{i}$'s performance on the task $z$ using allocation $f$. Then the end-to-end performance can be decomposed as $p(f, z) = h(p_{1}(f, z), p_{2}(f, z),\\cdots, p_{|V|}(f, z))$, where $h(\\cdot)$ is monotonically increasing.\nMonotone module-wise performance. The module-wise performance needs to satisfy certain properties to enable us to analyze the interplay between individual modules and the compound systems. In this paper, we focus on module-wise performance $p_{i}$ with the following two conditions.\n\u2022 $p_{i}$ is intra-monotone, which means that\n$p_{i}(f_{i \\rightarrow k}, z) \\geq p_{i}(f_{i \\rightarrow k'}, z)$\n$\\forall_{j}, p_{j} (f_{i\\rightarrow k}, z) \\geq p_{j}(f_{i\\rightarrow k'}, z)$\n\u2022 $p_{i}$ is inter-monotone, which indicates that\n$p_{i}(f_{i \\rightarrow k}, z) > p_{i}(f_{i \\rightarrow k'}, z)$\n$\\forall_{j}, p_{j} (f_{i\\rightarrow k}, z) \\geq p_{j}(f_{i\\rightarrow k'}, z)$\nIn other words, if module ith performance is higher by replacing its allocated model from A to B, then such replacement should not hurt other modules' performance no matter what models are allocated to other modules.\nDo the assumptions always hold? The above two conditions simplify our analysis, but they are not always satisfied in practice. In these cases, while our analysis may not hold, the derived algorithm is still applicable and demonstrates superior performance (as shown later in Section 5).\nOptimality Characterization. Suppose the module-wise performance is both intra-monotone and inter-monotone. Then we are able to study the optimal allocation via the lens of module-wise per-formance. In particular, we first argue that it is possible to find a model allocation that maximizes the performance for each module. This is because the module-wise performance is inter-monotone: improving the model used for one module can only improve the performance for other modules. The second observation is that a module-wise optimal allocation must also be the globally optimal alloca-tion. This is due to the fact that the end-to-end performance is a monotone function of all individual module-wise performance."}, {"title": "4.3 The LLMSelector framework", "content": "The above analysis motivates our design of LLMSELECTOR, a principled framework for optimizing model allocation in compound AI systems within a budget constraint.\nFigure 3 gives an overview of how LLMSELECTOR works. It takes the compound AI system ar-chitecture G, the set of LLM M, a training dataset DTr, and a training budget B as input, and returns an optimized model allocation f as the output. Here, each data point in the training dataset $z = (q, a) \\in DTr$ is a question-answer pair specifying a possible question and desired answer. LLMSE-LECTOR involves an iterative process. In each iteration, it nominates one module and then allocates to the module the model with the highest module-wise performance. This is repeated until running out of the training budget or no module can be further improved by updating one module at a time. The details can be found in Algorithm 1. The following result shows when LLMSELECTOR can identify the optimal allocation. The proof is left to the appendix.\nTheorem 4.1. Suppose for each task z in DTr, the optimal allocation is unique. Then Algorithm 1 converges to the optimal allocation on the training data after L iterations.\nThe LLM diagnoser. LLMSELECTOR requires access to the model-wise performance function $p_i$. In practice, however, this is often unavailable or too expensive to collect. Therefore, we propose to use a LLM diagnoser to estimate the model-wise performance function. In particular, we give an LLM as input a compound AI system $G = (V, E)$, a task $z = (q, a)$ consisting of a question q and the desired answer a, the inputs and outputs of each module $v \\in V$ using a specific allocation f, and ask it to determine module jth's performance. Let $p_{j}(f, z)$ denote the output by the LLM diagnoser. Then we approximate the module-wise performance by $p_{j}(f, z) = \\hat{p}_{j}(f, z) + \\gamma p(f, z)$, where $\\gamma > 0$ is a hyperparameter balancing the LLM's estimation and the end-to-end performance. The prompt used for the LLM diagnoser can be found in the appendix."}, {"title": "5 Experiments", "content": "We compare the performance of LLMSELECTOR with vanilla compound AI systems using real-world LLM models in this section. Our goal is three-fold: (i) validating that allocating different models to"}, {"title": "5.1 A case study on TableArithmetic", "content": "Let us start with a case study on TableArithmetic, a synthetic dataset consisting of 100 questions. Here, each question involves a table consisting of \"ID\" and \"task\" rows. The goal is to solve the task corresponding to a specific ID. The table in each question has a total of 200 entries, and the task in each entry is a simple arithmetic question \u201cWhat is X + (10.9 > 10.11)?\u201d, where X is a random integer between 1 and 100.\nThe locate-solve system. To address TableArithmetic, we use the locate-solve system consisting of two modules. The first module, locate, extracts the task with the corresponding ID, and the second module, solve, takes the first module's output and then answers the extracted task. For this specific case study, we only use five models: GPT-40, GPT-40 mini, Claude 3.5 Sonnet, Gemini 1.5 Pro, and Llama 3.1 405B.\nLLMSelector Setup. We use Gemini 1.5 Pro as the LLM diagnoser. For this case study, we set up $\\gamma = 0$, that is, we fully rely on the LLM diagnoser as the module-wise performance function for each module.\nPerformance Analysis. Figure 4 demonstrates how LLMSELECTOR performs on this task. We first note that allocating any fixed model to all modules leads to poor end-to-end performance, as shown in Figure 4 (a). This is because no model has high performance for all modules. Second, LLMSELECTOR's accuracy is perfect. This is because (i) there exists some model with perform accuracy on each module, and (ii) LLMSELECTOR learns this efficiently. For example, Claude 3.5 is perfect on the first Module, and Gemini 1.5 Pro makes no mistake on the second module. LLMSELECTOR learns to leverage the best model for each module, and thus reaches the best performance. To further understand this,\nlocate module using Claude 3.5 correctly identifies the task \"What is 48 + (10.9 > 10.11)?\", but the solve module using Claude 3.5 incorrectly suggests that 10.9 is less than 10.11 and thus gives a wrong answer. On the other hand, the locate module using Gemini 1.5 Pro extracts the wrong task, but it solves the task correctly. LLMSELECTOR learns to use Claude 3.5 for the first module and Gemini 1.5 Pro for the second module, and therefore correctly answers this query.\nOptimizer analysis. Next, we focus on understanding the search efficiency of LLMSELECTOR. In particular, we compare LLMSELECTOR with two baselines: random search and greedy search. Given an LLM API budget B, random search randomly chooses B model allocations from all possible allocations, and then returns the one with the highest end-to-end performance. The greedy search"}, {"title": "5.2 Quantitative Performance Improvement", "content": "Next, we study the performance of LLMSELECTOR on practical compound AI systems. In particular, we focus on three compound AI systems, namely, locate-solve, self-refine [Renze and Guven, 2024], and multi-agent-debate [Du et al., 2024]. The architectures of these systems are shown in Figure 5 in the appendix. We use six datasets: TableArithmetic and Table Bias for locate-solve, LiveCodeBench [Jain et al., 2024] and CGH [Renze and Guven, 2024] for self-refine, and SimpleQA [Wei et al., 2024] and FEVER [Thorne et al., 2018] for multi-agent-debate. We compare LLMSELECTOR with using any fixed model for all modules and DSPy [Khattab et al., 2024], an open-source library specialized For prompt optimization in compound systems. For DSPy, we use the optimizer MIPROv2, which searches for best prompts using Bayesian optimization. We use GPT-40 as the backbone LLM, and set max_bootstrapped_demos=2, max_labeled_demos=2, and all other parameters as default for MIPROv2."}, {"title": "5.3 Qualitative Understanding", "content": "To further understand when and why LLMSELECTOR outperforms allocating the same model to all modules, we dive into a few specific examples and compare how LLMSELECTOR's generations differ from these by allocating the same LLM. In particular, Figure 6 in the appendix gives one example from the SimpleQA dataset answered by the multi-agent-debate system. LLMSELECTOR learns to allocate GPT-40, Llama 3.1 405B, and Gemini 1.5 Pro for the three answer generators separately, and use GPT-40 for the three debaters. In this example, the three generators give completely different answers: 8, 3, and -18, and the GPT-40 debaters identify that 3 is the correct answer. Allocating GPT-40 to all modules leads to an incorrect answer, however. This is because the GPT-40 generators always return 8 and thus the debaters fail to identify this mistake. We leave more analysis to the appendix due to the space limit."}, {"title": "6 Conclusion", "content": "In this paper, we study how to select which LLMs to which modules to optimize a given compound AI system, an important but under-explored question. We propose and develop LLMSELECTOR, an efficient framework to address this question by leveraging two key insights: (i) end-to-end per-formance is often monotonic in per-module performance, and (ii) module-wise performance can be accurately estimated by an LLM. Our empirical evaluations with real-world LLM APIs show that LLMSELECTOR offers substantial performance gains (5%-70%) over allocating the same model to all modules, highlighting the importance of model selection. We also release our code and data via https://github.com/LLMSELECTOR/LLMSELECTOR to stimulate more research on optimizing com-pound AI systems."}, {"title": "A Proof of Theorem 4.1", "content": "Proof. The proof consists of two parts. First, we show that at iteration j, allocation $f_z$ allocates the same models to the first j modules as the optimal allocation for each task z. Second, we can show that taking the mode over all tasks' allocations leads to the optimal allocation for the training dataset.\nWe first note that the uniqueness of a task's optimal model allocation implies that for each module only one unique model maximizes the per-module quality. That is, for each i, there exists some k, such that for any k' \u2260 k, we have $p_{i}(f_{i \\rightarrow k} > p_{i}(f_{i \\rightarrow k'}$. Suppose not. Let $k^*$ be the model allocated to module i by the optimal allocation. Due to the monotone assumption, $k^*$ should also maximize module i's performance. Let k' be another model that maximizes module i's performance. By the inter-monotone assumption, switching from $k^*$ to k' does not hurt any other module's performance. By the monotone assumption, k' also maximizes the overall performance. A contradiction. Therefore, for each module, there is only one unique model that maximizes its performance, regardless of how other modules are allocated.\nNow we can show that at iteration j, allocation $f_z$ allocates the same models to the first j modules as the optimal allocation. To see this, one can simply notice that the unique \"best\" model for each module must also be the optimal model for the end-to-end system. This is again because of the monotone assumption: otherwise, one can change the model in the optimal allocation to have better performance of one module and thus the overall system. Therefore, allocating the per-module optimal model is the same as allocating the optimal model for the entire system. Thus, at iteration j, allocation $f_z$ allocates the same models to the first j modules as the optimal allocation.\nNow we study the second part. By the first part, after L iterations, each $f_z$ has become the best allocation for task z. Recall that we focus on binary performance, i.e., $p() \\in {0,1}$. Hence, if the model allocation is not one of $f_z$, its end-to-end performance is simply 0. Now, for any $f_z$, its performance on the training dataset is the average over its performance on each data point, i.e.,\n$\\frac{1}{|D_{Tr}|} \\sum_{z' \\in D_{Tr}} p(f_z, z')$ \nNow recall that the optimal allocation for each query is unique. That is, $p(f_z, z')$ is 1 if $f_z = f'$, and 0 otherwise. Hence, the training performance is proportional to\n$\\sum_{z' \\in D_{Tr}} \\mathbb{1}_{f_z = f'}$\nThat is, the performance of allocation $f_z$ is proportional to the number of training data points whose optimal allocation is the same as $f_z$. Therefore, taking the mode of all optimal allocations is sufficient to obtain the best allocation for the training dataset."}, {"title": "B Experiment Details", "content": "B.1 Compound AI systems\nIn this paper, we focus on three compound AI systems, locate-solve, self-refine and multi-agent-debate. Their architectures are shown in Figure 5. Locate-solve consists of two modules: the first module extracts the task associated with an ID from an input table, and the second module returns the answer to the extracted task. Self-refine has a generator, a critic, and a refiner. The generator gives an initial answer to a question, the critic gives feedback to this answer, and the refiner uses the feedback to refine the original answer. Multi-agent-debate has two types of modules, answer generators and debaters. The answer generators offer initial answers to a question. The debaters take the initial answers and then debate which one is correct. In this paper, we focus on a six-module multi-agent-debate: three modules are answer generators, and the other three are the debaters.s\nB.2 Datasets and evaluation metrics\nNow we provide details of all datasets used in this paper."}, {"title": "B.3 LLM models", "content": "We use 10 LLMs offered by third-party providers, including GPT-40, GPT-40 mini, GPT-4-Turbo, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash, Llama 3.1 405B, Llama 3.1 70B, and Qwen 2.5 72B. In particular, GPT-40, GPT-40 mini, and GPT-4 Turbo correspond to gpt-40-2024-05-13, gpt-4o-mini-2024-07-18 and gpt-4-turbo-2024-04-09 offered by OpenAI. Claude 3.5 Sonnet and Claude 3.5 Haiku refer to claude-3-5-sonnet-20240620 and claude-3-haiku-20240307 by"}, {"title": "B.4 Prompt for the LLM diagnoser", "content": "The following box gives the prompt template for the LLM diagnoser."}, {"title": "B.5 Qualitative example analysis", "content": "To better understand why LLMSELECTOR can outperform allocating the same LLM to all modules, we give more examples for self-refine and multi-agent-debate, as shown in Figure 6 and Figure 7.\nIn addition to the examples shown in Figure 6 analyzed in the main paper, another example from the LiveCodeBench dataset answered by the self-refine system is shown in Figure 7. In this case, LLMSELECTOR learns to use Claude 3.5 Sonnet for the generator and refiner, and uses GPT-40 for the critic module. Recall that always allocating Claude 3.5 Sonnet is better than always allocating any other LLMs. However, this leads to an incorrect answer on this example, as Claude 3.5 Sonnet as the critic fails to realize its own generation is incorrect. However, GPT-40 as the critic correctly identifies the initial generation is incorrect. Thus LLMSELECTOR correctly answers this question."}]}