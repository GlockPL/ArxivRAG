{"title": "PARETOFLOW: GUIDED FLOWS IN MULTI-OBJECTIVE OPTIMIZATION", "authors": ["Ye Yuan", "Can (Sam) Chen", "Christopher Pal", "Xue Liu"], "abstract": "In offline multi-objective optimization (MOO), we leverage an offline dataset of designs and their associated labels to simultaneously minimize multiple objectives. This setting more closely mirrors complex real-world problems compared to single-objective optimization. Recent works mainly employ evolutionary algorithms and Bayesian optimization, with limited attention given to the generative modeling capabilities inherent in such data. In this study, we explore generative modeling in offline MOO through flow matching, noted for its effectiveness and efficiency. We introduce ParetoFlow, specifically designed to guide flow sampling to approximate the Pareto front. Traditional predictor (classifier) guidance is inadequate for this purpose because it models only a single objective. In response, we propose a multi-objective predictor guidance module that assigns each sample a weight vector, representing a weighted distribution across multiple objective predictions. A local filtering scheme is introduced to address non-convex Pareto fronts. These weights uniformly cover the entire objective space, effectively directing sample generation towards the Pareto front. Since distributions with similar weights tend to generate similar samples, we introduce a neighboring evolution module to foster knowledge sharing among neighboring distributions. This module generates offspring from these distributions, and selects the most promising one for the next iteration. Our method achieves state-of-the-art performance across various tasks. Our code will be available here.", "sections": [{"title": "1 INTRODUCTION", "content": "Offline optimization, a fundamental challenge in science and engineering, involves minimizing a black-box function using solely an offline dataset, with diverse applications ranging from protein design Sarkisyan et al. (2016); Angermueller et al. (2019) to neural architecture design Lu et al. (2023). Previous research primarily focuses on single-objective optimization, aiming to optimize a single desired property Trabucco et al. (2022); however, this fails to capture the complexities of real-world challenges that often require balancing multiple conflicting objectives, such as designing a neural architecture that demands both high accuracy and minimal parameter count Lu et al. (2023). In this study, we explore offline multi-objective optimization (MOO), leveraging an offline dataset of designs and their associated labels to simultaneously minimize multiple objectives.\nThe pioneering work Xue et al. (2024) adapts evolutionary algorithms Deb et al. (2002); Zhang & Li (2007) and Bayesian optimization Daulton et al. (2023); Zhang & Golovin (2020); Qing et al. (2023) to handle the offline MOO setting. Besides, some studies design controllable generative models that manage multiple properties Wang et al. (2024). However, these studies generally either focus on different settings, such as online optimization Gruver et al. (2024) and white-box optimization Yao et al. (2024), or utilize less advanced generative models, such as VAEs Wang et al. (2022). None of these studies fully exploits the potential of advanced generative modeling in offline MOO.\nTo bridge this gap, we employ a flow matching framework, renowned for its effectiveness and efficiency over diffusion models Lipman et al. (2022); Le et al. (2024); Polyak et al. (2024), to in-"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 OFFLINE MULTI-OBJECTIVE OPTIMIZATION", "content": "Offline multi-objective optimization (MOO) seeks to simultaneously minimize multiple objectives using an offline dataset D of designs and their corresponding labels. Consider a design space denoted as X \\subseteq \\mathbb{R}^d, where d represents the dimension of the design. In MOO, we aim to find solutions that achieve the best trade-offs among conflicting objectives. Formally, the multi-objective optimization problem is defined as:\nFind x^* \\in X such that there is no x \\in X with f(x) \\prec f(x^*), \\tag{1}\nwhere f: X \\rightarrow \\mathbb{R}^m is a vector of m objective functions, and \\prec denotes Pareto dominance. A solution x is said to Pareto dominate another solution x^* (denoted as f(x) \\prec f(x^*)) if:\n\\forall i \\in \\{1,...,m\\}, f_i(x) \\leq f_i(x^*) \\text{ and } \\exists j \\in \\{1, ..., m\\} \\text{ such that } f_j(x) < f_j(x^*). \\tag{2}\nIn other words, x is no worse than x^* in all objectives and strictly better in at least one objective. A solution x^* is Pareto optimal if there is no other solution x \\in X that Pareto dominates x^*. The set of all Pareto optimal solutions constitutes the Pareto set (PS). The corresponding set of objective vectors, defined as \\{f(x) | x \\in PS\\}, is known as the Pareto front (PF).\nThe goal of MOO is to identify a set of solutions that effectively approximates the PF, providing a comprehensive representation of the best possible trade-offs among the objectives."}, {"title": "2.2 FLOW MATCHING", "content": "Flow matching, an advanced generative modeling framework, excels in effectiveness and efficiency over diffusion models Lipman et al. (2022); Le et al. (2024); Polyak et al. (2024). At the core of this framework lies a conditional probability path p_t(x | x_1), t \\in [0,1], evolving from an initial distribution p_0 (x | x_1) = q(x) to an approximate Dirac delta function p_1 (x | x_1) \\approx \\delta(x \u2013 x_1). This evolution is conditioned on a specific point x_1 from the distribution p_{data} and is driven by the conditional vector field u_t(x | x_1). A neural network, parameterized by \\theta, learns the marginal vector field v(x, t):\nv(x, t;\\theta) \\approx v(x, t) = \\mathbb{E}_{x_1\\sim p_t(x_1|x)}[u_t(x | x_1)] \\tag{3}\nThis modeled vector field, \\hat{v}(x, t; \\theta), functions as a neural Ordinary Differential Equation (ODE), guiding the transition from q(x) to p_{data}(x).\nFollowing (Pooladian et al., 2023), the process begins by drawing initial noise x_0 from q(x_0). This noise is then linearly interpolated with the data point x_1:\nx | x_1, t = (1 - t) \\cdot x_0 + t \\cdot x_1, x_0 \\sim q(x_0) \\tag{4}\nThe derivation of the conditional vector field is straightforward: u_t(x | x_1) = (x_1 - x)/(1 - t). Alternatively, this can be expressed as u_t(x | x_1) = \\frac{x_1 - x_0}{1 - t}. Training this conditional flow matching model involves optimizing the following loss function:\n\\mathbb{E}_{t, p_{data} (x_1), q(x_0)} || \\hat{v}(x, t; \\theta) \u2013 \\frac{(x_1 - x_0)}{1-t} ||^2 \\tag{5}\nWe can then use the learned vector field \\hat{v}(x, t; \\theta) to generate samples by solving the neural ODE."}, {"title": "3 METHOD", "content": "We describe two modules of our ParetoFlow method: multi-objective predictor guidance in Section 3.1 and neighboring evolution in Section 3.2. The full algorithm is detailed in Algorithm 1."}, {"title": "3.1 MULTI-OBJECTIVE PREDICTOR GUIDANCE", "content": "In this section, we first elucidate the concept of predictor guidance within the flow matching framework. Next, we detail the formulation of a weighted distribution driven by a uniform weight vector. Finally, we introduce a local filtering scheme designed to effectively manage non-convex PFs."}, {"title": "Neighboring Evolution", "content": "Distributions with similar weight vectors tend to generate similar samples. As shown in Figure 1(c), the distributions with weights \\omega^2 and \\omega^3 are neighboring and they generate similar sample along the sampling trajectory. This motivates us to introduce Module 2, termed neighboring evolution, to foster knowledge sharing among these neighboring distributions. We propose to generate diverse offspring samples from neighboring distributions, and select the most promising one for the next iteration. For instance, consider \\omega^2 with \\omega^3 as its sole neighbor in Figure 1(c). We generate offspring samples from both \\omega^2 and \\omega^3, and then select the most promising one\u2014identified by a dashed circle\u2014as the next iteration state for the \\omega^2 distribution. A similar scheme applies to \\omega^3, assuming \\omega^2 as its neighbor. This module facilitates valuable knowledge sharing between neighboring distributions (\\omega^2 and \\omega^3), enhancing the effectiveness of the overall sampling process.\nTo summarize, our contributions are three-fold:\n\u2022 We explore the use of generative modeling in offline MOO by introducing ParetoFlow, specifically designed to effectively steer flow sampling to approximate the Pareto front.\n\u2022 We propose a multi-objective predictor guidance module that assigns a uniform weighted distribution to each sample, ensuring comprehensive coverage of the objective space.\n\u2022 We establish a neighboring evolution module to enhance knowledge sharing among distributions with close weight vectors, which improves the sampling effectiveness."}, {"title": "Predictor Guidance", "content": "Predictor Guidance. Originally, classifier guidance was proposed to direct sample generation to- ward specific image categories Dhariwal & Nichol (2021). This concept has been adapted for re- gression settings to guide molecule generation Lee et al. (2023); Jian et al. (2024). In this paper, we term this technique predictor guidance for a generalization. Based on Lemma 1 in Zheng et al. (2023), we derive predictor guidance in flow matching as:\n\\tilde{v}(x_t, t, y; \\theta) = \\hat{v}(x_t, t; \\theta) + \\frac{1-t}{t} \\nabla_{x_t} log p_p(y | x_t, t). \\tag{6}\nwhere p_p(y | x_{t,t}) represents the predicted property distribution. Further details can be found in Appendix A.1. Training the proxy at different time steps t is resource-intensive. Therefore, we approximate this by leveraging the relationship between x_1 and x_t:\np_p(y | x_{t,t}) = p_p(y | x_1(x_t),1),\nsimplified to p_p (y | x_1(x_t)). This guides the generation of x_t towards samples with the property y."}, {"title": "Weighted Distribution", "content": "Weighted Distribution. The preceding discussion typically pertains to generating samples to satisfy a single property y, whereas our framework is designed to optimize multiple properties simultaneously, denoted as y = [f_1(x),\\ldots, f_m(x)]. To manage this complexity, we decompose the multi- objective generation challenge into individual weighted objective generation subproblems. Specifi- cally, we define a weight vector \\omega = [\\omega_1,\\omega_2,\\ldots, \\omega_m], where each \\omega_i > 0 and \\sum_{i=1}^m \\omega_i = 1. The weighted property prediction is expressed as:\nf_{\\omega}(x_t; \\beta) = \\sum_{i=1}^{m} -f_i (x_1(x_t); \\beta_i)\\omega_i, \\tag{7}\nwhere f_i predicts the ith objective for x_t, trained using only x_1 data, and the negative sign indicates minimization. We then formulate the weighted distribution as Lee et al. (2023):\np_{\\beta}(y | x_1(x_t), \\omega) = \\gamma e^{xf_{\\omega}(x_t;\\beta)} /Z, \\tag{8}\nwhere \\gamma is a scaling factor and Z is the normalization constant. Integrating this into Eq.(6) leads to:\n\\tilde{v}(x_t, t, y; \\theta) = \\hat{v}(x_t, t; \\theta) + \\frac{1-t}{t} \\gamma \\nabla_{x_t} f_{\\omega} (x_t; \\beta). \\tag{9}"}, {"title": "Local Filtering", "content": "This vector field drives sampling towards desired properties within the weighted distribution. Equa- tions (8) and (9) are applied in Algorithm 1, Lines 12 and 13, respectively.\nUsing the Das-Dennis approach Das & Dennis (1998), which subdivides the objective space into equal partitions to generate uniform weight vectors, we produce N weights \\omega. Each weight maps to a sample, effectively covering the entire objective space. For the ith sample x at time step t, the Euler-Maruyama method Kloeden et al. (1992) is applied to advance to the next time step \\Delta t:\nx^s_t = x_t + \\tilde{v}(x_t, t, y; \\theta)\\Delta t + g\\sqrt{\\Delta t}\\epsilon, \\tag{10}\nwhere s = t + \\Delta t indicates the next time step, g = 0.1 denotes the noise factor, and \\epsilon is a standard Gaussian noise term. This process is on Line 14 in Algorithm 1. Unlike standard ODE sampling, this additional noise term g enhances diversity and improves exploration of the design space.\nLocal Filtering. Using Eq. (10), sampling can reach any point on the Pareto Front (PF) if it is con- vex. As shown in Figure 2(a), a weight vector \\omega = [0.5, 0.5] successfully guides sample generation to the f_1 = f_2 Pareto-optimal point. In such convex case, a set of uniform weight vectors can ef- fectively direct sample generation across the entire PF. However, with a non-convex PF as depicted in Figure 2(b), the same weight vector skews the sampling toward favoring a single objective, either f_1 or f_2, making it challenging to approach the f_1 = f_2 Pareto-optimal point or its vicinity."}, {"title": "3.2 NEIGHBORING EVOLUTION", "content": "In the prior module, we discuss sampling from a single weighted distribution while overlooking the potential interactions between different distributions. In this section, we define neighboring distributions and introduce a module to foster knowledge sharing among them.\nNeighboring Distribution. Weighted distributions with similar weight vectors are likely to pro- duce similar samples, which could benefit from potential knowledge sharing. Since each weighted distribution is defined by a unique weight vector, we define neighboring distributions based on the proximity of their weight vectors. For a distribution associated with \\omega^i, its neighbors are identified as the K distributions whose weight vectors have the smallest angular distances to \\omega^i:\nN(i) = \\{j : \\omega^j \\in KNN(\\omega^i, K, \\{\\omega^i\\}_{i=1}^N)\\} \\tag{11}"}, {"title": "Neighboring Update", "content": "Neighboring Update. As mentioned, neighboring distributions generate similar samples, and we aim to leverage this similarity to foster knowledge sharing. Since \\epsilon introduces randomness in Eq.(10), we can obtain different next step states x^s where each state can be viewed as an offspring. We can generate O offspring for x_t^i, denoted as \\{x^{i,o}_{t}\\}_{o=1}^{O}. Given that there are K neighboring sam- ples for sample i, this results in a set of K \\cdot O offspring X_i = \\{x^{j,o} | j \\in \\mathcal{N}(i), o \\in \\{1, 2,\\cdots,O\\}\\}. Line 16 in Algorithm 1 outlines this step. All candidates in this set are likely to satisfy the weighted distribution i well, as they are guided by similar weighted distributions j \\in \\mathcal{N}(i).\nWe aim to update the current sample x_t^i using the neighboring set X_i. The local filtering scheme from the previous module filters out X_i to exclude objective predictions not aligned with \\omega^2. The remaining viable offspring are termed X'_i. Subsequently, the next iteration for x^i is updated as:\nx^s = \\arg \\max_{x^o \\in X'_i} f_{\\omega^i} (x^o; \\beta). \\tag{12}\nThis determines the next state x_t^s for each of N samples and is detailed in Line 18 of Algorithm 1.\nPareto-optimal Set Update. While directly selecting the final N samples from the flow generation is effective, we also aim to retain all high-quality samples during generation. To achieve this, we maintain a PS consisting of N samples, where the i-th sample is the best for f_i (\\cdot; \\beta). The PS is initialized with non-dominated samples in the offline dataset following Xue et al. (2024). Using Eq. (12), we compare x_t^s with the i-th sample in PS. If x_t^s is superior, we update PS with x_1(x_t^s); otherwise, we retain the existing sample. This step is specified at Line 19 in Algorithm 1. Finally, we apply non-dominant sorting to PS and select 256 candidates for evaluation."}, {"title": "4 EXPERIMENTS", "content": "We conduct comprehensive experiments to evaluate our method. In Section 4.4, we compare our ap- proach to several baselines to assess performance. In Section 4.5, we demonstrate the effectiveness of our proposed modules."}, {"title": "4.1 BENCHMARK OVERVIEW", "content": "We utilize the Off-MOO-Bench, which summarizes and collects several established bench- marks Xue et al. (2024). We explore five groups of tasks, each task with a dataset D and a ground-truth oracle f for evaluation, which is not queried during training. For discrete inputs, we convert them to continuous logits as suggested by Trabucco et al. (2022); Xue et al. (2024).\nTasks. (1) Synthetic Function (Synthetic) Xue et al. (2024): This task encompasses several subtasks involving popular functions with 2-3 objectives, aiming to identify PS with 60,000 offline designs. We exclude the DTLZ2-6 tasks as recommended by the authors due to evaluation errors 2.\n(2) Multi-Objective Neural Architecture Search (MO-NAS) Dong & Yang (2020); Lu et al. (2023); Li et al. (2021): This task consists of multiple sub-tasks, searching for a neural architecture that optimizes multiple metrics, such as prediction error, parameter count, and GPU latency.\n(3) Multi-Objective Reinforcement Learning (MORL) Todorov et al. (2012): (a) The MO-Swimmer sub-task involves finding a dimension-9,734 control policy for a robot to maximize speed and energy efficiency; (b) The MO-Hopper sub-task aims to find a dimension-10,184 control policy for a robot to optimize two objectives related to running and jumping.\n(4) Scientific Design (Sci-Design): (a) This Molecule Zhao et al. (2021) sub-task aims to optimize two activities against biological targets GSK3\\beta and JNK3 in a dimension-32 molecule latent space, using 49,001 offline points. (b) The Regex sub-task aims to optimize protein sequences to maximize the counts of three bigrams, using 42,048 offline points. (c) The ZINC sub-task aims to maximize the logP (the octanol-water partition coefficient) and QED (quantitative estimate of drug-likeness) of a small molecule. (d) The RFP sub-task aims to maximize the solvent-accessible surface area and the stability of RFP in protein sequence designs."}, {"title": "4.3 TRAINING DETAILS", "content": "Our objective is to derive 256 design samples. However, since the Das-Dennis method may not generate exactly 256 uniform weights, we generate slightly more, resulting in over 256 samples. We then use learned predictors for non-dominant sorting to select the top 256 samples. We set the number of neighboring distributions, K, to be m + 1, where m is the number of objective functions, and set the number of offspring, O, to be 5. The sensitivity of these hyperparameters is further examined in Appendix A.3. We follow the predictor training configurations outlined in Xue et al. (2024) and flow matching training protocols described in Tomczak (2022). Additional hyperparameter details are provided in Appendix A.4 and the computational overhead is discussed in Appendix A.5."}, {"title": "4.4 RESULTS AND ANALYSIS", "content": "Table 1 displays the average ranks of the 100th percentile results for all methods across various task types. Detailed hypervolume results for both the 100th and 50th percentiles are reported in Appendix A.6 and Appendix A.7, respectively. Two separator lines distinguish: (1) DNN-based methods from GP-based methods, and (2) GP-based methods from generative modeling methods. D(best) denotes the best solution set in the offline set, characterized by the largest HV value. The last column summarizes the average rank of each method across all tasks. In each task, the best and second-best ranks are highlighted in bold and underlined, respectively. We provide visualization results for C-10/MOP1 and MO-Hopper, and a case study on C-10/MOP5, in Appendix A.8.\nWe make the following observations: (1) As shown in Table 1 and Figure 3, our method ParetoFlow consistently achieves the highest ranks across all tasks, underscoring its effectiveness. (2) Both DNN-based and generative modeling-based methods frequently outperform D(best), illustrating the strength of predictor and generative modeling. (3) GP-based methods often underperform D(best). We hypothesize this is because these methods, typically used in online optimization to select subse- quent samples, are less effective in this offline context. (4) Within the generative modeling category, ParetoFlow surpasses other methods, including diffusion-based methods like PROUD and LaMBO- 2, the VAE-based method CorrVAE, and the GFlowNet-based method MOGFN, highlighting the superiority of our ParetoFlow method. (5) MO-NAS and Sci-Design tasks are predominantly dis- crete, with MO-NAS having a higher dimensionality. Generative modeling methods show reduced effectiveness on MO-NAS relative to Sci-Design. This performance gap may stem from the diffi- culty in modeling high-dimensional discrete data."}, {"title": "4.5 ABLATION STUDIES", "content": "We use ParetoFlow as the baseline to assess the impact of removing specific modules, with results detailed in Table 2. We conduct these ablation studies on representative subtasks: ZDT2 for Syn- thetic, C-10/MOP1 for MO-NAS, MO-Hopper for MORL, Zinc for Sci-Design, and RE23 for RE.\nMulti-Objective Predictor Guidance: This module employs uniform weights for batch samples. In our ablation study, we explore: (1) Equal: Equal weight assigned to every sample across all ob- jectives. (2) First: Weight applied solely to the first objective. Both variants underperform compared to the full ParetoFlow, demonstrating the advantage of our uniform weight scheme. Equal generally outperforms First, suggesting that focusing on a single objective can bias sample generation.\nAdditionally, we evaluate the impact of excluding the local filtering scheme (w/o local) to deter- mine its importance. The performance drop observed without this scheme underscores its effec- tiveness in managing non-convex Pareto Fronts. Additionally, we measure pairwise diversity using \\frac{N(N-1)}{2} \\sum_{i=1}^{N} \\sum_{j=i+1}^{N} d(y_i, y_j), where d denotes the Euclidean distance. This metric is applied to samples from both ParetoFlow and ParetoFlow w/o local. For ParetoFlow w/o local, diversity decreases from 5.144 to 2.080 in ZDT2, from 0.832 to 0.827 in C-10/MOP1, from 0.897 to 0.181 in MO-Hopper, from 0.721 to 0.495 in Zinc, and from 0.991 to 0.814 in RE23. This indicate that the local filtering scheme enhances performance by improving the diversity of the solution set. We further compare local filtering performance on convex and non-convex tasks in Appendix A.9. We also include in Appendix A.10 detailed comparisons between flow matching and diffusion models, as well as between the Das-Dennis method and another weight generation strategy.\nNeighboring Evolution: We omit this module (w/o neighbor) to observe the effects on sample gen- eration, focusing exclusively on direct offspring without leveraging neighboring samples. Removing this module leads to performance decrease as detailed in Table 2, demonstrating the effectiveness of neighboring information. Besides, we found that employing the neighboring module significantly improves the selection of the next step's offspring. In the sampling process, the majority of offspring selected from the neighborhood outperform those from their own distribution: 67.33% for ZDT2, 73.67%for C-10/MOP1, 58.33% for MO-Hopper, 81.33% for Zinc, and 61.98% for RE23, high-lighting the pivotal role of this module in the sampling process. Besides, we observe that only 12% of the points in C-10/MOP1 and 1% in MO-Hopper are duplicates. The higher duplication rate in C-10/MOP1 is primarily due to the decoding of continuous logits back to the same discrete values. This observation underscores the effectiveness of ParetoFlow.\nLastly, we examine the performance of our method without the Pareto Set (PS) update, relying only on the final samples produced through the sampling process. The observed performance degradation confirms the critical role of the PS update, indicating that final samples alone are insufficient."}, {"title": "5 RELATED WORK", "content": "Offline Multi-Objective Optimization. The primary focus of MOO research is the online setting, which involves interactive queries to a black-box function for optimizing multiple objectives simul- taneously Jiang et al. (2023); Park et al. (2023); Gruver et al. (2024). However, offline MOO presents a more realistic setting, as online querying can be costly or risky Xue et al. (2024). In this context, two traditional methods are adapted with a trained predictor as the oracle: Evolutionary algorithms employ a population-based search strategy that includes iterative parent selection, reproduction, and survivor selection Deb et al. (2002); Zhang & Li (2007). Alternatively, Bayesian optimiza- tion leverages the learned predictor model to identify promising candidates through an acquisition function, with sampled queries advancing each iteration Daulton et al. (2023); Zhang & Golovin (2020); Qing et al. (2023). Additionally, several predictor training techniques such as COMS Tra- bucco et al. (2021), ROMA Yu et al. (2021), NEMO Fu & Levine (2021), ICT Yuan et al. (2023), Tri-Mentoring Chen et al. (2023a), GradNorm Chen et al. (2018), and PcGrad Yu et al. (2020) are adopted to enhance training efficacy.\nOur ParetoFlow method is inspired by the seminal evolutionary algorithms MOEA/D Zhang & Li (2007) and LWS Wang et al. (2016), which use a weighted sum approach Ma et al. (2020) to guide populations and facilitate mutation among neighbors. The generation concept in these algorithms corresponds to the time step concept in our method. The primary distinction of our method is its gen- erative modeling aspect: we train an advanced flow matching model on the entire dataset, enabling the exploration of data generative properties. This capability allows our sampling process to access the sample space that traditional evolutionary algorithms are unlikely to reach. We further explore"}, {"title": "Guided Generative Modeling", "content": "Guided Generative Modeling. Several studies have developed generative models to produce sam- ples meeting multiple desired properties. For instance: Wang et al. (2021) integrates structure- property relations into a conditional transformer for a biased generative process. Wang et al. (2022) employs a VAE model to recover semantics and property correlations, modeling weights in the la- tent space. Tagasovska et al. (2022) applies multiple gradient descent on trained EBMs to generate new samples, although training EBMs for each property can be complex. Han et al. (2023) explores a distinct setting aimed at generating modules that fulfill specific conditions. Zhu et al. (2023) uses GFlowNet as the acquisition function and Jain et al. (2023) integrates multiple objectives into GFlowNet. Yao et al. (2024) introduces diversity through hand-designed diversity penalties instead of uniform weight vectors, focusing on a white-box setting. Gruver et al. (2024) investigates online multi-objective optimization within a diffusion framework, using the acquisition to guide sample generation. Kong et al. (2024) applies multi-objective guidance under a diffusion framework but only uses equal weights for all properties, failing to capture the Pareto Front. Chen et al. (2024); Yuan et al. (2024) also explore guided diffusion models; however, their focus is limited to single- objective optimization. These studies vary in setting and approach, often using generative models that are either less advanced or challenging to train. Unlike these efforts, our work combines the advanced generative model of flow matching with evolutionary priors in traditional algorithms, an intersection never explored in the existing literature."}, {"title": "6 CONCLUSION", "content": "In this work, we apply flow matching to offline multi-objective optimization, introducing ParetoFlow. Our multi-objective predictor guidance module employs a uniform weight vector for each sample generation, guiding samples to approximate the Pareto-front. Additionally, our neigh- boring evolution module enhance knowledge sharing between neighboring distributions. Extensive experiments across various benchmarks confirm the effectiveness of our approach. We discuss ethics statement and limitations in Appendix A.13."}, {"title": "A.1 PREDICTOR GUIDANCE IN FLOW MATCHING", "content": "From Lemma 1 in Zheng et al. (2023), the guided vector field is derived as:\n\\tilde{v}(x_t, t, y; \\theta) = a_t x_t + b_t \\nabla_{x_t} log p(x_t | y) \\tag{13}\nwhere a_t = \\frac{\\alpha_1}{\\alpha_t} and b_t = (\\alpha_t\\sigma_1 \u2013 \\alpha_1\\sigma_t)\\frac{1}{\\alpha_t}. With \\alpha_t = t and \\sigma_t = 1 - t, we simplify Eq. (13):\n\\tilde{v}(x_t, t, y; \\theta) = \\frac{1-t}{t} x_t + \\frac{1}{1-t}\\nabla_{x_t} log p(x_t | y) \\tag{14}\nThe log-probability function is expressed as:\nlog p(x_t | y) = log p_0(x_t) + log p_p (y | x_t, t) \u2013 log p(y) \\tag{15}\nwhere p_0(x_t) represents the data distribution learned by the flow matching model and p_p(y | x_{t,t})\ndenotes the predicted property distribution.\nSubstituting these expressions leads to:\n\\tilde{v}(x_t, t, y; \\theta) = \\frac{1-t}{t} x_t + \\frac{1}{1-t} \\nabla_{x_t} log p_0(x_t) + \\nabla_{x_t} log p_p (y | x_t,t)\\\n = \\hat{v}(x_t, t; \\theta) + \\frac{1}{1-t}\\nabla_{x_t} log p_p (y | x_t,t) \\tag{16}"}, {"title": "A.2 EXTENDED COMPARISONS", "content": "We have expanded our analysis to include widely recognized methods such as NSGD-III Deb & Jain (2013) and SMS-EMOA Beume et al. (2007), applied to the same five tasks highlighted in our ablation studies. Our findings in Table 3 demonstrate that ParetoFlow consistently outperforms these traditional approaches, reinforcing the effectiveness and robustness of our method."}, {"title": "A.3 HYPERPARAMETER SENSITIVITY", "content": "This section examines the sensitivity of our method to various hyperparameters\u2014namely, the num- ber of neighbors (K), the number of offspring (O), the number of sampling steps (t), the scaling factor (\\gamma) in Eq.(8), the noise factor (g) in Eq.(10)\u2014across two tasks: the continuous MO-Hopper and the discrete C-10/MOP1. Hypervolume metrics are normalized by dividing by the default hy- perparameter result to facilitate comparative analysis, unless specified otherwise."}, {"title": "A.4 TRAINING DETAILS", "content": "We adopt the predictor training configurations from Xue et al. (2024), utilizing a multiple model setup. Each predictor consists of a 3-layer MLP with ReLU activations, featuring a hidden layer size of 2048. These models are trained over 200 epochs with a batch size of 128, using the Adam optimizer Kingma (2014) at an initial learning rate of 1 \u00d7 10-3, and a decay rate of 0.98 per epoch.\nFlow matching training follows protocols from Tomczak (2022), employing a 4-layer MLP with SeLU activations and a hidden layer size of 512. The model undergoes 1000 training epochs with early stopping after 20 epochs of no improvement, with a batch size of 128 and the Adam optimizer.\nThe approximation x_1(x_t) proves inaccurate when t is near 0, leading to an unreliable predictor. Consequently, we set \\gamma = 2 only if t exceeds a predefined threshold; otherwise, \\gamma = 0. We determine this threshold by evaluating the reconstruction loss between x_1(x_t) and x_1. As illustrated in Figure 9 for the tasks C-10/MOP1 and MO-Hopper, the reconstruction loss remains below 0.2 when t exceeds 0.8. Therefore, we establish the threshold at 0.8.\nWe employ the simplest setup of Multiple Models (MM) within ParetoFlow. As detailed in Table 4, we experiment with the IOM setup for comparison. The results indicate that the IOM setup performs similarly or slightly worse"}]}