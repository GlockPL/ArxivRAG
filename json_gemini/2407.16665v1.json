{"title": "A Framework for Pupil Tracking with Event Cameras", "authors": ["Khadija Iddrisu", "Waseem Shariff", "Suzanne Little"], "abstract": "Saccades are extremely rapid movements of both eyes that occur simultaneously, typically observed\nwhen an individual shifts their focus from one object to another. These movements are among the swiftest\nproduced by humans and possess the potential to achieve velocities greater than that of blinks. The peak\nangular speed of the eye during a saccade can reach as high as 700\u00b0/s in humans, especially during larger\nsaccades that cover a visual angle of 25\u00b0. Previous research has demonstrated encouraging outcomes in\ncomprehending neurological conditions through the study of saccades. A necessary step in saccade detec-\ntion involves accurately identifying the precise location of the pupil within the eye, from which additional\ninformation such as gaze angles can be inferred. Conventional frame-based cameras often struggle with the\nhigh temporal precision necessary for tracking very fast movements, resulting in motion blur and latency\nissues. Event cameras, on the other hand, offer a promising alternative by recording changes in the visual\nscene asynchronously and providing high temporal resolution and low latency. By bridging the gap between\ntraditional computer vision and event-based vision, we present events as frames that can be readily utilized\nby standard deep learning algorithms. This approach harnesses YOLOv8, a state-of-the-art object detec-\ntion technology, to process these frames for pupil tracking using the publicly accessible Ev-Eye dataset.\nExperimental results demonstrate the framework's effectiveness, highlighting its potential applications in\nneuroscience, ophthalmology, and human-computer interaction.", "sections": [{"title": "1 Introduction", "content": "Saccades have been extensively studied in the context of various neurological and psychiatric disorders, such as\nschizophrenia, concussions, traumatic brain injuries (TBI), and Parkinson's disease [Bittencourt et al., 2013].\nAdditionally, the relationship between saccades and cognition has shown strong correlation in identifying neu-\nrological disorders [Yang et al., 2024]. Assessing saccade changes during non-visual cognitive tasks may help\nto identify subtle brain changes due to aging and neuro-degenerative diseases. To make saccade analysis broadly\nuseful for both diagnostic and research purposes, it is crucial to identify specific eye movement parameters. Key\nmetrics, including velocity and duration, must be extracted from data that reflects a wide range of patient char-\nacteristics, such as differing eye shapes, and iris, hair, and skin pigmentation. However, to analyze saccades,\nthe first step is to detect the pupil.\nAutomatic and non-invasive pupil tracking is a significant task in computer vision with applicability in\nareas such as Human-Computer Interactions (HCI), Virtual Reality and Extended Reality. This technology\ncan contribute to greater understanding of psychological conditions by analysing the behaviour and track-\ning the movement of the pupil position over time. Recently, a few studies to improve pupil tracking have\nemerged [Kang et al., 2019, Khan et al., 2020, Lee et al., 2020]. The majority of this research falls into two\ncategories: remote pupil tracking and near-eye pupil tracking. Remote-eye pupil tracking involves pupil track-\ning from a distance from the eye which is applicable in areas such as driver monitoring while the latter involves\npupil tracking with a setup in direct contact with the eyes (e.g., AR/VR). For effective pupil detection in non-\nclinical, real-world, environments, optimized sensors, such as event cameras, are essential."}, {"title": "2 Related Work", "content": "Despite the demonstrated potential of event-based pupil tracking, the body of research addressing this area\nremains limited. To date, only a small number of recent studies have undertaken this task.\nKang et al [Kang and Kang, 2023] developed a large-scale event face training database through RGB-to-\nEvent domain translation. They achieved this by using the StyleFlow algorithm to generate event-like images\nfrom RGB face images, leveraging existing annotations. Subsequently, the study evaluated a cross-modal\nlearning-based pupil localization algorithm on real event camera images, captured with a DAVIS 346 camera,\nusing a mixed dataset of RGB and event-like images. The algorithm, based on the RetinaFace algorithm and\ntrained under this strategy, achieved a high accuracy in pupil localization, outperforming methods trained solely\non RGB or event-like databases. In another study, the authors accumulated events over 33 ms intervals into\nframes [Kang et al., 2023]. Following this representation, they use cascaded Adaboost classifiers with multi-\nblock local binary patterns (LBPs) to detect eye-nose region on which a Shape-Deformation Model (SDM)\nis applied for precise alignment, utilizing SIFT features and a Support Vector Machine (SVM) to maintain\ntracking across frames.\nKagemoto et al [Kagemoto and Takemura, 2023] presents the first purely event-based approach for pupil\ntracking with the bright and dark pupil effect elicited by alternately blinking two near-infrared illumination\nsources. The pupil center is detected by manually defining the region of interest and calculating the center of\ngravity of the more eventful polarity. This method processes events in real-time at 2000 Hz, significantly faster"}, {"title": "3 Methodology", "content": "Deep learning algorithms such as Convolutional Neural Networks (CNNs) accept data inputs of fixed size such\nas images. Hence to perform pupil tracking with CNNs, there is a need to convert events into representations\nsuch as frames or voxel grids which can be readily used by these networks. In this study, we convert events into\n2D frames by accumulating polarity pixel-wise.\nEvent cameras (ECs) possess an extremely high temporal resolution, theoretically capable of achieving up\nto 1 million frames per second (FPS) due to their microsecond (1\u00b5) temporal precision [Wang et al., 2019].\nThis contrasts with conventional frame-based cameras, which are typically limited to a maximum of about\n100-145 FPS. We make use of the high temporal resolution by accumulating events over a 10 ms period to\ngenerate equivalent frames at 100 FPS. This approach allows us to significantly reduce the sampling time while\npreserving the temporal information inherent in the events. By doing so, we mitigate the risk of under-sampling\nand ensure that relevant information is maintained within the frames, all while preserving the asynchronous\nnature of the events. Additionally, this approach allows us to generate more frames for training (over 600\nframes per event file) as compared to a few frames when accumulating based on a fixed size window. Sampling\nat 10 ms also aided in generation of frames captured during very rapid movements such as a half/almost full\nblink periods which may otherwise be lost in traditional imaging.\nThere are two methods commonly employed for 2D event accumulation; by accumulating in a fixed time\nwindow or fixed size window. The fixed time window approach conserves the temporal information of events,\nhowever for tasks such as pupil/eye tracking, it may lead to overlapping of eye features in time windows where\nthere are many events generated due to motion. Therefore a fixed size approach is often employed for eye\ntracking applications, however due to the sparse nature of events, a larger window size is needed to make the\npupil visible which results to low frame rate and subsequently can lead to under-sampling.\nIn this study, we employ a fusion of both approaches such that, we sample a fixed duration window, i.e The\ntotal duration of events, from the minimum timestamp (Tmin) to the maximum timestamp (Tmax), is divided\ninto frames of duration 10 milliseconds (ms) each. The number of frames (Nframes) is determined as $(Nframes =\n\\frac{Tmax-Tmin}{duration\\_ms})$.\nFor each frame, events occurring within the time window ([Tstart, Tend)) are selected, where\n(Tstart = Tmin + i \u00d7 duration_ms) and (Tend = Tstart + duration_ms).\nThis process ensures that each frame represents a specific interval of time. To avoid creating frames with\ntoo few events, a threshold is applied. Frames are only generated if the number of events within the current\ntime window exceeds the specified event threshold (set to 2000). This helps to filter out frames that might\nbe empty or contain too little information. The events within the selected time window are then processed to"}, {"title": "3.1 Event Representation", "content": "Deep learning algorithms such as Convolutional Neural Networks (CNNs) accept data inputs of fixed size such\nas images. Hence to perform pupil tracking with CNNs, there is a need to convert events into representations\nsuch as frames or voxel grids which can be readily used by these networks. In this study, we convert events into\n2D frames by accumulating polarity pixel-wise.\nEvent cameras (ECs) possess an extremely high temporal resolution, theoretically capable of achieving up\nto 1 million frames per second (FPS) due to their microsecond (1\u00b5) temporal precision [Wang et al., 2019].\nThis contrasts with conventional frame-based cameras, which are typically limited to a maximum of about\n100-145 FPS. We make use of the high temporal resolution by accumulating events over a 10 ms period to\ngenerate equivalent frames at 100 FPS. This approach allows us to significantly reduce the sampling time while\npreserving the temporal information inherent in the events. By doing so, we mitigate the risk of under-sampling\nand ensure that relevant information is maintained within the frames, all while preserving the asynchronous\nnature of the events. Additionally, this approach allows us to generate more frames for training (over 600\nframes per event file) as compared to a few frames when accumulating based on a fixed size window. Sampling\nat 10 ms also aided in generation of frames captured during very rapid movements such as a half/almost full\nblink periods which may otherwise be lost in traditional imaging."}, {"title": "3.2 Pupil Tracking with Yolo", "content": "Data Preparation: Following the method for event representations, we generated frames from the EV-Eye\nDataset for training with YOLOv8. EV-Eye [Zhao et al., 2024] consist of event data collected from 48 par-\nticipants. Each participant participates in four sessions; fixation, saccades, and the last two session capturing\nsmooth pursuit movement. We utilise the raw unprocessed data and generate 20 random frames from each\nparticipant from both the left and right eye which allows us to generate a diverse range of frames from differ-\nent eye movements for the resulting dataset. We select 38 subjects for training while reserving the remaining\n10 for validation and testing. Subsequently, we generate 2400 frames from the training set and 400 each for\nthe validation and test sets forming a total of 3200 frames. We then manually label these images with Labe-\nlImg [Tzutalin, 2015] annotation tool with resulting bounding boxes in YOLOv8 format for training. Fig 1\nillustrates the process of manual annotation.\nTraining: YOLOv8 [Jocher et al., 2023] offers\nenhanced accuracy and speed, making it suitable for\nreal-time applications. YOLOv8 utilizes a novel ar-\nchitecture that builds upon the success of previous\nYOLO versions while introducing several key im-\nprovements. The backbone of YOLOv8 is a modified\nversion of the CSPDarknet53 feature extractor, which\nemploys cross-stage partial connections to enhance\ninformation flow between layers. Instead of the tra-\nditional Feature Pyramid Network (FPN), YOLOv8\nutilizes a C2f module to combine high-level semantic\nfeatures. C2f represents two 3 \u00d7 3 convolutions with\na residual connection which accepts outputs from\nthe bottleneck and concatenates it. The bottleneck\nis the same as in YOLOv5 with changes made to the size of the first convolutional layer from 1\u00d71 to\n3 \u00d7 3 [Sohan et al., 2024]. The head of the network consists of multiple detection heads, each responsible for\npredicting bounding boxes, class probabilities, and objectness scores at different scales. YOLOv8 also incorpo-\nrates a self-attention mechanism in the head to focus on relevant features and adjust their importance based on\nthe task. In our study, we utilised the event frames generated to train YOLOv8. The dataset, comprising 3200\naccumulated frames, was divided into a training, validation and test set with a ratio of 70:15:15 respectively.\nThe training process was implemented in PyTorch and trained on an NVIDIA GeForce RTX 2080 Ti GPU.\nAn AdamW optimizer was utilised in both models to achieve the highest performance with a learning rate of\n1 \u00d7 10-3 and a weight decay of 1 \u00d710\u22123."}, {"title": "4 Experiments and Results", "content": "Table 1 presents the performance metrics of four different variants of the YOLOv8 model (YOLOv8n, YOLOv8s,\nYOLOv8m, and YOLOv81) on the task of pupil tracking using the EV-EYE data. The models are evaluated\nbased on mean average precision (mAP), precision, recall, F1 score, and the number of parameters (in millions).\nIn terms of mean average precision (mAP), which measures the accuracy and completeness of the model's\npredictions by calculating the average precision across all classes, YOLOv8n achieved the highest score of\n0.981, indicating its superior performance in accurately identifying the pupil. The other models, YOLOv8s,\nYOLOv8m, and YOLOv81, achieved slightly lower mAP scores, all around 0.975 to 0.976. Precision, which\nmeasures the accuracy of the positive predictions, was also highest for YOLOv8n at 0.965, signifying the lowest"}, {"title": "5 Limitations", "content": "Our current model faces challenges when tested on remote datasets, which is not surprising given that we trained\nthe models using a near-eye dataset. Although our focus is on applications in both near and remote eye sensing,\nwe did not have a sufficient remote-eye dataset for training. As a result, the model's performance in identifying\npupils in the presence of occlusions is less robust, and this may lead to significant errors. To address this issue,\nwe plan to focus on acquiring and incorporating comprehensive remote eye datasets for further experiments,\nand we also intend to incorporate Region-of-Interest (ROI) techniques for directly extracting eye features to\nensure robustness in all conditions and even in the presence of occlusions. This addition is expected to enhance\nthe accuracy and robustness of our pupil tracking pipeline, paving the way for a saccade detection algorithm that\ncan provide more reliable and precise outcomes. However, it is important to note that the model still performs\nfairly well in terms of pupil localization. We show this in the results presented in supplementary materials.\nAdditionally, while the YOLOv8n model performed exceptionally well in terms of mean average precision\n(mAP) and precision, its recall was slightly lower compared to the other models. Specifically, the YOLOv8n\nhad a recall of 0.919, which is lower than the recall of 0.938 for YOLOv81, and slightly below the recall values"}, {"title": "6 Conclusion", "content": "In this study, we demonstrated pupil tracking in event cameras using a frame-based representation of events.\nBy converting data from Event Cameras into a format understood by standard deep learning algorithms, we\novercame issues such as under-sampling by generating events with duration of 10ms (100 fps) and highlighted\nthe potential of the high frame rate offered by ECs. The presented results affirm the reliability and applicability\nof event camera imaging in pupil tracking. Our findings and evaluation of various iterations of YOLOv8\nhighlights how deep learning algorithms can be adopted and transferred to dense representations such as event\nframes. ECs have demonstrated promising potential for object detection, especially in scenarios involving rapid\nmotion and have the potential for the detection of high-speed changes. The use of ECs for pupil tracking has\noffered opportunities for high speed tracking in applications in XR, VR, Human Computer Interactions, e.t.c.\nMoving forward, the integration of ECs in pupil tracking presents exciting prospects for other areas in-\nvolving analyzing eye movement. For instance, saccade detection which is essential for understanding human\ncognition and improving visual diagnostics of neurological conditions. The potential of such applications can\nlead to new avenues and breakthroughs in non-invasive diagnosis of neurological conditions such as Dementia\nand Parkinson's disease with further research and development required."}]}