{"title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs", "authors": ["Md. Arid Hasan", "Maram Hasanain", "Fatema Ahmad", "Sahinur Rahman Laskar", "Sunaya Upadhyay", "Vrunda N Sukhadia", "Mucahid Kutlu", "Shammur Absar Chowdhury", "Firoj Alam"], "abstract": "Natural Question Answering (QA) datasets\nplay a crucial role in developing and evaluat-\ning the capabilities of large language models\n(LLMs), ensuring their effective usage in real-\nworld applications. Despite the numerous QA\ndatasets that have been developed, there is a\nnotable lack of region-specific datasets gener-\nated by native users in their own languages.\nThis gap hinders the effective benchmarking\nof LLMs for regional and cultural specifici-\nties. In this study, we propose a scalable frame-\nwork, NativQA, to seamlessly construct cul-\nturally and regionally aligned QA datasets in\nnative languages, for LLM evaluation and tun-\ning. Moreover, to demonstrate the efficacy of\nthe proposed framework, we designed a multi-\nlingual natural QA dataset, MultiNativQA, con-\nsisting of ~72K QA pairs in seven languages,\nranging from high to extremely low resource,\nbased on queries from native speakers covering\n18 topics. We benchmark the MultiNativQA\ndataset with open- and closed-source LLMs.\nWe made both the framework NativQA and\nMultiNativQA dataset publicly available for the\ncommunity.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models\n(LLMs) have revolutionized the landscape of artifi-\ncial intelligence, significantly pushing the state-of-\nthe-art for a broad array of Natural Language Pro-\ncessing (NLP) and Speech Processing tasks, such\nas machine translation, question answering, auto-\nmatic speech recognition, text-to-speech generation\namong others. Their potential in language under-\nstanding and generation, across multiple (high- and\nlow-resourced) languages has attracted researchers\nto benchmark the LLM capabilities across diverse\ntasks, domains, and disciplines. Moreover, the rapid in-\ntegration of LLMs with various applications ne-\ncessitates measuring cultural discrepancies in the\nresponses generated by LLMs to ensure alignment\nwith users' cultural values and contexts. Evaluat-\ning the generalization capabilities of LLMs across\ndifferent tasks and languages has recently garnered\nsignificant attention. The HELM project assessed English LLMs across vari-\nous metrics and scenarios. BIG-Bench introduced a large-scale evaluation\nwith 214 tasks, including low-resource languages.\nRecently, GPT-2.5, Chat-\nGPT, and BLOOM were evaluated by Bang et al. Such evaluations have been conducted on stan-\ndard QA, NLP, and/or speech datasets. LLM de-\nvelopers measure different capabilities of their\nreleased models, such as common sense reason-\ning (e.g., HellaSwag), world\nknowledge (e.g., MMLU and Natural Questions, TriviaQA), and reading compre-\nhension (e.g., SQUAD)."}, {"title": "2 NativQA Framework", "content": "Figure 3 presents the NativQA framework consist-\ning of three inter-connected modules \u2013 Query Col-\nlection, QA Collection and QA Validation."}, {"title": "2.1 Query Collection (QC)", "content": "The objective of this module is to gather open-\nended queries focusing on various predetermined\ntopics derived from common concepts in everyday\ncommunication. We believe that the set of topics\nshould be manually constructed, as this step re-"}, {"title": "2.2 QA Collection (QAC)", "content": "The next step is to collect QA pairs that potentially\ncover topics represented by Qo, using a search en-\ngine, e.g., Google. We specifically select Google,\nsince when a query is issued against it, it can return\na data structure called \"People also ask\" that lists\nfew questions asked by real users and potentially\nrelevant to the initial user query, as shown in Fig-\nure 4. Moreover, the questions are associated with\nanswers extracted by the search engine and links to\nthe answers sources.\nOur QA curation module implements Algo-\nrithm 1, using the seed queries Qo along with the\nnumber of iteration, Niter, as input. For each it-\neration i \u2208 Niter, we collect QA pairs PA, and\nrelated queries Srel for each query, q \u2208 Q, and\nthen pass it to the filtering module and update the\ncurrent query set Q. We repeat the process for all\nthe iterations to obtain the final QA set, SQA for\nenriched queries Q."}, {"title": "2.3 QA Validation (QAV)", "content": "The last step of the NativQA framework is to vali-\ndate the QA pairs, considering at least two aspects:\n(i) the quality and answerability of questions, and\n(ii) reliability and completeness of answers. We\nvalidate the QA pairs through the following steps.\nDomain Reliability Checking (DRC). The an-\nswers collected by our approach include a link to\nthe web page from which an answer was extracted.\nThus, our answer validation step starts by a semi-\nsupervised approach that aims to keep QA pairs\nbased on the reliability of the Web domain where\nthe answer appears. We hypothesize that answers\nfrom web pages of a reliable domain are likely to\nbe trustworthy. In our approach, unique domains\nfor the QA pairs in SQA are listed. Next, annotators\nmanually annotate each domain by reliability based\non an annotation guideline we designed for this\ntask, inspired by several relevant studies. We then only keep the QA\npairs with answers from reliable sources.\nFor developing large-scale fine- and instruction-\ntuning data, this approach is practical and scalable\nbecause it reduces manual effort required to ob-"}, {"title": "3 MultiNativQA Dataset", "content": "Our MultiNativQA dataset encompasses 7 lan-\nguages, ranging from high- to extremely low-\nresource on 7 different location/cities, covering\n18 predetermined topics (see Table 1 for details).\nMultiNativQA captures linguistic diversity, by in-\ncluding several dialects for dialect-rich languages\nlike Arabic. We also added two linguistic vari-\nations of Bangla to reflect differences between\nspeakers in Bangladesh and West Bengal, India.\nFurthermore, we opted to cover English queries\nfrom Dhaka and Doha, where English is commonly\nused as a second language."}, {"title": "3.1 Implementing NativQA Framework", "content": "Query Collection: For multilingual query collec-\ntion, we started with various predetermined topics\n(see Table 2) derived from common concepts in\neveryday lives of users (more details are discussed\nin Section A.1). Next, we asked the native speak-\ners to write 10 to 50 queries per topic focusing on\nissues they encounter in their major cities and then"}, {"title": "3.2 Annotation Guidelines", "content": ""}, {"title": "3.2.1 Domain Reliability", "content": "The objective for the domain reliability annotation\ntask is to verify the credibility of the source do-\nmain, which can be used to judge the factuality and\nreliability of answers sourced from that domain.\nWe adopt the following definition of the credibil-\nity of the domain/website: \u201cA credible webpage\nis one whose information one can accept as the\ntruth without needing to look elsewhere. If one\ncan accept information on a page as true at face\nvalue, then the page is credible; if one needs to go\nelsewhere to check the validity of the information\non the page, then it is less credible\u201d.\nAnnotators were tasked to review each web do-"}, {"title": "3.2.2 QA Annotation", "content": "This phase of the NativQA framework involves\nthree types of annotations. Below, we discuss the\nguidelines for each type.\n1. Question selection: The purpose of this task\nis to evaluate the quality of the questions. The\nannotators assessed whether the questions are fac-\ntual or meet the criteria discussed below. We de-\nfined two types of questions inspired by the NQ\ndataset.\n\u2022 Good question: A good question is a fact-\nseeking question that can be answered with an\nentity or explanation.\n\u2022 Bad question: A question is considered a bad\nquestion if it meets any of the following criteria:\nAmbiguous or based on a false presupposition,\nmaking it incomprehensible.\nOpinion-seeking, such as \u201cCan you give me\nyour thoughts on...?\u201d\nDoes not ask for factual information.\nBased on whether a question is marked as good\nor bad, the annotator's subsequent tasks will vary.\nIf a question is marked as good, the annotator will\nreview the answer, its source page, and perform\nanswer categorization tasks. Otherwise, further\nannotation is skipped, and the annotator proceeds\nto the next QA pair.\n2. Answer categorization: An answer can be\ncategorized into one of these categories: (i) correct\nanswer, (ii) partially correct answer, and (iii) in-\ncorrect answer, and (iv) the answer can't be found\nin the source page. Complete definition for each\ncategory is provided in Section A.3.\n3. Answer editing: The purpose of this step is to\nensure that the answer accurately responds to the\nquestion and is correct, fluent, and informative. If\nthe answer was incomplete, annotators are required"}, {"title": "3.3 Annotation Task Setup", "content": "The annotation team consists of native speakers of\nthe respective languages, who worked on the en-\ntire process starting from query collection to QA\npair annotation. The annotators have diverse edu-\ncational backgrounds, ranging from undergraduate\nstudents to those holding graduate and PhD de-\ngrees. The team was trained and monitored by an\nin-house expert annotator. To ensure quality, pe-\nriodic checks of random annotation samples were\nconducted, and feedback was provided. Depending\non the availability of the annotators for a language,\nwe opted to go for one to three annotators for the\ndomain reliability task. For the DRC task, three\nannotators were assigned for the annotation. When\nmultiple annotators label a domain, the majority\nlabel is used as its final label. For other languages,\ndomains were automatically matched with those\nalready identified as reliable by annotators. For\nthe QAA task, each QA pair was annotated by two\nannotators."}, {"title": "3.4 Annotation Platform", "content": "We utilized our in-house annotation platform for\nthe annotation task. Separate annotation interfaces\n(as presented in Section B) were designed for each\nphase and each language. To facilitate the annota-\ntion process, the annotation interface included the\nannotation guidelines throughout the phases."}, {"title": "3.5 Annotation Agreement", "content": "To evaluate the reliability of manual annotations,\nwe computed the Inter-Annotator Agreement (IAA)\nusing a Fleiss' Kappa coefficient (K) for the domain\nreliability task for Arabic, Bangla, and English.\nThe Kappa (K) values for these languages are 0.53,\n0.66, and 0.37, respectively, which correspond to\nfair to substantial agreement according to Landis\nand Koch's scale. Note\nthat we selected the final label where the majority\nagreed, meaning that we have above 66% agree-\nment on the final label.\nFor the QA annotation task, we first directly se-\nlect only the questions where both annotators agree.\nFor the disagreed cases, another annotator revises\nthem; ultimately, we select based on the agreement\nof at least two annotators. For the answer editing,\n75.79% (Bangla) to 88.5% (Arabic) of the cases"}, {"title": "3.6 Statistics and Analysis", "content": "In Figure 1, we report the initial collection of data\ndistribution across languages, irrespective of the\ncountry they were collected from. English, Ara-\nbic, and Bangla are higher in proportion due to\nthe fact that (i) English consists of data collected\nfrom Qatar and Bangladesh, (ii) Arabic consists\nof queries from different dialects, and (iii) Bangla\nconsists of data from Bangladesh and India. As\ntable 1 shows, our annotation process resulted in\na decrease in QA set size by half (comparing ini-\ntial QA set (column #QA) to final QA set (column\nF.QA)). We also faced a significant drop for As-\nsamese and Nepali. This drop is due to the fact that\nthe search engine returned QA pairs in non-native\nlanguages (in these cases, either Hindi or English)\nrather than the native language. As part of our pro-\ncess, we filtered out QA pairs that are not in the\ntarget language. We identify the native language\nusing a language detection tool and then manually\nrevise them.\nIn Figure 10 and 11 (in Appendix), we report\ntopic wise distribution for all languages and regions.\nIt appears that we have a very good coverage of\ntopics for all languages."}, {"title": "4 Experimental Setup", "content": "To establish baselines over MultiNativQA, we\nbenchmark LLMs performance in the QA task over\nit. Our experiments setup is described next.\nData Splits. The dataset for each language is split\ninto training, development, and test sets using strat-\nified sampling, considering topics as labels, with"}, {"title": "5 Results", "content": "Open vs Close LLMs Table 4 shows the com-\nplete results for each LLM and language. We\nobserve that closed models, especially GPT-40,\nclearly outperform all other models across major-ity of languages with an average BLEU score of\n0.278. This performance is then followed by GPT-4\nand Gemini, with BLEU scores of 0.258 and 0.223,\nrespectively. Results also show that in terms of\nopen models, considering BLEU scores, Mistral\nis outperforming LLama3 in three languages such\nas Arabic, Assamese, and Bangla (BD, IN) and\nunderperforming for the rest.\nHigh- vs Low-resource Languages We also look\nat the the average performance of models per lan-\nguage (Reporting average BLEU scores in Figure\n5). The average performance over English sur-\npasses that over other languages, which is expected\ngiven that English is the highest resource language\namong those we consider. Medium-resource lan-\nguages such as Arabic, Turkish, and Hindi rank\njust below English in terms of performance. As-\nsamese, categorized as an extremely low-resource\nlanguage, exhibits very poor performance. Overall,\nthe representation and/or richness of digital content\nfor a language is reflected in the performance of\nthe models."}, {"title": "6 Related Work", "content": "LLMs have consistently showcased impressive ca-\npabilities spanning diverse disciplines and tasks\nand there have been efforts to evaluate the perfor-\nmance of LLMs on standard NLP tasks"}, {"title": "7 Conclusions", "content": "In this study, we propose the NativQA frame-\nwork, which enables constructing culturally and\nregionally-aligned natural QA datasets. Resulting\ndatasets can aid in training/fine-tuning and eval-\nuating LLMs over native and culturally-aligned\nreal users information needs and tasks. The pro-\nposed framework is scalable and reduces human\ninvolvement in a dataset construction by automat-\ning several processes. We show the efficacy of the\nNativQA framework, by designing and developing\na multilingual native QA dataset, MultiNativQA.\nWe further enrich our study by benchmarking QA\nperformance of five open and closed LLMs over\nMultiNativQA. Our results demonstrate that the\nlatest closed model, GPT-4o, shows superior per-\nformance in general across languages. Our study\nis an ongoing effort; therefore, we aim to extend\nthe framework to include more languages and im-\nplement more measures to improve the quality of\nboth the framework and the dataset. In future, the\ndataset will be used to tune LLMs to improve cul-\ntural and regional alignment."}, {"title": "8 Limitations", "content": "While the proposed framework enables the devel-\nopment of datasets with cultural and native infor-\nmation, it currently has several limitations. Firstly,\nthe NativQA framework still relies on human-in-\nthe-loop processes, from seed query creation to\nmanual revision of QA pairs. This dependency\nlimits large-scale data collection. Although we\nconsider the human-in-the-loop setting a limitation,\nwe also note that ensuring a high-quality dataset\nwithout it would be challenging. Secondly, the\nsemi-supervised approach, based on domain re-\nliability (DRC), to dataset development is a rea-\nsonable starting point; however, full supervision\n(QAA) would ensure higher quality. Thirdly, in our\ncurrent study, we relied on one search engine. This\ncan be extended to include other search engines and\nuse a mixture of engines to enrich QA pair collec-\ntion. Fourth, due to resource limitations, including\nthe availability of language-specific annotators, we\nhave successfully annotated QA pairs for the test\nsets. Annotation for the development and training\nsets would be a part of our ongoing efforts. Finally,\nour study is currently limited to benchmarking var-\nious open and closed models. Future research will\nfocus on fine-tuning and training new models."}, {"title": "Ethics and Broader Impact", "content": "The proposed NativQA framework does not involve\ncollecting any personally identifiable information.\nAdditionally, the proposed dataset does not include\nany information that can offend or harm any in-\ndividual, entity, organization, or society. There-\nfore, we do not foresee any issues that may lead\nto potential risks. Human annotators were paid\nthrough external companies at standard payment\nrates applicable to their region. Information about\nhuman annotators is not part of the dataset, and\ntheir identities remain confidential. The proposed\nframework and dataset will be released publicly for\nnon-commercial research purposes. Therefore, we\nstrongly believe that they will be beneficial for the\nresearch community."}, {"title": "A Query on Search Engine", "content": "In Figure 6, we show an example of a query to a\nsearch engine, that demonstrates related queries\nunder \"People also ask\", which we have also con-\nsidered as queries in the several iterations of QA\npair collection."}, {"title": "A.1 Guideline for Collecting Seed Queries)", "content": "The purpose of this study is to collect natural ba-\nsic Question Answer (QA) pairs to evaluate and"}, {"title": "A.2 Domain Reliability (Detail Annotation Guideline)", "content": "General Characteristics Below are the charac-\nteristics that we have considered as criteria for a\ndomain to be more reliable:\nOverall Design:\n\u2022 The domain has a professional, polished, and\nattractive design. It has interactive features,\nis well organized, easy to navigate, loads fast,\nand has good response speed.\n\u2022 There are no errors or broken links.\n\u2022 It might have paid access to information.\n\u2022 The domain name suffix is considered trust-\nworthy (e.g., \".gov\").\n\u2022 Absence/limited advertising. If advertising\nis present, they are good quality ads for rep-\nutable and decent products and organizations.\n\u2022 The domain might be sponsored by or shows\nlinks to reputable organizations.\n\u2022 Presence of privacy and security policies sec-\ntion or page. Presence of an About page, con-\ntact info, and address.\n\u2022 If videos, images, and graphics are used on\nthe website, they are high-quality and profes-\nsional.\nContent Quality:\n\u2022 Author/entity names, qualifications, creden-\ntials, and contact information are present, and\nthey are relevant to the topic of the content.\n\u2022 Author/entity is reputable."}, {"title": "A.3 Definitions of Answer Categorization", "content": "Below we provide the definition of the categories\nthat are defined for the answer categorization task.\n\u2022 Correct answer: When the answer aligns with\nthe information provided by the source. Note that\nthe answer must be complete and address all parts\nof the question, but it does not need to match the\nsource webpage verbatim. The answer can be a\nlong, detailed response or a short snippet.\n\u2022 Partially correct answer: When the answer\ndoes not address all parts of the question. In this\ncase, the goal is to edit the answer using informa-tion from the source page. This involves directly\ncopying text from the source webpage. Minimal\nediting may be needed to make the answer more\ncomprehensive.\n\u2022 Incorrect answer: When the answer text does\nnot address the question. In this case, the goal\nis to edit the answer using information from the\nsource page.\n\u2022 Cannot find answer: When the answer is not\navailable on the provided link/page."}, {"title": "B Annotation Interface", "content": "In Figure 7, we present an example of domain re-\nliability checking, which consists of a URL of the\ndomain, annotation guidelines, and four different\noptions associated with the four categories we de-\nfined for this annotation task. Annotators select\none of these options and submit.\nIn Figure 8 and 9 we demonstrate the two steps\nof question selection and answer editing and cat-\negorization tasks, respectively. Depending on the\ntype of question selected, the annotator will be able\nto choose whether to edit the answer or not."}, {"title": "C Dataset: Additional Details", "content": "In Figure 10 and 11 we present the topic-wise\ndata distribution for different datasets associated\nwith various languages. Starting with the Ara-\nbic dataset, the predominant topic is names, com-\nprising 10.2% of the data, a trend that also holds\ntrue for Assamese (8.2%). For Bangla, whether\nfrom Bangladesh or India, the major topic is gen-\neral, representing 8.8% and 10.0% respectively. In\nBangladesh, religion (10.4%) is the major topic\nfor English, whereas in Qatar, general dominates\nat 26.6%. For Nepali, the leading topic is Busi-\nness (22.9%), for Hindi it is Travel (8.2%), and for\nTurkish, names is the primary topic at 8.6%."}, {"title": "D Data Release and License", "content": "The NativQA dataset will be publicly released un-\nder the Creative Commons Attribution Non Com-\nmercial Share Alike 4.0: https://creativecommons.\norg/licenses/by-nc-sa/4.0/."}]}