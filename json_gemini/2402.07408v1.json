{"title": "Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples", "authors": ["Mingrui Ma", "Lansheng Han", "Chunjie Zhou"], "abstract": "The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation strategies and artificial intelligence-based webshell detection algorithms. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection technology, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models. As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various components such as webshell hierarchical module and few-shot example to facilitate the LLM in learning and reasoning webshell escape strategies. Experimental results show that the Hybrid Prompt algorithm can work with multiple LLMs with excellent code reasoning ability to generate high-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on VIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).", "sections": [{"title": "I. INTRODUCTION", "content": "Webshell [1], as a typical example of malicious scripts exploiting injection vulnerabilities, allows hackers to remotely access and invade web servers, posing serious threats to socio- economic and network security. Webshells come in various forms, ranging from a single line of code that allows remote execution of user-provided system commands to large-scale complex script files. The code can also be written in multiple programming languages such as asp, aspx, php, jsp, pl, py, etc. Similar to the research on malware detection, webshell genera- tion and detection are non-stationary, adversarial problems [2], which have been engaged in a constant game of cat and mouse, with an escalating spiral trend. From the attacker's perspective, mainstream webshell detection tools and engines like VIRUS- TOTAL [3], WEBDIR+ and AVAST are frequently updated and maintained, incorporating the rules and characteristics of new webshells within days or even shorter periods. This forces attackers to constantly develop new webshell generation methods to bypass the detection of such engines. On the detection side, research is still in its infancy [4]. There is a lack of publicly available benchmark datasets and open-source baseline methods for webshell detection. Most models using neural networks or intelligent algorithms claim to have high accuracy and low false positives. However, the fact is that these models are basically tested on private datasets, which usually consist of only a few hundred or fewer samples, with obvious malicious features. Even the simplest multi- layer perceptron (MLP) structure can achieve high-precision detection on such datasets through overfitting. In a real cyber- attack environment, the authenticity and generalization ability of such methods are difficult to guarantee. For a limited num- ber of publicly available webshell repositories on the internet, detection engines can also achieve high-precision detection , and the superiority of artificial intelligence (AI)- based methods is not fully demonstrated. In fact, Abdelhakim et al. [4] argued that AI methods excel at extracting abstract features in webshell, which are advanced features that go beyond lexical, syntactical, and semantical features. These advanced features help reveal hidden aspects in webshells that cannot be detected through syntax and semantic analysis. However, unlike the research on malware adversarial sample generation [2], [5]-[7], research on webshell escape sample generation is still a blank field, which is due to the fact that the existing webshell bypass strategies are numerous and complicated, and there is no specific systematic method to follow. Therefore, it is an urgent and highly significant work to propose a webshell escape sample generation algorithm and construct a corresponding webshell benchmark dataset. On the other hand, the blooming development of large language model (LLM) and artificial intelligence generated content (AIGC) technologies [8] has already made an indelible impact in various domains such as chat and image generation [9]. As the latest achievement in the field of natural language processing (NLP), LLM has taken a significant lead over earlier neural network structures (i.e. LSTM [10], GRU [11], etc.) in contextual reasoning and semantic understanding capa- bilities. The widespread application of LLM in various code- related tasks (i.e. code generation [12], penetration testing [13], vulnerability detection [14], automated program repair [15], LLM fuzz tuning [16], vulnerability repair [17]) has fully showcased its excellent code reasoning abilities, making it possible to utilize LLM for generating webshell escape samples. Prompt engineering [18] plays a crucial role in the vertical research application of LLM, which aims to explore better ways of human interaction with LLMs to fully leverage their performance potential. Although the research related to prompt engineering is controversial, scholars opposing it argue that prompt engineering is too \"mystical\" and exacerbates the lack of interpretability of neural network models. However, it is undeniable that many key techniques in prompt engineering, such as Chain of Thoughts (CoT) [19], Tree of Thoughts (ToT) [20], Zero-Shot CoT [21], etc., have improved the reasoning abilities of LLMs. In addition, the application of techniques like Language Model Analysis (LAMA) probes [22] have been gradually enhancing the interpretability of the models. Novel studies in prompt engineering, such as prompt finetuning, have been able to fine-tune the parameters in LLM, thus simplifying the traditional fine-tune process [23]. Moreover, AIGC technology is so \"creative\", that just a simple prompt can make LLM produce a 0-Day webshell. Therefore, in this work, we explore the unexplored research area of AIGC-enabled webshell escape sample generation strategies. We propose Hybrid Prompt, a hierarchical and modular prompt generation algorithm, and apply it to different LLM models to generate multiple webshell samples with high escape capabilities. Experimental results demonstrate that the escape samples generated by the Hybrid Prompt algorithm + LLM model can bypass detection by mainstream detection engines with high Escape Rate (ER) and Survival Rate (SR). The main contributions of this paper are summarized as follows:\n\u2022 We propose Hybrid Prompt algorithm, which combines the advantages of multiple prompt schemes such as ToT [20], few-shot prompting, CoT, etc. By synthesizing key features related to webshell escape and designing prompt strategies tailored to different sizes of webshells, the algorithm effectively enhances the code reasoning ability of LLM models and generates high-quality webshell escape samples.\n\u2022 We construct a webshell benchmark dataset generated by the Hybrid Prompt algorithm. This dataset achieves high ER and SR among mainstream detection engines and reflects the performance of rule-based detection engines more realistically and effectively. It has a high reference value to the research in the field of webshell detection.\n\u2022 We investigate and compare the quality of escape samples generated by different LLM models using the Hybrid Prompt algorithm. All these samples exhibit high ER, surpassing webshell samples generated by other intelli- gent algorithms (e.g. genetic algorithm [24]).\n\u2022 Our study confirms the possibility of applying LLM techniques in the webshell domain and inspires future research in the field of LLM-based webshell detection.\nWe believe that the incorporation of LLM and prompt engineering techniques can inject fresh blood into the research of webshell generation and detection."}, {"title": "II. PRELIMINARY", "content": "With the development of AIGC and LLM technologies, there are numerous LLM models in different subfields with different focuses. For example, GLM [25] and GLM2 mod- els tend to prioritize open-source and lightweight to meet the deployment needs of personal terminals. DALLE [26] focuses on AI image generation, while FATE-LLM [27] is biased towards application scenarios under the federal learning paradigm. Hybrid Prompt performs exceptionally well on LLM models with strong code reasoning abilities. We have tried to apply basic prompts on Chatglm-6B, Chatglm2-6B, Chatglm-13B, and Chatglm2-13B models, but the performance is unsatisfactory, as shown in Figure 3."}, {"title": "III. ALGORITHM DESIGN", "content": "A. Overall Workflow\nThe overall flow from collecting multi-source webshell scripts to generating webshell escape samples is shown in Figure 4.\nThe principle and operation process of each part will be expanded in detail later.\nB. Data Filtering\nTo facilitate the implementation of the Hybrid Prompt algorithm, we need to construct the Template webshell dataset. Since webshell scripts collected from multiple sources are di- verse in types and have confusing names (i.e. China Chopper, b374k, etc.), and the Template webshell dataset requires clean and well-characterized webshell scripts, we perform triple data filtering process on Multi-source webshell scripts, as shown in Figure 5.\nIn the first filtering step, we calculate the MD5 hash value of all scripts to filter out webshell scripts with consistent content but confusing names. The filtered scripts are then renamed using their corresponding hash values. In the second filtering step, we convert the webshell scripts into Abstract Syntax Tree (AST) structures to filter out the scripts with the same syntax structure. For php scripts, we use \"php-ast\" to perform the translation (ast\\parse_code) and add the nameKind attribute to the nodes. We process the child nodes belonging to the array and AST separately. The pseudocode and example for this step are shown in Algorithm 1 and Figure 6.\nIn the third filtering step, the Vulcan Logic Disassembler (VLD) module in Zend disassembler is used to disassemble the scripts into opcode structures, aiming to filter out webshell scripts with consistent execution sequences. A typical example is shown in Figure 7.\nC. Hybrid Prompt\nThe ToT method has significant performance advantages over CoT, Self Consistency (SC) [28] method in solving complex reasoning problems by searching for multiple solu- tion paths, using strategies such as backtracking and pruning, similar to human thinking rather than the traditional auto- regressive mechanism of making token level decisions one by one in a left-to-right manner. This allows it to better handle heuristic problems like genuine problems. Therefore, we also leverage and innovate this process paradigm in the complex reasoning task of webshell escape sample generation. The overall flowchart of the Hybrid Prompt algorithm is illustrated in Figure 8.\nBefore proceeding, let's first formalize some relevant sym- bols. We use $M$ to denote LLM, $o$ to denote one of the candidates generated by each thought of Hybrid Prompt, $O$ to denote the set composed of candidates, $a$ to denote the original input of Hybrid Prompt, $F_e$ to denote the few-shot example, $N$ to denote the tree depth of Hybrid Prompt and $p$ to denote the number of candidates.\n1) Thought Decomposition:\nToT argues that a suitable thought should be able to generate promising and diverse samples, facilitating LLM in assessing its problem-solving prospects. However, compared to tasks with clear rules such as Game of 24, 5*5 Crosswords, etc., the thought search space for webshell escape sample generation is broader and more challenging.\nTo address this, we have developed a webshell escape sample generation whitepaper by taking into account the characteristics of webshell escape samples. The whitepaper consists of numerous escape keywords representing different escape methods. We refer to each keyword as a module, and some modules further have secondary and tertiary modules. This hierarchical structure of modules constitutes a forest structure, in which each primary module is the root node of the tree in the forest. This modular design concept has strong scalability, allowing for the real-time addition of modules to increase the number of escape methods for the Hybrid Prompt algorithm, as shown in Figure 9."}, {"title": "Therefore, $G(M, o) = M(F_e, o)$.", "content": "Each node in the $F_e$ chain includes the original webshell sample, as well as the webshell sample processed by the corresponding module, and a brief description explaining the processing method and core ideas of the module. When filtering the $F_e$ chain, we follow the following principles:\n\u2022 The structure of the example webshell code should be as simple as possible.\n\u2022 Each node contains, as far as possible, only the processing methods corresponding to that module.\nThe purpose is to reduce the difficulty of LLM in learning the corresponding method through an example that is as simple as possible and contains the core idea. The descriptive expla- nation further enhances the interpretability of the solutions. This idea is also in line with the logical process of human learning and cognition, e.g., \"from shallow to deep,\" to help LLM better learn the features of the methods.\n$F_e$ can essentially \"modify\" the LLM's thinking direction to a certain extent so that the LLM can be generated in a Few-shot CoT mindset. In most cases, each $F_e$ chain contains multiple $F_e$ examples to provide more comprehensive coverage of different scenarios. In this case, multiple nodes are used as input prompt components for the current iteration round, to help LLM better learn multiple segmented strategies. Due to the large search space and sample diversity for each module, this Few-shot CoT method yields better results.\nMeanwhile, based on the input webshell size, we design 2 different generation approaches. For small webshells, we in- clude $p$ candidate webshell samples in a single conversation re- turned by the LLM. In this case, the average maximum length of each candidate webshell sample $L(Avg\\_Candidate)$ is calculated as (1):\n$L(Avg\\_Candidate) = (L(MaxToken) \u2013 L(InputPrompt))/p$. (1)\nWhere $L(MaxToken)$ denotes the maximum context length that the current LLM model can receive, and $L(InputPrompt)$ denotes the length of the input prompt in the current thought. Since small webshells are generally shorter, this approach can save the consumption of LLM's token resources, and enable LLM to generate more diverse samples in the returned message of a single conversation through specific \"key prompts\".\nFor large webshells, we enable the $p$ parameter function to generate $p$ candidate webshell samples by receiving mul- tiple return messages from LLM. In this case, the maximum length of each candidate webshell sample $L(Candidatei)$ is calculated as (2):\n$L(Candidatei) = L(MaxToken)-L(InputPrompt)-L(Descriptioni)$ (2)\nWhere $Description_i$ represents the brief description gener- ated by LLM for the ith candidate webshell sample, which is used to summarize the idea of candidate webshell generation and facilitate the subsequent voting process. This approach maximizes the length of the generated candidate webshell sample at the expense of consuming more token resources.\n3) State Evaluator $V(M,O)$:\nCorresponding to Thought Generator, State Evaluator is also designed to have 2 different voting methods for large and small webshells. For small webshells, Hybrid Prompt uses LLM to vote on multiple intermediate webshell samples (states) and filter out the optimal ones. The reason for voting on multiple samples instead of voting on solutions is as follows:\n\u2022 Since the Thought Generator operates in a few-shot CoT mindset, webshell samples help LLM evaluate and assess the differences between generated examples more intuitively to make optimal judgments.\n\u2022 Voting directly on the samples can preserve all the original information of the candidate webshells.\nIn this case, $L(Generator(Input + Output))= L(Evaluator(Input + Output))$<$L(MaxToken)$. Because both contain $F_e$, the webshell contents of $p$ candidates, and additional prompt information.\nFor large webshells, it is not feasible to directly input the webshell contents of $p$ candidates into LLM because $p \u00d7 (L(Candidatei)) + L(F_echain) + L(Additional Prompt)>L(MaxToken)$. Therefore, we use $Description_i$ instead of $Candidate_i$ as the input component of the vote procedure. This kind of information compression idea will inevitably lose the original code information.\nRegardless of the vote idea, for $V(M,O)$, where $O = {o_1, o_2, ..., o_p}, V(M,o_i) = 1$ is considered a good state, when $o_i \u223c Mvote(o|O)$. For Hybrid Prompt, the evaluation of a good state is to synthesize both the confusion level of the intermediate results generated by LLM for a module and the distance between them and the $Fes$. By allowing LLM to pursue local optimal solutions at each step of sample generation, this \"greedy\" idea makes it easier for the LLM to approximate the global optimal solution for the heuristic problem of escape sample generation.\n4) Search Algorithm:\nFor the Hybrid Prompt method, the depth of the tree $N$ corresponds to the number of modules (including the complete module structure such as primary and secondary modules). The DFS strategy leads to an excessive state space of LLM during the backtracking and pruning stages, which reduces the efficiency of the algorithm operation. Therefore, we consider using the BFS search algorithm. The pseudocode of the corresponding Hybrid Prompt-BFS algorithm is shown in Algorithm 2.\nTaking into account performance and efficiency considera- tions, for the escape sample generation task, we set the number of candidates $p$ to 1. The final output of the webshell escape sample is the candidate that wins in the vote process at the $N$th layer.\n5) Contextual Memory Range:\nSince LLM has a limited range of contextual memory, we cannot let LLM memorize the entire Hybrid Prompt context but should set its local memory range. For Hybrid Prompt itself, it is impossible to compress the history information like many NLP tasks (e.g. contextual conversations) because it would result in a significant loss of raw webshell information. For this reason, our approach is to set the Contextual Memory Range for the Hybrid Prompt, as shown in Figure 8. Contex- tual Memory Range refers to the scope of each iteration in the Hybrid Prompt algorithm. At this stage, the only contextual in- formation required for the next iteration round is the candidate"}, {"title": "Algorithm 2 Hybrid Prompt-BFS Algorithm", "content": "Require: Input $x$, Thought Generator $G(M, o)$, State Eval- uator $V(M, O)$, Tree Depth $N$, Candidate num $p$, Step Output $O_i(0 \u2264 i \u2264 N)$\n1: $O_0 = x$\n2: for $n = 1$ to $N$ do\n3:   $O_n = \\{ [o_1, o_2] | o \u2208 O_{n-1}, o_n \u2208 G(M, o) \\}$\n4:   $V_n = V(M, O_n)$\n5:   $O_n = sort(V_n, p)$\n6: end for\n7: Return $O_n$\noutput selected by the winning vote strategy in the previous iteration. Therefore, defining the Contextual Memory Range ensures the continuity of information memory throughout the complete Hybrid Prompt algorithm. Correspondingly, $O_n$, $V_n$ within the body of the \"for\" loop in the pseudocode are the local contextual contents that LLM needs to memorize.\n6) Additional Explanation:\nFor the webshell escape sample generation task, an impor- tant guiding principle is to ensure the validity of generated samples. This means that the escaped samples should not lose the attack behavior and malicious features of the original samples and can be executed correctly without any syntax or lexical errors. To achieve this, Hybrid Prompt introduces Safeguard Prompt to constrain sample generation and improve SR. In addition, common techniques in prompt engineering, such as delimiter, are also applied in the Hybrid Prompt algorithm to normalize the output of LLMs."}, {"title": "Experimental Environment", "content": "We first introduce the specific experimental environment, as shown in Table I.\nEvaluation Metrics. To better compare the quality of samples generated by different LLM models using the Hybrid Prompt algorithm, we choose two evaluation metrics: ER and SR, which are calculated as follows:\n$ER = 1 - DR = 1 - \\frac{N_{Detected\\_samples}}{N_{Total\\_samples}}$ (3)\n$SR = \\frac{N_{Malicious\\_samples}}{N_{Total\\_samples}}$ (4)\nWhere Detection Rate (DR) represents the detection ac- curacy of the detection engine, $N_{Total\\_samples}$ is the total number of samples generated by LLM under the Hybrid Prompt algorithm, $N_{Detected\\_samples}$ is the number of web- shells successfully identified by the detection engine, and $N_{Malicious\\_samples}$ is the number of samples generated by LLM under the Hybrid Prompt algorithm that still retain malicious functionality.\nModels & Detection Engines. We test the ER and SR of samples generated by Hybrid Prompt under three detection engines: Web Shell Detector, WEBDIR+ and VIRUSTOTAL respectively. By calling the VIRUSTOTAL scanning API, we test a total of more than 58 different detection engines (i.e. AVG, ClamAV, AVAST, etc.). When any of these detection engines recognize the current test sample as a webshell, we consider that the sample fails to escape from VIRUSTOTAL.\nComparative Methods. Due to the lack of relevant research in the field of webshell escape sample generation, we also include a comparison with the dataset from CWSOGG [24], an obfuscated webshell dataset generated using the genetic algorithm.\nAdditional Explanations. By default, we set the number of candidates $p$ to 3. It should be noted that to correctly calculate the SR of the samples, we set all the test sets to be labeled as"}, {"title": "B. Comparative Experiment", "content": "All LLMs are generated under the complete Hybrid Prompt algorithm to achieve optimal performance. To answer RQ1, the comparative results are shown in Table II.\nAs can be seen in Table II, the GPT-4 + Hybrid Prompt algorithm has the best comprehensive performance, leading to both ER and SR. This is due to the fact that GPT-4 is more capable of following complex instructions carefully, while Hybrid Prompt contains multiple detailed instructions with normalized constraints. GPT-3.5, on the other hand, has a higher probability of partially following complex instructions, resulting in a higher probability of generating escape samples that prioritize either ER or SR, making it difficult to balance both. It is encouraging to note that the comprehensive perfor- mance of the open-source fine-tuned LLM Code-llama-34B, is very close to that of the GPT-3.5 model, confirming the per- formance potential of the open-source models. Meanwhile, we note that the ER of webshell samples generated by the 3 LLM models + Hybrid Prompt algorithm have far exceeded those of the Original Template Dataset and the CWSOGG Dataset, which fully demonstrate the performance superiority and dom- inance of the LLM models over rule-based artificial escape strategies and the traditional intelligent algorithms (i.e., genetic algorithm). As for the detection engines, VIRUSTOTAL, due to its integration of dozens of different detection engines, has a higher overall DR compared to Web Shell Detector and WEBDIR+. However, even VIRUSTOTAL struggles with the creativity of LLMs and the uncertainty of the generated escape samples, which illustrates the limitations and drawbacks of this type of specific rule-based detection engines."}, {"title": "C. Ablation Analysis", "content": "To further validate the effectiveness of the Hybrid Prompt algorithm and address RQ2, we test the performance of sample sets generated by removing different components of Hybrid Prompt under ER and SR evaluation metrics in the GPT-3.5 model. Specifically, we refer to the complete Hybrid Prompt algorithm as Strategy 1, removing the Safeguard Prompt as Strategy 2, removing $F_e$ chain as Strategy 3, removing the vote strategy and generating only 1 sample per module as Strategy 4, and not using the Hybrid Prompt algorithm at all and letting the LLM directly generate webshell as Strategy 5. The experimental results are shown in Table III.\nFrom Table III, it can be seen that Strategy 5 has poor performance and a high probability of hallucination due to the absence of any additional prompt. Both Strategy 3 and Strategy 4 produce different degrees of performance degradation. For Strategy 3, LLM loses reference examples, leading to a higher probability of generating corrupted samples. Strategy 3 also in-directly reflects that the current LLM's code reasoning ability still relies on $F_e$ chains to achieve better task performance. For Strategy 4, LLM is unable to explore multiple reasoning paths, so the generation space and diversity of samples are limited, which leads to a lower ER. Strategy 2 has the least impact on the quality of generated escape samples. Although the probability of generating corrupted samples increases and the SR decreases due to the loss of Safeguard Prompt's nor- malization measures, the impact on the ER is not significant. However, collectively, for Strategy 2 - Strategy 5, all produce varying degrees of performance degradation compared to the complete Hybrid Prompt algorithm, fully demonstrating the effectiveness of various components of the Hybrid Prompt algorithm."}, {"title": "D. Sensitivity Analysis", "content": "We investigate the impact of the candidate number, $p$, on the SR and ER evaluation metrics of generated samples in the GPT-3.5 model. The experimental results of RQ3 are shown in Table IV.\nFrom Table IV, it can be observed that a larger number of candidates can increase the search space of LLM, which in turn enriches the diversity of generated samples, enables better selection of the optimal solution, and improves the sample ER and SR. However, the increase of $p$ will also result in a higher token consumption and, in the case of small webshells, further reduces the $L(Avg\\_Candidatei)$ for each sample.\nFigure 16 is able to visualize the \"marginal effect\" that occurs as $p$ increases (The yellow and purple folds in Figure 16 almost overlap). When $p$ exceeds 3, the performance improvement of ER and SR metrics is not obvious, which can be attributed to the fact that the search space of LLM's self-inference is approaching the local upper limit. However, it is noteworthy that the consumption of tokens exhibits an almost linear relationship with the increase in $p$, despite the limited performance gains in ER and SR metrics. Therefore, the pros and cons between evaluation metrics and resource consumption should be weighed in practical applications."}, {"title": "V. DISCUSSIONS & LIMITATIONS", "content": "1) The Hybrid Prompt algorithm currently supports a lim- ited number of webshell languages (initially only sup-"}, {"title": "VI. RELATED WORK", "content": "Due to the lack of relevant research in the field of webshell escape sample generation, we survey the technology domains associated with the Hybrid Prompt algorithm and the webshell detection task."}, {"title": "A. Prompt Engineering Algorithm.", "content": "As one of the most classic prompt algorithms, CoT [19] aims to assist LLMs in achieving complex reasoning abilities through intermediate inference steps. Zero-shot CoT [21], as a follow-up to CoT, enables LLM to perform self-reasoning through twice generation, involving 2 separate prompting pro- cesses. The second prompt is self-augmented. SC [28] serves as another complement to the CoT algorithm by sampling a diverse set of reasoning paths and marginalizing out reasoning paths to aggregate final answers. However, the algorithm is still limited by the left-to-right reasoning mechanism of LLM. Least to Most Prompting (LtM) [29], also an advancement of the CoT algorithm, decomposes a problem into a set of subproblems built upon each other and inputs the solutions of the previous sub-problem into the prompt of the next sub-problem to gradually solve each sub-problem. Generated Knowledge Approach (GKA) [30] enables LLM to generate potentially useful information related to a given question before generating the response through 2 intermediate steps: knowledge generation and knowledge integration. Diverse Verifier on Reasoning Steps (DiVeRSe) [31], on the other hand, improves the reliability of LLM answers by generating multiple reasoning paths."}, {"title": "B. The Application Of LLM In Code Related Tasks.", "content": "Since the introduction of ChatGPT, LLM has been widely applied in various subfields of code security research. Zhang et al. [32] utilized ChatGPT to generate vulnerability exploitation code. Liu et al. [33] applied GPT to the task of vulnerability description mapping and evaluation tasks. They provided cer- tain prompts to ChatGPT and extracted the required informa- tion from its responses using regular expressions."}, {"title": "VII. CONCLUSION", "content": "In this paper, we propose Hybrid Prompt, a webshell escape sample generation prompt algorithm that combines various prompt strategies such as ToT, Few-shot CoT, SC, etc. Hybrid Prompt combines structured webshell module and Fe chain, utilizes auxiliary methods to inspire LLMs to perform self-assessment and optimization, and demonstrates excellent performance on LLMs with strong code reasoning capabilities (GPT-3.5, GPT-4, Code-llama-34B), enabling the generation of high-quality webshell escape sample datasets. The Hybrid Prompt algorithm also exhibits strong scalability and generalization capability, allowing for the addition of more modules and corresponding Fe chains to update escape strategies and expand to more webshell languages. Our further work includes combining LLM fine-tuning techniques with the Hybrid Prompt algorithm to further enhance the code generation capability of LLM and designing more advanced information compression algorithms to enhance the quality of sample generation."}]}