{"title": "Novel Deep Neural Network Classifier Characterization Metrics with Applications to Dataless Evaluation", "authors": ["Nathaniel Dean", "Dilip Sarkar"], "abstract": "The mainstream AI community has seen a rise in large-scale open-source classifiers, often pre-trained on vast\ndatasets and tested on standard benchmarks; however, users facing diverse needs and limited, expensive test data\nmay be overwhelmed by available choices. Deep Neural Network (DNN) classifiers undergo training, validation, and\ntesting phases using example dataset, with the testing phase focused on determining the classification accuracy of test\nexamples without delving into the inner working of the classifier. In this work, we evaluate a DNN classifier's training\nquality without any example dataset. It is assumed that a DNN is a composition of a feature extractor and a classifier\nwhich is the penultimate completely connected layer. The quality of a classifier is estimated using its weight vectors.\nThe feature extractor is characterized using two metrics that utilize feature vectors it produces when synthetic data\nis fed as input. These synthetic input vectors are produced by backpropagating desired outputs of the classifier. Our\nempirical study of the proposed method for ResNet18, trained with CAFIR10 and CAFIR100 datasets, confirms that\ndata-less evaluation of DNN classifiers is indeed possible.", "sections": [{"title": "1 Introduction", "content": "The mainstream artificial intelligence community has experienced a surge in the availability of large-scale open-source\nclassifiers [1], catering to diverse end-users with varying use cases. Typically, these models are pre-trained on very\nlarge datasets and tested on some standard benchmark dataset, but end-users needing to select an open-source model\nmay have a different use case and be overwhelmed by the possible choices. Further, the end-user may only have a small\namount of test data that may not accurately reflect their use-case or it would be costly to setup or bolster their data\npipeline for every possible open-source model [2, 3]. We propose a dataless evaluation method to pre-screen available\nmodels using only the weights and architecture of the network to eliminate poorly trained models and allow users to\nfocus their data testing on only a few down-selected possibilities.\nOften Machine Learning (ML) models are trained, validated, and tested with example data. A significant part of the\ndata is used to estimate the parameters of a model, next validation data is used for hyperparameter and model selection,\nand finally the test data is used to evaluate performance of the selected model [4,5]. Deep Neural Networks (DNNs)\nfollow a similar method, where training examples are used to estimate parameters of a DNN. For a given example\ndataset, different model-architectures are trained and using a validation dataset a model is selected. Modern DNN\nmodels have hyper-parameters and their good values are selected using validation datasets as well. Finally, performance\nof the selected DNN (network parameters and hyper-parameters) is tested using a test dataset.\nEven when training, validation, and test datasets are coming from the same distribution or created by partitioning\na larger dataset, training accuracy is often much higher than test accuracy. For example, ResNet18 [6] trained with\nCIRAR100 [7] achieves almost 100% accuracy very quickly, but testing accuracy is between 72 to 75%. This gap"}, {"title": "1.1 Notations", "content": "Let [n] be the set {1,2,\u2026,n}. Given, $D = \\{(x_i,Y_i)|x_i \\in \\mathbb{R}^p, Y_i \\in [k] \\text{ and } i \\in [n]\\}$, a dataset of n labeled data coming\nfrom k classes $C_1$ to $C_k$. Let $D_l = \\{(x_{i,l})|(x_{i,l}) \\in D \\text{ and } l \\in [k]\\}$ be the set of data in classes $C_l$. Denoting $|D_l| = n_l$,\nwe have $n = \\sum_{l=1}^k N_i$\nWe consider multiclass classifier functions $f : \\mathbb{R}^p \\rightarrow \\mathbb{R}^k$ that maps an input $x \\in \\mathbb{R}^p$ to a class $l \\in [k]$. For our\npurposes, we assume f is a composition of a feature extraction function $g : \\mathbb{R}^p \\rightarrow \\mathbb{R}^q$ and a feature classification\nfunction $h : \\mathbb{R}^q \\rightarrow \\mathbb{R}^k$. Assume that f is differentiable and after training with a dataset D, a parametric function $f (\\cdot ;\\theta)$\nis obtained. Because f is a composition of g and h, training of $f(\\cdot ; \\theta)$ produces $g(\\cdot;\\theta_g)$ and $h(\\cdot;\\theta_h)$, where $\\theta = \\theta_g\\cup\\theta_h$\nand $\\theta_g \\cap \\theta_h = \\emptyset$.\nLet $v = g(x; \\theta_g)$ and $\\hat{y} = h(v;\\theta_h)$, where $\\hat{y}_l = h_l(v;\\theta_h)$ represents the probability $h_l(v;\\theta_h)$ belongs to class $l \\in [k]$\nsuch that $\\sum_{l=1}^k \\hat{y}_l = 1$. Correspondingly, for any given x, which possess a true label l, the classifier could generate k\npossible class assignments and is correct only when $l \\approx arg \\text{ max } (\\hat{y})$.\nUnless stated otherwise, we assume that activation functions at the output layer of the classifier $g(\\cdot;\\theta_g)$ are ReLu,\nand $h(\\cdot; \\theta_h)$ is a fully connected one layer neural network, where $W_{i,;}$, $i \\in [k]$ be the ith row of $h(\\cdot;\\theta_h)$'s weight matrix\nW. It receives input from a feature extractor $g(\\cdot;\\theta_g)$, and $h(\\cdot; \\theta_h)$'s output is using a softmax function (see Eqn.1) to\ngenerate one-hot coded outputs.\n$\\sigma_l(Wv) = \\frac{e^{(W_{l,:}v)}}{\\sum_{i=1}^k e^{(W_{i,:}v)}}$  (1)\nProblem Statement We are given a trained neural network $f(\\cdot;\\theta) = h(g(\\cdot;\\theta_g);\\theta_h)$ for the classification of input\nvectors $x \\in \\mathbb{R}^p$ to one of the k classes; that is, we are given the architecture of the network and values of the parameters"}, {"title": "2 Characteristics of the Classifier $h(\\cdot; \\theta_h)$", "content": "An optimal representation of training data minimize variance of intra-class features and maximize inter-class variance\n[13, 14]. Inspired by these ideas in these hypotheses, we define training quality metrics using cosine similarity of\nfeatures of prototypes generated from trained DNNs (see Sec 4). Let us deal with a much simpler task of defining a\nmetric for a classifier $h(\\cdot;\\theta_h)$ defined as the last completely-connected layer."}, {"title": "2.1 Training Quality of the Classifier", "content": "Let $W_{i,:}$ be the ith row of $h(\\cdot;\\theta_h)$'s weight matrix W. Recall that elements of the input vector v to $h(\\cdot; \\theta_h)$ are\nnon-negative, because they are outputs of ReLU functions."}, {"title": "Definition 1 (Well trained Classifier $h(\\cdot;\\theta_h)$)", "content": "A one-hot output classifier $h(\\cdot;\\theta_h)$ with weight matrix $W^{k \\times q}$ is well\ntrained, if for each class l there exists a feature vector $v^{(l)}$ such that\n$e^{(W_{l,:}\\cdot v^{(l)})} >> e^{(W_{i,:}\\cdot v^{(l)})} \\text{ for all } i \\neq l, \\text{ and }$  (2)\n$e^{(W_{i,:}\\cdot v^{(l)})} < 1 \\text{ for all } i \\neq 1.$  (3)\nFor correct classification, the condition 2 is sufficient. The condition 3 is a stronger requirement but enhances the\nquality of the classifier, and it leads us to the following lemma."}, {"title": "Lemma 1 (Orthogonality of Weight Vectors of $h(\\cdot;\\theta_h)$)", "content": "Weight vectors $W_{i,:}$ of a well trained $h(\\cdot;\\theta_h)$ are (almost)\northogonal.\nProof: From the condition 3 of the Def. 1 we assume, $e^{(W_{l,:}\\cdot v^{(l)})} = 1 \\text{ for all } i \\neq 1$. This implies that $(W_{i,:}\\cdot v^{(l)}) = 0$\nfor all $i \\neq l$, that is, (k \u2013 1) weight vectors $W_{i,:}$ and $i \\neq l$ are orthogonal to $v^{(l)}$.\nNow the value of $(W_{l,:}\\cdot v^{(l)})$ will determine the angle between $W_{l,:}$ and $v^{(l)}$. Because of the condition 2 of\nthe Def. 1, it is much higher than zero and a maximum of $(W_{l,:}\\cdot v^{(l)}) = ||(W_{l,:}||||v^{(l)})||$ is obtained, when they\nare parallel to each other. If maximum is reached then $W_{l,:}$ is perpendicular to all other weight vectors. When this\nmaximum is reached for all l, the weight vectors W are mutually orthogonal. Otherwise, they are almost orthogonal.\nOur empirical studies, reported in Sec. 5, found that weight vectors of ResNet18 trained with CIFAR10 and\nCIFAR100 have average angles are 89.99 and 89.99 degrees, respectively. An obvious consequence of the Lemma 1 is\nthat there exists a set of k feature vectors each of which are (almost) parallel to the corresponding weight vectors, which\nis stated as a corollary next."}, {"title": "Corollary 1 (Parallelity of $W_{l,:}$ and $v^{(l)}$)", "content": "If a classifier is well trained, for each weight vector $W_{l,:}$ there is one or\nmore feature vectors $v^{(l)}$ that are (almost) parallel to it.\nClassifier's Quality Metric, $H_w$, postulates that weight vectors of a well trained classifier are (almost) orthogonal to\neach other. A method for its estimation is defined next."}, {"title": "Definition 2 (Estimation of metric $H_w$)", "content": "An estimation of $H_w$ is calculated by consider a pair-wise combinations of\nthe weight matrix of h, and given by\n$\\hat{H}_\\omega = 1 - \\frac{2}{k(k-1)} \\sum_{j<i<l} (\\frac{W_{i,:} \\cdot W_{j,:}}{||W_{i,:}| ||W_{j,:}||})$  (4)\nA well trained classifier will have a value of $\\hat{H}_w$ very close to 1.\nNote that k weight vectors are projecting a q-dimension feature vector to a k-dimension vector space, for $q > k$. If\nthe weight vectors are orthogonal, their directions define an orthogonal basis in the k-dimension space. Because of\nthe isomorphism of the Euclidean space, once the weight vectors of classifier $h(\\cdot;\\theta_h)$ are (almost) orthogonal their\nupdating is unlikely to improve overall performance of the DNN and at that point, the focus should be to improve the\nfeature extractor's parameters.\nIn the next section, we develop a method for evaluation of training quality of the feature extractor $g(\\cdot;\\theta_g)$. It is a\nmuch harder task because feature extractors are, in general, much more complex and directly quantifying its parameters\nwith a few metrics is a near impossible task; we develop a method for indirectly evaluating the feature extractor without\ntraining or testing data."}, {"title": "3 Evaluataion of Feature Extractor $g(\\cdot;\\theta_g)$", "content": ""}, {"title": "3.1 Evaluation Metrics for the Feature Extractor $g(;\\theta_g)$", "content": "For evaluation of a feature extractor $g(\\cdot;\\theta_g)$ we use a labeled data set $D_E$ with $|D_E| = n_E$. We assume, for simplicity\nof exposition, each evaluation class has the same number of examples making $n_E =n_E/k$. Let $V_E$ be the set of feature\nvectors generated by $g(\\cdot;\\theta_g)$ from all evaluation data in $D_F$. We define two metrics for measuring performance of the\nfeature extractor $g(\\cdot;\\theta_g)$ with the evaluation data $D^l$: one for within class training quality and the other for between\nclass training quality. Within class features of a class is considered perfect, when all feature vectors for a given class are\npointing in the same direction, which is determined by computing pair-wise cosine of the angle between them. We\ndiscuss between class training quality later. Let us formally define cosine similarity."}, {"title": "Definition 3 (Cosine Similarity)", "content": "Given two vectors $v_1$ and $v_2$ of same dimension, cosine similarity\n$CosSim(v_1,v_2)) = cos(\\theta) = (v_1.v_2)/(||v_1||||v_2||)$  (5)\nComputationally, $CosSim(v_1, v_2)$ of $v_1$ and $v_2$ is the dot product of their unit vectors. We normalize all vectors\nin $V_E$ for ease of computing all-pair CosSim of the vectors in it. For a set of $n_f$ vectors in each class, we need to\ncompute $n_f (n_f - 1)/2$ CoSim since that is the number of possible pairwise combinations. Matrix multiplication\nprovides a convenient method for computing these values. Let $v_i^l$ be the ith row of the matrix $G^{(l)}$ and let $(G^{(l)})^T$ be\nthe transpose of $G^{(l)}$. The elements of the $k\\times k$ matrix $(G^{(l)} (G^{(l)})^T)$ are the $CosSim(v_{c,i}^{(l)}, v_{c,j} ^{(l)})$. Each diagonal\nelement, being dot products of a unit vector with itself, is \u2018one' and because $(G^{(l)}(G^{(l)})^T)_{i,j} =(G^{(l)} (G^{(l)})^T)_{j,i}$, we\nneed to consider only off-diagonal the upper or lower triangular part of the matrix."}, {"title": "3.1.1 Within-Class Similarity Metric", "content": "Intra-class or within-class Similarity Metric, $M_{in}$, postulates intra- or within-class features of a well trained DNN\nare very similar for a given class and their pairwise $CosSim$ values are close to one. To get an estimate within-class\nsimilarity of a DNN, estimates of all classes are combined together."}, {"title": "Definition 4 (Estimate of intra- or within-class similarity metric)", "content": "An estimate of intra- or within-class similarity\nmetric $M_{in}$ is given by\n$M_{in} = \\frac{1}{k}\\sum_{l\\in[k]} (\\frac{2}{n_l(n_l-1)}\\sum_{j<i<n} (G^{(l)} (G^{(l)})^T)_{i,j})$  (6)\nThe variance of $M_{in}$ of a well trained DNN should be very low as well. Our evaluations reported in Sec. 5 show\nthat $M_{in}$ for CIFAR10 is above 0.972 even when it is trained with 25% of the trained data and variance is also small,\nan indication that the DNN was probably trained well. But for CIFAR100 $M_{in}$ and variance are relatively higher,\nindicating that it was not trained well. We propose to use within-class similarity metric to define upper bound for test\naccuracy (see Sec. 5)."}, {"title": "3.1.2 Between-Class Separation Metric", "content": "Inter-class or between-class Separation Metric, $M_{bt}$, postulates that two prototypes of different classes should\nbe less similar and their pair-wise $CosSim$ value should be close to zero. For making $M_{bt}$ increase towards one as\nthe quality of a trained DNN increases, we define $M_{bt} =1-CosSim(v_{c,i}^{(l_1)}, v_{c,j} ^{(l_2)})$, where $v_{c,i}^{(l_1)}$ and $v_{c,j} ^{(l_2)}$ are feature\nvectors from two different classes $l_1$ and $l_2$, respectively.\nBecause every class $l \\in [k]$ has $n_f$ evaluation examples, estimation of $M_{bt}$ requires further considerations. Consider\ntwo classes $l_1$ and $l_2$, we can pick one evaluation example from class $l_1$ and compare it with $n_f$ examples in $l_2$ (recall\nthat we assumed that all classes have same number evaluation examples); repeating this process for all examples in\nthe class $l_1$, require $O((n_F)^2)$ CosSim computations. Also, because each example in class $l_1$ must be compared"}, {"title": "Definition 5 (Estimate of inter- or between-class separation metric)", "content": "An estimate of inter- or between-class separa-\ntion metric $M_{bt}$ is given by\n$M_{bt} = 1- \\frac{1}{k(k-1)} \\sum_{l \\in [k]} \\sum_{l_i \\in [k]} \\sum_{j=1}^{n_j} (H_{i,j}^{(l_i)} [F]^T)$  (7)"}, {"title": "3.2 Training Quality of Feature Extractor", "content": "When a feature classifier's weights are pairwise orthogonal, overall classification accuracy will depend on the features\nthat the feature extractor generates. If feature vectors of all testing examples of a class are similar and their standard\ndeviation is very low, the value of $M_{in}$ will be high; but features of the between-class are very dissimilar and their\nstandard deviation is high, the value of $M_{bt}$ will be low. This situation may cause classification errors, when feature\nvector of an example is too far from $M_{in}$. Our empirical observations is that during training time within-class feature\nvectors' become very close to each other faster and this closeness represents an upper bound for testing accuracy.\nAlso, we observed that it is much harder to impart training so that feature vectors of one class move far away from\nthat of other classes, which keeps the value of the metric $M_{bt}$ lower, and hence, it will act as a lower bound for the\ntesting accuracy. This hypotheis is supported by our empirical observations support.\nIn the next section, we empirically demonstrate that estimates of $M_{in}$ and $M_{bt}$ are proportional to test accuracy.\nMoreover, estimated values of $M_{in}$ and $M_{bt}$ serve as predictions of the upper- and lower-bounds of the test accuracy\nof the network."}, {"title": "3.2.1 Evaluation of Training quality of DNNs without Testing Data", "content": "The metrics defined above and methods for estimation of their values for a network requires data, which are used for\ngenerating feature vectors. Thus, estimation of within-class similarity and between-class separation metrics of a trained\nDNNs can be a routine task when testing data is available. But can we evaluate a DNN when no data is available? We\nanswer this affirmatively.\nIn the next section, we propose a method for generation of evaluation examples from a DNN itself."}, {"title": "4 Data Generation Methods for Feature Extractor Evaluation", "content": "We employ the provided DNN to synthesize or generate input data vectors, which are then utilized to evaluate the\nfeature extractor. We use the terms 'synthetic' or 'generated' interchangeably. These generated data are referred to as\nprototypes to differentiate them from the original data used in developing the DNN. The generation methods described\nnext ensure that the synthesized data is correctly classified.\nA prototype generation starts with an random input image/vector and a target output class vector. he input (image)\nundergoes a forward pass to produce an output. The output of the DNN and the target class are then used to compute\nloss, which is subsequently backpropagated to the input. The input image is updated iteratively until the loss falls below\na predetermined threshold. Since the synthesized data is generated using the DNN's parameters, it reflects the inherent\ncharacteristics of the trained DNN. Below, we outline our proposed data generation methods."}, {"title": "4.1 Generation of Prototype Datasets", "content": "To iteratively generate a prototype, we employ the loss function defined by Eqn. 8 (although this is not a limitation, as\nthe method can be used with other loss functions as well) and the update rule described in Eqn. 9, where t represents\nthe iteration number and $m_t$ denotes the current input to the network.\nLoss Function: $\\mathcal{L} = - \\sum_{j=1}^k y_{m_t,j} log(f_j (m_t))$  (8)\nUpdate Rule: $m_{t+1} \\leftarrow m_t -\\eta \\nabla_{m_t} \\mathcal{L} / || \\nabla_{m_t} \\mathcal{L} ||$  (9)\nLet us consider the one-hot encoded output vector $\\bar{y}_l$, where for a particular class I belonging to the set [k], the\nelement $y_l$ is set to 1 to indicate membership in that class, while all other elements $y_j$ for $j$ not equal to l are set to 0 to\nsignify non-membership in those classes.\nTo generate a prototype example $m_c^{(1)}$ for class l, we begin by initializing $m_c^{(1)}$ with a random vector $r \\in \\mathbb{R}^p$. We\nthen perform a forward pass with $m_c^{(1)}$ to compute the cross-entropy loss between the output $f(m_c^{(1)}; \\theta)$ and the one-hot\nencoded target vector $\\bar{y}_l$. Subsequently, we backpropagate the loss through the network and update the input vector\nusing the rule described in Eqn. 9 to obtain $m_c^{(1)}$. This iterative process continues until the observed loss falls below a\ndesired (small) threshold.\nProbabilities for Core Prototypes:\nA defining feature of a core prototype for a class $l \\in [k]$ lies in its capacity to produce an optimal output. In\nessence, the output probability $p_l$ corresponding to the intended class ideally tends towards \u20181', reflecting a high level\nof confidence in the classification. Conversely, the probabilities associated with all other classes ideally tend towards\n'0', denoting minimal likelihood of membership in those classes. This characteristic ensures that the prototype adeptly\nencapsulates the unique attributes of its assigned class, thereby facilitating precise and reliable classification.\n$p_l \\approx 1 \\text{ and } p_j \\approx 0 \\text{ for } j\\neq l$  (10)\nFirst, we use the above probability distribution to generate a set of seed prototypes, which are then used to generate\ncore prototypes (see description below)."}, {"title": "4.2 Generation of Seed Prototypes", "content": "First we generate a set of k seed prototypes, exactly one seed for each category. The starting input vector for a seed\nprototype is a random vector $m_0 \\in \\mathbb{R}^p$, a target class label $l \\in [k]$ and a small loss-value $\\delta_{loss}$ for termination of iteration\n(see Sec. 4.1). We use these seed prototypes to generate (k-1) core prototypes for each category.\nLet $S_0 = \\{S_0^{(l)}|l \\in [k]\\}$ be the set of k random vectors drawn from $\\mathbb{R}^p$. Starting with each $S_0^{(l)}$ generate one seed\nprototype for class l by iteratively applying update rule given by Eqn. 9 and output probability distribution defined by\nEqn. 10. Let the seed vector obtained after termination of iterations that started with initial vector $S_0^{(l)}$ be denoted by\n$S^{(l)}$ and let S be the collection of all such k vectors. A procedure for generating n seed-prototypes is outlined in the\nAlgorithm 1.\nThese seed vectors serve two purposes: a preliminary evaluation of inter-class separation and initial or starting\ninput for generating prototypes to characterize intra-class compactness. We generate (k-1) prototypes for each class."}, {"title": "4.3 Generation of Core or Saturating Prototypes", "content": "Let $S_c^{l,i}$ be the prototype for class l generated starting from the seed $S^{(i)}$ of the class i. We expect that $S_c^{l,i}$ to be the\nclosest input data that is within the boundary of the class l and closest to the boundary of the class i. It is important\nto note that output produced by prototype $S_0^{(i)}$ satisfies the Eqn. 10. As shown in the Algorithm 2, the generation"}, {"title": "5 Empirical Evaluation of DNN Classifiers", "content": "Datasets We have used the image classification datasets CIFAR10 [10], and CIFAR100 [10]. All of these datasets\ncontain labeled 3-channel 32x32 pixel color images of natural objects such as animals and transportation vehicles.\nCIFAR10 and CIFAR100 have 10 and 100 categories/classes, respectively. The number of training examples per\ncategory are 5000 and 500 for CIFAR10 and 100, respectively. We created 7 datasets from each original dataset by\nrandomly selecting 25%, 40%, 60%, 70%, 80%, 90%, and 100% of the data.\nTraining For results reported we have used ResNet18 [6], defining $g( \u00b7 ;\\theta_g)$ as the input to the flattened layer (having\n256 neurons) after global average pooling and $h(\u00b7 ; \\theta_h)$ as the fully connected and softmax layers. All evaluations were"}, {"title": "5.1 Evaluation of the Classifier", "content": "In Section 2.1 we have proved that a well trained classifier's weight vectors are (almost) orthogonal. Here we empirically\nvalidate the Lemma 1. With the DNN classifiers trained with CIFAR 10 and CIFAR 100 and then their weights were\nfrozen. Values of the metric $\\hat{H}_w$ were calculated using the weights of the last layer of the trained networks. Because\nthe values were so close to one we converted them into angles and they are summarized in Table 1. It shows that the\nweight vectors are almost orthogonal even when only 25% of the data is used."}, {"title": "5.2 Evaluation of the Feature Extractor", "content": "For evaluating the feature extractor, we used the frozen networks to generate seed prototype images using Algorithm 1\nwith a learning rate of 0.01 for CIFAR10 and 0.1 for CIFAR100, respectively. Then Algorithm 2 was used to generate\n(k-1) core prototypes (see Sec.4.3) for each category. Thus, for CIFAR10 we generate a total of 100 prototypes and\nfor CIFAR100 we generate a total of 10,000 prototypes. The process was repeated five times to eliminate random\nselection bias and reported results are averages of these five runs. It is worth mentioning that we examined the data\nfrom each run and found no glaring differences.\nOur observations are summarized in Tables 2 and 3. Table 2 shows results for the CIFAR10 dataset and its fractions.\nThe 7 rows are for the 7 datasets we created from the original dataset by partitioning it. The 1st column shows percent of\ndata used. The second and third columns show values of the mean and standard deviation of the within-class similarity\nmeasure $M_{in}$. Column 8 shows that as the training dataset size increases, so does the testing accuracy."}, {"title": "An Upper Bound for Testing Dataset Accuracy:", "content": "The column 4 shows adjusted values of $M_{in}$, which is obtained\nby subtracting two standard deviations for increasing confidence of the observations. These values give us an upper\nbound for accuracy that one would have observed from the testing dataset. As discussed before, for a given class as the\nvalue of $M_{in}$ increases and its standard deviation decreases, the features of the class are close to each other and test\ndata performance should be high."}, {"title": "A Lower Bound for Test Dataset Accuracy:", "content": "The column 8 is obtained after subtracting the values in column 7 (see\nEqn. 7). These values give us a lower bound for accuracy one would get from the testing dataset. As discussed earlier,\nfor a given class as the value of $M_{bt}$ increases and its standard deviation decreases, the features of the class are much\ndifferent from those of other classes and accuracy observed from testing dataset should increase.\nThe data in the column 9 shows accuracy of the trained networks obtained when tested with test a dataset. Note that\nwe used all data in the test dataset (not a fraction as used for the training of the network). For ease of comparing and\ncontrasting, the values of the Upper and Lower Bounds as well as test Accuracy for CIFAR10 is shown in Fig. 1. We\ncan see that observed accuracy is correctly bounded by calculated values of the metrics.\nThe Table 3 and the Fig. 2 are for CIFAR100 dataset. Except for 25% and 40% of the training data the bounds\nobtained from the network have enclosed the accuracy correctly. But a closer examination of the data in the table, tells\nus that 25% and 40% data created a very poorly trained network. Thus, it may not be worth using them for any serious\nclassification applications."}, {"title": "6 Related Work", "content": "To the best of our knowledge, no work for estimation of testing accuracy of a trained DNN classifier without dataset\nexists, except in [15] where it was shown that test accuracy is correlated to cosine similarity of between class prototypes,"}, {"title": "7 Conclusion", "content": "In this paper we have proposed and evaluated a method for dataless evaluation of trained DNN that uses one-hot coding\nat the output layer, and usually implemented using softmax function. We assumed that the network is a composition of\na feature extractor and a classifier. The classifier is a one-layer fully-connected neural network that classifies features\nproduced by the feature extractor. We have shown that the weight vectors of a well trained classifier-layer are (almost)\northogonal. Our empirical evaluations have shown that the orthogonality is achieved very quickly even with a small\namount of data and test accuracy of the overall DNN is quite poor.\nThe feature extractor part of a DNN is very complex and has a very large number (usually many millions) of\nparameters and their direct evaluation taking into consideration all parameters is an extremely difficult, if not impossible,\ntask. We have proposed two metrics for indirect estimation of feature extractor's quality. Values of these metrics are"}]}