{"title": "Novel Deep Neural Network Classifier Characterization Metrics with Applications to Dataless Evaluation", "authors": ["Nathaniel Dean", "Dilip Sarkar"], "abstract": "The mainstream AI community has seen a rise in large-scale open-source classifiers, often pre-trained on vast datasets and tested on standard benchmarks; however, users facing diverse needs and limited, expensive test data may be overwhelmed by available choices. Deep Neural Network (DNN) classifiers undergo training, validation, and testing phases using example dataset, with the testing phase focused on determining the classification accuracy of test examples without delving into the inner working of the classifier. In this work, we evaluate a DNN classifier's training quality without any example dataset. It is assumed that a DNN is a composition of a feature extractor and a classifier which is the penultimate completely connected layer. The quality of a classifier is estimated using its weight vectors. The feature extractor is characterized using two metrics that utilize feature vectors it produces when synthetic data is fed as input. These synthetic input vectors are produced by backpropagating desired outputs of the classifier. Our empirical study of the proposed method for ResNet18, trained with CAFIR10 and CAFIR100 datasets, confirms that data-less evaluation of DNN classifiers is indeed possible.", "sections": [{"title": "1 Introduction", "content": "The mainstream artificial intelligence community has experienced a surge in the availability of large-scale open-source classifiers [1], catering to diverse end-users with varying use cases. Typically, these models are pre-trained on very large datasets and tested on some standard benchmark dataset, but end-users needing to select an open-source model may have a different use case and be overwhelmed by the possible choices. Further, the end-user may only have a small amount of test data that may not accurately reflect their use-case or it would be costly to setup or bolster their data pipeline for every possible open-source model [2, 3]. We propose a dataless evaluation method to pre-screen available models using only the weights and architecture of the network to eliminate poorly trained models and allow users to focus their data testing on only a few down-selected possibilities.\nOften Machine Learning (ML) models are trained, validated, and tested with example data. A significant part of the data is used to estimate the parameters of a model, next validation data is used for hyperparameter and model selection, and finally the test data is used to evaluate performance of the selected model [4,5]. Deep Neural Networks (DNNs) follow a similar method, where training examples are used to estimate parameters of a DNN. For a given example dataset, different model-architectures are trained and using a validation dataset a model is selected. Modern DNN models have hyper-parameters and their good values are selected using validation datasets as well. Finally, performance of the selected DNN (network parameters and hyper-parameters) is tested using a test dataset.\nEven when training, validation, and test datasets are coming from the same distribution or created by partitioning a larger dataset, training accuracy is often much higher than test accuracy. For example, ResNet18 [6] trained with CIRAR100 [7] achieves almost 100% accuracy very quickly, but testing accuracy is between 72 to 75%. This gap"}, {"title": "1.1 Notations", "content": "Let $[n]$ be the set $\\{1, 2, \\dots, n\\}$. Given, $D = \\{(x_i, y_i) | x_i \\in \\mathbb{R}^p, y_i \\in [k] \\text{ and } i \\in [n]\\}$, a dataset of $n$ labeled data coming from $k$ classes $C_1$ to $C_k$. Let $D_l = \\{(x_{i,l}) | (x_{i,l}) \\in D \\text{ and } l \\in [k]\\}$ be the set of data in classes $C_l$. Denoting $|D_l| = n_l$, we have $n = \\sum_{l=1}^k n_l$\nWe consider multiclass classifier functions $f : \\mathbb{R}^p \\rightarrow \\mathbb{R}^k$ that maps an input $x \\in \\mathbb{R}^p$ to a class $l \\in [k]$. For our purposes, we assume $f$ is a composition of a feature extraction function $g : \\mathbb{R}^p \\rightarrow \\mathbb{R}^q$ and a feature classification function $h : \\mathbb{R}^q \\rightarrow \\mathbb{R}^k$. Assume that $f$ is differentiable and after training with a dataset $D$, a parametric function $f(\\cdot; \\theta)$ is obtained. Because $f$ is a composition of $g$ and $h$, training of $f(\\cdot; \\theta)$ produces $g(\\cdot; \\theta_g)$ and $h(\\cdot; \\theta_h)$, where $\\theta = \\theta_g \\cup \\theta_h$ and $\\theta_g \\cap \\theta_h = \\emptyset$.\nLet $v = g(x; \\theta_g)$ and $\\hat{y} = h(v; \\theta_h)$, where $\\hat{y}_l = h_l(v; \\theta_h)$ represents the probability $h_l(v; \\theta_h)$ belongs to class $l \\in [k]$ such that $\\sum_{l=1}^k \\hat{y}_l = 1$. Correspondingly, for any given $x$, which possess a true label $l$, the classifier could generate $k$ possible class assignments and is correct only when $l \\eq \\text{arg max}(\\hat{y})$.\nUnless stated otherwise, we assume that activation functions at the output layer of the classifier $g(\\cdot; \\theta_g)$ are ReLu, and $h(\\cdot; \\theta_h)$ is a fully connected one layer neural network, where $W_{i, :}, i \\in [k]$ be the $i$th row of $h(\\cdot; \\theta_h)$'s weight matrix $W$. It receives input from a feature extractor $g(\\cdot; \\theta_g)$, and $h(\\cdot; \\theta_h)$'s output is using a softmax function (see Eqn.1) to generate one-hot coded outputs.\n$\\sigma_l(Wv) = \\frac{e^{(W_{l,:}\\cdot v)}}{\\sum_{i=1}^k e^{(W_{i,:}\\cdot v)}}$  (1)\nProblem Statement We are given a trained neural network $f(\\cdot; \\theta) = h(g(\\cdot; \\theta_g); \\theta_h)$ for the classification of input vectors $x \\in \\mathbb{R}^p$ to one of the $k$ classes; that is, we are given the architecture of the network and values of the parameters"}, {"title": "2 Characteristics of the Classifier $h(\\cdot; \\theta_h)$", "content": "An optimal representation of training data minimize variance of intra-class features and maximize inter-class variance [13, 14]. Inspired by these ideas in these hypotheses, we define training quality metrics using cosine similarity of features of prototypes generated from trained DNNs (see Sec 4). Let us deal with a much simpler task of defining a metric for a classifier $h(\\cdot; \\theta_h)$ defined as the last completely-connected layer."}, {"title": "2.1 Training Quality of the Classifier", "content": "Let $W_{i,:}$ be the $i$th row of $h(\\cdot; \\theta_h)$'s weight matrix $W$. Recall that elements of the input vector $v$ to $h(\\cdot; \\theta_h)$ are non-negative, because they are outputs of ReLU functions."}, {"title": "Definition 1 (Well trained Classifier $h(\\cdot; \\theta_h)$)", "content": "A one-hot output classifier $h(\\cdot; \\theta_h)$ with weight matrix $W^{k \\times q}$ is well trained, if for each class $l$ there exists a feature vector $v^{(l)}$ such that\n$e^{(W_{l,:} \\cdot v^{(l)})} >> e^{(W_{i,:} \\cdot v^{(l)})}$ for all $i \\neq l$, and (2)\n$e^{(W_{i,:} \\cdot v^{(l)})} < 1$ for all $i \\neq l$. (3)\nFor correct classification, the condition 2 is sufficient. The condition 3 is a stronger requirement but enhances the quality of the classifier, and it leads us to the following lemma."}, {"title": "Lemma 1 (Orthogonality of Weight Vectors of $h(\\cdot; \\theta_h)$)", "content": "Weight vectors $W_{i,:}$ of a well trained $h(\\cdot; \\theta_h)$ are (almost) orthogonal.\nProof: From the condition 3 of the Def. 1 we assume, $e^{(W_{l,:} \\cdot v^{(l)})} = 1$ for all $i \\neq l$. This implies that $(W_{i,:} \\cdot v^{(l)}) = 0$ for all $i \\neq l$, that is, $(k - 1)$ weight vectors $W_{i,:}$ and $i \\neq l$ are orthogonal to $v^{(l)}$.\nNow the value of $(W_{l,:} \\cdot v^{(l)})$ will determine the angle between $W_{l,:}$ and $v^{(l)}$. Because of the condition 2 of the Def. 1, it is much higher than zero and a maximum of $(W_{l,:} \\cdot v^{(l)}) = ||(W_{l,:}| || |v^{(l)}| ||)$ is obtained, when they are parallel to each other. If maximum is reached then $W_{l,:}$ is perpendicular to all other weight vectors. When this maximum is reached for all $l$, the weight vectors $W$ are mutually orthogonal. Otherwise, they are almost orthogonal."}, {"title": "Corollary 1 (Parallelity of $W_{l,:}$ and $v^{(l)}$)", "content": "If a classifier is well trained, for each weight vector $W_{l,:}$ there is one or more feature vectors $v^{(l)}$ that are (almost) parallel to it."}, {"title": "Classifier's Quality Metric, $H_\\omega$,", "content": "postulates that weight vectors of a well trained classifier are (almost) orthogonal to each other. A method for its estimation is defined next."}, {"title": "Definition 2 (Estimation of metric $H_\\omega$)", "content": "An estimation of $H_\\omega$ is calculated by consider a pair-wise combinations of the weight matrix of $h$, and given by\n$\\hat{H}_\\omega = 1 - \\frac{2}{k(k-1)} \\sum_{j<i<l} \\frac{|W_{i,:} \\cdot W_{j,:}|}{||W_{i,:}|| ||W_{j,:}||}   (4)$\nA well trained classifier will have a value of $\\hat{H}_\\omega$ very close to 1.\nNote that $k$ weight vectors are projecting a $q$-dimension feature vector to a $k$-dimension vector space, for $q > k$. If the weight vectors are orthogonal, their directions define an orthogonal basis in the $k$-dimension space. Because of the isomorphism of the Euclidean space, once the weight vectors of classifier $h(\\cdot; \\theta_h)$ are (almost) orthogonal their updating is unlikely to improve overall performance of the DNN and at that point, the focus should be to improve the feature extractor's parameters.\nIn the next section, we develop a method for evaluation of training quality of the feature extractor $g(\\cdot; \\theta_g)$. It is a much harder task because feature extractors are, in general, much more complex and directly quantifying its parameters with a few metrics is a near impossible task; we develop a method for indirectly evaluating the feature extractor without training or testing data."}, {"title": "3 Evaluataion of Feature Extractor $g(\\cdot; \\theta_g)$", "content": ""}, {"title": "3.1 Evaluation Metrics for the Feature Extractor $g(\\cdot; \\theta_g)$", "content": "For evaluation of a feature extractor $g(\\cdot; \\theta_g)$ we use a labeled data set $D_E$ with $|D_E| = n_E$. We assume, for simplicity of exposition, each evaluation class has the same number of examples making $n_E^l = n_E/k$. Let $V_E$ be the set of feature vectors generated by $g(\\cdot; \\theta_g)$ from all evaluation data in $D_E^l$. We define two metrics for measuring performance of the feature extractor $g(\\cdot; \\theta_g)$ with the evaluation data $D_E^l$: one for within class training quality and the other for between class training quality. Within class features of a class is considered perfect, when all feature vectors for a given class are pointing in the same direction, which is determined by computing pair-wise cosine of the angle between them. We discuss between class training quality later. Let us formally define cosine similarity."}, {"title": "Definition 3 (Cosine Similarity)", "content": "Given two vectors $v_1$ and $v_2$ of same dimension, cosine similarity\n$CosSim(v_1, v_2)) = cos(\\theta) = (v_1 . v_2)/(||v_1|| ||v_2||)$  (5)\nComputationally, $CosSim(v_1, v_2)$ of $v_1$ and $v_2$ is the dot product of their unit vectors. We normalize all vectors in $V_E$ for ease of computing all-pair $CosSim$ of the vectors in it. For a set of $n_f^l$ vectors in each class, we need to compute $n_f^l(n_f^l - 1)/2$ $CoSim$ since that is the number of possible pairwise combinations. Matrix multiplication provides a convenient method for computing these values. Let $v_{c,i}^l$ be the $i$th row of the matrix $G^l$ and let $(G^l)^T$ be the transpose of $G^l$. The elements of the $k \\times k$ matrix $(G^l(G^l)^T)$ are the $CosSim(v_{c,i}^l , v_{c,j}^l )$. Each diagonal element, being dot products of a unit vector with itself, is \u2018one' and because $(G^l(G^l)^T)_{i,j} =(G^l(G^l)^T)_{j,i}$, we need to consider only off-diagonal the upper or lower triangular part of the matrix."}, {"title": "3.1.1 Within-Class Similarity Metric", "content": "Intra-class or within-class Similarity Metric, $M_{in}$, postulates intra- or within-class features of a well trained DNN are very similar for a given class and their pairwise $CosSim$ values are close to one. To get an estimate within-class similarity of a DNN, estimates of all classes are combined together."}, {"title": "Definition 4 (Estimate of intra- or within-class similarity metric)", "content": "An estimate of intra- or within-class similarity metric $M_{in}$ is given by\n$M_{in} = \\frac{1}{k} \\sum_{l\\in[k]} (\\frac{2}{n^l(n^l-1)} \\sum_{j<i<n^l} ((G^l((G^l)^T))_{i,j}))   (6)$\nThe variance of $M_{in}$ of a well trained DNN should be very low as well. Our evaluations reported in Sec. 5 show that $M_{in}$ for CIFAR10 is above 0.972 even when it is trained with 25% of the trained data and variance is also small, an indication that the DNN was probably trained well. But for CIFAR100 $M_{in}$ and variance are relatively higher, indicating that it was not trained well. We propose to use within-class similarity metric to define upper bound for test accuracy (see Sec. 5)."}, {"title": "3.1.2 Between-Class Separation Metric", "content": "Inter-class or between-class Separation Metric, $M_{bt}$, postulates that two prototypes of different classes should be less similar and their pair-wise $CosSim$ value should be close to zero. For making $M_{bt}$ increase towards one as the quality of a trained DNN increases, we define $M_{bt} = 1 - CosSim(v_{c,i}^{l_1}, v_{c,j}^{l_2})$, where $v_{c,i}^{l_1}$ and $v_{c,j}^{l_2}$ are feature vectors from two different classes $l_1$ and $l_2$, respectively.\nBecause every class $l \\in [k]$ has $n_f^l$ evaluation examples, estimation of $M_{bt}$ requires further considerations. Consider two classes $l_1$ and $l_2$, we can pick one evaluation example from class $l_1$ and compare it with $n_f^{l_2}$ examples in $l_2$ (recall that we assumed that all classes have same number evaluation examples); repeating this process for all examples in the class $l_1$, require $O((n_F^l)^2)$ $CosSim$ computations. Also, because each example in class $l_1$ must be compared"}, {"title": "Definition 5 (Estimate of inter- or between-class separation metric)", "content": "An estimate of inter- or between-class separa- tion metric $M_{bt}$ is given by\n$M_{bt} = 1 - \\frac{1}{k(k-1)} \\sum_{l\\in[k]} \\sum_{\\substack{l_i\\in [k] \\l \\neq l_i}} (\\frac{1}{n_f^l} \\sum_{j=1}^{n_f^l} (H_{li,j} \\cdot [F^l_i]^T))   (7)$"}, {"title": "3.2 Training Quality of Feature Extractor", "content": "When a feature classifier's weights are pairwise orthogonal, overall classification accuracy will depend on the features that the feature extractor generates. If feature vectors of all testing examples of a class are similar and their standard deviation is very low, the value of $M_{in}$ will be high; but features of the between-class are very dissimilar and their standard deviation is high, the value of $M_{bt}$ will be low. This situation may cause classification errors, when feature vector of an example is too far from $M_{in}$. Our empirical observations is that during training time within-class feature vectors' become very close to each other faster and this closeness represents an upper bound for testing accuracy.\nAlso, we observed that it is much harder to impart training so that feature vectors of one class move far away from that of other classes, which keeps the value of the metric $M_{bt}$ lower, and hence, it will act as a lower bound for the testing accuracy. This hypotheis is supported by our empirical observations support.\nIn the next section, we empirically demonstrate that estimates of $M_{in}$ and $M_{bt}$ are proportional to test accuracy. Moreover, estimated values of $M_{in}$ and $M_{bt}$ serve as predictions of the upper- and lower-bounds of the test accuracy of the network."}, {"title": "3.2.1 Evaluation of Training quality of DNNs without Testing Data", "content": "The metrics defined above and methods for estimation of their values for a network requires data, which are used for generating feature vectors. Thus, estimation of within-class similarity and between-class separation metrics of a trained DNNs can be a routine task when testing data is available. But can we evaluate a DNN when no data is available? We answer this affirmatively.\nIn the next section, we propose a method for generation of evaluation examples from a DNN itself."}, {"title": "4 Data Generation Methods for Feature Extractor Evaluation", "content": "We employ the provided DNN to synthesize or generate input data vectors, which are then utilized to evaluate the feature extractor. We use the terms 'synthetic' or 'generated' interchangeably. These generated data are referred to as prototypes to differentiate them from the original data used in developing the DNN. The generation methods described next ensure that the synthesized data is correctly classified.\nA prototype generation starts with an random input image/vector and a target output class vector. he input (image) undergoes a forward pass to produce an output. The output of the DNN and the target class are then used to compute loss, which is subsequently backpropagated to the input. The input image is updated iteratively until the loss falls below a predetermined threshold. Since the synthesized data is generated using the DNN's parameters, it reflects the inherent characteristics of the trained DNN. Below, we outline our proposed data generation methods."}, {"title": "4.1 Generation of Prototype Datasets", "content": "To iteratively generate a prototype, we employ the loss function defined by Eqn. 8 (although this is not a limitation, as the method can be used with other loss functions as well) and the update rule described in Eqn. 9, where $t$ represents the iteration number and $m_t$ denotes the current input to the network.\n$Loss Function: L = - \\sum_{j=1}^k y_{m_t,j} log(f_j (m_t))   (8)$\n$Update Rule: m_{t+1} \\leftarrow m_t -\\eta \\nabla_{m_t} L / || \\nabla_{m_t} L ||   (9)$\nLet us consider the one-hot encoded output vector $\\bar{y}_l$, where for a particular class $l$ belonging to the set $[k]$, the element $y_l$ is set to 1 to indicate membership in that class, while all other elements $y_j$ for $j$ not equal to $l$ are set to 0 to signify non-membership in those classes.\nTo generate a prototype example $m^{(l)}$ for class $l$, we begin by initializing $m^{(l)}$ with a random vector $r \\in \\mathbb{R}^p$. We then perform a forward pass with $m^{(l)}$ to compute the cross-entropy loss between the output $f(m^{(l)}; \\theta)$ and the one-hot encoded target vector $\\bar{y}_l$. Subsequently, we backpropagate the loss through the network and update the input vector using the rule described in Eqn. 9 to obtain $m_t^{(l)}$. This iterative process continues until the observed loss falls below a desired (small) threshold."}, {"title": "Probabilities for Core Prototypes:", "content": "A defining feature of a core prototype for a class $l \\in [k]$ lies in its capacity to produce an optimal output. In essence, the output probability $p_l$ corresponding to the intended class ideally tends towards \u20181', reflecting a high level of confidence in the classification. Conversely, the probabilities associated with all other classes ideally tend towards '0', denoting minimal likelihood of membership in those classes. This characteristic ensures that the prototype adeptly encapsulates the unique attributes of its assigned class, thereby facilitating precise and reliable classification.\n$p_l \\approx 1$ and $p_j \\approx 0$ for $j \\neq l   (10)$\nFirst, we use the above probability distribution to generate a set of seed prototypes, which are then used to generate core prototypes (see description below)."}, {"title": "4.2 Generation of Seed Prototypes", "content": "First we generate a set of k seed prototypes, exactly one seed for each category. The starting input vector for a seed prototype is a random vector $m_0 \\in \\mathbb{R}^p$, a target class label $l \\in [k]$ and a small loss-value $\\delta_{loss}$ for termination of iteration (see Sec. 4.1). We use these seed prototypes to generate $(k-1)$ core prototypes for each category.\nLet $S_0^l = \\{S_0^l | l \\in [k]\\}$ be the set of k random vectors drawn from $\\mathbb{R}^p$. Starting with each $S_0^l$ generate one seed prototype for class $l$ by iteratively applying update rule given by Eqn. 9 and output probability distribution defined by Eqn. 10. Let the seed vector obtained after termination of iterations that started with initial vector $S_0^l$ be denoted by $S^{(l)}$ and let $S$ be the collection of all such k vectors. A procedure for generating n seed-prototypes is outlined in the Algorithm 1.\nThese seed vectors serve two purposes: a preliminary evaluation of inter-class separation and initial or starting input for generating prototypes to characterize intra-class compactness. We generate $(k-1)$ prototypes for each class."}, {"title": "4.3 Generation of Core or Saturating Prototypes", "content": "Let $S_{c,i}^l$ be the prototype for class $l$ generated starting from the seed $S^{(i)}$ of the class $i$. We expect that $S_{c,i}^l$ to be the closest input data that is within the boundary of the class $l$ and closest to the boundary of the class $i$. It is important to note that output produced by prototype $S_{c,i}^l$ satisfies the Eqn. 10. As shown in the Algorithm 2, the generation"}, {"title": "5 Empirical Evaluation of DNN Classifiers", "content": "Datasets We have used the image classification datasets CIFAR10 [10], and CIFAR100 [10]. All of these datasets contain labeled 3-channel 32x32 pixel color images of natural objects such as animals and transportation vehicles. CIFAR10 and CIFAR100 have 10 and 100 categories/classes, respectively. The number of training examples per category are 5000 and 500 for CIFAR10 and 100, respectively. We created 7 datasets from each original dataset by randomly selecting 25%, 40%, 60%, 70%, 80%, 90%, and 100% of the data.\nTraining For results reported we have used ResNet18 [6], defining $g( \\cdot ; \\theta_g)$ as the input to the flattened layer (having 256 neurons) after global average pooling and $h(\\cdot ; \\theta_h)$ as the fully connected and softmax layers. All evaluations were"}, {"title": "5.1 Evaluation of the Classifier", "content": "In Section 2.1 we have proved that a well trained classifier's weight vectors are (almost) orthogonal. Here we empirically validate the Lemma 1. With the DNN classifiers trained with CIFAR 10 and CIFAR 100 and then their weights were frozen. Values of the metric $\\hat{H}_\\omega$ were calculated using the weights of the last layer of the trained networks. Because the values were so close to one we converted them into angles and they are summarized in Table 1. It shows that the weight vectors are almost orthogonal even when only 25% of the data is used."}, {"title": "5.2 Evaluation of the Feature Extractor", "content": "For evaluating the feature extractor, we used the frozen networks to generate seed prototype images using Algorithm 1 with a learning rate of 0.01 for CIFAR10 and 0.1 for CIFAR100, respectively. Then Algorithm 2 was used to generate $(k-1)$ core prototypes (see Sec.4.3) for each category. Thus, for CIFAR10 we generate a total of 100 prototypes and for CIFAR100 we generate a total of 10,000 prototypes. The process was repeated five times to eliminate random selection bias and reported results are averages of these five runs. It is worth mentioning that we examined the data from each run and found no glaring differences.\nOur observations are summarized in Tables 2 and 3. Table 2 shows results for the CIFAR10 dataset and its fractions. The 7 rows are for the 7 datasets we created from the original dataset by partitioning it. The 1st column shows percent of data used. The second and third columns show values of the mean and standard deviation of the within-class similarity measure $M_{in}$. Column 8 shows that as the training dataset size increases, so does the testing accuracy."}, {"title": "An Upper Bound for Testing Dataset Accuracy:", "content": "The column 4 shows adjusted values of $M_{in}$, which is obtained by subtracting two standard deviations for increasing confidence of the observations. These values give us an upper bound for accuracy that one would have observed from the testing dataset. As discussed before, for a given class as the value of $M_{in}$ increases and its standard deviation decreases, the features of the class are close to each other and test data performance should be high."}, {"title": "A Lower Bound for Test Dataset Accuracy:", "content": "The column 8 is obtained after subtracting the values in column 7 (see Eqn. 7). These values give us a lower bound for accuracy one would get from the testing dataset. As discussed earlier, for a given class as the value of $M_{bt}$ increases and its standard deviation decreases, the features of the class are much different from those of other classes and accuracy observed from testing dataset should increase.\nThe data in the column 9 shows accuracy of the trained networks obtained when tested with test a dataset. Note that we used all data in the test dataset (not a fraction as used for the training of the network). For ease of comparing and contrasting, the values of the Upper and Lower Bounds as well as test Accuracy for CIFAR10 is shown in Fig. 1. We can see that observed accuracy is correctly bounded by calculated values of the metrics.\nThe Table 3 and the Fig. 2 are for CIFAR100 dataset. Except for 25% and 40% of the training data the bounds obtained from the network have enclosed the accuracy correctly. But a closer examination of the data in the table, tells us that 25% and 40% data created a very poorly trained network. Thus, it may not be worth using them for any serious classification applications."}, {"title": "6 Related Work", "content": "To the best of our knowledge, no work for estimation of testing accuracy of a trained DNN classifier without dataset exists, except in [15] where it was shown that test accuracy is correlated to cosine similarity of between class prototypes,"}, {"title": "Term Prototype in Other Contexts:", "content": "We want the reader to be aware that term prototype we use have no direct relation to the training data and it should not be confused with or associated with same term has been used in many other contexts [18-25]."}, {"title": "7 Conclusion", "content": "In this paper we have proposed and evaluated a method for dataless evaluation of trained DNN that uses one-hot coding at the output layer, and usually implemented using softmax function. We assumed that the network is a composition of a feature extractor and a classifier. The classifier is a one-layer fully-connected neural network that classifies features produced by the feature extractor. We have shown that the weight vectors of a well trained classifier-layer are (almost) orthogonal. Our empirical evaluations have shown that the orthogonality is achieved very quickly even with a small amount of data and test accuracy of the overall DNN is quite poor.\nThe feature extractor part of a DNN is very complex and has a very large number (usually many millions) of parameters and their direct evaluation taking into consideration all parameters is an extremely difficult, if not impossible, task. We have proposed two metrics for indirect estimation of feature extractor's quality. Values of these metrics are"}]}