{"title": "LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring Game Difficulty with LLM Agents", "authors": ["Chang Xiao", "Brenda Z. Yang"], "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated their potential as autonomous agents across various tasks. One emerging application is the use of LLMs in playing games. In this work, we explore a practical problem for the gaming industry: Can LLMs be used to measure game difficulty? We propose a general game-testing framework using LLM agents and test it on two widely played strategy games: Wordle and Slay the Spire. Our results reveal an interesting finding: although LLMs may not perform as well as the average human player, their performance, when guided by simple, generic prompting techniques, shows a statistically significant and strong correlation with difficulty indicated by human players. This suggests that LLMs could serve as effective agents for measuring game difficulty during the development process. Based on our experiments, we also outline general principles and guidelines for incorporating LLMs into the game testing process.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the dawn of the digital era, video games have become a significant part of human culture. From early, primitive games like Pong to modern, high-profile titles like Grand Theft Auto, this medium has evolved into a multibillion-dollar industry, playing a pivotal role in the entertainment sector. According to statistics from the Steam gaming platform [46], more than ten thousand new video games are released each year.\n\nA key aspect of developing a successful video game lies in achieving the right balance of difficulty. Players seek challenges that feel rewarding but not overwhelming, and poorly balanced difficulty can lead to frustration and disengagement [21]. In game design theory [41], this is referred to as designing a smooth difficulty curve, where players face increasing challenges that remain manageable as they progress through different levels.\n\nA notable example illustrating the importance of balanced difficulty is the recent hit Elden Ring: Shadow of the Erdtree. Upon its initial release, the game was criticized for being excessively difficult [11, 49], with only a limited number of players able to progress through even the first few areas. In response to widespread complaints, the developers had to release a patch to reduce the difficulty, leading to additional development costs and reduced consumer satisfaction.\n\nEnsuring that a game's difficulty curve aligns with the designer's vision requires rigorous testing throughout the development phase. Traditionally, human players are recruited to assess the game's difficulty [4]. These testers comprehensively navigate through various levels and scenarios to plot out the difficulty curve and see if it match with the developer's intent. While this method is thorough, it is also inherently tedious, complex, and resource-intensive, requiring significant time and human power, especially when iterative changes are necessary.\n\nTo address this issue, researchers and developers have explored automated testing techniques, using AI to simulate human players and analyze AI's performance to assess game difficulty [13, 36]. However, there is still uncertainty regarding how accurately AI can approximate the abilities of real players. Additionally, training such AI models, particularly those utilizing advanced techniques like deep reinforcement learning, demands significant computational resources. Moreover, these models are often tailored to specific games, limiting their reusability across different games. This raises concerns about the cost-effectiveness of creating game-specific AI systems for game testing [44].\n\nAs a result, there is a growing need for creating a General Video Game Playing (GVGP) agent that can be easily deployed across different games while exhibiting human-like behavior to a reasonable extent. The recent rise of Large Language Models (LLMs) has sparked interest among researchers and developers in their potential to act as GVGP agents. Research has demonstrated that LLMs can play video games effectively, and with carefully designed system architectures and prompts, they can even perform at an advanced player level in specific cases [18, 19, 57].\n\nGiven the reasoning capabilities of LLMs and their demonstrated success in gameplay scenarios, we aim to explore whether LLMs can be effectively utilized in game testing, specifically to assess game difficulty. Our target is to establish a general framework for LLMs to play games without extra fine-tuning, and assess difficulty in a way that closely aligns with human.\n\nOur contributions to this work are threefold:\n\n\u2022 We propose a general game-testing framework that leverages LLMs to evaluate whether the various game challenges reflect the difficulty intended by the game developers.\n\n\u2022 We deployed this framework in two popular video gamesWordle and Slay the Spire. Our findings show that LLMs exhibit a strong correlation with human on difficulty across various challenges, even though they generally do not match the gameplay performance (e.g., scores, win rates) of the average human player. Specifically, when the LLM performed poorly on certain challenges, human players also found them difficult, and vice versa. This suggests that LLMs can serve as effective testers for assessing relative difficulty of different challenges.\n\n\u2022 Based on our experiments using LLM agents to play games, we outline several guidelines for effectively leveraging LLMs"}, {"title": "1.1 Measurement of Difficulty", "content": "To this point, we have used the term \"difficulty\" without providing a formal definition. According to prior literature [3, 4, 20], difficulty, in the context of video game, is generally related to the player's skills or abilities to overcome a given \"challenge\". A \"challenge\" in a game can be understood as a sub-game: a rule-based system with variable and quantifiable outcomes [20].\n\nWhile difficulty for a specific challenge could theoretically be represented by an absolute metric, in game development, what is often more important is the relative scaling of difficulty across different challenges [3]. A well-structured difficulty curve helps players achieve a state of flow [8], where the task is neither too hard nor too easy, thereby increasing player engagement and enjoyment. When applying LLMs for difficulty assessment, the goal should not be to obtain an absolute difficulty value for any individual challenge. Instead, the focus should be on determining the relative difficulty and its scaling among a series of challenges.\n\nAs discussed by Aponte et al. [4], difficulty can be measured experimentally by having many players attempt a specific set of challenges and evaluating the quality of their performance. The quality measurement can involve any game-related statistics that correlate with difficulty, such as the player's win rate in overcoming a challenge, the score achieved, or the time taken. The specific metric chosen depends on the context and game, as there is no universally correct measure of difficulty."}, {"title": "2 RELATED WORK", "content": "Before the rise of intelligence models in this decade, the term \u201cAI\u201d was commonly used in the context of gaming, where it referred to algorithms used to control non-player characters or automate in-game mechanics. At the same time, due to the high cost of human testing in game development, the idea of using AI for game testing has been explored since the early days of video games. Work by Macleod [29] investigated this by simulating gameplay in Perudo, a bidding dice game, using a multi-agent system. Similarly, Kirby et al. [23] studied the classic game Minesweeper by replacing human players with a rule-based algorithm, finding that a simple algorithm could solve most cases. Although these are relatively simple games with straightforward mechanics, the studies demonstrated the potential of using rule-based AI for game testing. Nowadays, the concept of automated game testing has been widely studied in game research [4, 13] and integrated into modern game development frameworks such as Unity [54].\n\nAs video games have evolved, their scale has expanded, and the mechanics have become increasingly complex [3]. In modern video game development, creating AI strategies for these games has become as challenging as, if not more challenging than, designing the games themselves [36]. As a result, researchers are exploring"}, {"title": "2.1 AI for Game Testing", "content": "more advanced and generalized methods for developing game AI, with a focus on potentially creating GVGP agents. Notably, the use of Deep Learning (DL) to train game Al has seen tremendous success in recent years, whether through supervised learning from human gameplay [5, 48, 60] or reinforcement learning via self-play [15, 40, 45]. These approaches have been applied to a wide range of popular video games, including Atari [32], Starcraft [33, 56], and Doom [24]. These techniques have enabled game AI to reach superhuman levels of performance, as measured by metrics like scores and win rates. For a comprehensive review of DL-based game AI, refer to the survey by Shao et al. [44]. However, many of these works have focused on building better AI models rather than on utilizing AI to assist in the game development process.\n\nTo address this gap, researchers have recently expanded their focus beyond simply developing stronger game AI, to explore how Al can provide insights into game design and player experience. For example, Zhu et al. [63] analyzed over 20 games, identifying dominant player-Al interaction metaphors and patterns in these games. Villareale et al. [55] conducted a study where participants played against adversarial AI models, investigating how players developed mental models of the AI during gameplay. Liang et al. [26] offered insights into how AI can enhance the natural and efficient communication of actionable information in games involving human-AI collaboration. These studies suggest that AI systems can offer valuable insights for game design. Additionally, there is a growing body of research discussing the design space for Al's role in games and how it can improve player experience [14, 51, 63].\n\nDespite this progress, creating Al systems for games remains a significant challenge. Game AI often needs to be designed or trained on a case-by-case basis for each game, particularly for DL-based AI, which demands substantial computational resources. Another major challenge is the lack of interpretability in DL-based Al systems [44]. For instance, it can be difficult to understand why an AI chooses a particular move over another, which limits the insights that game developers can derive from these systems. Therefore, it is still unclear how AI can be used to increase the productivity of game development."}, {"title": "2.2 LLM Agent for Gaming", "content": "The increasing capabilities of LLMs have demonstrated their potential across a wide range of tasks, from summarizing an article to complex code generation, leading people to consider their potential as universal agents for GVGP. This potential is attributed to their ability to process diverse textual inputs, interpret natural language instructions, and reason effectively to generate appropriate outputs. These features make LLMs well-suited for processing game-related data and producing human-like actions with minimal training effort.\n\nSimilar to the trend in DL-based AI, there is a significant amount of work on developing LLM agents to achieve better gameplay performance. Examples of these efforts span various game genres, including conversational games like Werewolf [59] and Avalon [27], board and card games like Poker [19], Chess [9], and Crossword [42], as well as more intricate video games like Street Fighter [12], StarCraft II [28], Pok\u00e9mon [18], Civilization [37], Minecraft [57] and"}, {"title": "2.3 LLM as Human-like Simulator", "content": "Outside the gaming domain, a considerable body of research has explored leveraging LLMs as human simulators across various fields, including economics [2, 16, 25], politics [38, 58], healthcare [35, 52], and social science [30, 34, 64]. These studies suggest that, to a certain extent, LLMs can effectively model human behavior, decision-making, and social interactions across different contexts. Given these promising applications, we are interested in understanding whether similar capabilities can be observed in the gaming domain."}, {"title": "3 FRAMEWORK OVERVIEW AND GAME CHOICE", "content": "Our goal is to provide a general framework for game difficulty measurement, using the LLM's gameplay performance as the difficulty metric. Figure 1 depicts our framework architecture. Our framework includes two major components: the game I/O component and the instruction component.\n\n\u2022 Game I/O Component: This component handles the interaction between the LLM and the game environment. It parses the game state, summarizes the necessary information for the LLM, and converts the LLM's responses into executable game actions.\n\n\u2022 Instruction Component: This component consists of three parts: Game Rules, Game Strategies, and Prompting Techniques. The Game Rules provide essential explanations of how the game operates, ensuring that LLMs can play in a reasonable manner. The Game Strategies introduce external knowledge of gameplay tactics, enhancing the LLM's performance and simulating a human-like playstyle. Prompting Techniques involve the methods used to guide the LLM, such as Chain-of-Thought (CoT). This part also dictates how the LLM utilizes the game knowledge, for example, by referencing specific game rules or following customized strategies during decision-making."}, {"title": "3.1 Choice of Game", "content": "In the following, we outline the principles used to select games for LLMs to evaluate, considering both the game design and the capabilities of LLMs.\n\nFirst, all necessary game information must be representable as text. This follows standard practice in existing LLM-based game agent research [28]. Consequently, games that rely heavily on visual information, such as First-Person Shooters, are excluded from our current selection. For this study, we focus on games that can be expressed in text form (though not necessarily text-based games) and evaluate them using LLMs like GPT-3.5 and GPT-4.\n\nSecond, we aim to test our framework on the original versions of real, widely played games. Popular games often achieve their status through polished and balanced design, making them engaging and successful across diverse audiences. Thus, these games are more representative of real-world scenarios encountered in game development. By focusing on popular, full-scale games, this evaluation seeks to provide more applicable and relevant insights for future research in LLM-based game testing.\n\nThird, to evaluate how closely LLMs can represent game difficulty as experienced by human players, we need to choose games that have publicly available human gameplay data. Comparing LLMs' performance with human gameplay data is the most straightforward and convincing method for determining how accurately LLMs assess game difficulty relative to real players. Using games with available human play data establishes a shared foundation for LLMs and humans, allowing difficulty assessments to be made under the same conditions.\n\nBased on these considerations, we selected two games for our evaluation: Wordle and Slay the Spire. These widely regarded, award-winning games represent high-quality game design and vary significantly in the complexity of their rules and action space. More information on the game rules and how they align with our criteria will be provided in the following sections."}, {"title": "4 WORDLE", "content": "Wordle is a web-based word puzzle game published by The New York Times\u00b9 which has approximately 300,000 active daily users. In this game, players have six attempts to guess a five-letter word. After each guess, players receive feedback indicating how close"}, {"title": "4.1 Background", "content": "their guess is to the correct word, and their goal is to arrive at the correct answer in as few guesses as possible.\n\nMore specifically, when a player makes a guess, the game provides feedback by highlighting each letter in one of three colors: green, yellow, or gray. A green letter means the letter is in the correct position in the word. A yellow letter means that the letter is in the word but in a different position. A gray letter indicates that the letter is not in the word at all. For example, if the target word is \"APPLE\" and the player guesses \"ALERT,\" the \u201cA\u201d would be highlighted in green, the \"L\" and \"E\" in yellow, and the other letters in gray. Players use this feedback to adjust their subsequent guesses, aiming to identify the correct word within the six attempts.\n\nAs a game focused purely on wordplay, Wordle is an ideal example for our use case because the entire game environment can be described in text. Solving the puzzle requires strategic reasoning, as players must balance the trade-off between using untested letters to gather more information or relying on tested letters to make a strategic bet on the next guess. Additionally, the puzzles are independent of each other, which allows us to assess the difficulty level of each one individually.\n\nFurthermore, Wordle offers publicly available human play data. Hosted on The New York Times website, the game has an accompanying tool of WordleBot, which records and analyzes player performances. WordleBot documents over 600,000 plays from real users, providing detailed statistics such as the average number of attempts required to solve the puzzle and the distribution of scores. This dataset [7] allows us to assess how closely the LLM's assessment of puzzle difficulty aligns with that indicated by human performance.\n\nDue to Wordle's simple game mechanics, the difficulty of each puzzle can be directly measured by the average number of attempts needed to solve it or by the percentage of players who solve the puzzle within six guesses (commonly referred to as the win rate). Generally, the more attempts required or the lower the win rate, the more challenging the puzzle is considered."}, {"title": "4.2 Implementation", "content": "Thanks to Wordle's straightforward game rule, we were able to implement the game locally on our machine without the need to connect to the web server. The core process of Wordle is essentially verifying whether the letters in the player's guess match those in the target word and providing feedback based on their position. We developed a Python program to implement this game logic. The program takes a player's guess as input and compares it to the target word. It then generates feedback by identifying which letters are correct and in the correct position, which letters are correct but in the wrong position, and which letters are not in the word at all. This feedback is described in natural language, along with additional information such as the number of guesses taken and the guess history, to be processed by the AI agent.\n\nTo set up the context for the LLM to play the Wordle game, we first prepare a generic description of the game's rules and an explanation of the input/output format from the game environment. We then provide real-time in-game information as the feedback of the last attempt, and instruct the LLM to generate the next guess.\n\nTo evaluate how different prompt engineering techniques affect the LLM agent's ability to play and how closely this ability mirrors human performance, we use three distinct prompting methods. The first method is zero-shot prompting, where we simply describe the game rules and ask the LLM to generate a guess directly after receiving in-game information. The second method is CoT prompting. In this approach, we not only provide the game rules but also"}, {"title": "4.3 Experiment", "content": "The goal of this study is to explore two key questions: (i) Are puzzles that are difficult for human players also challenging for LLM agents, requiring more steps to solve? Even if some LLM agents may generally take more guesses than humans, we are interested in whether they follow a similar trend in terms of relative difficulty. In other words, we want to determine if the LLM agents' performance correlates with human on which puzzles are more challenging or easier, focusing on the relative difficulty among different puzzles rather than the absolute number of guesses. (ii) Do the LLM model and prompting methods influence the correlation between LLM performance and human play? If so, how? Understanding this is crucial for identifying which LLM models and prompting techniques align more closely with the human play experience.\n\nTo address the key questions, we collected Wordle puzzles from the dataset [7] spanning the period from March 7, 2024, to August 16, 2024, resulting in a set of 529 distinct puzzles.\n\nTo evaluate each agent's performance, we had them attempt to solve every puzzle. Due to the inherent randomness in the LLM agents, we conducted 20 trials per puzzle and calculated the average number of guesses required to solve each one. For the Wordle Solver agent, which uses a deterministic algorithm, we ran the simulation only once per puzzle. We observed that under the original rule"}, {"title": "5 SLAY THE SPIRE", "content": "Slay the Spire (StS) [31] is an award-winning single-player video game that combines deck-building with roguelike 3 elements. It features a card battle system known for its intricate mechanics, which demand strategic planning and effective card synergy. In the gameplay, players progress through levels, using their evolving deck to battle random enemies. After each victory, the player selects a new card from a random set to add to their deck, gradually strengthening the deck to face increasingly challenging enemies. StS could be a representative candidate for testing by LLM agents for several reasons. First, all game mechanics, including the rules, card descriptions, and enemy strategies, can be represented as text. There is no need to use method for graphics understanding to execute gameplay.\n\nSecond, the game requires continuous understanding of the current game state, such as the cards in hand and the enemy's health points. Players must constantly consider optimal strategies and calculate odds to maximize their chances of winning. To illustrate why StS is an appropriate environment for our experiments, consider the following example cards:\n\n\u2022 Strike (cost 1): Effect: Deal 6 damage.\n\n\u2022 Bash (cost 2): Effect: Deal 8 damage. Apply vulnerable to the enemy.\n\nAs shown above, the Strike card deals a straightforward 6 damage, but if a card like Bash is used first, applying vulnerable, the enemy will take 50% more damage from subsequent attacks. Therefore, when Strike is played on a vulnerable enemy, it deals 9 damage instead of 6. This highlights the importance of card play order during combat. With more than 100 different types of cards, there is a wide range of possible combinations that LLMs can reason about.\n\nThird, the developers have released a dataset of real human gameplay, containing 75 million+ runs of game sessions4. This dataset enables us to analyze the human performance when combating different enemies and compare it with our LLM agent or other baselines.\n\nIt is also worth noting that StS has been used in previous work involving LLMs in gameplay [6]. However, their focus is primarily on developing LLM agents using various prompting techniques to play a simplified version of StS and analyze its performance, without comparing it to real human data. In contrast, we aim to evaluate the original version of StS using a generic LLM framework, with"}, {"title": "5.1 Background", "content": "Second Mod is CommunicationMod, which does not modify gameplay but enables the extraction of game information and submission of input actions via standard streams (stdin/stdout). It also offers a Python interface, allowing interaction with the game through APIs.\n\nWith these two Mods, we can easily treat the game environment as a black box and allow LLM to interact with it through the API. The Python interface serves as the game I/O component, converting all game information into text and translating the LLM's output into executable game actions."}, {"title": "5.2 Implementation", "content": "The original StS is a closed-source game released on video game platforms like Steam and Xbox, which do not support a programming interface. However, thanks to the Steam Workshop 5 community, many developers have built Mods that can be utilized for automated gameplay. For our experiment purposes, we installed two Mods to implement the automation system.\n\nThe first is BasicMod, a fundamental Mod required for the operation of other Mods. It provides the infrastructure that allows additional Mods to run on top of the original game, along with a console feature that enables players to use command lines to directly modify game elements, such as the player's health, the enemies encountered, or the cards in hand.\n\nLLM Agent and Prompting In this study, we use two types of LLM agent: Zero-Shot Agent with game rules, and CoT Agent with both game rules and Chain-of-Thought reasoning.\n\nTo prompt the Zero-Shot LLM agent, we combine game rules with real-time game information obtained through the CommunicationMod module, forming a natural language input that includes the basic objectives, core mechanics, cards in the deck, along with player and enemies' statuses (e.g., hp, block, and vulnerable as mentioned in the previous example). Based on this input, the LLM is asked to generate the next card to play and the target of the card. Once the LLM determines the next action, the response will be parsed by a Python script and sent to the CommunicationMod module as executable action. The action will then be executed by the game environment as if they were human inputs, allowing the game to run normally. After the player ends their turn, the enemy will take its actions, and the LLM will receive updated in-game information to repeat the process. This loop will continue until the player's health reaches zero (resulting in failure) or all enemies are defeated (resulting in a win).\n\nIn addition to the basic game knowledge mentioned above, the CoT agent is provided with step-by-step reasoning instructions. In CoT prompting, we ask the LLM to analyze the in-game situation, determine the optimal strategy, and evaluate each playable card before selecting the card to play. This process encourages the agent to exhibit more logical and human-like behavior. To keep the prompt concise, only high-level instructions are provided, while avoiding detailed tactics on specific cards or combinations. This"}, {"title": "5.3 Experiment", "content": "We tested the agents by having them battle all the bosses they could encounter in Act 1 and Act 2 (\"Act\u201d is a game-specific term equivalent to \"Level\"). To maintain a controlled experiment, we fixed the character as Ironclad and used a well-balanced, representative deck. We found that when using a deck of average strength, similar to that of a human player in the corresponding act, the LLMs had a win rate of less than 5% against the bosses. Using a stronger deck helped compensate for the LLM's sub-human gameplay performance and ensured an average win rate of 65%.\n\nAs an indicator of human-perceived enemy difficulty, we use the average win rate of each boss, calculated from gameplay data released by the developers. This metric provides a baseline for how challenging human players find each boss, offering a reference point for comparison with agent performance.\n\nTo assess the agents' gameplay performance, we use \"HP remaining\" as an indicator of enemy difficulty. This metric, which reflects the agent's remaining health at the end of each battle, provides"}, {"title": "6 DISCUSSION", "content": "In this section, we share insights from our experiences with Wordle and Slay the Spire and synthesize key guidelines on how to effectively use LLMs to measure game difficulty.\n\nG1: Text representation format matters. During our experiment, we found that the way game information is represented for LLM processing is crucial. Game-specific information should be presented in natural language and in a manner that aligns with the LLM's inherent processing structure. For example, in the Slay the Spire experiment, game-specific mechanics (e.g., Exhaust a card) should be explained in a way that is understandable to an average person without extensive gaming experience. Moreover, in the Wordle experiment, we initially formatted all the 5-letter words in plain text. However, LLMs process text by splitting words into common subwords. For instance, the word \"APPLE\u201d might be tokenized into a sequence of \"APP\" and \"LE\u201d rather than a sequence of five individual characters. This tokenization phenomenon limits the LLM's ability to accurately understand the position of letters within a word. To address this, we reformatted the words explicitly as lists (e.g., \"[A, P, P, L, E]\"), utilizing the LLM's ability to process structured input, and observed a substantial improvement in gameplay performance.\n\nG2: Some games may require compensation mechanisms to accommodate LLM performance. As observed in our experiment, in most cases, LLMs do not perform as well as average human players. Additionally, it is possible that LLM agents may consistently fail to accomplish certain tasks. For instance, they may exceed the original limit of 6 guesses in Wordle or have very low win rates against specific bosses in Slay the Spire. If the win rates are consistently low and no distinction between easy and difficult challenges, it becomes hard to assess relative difficulty for these challenges using LLMs. In our practice, we increased the guessing limit in Wordle to give the LLM a better chance of success, resulting in greater variation of LLM's performance across puzzles. In Slay the Spire, we provided the LLM with a stronger deck than an average human player would have at the same level, ensuring the LLM could consistently defeat most bosses. However, the method"}, {"title": "7 CONCLUSION", "content": "In this work, we present an LLM-based framework for measuring game difficulty. Our framework is adaptable to a wide range of games, and we have specifically tested it on Wordle and Slay the Spire. Our experiments demonstrate that LLMs exhibit a high correlation with human players in difficulty assessment, requiring only simple and generic prompting. Compared to rule-based agents, which take significant time to develop, and machine learning-based agents, which require extensive computational resources for training, LLMs offer a more generalized capability to play a variety of games with minimal adjustment. We hope this work opens the door for new approaches in game testing within the gaming industry, while also inspiring further applications of LLMs in this field."}]}