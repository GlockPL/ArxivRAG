{"title": "PERFORMANT, MEMORY EFFICIENT AND SCALABLE\nMULTI-AGENT REINFORCEMENT LEARNING", "authors": ["Omayma Mahjoub", "Sasha Abramowitz", "Ruan de Kock", "Wiem Khlifi", "Simon du Toit", "Jemma Daniel", "Louay Ben Nessir", "Louise Beyers", "Claude Formanek", "Liam Clark", "Arnu Pretorius"], "abstract": "As the field of multi-agent reinforcement learning (MARL) progresses towards\nlarger and more complex environments, achieving strong performance while main-\ntaining memory efficiency and scalability to many agents becomes increasingly\nimportant. Although recent research has led to several advanced algorithms, to\ndate, none fully address all of these key properties simultaneously. In this work, we\nintroduce Sable, a novel and theoretically sound algorithm that adapts the retention\nmechanism from Retentive Networks to MARL. Sable's retention-based sequence\nmodelling architecture allows for computationally efficient scaling to a large num-\nber of agents, as well as maintaining a long temporal context, making it well-suited\nfor large-scale partially observable environments. Through extensive evaluations\nacross six diverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in the majority of tasks (34 out of\n45, roughly 75%). Furthermore, Sable demonstrates stable performance as we\nscale the number of agents, handling environments with more than a thousand\nagents while exhibiting a linear increase in memory usage. Finally, we conduct\nablation studies to isolate the source of Sable's performance gains and confirm its\nefficient computational memory usage. Our results highlight Sable's performance\nand efficiency, positioning it as a leading approach to MARL at scale.", "sections": [{"title": "1 INTRODUCTION", "content": "When considering large-scale practical applications of multi-agent reinforcement learning (MARL)\nsuch as autonomous driving (Lian & Deshmukh, 2006; Zhou et al., 2021; Li et al., 2022) and\nelectricity grid control (Kamboj et al., 2011; Li et al., 2016), it becomes increasingly important to\nmaintain three key properties for a system to be effective: strong performance, memory efficiency,\nand scalability to many agents. Although many existing MARL approaches exhibit one or two of\nthese properties, a solution effectively encompassing all three remains elusive.\nTo briefly illustrate our point, we consider the spectrum of approaches to MARL. On the one end\nlie fully decentralised algorithms (Tan, 1997; Witt et al., 2020). Such algorithms demonstrate\nproficiency in handling many agents in a memory efficient way by typically using shared parameters\nand conditioning on an agent identifier. However, at scale, the performance of fully decentralised\nmethods remains suboptimal compared to more centralised approaches (Papoudakis et al., 2021; Yu\net al., 2022; Wen et al., 2022).\nBetween decentralised and centralised methods, lie CTDE approaches (Lowe et al., 2017; Papoudakis\net al., 2021; Yu et al., 2022). These methods typically employ a centralised critic during training but\nhave independent policies that can be scaled during execution. Although such methods have been\nshown to be more performant than decentralised methods while maintaining their scaling properties,\nthey do not represent the state-of-the-art (SOTA) in terms of performance.\nOn the other end of the spectrum, lie centralised algorithms such as the Multi-Agent Transformer\n(MAT) (Wen et al., 2022). MAT re-frames MARL as a sequence modeling problem by leveraging"}, {"title": "2 BACKGROUND", "content": "Problem Formulation Cooperative MARL in partially observable settings can be modeled using\na decentralised-POMDP with $(N, O, A, R, P, \\gamma)$. Here $N$ is the number of agents, $\u039f = \\Pi_{i=1}^{N} \u041e$\nis the joint observation space of all agents, $\u0391 = \\Pi_{i=1}^{N} A^{i}$ is the joint action space of all agents,\n$R:O \\times A \\rightarrow R$ is the joint reward function, $P : O \\times A \\times O \\rightarrow [0,1]$ is the environment\ntransition probability function and $\\gamma\\in [0, 1)$ is a discounting factor. At timestep t, each agent\nreceives a separate observation $o^{i}_{t} \\in O^{i}$, collectively forming the joint observation $o_{t} \\in O$, and\nexecutes a separate action $a^{i}_{t} \\in A^{i}$, forming the joint action $a_{t} \\in A$, sampled from a joint policy\n$\\pi(a|o) = \\Pi_{i=1}^{N} \\pi^{i}(a^{i}|o^{i})$. All agents receive a shared reward $r_{t} = R(o_{t}, a_{t})$. The goal is to learn an\noptimal joint policy which maximises the expected joint discounted reward $J = \u0395_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^{t}r_{t}]$.\nMARL as Sequence Modelling Prior work has investigated formulating MARL as a sequence\nmodelling problem. Specifically, the multi-agent transformer (Wen et al., 2022) uses an encoder-"}, {"title": "3 METHOD", "content": "In this section, we introduce Sable, our approach to MARL as sequence modelling using a modified\nversion of retention suitable for RL. Sable enables parallel training and memory-efficient execution\nat scale, with the ability to capture temporal dependencies across entire episodes. We explain how\nSable operates during both training and execution, how we adapt retention to work in MARL, and\nprovide different strategies for scaling depending on the problem setting.\nExecution Sable interacts with the environment for a defined rollout length, L, before each training\nphase. During this interaction, the encoder uses a chunkwise representation, processing the obser-\nvation of all agents at each timestep in parallel. A hidden state $h_{enc}$ maintains a memory of past\nobservations and is reset at the end of each episode. During execution, the decay matrix, D, is set to\nall ones, allowing for full self-retention over all agents' observations within the same timestep. These\nadjustments to retention, particularly the absence of decay across agents and the resetting of memory"}, {"title": "4 EXPERIMENTS", "content": "We validate the performance, memory efficiency and scalability of Sable by comparing it against\nseveral SOTA baseline algorithms from the literature. These baselines can broadly be divided into\ntwo groups. The first group consists of heterogeneous agent algorithms that leverage the advantage\ndecomposition theorem. To the best of our knowledge, the Multi-Agent Transformer (MAT) (Wen\net al., 2022) represents the current SOTA for cooperative MARL on discrete environments, and\nHeterogeneous Agent Soft Actor-Critic (HASAC) (Liu et al., 2023) the current SOTA on continuous\nenvironments. The second group includes well-established baseline algorithms, including IPPO (Witt\net al., 2020), MAPPO (Yu et al., 2022), QMIX (Rashid et al., 2020) and MASAC. For all baselines,\nwe use the JAX-based MARL library Mava (de Kock et al., 2023).\nEvaluation protocol We train each algorithm for 10 independent trials for each task. Each training\nrun is allowed 20 million environment timesteps with 122 evenly spaced evaluation intervals. At each\nevaluation, we record the mean episode return over 32 episodes and, where relevant, any additional\nenvironment specific metrics (e.g. win rates). In line with the recommendations of Gorsane et al.\n(2022), we also record the absolute performance. For task-level aggregation, we report the mean\nwith 95% confidence intervals while for aggregations over entire environment suites, we report the\nmin-max normalised inter-quartile mean with 95% stratified bootstrap confidence intervals. Following\nfrom Agarwal et al. (2021), we consider algorithm X to have significant improvement over algorithm\nY if the probability of improvement score and all its associated confidence interval values are greater\nthan 0.5. All our evaluation aggregations, metric calculations and plotting leverages the MARL-eval\nlibrary from Gorsane et al. (2022).\nEnvironments We evaluate Sable on several benchmark environments including Robotic Ware-\nhouse (RWARE) (Papoudakis et al., 2021), Level-based foraging (LBF) (Christianos et al., 2020),\nConnector (Bonnet et al., 2023), The StarCraft Multi-Agent Challenge in JAX (SMAX) (Rutherford\net al., 2023), Multi-agent Brax (MABrax) (Peng et al., 2021) and the Multi-agent Particle Environment\n(MPE) (Lowe et al., 2017). All environments have discrete action spaces with dense rewards, except\nfor MABrax and MPE, which have continuous action spaces and RWARE which has sparse rewards.\nFurthermore, we compare to HASAC and MASAC only on continuous tasks given their superiority\nin this setting and QMIX only on SMAX as it has been shown to perform suboptimally in other\ndiscrete environments (most notably in spare reward settings such as RWARE) (Papoudakis et al.,\n2020). Finally, we highlight that our evaluation suite comprised of 45 tasks represents nearly double"}, {"title": "4.1 PERFORMANCE", "content": "In Figure 1, we report the amount of times that an algorithm had a significant probability of im-\nprovement over all other algorithms on a given task. Furthermore, we present the per environment\naggregated sample efficiency curves and probability of improvement scores in Figure 3, as well as the\nmean episode return for all tasks in Table 1. Our experimental evidence shows Sable achieving SOTA\nperformance across a wide range of tasks. Specifically, Sable exceeds baseline performance on 34 out\nof 45 tasks. The only environment where this is not the case is on MABrax. For continuous robotic\ncontrol tasks SAC is a particularly strong baseline, typically outperforming on-policy methods such\nas PPO (Haarnoja et al., 2018; Huang et al., 2024). However, Sable still manages to achieve SOTA\nperformance in continuous control tasks on MPE. We note that previous benchmarking and evaluation\nwork (Papoudakis et al., 2020; Gorsane et al., 2022) has recommended training off-policy algorithms\nfor a factor of 10 less environment interactions than on-policy algorithms due to more gradient\nupdates for the same number of environment interactions. In our case, we find that off-policy systems\ndo roughly 15 times more gradient updates for the same amount of environment interactions. If we\nhad done this the performance of HASAC, MASAC and QMIX would have been less performant\nthan reported here. Additional tabular results, task and environment level aggregated plots are given\nin Appendix B."}, {"title": "4.2 \u039c\u0395\u039cORY USAGE AND SCALABILITY", "content": "We assess Sable's ability to efficiently utilise computational memory, focusing primarily on scaling\nacross the agent axis.\nChallenges in testing scalability using standard environments Testing scalability and memory\nefficiency in standard MARL environments poses challenges, as many environments such as SMAX,\nMPE and Connector, expand the observation space as the number of agents grows. MABrax has\nuniquely assigned roles per agent making it difficult to scale up and RWARE does not have a\nstraightforward way to ensure task difficulty as the number of agents increases. For these reasons,\nthe above environments are difficult to use when testing algorithmic scalability without significantly\nmodifying the original environment code. Among these, LBF is unique in that it is easier to adjust\nby reducing agents' field of view (FOV) while maintaining reasonable state size and offering faster\ntraining. However, it still requires modifications to ensure a fixed FOV and a limited observation size\n(see Appendix A.3 for more details). Despite these adjustments, LBF could not fully demonstrate\nSable's scaling capability, as it could not scale past 128 agents due to becoming prohibitively slow.\nTherefore, to explore scaling up to a thousand agents, we introduce Neom, a fully cooperative\nenvironment specifically designed to test algorithms on larger numbers of agents.\nA new environment for testing agent scalability in cooperative MARL A task in Neom is\ncharacterised by a periodic, discretised 1-dimensional pattern that is repeated for a given num-\nber of agents. Each agent observes whether it is in the correct position and the previous ac-\ntions it has taken. Agents receive a shared reward which is calculated as the Manhattan distance\nbetween the team's current pattern and the underlying task pattern. We design three task pat-\nterns: (1) simple-sine: {0.5, 0.7, 0.8, 0.7, 0.5, 0.3, 0.2, 0.3}, (2) half-1-half-0: {1,0}, (3)\nquick-flip: {0.5, 0, -0.5,0}. For more details, see Appendix A.7."}, {"title": "4.3 ABLATIONS", "content": "We aim to better understand the source of Sable's performance gains as compared to MAT. There are\ntwo specific implementation details that Sable inherits from the retention mechanism that can easily\nbe transferred to attention, and therefore to MAT. The first is using root mean square normalization\n(RMSNorm) (Zhang & Sennrich, 2019) instead of layer normalization (Lei Ba et al., 2016) and the\nsecond is using SwiGLU layers (Shazeer, 2020; Ramachandran et al., 2017) instead of feed forward\nlayers. To determine if this is the cause of the performance differences between Sable and MAT,\nwe adapt MAT's implementation to use the above changes, both independently and simultaneously.\nWe test all three variants of MAT on two RWARE tasks (tiny-4ag and medium-4ag) and two\nSMAX tasks (3s5z and smacv2_5_units) and compare them with the original implementation.\nWe tune all methods using the same protocol as for the main results."}, {"title": "5 RELATED WORK", "content": "Linear recurrent models Recent work in RL has leveraged structured state space models (Lu et al.,\n2024) for efficient long context memory. It has also been shown that various linear recurrent models\nincluding Linear Transformers (Katharopoulos et al., 2020), Fast and Forgetful Memory (Morad\net al., 2024a), and Linear Recurrent Units (Orvieto et al., 2023) can be used for temporal memory in\nRL (Morad et al., 2024b). Sable falls into this category of algorithms due to leveraging the RetNet\narchitecture, a linear recurrent model, instead of the Transformer.\nTransformers and RetNets in reinforcement learning Other works have applied Transformers in\nthe context of MARL (Hu et al., 2021; Wen et al., 2022). The closest to our work is MAT (Wen et al.,\n2022). In single-agent RL, transformers have been used to enable long range memory (Parisotto\net al., 2020; Esslinger et al., 2022), most notably the Gated Transformer-XL (Parisotto et al., 2020).\nSable differs from these works for two main reasons: (1) it is a distinctly multi-agent algorithm\nand (2) it has no need for appending observation histories to input sequences since it can retain all\nnecessary information from previous timesteps with a hidden state. Moreover, and to the best of our\nknowledge, Sable is the first architecture to leverage RetNets for learning policies in RL. The only\nother application of RetNets has been to learn an efficient world model (Cohen et al., 2024).\nHeterogeneous agent algorithms Sable is built to leverage the advantage decomposition theorem,\nand while MAT and HASAC (Liu et al., 2023) are the SOTA heterogeneous agent algorithms there\nare other notable heterogeneous agent algorithms including heterogenous agent versions of proximal\npolicy optimisation (HAPPO) and trust region policy optimisation (HATRPO) (Kuba et al., 2022)."}, {"title": "6 CONCLUSION", "content": "In this work, we introduced Sable, a novel cooperative MARL algorithm that employs retentive net-\nworks to achieve significant advancements in memory efficiency, agent scalability and performance.\nSable's ability to condition on entire episodes provides it with an enhanced temporal awareness, lead-\ning to SOTA performance. This is evidenced by our extensive evaluation, where Sable significantly\noutperforms other leading approaches in 75% of tasks tested. Moreover, Sable's memory efficiency\ncomplements its performance by addressing the significant challenge of scaling MARL algorithms as\nit is able to maintain stable performance even when scaled to over 1000 agents. Looking ahead, we\naim to explore Sable's integration into more complex, larger-scale, real-world environments."}, {"title": "A ENVIRONMENT DETAILS", "content": "A.1 ROBOT WAREHOUSE\nThe Robot Warehouse (RWARE) environment simulates a warehouse where robots autonomously\nnavigate, fetching and delivering requested goods from specific shelves to workstations and then\nreturning them. Inspired by real-world autonomous delivery depots, the goal in RWARE is for a team\nof robots to deliver as many randomly placed items as possible within a given time budget.\nThe version used in this paper is a JAX-based implementation of the original RWARE environment\n(Papoudakis et al., 2021) from the Jumanji environment suite (Bonnet et al., 2023). For this reason,\nthere is a minor difference in how collisions are handled. The original implementation has some\nlogic to resolve collisions, whereas the Jumanji implementation simply ends an episode if two agents\ncollide.\nNaming convention The tasks in the RWARE environment are named according to the following\nconvention:\n<size>-<num_agents>ag<diff>\nEach field in the naming convention has specific options:\n\u2022 <size>: Represents the size of the Warehouse which defines the number of rows and\ncolumns of groups of shelves within the warehouse (e.g. tiny, small, medium, large).\n\u2022 <num_agents>: Indicates the number of agents.\n\u2022 <diff>: Optional field indicating the difficulty of the task, where 'easy' and 'hard' imply\n2N and N/2 requests (shelves to deliver) respectively, with N being the number of agents.\nThe default is to have N requests.\nIn this environment, we introduced an extra grid size named \u201cxlarge\u201d which expands the default\n\"large\" size. Specifically, it increases the number of rows in groups of shelves from three to four,\nwhile maintaining the same number of columns.\nObservation space In this environment observation are limited to partial visibility where agents\ncan only perceive their surroundings within a 3x3 square grid centred on their position. Within this\narea, agents have access to detailed information including their position and orientation, as well as\nthe positions and orientations of other agents. Additionally, they can observe shelves and determine\nwhether these shelves contain a package for delivery."}, {"title": "A.2 SMAX", "content": "SMAX, introduced by Rutherford et al. (2023), is a re-implementation of the StarCraft Multi-agent\nChallenge (SMAC) (Samvelyan et al., 2019) environment using JAX for improved computational\nefficiency. This redesign eliminates the necessity of running the StarCraft II game engine, thus results\non this environment are not directly comparable to results on original SMAC. In this environment,\nagents collaborate in teams composed of diverse units to win the real-time strategy game StarCraft.\nFor an in-depth understanding of the environment's mechanics, we refer the reader to the original\npaper (Samvelyan et al., 2019).\nObservation space Each agent observes all allies and enemies within its field of view, including\nitself. The observed attributes include position, health, unit type, weapon cooldown, and previous\naction.\nAction space Discrete action space that includes 5 movement actions: four cardinal directions, a\nstop action, and a shoot action for each visible enemy.\nReward In SMAX, unlike SMAC, the reward system is designed to equally incentivise tactical\ncombat and overall victory. Agents earn 50% of their total return from hitting enemies and the other\n50% from winning the episode which ensures that immediate actions and ultimate success are equally\nimportant."}, {"title": "A.3 LEVEL BASED FORAGING", "content": "In the Level-Based Foraging environment (LBF) agents are assigned different levels and navigate\na grid world where the goal is to collect food items by cooperating with other agents if required.\nAgents can only consume food if the combined level of the agents adjacent to a given item of food"}]}