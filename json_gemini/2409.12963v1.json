{"title": "INTERPOLATING VIDEO-LLMS: TOWARD LONGER-SEQUENCE LMMS IN A TRAINING-FREE MANNER", "authors": ["Yuzhang Shang", "Bingxin Xu", "Weitai Kang", "Mu Cai", "Yuheng Li", "Zehao Wen", "Zhen Dong", "Kurt Keutzer", "Yong Jae Lee", "Yan Yan"], "abstract": "Advancements in Large Language Models (LLMs) inspire various strategies for integrating video modalities. A key approach is Video-LLMs, which incorporate an optimizable interface linking sophisticated video encoders to LLMs. However, due to computation and data limitations, these Video-LLMs are typically pre-trained to process only short videos, limiting their broader application for understanding longer video content. Additionally, fine-tuning Video-LLMs to handle longer videos is cost-prohibitive. Consequently, it becomes essential to explore the interpolation of Video-LLMs under a completely training-free setting. In this paper, we first identify the primary challenges in interpolating Video-LLMs: \u25cf the video encoder and modality alignment projector are fixed, preventing the integration of additional frames into Video-LLMs, and the LLM backbone is limited in its content length capabilities, which complicates the processing of an increased number of video tokens. To address these challenges, we propose a specific INTerPolation method for Video-LLMS (INTP-Video-LLMs). We introduce an alternative video token rearrangement technique that circumvents limitations imposed by the fixed video encoder and alignment projector. Furthermore, we introduce a training-free LLM context window extension method to enable Video-LLMs to understand a correspondingly increased number of visual tokens. We analyze the deployment costs of INTP-Video-LLM, and find its efficiency bottleneck is on its KV cache cost. Accordingly, we introduce a training-free KV-cache compression mechanism that reduces memory overhead during inference. INTP-VideoLLM not only supports the processing of longer video sequences but also optimizes memory usage during inference\u2014all achieved without the need for additional training. In practice, whereas pre-trained Video-LLaVA [Lin et al., 2023] models are configured to process just 8 frames, INTP allows these models to comprehend 32 frames.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) [OpenAI, 2023, Touvron et al., 2023a;b], have shown surprising capabilities in question answering and reasoning. Building on this foundation, Vision LLMs extend these abilities to include images, employing a vision encoder and an LLM to generate text responses given an image and a related question [Liu et al., 2023a, Zhang et al., 2024]. Recent advancements aim to extend this capability from whole-image understanding to video understanding [Lin et al., 2023, Zhang et al., 2023, Kim et al., 2024]. Video-LLMs combine video data with language models by integrating learnable interfaces that capture both spatial and temporal representative of video. Typically, these interfaces use a projection network [Li et al., 2023b, Maaz et al., 2023, Li et al., 2023a] to transform video content into video tokens that can be interpreted by LLMs, thereby bridging the gap between video information and text processing capabilities of LLMs (see Fig.1).\nHowever, existing Video-LLMs [Li et al., 2023b, Lin et al., 2023, Maaz et al., 2023, Li et al., 2023a, Kim et al., 2024] struggle to process long videos. In particular, Video-LLMs lack the temporal resolution necessary to interpolate precision time, limiting its application to long video [Huang et al., 2024]. For instance, Video-LLaMA [Li et al., 2023b] and Video-LLaVA [Lin et al., 2023] only sample 8 frames uniformly across an entire video, which often falls short for detailed temporal analysis [Huang et al., 2024]. Simply feeding more frames into these models does not resolve the\nissue, as their architectures cannot effectively process a larger number of frames. Consequently, there is an urgent need for Video-LLMs that can understand larger numbers of video frames.\nTo reach this goal, one could consider training Video-LLMs from scratch to handle more extensive video frames. Unfortunately, this approach is impractical due to prohibitive training costs and data accessibility issues. On the one hand, the Video-LLMs consume more and more data, which making the accessibility of data more and more difficult. In some cases, the training data even are not released. Sometimes, the weights of those Video-LLM are released while the data is not open. On the other hand, the computation cost of training a Video-LLM is expensive. For example, even the relatively less costly Video-LLaVA [Lin et al., 2023] requires thousands of hours on Nvidia A100 GPUs. Therefore, retraining a Video-LLM to process more frames is not feasible in many scenarios.\nIn this paper, we aim to enable Video-LLMs to understand videos with an extensive number of frames in a training-free manner. We first delineate the primary challenges associated with interpolating Video-LLMs: 1 the video encoder and modality alignment projector are fixed, which prevents the integration of additional frames, and 2 the content length capacity of the LLM backbone is limited, hindering the processing of an increased number of visual tokens.\nTo overcome these obstacles, we propose a specific interpolation method for Video-LLMs, dubbed INTP. Addressing the 1 challenge, we propose an alternative video token rearrangement technique that bypasses the restrictions imposed by the fixed video encoder and alignment projector. This approach allows us to utilize the pre-trained video encoder and alignment projector to generate an unlimited number of video tokens, while maintain the video tokens' consistency. For the 2 challenge, derived from the positional embedding mechanism in Video-LLMs (i.e., Rotary Position Embedding, ROPE [Su et al., 2024]), we develop a training-free Video-LLM context window extension method. This design ensures that the interpolated Video-LLM can handle any number of video frames.\nUpon developing the INTP-Video-LLM, we examined its deployment constraints and identified that the handling of an increased number of video tokens incurs additional memory usage. This finding echos the memory bottleneck in the long-sequence LLM deployment [Yuan et al., 2023; 2024]. To address this issue and enhance the deployment efficiency of INTP-Video-LLM, we introduce a training-free KV-cache compression technique that reduces memory overhead during inference. Consequently, the INTP-Video-LLM not only facilitates the processing of longer video sequences but also optimizes memory usage during inference, all without necessitating further training."}, {"title": "2 RELATED WORK", "content": "Video Language Models. Prior to the emergence of LLMs, Yang et al. [2022] introduce FrozenBiLM, which combined a frozen vision encoder with a bidirectional language model for efficient video processing. In the LLM era, video-language models continue to evolve. For instance, Video-ChatGPT enhances video instruction tuning using high-quality instructional data [Maaz et al., 2023]. Li et al. [2023a] propose VideoChat, in which they utilize cross-attention mechanisms to integrate video\ntokens with user queries and dialogue context. Then, Li et al. [2024] propose the follow-up work, VideoChat2, in which they advance this integration with a multi-stage bootstrapping method, focusing on modality alignment and further instruction tuning. Video-LLaVA [Lin et al., 2023] introduces a pre-aligned encoder that supports both image and video modalities, enabling shared projections and joint training across tasks. However, handling long videos is very challenging due to the high computational complexity and memory demands of representing extensive video content with video tokens. To address these issues, various advanced temporal modeling techniques have been deployed. Chat-UniVi [Jin et al., 2023] proposes a unified model that dynamically merges spatial and temporal tokens using k-NN techniques to streamline processing. LLaMA-VID [Li et al., 2023b] utilize a dual token system to effectively compress video tokens. Despite these explorations, all previous methods have relied on some form of training. In contrast, our work is the first to explore the extension of the Video-LLM context window in a training-free manner, offering an approach to handling long video sequences without the computational expense and data requirements of traditional training methods.\nLong-context LLMs. LLMs are generally pre-trained within a specified context length, with models like LLaMA and LLaMA2 employing 2k and 4k tokens respectively [Touvron et al., 2023a;b]. Training LLMs from scratch to handle extended contexts is often prohibitively expensive for many researchers. Nevertheless, recent innovations have focused on extending the context length through fine-tuning methods. Recent advancements focus on modifying the position embedding (PE), par-ticularly ROPE [Su et al., 2024], employed in LLMs such as LLaMA and Mistral [Jiang et al., 2023]. One of the main strategies is embedding scaling [Chen et al., 2023, Liu et al., 2023b, Peng et al., 2023]. Embedding scaling techniques modify position embeddings to accommodate longer sequences, ensuring they remain within the pre-training scope and preventing feature extrapolation. For instance, Chen et al. [2023] condense position indices to maintain alignment with the pre-training range, effectively expanding LLaMA's context to 16,000 tokens with minimal fine-tuning, requiring only 1,000 steps. In a different approach, Liu et al. [2023b], Roziere et al. [2023] adjust the rotary base of RoPE, known as \u201cNTK-aware\u201d scaling. To the best of our knowledge, our work is the first exploration to extend the context window of Video-LLMs in a training-free manner.\nPost-training Compression for KV Cache. Recently, the compression of Key-Value (KV) caches has garnered significant attention due to the prohibitively high memory consumption associated with generating long contextual sequences in LLMs. Current methods can be briefly categorized into three types: Quantization-aware compression, eviction-based compression and attention-based compression. Quantization-aware compression reduces KV cache size by substituting the original KV cache with lower-precision approximations. KVQuant [Hooper et al., 2024], KIVI [Liu et al., 2024] and WKVQuant [Yue et al., 2024] are pioneering studies in KV cache quantization, revealing that keys and values should be quantized along different dimensions. Notably, KIVI compresses the KV cache to as few as 2 bits. Yang et al. [2024], Dong et al. [2024] further enhance quantization performance by identifying and selectively preserving more significant keys and values. SKVQ [Duanmu et al., 2024] advances upon KIVI using a sliding window technique, wherein quantization parameters are determined on a window-wise basis. ZipCache [He et al., 2024] establish a robust baseline for KV cache quantization, achieving compression ratios as high as 5x with negligible performance degradation. In this wrok, we analyse the development bottlenecks of long-context Video-LLMs, and propose using the quantization techniques to address the computational overhead challenges."}, {"title": "3 METHOD: INTERPOLATING VIDEO-LLMs", "content": "In this section, we first review the basic implementation of Video Large Language Models (Video-LLMs), with a particular focus on the main components and their training pipelines. We highlight the difficulties of obtaining a long-video-LLM from scratch (Sec.3.1). Next, we present a totally training-free method specifically designed for Video-LLM INTerPolation, called INTP. There are three advancements in the development and optimization of INTP: (1) A video encoder and modality-alignment projector extension method (Sec.3.2); (2) A video-LLM context window extension method (Sec. 3.3); and (3) An inference bottleneck analysis and a post-training quantization solution (Sec.3.4)."}, {"title": "3.1 PRELIMINARIES: VIDEO LARGE LANGUAGE MODELS (VIDEO-LLM)", "content": "Video-LLMs are typically composed of three core modules: a video encoder, an alignment projector layer, and a fine-tuned large language model (LLM) backbone. The process begins with a video\nencoder often a Vision Transformer (ViT)\u2014which converts video input $X \\in \\mathbb{R}^{N \\times W \\times H}$ into a series of visual tokens Z, in which N is number of frames, W and H is the pixel width and height of each frame. These tokens are processed by the input projector $O_{Z \\rightarrow H}$, which maps the encoded visual features to the text feature space H. The aligned features, along with text prompts $H_q$, are then input into a LLM backbone, and then the language model can generate the response. The architecture of a typical Video-LLMs is illustrated in Fig. 1.\nVideo encoders are crucial for compressing the raw video into compact representations. These encoders are often trained from scratch via multimodal alignment pretraining [Yin et al., 2023]. For instance, LanguageBind [Zhu et al., 2023] employs contrastive learning during the pretraining phase to gradually align video modality with language modality. Given that LLMs inherently process text, bridging the modality gap between natural language and video is essential. However, training a Video-LLM end-to-end is prohibitively expensive. A more feasible approach involves using a learnable connector between the pre-trained visual encoder and the LLM, along with fine-tuning the LLM to better interpret visual tokens. Although those components can be trained via instruction tuning [Gong et al., 2023] or alignment tuning [Sun et al., 2023], their training remains costly. For example, to training the video encoder, Zhu et al. [2023] collect a large-scale video-and-text dataset, VIDAL-10M. In addition, even without pre-training the video encoder, fine-tuning the Video-LLaVA [Lin et al., 2023] still consumes approximately 200 hours on Nvidia A100 GPUs.\nIn summary, given the significant data and computational expenses associated with training Video-LLMs, developing these models from scratch to process long videos is challenging. This underscores the importance of exploring methods to adapt existing Video-LLMs for extended video processing in a training-free manner.\nIn practice, the challenges in extending the Video-LLMs can be summarized from two key aspects: 1 the fixed nature of the video encoder and modality alignment projector, and 2 the limited content length capacity of the LLM backbone. Firstly, the video encoder and the alignment projector are typically configured during initial training and remain unchanged thereafter. This prevents the system from incorporating additional frames beyond the preset limit, which limits the model's ability to adapt to videos of different lengths or to capture more granular temporal details within prolonged video sequences. Secondly, the LLM backbone, which is designed to process textual information, has a predefined limit on the number of tokens it can handle. This constrains the model's capacity to interpret a larger number of visual tokens, effectively capping the amount of visual information that can be processed. As the complexity and length of videos increase, these constraints become significant bottlenecks, hindering the model's ability to fully understand and generate coherent responses based on longer video inputs."}, {"title": "3.2 ALTERNATIVE VIDEO TOKEN REARRANGEMENT", "content": "To address challenge 1, we enable the video encoder and alignment projector to map video input with an increased number of frames into the text feature space. As shown in Fig.2, the pre-trained video encoder is configured to process a fixed-length video sequence, $X$, consisting of $N$ frames. The most straightforward approach is repeatedly using the encoder and projector to process $m \\cdot N$ frames by sampling and encoding multiple subsets of frames $m$ times. Once we have collected $m$ groups of video tokens, we can concatenate them together to get the whole long video's visual tokens.\nHowever, since these video encoders and projectors are pre-trained to process only $N$ frames collectively, such repetitive usages lead to distorted temporal representations in the video token sequence. The concatenating video tokens may carry the frame's relative information\u2014such as the inconsistency between the tokens of the $N$-th frame and those of the $(N + 1)$-th frame (i.e., tokens of the 1-th frame and those of the $(N + 1)$-th frame share more similarity). This discrepancy emerges because the transformer-processed tokens are not designed to naturally accommodate the increased spatial and temporal variety, potentially causing misalignments and inconsistencies in the encoded video features [Xiao et al., 2023, Shang et al., 2024a].\nTo overcome this issue, we propose an alternative video token rearrangement technique that preserves the temporal consistency of the video tokens. The technique allows the generation of a infinite number of video tokens, which enable model to process extended video sequences without requiring retraining. The module employs a dynamic rearrangement of input frames and their corresponding tokens. Here's how the rearrangement works:"}, {"title": "3.3 INTERPOLATING VIDEO-LLM BACKBONE", "content": "Addressing the challenge 2, we introduce a post-training interpolation method for the Video-LLM's backbone, specifically targeting its positional embedding. As discussed in the related work (Sec.2), Rotary Position Embedding (RoPE) [Su et al., 2024] is a widely-used positional encoding scheme adopted by several prominent LLMs, including Llama, Phi, Mistral, and Gemma. Consequently, ROPE has become the standard method for position embedding in Video-LLMs.\nROPE Fundamentals: RoPE integrates positional information into both the query vector q and key vector k of transformer models. Specifically, for a sequence of tokens $s_1, s_2, \\dots, s_L$ with embeddings $x_1, \\dots, x_L \\in \\mathbb{R}^D$ (D is the dimension of the embedding), RoPE applies the following transformations:\n$q_m = f_q(x_m, m) \\in \\mathbb{R}^D \\text{ and } k_n = f_k(x_n, n) \\in \\mathbb{R}^D$.\n(3.1)\nThe transformation functions $f_q$ and $f_k$ incorporate positional information as:\n$f_q(x_m, m) = e^{im\\Theta}W_q x_m \\text{ and } f_k(x_n, n) = e^{in\\Theta}W_k x_n$,\n(3.2)\nwhere $\\theta_d = b^{-2d/|D|}$ with a base $b = 10000$. These transformations ensure that the relative positions of tokens are reflected in their interactions through the inner product operations, represented by m-n\nof the tokens as follows: $(f_q(x_m, m), f_k(x_n, n))_{\\mathbb{R}} =$\n$\\text{Re}((f_q(x_m, m), f_k(x_n, n))_{\\mathbb{C}}) = \\text{Re}(x_m W_q^* W_k x_n e^{i \\Theta (m-n)}) = g(x_m, x_n, m - n)$ (3.3)\nwhere $\\text{Re}(q, k)$ is the real part of the inner product of q, k, and g(\u00b7) is an abstract mapping function. In real coordinates, the RoPE can be expressed as follows:\n$f_W(x_m, m, \\theta_d) = \\begin{bmatrix}\n\\cos m\\theta_1 & -\\sin m\\theta_1 & 0 & 0 & 0 & 0 \\\\\n\\sin m\\theta_1 & \\cos m\\theta_1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\cos m\\theta_2 & -\\sin m\\theta_2 & 0 & 0 \\\\\n0 & 0 & \\sin m\\theta_2 & \\cos m\\theta_2 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cos m\\theta_l & -\\sin m\\theta_l \\\\\n0 & 0 & 0 & 0 & \\sin m\\theta_l & \\cos m\\theta_l\n\\end{bmatrix} W x_m$.\n(3.4)\nContext Window Scaling: Given the fixed context lengths of pre-trained Video-LLMs, a significant question is how to extend these lengths cost-effectively. Utilizing the inherent flexibility of ROPE, we modify the embedding functions to handle extended sequences without extensive retraining:\n$f_W(x_m, m, \\theta_d) = f_W(\\frac{m}{L} \\frac{L'}{L} x_m, \\theta_d)$, \n(3.5)\nwhere $L' > L$ denotes a new, extended context window. It allows us to adapt the pre-trained positional embeddings to longer contexts, maintaining accuracy.\nNTK-aware Interpolation: The \u201cNTK-aware\" interpolation further refines this adjustment by recalibrating the base of the RoPE function:\n$\\theta' = b'^{-2d/|D|} \\text{ and } b' = b \\cdot \\text{s}^{\\frac{D}{|D|-2}}$,\n(3.6)\nwhere $s = \\frac{L'}{L}$ is the scaling ratio. This adjustment ensures that the extended embeddings retain their functionality within the longer context length [Roziere et al., 2023]."}, {"title": "3.4 EFFICIENCY BOTTLENECK ANALYSIS AND A POST-TRAINING SOLUTION", "content": "Implementing the Alternative Video Token Rearrangement (Sec.3.2) and the Interpolating Video-LLM Backbone (Sec.3.3) lead to the development of an interpolated Video-LLM (INTP-Video-LLM). Indeed, this enhances long-video understanding capabilities without incurring additional training costs. This advancement, however, raises a crucial question: does INTP-Video-LLM introduce any additional costs during the model inference phase?\nInference Cost Analysis. To quantify the inference costs, we employ a modified roofline-based LLM-Viewer analysis, initially developed in [Yuan et al., 2024]. Consider a typical scenario in which each frame is represented by 256 visual tokens. In standard Video-LLaVA [Lin et al., 2023] configurations, 8 frames represent a video, totaling 2048 visual tokens. With the implementation of INTP, more frames can be processed, escalating the number of visual tokens significantly. This increase is thoroughly analyzed in Tab. 1, which details the computational cost implications for the LLM backbone's decoding process, especially focusing on the model latency and memory cost.\nBy implementing this quantization technique, INTP-Video-LLM can manage the increased volume of visual tokens during inference, ensuring that the model's enhanced capabilities are aligned with practical deployment constraints.\n$K_Q = S(\\text{clamp}([\\frac{K_F}{S}] - Z, P_{min}, P_{max}) + Z)$,\n(3.7)\nwhere $[P_{min}, P_{max}]$ is the quantization range determined by bit-width, for 2 bit integer, the bins are {-2,-1,0,1}. However, direct application of this method to KV quantization is not straightforward because KV tensors require input to be accessed. To address this, we employ a calibration dataset (a significantly smaller set of input samples compared to the training dataset) to collect the necessary tensors. Once the full-precision KV tensors at the k-th layer, $K^k_F$ and $V^k_F$, are obtained, they can be quantized using the same method to produce $K^k_Q$ and $V^k_Q$ based on the quantization equation above."}, {"title": "4 EXPERIMENTS", "content": "We detail the experimental setup and model configurations in Sec. 4.1, followed by an analysis of the quantitative and qualitative performance of our approach in Sec. 4.2 and Sec. 4.4. Finally, we demonstrate the effectiveness of each component in our model in Sec.4.3."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Model Settings. We use the Video-LLaVA [Lin et al., 2023] framework to explore Video-LLM interpolation\u00b9. We employ the Vicuna-7B v1.5 as the LLM backbone, mirroring the architecture used in Video-LLaVA. The visual encoders are adapted from LanguageBind, and the text tokenization is handled by the LLaMA tokenizer, which includes approximately 32,000 classes. The projection layers consist of 2 fully connected layers, facilitating the integration of visual and textual data.\nData and Training Details. Consistent with our focus on developing a training-free method, our approach requires no traditional data training processes. This aspect underscores the innovative nature of our interpolation method, which leverages existing model architectures and tools without the need for retraining or additional data, simplifying the implementation and reducing the computational overhead typically associated with training new models.\nDatasets and Evaluation Metric. We assess INTP across a variety of zero-shot video question-answer benchmarks, which we classify as either open-ended or multiple-choice, depending on the type of question posed [Kim et al., 2024]. Our evaluation for open-ended VQA includes benchmarks like MSVD-QA [Xu et al., 2017], MSRVTT-QA [Xu et al., 2017], and ActivityNet-QA [Yu et al., 2019]. We utilize GPT-assisted assessments as outlined in Video-ChatGPT [Maaz et al., 2023], which provide a thorough evaluation of the model's response accuracy and correctness. For multiple-choice VQA tasks, we evaluate the model's performance on NExT-QA [Xiao et al., 2021] and EgoSchema [Mangalam et al., 2024]. We determine accuracy by the model's ability to correctly select the appropriate answer from the provided options."}, {"title": "4.2 QUANTITATIVE RESULTS", "content": "The experimental results for three open-ended VQA benchmarks are detailed in Tab.2, and the results for two multiple-choice VQA benchmarks are presented in Tab.3. These results demonstrate that our training-free INTP enhances the performance of existing Video-LLMs. Notably, INTP acts as a plug-and-play enhancement, boosting Video-LLM capabilities without incurring additional costs. This compatibility underscores INTP's potential as a universally applicable upgrade for current Video-LLM frameworks."}, {"title": "4.3 ABLATION STUDIES", "content": "We consider Alternative Video Token Rearrangement and Interpolating Video-LLM Backbone as one unit for realizing a long-video-LLM. Therefore, in our ablation study, we assess the impact of varying the number of frames processed by INTP-Video-LLaVA. The results are presented in Tab.4. This analysis leads to two main conclusions: (i, Performance Improvement with Increased Frames) A general improvement in performance as the number of frames increases, validating the effectiveness of the INTP. By feeding more frames into INTP-Video-LLMs are able to comprehend more extensive video content, thereby enhancing their performance. Note that INTP is realized on the pre-trained Video-LLMs in a training-free manner. (ii, Performance Plateau at Higher Frame Number) However, a performance plateau is observed when the number of frames is scaled to 64. This suggests a limitation in the NTK-based LLM backbone extension method, which appears to struggle with significantly expanding the LLM content window capacity beyond a certain point. This finding highlights a potential area for future research and optimization to further enhance the capacity of Video-LLMs to handle long video sequences effectively."}, {"title": "4.4 QUALITATIVE RESULTS", "content": "In addition to the quantitative results, we visualize the question-and-answer samples within the ActivityNet dataset to assess the qualitative performance of our approach in Fig.3. These visual-izations demonstrate that the INTP-enhanced Video-LLM is capable of effectively handling longer sequences of frames, showcasing a tangible improvement in processing and understanding extended\nvideo content. This qualitative assessment further validates the robustness of INTP-Video-LLM in real-world scenarios."}, {"title": "5 CONCLUSION", "content": "In conclusion, this paper presents INTP, an innovative interpolation method for Video-LLMs that effectively addresses the limitations of current models in processing extended video sequences. By developing an alternative video token rearrangement technique and a training-free LLM context window extension, INTP enables Video-LLMs to handle significantly more visual data (32 frames) without the need for additional training or substantial computational resources. Furthermore, the introduction of a KV-cache compression module optimizes memory usage during inference, enhancing deployment efficiency. These advancements not only extend the practical utility of Video-LLMs but also demonstrate the potential of training-free approaches in advancing Video-LLMs.\nSocietal Impacts. In this study, we introduce INTP, a interpolation method designed to enhance the capabilities of Video-LLMs for processing extended video sequences without requiring additional training. This approach can democratize the access to advanced video processing technologies by reducing computational demands. Despite these advancements, it is important to note that INTP does\nnot address potential security concerns related to the misuse of Video-LLMs. The ease of accessibility and enhanced capabilities could potentially be exploited by malicious actors to generate or manipulate video content in harmful ways. Thus, while INTP expands the practical applications of Video-LLMs and reduces barriers to their use, it also necessitates careful consideration of the ethical implications and security measures associated with their deployment."}]}