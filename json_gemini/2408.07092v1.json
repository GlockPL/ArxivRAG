{"title": "Post-Training Sparse Attention with Double Sparsity", "authors": ["Shuo Yang", "Ying Sheng", "Joseph E. Gonzalez", "Ion Stoica", "Lianmin Zheng"], "abstract": "The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve \ub04a token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1\u00d7 acceleration in attention operations and a 1.9\u00d7 improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3\u00d7 compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have significantly advanced machine learning capabilities, enabling a wide range of applications from natural language processing to complex problem-solving tasks (OpenAI, 2023; Touvron et al., 2023; Google, 2023). However, their inference remains costly and slow due to token-by-token decoding. This decoding process exhibits low arithmetic intensity, making it largely memory-bound. During decoding, access to two types of memory is required: model weights and the Key-Value (KV) cache in the self-attention layers (Vaswani et al., 2017). Both can be very large and thus become bottlenecks. When the batch size is large or the sequence length is long, the size of the KV cache can easily surpass that of the model weights (Pope et al., 2023). While extensive research has focused on reducing access to model weights through quantization and sparsification, the reduction of access to the KV cache has received less attention.\nIn this paper, we explore methods to reduce access to the KV cache during inference, thereby making attention computation more bandwidth-efficient and accelerating its execution. Our focus is on post-training methods that can be directly applied to a pre-trained model to provide wall-clock acceleration without requiring excessive additional training or fine-tuning overhead. Prior work has attempted to leverage quantization (Hooper et al., 2024; Liu et al., 2024b), compression (Nawrot et al., 2024), and sparsity (Zhang et al., 2024; Anagnostidis et al., 2024; Ge et al., 2024; Ribar et al., 2023) to achieve these goals. Among them, sparsity holds significant potential if a high sparsity ratio can be achieved. The intuition of sparsification is that not every token is equally important for decoding the next token. Therefore, during the decoding process, we can rely on a small subset of important tokens to compute the self-attention, achieving nearly the same results. While the approach of sparse attention seems intuitive, previous research has struggled to find a post-training sparse attention method that maintains high accuracy while being runtime-efficient.\nThe primary challenge in post-training sparse attention lies in accurately and efficiently identifying important tokens. A naive approach entails calculating the entire attention weight matrices and then sorting the tokens based on the accumulated attention weights. Although this method can precisely identify important tokens, it fails to offer a runtime speedup, as it requires computing the full attention weight matrices, which is precisely the step we aim to avoid. Previous studies have proposed various methods for selecting important tokens; however, these methods either lead to significant accuracy losses or fail to achieve practical wall-clock acceleration. Notably, H2O (Zhang et al., 2024) employs a dynamic strategy that maintains a small fixed-size cache of important tokens. Due to its limited size, it must evict many tokens. Since it cannot predict future important tokens, it often inadvertently evicts them, leading to accuracy degradation. SparQ (Ribar et al., 2023), in contrast, retains all tokens and dynamically selects important ones at each step. Yet, its design falls short of achieving the desired speedup and incurs considerable additional memory overhead. In summary, designing an efficient method capable of accurately identifying important tokens remains a significant challenge.\nWe propose \"Double Sparsity,\" a method that leverages both token sparsity and channel sparsity to achieve accurate and efficient post-training sparse attention. Token sparsity refers to the sparse attention method mentioned above (Zhang et al., 2024), which uses only important tokens to compute self-attention. Channel sparsity, a new method we introduced, selects important tokens at runtime using significant feature channels. Our key insight is that while token sparsity is highly dynamic, channel sparsity exhibits relatively static behavior, enabling us to identify and select important channels through offline calibration. This static channel sparsity thus provides an efficient means to achieve dynamic token sparsity at runtime. Building on this concept, we carefully explored how to select important channels based on statistics from offline calibrations. Furthermore, once we can quickly identify important tokens for the current layer, we extend this process by predicting the important tokens of the next layer. We achieve this by utilizing the embedding similarity between adjacent layers. This approach enables us to offload the entire KV cache to host memory and prefetch only the important tokens to GPU memory, significantly reducing GPU memory footprint.\nShown in Figure 1, we demonstrate that Double Sparsity can achieve both an \uacef token sparsity and an channel sparsity simultaneously, while incurring only a negligible accuracy loss across a broad array of benchmarks, including language modeling, question answering, and retrieval tasks. The sparsity directly leads to the reduction of memory access and runtime speedup. Double Sparsity accelerates the attention operation by up to 14.1\u00d7 at a sparsity level of on NVIDIA A10G and A100G GPUs, closely approaching the theoretical acceleration upper bound. It accelerates end-to-end inference for various workloads by up to 1.9\u00d7. When turning on offloading, it achieves a decoding throughput that is 16.3\u00d7 higher than the state-of-the-art offloading-based solutions at a sequence length of 256K."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Preliminaries on Self-Attention and Notations", "content": "Attention computation is one of the major bottlenecks in LLM Inference, especially when the sequence length is large (Tay et al., 2022). This is caused by its quadratic computational complexity. Let dh denote the head dimension, and S denote the number of tokens. We use the decoding step as an example to illustrate the self-attention computation. Each token carries three tensors to embed its information, which are called query, key, and value. In an attention layer, let q \u2208 Rdan represents the query tensor for input token, K \u2208 RS\u00d7dh represents the key tensor for all tokens, and V \u2208 RS\u00d7dh represents the value tensor for all tokens. The attention is obtained through the formula shown below:\ny = softmax(QK/\u221adh)\u22c5V"}, {"title": "2.2 Post-training Sparse Attention", "content": "In this work, we introduce the term \"post-training sparse attention,\" analogous to \"post-training quantization.\" Post-training sparse attention refers to techniques that exploit inherent model sparsity, such as token-level sparsity, to accelerate attention calculations without requiring additional training. In the field of LLMs, many works have utilized post-training sparse attention, including H2O, StreamingLLM (Xiao et al., 2024) and SparQ. However, these methods come with significant limitations, presenting serious challenges for post-training sparse attention."}, {"title": "3 Challenges in Post-Training Sparse Attention", "content": "In this section, we discuss prior research on post-training sparse attention, identifying the challenges and shortcomings that have prevented these approaches from achieving their full potential. More related work is included in Section 7."}, {"title": "3.1 Retrieval Accuracy", "content": "One of the most challenging issues for post-training sparse attention is maintaining retrieval accuracy. For instance, StreamingLLM discards earlier tokens, while H2O selectively drops tokens based on previous attention scores. Although discarding tokens can accelerate computations, this exclusion leads to the loss of critical information, potentially compromising the model's retrieval accuracy. As highlighted in Jelassi et al. (2024), this issue is inherent to techniques that rely on discarding tokens, prompting the exploration of sparse attention methods that preserve the complete KV cache."}, {"title": "3.2 Hardware Friendliness", "content": "Achieving wall-clock speedup poses a greater challenge while maintaining model retrieval accuracy, particularly because some post-training sparse attention techniques are not hardware-friendly. For instance, SparQ retains the complete KV cache and computes attention selectively on a subset of the KV cache based on the query. This approach theoretically allows for acceleration while maintaining accuracy. However, SparQ's method of selecting channels and tokens results in non-contiguous memory access, causing substantial L1/L2 cache misses and wasting GPU bandwidth with the standard 128-byte memory access. Despite being designed to accelerate processing, SparQ achieves only a modest 1.3 times speed increase in attention computations. Therefore, it is crucial to develop an algorithm that ensures continuous memory access patterns and avoids dynamic selection of channels to accelerate attention while preserving accuracy."}, {"title": "3.3 Memory Usage", "content": "Methods that preserve the complete KV cache inherently require substantial GPU memory con- sumption. To mitigate the heavy memory demand, the FlexGen (Sheng et al., 2023b) approach offloads the KV cache of each layer to the GPU only during the computation phase. However, a significant challenge arises because FlexGen needs to offload the complete KV cache, and the communication overhead can drastically affect overall system performance. Considering that selected tokens constitute just a small fraction of all tokens, the time taken to offload these specific tokens to the GPU is considerably less than the time required for offloading the entire KV cache as in FlexGen. By efficiently managing when and how data is transferred and processed, it's possible to significantly reduce both the time and memory overhead typically associated with maintaining a full KV cache.\nTo address these challenges, we propose two post-training sparse attention techniques. In Section 4, we introduce Double Sparsity, which accelerates attention by up to 16 \u00d7 with minimal additional memory consumption. In Section 5, we present Double Sparsity-Offload, which reduces memory usage to 1/16 without increasing latency."}, {"title": "4 Double Sparsity", "content": "Based on the insights of Section 3, we propose Double Sparsity, a hardware-friendly and bandwidth- efficient post-training sparse attention mechanism. This approach overcomes the challenges high- lighted in previous post-training sparse attention techniques by ensuring no loss of information, as it"}, {"title": "4.1 Offline Calibration", "content": "Offline calibration is a commonly used technique to identify channel sparsity, particularly effective for pinpointing outlier channels. For example, AWQ (Lin et al., 2023) utilizes offline calibration to identify salient weight channels that significantly impact model performance. Inspired by this approach, we employ offline calibration to pre-determine the channels that most influence attention scores. Attention computation can be expressed as A = Q \u00b7 KT, which can be broken down into A = \u2211an Si where S\u2081 = Qi * K\u2081. Due to channel sparsity, only a few S\u2081 have a significant impact on A. Therefore, by conducting offline calibration on a small validation set, we can efficiently identify these critical channels by computing the argmax S\u2081. Figure 7a in Appendix A illustrates the outlier channels identified by AWQ and Double Sparsity.\nTo validate the efficacy of outlier channels identified through offline calibration, we conducted a comparison in Appendix A between the outlier channel indices derived from offline calibration and those determined during the online decoding process. A significant overlap between the two sets underscores the reliability of offline-calibrated outliers. Figure 7b illustrates this relationship. An observation from the comparison is that when the ratio surpasses 0.25, the overlap reaches 0.95."}, {"title": "4.2 Forwarding with Label Cache", "content": "After identifying the outlier channel indices, it becomes crucial to access them efficiently. Reading these channels directly from the Key cache can lead to non-contiguous memory accesses, which significantly underutilized the bandwidth. To alleviate non-contiguous memory accesses, we leverage a label cache to store pre-determined heavy channel values. This label cache allows for continuous memory access when computing approximate attention, avoiding the need to retrieve non-contiguous segments from the Key cache. During the prefilling stage, all heavy channel values from the Key cache are stored in the label cache; in the decoding phase, only the heavy channel values of new tokens are added. Since approximate attention is not sensitive to precision, we can store the label cache in 4-bit. This approach enables us to maintain a label cache that is only 1/16 the size of the K cache, facilitating contiguous memory access and significantly improving the hit rate of L1/L2 caches, thereby optimizing inference speed and efficiency. In Appendix B.1, an ablation study was conducted to evaluate the impact of label caches. The results demonstrated that a label cache accelerates decoding speeds by 2 to 4 times compared to configurations without a label cache."}, {"title": "5 Reducing GPU Memory Usage with Double Sparsity-Offload", "content": "Building upon Double Sparsity, we propose the Double Sparsity-Offload technique to further reduce the GPU memory overhead in large language models. This approach significantly diminishes the memory requirement to 1/16 of the original KV caches. By optimizing memory usage, Double Sparsity-Offload enables more efficient decoding, especially with limited GPU memory resources."}, {"title": "5.1 Prefetching Tokens with Double Buffer", "content": "The Double Sparsity-Offload algorithm introduces a double buffer prefetching system during for decoding process. The complete KV cache is stored on the CPU, while the GPU maintains only the label cache and a double buffer. During the decoding process, each layer processes its embeddings through the next layer's query projection to generate an approximate query for the subsequent layer. This approximate query is then used to compute the next layer's approximate attention. While the current layer's attention and feed-forward network computations are being performed, the tokens corresponding to the approximate attention results for the next layer are offloaded to the GPU. This use of double buffering allows for a smooth and efficient overlap of computation and memory transfer."}, {"title": "5.2 Empirical Analysis: Embedding Similarity Between Layers", "content": "The feasibility of the Double Sparsity-Offload algorithm is based on the high degree of similarity between embeddings across consecutive layers. To empirically validate this assumption, we conducted an analysis using the Pile validation dataset, applied to the Llama-2-7B model. We measured the cosine similarity of embeddings between every two consecutive layers throughout the model. The results show that apart from the first two layers, the second and third layers, and the very last layers (30 and 31), all other layer pairs exhibited a cosine similarity exceeding 90%, with the majority of layers showing similarities above 95%. These high similarity scores support the viability of utilizing prior layer embeddings to predict queries for subsequent layers in Double Sparsity-Offload."}, {"title": "5.3 Complexity Analysis", "content": "To understand the potential speedup of Double Sparsity, we need to analyze its Cache IO-Complexity since attention mechanisms are bandwidth-bounded. Double Sparsity can be simplified into two steps: calculating approximate attention and computing attention over k tokens. Memory-wise, the total access comprises O(d) bytes for Q, O(S \u00d7 r) for the label cache, O(2 \u00d7 k \u00d7 d) for the KV cache, leading to a total of O(S\u00d7r +2 \u00d7 k \u00d7 d) = O(a \u00d7 S \u00d7 d + 2 \u00d7 \u03b2 \u00d7 S \u00d7 d). Given that the approximate attention phase of Double Sparsity does not involve softmax operations, it allows for high parallelism compared to the following step. Therefore, the overall IO complexity of Double Sparsity primarily depends on the latter step, which can be approximated as O(2 \u00d7 \u03b2 \u00d7 S \u00d7 d). This analysis reveals that Double Sparsity's time complexity is linearly dependent on \u1e9e, and the extra memory overhead is linearly proportional to a. Table 1 summarizes all the sparsity works discussed, specifying their overhead, complexity, and speedup."}, {"title": "6 Experiment", "content": "In Section 6.1, we demonstrate that both Double Sparsity and Double Sparsity-Offload maintain robust performance with a sparsity setting of 1/16 across various benchmarks, including Wiki-2 perplexity (Merity et al., 2016), MultifieldQA (Bai et al., 2023), GovReport (Huang et al., 2021), TriviaQA (Joshi et al., 2017), and MMLU (Hendrycks et al., 2021). In key-value retrieval tasks, Double Sparsity significantly outperforms other post-training sparse attention techniques. In Section 6.2, we compare Double Sparsity against state-of-the-art attention and end-to-end implementations. Results show that Double Sparsity achieves up to a 16-fold acceleration in attention mechanisms and up to a twofold increase in overall end-to-end processing speed. Additionally, Double Sparsity- Offload achieves a 16-fold acceleration compared to FlexGen Offload."}, {"title": "6.1 Accuracy Evaluation", "content": ""}, {"title": "6.1.1 Wiki-2 Perplexity", "content": "Wiki-2 perplexity is a benchmark derived from Wikipedia articles, offering a comprehensive test with its broad vocabulary and authentic text features. A lower score indicates better model performance. Table 2 illustrates the changes in perplexity across different sparsity levels for each model.\nTo demonstrate the model's performance at different sparsity levels and justify our selection of a sparsity level of 1/16, we constructed a 3D bar chart. According to Figure 9 in Appendix C, a noticeable shift in perplexity is observed as the sparsity level goes beyond 1/16.\nTo validate the robustness of Double Sparsity and the effectiveness of the 1/16 sparsity level, we conducted a series of ablation studies across various model configurations and conditions. Table 3 demonstrates the effectiveness of Double Sparsity at a sparsity level of 1/16 across various model sizes, attention mechanisms, and MoE configurations."}, {"title": "6.1.2 Long Context Benchmarks", "content": "We used Llama-2-7B to evaluate the performance of Double Sparsity across multiple long context benchmarks at various levels of sparsity, comparing its effectiveness with that of StreamingLLM and H2O. As illustrated in Figure 3, Double Sparsity maintains its performance with nearly no drop in accuracy at a sparsity level of 1/16, outperforming other techniques."}, {"title": "6.1.3 Key-Value Retrieval", "content": "The key-value retrieval benchmark is designed to assess a model's in-context retrieval capabilities. Our experiments compared Double Sparsity against other post-training sparsity techniques, including H2O, StreamingLLM, and RTN quantization (Nagel et al., 2020). We also tested the performance of Double Sparsity with the Vicuna-7B-16K model to observe how accuracy changes as context length increases. As shown in Figure 4, we demonstrate that Double Sparsity significantly surpasses the other techniques in key-value retrieval tasks. Notably, Double Sparsity and Double Sparsity-Offload show equivalent performance, highlighting that the offloading mechanism exhibits almost no decay."}, {"title": "6.2 Speedup Evaluation", "content": ""}, {"title": "6.2.1 Setups", "content": "Hardware. Our experiments were conducted on two types of GPUs: the A10G and the A100-SXM.\nImplementation. For the Double Sparsity Attention, we utilized PyTorch to compute approximate attention and select the top-k tokens. The kernel for attention over top-k tokens was designed using OpenAI Triton. For end-to-end testing, we replaced the full attention mechanism in gpt-fast (Py- Torch, 2023) with our Double Sparsity Attention. For Double Sparsity-Offload, we implemented asynchronous CPU to GPU memory copying using CUDA streams and DGL (Wang et al., 2019)'s gathering kernel.\nWorkload. We focused on high-workload scenarios to push the limits of Double Sparsity. This included a range of batch sizes from 4 to 32 and sequence lengths from 1024 to 16384. For Double Sparsity-Offload, we extended testing to extreme conditions on the A100 GPU, exploring sequence lengths from 64K to 256K. Given that gpt-fast's KV cache is pre-allocated, the tokens-per-second throughput depends solely on the batch size and sequence length.\nBaseline. For attention acceleration evaluations, we use the \u2018scaled_dot_product_attention\u2019 as our baseline. This implementation ranks among the fastest attention mechanisms, dynamically allocating computation among the most efficient options including FlashAttention-2 (Dao, 2023), Memory-Efficient Attention (Lefaudeux et al., 2022), and the top-performing kernels from the PyTorch team. In the end-to-end speed evaluations of Double Sparsity, gpt-fast serves as the baseline, distinguished as the state-of-the-art for Llama models on the A100 GPU. It offers exceptionally low"}, {"title": "6.2.2 Attention Operator Speedup", "content": "Figure 5 provides a comprehensive view of the latency and speedup of Double Sparsity compared to 'scaled_dot_product_attention' across different batch sizes and sequence lengths. On the A10G GPU, every case achieves at least a fivefold speedup, with more than half exceeding ninefold. Notably, Double Sparsity achieves a linear speedup at a sequence length of 4096 with large batch sizes. On the A100 GPU, nearly all cases see at least fourfold faster processing, with larger batches reaching up to tenfold speedup. The greater speedup for smaller batches on the A10G might be due to the launch time of Triton kernels, which becomes significant when the kernel execution time on the A100 is short."}, {"title": "6.2.3 End-to-End Inference Speedup", "content": "Figure 6 (a)(b) presents the throughput comparison between Double Sparsity and gpt-fast, measured in tokens per second across various batch sizes and sequence lengths. We deployed the Llama-2-7B model and maximized memory usage to achieve high workload conditions. The results indicate that Double Sparsity yields a minimum speedup of 1.3x across all tested conditions. In certain scenarios, the speedup approached twofold, showcasing Double Sparsity's overall efficiency.\nIn Figure 6 (c)(d), we compare the throughput of Double Sparsity-Offload to FlexGen under a constrained memory footprint, set at 1/16 of a full KV cache for both methods. Both techniques utilize a double buffer for asynchronous data copying. The results show that Double Sparsity-Offload achieves a 4-8\u00d7 speedup over FlexGen under regular workloads, and a 16\u00d7 speedup in scenarios with long texts ranging from 64K to 256K in sequence length."}, {"title": "7 Related Work", "content": "Sparse Attention Inference Due to the significant overhead of attention mechanisms, many studies have focused on exploiting attention sparsity to accelerate inference. These efforts can be categorized under three main criteria: 1) static or dynamic sparse patterns; 2) the presence of token eviction; 3) accelerating pre-filling or decoding. StreamingLLM (Xiao et al., 2024) and LM-Infinite (Han et al., 2023) utilize static sparse patterns with token eviction to accelerate decoding. These approaches achieve inference acceleration by preserving only a small number of initial tokens along with local tokens. H2O (Zhang et al., 2024) and Scissorhands (Liu et al., 2024a) employ dynamic sparse patterns with token eviction for decoding, preserving only a small fraction of the KV cache called heavy hitters according to accumulated attention scores, while FastGen (Ge et al., 2024) uses adaptive sparse attention patterns for different attention heads. MInference (Jiang et al., 2024) serves as a prefilling acceleration method that retains all tokens. It first identifies sparse patterns within the model, and then leverages these identified patterns to compute the pre-filling stage. SparQ (Ribar et al., 2023) and Quest (Tang et al., 2024) implement dynamic sparse decoding while also preserving all tokens. SparQ filters the top-k tokens using heavy channels of queries. Quest segments tokens into multiple pages and computes attention on the top-k pages to facilitate the decoding process.\nSparse Attention Training There are also many efforts to reduce attention complexity through training (Qiu et al., 2020; Ding et al., 2023; Tay et al., 2020; Chen et al., 2021). For example, Sparse transformer (Child et al., 2019) reduces the complexity to O(n\u221an) by introducing sparse factorization of the attention matrix. Reformer (Kitaev et al., 2019) achieves O(nlog n) complexity via locality-sensitive hashing. Longformer (Beltagy et al., 2020), BigBard (Zaheer et al., 2020), and Linformer (Wang et al., 2020) further reduce the complexity to linear. Linear attention architectures have also been proposed in Katharopoulos et al. (2020).\nOther Attention and Inference Optimizations Despite efforts to sparsify the attention compu- tation, there are many other optimizations for attention efficiency. Common techniques include quantization and compression (Hooper et al., 2024; Liu et al., 2024b; Kang et al., 2024; Nawrot et al., 2024), efficient attention architecture like multi-query attention (Shazeer, 2019) and group-query attention (Ainslie et al., 2023), and memory-efficient attention algorithms (Rabe & Staats, 2021; Dao et al., 2022). Alternatives to transformers include using the state space model to remove the attention mechanism (Gu et al., 2021). Other common inference optimizations for LLMs include batching Yu et al. (2022), memory optimizations Sheng et al. (2023b); Kwon et al. (2023); Aminabadi et al. (2022), parameter sharing Sheng et al. (2023a); Chen et al. (2023), speculative decoding Stern et al. (2018); Leviathan et al. (2023); Miao et al. (2023), scheduling Han et al. (2022); Agrawal et al. (2023); Patel et al. (2023); Zhong et al. (2024), quantization Xiao et al. (2023); Lin et al. (2023); Dettmers et al. (2022); Frantar et al. (2022), and sparsification Frantar & Alistarh (2023)."}, {"title": "8 Future Directions and Conclusion", "content": "Future Directions. Despite the progress made with Double Sparsity, several limitations remain that reveal promising directions for future research. It is challenging to perfectly overlap communication with computation. Enhancing asynchronous capabilities to mask communication overheads presents a promising direction that allows for significant acceleration with a minimal memory footprint.\nConclusion. In this work, we introduced Double Sparsity and Double Sparsity-Offload, innovative post-training sparse attention techniques. Double Sparsity leverages offline calibration and label cache to achieve nearly lossless performance across various benchmarks at a 1/16 sparsity level. Performance tests showed that Double Sparsity could accelerate attention computations by up to 16\u00d7 and achieve an end-to-end speedup of 1.9\u00d7. Double Sparsity-Offload significantly reduced KV Cache memory usage to 1/16, outperforming the throughput of previous SOTA offloading techniques by 16 times."}, {"title": "A Offline Calibration Illustration", "content": "The x-axis of the Figure 7b denotes the ratio of the selected top-k channels to the total number of channels, while the y-axis quantifies the degree of overlap between the offline and online outliers."}, {"title": "B Ablation Study", "content": ""}, {"title": "B.1 Forward without Label Cache", "content": "To investigate the significance of the label cache in the forward pass of Double Sparsity, we conducted experiments comparing performance with and without the label cache. As depicted in Table 4, label caches significantly enhance the forward pass, yielding a speedup ranging from 2 to 4 times."}, {"title": "B.2 Embedding Similarity Across Layers", "content": ""}, {"title": "C Perplexity Selection Illustration", "content": "Figure 9 uses token-level sparsity as the x-axis, channel-level sparsity as the y-axis, and 10-perplexity values as the z-axis, where higher bars indicate better performance. A sudden shift in perplexity is observed as the sparsity level goes beyond 1/16."}]}