{"title": "Post-Training Sparse Attention with Double Sparsity", "authors": ["Shuo Yang", "Ying Sheng", "Joseph E. Gonzalez", "Ion Stoica", "Lianmin Zheng"], "abstract": "The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve \ub04a token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1\u00d7 acceleration in attention operations and a 1.9\u00d7 improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3\u00d7 compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have significantly advanced machine learning capabilities, enabling a wide range of applications from natural language processing to complex problem-solving tasks (OpenAI, 2023; Touvron et al., 2023; Google, 2023). However, their inference remains costly and slow due to token-by-token decoding. This decoding process exhibits low arithmetic intensity, making it largely memory-bound. During decoding, access to two types of memory is required: model weights and the Key-Value (KV) cache in the self-attention layers (Vaswani et al., 2017). Both can be very large and thus become bottlenecks. When the batch size is large or the sequence length is long, the size of the KV cache can easily surpass that of the model weights (Pope et al., 2023). While extensive research has focused on reducing access to model weights through quantization and sparsification, the reduction of access to the KV cache has received less attention.\nIn this paper, we explore methods to reduce access to the KV cache during inference, thereby making attention computation more bandwidth-efficient and accelerating its execution. Our focus is on post-training methods that can be directly applied to a pre-trained model to provide wall-clock acceleration without requiring excessive additional training or fine-tuning overhead. Prior work has attempted to leverage quantization (Hooper et al., 2024; Liu et al., 2024b), compression (Nawrot et al., 2024), and sparsity (Zhang et al., 2024; Anagnostidis et al., 2024; Ge et al., 2024; Ribar et al., 2023) to achieve these goals. Among them, sparsity holds significant potential if a high sparsity ratio can be achieved. The intuition of sparsification is that not every token is equally important for decoding the next token. Therefore, during the decoding process, we can rely on a small subset of important tokens to compute the self-attention, achieving nearly the same results. While the approach"}, {"title": "Background", "content": null}, {"title": "Preliminaries on Self-Attention and Notations", "content": "Attention computation is one of the major bottlenecks in LLM Inference, especially when the sequence length is large (Tay et al., 2022). This is caused by its quadratic computational complexity. Let dh denote the head dimension, and S denote the number of tokens. We use the decoding step as an example to illustrate the self-attention computation. Each token carries three tensors to embed its information, which are called query, key, and value. In an attention layer, let $q \\in R^{d_h}$ represents the query tensor for input token, $K \\in R^{S \\times d_h}$ represents the key tensor for all tokens, and $V \\in R^{S \\times d_h}$ represents the value tensor for all tokens. The attention is obtained through the formula shown below:\n$y = \\text{softmax} (\\frac{q.K^T}{\\sqrt{d_h}}) . V$"}, {"title": "Post-training Sparse Attention", "content": "In this work, we introduce the term \"post-training sparse attention,\" analogous to \"post-training quantization.\" Post-training sparse attention refers to techniques that exploit inherent model sparsity, such as token-level sparsity, to accelerate attention calculations without requiring additional training. In the field of LLMs, many works have utilized post-training sparse attention, including H2O, StreamingLLM (Xiao et al., 2024) and SparQ. However, these methods come with significant limitations, presenting serious challenges for post-training sparse attention."}, {"title": "Challenges in Post-Training Sparse Attention", "content": "In this section, we discuss prior research on post-training sparse attention, identifying the challenges and shortcomings that have prevented these approaches from achieving their full potential. More related work is included in Section 7."}, {"title": "Retrieval Accuracy", "content": "One of the most challenging issues for post-training sparse attention is maintaining retrieval accuracy. For instance, StreamingLLM discards earlier tokens, while H2O selectively drops tokens based on previous attention scores. Although discarding tokens can accelerate computations, this exclusion leads to the loss of critical information, potentially compromising the model's retrieval accuracy. As highlighted in Jelassi et al. (2024), this issue is inherent to techniques that rely on discarding tokens, prompting the exploration of sparse attention methods that preserve the complete KV cache."}, {"title": "Hardware Friendliness", "content": "Achieving wall-clock speedup poses a greater challenge while maintaining model retrieval accuracy, particularly because some post-training sparse attention techniques are not hardware-friendly. For instance, SparQ retains the complete KV cache and computes attention selectively on a subset of the KV cache based on the query. This approach theoretically allows for acceleration while maintaining accuracy. However, SparQ's method of selecting channels and tokens results in non-contiguous memory access, causing substantial L1/L2 cache misses and wasting GPU bandwidth with the standard 128-byte memory access. Despite being designed to accelerate processing, SparQ achieves only a modest 1.3 times speed increase in attention computations. Therefore, it is crucial to develop an algorithm that ensures continuous memory access patterns and avoids dynamic selection of channels to accelerate attention while preserving accuracy."}, {"title": "Memory Usage", "content": "Methods that preserve the complete KV cache inherently require substantial GPU memory consumption. To mitigate the heavy memory demand, the FlexGen (Sheng et al., 2023b) approach offloads the KV cache of each layer to the GPU only during the computation phase. However, a significant challenge arises because FlexGen needs to offload the complete KV cache, and the communication overhead can drastically affect overall system performance. Considering that selected tokens constitute just a small fraction of all tokens, the time taken to offload these specific tokens to the GPU is considerably less than the time required for offloading the entire KV cache as in FlexGen. By efficiently managing when and how data is transferred and processed, it's possible to significantly reduce both the time and memory overhead typically associated with maintaining a full KV cache.\nTo address these challenges, we propose two post-training sparse attention techniques. In Section 4, we introduce Double Sparsity, which accelerates attention by up to 16 \u00d7 with minimal additional memory consumption. In Section 5, we present Double Sparsity-Offload, which reduces memory usage to 1/16 without increasing latency."}, {"title": "Double Sparsity", "content": "Based on the insights of Section 3, we propose Double Sparsity, a hardware-friendly and bandwidth-efficient post-training sparse attention mechanism. This approach overcomes the challenges highlighted in previous post-training sparse attention techniques by ensuring no loss of information, as it"}, {"title": "Offline Calibration", "content": "Offline calibration is a commonly used technique to identify channel sparsity, particularly effective for pinpointing outlier channels. For example, AWQ (Lin et al., 2023) utilizes offline calibration to identify salient weight channels that significantly impact model performance. Inspired by this approach, we employ offline calibration to pre-determine the channels that most influence attention scores. Attention computation can be expressed as A = Q \u00b7 KT, which can be broken down into A = \u2211an Si where S\u2081 = Qi * K\u2081. Due to channel sparsity, only a few S\u2081 have a significant impact on A. Therefore, by conducting offline calibration on a small validation set, we can efficiently identify these critical channels by computing the argmax S\u2081. Figure 7a in Appendix A illustrates the outlier channels identified by AWQ and Double Sparsity.\nTo validate the efficacy of outlier channels identified through offline calibration, we conducted a comparison in Appendix A between the outlier channel indices derived from offline calibration and those determined during the online decoding process. A significant overlap between the two sets underscores the reliability of offline-calibrated outliers. Figure 7b illustrates this relationship. An observation from the comparison is that when the ratio surpasses 0.25, the overlap reaches 0.95."}, {"title": "Forwarding with Label Cache", "content": "After identifying the outlier channel indices, it becomes crucial to access them efficiently. Reading these channels directly from the Key cache can lead to non-contiguous memory accesses, which significantly underutilized the bandwidth. To alleviate non-contiguous memory accesses, we leverage a label cache to store pre-determined heavy channel values. This label cache allows for continuous memory access when computing approximate attention, avoiding the need to retrieve non-contiguous segments from the Key cache. During the prefilling stage, all heavy channel values from the Key cache are stored in the label cache; in the decoding phase, only the heavy channel values of new tokens are added. Since approximate attention is not sensitive to precision, we can store the label cache in 4-bit. This approach enables us to maintain a label cache that is only 1/16 the size of the K cache, facilitating contiguous memory access and significantly improving the hit rate of L1/L2 caches, thereby optimizing inference speed and efficiency. In Appendix B.1, an ablation study was conducted to evaluate the impact of label caches. The results demonstrated that a label cache accelerates decoding speeds by 2 to 4 times compared to configurations without a label cache."}, {"title": "Reducing GPU Memory Usage with Double Sparsity-Offload", "content": "Building upon Double Sparsity, we propose the Double Sparsity-Offload technique to further reduce the GPU memory overhead in large language models. This approach significantly diminishes the memory requirement to 1/16 of the original KV caches. By optimizing memory usage, Double Sparsity-Offload enables more efficient decoding, especially with limited GPU memory resources."}, {"title": "Prefetching Tokens with Double Buffer", "content": "The Double Sparsity-Offload algorithm introduces a double buffer prefetching system during for decoding process. The complete KV cache is stored on the CPU, while the GPU maintains only the label cache and a double buffer. During the decoding process, each layer processes its embeddings through the next layer's query projection to generate an approximate query for the subsequent layer. This approximate query is then used to compute the next layer's approximate attention. While the current layer's attention and feed-forward network computations are being performed, the tokens corresponding to the approximate attention results for the next layer are offloaded to the GPU. This use of double buffering allows for a smooth and efficient overlap of computation and memory transfer."}, {"title": "Empirical Analysis: Embedding Similarity Between Layers", "content": "The feasibility of the Double Sparsity-Offload algorithm is based on the high degree of similarity between embeddings across consecutive layers. To empirically validate this assumption, we conducted an analysis using the Pile validation dataset, applied to the Llama-2-7B model. We measured the cosine similarity of embeddings between every two consecutive layers throughout the model. The results show that apart from the first two layers, the second and third layers, and the very last layers (30 and 31), all other layer pairs exhibited a cosine similarity exceeding 90%, with the majority of layers showing similarities above 95%. These high similarity scores support the viability of utilizing prior layer embeddings to predict queries for subsequent layers in Double Sparsity-Offload."}, {"title": "Complexity Analysis", "content": "To understand the potential speedup of Double Sparsity, we need to analyze its Cache IO-Complexity since attention mechanisms are bandwidth-bounded. Double Sparsity can be simplified into two steps: calculating approximate attention and computing attention over k tokens. Memory-wise, the total access comprises O(d) bytes for Q, O(S \u00d7 r) for the label cache, O(2 \u00d7 k \u00d7 d) for the KV cache, leading to a total of O(S\u00d7r +2 \u00d7 k \u00d7 d) = O(a \u00d7 S \u00d7 d + 2 \u00d7 \u03b2 \u00d7 S \u00d7 d). Given that the approximate attention phase of Double Sparsity does not involve softmax operations, it allows for high parallelism compared to the following step. Therefore, the overall IO complexity of Double Sparsity primarily depends on the latter step, which can be approximated as O(2 \u00d7 \u03b2 \u00d7 S \u00d7 d). This analysis reveals that Double Sparsity's time complexity is linearly dependent on \u1e9e, and the extra memory overhead is linearly proportional to a. Table 1 summarizes all the sparsity works discussed, specifying their overhead, complexity, and speedup."}, {"title": "Experiment", "content": "In Section 6.1, we demonstrate that both Double Sparsity and Double Sparsity-Offload maintain robust performance with a sparsity setting of 1/16 across various benchmarks, including Wiki-2 perplexity (Merity et al., 2016), MultifieldQA (Bai et al., 2023), GovReport (Huang et al., 2021), TriviaQA (Joshi et al., 2017), and MMLU (Hendrycks et al., 2021). In key-value retrieval tasks, Double Sparsity significantly outperforms other post-training sparse attention techniques. In Section 6.2, we compare Double Sparsity against state-of-the-art attention and end-to-end implementations. Results show that Double Sparsity achieves up to a 16-fold acceleration in attention mechanisms and up to a twofold increase in overall end-to-end processing speed. Additionally, Double Sparsity-Offload achieves a 16-fold acceleration compared to FlexGen Offload."}, {"title": "Accuracy Evaluation", "content": null}, {"title": "Wiki-2 Perplexity", "content": "Wiki-2 perplexity is a benchmark derived from Wikipedia articles, offering a comprehensive test with its broad vocabulary and authentic text features. A lower score indicates better model performance. Table 2 illustrates the changes in perplexity across different sparsity levels for each model.\nTo demonstrate the model's performance at different sparsity levels and justify our selection of a sparsity level of 1/16, we constructed a 3D bar chart. According to Figure 9 in Appendix C, a noticeable shift in perplexity is observed as the sparsity level goes beyond 1/16.\nTo validate the robustness of Double Sparsity and the effectiveness of the 1/16 sparsity level, we conducted a series of ablation studies across various model configurations and conditions. Table 3 demonstrates the effectiveness of Double Sparsity at a sparsity level of 1/16 across various model sizes, attention mechanisms, and MoE configurations."}, {"title": "Long Context Benchmarks", "content": "We used Llama-2-7B to evaluate the performance of Double Sparsity across multiple long context benchmarks at various levels of sparsity, comparing its effectiveness with that of StreamingLLM and H2O. As illustrated in Figure 3, Double Sparsity maintains its performance with nearly no drop in accuracy at a sparsity level of 1/16, outperforming other techniques."}, {"title": "Key-Value Retrieval", "content": "The key-value retrieval benchmark is designed to assess a model's in-context retrieval capabilities. Our experiments compared Double Sparsity against other post-training sparsity techniques, including H2O, StreamingLLM, and RTN quantization (Nagel et al., 2020). We also tested the performance of Double Sparsity with the Vicuna-7B-16K model to observe how accuracy changes as context length increases. As shown in Figure 4, we demonstrate that Double Sparsity significantly surpasses the other techniques in key-value retrieval tasks. Notably, Double Sparsity and Double Sparsity-Offload show equivalent performance, highlighting that the offloading mechanism exhibits almost no decay."}, {"title": "Speedup Evaluation", "content": null}, {"title": "Setups", "content": null}, {"title": "Hardware.", "content": "Our experiments were conducted on two types of GPUs: the A10G and the A100-SXM."}, {"title": "Implementation.", "content": "For the Double Sparsity Attention, we utilized PyTorch to compute approximate attention and select the top-k tokens. The kernel for attention over top-k tokens was designed using OpenAI Triton. For end-to-end testing, we replaced the full attention mechanism in gpt-fast (Py-Torch, 2023) with our Double Sparsity Attention. For Double Sparsity-Offload, we implemented asynchronous CPU to GPU memory copying using CUDA streams and DGL (Wang et al., 2019)'s gathering kernel."}, {"title": "Workload.", "content": "We focused on high-workload scenarios to push the limits of Double Sparsity. This included a range of batch sizes from 4 to 32 and sequence lengths from 1024 to 16384. For Double Sparsity-Offload, we extended testing to extreme conditions on the A100 GPU, exploring sequence lengths from 64K to 256K. Given that gpt-fast's KV cache is pre-allocated, the tokens-per-second throughput depends solely on the batch size and sequence length."}, {"title": "Baseline.", "content": "For attention acceleration evaluations, we use the \u2018scaled_dot_product_attention\u2019 as our baseline. This implementation ranks among the fastest attention mechanisms, dynamically allocating computation among the most efficient options including FlashAttention-2 (Dao, 2023), Memory-Efficient Attention (Lefaudeux et al., 2022), and the top-performing kernels from the PyTorch team. In the end-to-end speed evaluations of Double Sparsity, gpt-fast serves as the baseline, distinguished as the state-of-the-art for Llama models on the A100 GPU. It offers exceptionally low"}, {"title": "Attention Operator Speedup", "content": "Figure 5 provides a comprehensive view of the latency and speedup of Double Sparsity compared to 'scaled_dot_product_attention' across different batch sizes and sequence lengths. On the A10G GPU, every case achieves at least a fivefold speedup, with more than half exceeding ninefold. Notably, Double Sparsity achieves a linear speedup at a sequence length of 4096 with large batch sizes. On the A100 GPU, nearly all cases see at least fourfold faster processing, with larger batches reaching up to tenfold speedup. The greater speedup for smaller batches on the A10G might be due to the launch time of Triton kernels, which becomes significant when the kernel execution time on the A100 is short."}, {"title": "End-to-End Inference Speedup", "content": "Figure 6 (a)(b) presents the throughput comparison between Double Sparsity and gpt-fast, measured in tokens per second across various batch sizes and sequence lengths. We deployed the Llama-2-7B model and maximized memory usage to achieve high workload conditions. The results indicate that"}, {"title": "Future Directions and Conclusion", "content": "Future Directions. Despite the progress made with Double Sparsity, several limitations remain that reveal promising directions for future research. It is challenging to perfectly overlap communication with computation. Enhancing asynchronous capabilities to mask communication overheads presents a promising direction that allows for significant acceleration with a minimal memory footprint.\nConclusion. In this work, we introduced Double Sparsity and Double Sparsity-Offload, innovative post-training sparse attention techniques. Double Sparsity leverages offline calibration and label cache to achieve nearly lossless performance across various benchmarks at a 1/16 sparsity level. Performance tests showed that Double Sparsity could accelerate attention computations by up to 16\u00d7 and achieve an end-to-end speedup of 1.9\u00d7. Double Sparsity-Offload significantly reduced KV Cache memory usage to 1/16, outperforming the throughput of previous SOTA offloading techniques by 16 times."}, {"title": "Related Work", "content": "Sparse Attention Inference Due to the significant overhead of attention mechanisms, many studies have focused on exploiting attention sparsity to accelerate inference. These efforts can be categorized under three main criteria: 1) static or dynamic sparse patterns; 2) the presence of token eviction; 3) accelerating pre-filling or decoding. StreamingLLM (Xiao et al., 2024) and LM-Infinite (Han et al., 2023) utilize static sparse patterns with token eviction to accelerate decoding. These approaches achieve inference acceleration by preserving only a small number of initial tokens along with local tokens. H2O (Zhang et al., 2024) and Scissorhands (Liu et al., 2024a) employ dynamic sparse patterns with token eviction for decoding, preserving only a small fraction of the KV cache called heavy hitters according to accumulated attention scores, while FastGen (Ge et al., 2024) uses adaptive sparse attention patterns for different attention heads. MInference (Jiang et al., 2024) serves as a prefilling acceleration method that retains all tokens. It first identifies sparse patterns within the model, and then leverages these identified patterns to compute the pre-filling stage. SparQ (Ribar et al., 2023) and Quest (Tang et al., 2024) implement dynamic sparse decoding while also preserving all tokens. SparQ filters the top-k tokens using heavy channels of queries. Quest segments tokens into multiple pages and computes attention on the top-k pages to facilitate the decoding process.\nSparse Attention Training There are also many efforts to reduce attention complexity through training (Qiu et al., 2020; Ding et al., 2023; Tay et al., 2020; Chen et al., 2021). For example, Sparse transformer (Child et al., 2019) reduces the complexity to O(n\u221an) by introducing sparse factorization of the attention matrix. Reformer (Kitaev et al., 2019) achieves O(nlog n) complexity via locality-sensitive hashing. Longformer (Beltagy et al., 2020), BigBard (Zaheer et al., 2020), and Linformer (Wang et al., 2020) further reduce the complexity to linear. Linear attention architectures have also been proposed in Katharopoulos et al. (2020).\nOther Attention and Inference Optimizations Despite efforts to sparsify the attention computation, there are many other optimizations for attention efficiency. Common techniques include quantization and compression (Hooper et al., 2024; Liu et al., 2024b; Kang et al., 2024; Nawrot et al., 2024), efficient attention architecture like multi-query attention (Shazeer, 2019) and group-query attention (Ainslie et al., 2023), and memory-efficient attention algorithms (Rabe & Staats, 2021; Dao et al., 2022). Alternatives to transformers include using the state space model to remove the attention mechanism (Gu et al., 2021). Other common inference optimizations for LLMs include batching Yu et al. (2022), memory optimizations Sheng et al. (2023b); Kwon et al. (2023); Aminabadi et al. (2022), parameter sharing Sheng et al. (2023a); Chen et al. (2023), speculative decoding Stern et al. (2018); Leviathan et al. (2023); Miao et al. (2023), scheduling Han et al. (2022); Agrawal et al. (2023); Patel et al. (2023); Zhong et al. (2024), quantization Xiao et al. (2023); Lin et al. (2023); Dettmers et al. (2022); Frantar et al. (2022), and sparsification Frantar & Alistarh (2023)."}, {"title": "Offline Calibration Illustration", "content": "The x-axis of the Figure 7b denotes the ratio of the selected top-k channels to the total number of channels, while the y-axis quantifies the degree of overlap between the offline and online outliers."}, {"title": "Ablation Study", "content": null}, {"title": "Forward without Label Cache", "content": "To investigate the significance of the label cache in the forward pass of Double Sparsity, we conducted experiments comparing performance with and without the label cache. As depicted in Table 4, label caches significantly enhance the forward pass, yielding a speedup ranging from 2 to 4 times."}, {"title": "Embedding Similarity Across Layers", "content": null}, {"title": "Perplexity Selection Illustration", "content": "Figure 9 uses token-level sparsity as the x-axis, channel-level sparsity as the y-axis, and 10-perplexity values as the z-axis, where higher bars indicate better performance. A sudden shift in perplexity is observed as the sparsity level goes beyond 1/16."}, {"title": "NeurIPS Paper Checklist", "content": null}, {"title": "Claims", "content": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes]\nJustification: The claims in the abstract and introductions are empirically verified by the experimental section 6.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper."}, {"title": "Limitations", "content": "Question: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The limitations and future directions to address them are discussed in 8.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations."}, {"title": "Theory Assumptions and Proofs", "content": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\nAnswer: [NA]"}, {"title": "Experimental Result Reproducibility", "content": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: The experimental setups are described in Section 6.1 and Section 6.2. The code is available.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results."}, {"title": "Open access to data and code", "content": null}, {"title": "Experimental Setting/Details", "content": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\nAnswer: [Yes]\nJustification: The experimental setups are described in Section 6.1 and Section 6.2. The code is available. The code is available.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material."}, {"title": "Experiment Statistical Significance", "content": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: We measure the throughput and latency by running a sufficiently large number of tests and reporting the average. These results are highly deterministic, so we do not include error bars in the figures to maintain clarity. The improvements are statistically significant.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper."}, {"title": "Experiments Compute Resources", "content": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\nAnswer: [Yes]\nJustification: The experimental setups are described in Section 6.2. We use common NVIDIA GPUs to conduct experiments. The code is available.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper)."}, {"title": "Code Of Ethics", "content": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The research respects NeurIPS Code of Ethics.\nGuidelines:\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid-eration due to laws or regulations in their jurisdiction)."}, {"title": "Broader Impacts", "content": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\nAnswer: [NA", "nJustification": "The techniques introduced in this paper make the use of large language models easier and more efficient. They do not have additional direct"}]}