{"title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis", "authors": ["Qiushi Sun", "Kanzhi Cheng", "Zichen Ding", "Chuanyang Jin", "Yian Wang", "Fangzhi Xu", "Zhenyu Wu", "Liheng Chen", "Chengyou Jia", "Zhoumianze Liu", "Ben Kao", "Guohao Li", "Junxian He", "Yu Qiao", "Zhiyong Wu"], "abstract": "Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at OS-Genesis Homepage.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Vision-Language Models (VLMs; Chen et al., 2024b; Wang et al., 2024b) have driven researchers to build a variety of language agents (Sumers et al., 2024). As an emerging class of AI systems, these agents are being explored for their potential to automate complicated computer tasks on Graphical User Interfaces (GUIs), aiming to achieve digital automation (Anthropic, 2023; Hu et al., 2024). To complete GUI tasks autonomously, an agent must possess key capabilities: understanding user intentions, planning tasks, and executing actions. Therefore, using high-quality trajectories for training is essential for improving their agentic capabilities (Zheng et al., 2024c).\nAs illustrated in Figure 1, ideal GUI agent trajectories contain the following key components: (1) a high-level instruction that defines the overall goal the agent aims to accomplish, (2) a series of low-level instructions that each describe specific steps required, (3) actions (e.g., CLICK, TYPE) and (4) states, which include visual representations like screenshots and textual representations such as a11ytree\u00b9. Such data enable end-to-end training of GUI agents, extending their capabilities from automating actions (Cheng et al., 2024b) to achieving full-process autonomy (Zhang et al., 2024a).\nHowever, collecting such trajectories is far from trivial. Existing task-driven methods, which rely on humans or machines executing pre-defined tasks, face the following limitations: human collection requires annotators to label entire trajectories and pre-define high-level tasks manually (Li et al., 2024; L\u00f9 et al., 2024), making it both costly and labor-intensive. Model-based synthesis also faces critical challenges: (1) it heavily depends on pre-defined high-level tasks (Lai et al., 2024), which not only limit the scalability of synthesized data but also constrain its diversity; and (2) it struggles to ensure data quality, as errors in intermediate steps or mismatched task objectives can lead to incomplete or incoherent trajectories (Murty et al., 2024b; Patel et al., 2024). Above mentioned issues pose a bottleneck for advancing GUI agents. These issues lead to a critical bottleneck for advancing GUI agents. Thus, effective trajectory construction methods are a clear desideratum for addressing these challenges.\nIn this paper, we present OS-Genesis, a pipeline for synthesizing high-quality and diverse GUI agent trajectories without involving human supervision or pre-defined tasks. Recognizing the limitations of the aforementioned task-driven methods, we draw inspiration from how humans learn to interact with GUI applications and adopt an interaction-driven approach. OS-Genesis begins by exploring the functionality of GUI environments through traversing interactive UI elements with actions (e.g., CLICK). This forms the basis for reverse task synthesis, where observed states and actions are retroactively transformed into low-level instructions. These low-level instructions are then derived into high-level instructions, which can seed the collection of GUI trajectories. By uncovering considerable functionalities, reverse task synthesis facilitates the creation of meaningful and executable tasks. Moreover, it naturally bridges the gap between abstract instructions and the dynamic nature of GUIs. Once synthesized tasks are converted into trajectories, we introduce a reward model to ensure data quality and effective utilization.\nExperiments on two challenging online benchmarks,\na11ytree: Accessibility (a11y) trees are informative structures in software or web applications, each allytree node corresponds to a UI element on the screen.\nmarks, AndroidWorld and WebArena, demonstrate the effectiveness of OS-Genesis. It surpasses task-driven methods by a large margin, nearly doubling the performance from 9.82% to 17.41% on AndroidWorld. This highlights the high quality of trajectories synthesized by OS-Genesis and its great potential to transform general-purpose VLMs into specialized GUI agents.\nOur primary contributions are as follows:\n\u2022 By shifting from task-driven approaches to interaction-driven GUI agent data construction, we introduce reverse task synthesis to improve trajectory quality and diversity.\n\u2022 We propose a novel pipeline, OS-Genesis, capable of efficiently synthesizing high-quality trajectory data. Without human supervision, OS-Genesis supports end-to-end training of GUI agents across environments.\n\u2022 Extensive experiments across mobile and web tasks on dynamic benchmarks demonstrate the superior performance of OS-Genesis over a suite of strong baselines."}, {"title": "2 Related Works", "content": "Agents for Digital Automation. The recent proliferation of LLMs has significantly boosted researchers' interest in developing language agents (Durante et al., 2024) to explore the digital world (Feng et al., 2024; Wu et al., 2024a). One line of work leverages the capabilities of fixed LLMs to create agents using methods like prompt engineering, model collaboration (Wu et al., 2023; Sun et al., 2023; Jia et al., 2024), code or tool use (Sun et al., 2024a), self-improvement (Shinn et al., 2024; Xu et al., 2024a; Cheng et al., 2024a), or integration with world or agent models (Hu and Shu, 2023; Jin et al., 2024; Zhang et al., 2023). Another line focuses on fine-tuning to augment models with agentic abilities, including (1) the ability to perceive the state of the computer, such as understanding screens (Cheng et al., 2024b; Gou et al., 2024; Wu et al., 2024b) or application UI trees (Xie et al., 2024; Zheng et al., 2024a), (2) the ability to generate actions (click, type, scroll, etc. Chen et al., 2024a), and (3) the flexibility to operate across diverse environments, including web (Yao et al., 2022; Deng et al., 2023), desktop (Kapoor et al., 2024; Niu et al., 2024), and mobile platforms (Li et al., 2024; Wang et al., 2024a). Collectively, these efforts pave the way for digital automation, with agents engaging across a diverse digital landscape."}, {"title": "3 OS-Genesis", "content": "In this section, we present the pipeline of OS-Genesis, detailing the process from automated data collection to the construction of complete GUI agent trajectories."}, {"title": "3.1 Interaction-Driven Functional Discovery", "content": "As illustrated in Figure 2, OS-Genesis begins with human-free exploration in dynamic environments $E$ = mobile, web, etc., systematically traversing interactive elements through actions $a \\in A$ = {CLICK, TYPE, SCROLL}. With the goal of constructing mobile and web agents, this process is conducted in both the Android emulator and a chrome browser \u00b2. It to some extent mirrors human interaction with GUIs, uncovering potential functionalities without requiring pre-defined tasks. The entire exploration phase is rule-based, except when interacting with input fields, where GPT-40 is invoked to generate contextually appropriate contents. At the end of this phase, massive\n\u00b2We build dynamic environments on the basis of Zhou et al. (2024) and Rawles et al. (2024)."}, {"title": "3.2 Reverse Task Synthesis", "content": "Following the discovery, OS-Genesis leverages collected triplets $(s_{pre}, a, s_{post})$ to construct meaningful task instructions. This process involves generating low-level tasks using an annotation model (e.g., GPT-40) and subsequently transforming them into high-level tasks. The annotation model $M$ transforms each triplet $(s_{pre}, a, s_{post})\\in T$ into a specific low-level task instruction:\n$f_{low}:(s_{pre}, a, s_{post}) \\xrightarrow{M} T_{low}$.\nHere, $T_{low}$ represents an atomic, executable operation derived from the observed state transition caused by the action $a$. For example, if the action $a$ = CLICK reveals a dropdown menu, the corresponding task might be \u201cclick the dropdown to display options.\u201d The annotation model integrates visual, contextual, and action semantics to ensure that $T_{low}$ aligns with the functions of $E$.\nBuilding on the synthesized low-level tasks, OS-Genesis constructs high-level tasks by associating each low-level task $T_{low}$ with broader objectives that could plausibly encompass it. This process, performed by the annotation model $M$, maps individual low-level steps to high-level tasks by leveraging contextual information and domain knowledge:\n$f_{high}: T_{low} \\xrightarrow{M} T_{high}$.\nHere, $T_{high}$ represents a goal-oriented instruction that contextualizes the low-level operation within a larger user intent. For instance, a low-level task such as \"click the dropdown to display options\" might be linked to a high-level task like \u201cconfigure application settings,\" as the dropdown interaction is often a prerequisite for such configurations. Details and prompts for transforming triples into high-level instructions are provided in Appendix C.\nAfter this reverse task synthesis process, OS-Genesis generates a diverse set of high-level instructions $T = {T_1, T_2, ..., T_N}$ that are aligned with dynamic environments and semantically rich. This entire process is completed without any human intervention.\nSubsequently, these synthetic instructions $T$ are executed in environment $E$ by a model like GPT-40, producing a complete set of trajectories, denoted as $G = {g_1, g_2,..., g_N}$."}, {"title": "3.3 Trajectory Reward Model", "content": "Considering the potential limitations of a model's agentic ability, errors or incomplete steps may arise when using high-level instructions to explore and generate trajectories. To address this, we incorporate a Trajectory Reward Model (TRM) to ensure the quality and utility of trajectories synthesized by OS-Genesis, as illustrated in Figure 3. Previous methods commonly rely on labeler functions (He et al., 2024; Murty et al., 2024a, inter alia), which discard trajectories deemed incomplete directly (Pan et al., 2024). However, even"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Settings", "content": "Evauation Benchmarks. For mobile tasks, we select (1) AndroidControl (Li et al., 2024), which evaluates the ability of GUI agents to perform both low- and high-level tasks, and (2) AndroidWorld (Rawles et al., 2024), a challenging online benchmark running in Android emulators, to demonstrate the practicability of our agents in solving human daily tasks. Regarding web tasks, More information about the benchmark settings and evaluation details are presented in Appendix A.\nModel Settings. We primarily use GPT-40 (Hurst et al., 2024) for reverse task synthesis and reward modeling. As for the backbone models used to construct agents, we consider (1) InternVL2-4B/8B (Chen et al., 2024b), which is trained without GUI data, and (2) Qwen2-VL-7B-Instruct (Wang et al., 2024b), which claims to possess certain agentic capabilities to conduct thorough and comparative experiments. All training is performed as VLM full fine-tuning on interconnected clusters of 8 \u00d7 A100 80GB GPUs, with detailed training settings provided in Appendix B and prompt settings in Appendix D."}, {"title": "4.2 Baseline Construction and Training", "content": "Baselines. As a pioneering study in synthesizing GUI agent data, we design the following baselines to demonstrate the superiority of trajectories obtained through OS-Genesis. All settings uniformly accept a11ytree and screenshots as inputs.\n\u2022 Zero-Shot: This baseline leverages CoT (Wei et al., 2022) prompting to guide the model in perceiving environments and taking actions. For AndroidWorld tasks, we follow Rawles et al. (2024) to adopt M3A agent setup with multimodal input for this setting.\n\u2022 Task-Driven: We build this baseline to compare with the common approach for agent data synthesis (Lai et al., 2024, inter alia). Given the initial screenshots of the app/web page and task examples, use GPT-40 to generate high-level instructions and explore the environment to collect trajectories. These trajectories are then used for training.\n\u2022 Self-Instructions: Building upon the task-driven baseline, this approach employs GPT-40 to perform self-instruction (Wang et al., 2023), generating additional high-level"}, {"title": "4.3 Main Results", "content": "AndroidWorld. To prove the effectiveness of OS-Genesis under dynamic environment, we evaluate it on AndroidWorld (Rawles et al., 2024) that leverages a Pixel 6 phone simulator as testbed. As shown in Table 1, OS-Genesis significantly narrows the performance gap between open-source agents and the SOTA GPT-40-based M3A agent. Compared to task-driven methods, training with OS-Genesis achieves performance improvements that are often double those of the baselines. Even self-instruct baseline utilize 1.5 \u00d7 the amount of data compared to OS-Genesis, they fail to match the quality of data generated by OS-Genesis. underscoring the importance of using high-quality trajectory data in online settings.\nBeyond improvements in planning and action, some gains also stem from OS-Genesis 's ability to cover subtle yet critical app functionalities during the reverse task synthesis process. These functionalities, often overlooked by task-driven methods, are essential for completing intricate tasks.\nAndroidControl. We then evaluate OS-Genesis on AndroidControl (Li et al., 2024). Out of the 833 apps covered by AndroidControl, only 20 have been directly encountered during data synthesis, making this evaluation a test of OS-Genesis 's out-of-distribution (OOD) performance. In the high-"}, {"title": "5 Analysis", "content": ""}, {"title": "5.1 How Diverse is Our Synthesized Data?", "content": "Ensuring the diversity of synthetic data is crucial for effective model training. Traditional approaches that rely on pre-defined high-level tasks are inherently constrained, as it is practically impossible to enumerate and cover the full spectrum of potential interactions within a complex environment. In contrast, OS-Genesis employs an exploration-driven method that naturally adapts to the environment by interacting with diverse interface elements, systematically uncovering a broader range of functional capabilities.\nTo validate the effectiveness of our method in generating more diverse data, we examine both instruction diversity and trajectory diversity. We begin by analyzing the variety of generated in-"}, {"title": "5.2 How TRM Impacts Performance?", "content": "We introduce a Trajectory Reward Model (TRM) for data quality control and exploitation, substituting traditional labeler filtering methods (He et al., 2024; Murty et al., 2024a). To analyze its impact and for ablation purposes, we include additional settings for comparison: (1) training without an RM, where all synthesized data is treated equally during training, and (2) using a labeler, similar to previous approaches where only complete trajectories are retained for training."}, {"title": "5.3 How Scaling Trajectory Data Improves Agentic Ability?", "content": "We investigate the impact of data scale on building GUI agents. To explore this, we partition the data synthesized by OS-Genesis into subsets, ranging from small-scale trajectories to those exceeding the size used in main experiments. Using Android-World as our testbed, we focus on two primary questions: (1) How does performance improve as the data scale increases? (2) Does performance saturate at higher data scales?"}, {"title": "5.4 How Far are We from Human Data?", "content": "We analyze the gaps between OS-Genesis and human data in two key aspects: (1) high-level instructions synthesized through OS-Genesis v.s. human-written instructions, and (2) trajectories from OS-Genesis v.s. human-annotated trajectories."}, {"title": "6 Conclusion", "content": "We introduce OS-Genesis, a data synthesis pipeline to fuel diversified computer control agents. By leveraging a novel interaction-driven approach, OS-Genesis overcomes the critical bottlenecks of constructing meaningful and diverse GUI tasks in previous practices. Through extensive evaluations on challenging online benchmarks, we demonstrate that OS-Genesis-synthesized data has led to a breakthrough in GUI agents' planning and action capabilities. Moreover, our synthesized trajectories exhibit greater diversity and substantially narrow the quality gap between synthetic data and human annotations. OS-Genesis provides a promising direction for generating high-quality trajectory data for GUI agent training, bringing the community one step closer to achieving digital automation."}, {"title": "Limitations", "content": "While OS-Genesis demonstrates the potential to overcome critical challenges in acquiring GUI trajectory data, it is important to acknowledge certain limitations:\nProprietary Models. We build our GUI agents upon open-source VLMs, but for data quality, we leverage GPT-40 for exploration and reward modeling in the annotation process. The reason we did not replace this process with open-source counterparts is that existing open-source VLMs lack the ability to follow user instructions and proactively complete exploration in online environments. We believe that in the future, more capable action models can bridge this gap and replace proprietary components in this pipeline.\nData usage. Throughout this work, we employ textual and visual representations to train and evaluate our GUI agents. This is designed to (1) maximize agents' planning and action capabilities in semantically rich environments, and (2) ensure evaluation consistency across different environments. We are aware that using either textual or visual data alone could also contribute to constructing GUI agents, provided that the I/O format and training strategies are appropriately adjusted. We leave the partial use of full trajectory data as future works.\nBroader Impacts\nComputer agents operating in an OS environment could potentially affect the normal functioning of the system. However, considering that all settings in this work are conducted within virtual environments, we do not view this as a concern."}, {"title": "A Details of Benchmarks", "content": "Here we present more information about the benchmarks involved in evaluating OS-Genesis.\nAndroidControl. AndroidControl (Li et al., 2024) is a benchmark designed to evaluate real-world mobile control agents, created from human-collected tasks within the Android environment, consisting of 7,708 tasks across 1,412 trajectories. It includes two SeqIO tasks: (i) SeqIO HL (high-level), where the prompt contains only a high-level instruction, and (ii) SeqIO LL (low-level), where both a low-level instruction and its corresponding high-level instruction are included. In terms of evaluation metrics, AndroidControl calculates the success rate (SR) and action type accuracy (\u0422\u0443\u0440\u0435) based on ground truth action labels. In our experimental setup, we add the screenshot's accessibility tree and historical actions from the current trajectory as additional observation space to better simulate the agent's execution environment. In addition, following Lu et al., 2024, we consider the coordinates correct if they fall within a distance of 14% screen width from the ground truth.\nAndroidWorld. AndroidWorld (Rawles et al., 2024) is an online benchmark for evaluating autonomous agents in Android environments, featuring 116 tasks across 20 real-world apps. Tasks are parameterized with randomized inputs, enabling diverse scenarios and robust evaluations. Success rates (SR) are assessed using system state inspections without modifying the app source code. Due to app unavailability, a total of 112 tasks are actually used. Tasks marked as \u201cNaN\u201d are re-tested, and those that remain incomplete after re-testing are uniformly marked as false to ensure fair comparisons.\nWebArena. WebArena (Zhou et al., 2024) is a realistic web benchmark for autonomous digital agents, comprising 812 challenging web navigation tasks derived from 241 task templates, including maps, e-commerce, Reddit forums, and software development. It features robust evaluation programs that assess the success rate (SR) based on functional correctness. We follow the standard practices of WebArena by using the default action space (including actions such as clicks and inputs) and employing screenshots and the accessibility tree as the observation space for multimodal GUI agents. For hosting the online evaluation environment, we use an Amazon EC2 instance (t3a.xlarge,"}, {"title": "B Experimental Details", "content": "Action Spaces. All actions included in the data synthesized by OS-Genesis are covered within the types listed in Table 3 (mobile) and Table 4 (web). For AndroidWorld, additional two actions: terminate and keyboard_enter are incorporated to meet the requirements of evaluation."}, {"title": "C Reverse Task Synthesis Details", "content": "Our reverse task synthesis process simulates how humans explore new tasks in an unknown GUI environment. After performing actions on random elements, humans infer possible subsequent actions by observing changes on the screen, thus continuing their exploration to construct a complete trajectory for executing a particular task. In our reverse task synthesis, we provide GPT-40 with the current action being executed, before-and-after screenshots of the screen changes, and a red bounding box highlighting the interacted element in the screenshots. This allows GPT-40 to first comprehend the action being performed and then associate the possible high-level task based on the observed screen changes. The detailed association prompts for synthesizing high-level instruction data for both Android and Web are provided in Prompt 14 and Prompt 15 respectively."}, {"title": "D Model and Training Details", "content": "InternVL2-{4B,8B}. InternVL2 (Chen et al., 2024b) utilizes Dynamic Aspect Ratio Matching to handle dynamic high-resolution inputs. In our training setting, we set the max_dynamic_patch parameter to 24 to comprehensively capture the fine-grained details of the image. Consequently, the resized input image is partitioned into a maximum of 24 tiles, each of 448\u00d7448 pixels, while a thumbnail of the entire image is included to preserve global contextual information.\nQwen2-VL-7B-Instruct. Qwen2-VL (Wang et al., 2024b) introduces the Naive Dynamic Resolution mechanism, which is capable of handling images of any resolution by mapping them into a dynamic number of visual tokens, providing a more human-like visual processing experience. Through our experiments, we found that configuring the image_resolution parameter to 1024 for both training and inference produces outstanding results in GUI agent tasks, while also contributing to the optimization of the model's training and inference costs.\nAccessibility Tree. The accessibility tree represents the hierarchical relationships and attributes of all interactive or accessible elements on a screen, providing rich GUI information in text form to train GUI agents. In constructing the training data, we filter the accessibility tree to retain only the position or index information of elements visible on the screen, reducing the interference of excessive redundant text in model training.\nData Format. We follow the data formats of AndroidWorld and WebArena to construct our training data, ensuring consistency in formatting between the training and evaluation phases. The detailed training instructions for Android and Web data are listed in Prompt 12 and Prompt 13 respectively."}, {"title": "E Baseline Settings", "content": ""}, {"title": "E.1 Task-Driven", "content": "Following prior work (He et al., 2024; Lai et al., 2024) on collecting tasks for GUI agents, we guide GPT-40 to infer possible high-level instructions based on the initial GUI interface (e.g., the homepage of a social forum like Reddit). Some examples of initial screens are demonstrated in Figure 10 (mobile) and Figure 11 (web)."}, {"title": "E.2 Task-Driven w. Self Instruct", "content": "Building upon the task-driven baseline in E.1, we incorporate self-instruction (Wang et al., 2023) data as a second baseline. This is constructed by randomly sampling 3 demonstrations from the above task-driven high-level instructions as in-context examples for each synthesis iteration.\nNotably, we make certain that the total number of trajectories for the baseline is at least equal to that of our method to avoid data imbalance and maintain fairness in comparisons."}, {"title": "F Details of Trajectory Reward Model", "content": "The Trajectory Reward Model (TRM) primarily assesses the quality of agent trajectories by focusing on completion and coherence. Based on a high-level instruction to complete, the agent's entire action history (e.g., low-level instructions), and screenshots from the last three timesteps, GPT-40 is prompted to assign a score between 1 and 5 for the trajectory. Instead of instruction and in-context learning (Sun et al., 2024b), we include in the prompt specific aspects of coherence and completion to consider, along with detailed descriptions of what each score from 1 to 5 represents. Given the similarity between mobile and web tasks, we"}, {"title": "G Details about Diversity Analysis", "content": "We visualize the instruction embeddings calculated in Section 5.1 in Figure 9. This demonstrates that OS-Genesis generates more diverse instructions using an exploration-driven method.\nWe analyze the average word count in synthesized and human-annotated task instructions. For mobile tasks, Task-Driven and Self-Instruction yield average word counts of 9.64 and 9.84, respectively. In contrast, OS-Genesis generates longer instructions with an average of 18.01 words, closely matching the 18.71 words in human data. For web tasks, Task-Driven and Self-Instruction produce averages of 11.79 and 8.45 words, while OS-Genesis generates instructions with an average of 19.68 words. These results indicate that OS-Genesis produces more detailed instructions with sufficient information and context.\nRegarding the average number of steps per task, for mobile tasks, Task-Driven, Self-Instruction, OS-Genesis, and Human data have averages of 5.64, 3.43, 5.60, and 5.31 steps, respectively. These are comparable, except that Self-Instruction generates tasks with fewer steps. For web tasks, Task-Driven and Self-Instructions have averages of 8.74 and 7.37 steps, while OS-Genesis generates tasks with a shorter average of 4.46 steps."}]}