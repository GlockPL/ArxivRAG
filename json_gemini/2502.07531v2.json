{"title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation", "authors": ["SIXIAO ZHENG", "ZIMIAN PENG", "YANPENG ZHOU", "YI ZHU", "HANG XU", "XIANGRU HUANG", "YANWEI FU"], "abstract": "Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence.", "sections": [{"title": "INTRODUCTION", "content": "Image-to-video (I2V) generation is a powerful technique that brings still images to life, enabling widespread applications in the fields of content creation, advertising, and art animation. Typically, I2V generation aims to animate a reference image according to user-provided control signals, such as text descriptions, object motion, and camera motion, while maintaining the quality and natural dynamics of the generated video. Thanks to the availability of large-scale web data and the development of highly expressive neural architectures, I2V has made significant progress in recent years. Early research work has demonstrated their ability to generate temporally consistent videos [Chen et al. 2023b; Guo et al. 2024, 2023; Xing et al. 2025]. More recently, researchers have been working to incorporate a wider range of control signals into I2V models, building on the successes in the field of image manipulation [Geng et al. 2024; He et al. 2024b; Li et al. 2024; Wang et al. 2024d].\nTraining I2V generation models require high-quality video datasets that are annotated with all forms of control signals. However, such datasets are difficult to obtain as additional control signals are considered. To mitigate this issue, previous approaches [Li et al. 2024; Mou et al. 2024; Wang et al. 2024d] propose using powerful perception models to generate accurate annotations of some forms of control signals from real-world videos. However, such powerful models may not be available for other forms of control signals. For example, predicting the lighting from real-world videos is an ill-posed problem. Lighting is intrinsically tied to an object's appearance model, specifically how much light each object scatters, reflects, and transmits [Pharr et al. 2023]. For lighting control, techniques like DILightNet [Zeng et al. 2024a] and NeuLighting [Li et al. 2022], while effective for static scenes, struggle to generalize to dynamic videos due to their reliance on precomputed radiance fields or datasets that lack temporal lighting annotations. This limitation is further compounded by recent work on lighting control in I2V generation [Huynh et al. 2021; Lin et al. 2024; Qiu et al. 2024; Zhang et al. 2021], which primarily focuses on specific object categories, such as human faces, resulting in limited generalization to open-domain objects.\nTo overcome these challenges, we propose VidCRAFT3, which integrates three core technical advancements. First, the Image2Cloud module leverages DUSt3R [Wang et al. 2024b] to reconstruct a 3D point cloud from a single reference image, enabling precise camera motion control by rendering the point cloud along user-defined camera trajectories. Second, ObjMotionNet encodes sparse object motion trajectories by extracting multi-scale motion features from Gaussian-smoothed optical flow maps to guide realistic object motion. Third, the Spatial Triple-Attention Transformer integrates lighting direction control by integrating lighting embedding with image-text embeddings through parallel cross-attention layers, ensuring consistent illumination effects."}, {"title": "RELATED WORK", "content": ""}, {"title": "Image-to-video Generation", "content": "Image-to-video (I2V) generation aims to animate static images into dynamic videos while preserving visual content and introducing realistic motion. Recent advances in diffusion models [Ho et al. 2020; Sohl-Dickstein et al. 2015] have revolutionized video synthesis by extending pre-trained T2I models like AnimateDiff [Guo et al. 2023] to incorporate temporal dimensions for motion generation. These methods integrate the input image as a condition, either through CLIP-based [Radford et al. 2021] image embeddings or by concatenating the image with noisy latent representations. For example, VideoCrafter1 [Chen et al. 2023b], DynamiCrafter [Xing et al. 2025], and I2V-Adapter [Guo et al. 2024] use dual cross-attention layers to fuse image embeddings with noisy frames, ensuring spatial-aligned guidance. Similarly, Stable Video Diffusion (SVD) [Blattmann et al. 2023] replaces text embeddings with CLIP image embeddings, maintaining semantic consistency in an image-only manner. Another line of work, exemplified by SEINE [Chen et al. 2023a], DynamiCrafter [Xing et al. 2025] and PixelDance [Zeng et al. 2024b], expands the input channels of diffusion models to concatenate the static image with noisy latents, effectively injecting image information into the model. However, these methods preserve input image fidelity while generating dynamic videos but often struggle with fine-grained details due to reliance on global conditions."}, {"title": "Motion-controlled Video Generation", "content": "Motion-controlled video generation is a key research direction in video synthesis, focusing on creating high-fidelity videos that follow user-defined motion dynamics. Existing methods fall into three categories: (1) camera motion control, (2) object motion control, and (3) joint motion control. For camera motion control, MotionCtrl [Wang"}, {"title": "Lighting-controlled Visual Generation", "content": "Lighting-controlled visual generation aims to manipulate illumination while preserving scene geometry and materials. Recent advances in diffusion models have significantly enhanced lighting control quality and flexibility. Methods like DiLightNet [Zeng et al. 2024a] and GenLit [Bharadwaj et al. 2024] achieve fine-grained and realistic relighting through radiance hints and SVD, respectively. Facial relighting methods, including DifFRelight [He et al. 2024a] and DiFaReli [Ponglertnapakorn et al. 2023], produce high-quality portrait images, while frameworks like NeuLighting [Li et al. 2022] focus on outdoor scenes using unconstrained photo collections. Additionally, Lite2Relight [Rao et al. 2024] and GSR [Poirier-Ginter et al. 2024] combine diffusion models with neural radiance fields for realistic 3D-aware relighting. Extending lighting control to video introduces challenges like temporal consistency and dynamic lighting effects. Recent techniques leverage 3D-aware generative models for temporally consistent relighting, as seen in EdgeRelight360 [Lin et al. 2024] and ReliTalk [Qiu et al. 2024]. Neural rendering approaches [Cai et al. 2024; Zhang et al. 2021] use datasets like dynamic OLAT for high-quality portrait video relighting, while reflectance field-based methods [Huynh et al. 2021] infer lighting from exemplars. Despite these advancements, most prior works focus on portraits and HDR lighting conditions. In contrast, we target general scenes with directional lighting, allowing users to interactively adjust lighting directions in a fast and intuitive manner."}, {"title": "METHOD", "content": ""}, {"title": "Overview", "content": "We present VidCRAFT3, a novel image-to-video generation model that, for the first time, enables simultaneous control over camera motion, object motion, and lighting direction. VidCRAFT3 comprises three core components: Image2Cloud, which converts the input image into a 3D point cloud for accurate camera motion control; ObjMotionNet, which encodes sparse object motion trajectories to guide object movements; and Spatial Triple-Attention Transformer, which integrates lighting direction with image and text features to enable fine-grained lighting control. Given a reference image I and text description, image-to-video generation aims to generate a sequence of video frames \\(I = \\{I_1, I_2, ..., I_F\\}\\). In this paper, users can additionally provide three types of control signals: camera motion, object motion, and lighting direction. Specifically, camera motion is represented by a sequence of camera extrinsics \\(E = \\{E_1, E_2, ..., E_F\\}\\); object motion is represented by sparse trajectories of image pixels \\(\\{s_1, s_2, s_3, ..., s_\\}\\), \\(\\{s_1, s_2, s_3, ..., s_\\}\\), ...; the lighting direction is simply a unit 3d vector \\(L \\in \\mathbb{R}^3\\)."}, {"title": "Model Architecture", "content": "Camera Motion Control via Point Cloud Rendering. To achieve precise camera motion control in image-to-video generation, we first reconstruct a high-quality 3D representation of the scene from a single input image. This step ensures spatial consistency and dynamic variation in the generated video. We use the Image2Cloud module, specifically DUSt3R, to convert the image into a 3D point cloud and estimate camera parameters. DUSt3R, an unconstrained stereo 3D reconstruction model, performs monocular and binocular reconstruction via point regression and uses a global alignment strategy for multi-view reconstruction. The input image is duplicated to create paired inputs, allowing for point cloud and camera parameter estimation. The reconstructed point cloud is then rendered along the input camera motion trajectory to produce point cloud renderings \\(R = \\{r_1, r_2, ..., r_f\\}\\). However, due to the limitations of point cloud representation and 3D cues from a single image, artifacts such as missing regions, occlusions, and distortions may affect rendering quality.\nTo improve the quality of point cloud rendering from the camera's perspective, we train a VDM conditioned on the input reference image, text description, and point cloud renderings, generating high-quality and temporally consistent video outputs. As illustrated in Fig. 2, our model is based on the open-source image-to-video generation model DynamiCrafter, which consists of a VAE encoder \\(\\varepsilon\\) and decoder \\(\\mathcal{D}\\) for image compression, a UNet architecture with spatial and temporal layers, and CLIP Text Encoder, CLIP Image Encoder. We adopt a dual-stream injection approach to input both point cloud renderings and the reference image into UNet. Specifically, the reference image is encoded using the CLIP Image Encoder, and the encoded features are injected into UNet via Image Cross-Attention. Meanwhile, we replace the first frame of the point cloud rendering with the reference image, encode it using the VAE Encoder, and then concatenate the encoded image with noise along the channel dimension before feeding it into UNet."}, {"title": "Object Motion Control through Trajectory Learning", "content": "Object motion is represented as a set of sparse spatial trajectories that capture the movement of objects across frames. Each trajectory consists of pixel coordinates (x, y) over F frames for up to N objects, formally defined as \\(T = \\{s_n = (x, y) | n = 1, 2, ..., N; f = 1, 2, ..., F\\}\\), where \\(s_n^f\\) denotes the position of the n-th object in the f-th frame. Then, we compute the optical flow \\(V\\) between consecutive frames. For each trajectory point \\(s_n^f\\), the optical flow vector \\(v_n^f\\) is computed as \\(v_n^f = s_{n+1}^f - s_n^f = (x_{n+1}^f - x_n^f, y_{n+1}^f - y_n^f)\\). These sparse motion vectors are assigned to their corresponding pixel positions in the optical flow map \\(V_f(x, y)\\):\n\n\n\\(V_f(x, y) = \\begin{cases}v_n^f & \\text{if } (x, y) = (x_{n+1}^f, y_{n+1}^f), \\\\(0, 0) & \\text{otherwise}.\\end{cases}\\)\n\n\nThe optical flow for the first frame is initialized as a zero tensor \\(V_1(x, y) = (0, 0), \\forall (x, y) \\in [H, W]\\). After processing all frames, the resulting optical flow map \\(V \\in \\mathbb{R}^{F \\times H \\times W \\times 2}\\), where H and W are the height and width of the map. To transform these optical flow into dense motion representations, we apply Gaussian smoothing to the optical flow maps.\nThe ObjMotionNet is an object motion encoder specifically designed to process the smoothed optical flow tensor \\(\\tilde{V} \\in \\mathbb{R}^{F \\times H \\times W \\times 2}\\). It extracts multi-scale motion features \\(\\phi(\\tilde{V})\\) through a series of convolutional layers with downsampling operations, enabling the network to capture both local and global motion dynamics effectively. The extracted features \\(\\phi(\\tilde{V})\\) are integrated into the UNet encoder by element-wise addition to its convolutional inputs. By focusing on encoder-level integration, ObjMotionNet ensures a balance between precise motion control and high visual fidelity."}, {"title": "Lighting Direction Control with Spatial Triple-Attention Transformer", "content": "The lighting direction is represented as a per-frame 3D vector \\(L = (l_x, l_y, l_z)\\), describing the orientation of the light source in Cartesian coordinates. To effectively encode this directional information into a high-dimensional feature space, we employ Spherical Harmonic (SH) Encoding. SH encoding captures the angular characteristics of the lighting using basis functions up to degree 4, resulting in 16 coefficients. The resulting SH-encoded vector \\(L_{SH} \\in \\mathbb{R}^{16}\\) is projected into the feature space of the UNet using a multi-layer perceptron (MLP). \\(E_{light} = MLP(L_{SH})\\), where \\(E_{light}\\) is a lighting embedding aligned with the dimensionality of text embedding.\nTo incorporate the lighting embedding into the UNet, we propose the lighting cross-attention, which integrates the encoded lighting embedding \\(E_{light}\\) into the model. This attention mechanism modulates the spatial features based on the input lighting direction. The operation is defined as:\n\n\n\\(\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\\)\n\n\nwhere \\(Q\\) (Query) comes from the self-attention output of the UNet, \\(K\\) (Key) and \\(V\\) (Value) are derived from \\(E_{light}\\). To enhance multimodal interactions, the lighting cross-attention is combined with two existing cross-attention layers, image cross-attention and text cross-attention. The proposed Spatial Triple-Attention Transformer integrates these three attention modules in parallel, with the outputs summed to produce a fused feature representation \\(O = O_{image} + O_{text} + O_{light}\\) where \\(O_{image}\\), \\(O_{text}\\), and \\(O_{light}\\) are the outputs of the image, text, and lighting cross-attention modules, respectively. This novel mechanism ensures the generated videos maintain consistency across lighting, textual, and image-based conditions."}, {"title": "Training Strategy", "content": "We introduce a three-stage training strategy to enable precise control over camera motion, object motion, and lighting direction in image-to-video generation. This approach fine-tunes the model progressively, balancing global and local feature learning while optimizing computational efficiency.\nStage 1: Camera Motion Control Training. We fine-tune the entire UNet architecture using the Camera Motion Control Dataset, which provides camera poses and point cloud renderings. This enables the model to capture both spatial structures and temporal dynamics across video frames. The model is trained for 40,000 iterations during this stage.\nStage 2: Dense Object Trajectories and Lighting Mixed Fine-tuning. We combine the Object Motion Control and Lighting Direction Control Datasets, annotating camera motion, object motion, and lighting direction. This mixed training approach helps the model jointly learn control over these aspects, enhancing video realism and consistency. Dense object motion trajectories provide rich spatial-temporal information, while lighting directions and camera poses enable the model to understand lighting interactions with objects and scenes. In this stage, we freeze the temporal layers to preserve the model's ability to capture time-dependent dynamics, while fine-tuning the spatial layers (e.g., 2D convolutions and spatial transformer) to optimize spatial details like object contours, shadows, and highlights. This approach ensures the model learns both complex motion patterns and fine-grained spatial features. This stage is trained for 20,000 iterations, balancing computational efficiency with model performance.\nStage 3: Sparse Object Trajectories and Lighting Mixed Fine-tuning. We fine-tune the model with sparse object trajectories to adapt to real-world inputs, where motion guidance is limited. Sparse trajectories, consisting of key points, require the model to generalize, enhancing its robustness in generating realistic motion effects with minimal input. To improve stability, we smooth trajectories with a Gaussian filter, handling noise while preserving key features. This stage, trained for 20,000 iterations, strengthens the model's ability to infer complex motion patterns and improves generalization."}, {"title": "DATASET CONSTRUCTION", "content": "To enable fine-grained control over camera motion, object motion, and lighting direction in I2V generation, detailed annotations for camera poses, object trajectories, and lighting directions are essential. Existing datasets lack such comprehensive annotations, hindering model training for simultaneous control. To address this, we construct three specialized datasets:"}, {"title": "Camera Motion Control Dataset", "content": "We constructed a Camera Motion Control Dataset from RealEstate10K [Zhou et al. 2018], creating 62,000 25-frame clips with smooth camera transitions. DUSt3R was used to generate globally aligned point clouds, and the first frame's point cloud was rendered along the ground truth camera trajectory to visualize camera motion. Since RealEstate10K lacks captions, we sampled four frames per clip and used Qwen2-VL-7B-Instruct [Wang et al. 2024a] to generate comprehensive descriptions. Although focused on indoor scenes, the dataset provides diverse camera trajectories for fine-grained motion control in video generation."}, {"title": "Object Motion Control Dataset", "content": "To enable precise control over object motion in image-to-video generation, we constructed an Object Motion Control Dataset from WebVid-10M [Bain et al. 2021], consisting of 60,000 video clips with high-quality object motion trajectories. (1) Video Preprocessing: We used PySceneDetect to remove clips with abrupt scene changes, sampling 25-frame sequences with temporal intervals from 1 to 16 frames. (2) Motion Estimation: Optical flow was computed using MemFlow [Dong and Fu 2024], filtering out the bottom 25% of clips with low motion scores. (3) Cropping and Tracking: The clips are center-cropped to 320 \u00d7 512 \u00d7 25 dimensions. We then apply CoTrackerV3 [Karaev et al. 2024] to generate dense trajectories on a 16 \u00d7 16 grid, retaining only those trajectories longer than the average length to ensure meaningful object motion. (4) Sampling and Gaussian Filtering: To enhance usability, we sample 1 to 8 sparse trajectories from the dense set, with probability proportional to trajectory length. Optical flow is computed between adjacent frames to capture motion intensity and direction, and the sparse matrix is smoothed with a Gaussian filter for stable training."}, {"title": "Lighting Direction Control Dataset", "content": "As shown in Fig. 3, we introduce the VideoLightingDirection (VLD) Dataset, a synthetic dataset created in Blender with artist-designed 3D models, HDR environments, and procedural scene generation for precise lighting control. (1) Scene Creation: We create diverse scenes using 3D models and backgrounds from Poly Haven and the BOP dataset. Poly Haven assets pair 3D models with HDR environments, while BOP scenes use rooms of six planes with randomly placed 3D models, ensuring varied and dynamic environments. (2) Camera Trajectory Sampling: To simulate realistic camera motion, we define a spherical region (0.7-1.3 meters radius) around each 3D model. Cameras are randomly placed on the sphere, pointing toward the model center, with smooth trajectories for natural orbital motion, ensuring stable scene representation. (3) Lighting Direction Sampling: To sample light sources, we define a hemisphere centered on the model, with its base-plane normal aligned to the camera's viewing direction. Sixteen points are uniformly sampled on the hemisphere as 2kW spotlights, each with a radius of 1 and oriented toward the model center. The sampled positions are mapped into the coordinate systems of all camera views and normalized to calculate lighting directions for each view. (4) Rendering and Annotation: We render 25 frames at a resolution of 320 \u00d7 512 using Cycles, a path-tracing renderer in Blender known for its photorealistic output. For each scene, we render 16 videos corresponding to the 16 sampled light positions, while maintaining a consistent camera trajectory. This results in a total of 57,600 videos from 3,600 unique scenes. Each frame is annotated with the corresponding camera pose and lighting direction, providing precise control signals for training."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "Experimental Setup", "content": "Implementation Details. Our model is built upon the DynamiCrafter and ViewCrafter [Yu et al. 2024] architecture, leveraging its pretrained weights for initialization. During both training and inference, we set the resolution to 320 \u00d7 512 and process video clips containing 25 frames. To optimize the model, we employ the Adam optimizer with a learning rate of \\(1 \\times 10^{-5}\\) and a batch size of 96. The training is conducted on 8 NVIDIA H100 GPUs, with each GPU handling a batch size of 6. For inference, we adopt the DDIM sampler along with classifier-free guidance to enhance generation quality and multimodal consistency.\nEvaluation Datasets. To evaluate the performance of our model in camera motion control, object motion control, and lighting direction control, we construct three domain-specific evaluation datasets and one generalized testing dataset. For camera motion control, we sample 1,000 samples from the RealEstate10K test set. For object motion control, we sample 1,000 samples from the WebVid10M test set. Lighting direction control is evaluated on 1,000 examples sampled from the proposed VLD dataset, covering a wide range of lighting directions. To provide a broader evaluation, we create a generalized testing dataset consisting of 80 videos sourced from copyright-free websites like Pixabay and Pexels, as well as videos generated by T2V models. This dataset spans categories such as human activities, animals, vehicles, indoor scenes, artworks, natural landscapes, and AI-generated images.\nEvaluation Metrics. We evaluate VidCRAFT3 using three types of metrics. Video quality is assessed with FID, FVD, and CLIPSIM, measuring visual quality, temporal coherence, and semantic alignment, respectively. Motion control performance is quantified with CamMC and ObjMC, computed as the Euclidean distance between predicted and ground truth camera poses and object trajectories. DUSt3R and CoTrackerV3 are used for camera pose estimation and trajectory extraction. Lighting direction control is evaluated with LPIPS, SSIM, and PSNR, comparing generated frames against ground truth images for perceptual quality and structural fidelity."}, {"title": "Comparisons with Other State-of-the-Art Methods", "content": "Camera Motion Control. VidCRAFT3 demonstrates superior performance in camera motion control compared to state-of-the-art methods like CameraCtrl, CamI2V, and MotionCtrl on the RealEstate 10K dataset. As shown in Table 1 and Fig. 4, VidCRAFT3 excels in camera motion control, as evidenced by its CamMC score of 4.07, which is lower than CameraCtrl (4.19), CamI2V (4.24), and MotionCtrl (4.23). VidCRAFT3 also achieves better results in other key metrics indicate that VidCRAFT3 not only controls camera motion more accurately but also generates videos with higher visual quality, temporal coherence, and semantic alignment. Qualitatively, VidCRAFT3 generates smoother and more realistic camera movements with fewer artifacts, particularly in complex scenes. This success stems from the Image2Cloud module, which enable precise 3D scene reconstruction and fine-grained camera control.\nObject Motion Control. VidCRAFT3 demonstrates exceptional performance in object motion control, as highlighted in Table 2. The model achieves an ObjMC score of 3.51, which is lower than Image Conductor (12.96) and Motion-I2V (3.96). This indicates that VidCRAFT3 more accurately aligns the generated object trajectories with the ground truth, resulting in more realistic and faithful object motion. Additionally, VidCRAFT3 outperforms in other key metrics, showcasing superior visual quality, temporal coherence, and semantic alignment. Qualitatively, as shown in Fig. 5, VidCRAFT3 generates more realistic and consistent object movements compared to Image Conductor and Motion-I2V. The model effectively captures the dynamics of object motion, ensuring smooth transitions and natural interactions within the scene. These results highlight VidCRAFT3's robust object motion control, driven by its advanced ObjMotionNet, which effectively captures and controls complex motion patterns."}, {"title": "Ablation Study", "content": "Training Strategy. We compare training with Dense, Sparse, and Dense+Sparse trajectories. As shown in Table 3, dense training (ObjMC: 4.39) provides rich motion information but struggles during inference due to sparse inputs. Sparse training (ObjMC: 4.05) improves inference alignment but lacks motion detail. The Dense+Sparse approach achieves the best results (ObjMC: 3.51), leveraging dense trajectories for robust learning and fine-tuning with sparse trajectories for better generalization. This hybrid strategy enhances object motion control and video quality, as shown by superior FID (87.12) and FVD (120.65) scores. The qualitative results in Fig. 9 further demonstrate more accurate object trajectories and better alignment with the ground truth, confirming the effectiveness of the combined approach.\nLighting Embedding Integration Strategies. We compare different lighting embedding integration strategies. As shown in Table 4, Lighting Cross-Attention achieves the best results, with an PSNR of 19.49, SSIM of 0.74, and LPIPS of 0.11, outperforming other methods. The qualitative results in Fig. 10 further demonstrate that Lighting Cross-Attention produces more realistic lighting effects and better alignment with the reference image and lighting direction. These findings confirm the effectiveness of the proposed approach for precise lighting control in image-to-video generation.\nRepresentation of Lighting Direction. We compare Fourier Embedding [Mildenhall et al. 2021] and SH Encoding for representation of lighting direction. As shown in Table 5, SH Encoding outperforms Fourier Embedding, achieving better PSNR (19.49), SSIM (0.74), and LPIPS (0.11). The qualitative results in Fig. 11 further demonstrate that SH Encoding produces more realistic lighting effects and better alignment with the reference image and lighting direction. These results confirm SH Encoding's effectiveness for precise lighting control in image-to-video generation."}, {"title": "CONCLUSIONS AND LIMITATIONS", "content": "This paper presents VidCRAFT3, a framework for high-quality image-to-video generation with simultaneous control over camera motion, object motion, and lighting direction. Combining 3D point cloud rendering, sparse trajectory encoding, and lighting-aware attention mechanisms, VidCRAFT3 achieves fine-grained, disentangled control. The Spatial Triple-Attention Transformer ensures temporal consistency and visual realism by integrating lighting, text, and image conditions. To address data limitations, we introduce the VLD dataset and a three-stage training strategy, removing the need for joint multi-element annotations. Experiments show VidCRAFT3 outperforms SOTA methods in control precision, video quality, and generalization. However, it struggles with large human motion, physical interactions, or significant lighting changes due to limited training data and challenges in understanding physics and 3D spatial relationships."}]}