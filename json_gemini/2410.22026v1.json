{"title": "Enhance Hyperbolic Representation Learning via Second-order Pooling", "authors": ["Kun Song", "Ruben Solozabal", "Li hao", "Lu Ren", "Moloud Abdar", "Qing Li", "Fakhri Karray", "Martin Tak\u00e1\u010d"], "abstract": "Hyperbolic representation learning is well known for its ability to capture hierarchical information. However, the distance between samples from different levels of hierarchical classes can be required large. We reveal that the hyperbolic discriminant objective forces the backbone to capture this hierarchical information, which may inevitably increase the Lipschitz constant of the backbone. This can hinder the full utilization of the backbone's generalization ability. To address this issue, we introduce second-order pooling into hyperbolic representation learning, as it naturally increases the distance between samples without compromising the generalization ability of the input features. In this way, the Lipschitz constant of the backbone does not necessarily need to be large. However, current off-the-shelf low-dimensional bilinear pooling methods cannot be directly employed in hyperbolic representation learning because they inevitably reduce the distance expansion capability. To solve this problem, we propose a kernel approximation regularization, which enables the low-dimensional bilinear features to approximate the kernel function well in low-dimensional space. Finally, we conduct extensive experiments on graph-structured datasets to demonstrate the effectiveness of the proposed method.", "sections": [{"title": "1 Introduction", "content": "Many datasets contain hierarchical information. For example, in datasets like Cifar (Krizhevsky, Hinton et al. 2009) and fine-grained image datasets (Wah et al. 2011), and graph-structured data (e.g., drug molecules, social media networks), categories belonging to superclasses that can be further divided into subclasses, which is a typical hierarchical structure. Thus, inducing the right bias to capture this hierarchy is key to enhancing generalization in machine learning models (Nickel and Kiela 2018; Peng et al. 2021).\nAs hyperbolic space can present hierarchical information (Peng et al. 2021; Nickel and Kiela 2018) well, many representation learning techniques based on hyperbolic distance have been proposed, such as the various hyperbolic neural networks (Chamberlain, Clough, and Deisenroth 2017; Peng et al. 2021; Chami et al. 2019) etc. Besides those deep hyperbolic structures, there is a routine of approaches depicting hierarchical information by simply using the hyperbolic discriminant objective to train the traditional backbones (Ermolov et al. 2022; Ge et al. 2023; Zhang et al. 2022). Compared with the deep hyperbolic structures, those methods have fewer hyperparameters to tune and thus are easy to train (Yang et al. 2023).\nThe key step of performing hyperbolic objectives on traditional backbones is to project the extracted features into the hyperbolic space via the exponential map. Thus, the features extracted by the backbone present two different distributions: one in Euclidean space (before the exponential map) and the other in the hyperbolic space. In this paper, we find that the hyperbolic distance $d_h(z_i, z_j)$ ($z_i = exp(x_i)$) can be upper-bounded by a linear expansion of the Euclidean distance $d_e(x_i, x_j)$, i.e., $d_h(x_i, x_j) < Kd_e(x_i, x_j)$ where $K > 0$ is a constant. The linear upper bound implies that a large hyperbolic distance corresponds to a large Euclidean distance, and the exponential map minimally changes the distribution of the samples. Specifically, we can claim that the hyperbolic geometry-based discriminant analysis forces the features extracted by the backbone to present a hierarchical structure.\nHowever, capturing hierarchical information inevitably leads to large distances among samples belonging to different superclasses. As seen from Figure 1, C1 and C2 are separated in the level 2-superclass (E) with a distance of $d_{12}$ between them. Classes C2 and C3 belong to different super-classes, which are separated in the level one-superclass (B). Thus, their distance is $d_{23}$. We can find $d_{23} > d_{12}$. Because C4 and C3 are separated in the root level-superclass, the distance $d_{34} > d_{23} + d_{12}$. In this way, we can claim that the level of the superclass for separation is closer to the root, the distance between samples considered will be larger. Moreover, the distance will increase exponentially when the depth of the hierarchical structure is large (Dhingra et al. 2018; Chamberlain, Clough, and Deisenroth 2017).\nThe Lipschitz constant of a function $f_\\Theta(I)$ is\n$sup_{I_i, I_j} \\frac{||f_\\Theta(I_i) - f_\\Theta(I_j)||_2}{||I_i - I_j||_2}$. Thus, the large distance between extracted features may require a large Lipschitz constant of the backbone. This leads to two significant problems: (1) to enable the backbone to fit a function with a large Lipschitz constant, more layers may be required (Song et al. 2021). For some deep models, such as graph neural networks, how to effectively train a deeper model is still an open problem (Liu et al. 2021; Zhu et al. 2020a); (2) A trained backbone with a large Lipschitz constant may suffer from a loss of generalization ability, as a large Lipschitz constant indicates reduced smoothness in the backbone's landscape (Fazlyab et al. 2019, 2024; Krishnan et al. 2020). Given the two problems arising from the requirements of hierarchical information extraction, we pose the following question:\nCan we design a deep structure that captures hierarchical\ninformation without requiring a large Lipschitz constant of\nthe backbone?\nTo address this question, we introduce bilinear pooling into hyperbolic representation learning to help capture hierarchical information. The reason why we adopt bilinear pooling is that bilinear pooling non-linearly increases distances among its inputted features without compromising their generalization ability. With the help of bilinear pooling, the distances among the backbone's features are not required to be very large. As a result, the Lipschitz constant of the backbone can remain small. As illustrated in Figure 2, the hierarchical structure in bilinear pooling does not require its input features to present a hierarchical structure.\nSimilar to implementing traditional bilinear pooling approaches, integrating bilinear pooling with hyperbolic representation learning requires reducing the dimension of bilinear features. Although many off-the-shelf low-dimensional bilinear pooling methods are available, they cannot be directly adopted in the hyperbolic representation learning paradigm. We take the low-rank bilinear pooling (Gao et al. 2020; Kim et al. 2016) and random Maclaurin bilinear pooling (Gao et al. 2016; Pham and Pagh 2013) as examples. Low-rank bilinear pooling can produce low-dimensional bilinear features, but it loses the ability to expand the distances of input features. Random Maclaurin projection can keep that ability of distance expansion, but its performance is not stable, requiring the high dimension of its outputs (Yu, Li, and Li 2021).\nTo address the above problem, we propose a regularization term named kernel approximation regularization (KR), which lets the inner product of the low-dimensional bilinear features approximate a given kernel function. The benefits of KR consist of two aspects: (1) the extracted low-dimensional bilinear features do not lose the ability to enlarge the distance, and its performance is stable; (2) we can control the non-linearity of learned low-dimensional features by selecting the appropriate kernel functions. At last, we conduct experiments on graph-structure data to demonstrate the effectiveness of the proposed methods."}, {"title": "2 Preliminaries and Notations", "content": "2.1 Hyperbolic Geometry\nThere are several well-studied isometric models that endow Euclidean space with a hyperbolic metric, such as the Poincar\u00e9 model and the Lorentz model, which are equivalent (Nickel and Kiela 2018). For better understanding, we discuss hyperbolic geometry using the Poincar\u00e9 model because of its explicit geometrical meaning.\nThe Poincar\u00e9 model is on a n-dimensional sphere, which is defined as a c curvature manifold $D^n = \\{z \\in \\mathbb{R}^n: c||z||^2 < 1\\}$, with the Riemannian metric $g^D = \\lambda^2(z) \\cdot g^E$, in which $\\lambda_z = \\frac{2}{1-c||z||^2}$ is the conformal factor, and $g^E = I_n$ is the Euclidean metric tensor. Using the defined distance, hyperbolic discriminant analysis on a set of data can be made.\nThe geometry of the Poincar\u00e9 model can be described using the M\u00f6bius gyrovector space as well, which induces a new type of hyperbolic distance widely used in deep learning (). Firstly, we introduce the M\u00f6bius addition for $z_i, z_j \\in D^n$ defined as follows:\n$z_i \\oplus_c z_j = \\frac{(1+2c\\langle z_i, z_j \\rangle +c||z_j||^2)z_i +(1-c||z_i||^2)z_j}{1+2c\\langle z_i, z_j \\rangle + c^2 || z_i ||^2||z_j ||}$ (1)\nThe hyperbolic distance between two samples on $D^n$ is:\n$d_c(z_i, z_j) = \\frac{2}{\\sqrt{c}} tanh^{-1}(\\sqrt{c}||-z_i \\oplus_c z_j ||)$. (2)"}, {"title": "2.2 Deep Hyperbolic Representation Learning", "content": "Given a set of samples $I = \\{(I_i, y_i)\\}_{i=1}^N$, where $I_i$ is the i-th sample (e.g., images or graphs) and $y_i$ is the corresponding label. A deep neural network $x_i = f_\\Theta(I_i)$ extracts the feature of the sample $I_i$. Then, $x_i$ is projected to the hyperbolic space via the exponential map $z_i = exp(x_i)$ defined by Eq.(3). Thus, the distances among those samples $\\{z_i\\}_{i=1}^N$ in the hyperbolic space can be calculated via Eq.(2). In this way, the loss function based on the hyperbolic distance is calculated as:\n$\\mathcal{J}(\\Theta,W) = \\sum_{i=1}^N \\log \\frac{exp(d_c(z_i, W_{y_i}))}{\\sum_{k=1}^C exp(d_c(z_i, W_k))}$ (5)\nwhere $W_k$ is the proxy of the k-th class. In this way, the discriminant analysis is performed in the hyperbolic space.\nIn the following content, we want to explore how the hyperbolic distance-based objective affects the training of the backbone.\nTheorem 1. Given two samples $x_i$ and $x_j$ in the tangent space of 0, Eq.(3) projects them into the hyperbolic space denoted by $z_i$ and $z_j$, respectively, whose hyperbolic distance is $d_h(z_i, z_j)$. The relation with the distances satisfy:\n$d_h(z_i, z_j) \\approx \\sqrt{c}(\\|x_i\\|^2 + \\|x_j\\|^2), x_i\\perp x_j$ (6)\n$d_e(z_i, z_j) = \\sqrt{c}\\|x_i - x_j \\|, x_i = kx_j, k \\in \\mathbb{R}$\nThe proof is attached in the appendix.\nRemark 1. From theorem 1, we claim that the hyperbolic distance is bounded by a linear expansion of the Euclidean distance.\nLet us consider the case $x_i\\perp x_j$. Because $2\\sqrt{a^2 + b^2} > |a| + |b|$, there is $d_c(z_i, z_j) \\approx \\sqrt{c}(\\|x_i\\|^2 + \\|x_j\\|^2) \\leq 2\\sqrt{c}\\|x_i - x_j \\|$.\nLet us discuss the case $x_i\\not\\perp x_j$. Because Eq.(3) and Eq.(4) are two mutually inverse functions, each point in the tangent space has a map in the hyperbolic space, and vice versa. Thus, given two samples $x_i$ and $x_j$, there is a function between the value of the Euclidean distance $d_e(x_i, x_j)$ and the value of the hyperbolic distance $d_h(z_i, z_j)$, where $z_i = exp_O(x_i)$. Thus, by dividing the two distances, we can obtain a new function denoted by $\\rho(d_e(x_i, x_j)) = \\frac{d_e (z_i,z_j)}{dh (x_i,x_j)}$. If we fix the norm $||x_i||^2$ and $||x_j||^2$ and let $||x_i||^2 \\neq ||x_j||^2$, the function $\\rho(d_e(x_i, x_j))$ can be rewritten as $\\rho(\\theta)$ where $\\theta$ is the angle between $x_i$ and $x_j$. In this way, we have $d_h (z_i, z_j) = \\rho(\\theta)d_e(x_i, x_i)$. From Theorem 1, we know that $\\rho(\\theta) = \\rho(\\pi) = \\sqrt{c}$ and $\\rho(\\pi/2) = \\rho(3\\pi/2) \\approx 2\\sqrt{c}$. Because the maps in Eq.(3) and Eq.(4) connect two Riemannian manifolds, the maps should be smooth and continuous. Thus, the function $\\rho(\\theta)$ is smooth and continuous. Considering $\\rho(0) = \\rho(\\pi) = \\sqrt{c}$ and $\\rho(\\pi/2) = \\rho(3\\pi/2) \\approx 2\\sqrt{c}$, $\\rho(\\theta)$ has the maximal value K when $\\theta \\in [0, 2\\pi]$. Therefore, we can have $d_{he} (z_i, z_j) < Kd_h(x_i, x_j)$.\nThe content in remark 1 indicates that if a set of clusters is distributed hierarchically in the hyperbolic space, their origins of the exponential map are also hierarchically distributed (in Euclidean space). This is because the hierarchical distribution inevitably contains samples with a very large distance, while the non-hierarchical distribution may not have. Therefore, if a set of non-hierarchically distributed samples becomes hierarchically distributed after an exponential map, the exponential map should have a strong distance expansion ability, which contradicts the content in Remark 1.\nTherefore, Remark 1 implies that the hyperbolic discriminant objective makes the backbone extract the hierarchical information."}, {"title": "2.3 Suffering from Deep Hyperbolic Representation Learning", "content": "Firstly, we introduce the definition of the Lipschitz constant of the function $f(I)$, which is presented as follows:\n$Lip(f) = \\underset{\\forall I_i, I_j}{sup} \\frac{|| f(I_i) - f(I_j) ||_2}{||I_i - I_j ||_2}$ (7)\nThe Lipschitz constant of a backbone is widely used to explore the learning ability of the backbone. For example, in (Krishnan et al. 2020; Fazlyab et al. 2024), they propose a regularization term to reduce the Lipschitz constant of the backbone, and increase the generalization ability of the backbone as a consequence. (Fazlyab et al. 2019) employs the Lipschitz constant of the projection to estimate the testing error of the model. (Song et al. 2021) indicates if the objective function requires a very large Lipschitz constant, the model training will collapse. All those works suggest that we should avoid enlarging the Lipschitz constant of the backbone when we train the deep models.\nAs discussed in the previous section, the hyperbolic discriminant objective directly forces the backbone to extract the hierarchy information, which also leads to a very large distance between samples. This means the Lipschitz constant of the backbone in hyperbolic representation learning is required to be large, which may harm the generalization ability and the training of the backbone."}, {"title": "3 The Proposed Approach", "content": "3.1 Motivation\nIn this section, we show that bilinear pooling and its variants can non-linearly increase the distance between two input samples. In this way, if we let the hierarchy information be formed in the space of bilinear pooling, its inputted samples can avoid having large distances. Therefore, when the backbone in hyperbolic representation learning is followed by a bilinear pooling, the Lipschitz constant of the backbone can be reduced.\nGiven a set of samples $\\{x \\in \\mathbb{R}^{n\\times 1}\\}_{i=1}^N$, the bilinear pooling is calculated by an outer product, i.e., $b_i = vec(x_ix_i^T) \\in \\mathbb{R}^{n^2\\times 1}$. Because $b_i^Tb_j = (x_i^Tx_j)^2$, the bilinear pooling can be seen as the explicit projection of the second-order polynomial kernel function $\\kappa(x_i, x_j) = (|x_i^Tx_j| + d)^2$ with $d = 0$. Imitating the polynomial kernel function, we can construct a general kernel function $\\kappa(x_i, x_j) = f(|x_i^Tx_j|)$ where $f(a) > 0$ is a non-linearly increasing function. (The proof of $f(|x_i^Tx_j|)$ is a kernel function is presented in the Appendix.) In this way, we analyze how bilinear pooling increases the distance of its inputted samples from the perspective of the general kernel function.\nTheorem 2. If $f (a) > 0$ is a non-linearly increasing function with respective to $a$, determining a kernel function $\\kappa(x_i, x_j) = f(x_i^Tx_j)$, there exists another increasing function $g(a)$ to let $d_\\Phi(x_i, x_j) > g(||x_i - x_j ||^2)$.\nThe proof is presented in the Appendix.\nThe above theorem demonstrates that the kernel function can non-linearly increase the distance between its inputted samples. As $f(a) = a^2 (a > 0)$ is an increasing function, the second-order polynomial kernel function satisfies the requirement of the above theorem. Therefore, bilinear pooling can non-linear increase the distance of its input samples. Besides the second-order polynomial function, we can set $f(a) = exp(ra) (r > 0)$ to generate other kernel function that meets the requirement in Theorem 2. Therefore, Theorem 2 gives an instruction to construct other types of kernel functions that can increase the distances of their inputted samples, which extends the spectrum of the section of kernel functions.\nIn deep learning, approaches prefer the explicit projection rather than the implicit one, like the general kernel function. This problem can be solved using the random Maclaurin projection, which approximates the product-based kernel function with a relatively low-dimensional feature compared with the dimensionality of the bilinear pooling (Gao et al. 2016). However, (Yu, Li, and Li 2021) points out that the random Maclaurin projection is unstable because of the involvement of the random projection. To make the result stable, the dimension of the random Maclauri projection is relatively large.\nThe calculation of random Maclaurin projection (RMP) has the same formulation as the low-rank Hadamard product-based bilinear projection (LK-HPBP) (Gao et al. 2016), i.e.,\n$b_i = W_1x_i^T W_2x_i$ (8)\nIn RMP, $W_1 \\in \\mathbb{R}^{n\\times h}$ and $W_2 \\in \\mathbb{R}^{n\\times h}$ are two random projections, while they are learnable in the LK-HPBP. In the literature, the dimensionality of LK-HPBP is about 2048, while that of RMP is about 8194. This means that the learnable projections can output more stable and low-dimensional bilinear features.\nHowever, in practice, the experimental results of adopting the LK-HPBP into hyperbolic representation learning are not good. This is because it loses the ability of the distance expansion mentioned in Theorem 2, which makes the LK-HPBP less effective in capturing the hierarchy information. This is theoretically supported by the following theorem 3.\nTheorem 3. If a set of bilinear features are linearly separated, their dimension can by reduced by a linear projection without harming the discriminant ability, and the distances among the obtained low-dimensional features are upper-bounded by a constant.\nThe proof is presented in the Appendix.\nIn the following section, we want to introduce a novel approach to utilize the benefits of the two types of compact bilinear pooling and discard their shortcomings."}, {"title": "3.2 Kernel approximation regularization", "content": "We also employ the Hadamard product bilinear projection like Eq.(8) to learn the low-dimension bilinear features be-"}, {"title": "H Related Work", "content": "Bilinear Pooling. Bilinear pooling was initially used to enhance the discriminative ability of visual features in a bag-of-word approach (Koniusz et al. 2016). Later, it was used to significantly improve fine-grained classification in an end-to-end learning framework (Lin, RoyChowdhury, and Maji 2017). Subsequently, numerous studies were conducted on bilinear pooling and similar methods for various visual tasks (Gao et al. 2016, 2020; Kong and Fowlkes 2017). These include designing novel bilinear pooling, normalization strategies, and studying dimensionality reduction techniques. Beyond image recognition tasks, research expanded into graph-structured data. For instance, some studies investigated the use of bilinear pooling for drug discovery (Bai et al. 2023). Other research showed that bilinear pooling could enhance features of graph neural networks on graph classification, as it obtains properties like invariance (Wang and Ji 2020). However, whether it is on fine-grained data or graph-structured data, few studies suggest that bilinear pooling can be used to enhance hierarchical representations.\nHyperbolic Representation Learning. Hyperbolic representation learning on the other side was first applied in natural language processing. It uses the distortion-free properties of hyperbolic space to approximate tree structures and learn vectors representing hierarchical category information. To effectively utilize the properties of hyperbolic space, various learning algorithms were developed, such as hyperbolic space-based kernel methods (Fang, Harandi, and Petersson 2021), hyperbolic neural networks (Chami et al. 2019), hyperbolic space metric learning (Vinh Tran et al. 2020), hyperbolic discriminative classifiers (Cho et al. 2019), among others. Later, to enhance the performance of hyperbolic space representation learning, regularization methods such as alignment and curvature learning algorithms (Yang et al. 2023), numerical stability regularization (Mishne et al. 2023), and norm-wrap-based gradient updating methods (Nagano et al. 2019) were proposed. However, these methods assume that projecting features into hyperbolic space automatically leads to learning hierarchical information. In contrast, this paper focuses on enhancing the ability of hyperbolic space learning to capture hierarchical features.\nGraph Neural Networks. On the other side, the field of graph learning has witnessed significant research interest and GNNs have become a standard in many machine learning tasks. Graph Convolution Networks (GCN) (Kipf and Welling 2016) are one type of powerful model for representing graph-structured data. Despite their success, GCNs are inherently shallow due to the over-smoothing problem (Keriven 2022). Although some approaches as GCNII (Chen et al. 2020) have extended the learning ability enabling deeper models, or powerful attention-based feature extractors (Zhang et al. 2021; Shi et al. 2020) have been embedded into the message-passing strategy, effectively inducing the right representation remains an open challenge.\nIn that sense, several works in the literature have used hyperbolic learning to enhance the representation learned by the GNN. However, this has not always translated into beneficial models from the performance and stability perspective. E.g., (Chami et al. 2019) discusses the sensitivity issues related to learning the curvature in the intermediate layers. Also, simple operations required in message passing and aggregation steps require in some cases to operate back in the tangent space, (Dai et al. 2021). Our approach differs from the mentioned ones in the sense that we can directly adopt the potential of Euclidean GNNs, i.e., our method is directly applicable to competitive architectures, and bringing this architecture into the hyperbolic space using a simple projection. Lastly, differently from (Chami et al. 2019), that does not report benefit in classifying in the hyperbolic space, and requires mapping the features to the tangent space."}]}