{"title": "QUALITY DIVERSITY IMITATION LEARNING", "authors": ["Zhenglin Wan", "Xingrui Yu", "David M. Bossens", "Yueming Lyu", "Qing Guo", "Flint Xiaofeng Fan", "Ivor Tsang"], "abstract": "Imitation learning (IL) has shown great potential in various applications, such as robot control. However, traditional IL methods are usually designed to learn only one specific type of behavior since demonstrations typically correspond to a single expert. In this work, we introduce the first generic framework for Quality Diversity Imitation Learning (QD-IL), which enables the agent to learn a broad range of skills from limited demonstrations. Our framework integrates the principles of quality diversity with adversarial imitation learning (AIL) methods, and can potentially improve any inverse reinforcement learning (IRL) method. Empirically, our framework significantly improves the QD performance of GAIL and VAIL on the challenging continuous control tasks derived from Mujoco environments. Moreover, our method even achieves 2\u00d7 expert performance in the most challenging Humanoid environment.", "sections": [{"title": "1 Introduction", "content": "Imitation learning (IL) enables intelligent systems to quickly learn complex tasks by learning from demonstrations, which is particularly useful when manually designing a reward function is difficult. IL has been applied to many"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Quality Diversity Optimization", "content": "Distinct from traditional optimization which aims to find a single solution to maximize the objective, Quality Diversity (QD) optimization aims to find a set of high-quality and diverse solutions in an n-dimensional continuous space R^n. Given an objective function f : R^n \u2192 R and k-dimensional measure function m : R^n \u2192 R^k, the goal is to find solutions \u03b8\u2208 R^n for each local region in the behavior space B = m(R^n)."}, {"title": "2.2 Quality Diversity Reinforcement Learning", "content": "The Quality Diversity Reinforcement Learning (QD-RL) problem can be viewed as maximizing f(\u03b8) = \u0395_{\u03c0\u03b8} [\u03a3_{t=0}^{T-1} \u03b3^t r(s_k,a_k)] with respect to diverse \u03b8 in a policy archive defined by measure m [Cideron et al., 2020]. In QD-RL, both the objective and measure are non-differentiable, requiring approximations by DQD approaches. Previous work employs TD3 to approximate gradients and ES for exploration [Nilsson and Cully, 2021, Pierrot et al., 2021], but is constrained to off-policy methods. The state-of-the-art QD-RL algorithm, Proximal Policy Gradient Arborescence (PPGA), employs a vectorized PPO architecture to approximate the gradients of the objective and measure functions [Batra et al., 2023]. While the policy gradient can approximate the cumulative reward, the episode-based measure is harder to differentiate. PPGA addresses this by introducing the Markovian Measure Proxy (MMP), a surrogate measure function that correlates strongly with the original measure and allows gradient approximation via policy gradient by treating it as a reward function. PPGA uses k + 1 parallel environments with distinct reward functions\u2014one for the original reward and k for the surrogate measures. It approximates the gradients of both the objective and the k measure functions by comparing the policy parameters before and after multiple PPO updates. These gradients are then passed to the modified CMA-MAEGA to update the policy archive. We recommend readers to explore prior works in depth [Batra et al., 2023] or refer to Appendix F for further details on PPGA and related QD-RL methodologies."}, {"title": "2.3 Imitation Learning", "content": "Imitation learning (IL) trains an agent to mimic expert behaviors from demonstrations [Zare et al., 2024]. Imitation From Observation (IFO) is a branch of IL where only the expert's state sequence is available [Liu et al., 2018]. Behavior Cloning (BC) uses supervised learning to imitate expert behavior but suffers from severe error accumulation [Ross et al., 2011].\nInverse Reinforcement Learning (IRL) seeks to recover a reward function from the demonstrations, and use reinforcement learning (RL) to train a policy that best mimics the expert behaviors [Abbeel and Ng, 2004]. Early IRL methods estimate rewards in the principle of maximum entropy [Ziebart et al., 2008, Wulfmeier et al., 2015, Finn et al., 2016]. Recent adversarial IL methods treat IRL as a distribution-matching problem. Generative Adversarial Imitation Learning (GAIL) [Ho and Ermon, 2016] trains a discriminator to differentiate between the state-action distribution of the demonstrations and the state-action distribution induced by the agent's policy, and output a reward to guide policy improvement. Variational Adversarial Imitation Learning (VAIL) [Peng et al., 2018b] improves GAIL by applying a variational information bottleneck (VIB) [Alemi et al., 2016] to the discriminator, improving the stability of adversarial learning. Adversarial Inverse Reinforcement Learning (AIRL) [Fu et al., 2017] learns a robust reward function by training the discriminator via logistic regression to classify expert data from policy data.\nPrimal Wasserstein Imitation Learning (PWIL) [Dadashi et al., 2021] introduces an offline reward function based on an upper bound of the Wasserstein distance between the expert and agent's state-action distributions, avoiding the instability of adversarial IL methods. Similarly, Generative Intrinsic Reward-driven Imitation Learning (GIRIL) [Yu et al., 2020] computes rewards offline by pretraining a reward model using a conditional VAE [Sohn et al., 2015], which combines a backward action encoding model with a forward dynamics model. The reward is derived from the prediction error between the actual next state and its reconstruction. GIRIL has demonstrated superior performance even with limited demonstrations. In this work, we focus on QD-IL with limited demonstrations.\nOther IL methods include IDIL [Seo and Unhelkar, 2024] and IQ-learn [Garg et al., 2021]. IDIL assumes expert behavior is intent-driven, while IQ-learn learns a Q-function instead of a reward function. These methods involve specific assumptions or alternative problem definitions. Since our focus is on generic IRL methods, we do not explore them further. For simplicity, we refer to methods that learn a reward function (fixed or dynamic) as IRL methods. We concentrate on two widely used IRL methods\u2014GAIL and VAIL as our base models. More details are provided in Appendix C."}, {"title": "3 Problem Definition", "content": "Definition 1 (Quality-Diversity Imitation Learning). Given expert demonstrations D = {(s_i,a_i)}_{i=1}^l and their measures, where s_i and a_i are states and actions, QD-IL aims to learn an archive of diverse policies {\u03c0_{\u03b8_i}}_{i=1}^M that collectively maximizes f(\u03b8) (e.g., cumulative reward) without access to the true reward. The archive is defined by a k-dimensional measure function m(\u03b8), representing behavior patterns. After dividing the archive into M cells, the objective of QD-IL is to find M solutions, each occupying one cell, to maximize:\nmax {\u03b8_i}_{i=1}^M  \u2211_{i=1}^M f(\u03b8).  (1)"}, {"title": "4 Proposed Method", "content": "In this section, we will introduce our QD-IL framework, which aims to learn a QD-enhanced reward function using the QD-RL algorithm PPGA to learn the policy archive. Specifically, we propose the measure bonus to address the challenge of unbalanced exploration and exploitation and measure conditioning to address the challenge of localized reward. Figure 3 shows the main components of our framework, Measure conditioned and bonus-driven Inverse Reinforcement Learning (MConbo-IRL). We provide the pseudo-code of our framework in Appendix A."}, {"title": "4.1 Measure Bonus", "content": "The objective of QD-RL optimization in PPGA is: g(\u03b8) = |c_0|f(\u03b8) + \u03a3_{j=1}^k c_j m_j(\u03b8), where dynamic coefficients c_i balance maximizing cumulative reward f(\u03b8) and achieving diverse measures m(\u03b8). However, we observed that the fitness term f heavily influences PPGA's search policy update direction, as archive improvement is primarily driven by f. PPGA frequently becomes stuck in local regions, generating overlapping solutions with only marginal improvements in the archive due to limited exploration. Therefore, it will explore less in other areas, as illustrated in Figure 1(a). Additionally, a key challenge in QD-IL is the conflict between imitation learning and diversity. Limited and monotone expert demonstrations lead to highly localized and sometimes misleading reward functions, further exacerbating the problem by restricting search policy updates. Hence, we aim to encourage the search policy to find new behavior patterns (i.e., the empty area in the policy archive).\nLemma 1. Suppose the reward function of one MDP is given by r(s,a) = I(m_i \u2208 A_e), where s and a represent the state and action at time step t of episode i, A_e means the empty area of archive A and I(m_i \u2208 A_e) is indicator function indicating whether the measure of i \u2013 th episode falls into A_e. Then if one iteration of PPO successfully increases the"}, {"title": "4.2 MConbo-IRL", "content": "Measure bonus improves policy diversity but doesn't guarantee the performance of diverse policies. To address this, we introduce measure conditioning, which can potentially be integrated into most IRL methods. We demonstrate this using two popular IRL methods, GAIL and VAIL."}, {"title": "4.2.1 MConbo-GAIL", "content": "The GAIL discriminator receives a state-action pair (s, a) and outputs how closely the agent's behavior resembles that of the expert, serving as a reward function. However, GAIL tends to overfit specific behaviors with limited demonstrations. In large state spaces, the discriminator struggles to generalize to unseen states [Kostrikov et al., 2018]. This results in localized and sparse rewards, hindering quality diversity. Therefore, the core question in QD-IL is how to generalize knowledge from limited demonstrations to the entire policy archive while avoiding localized rewards.\nTo address this, we use the Markovian Measure Proxy [Batra et al., 2023]. It decomposes trajectory-based measures into individual steps: m_i(\u03b8) = 1/T \u03a3_{t=0}^{T-1} \u03b4_i(s_t). This makes the measure state-dependent and Markovian. We make the key observation that the single-step measure \u03b4_i(s_t) abstracts higher-level task features such as ground contact in locomotion, while filtering out lower-level state details (e.g., joint angles and velocities). This provides a more general representation, enabling better generalization across the policy archive. By simply incorporating \u03b4_i(s_t) as an additional input to the GAIL discriminator, we propose Measure-Conditional-GAIL with the following modified objective:\nmax_{\u03c0} min_{D_e} E_{(s,a)~\u03c0_e} [-log D_e(s,a, \u03b4(s))] + E_{(s,a)~\u03c0(s)} [-log(1 \u2013 D_e(s,a, \u03b4(s)))].  (3)\nThis approach encourages the discriminator to generalize by focusing on higher-level state descriptors \u03b4(s), capturing essential task-relevant features. It enables the agent to learn broader behavior patterns from limited demonstrations rather than specific state-action mappings, improving generalization to unseen states. This helps imitator to answer this question: How should the agent behave in a specific type of state, rather than mimicking exact state-action pairs.\nWe then formulate the total reward function computed by MConbo-GAIL as follows:\nr(s,a,m) = -log (1 \u2013 D_o (s, a, \u03b4(s))) + r_{diversity} (s_i, a, m_i).  (4)"}, {"title": "4.2.2 MConbo-VAIL", "content": "To extend our framework to other IRL algorithms, we begin with another generic IL method - Variational Adversarial Imitation Learning (VAIL). To facilitate behavior exploration and knowledge generalization, we slightly modify the VAIL objective as follows:\nmin_{D_e,E'} max_{\u03b2\u22650} E_{(s,a)~\u03c0_e} [E_{z~E'(z/s,a,\u03b4(s))} [log(-D_e(z))]] + E_{(s,a)~\u03c0_t} [E_{z~E'(z|s,a,\u03b4(s))} [ - log(1 \u2013 D_e(z))]]\n+\u03b2E_{s~\u03c0_t} [d_{KL}(E' (z|s,a,\u03b4(s))||p(z)) \u2013 I_c],  (5)"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experiment Setup", "content": "We evaluated our framework on three popular Mujoco [Todorov et al., 2012] environments: Halfcheetah, Humanoid, and Walker2d. The goal in each task is to maximize forward progress and robot stability while minimizing energy consumption. Our experiments are based on the PPGA implementation using the Brax simulator [Freeman et al., 2021], enhanced with QDax wrappers for measure calculation [Lim et al., 2022]. We leverage pyribs [Tjanaka et al., 2023] and CleanRL's PPO [Huang et al., 2020] for implementing the PPGA algorithm. The observation space sizes for these environments are 17, 18, and 227, with corresponding action space sizes of 6, 6, and 17. The measure function is defined as the number of leg contacts with the ground, divided by the trajectory length. All Experiments were conducted on a system with four A40 48G GPUs, an AMD EPYC 7543P 32-core CPU, and a Linux OS."}, {"title": "5.2 Demonstrations", "content": "We use a policy archive obtained by PPGA to generate expert demonstrations. To follow a real-world scenario with limited demonstrations, we first sample the top 500 high-performance elites from the archive as a candidate pool. Then from this pool, we select a few demonstrations such that they are as diverse as possible. This process results in 4 diverse demonstrations (episodes) per environment. Appendix B provides the statistical properties, and Figure 4 visualizes the selected demonstrations."}, {"title": "5.3 Overall Performance", "content": "To validate the effectiveness of our approach as a generic QD-IL framework, we compare it against the most recent QD-RL method-PPGA with true reward function, and several widely-used and state-of-the-art IL methods: 1) Traditional IRL: Max-Entropy, 2) Online reward methods: GAIL, VAIL, and AIRL, and 3) Offline reward methods: GIRIL and PWIL. Each baseline learns a reward function, which is then used to train standard PPGA under identical settings for all baselines. Hyperparameter details are provided in Appendix D. All the experiments are averaged with three random seeds.\nWe evaluate using four common QD-RL metrics: 1) QD-Score, the sum of scores of all nonempty cells in the archive. QD-score is the most important metric in QD-IL as it aligns with the objective of QD-IL as in equation (1); 2) Coverage, the percentage of nonempty cells, indicating the algorithm's ability to discover diverse behaviors; 3) Best Reward, the highest score found by the algorithm; and 4) Average Reward, the mean score of all nonempty cells, reflecting the ability to discover both diverse and high-performing policies. We use the true reward functions to calculate these metrics."}, {"title": "5.4 Ablation on Measure Bonus", "content": "In this section, we examine the effect of the measure bonus by comparing the performance of MConbo-GAIL with Measure-Conditional-GAIL (without the measure bonus) on Walker2d. The results in Figure 8 show a significant performance drop across all metrics without the measure bonus. This highlights the synergy between the measure bonus and CMA-MEGA. Without the exploration bonus, the algorithm struggles with highly localized rewards and the"}, {"title": "6 Conclusion and Future Work", "content": "In this work, we proposed MConbo-IRL which can potentially improve any IRL method in QD task. Additionally, our framework opened the possibility of QD-IFO, providing the first generic QD-IL framework for future research. Our framework follows the paradigm of IRL to learn a QD-enhanced reward function, and use a QD-RL algorithm to optimize policy archive. By encouraging behavior-level exploration and facilitating knowledge generalization from limited expert demonstrations, our framework addresses the key challenges of QD-IL. Extensive experiments show that our framework achieves near-expert or beyond-expert performance, and significantly outperforms baselines.\nTo establish our framework as a generic QD-IL solution, we focused on improving the two widely used IRL algorithms in this paper to make our framework as simple and effective as possible. However, we believe that our framework has the potential to be compatible with more IRL algorithm backbones. Additionally, exploring the development of a new IL architecture inherently designed for quality diversity remains an important avenue for our future research. We also discuss the potential limitations of our work in Appendix J."}, {"title": "A Algorithm Pseudo Code", "content": "Algorithm 1 presents the pseudocode for using MConbo-GAIL as our reward module and PPGA as the QD-RL algorithm. The parts highlighted in red indicate the key distinctions from PPGA. We utilize a reward model to compute the fitness value (reward) for the QD-RL problem, with Algorithm 2 explaining how our reward model functions."}, {"title": "B Demonstration Details", "content": "Figure 9 shows the Mujoco environments used in our experiments. Table 3 shows the detailed information of the demonstrations in our experiment."}, {"title": "C Baseline Imitation Learning Methods", "content": "This section summarizes the details for the related IRL methods used as baselines in this paper:"}, {"title": "D Hyperparameter Setting", "content": ""}, {"title": "D.1 Hyperparameters for PPGA", "content": "Table 4 summarizes a list of hyperparameters for PPGA policy updates."}, {"title": "D.2 Hyperparameters for IL", "content": "Table 5 summarizes a list of hyperparameters for AIRL, GAIL, measure-conditioned GAIL, and MConbo-GAIL."}, {"title": "E Ablation on Measure Conditioning", "content": "To verify the effect of measure conditioning in GAIL, we compare the performance of Mbo-GAIL (GAIL with measure bonus) and MConbo-GAIL (GAIL with measure conditioning and bonus) in the Halfcheetah environment, as illustrated in Figure 10."}, {"title": "F Details about PPGA and Related Background", "content": "To help readers to better understand the background of QD-RL, we begin with Covariance Matrix Adaptation MAP-Elites via a Gradient Arborescence (CMA-MEGA) [Fontaine and Nikolaidis, 2021].\nFor a general QD-optimization problem, the objective of CMA-MEGA is:\ng(\u03b8) = |c_0|f(\u03b8) + \u2211_{j=1}^k c_j m_j(\u03b8),  (15)\nIn this context, m_j(\u03b8) represents the j-th measure of the solution \u03b8, and k is the dimension of the measure space. The objective function of CMA-MEGA is dynamic because the coefficient for each measure, c_j, is updated adaptively to encourage diversity in m. For instance, if the algorithm has already found many solutions with high m\u2081 values, it may favor new solutions with low m\u2081 values by making c\u2081 negative, thus minimizing m\u2081. However, the coefficient for the fitness function f will always be positive, as the algorithm always seeks to maximize fitness. This objective function ensures that CMA-MEGA simultaneously maximizes fitness f and encourages diversity across the measures m. We update \u03b8 by differentiating objective (15) and use gradient-descend-based optimization approaches, since DQD assumes f and m are differentiable.\nFurthermore, the coefficients c_j are sampled from a distribution, which is maintained using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [Hansen, 2016]. Specifically, CMA-ES updates the coefficient distribution by iteratively adapting the mean \u00b5 and covariance matrix \u03a3 of the multivariate Gaussian distribution N(\u03bc, \u03a3), from which the coefficients c_j are sampled. At each iteration, CMA-MEGA ranks the solutions based on their archive improvement (i.e. How much they improve the existing solutions of occupied cell). The top-performing solutions are used to update \u03bc, while \u03a3 is adjusted to capture the direction and magnitude of successful steps in the solution space, thereby refining the search distribution over time.\nIn CMA-MAEGA [Fontaine and Nikolaidis, 2023], the concept of soft archives is introduced to improve upon CMA-MEGA. Instead of maintaining the best policy in each cell, the archive employs a dynamic threshold, denoted as t_e. This threshold is updated using the following rule whenever a new policy \u03c0_{\u03b8_i} surpasses the current threshold of its corresponding cell e:\nt_e\u2190 (1-\u03b1)t_e + \u03b1f (\u03c0_{\u03b8_i})\nHere, \u03b1 is a hyperparameter called the archive learning rate, with 0 < \u03b1 < 1. The value of \u03b1 controls how much time is spent optimizing within a specific region of the archive before moving to explore a new region. Lower values of \u03b1 result in slower threshold updates, emphasizing exploration in a particular region, while higher values promote quicker transitions to different areas. The concept of soft archives offers several theoretical and practical advantages, as highlighted in previous studies.\nPPGA [Batra et al., 2023] is directly built upon CMA-MAEGA. We summarize the key synergies between PPGA and CMA-MAEGA as follows:(1) In reinforcement learning (RL), the objective functions f and m in Equation 15 are not directly differentiable. To address this, PPGA employs Markovian Measure Proxies (MMP), where a single-step proxy \u03b4(s_t) is treated as the reward function of an MDP. PPGA utilizes k + 1 parallel PPO instances to approximate the gradients of f and each measure m, where k is the number of measures. Specifically, the gradient for each i-MDP is computed as the difference between the parameters \u03b8_{i,new} after multi-step PPO optimization and the previous parameters \u03b8_{i,old}. (2) Once the gradients are approximated, the problem is transformed into a standard DQD problem. PPGA then applies a modified version of CMA-MAEGA to perform quality diversity optimization. The key modifications include:\n1. Replacing CMA-ES with xNES for Stability: To improve stability in noisy reinforcement learning environ- ments, CMA-ES was replaced with Exponential Natural Evolution Strategy (xNES). While CMA-ES struggled with noisy, high-dimensional tasks due to its cumulative step-size adaptation mechanism, xNES provided more stable updates to the search distribution, especially in low-dimensional objective-measure spaces, and maintained search diversity."}, {"title": "G Synergy of Measure Bonus with CMA-MEGA", "content": "In our QD-IL framework, we made a key observation that by introducing essential guiding signals into the fitness function f, we can effectively encourage exploration at the behavior level. Firstly, note that in a traditional QD-RL setting, the elite of one cell is a policy \u03b8. However, the performance of this elite is computed by the random episodes produced by the policy \u03b8. Thus, the same policy may produce different episodes which occupy different cells. Hence, the motivation of our method is to improve the probability that the new policy produce episodes occupying the empty area of archive, which is the conclusion of Lemma 1. We first give the proof of Lemma 1:\nProof. Proof of (1): The objective of policy optimization is:\nh(\u03b8, A_e) = E_{\u03c4_i~\u03c0_\u03b8} [ \u2211_{t=0}^T r(s_t,a_t)]  = E_{\u03c4_i~\u03c0_\u03b8} [ \u2211_{t=0}^T  \u03b3^t I(m_i \u2208 A_e)]\n = \u2211_{t=0}^T \u03b3^t  E_{\u03c4_i~\u03c0_\u03b8} [I(m_i \u2208 A_e)] =  \u2211_{t=0}^T \u03b3^t 1/ (1-\u03b3) E_{\u03c4_i~\u03c0_\u03b8} [\u2161(m_i \u2208 A_e)] (16)\nwhere T is the episode length (rollout length).\nOptimizing h(\u03b8_{old}, A_e) through multiple rounds of PPO will result in \u03b8_{new} such that h(\u03b8_{new}, A_e) > h(\u03b8_{old}, A_e), since PPO is assumed to steadily improve the policy, thus increasing the objective.\nTherefore, we have:\n E_{\u03c4_i~\u03c0_{\u03b8_{new}}} [I(m_i \u2208 A_e)] \u2265 E_{\u03c4_i~\u03c0_{\u03b8_{old}}} [I(m_i \u2208 A_e)] (17)\nSince E_{\u03c4_i~\u03c0_\u03b8} [I(m_i \u2208 A_e)] = P(m \u2208 A_e |\u03c0_\u03b8) where m_i is the measure of episode \u03c4_i, it follows that:\nP(m\u2208 A_e | \u03c0_{\u03b8_{new}}) \u2265 P(m \u2208 A_e | \u03c0_{\u03b8_{old}})\nProof of (2): Based on Bayes' rule, we have:\nP(\u03c0_\u03b8 m\u2208 A_e) =  (P(m \u2208 A_e |\u03c0_\u03b8)P(\u03c0_\u03b8)) / (P(m \u2208 A_e))\nSince P(\u03c0_\u03b8) and P(m \u2208 A_e) can be treated as constants when \u03b8 changes (assuming \u03b8 has uniform prior), we have:\nP(\u03c0_{\u03b8_{new}}| m\u2208 A_e) > P(\u03c0_{\u03b8_{old}}| m\u2208 A_e)\nThus, the lemma is proved.\nIt is worthy noting that, 1) while Lemma 1 offers valuable intuition for our approach, our method's practical effectiveness is not solely dependent on the theoretical guarantee of monotonic improvement in PPO. 2) the solution will only be added to the archive during the update_archive step in Algorithm 1. However, the scope of Lemma 1 is limited to VPPO.compute_jacobian and VPPO.train. This implies that the episodes generated during the training phase and the gradient-approximating stage will not be inserted into the archive.\nIf we apply a measure bonus to the original GAIL reward, the objective of CMA-MEGA transforms into:\ng(\u03b8) = |c_0|[f(\u03b8)+h(\u03b8, A)] + \u2211_{j=1}^k c_j m_j(\u03b8), (18)\nwhere h(\u03b8, A) represents the cumulative bonus reward based on the current policy archive A. The gradient of \u03b8 becomes:\n\u2207_\u03b8g(\u03b8) = |c_0|\u2207f(\u03b8)+ |c_0|\u2207_\u03b8h(\u03b8,A) + \u2211_{j=1}^k c_j\u2207_\u03b8m_j(\u03b8).\nNotably, the fitness function f(\u03b8) is calculated using the GAIL reward in the QD-IL setting. Lemma 1 shows that the measure bonus leads to a new policy that has a higher probability of producing episodes with measures in the empty regions of the archive. As a result, a higher c\u2080 value will guide the search policy towards unoccupied areas in the archive, leading to significant archive improvements (since occupying a new cell naturally results in larger archive improvements compared to replacing an existing elite in a cell)."}, {"title": "H Scalability Study", "content": "We also explore the scalability of the MConbo framework. The key challenge of QD-IL is learning diverse policies from homogeneous expert demonstrations, so we test MConbo-GAIL's ability to scale with fewer demonstrations, representing more uniform expert behavior. Using the Walker2d environment, we reduce the number of demonstrations to 2 and 1 and compare the performance of MConbo-GAIL and GAIL.\nFigure 11 shows the learning curves of MConbo-GAIL, GAIL, and PPGA (true reward), while Figure 12 compares their performance of QD-score and coverage. Notably, the coverage of MConbo-GAIL remains close to 100% despite the decrease in expert demonstration numbers, highlighting the robustness of Measure Bonus to consistently find diverse policies. This robustness is attributed to the synergy between Measure Bonus and CMA-MEGA (Appendix G). On the other hand, fewer demonstrations reduce the quality of expert data, leading to lower QD scores. This is especially true for MConbo-GAIL, which will inherently explore some behavior space regions which is distant from the expert behavior. Hence, learning high-performing policy will be difficult, when the algorithm can't find relevant behavior patterns in expert demonstrations. However, MConbo-GAIL still outperforms GAIL. It can learn diverse and relatively high-performing policies even with just one demonstration, demonstrating its scalability with limited expert data."}, {"title": "I Improve imitation from observation", "content": "Imitation from Observation (IFO) is a type of imitation learning where agents learn behaviors by observing state trajectories, without needing access to the actions that generated them. Unlike traditional methods that require both states and actions, IFO mimics behavior solely from state sequences, making it ideal for situations like video demonstrations. This approach aligns more naturally with how humans and animals learn, as we often imitate behaviors by observation without knowing the exact actions involved (e.g., muscle movements) [Zare et al., 2024]. IFO is particularly useful in scenarios where action data is unavailable, using techniques like inverse reinforcement learning to infer the underlying policy.\nWe further observe the potential of our MConbo framework to handle IFO problem, as illustrated in Figure 13. In the setting of IFO, we modify the objective of MConbo-GAIL as:\nmax_{\u03c0} min_{D_e} E_{s~\u03c0_e} [-log D_e(s, \u03b4(s))] + E_{s\u223c\u03c0(s)} [\u2212log(1 \u2013 D_e(s, \u03b4(s)))].  (19)\nWhen expert actions are not accessible, we found that MConbo-GAIL can still effectively learn diverse policies without performance degradation. We attribute this to measure conditioning, which allows the algorithm to more easily infer actions from high-level state abstractions. We believe our QD-IL framework opens the door to the possibility of QD-IFO, and we look forward to future research providing more detailed studies in this area."}, {"title": "J Limitations", "content": "Since the reward functions learned by GAIL and VAIL are dynamically updated, using traditional MAP-Elites to maintain the archive may not be ideal. MAP-Elites only preserves the best-performing policy at a given time, and the"}]}