{"title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement", "authors": ["Hui Yuan", "Yifan Zeng", "Yue Wu", "Huazheng Wang", "Mengdi Wang", "Liu Leqi"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for aligning language models (LMs) to be more helpful and less harmful. At its core, RLHF uses a margin-based loss for preference optimization, which specifies the ideal LM behavior only in terms of the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods\u2014the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which results in two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability to the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product between the gradient of preferred log-probability and the gradient of dispreferred log-probability is large relative to the individual gradient norms. We theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework further extend to explaining important differences in the training dynamics of various preference optimization algorithms, and suggesting potential algorithm designs to mitigate the under-specification issue of margin-based methods and thereby improving language model alignment.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) has become a primary approach for aligning Language Models (LMs) to improve their helpfulness and mitigate harmfulness (Stiennon et al., 2020; Bai et al., 2022; Ouyang et al., 2022). This pipeline typically consists of two stages: supervised fine-tuning (SFT), where demonstration data is used to directly teach the model desirable behaviors, and the reinforcement learning (RL) stage, which uses preference data-comparisons between different responses to the same prompt to highlight the contrast between chosen and rejected responses, with the goal of helping the model learn distinctions between good and bad behaviors.\nIn its vanilla form, the RL stage first employs a contrastive loss-based on the margin between the scores of the chosen and rejected responses to train a reward model, followed by policy optimization methods to fine-tune the LM based"}, {"title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement", "content": "on the reward model. Leveraging the structure of the problem, a recent line of work has combined these two steps by directly optimizing the language model using a margin-based preference optimization loss of the following general form (Rafailov et al., 2024; Azar et al., 2024; Xu et al., 2024; Ethayarajh et al., 2024; Hong et al., 2024; Pal et al., 2024; Park et al., 2024; Yuan et al., 2024; Meng et al., 2024; Zhao et al., 2023; Wu et al., 2024):\n$$l(x, y_w, y_l; \\theta) = m(h_w(\\log \\pi_\\theta(y_w|x)) - h_l(\\log \\pi_\\theta(y_l|x))),$$ \nwhere for a language model \\(\\pi_\\theta\\), \\(\\log \\pi_\\theta(y_w|x)\\) specifies the log-probability of the chosen response \\(y_w\\) and \\(\\log \\pi_\\theta(y_l|x)\\) specifies that of the rejected response \\(y_l\\), given the same prompt \\(x\\). Most of the existing preference optimization losses can be interpreted as varying the scalar functions \\(m, h_w, h_l\\) (Section 3.2 and Table 2). At the core, they all rely on the margin between the chosen log-probability \\(\\log \\pi_\\theta(y_w|x)\\) and the rejected log-probability \\(\\log \\pi_\\theta(y_l|x)\\).\nThe training dynamics of these margin-based preference optimization are quite intriguing-the log-probabilities of the chosen and rejected responses often show a synchronized increase and decrease (Figure 1). It is worth noting that, by the end of the training, even though the margin increases (resulting in minimization of the margin-based loss), the log probability of both the chosen and rejected responses may increase (Figure 1a), or both may decrease (Figure 1b).\nThis synchronized log-probability change exposes a fundamental issue with using margin-based loss for preference optimization in language model alignment: it only specifies the ideal behavior of the margin between chosen and rejected log-probabilities, but not the ideal behavior of individual terms. This under-specification may have two problematic consequences:\n\u2022 First, when the primary goal is to reduce the probability of generating rejected responses (e.g., in safety-related alignment tasks where certain undesirable responses should not be generated), merely increasing the margin (i.e., ensuring that the chosen response is preferred over the rejected one) does not guarantee that the log-probability of the rejected response is actually decreasing (Figure 1a).\n\u2022 Second, even when the log-probability of the rejected response does decrease, the current margin-based losses often lead to a simultaneous reduction in the log-probability of the chosen response (Figure 1b). This becomes particularly concerning when we want to retain or even increase the probability of generating the preferred responses. For example, for distilling strong language models into smaller ones (Dubey et al., 2024; Chiang et al., 2023; Tunstall et al., 2024; Taori et al., 2023), a common practice is to synthesize chosen samples with those strong models; in some alignment applications (e.g., math problem-solving and coding), chosen samples can be the human demonstrations collected during the SFT phase (Chen et al., 2024). In both scenarios, the chosen responses are ideal and we want the probability of the chosen response to increase or at least not decrease\u2014to ensure the model retains a high probability of generating these ideal responses."}, {"title": "3 Gradient Entanglement", "content": "Margin-based preference optimization often results in synchronized increase/decrease in chosen and rejected log- probabilities (Section 1). Our key finding is that the synchronized change is caused by an effect we term as gradient entanglement. Starting with a case study on DPO in Section 3.1, we formally define the gradient entanglement effect, from the definition we will see the entanglement is passed through the inner product between chosen and rejected gradients. We derive conditions on such inner product under which the gradient entanglement causes concerning synchronized change. In Section 3.2, we identify gradient entanglement for general margin-based preference optimiza- tion methods and apply our framework to explain the training dynamics of those methods. We validate our findings empirically in Section 3.3."}, {"title": "3.1 Case study: gradient entanglement in DPO", "content": "Let us start with deriving the gradient of the DPO objective (2). To simplify the formula of DPO gradient, we define the implicit reward \\(f_\\theta(x, y) := \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)}\\) (which is a scalar) and introduce the notations:\n\\(\\log \\pi_w(\\theta) := \\log \\pi_\\theta(y_w|x), \\log \\pi_l(\\theta) := \\log \\pi_\\theta(y_l|x), c(\\theta) := \\sigma(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta(x, y_w)) > 0\\).\nThen considering a single sample \\((x, y_w, y_l)\\), the DPO gradient can be rewritten as\n$$\\nabla_\\theta l_\\text{DPO} = -\\beta c(\\theta) \\cdot (\\nabla_\\theta \\log \\pi_w(\\theta) - \\nabla_\\theta \\log \\pi_l(\\theta)).$$"}, {"title": "3.1.1 When will the gradient entanglement be concerning?", "content": "If we measure the change in the margin between \\(\\log \\pi_w\\) and \\(\\log \\pi_l\\), i.e., the quantitiy \\(\\Delta(\\log \\pi_w - \\log \\pi_l)\\), then the Cauchy-Schwarz inequality ensures:\n$$\\Delta(\\log \\pi_w - \\log \\pi_l) \\approx C \\cdot (||\\nabla \\log \\pi_w ||^2 - 2\\langle\\nabla \\log \\pi_w, \\nabla \\log \\pi_l\\rangle + ||\\nabla \\log \\pi_l||^2) \\ge 0,$$ \nwhich fulfills the contrastive goal of the DPO loss: enlarging the difference between the chosen log-probability \\(\\log \\pi_w\\) and rejected log-probability \\(\\log \\pi_l\\). However, due to the gradient entanglement effect, to individually ensure the increment of \\(\\log \\pi_w\\) and the decrement of \\(\\log \\pi_l\\), the inner product between chosen and rejected gradient should satisfy conditions listed in Condition 1. We will refer to Condition 1 as \"gradient condition\" as it is imposed on the inner product of gradients.\nBased on the two conditions above, in Table 1 we summarize three cases that depict all possible changes on the chosen and rejected log-probabilities and are categorized by the value of \\(\\langle\\nabla \\log \\pi_w, \\nabla \\log \\pi_l\\rangle\\)."}, {"title": "3.2 General gradient entanglement effect", "content": "We now move on to the general margin-based loss (1). Here, we additionally consider regularizers used in these losses:\n$$l(\\theta) = - (m(h_w(\\log \\pi_w) - h_l(\\log \\pi_l)) + \\Lambda(\\log \\pi_w)),$$ \nwhere \\(\\Lambda(\\log \\pi_\\theta(y_w|x))\\) is a scalar regularizer depending on the chosen log-probability. We instantiate popular pref- erence optimization methods from this general form in Table 2, where we denote \\(c_\\text{ref} := \\log \\pi_\\text{ref}(y_w|x), c_\\text{ref}^l = \\log \\pi_\\text{ref}(y_l|x), c_\\text{ref} := c_\\text{ref}^w - c_\\text{ref}^l\\). Terms that only depend on \\(\\pi_\\text{ref}(y|x)\\) shall be viewed as constant, independent of \\(\\theta\\).\nBased on this unified formulation of preference optimization objectives (6), we derive general gradient entanglement for all margin-based losses (derivations in Appendix A.1):\nIn the general form of gradient entanglement, \\(d_w\\) and \\(d_l\\) are scalars defined as\n$$d_w := m'(h_w(\\log \\pi_w) - h_l(\\log \\pi_l))h'_w(\\log \\pi_w) + \\Lambda'(\\log \\pi_w),$$ \n$$d_l := m'(h_w(\\log \\pi_w) - h_l(\\log \\pi_l))h'_l(\\log \\pi_l).$$ \nWe derive a generalized version of DPO's gradient condition (Condition 1) for general margin-based losses."}, {"title": "3.2.1 How do other margin-based methods work differently from DPO?", "content": "Utilizing the gradient condition we derived, we provide in the following a brief discussion on some existing preference optimization algorithms and explain why these algorithms may work differently from DPO under certain settings.\n\u2022 DPO: \\(d_w = d_l = 1\\), reproducing the Condition 1.\n\u2022 SPPO: \\( \\frac{d_w}{d_l} = \\frac{\\beta^{-1} - \\log \\pi_w}{\\beta^{-1} + \\log \\pi_l} > 1\\), where \\(\\beta^{-1}\\) is a large constant. Compared with DPO, SPPO loss ensures that it is easier for \\(\\log \\pi_w\\) to increase based on (9) and harder for \\(\\log \\pi_l\\) to decrease due to (10).\n\u2022 Length-normalization (SimPO, RRHF and IPO): In SimPO, \\(\\frac{d_w}{d_l} = \\frac{|y_w|}{|y_l|}\\) and condition (9) and (10) can be rewritten as:\n$$\\frac{\\langle \\nabla \\log \\pi_w, \\nabla \\log \\pi_l \\rangle}{||\\nabla \\log \\pi_w||^2} < \\frac{|y_w|}{|y_l|}; \\qquad \\frac{\\langle \\nabla \\log \\pi_w, \\nabla \\log \\pi_l \\rangle}{||\\nabla \\log \\pi_l||^2} < \\frac{|y_l|}{|y_w|}.$$"}, {"title": "3.3 Empirical observations", "content": "We conduct experiments on the TL;DR dataset (Stiennon et al., 2020) to showcase the widely-existing phenomenon that the chosen and rejected log-probabilities have synchronized changes during preference optimization. In addition, Figure 1 depicts how different margin-based preference optimization algorithms influence the log-probability of chosen and rejected responses.\nFor DPO and R-DPO, both the chosen and rejected log-probabilities tend to decrease simultaneously. This behavior proofs the existence of gradient entanglement, showing that methods purely dependent on the margin might result in both terms decreasing, with the rejected log-probability decreasing more significantly. This leads to an increase in the margin, which is the original learning objective, but not necessarily an increase in the chosen log-probability.\nSPPO demonstrates a distinct trend where the log-probability of the chosen responses increases, while the log- probability of the rejected responses decreases. This matches the theoretical intuition obtained from the specialized gradient conditions for SPPO in Section 3.2."}, {"title": "4 Investigation on Gradient Inner Product", "content": "The previous section reveals that the gradient entanglement effect is driven by the key quantity: the inner product \\(\\langle\\nabla_\\theta \\log \\pi_w, \\nabla_\\theta \\log \\pi_l\\rangle\\) between chosen and rejected log-probabilities (Condition 1, 2: gradient condition). As demon- strated in Section 3.3 and widely observed in practice, margin-based objectives are often triggered to not behave in the ideal way, suggesting that the gradient condition is violated due to a large gradient inner product. Therefore, in this section, we investigate into such inner product to understand why it can be large when aligning language models. Our investigation focuses on the representative margin-based objective DPO.\nTo build our theoretical intuition, we use synthetic toy settings to analyze the gradient inner product and the changes in log-probabilities. Our theory offers explanations from two perspectives: (1) when the gradient condition holds and which factors do not contribute to enlarging the gradient inner product (Theorem 1, Corollary 2) and (2) when the gradient condition is violated and which factors do cause the gradient inner product to grow, leading to a decrease in the"}, {"title": "4.1 Positive result: when the gradient condition holds", "content": "We first provide a positive result on when the gradient inner product is small, thus Condition 1 holds and DPO exhibits the ideal behavior that pushes up the log-probability of the chosen response and pushes down the log-probability of the rejected one. In the first synthetic setting, we analyze DPO for optimizing an LM with a learnable last linear layer in a single-token prediction task.\n$$ \\pi_\\theta(y^i | x, y_{<i}) = s(h_\\theta)[y]. $$\nTheorem 1. Under Model Setup 1 and Data Setup 1, assume after the SFT stage, given prompt x, the model prediction on the first token in response is uniformly concentrated on M < V tokens in the vocabulary V, then we have\n$$\\langle \\nabla \\log \\pi_w, \\nabla \\log \\pi_l \\rangle = - \\frac{1}{M} ||h||^2, \\quad ||\\nabla \\log \\pi_w||^2 = ||\\nabla \\log \\pi_l||^2 = \\frac{M-1}{M} ||h||^2,$$ \nwith h being the hidden state of the last token in prompt x. Thus, both parts of Condition 1 hold, resulting in \\(\\log \\pi_w\\) increases and \\(\\log \\pi_l\\) decreases."}, {"title": "4.2 Negative result: when the gradient condition is violated", "content": "From the previous results, we can see that the gradient inner product condition is not violated and DPO has the ideal behavior when the chosen and rejected responses differ only at the last token. To gain theoretical insights on what causes the violation of the condition, we level up our previous data setup to the following.\nFor index \\(i \\in [L]\\),\n$$\\pi_\\theta(\\cdot | x, y_w^i) = s_{\\omega, i}, \\quad \\pi_\\theta(\\cdot | x, y_l^i) = s_{l, i},$$"}, {"title": "5 Empirical Implications: Algorithmic Design and More", "content": "Using our insights from the gradient inner product conditions (Section 3) and our investigation on when such conditions may be violated (Section 4), we present two potential ways to mitigate gradient entanglement, thus allowing the chosen and rejected probability to change in different directions simultaneously."}, {"title": "5.1 Design 1: pairwise normalized gradient descent", "content": "As discussed in Section 3, to specify an increasing log-probability of the chosen response and a decreasing log- probability of the rejected response, we can set \\(\\frac{d_w}{d_l} = \\frac{||\\nabla \\log \\pi_l||}{||\\nabla \\log \\pi_w||}\\) so that (9) and (10) will hold simultaneously. This leads to the following gradient update rule:\n$$ \\nabla_\\theta l := C \\cdot \\left(\\frac{\\nabla_\\theta \\log \\pi_w}{||\\nabla_\\theta \\log \\pi_w||} - \\frac{\\nabla_\\theta \\log \\pi_l}{||\\nabla_\\theta \\log \\pi_l||}\\right), $$\nwhere C is a quantity relying on the specific preference optimization loss design. This update rule turns out to be the normalized gradient for the chosen and rejected responses respectively. For example, we can modify the gradient update for the DPO loss as:\n$$ \\nabla_\\theta l_\\text{DPO}^*(0) := -\\beta \\sigma (\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta(x, y_w)) \\left[\\frac{\\nabla_\\theta \\log \\pi_\\theta (y_w | x)}{||\\nabla_\\theta \\log \\pi_\\theta (y_w | x) ||} - \\frac{\\nabla_\\theta \\log \\pi_\\theta (y_l | x)}{||\\nabla_\\theta \\log \\pi_\\theta (y_l | x) ||}\\right], $$\nand adjust the learning rate accordingly."}, {"title": "5.2 Design 2: sparsity regularized token masking", "content": "An alternative approach to reduce gradient entanglement is by designing a fine-grained margin-based loss that only contrasts significant tokens, as suggested in Section 4.3. For example, the following loss design could be a potential good candidate for adapting the original DPO objective in this direction:\n$$l (\\theta, u_w, u_l) = \\sum_{i=1}^L I\\{u_w^i \\ge r\\} \\log \\frac{\\pi_\\theta(y_w|x, y_w^i)}{\\pi_\\text{ref}(x, y_w^i)} - I\\{u_l^i \\ge r\\} \\log \\frac{\\pi_\\theta(y_l|x, y_w^i)}{\\pi_\\text{ref}(y_l|x, y_w^i)} + \\eta (||I\\{u_w \\ge r\\}||_1 + ||I\\{u_m \\ge r\\}||_1),$$\nIn this paper, we touch upon a common pitfall of margin-based preference optimization methods in language alignment: it under-specifies the ideal behavior of the LM on the chosen and rejected responses individually. Due to the gradient"}, {"title": "5.3 Further Discussion", "content": "In this paper, we touch upon a common pitfall of margin-based preference optimization methods in language alignment: it under-specifies the ideal behavior of the LM on the chosen and rejected responses individually. Due to the gradient"}]}