{"title": "Correcting the Mythos of KL-Regularization:\nDirect Alignment without Overoptimization via x\u00b2-Preference Optimization", "authors": ["Audrey Huang", "Wenhao Zhan", "Tengyang Xie", "Jason D. Lee", "Wen Sun", "Akshay Krishnamurthy", "Dylan J. Foster"], "abstract": "Language model alignment methods, such as reinforcement learning from human feedback (RLHF),\nhave led to impressive advances in language model capabilities, but existing techniques are limited by\na widely observed phenomenon known as overoptimization, where the quality of the language model\nplateaus or degrades over the course of the alignment process. Overoptimization is often attributed to\noverfitting to an inaccurate reward model, and while it can be mitigated through online data collection,\nthis is infeasible in many settings. This raises a fundamental question: Do existing offline alignment\nalgorithms make the most of the data they have, or can their sample-efficiency be improved further?\nWe address this question with a new algorithm for offline alignment, x\u00b2-Preference Optimization (XPO).\nXPO is a one-line change to Direct Preference Optimization (DPO; Rafailov et al. (2023)), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite this minimal change, XPO\nimplicitly implements the principle of pessimism in the face of uncertainty via regularization with the\n$\\chi^2$-divergence which quantifies uncertainty more effectively than KL-regularization and provably allevi-\nates overoptimization, achieving sample-complexity guarantees based on single-policy concentrability\u2014the\ngold standard in offline reinforcement learning. XPO's simplicity and strong guarantees make it the first\npractical and general-purpose offline alignment algorithm that is provably robust to overoptimization.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) trained on unsupervised text data exhibit impressive and surprising capabilities\n(Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; OpenAI, 2023; Google, 2023), but can be difficult\nto control without further guidance. Reinforcement learning from human feedback (RLHF) and other alignment\nmethods have emerged as a central tool to align these models to human values and elicit desired behavior\n(Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022; Rafailov et al., 2023). This is achieved by treating\nthe language model as a policy, and using techniques from reinforcement learning to optimize for desirable\noutcomes under a (explicit or implicit) reward model learned from a dataset of human-labeled responses.\nAlignment methods like RLHF have led to significant advances in language model capabilities, particularly\nin chat domains, but existing techniques are limited by a widely observed phenomenon known as reward\noveroptimization or reward hacking (Michaud et al., 2020; Tien et al., 2022; Gao et al., 2023; Rafailov et al.,\n2024a). Since the reward model used implicitly or explicitly by these methods is an imperfect proxy for human\npreferences, the true quality of the language model can degrade as training proceeds, even as performance\nunder the reward model continues to improve. Intuitively, this occurs because the language model may drift\naway from the manifold covered by the human-labeled data used to train the reward model and end up in a\nregion where the reward model is inaccurate.\nOveroptimization is distinct from the classical concept of overfitting because it is a causal or counterfactual\nphenomenon: When the human-labeled dataset does not cover all possible alternatives, the decision maker-in"}, {"title": "1.  1 Contributions", "content": "We introduce a new algorithm for offline alignment, x\u00b2-Preference Optimization (XPO). XPO is simple and\nstraightforward to implement, requiring only a single-line change to Direct Preference Optimization (Rafailov\net al. (2023)), yet it is provably robust to overoptimization. Algorithmically, XPO only differs from DPO\nin that we replace the usual logarithmic link function in the DPO objective with a new link function that\nimplicitly implements pessimism via regularization with the $\\chi^2$-divergence a divergence that (i) plays a\nfundamental role in statistics due to its ability to quantify uncertainty (Tsybakov, 2008); and (ii) penalizes\noff-manifold behavior more effectively than KL-regularization. Statistically, we formalize robustness to\noveroptimization via a sample complexity guarantee based on single-policy concentrability-the gold standard\nin offline reinforcement learning-which we establish under minimal statistical and function approximation\nassumptions. This result implies that, in contrast to most prior work, XPO enjoys meaningful guarantees even\nwhen the reference policy has poor coverage. Summarizing:\nXPO is the first practical, general-purpose algorithm for offline alignment\nwith provable robustness to overoptimization."}, {"title": "Background", "content": "In this section, we provide necessary background. We formally introduce the problem of language model align-\nment from human feedback (offline alignment), review standard algorithms (PPO and DPO), and highlight that\nin general, these algorithms suffer from provably suboptimal sample complexity arising from overoptimization,\nnecessitating algorithmic interventions."}, {"title": "2.  1 Alignment from Human Feedback", "content": "Following prior work (e.g., Rafailov et al. (2023); Ye et al. (2024)), we adopt a contextual bandit formulation\nof the alignment problem. We formalize the language model as a policy $\\pi : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})$ which maps a context\n(prompt) $x \\in \\mathcal{A}$ to an action (response) $a \\in \\mathcal{A}$ via $a \\sim \\pi(\\cdot | x)$, and let $\\rho\\in \\Delta(\\mathcal{X})$ denote the distribution over\ncontexts/prompts."}, {"title": "Overoptimization and Insufficiency of KL-Regularization", "content": "Empirically, both classical RLHF and direct alignment methods like DPO have been observed to suffer from\noveroptimization (Gao et al., 2023; Guo et al., 2024; Rafailov et al., 2024a; Song et al., 2024), wherein model\nquality degrades during the optimization process as the learned policy drifts away from $\\pi_{\\text{ref}}$. This can be\nmitigated by online alignment techniques (Gao et al., 2024; Guo et al., 2024; Dong et al., 2024; Xie et al.,\n2024), which collect labeled preference data on-policy during training, but there are many settings where\nthis is impractical or infeasible. As we will see, the overoptimization phenomena in offline alignment methods\nis an issue of sample-inefficiency, which can be understood through the lens of coverage coefficients developed\nin the theory of offline reinforcement learning (Liu et al., 2020; Jin et al., 2021; Rashidinejad et al., 2021).\nCoverage coefficients. In the theory of offline reinforcement learning, the sample efficiency of offline\nalgorithms is typically quantified through coverage coefficients (or, concentrability coefficients), which measure\nthe quality of the data collected by the policy $\\pi_{\\text{ref}}$ (Farahmand et al., 2010; Xie and Jiang, 2020; Zanette\net al., 2021). Define the $L_\\infty$-concentrability coefficient for a policy $\\pi$ via\n$C_\\pi := \\sup_{(x,a) \\in \\mathcal{X} \\times \\mathcal{A}} \\frac{\\pi(a|x)}{\\pi_{\\text{ref}}(a|x)} .$\nFor general offline RL, algorithms based on the principle of pessimism can achieve sample complexity\nguarantees that scale with the single policy concentrability coefficient $C_{\\pi^*}$ for the optimal policy $\\pi^*$ (or more\ngenerally, any comparator policy), while non-pessimistic algorithms typically achieve guarantees based on the\nless favorable all-policy concentrability coefficient $\\max_{\\pi \\in \\Pi} C_\\pi$ (Liu et al., 2020; Jin et al., 2021; Rashidinejad\net al., 2021). Sample complexity guarantees scaling with single-policy concentrability reflect robustness\nto overoptimization, as they ensure that the algorithm has non-trivial sample complexity even if the data\ncollection policy $\\pi_{\\text{ref}}$ has poor coverage. In particular, the algorithm's performance is never much worse than\nthat of $\\pi_{\\text{ref}}$ itself. On the other hand, an algorithm with performance depending on all-policy concentrability\nis susceptible to the overoptimization phenomena and can perform poorly when $\\pi_{\\text{ref}}$ has poor coverage.\nPessimism in offline alignment. For the stylized special case of alignment with linearly parameterized\npolicies where $\\pi_{\\rho}(a | x) \\propto \\exp( \\langle \\phi(x, a), \\theta \\rangle )$ for a known feature embedding $\\phi(x, a) \\in \\mathbb{R}^d$, Zhu et al. (2023)\n(see also Zhu et al. (2024); Song et al. (2024)) present analogous findings, highlighting that PPO and DPO are\nsuboptimal with respect to dependence on the concentrability coefficient. They show that there exists an\nalgorithm (based on pessimism) which for all policies $\\pi^*$ simultaneously achieves\n$J(\\pi^*) - J(\\tilde{\\pi}) \\le \\sqrt{\\frac{\\text{poly}(C, d)}{n}} ,$\nMeanwhile, PPO and DPO must suffer sample complexity scaling with $\\max C_{\\pi}.^1$\nWhile the results above are encouraging, they are restricted to linearly parameterized policies, and cannot\nbe directly applied to large language models. Most existing theoretical algorithms for offline alignment are\nsimilar in nature, and either place restrictive assumptions on the policy class $\\Pi$ (Zhu et al., 2023; Zhan et al.,\n2023b; Li et al., 2023; Xiong et al., 2023) or are not practical to implement in a way that is faithful to theory\n(Ye et al., 2024; Ji et al., 2024).\nMost relevant to our work, a series of recent papers (Liu et al., 2024; Cen et al., 2024; Fisch et al., 2024)\npropose implementing pessimism for general policy classes $\\Pi$ by solving the so-called \"DPO+SFT\" objective\n$\\underset{\\pi \\in \\Pi}{\\text{argmax}} \\quad \\alpha. \\mathbb{E}_{\\pi_{\\text{ref}}} [\\beta \\log \\pi(a | x)] + \\frac{1}{n} \\sum_{(x,a_+,a_-) \\in D_{\\text{pref}}} \\left[ \\log \\sigma(\\beta \\log \\frac{\\pi(a_+ |x)}{\\pi_{\\text{ref}}(a_+|x)} - \\beta \\log \\frac{\\pi(a_- |x)}{\\pi_{\\text{ref}}(a_-|x)}) \\right]$\nwhich augments the DPO objective (the second term) with an additional supervised fine-tuning-like (SFT) loss\n(the first term). While this objective is simple to apply to general policy classes, the existing single-policy"}, {"title": "x\u00b2-Preference Optimization", "content": "This section presents our main algorithm, XPO. We begin by introducing $\\chi^2$-regularization as a general\nframework for mitigating overoptimization in offline alignment (Section 3.1), then derive the XPO algorithm\n(Section 3.2) and finally present our main theoretical guarantee (Section 3.3)."}, {"title": "3.  1 Framework: x\u00b2-Regularized Reward Optimization", "content": "The central algorithm design principle for our work is to (implicitly or explicitly) optimize a variant of the\nclassical RLHF objective (Eq. (2)) that replaces KL-regularization with regularization via $\\chi^2$-divergence,\ndefined for a pair of probability measures P and Q with P \u226a Q via\n$D_{\\chi^2}(P || Q) := \\frac{1}{2} \\int (\\frac{dP}{dQ} - 1)^2 dQ .$\n$\\chi^2$-divergence is a more aggressive form of regularization than KL-divergence; we have $D_{KL}(P|| Q) <\n2D_{\\chi^2}(P || Q)$, but the converse is not true in general. We consider the following $\\chi^2$-regularized RL objective:\n$J(\\pi) := \\mathbb{E}_\\pi[r^*(x, a)] - \\beta \\cdot D_{\\chi^2}(\\pi || \\pi_{\\text{ref}}).$\nMoving to a form of regularization that penalizes deviations from $\\pi_{\\text{ref}}$ more forcefully than KL-regularization\nis a natural approach to mitigating overoptimization, but an immediate concern is that this may lead to overly\nconservative algorithms. As we will show, however, $\\chi^2$-divergence is better suited to the geometry of offline\nalignment, as it has the unique property (not shared by KL-divergence) that its value quantifies the extent to\nwhich the accuracy of a reward model trained under $\\pi_{\\text{ref}}$ will transfer to a downstream policy of interest\n(Lemma E.3). This implies that the $\\chi^2$-regularized RL objective in Eq. (6) meaningfully implements a form\nof pessimism in the face of uncertainty, and by tuning the regularization parameter $\\beta > 0$, we can keep the\nlearned policy close to $\\pi_{\\text{ref}}$ in the \u201cright\u201d (uncertainty-aware) way and fully utilize the offline dataset $\\mathcal{D}_{\\text{pref}}$.\nAs such, we view optimizing $\\chi^2$-regularized rewards, i.e., $\\underset{\\pi \\in \\Pi}{\\text{argmax}} J(\\pi)$ as a general principle to guide\nalgorithm design for offline alignment (as well as offline RL more broadly), which we expect to find broader use.\nWe now turn our attention to the matter of how to optimize this objective. One natural approach, in the vein\nof classical RLHF algorithms (Christiano et al., 2017; Ouyang et al., 2022), is to estimate a reward model $\\hat{r}$\nusing maximum likelihood (Eq. (3)), and then use PPO or other policy optimization methods to solve\n$\\underset{\\pi \\in \\Pi}{\\text{argmax}} \\quad \\mathbb{E}_\\pi [\\hat{r}(x, a)] - \\beta \\cdot D_{\\chi^2}(\\pi || \\pi_{\\text{ref}}) = \\underset{\\pi \\in \\Pi}{\\text{argmax}} \\quad \\mathbb{E}_\\pi [\\hat{r}(x, a)] - \\beta \\frac{\\pi(a|x)}{\\pi_{\\text{ref}}(a|x)} .$\nWhile this indeed leads to meaningful statistical guarantees (cf. Appendix C), we adopt a simpler and more\ndirect approach inspired by DPO, which removes the need for a separate reward estimation step."}, {"title": "3.  2 The XPO Algorithm", "content": "Our main algorithm, XPO, is described in Algorithm 1. Given a preference dataset $\\mathcal{D}_{\\text{pref}}$ and user-specified\npolicy class $\\Pi$, the algorithm learns a policy $\\pi$ by solving the DPO-like optimization objective Eq. (9), which"}, {"title": "3.  3 Theoretical Guarantees", "content": "To state our main sample complexity guarantee for XPO, we begin by making standard statistical assumptions.\nLet the regularization parameter $\\beta > 0$ in XPO be fixed. We first make a realizability assumption, which states\nthat the policy class $\\Pi$ used in XPO is sufficiently expressive to represent the optimal policy $\\pi$ under mixed\n$\\chi^2$-regularization (Eq. (11)); recall that in the context of language modeling, $\\Pi$ represents a class of language\nmodels with fixed architecture and varying weights.\nAssumption 3.1 (Policy realizability). The policy class $\\Pi$ satisfies $\\pi \\in \\Pi$, where $\\pi$ is the optimal policy\nunder mixed $\\chi^2$-regularization (Eq. (11)).\nPolicy realizability is a standard assumption for sample-efficient reinforcement learning (Agarwal et al., 2019;\nLattimore and Szepesv\u00e1ri, 2020; Foster and Rakhlin, 2023), and is equivalent to reward model realizability in\nour setting via reparameterization.\nOur second assumption asserts that the \"implicit\u201d reward models induced by the policy class $\\Pi$ in XPO have\nbounded range.\nAssumption 3.2 (Bounded implicit rewards). For a parameter $V_{\\text{max}} \\ge R_{\\text{max}}$, it holds that for all $\\pi \\in \\Pi$,\n$x \\in \\mathcal{X}$, and $a, b \\in \\mathcal{A}$,\n$\\beta \\varphi(\\frac{\\pi(a | x)}{\\pi_{\\text{ref}}(a|x)}) - \\beta \\varphi(\\frac{\\pi(b|x)}{\\pi_{\\text{ref}}(b|x)}) < V_{\\text{max}}.$\nAssumption 3.2 generalizes analogous assumptions made in the analysis of DPO-like algorithms in prior\nwork (Rosset et al., 2024; Xie et al., 2024), and our guarantees scale polynomially with this parameter; see\nSection 4.4 for a detailed comparison. We emphasize that in practice, $V_{\\text{max}}$ can be measured and directly\ncontrolled (e.g., via clipping).\nFinally, to quantify the coverage of the offline preference dataset $\\mathcal{D}_{\\text{pref}}$ generated by $\\pi_{\\text{ref}}$, we make use of\n$L_1$-concentrability (Farahmand et al., 2010; Xie and Jiang, 2020; Zanette et al., 2021), a sharper notion of\ncoverage than the $L_\\infty$-concentrability coefficient discussed in the prequel."}, {"title": "Understanding XPO: The Bias-Overoptimization Tradeoff", "content": "Having derived XPO from the mixed $\\chi^2$-regularized RLHF objective and analyzed its performance, we now\ntake a moment to better understand the statistical properties of the policies the algorithm learns. We focus\non the tradeoff between overoptimization and bias (i.e., underoptimization) achieved by the regularization\nparameter $\\beta > 0$, highlighting through examples how this leads to statistical benefits over naive alignment\nmethods like DPO."}, {"title": "4.  1 Properties of Optimal Policy under Mixed x\u00b2-Regularization", "content": "We begin by deriving a (nearly) closed form solution for the optimal mixed $\\chi^2$-regularized policy in Eq. (11);\nrecall that we expect XPO to converge to this policy in the limit of infinite data.\nWe first observe that the link function $\\phi(\\cdot)$ is strictly increasing over $\\mathbb{R}_+$, and its inverse is given by\n$\\phi^{-1}(z) = W_0(\\exp(z)); here, W_0(y) denotes the Lambert W-function (Corless et al., 1996), defined for\n$y > - e^{-1}$ as the inverse of the function $x \\rightarrow xe^x$. Consequently, for any x, the optimal policy under mixed\n$\\chi^2$-regularization satisfies\n$\\pi(a | x) = \\pi_{\\text{ref}}(a | x) \\cdot W_0(\\exp(\\beta^{-1}(\\hat{r}^*(x, a) - Z_{\\beta,\\hat{r}^*}(x)))),$\nwhere $Z_{\\beta,\\hat{r}^*}(x)$ is chosen such that $\\sum_a \\pi(a | x) = 1$. We can better understand how this policy behaves\nusing the following simple upper and lower bounds on the inverse link function $\\phi^{-1}(z) = W_0(\\exp(z)).$"}, {"title": "4.  2 The Bias-Overoptimization Tradeoff", "content": "We are now well equipped to understand how XPO modulates the tradeoff between overoptimization and bias\nusing the regularization parameter $\\beta$, and how this tradeoff compares to vanilla DPO. To showcase this, we\ntake a reward modeling perspective, and consider the setting in which the policy class $\\Pi$ is induced by a\ngiven reward model class $\\mathcal{R}$, similar to Example 3.1.\nSuppose we start with a reward model class $\\mathcal{R} \\subset (\\mathcal{X} \\times \\mathcal{A} \\rightarrow [0, R_{\\text{max}}])$ such that $r^* \\in \\mathcal{R}$. If we use the induced\npolicy class\n$\\Pi_{\\text{DPO},\\beta} := {\\pi(a | x) = \\pi_{\\text{ref}}(a | x) \\cdot \\exp(\\beta^{-1}(\\hat{r}(x, a) - Z_{\\beta,\\hat{r};KL}(x))) | \\hat{r} \\in \\mathcal{R}},$\nthen DPO can be interpreted as fitting a reward model $\\hat{r}$ using maximum likelihood (Eq. (3)) and then\noutputting the policy $\\hat{\\pi}_{\\text{PPO}}(a | x) = \\pi_{\\text{ref}}(a | x)\\cdot \\exp(\\beta^{-1}(\\hat{r}(x, a) - Z_{\\beta,\\hat{r};KL}(x))).$ Meanwhile, if we use the\ninduced policy class\n$\\Pi_{\\text{XPO},\\beta} := {\\pi(a | x) = \\pi_{\\text{ref}}(a | x) \\cdot \\varphi^{-1}(\\beta^{-1}(\\hat{r}(x, a) - Z_{\\beta,\\hat{r}}(x))) | \\hat{r} \\in \\mathcal{R}},$"}, {"title": "4.  3 An Illustrative Example", "content": "We now give a concrete example in which XPO allows the user to tune $\\beta$ to achieve tight statistical rates,\nyet no choice of $\\beta$ for DPO leads to comparable performance (effectively, any choice of $\\beta$ is either susceptible\nto overoptimization, or has unacceptably high bias). This illustrates the favorable tradeoff between bias and\noveroptimization achieved by XPO.\nLet $n \\in \\mathbb{N}$ with $n \\ge 2$ be given. We consider a problem instance with $\\mathcal{X} = {\\emptyset}$ and $\\mathcal{A} = {a_0, a_1,a_2, a_3}$. We\ndefine $\\pi_{\\text{ref}}$ via\n$\\pi_{\\text{ref}}(a_0) = \\frac{n-2}{n}, \\pi_{\\text{ref}}(a_1) = \\pi_{\\text{ref}}(a_2) = \\frac{1}{2n}, \\text{and} \\pi_{\\text{ref}}(a_3) = \\frac{2}{n}.$\nWe define a reward class with two reward functions $\\mathcal{R} := {r_1,r_2}$ as follows. For $i \\in {1,2}$:\n$r_i(a_0) = 1/2, r_i(a_i) = 1, r_i(a_j) = 0, \\forall j \\neq i.$\nLet $\\beta > 0$ be fixed. To compare XPO and DPO, we consider their behavior when invoked with the induced policy\nclasses $\\Pi_{\\text{XPO},\\beta}$ and $\\Pi_{\\text{DPO},\\beta}$ defined above. Recall that with this choice, the two algorithms can be interpreted as"}, {"title": "Nontriviality and Role of Vmax Parameter", "content": "To close this section, we discuss the role of the $V_{\\text{max}}$ parameter (Assumption 3.2) used in the analysis of XPO\n(Theorem 3.1) in depth, motivating it from the perspective of the induced policy class $\\Pi_{\\text{XPO},\\beta}$ from Section 4.2.\nAssumption 3.2 effectively implies that all policies $\\pi \\in \\Pi$ satisfy $|| \\frac{\\pi}{\\pi_{\\text{ref}}} ||_\\infty \\le V_{\\text{max}}$; in other words, the policy\nclass we use in XPO satisfies all-policy $L_\\infty$-concentrability with $\\max_{\\pi \\in \\Pi} C_\\pi \\le V_{\\text{max}}$. This might seem to\ntrivialize the offline alignment problem at first glance, since it would suffice to prove a generalization guarantee\nbased on all-policy concentrability, and then plug this bound in. We will show that this is not the case, and\nthat this is actually an intrinsic feature of $\\chi^2$-regularization.\nIn more detail, recall that for XPO, we require the realizability assumption that $\\pi \\in \\Pi$ (Assumption 3.1), where $\\pi$\nis the optimal mixed $\\chi^2$-regularized policy that satisfies $\\hat{r}^*(x, a) = \\beta \\varphi( \\frac{\\pi(a | x)}{\\pi_{\\text{ref}}(a|x)}) + Z_{\\beta,\\hat{r}^*}(x)$. This policy,\nvia Proposition 4.2, satisfies $|| \\frac{\\pi}{\\pi_{\\text{ref}}} ||_\\infty \\le \\exp(R_{\\text{max}})$, so from a statistical perspective, we can take Assumption 3.2 to\nhold without loss of generality by removing any policy that violates this bound. In addition, as highlighted\nby Example 3.1, if we begin from a class of bounded reward models $\\mathcal{R}$ with $r^* \\in \\mathcal{R}$, Assumption 3.2 holds\nwith $V_{\\text{max}} \\le R_{\\text{max}}$ for the induced class $\\Pi_{\\text{XPO},\\beta}$ defined in Eq. (18), even though knowledge of such a reward\nmodel class is a mild statistical assumption that clearly does not trivialize the learning problem.\nOn the other hand, for DPO, a minimal assumption is that $\\pi_{\\beta;KL} \\in \\Pi$ (Xie et al., 2024), where $\\pi_{\\beta;KL}$ is\nthe optimal KL-regularized policy that satisfies $\\hat{r}^*(x, a) = \\beta \\log \\frac{\\pi_{\\beta;KL}(a | x)}{\\pi_{\\text{ref}}(a|x)} + Z_{\\beta,\\hat{r}^*;KL}(x)$. Unlike the opti-\nmal mixed $\\chi^2$-regularized policy, $\\frac{\\pi(a)}{\\pi_{\\text{ref}}(a)} $has no exponential upper bound. This means that it is impossible to\nfind a policy class that simultaneously (1) realizes $\\hat{\\pi}_{\\beta;KL}$, and (2) satisfies all-policy concentrability with\n$\\max_{\\pi \\in \\Pi} C_\\pi << \\exp(\\frac{R_{\\text{max}}}{\\beta})$.\nAs the bias of DPO is unacceptably large unless $\\beta = \\text{poly}(1/n)$ (the \"small-$\beta\"\nregime), this leads to vacuous guarantees.\nIn view of these observations, our analysis of XPO can be interpreted as (implicitly) showing that for any\nbounded reward class $\\mathcal{R}$, there exists a policy class $\\Pi$ (precisely, the class $\\Pi_{\\text{XPO},\\beta}$ defined in Eq. (18)) such\nthat the following properties hold:"}, {"title": "Analysis of XPO: Proof Sketch for Theorem 3.1", "content": "In this section, we sketch the proof of the main guarantee for XPO, Theorem 3.1, with the full proof deferred\nto Appendix E. A central object in the proof is the implicit reward model induced by the XPO policy $\\pi$, which\nwe define via\n$\\hat{r}(x, a) := \\beta \\varphi\\left( \\frac{\\pi(a | x)}{\\pi_{\\text{ref}}(a|x)} \\right).$\nAs we will show, this reward model is a natural bridge between XPO and the corresponding mixed $\\chi^2$-regularized\nRLHF objective in Section 3.1, and allows us to view XPO from a reward-based perspective. In particular,\nnote that if we analogously define an induced reward model class $\\mathcal{R}_\\pi:= {\\hat{r}(x, a) = \\beta \\varphi(\\frac{\\pi(a | x)}{\\pi_{\\text{ref}}(a|x)}) : \\pi \\in \\Pi}$,\nthen Line 2 of XPO can be viewed as performing maximum likelihood estimation over this class (in the sense\nof Eq. (3)) under the Bradley-Terry model. Under Assumption 3.1, $\\mathcal{R}_\\pi$ realizes the true reward function\nr up to an action-independent shift. As a result, if we define $\\Delta'(x,a,b) := \\hat{r}(x, a) - \\hat{r}(x,b)$, then using a\nfairly standard generalization bound for maximum likelihood estimation (e.g., Wong and Shen (1995); Zhang\n(2006); de Geer (2000); see Lemma E.1), we can show that\n$\\\\mathcal{E}_{\\text{stat}} := \\mathbb{E}_{ \\substack{ (x,a,b) \\sim \\text{\\tiny Tref} } } [ [\\Delta(x, a, b) - \\Delta^*(x, a, b)]^2] \\le O(\\frac{ V_{max}e^{2R_{max}} \\log(\\Pi|/\\delta)}{n}).$\nIn other words, the estimated reward model $\\hat{r}$ is accurate under the action distribution induced by $\\pi_{\\text{ref}}$.\nHowever, $\\hat{r}$ may still be inaccurate for policies that select different actions from $\\pi_{\\text{ref}}$, raising concerns of\noveroptimization. To address this issue, we use the following lemma, which shows that $\\chi^2$-divergence bounds\nthe extent to which the accuracy of a reward model trained under $\\pi_{\\text{ref}}$ will transfer to a downstream policy\n$\\pi$ of interest; this will motivate our use of $\\chi^2$-regularization.\nLemma 5.1 (Informal version of Lemma E.3). For any policy $\\pi : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})$, it holds that\n$\\mathbb{E}_{ z \\sim \\rho, a \\sim \\pi(\\cdot|x), b \\sim \\pi_{\\text{ref}}(\\cdot|x) } [|\\Delta(x, a, b) - \\Delta^*(x, a, b)|] \\lesssim \\sqrt{(1 + D_{\\chi^2}(\\pi || \\pi_{\\text{ref}}))} \\cdot \\mathcal{E}_{\\text{stat}}$\nGoing forward, let us abbreviate $\\mathbb{E}_{\\pi, \\pi_{\\text{ref}}} [\\cdot] = \\mathbb{E}_{x \\sim \\rho, a \\sim \\pi(\\cdot|x), b \\sim \\pi_{\\text{ref}}(\\cdot|x)}[\\cdot]$. Let $\\pi^*$ be an arbitrary policy. Noting\nthat $C_{\\pi} = 1 + 2D_{\\chi^2}(\\pi || \\pi_{\\text{ref}})$ and that\n$J(\\pi^*) - J(\\tilde{\\pi}) \\le \\mathbb{E}_{\\pi^*, \\pi_{\\text{ref}}} [|\\Delta(x, a, b) - \\Delta^*(x, a, b)|] + \\mathbb{E}_{\\tilde{\\pi}, \\pi_{\\text{ref}}} [|\\Delta(x, a, b) - \\Delta^*(x, a, b)|],$\nit follows immediately from Lemma 5.1 that XPO obtains a crude guarantee scaling with all-policy con-\ncentrability, i.e. $J(\\pi^*) - J(\\tilde{\\pi}) \\le \\sqrt{(C_{\\pi^*} + C)} \\mathcal{E}_{\\text{stat}} \\le \\sqrt{(C_{\\pi^*} + \\max_{\\pi \\in \\Pi} C_{\\pi})}\\mathcal{E}_{\\text{stat}}$. This inequality is tight\nfor non-pessimistic algorithms like DPO, which reflects their sensitivity to overoptimization. To obtain the\nimproved guarantee for XPO in Theorem 3.1, which scales only with single-policy concentrability $C_{\\pi^*}$, the crux\nof the remaining proof will be to show that XPO implicitly implements pessimism via mixed $\\chi^2$-regularization.\nFor this, we appeal to the following central technical lemma, which we expect to find broader use.\nLemma 5.2 (Informal version of Lemma E.2). Let f be a convex function with dom(f) = $\\mathbb{R}_+$ that is\ndifferentiable over its domain. Given any parameter $\\beta > 0$ and policy $\\bar{\\pi} : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})$ with $\\pi(a | x) \\in \\text{dom}(f')$\nfor all $x, a$, define the reward model $\\hat{r}(x, a) = \\beta f'(\\frac{\\pi(a|x)}{\\pi_{\\text{ref}}(a | x)})$. Then\n$\\bar{\\pi} \\in \\underset{\\pi}{\\text{argmax}} \\quad \\mathbb{E}_\\pi[\\hat{r}(x,a)] - \\beta \\cdot D_f(\\pi || \\pi_{\\text{ref}}).$"}, {"title": "Discussion", "content": "Our work gives the first practical"}, {"title": "Correcting the Mythos of KL-Regularization:\nDirect Alignment without Overoptimization via x\u00b2-Preference Optimization", "authors": ["Audrey Huang", "Wenhao Zhan", "Tengyang Xie", "Jason D. Lee", "Wen Sun", "Akshay Krishnamurthy", "Dylan J. Foster"], "abstract": "Language model alignment methods, such as reinforcement learning from human feedback (RLHF),\nhave led to impressive advances in language model capabilities, but existing techniques are limited by\na widely observed phenomenon known as overoptimization, where the quality of the language model\nplateaus or degrades over the course of the alignment process. Overoptimization is often attributed to\noverfitting to an inaccurate reward model, and while it can be mitigated through online data collection,\nthis is infeasible in many settings. This raises a fundamental question: Do existing offline alignment\nalgorithms make the most of the data they have, or can their sample-efficiency be improved further?\nWe address this question with a new algorithm for offline alignment, x\u00b2-Preference Optimization (XPO).\nXPO is a one-line change to Direct Preference Optimization (DPO; Rafailov et al. (2023)), which only\ninvolves modifying the logarithmic link function in the DPO objective. Despite this minimal change, XPO\nimplicitly implements the principle of pessimism in the face of uncertainty via regularization with the\n$\\chi^2$-divergence which quantifies uncertainty more effectively than KL-regularization and provably allevi-\nates overoptimization, achieving sample-complexity guarantees based on single-policy concentrability\u2014the\ngold standard in offline reinforcement learning. XPO's simplicity and strong guarantees make it the first\npractical and general-purpose offline alignment algorithm that is provably robust to overoptimization.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) trained on unsupervised text data exhibit impressive and surprising capabilities\n(Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; OpenAI, 2023; Google, 2023), but can be difficult\nto control without further guidance. Reinforcement learning from human feedback (RLHF) and other alignment\nmethods have emerged as a central tool to align these models to human values and elicit desired behavior\n(Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022; Rafailov et al., 2023). This is achieved by treating\nthe language model as a policy, and using techniques from reinforcement learning to optimize for desirable\noutcomes under a (explicit or implicit) reward model learned from a dataset of human-labeled responses.\nAlignment methods like RLHF have led to significant advances in language model capabilities, particularly\nin chat domains, but existing techniques are limited by a widely observed phenomenon known as reward\noveroptimization or reward hacking (Michaud et al., 2020; Tien et al., 2022; Gao et al., 2023; Rafailov et al.,\n2024a). Since the reward model used implicitly or explicitly by these methods is an imperfect proxy for human\npreferences, the true quality of the language model can degrade as training proceeds, even as performance\nunder the reward model continues to improve. Intuitively, this occurs because the language model may drift\naway from the manifold covered by the human-labeled data used to train the reward model and end up in a\nregion where the reward model is inaccurate.\nOveroptimization is distinct from the classical concept of overfitting because it is a causal or counterfactual\nphenomenon: When the human-labeled dataset does not cover all possible alternatives, the decision maker-in"}, {"title": "1.  1 Contributions", "content": "We introduce a new algorithm for offline alignment, x\u00b2-Preference Optimization (XPO). XPO is simple and\nstraightforward to implement, requiring only a single-line change to Direct Preference Optimization (Rafailov\net al. (2023)), yet it is provably robust to overoptimization. Algorithmically, XPO only differs from DPO\nin that we replace the usual logarithmic link function in the DPO objective with a new link function that\nimplicitly implements pessimism via regularization with the $\\chi^2$-divergence a divergence that (i) plays a\nfundamental role in statistics due to its ability to quantify uncertainty (Tsybakov, 2008); and (ii) penalizes\noff-manifold behavior more effectively than KL-regularization. Statistically, we formalize robustness to\noveroptimization via a sample complexity guarantee based on single-policy concentrability-the gold standard\nin offline reinforcement learning-which we establish under minimal statistical and function approximation\nassumptions. This result implies that, in contrast to most prior work, XPO enjoys meaningful guarantees even\nwhen the reference policy has poor coverage. Summarizing:\nXPO is the first practical, general-purpose algorithm for offline alignment\nwith provable robustness to overoptimization."}, {"title": "Background", "content": "In this section, we provide necessary background. We formally introduce the problem of language model align-\nment from human feedback (offline alignment), review standard algorithms (PPO and DPO), and highlight that\nin general, these algorithms suffer from provably suboptimal sample complexity arising from overoptimization,\nnecessitating algorithmic interventions."}, {"title": "2.  1 Alignment from Human Feedback", "content": "Following prior work (e.g., Rafailov et al. (2023); Ye et al. (2024)), we adopt a contextual bandit formulation\nof the alignment problem. We formalize the language model as a policy $\\pi : \\mathcal{X} \\rightarrow \\Delta(\\mathcal{A})$ which maps a context\n(prompt) $x \\in \\mathcal{A}$ to an action (response) $a \\in \\mathcal{A}$ via $a \\sim \\pi(\\cdot | x)$, and let $\\rho\\in \\Delta(\\mathcal{X})$ denote the distribution over\ncontexts/prompts."}, {"title": "Overoptimization and Insufficiency of KL-Regularization", "content": "Empirically, both classical RLHF and direct alignment methods like DPO have been observed to suffer from\noveroptimization (Gao et al., 2023; Guo et al., 2024; Rafailov et al., 2024a; Song et al., 2024), wherein model\nquality degrades during the optimization process as the learned policy drifts away from $\\pi_{\\text{ref}}$. This can be\nmitigated by online alignment techniques (Gao et al., 2024; Guo et al., 2024; Dong et al., 2024; Xie et al.,\n2024), which collect labeled preference data on-policy during training, but there are many settings where\nthis is impractical or infeasible. As we will see, the overoptimization phenomena in offline alignment methods\nis an issue of sample-inefficiency, which can be understood through the lens of coverage coefficients developed\nin the theory of offline reinforcement learning (Liu et al., 2020; Jin et al., 2021; Rashidinejad et al., 2021).\nCoverage coefficients. In the theory of offline reinforcement learning, the sample efficiency of offline\nalgorithms is typically quantified through coverage coefficients (or, concentrability coefficients), which measure\nthe quality of the data collected by the policy $\\pi_{\\text{ref}}$ (Farahmand et al., 2010; Xie and Jiang, 2020; Zanette\net al., 2021). Define the $L_\\infty$-concentrability coefficient for a policy $\\pi$ via\n$C_\\pi := \\sup_{(x,a) \\in \\mathcal{X} \\times \\mathcal{A}} \\frac{\\pi(a|x)}{\\pi_{\\text{ref}}(a|x)} .$\nFor general offline RL, algorithms based on the principle of pessimism can achieve sample complexity\nguarantees that scale with the single policy concentrability coefficient $C_{\\pi^*}$ for the optimal policy $\\pi^*$ (or more\ngenerally, any comparator policy), while non-pessimistic algorithms typically achieve guarantees based on the\nless favorable all-policy concentrability coefficient $\\max_{\\pi \\in \\Pi} C_\\pi$ (Liu et al., 2020; Jin et al., 2021; Rashidinejad\net al., 2021). Sample complexity guarantees scaling with single-policy concentrability reflect robustness\nto overoptimization, as they ensure that the algorithm has non-trivial sample complexity even if the data\ncollection policy $\\pi_{\\text{ref}}$ has poor coverage. In particular, the algorithm's performance is never much worse than\nthat of $\\pi_{\\text{ref}}$ itself. On the other hand, an algorithm with performance depending on all-policy concentrability\nis susceptible to the overoptimization phenomena and can perform poorly when $\\pi_{\\text{ref}}$ has poor coverage.\nPessimism in offline alignment. For the stylized special case of alignment with linearly parameterized\npolicies where $\\pi_{\\rho}(a | x) \\propto \\exp( \\langle \\phi(x, a), \\theta \\rangle )$ for a known feature embedding $\\phi(x, a) \\in \\mathbb{R}^d$, Zhu et al. (2023)\n(see also Zhu et al. (2024); Song et al. (2024)) present analogous findings, highlighting that PPO and DPO are\nsuboptimal with respect to dependence on the concentrability coefficient. They show that there exists an\nalgorithm (based on pessimism) which for all policies $\\pi^*$ simultaneously achieves\n$J(\\pi^*) - J(\\tilde{\\pi}) \\le \\sqrt{\\frac{\\text{poly}(C, d)}{n}} ,$\nMeanwhile, PPO and DPO must suffer sample complexity scaling with $\\max C_{\\pi}.^1$\nWhile the results above are encouraging, they are restricted to linearly parameterized policies, and cannot\nbe directly applied to large language models. Most existing theoretical algorithms for offline alignment are\nsimilar in nature, and either place restrictive assumptions on the policy class $\\Pi$ (Zhu et al., 2023; Zhan et al.,\n2023b; Li et al., 2023; Xiong et al., 2023) or are not practical to implement in a way that is faithful to theory\n(Ye et al., 2024; Ji et al., 2024).\nMost relevant to our work, a series of recent papers (Liu et al., 2024; Cen et al., 2024; Fisch et al., 2024)\npropose implementing pessimism for general policy classes $\\Pi$ by solving the so-called \"DPO+SFT\" objective\n$\\underset{\\pi \\in \\Pi}{\\text{argmax}} \\quad \\alpha. \\mathbb{E}_{\\pi_{\\text{ref}}} [\\beta \\log \\pi(a | x)] + \\frac{1}{n} \\sum_{(x,a_+,a_-) \\in D_{\\text{pref}}} \\left[ \\log \\sigma(\\beta \\log \\frac{\\pi(a_+ |x)}{\\pi_{\\text{ref}}(a_+|x)} - \\beta \\log \\frac{\\pi(a_- |x)}{\\pi_{\\text{ref}}(a_-|x)}) \\right]$\nwhich augments the DPO objective (the second term) with an additional supervised fine-tuning-like (SFT) loss\n(the first term). While this objective is simple to apply to general policy classes, the existing single-policy"}, {"title": "x\u00b2-Preference Optimization", "content": "This section presents our main algorithm, XPO. We begin by introducing $\\chi^2$-regularization as a general\nframework for mitigating overoptimization in offline alignment (Section 3.1), then derive the XPO algorithm\n(Section 3.2) and finally present our main theoretical guarantee (Section 3.3)."}, {"title": "3.  1 Framework: x\u00b2-Regularized Reward Optimization", "content": "The central algorithm design principle for our work is to (implicitly or explicitly) optimize a variant of the\nclassical RLHF objective (Eq. (2)) that replaces KL-regularization with regularization via $\\chi^2$-divergence,\ndefined for a pair of probability measures P and Q with P \u226a Q via\n$D_{\\chi^2}(P || Q) := \\frac{1}{2} \\int (\\frac{dP}{dQ} - 1)^2 dQ .$\n$\\chi^2$-divergence is a more aggressive form of regularization than KL-divergence; we have $D_{KL}(P|| Q) <\n2D_{\\chi^2}(P || Q)$, but the converse is not true in general. We consider the following $\\chi^2$-regularized RL objective:\n$J(\\pi) := \\mathbb{E}_\\pi[r^*(x, a)] - \\beta \\cdot D_{\\chi^2}(\\pi || \\pi_{\\text{ref}}).$\nMoving to a form of regularization that penalizes deviations from $\\pi_{\\text{ref}}$ more forcefully than KL-regularization\nis a natural approach to mitigating overoptimization, but an immediate concern is that this may lead to overly\nconservative algorithms. As we will show, however, $\\chi^2$-divergence is better suited to the geometry of offline\nalignment, as it has the unique property (not shared by KL-divergence) that its value quantifies the extent to\nwhich the accuracy of a reward model trained under $\\pi_{\\text{ref}}$ will transfer to a downstream policy of interest\n(Lemma E.3). This implies that the $\\chi^2$-regularized RL objective in Eq. (6) meaningfully implements a form\nof pessimism in the face of uncertainty, and by tuning the regularization parameter $\\beta > 0$, we can keep the\nlearned policy close to $\\pi_{\\text{ref}}$ in the \u201cright\u201d (uncertainty-aware) way and fully utilize the offline dataset $\\mathcal{D}_{\\text{pref}}$.\nAs such, we view optimizing $\\chi^2$-regularized rewards, i.e., $\\underset{\\pi \\in \\Pi}{\\text{argmax}} J(\\pi)$ as a general principle to guide\nalgorithm design for offline alignment (as well as offline RL more broadly), which we expect to find broader use.\nWe now turn our attention to the matter of how to optimize this objective. One natural approach, in the vein\nof classical RLHF algorithms (Christiano et al., 2017; Ouyang et al., 2022), is to estimate a reward model $\\hat{r}$\nusing maximum likelihood (Eq. (3)), and then use PPO or other policy optimization methods to solve\n$\\underset{\\pi \\in \\Pi}{\\text{argmax}} \\quad \\mathbb{E}_\\pi [\\hat{r}(x, a)] - \\beta \\cdot D_{\\chi^2}(\\pi || \\pi_{\\text{ref}}) = \\underset{\\pi \\in \\Pi}{\\text{argmax}} \\quad \\mathbb{E}_\\pi [\\hat{r}(x, a)] - \\beta \\frac{\\pi(a|x)}{\\pi_{\\text{ref}}(a|x)} .$\nWhile this indeed leads to meaningful statistical guarantees (cf. Appendix C), we adopt a simpler and more\ndirect approach inspired by DPO, which removes the need for a separate reward estimation step."}, {"title": "3.  2 The XPO Algorithm", "content": "Our main algorithm, XPO, is described in Algorithm 1. Given a preference dataset $\\mathcal{D}_{\\text{pref}}$ and user-specified\npolicy class $\\Pi$, the algorithm learns a policy $\\pi$ by solving the DPO-like optimization objective Eq. (9), which"}, {"title": "3.  3 Theoretical Guarantees", "content": "To state our main sample complexity guarantee for XPO, we begin by making standard statistical assumptions.\nLet the regularization parameter $\\beta > 0$ in XPO be fixed. We first make a realizability assumption, which states\nthat the policy class $\\Pi$ used in XPO is sufficiently expressive to represent the optimal policy $\\pi$ under mixed\n$\\chi^2$-regularization (Eq. (11)); recall that in the context of language modeling, $\\Pi$ represents a class of language\nmodels with fixed architecture and varying weights.\nAssumption 3.1 (Policy realizability). The policy class $\\Pi$ satisfies $\\pi \\in \\Pi$, where $\\pi$ is the optimal policy\nunder mixed $\\chi^2$-regularization (Eq. (11)).\nPolicy realizability is a standard assumption for sample-efficient reinforcement learning (Agarwal et al., 2019;\nLattimore and Szepesv\u00e1ri, 2020; Foster and Rakhlin, 2023), and is equivalent to reward model realizability in\nour setting via reparameterization.\nOur second assumption asserts that the \"implicit\u201d reward models induced by the policy class $\\Pi$ in XPO have\nbounded range.\nAssumption 3.2 (Bounded implicit rewards). For a parameter $V_{\\text{max}} \\ge R_{\\text{max}}$, it holds that for all $\\pi \\in \\Pi$,\n$x \\in \\mathcal{X}$, and $a, b \\in \\mathcal{A}$,\n$\\beta \\varphi(\\frac{\\pi(a | x)}{\\pi_{\\text{ref}}(a|x)}) - \\beta \\varphi(\\frac{\\pi(b|x)}{\\pi_{\\text{ref}}(b|x)}) < V_{\\text{max}}.$\nAssumption 3.2 generalizes analogous assumptions made in the analysis of DPO-like algorithms in prior\nwork (Rosset et al., 2024; Xie et al., 2024), and our guarantees scale polynomially with this parameter; see\nSection 4.4 for a detailed comparison. We emphasize that in practice, $V_{\\text{max}}$ can be measured and directly\ncontrolled (e.g., via clipping).\nFinally, to quantify the coverage of the offline preference dataset $\\mathcal{D}_{\\text{pref}}$ generated by $\\pi_{\\text{ref}}$, we make use of\n$L_1$-concentrability (Farahmand et al., 2010; Xie and Jiang, 2020; Zanette et al., 2021), a sharper notion of\ncoverage than the $L_\\infty$-concentrability coefficient discussed in the prequel."}, {"title": "Understanding XPO: The Bias-Overoptimization Tradeoff", "content": "Having derived XPO from the mixed $\\chi^2$-regularized RLHF objective and analyzed its performance, we now\ntake a moment to better understand the statistical properties of the policies the algorithm learns. We focus\non the tradeoff between overoptimization and bias (i.e., underoptimization) achieved by the regularization\nparameter $\\beta > 0$, highlighting through examples how this leads to statistical benefits over naive alignment\nmethods like DPO."}, {"title": "4.  1 Properties of Optimal Policy under Mixed x\u00b2-Regularization", "content": "We begin by deriving a (nearly) closed form solution for the optimal mixed $\\chi^2$-regularized policy in Eq. (11);\nrecall that we expect XPO to converge to this policy in the limit of infinite data.\nWe first observe that the link function $\\phi(\\cdot)$ is strictly increasing over $\\mathbb{R}_+$, and its inverse is given by\n$\\phi^{-1}(z) = W_0(\\exp(z)); here, W_0(y) denotes the Lambert W-function (Corless et al., 1996), defined for\n$y > - e^{-1}$ as the inverse of the function $x \\rightarrow xe^x$. Consequently, for any x, the optimal policy under mixed\n$\\chi^2$-regularization satisfies\n$\\pi(a | x) = \\pi_{\\text{ref}}(a | x) \\cdot W_0(\\exp(\\beta^{-1}(\\hat{r}^*(x, a) - Z_{\\beta,\\hat{r}^*}(x)))),$\nwhere $Z_{\\beta,\\hat{r}^*}(x)$ is chosen such that $\\sum_a \\pi(a | x) = 1$. We can better understand how this policy behaves\nusing the following simple upper and lower bounds on the inverse link function $\\phi^{-1}(z) = W_0(\\exp(z)).$"}, {"title": "4.  2 The Bias-Overoptimization Tradeoff", "content": "We are now well equipped to understand how XPO modulates the tradeoff between overoptimization and bias\nusing the regularization parameter $\\beta$, and how this tradeoff compares to vanilla DPO. To showcase this, we\ntake a reward modeling perspective, and consider the setting in which the policy class $\\Pi$ is induced by a\ngiven reward model class $\\mathcal{R}$, similar to Example 3.1.\nSuppose we start with a reward model class $\\mathcal{R} \\subset (\\mathcal{X} \\times \\mathcal{A} \\rightarrow [0, R_{\\text{max}}])$ such that $r^* \\in \\mathcal{R}$. If we use the induced\npolicy class\n$\\Pi_{\\text{DPO},\\beta} := {\\pi(a | x) = \\pi_{\\text{ref}}(a | x) \\cdot \\exp(\\beta^{-1}(\\hat{r}(x, a) - Z_{\\beta,\\hat{r};KL}(x))) | \\hat{r} \\in \\mathcal{R}},$\nthen DPO can be interpreted as fitting a reward model $\\hat{r}$ using maximum likelihood (Eq. (3)) and then\noutputting the policy $\\hat{\\pi}_{\\text{PPO}}(a | x) = \\pi_{\\text{ref}}(a | x)\\cdot \\exp(\\beta^{-1}(\\hat{r}(x, a) - Z_{\\beta,\\hat{r};KL}(x))).$ Meanwhile, if we use the\ninduced policy class\n$\\Pi_{\\text{XPO},\\beta} := {\\pi(a | x) = \\pi_{\\text{ref}}(a | x) \\cdot \\varphi^{-1}(\\beta^{-1}(\\hat{r}(x, a) - Z_{\\beta,\\hat{r}}(x))) | \\hat{r} \\in \\mathcal{R}},$"}, {"title": "4.  3 An Illustrative Example", "content": "We now give a concrete example in which XPO allows the user to tune $\\beta$ to achieve tight statistical rates,\nyet no choice of $\\beta$ for DPO leads to comparable performance (effectively, any choice of $\\beta$ is either susceptible\nto overoptimization, or has unacceptably high bias). This illustrates the favorable tradeoff between bias and\noveroptimization achieved by XPO.\nLet $n \\in \\mathbb{N}$ with $n \\ge 2$ be given. We consider a problem instance with $\\mathcal{X} = {\\emptyset}$ and $\\mathcal{A} = {a_0, a_1,a_2, a_3}$. We\ndefine $\\pi_{\\text{ref}}$ via\n$\\pi_{\\text{ref}}(a_0) = \\frac{n-2}{n}, \\pi_{\\text{ref}}(a_1) = \\pi_{\\text{ref}}(a_2) = \\frac{1}{2n}, \\text{and} \\pi_{\\text{ref}}(a_3) = \\frac{2}{n}.$\nWe define a reward class with two reward functions $\\mathcal{R} := {r_1,r_2}$ as follows. For $i \\in {1,2}$:\n$r_i(a_0) = 1/2, r_i(a_i) = 1, r_i(a_j) = 0, \\forall j \\neq i.$\nLet $\\beta > 0$ be fixed. To compare XPO and DPO, we consider their behavior when invoked with the induced policy\nclasses $\\Pi_{\\text{XPO},\\beta}$ and $\\Pi_{\\text{DPO},\\beta}$ defined above. Recall that with this choice, the two algorithms can be interpreted as"}, {"title": "Nontriviality and Role of Vmax Parameter", "content": "To close this section, we discuss the role of the $V_{\\text{max}}$ parameter (Assumption 3.2) used in the analysis of XPO\n(Theorem 3.1) in depth, motivating it from the perspective of the induced policy class $\\Pi_{\\text{XPO},\\beta}$ from Section 4.2.\nAssumption 3.2 effectively implies that all policies $\\pi \\in \\Pi$ satisfy $|| \\frac{\\pi}{\\pi_{\\text{ref}"}]}]}