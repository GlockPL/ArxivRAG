{"title": "How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation", "authors": ["DEWU ZHENG", "YANLIN WANG", "ENSHENG SHI", "HONGYU ZHANG", "ZIBIN ZHENG"], "abstract": "Recently, large language models have demonstrated exceptional performance in code generation. An increasing number of AI-driven programming assistants powered by these models have been integrated into various real-world software development environments, significantly boosting developer productivity. However, existing code generation benchmarks primarily focus on general-purpose scenarios, leaving the code generation performance of LLMs for specific application domains largely unknown. In this paper, we introduce a new benchmark, MultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming tasks, covering 12 popular software development domains and 15 programming languages. Specifically, we perform in-depth research to identify these 12 application domains. Given that each domain may involve multiple technical frameworks, and that different frameworks present distinct challenges in the coding process, we categorize the commonly used frameworks and platforms within each domain. We then sample programming problems from GitHub repositories related to these subdomains. To ensure the quality of the tasks and mitigate data leakage issues, we invite annotators with relevant domain experience to rewrite the docstrings for each task in MultiCodeBench. Additionally, we build a static analysis-based dependency parsing tool to extract the dependencies in the ground truth for each task, enabling deeper performance analysis. Through extensive experiments on MultiCodeBench with eleven representative mainstream LLMs, we reveal the code generation performance of the LLMs across different application domains, providing practical insights for developers in downstream fields when selecting LLMs. Furthermore, we analyze the reasons behind the models' failures in completing software application development tasks, offering guidance for model developers to enhance domain-specific code generation capabilities. Our replication package, including the benchmark and the evaluation results, is available at https://github.com/DeepSoftwareAnalytics/MultiCodeBench.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks and are being rapidly applied in real-world development scenarios [20, 21, 30, 33, 51, 60, 64, 77, 78]. Developers only need to describe their intent in natural language (NL), then LLMs can efficiently generate accurate code [17, 37, 57, 62, 63]. Furthermore, to further enhance the performances of\nAuthors' addresses: Dewu Zheng, Sun Yat-sen University, China; Yanlin Wang, Sun Yat-sen University, China, wangylin36@\nmail.sysu.edu.cn; Ensheng Shi, Xi'an Jiaotong University, China, s1530129650@stu.xjtu.edu.cn; Hongyu Zhang, Chongqing\nUniversity, China, hongyujohn@gmail.com; Zibin Zheng, Sun Yat-sen University, China, zhzibin@mail.sysu.edu.cn."}, {"title": "2 BACKGROUND", "content": "2.1 Large Language Models for Code\nWith the remarkable success of LLMs in natural language processing tasks [6, 11, 12, 28, 34, 35,\n68, 74], LLMs for Code have rapidly emerged [2, 14, 25, 40, 49, 76], including the CodeLLaMa,\nStarCoder, and DeepSeekCoder series. Typically, these code LLMs are trained on vast amounts of"}, {"title": "2.2 Code Generation Benchmarks", "content": "To evaluate the code generation capabilities of LLMs, researchers have introduced numerous bench- marks tailored to code generation tasks [4, 5, 9, 13, 15, 22, 29, 75]. HumanEval [13] includes 164 manually constructed programming problems, requiring LLMs to generate Python function based on NL descriptions. It simulates real-world programming tasks, assessing LLMs' ability to under- stand NL descriptions and generate executable code. MBPP [5] contains 974 Python programming problems, covering a range of common tasks such as list processing, string manipulation, and mathematical computations.\nAlthough these benchmarks effectively reveal the general-purpose code generation capabilities of current LLMs, they overlook the fact that these LLMs will ultimately be applied to a wide range of downstream domains, and performance in general-purpose code generation tasks does not necessarily translate to performance in domain-specific development scenarios. Most existing code generation benchmarks primarily assess LLMs' ability to generate general-purpose functions without targeting specific domains, which limits their practicality for developers working in specialized software application domains.\nFortunately, recent efforts have started to shift the focus of code generation benchmarks to- ward downstream domains [26, 36, 79, 81]. BioCoder [59] specifically assesses LLMs' ability to generate code for bioinformatics tasks. BigCodeBench [81] offers tasks involving multiple func- tion calls, covering 139 libraries and 1,140 tasks across seven fundamental programming domains. DomainEval [79] evaluates LLMs' code generation capabilities across multiple domains (e.g., com- puting, systems, and cryptography). However, despite the advancements made by BigCodeBench and DomainEval in addressing the need for domain-specific code generation benchmarks, the domains emphasized in these benchmarks still largely revolve around foundational programming domains. As a result, developers in downstream development domains may struggle to directly determine the LLMs' code generation performance in their respective domains based on these benchmarks.\nMoreover, some studies evaluate LLMs' ability to generate functions that utilize third-party libraries in specific domains. Lin et al. [26] evaluate mainstream LLMs' abilities to generate functions that invoke domain-specific third-party libraries in game development and web development."}, {"title": "3 MULTICODEBENCH", "content": "In this section, we present a detailed introduction to MULTICODEBENCH, a new multi-domain, multi- language code generation benchmark. The section is organized as follows: Benchmark Overview, Benchmark Construction Pipeline, and Benchmark Characteristics.\n3.1 Benchmark Overview\nMultiCodeBench consists of 2,400 task instances, covering 12 popular application domains and 15 programming languages. Each domain includes 200 carefully constructed programming tasks. Figure 1 provides an example of a task instance in MultiCodeBench. Each task has a unique identifier, represented by the instance_id. The Domain field indicates the software application development domain to which the task belongs, while the Subdomain field represents the specific subdomain of development within that domain. The Owner/Repo field specifies the GitHub project from which the task instance is sampled, and the Function declaration is included as part of the prompt provided to the LLM, containing information about the input and output of the target function, which helps guide the LLM in generating the correct code. The Local file context refers to the code snippet located above the target function in the local file, where the local file refers to the file containing the target function. Together with the function declaration, this simulates a real development"}, {"title": "3.2 Benchmark Construction Pipeline", "content": "As shown in Figure 2, the construction process of MultiCodeBench consists of two main stages: the investigation of specific software application domains and the construction of the MultiCodeBench benchmark dataset.\n3.2.1 Investigation of Specific Application Domains. Due to the vast number of software application domains, it is challenging to cover them all. Therefore, we focus on specific domains that are frequently discussed online as the subjects for MultiCodeBench. We conduct an investigation by analyzing domains with high exposure and frequent discussions since January 1, 2020, on relevant technical websites. Specifically, we collect all posts from active technology news sites (Ars Technica,2 TechCrunch,\u00b3 Hacker News\u2074), developer communities (Dev,5 Stack Overflow), and blogs (GitHub Blog, Medium\u2077) from January 1, 2020 onward. After aggregating all the content, we apply the Latent Dirichlet Allocation (LDA) [32] algorithm to identify the underlying topics of the collected posts. LDA is a topic modeling algorithm that can be used to extract the keywords that represent the underlying topics in NL texts. Stack Overflow, on the other hand, has a well- established tag management mechanism where each post is associated with several tags. These tags serve as keywords that effectively reflect the post's topic. Therefore, for Stack Overflow, we only collect and aggregate the tags from all corresponding posts. Finally, we rank the keywords based on their frequency of occurrence and analyze the domains corresponding to the top 200\n\u00b2https://arstechnica.com/information-technology/\n\u00b3https://techcrunch.com/category/artificial-intelligence/\n\u2074https://news.ycombinator.com/\n\u2075https://dev.to/\n\u2076https://stackoverflow.com/\n\u2077https://github.blog/\n\u2078https://medium.com/"}, {"title": "3.2.2 Construction of the MultiCodeBench Benchmark", "content": "The construction of the MultiCodeBench benchmark involves four main steps, namely, selecting high-quality open-source repositories for specific domains, sampling programming problems from the these repositories, manually annotating docstrings through cross-validation, and dependency analysis.\nDomain-specific high-quality projects selection. Based on the taxonomy in Table 2, we start by searching for real open-source projects on GitHub that are built upon specific technology frameworks or development platforms corresponding to each subdomain. We rank these projects based on their star count, selecting a set of high-quality projects for each domain. The advantage of sampling problems from these high-quality projects is that they generally adhere to good maintenance practices and coding standards, making the selected problems more representative of real-world scenarios.\nSampling programming problems from the selected repositories. We do not randomly select functions from the selected projects as programming problems. Instead, we conduct careful manual reviews to ensure that the chosen programming problems are highly domain-specific rather than general-purpose functions. For example, in web development projects, general algorithms and data structure implementations are not considered. Instead, we select functions that are closely related to business logic, including but not limited to core application functionality and functions that invoke domain-specific APIs. Ultimately, we carefully select 2,400 programming problems for MultiCodeBench, with 200 for each domain.\nManually annotating docstrings through cross-validation. Since the quality of docstrings in the selected projects can vary and using existing docstrings directly may lead to data leakage issues, we manually rewrite the docstrings for every programming problem in MultiCodeBench. We invite professional software developers to rewrite the requirement descriptions for each instance in MultiCodeBench. Specifically, we invite 24 developers, two per domain, each with over five years of experience in their respective domains. Then we conduct a cross-validation annotation precess: one annotator is responsible for writing the requirement description for each task instance, while another annotator conducts a manual review to determine whether the rewritten docstring matches the corresponding ground truth. In cases of disagreement, both annotators should reach a final resolution through discussion. This ensures that the docstrings in MultiCodeBench accurately"}, {"title": "3.3 Benchmark Characteristics", "content": "Multi-domain multi-language benchmark. MultiCodeBench spans 12 specific application domains, aiming at shifting the focus of LLM evaluation in code generation toward real-world application scenarios. Our benchmark includes 2,400 programming tasks, with 200 carefully con- structed tasks for each domain. To comprehensively evaluate the code generation capability of LLMs within specific domains, we first explore the subdomains within each specific application domain, aiming to cover as many development branches as possible. Furthermore, MultiCodeBench also supports 15 different programming languages, allowing for the assessment of LLMs' performance across various programming tasks. Previous benchmarks that focus on general-purpose tasks only provide pass rates, limiting their ability to offer deeper insights for future research. In contrast, MultiCodeBench not only offers practical advice for LLM users, helping them choose the most suitable LLM in their respective domains, but also enables LLM developers to identify specific weaknesses, thereby facilitating targeted performance improvements. We advocate for a shift in LLMs' evaluation towards real-world scenarios, providing more direct feedback to researchers and practitioners.\nHuman annotation. We enlist experienced developers in each domain to rewrite the docstrings for every task instance in MultiCodeBench, and ensure the quality of the manual annotations through cross-validation. During the benchmark construction process, we observe that the quality of docstrings in real-world projects varies significantly, with many failing to clearly and accurately convey the target function's purpose. Therefore, we deem it necessary to rewrite a high-quality docstring for each programming task. To ensure the quality of the docstring annotations, we require annotators to have relevant industry experience in the corresponding domain. Additionally, human annotation process helps mitigate the risk of data leakage by providing fresh annotations, offering a more accurate reflection of the LLMs' code generation abilities in specific domains.\nRich dependency information. Each task in MultiCodeBench is accompanied by rich relevant dependency information, which facilitates deeper analysis of LLMs' performance. We extract various dependencies related to the target function, including import statements from the local file and API dependencies of the target function. The APIs that the target function relies on are categorized into three types: standard library APIs, project-defined APIs, and third-party library APIs. This categorization allows for more detailed analysis of LLMs, such as investigating the reasons behind LLMs' failures, assessing the LLMs' familiarity with these three types of APIs, and evaluating how well the LLMs handles domain-specific third-party libraries. This deeper insight aids in understanding the LLMs' strengths and weaknesses in handling complex, real-world tasks."}, {"title": "4 EXPERIMENTAL SETUP", "content": "4.1 Research Questions\nOur experiments are designed to answer the following research questions (RQs):"}, {"title": "4.2 Model Selection and Settings", "content": "We select the mainstream LLMs commonly used in recent code-related studies, including both open- source and closed-source LLMs. For open-source LLMs, we choose the StarCoder series (including StarCoder, StarCoder2-3B, StarCoder2-7B, and StarCoder2-15B), the CodeLLaMa series (including CodeLLaMa-7B, CodeLLaMa-13B, and CodeLLaMa-34B), and the DeepSeekCoder series (including DeepSeekCoder-6.7B and DeepSeekCoder-33B). For closed-source LLMs, we select the widely-used GPT-4 (GPT-4-0125-preview) and GPT-3.5 (GPT-3.5-turbo-turbo-0125). Table 3 provides detailed information about the studied LLMs, including type, model, and performance on HumanEval, with performance data on HumanEval derived from EvoEval [66]. The values in brackets in the \u201cPass@1 on HumanEval\" column represent the performance ranking of each LLM relative to all the studied LLMs. For all studied LLMs, we set the temperature to 0.4 and top-p to 0.95. Note that, to mitigate issues stemming from the randomness of model generation, the experimental results presented in this paper are obtained by conducting three repeated experiments and averaging the results. The experiments are conducted on a server running Ubuntu 18.04.6 LTS with 128 Intel(R) Xeon(R) Platinum 8336C @ 2.30GHz CPUs and 8 NVIDIA A800 80GB PCIe GPUs.\""}, {"title": "4.3 Evaluation Metric", "content": "We use the CodeBLEU [52] metric to evaluate the code generation performance of the studied LLMs on MultiCodeBench, rather than the commonly used pass@k metric in recent code generation works [38, 69]. This choice is motivated by the complexity of the tasks in MultiCodeBench, which spans 12 application domains, 15 programming languages. Projects in each development domain tend to be highly complex, with intricate dependencies and specific platform requirements, often relying on numerous third-party libraries. Deploying, debugging, and running these projects could demand vast time and effort. Furthermore, the tests within these projects may not fully cover all functionalities of the target functions. Additionally, with 2,400 task instances, execution- based evaluation would result in prohibitively long evaluation times, making MultiCodeBench less practical for future use. Given these factors, using execution-based metrics on MultiCodeBench would be excessively burdensome. Thus, we adopt the CodeBLEU metric, which is commonly used for code generation tasks [24, 26]."}, {"title": "5 EVALUATION RESULTS", "content": "5.1 RQ1: How Well Do Current Mainstream LLMs Perform in code Generation for Specific Domains?\n5.1.1 Overall Performance. Table 4 presents the code generation performance of eleven mainstream LLMs on MultiCodeBench. Experimental results show that, overall, all studied LLMs perform poorly on MultiCodeBench. The most surprising result is that, closed-source LLMs such as GPT-4 and GPT- 3.5 exhibit relatively suboptimal performance in domain-specific application development tasks, with performance even falling below that of the open-source DeepSeekCoder-33B. DeepSeekCoder- 33B demonstrates superior code generation performance across most domains, while CodeLLaMa- 34B only surpasses DeepSeekCoder-33B in the domain of mobile application development. However, as shown in Table 3, DeepSeekCoder-33B only rank 4th in the HumanEval Pass@1 metric among the eleven studied LLMs, suggesting that general-purpose benchmarks may not accurately reflect the LLMs' true capabilities in downstream software engineering tasks.\nMoreover, as illustrated in Figure 4 (a) and (b), LLMs with similar performance on HumanEval can exhibit varying levels of performance in different application domains. For example, as presented in Table 3, DeepSeekCoder-6.7B and StarCoder2-15B perform similarly in terms of code generation capability on HumanEval, yet their performance diverges significantly in downstream software engineering code generation tasks. DeepSeekCoder-6.7B excels in desktop application development, blockchain development, and mobile application development, while StarCoder2-15B outperforms DeepSeekCoder-6.7B in game development and deep learning. Therefore, we think researchers should shift their focus from general-purpose benchmarks to more specific downstream domains, which will provide deeper insights into LLM performance and offer more practical, actionable guidance for users of these LLMs. Similarly, as depicted in Figure 4 (b), StarCoder and StarCoder2- 7B also show considerable code generation performance on HumanEval. However, the domains in which these two LLMs excel are also different. Instead of simply reporting a single pass@k"}, {"title": "5.1.2 Performance Comparison Among Subdomains", "content": "Table 5 presents a detailed analysis of the studied LLMs' code generation performance across different development subdomains within the 12 specific domains. In general, all LLMs exhibit varying performance across different subdo- mains. For example, in the data analysis (DA) domain, all LLMs demonstrate poor performance in code generation based on the statistical library statsmodels, significantly lower than their perfor- mance in other subdomains of the same domain. Besides, in the blockchain development domain, DeepSeekCoder-33B demonstrates a stronger proficiency in developing Ethereum-based blockchain projects compared to those based on the original Bitcoin chain, with a performance increase of 21.8%. Furthermore, we observe that an LLM may exhibit suboptimal overall performance in a particular domain, yet demonstrate better code generation capabilities in a specific subdomain within that domain. For example, in web development, CodeLLaMa-13B achieves a performance level comparable to that of DeepSeekCoder-33B on React-based programming tasks, despite its smaller parameter size and overall weaker performance in web development tasks. These results highlight the importance of evaluating LLMs' code generation capabilities in specific subdomains, as performance can vary considerably between subdomains within the same domain. Such findings offer practical insights and guidance for downstream developers in selecting AI programming assistants best suited to their specialized development needs.\nRQ1 Summary: Mainstream LLMs often perform poorly in domain-specific code generation tasks. Strong performance on HumanEval does not guarantee success in these tasks, and LLMs with similar performance on HumanEval can perform significantly different in domain-specific code generation. Furthermore, larger parameter scales do not necessarily lead to better code generation performance in specific domains."}, {"title": "5.2 RQ2: What are the Reasons that LLMs Fail to Correctly Generate Domain-specific Code?", "content": "Given that GPT-3.5 unexpectedly demonstrates poor domain-specific code generation performance, we conduct an error analysis of its generation results in Table 4 by reviewing each function generated by the model and compare it with the ground truth to identify why the model fails to produce the correct functions. After a thorough manual error analysis process, we identify five key reasons that contribute to the LLM's inability to generate the domain-specific function correctly.\n\u2022 Reason 1: The LLM does not sufficiently understand the requirements in the docstring.\n\u2022 Reason 2: The LLM lacks the repository context.\n\u2022 Reason 3: The LLM is unfamiliar with third-party libraries and APIs related to specific domain.\n\u2022 Reason 4: The LLM does not fully understand domain-specific algorithms, processing flows, or methods.\n\u2022 Reason 5: The LLM has insufficient capabilities in some programming or language features."}, {"title": "5.3 RQ3: Can Different Types of Context Information Improve the LLM's Code Generation Performance in Specific Domains?", "content": "Table 6 shows the code generation performance of LLMs enhanced by various contexts. We consider the combination of docstring and function declaration as a basic prompt, which provides only a description of the target function along with a simple introduction to the input and output types of parameters. This is far from sufficient for the challenging task of domain-specific code generation. According to the error analysis results in the Section 5.2, LLMs fails in specific domain code generation tasks primarily due to a lack of project background knowledge and a lack of domain- related knowledge. Therefore, in this experiment, we attempt to integrate more information into the basic prompt to explore whether the LLM can enhance its code generation performance with the assistance of additional context. Our aim is to provide empirical recommendations for developers to better utilize LLMs in real-world development scenarios. The additional information we provide for the LLMs is as follows:\n(1) Import statements. We provide the import statements from the file containing the ground truth as auxiliary information to explore whether this information can effectively help the LLMs improve code generation performance. CodeGen4Libs [42] has found that generating import statements first can significantly enhance the LLM's ability to call APIs, as import statements help the LLMs understand the potential dependencies of the target function.\n(2) Local file context. Local file context refers to all code snippets in the file containing the ground truth that precede the ground truth itself. Experiments in ComplexCodeEval [24] have shown that local file context can effectively improve the LLMs' code generation performance.\n(3) Dependency context. Dependency context includes the project-specific APIs called by the ground truth and their corresponding function code. This information helps the LLM clarify the"}, {"title": "RQ3 Summary", "content": "Rich repository context and domain-specific knowledge can assist LLMs in analyzing project dependencies and requirements, enhancing their code generation performance in specific domains. However, excessively long contexts can distract LLMs, which may lead to a decline in code generation performance."}, {"title": "6 THREATS TO VALIDITY", "content": "We have identified the following threats to the validity of our study.\nCovered domains. MultiCodeBench encompasses 12 specific application domains. However, due to the vast number of domains, it is indeed challenging to cover all of them at once, which may somewhat limit the applicability of MultiCodeBench. Additionally, our research on specific domains from active websites may have overlooked some equally prominent domains. Therefore, we will continue to expand MultiCodeBench in future work to promote it across more domains.\nEvaluated LLMs. Due to limitations in computational resources, we are unable to evaluate all code-related LLMs, such as WizardCoder [45], PolyCoder [67], and the CodeGen series [49]. Nonetheless, we have made every effort to cover both open-source and closed-source LLMs to enhance the representativeness of the experimental results.\nEvaluation metric. We chose CodeBLEU as the evaluation metric for all experiments. Although this metric reflects the accuracy of LLMs' generated results from various aspects, it does not fully capture whether the generated code can run correctly or pass the corresponding test cases."}, {"title": "7 CONCLUSION", "content": "In this paper, we emphasize the need to assess LLMs in specific domains. We introduce Multi- CodeBench, a code generation benchmark covering 12 specific software application domains and 15 programming languages. We conduct a comprehensive experimental evaluation to explore the code generation performance of current mainstream LLMs. Additionally, we perform an in-depth analysis of the reasons for LLMs' failures in code generation tasks and identify factors contributing to their suboptimal performance. We also investigate how to improve LLMs' code generation capabilities in specific application domains. The experimental results provide many practical insights, which we believe can assist developers in selecting more suitable code LLMs and help model developers further enhance these LLMs' coding abilities."}, {"title": "8 DATA AVAILABILITY", "content": "To facilitate the replication study, we have released our benchmark and the experimental data at our project homepage: https://github.com/DeepSoftwareAnalytics/MultiCodeBench."}]}