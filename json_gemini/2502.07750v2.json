{"title": "PFedDST: Personalized Federated Learning with Decentralized Selection Training", "authors": ["Mengchen Fan", "Keren Li", "Tianyun Zhang", "Qing Tian", "Baocheng Geng"], "abstract": "Distributed Learning (DL) enables the training of machine learning models across multiple devices, yet it faces challenges like non-IID data distributions and device capability disparities, which can impede training efficiency. Communication bottlenecks further complicate traditional Federated Learning (FL) setups. To mitigate these issues, we introduce the Personalized Federated Learning with Decentralized Selection Training (PFedDST) framework. PFedDST enhances model training by allowing devices to strategically evaluate and select peers based on a comprehensive communication score. This score integrates loss, task similarity, and selection frequency, ensuring optimal peer connections. This selection strategy is tailored to increase local personalization and promote beneficial peer collaborations to strengthen the stability and efficiency of the training process. Our experiments demonstrate that PFedDST not only enhances model accuracy but also accelerates convergence. This approach outperforms state-of-the-art methods in handling data heterogeneity, delivering both faster and more effective training in diverse and decentralized systems.", "sections": [{"title": "I. INTRODUCTION", "content": "There has been growing interest in applying signal processing and machine learning (ML) to critical predictive tasks across various disciplines [1-6]. The strategy of integrating data from multiple sources, such as sensors, enhances outcomes by providing multiple perspectives on a single phenomenon. Moreover, the global trend toward stricter data privacy laws and the growing implementation of regulations that restrict the sharing of sensitive data, such as health information, has accelerated advancements in distributed learning and decision-making processes that function without exchanging raw data [7-11]. Federated learning (FL) has gained prominence for its ability to train models on decentralized devices [12]. FL systems facilitate multi-client learning without centralizing raw data, addressing both privacy and communication challenges. However, heterogeneity in data distribution, resource allocation, task objectives, or network characteristics across nodes poses challenges to model accuracy and convergence [13]. Personalized Federated Learning (PFL) [14] addresses these issues by tailoring models to specific client needs, thereby enhancing the effectiveness beyond the conventional single-model approach (e.g., [15]) in FL. PFL is categorized into Centralized Personalized Federated Learning (CPFL) and Decentralized Personalized Federated Learning (DPFL) [16]. CPFL can suffer from communication bottlenecks and server failures, leading to increased communication traffic and potential system crashes. In contrast, DPFL emphasizes peer-to-peer interactions among edge clients, reducing communication loads on local nodes and promoting faster convergence. In this topology, clients maintain an undirected and symmetric communication structure, facilitating model exchanges with peers. Most existing PFL approaches finely tune the interactions between global and personalized models to accommodate local data variations using methods such as regularization [17], knowledge distillation [18], multi-task learning [19], and clustering [20]. These techniques aim to enhance personalized performance in the heterogeneous setting. For example, approaches like FedPer [15] propose to capture personalization aspects in federated learning by viewing deep learning models as base and personalization layers. And FedBABU [21] utilize a single global feature representation coupled with multiple local classifiers, differing in how they manage the relationship between the shared representation and the individual linear components. FedFusion [2] utilizes a representation method to fuse the batch information to solve the heterogeneity problem. Cho et al. [22] provides theoretical convergence analysis for these algorithms under general non-convex conditions. DFedAvgM [23] employs multiple local iterations with SGD and quantization techniques to reduce communication overhead. Dis-PFL [24] designs personalized models and pruned masks for each client to personalized convergence. OSGP [25], DfedPGP [26], and AsyNG [27] utilize the push-sum method to enhance training efficiency. Despite ongoing efforts, DPFL methodologies continue to face slow convergence rates during aggregations, a challenge compounded by heterogeneous data distributions among clients. Additionally, disparities in communication bandwidth and computational capabilities complicate these issues further, leading to unstable communication channels between clients. As a result, clients are compelled to selectively engage with only a limited subset of peers for communication. To address these challenges, we introduce Personalized Federated Learning with Decentralized Selection Training (PFedDST), a decentralized selection training-based Personalized Federated Learning approach. This method ensures that each client maintains a model of the same dimensionality, facilitating efficient aggregation and strategic communication among clients. During each communication round, clients selectively engage with a subset of peers, chosen through a strategic scoring strategy for their relevance to the current learning context. They then aggregate their own model with those selected from their peers. After local updates, clients share their newly trained model parameters with the required peers, thereby enhancing the collective learning process and ensuring continuous improvement and relevance of the shared data. We employ an innovative scoring scheme that evaluates potential peer clients based on three key factors: feature extraction capability, task heterogeneity, and communication frequency. Simulations in heterogeneous settings demonstrate that PFedDST not only increases the average test accuracy on local test data but also reduces the number of communication rounds required to achieve the same performance targets. We summarize our contributions as following:  We propose the PFedDST framework, a personalized federated learning approach where each client continuously learns from selected peers to update its feature extraction capabilities while maintaining a personalized prediction header. This integration of peer selection and partial model personalization enhances robust communication and accelerates convergence. Strategic selection enables clients to enhance their feature extraction capabilities from the most informative and relevant neighbors. It also prioritizes communication with clients that have not recently interacted, thereby diversifying and refreshing the learning inputs. Experimental results demonstrate that PFedDST outperforms various state-of-the-art baselines. It proves particularly effective in environments characterized by data heterogeneity and limited computational resources. It should be noted that our strategy is different from traditional directed DFL methods such as Dis-PFL and AsyNG, which typically involve exchanging all parameters for a single consensus model or selecting communication targets randomly. Instead, our approach incorporates score-based neighbor selection, partial freeze [28] training, and alternating optimization to accelerate convergence. This method not only ensures model robustness and enhances personalization but also optimizes communication efficiency."}, {"title": "II. SYSTEM MODEL AND METHODOLOGY", "content": "In centralized model training, consider a classification or multiclass detection task in which each data sample is a pair of (x, y), where $x \\in \\mathbb{R}^d$ represents the input features, and y\u2208 {0,1,..., k-1} signifies the corresponding labels, with k being the number of possible classes. The goal is to classify the variable x into one of k categories. This classification is achieved using a model parameterized by $w : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$. Each component of the output, y(x) for y = 0, . . ., k-1, represents the likelihood (or confidence score) that the instance x belongs to class y. The primary objective is to minimize the expected loss, defined by the equation: $L(W) := \\mathbb{E}_{(x,y)\\sim D}[L(w; x, y)],$ (1) where $L(w; x, y)$ measures the loss of the decision margins $Y_y(x; w) \\in \\mathbb{R}^k$ when the true label of x is y, and the expectation is taken over the joint distribution of the dataset D. Optimization of this expected loss commonly uses gradient-based algorithms like Stochastic Gradient Descent (SGD) or Adam. These methods iteratively adjust the parameters w to minimize the empirical loss for data (x,y), $W_{new} = W_{old} - \\eta \\nabla_wL(w; x, y)$, where \u03b7 denotes the learning rate."}, {"title": "A. Decentralized Personalized Federated Learning with Partial Freezing", "content": "In decentralized personalized federated learning, where data distribution varies across clients, each client i has a distinct data distribution $D_i$ and maintains a personalized model parameterized by $w_i$. The index i ranges over a total of M clients, $i \\in \\{1,...,M\\}$. In this setting, a objective is to optimize the local models jointly [29]: $\\min_{W_1,..., W_M} L(w_1,...,w_M) = \\frac{1}{M}\\Sigma_{i=1} L_i(W_i),$ (2) where $L_i(W_i) = \\mathbb{E}_{(x,y)\\sim D_i} [L(W_i; x, y)]$ represents the empirical risk associated with the i-th client's local data and $L$ denotes the loss function. To enhance personalized model performance and expedite convergence, we integrate the concept of partially frozen training. The model is structured into two distinct parts: the header and the feature extraction layers. The header comprises the model's final fully-connected layers, primarily responsible for classification tasks. This component is essential for customizing the model to meet each client's specific re-quirements, allowing personalized adjustments in the decision-making process. The feature extraction layers consist of the earlier stages of the model, which are tasked with processing and extracting pertinent features from the input data. During each communication round, each client i strategi-cally selects a subset of peer clients, denoted by $M_i$, and aggregates (e.g., simple average) its own feature extraction layers with those from its peers to obtain the aggregated feature extraction layer $e_i$. The header layers of client i, denoted by $h_i$, remain unchanged and do not participate in the model aggregation. Then, client i undergoes local training to sequentially update $e_i$ (with frozen $h_i$) and $h_i$ (with frozen $e_i$). Specifically, $h_i$ is frozen first and the aggregated $e_i$ is updated using the local data distribution $D_i$. $\\min_{e_i} L_i(e_i) = \\mathbb{E}_{(x,y)\\sim D_i} [L((e_i,h_i); x,y)]$ (3) where the subscript f in $h_i$ indicates that the parameters are frozen. Upon updating $e_i$, it is sent to the required peers and the parameters in $h_i$ are unfrozen and updated next: $\\min_{h_i} L_i(h_i) = \\mathbb{E}_{(x,y)\\sim D_i} [L((e_i, h_i); x, y)]$ (4) Once the local update is complete, client i shares its updated $h_i$ back to the network. This updated information"}, {"title": "B. Strategic Peer Selection for Communication", "content": "For each client, we quantify the degree of information contribution from others by assigning a score to each peer. A higher score indicates that a peer holds more valuable information for enhancing the feature extraction capabilities of the local client, thus facilitating more targeted updates. Essentially, our aim is for clients to augment their feature extraction abilities from peers that are better equipped to guide them, particularly those undertaking similar tasks. The scoring system is based on a composite evaluation of three factors: the loss disparity score $s_i$, the header distance $s_d$, and the peer recency $s_p$. Specifically:  The loss disparity score ($s_l$) measures the potential to enhance the generality of a client's feature extraction capabilities. This score is calculated by assessing the loss when predictions are made using this client's model on a peer's local dataset. A higher loss indicates a significant gap in the client's ability to predict the peer's data, signaling a stronger need for adaptation. The header distance score ($s_d$) identifies peers whose tasks are more closely related to the current client's tasks. It is measured by the weight distance between the header layers of the two clients. A smaller distance indicates that the label distributions, or tasks, of the two clients are more similar, making learning from such peers particularly beneficial. The peer recency score ($s_p$) is designed to enhance learn-ing generalization by avoiding repetitive communication with the same few peers and encouraging engagement with those not recently communicated with. This ap-proach helps prevent overfitting and promotes a more diverse and robust learning process. By employing this holistic scoring mechanism, we strate-gically select the most beneficial peers for communication, thereby optimizing the efficiency and effectiveness of the distributed learning environment. Loss Disparity Score. The concept of selection skew, denoted by $p$, was defined in [22] within the context of centralized FL. This skew quantifies the disparity in loss outcomes when evaluating the unified model on the data of a strategically selected subset of clients, as opposed to a random selection. The findings in [22] suggest that a larger lower bound on pleads to faster convergence during the training process, indicating the advantage of selecting clients whose data produce larger losses because the current model underperforms on these and requires further training. Inspired by this, we define a decentralized version of p for a specific local client i, representing the model loss difference between selecting a subset of peers $M_i$ and a full random selection of peers in model aggregation: $\\rho_i = \\frac{\\Sigma_{j\\in M_i} n_j (L_j(\\mathcal{W}_i) - L_j(\\mathcal{w})) / \\Sigma_{j\\in M} N_j} {L_i(\\mathcal{W}_i) - \\Sigma_{j\\in M} n_j L_j(\\mathcal{w}) / \\Sigma_{j\\in M} n_j} \\ge 0$ (5) where M represents the collection of available clients to the client i, $n_j$ is the fraction of data at the j-th client and $\\mathcal{w} = \\arg \\min_{W_j} L_j$ is the optimized $w_j$ for client j. With purely random selection, p = 1 since the numerator and denominator in (5) are equal. Inspired by the findings in [22], we adopt a client selection strategy that seeks to maximize lower bound of p, thereby accelerating the convergence rate. However, evaluating p for all potential subsets of peers is computationally impractical due to its NP-hard nature. Instead, we utilize the loss of applying the ith client's model on the jth peer's data, denoted by $l_j(W_{i,j})$, as a surrogate to measure the desirability of selecting peer j. Mathematically, the loss score between client i and its peer j is given by: $s_i^{ij} = ||l_j(W_{i,j}) || = ||\\mathbb{E}_{(x,y)\\sim D_j} [L(W_{i}; x, y)] ||$ (6) where $D_j$ is the data distribution at the jth peer. A higher $s_i^{ij}$ indicates that the ith client's model struggles with handling the jth peer's data, suggesting a greater preference for selecting jin the next communication round. Header Distance Score. Unlike traditional centralized FL, which aims to develop a unified model across diverse data types, decentralized personalized FL focuses on creating mod-els tailored to specific local data. When two clients have similar tasks (e.g., comparable label distributions), they are likely to benefit from communication and model aggregation. Similar tasks imply compatible data or learning objectives, enhancing the learning process through effective information sharing. However, if two clients have significantly different tasks and data distributions, such as one client focusing on im-ages of animals and another on images of plants, their feature extraction layers possess distinct properties. Aggregating their models might not only fail to improve but could potentially deteriorate each other's performance. Based on this reasoning, it is preferable for clients to select peers with similar tasks. We propose using the element-wise cosine similarity between header layers' weights to measure this similarity. We prefer cosine similarity over Euclidean distance as a metric because it emphasizes the directional trends (patterns) of the weights rather than their absolute magnitudes. This approach is preferred as it focuses on the relative importance of input features, which reflects the true nature of the task. In addition, it is important to note that other distance metrics like Kullback-Leibler (KL) divergence are unsuitable for measuring distances between model param-eters, as these weight parameters do not inherently possess probabilistic properties. Let $H = (h_1,h_2,...,h_n)$ and $G = (g_1, 92, ..., gn)$ represent the weight parameters corresponding to the header layers and the header distance score (coefficient) can be computed as follows: $S_d = \\frac{\\Sigma_{i=1}^n h_i g_i}{\\sqrt{\\Sigma_{i=1}^n h_i^2} \\sqrt{\\Sigma_{i=1}^n g_i^2}}$ (7) where $h_i$ and $g_i$ are the i-th elements of H and G, and n is the number of elements in H and G.  Here, we validate the effectiveness of client selection based solely on the header distance score. Peer Recency Score. A critical factor in decentralized FL is the communication frequency. The peer recency score helps determine a local client's priority for selecting a peer based on how recently local client has communicated. This score is designed to prevent the local client from \"forgetting\" the knowledge it might have acquired from peers that have not been engaged for several rounds. A higher peer recency score, indicating a longer interval since last communica-tion, increases a peer's probability of being selected. This mechanism aims to enhance both the model's convergence rate and its generalization, thereby improving overall training effectiveness. In a fully decentralized network, global training information and iteration counts are not accessible for each peer. Therefore, we calculate the peer recency score using only local iteration data. For a local client, let $n_{o,j}$ be the iteration number at which peer j was last selected, and $n_t$ denote the current iteration number. The peer recency score of peer j, $s_{t,j}$ is designed to range from 0 to 1, where it approaches 0 if $\\frac{N_t}{no,j}$ is small, discouraging the repetitive selection of the same peer and promoting diversity in peer engagement. Conversely, as $\\frac{n_t}{no,j}$ exceeds a certain threshold $C_o$, $s_{t,j}$ increases to its maximum value of 1. To achieve this property, we use the cumulative distribution function (CDF) of the exponential distribution: $S_p = \\Phi(1 - e^{-\\lambda(n_t - no,j)})$ (8) where \u03a6 represents the CDF and \u03bb is the rate (scaling) parameter of the exponential distribution. Holistic Determination of the Final Score. For a specific client, the cumulative score for selecting a peer j indicating its preference for selecting it incorporates three factors: the loss score $s_i$ from (6), header distance score $s_d$ from (7), and peer recency score $s_p$ from (8). For a specific local client, a peer's overall communication score is intelligently designed as follows, $S = S_p(\\alpha \\frac{s_i}{S_d + c})$ (9) where \u03b1 is a scaling parameter, and c is a constant that repre-sents the communication cost score between the corresponding peer. This overall score increases under the following conditions: a. when $s_i$ increases, indicating a larger loss on the peer's data and a greater need for the client to learn from it; b. when $s_d$ decreases, reflecting a higher task similarity with the peer; and c. when $s_p$ increases, suggesting that the client has not communicated with this peer recently. In addition, the peer recency score $s_p$, ranging from 0 to 1, converges quickly to 1 as \u2758$n_t$ - $n_{o,j}$\u2758 grows large. The multiplication of $s_p$ and $\\alpha \\frac{s_i}{S_d + c}$ ensures that $s_p$ does not dominate the selection process. This design prevents the selection of peers that are significantly different from the local client solely based on infrequent prior communication, therefore we enhance the stability of personalized training."}, {"title": "C. Algorithm", "content": "In this section, we propose the PFedDST algorithm, which facilitates peer selection under a fully decentralized setting. The algorithm is designed to operate on each client, allowing for local decision-making without centralized oversight. Each client maintains two context information arrays to support decision-making processes, the loss array I and the peer recency array t. The loss array I stores the loss information calculated from aggregated parameters with each peer, and the peer recency array t records the iteration numbers that each peer was last selected by the local client. By leveraging data from loss, header distance, and selection frequency, each eligible client is assigned a score that reflects its priority as a potential peer. This score is then used to determine the selected communication peers $M_i$. The selection process ensures that communication efforts are focused on the most relevant and beneficial peers, optimizing model performance and enhancing training efficiency. After completing peer selection, the feature extraction layers are aggregated from each selected peer. The training then proceeds in two phases. In the first phase, the header layers are frozen, and the feature extraction layers are trained. This targeted training helps to enhance the model's ability to accurately interpret and process input data. Once this training phase is complete, the trained parameters can immediately be dispatched to peers that have already made requests. In the second phase, the feature extraction layers are frozen, and the training focuses on the header layers. This phase is dedicated to fine-tuning the header, which is responsible for making the final decisions and classifications based on the processed features. The overall training approach allows each component of the model to be optimized for its specific role, enhancing the overall performance and efficiency of the distributed learning system."}, {"title": "III. EXPERIMENTS", "content": "In this section, we conduct extensive experiments to verify the effectiveness of the proposed PFedDST algorithm in sce-narios characterized by data heterogeneity and computation resources heterogeneity. These experiments are designed to evaluate how well PFedDST handles diverse datasets and varying computational resources across different nodes in a distributed system."}, {"title": "A. Experimental Setup", "content": "To evaluate the performance of the proposed algorithm, we use the CIFAR-10 and CIFAR-100 datasets, which are real-life image classification datasets containing images distributed across 10 and 100 classes, respectively. These datasets are commonly used in machine learning research to benchmark image classification algorithms and are particularly useful for assessing performance in heterogeneous data distribution sce-narios. The data for each dataset is partitioned in a Pathological distribution manner, intended to simulate a realistic scenario where each client may have access to only a limited subset of the total classes. Specifically, for CIFAR-10, we sample 2 classes from the total of 10 for each client. Similarly, for CIFAR-100, each client is assigned 5 classes from the total of 100. This partitioning method ensures that each client's training and testing data are distributed according to the same class subset, which introduces challenges typical of federated learning environments where data may not be identically and independently distributed across clients. To ensure a fair comparison across all methods, we maintain consistent experimental conditions for each baseline. The experiments are conducted over 500 communication rounds involving 100 clients. Each client in the federated learning setup communicates with 10 neighbors, and similarly, in the PFedDST method, 10 clients are also chosen at each communication round. The client sampling ratio is set at 0.1. The training involves using a batch size of 128. For the PFedDST method, the feature extraction part is trained for 5 epochs per round, matching the training duration of other baselines. The header part is only trained for 1 epoch per round to reduce computational overhead. All methods employ Stochastic Gradient Descent (SGD) as the optimizer, with a learning rate 0.1. Additionally, all methods implement a decay rate of 0.005 and a local momentum of 0.9 to optimize the convergence and stability of training. The communication cost is equal between each client."}, {"title": "B. Experimental Evaluation", "content": "We assess our proposed methods against current state-of-the-art baselines in PFL. The evaluation includes cen-tralized federated learning methods such as FedAvg [30], FedPer[15] and FedBABU [21], and decentralized federated learning methods such as DFedAvgM [23], Dis-PFL[24], and DFedPGP [26] and reproduced the result of [26]. Each method is tested using a ResNet-18 architecture. In our setup for partial PFL methods, the header layers are personalized for complex pattern recognition, while the remaining layers are shared for feature extraction. Our primary evaluation metric is personalized test accuracy, which aligns with our goal of addressing the challenges in PFL. As presented in Figure 3 and 4, the proposed Personalized Federated Learning Decentralized Selection Training (PFed-DST) shows superior stability and performance over baseline methods across diverse datasets and scenarios of data het-erogeneity. Specifically, on the CIFAR-10 dataset, PFedDST achieves a remarkable accuracy of 92.25%, outperforming the nearest baseline method, by 1.0%. On the CIFAR-100 dataset, DFedPGP leads with an accuracy of 79.41%, which is at least 0.7% higher than other baseline methods. The implementation of a communication protocol based on a directed graph allows clients to flexibly select their peers, thus facilitating the choice of pertinent information for their local training processes. In Table 2, we present the learning curves illustrating the convergence speeds of the methods compared. PFedDST has the fastest convergence among the methods tested, which benefited from the peers selection algorithm. Notably, DFedPGP demonstrates that a convergence rate is much better than other methods in both CIFAR-10 and CIFAR-100 scenarios. Compared to other methods, PFedDST further optimizes the clients' communication and aggregation. It enhances conver-gence speed and generalization capability by selecting peers based on their relevance scores, ensuring a more balanced choice of beneficial peers. Additionally, the use of a partially frozen training approach speeds up the training process and enhances transfer efficiency, which minimize the cost consumption while maximizing the information gain."}, {"title": "IV. DISCUSSION", "content": "Our framework is designed to prioritize the selection of the most beneficial communication peers and utilize partial personalization, ensuring optimal performance and efficiency in distributed learning scenarios. The simulation demonstrates that our method outperforms the current state-of-the-art methods in terms of accuracy and convergence rate. This improvement is more distinct as the model complexity grows, data heterogeneity intensifies, and the number of clients increases. This enhancement is attributed to the combined training of a fully personalized header and a shared feature extraction layer, supplemented by an effective benefit selection strategy. Initially, we implement a partially frozen training method. During local optimization, the header is frozen while the feature extraction layers are actively trained. Upon completion, the trained component is shared with the required peers, and the previously frozen sections are then unfrozen for further training. This method diverges from traditional training approaches by reducing the number of model parameters trained and communicated, enabling faster training completion. Additionally, it promotes stable parameter optimization and minimizes gradient conflicts. Secondly, we employ a score selection strategy, evaluating potential communication partners across various dimensions, including loss, selection frequency, communication costs, and task similarity. This comprehensive scoring method facilitates the identification of the most suitable partners for exchange, consequently improving the overall training outcomes by increasing accuracy and speeding up convergence. A notable feature of PFedDST is robustness. This selection mechanism automatically filters out potential attackers and clients with noisy data by measuring header distances, improving the robustness of the local model aggregation."}, {"title": "V. CONCLUSION", "content": "In conclusion, we propose a unified decentralized federated learning selection framework PFedDST for personalization, fast convergence, privacy, robustness, and communication effi-ciency within distributed learning environments. By employing score selection score based on loss, peer recency, and task similarity on decentralized devices, we offer the PFedDST that enhances the ability to communicate with beneficial peer models while ensuring a fast convergence rate and privacy. Theoretical findings and experimental results show that our method achieved a faster convergence rate and higher model accuracy compared to other state-of-the-art methods."}]}