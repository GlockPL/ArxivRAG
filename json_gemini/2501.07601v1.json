{"title": "Real-Time Decision-Making for Digital Twin in Additive Manufacturing with Model Predictive Control using Time-Series Deep Neural Networks", "authors": ["Yi-Ping Chen", "Vispi Karkaria", "Ying-Kuan Tsai", "Faith Rolark", "Daniel Quispe", "Robert X. Gao", "Jian Cao", "Wei Chen"], "abstract": "Digital Twin\u2014a virtual replica of a physical system enabling real-time monitoring, model updating, prediction, and decision-making\u2014combined with recent advances in machine learning (ML), offers new opportunities for proactive control strategies in autonomous manufacturing. However, achieving real-time decision-making with Digital Twins requires efficient optimization driven by accurate predictions of highly nonlinear manufacturing systems. This paper presents a simultaneous multi-step Model Predictive Control (MPC) framework for real-time decision-making, using a multi-variate deep neural network (DNN), named Time-Series Dense Encoder (TiDE), as the surrogate model. Different from the models in conventional MPC which only provide one-step ahead prediction, TiDE is capable of predicting future states within the prediction horizon in one shot (multi-step), significantly accelerating MPC. Using Directed Energy Deposition (DED) additive manufacturing (AM) as a case study, we demonstrate the effectiveness of the proposed MPC in achieving melt pool temperature tracking to ensure part quality, while reducing porosity defects by regulating laser power to maintain melt pool depth constraints. In this work, we first show that TiDE is capable of accurately predicting melt pool temperature and depth. Second, we demonstrate that the proposed MPC achieves precise temperature tracking while satisfying melt pool depth constraints within a targeted dilution range (10%-30%), reducing potential porosity defects. Compared to Proportional-Integral-Derivative (PID) controller, MPC results in smoother and less fluctuating laser power profiles with competitive or superior melt pool temperature control performance. This demonstrates MPC\u2019s proactive control capabilities, leveraging time-series prediction and real-time optimization, positioning it as a powerful tool for future Digital Twin applications and real-time process optimization in manufacturing.", "sections": [{"title": "1 Introduction", "content": "Autonomous manufacturing is essential for achieving efficiency, precision, and adaptability in modern production, leading to faster, reliable processes through machine learning integration [1]. Digital Twins, an emerging paradigm for manufacturing, serve as virtual counterparts of physical systems, facilitating bi-directional interaction, prediction, and decision-making under varying operational conditions and uncertainties where the digital and the physical systems evolve together. [2\u20134]. In manufacturing, Digital Twins enable proactive real-time decision-making to optimize and control the process under rapid changes in operation and uncertain conditions [2, 3]. One key element of realizing the Digital Twin in manufacturing is to perform real-time decision-making using the virtual model, providing proactive control actions to ensure part quality. Take additive manufacturing (AM) as an example, proactive control approaches, by incorporating process constraints and real-time process feedback, empower AM to anticipate and prevent deviations that could lead to defects [5, 6], thereby improving part quality and streamlining the manufacturing process. However, constructing a"}, {"title": "2 Technical Background", "content": null}, {"title": "2.1 Model Predictive Control", "content": "Model Predictive Control (MPC), also known as receding horizon control, is an advanced control technique that uses a model of the system to predict its future behavior and optimizes control actions by solving a finite-horizon optimal control problem at each sampling instant [8], as illustrated in Fig. 2. In standard MPC, once the control sequence is solved at the current step, the first action is applied to the plant, and the process is repeated as the system advances to the next step with updated observations. Assuming the prediction horizon is  , the MPC problem to optimize future control inputs u given the current state x   at current time    can be formulated as:\n\nmin\\limits_{u=[u_t,...,u_{t+H-1}]} J(u, x_t) = \\sum\\limits_{i=0}^{H-1} (||x_{t+i}||_Q^2 + ||u_{t+i}||_R^2), (1a)\ns.t. \\hat{x}_{t+i+1} = \\hat{f}(x_{t+i}, u_{t+i}), \\forall i \\in N[0, H], (1b)\nx_{t+i} \\in X, \\forall i \\in N[0, H-1], (1c)\nu_{t+i} \\in U, \\forall i \\in N[0, H-1], (1d)\n\nwhere ||x||2\nQ = x\u22a4Qx represents the quadratic operation of state vector x, the weighting matrices Q \u227b 0 and R \u227b 0 are symmetric. Equation (1b) is the general representation of the dynamic equation in which \u02c6f is the predictive model, and Eq. (1c) and Eq. (1d) are the constraints of states and control actions, respectively."}, {"title": "2.2 Time Series Model: Time Series Dense Encoder (TiDE)", "content": "When selecting an appropriate predictive model for MPC using the time series DNN, two key considerations are essential: (1) The inference speed as MPC requires multiple function evaluation in each iteration. (2) The data format that is compatible with the DED. In this work, we selected Time Series Dense Encoder (TiDE) [28], a residual neural network (ResNet) specifically designed for multi-variate time series forecasting, due to its efficiency and accuracy, as shown in Fig. 3. Its residual connection feature allows it to capture long-term dependency of the history without vanishing gradient. TiDE operates in linear time complexity, making it faster than other DNN-based time-series models like RNNs, which require recursive rollouts for sequence output, and Transformer-based methods, which involve a minimum   (  2) complexity due to self-attention mechanisms. Moreover, TiDE exhibits strong predictive performance in several benchmarks [28] and can quantify aleatoric (data) uncertainty in a one-shot manner using Gaussian or quantile regression (See examples in Darts package [29]). Note that different from our previous work [3] where a Bayesian LSTM is proposed to predict the complete temperature profile for the whole part for offline optimization, in this work, TiDE model is an accurate representation of local system behavior, which is more suitable for MPC."}, {"title": "3 Thermal Simulation Model", "content": null}, {"title": "3.1 Explicit Finite Element Solver", "content": "In this work, an in-house developed explicit FEA code is employed for part-scale transient heat transfer simulations of DED process [31]. The code, named GAMMA, is implemented in Python with temperature-dependent material properties and accelerated by GPU computation using CuPy 9.0 [32]. The heat transfer model is based on Fourier\u2019s law of heat conduction, with the governing equation expressed as:\n\n\\rho C_p(T) \\frac{dT}{dt} = k(T)\\nabla^2 T + q_{laser} + q_{conv} + q_{rad}, (2)\n\nwhere    is the temperature (K),    is the material density (  /  3),      (  ) and    (  ) are the temperature-dependent specific heat capacity (  /  /  ) and thermal conductivity (  /  /  ), respectively.              is the heat flux (  /  2) from the laser, and           and           represent the convective and radiative heat fluxes (  /  2). The heat source is modeled using a Gaussian surface flux distribution, expressed as:\n\nq_{laser} = \\frac{-2 P \\eta}{\\pi r_{beam}^2} exp \\left( \\frac{-2d^2}{r_{beam}^2} \\right), (3)"}, {"title": "3.2 Model Calibration", "content": "Since the GAMMA only simulates heat conduction for a part-scale simulation and neglects the Marangoni flow which contributes to convective heat dissipation, the GAMMA will overestimate the melt pool temperature compared to the sensor values, particularly when the melt pool temperature exceeds the liquidus temperature of the material. One way to compensate for that is to calibrate the artificial thermal conductivity of the material using the experimental data when the temperature of an element is higher than the liquidus temperature [7, 33]. In this work, we multiplied the extracted melt pool temperature from GAMMA by 0.5 to make the values fall in the range of the sensor values, without experimental calibration. Note that the purpose of scaling it is to show a reasonable value and to demonstrate the framework for MPC, but not to serve as a rigorous calibration method. As for the melt pool depth, since the temperature at the boundary of the melt pool won\u2019t exceed the liquidus temperature and be overestimated, we assume the simulated melt pool depth resembles the true value and does not apply any calibration or scaling."}, {"title": "4 Data and Model Preparation", "content": null}, {"title": "4.1 Target Geometry and Material", "content": "In this work, we chose a single-track square as the target geometry made of 316L on a thick substrate of AISI 1018,"}, {"title": "4.2 Feature Extraction", "content": "We developed an algorithm to extract the target features \u2013 melt pool temperature and depth \u2013 from the temperature profiles of all the nodes that belongs to the geometry. This algorithm runs one sampling timestep every five GAMMA simulation time steps (1 sampling time step = 5 \u00d7 0.00714 sec/step) to save memory sizes. As the laser location at each sampling time step is known, the melt pool temperature can be extracted by the following steps, also illustrated in Fig. 5(a):\n\nSelect the activated nodes that belong to the top layer of the current printed geometry (current max laser location)\nAmong the selected nodes, further select the nodes around the laser location, i.e., +/\u2212 3 mm on   - and   - directions, centered on the laser location.\nFit a radial basis function (RBF) surface [34] using the nodes selected from the previous step since the selected nodes are sparsely distributed. Then, the RBF surface will be interpolated with finer mesh grids (0.2 mm) to get a higher resolution of the temperature map, reducing the numerical errors induced by the coarse meshes.\nCalculate the average temperature within the scanning radius (0.9 mm) around the laser location as the melt pool temperature           . This approach is to emulate how coaxial photodiodes measure temperature in physical DED machine as it only calculates the mean temperature within its sensing region.\n\nWe also developed an algorithm for extracting melt pool depth from the simulation. Note that although the in-situ measurement of melt pool depth may not be available for most DED machines, it can still be obtained via online estimation methods [35\u201337]. Here, we assume that the melt pool depth is accessible in-situ. The algorithm can be detailed into the following steps, as illustrated in Fig 5 (b):\n\nSelect nodes within a cube of size 1.5 \u00d7 1.5 \u00d7 4 mm centered on the laser, extending \u00b10.75 mm on   - and   - directions, and \u22124 mm in   - direction.\nFit a three-dimensional RBF using the selected nodes from the previous step, with its coordinates as input and corresponding temperature as output. Interpolate the RBF with finer grid meshes.\nExtract the melt pool depth by calculating the maximum distance along the   -axis for all grid points where the temperature exceeds the solidus temperature of 316L. Subtract the clad/layer height (0.75 mm in this case) to obtain the final melt pool depth,             \u210e.\n\nNote that the melt pool depth obtained using this algorithm can be negative due to the heat conduction simulation, especially in the first three layers and at the start of a new layer. In this work, we will keep the raw data as it is collected without modifying/clamping the negative melt pool depth extractions to reflect the GAMMA simulation even though it is different from the experimental setting. The feature extraction algorithm will be used in both data collection in offline, and to emulate the function of the in-situ sensors in online MPC."}, {"title": "4.3 Data Collection", "content": "The objective of data generation is to create a series of laser power profiles that uniformly span the design space throughout the entire printing process. Given the high dimensionality of the laser power profile design space, traditional design of experiments (DOE) methods are impractical. To address this issue, we adopted the approach proposed by Karkaria et al. [3], which represents each laser power profile using 10 key parameters. These parameters include the amplitude, number of terms, frequency, and phase from the Fourier series approximation, as well as the rate of change for the amplitude, frequency, and phase of the wave. Additionally, three parameters account for the slope, fluctuation, and amplitude of the seasonal component of the laser power time series, providing greater flexibility in the representation. This dimensional reduction enables the application of DOE with 10 parameters using the optimal Latin hypercube sampling method [38]. These designs can subsequently be used to reconstruct temperature profiles for the entire print process. In this work, 100 laser power profiles are generated using this method.\n\nAs the laser power profiles are generated, part-scale simulations of the printing process are performed using GAMMA, with the generated laser profiles serving as input under ideal, noiseless, open-loop conditions. For convenience, all 100 time series are concatenated into a single continuous time series. Key features of interest, such as melt pool temperature (          ) and melt pool depth (            \u210e), are extracted using the aforementioned algorithm and saved as time series profiles upon the completion of the simulation. To mitigate the effect of the numerical errors introduced during the GAMMA simulation, both melt pool temperature and depth are smoothed using the moving average method with a window size of four. In addition to these features and the laser input (  ), three other parameters are recorded at each time step: the   -coordinate of the laser position, and the distances from the current laser position to the nearest geometry boundary along the   -axis and   -axis, denoted as      and     , respectively. Each time series profile is further divided into snapshots using a moving window approach with step size equals one. Consequently, each segment has a length of   +   , where    represents the window size (i.e., the length of the history) and    represents the prediction horizon. The resulting data collected from this stage are represented as x      \u2208 R  +  , d      \u2208 R  +  , z    \u2208 R  +  , x          \u210e \u2208 R  +  , d      \u2208 R  +  , u    \u2208 R  +  , \u2200   \u2208 N[1,   ] ,\n\nwhere    is the total number of fractions of time series,"}, {"title": "4.4 Model Training", "content": "TiDE is a forecasting model that supports data in a specific time-series format [28]. In forecasting, a covariate is an external variable that influences the target, the quantity to be predicted, but is not the target itself, providing context for the predictive model [39]. Past covariates are obtained in previous time steps, while future covariates are known or estimated inputs affecting future target predictions. This framework aligns well with dynamical systems and multi-step MPC, where control inputs serve as covariates and observations or states represent the target.\n\nIn this work, we defined melt pool temperature (          ) and depth (            \u210e) as the target, and the distances to part boundaries in    and    (    ,     ), laser    position (  ), and laser power (  ) as covariates, as shown in Fig. 7. The relationship between covariates and target is represented by:\n\n\\begin{bmatrix}\n\\hat{x}_{temp,t+1:t+H}\\\\\n\\hat{x}_{depth,t+1:t+H}\n\\end{bmatrix} = TiDE(x_{temp,t-\\tau+1:t}, x_{depth,t-\\tau+1:t},\nd_{x,t-\\tau:t+H-1}, d_{y,t-\\tau:t+H-1},\nz_{t-\\tau:t+H-1}, u_{t-\\tau:t+H-1}). (6)\n\nA key advantage of TiDE is its ability to directly incorporate both past and future covariates, along with past targets, to predict future targets. Unlike most generic sequence-to-sequence models, such as RNNs, GRUs, LSTMs and Transformers, which do not distinguish between covariates and targets and require target masking to align with the input format [27], TiDE maintains full predictive power without leaving any network parameters idle. While static covariates, shown in Fig. 7, are not used in this study, they offer additional flexibility for future work, such as encoding different material properties or part geometry.\n\nWe chose quantile loss [40] as the loss function for our model training because it offers several advantages. First, quantile loss is less sensitive to outliers than mean square error (MSE) loss, making it more robust when dealing with noisy or skewed data. Second, for probabilistic learning, in contrast to Gaussian loss [41] which learns the mean and variance during training, it does not require any assumptions about the underlying data distribution to learn and predict specific quantile levels. Moreover, with quantile prediction, it can directly quantify aleatoric (data) uncertainty without the need for Monte Carlo sampling or Bayesian inference. Although in this work we do not consider the uncertainty quantification of the model, the fast prediction of data uncertainty makes it well-suited for future applications toward robust MPC. In the rest of the work, we used the predicted median (0.5 quantile) as the response prediction. The PyTorch realization for both TiDE and quantile loss are modified from [29] to enable automatic differentiation.\n\nWe generated a total of    = 640, 277 time series segments, each with a length of 100, using    = 50 and    = 50. These were split into training, validation/sets with an 9:1 ratio. The hyperparameters of the multi-variate model (predicting temperature and depth) and training setup are detailed in Table 2. We also trained a uni-variate TiDE for temperature prediction in order to compare with PID controller with only 200 epochs. The TiDE model was trained using the Adam optimizer with a learning rate scheduler that decays the rate by 5% every two steps. Robustness and generality are key priorities during training. Since sensor noise and environmental uncertainties can corrupt past target data, the model must maintain predictive power even when noisy past targets and covariates are used. Additionally, smooth predictions of future states are essential for improving MPC performance, even with noisy historical data. To achieve these improvements, regularization and dropout techniques were applied. For future work, combining experimental and simulation data using the co-teaching method [42] may further improve the model generality to noisy history."}, {"title": "5 Model Predictive Control", "content": "The purpose of implementing MPC in DED is to provide a proactive control policy that mitigates defects when an arbitrary reference trajectory for melt pool temperature is provided. In DED, porosity is the most common and critical defect, directly affecting the mechanical properties of printed parts [1, 43]. There are two main types of porosity: interlayer porosity, typically caused by low dilution, and intralayer porosity, resulting from high dilution. To mitigate these defects, it is recommended to maintain the melt pool depth within a dilution range of 10% to 30% [43]. While offline process optimization can generate melt pool temperature references that improve the properties of the fabricated material, it overlooks constraining the melt pool depth [3, 7]. As a result, it potentially leads to the violation of the suggested dilution range during implementation, resulting porosity defects. In such cases, MPC can prioritize part quality over strict temperature tracking. Even when melting pool depth constraints are considered offline, MPC can still act as a safeguard to ensure these constraints are consistently met. Lastly, although the sensors for in-situ melt pool depth measurements are almost unavailable at the current stage, we assume that the melt pool depth is observable via inference methods [35\u201337]."}, {"title": "5.1 Multi-Step MPC Formulation for DED with Constraints", "content": "The multi-step MPC for melt pool temperature tracking and constraining melt pool depth at time    can be formulated as follows:\n\n\\begin{aligned}\n&\\min_{u_{t:t+H-1}} \\sum_{i=1}^H ||\\hat{x}_{temp,t+i} - x_{ref,t+i}||_Q^2 + ||\\Delta u_{t+i-1}||_R^2\\tag{7a} \\\\\n&s.t. \\quad  \\mathcal{G}_1(x) : \\hat{x}_{depth,t+i} \\geq x_{depth}^{lower}, \\quad \\forall i \\in N[1,H-1] \\tag{7b} \\\\\n&\\mathcal{G}_2(x) : \\hat{x}_{depth,t+i} \\leq x_{depth}^{upper}, \\quad \\forall i \\in N[1,H-1] \\tag{7c} \\\\\n&[\\hat{x}_{t+1}^{temp}, \\hat{x}_{t+1}^{depth}] = TiDE(x_{t+1}^{temp}, x_{t+1}^{depth}, d_{t:t+\\tau}^x, d_{t:t+\\tau}^y, z_{t:t+\\tau}, u_{t:t+\\tau}) \\tag{7d} \\\\\n&u_{t+i} \\in U := \\{ u \\in R \\mid 50W \\leq u_i \\leq 750W \\}, \\tag{7e}\n\\end{aligned}\n\nwhere \u0394    +  \u22121 =     +   \u2212     +  \u22121 represents the differences between two consecutive terms in the designed future laser power. To simplify the notation, we use the superscript   ,    , and    :    to denote the past (   \u2212    :    \u2212 1), the future (   :    +    \u2212 1), and the past and future (   \u2212    :    +    \u2212 1), respectively.\n\nThe MPC objective function includes the mismatch between the predicted future melt pool temperature and the reference trajectory, represented by the sum of square error, as well as the control effort, represented by the sum of \u0394    +  \u22121. The two types of loss are balanced by the weighting matrices Q = I   and R = 10I   in this work. The constraints involve maintaining the melt pool depth within bounds, specifically   \n    \n          \u210e = 0.075 mm and   \n    \n          \u210e = 0.225 mm. These constraints are enforced only after the fourth layer, as the melt pool depth from the GAMMA simulation remains below 10% dilution due to the substrate\u2019s boundary conditions during the first three layers. Additionally, the constraints are not considered near corners, i.e.,      \u2264 2 mm \u2229      \u2264 2 mm, due to the unavoidable heat accumulation. The trained TiDE model is embedded into the MPC as the prediction model providing x\u02c6  \n  \n, and requires only one forward pass to generate the full prediction over the defined horizon."}, {"title": "5.2 Optimization Setup", "content": "Several techniques have been implemented in this work to accelerate the solving process in real-time optimization. First, the gradient computation is handled using automatic differentiation through PyTorch\u2019s autograd, enabling efficient calculation of first-order derivatives without relying on numerical approximations [44]. For optimization, we employed the l-bfgs algorithm [45], implemented in the PyTorch-minimize package [46], which offers a balance between performance in large-scale optimization and computational efficiency, as it avoids the need for second-order derivative calculations (i.e., Hessians). Further, we employed a warm-start strategy in each MPC step to accelerate the solving process, using the optimal solution from the previous step as the initial guess for the current one.\n\nNext, the constrained optimization problem in MPC is reformulated as an unconstrained optimization problem using the augmented Lagrangian method, which transform constraints into penalty [45]. This approach avoids the complexity of explicitly managing constraints (i.e., satisfying optimality conditions), and also enables the optimization to proceed in a smooth, continuous space, enhancing the solver\u2019s efficiency. Even so, implementing the augmented Lagrangian method will still increase the complexity of the objective function, and thus the optimizer might fail to terminate successfully. To maintain the feasibility of MPC, when the solver terminates unsuccessfully, we resolve the problem using the default initial starting point instead of warm-start."}, {"title": "5.3 Execution of MPC with GAMMA", "content": "Our pipeline for integrating MPC with DED in GAMMA simulation is shown in Fig. 8. Due to the requirement of past covariate and target for TiDE, we first simulate GAMMA in an open-loop manner, then let MPC take the rest of the process using closed-loop control as the required past targets are collected. The MPC updates the control action (the first element of the optimal control sequence u\u2217) every five GAMMA simulation timestep (i.e., 0.0355 sec/iter for MPC and 0.0071 sec/iter. for GAMMA). In other words, the GAMMA will simulate the fabrication using the same control input throughout five simulation steps.\n\nSince the step size of the laser nozzle toolpath does not match the element size, the extracted melt pool temperature and depth exhibit significant fluctuations and need to be filtered. These fluctuations are primarily periodic, arising from the mismatch between element size and scanning rate. To address this, we average the melt pool temperature and depth extracted from the GAMMA simulation x\n\nover the past 10 simulation steps as the measurement for the current MPC step x\n     \n, i.e., x\n      \n\n= (x\n           \n\n+ x\n           \n\n\u22121 + \u00b7 \u00b7 \u00b7 + x\n           \n\n\u22129)/10, where    denotes the MPC step and    the GAMMA simulation step. This approach effectively reduces fluctuations in the extracted data from GAMMA."}, {"title": "5.4 Bench Marking: PID controller", "content": "To evaluate the performance of the proposed MPC framework, we implemented a PID controller for melt pool temperature tracking as a benchmark, following [47]. Since the PID controller is primarily suited for single-input, single output (SISO) systems and lacks intrinsic mechanisms to handle constraints explicitly, the comparison focuses solely on temperature reference tracking, with the melt pool depth constraint ignored for this benchmarking. The control input provided by the discrete-time PID controller can be obtained by:\n\nu_k = u_{k-1} + K_p e_k + K_i \\Delta t \\sum_{i=1}^{k} e_i + K_d \\frac{e_k - e_{k-1}}{\\Delta t}, (8)"}, {"title": "6 Results", "content": null}, {"title": "6.1 Model Evaluation", "content": "In this section, we evaluate the TiDE model using the GAMMA simulation results with unseen laser power inputs as the ground truth for future targets. A local comparison between the TiDE predictions and the test set is illustrated in Fig. 9. To emphasize the model performance, two challenging scenarios during a single printing process\u2014layer transitions and turning at corners\u2014are highlighted in Fig. 9(a)(b) and Fig. 9(c)(d), respectively, where the melt pool temperature and depth show significant rises and drops. In Fig. 9(a), both the uni-variate and multi-variate TiDE models successfully capture the changes in melt pool temperature after the laser is turned off and then reactivated while it starts printing a new layer. Similarly, Fig. 9(b) demonstrates that the TiDE model accurately tracks the dynamics of the melt pool depth during this transition phase.\n\nFurthermore, Fig. 9(c)(d) highlights how the TiDE models capture the temperature and depth dynamics when the laser nozzle turns at a corner, where heat tends to accumulate due to the change of laser speed and direction. The discrepancy between the predicted and actual temperatures at the corner is within the range of 5 to 15 K, as shown in Fig. 9(c). Notably, the TiDE models provide smoother predictions for temperature and depth compared to the more fluctuating ground truth values.\n\nOn a global scale, we assess the model\u2019s accuracy using the test set. The mean absolute percentage error (MAPE) and relative root mean square error (RRMSE) for melt pool temperature prediction are 1.29% and 0.054, respectively, for the uni-variate TiDE model, and 1.24% and 0.0515 for the multi-variate model. Additionally, for depth prediction, the multi-variate TiDE model achieves a MAPE of 4.25% and an RRMSE of 0.0441, indicating the high accuracy of the model.\n\nFinally, we present the loss history for both uni-variate and multi-variate TiDE models in Fig. 9(e)(f). The validation loss consistently converges without signs of overfitting, despite the large fluctuations in the training loss. These validations confirm that the TiDE model is accurate and reliable for use in model predictive control (MPC) applications."}, {"title": "6.2 Melt Pool Temperature Control using MPC", "content": "We first demonstrate the implementation of the proposed MPC for tracking an arbitrary melt pool temperature reference, comparing its performance against a PID controller as a benchmark. Fig. 10(a) illustrates the complete temperature trajectory over 10 printed layers, comparing PID and MPC using uni-variate TiDE prediction during MPC. Data from the layer transition phases, where the laser is turned off, has been removed for clarity. Although TiDE predicts the entire horizon at each iteration, we only display the first step to simplify visualization. The MPC clearly produces a smoother trajectory than the PID controller, with TiDE predictions closely following the MPC trajectory, demonstrating the accuracy of the model.\n\nThis advantage of MPC is further reflected in the applied laser input shown in Fig. 10(b). While both MPC and PID controllers apply similar trends in laser power, MPC results in smoother inputs, reducing fluctuations. Notably, MPC prevents the peak laser power observed at the beginning of each layer in the PID controller, thanks to its ability to anticipate the rise in melt pool temperature.\n\nThree key scenarios from the printing process are highlighted: MPC\u2019s takeover at the start (Fig. 10(c)), corner transitions on each layer (Fig. 10(d)), and the start of a new layer (Fig. 10(e)). In Fig. 10(c), MPC demonstrates superior reference tracking compared to PID which exhibits more fluctuations. In Fig. 10(d), MPC minimizes overshoot at corners, as it accounts for the learned dynamics, whereas PID produces larger tracking errors, resulting in greater overshoots. Similarly, Fig. 10(e) shows a more pronounced overshoot in the PID controller, while MPC maintains a smoother trajectory, leveraging its predictive capabilities for better reference tracking.\n\nIn all scenarios, the predictions from the TiDE model align closely with the MPC trajectory, further verifying the model accuracy. The   2 values for the MPC and PID trajectories, compared to the reference, are 0.9907 and 0.9827, respectively, indicating competitive performance. In conclusion, the developed MPC delivers performance comparable to PID while significantly reducing overshoots and smoothing control inputs."}, {"title": "6.3 Melt Pool Temperature Control with Melt Pool Depth as Constraints", "content": "We further demonstrate MPC\u2019s constraint-handling capability to prevent defects by keeping the melt pool depth within the 10%-30% dilution range. Figure 11(a)-(c) compares the melt pool temperature, depth, and laser power trajectories, respectively, for constrained and unconstrained MPC. The gray dashed boxes are the region for closed examination, detailed in Fig. 11(d)-(e).\n\nIn Fig. 11(a), the temperature profiles are nearly identical when the constraints are relaxed, with minor differences caused by GPU computation randomness. When constraints are enforced, constrained MPC sacrifices certain performance of reference tracking to satisfy the constraints, leading to a significant deviation from the reference. Despite the fluctuations induced by constraint enforcement, the TiDE prediction accurately captures the system response. In Fig. 11(b), constrained MPC effectively bounds the melt pool depth, with only minor violations, primarily at the start of a new layer, where some violation is inevitable. In contrast, unconstrained MPC maintains temperature tracking via the multi-variate TiDE model, but the melt pool depth exceeds 30% dilution after the fifth layer.\n\nOne consequence of constraint handling is the increased fluctuations in laser power, as shown in Fig. 11(c). Unlike the smooth trajectory of unconstrained MPC, constrained MPC continuously adjusts the laser power to balance constraint satisfaction and reference tracking. Additional fluctuations arise from occasional unsuccessful MPC solutions, where the system either reuses the previous iteration\u2019s solution or restarts with the default initial guess. This issue is exacerbated by the penalty method\u2014if MPC begins in an infeasible region, penalties can increase sharply, presenting a challenge in finding feasible solutions.\n\nWe also select two segments of the trajectories for detailed examinations. The first region, shown in Fig. 11(d), demonstrates that the trajectories in layer 5, where we can observe how MPC leverages tracking performance and constraint satisfaction. This segment also includes two corners where both the melt pool temperature and depth change dramatically due to the change of laser speed and direction. When the laser nozzle is entering the corner, although the penalty is ignored locally, the MPC still lowers the laser power to prevent violating depth constraints. As MPC senses that the margin exists between the current melt pool depth and the constraint, the laser power increases so that the tracking error can be reduced. In Fig. 11(e), we show the segment at the transition between layers 6 and 7. For unconstrained MPC, the controlled melt pool temperature follows the reference well, while the melt pool depth is frequently above 30% dilution. In contrast, for constrained MPC, as the depth reaches the feasible region, it barely exceeds 30% dilution, showing the effectiveness of enforcing depth constraints. Note that the melt pool depth in our simulation will always start from negative values as a"}, {"title": "6.4 Computational Time", "content": "The histogram of solving times for both constrained and unconstrained MPC, specifically during the solving process, is shown in Fig. 12, computed using an AMD Ryzen Threadripper PRO 3975WX 32-Cores CPU. For unconstrained MPC, the mean solving time is 0.2575 seconds, with a maximum of 0.5437 seconds. In contrast, the solving time for constrained MPC, i.e., the solving time using the warm start plus the backup initial guess if necessary, yields an average of 0.2775 seconds. However, as can be seen from Fig. 12, the distribution of the solving time has a long tail with 0.3% of the data greater than 1 second, showing that the bound of the solving time is hard to determine. This highlights the fact that, even with the augmented Lagrangian method to handle constraints, the number of iterations and solving times increase as compared to unconstrained problems, and it also introduces some instability to the solver, increasing the variation of solving time."}, {"title": "7 Summary and Future Work", "content": "In this work, we introduce a multi-step MPC framework using time-series DNN as an embodiment of real-time decision-making for Digital Twins in autonomous manufacturing. This framework utilizes a data-driven time-series model, TiDE, to predict future states required for MPC in one shot, then implement gradient-based optimization for MPC. Although we focus on DED as an example in this work, the demonstrated nonlinear system identification with TiDE and the multi-step MPC framework can be seamlessly generalized for other manufacturing systems. While using DED as the case study, our method leverages the strengths of TiDE, such as its ability to handle both past and future covariates and fast prediction, making it well-suited for the dynamic nature of DED and MPC controllers, respectively. Through rigorous validation using a single-track multi-layer square geometry, we demonstrate the accuracy and reliability of the TiDE model in predicting melt pool features. The results show that the proposed MPC effectively handles melt pool depth constraint, which is a challenging task for the PID"}]}