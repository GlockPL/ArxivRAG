{"title": "WaveGNN: Modeling Irregular Multivariate Time Series for Accurate Predictions", "authors": ["Arash Hajisafi", "Maria Despoina Siampou", "Bita Azarijoo", "Cyrus Shahabi"], "abstract": "Accurately modeling and analyzing time series data is crucial for downstream applications across various fields, including healthcare, finance, astronomy, and epidemiology. However, real-world time series often exhibit irregularities such as mis-aligned timestamps, missing entries, and variable sampling rates, complicating their analysis. Existing approaches often rely on imputation, which can introduce biases. A few approaches that directly model irregularity tend to focus exclu-sively on either capturing intra-series patterns or inter-series relationships, missing the benefits of integrating both. To this end, we present WaveGNN, a novel framework designed to directly (i.e., no imputation) embed irregularly sampled multi-variate time series data for accurate predictions. WaveGNN uti-lizes a Transformer-based encoder to capture intra-series pat-terns by directly encoding the temporal dynamics of each time series. To capture inter-series relationships, WaveGNN uses a dynamic graph neural network model, where each node repre-sents a sensor, and the edges capture the long- and short-term relationships between them. Our experimental results on real-world healthcare datasets demonstrate that WaveGNN con-sistently outperforms existing state-of-the-art methods, with an average relative improvement of 14.7% in F1-score when compared to the second-best baseline in cases with extreme sparsity. Our ablation studies reveal that both intra-series and inter-series modeling significantly contribute to this notable improvement.", "sections": [{"title": "Introduction", "content": "The widespread use of sensors across various sectors, in-cluding healthcare, finance, astronomy, and urban applica-tions [1-18], has led to a significant increase in the collec-tion of time series data, ranging from univariate to multi-variate types, where multiple measures are recorded simul-taneously. Effectively modeling and analyzing these time series datasets is crucial for downstream applications such as monitoring patient health, optimizing treatments, and man-aging disease outbreaks, where accurate and timely data in-terpretation can significantly impact outcomes. In real-world applications, though, time series often exhibit irregularities, frequently caused by sensor malfunctions, different sampling rates across sensors, and cost-saving measures [19]. This vari-ability in data collection renders traditional machine learning (ML) approaches, which typically rely on a fixed number of regularly sampled observations across all sensors, ineffective for accurate predictions [19, 20].\nModeling irregularly sampled time series can be complex due to several factors: measurements from different sensors may not align, data may be missing, and each measurement sequence may vary in size [19,21,22]. To address these chal-lenges, recent developments have concentrated on creating specialized algorithms and model architectures designed to effectively manage these irregularities [23].\nOne category of approaches tackles irregularity by filling in the missing values, thereby converting the datasets into regularly sampled time series. These imputation techniques vary from traditional hand-crafted imputation methods [24] to more advanced learned interpolation methods [21,25]. Once the missing values are filled, the resulting regular time series are utilized to perform the downstream predictions [26-28]. While these techniques address data gaps, they introduce biases and distortions in the data distribution. Furthermore, the absence of measurements can itself provide valuable insights; for instance, periodically missing heart rate data might indicate that a patient underwent a medical procedure during which the monitoring device was removed.\nAnother category of approaches focuses on directly learn-ing from the irregularities by designing specialized model architectures [19\u201321, 29]. For instance, the GRU-D model in-corporates a decaying mechanism within gated recurrent units (GRUs) to better handle irregular sampling intervals [29], while mTAND utilizes multi-time attention to analyze non-uniform measurements effectively [21]. These techniques predominantly capture the correlations within a single sen-sor's measurements over time, focusing on intra-series dy-namics. Meanwhile, other methods have been proposed to capture inter-series dependencies, modeling explicitly the relationships across sensors within the dataset [19].\nDespite these advancements, most approaches tend to spe-cialize in either capturing intra-series dynamics or inter-series relationships, but rarely both simultaneously. Yet, effectively modeling both types of relationships must lead to more accu-rate predictions and deeper insights. For example, understand-ing both the individual progression of patients' vital signs (e.g., heart rate) and how they interact with one another (e.g., the co-dependencies between heart rate and blood pressure over time) can provide a holistic view of a patient's condition,"}, {"title": "Related Work", "content": "Approaches for handling irregularities in multivariate time se-ries vary widely. Temporal Discretization-based Embedding (TDE) methods transform irregular time series into regular forms through imputation [26]. Such approaches enable the use of standard deep learning models but inherently introduce biases and distort data distributions [24, 27, 28]. To address their shortcomings, approaches such as IP-Net and DGM2 proposed using kernel functions to interpolate data against ref-erence points to achieve temporal alignment [25,30]. Shukla et al. further proposed a time attention mechanism with time embeddings to learn interpolation representations [21], while Zhang et. al [26] introduced a hybrid model that combines different TDE methods and integrated hand-crafted imputa-tion embeddings into learned interpolation embeddings to improve medical predictions [26]. These approaches still fun-damentally rely on imputation, retaining the inherent draw-backs of TDE methods.\nAnother group of approaches model irregularity directly through specialized algorithms or model architecture designs. To that end, GRU-D employs decaying mechanisms within GRUs to handle the irregular sampling of the data [29]. SeFT applies set function learning to time series classification, treating the series as a set of observations rather than a se-quence [20]. Additionally, Raindrop employs graph neural networks to conceptualize multivariate time series as sepa-rate sensor graphs, capturing complex inter-variable relation-ships [19]. While these approaches avoid the TDE drawbacks, they often require extensive customization and typically focus on capturing either intra-variable dynamics or inter-variable dependencies, but not both, potentially overlooking essential data patterns. In contrast, WaveGNN introduces a more inte-grated approach, utilizing a standard transformer architecture with masking and time encoding to effectively model intra-series relationships and a graph neural network architecture to capture short and long-term inter-variable dependencies simultaneously. We evaluate WaveGNN against the afore-mentioned approaches in Section 5.2.\nGraph-based approaches have been extensively applied to model multivariate time series datasets. As an example of dy-namic graph-based approaches, BysGNN creates a dynamic multi-context graph for forecasting visits to points of interest (POIs) based on spatial, temporal, semantic, and taxonomic contexts [6]. Similarly, NeuroGNN, captures multi-context correlations in EEG data for seizure classification [4]. These approaches are designed for regular time series data and rely on having external context information to accurately infer inter-series relationships. In contrast, WaveGNN models the inter-series relationships given irregular multivariate time series without requiring additional context."}, {"title": "Preliminaries", "content": "Definition 1 (Time Series Observation) A time series ob-servation, s, refers to a single measurement recorded by sensor v at timestamp t for sample i. Sequentially recorded observations over time form a time series si,v for that sample and sensor.\nDefinition 2 (Irregular Multivariate Time Series Dataset) Let $D = \\{(S_i, t_i, P_i, Y_i)\\}_{i=1}^N$ be a dataset where each Si is an irregularly sampled multivariate time series for the i-th sample, $t_i = \\{t_{i,v}\\}_{v=1}^n$ are the corresponding timestamps, $Y_i \\in \\{1, ..., C\\}$ is the associated categorical outcome label, with C representing the total number of classes, and $p_i$ is an optional set of static features (e.g., patient demographics). Each Si consists of multiple time series, one for each sensor v. Specifically, $S_i = \\{S_{i,v}\\}_{v=1}^n$, where si,v is the time series of observations recorded by sensor v for sample i. The timestamps in ti,v are irregularly spaced, meaning the intervals between consecutive timestamps can vary.\nProblem Definition (Irregular Multivariate Time Series"}, {"title": "Methodology", "content": "Figure 1 presents the end-to-end pipeline of WaveGNN. In the first step, WaveGNN allocates nodes for each sensor, ini-tializing their states using static features from the dataset, like patient demographics, if available, or randomly if not. Next, each sensor's irregular observation sequence is embed-ded using a customized Transformer encoder. This encoder employs masked attention, modified temporal encoding, and a decay mechanism to handle irregularities and capture cor-relations within each sensor's sequence. The node states are updated based on these sequence embeddings to represent the latent state of each sensor after the observed window. In the third step, WaveGNN establishes edges that represent the correlations across sensors to effectively utilize inter-series correlations. This allows WaveGNN to compensate for missing observations during the input window by leveraging information from other related sensors to each sensor. These edges are formed based on the updated node states, capturing short-term relationships during the observed window, and on learned global node embeddings, capturing long-term re-lationships derived from the entire training dataset. Next, WaveGNN performs message passing, which updates the node states to incorporate information from these inferred relationships. This process allows each sensor to benefit from the data captured by other sensors, reducing the impact of irregularity. Finally, the combined node states are aggregated into a single graph-level embedding vector, which is then fed into the prediction head to produce the final prediction. The detailed steps are presented in the following."}, {"title": "Sensor Graph Construction", "content": "Initialization. In WaveGNN, the graph initialization step es-tablishes the preliminary graph structure without connecting any nodes. These will later be updated to reflect the relation-ships and state of the given sample during the observed time window. The graph comprises nodes, each for a different sensor involved in measuring a time-dependent feature.\nFor each sample i, each node is initialized with a state vector derived from static features $p_i$ corresponding to the sample (e.g., demographics such as age and gender and clin-ical contexts such as ICU types in the medical downstream tasks). If this information is not available for a task, the vec-tors are initialized randomly. These features are transformed using a two-layer MLP with ReLU activations to create high-dimensional representations that encapsulate the sample's baseline characteristics and provide a sample-specific initial context for the task:\n$Z_i = MLP(p_i)$\nHere, $p_i$ represents the sample i's static features and $Z_i = \\{Z_{i,1}, ..., Z_{i,n}\\} \\in \\mathbb{R}^{n \\times M}$ represents the matrix of initial state vectors for all nodes where M is the embedding dimension.\nObservation Embedding. For sample i, for each sensor v, let the observation at time t be denoted as $s_v^t$. To enhance the expressive power of our model [31], each observation is mapped to a higher-dimensional space using an MLP with tanh activations:\n$h_v^t = MLP(s_v^t)$\nWe use tanh activation to ensure that embeddings are cen-tered around zero, which helps in capturing the inconsisten-cies in the observations. For instance, two observations that are inconsistent with each other can result in embeddings that are oppositely signed.\nTemporal Encoding. To effectively capture the temporal dynamics and patterns within each sensor's time series, we augment the observation embeddings with time information. Inspired by GRU-D [29], we similarly embed the relative timestamps that reflect the time difference between consecu-tive observations for each sensor.\nTo represent these time differences, we apply Time2Vec [32], which encodes the time information into a vector. Time2Vec includes two components: a sinusoidal component that captures periodic patterns, and a non-periodic transformation component that captures non-periodic patterns and time gaps. This dual encoding enables the model to incorporate both regular temporal cycles and irregular time gaps between observations. This turns the time intervals themselves into a valuable source of information rather than merely considering the relative position of observations, as done in the original Transformer [33]."}, {"title": "Graph-based Prediction", "content": "With the updated node states Z and adjacency matrix A, the graph G = (Z, A) for the given window of observations is constructed, encapsulating the latest node states and their inter-relationships. We perform GNN-based message passing on this graph to capture the node relationships within the node embeddings. Afterward, we apply pooling and transfor-mations to obtain a single embedding vector representing all important node embeddings. The final embedding is used to make the prediction.\nMessage Passing. To perform message passing, the graph is passed through a GNN block that utilizes a modified Graph Convolutional Networks (GCN) variant [35]. In this variant, we remove the normalization term and add residual con-nections between the message-passing layers to preserve directed relationships in the graph and mitigate oversmooth-ing [36], respectively. This yields the node embeddings ma-trix $V \\in \\mathbb{R}^{n \\times M'}$, where M' represents the embedding di-mension.\nGraph Embedding. To derive a graph-level representation from the node embeddings, two operations are performed:\n\u2022 Max Pooling: Applied to the node embeddings to extract those significant features, helping to capture the most expressive node attributes relevant in the prediction task.\n\u2022 Transformation: All node embeddings are concatenated into a single vector, which is then transformed via an MLP to integrate information across all nodes.\nFinal Prediction. The final l-dimensional graph-level rep-resentation $g \\in \\mathbb{R}^{l}$ is obtained by first concatenating the outputs of the max pooling layer and the MLP-transformed concatenated embeddings, followed by an MLP layer for dimensionality reduction.\nFinally, the graph-level embedding g is utilized for the downstream task of interest. In our approach, we pass this through another MLP to predict the label of the samples:\n$\\hat{y} = MLP(g)$\nWhere y represents the predicted class probabilities."}, {"title": "Experiments", "content": "Datasets and Evaluation Setup. We evaluate WaveGNN on four real-world healthcare datasets. P12 [37] and MIMIC III [38] contain ICU patient data, with labels indicating survival during hospitalization (48-IHM). MIMIC III also includes labels for multilabel phenotype classification (24-PHE). P19 [39] focuses on sepsis detection, while PAM [40] is an activity monitoring dataset with labels for different activities. P12, P19, and MIMIC III are naturally irregular datasets, while irregularity in PAM is introduced artificially."}, {"title": "Ablation Study", "content": "We created different variations of WaveGNN to understand the effectiveness of its different components: w/o short-term and w/o long-term variants remove the As and AL similarity matrices, so that the short- and long-term inter-series relation-ships are not considered, respectively. The w/o inter-series variant excludes the graph message passing part, so neither of the inter-series similarity measures are considered, while the w/o intra-series variant removes the transformer-based module and simply aggregates the observation embeddings for each sensor to obtain a univariate sequence embedding. Finally, the w/o temporal encoding variant discards the tem-poral encoding module and uses a simple positional encoding for the Transformer, and the w/o decay rate variant removes WaveGNN's decay mechanism, aggregating the output se-quence embedding of the Transformer with equal weights, thus treating all observations in a sequence equally."}, {"title": "Conclusion", "content": "In this study, we introduced WaveGNN, a novel framework designed to directly embed irregularly sampled multivari-ate time series datasets for accurate predictions. Unlike pre-vious imputation-based and specialized model approaches, WaveGNN simultaneously captures both intra-series corre-lations and inter-series relationships, enabling the model to effectively compensate for missing observations. Our evalua-tion demonstrated that WaveGNN outperforms state-of-the-art baselines across diverse datasets, even under conditions of extreme sparsity where entire sensors are missing, thereby validating the importance of each proposed component within the framework. In the future, we plan to expand WaveGNN to support multimodal scenarios."}]}