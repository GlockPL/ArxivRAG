{"title": "From Text to Insight: Leveraging Large Language Models for Performance Evaluation in Management", "authors": ["Ning Li", "Huaikang Zhou", "Mingze Xu"], "abstract": "This study explores the potential of Large Language Models (LLMs), specifically GPT-4, to enhance objectivity in organizational task performance evaluations. Through comparative analyses across two studies, including various task performance outputs, we demonstrate that LLMs can serve as a reliable and even superior alternative to human raters in evaluating knowledge-based performance outputs, which are a key contribution of knowledge workers. Our results suggest that GPT ratings are comparable to human ratings but exhibit higher consistency and reliability. Additionally, combined multiple GPT ratings on the same performance output show strong correlations with aggregated human performance ratings, akin to the consensus principle observed in performance evaluation literature. However, we also find that LLMs are prone to contextual biases, such as the halo effect, mirroring human evaluative biases. Our research suggests that while LLMs are capable of extracting meaningful constructs from text-based data, their scope is currently limited to specific forms of performance evaluation. By highlighting both the potential and limitations of LLMs, our study contributes to the discourse on Al's role in management studies and sets a foundation for future research to refine Al's theoretical and practical applications in management.", "sections": [{"title": "Theoretical Concept and Its Measurement in Organizational Research", "content": "In organizational research, understanding the variances in employee performance stands as a cornerstone, encompassing a broad spectrum from productivity and creativity to prosocial and deviant behaviors. Traditionally, this endeavor has largely depended on evaluations conducted by human observers\u2014leaders and coworkers\u2014who assess the performance of focal employees. While necessary, this approach is not without its challenges. Primarily, it suffers from subjectivity and bias, relying on recollections of past behaviors over extended periods. Moreover, evaluations often represent aggregated perceptions across diverse situations, mostly derived from a single source of rating, typically a direct supervisor. Such methodologies are inherently limited in their ability to capture the nuanced contributions of knowledge workers accurately.\nRecognizing these limitations, there's a growing acknowledgment of the value inherent in knowledge-based performance outputs within management research. These outputs, which timely capture the contributions of knowledge employees, offer a window into the granular aspects of performance. For instance, within the domain of individual creativity, there has been a shift away from evaluating aggregated creative behaviors towards a finer grained analysis that zooms in on concrete, specific creative outputs\u2014such as detailed plans, innovative ideas, and prototypes. This nuanced approach of evaluating creativity at the idea level has not only moved the field forward but also exemplified the broader utility of knowledge-based data in management research. By adopting this detailed analytical lens, we gain a more refined and precise understanding of key management constructs, showcasing the extensive applicability of text-based information to capture the meaningful constructs in organizational research beyond creativity. However, the logistical challenges of processing and extracting theoretically relevant constructs from the voluminous text-based outputs generated by knowledge workers have traditionally been daunting. The labor-intensive, time-consuming, and costly nature of this process has often limited the use of human raters to laboratory settings or small-scale investigations. While previous Natural Language Processing (NLP) techniques show some promise in extracting text information for applications like sentiment analysis, they frequently struggle with the complex nature of management constructs such as performance, novelty, and creativity. Not only do these techniques necessitate extensive human involvement for pre-labeling and training, but they are also inherently restricted to a narrow scope of concepts."}, {"title": "LLMs in Organizational Research", "content": "Unlike traditional AI systems that relied heavily on specific, pre-defined tasks and structured datasets, large language models (LLMs) such as GPT-4 represent a significant advancement. These generative Al models can engage in tasks without explicit prior training\u2014a phenomenon known as zero-shot learning. For instance, when tasked with extracting creative performance from text data, these models can discern relevant information and provide ratings based on clearly defined criteria for creativity, mirroring human judgment.\nIn organizational research, human raters and experts have traditionally been integral to research processes. They evaluate and code content to understand various constructs. However, LLMs show significant potential to supplement human raters due to their emerging human-like abilities. These abilities are evident in two main ways: First, LLMs can act without extensive pre-training, requiring only clear definitions and explanations, akin to the instructions given to human raters such as research assistants. This reliance on prompts ensures that LLMs can adapt to various tasks with minimal setup, enhancing their validity and flexibility in research applications. Second, LLMs introduce a level of randomness in their responses, similar to the variability seen in human judgments. This randomness can be adjusted through the \u201ctemperature\" setting of the model. A higher temperature results in more varied and creative responses, while a lower temperature yields more deterministic and focused outputs. This adjustability allows researchers to fine-tune the balance between consistency and variability, aligning the model's behavior with specific research needs.\nThe core human-like capabilities of LLMs make them promising tools in organizational research. For example, they can perform tasks such as evaluating the novelty and usefulness of ideas, which is the focus of our study. For example, a recent study exploring the use of LLMs as substitutes for human participants in marketing research found that LLM-generated outputs closely matched those from human surveys, with agreement rates exceeding 75% for both brand similarity measures and product attribute ratings. Therefore, we expect that an LLM can assess a task output like product naming for its novelty and usefulness by referencing defined criteria, providing insights directly relevant to theoretical constructs of creativity and effectiveness. While the potential of LLMs in these applications is promising, their viability as reliable tools for performance evaluation in organizational research requires thorough examination against well-established methods. Specifically, we need to compare LLM-based evaluations with those conducted by human evaluators, particularly multiple human raters who serve as proxies for ground truth. This comprehensive examination will assess the extent to which LLM evaluations are consistent with human judgments, highlighting their strengths and identifying any weaknesses. In the following section, we develop specific hypotheses to systematically evaluate the abilities of LLMs in this domain."}, {"title": "LLMs and Performance Evaluation Accuracy", "content": "The potential capabilities of LLMs like GPT-4 in generating human-like ratings stem from their extensive training on vast corpuses comprising trillions of words. This extensive training endows these models with a knowledge base that far exceeds that of any single expert or group of human raters, equipping them with the ability to grasp a broad spectrum of complex concepts with a level of nuance and depth that traditional methods may not match. Empirical evidence from various tests, such as writing assessments and idea generation challenges, suggests that LLMs can effectively understand and apply evaluation criteria similar to human evaluators. When provided with explicit standards of evaluation, LLMs can accurately comprehend both the criteria and the task outputs. This allows them to conduct evaluations that reflect a deep understanding of what constitutes good performance versus poor performance. Because LLMs are pre-trained on human language data, they are able to interpret definitions and standards effectively, leading to evaluations that should be comparable to those provided by human raters. Therefore, we propose:\nHypothesis 1 (H1): Evaluations generated by LLMs will be significantly related to evaluations conducted by human raters, demonstrating comparable understanding and application of evaluation criteria.\nBeyond their capability to match human evaluators in understanding criteria, LLMs also offer strengths that may lead to more reliable and consistent assessments. Unlike human raters, LLMs are not subject to fatigue, mood variations, or other idiosyncratic biases that can affect judgment. LLMs can assess each output solely against the defined standards without the influence of previous judgments or personal biases. Research in performance rating has highlighted that a significant portion of variance in human ratings is due to individual idiosyncratic differences, with low to moderate correlation observed between raters.\nFor example, in a meta-analysis, Conway and Huffcutt (1997) reported moderate correlations between different rater sources, with supervisor-peer correlations being the highest (\u03c1 = .34), followed by supervisor-self (\u03c1 = .22) and peer-self (\u03c1 = .19) correlations.\nThe consistency of LLMs means that, when constructs are well-defined, LLMs can provide more reliable assessments. They are less subject to individual errors and inconsistencies compared to human raters. As a result, LLMs are likely to offer more consistent and reliable evaluations, capturing the intended constructs with higher fidelity. Although it might seem obvious that LLMs would be consistent with the same prompt, it's important to note that each response generated by an LLM is independent and can vary due to the inherent randomness in the model's output. This randomness can be controlled to some extent by adjusting the \u201ctemperature\u201d setting, which influences the variability of the responses. Therefore, even with the same prompt, LLMs can produce different outputs, making their consistent performance noteworthy. Thus, we propose:\nHypothesis 2 (H2): Evaluations generated by LLMs will demonstrate a higher degree of consistency and reliability in rating performance outcomes compared to evaluations conducted by individual human raters, showing less susceptibility to idiosyncratic biases.\nMoreover, the inherent randomness programmed into LLMs' algorithms introduces a level of variability in ratings, which could paradoxically enhance their utility. This built-in variance allows LLMs to simulate a range of perspectives, akin to multiple human raters independently evaluating the same output. Individual human raters often exhibit variance in their assessments; however, their independent evaluations of the same object tend to offset random and idiosyncratic variances, converging towards a more accurate and true valuation of the concept at hand, serving as a proxy for ground truth. Specifically, the randomness embedded within LLMs allows for diverse evaluations on the same text output, effectively mimicking the collective judgment of multiple human raters. Consequently, when multiple GPT-generated ratings of the same output are aggregated, they tend to converge towards a more accurate reflection of the evaluated concept's true value. This aggregation of multiple GPT ratings for the same performance reduces the impact of each GPT's biases and measurement errors, aligning more closely with the consensus ratings of multiple human experts.\nHypothesis 3 (H3): Aggregated LLM evaluations will more closely align with the consensus ratings of multiple human experts, serving as a more accurate proxy for the ground truths of the evaluated concepts."}, {"title": "LLMs and Performance Evaluation Biases", "content": "Our results suggest that GPT ratings are comparable to human ratings but exhibit higher consistency and reliability. Despite these advantages, it is crucial to examine whether LLMs are susceptible to the same cognitive biases that affect human raters. Human raters are inherently susceptible to a variety of cognitive biases in performance evaluations, including but not limited to the halo effect, leniency or severity bias, and confirmation bias. Among these, the halo effect stands out as a fundamental and pervasive bias in performance appraisals. For this study, we specifically focus on the halo effect due to its significant impact on performance evaluations.\nThe halo effect is particularly significant due to its pervasive and unconscious nature (Nisbett & Wilson, 1977). It can cause raters to overlook specific weaknesses or strengths, resulting in evaluations that reflect general impressions rather than detailed, attribute-specific assessments. This bias systematically distorts performance appraisals across various contexts, from employee evaluations to consumer choices. Its impact on decision-making can lead to unfair or inaccurate assessments, and it often resists correction even when raters are aware of its influence. These characteristics make the halo effect a crucial aspect to investigate in the context of LLM-generated evaluations, as understanding its potential presence in AI systems could provide valuable insights into the broader implications of cognitive biases in automated decision-making processes.\nLLMs, by contrast, evaluate based on predefined criteria and are designed to assess text-based outputs independently of prior knowledge or impressions of the individual being evaluated. This objective, criteria-based approach should, in theory, make LLMs less prone to the halo effect. Even when presented with background information intended to introduce halo biases, LLMs process this information differently from humans. They do not form holistic impressions of individuals; instead, they analyze and rate each attribute based on the content and context of the provided text, thereby minimizing the risk of one attribute disproportionately influencing the overall evaluation.\nFrom a theoretical perspective, the absence of cognitive shortcuts in LLMs-shortcuts that humans often rely on\u2014should reduce the susceptibility to the halo effect (Tversky & Kahneman, 1974). Cognitive psychology posits that humans use heuristics to simplify decision-making processes, which can lead to systematic biases like the halo effect. LLMs, however, process information systematically and adhere to the criteria set forth for evaluation, without the influence of extraneous factors such as an evaluator's mood, previous interactions with the subject, or irrelevant contextual information. Additionally, LLMs' processing algorithms are designed to focus on specific prompts and tasks. When provided with explicit evaluation criteria, LLMs consistently apply these criteria to the text they analyze, ensuring that their assessments are based on the predefined standards rather than the overall impression introduced by background information. This systematic approach helps mitigate the halo effect because the model does not weigh background information as heavily as humans might. Instead, it evaluates each piece of information within the context of its relevance to the specific criteria being assessed.\nHypothesis 4 (H4): LLM-generated evaluations will exhibit significantly lower susceptibility to the halo effect in performance assessments compared to evaluations conducted by human raters.\nIn addition to the baseline hypotheses, our study recognizes the potential for LLMs to offer more nuanced insights into performance evaluations, warranting further exploration through specific research questions. Without developing a priori assumptions, we seek to explore several critical aspects of LLM evaluations. First, we investigate how the number of LLM-generated ratings impacts the accuracy of performance evaluations and how this relationship is moderated by the temperature setting used during evaluation. This focus aims to understand how varying the number of evaluations and adjusting the model's randomness can mimic the diverse perspectives and collective wisdom of human experts (Hong & Page, 2004). Additionally, we examine the extent to which LLM evaluations reflect contextual biases present in the text, such as framing effects introduced by additional context about the evaluated individual. This exploration will help us refine LLM applications and mitigate unintended biases. Furthermore, we assess how well LLMs distinguish between different evaluative dimensions, such as overall performance, novelty, and usefulness dimensions, compared to human raters, providing insights into their nuanced understanding and evaluation capabilities."}, {"title": "Methods", "content": "Our research uses two distinct datasets to examine and compare the evaluative precision of LLMs against human judgment across varied environments. The first study uses performance outputs from a controlled laboratory setting, where participants, including professionals and students, completed professional tasks. These performance outputs were subsequently evaluated by both human raters and LLMs. This setup allows us to compare LLM evaluations to human evaluations against various benchmarks, providing a controlled environment to assess the accuracy and reliability of LLM-generated ratings.\nTo achieve this comparison, we employed both descriptive and meta-analytical approaches. The descriptive analysis examined the correlation between GPT ratings and human ratings to determine their comparability. Additionally, we conducted a meta-analysis using Hunter and Schmidt (2004) methods to further validate the comparability of GPT evaluations with human raters. This approach allowed us to estimate the true correlation between raters and its variability, providing a robust statistical foundation for our comparisons.\nThe second study transitions to a field setting at a prominent taxi company in China, providing an ecologically valid examination of real-world performance evaluations within the context of a promotional examination. In this study, we replicate our findings using text outputs from a real organizational setting. We also examine potential biases by manipulating the background information of the employees being evaluated to assess the extent to which LLMs exhibit the halo effect. Both studies aim to provide a comprehensive analysis of how LLMs perform relative to human evaluators across different performance tasks, elucidating the strengths and potential limitations of LLMs in performance evaluation."}, {"title": "Study 1", "content": "The first study used performance outputs generated by 130 participants who took part in a laboratory setting to complete professional tasks. The sample included working professionals as well as college students. Each participant completed four tasks, resulting in a total of 520 textual performance outputs. The tasks were designed to assess different professional skills: writing a job search cover letter to evaluate persuasive writing capabilities, creating and justifying new product names to gauge creative thinking, developing solutions for team conflict scenarios to assess interpersonal and conflict resolution skills, and designing an AI-integrated college course curriculum to test innovative thinking in educational contexts.\nThe text-based performance outcomes of these tasks were evaluated on three dimensions: overall quality, novelty, and usefulness, each rated on a scale from one to ten. In later descriptions, we refer to these dimensions as Overall, Novelty, and Usefulness. An online panel of independent judges, each blind to the study's hypotheses and the conditions under which each task output was created, assessed each performance output. Prior to evaluation, the raters were provided brief instructions to clarify the evaluation criteria. Each work was rated by six raters, with a total of 486 human raters initially participating in the evaluation process. Raters who spent an unreasonably short amount of time on the tasks were removed, resulting in a final pool of 382 valid raters. On average, each valid rater provided ratings for approximately 8 performance outputs. The tasks being rated were presented in a random sequence to each rater, which helped eliminate figure, sequence, and other potential confounding factors in the evaluations.\nTo evaluate the internal consistency of the ratings, we calculated Cronbach's alpha for each dimension, which resulted in values of .72 for overall quality, .71 for novelty, and .69 for usefulness. Cronbach's alpha was chosen because it is a widely used measure of internal consistency, indicating how closely related a set of items are as a group.\nFor evaluating performance using LLMs, we employed the OpenAI API (GPT-4 Model, OpenAI, 2023), programmed in Python. Each response was independently rated six times by GPT-4. To ensure a fair comparison, GPT-4 was provided with the same clear evaluation criteria in their prompts as given to the human judges. This method involves no prior training on specific tasks, ensuring that each evaluation adheres to predefined standards similar to those applied in human assessments. Unlike traditional NLP methods that generate deterministic ratings, GPT ratings introduce an element of variability akin to human judgment, where identical inputs might produce varied ratings. This variability is regulated by the \u201ctemperature\u201d parameter in the model, which can be adjusted between 0 and 2; a higher temperature increases randomness. For our comparative analysis between GPT and human ratings, we maintained the temperature setting at the default value of 1. Further investigations were conducted to assess the effects of this inherent randomness on the consistency and accuracy of GPT ratings. Our analysis revealed high reliability of GPT ratings across the three dimensions, with Cronbach's alpha values of .93 for overall quality, .93 for novelty, and .90 for usefulness."}, {"title": "Results", "content": "Comparable GPT Ratings to Human Ratings. Our analysis begins by examining the correlation between single GPT ratings and single human ratings to determine their comparability. The descriptive analysis in Figure 1 shows that the correlation between a single GPT rating and a single human rating (noted as GPT[1]-Human[1]) is .41 for Overall in Task 1. This correlation is comparable to the correlation between two human ratings (noted as Human[1]-Human[1]), which is .35 for Overall in Task 1. The results suggest that individual GPT ratings are as comparable to individual human ratings. In addition, the descriptive analysis in Figure 1 reveals that the correlations between two GPT ratings (noted as GPT[1]-GPT[1]) are higher than the correlations between two human ratings (Human[1]-Human[1]), implying that GPT ratings may have an advantage over human ratings in terms of evaluation consistency. For instance, in Task 1 for Overall, GPT(1)-GPT(1) shows a correlation of .72, which is higher than Human(1)-Human(1)'s .35. Moreover, Figure 1 also illustrates that the correlations between six GPT ratings and six human ratings (noted as GPT[6]-Human[6]) are substantially higher than the correlations for GPT(1)-Human(1), suggesting the potential for improving rating performance through aggregating multiple raters. For example, GPT(6)-Human(6) has a correlation of .70 compared to GPT(1)-Human(1)'s .41 in Task 1 for Overall.\nTo further validate the comparability of GPT evaluations with human raters, we conducted a meta-analysis using Hunter and Schmidt (2004) methods to estimate the true correlation between raters and its variability. Each task output was evaluated by six human raters, allowing us to estimate the correlation between one human rater and another human rater (Human[1]-Human[1]). Similarly, each task was rated independently by GPT-4 six times, enabling us to estimate the correlation between one GPT rating and one human rating (GPT[1]-Human[1]). This approach allowed us to directly compare the comparability of GPT ratings with human ratings.\nEach performance outcome was rated by six human individuals and by GPT-4 six times, generating multiple pairwise correlations. For each outcome, the ratings by human raters resulted in 15 unique correlations (Human[1]-Human[1]). Similarly, the ratings by GPT-4 produced 36 unique correlations with human raters (GPT[1]-Human[1]). These correlations were treated as individual data points in our meta-analysis. By aggregating these correlations across many performance outcomes, we conducted meta-analyses to compute an overall correlation coefficient, providing a more accurate and reliable estimate of the true score correlations. This meta-analytic approach synthesizes multiple correlation estimates, offering a generalized understanding of the comparability between GPT and human ratings.\nThe meta-analysis uses 95% confidence intervals (CIs) to evaluate the correlations. For instance, as Table 1 shows, in Task 1 for Overall, the 95% CI for the correlation coefficient between a single GPT rating and a single human rating (GPT[1]-Human[1]) is [.39, .44], which slightly overlaps with the 95% CI for the correlation between two human ratings (Human[1]-Human[1]), which is [.29, .40]. And in Task 4 for Novelty, GPT(1)-Human(1)'s 95% CI [.38, .43] overlaps with Human(1)-Human(1)'s 95% CI [.30, .42]. The overlap indicates that the comparability of GPT ratings is similar to that of human ratings.\nAdditionally, we compared the correlation of a single GPT rating with the average of five human ratings (noted as GPT[1]-Human[5]) to the correlation of a single human rating with the average of five human ratings (Human[1]-Human[5]). The average scores from five human judges (Human[5]) serve as a proxy for the ground truth, combining multiple independent evaluations to reduce individual biases and provide a more accurate benchmark.\nIn Figure 2, we can observe that the correlations for GPT(1)-Human(5) and Human(1)-Human(5) are comparable in most cases. For instance, in Task 1 for Overall, GPT(1)-Human(5) shows a correlation of .60, whereas Human(1)-Human(5) shows a correlation of .50. Similar trends are observed across other tasks, where GPT(1)-Human(5) often shows slightly higher or comparable correlation coefficients to Human(1)-Human(5). For example, in Task 2 for Usefulness, GPT(1)-Human(5) has a correlation of .42 compared to Human(1)-Human(5)'s .44, and in Task 3 for Overall, GPT(1)-Human(5) shows a correlation of .64 compared to Human(1)-Human(5)'s .55.\nThe meta-analysis confirms this observation, Table 2 indicates that the correlation between GPT(1) and Human(5) is comparable to, and sometimes higher than, the correlation between Human(1) and Human(5). For example, in Task 1 for Overall, the 95% CI for GPT(1)-Human(5) is [.58, .62], which slightly exceeds the 95% CI for Human(1)-Human(5), which is [.44, .57]. And in Task 2 for Usefulness, the 95% CI for GPT(1)-Human(5) is [.42, .47], which is notably higher than Human(1)-Human(5)'s [.19, .32].\nThese findings from both the descriptive analysis and the meta-analysis support Hypothesis 1, affirming that LLM-generated evaluations possess a degree of comparability in rating performance outcomes similar to human ratings. By matching the evaluative precision traditionally expected from human raters, GPT ratings demonstrate their potential as reliable substitutes for human evaluations in performance assessment contexts.\nHigher Consistency and Reliability of GPT Ratings. Our analysis reveals a clear pattern of higher consistency and reliability in single GPT ratings compared to single human ratings across various tasks and scoring dimensions. Figure 1 illustrates these findings, showing that in Task 1, the correlation coefficients for single GPT ratings (noted as GPT[1]-GPT[1]) stand at .72 for Overall, .56 for Novelty, and .69 for Usefulness. These values significantly outperform the correlations for single human ratings, which are .35 for Overall, .32 for Novelty, and .33 for Usefulness. Similar patterns of GPT's superior performance are observed in Tasks 2, 3, and 4, demonstrating a consistent trend of higher reliability in GPT evaluations.\nBuilding on these descriptive findings, the meta-analysis results presented in Table 1 further validate the superior consistency of GPT ratings. The meta-analysis employs 95% confidence intervals (CIs) to assess the statistical significance and reliability of the correlations. For instance, in Task 1 for Overall, the 95% CI for the correlation coefficient for GPT(1)-GPT(1) is [.70, .74], which notably surpasses the [.29, .40] observed for Human(1)-Human(1). This trend of enhanced consistency is evident across Novelty and Usefulness dimensions, where GPT ratings achieve correlations with 95% CIs of [.53, .59] and [.67, .71], respectively, indicating significant improvements over the lower correlations noted in human ratings, which are [.26, .39] for Novelty and [.29, .37] for Usefulness.\nThese findings suggest that GPT ratings exhibit lower variability and thus provide more consistent evaluations compared to individual human raters. This enhanced consistency and reliability support Hypothesis 2, affirming that GPT-generated evaluations demonstrate a higher degree of consistency and reliability in rating performance outcomes compared to human ratings.\nAggregating of multiple GPTs. Both Figure 3 and Table 3 collectively demonstrate that while increasing the number of evaluators generally enhances correlation scores for both human and GPT ratings, GPT consistently achieves higher correlations at each level. This trend indicates that for both GPT and human, idiosyncratic variances can be offset by increasing the number of raters. These findings underscore GPT's capability to minimize individual idiosyncratic errors and enhance reliability, making it particularly suited for scenarios demanding high precision and consistent evaluations.\nFigure 4 illustrates that GPT(6) achieves a reliability measure that adheres to established standards, where correlations greater than .5 are considered strong and those above .7 as very strong. For example, in Task 1, GPT(6) shows high correlations with the ground truths for Overall, Novelty, and Usefulness at .70, .60, and .69, respectively. These strong correlations indicate that aggregated GPT ratings align closely with the proxy ground truths provided by multiple human raters. Furthermore, GPT ratings exhibit lower idiosyncratic variance, leading to diminishing returns when increasing the number of GPT raters compared to human raters. The meta-analysis in Table Al of the Appendix demonstrates that fewer GPT raters are required to achieve an acceptable level of agreement compared to human raters. For instance, while the 95% CI for Human(1)-GPT(6) starts at [.41, .53], increasing human raters to five enhances this range to [.65, .72]. Conversely, starting from a higher baseline, a single GPT rater's 95% CI of [.57, .66] quickly extends to [.64, .69] with just one additional GPT rater, indicating a more rapid convergence towards higher reliability.\nOverall, these results provide support for Hypothesis 3. The evidence demonstrates that aggregated GPT evaluations more closely align with the consensus ratings of multiple human experts, serving as a more accurate proxy for the ground truths of the evaluated concepts. This underscores the effectiveness of GPT ratings in producing reliable and precise evaluations, confirming their potential as robust alternatives to traditional human assessments.\nAdditional Analyses\nThe discriminative validity of GPT ratings compared to human ratings. To further understand the performance of GPT ratings, we conducted an analysis to evaluate their discriminative validity compared to human ratings. This analysis aimed to determine whether GPT raters could differentiate between score dimensions more effectively than human raters, which is crucial for ensuring nuanced and accurate evaluations in performance assessment.\nWe found that GPT demonstrates a distinct capability to differentiate between score dimensions more effectively than human raters. For each task, we calculated the correlations within each rater (single GPT or single human) across different score dimensions, namely O-N (Overall to Novelty), O-U (Overall to Usefulness), and N-U (Novelty to Usefulness). Meta-analysis was employed to synthesize these results.\nAs detailed in Figure 5 and supported by the data in Table A2 of the Appendix, GPT ratings consistently exhibit lower correlations between dimensions than human ratings across all tasks, indicating a higher discriminative capacity. For example, in Task 1, while human raters show a high correlation range between Overall and Usefulness (O-U) ([.89, .91]), GPT raters exhibit a lower correlation range ([.84, .88]). This pattern of lower correlations in GPT ratings is also evident in Task 2, where the correlation between Novelty and Usefulness (N-U) is notably lower for GPT ([.21, .35]) compared to humans ([.52, .65]). Similarly, in Task 3 and Task 4, GPT continues to demonstrate more variability and lower correlations between different dimensions (e.g., O-U in Task 4 for GPT [.68, .77] vs. Human [.83, .89]). These findings suggest that GPT ratings can better distinguish between the meanings of different score dimensions.\nTrade-off Between Individual Variance and Group Accuracy in GPT Ratings. To explore the implications of using multiple GPT raters, we investigated the relationship between individual variance and the accuracy of GPT ratings. This analysis aimed to understand how the variability introduced by different temperature settings and the number of raters affects the overall accuracy of evaluations, which is essential for leveraging the \u201cwisdom of the crowd\" principle in performance assessments (Surowiecki, 2004).\nPrior research suggests that a diverse group of independent judges typically yields more accurate ratings. Diversity implies greater variance at the individual level. To investigate the relationship between individual variance and GPT rating accuracy, we manipulated the temperature parameter of GPT to introduce varying levels of randomness.\nFigure 6 illustrates the accuracy of GPT ratings across different tasks, scoring dimensions, rater sizes, and temperature settings, using Human(6) as the proxy for ground truth. The results reveal that GPT rating accuracy increases with the number of raters, although each additional rater contributes progressively smaller gains. Temperature also significantly impacts accuracy; notably, lower temperatures (e.g., .05, .25) enhance accuracy in smaller rater groups (e.g., 1, 2), while higher temperatures (e.g., .50, .75, 1.00) prove more effective in larger rater groups (e.g., 5, 6). Moreover, the optimal temperature varies, indicating a nuanced trade-off between randomness and accuracy across different tasks and score dimensions.\nRegression models further confirm this dynamic. Table 4 shows that Rater Size positively influences GPT rating accuracy (b = .035, p < .001 for Overall; b = .039, p < .001 for Novelty; b = .050, p < .001 for Usefulness), suggesting that increasing the number of raters improves accuracy. However, the diminishing returns from additional raters are evident, as indicated by the negative coefficients of the squared term Rater Size*Rater Size (b = -.004, p < .001 for Overall; b = -.005, p < .01 for Novelty; b = -.006, p < .001 for Usefulness). The relationship between temperature and accuracy is significantly negative (b = -.096, p < .001 for Overall; b = -.061, p < .01 for Novelty; b = -.039, p < .05 for Usefulness), yet positively moderated by Rater Size (b = .021, p < .001 for Overall; b = .018, p < .01 for Novelty; b = .018, p < .001 for Usefulness). This suggests that increasing the number of raters can not only mitigate but actually reverse the adverse effects of higher temperature settings, leading to higher accuracy. High temperature settings require a larger number of raters to achieve more accurate evaluations, effectively balancing individual variance and group accuracy.\""}, {"title": "Study 2", "content": "The performance outputs in Study 2 were from an internal promotion selection test conducted by a prominent taxi company in China. As part of the selection process for middle management positions, 112 employees were tasked with responding to two out of fourteen promotion tasks. These tasks, designed by the company's management team, aimed to address prevalent business management challenges, such as identifying reform opportunities for significant issues within the taxi industry, enhancing corporate branding and image, and advancing the digital transformation of the enterprise.\nThis dataset comprised 224 textual responses and served as the basis for comparing performance evaluations within a practical setting. Similar to Study 1, each response was rated by six GPT and six human raters across three score dimensions: Overall, Novelty, and Usefulness (for detailed task descriptions, human ratings, and GPT ratings, refer to Supplementary Materials). We recruited a total of 420 raters participating in the evaluation.\nStudy 2 aimed to replicate the findings of Study 1 in a field setting while also examining prevalent biases in performance evaluations, particularly focusing on halo effects. The real organizational context provided a rich environment for manipulating the employees' prior backgrounds, such as education and prior performance, to create different halo conditions. Specifically, we provided raters with background information about the target employees alongside their performance outputs. This background information was carefully crafted to be theoretically independent of the actual performance outcomes, allowing us to isolate its influence on ratings. We created three variations of this background information: a positive version that presented strongly favorable details about the employee's education, work experience, and past performance; a neutral version that offered a balanced view of the employee's background; and a negative version that included critical information about the employee's history. While maintaining the same underlying content across all versions, we subtly altered the phrasing and specific details, such as the prestige of the employee's alma mater, to create distinct impressions. Our goal was to assess whether the presence of this extraneous background information would sway raters' evaluations of the performance outputs, which ideally should be judged solely on their merits against the established criteria.\nWe created three conditions for both human and GPT ratings. The halo condition consisted of performance outputs randomly assigned one of the halo conditions, with roughly equal numbers receiving positive, neutral, and negative halos. Raters (both human and GPT) were provided both the halo text and the performance output text. The halo mitigation group maintained the same halo distribution but included instructions to the raters to not consider the background information when they rate performance. In the control group, raters were provided only the performance output text for evaluation, without any halo background information\u00b2."}, {"title": "Results", "content": "Replication of Study 1 Findings. Utilizing the dataset from Study 2, we validated the primary findings of Study 1 (for detailed results, please see Appendix\u00b3). First, GPT ratings were comparable to human ratings. Comparing single GPT ratings with single human ratings (GPT[1", "Human[1": "and the correlations between two human ratings (Human[1"}, {"Human[1": "the results showed that GPT evaluations matched the evaluative precision of human raters. For instance, Figure Al of the Appendix showed that, for Overall, GPT(1)-Human(1) had a correlation of .23, and Human(1)-Human(1) had a correlation of .22. The 95% confidence intervals (CIs) for these correlations confirmed the alignment of GPT ratings with human ratings, demonstrating their comparability. For example, for Overall, the 95% CI for GPT(1)-Human(1) was [.21, .25", "25": ".", "GPT[1": "GPT[1"}, {"Human[1": "Human[1", "GPT[1": "Human[5"}]}