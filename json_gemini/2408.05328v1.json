{"title": "From Text to Insight: Leveraging Large Language Models for Performance Evaluation in Management", "authors": ["Ning Li", "Huaikang Zhou", "Mingze Xu"], "abstract": "This study explores the potential of Large Language Models (LLMs), specifically GPT-4, to enhance objectivity in organizational task performance evaluations. Through comparative analyses across two studies, including various task performance outputs, we demonstrate that LLMs can serve as a reliable and even superior alternative to human raters in evaluating knowledge-based performance outputs, which are a key contribution of knowledge workers. Our results suggest that GPT ratings are comparable to human ratings but exhibit higher consistency and reliability. Additionally, combined multiple GPT ratings on the same performance output show strong correlations with aggregated human performance ratings, akin to the consensus principle observed in performance evaluation literature. However, we also find that LLMs are prone to contextual biases, such as the halo effect, mirroring human evaluative biases. Our research suggests that while LLMs are capable of extracting meaningful constructs from text- based data, their scope is currently limited to specific forms of performance evaluation. By highlighting both the potential and limitations of LLMs, our study contributes to the discourse on Al's role in management studies and sets a foundation for future research to refine Al's theoretical and practical applications in management.", "sections": [{"title": "Introduction", "content": "In organizational research, understanding the variances in employee performance stands as a cornerstone, encompassing a broad spectrum from productivity and creativity to prosocial and deviant behaviors (Berry, Ones, & Sackett, 2007; Campbell & Wiernik, 2015; Organ & Ryan, 1995; Schleicher, Baumann, Sullivan, & Yim, 2019; Viswesvaran & Ones, 2000; Zhou & George, 2001). Traditionally, this endeavor has largely depended on evaluations conducted by human observers\u2014leaders and coworkers\u2014who assess the performance of focal employees (Borman, 1991). While necessary, this approach is not without its challenges. Primarily, it suffers from subjectivity and bias, relying on recollections of past behaviors over extended periods (Landy & Farr, 1980; Murphy & Cleveland, 1995). Moreover, evaluations often represent aggregated perceptions across diverse situations, mostly derived from a single source of rating, typically a direct supervisor (Woehr & Huffcutt, 1994). Such methodologies are inherently limited in their ability to capture the nuanced contributions of knowledge workers accurately (Scullen, Mount, & Goff, 2000; Viswesvaran & Ones, 2000).\nRecognizing these limitations, there's a growing acknowledgment of the value inherent in knowledge-based performance outputs within management research (Spector & Pindek, 2016). These outputs, which timely capture the contributions of knowledge employees, offer a window into the granular aspects of performance (Berg, 2016, 2019; Perry-Smith & Mannucci, 2017; Sijbom, Janssen, & Van Yperen, 2015). For instance, within the domain of individual creativity, there has been a shift away from evaluating aggregated creative behaviors towards a finer grained analysis that zooms in on concrete, specific creative outputs\u2014such as detailed plans, innovative ideas, and prototypes (Berg, 2016; Lu, Bartol, Venkataramani, Zheng, & Liu, 2019). This nuanced approach of evaluating creativity at the idea level has not only moved the field forward but also exemplified the broader utility of knowledge-based data in management research. By adopting this detailed analytical lens, we gain a more refined and precise understanding of key management constructs, showcasing the extensive applicability of text- based information to capture the meaningful constructs in organizational research beyond creativity (Carton, Murphy, & Clark, 2014; Maynes & Podsakoff, 2014; Sonenshein, 2010). However, the logistical challenges of processing and extracting theoretically relevant constructs from the voluminous text-based outputs generated by knowledge workers have traditionally been daunting (Short, McKenny, & Reid, 2018). The labor-intensive, time- consuming, and costly nature of this process has often limited the use of human raters to laboratory settings or small-scale investigations. While previous Natural Language Processing (NLP) techniques show some promise in extracting text information for applications like sentiment analysis, they frequently struggle with the complex nature of management constructs such as performance, novelty, and creativity (George, Haas, & Pentland, 2014; Hannigan et al., 2019; Kobayashi, Mol, Berkers, Kismih\u00f3k, & Den Hartog, 2017). Not only do these techniques necessitate extensive human involvement for pre-labeling and training, but they are also inherently restricted to a narrow scope of concepts (Harrison, Thurgood, Boivie, & Pfarrer, 2019; Tonidandel, King, & Cortina, 2018).\nThe development of LLMs like GPT-4 introduces new capabilities in processing nuanced text-based data, potentially overcoming the limitations faced by traditional methodologies (Brown et al., 2020; Vaswani et al., 2017). As a result, those technologies represent a potential transformative shift in how we can approach the extraction of meaningful constructs from textual data for organizational research. With their zero-shot learning capabilities\u2014where models can make predictions on tasks they were not explicitly trained on\u2014these models negate the need for pre-labeled data or extensive training. They require only clear construct definitions and prompts to extract pivotal management concepts\u2014the \u201cwhat\u201d dimension fundamental to theoretical development (Ouyang et al., 2022; Whetten, 1989). Prompts act as instructions to LLMs, allowing them to process relevant information similarly to how humans perform tasks with instructions. This promising frontier, however, raises pivotal questions about the feasibility and efficacy of integrating LLMs into the organizational research toolkit (Shrestha, Ben-Menahem, & von Krogh, 2019).\nFirst, how do the evaluations conducted by LLMs compare with those made by human judges in terms of accuracy and consistency (Brynjolfsson & Mitchell, 2017)? Second, do LLMs replicate known human biases in performance evaluations, such as the halo effect, or do they offer a more objective lens through which we can view employee contributions (Caliskan, Bryson, & Narayanan, 2017; Thorndike, 1920)? To address these questions, we conduct a comprehensive comparison, analyzing two sets of human-generated text-based performance outputs using both LLM and human evaluations. This design not only allows us to examine the relative merits of LLMs versus traditional human assessments but also to examine the nuances of how AI interprets and rates complex work-related constructs.\nOur investigation shows compelling evidence of the feasibility and reliability of LLMs like GPT in assessing performance outputs. The analysis indicates a high degree of convergence between GPT ratings and the aggregated judgments of six human raters, which we use as a proxy for ground truth (Amabile, 1982; Kaufman, Baer, Cole, & Sexton, 2008). Across two studies, we evaluated a total of 520 task outputs in Study 1 and 224 task outputs in Study 2. Each task was assessed by a diverse and competent group of six judges, resulting in a total of 486 raters in Study 1 and 420 raters in Study 2. Notably, GPT demonstrates a remarkable consistency in providing reliable ratings across a spectrum of tasks, even those with varying levels of subjectivity and complexity. This consistency highlights GPT's potential to provide more accurate evaluations than individual human raters, demonstrating its viability as an alternative for performance assessment in organizational research.\nOur investigation shows compelling evidence of the feasibility and reliability of LLMs like GPT in assessing performance outputs. The analysis indicates a high degree of convergence between GPT ratings and the aggregated judgments of six human raters, which we use as a proxy for ground truth (Amabile, 1982; Kaufman, Baer, Cole, & Sexton, 2008). Notably, GPT demonstrates a remarkable consistency in providing reliable ratings across a spectrum of tasks, even those with varying levels of subjectivity and complexity. This consistency highlights GPT's potential to provide more accurate evaluations than individual human raters, demonstrating its viability as an alternative for performance assessment in organizational research.\nMoreover, our findings highlight the precision of GPT evaluations, evidenced by significantly less variance in ratings compared to human assessments. This precision suggests that LLMs can achieve a level of objectivity and reliability in performance evaluation that surpasses traditional human-based methods. However, despite these promising results, our study also uncovers a notable limitation of generative AI: the manifestation of human-like evaluative biases (Nisbett & Wilson, 1977). Specifically, the introduction of halo-effect information systematically influences GPT ratings, mirroring a common bias observed in human performance evaluations. This discovery not only underscores the nuanced complexities of integrating AI into management research but also prompts a critical examination of how such biases might be mitigated.\nBy highlighting the capabilities of LLMs to enhance the analytical tools available to organizational researchers, our study represents a key advancement in the integration of AI technologies in organizational research. This investigation not only contributes to the ongoing discussion on the integration of AI in organizational studies but also sets the stage for future research to build upon our findings, further refining and expanding the use of LLMs in theoretical and empirical inquiries. By leveraging LLMs, we illuminate a path forward for extracting nuanced concepts from text-based data, thereby refining and expanding the ways we measure constructs that are pivotal to management theory.\nThis approach not only enhances the precision of existing measures but also allows the identification and evaluation of previously unmeasurable constructs. For instance, consider the extraction of voice behaviors from meeting transcripts\u2014a task that traditional methods might approach with significant difficulty due to its reliance on subjective interpretations or labor- intensive content analysis (Morrison, 2011). LLMs could dissect these transcripts to accurately identify instances of voice, offering a more precise and objective measure than ever before achievable. Through this inquiry, we pave the way for a novel wave of research that integrates cutting-edge computational techniques with the diverse organizational phenomena."}, {"title": "The Literature Review and Hypothesis", "content": null}, {"title": "Theoretical Concept and Its Measurement in Organizational Research", "content": "In organizational research, key concepts such as leadership, performance, and creativity form the bedrock for building and testing theories (Mathieu, 2016; Whetten, 1989). These concepts are often quantified through psychometric scales or proxy objective data to encapsulate the constructs central to our models and hypotheses (Edwards, 2001; Hinkin, 1995). However, traditional methods of measurement come with limitations, particularly the inability to capture spontaneous, real-time expressions of creativity and performance (Amabile & Pratt, 2016; Fisher & To, 2012). For instance, creativity, defined by the generation of ideas that are both novel and useful, frequently remains an untapped resource in research, with actual ideas seldom making it into empirical analysis (Berg, 2019; George, 2007; Zhou, Wang, Song, & Wu, 2017). Performance, representing the tangible contributions individuals make to their workplace, for knowledge workers, is predominantly manifested through knowledge-based outputs such as ideas, reports, memos, strategic plans, and marketing campaigns\u2014yet, these rich data sources have historically been underutilized in management studies.\nRecent shifts in research focus, for instance, within the domains of creativity and voice, have begun to address limitations in performance evaluation by concentrating on specific outputs rather than aggregated perceptions (Alvesson, 2001; George et al., 2014). This approach significantly broadens the theoretical landscape of performance assessment by leveraging language descriptions to encapsulate complex constructs. Human raters have traditionally served as the primary evaluators of such constructs (Viswesvaran & Ones, 2000). For instance, Berg's (2016) study, which involved using over 10,000 humans to assess idea novelty, exemplifies the extensive efforts to evaluate creative outputs objectively. Similarly, research on voice behaviors has highlighted the importance of specific voice content in organizational settings (Maynes & Podsakoff, 2014). These examples of creativity and voice are special forms of performance that demonstrate a key shortcoming in the literature: a vast array of potentially insightful data remains largely untapped and unevaluated (George et al., 2014; Tonidandel et al., 2018). By focusing on these specific outputs, researchers can gain a more precise understanding of performance, addressing the nuanced contributions of knowledge workers that traditional evaluations often miss (Spector & Pindek, 2016).\nThe advantage of focusing on language-based data lies in their ability to provide a more refined and precise measurement of constructs that are otherwise challenging to quantify (Short et al., 2018). Such an approach not only captures the content directly related to key concepts but also enables the exploration of constructs that are difficult to assess through conventional methods. Despite the availability of data recorded by digital tools and big data technologies, the capacity of humans to process and analyze this information remains constrained by time, resources, and cognitive limitations (George et al., 2014). Thus, this shift towards leveraging unobtrusive data sources aligns with the broader trends in management research, where there is an increasing emphasis on using digital footprints and naturally occurring data to gain insights into organizational phenomena (Hill, White, & Wallace, 2014; Knight, 2018). This approach can significantly expand the ways we measure constructs, offering a more comprehensive and accurate depiction of performance and behavior in organizational settings (Lazer et al., 2009)."}, {"title": "Review of Traditional NLP in Organizational Research", "content": "The exploration of NLP and its applications within organizational research has introduced new methodologies for analyzing the vast quantities of textual data generated by and within organizations (Kobayashi, Mol, Berkers, Kismih\u00f3k, & Den Hartog, 2018). Techniques such as Latent Dirichlet Allocation (LDA) for topic modeling, machine learning (ML) for text classification, and sentiment analysis have been widely adopted, each offering unique insights into organizational phenomena (Blei, Ng, & Jordan, 2003; Hannigan et al., 2019; Liu, 2020). For instance, sentiment analysis, often used to gauge the emotional valence of individuals, categorizes expressions as either positive or negative (Ashkanasy & Dorris, 2017). However, this simplistic classification fails to capture the nuanced spectrum of human emotions, a critical aspect in developing emotion-based theories within workplaces (Ashkanasy, Humphrey, & Huy, 2017). Traditional sentiment analysis is limited in its ability to measure the intensity and complexity of emotions. For example, feelings of anger and sadness can both be intense, yet they signify very different emotional states and require more precise measures to capture the richness of theoretically relevant constructs(Russell, 2003). Consequently, the inadequacy of traditional NLP techniques in capturing these nuances highlights the need for more advanced approaches that align with the detailed dimensions proposed in management theories.\nSimilarly, LDA and other topic modeling approaches have enabled researchers to distill thematic patterns from large text corpora, such as corporate documents or online forums (Corritore, Goldberg, & Srivastava, 2020). However, these methods primarily illuminate the presence of topics without providing deeper insights into the underlying reasons for their association or distinguishing the specific ways in which they differ (Grimmer & Stewart, 2013). This limitation constrains the depth of theoretical insights that can be extracted, hindering the development of theories that require a nuanced understanding of textual content (Schmiedel, Mu, & Brocke, 2018).\nMoreover, while machine learning techniques have shown promise in extracting more complex and abstract theoretical constructs, such as personality traits, the necessity for extensive data labeling and validation processes has limited their practical application (Harrison et al., 2019; Kern, Rogge, & Howlett, 2019). The validity of these ML-based approaches often remains a subject of debate, presenting a barrier to their widespread adoption in organizational research (Tonidandel et al., 2018). Consequently, the potential of NLP in analyzing textual data in management studies faces significant limitations, highlighting a tension between the abundance of rich data and the current methodological toolkit's ability to fully harness it for theory development."}, {"title": "LLMs in Organizational Research", "content": "Unlike traditional AI systems that relied heavily on specific, pre-defined tasks and structured datasets, large language models (LLMs) such as GPT-4 represent a significant advancement. These generative Al models can engage in tasks without explicit prior training-a phenomenon known as zero-shot learning (Brown et al., 2020). For instance, when tasked with extracting creative performance from text data, these models can discern relevant information and provide ratings based on clearly defined criteria for creativity, mirroring human judgment (Bommasani et al., 2021).\nIn organizational research, human raters and experts have traditionally been integral to research processes (Duriau, Reger, & Pfarrer, 2007). They evaluate and code content to understand various constructs. However, LLMs show significant potential to supplement human raters due to their emerging human-like abilities. These abilities are evident in two main ways: First, LLMs can act without extensive pre-training, requiring only clear definitions and explanations, akin to the instructions given to human raters such as research assistants (Wei et al., 2022). This reliance on prompts ensures that LLMs can adapt to various tasks with minimal setup, enhancing their validity and flexibility in research applications. Second, LLMs introduce a level of randomness in their responses, similar to the variability seen in human judgments. This randomness can be adjusted through the \u201ctemperature\" setting of the model (Holtzman, Buys, Du, Forbes, & Choi, 2020). A higher temperature results in more varied and creative responses, while a lower temperature yields more deterministic and focused outputs. This adjustability allows researchers to fine-tune the balance between consistency and variability, aligning the model's behavior with specific research needs.\nThe core human-like capabilities of LLMs make them promising tools in organizational research. For example, they can perform tasks such as evaluating the novelty and usefulness of ideas, which is the focus of our study. For example, a recent study exploring the use of LLMs as substitutes for human participants in marketing research found that LLM-generated outputs closely matched those from human surveys, with agreement rates exceeding 75% for both brand similarity measures and product attribute ratings (Li, Castelo, Katona, & Sarvary, 2024). Therefore, we expect that an LLM can assess a task output like product naming for its novelty and usefulness by referencing defined criteria, providing insights directly relevant to theoretical constructs of creativity and effectiveness (Berg, 2019). While the potential of LLMs in these applications is promising, their viability as reliable tools for performance evaluation in organizational research requires thorough examination against well-established methods. Specifically, we need to compare LLM-based evaluations with those conducted by human evaluators, particularly multiple human raters who serve as proxies for ground truth (Amabile, 1982). This comprehensive examination will assess the extent to which LLM evaluations are consistent with human judgments, highlighting their strengths and identifying any weaknesses. In the following section, we develop specific hypotheses to systematically evaluate the abilities of LLMs in this domain."}, {"title": "LLMs and Performance Evaluation Accuracy", "content": "The potential capabilities of LLMs like GPT-4 in generating human-like ratings stem from their extensive training on vast corpuses comprising trillions of words (Brown et al., 2020). This extensive training endows these models with a knowledge base that far exceeds that of any single expert or group of human raters, equipping them with the ability to grasp a broad spectrum of complex concepts with a level of nuance and depth that traditional methods may not match (Bommasani et al., 2021). Empirical evidence from various tests, such as writing assessments and idea generation challenges, suggests that LLMs can effectively understand and apply evaluation criteria similar to human evaluators (Girotra, Meincke, Terwiesch, & Ulrich, 2023; Wei et al., 2022).\nWhen provided with explicit standards of evaluation, LLMs can accurately comprehend both the criteria and the task outputs (Ouyang et al., 2022). This allows them to conduct evaluations that reflect a deep understanding of what constitutes good performance versus poor performance. Because LLMs are pre-trained on human language data, they are able to interpret definitions and standards effectively, leading to evaluations that should be comparable to those provided by human raters (Raffel et al., 2020; Yin et al., 2022). Therefore, we propose:\nHypothesis 1 (H1): Evaluations generated by LLMs will be significantly related to evaluations conducted by human raters, demonstrating comparable understanding and application of evaluation criteria.\nBeyond their capability to match human evaluators in understanding criteria, LLMs also offer strengths that may lead to more reliable and consistent assessments. Unlike human raters, LLMs are not subject to fatigue, mood variations, or other idiosyncratic biases that can affect judgment(Bernardin & Buckley, 1981; Heilman, 2012; Kahneman, Sibony, & Cass.R.Sunstein, 2022). LLMs can assess each output solely against the defined standards without the influence of previous judgments or personal biases (Brown et al., 2020; Yin et al., 2022). Research in performance rating has highlighted that a significant portion of variance in human ratings is due to individual idiosyncratic differences, with low to moderate correlation observed between raters (Hoffman, Lance, Bynum, & Gentry, 2010; O'Neill, Mclarnon, & Carswell, 2015).\nFor example, in a meta-analysis, Conway and Huffcutt (1997) reported moderate correlations between different rater sources, with supervisor-peer correlations being the highest (\u03c1 = .34), followed by supervisor-self (\u03c1 = .22) and peer-self (\u03c1 = .19) correlations."}, {"title": "", "content": "The consistency of LLMs means that, when constructs are well-defined, LLMs can provide more reliable assessments. They are less subject to individual errors and inconsistencies compared to human raters (Zhao et al., 2023). As a result, LLMs are likely to offer more consistent and reliable evaluations, capturing the intended constructs with higher fidelity (Khashabi, Kordi, & Hajishirzi, 2022; Wei et al., 2022). Although it might seem obvious that LLMs would be consistent with the same prompt, it's important to note that each response generated by an LLM is independent and can vary due to the inherent randomness in the model's output. This randomness can be controlled to some extent by adjusting the \u201ctemperature\u201d setting, which influences the variability of the responses (Holtzman et al., 2020). Therefore, even with the same prompt, LLMs can produce different outputs, making their consistent performance noteworthy. Thus, we propose:\nHypothesis 2 (H2): Evaluations generated by LLMs will demonstrate a higher degree of consistency and reliability in rating performance outcomes compared to evaluations conducted by individual human raters, showing less susceptibility to idiosyncratic biases.\nMoreover, the inherent randomness programmed into LLMs' algorithms introduces a level of variability in ratings, which could paradoxically enhance their utility. This built-in variance allows LLMs to simulate a range of perspectives, akin to multiple human raters independently evaluating the same output (Binz & Schulz, 2023; Brown et al., 2020; Dillion, Tandon, Gu, & Gray, 2023). Individual human raters often exhibit variance in their assessments (DeNisi & Murphy, 2017; Landy & Farr, 1980); however, their independent evaluations of the same object tend to offset random and idiosyncratic variances, converging towards a more accurate and true valuation of the concept at hand, serving as a proxy for ground truth (Hong & Page, 2004; Larrick & Soll, 2006; Scullen et al., 2000). Specifically, the randomness embedded within LLMs allows for diverse evaluations on the same text output, effectively mimicking the collective judgment of multiple human raters (Dillion et al., 2023). Consequently, when multiple GPT-generated ratings of the same output are aggregated, they tend to converge towards a more accurate reflection of the evaluated concept's true value (Larrick & Soll, 2006). This aggregation of multiple GPT ratings for the same performance reduces the impact of each GPT's biases and measurement errors, aligning more closely with the consensus ratings of multiple human experts.\nHypothesis 3 (H3): Aggregated LLM evaluations will more closely align with the consensus ratings of multiple human experts, serving as a more accurate proxy for the ground truths of the evaluated concepts."}, {"title": "LLMs and Performance Evaluation Biases", "content": "Our results suggest that GPT ratings are comparable to human ratings but exhibit higher consistency and reliability. Despite these advantages, it is crucial to examine whether LLMs are susceptible to the same cognitive biases that affect human raters. Human raters are inherently susceptible to a variety of cognitive biases in performance evaluations, including but not limited to the halo effect, leniency or severity bias, and confirmation bias (Landy & Farr, 1980). Among these, the halo effect stands out as a fundamental and pervasive bias in performance appraisals(Fisicaro & Lance, 1990; Nisbett & Wilson, 1977). For this study, we specifically focus on the halo effect due to its significant impact on performance evaluations.\nThe halo effect occurs when an observer's overall impression of a person, object, or entity influences their evaluation of specific attributes, leading to unduly favorable or unfavorable assessments (Nisbett & Wilson, 1977). This bias is prevalent in performance evaluations, where evaluators' general impressions of an employee can overshadow their objective assessment of specific performance dimensions (Balzer & Sulsky, 1992).\nThe halo effect is particularly significant due to its pervasive and unconscious nature (Nisbett & Wilson, 1977). It can cause raters to overlook specific weaknesses or strengths, resulting in evaluations that reflect general impressions rather than detailed, attribute-specific assessments (Murphy, Jako, & Anhalt, 1993; Thorndike, 1920). This bias systematically distorts performance appraisals across various contexts, from employee evaluations to consumer choices (Asch, 1946; Landy & Farr, 1980). Its impact on decision-making can lead to unfair or inaccurate assessments (Fisicaro & Lance, 1990), and it often resists correction even when raters are aware of its influence (Wetzel, Wilson, & Kort, 1981). These characteristics make the halo effect a crucial aspect to investigate in the context of LLM-generated evaluations, as understanding its potential presence in AI systems could provide valuable insights into the broader implications of cognitive biases in automated decision-making processes(Shrestha et al., 2019).\nLLMs, by contrast, evaluate based on predefined criteria and are designed to assess text- based outputs independently of prior knowledge or impressions of the individual being evaluated (Bommasani et al., 2021; Vaswani et al., 2017). This objective, criteria-based approach should, in theory, make LLMs less prone to the halo effect. Even when presented with background information intended to introduce halo biases, LLMs process this information differently from humans (Wei et al., 2022). They do not form holistic impressions of individuals; instead, they analyze and rate each attribute based on the content and context of the provided text, thereby minimizing the risk of one attribute disproportionately influencing the overall evaluation(Brown et al., 2020; Gilardi, Alizadeh, & Kubli, 2023).\nFrom a theoretical perspective, the absence of cognitive shortcuts in LLMs-shortcuts that humans often rely on\u2014should reduce the susceptibility to the halo effect (Tversky & Kahneman, 1974). Cognitive psychology posits that humans use heuristics to simplify decision- making processes, which can lead to systematic biases like the halo effect (Luan, Reb, & Gigerenzer, 2019). LLMs, however, process information systematically and adhere to the criteria set forth for evaluation, without the influence of extraneous factors such as an evaluator's mood, previous interactions with the subject, or irrelevant contextual information(Brown et al., 2020). Additionally, LLMs' processing algorithms are designed to focus on specific prompts and tasks. When provided with explicit evaluation criteria, LLMs consistently apply these criteria to the text they analyze, ensuring that their assessments are based on the predefined standards rather than the overall impression introduced by background information(Bommasani et al., 2021). This systematic approach helps mitigate the halo effect because the model does not weigh background information as heavily as humans might. Instead, it evaluates each piece of information within the context of its relevance to the specific criteria being assessed.\nHypothesis 4 (H4): LLM-generated evaluations will exhibit significantly lower susceptibility to the halo effect in performance assessments compared to evaluations conducted by human raters.\nIn addition to the baseline hypotheses, our study recognizes the potential for LLMs to offer more nuanced insights into performance evaluations, warranting further exploration through specific research questions. Without developing a priori assumptions, we seek to explore several critical aspects of LLM evaluations. First, we investigate how the number of LLM- generated ratings impacts the accuracy of performance evaluations and how this relationship is moderated by the temperature setting used during evaluation. This focus aims to understand how varying the number of evaluations and adjusting the model's randomness can mimic the diverse perspectives and collective wisdom of human experts (Hong & Page, 2004). Additionally, we examine the extent to which LLM evaluations reflect contextual biases present in the text, such as framing effects introduced by additional context about the evaluated individual. This exploration will help us refine LLM applications and mitigate unintended biases. Furthermore, we assess how well LLMs distinguish between different evaluative dimensions, such as overall performance, novelty, and usefulness dimensions, compared to human raters, providing insights into their nuanced understanding and evaluation capabilities."}, {"title": "Methods", "content": null}, {"title": "Overview Design", "content": "Our research uses two distinct datasets to examine and compare the evaluative precision of LLMs against human judgment across varied environments. The first study uses performance outputs from a controlled laboratory setting, where participants, including professionals and students, completed professional tasks. These performance outputs were subsequently evaluated by both human raters and LLMs. This setup allows us to compare LLM evaluations to human evaluations against various benchmarks, providing a controlled environment to assess the accuracy and reliability of LLM-generated ratings.\nTo achieve this comparison, we employed both descriptive and meta-analytical approaches. The descriptive analysis examined the correlation between GPT ratings and human ratings to determine their comparability. Additionally, we conducted a meta-analysis using Hunter and Schmidt (2004) methods to further validate the comparability of GPT evaluations with human raters. This approach allowed us to estimate the true correlation between raters and its variability, providing a robust statistical foundation for our comparisons.\nThe second study transitions to a field setting at a prominent taxi company in China, providing an ecologically valid examination of real-world performance evaluations within the context of a promotional examination. In this study, we replicate our findings using text outputs from a real organizational setting. We also examine potential biases by manipulating the background information of the employees being evaluated to assess the extent to which LLMs exhibit the halo effect. Both studies aim to provide a comprehensive analysis of how LLMs perform relative to human evaluators across different performance tasks, elucidating the strengths and potential limitations of LLMs in performance evaluation."}, {"title": "Study 1", "content": null}, {"title": "Sample and Procedurals", "content": "The first study used performance outputs generated by 130 participants who took part in a laboratory setting to complete professional tasks. The sample included working professionals as well as college students. Each participant completed four tasks, resulting in a total of 520 textual performance outputs. The tasks were designed to assess different professional skills: writing a job search cover letter to evaluate persuasive writing capabilities, creating and justifying new product names to gauge creative thinking, developing solutions for team conflict scenarios to assess interpersonal and conflict resolution skills, and designing an AI-integrated college course curriculum to test innovative thinking in educational contexts. Detailed task descriptions can be found in the online appendix\u00b9.\nThe text-based performance outcomes of these tasks were evaluated on three dimensions: overall quality, novelty, and usefulness (Montag, Maertz Jr., & Baer, 2012), each rated on a scale from one to ten. In later descriptions, we refer to these dimensions as Overall, Novelty, and Usefulness. An online panel of independent judges, each blind to the study's hypotheses and the conditions under which each task output was created, assessed each performance output. Prior to evaluation, the raters were provided brief instructions to clarify the evaluation criteria. Each work was rated by six raters, with a total of 486 human raters initially participating in the evaluation process. Raters who spent an unreasonably short amount of time on the tasks were removed, resulting in a final pool of 382 valid raters. On average, each valid rater provided ratings for approximately 8 performance outputs. The tasks being rated were presented in a random sequence to each rater, which helped eliminate figure, sequence, and other potential confounding factors in the evaluations.\nTo evaluate the internal consistency of the ratings, we calculated Cronbach's alpha for each dimension, which resulted in values of .72 for overall quality, .71 for novelty, and .69 for usefulness. Cronbach's alpha was chosen because it is a widely used measure of internal consistency, indicating how closely related a set of items are as a group.\nFor evaluating performance using LLMs, we employed the OpenAI API (GPT-4 Model, OpenAI, 2023), programmed in Python. Each response was independently rated six times by GPT-4. To ensure a fair comparison, GPT-4 was provided with the same clear evaluation criteria in their prompts as given to the human judges. This method involves no prior training on specific tasks, ensuring that each evaluation adheres to predefined standards similar to those applied in human assessments. Unlike traditional NLP methods that generate deterministic ratings, GPT ratings introduce an element of variability akin to human judgment, where identical inputs might produce varied ratings. This variability is regulated by the \u201ctemperature\u201d parameter in the model, which can be adjusted between 0 and 2; a higher temperature increases randomness. For our comparative analysis between GPT and human ratings, we maintained the temperature setting at the default value of 1. Further investigations were conducted to assess the effects of this inherent randomness on the consistency and accuracy of GPT ratings. Our analysis revealed high reliability of GPT ratings across the three dimensions, with Cronbach's alpha values of .93 for overall quality, .93 for novelty, and .90 for usefulness."}, {"title": "Results", "content": null}, {"title": "Comparable GPT Ratings to Human Ratings", "content": "Our analysis begins by examining the correlation between single GPT ratings and single human ratings to determine their comparability. The descriptive analysis in Figure 1 shows that the correlation between a single GPT rating and a single human rating (noted as GPT[1", "Human[1": "is .41 for Overall in Task 1. This correlation is comparable to the correlation between two human ratings (noted as Human[1"}, {"Human[1": "which is .35 for Overall in Task 1. The results suggest that individual GPT ratings are as comparable to individual human ratings. In addition, the descriptive analysis in Figure 1 reveals that the correlations between two GPT ratings (noted as GPT[1", "GPT[1": "are higher than the correlations between two human ratings (Human[1"}, {"Human[1": "implying that GPT ratings may have an advantage over human ratings in terms of evaluation consistency. For instance, in Task 1 for Overall, GPT(1)-GPT(1) shows a correlation of .72, which is higher than Human(1)-Human(1)'s .35. Moreover, Figure 1 also illustrates that the correlations between six GPT ratings and six human ratings (noted as GPT[6", "Human[6": "are substantially higher than the correlations for GPT(1)-Human(1), suggesting the potential for improving rating performance through aggregating multiple raters. For example, GPT(6)-Human(6) has a correlation of .70 compared to GPT(1)-Human(1)'s .41 in Task 1 for Overall.\nTo further validate the comparability of GPT evaluations with human raters, we conducted a meta-analysis using Hunter and Schmidt (2004) methods to estimate the true correlation between raters and its variability. Each task output was evaluated by six human raters, allowing us to estimate the correlation between one human rater and another human rater (Human[1"}, {"Human[1": "."}]}