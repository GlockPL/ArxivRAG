{"title": "ADAPTIVE k-NEAREST NEIGHBOR CLASSIFIER BASED ON THE LOCAL ESTIMATION OF THE SHAPE OPERATOR", "authors": ["Alexandre Lu\u00eds Magalh\u00e3es Levada", "Frank Nielsen", "Michel Ferreira Cardia Haddad"], "abstract": "The k-nearest neighbor (k-NN) algorithm is one of the most popular methods for nonparametric classification. However, a relevant limitation concerns the definition of the number of neighbors k. This parameter exerts a direct impact on several properties of the classifier, such as the bias-variance tradeoff, smoothness of decision boundaries, robustness to noise, and class imbalance handling. In the present paper, we introduce a new adaptive k-nearest neighbours (kK-NN) algorithm that explores the local curvature at a sample to adaptively defining the neighborhood size. The rationale is that points with low curvature could have larger neighborhoods (locally, the tangent space approximates well the underlying data shape), whereas points with high curvature could have smaller neighborhoods (locally, the tangent space is a loose approximation). We estimate the local Gaussian curvature by computing an approximation to the local shape operator in terms of the local covariance matrix as well as the local Hessian matrix. Results on many real-world datasets indicate that the new kK-NN algorithm yields superior balanced accuracy compared to the established k-NN method and also another adaptive k-NN algorithm. This is particularly evident when the number of samples in the training data is limited, suggesting that the kK-NN is capable of learning more discriminant functions with less data considering many relevant cases.", "sections": [{"title": "1 Introduction", "content": "The k-nearest neighbor classifier (k-NN) is a nonparametric method known for its simplicity, versatility, and intuitive approach to classification tasks [Cover and Hart, 1967, Nielsen, 2016]. The k-NN algorithm is well-suited to handling complex, non-linear relationships and high-dimensional datasets where the underlying structure may be difficult to specify [Zhang, 2022]. Another advantage refers to its ease of implementation and interpretability, being the classification decision determined by the majority class among the k-nearest neighbors of a given data point. Moreover, the k-NN classifier requires minimal training time since it essentially memorizes the training dataset, becoming suitable for both online and offline learning scenarios. Furthermore, the k-NN algorithm adapts dynamically to changes within the dataset, being more robust to noise and outliers [Syriopoulos et al., 2023].\nThe parameter k controls the neighborhood size, playing a crucial role in determining behavior and performance of the k-NN classifier. This parameter represents the number of nearest neighbors considered when making predictions for a new data point [Jodas et al., 2022]. A smaller k value leads to a more flexible model with decision boundaries that closely follow the training data, potentially capturing intricate patterns and local variations. However, smaller k values may increase the susceptibility to noise and outliers, due to the fact that it excessively relies on the nearest neighbors"}, {"title": "2 Adaptive curvature based k-NN classifier", "content": "One of the main limitations of the k-NN classifier refers to its parameter sensitivity, as the performance of the k-NN is highly dependent upon the choice of the parameter k - i.e., the number of nearest neighbors being considered. Selecting an inappropriate value of k may result in a series of negative effects to its performance such as [Zhang, 2022, Syriopoulos et al., 2023, Daulay et al., 2023]:\n\u2022 Overfitting and underfitting: The parameter k controls the flexibility of the decision boundary of the k-NN classifier. A smaller value of k yields a more flexible (less smoothing) decision boundary, which may lead to overfitting - particularly in noisy or high variance datasets. Conversely, a larger value of k results in a smoother decision boundary, which may lead to underfitting and inappropriate generalization in the case that the k is excessively large compared to the dataset size or the underlying structure of the data.\n\u2022 Bias-variance tradeoff: The choice of k in the k-NN classifier involves a trade-off between bias and variance. A smaller k leads to low bias and high variance. This means that the classifier might capture more complex patterns in the data, although it is sensitive to noise and fluctuations. On the other hand, a larger k reduces variance while increases bias, potentially leading to simpler decision boundaries that may not capture the true underlying structure of the data.\n\u2022 Robustness: The sensitivity of the k-NN classifier to the k parameter affects its robustness to noisy data points and outliers. A larger k may mitigate the effects of noise by considering a larger number of neighbors, whereas a smaller k may lead to overfitting - in which case the classifier is more affected by noisy data points.\n\u2022 Impact on class-imbalanced data: The minority class (i.e., a class with fewer instances) tends to be underrepresented compared to the majority class. The choice of the k parameter may influence the classification of minority class instances. A smaller k may result in the majority class dominating the prediction for the minority class instances. Since the nearest neighbors are predominantly from the majority class, the minority class observations might be misclassified or even ignored. A larger k may incorporate more neighbors, potentially improving the representation of the minority class. However, it might also introduce noise from the majority class, leading to misclassification of minority class observations."}, {"title": "2.1 Algorithm to estimate the shape operator curvature", "content": "In order to propose our method to compute Gaussian curvatures, suppose that $X_i = {x_1, ..., x_n}$, where $x_i \\in R^m$, denotes the input of the data matrix, in which each column of $X_i$ represents a sample of the dataset. Given matrix $X_i$, we may build a graph from the k-nearest neighbors of each sample (k-nearest neighbor graph), known as k-NNG. For each sample, calculate its k-nearest neighbors based on a distance metric (e.g., Euclidean distance, Manhattan distance) in the feature space. Then, include an edge linking each sample to its k-nearest neighbors, preparing a graph where each sample is a node, which edges represent the connections between a pair of nodes [Eppstein et al., 1997].\nThe k-NNG in Figure 1 adopts the Euclidean distance in the computation of the nearest neighbors of each sample $z_i$. Let $n_i$ be the k nearest neighbors of $z_i$. Then, a patch $P_i$ may be defined as the set ${z_i, z_{i1}, ..., z_{ik}}$. It is worth noticing that the cardinality of this set is k + 1. In matrix notation, the patch $P_i$ is given by:\n$P_i = \\begin{bmatrix}\nx_{i (1)} & x_{i1 (1)} & ... & x_{ik (1)} \\\\\nx_{i(2)} & x_{i1 (2)} & ... & x_{ik (2)} \\\\\n: & : & : & : \\\\\n: & : & : & : \\\\\nx_{i(m)} & x_{i1(m)} & ... & x_{ik(m)}\\end{bmatrix}_{mx(k+1)}$ (1)\nIn the proposed method, we approximate the local metric tensor at a point $z_i$ as the inverse of the local covariance matrix, $\\Sigma_i^{-1}$, estimated from the patch $P_i$. Our motivations for this choice are listed below, following [Li and Tian, 2018, Wang and Sun, 2015, Hair-Jr. et al., 2018]:\n\u2022 Positive-definiteness: The covariance matrix is positive-definite, implying that it defines a positive-definite inner product on the space of the data. Similarly, its inverse retains this property, ensuring that it represents a valid metric tensor.\n\u2022 Measurement of distances: The elements of the inverse covariance matrix (i.e., precision matrix) provide information about the distances between points in the feature space. A larger value in the inverse covariance matrix indicates a smaller distance between the corresponding features, while a smaller value indicates a larger distance. This information reflects the relationships and correlations between features in the dataset.\n\u2022 Directional sensitivity: Similarly to a metric tensor in differential geometry, the inverse covariance matrix is sensitive to changes in the direction within the feature space. It quantifies how distances between points change as one moves along different directions in the space, capturing the curvature and geometry of the data manifold."}, {"title": "2.2 Curvature-based kK-NN classifier", "content": "The intuition behind the new kK-NN classifier is to explore the local curvature to define the size of the neighborhood k at each vertex of the k-NNG in an adaptive fashion. In the case of points with lower curvature values, the tangent plane tend to be tightly adjusted to a manifold as the geometry is relatively flat in the neighborhood of that point. This means that a larger parameter k should be considered. Conversely, points with high curvature values lead to a relevant bending or deformation of the manifold. In such cases, it is challenging to perform an accurate approximation with a flat tangent plane. The tangent plane may be loosely adjusted to the surface in these regions, meaning that a smaller parameter k should be considered.\nThe kK-NN classifier is composed by two phases, namely training and testing. In the training stage, the first step consists in building the k-NNG from the input feature space using $k = log_2 n$, where n is the number of samples. Then, the curvature of all vertices of the k-NNG is computed using the shape operator based algorithm detailed in the previous section."}, {"title": "3 Results", "content": "Extensive computational experiments are performed to compare the balanced accuracy, Kappa coefficient, Jaccard index, and F1-score of the proposed adaptive kK-NN classifier against the regular k-NN and a further competing adaptive k-NN algorithm [LeJeune et al., 2019]. This competing algorithm performs adaptive estimation of the distances, exploiting the fact that finding a set of the k nearest neighbors does not require computing their exact corresponding distances. Several real-world datasets are collected from the public repository openml.org. The title, sample size, number of features, and number of classes of the 30 datasets used in the first round of experiments are detailed in Table 1. Those datasets cover a wide range of sample sizes, number of features, and classes from many domains.\nIn the first round of experiments, it is adopted a holdout strategy to divide the samples into the training and test datasets. The training partition varies from 10% to 90% of the total samples, using increments of 5% - leading to a total of 17 possible divisions in training and testing stages. The number of neighbors in the regular k-NN is defined as $k = log_2(n)$, where n is the number of samples. This is the same value used in the proposed kK-NN method.\nAll three competing algorithms are trained in each one of the 17 training sets, while testing them in the respective test partition. The median balanced score, Kappa coefficient, Jaccard index, and F1-score are computed over the 17 executions. The objective is to test the behavior of the classifier in all scenarios - small, medium, and large training datasets.\nThe results are reported in Table 2. Considering all evaluation metrics, the proposed kK-NN classifier provides the best results in 19 cases (63% of the datasets), the competing adaptive k-NN algorithm is the best algorithm of 11 cases (37% of the datasets), and the regular k-NN is outperformed in all datasets. These results indicate a superior performance not only on the relatively unsophisticated k-NN but also over a competing adaptive k-NN classifier. A limitation of the proposed kK-NN method refers to its comparatively higher computational cost.\nIn order to visualize how the proposed method performs in a single dataset, the curves of the balanced accuracies obtained for the datasets vowel, Olivetti_Faces, ionosphere, and parkinsons are depicted in Figure 2. The kK-NN classifier is consistently superior compared to both the regular k-NN and adaptive k-NN [LeJeune et al., 2019]. The rationale behind the capacity of the proposed method to improve the regular k-NN may be summarized through three relevant aspects. Firstly, through the use of an adaptive strategy to define neighborhood sizes, the kK-NN is more capable of avoiding both underfitting and overfitting. Secondly, decision boundaries become more adjusted to the samples in regions with higher density, while the boundaries become smoother in regions of lower density. Thus, the classification rule works in an adaptive fashion considering the region of the feature space. Thirdly, high curvature points are commonly related to outliers, being the kK-NN classifier capable of isolating such samples by drastically reducing its neighborhoods - which, consequently, decreases its overall sensitivity to outliers."}, {"title": "4 Conclusion", "content": "In the present work, we introduce a new curvature-based classification method. The kK-NN consists of an adaptive approach to overcome relevant limitations of widely adopted k-NN classification algorithms. The rationale behind the new kK-NN is to adapt the neighborhood size locally, leveraging the curvature information inherent to the dataset in order to improve classification accuracy. Our theoretical analysis and experimental results provide relevant insights into the effectiveness and versatility of the proposed method. Our main findings over 30 real-world datasets may be summarized into three main methodological improvements.\nThe first methodological improvement concerns a curvature-based adaptation, in which the embodiment of curvature information into the k-NN framework may increase classification performance. By dynamically adjusting the neighborhood size based on the characteristics of the local curvature, the kK-NN is adaptable to different regions of the feature space. The second methodological improvement refers to robustness and generalization. The kK-NN classifier exhibits robustness to noise and outliers, indicating its ability to handle complex and real-world datasets more effectively. Moreover, the proposed method showcases promising generalization capabilities across different domains, emphasizing its potential for a wide range of applications. Thirdly, extensive experiments exploring many diverse datasets indicate that the kK-NN outperforms established k-NN classifiers in various scenarios. The adaptive nature of the kK-NN enables it to excel in situations where the local density of samples and curvature patterns vary significantly.\nTherefore, the adaptive curvature-based approach adopted in the kK-NN introduces a promising advancement within k-NN classification methods. Empirical analyses provide evidence of its efficacy considering diverse scenarios, indicating relevant possibilities for future research and applications in machine learning (e.g., pattern recognition, computer vision). The incorporation of curvature information into classification frameworks offers a nuanced perspective that unfolds new possibilities for enhancing the adaptability and robustness of classification algorithms.\nSuggestions of future extensions include additional theoretical investigations into the properties of curvature-adaptive classifiers, further exploring its underlying mechanisms. Analyzing the convergence properties and establishing theoretical bounds on the performance of kK-NN under different conditions would contribute to the theoretical foundations of the curvature-based classification. Moreover, the curvature-based adaptation of the kK-NN could be applied to image processing tasks, particularly in scenarios where local variations and intricate data patterns play a crucial role. Lastly, considering that the principles underlying the kK-NN are rooted in curvature analysis, the proposed method may find applications in dimensionality reduction and metric learning."}, {"title": "A Shape operator and curvatures", "content": "Differential geometry provides a framework to study properties that are invariant under smooth mappings, preserving differentiability. It focuses on studying geometric objects such as curves and surfaces, while understanding their properties regarding differential calculus [do Carmo, 2017, Needham, 2021]. Potential applications across various domains include physics, engineering, computer science, robotics, among many more. In physics, it is applied to describe the geometry of spacetime in general relativity, whereas in computer graphics it is employed to model and manipulate smooth surfaces [Tu, 2017]. The mathematical concepts and tools of differential geometry provide a powerful framework to understand the intrinsic geometry of spaces as well as their applications to diverse fields of knowledge [Oprea, 2007]. The notion of surface in R\u00b3 is generalized to higher dimensions by the definition of manifold.\nManifolds consist of a fundamental framework for studying spaces with intrinsic geometry, finding applications in many areas of mathematics, including data analysis and machine learning, where they are employed to model complex datasets and high-dimensional spaces [Gorban and Tyukin, 2018, Fefferman et al., 2016].\nA tangent space is a fundamental concept in differential geometry, providing an understanding of the local behavior of a manifold at a particular point. The tangent space to a manifold at a point captures the notion of \u201cinfinitesimal\" directions at that point. Tangent vectors represent directions and rates of change at a point on the manifold. Intuitively, in a smooth surface such as a sphere or curve, the tangent vectors at a particular point represent possible directions to move along the surface or the direction of velocity if it is passing through that point.\nThe tangent space captures the local geometry of the manifold at a specific point, being instrumental in defining notions such as tangent bundles, differential forms, and differential operators on manifolds. The metric tensor (also known as the first fundamental form) is another mathematical object that plays an important role in the computation of inner products and arc lengths in a manifold."}, {"title": "B The k-nearest neighbor classifier", "content": "The k-NN classifier is a simple and effective algorithm employed in supervised learning. It belongs to the family of instance-based learning or lazy learning methods, where the algorithm does not build an explicit model during the training phase. Instead, it memorizes the entire training dataset to perform predictions based on the similarity of new instances compared to the training dataset [Taunk et al., 2019].\nThe k-NN may be understood as the Bayesian classifier when the densities of the conditional class are estimated using a nonparametric approach [Webb and Copsey, 2011]. The probability of a sample x be in a region of interest of volume V(x), centered on x is given by:\n$\u03b8 = \\int_{V(x)} p(x)dx$ (11)\nwhich is a generalization of the area under the curve defined by the probability density function (PDF). In the case of multiple dimensions, a volume is calculated instead of an area. However, for a small volume V(x), we have the following approximation:"}, {"title": "B.1 Balloon and sample-point estimator", "content": "In general, the degree of smoothing applied to the data is determined by a bandwidth. The underlying PDF is smoothed as a consequence of a larger bandwidth, which decreases the variance while increasing the bias. On the other hand, if the bandwidth is reduced, the variance increases although the bias reduces. Selecting the ideal bandwidth is frequently difficult without understanding the true underlying PDF. Adopting only one bandwidth may provide suboptimal results when considering the whole domain. This approach may induce to oversmoothing in high density neighbourhoods.\nConversely, in cases of small sample sizes, it may lead to undersmoothing in the neighbourhood of the extreme regions of the distribution (tails). Without a relevant understanding of the density function, it is challenging to select the ideal bandwidth. To mitigate this issue, locally adaptive approaches empower the bandwidth to vary over the domain of the PDF [Jarosz, 2008]. One of such locally adaptive approaches refers to the balloon estimator, which was originally proposed as a k-NN estimator [Loftsgaarden and Quesenberry, 1965]. A general form of the balloon estimator is detailed as:\n$p(x) = \\frac{1}{Nh(x)} \\sum_{i=0}^{N-1} K [\\frac{x-x_i}{h(x)}]$ (21)\nwhere N is the number of samples, h(x) is the bandwidth as a function of x, and K refers to its kernel. The issue of selecting bin locations is addressed by the kernel estimator, which generalizes the naive estimator to remove discontinuities of the corresponding PDF. Nonetheless, the balloon estimator is subject to several inefficiencies, particularly regarding univariate data. Whether applied globally, its estimate commonly does not integrate to one over the domain. An additional issue refers to the fact that the bandwidth consists of a discontinuous function, which affects the associated PDF [Terrell and Scott, 1992]."}]}