{"title": "Universal Sound Separation with Self-Supervised Audio Masked Autoencoder", "authors": ["Junqi Zhao", "Xubo Liu", "Jinzheng Zhao", "Yi Yuan", "Qiuqiang Kong", "Mark D. Plumbley", "Wenwu Wang"], "abstract": "Universal sound separation (USS) is a task of separating mixtures of arbitrary sound sources. Typically, universal separation models are trained from scratch in a supervised manner, using labeled data. Self-supervised learning (SSL) is an emerging deep learning approach that leverages unlabeled data to obtain task-agnostic representations, which can benefit many downstream tasks. In this paper, we propose integrating a self-supervised pre-trained model, namely the audio masked autoencoder (A-MAE), into a universal sound separation system to enhance its separation performance. We employ two strategies to utilize SSL embeddings: freezing or updating the parameters of A-MAE during fine-tuning. The SSL embeddings are concatenated with the short-time Fourier transform (STFT) to serve as input features for the separation model. We evaluate our methods on the AudioSet dataset, and the experimental results indicate that the proposed methods successfully enhance the separation performance of a state-of-the-art ResUNet-based USS model.", "sections": [{"title": "I. INTRODUCTION", "content": "Computational auditory scene analysis [1] aims to equip machine listening systems with the capability to selectively perceive numerous distinct events in the surrounding environment. Audio source separation [2], as a fundamental task in computational auditory scene analysis, has been studied for many years [3]. It has numerous applications across various domains, including automatic speech recognition [4], music transcription [5], and sound monitoring [6].\nIn monaural source separation, the task is to segregate individual source tracks from a single-channel sound mixture without relying on any spatial cues. Many previous studies have primarily focused on specific types of sounds, such as speech [7] or music [8], and are only able to separate a limited number of sound sources. A more challenging problem is to separate arbitrary sound sources from each other using a single model, known as universal sound separation (USS).\nFor single-channel USS tasks, separating all sources from an audio mixture is extremely challenging. In practical scenarios, we are generally interested in one particular source. Therefore, sound separation models can be designed to have a single output. To empower the model to extract arbitrary sources, we can condition the system on a query embedding derived from reference recordings of the desired source. This type of task is referred to as query-based source separation (QSS) [9]. The query information can take the form of different modalities, such as audio [10], vision [11], or language [12], [13].\nIn recent years, self-supervised learning (SSL) approaches have advanced rapidly and become the predominant method for pre-training models. SSL alleviates the reliance of supervised learning on large amounts of labeled data while exhibiting great performance and generalization capabilities [14]. A growing number of SSL methods have been successfully applied in the field of audio, including applications such as audio classification [15], speaker recognition [16], and speech recognition [17]. However, all these methods focus on representation learning for classification-based tasks. On the other hand, the source separation problem requires the model to estimate continuous target source signals.\nOne SSL model that has been applied for source separation is WavLM [18] for speech separation [19]. WavLM is a pre-trained model based on HuBERT [20], which includes a CNN encoder and a Transformer. Another model, Pac-HuBERT [21], has been proposed for music source separation, where HuBERT was adapted into a time-frequency domain separation model, achieving better separation performance compared to the ResUNet [22]. These SSL models undergo pre-training on the mask prediction task, where the objective is to predict masked tokens from visible tokens. While the representations learned by these models can be applied to source separation tasks, it's important to note that pre-training is typically performed using specific datasets, such as speech or music data. Therefore, identifying a suitable self-supervised model pre-trained on general audio data and extending it to universal sound separation has yet to be explored.\nIn this work, we propose to use a self-supervised pre-trained audio model, i.e., the audio masked autoencoder (A-MAE), to extract general audio representations for improving USS models. Specifically, during the USS training stage, we propose to either freeze or partially update the parameters of the A-MAE to obtain the SSL representations, which are then concatenated with the short-time Fourier transform (STFT) features as input. Based on this, the downstream separator predicts the mask of the desired source. We evaluate our methods on the AudioSet dataset. Experimental results indicate that our proposed methods enhance the separation performance of a state-of-the-art (SOTA) ResUNet-based USS model [23]."}, {"title": "II. RELATED WORK", "content": "The USS task was initially introduced in [24], where the authors proposed utilizing the iterative improved time-dilated convolutional network (iTDCN++) to separate mixtures with known numbers of sound sources. Next, the conditional information about which sound classes are present is used to improve universal sound separation performance in [25]. In order to reduce the cost of annotating data, Wisdom et al. [26] proposed the mixture invariant training (MixIT) approach, a purely unsupervised source separation paradigm for training single-channel sound separation models on a large amount of unlabeled, in-the-wild data. Then, they also [27] introduced a time-domain convolutional network (TDCN++) capable of separating an unknown number of sources in a mixture. The free universal sound separation (FUSS) dataset they used only consists of 357 sound categories.\nA zero-shot universal source separator was put forward by Chen et al. [10], capable of leveraging AudioSet data for training and supporting unseen sources. Nonetheless, they did not report the separation performance of the system when using average embedding as query conditions on AudioSet. Recently, Kong et al. [23] proposed a method for training a USS system using weakly labeled data, achieving SOTA separation performance across the 527 sound classes in the AudioSet dataset. The method of Kong et al. [23] will serve as our baseline."}, {"title": "III. FRAMEWORK AND METHOD", "content": "Most existing separation models require clean source and mixture pairs for training, and they can only separate a limited number of audio classes. To address this issue, Kong et al. proposed a method that utilizes a pre-trained sound event detection system to explore relatively clean target sound events [23], making full use of the large-scale weakly labeled dataset AudioSet [28]. As shown in Figure 1, a typical query-based USS system with weakly labeled data comprises three components: a sound event detection (SED) system that localizes the occurrence of events in weakly labeled AudioSet training data; a query-based source separator trained on the refined data to separate an audio mixture into individual sources; and a latent source embedding (LSE) processor. The latent source processor controls the selection of sources to separate from a mixture, empowering the separator to segregate arbitrary sound sources.\nThe aim of the sound event detection task is to recognize the sound events and localize their occurrence (start and end) time in an audio recording. By leveraging a pre-trained SED model, we can utilize the frame-wise presence probability of the sound event to identify relatively clean sound sources from weakly labeled audio samples. The selected segment is referred to as the target anchor segment. Utilizing target anchor segments detected by the SED system as source data, mixtures can be constructed for training the USS system.\nThe SED model we employed is the Pretrained Audio Neural Networks (PANNs) [29], which include VGG-like CNNs to convert an audio Mel-spectrogram into a (T, K) feature map, where T denotes the number of time frames, and K denotes the number of sound classes. The feature map is a frame-wise prediction that indicates the probability of presence for each sound event at each time frame. We also utilize PANNs as our latent source embedding processor. To obtain the latent source embedding used in query-based sound separation, we average the output of the penultimate layer of PANNs along the time axis.\nFor a query-based USS system, we begin by randomly selecting two different audio samples from AudioSet. These samples are then fed into the SED system to extract target anchor segments for their respective classes. The 2-second target anchor segments for these two sound events are denoted as $s_1$ and $s_2$, respectively. Subsequently, these two segments are fed into the latent source processor to obtain two latent source embeddings, $e_1$ and $e_2$.\nAfter obtaining $s_1$, $s_2$, $e_1$ and $e_2$, we mix two anchor segments $s_1$, $s_2$ with data augmentation to constitute a mixture $x = s_1 + s_2$. Then the mixture is fed into the query-based source separator, described by the following regression:\n$$f(x, e_j) \\rightarrow s_j, j \\in \\{1, 2\\}$$\nEquation (1) shows that the separated sound depends on both the input mixture and the latent source embedding. The latent source embedding provides information on which source is to be separated.\nWe follow the same setup as described in [23] and utilize a residual UNet30 (ResUNet30) to construct our source separator. The ResUNet30 is composed of 6 encoder blocks, 1 bottleneck block, and 6 decoder blocks. Each encoder block includes a single residual convolutional block to downsample the audio spectrogram into a bottleneck feature map. Each decoder comprises a single residual deconvolutional block to upsample the feature and obtain the individual sources. A skip connection is established from each encoder block to the corresponding decoder block with the same downsampling/upsampling rate. For a more detailed model architecture, refer to [22]. The latent source embedding is incorporated into the ResUNet separator using feature-wise linear modulation (FiLM) [30] method. The separation network predicts a complex ideal ratio mask (IRM), which can then be multiplied by the STFT of the mixture to derive the STFT of the separated source. By applying the inverse STFT (iSTFT), the waveform of the separated source can be obtained."}, {"title": "B. Proposed Approach", "content": "A-MAE originates from the image masked autoencoder (MAE) and learns self-supervised representations from Mel-spectrograms [15]. The architecture of A-MAE consists of a 12-layer Vision Transformers-Base (ViT-B) encoder, followed by a decoder comprising 8 Transformer blocks. The encoder of A-MAE partitions the spectrogram of each 10-second AudioSet recording into non-overlapping grid patches, where each patch comprises 16-by-16 time-mel bins. Subsequently, features are extracted based on the sequential positions of these patches in the Mel-spectrogram, ultimately resulting in a 768-dimensional embedding sequence with a length of 512. Eighty percent of the spectrogram patches are randomly masked, and the remaining non-masked patches are used to reconstruct the input spectrogram.\nWe propose using a pre-trained A-MAE encoder as an upstream model to extract universal features. In downstream USS tasks, the original decoder is discarded and replaced with ResUNet. The ResUNet takes the concatenation of the mixture STFT magnitude and A-MAE encoder features as input, as depicted by the dashed box in Figure 1. Since A-MAE is pre-trained on the full AudioSet training set through self-supervised learning, it is expected to enhance the performance of downstream separation tasks.\nAdditionally, in our baseline system, an energy-matching data augmentation strategy has been proven effective in enhancing separation performance [23]. However, we observed that mixture source pairs obtained from this strategy sometimes have amplitudes that are too large, leading to waveform distortion. Therefore, we use Equation (2) to normalize the amplitudes of the mixture source pairs (x, s) obtained after energy matching [13].\n$$x = x/\\max|x_i|, S = s/\\max s_i$$"}, {"title": "IV. EXPERIMENTS", "content": "AudioSet [28] is a large-scale weakly labeled audio dataset consisting of 527 audio events, spanning a broad spectrum of human and animal sounds, musical instruments and genres, as well as everyday environmental sounds. The balanced subset consists of over 20,000 sound clips, upon which our USS system is trained. Weakly labeled data means having labels for what types of sounds are present in a sound clip, but without exact information about when these sound events occur. We preprocessed all 10-second audio samples by resampling them to 32 kHz and converting them to single-channel.\nThe duration of each sound mixture and target source is 2 seconds. To compute the STFT, we utilize a Hann window with a window size of 1024 and a hop size of 320. Given a sampling rate of 32 kHz, this results in 100 frames per second, maintaining alignment with PANNs. Each 2-second sound mixture contains 200 time frames, and we pad them with 24 zero-frames (T = 224). To reduce the length of the input sequence, we introduce an average-max pooling operation [31] for the features of the A-MAE encoder. The shape of the A-MAE encoder feature after pooling is (32, 768), where 32 is the length of sequence, 768 is the embedding dimension. Therefore, along the sequence dimension, we duplicate the pretraining features 7 times to enable their concatenation with the STFT features along the feature dimension.\nThe embedding layer of PANNs has a dimension of 2048. Throughout the training of the USS system, the parameters of PANNs are frozen. The $l_1$ loss between the predicted separated source and the ground truth target source is used for training the query-based USS system. We adopt an Adam optimizer [32] with a learning rate of 0.001. The entire training process consists of 60 epochs, with 10,000 iterations per epoch. The batch size is 16.\nThe evaluation set of AudioSet consists of 18,887 sound clips with 527 sound events. The creation of the evaluation data follows the same process as that of the training data. First, we use a pre-trained SED model to extract target anchor segments from 10-second audio clips. Then, we select anchor segments from two different sound classes and combine them to create a mixture. For each sound class, we create 100 2-second mixtures to evaluate the separation results. In total, there are 52,700 evaluation pairs. In a similar manner, we create 52,700 mixture source pairs from the AudioSet balanced subset for the calculation of average embedding during the inference stage.\nFor query-based USS, there are two types of latent source embeddings used to evaluate the performance of the separation system."}, {"title": "V. RESULTS AND DISCUSSION", "content": "We evaluate our proposed methods and the baseline on the evaluation dataset in Section IV-B. To ensure a fair comparison, we have reproduced the results of the baseline system\u00b9 to serve as our reference for comparison.\nFrom the table, it can be observed that our frozen method achieved an AudioSet SDRi of 5.62 dB using the average embedding, surpassing the SOTA system's 5.18 dB by 0.44 dB. This indicates that the use of universal features extracted by the self-supervised pre-trained model A-MAE and our data augmentation strategy contributes to the enhancement of the USS system's performance. However, compared to the frozen method, the effects of our updated method are not significant. Perhaps there are better updating methods worthy of further exploration.\nTo further compare our frozen approach with the SOTA method, we plotted the class-wise SDRi results of AudioSet separation for 100 sound classes out of 527, as shown in Figure 2. The red line illustrates the SDRi of the SOTA method using the average embedding. The black line represents the SDRi of our proposed method under the condition of average embedding. We observed that the black curve typically lies above the red curve, indicating an overall performance improvement of our proposed method over the SOTA method in most sound classes. Figure 2 indicates that our proposed method achieved an SDRi of over 15 dB in some sound classes, such as dial tone and smoke detector. All classes achieved positive SDRi scores. Compared to the SOTA method, certain sound classes, such as sine wave, smoke detector, and dial tone, exhibited the maximum improvement in SDRi. We discovered that these sounds share common line spectrum characteristics. However, for a very small number of sound classes, SDRi may not have improved or may have even worsened. Additionally, for speech, our method achieves an SDRi of 5.86 dB."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "For the first time, we proposed the application of self-supervised pre-trained audio MAE for universal sound separation tasks. By integrating STFT features and SSL embeddings, the performance of universal sound separation is improved. Experimental results indicate that, compared to the SOTA method, our proposed method achieves a 0.44 dB improvement in SDRi across 527 sound classes in AudioSet. In future work, we aim to enhance the separation ability of our system for unseen sound sources and plan to explore more modalities for universal sound separation."}]}