{"title": "UNIFIED APPROACHES IN SELF-SUPERVISED EVENT STREAM\nMODELING: PROGRESS AND PROSPECTS", "authors": ["Levente Z\u00f3lyomi", "Tianze Wang", "Sofiane Ennadir", "Oleg Smirnov", "Lele Cao"], "abstract": "The proliferation of digital interactions across diverse domains, such as healthcare, e-commerce,\ngaming, and finance, has resulted in the generation of vast volumes of event stream (ES) data. ES\ndata comprises continuous sequences of timestamped events that encapsulate detailed contextual\ninformation relevant to each domain. While ES data holds significant potential for extracting ac-\ntionable insights and enhancing decision-making, its effective utilization is hindered by challenges\nsuch as the scarcity of labeled data and the fragmented nature of existing research efforts. Self-\nSupervised Learning (SSL) has emerged as a promising paradigm to address these challenges by\nenabling the extraction of meaningful representations from unlabeled ES data. In this survey, we\nsystematically review and synthesize SSL methodologies tailored for ES modeling across multiple\ndomains, bridging the gaps between domain-specific approaches that have traditionally operated in\nisolation. We present a comprehensive taxonomy of SSL techniques, encompassing both predictive\nand contrastive paradigms, and analyze their applicability and effectiveness within different appli-\ncation contexts. Furthermore, we identify critical gaps in current research and propose a future\nresearch agenda aimed at developing scalable, domain-agnostic SSL frameworks for ES modeling.\nBy unifying disparate research efforts and highlighting cross-domain synergies, this survey aims\nto accelerate innovation, improve reproducibility, and expand the applicability of SSL to diverse\nreal-world ES challenges.", "sections": [{"title": "1 Introduction", "content": "The rapid growth of digital interactions, ranging from online purchases and social media engagements to automated\nsensors in healthcare, continues to generate massive volumes of event stream (ES) data. Formally, an ES is a con-\ntinuous sequence of timestamped events, each encapsulating structured contextual information. Industries such as\nhealthcare, finance, gaming, and e-commerce have substantial ES repositories: healthcare systems store Electronic\nHealth Records (EHRs) tracking patient admissions and diagnostic tests (Johnson et al., 2023; Herrett et al., 2015),\nwhile e-commerce platforms log user interactions like clicks and purchases (Wang et al., 2019; Sun et al., 2019),\namong many other examples.\nThis surge of context-rich, time-dependent ES data holds potential for a range of downstream applications: e.g.,\ndiagnosing a patient's evolving condition in healthcare (Li et al., 2020), detecting fraudulent transactions in finance\n(Babaev et al., 2022), or personalizing game content to enhance player engagement (Wang et al., 2024; Pu et al.,\n2022). However, large-scale event datasets typically lack the extensive labeling needed to train traditional supervised\nlearning systems. Moreover, existing modeling efforts often remain fragmented across domains, sometimes replicating\nresearch efforts without leveraging the many structural similarities shared by event streams across industries. These"}, {"title": "1.1 Aim and Scope", "content": "Self-Supervised Learning (SSL) stands out as a powerful paradigm well-suited to address these challenges. By deriving\nsupervisory signals directly from unlabeled data, SSL can learn informative representations of sequences without\nrelying on manual annotation. In the context of ES, SSL enables models to capture temporal dynamics and contextual\nrelationships, which are often critical for tasks such as predicting future events, clustering entity behaviors, or detecting\nanomalies. Despite the evidence that SSL has shown major advances in fields like language modeling (Devlin et al.,\n2019; Brown et al., 2020) and computer vision (He et al., 2022; Oquab et al., 2023), its application to ES modeling\nhas not yet converged into a unified body of knowledge. Current ES research typically targets a specific domain (e.g.,\nhealthcare, gaming, e-commerce, finance) without systematically building on or comparing methods across these\nverticals.\nTherefore, the purpose of this survey is to (1) unify the progress in SSL for event streams across multiple domains, (2)\nidentify critical gaps and challenges that cut across those fields, and (3) propose future directions to foster development\nof more general, domain-agnostic SSL approaches for ES modeling. By addressing these diverse aspects, we aim\nto accelerate innovation, improve reproducibility, and highlight emerging themes that can benefit the ES modeling\ncommunity at large.\nTo ensure comprehensive yet focused coverage, we reviewed papers from leading conferences such as ICML, NeurIPS,\nAAAI, and KDD, as well as journals in machine learning, artificial intelligence, and knowledge discovery, along with\ndomain-specific journals, such as those in healthcare. Beginning with an initial pool of 100 works, we refined our\nselection by prioritizing recent studies introducing novel concepts alongside seminal works with substantial impact on\nthe field, as evidenced by their citation influence. The final distribution of surveyed works is presented in Figure 1.\nThis approach ensures our review captures both the breadth and depth of SSL methods applied to ES data."}, {"title": "1.2 Contribution and Structure", "content": "This survey offers several key contributions:\n\u2022 Comprehensive synthesis across domains: We provide the first (to our knowledge) cross-domain assess-\nment of SSL-based ES modeling. While prior work has surveyed either specific application domains (e.g.,\nhealthcare, e-commerce, and gaming) or other data modalities (e.g., tabular, time series, and continuous-time\nsequences), we argue that ES data has unifying structural properties that make it a distinct category meriting\nan overarching view. We provide an overview of how the present survey is positioned relative to others that\naddress different modalities or domains in Table 1."}, {"title": "2 Foundations of Event Streams", "content": "Event Stream (ES) data provides a flexible and expressive data format for representing and modeling sequential events\nacross diverse domains. In the following subsections, we (1) establish the definition and notation (Section 2.1) for the\nstructured foundation and key components for reasoning about ES data, (2) introduce the relation of ES data to other\ndata modalities (Section 2.2), and (3) highlight the application domains and potential challenges (Section 2.3) when\nworking with ES data."}, {"title": "2.1 Definition and Notation", "content": "An ES is a continuous, ordered sequence of events generated over time by one or more sources. Each event encap-\nsulates timestamped information about a specific action or state. While each event is a discrete occurrence, the ES\nrepresents an ongoing, continuous flow of events, which is a common characteristic of real-world scenarios."}, {"title": "2.2 Relation to Other Modalities", "content": "ES data can be transformed into various modalities to leverage established machine learning models and techniques.\nEach of these transformations facilitates the use of methods from well-studied modalities but introduces specific trade-\noffs, such as sparsity in tabular formats or limitations on event data space in time series representations. For instance,\none approach is to convert ES data into a tabular format where each row represents an event, with columns capturing\nits associated attributes. However, this approach often results in sparse datasets with numerous missing or irrelevant\nentries, which can adversely affect model performance. Furthermore, tabular representations fail to incorporate the\ntemporal dimension, making it difficult to capture the evolving dynamics inherent in ES data."}, {"title": "2.3 Application Domains and Challenges", "content": "Event stream data arises across a broad spectrum of real-world settings, each accompanied by domain-specific chal-\nlenges and modeling requirements. Below, we highlight four key application domains and discuss corresponding\nissues that underscore the complexity of ES data. These domains are carefully chosen to illustrate the breadth of ES\nmethodologies and their adaptability to varying real-world needs. A concise mapping of ES concepts across these\ndomains is presented in Table 2."}, {"title": "3 Self-Supervised Learning for Event Streams", "content": "Self-supervised learning has become a cornerstone for ES modeling, as it enables the effective utilization of vast\nvolumes of unlabeled data \u2013 a common characteristic of real-world ES datasets. By designing alternative tasks, known\nas \"pretext\" tasks, SSL uses the data itself as supervisory signals (Liu et al., 2021). This approach is particularly\ncrucial for ES modeling, where labeled data is often sparse, incomplete, or expensive to obtain. Through pretext\ntasks, SSL facilitates the extraction of meaningful representations during an initial pre-training phase, capturing the\ncomplex temporal and contextual relationships inherent in ES data. These pre-trained representations serve as a robust\nfoundation, allowing models to either be fine-tuned on limited labeled datasets for specific downstream tasks or directly\napplied in a zero-shot manner, bypassing the need for additional task-specific training. This dual capability makes SSL\nnot only instrumental but often a prerequisite for building effective ES modeling pipelines in diverse domains.\nIn the following subsections, we first give a brief overview of SSL principles (Section 3.1). We then organize current\nES research into two major families: (1) predictive SSL, which reconstructs or predicts missing or corrupted events\n(Section 3.2), and (2) contrastive SSL, which learns a representation space that brings similar event sequences closer\nand pushes dissimilar ones apart (Section 3.3). This taxonomy is visualized in Figure 3, where we also highlight how\nthese paradigms manifest across the diverse domains specified in Table 2."}, {"title": "3.1 Overview of SSL Principles", "content": "In essence, SSL aims to create pretext tasks that derive supervisory signals directly from unlabeled data. For event\nstreams, such tasks typically involve:\n\u2022 Masking or corrupting certain events or attributes, then training the model to reconstruct them.\n\u2022 Autoregressive next-event (or next-token) prediction, relying only on previously observed events.\n\u2022 Comparing sequences or subsequences under various augmentations to learn semantic representations.\nBecause ES data often contains rich metadata (Section 2.1) with timestamped and potentially high-dimensional event\nfeatures, SSL methods can exploit temporal structure, complex contextual cues, and large-scale unlabeled sequences\nfor representation learning. This stands in contrast to supervised learning, which is limited by the availability of high-\nquality labels in each domain. SSL, therefore, accelerates progress by leveraging all observed events, including those"}, {"title": "3.2 Predictive SSL", "content": "Predictive SSL focuses on learning representations by predicting or reconstructing masked or missing parts of the\nevent stream. This family of methods often draws inspiration from natural language processing (NLP), where large\nlanguage models (LLMs) are pre-trained to predict masked tokens such as BERT (Devlin et al., 2019) or the next token\nin an autoregressive manner, as exemplified by GPT (Radford et al., 2018). Below, we review three core predictive\ntechniques for ES: masked modeling, autoregressive modeling, and temporal point processes."}, {"title": "3.2.1 Masked Modeling", "content": "In masked (or denoising) modeling, the learner randomly masks out certain events, event attributes, or timestamps\nwithin a sequence and aims to predict the masked portions using the remaining context. This approach is inspired by\nthe masked language modeling (MLM) paradigm (Devlin et al., 2019), where tokens are hidden and the model must\nrecover them. For ES, this can mean masking entire events or selectively masking specific features (e.g., diagnosis\ncodes in healthcare, item identifiers in e-commerce). Examples of masked SSL for ES from different domains include:\n\u2022 Healthcare: BEHRT (Li et al., 2020), BRLTM (Meng et al., 2021), and Med-BERT (Rasmy et al., 2021)\npre-train transformer models to impute masked diagnoses or procedure codes. These models mainly vary\nin their event embedding strategies and the information extracted from patient records, typically discretizing\ncontinuous features and summing their embeddings to form final representations.\n\u2022 E-commerce: BERT4Rec (Sun et al., 2019) applies a BERT-like approach to sequences of user-item interac-\ntions, masking items that the user actually engaged with and learning to reconstruct them.\n\u2022 Finance: BERT4Eth (Hu et al., 2023) adapts the BERT framework to transaction data from the Ethereum\nblockchain, using masked crypto addresses as input. Adjustments like increased masking and dropout rates\naddress the specific properties of blockchain data.\n\u2022 Gaming: Pu et al. (2022) utilize a BERT-based framework for player representation learning in massively\nmultiplayer online games, incorporating a confidence-guided masking strategy to focus on significant actions.\nPlayer2Vec (Wang et al., 2024) adapts the masked objective to mobile game behavior modeling, leveraging\nthe Longformer (Beltagy et al., 2020) architecture to capture long-range dependencies within complex player\nsessions.\nNotably, further refinements to these models have emerged across domains. For example, SARD (Kodialam et al.,\n2021) enhances EHR modeling by adding a reverse distillation signal during pre-training and expanding the event\nvocabulary. DuETT (Labach et al., 2023) introduces a dual-attention architecture pre-trained with a masked objective.\nFor SRS, S3Rec (Zhou et al., 2020) extends masked modeling by introducing additional pre-training objectives around\nthe masking paradigm, coupled with mutual information maximization as a loss function.\nBy focusing on partial observations and contextual relationships across timestamps, masked modeling can effectively\nhandle the structured, high-dimensional attributes of ES data. In particular, its ability to attend to flexible contexts,\nrather than relying on uniform sampling intervals, makes it well-suited to irregular event arrivals that frequently occur\nin real-world ES scenarios."}, {"title": "3.2.2 Autoregressive Modeling", "content": "Autoregressive (AR) approaches for ES revolve around predicting the next event (or next attribute) given the historical\ncontext. These methods extend concepts from language modeling (Radford et al., 2018), forcing a strictly causal order\nusing only past events to predict future ones.\nMany SRS adopt this AR perspective, where the AR objective is commonly applied to sequences of user-item in-\nteractions, aiming to predict the next item (Tang & Wang, 2018; Luo et al., 2023; Yuan et al., 2019). While many\nmethods focus on architectural improvements, the learning objective often remains consistent. For example Li et al.\n(2018) introduced dual recurrent networks to capture short- and long-term dependencies, while attention-based ap-\nproaches (Wang et al., 2021a; Ying et al., 2018) have also been employed for similar purposes.\nTo avoid computing normalizing constants in loss functions, techniques like noise contrastive estimation (NCE) (Gut-\nmann & Hyv\u00e4rinen, 2010) have been adopted, as seen in Wang et al. (2021a), Yuan et al. (2019), Hou et al. (2022),\nor by measuring the distance between predicted and target item embeddings (Li et al., 2018). Contrastive learning ap-\nproaches for SRS often incorporate AR objectives as auxiliary signals due to their alignment with the primary task (Xie\net al., 2022; Wang et al., 2023a; Ma et al., 2020).\nBeyond SRS, AR objectives have also been applied in gaming. Zhao et al. (2021) used AR models to generate realistic\naction sequences for role-playing games, leveraging player behavior patterns. Kantharaju and Onta\u00f1\u00f3n (2020) used\nAR objectives to encode gameplay sessions, enabling unsupervised strategy label discovery through embeddings.\nSimilarly, Min et al. (2016) pre-trained action representations for open-world games using AR objectives, enhancing\nperformance on goal recognition tasks.\nAutoregressive models are intuitive for ES since many tasks, such as predicting future purchases, patient visits, or\nplayer actions, naturally align with next-event prediction. However, purely AR training can overemphasize short-\nhorizon patterns at the expense of longer-term dependencies. To address timing gaps more faithfully, Li et al. (2020)"}, {"title": "3.2.3 Temporal Point Processes", "content": "A Temporal Point Process (TPP) is a probabilistic framework designed to model when events occur over time. Rather\nthan predicting the next token in a sequence (as in standard AR models), TPPs specify a conditional intensity function\nthat depends on the history of previous events to determine the timing of future ones. In this sense, TPPs serve as\nAR models with a unique focus on irregular event arrivals. They are particularly suited to domains emphasizing\nthe temporal dimension, such as high-frequency trades in finance (Guo et al., 2018) or retweet cascades in social\nmedia (Zhao et al., 2015).\nWhen modeling ES data, TPPs can be viewed as a specialized form of predictive SSL, since they typically learn without\nexternal labels by maximizing the likelihood of observed event times (and optionally their attributes). Concretely, each\nevent is represented by a timestamp $t_{ui}$ (cf. Equation 1), and in the case of marked TPPs (MTTPs), also by a discrete\nlabel $m_i$. This representation can be limiting for ES with richer metadata D \u2013 for example, numerical or textual\nfeatures - since TPPs often restrict such attributes to a single discrete mark set M. Nonetheless, for problems where\ntiming is paramount and events can be categorized with discrete labels, TPPs offer a principled approach to capturing\ntemporal patterns within ES data.\nES-specific TPP methods. The pioneering work of Du et al. (2016) introduced neural architectures for MTPPs, en-\ncoding event histories with recurrent neural networks and conditioning the intensity function on these representations;\nlater, several methods have been developed for MTPPs (Omi et al., 2019; Shchur et al., 2020; Guo et al., 2018; Mei\net al., 2020; Chen et al., 2021; Zhang et al., 2024). Building on this foundation, Omi et al. (2019) proposed a fully neu-\nral TPP model, relaxing the parametric assumptions of earlier methods, while Shchur et al. (2020) moved away from\nintensity function modeling altogether by directly estimating the distribution of inter-event times. Other approaches\nincorporate Noise Contrastive Estimation (NCE)(Gutmann & Hyv\u00e4rinen, 2010) to optimize TPP parameters without\ncomputing normalizing constants (Guo et al., 2018; Mei et al., 2020). Further innovations include:\n\u2022 Extensions to spatial settings: Spatiotemporal point processes (Chen et al., 2021) incorporate event loca-\ntions, with $D = \\mathbb{R}^d$, but still focus on timing plus a continuous spatial component rather than general high-\ndimensional metadata.\n\u2022 Self-exciting dynamics: Neural adaptations of Hawkes processes (Hawkes, 1971), such as the Neural Hawkes\nProcess (Mei & Eisner, 2017), capture self-exciting event behavior, where occurrences of events raise the\nlikelihood of subsequent arrivals for a period.\n\u2022 Attention and ODE-based TPPs: Transformer adaptations (Zuo et al., 2020; Zhang et al., 2020; Yang et al.,\n2022) improve modeling of long-term dependencies, while neural ODE frameworks (Chen et al., 2021; Zhang\net al., 2024) support continuous-time hidden states for greater flexibility.\n\u2022 Combining contrastive signals: HYPRO (Xue et al., 2022) and the work of Wang et al. (2023b) integrate\ncontrastive learning at the sequence level to refine long-horizon TPP forecasts (see Section 3.3.1 for details).\nCollectively, these TPP variants remain largely domain-agnostic and have been evaluated on diverse benchmarks,\nincluding financial transactions and social streams. The main advantage of TPPs for ES modeling is their direct em-\nphasis on time modeling: instead of assuming regularly sampled data, TPPs natively handle irregular arrival patterns,\nwhich are ubiquitous in real-world ES. For tasks where when an event occurs is as crucial as what happens, e.g.,\npredicting the gap between trades or hospital visits, TPPs are a powerful predictive framework. On the downside,\ntraditional TPPs often require event attributes to be reduced to a discrete mark set, limiting their ability to capture\nricher, multi-dimensional features that many ES domains present (cf. Section 2.2). Consequently, if the ultimate goal\nis task-agnostic SSL-based representation learning (for instance, for clustering patients or analyzing user behavior\nbroadly), TPPs may need complementary mechanisms or hybrid approaches to fully leverage the wealth of metadata\nbeyond simple event timing and discrete categories."}, {"title": "3.3 Contrastive SSL", "content": "Contrastive SSL methods learn a representation space where similar event streams or subsequences are pulled together,\nwhile dissimilar ones are pushed apart (Liu et al., 2021). This high-level approach typically avoids reconstructing\nlow-level details, which may be noisy or irrelevant, and can thereby learn more robust entity-level embeddings. As\nillustrated in Figure 3, contrastive methods remain relatively underutilized in ES, yet they offer strong potential across\ndiverse application settings."}, {"title": "3.3.1 Instance Contrastive", "content": "Instance-based contrastive learning aims to learn discriminative representations by treating different augmentations of\nthe same event stream instance as positive pairs and instances (or their augmentations) from other entities as negative\npairs. This paradigm can be traced back to frameworks like SimCLR (Chen et al., 2020) and MoCo (He et al.,\n2020), which rely on semantic-preserving data transformations, or \u201caugmentations,\u201d to drive representation learning.\nFormally, if A is a set of possible augmentations, we draw two random transformations $a_1, a_2 \\sim A$ and apply them to\nan event stream $S_u$. The resulting views $a_1(S_u) = S_u^{(1)}$ and $a_2(S_u) = S_u^{(2)}$ form a positive pair, while event streams\nfrom other entities play the role of negative samples.\nES-specific instance contrastive. Unlike computer vision, where image augmentations include simple pixel-level\ntransformations such as rotation or color jitter, ES data is irregular and discrete, often containing complex contextual\nattributes. Consequently, augmentations typically focus on:\n\u2022 Subsequence sampling or cropping: Randomly selecting continuous chunks or prefix-suffix segments from\nthe entity's event history (Babaev et al., 2022; Jeong et al., 2024; Ma et al., 2020).\n\u2022 Event masking: Concealing certain events or attributes (like timestamps, categorical codes) to simulate partial\nobservation (Xie et al., 2022; Hou et al., 2022).\n\u2022 Noise injection: Slightly perturbing numeric features (e.g., lab results in healthcare) (Raghu et al., 2023).\n\u2022 Item/interaction reordering: Primarily in recommender systems with loosely constrained temporal order (Xie\net al., 2022; Wang et al., 2023a).\nThese ES-specific augmentations aim to preserve semantic integrity (i.e., the inherent structure of the entity's behavior)\nwhile ensuring enough variation to learn robust, high-level representations. Several representative instance contrastive\nmethods have emerged across different ES domains:\n\u2022 Sequential recommenders: CL4SRec (Xie et al., 2022) combines event masking, subsequence cropping,\nand item reordering to generate positive pairs of user interaction sequences, while negative samples come\nfrom other users. Other works (Qiu et al., 2022; Hou et al., 2022; Wang et al., 2023a) adapt similar ideas,\nsometimes adding supervised pair-mining (e.g., user sessions ending in the same item).\n\u2022 Finance: CoLES (Babaev et al., 2022) proposes random subsequence sampling for transaction logs, using the\nInfoNCE objective to pull together augmentations of the same account's activity while pushing apart different\naccounts.\n\u2022 Healthcare: Raghu et al. (2023) mask entire modalities (e.g., vitals vs. labs) and add noise to EHR data,\nemploying the SimCLR-based NT-Xent loss for contrast. Jeong et al. (2024) define positive pairs as prefix\nand suffix segments around critical hospital visits.\n\u2022 Combining TPP: Wang et al. (2023b) and Xue et al. (2022) combine TPP's autoregressive objectives with an\ninstance contrastive signal to differentiate authentic event continuations from artificially generated ones.\nInstance-based contrastive SSL is conceptually straightforward and avoids the need to reconstruct potentially noisy\nevent attributes, focusing instead on alignment of semantically consistent representations. This can yield robust entity-\nlevel embeddings for complex, high-dimensional streams. However, several practical challenges persist:\n\u2022 Defining valid augmentations: ES data is irregular and domain-specific, making it non-trivial to design trans-\nformations (e.g., reordering vs. partial masking) that preserve critical chronological or contextual relation-\nships.\n\u2022 Negative sampling: Methods must ensure negative pairs genuinely represent different entities or distinct\nbehaviors, especially if multiple entities have partially similar sequences.\n\u2022 Data sparsity: Sparse or short event sequences limit opportunities for subsequence-based augmentations or\nheavier masking without losing crucial information.\nDespite these hurdles, instance-based contrastive learning remains a promising strategy for leveraging unlabeled ES in\ndiverse domains, with growing evidence that carefully chosen augmentations and objectives can substantially improve\ndownstream performance."}, {"title": "3.3.2 Distillation", "content": "Distillation-based contrastive learning avoids explicit negative samples by coupling a teacher network and a student\nnetwork on differently augmented views of the same event stream. Inspired by methods such as BYOL (Grill et al.,"}, {"title": "3.3.3 Feature Decorrelation", "content": "Feature decorrelation methods aim to learn representations with minimal redundancy across latent dimensions, typi-\ncally without explicit negative pairs. Frameworks like Barlow Twins (Zbontar et al., 2021) and VICReg (Bardes et al.,\n2022) proceed by generating two augmented views of the same sequence and passing them through a shared model.\nThe loss function then combines two components: (1) an alignment term, pulling the two latent embeddings closer,\nand (2) a redundancy reduction term, encouraging different latent dimensions to capture distinct information.\nFeature decorrelation remains much less explored than predictive or instance-based contrastive methods in ES contexts.\nOne notable exception is Raghu et al. (2023), who adopt a VICReg-type loss for healthcare EHR data. Their method\nprocesses two augmented views (e.g., partial masking or noise injection across different clinical modalities) and learns\nembeddings via alignment plus decorrelation. Although originally demonstrated in a multimodal EHR setting, it can\nbe adapted to purely structured or partially unstructured event streams, thereby highlighting its potential generality.\nSimilar to distillation, feature decorrelation eliminates the need for negative samples, reducing complexity and en-\nabling diverse representations for high-dimensional ES data. However, its success relies on carefully designed aug-\nmentations and balancing alignment with redundancy reduction."}, {"title": "3.3.4 Multimodal Contrastive", "content": "Multimodal contrastive learning aims to align representations across multiple modalities associated with the same\ndata instance, ensuring that semantically related information from different sources is embedded closer together in\nthe learned space. This approach is commonly exemplified by models like CLIP (Radford et al., 2021), which aligns\nimage-text pairs by training encoders for each modality with a shared contrastive loss. Typically, a positive pair\nconsists of aligned representations (e.g., image and caption), while unrelated data serves as negative pairs. This setup\ncan be adapted for ES data when events are enriched with multimodal features.\nEvent streams in domains like healthcare often contain inherently multimodal data, such as time-series vital signs,\ndiagnostic codes, and accompanying textual notes or summaries. These rich contexts make multimodal contrastive\nlearning a natural fit, though its application remains limited due to the preprocessing required to align modalities\neffectively. For instance, King et al. (2023) explored a multimodal contrastive objective inspired by CLIP for Intensive\nCare Unit (ICU) data, where time-series signals from medical monitors were contrasted with clinical notes. This\nevent-level alignment enabled the model to integrate structured numeric data with unstructured text, providing a more\nholistic representation of patient states. Extending this idea, Ma et al. (2024) applied multimodal contrastive learning\nto align complete ICU stays. Their approach leveraged discharge summaries as high-level textual representations of\nthe patient's trajectory, contrasting these with aggregated time-series features from the entire hospital stay. While this\nextension captured broader temporal contexts, it also required comprehensive summaries as a secondary modality,\nwhich may not always be available.\nMultimodal contrastive methods offer the significant advantage of integrating diverse data types, enabling models to\ncapture richer and more holistic event representations. However, their success relies on the availability and alignment\nof multiple modalities, which can introduce challenges in preprocessing and data curation. Additionally, the reliance\non contrastive objectives across modalities may lead to representational biases if one modality dominates. Neverthe"}, {"title": "3.4 Summary", "content": "The reviewed literature underscores that predictive methods dominate SSL for ES due to their natural alignment with\nthe sequential and contextual structure of event streams. Masked modeling, inspired by advancements in NLP, excels\nat reconstructing missing event attributes and is widely applied across domains such as healthcare, e-commerce, gam-\ning, and finance. Autoregressive approaches similarly align pre-training goals with downstream tasks, making them\nparticularly effective in sequential recommendation systems and time-sensitive applications. TPPs, though highly spe-\ncialized, are uniquely suited for irregular event timing, offering mathematically rigorous frameworks for predicting\nevent occurrences, albeit with limited flexibility for richer metadata or task-agnostic learning.\nIn contrast, contrastive methods remain relatively underexplored, despite their potential to produce robust, entity-\nlevel representations. Instance-based contrastive learning has been applied to healthcare, finance, and recommender\nsystems, leveraging augmentations such as subsequence sampling and event masking. Distillation-based contrastive\nmethods avoid the complexity of negative sampling but require well-designed teacher-student alignments and effective\naugmentations. Feature decorrelation methods, while promising for high-dimensional ES data, have seen limited\napplications and demand further exploration. Emerging multimodal contrastive techniques, particularly in healthcare,\nshow great promise in integrating diverse modalities such as time-series vitals and textual notes, though their success\nhinges on data availability and preprocessing.\nAs seen in Figure 3, predictive approaches are dominant across domains, while contrastive and hybrid paradigms\nremain nascent but hold significant potential. Advancing SSL for ES modeling requires addressing key gaps, including\nthe need for unified frameworks, scalable methodologies, and better benchmarking resources. We believe future\nwork should prioritize exploring underutilized paradigms, such as multimodal and decorrelation-based methods, and\nintegrating insights from both predictive and contrastive approaches to create robust, domain-agnostic SSL solutions\nfor event streams."}, {"title": "4 Datasets and Downstream Tasks", "content": "The success of SSL for ES modeling is dependent on access to robust datasets and well-defined downstream tasks. This\nsection provides an overview of widely used public datasets across domains and highlights the diverse downstream\ntasks SSL methods target."}, {"title": "4.1 Overview of Public Datsets", "content": "ES datasets are categorized by their source applications, such as healthcare, e-commerce, and generic event streams.\nTable 3 lists commonly used datasets, their domains, and descriptions."}, {"title": "4.2 Downstream Tasks", "content": "SSL methods for ES aim to pre-train models that can be fine-tuned or directly applied to real-world tasks. These tasks\nvary by domain and are evaluated using diverse metrics and baselines.\nIn healthcare, tasks often involve predicting patient outcomes, such as risk stratification, disease diagnosis, or mortality\nprediction. For example, BEHRT (Li et al., 2020) and Hi-BEHRT (Li et al., 2022) have been evaluated on tasks like\npredicting hospitalizations or the onset of chronic conditions. Metrics include precision, recall, and area under the\nreceiver operating characteristic (AUROC) curve. Pre-training typically involves masked modeling or contrastive SSL\nto generate robust patient embeddings that can generalize across various prediction tasks (Rasmy et al., 2021; Raghu\net al., 2023).\nIn e-commerce, downstream tasks align closely with SRS objectives, such as next-item prediction or user behavior\nforecasting. Pre-trained models like BERT4Rec (Sun et al., 2019) and ContraRec (Wang et al., 2023a) fine-tune\non datasets like Amazon or Yelp to optimize metrics such as hit rate (HR), normalized discounted cumulative gain\n(NDCG), and mean reciprocal rank (MRR). These tasks often rely on autoregressive or instance-contrastive objectives.\nGaming tasks focus on player behavior modeling, such as goal prediction or style classification. While gaming datasets\nare typically proprietary (Wang et al., 2024), models like Player2Vec (Wang et al., 2024) demonstrate how SSL pre-\ntraining can infer meaningful embeddings for tasks like cheat detection or personalized matchmaking."}, {"title": "5 Progress and Prospects", "content": "The rapid progress and evolvement of SSL for ES modeling have highlighted both promising directions and key gaps.\nIn the following subsections, we (1) explore the potential for domain-agnostic SSL frameworks for ES modeling\n(Section 5.1), leveraging structure similarities across diverse application domains, (2) discuss underexplored modeling\nparadigms (Section 5.2), (3) outline the critical aspect of timestamp information (Section 5.3), and (4) highlight the\nneed for open-sourced benchmarks to enable progress in the field (Section 5.4)."}, {"title": "5.1 Domain-agnostic Learning", "content": "The review of SSL methods for ES modeling highlights their broad applicability across diverse domains. Despite\nthis diversity, many approaches converge in terms of architecture and learning objectives, which can be attributed to\nstructural similarities inherent in the data across these fields. Domains such as recommender systems, game event\nmodeling, and EHRs involve sequential event data generated by discrete entities, including users, players, or patients.\nIndividual interactions, such as user-item engagements, in-game actions, or medical assessments, are treated as discrete\nevents, emphasizing the potential for developing generalized SSL methods for ES data.\nThe consistent structure of ES data and the alignment of learning objectives across domains present an opportunity\nto design generalized SSL frameworks for ES modeling. Such domain-agnostic approaches"}]}