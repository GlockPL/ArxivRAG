{"title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Jiacai Liu", "Rui Yan", "Jujie He", "Chaojie Wang", "Shuicheng Yan", "Yang Liu", "Yahui Zhou"], "abstract": "In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairs-significantly smaller than existing datasets. Using this curated dataset, we developed the Skywork-Reward model series\u2014Skywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8B\u2014with the former currently holding the top position on the RewardBench leaderboard. Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have achieved unprecedented success, demonstrating cap\u0440\u0430-bilities that were previously unattainable in both scope and performance (Achiam et al., 2023; Dubey et al., 2024; Team, 2024; Team et al., 2023, 2024a,b). This rapid advancement has fueled extensive research into aligning LLM outputs with user preferences (Bai et al., 2022). Among the various alignment strategies, reward modeling has emerged as a prominent and scalable ap-proach for capturing these preferences (Lambert et al., 2024; Wang et al., 2024e). Reward models are explicitly trained to evaluate how well the LLM outputs align with the intended responses desired by users, effectively acting as evaluators during both fine-tuning and deployment (Cai et al., 2024; Dong et al., 2024; Wang et al., 2024a,c, 2023).\nDespite its potential, training reward models poses several significant challenges (Lambert et al., 2024), primarily due to the inherent complexity and variability of human preferences, which are difficult to represent exhaustively (Sanderson et al., 2010). Prior research has sought to address these challenges by improving model architectures (Wang et al., 2024a,b) and developing customized loss functions (Cai et al., 2024; Lou et al., 2024; Winata et al., 2024), enabling reward models to better differentiate between nuanced preference pairs. These methods enhance the models' capacity to prioritize preferred responses while minimizing rejected ones, thereby improving alignment with user preferences. In addition to these efforts, the availability and quality of preference data play a pivotal role in the success of reward modeling. Unfortunately,"}, {"title": "2. Related Work", "content": "Recent advancements in applying reinforcement learning techniques (Schulman et al., 2017), particularly Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022; Casper et al., 2023), have shown substantial potential for enhancing LLMs. A key component of RLHF is the development of reward models (Dubey et al., 2024; Gao et al., 2023; Team, 2024), which learn a reward function based on human preferences or task-specific objectives to guide LLMs toward desired behaviors. As discussed by Lambert et al. (2024), reward modeling techniques can be broadly categorized into three categories based on the underlying model types: discriminative models, generative models, and implicit reward models through Direct Preference Optimization (DPO). We briefly describe each of them as follows.\nDiscriminative Models Discriminative reward models are commonly trained using the Bradley-Terry (BT) (Bradley and Terry, 1952) loss, which aims to maximize the reward difference between pairwise comparisons\u2014specifically, between chosen responses and rejected responses. These models estimate the probability that a given response is preferred over an alternative, making them well-suited for binary ranking tasks. While the core BT loss remains a standard compo-nent, considerable research has focused on enhancing data quality and refining the modeling framework. For example, the InternLM2-Reward models (Cai et al., 2024), trained on 2.4 million human-annotated and AI-generated preference samples, are optimized to classify pairwise com-"}, {"title": "3. Method", "content": "In this section, we describe our approach within Skywork-Reward to constructing a lightweight yet high-quality preference dataset tailored for reward modeling. We outline the specific datasets"}, {"title": "3.3. Training Objective", "content": "Following Ouyang et al. (2022), our loss function is defined using the standard Bradley-Terry (BT) model with a pairwise ranking loss:\n$L_{ranking} = \u2212 log (\u03c3 (r_\u03b8 (x, y_c) \u2013 r_\u03b8 (x, y_r)))$, (1)\nwhere $r_\u03b8(x, y_c)$ and $r_\u03b8(x, y_r)$ denote the scalar rewards generated by the reward model \u03b8, given the same prompt x (or context, if x spans multiple conversation turns) with the chosen response $y_c$ and the rejected response $y_r$. We also experimented with several other loss functions that aim to maximize the margin between $r_\u03b8(x, y_c)$ and $r_\u03b8(x, y_r)$. However, we found no performance improvements and, in some cases, observed a decline in model effectiveness. Understanding the reasons behind this trend presents an interesting avenue for future research."}, {"title": "3.3.1. Loss Function Variants", "content": "Beyond the classic Bradley-Terry style loss function (Bai et al., 2022; Ouyang et al., 2022), we experimented with several alternative loss functions, each designed to increase or maximize the margin between the chosen and rejected responses.\nFocal Loss Focal loss (Lin, 2017) is often used in image classification to address class imbalance by emphasizing hard-to-classify examples. In our context, it emphasizes pairwise comparisons where the model struggles to distinguish between chosen and rejected responses. When the reward difference between chosen and rejected responses is negative or small, the weighting term increases. The loss is defined as:\n$L_{Focal} = \u2212 log \u03c3(r_\u03b8(x, y_c) \u2013 r_\u03b8(x, y_r)) \u00b7 (1 \u2212 \u03c3(r_\u03b8 (x, y_c) \u2013 r_\u03b8(x, y_r)))^\u03b3$, (2)\nwhere \u03b3 is the focal loss parameter controlling the down-weighting of easier examples.\nFocal Loss with Penalty (Cai et al., 2024) This variant introduces an additional penalty to further discourage predictions close to a tie (i.e., $\u03c3(r_\u03b8(x, y_c) \u2013 r_\u03b8(x, y_r)) \u2248 0.5$), encouraging the model to make more confident decisions. The loss function is given by:\n$L_{Focal-Penalty} = (1 \u2212 2 max (\u03c3(r_\u03b8(x, y_c) \u2013 r_\u03b8(x, y_r)) \u2013 0.5, 0))^\u03b3 log \u03c3(r_\u03b8 (x, y_c) \u2013 r_\u03b8(x, y_r))$, (3)\nwhere \u03b3 adjusts the emphasis on difficult comparisons.\nHinge Loss Hinge loss (Sch\u00f6lkopf et al., 2001) is widely used in classification problems, particularly with Support Vector Machines (SVMs), to enforce a margin between classes. Here, it enforces a margin between the reward scores of chosen and rejected responses:\n$L_{Hinge} = max(0, m \u2212 (r_\u03b8(x, y_c) \u2013 r_\u03b8(x, y_r)))$, (4)\nwhere m is the margin parameter, encouraging a separation of at least m between the reward scores."}, {"title": "4. Experiment", "content": "This section outlines the training setup, baseline methods for comparison, and evaluation criteria (section 4.1). We then present quantitative results and provide insights gained from the experiments (section 4.2)."}, {"title": "4.1. Experimental Setup", "content": ""}, {"title": "4.1.1. Training", "content": "Hyperparameters and Training We use existing aligned models, Meta-Llama-3.1-8B-Instruct (Dubey et al., 2024) and Gemma-2-27B-it (Team, 2024), as backbones, replacing the final layer with a randomly initialized reward head. Both models are trained with a global batch size of 128, using AdamW as the optimizer with a weight decay of 1e-3 and a cosine learning rate schedule. Training spans 2 epochs on the Skywork Reward Preference 80K dataset. The learning rate is set to 2e-6 for the 8B model and 1e-6 for the 27B model."}, {"title": "4.1.2. Baselines and Evaluation", "content": "Preference Dataset Baselines To demonstrate the advantages of the Skywork Reward Prefer-ence 80K dataset, we compare it with the dataset mixture from RLHFlow (Dong et al., 2024), which serves as a baseline. RLHFlow integrates data from several well-known preference sources, including HH-RLHF (Bai et al., 2022), SHP (Ethayarajh et al., 2022), HelpSteer (Wang et al., 2023), PKU-SafeRLHF (Ji et al., 2024), UltraFeedback (Cui et al., 2023), UltraInteract (Yuan et al., 2024), Distilabel-Capybara (Daniele and Suphavadeeprasit, 2023), and Distilabel-Orca (Lian et al., 2023). This dataset mixture comprises approximately 700K samples, which we denote as Preference 700K.\nWe train both the 8B and 27B models following the approach outlined by Dong et al. (2024). Additionally, we perform an ablation study by using only the 378K samples from our full dataset to validate the effectiveness of our filtering process. For the 378K dataset, we train for 2 epochs to ensure the number of gradient updates matches those used for Preference 700K and Skywork Reward Preference 80K.\nReward Model Baselines We compare the performance of our reward models, trained on Skywork Reward Preference 80K, with the top-performing models from the RewardBench leaderboard. As of this writing, the leading reward models include SFR-LLaMa-3.1-70B-Judge-I, Nemotron-4-340B-Reward (Wang et al., 2024e), ArmoRM (Wang et al., 2024b), SFR-nemo-12B-Judge-r, and InternLM-20B-Reward (Cai et al., 2024).\nEvaluation on RewardBench Our models are evaluated on RewardBench (Lambert et al., 2024), a benchmark designed to assess reward models across multiple tasks, such as chat, reasoning, and safety. RewardBench contains prompt-chosen-rejected trios that measure a model's ability to assign higher scores to the chosen response compared to the rejected one. These trios are derived from diverse datasets, covering general chat, safety, and reasoning domains. Successful performance on this benchmark requires reward models to exhibit balanced and robust capabilities across all categories, rather than excelling in only one area."}, {"title": "4.2. Experimental Results", "content": "We present our main results in table 2. Below are key observations:\nSmall but high-quality datasets yield the best reward models. Skywork-Reward-Gemma-2-27B ranks first on RewardBench, while Skywork-Reward-Llama-3.1-8B surpasses all models except SFR-LLaMa-3.1-70B-Judge-I. Despite the smaller model size, a straightforward training approach, and limited training data, our models demonstrate robust performance across all four categories, excelling particularly in the adversarial preference category on Chat Hard. Notably, the 27B reward model is the only model to achieve a score above 90 on Chat Hard, outperforming the next-best model, Nemotron-4-340B-Reward, by more than four points, with a score of 87.1.\nQuality over quantity. As shown in table 2, Llama 3 trained on the complete 378K samples outperforms both reward models trained on Preference 700K, as well as most other models, with the exception of SFR-LLaMa-3.1-70B-Judge-I and Nemotron-4-340B-Reward. Compared"}, {"title": "4.3. Potential Prompt Contamination", "content": "During the preparation of this manuscript, we were informed by the RewardBench (Lambert et al., 2024) team of a potential contamination involving approximately 5K prompts from the Magpie Ultra\u201d (Xu et al., 2024) subset, which may overlap with prompts present in the RewardBench evaluation set. Although the root cause of the overlap remains unclear, the RewardBench team suspects that Llama-3.1-405B-Instruct (Dubey et al., 2024), which was used to generate the Magpie Ultra dataset, may have been trained on these prompts.\nRewardBench evaluations rely on external sources (e.g., LLMBar (Zeng et al., 2023)), some of which contain prompts derived from widely utilized training datasets, such as Alpaca (Taori et al., 2023). This overlap has inadvertently introduced contamination into the Skywork Reward Preference 80K v0.1 datasets. To address this issue, we applied a decontamination script provided by the RewardBench leaderboard maintainers to compute detailed contamination statistics, as presented in table 4. We subsequently removed all pairs containing contaminated prompts from the Magpie Ultra subset, resulting in the creation of the v0.2 version of the Skywork Reward Preference 80K dataset.\nIt is worth noting that some minor contamination likely persists across other subsets. These instances are scattered and originate from various sources, making them challenging to detect, though they are likely benign. As we show in later sections, removing contamination leads to improved performance in our reward models.\nPervasive Contamination in (Synthetic) Preference Data It is important to acknowledge that the contamination issue is not unique to Skywork Reward Preference 80K. Other widely used preference datasets, such as Preference 700K (Dong et al., 2024) and Nectar (Zhu et al., 2023), are similarly affected. These datasets are frequently employed to train many open-weight reward models on the RewardBench leaderboard, including several top-ranking models. In table 4, we show that Preference 700K contains a considerable number of prompts matching those in the RewardBench test set, both in terms of coverage and absolute counts. This underscores the need for more comprehensive investigations into data contamination and stricter dataset selection criteria in evaluations."}, {"title": "5. Closing Remarks", "content": "In this report, we introduce the Skywork-Reward Preference 80K data collection and demonstrate that carefully curated smaller, high-quality datasets can outperform both the complete data composition and much larger counterparts. Despite using fewer samples and a straightforward training setup, our models\u2014Skywork-Reward-Gemma-2-27B and Skywork-Reward-Llama-3.1-8B-have achieved state-of-the-art performance on RewardBench, excelling across multiple categories and setting a new benchmark in the Chat Hard category. These results highlight the value of prioritizing data quality over quantity, as well as the importance of targeted filtering and selection in the construction of preference datasets. Our findings emphasize that careful curation not only reduces data redundancy but also improves overall performance. We also addressed the pervasive issue of prompt contamination by releasing a decontaminated v0.2 version of the dataset, which further empirically improved scores across most categories. Furthermore, our experiments reaffirmed the Bradley-Terry loss as the most effective loss function in our setting, striking the optimal balance across various tasks. These findings underscore the necessity of precise alignment between datasets and evaluation criteria, providing valuable insights for the development and assessment of reward models."}]}