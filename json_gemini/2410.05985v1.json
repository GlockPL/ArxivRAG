{"title": "ASYNCHRONOUS STOCHASTIC GRADIENT DESCENT\nWITH DECOUPLED BACKPROPAGATION AND LAYER-\nWISE UPDATES", "authors": ["Cabrel Teguemne Fokam", "Khaleelulla Khan Nazeer", "Lukas K\u00f6nig", "David Kappel", "Anand Subramoney"], "abstract": "The increasing size of deep learning models has created the need for more efficient\nalternatives to the standard error backpropagation algorithm, that make better use\nof asynchronous, parallel and distributed computing. One major shortcoming of\nbackpropagation is the interlocking between the forward phase of the algorithm,\nwhich computes a global loss, and the backward phase where the loss is back-\npropagated through all layers to compute the gradients, which are used to update\nthe network parameters. To address this problem, we propose a method that paral-\nlelises SGD updates across the layers of a model by asynchronously updating them\nfrom multiple threads. Furthermore, since we observe that the forward pass is of-\nten much faster than the backward pass, we use separate threads for the forward\nand backward pass calculations, which allows us to use a higher ratio of forward\nto backward threads than the usual 1:1 ratio, reducing the overall staleness of the\nparameters. Thus, our approach performs asynchronous stochastic gradient de-\nscent using separate threads for the loss (forward) and gradient (backward) com-\nputations and performs layer-wise partial updates to parameters in a distributed\nway. We show that this approach yields close to state-of-the-art results while\nrunning up to 2.97\u00d7 faster than Hogwild! scaled on multiple devices (Locally-\nPartitioned-Asynchronous-Parallel SGD). We theoretically prove the convergence\nof the algorithm using a novel theoretical framework based on stochastic differ-\nential equations and the drift diffusion process, by modeling the asynchronous\nparameter updates as a stochastic process.", "sections": [{"title": "1 INTRODUCTION", "content": "Scaling up modern deep learning models requires massive resources and training time. Asyn-\nchronous parallel and distributed methods for training them using backpropagation play a very\nimportant role in easing the demanding resource requirements for training these models. Back-\npropagation (BP) (Werbos, 1982) has established itself as the de facto standard method for learning\nin deep neural networks (DNN). Although BP achieves state-of-the-art accuracy on literally all rel-\nevant machine learning tasks, it comes with a number of inconvenient properties that prohibit an\nefficient implementation at scale.\nBP is a two-phase synchronous learning strategy in which the first phase (forward pass) computes the\ntraining loss, L, given the current network parameters and a batch of data. In the second phase, the\ngradients are propagated backwards through the network to determine each parameter's contribution\nto the error, using the same weights (transposed) as in the forward pass (see Equation 1). BP suffers\nfrom update locking, where a layer can only be updated after the previous layer has been updated.\nFurthermore, the computation of gradients can only be started after the loss has been calculated in\nthe forward pass."}, {"title": null, "content": "Moreover, the backward pass usually requires approximately twice as long as the forward pass (Ku-\nmar et al., 2021). The bulk of the computational load comes from the number of matrix multiplica-\ntions required during each phase. If we consider a DNN with M layers, then at any layer m \u2264 M\nwith pre-activations zm = 0mYm-1 and post-activations Ym = f(zm\u22121), the computations dur-\ning the forward pass are dominated by one matrix multiplication OmYm\u22121. During the backward\npass, the computations at layer m are dominated by two matrix multiplications:\n$\\frac{d\u13dd}{d\u0398_m}=f'(\u0398_mY_{m-1}) \u00d7 Y_{m-1}^T$ and $\\frac{d\u13dd}{dY_{m-1}}=f'(\u0398_mY_{m-1}) \u00d7 \u0398_m$, (1)\napproximately doubling the compute budget required for the backward pass compared to the for-\nward pass. In Eq. 1, Om denotes the network weights at layer m and f' the partial derivative with\nrespect to Om. This imbalance between forward and backward phase further complicates an efficient\nparallelization of BP, particularly in heterogeneous settings.\nIn this work, we propose a new approach to parallelize training of deep networks on non-convex\nobjective functions by asynchronously performing the forward and backward passes at a layer-wise\ngranularity in multiple separate threads that make lock-free updates to the parameters in shared\nmemory. The lock-free updates address the locking problem, performing layer-wise updates mit-\nigates the issue of conflicts between parameter updates, and performing the backward pass and\nupdates using more threads than the forward pass mitigates the staleness problem. Specifically, the\nimbalance in execution time between forward and backward passes is taken care of by having twice\nas many backward threads than forward threads, breaking the 1:1 ratio of vanilla Backpropagation,\ntherefore significantly speeding-up the training process.\nIn summary, the contributions of this paper are as follows:\n1. We introduce a novel asynchronous formulation of Backpropagation which allows the for-\nward pass and the backward pass to be executed separately and in parallel, which allows us\nto run more backward than forward threads. This approach accounts for the unequal time\nrequired by the forward and backward passes.\n2. We propose to asynchronously update the model's parameters at a layer-wise granularity\nwithout using a locking mechanism which reduces staleness.\n3. We give convergence guarantees of the algorithm to a stationary distribution centered\naround the local optima of conventional BP.\n4. We show that the algorithm can reach state-of-the-art performances while being signifi-\ncantly faster than competing asynchronous algorithms."}, {"title": "2 RELATED WORK", "content": "Asynchronous stochastic gradient descent (SGD). Asynchronous SGD has a long history, starting\nfrom Baudet (1978); Bertsekas & Tsitsiklis (2015). Hogwild! (Recht et al., 2011) allows multiple\nprocesses to perform SGD without any locking mechanism. Kungurtsev et al. (2021) proposed\nto partition the models parameters across the workers on the same device to perform SGD on the\npartitions. Chatterjee et al. (2022) decentralizes Hogwild! and PASSM+ to allow parameters or\ntheir partitions to be located on multiple devices and perform Local SGD on them. Zheng et al.\n(2017) compensated the delayed gradients with a gradient approximation at the current parameters.\nUnlike these methods, we don't run multiple forward passes in parallel and don't need any gradient\ncompensation scheme.\nNadiradze et al. (2021) provides a theoretical framework to derive convergence guarantees for a wide\nvariety of distributed methods. Mishchenko et al. (2022) proposes a method of \u201cvirtual iterates\" to\nprovide convergence guarantees independent of delays. More recently, Even et al. (2024) proposed\na unified framework for convergence analysis of distributed algorithms based on the AGRAF frame-\nwork. There have been lots of other analysis methods proposed for deriving convergence guarantees\nfor asynchronous distributed SGD (see Assran et al. (2020) for a survey). In our work, we propose\nan entirely novel framework that hasn't been used before based on stochastic differential equations,\nand provide convergence guarantees of the algorithm to a stationary distribution centered around the\nlocal optima of conventional BP.\""}, {"title": null, "content": "Communication-efficient algorithms. One of the bottlenecks when training on multiple devices\nor nodes in parallel is the synchronization step. The bigger or deeper the models get, the more time\nis consumed by synchronization. PowerSGD computes low-rank approximations of the gradients\nusing power iteration methods. Poseidon (Zhang et al., 2017) also factorizes gradients matrices\nbut interleaves their communication with the backward pass. Wen et al. (2017) and Alistarh et al.\n(2017) quantize gradients to make them lightweight for communication. Like Zhang et al. (2017),\nwe interleave the backward pass with gradients communication but without gradients averaging.\nBlock local learning. Dividing the network across multiple devices and performing local updates is\na widely recognized approach in distributed learning. The backward passes of the different blocks\ncan be done simultaneously. The global loss is used to provide feedback only to the output block\nwhile the remaining blocks get learning signals from auxiliary networks which each compute local\ntargets. Ma et al. (2024) uses a shallower version of the network as the auxiliary network at each\nlayer. Gomez et al. (2022) allows gradients to flow to k-neighboring blocks. N\u00f8kland & Eidnes\n(2019) don't allow gradients to flow to neighboring blocks, and instead use an auxiliary matching\nloss and a local cross-entropy loss to compute the local error. Decoupled Parallel Backpropagation\n(Huo et al., 2018) does full Backpropagation but uses stale gradients in the blocks to avoid update\nlocking. Kappel et al. (2023) take a probabilistic approach by interpreting layers outputs as param-\neters of a probability distribution. Auxiliary networks provide local targets, which are used to train\neach block individually. Similar to these distributed paradigms, we mimic the execution of multiple\nbackward passes in parallel by reordering the training sequence but without splitting the network\nexplicitly during forward and backward propagation."}, {"title": "3 METHODS", "content": null}, {"title": "3.1 ASYNCHRONOUS FORMULATION OF BACKPROPAGATION", "content": "We introduce a new asynchronous stochastic gradient descent method where, instead of performing\nthe forward and backward phases sequentially, we execute them in parallel and perform layer-wise\nparameter updates as soon as the gradients for a given layer are available. The dependencies between\nforward and backward phases are illustrated in Figure 1.\nSince the gradient computation in the backward pass tends to consume more time than the loss\ncalculation in the forward pass, we decouple these two into separate threads and use one forward\nthread and two backward threads to counterbalance the disproportionate execution time.\nFigure 1 illustrates the interaction among threads based on one example. Initially, only the first\nforward pass, F0, is performed. The resulting loss is then used in the first backward pass B1,"}, {"title": null, "content": "F1 ends, its loss is used by B2\nwhich starts in parallel to the second forward pass F1. Once F\nrunning in parallel to the next forward pass and B1."}, {"title": "3.2 LAYER-WISE UPDATES", "content": "Parallelizing the forward and backward passes can speed up training, but it violates several key\nassumptions of Backpropagation leading to sub-optimal convergence observed in different studies\n(Keuper & Preundt, 2016; Zheng et al., 2017). This happens because the losses and gradients are\noften calculated using inconsistent and outdated parameters.\nTo alleviate this problem, we update the layers as soon as the corresponding gradients are available\nfrom the backward pass. F1 receives partial parameter updates from B1 as soon as they are\navailable. Therefore, the parameters used in F1 will differ from those used in F0 because some\nlayers of the model would have been already updated by the thread B1. On average, we can expect\nthat the second half of the layers use a new set of parameters. It is important to note that the updates\nhappen without any locking mechanism and asynchronous to the backward pass as done by Zhang\net al. (2017)."}, {"title": "3.3 SPEED-UP ANALYSIS", "content": "Before discussing experimental results, we study the potential speed-up of the asynchronous with\nlayer-wise updates formulation over standard Backpropagation. To arrive at this result, we make the\nfollowing assumptions to estimate the performance gain\n\u2022 We assume that there are no delays between the end of a forward pass, the beginning of\nits corresponding backward pass and the next forward pass. This implies for example that\nF0 ends, F1 and B1 begin immediately. Multiples backward threads are\nas soon as\ntherefore running in parallel.\n\u2022 Vanilla Backpropagation performs b forward passes. We assume that this number also\ncorresponds to the number of backward passes and the number of batches of data to be\ntrained on.\n\u2022 A forward pass lasts T units of time and a backward pass \u03b2T units of time, with a scaling\nfactor \u03b2 > 1 (expected to be at around 2 as show in appendix A.3).\nThe speed-up factor A observed can be express as the fraction between the estimated time taken by\nthe standard over the Async version of BP. Let T\u2081 be the time taken by BP to be trained on b batches\nof data. Because the forward and backward passes are sequential for each batch of data, we have:\nT\u2081 = Tb + b\u03b2T\n= (b + \u03b2)T .\nNow let T2 be the time taken by the asynchronous algorithm to be trained on b batches of data, and\nn be the number of remaining backward passes to be executed after all the forward passes have been\nperformed.\nWe have n = \u03b2 and T2 = bT + nT = Tb + \u03b2T. The speed-up factor A is the ratio of T\u2081 by T2,\nwhich by inserting the results above gives\n\u03bb =\n(1 + \u03b2)b\n(b + \u03b2)\nBy taking the limit of large number of batches, b \u2192 \u221e, we have\n\u03bb = 1 + \u03b2.\nHence, the maximum achievable speedup is expected to be 1 + \u03b2, where \u03b2 is the scaling factor of\nthe backward pass time. In practice, the speed-up factor A can be influenced by multiples factors\nlike data loading which is sometimes a bottleneck (Leclerc et al., 2023; Isenko et al., 2022), or the\nsystem overhead, which reduce the achievable speedup."}, {"title": "3.4 STALENESS ANALYSIS", "content": "Here, we demonstrate the advantage of applying layer-wise updates (LU) compared to block updates\n(BU). BU refers to performing updates only after the entire backward pass is complete, a technique\nused in various previous asynchronous learning algorithms, e.g. (Recht et al., 2011; Chatterjee et al.,\n2022; Zheng et al., 2017). We use the same notation as in section 3.3.\nTo express this formally, we define the relative staleness 7 of BU compared to LU as the time delay\nbetween when the gradients become available and when they are used to update the model weights.\nThe intuition behind this lies in the fact that the more the updates are postponed, the more likely the\ngradients will become stale. The staleness will only increase with with time and accumulate across\nthe layers. Assuming that the time required to compute the gradients for each layer is uniform and\nequal to T, the relative staleness is expressed as \u03c4 =\n\u03b2T(M-1)\n2\nTo see this, we use that by definition, the staleness increases as we approach the output layer. At any\nlayer m, the layer-wise staleness \u03c4m =\nm/M. Averaging over the layers, we have\n\u03c4 =\nM\n\u2211\nm=1\n\u03c4m =\n\u03b2T\nM\nM\n\u2211\nm=1\nm =\n\u03b2T\n,(\u041c \u2013 1)\n2\nClearly, 7 increases with the network's depth and the time required to perform one backward pass.\nThus, the speedup is expected to scale approximately linearly with the network depth, showing the\nadvantage of LU over BU for large M."}, {"title": "3.5 ALGORITHM", "content": "The Async BP algorithm is illustrated in Figure 1 and described in Algorithm listing 1. The al-\ngorithm consists of two components: a single forward thread and multiple backward threads. All\nthreads work independently and asynchronously without any locking mechanism.\nThe forward thread is solely responsible for computing the loss Li(0u, xi, yi), given the current\nmini batch of data (xi, yz) \u2208 D and the latest set of updated weights \u03b8\u03c5. Since the algorithm\nworks asynchronously, the weights \u03b8 can be updated by any backward thread even while forward\npass progresses. Once the forward pass is done, Li is sent to one of the backward threads and the\nforward thread moves to the next batch of data.\nIn parallel, a backward thread k receives a loss Lj and performs the backward pass. At each layer\nm, the gradients G(0m,k) = \\frac{2L_j}{2\u0398_{m,k}} are computed, after which Omk is immediately used to update\nthe forward thread parameters. Note that the backward thread here can potentially calculate the\ngradients for a different values of parameters Omk than the ones used for the forward pass \u03b8\u03c5. In\nSection 5 and appendix B we show that this algorithm closely approximates conventional stochastic\ngradient descent, if asynchronous parameter updates arrive sufficiently frequent."}, {"title": "4 RESULTS", "content": "We evaluate our method on three vision tasks, CIFAR-10, CIFAR-100 and Imagenet, and on one\nsequence modeling task: IMDb sentiment analysis. We use Resnet18 and Resnet50 architectures for\nvision tasks and a LSTM network for sequence modelling. These networks are trained on a machine\nwith 3 NVIDIA A100 80GB PCIe GPUs with two AMD EPYC CPUs sockets of 64 cores each. The\nexperiment code is based on the C++ frontend of Torch (Paszke et al., 2019) - Libtorch.\nThe Performance of these tasks is compared to Locally-Asynchronous-Parallel SGD (LAPSGD)\nand Locally-Partitioned-Asynchronous-Parallel SGD (LPPSGD) (Chatterjee et al., 2022). These\nmethods extend the well-known Hogwild! algorithm and Partitioned Asynchronous Stochastic Sub-\ngradient (PASSM+) to multiple devices, respectively.\nWe record the achieved accuracy on the tasks and the wall-clock time to reach a target accuracy\n(TTA). If not stated otherwise, this accuracy is chosen to be the best accuracy achieved by the worst\nperforming algorithm. We used the code made available by Chatterjee et al. (2022) which uses\nPytorch Distributed Data-Parallel API."}, {"title": "4.1 ASYNCHNOUS TRAINING OF VISION TASKS", "content": "We follow the training protocol of LAPSGD and LPPSGD and chose the number of processes per\nGPU to be 1 since the GPU utilization was close to 100%. We trained them with a batch size of\n128 per-rank. We used Stochastic gradient descent (SGD) for both Async BU and LU, with an\ninitial learning rate of 0.005 for 5 epochs and 0.015 after the warm-up phase, a momentum of 0.9\nand a weight decay of 5 \u00d7 110-2. We use a cosine annealing schedule with a Tmax of 110. We\ntrained Resnet-50 on Imagenet-1K task (Table 5) for a total of 300 epochs with the same learning\nrate but with a cosine schedule with Tmax of 250 and weight decay of 3.5 \u00d7 110-2. Although,\nthe simulations were run with cosine annealing scheduler, implying the ideal number of training\nepochs, early stopping was applied, i.e. training was stopped if no improvement of the accuracy was\nachieved for 30 epochs.\nAs shown below, Async LU achieves the highest accuracies while Async BU converges the fastest\nin terms of time to reach the target accuracy. The CIFAR-10 and CIFAR-100 results are presented\nin Tables 1, 2 and Tables 3, 4 respectively. In Tables 1 and 3, the time to target accuracy (TTA) is\nchosen to be the time taken to achieved the best accuracy reached by the worst algorithm. Whereas\nin Tables 2 and 4, it represents taken by an algorithm achieve its best accuracy. Async BU achieves a\nspeed-up of up to 2.97\u00d7 over LPPSGD on CIFAR100 (see Table 3). The poor performance of both\nLAPSGD and LPPSGD can be explained by the influence of staleness, thus requiring large number\nof training epochs.\nWe also achieved promising results on the ImageNet-1k dataset (see Table 5). Async LU achieved\n73% accuracy \u00d73 faster than Backpropagation on single GPU, showing potential of ideal linear\nscaling. An extensive comparison of Async LU with multi-GPU Backpropagation (Data Distributed\nParallel) is provided in appendix A.2.\nAlthough Async BU converges quicker than Async LU, it reaches lower accuracy. This is particu-\nlarly visible on CIFAR100, a harder task than CIFAR10 (see Figures 2 and 3). Overall, Async BU\nshowed a good balance between convergence speed and reduction of staleness."}, {"title": "4.2 ASYNCHRONOUS TRAINING OF SEQUENCE MODELLING TASK", "content": "For demonstrating Async BP training on sequence modelling, we evaluated an LSTM networks on\nthe IMDb dataset (Maas et al., 2011). Sentiment analysis is the task of classifying the polarity of\na given text. We used a 2-Layer LSTM network with 256 hidden dimensions to evaluate this task.\nWe trained the network until convergence using the Adam optimizer with an initial learning rate of\n1 x 10-2. Results are shown in Table 6."}, {"title": "5 THEORETICAL ANALYSIS OF CONVERGENCE", "content": "Here, we theoretically analyse the convergence behavior of the algorithm outlined above. For the\ntheoretical analysis, we consider the general case of multiple threads, acting on the parameter set 0,\nsuch that the threads interact asynchronously and can work on outdated versions of the parameters.\nWe model the evolution of the learning algorithm as a continuous-time stochastic process (Bellec\net al., 2017) to simplify the analysis. This assumption is justified by the fact that learning rates are\ntypically small, and therefore the evolution of network parameters is nearly continuous.\nIn the model studied here, the stochastic interaction between threads is modelled as noise induced by\nrandom interference of network parameters. To arrive at this model, we use the fact that the dynam-\nics of conventional stochastic gradient descent (SGD) can be modelled as the system of stochastic\ndifferential equations that determine the dynamics of the parameter vector \u03b8\nd\u03b8k =\n-\u03b7\n\u2202\n\u2202\u03b8k\nL(0)dt +\n\u221a2\n\u03b7 \u03c3SGD\ndWk,\n(2)\nwith learning rate \u03b7 and where dWk are stochastic changes of the Wiener processes.\nEq. 2 describes the dynamics of a single parameter \u03b8k. The dynamics is determined by the gradient\nof the loss function L, and the noise induced by using small mini-batches modelled here as idealized\nWiener process with amplitude OSGD. Because of this noise, SGD does not strictly converge to a\nlocal optimum but maintains a stationary distribution p* (\u03b8k) xe\u00af/L(0%), that assigns most of the\nprobability mass to parameter vectors that reside close to local optima (Bellec et al., 2017)."}, {"title": null, "content": "In the concurrent variant of SGD studied here, however, the dynamics is determined by perturbed\ngradients for different stale parameters. When updating the network using the described asyn-\nchronous approach without locking, we potentially introduce noise in the form of partially stale\nparameters or from one thread overwriting the updates of another. This noise will introduce a de-\nviation from the ideal parameter vector \u03b8. We model this deviation as additive Gaussian noise\n\u03be ~ N(0, STALE) to the current parameter vector with variance osTALE. To approximate the noisy\nloss function, we use a first-order Taylor expansion around the noise-free parameters:\nL(0 + \u00a3) = L(0) + VoL(0)\u00af\u00a7 + O(\u03c3\u00b2)\n\u2248 L(0) + VoL(0)\u03a4\u03be,\n(3)\nand thus the gradient can be approximated as\n\u2207\u0473L(0+ \u00a7, X, Y) \u2248 \u2207\u00aeL(0) + \u22072L(0)\u00af\u00a7.\n(4)\nBased on this, we can express the update rule as a Stochastic Differential Equation (SDE) and model\nthe various noise terms using a Wiener Process W. The noise sources in the learning dynamics\ncome from two main sources, (1) noise caused by stochastic gradient descent, and (2) noise caused\nby learning with outdated parameters. We model the former as additive noise with amplitude OSTALE\nand the latter using the Taylor approximation Eq. (4). Using this, we can write the approximate\ndynamics of the parameter vector e as the stochastic differential equation\nd0k = \u03bc\u03ba(0,t) + \u221aDk(0) dWk,\n(5)\nwith\n\u03bc\u03b5 (\u03b8) = -\u03b7\n\u2202\n\u2202\u03b8k\nL(0)\nDk (0) =\n2\nSGD\n2\n(6)\n+\nSTALE\n22\n\u2202k\u2202l\nL(0),\nl"}, {"title": "6 DISCUSSION", "content": "In this work, we introduced a novel asynchronous approach to train deep neural networks that de-\ncouples the forward and backward passes and performs layer-wise parameter updates. Our method\naddresses key limitations of standard backpropagation by allowing parallel execution of forward and\nbackward passes and mitigating update locking through asynchronous layer-wise updates.\nThe experimental results demonstrate that our approach can achieve comparable or better accuracy\nthan synchronous backpropagation and other asynchronous methods across multiple vision and lan-\nguage tasks, while providing significant speedups in training time. On CIFAR-10 and CIFAR-100,\nwe observed speedups of up to 2.97\u00d7 compared to asynchronous SGD covering a broad range of\nparadigms. The method also showed promising results on a sentiment analysis task and the Ima-\ngeNet classification task where it reached close to ideal scaling.\nOur theoretical analysis, based on modeling the learning dynamics as a continuous-time stochastic\nprocess, provides convergence guarantees and shows that the algorithm converges to a stationary\ndistribution closely approximating that of standard SGD under certain conditions. This offers a\nsolid foundation for understanding the behavior of our asynchronous approach.\nWhile our implementation using C++ and LibTorch demonstrated the potential of this method, we\nalso identified some limitations related to GPU resource allocation in SIMT architectures. Future\nwork could explore optimizing the implementation for more efficient GPU utilization, or investigat-\ning hybrid CPU-GPU approaches to fully leverage the benefits of asynchronous execution.\nOverall, this work presents a promising direction for scaling up deep learning through asynchronous,\ndecoupled updates. The approach has the potential to enable more efficient training of large-scale\nmodels, particularly in distributed and heterogeneous computing environments. Further research\ncould explore extensions to even larger models, additional tasks, and more diverse hardware setups\nto fully realize the potential of this asynchronous training paradigm."}, {"title": "REPRODUCIBILITY", "content": "We ensure that the results presented in this paper are easily reproducible using just the information\nprovided in the main text as well as the supplement. Details of the models used in our simulations\nare presented in the main paper and further elaborated in the supplement. We provide additional\ndetails and statistics over multiple runs in the supplement section A.4. We use publicly available"}, {"title": "A FUTHER RESULTS", "content": null}, {"title": "A.1 LEARNING CURVES", "content": "Here, we provide additional details to the CIFAR10 and IMDb results provided in the main text.\nFigures 3 and 4 show the learning dynamics of Asynchrounous Backpropagation with blocks updates\n(Async BU) and with layer-wise updates (Async LU) on CIFAR10 and IMDb respectively. The\ndifference in convergence speed and accuracy observed with CIFAR100 2 is less noticeable on\nCIFAR10, probably because it is a simpler task. However, we clearly see the advantage of Async\nLU on the IMDb, where it not only converges faster but also to similar accuracy."}, {"title": "A.2 SPEED-UP COMPARISON WITH MULTI-GPU BACKPROPAGATION", "content": "Here we do a comparison of Asynchronous Backpropagation with layer-wise updates (Async LU)\nand multi-GPU Data Distributed Parallel(DDP) both trained on 3 GPUs residing on the same ma-\nchine (described in section 4) to achieve their accuracies. Since Async LU uses only one forward\npass, we set its batch size to be 128 and that of BP to 3\u00d7 higher (384). Async LU was implemented\non the c++ library of Pytorch, Libtorch, and implemented using Pytorch DataDistributedParallel\n(DDP) API. The hyperparameters used are the same as described in section 4.\nTo make the comparison fair, the relative speed-up is calculated with respect of the single GPU\nimplementation of Backpropagation on Libtorch for Async LU and Pytorch for DDP.\nIn this Settings, DDP should clearly the advantages since it uses a bigger batch size and all the GPUs\nare on the same machine, reducing considerably the communication bottleneck, hence making the\nsynchronization step faster. However with observe that Async LU achieves comparable relative\nspeed-up over single GPU compared to DDP on both CIFAR10 and CIFAR100. This shows the\neffectiveness of our asynchronous formulation (figure 1). We can expect Async LU to have some\nadvantage in a multi-node or heterogeneous setting because the synchronization barrier becomes\nproblem."}, {"title": "A.3 TIME MEASUREMENTS", "content": "Here we provide the results of a small scale experiment on the timing measurement of forward and\nbackward passes for CIFAR-100 with batch size 128 in table 9. As expected, a single backward\nrequires 2x of a single backward pass. Extensive experiments on this is provided by Kumar et al.\n(2021)"}, {"title": "A.4 HYPERPARAMETERS FOR THE EXPERIMENTS", "content": "Hyperparameters used in training experiments presented in section 4 are documented in table 10"}, {"title": "B CONVERGENCE PROOF", "content": "Here we provide the proof that the stochastic parameter dynamics, Eq. (5) of the main text, converges\nto a stationary distribution p* (0) given by\np*(0) =\n1\nZ\nexp\n(x(0)), with (0) =\n(7)\n\u03bc\u03ba (\u03b8)\nd0 - ln |Dk(0)| + C .\nDk (0)\nThe proof is analogous to the derivation given in Bellec et al. (2017), and relies on stochastic calculus\nto determine the parameter dynamics in the infinite time limit. Since the dynamics include a noise\nterm, the exact value of the parameters (t) at a particular point in time t > 0 cannot be determined,\nbut we can describe the distribution of parameters using the Fokker-Planck formalism, i.e. we\ndescribe the parameter distribution at time t by a time-varying function pFP (0, t)."}, {"title": null, "content": "To arrive at this result, we plug in the assumed stationary distribution into Eq. (8) and show the\nequilibrium  = 0, \u0456.\u0435.\n \u03a30 [\u03bc\u03b5 (0) PFP (0, 1)]\n22\nPFP(0,t) = -30 02 [Dk(0)pFP (0,t)] = 0\n\u03a30 [\u03bc\u03b5 (\u03b8)PFP (0, t)]\nk\n\u10db\n\u2194\n(9)\nwhere we used the simplifying assumption,  = = 0,\u2200j \u2260 k, as outlined above. Next, using\n \u03a30 [\u03bc\u03b5 (\u03b8)PFP (0, t)]\n\u10db \u03a3 20 Dk (0) PFP (0,t)\n, we get\n\u2194\nPFP(0,t) = -(0) (()()) -\nPFP = 0\nThis shows that the simplified dynamics, Eq. 6, leave the stationary distribution (7) unchanged.\nThis stationary distribution p* (0) is a close approximation to SGD. To see this, we study the maxima\nof the distribution, by taking the derivative\n  ln |Dk(0)|,\nwhich by inserting (6) can be written as\nhk(0) =\n  +\n  .\nIf ostale is small compared to osGD we recover the cannonical results for SGD 0hk(0) \u2248\n where smaller learning rates n make the probability of reaching local optima more\npeaked. Distortion of local optima, which manifests in the second term in the nominator, only de-\npend on third derivatives, which can be expected to be small for most neural network architectures\nwith well-behaved non-linearities."}]}