{"title": "Reward Models Identify Consistency, Not Causality", "authors": ["Yuhui Xu", "Hanze Dong", "Lei Wang", "Caiming Xiong", "Junnan Li"], "abstract": "Reward models (RMs) play a crucial role in aligning large language models (LLMs) with human\npreferences and enhancing reasoning quality. Traditionally, RMs are trained to rank candidate outputs\nbased on their correctness and coherence. However, in this work, we present several surprising find-\nings that challenge common assumptions about RM behavior. Our analysis reveals that state-of-the-art\nreward models prioritize structural consistency over causal correctness. Specifically, removing the prob-\nlem statement has minimal impact on reward scores, whereas altering numerical values or disrupting\nthe reasoning flow significantly affects RM outputs. Furthermore, RMs exhibit a strong dependence on\ncomplete reasoning trajectories-truncated or incomplete steps lead to significant variations in reward\nassignments, indicating that RMs primarily rely on learned reasoning patterns rather than explicit prob-\nlem comprehension. These findings hold across multiple architectures, datasets, and tasks, leading to\nthree key insights: (1) RMs primarily assess coherence rather than true reasoning quality; (2) The role\nof explicit problem comprehension in reward assignment is overstated; (3) Current RMs may be more\neffective at ranking responses than verifying logical validity. Our results suggest a fundamental limitation\nin existing reward modeling approaches, emphasizing the need for a shift toward causality-aware reward\nmodels that go beyond consistency-driven evaluation.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Hurst et al., 2024; Dubey et al., 2024; Team et al., 2024; Anthropic, 2024;\nJiang et al., 2023a; Liu et al., 2024a; Yang et al., 2024a) have emerged as a dominant paradigm in natural\nlanguage processing, demonstrating remarkable performance across a diverse range of tasks. The Scaling\nLaw (Kaplan et al., 2020) suggests that as model size increases, LLMs develop emergent abilities, enhancing\ntheir capacity to comprehend and solve complex tasks. This scalability enables LLMs to generate coherent,\ncontextually accurate responses, supporting a wide array of downstream applications, including summariza-\ntion (Zhang et al., 2019, 2024), code generation (Chen et al., 2021), mathematical reasoning (Hendrycks\net al., 2021; Zhou et al., 2023), and conversational AI (OpenAI, 2022; Hurst et al., 2024).\nA key factor contributing to the success of large language models is their ability to align model outputs\nwith user preferences(Christiano et al., 2017), which relies on training robust reward models. Beyond pref-\nerence alignment, reward models also play a crucial role in enhancing reasoning capabilities, serving as\nmechanisms to evaluate and refine logical correctness in complex tasks (Cobbe et al., 2021; Lightman et al.,\n2023). One promising approach to scaling test-time computation (Snell et al., 2024; Brown et al., 2024;\nCobbe et al., 2021; Dong et al., 2023) involves leveraging reward models to search for optimal solutions\namong multiple candidates. Despite these advancements, the intrinsic mechanisms of reward models remain\nunderexplored-specifically, the basis on which they assign rewards to generated trajectories and\nwhether they truly comprehend and reason about the questions they evaluate. In this paper,\nwe conduct a comprehensive empirical study of state-of-the-art reward models across multiple reasoning\ndatasets and uncover two surprising findings. First, our systematic error analysis (Figure 1) reveals that\nquestion truncation has the least impact on reward outputs, whereas modifying numerical values or shuffling\nthe question significantly disrupts reward assignments. This suggests that reward models prioritize internal\ncoherence over true causal understanding-they assess solutions based on structural consistency rather than\nverifying whether the reasoning directly corresponds to the given question. Second, when provided with\nincomplete trajectories (i.e., truncated reasoning steps or only given the final answer), the reward outputs\nchange significantly. This indicates that current reward models rely heavily on complete reasoning steps or\nlearned patterns to justify trajectory quality, rather than truly understanding the problem-solving process.\nFurthermore, our rank correlation analysis and Best-of-N experiments confirm that while reward models\nremain robust to question omission, they are highly sensitive to the completeness of reasoning steps and the\nconsistency between the question and solution.\nOur results advocate a rethinking of existing reward models. These findings highlight a fundamental limita-"}, {"title": "2 Related Work", "content": "Reward models play a crucial role in human preference alignment (Christiano et al., 2017; Bai et al., 2022;\nCasper et al., 2023) by guiding large language models (LLMs) toward desired behaviors. Broadly, reward\nmodeling methods can be categorized into two approaches. The first is the preference-based reward model,\nsuch as Bradley-Terry (BT) model (Bradley and Terry, 1952; Zhao et al., 2023; Rafailov et al., 2024; Etha-\nyarajh et al., 2024; Xiong et al., 2024a) and general preference model (Jiang et al., 2023b; Munos et al., 2023;\nTang et al., 2024; Ye et al., 2024; Azar et al., 2024), which defines the reward function by the preference\nbetween two responses. Conventional RLHF usually capture the human preference with BT model (Ouyang\net al., 2022; OpenAI, 2022), which has been widely proven to improve the quality of model outputs (Dubey\net al., 2024; Dong et al., 2024; Guo et al., 2024). The second approach estimate the probability of correctness\nas rewards, directly scoring outputs without relying on pairwise comparisons. In this paper, we primarily\nfocus on correctness-based cases, which are well-defined and more commonly used for selecting reasoning\ntrajectories during both training (Chen et al., 2024; Wang et al., 2024) and inference (Brown et al., 2024) in\nreasoning tasks. Depending on how reward signals are assigned, these models can be classified into Outcome\nReward Models (ORMs) and Process Reward Models (PRMs). ORMs (Yu et al., 2023) evaluate solutions\nbased solely on the final output, while PRMs (Lightman et al., 2023) provide step-level annotations, offering\ndense and granular reward signals at each reasoning step to encourage structured problem-solving. PRMS\nhave been proven to be effective in mathematical problems (Shao et al., 2024; Snell et al., 2024; Luo et al.,\n2024; Liao et al., 2025)."}, {"title": "2.1 LLM Reward Models", "content": "Reward models play a crucial role in human preference alignment (Christiano et al., 2017; Bai et al., 2022;\nCasper et al., 2023) by guiding large language models (LLMs) toward desired behaviors. Broadly, reward\nmodeling methods can be categorized into two approaches. The first is the preference-based reward model,\nsuch as Bradley-Terry (BT) model (Bradley and Terry, 1952; Zhao et al., 2023; Rafailov et al., 2024; Etha-\nyarajh et al., 2024; Xiong et al., 2024a) and general preference model (Jiang et al., 2023b; Munos et al., 2023;\nTang et al., 2024; Ye et al., 2024; Azar et al., 2024), which defines the reward function by the preference\nbetween two responses. Conventional RLHF usually capture the human preference with BT model (Ouyang\net al., 2022; OpenAI, 2022), which has been widely proven to improve the quality of model outputs (Dubey\net al., 2024; Dong et al., 2024; Guo et al., 2024). The second approach estimate the probability of correctness\nas rewards, directly scoring outputs without relying on pairwise comparisons. In this paper, we primarily\nfocus on correctness-based cases, which are well-defined and more commonly used for selecting reasoning\ntrajectories during both training (Chen et al., 2024; Wang et al., 2024) and inference (Brown et al., 2024) in\nreasoning tasks. Depending on how reward signals are assigned, these models can be classified into Outcome\nReward Models (ORMs) and Process Reward Models (PRMs). ORMs (Yu et al., 2023) evaluate solutions\nbased solely on the final output, while PRMs (Lightman et al., 2023) provide step-level annotations, offering\ndense and granular reward signals at each reasoning step to encourage structured problem-solving. PRMS\nhave been proven to be effective in mathematical problems (Shao et al., 2024; Snell et al., 2024; Luo et al.,\n2024; Liao et al., 2025)."}, {"title": "2.2 Robustness of Reward Models", "content": "Despite the success of reward models in aligning with human preferences, they still have issues. A common\nissue is reward hacking (Ibarz et al., 2018; Denison et al., 2024), where the policy achieves high reward scores\nfrom the reward model without exhibiting the desired behavior. This phenomenon leads to performance\ndegradation (Bai et al., 2022) and increases the discrepancy between the policy model's behavior and the\nintended objective (Stiennon et al., 2020). Reward hacking manifests in various patterns (Park et al., 2024),\nwith length hacking being one of the most prevalent and well-documented cases in large language model\nresearch. Singhal et al. (2024) investigate length-related issues in reward models, demonstrating a strong\ncorrelation between reward scores and text length. This finding aligns with the observation by Dubois et al.\n(2023) that output length distributions tend to increase after applying PPO. And Liu et al. (2024b) explore\nlength hacking with the popular DPO algorithm. In addition, ODIN (Denison et al., 2024) explores to\nmitigate the length hacking issue by disentangling the length from the original reward. In this work, rather\nthan exploring new general patterns of reward hacking or developing mitigation techniques, we focus on\nan empirical study that explores whether state-of-the-art reward models genuinely understand questions,\nreasoning steps, and their causal relationships in reasoning tasks."}, {"title": "3 LLM Reward", "content": "We formalize the interaction between a user and an LLM as a mapping from a given context or prompt,\ndenoted as x, to a generated response y. The response y consists of a sequence of steps, represented as\ny = [Y1,..., Yn]. The ideal reward function r*(x, y) quantifies the quality of the generated response in terms\nof its final performance. To ensure a well-defined reward function, we adopt an outcome-based formulation\nand focus on objective reasoning problems, where the reward is defined as the probability that y produces a\ncorrect or desirable outcome:\n$$r*(x, y) = P(y \\text{ is correct } | x).$$ \nThis probabilistic formulation provides a structured measure of response effectiveness. By framing the reward\nin this manner, we ensure that learning objectives align with producing accurate and reliable responses.\nIn real world, although we can access the reward for training data. People usually use another LLM to\nestimate the optimal reward re. Ideally, re \u2248 r*. To train a reward model, people use a dataset of labeled\nexamples {(xi, Yi, zi)}=1, where xi represents the input prompt, y\u2081 is the generated response, and zi \u2208 {1,0}\nis a binary label indicating whether the response is correct (1) or incorrect (0). The reward model re(x, y)\nis parameterized by 0 and is trained to approximate the ideal reward function r*(x, y) by minimizing a loss\nfunction that encourages consistency with the labeled data. A common approach is to optimize a binary\ncross-entropy loss:\n$$L(\\theta) = \\sum_{i=1}^{n} [z_i \\log r_\\theta(x_i, y_i) + (1 - z_i) \\log(1 - r_\\theta(x_i, y_i))],$$\nwhere ro(xi, Yi) is interpreted as the probability that yi is correct given xi. This formulation ensures that the\nreward model learns to distinguish between high-quality and low-quality responses. Once trained, the reward\nmodel can be used to guide response generation in reinforcement learning or ranking-based optimization\nframeworks.\nIn addition, people are refining the reward model with the process-based supervision.\n$$r*(x, Y_{1:k}) = P(Y_{1:k} \\text{ is correct } | x).$$ \nBy incorporating process-based supervision, reward models can capture fine-grained signals that improve\nalignment with human reasoning, ultimately leading to more interpretable and controllable LLM outputs.\nThis paradigm enables reward models to provide more structured feedback, particularly for tasks requiring\nmulti-step reasoning or sequential decision-making.\nIn this paper, we would like to investigate the reward behavior of incomplete inputs to identify what are\nreally matters for reward models. For example, empty input re(None, y), truncated output re(x, Y1:n/2) or\nro(X, Yn/2:n), shuffled input re(x', y)."}, {"title": "4 Experimental Setup", "content": "We outline the experimental setup used in our analysis in Sections 5 and 6.\nModels. We utilize two reward model families: Skywork-01-OpenPRM (01 Team, 2024) and RLHFlow (Xiong\net al., 2024b). Specifically, our experiments include Skywork-01-Open-PRM-Qwen-2.5-1.5B, Skywork-01-\nOpen-PRM-Qwen-2.5-7B, Llama3.1-8B-ORM-Deepseek-Data, and Llama3.1-8B-PRM-Deepseek-Data. For\nbase models, we employ both general-purpose and math-focused LLMs, specifically Llama-3 (Dubey et al.,\n2024) and Qwen-2.5-Math (Yang et al., 2024b)."}, {"title": "5 Questions Matters Little", "content": "To assess the relative importance of different components in reward model inputs, we systematically truncate\nvarious parts of the input and analyze their impact on model predictions. Specifically, we evaluate how the\nabsence of key information such as the question or portions of the solution-affects the reward assignment."}, {"title": "5.1 Which Input Matters Most?", "content": "To assess the relative importance of different components in reward model inputs, we systematically truncate\nvarious parts of the input and analyze their impact on model predictions. Specifically, we evaluate how the\nabsence of key information such as the question or portions of the solution-affects the reward assignment."}, {"title": "5.2 Consistency Matters", "content": "To gain deeper insights into the role of the question in reward modeling, we conduct a series of controlled\nexperiments designed to assess the extent to which the reward model relies on the problem statement for\nevaluating solution quality. Specifically, we investigate how disrupting the question-solution relationship and\naltering key numerical values affect the model's reward assignment.\nQuestion Shuffling: We shuffle the questions and their corresponding solution trajectories, disrupting the\noriginal question-solution pairings. This tests the reward model's reliance on the semantic coherence between\nthe problem statement and the reasoning steps.\nNumerical Value Modification: We replace numerical values in the question with random values, altering\nthe problem while preserving its overall structure. This evaluates the model's sensitivity to specific numerical"}, {"title": "6 Impact on Ranking of Rewards", "content": "A key application of reward models is to rank candidate outputs and select the best among them. Beyond\nabsolute error analysis, it is crucial to evaluate how input modifications influence the relative ranking of\nrewards assigned to different outputs. To this end, we investigate the impact of question and reasoning\nmodifications on ranking consistency through two complementary analyses:\n\u2022 Rank Correlation of Rewards: We assess how well the reward model preserves the relative ranking of\noutputs after input modifications by computing ranking correlation metrics.\n\u2022 Best-of-N Selection: We evaluate the effect of input modifications on Best-of-N selection performance,\nwhere the reward model is used to choose the best candidate from a set of generated outputs."}, {"title": "6.1 Rank Correlation of Rewards", "content": "Using Qwen2.5-Math-7B-Instruct as the base model and Skywork-01-Open-PRM-Qwen-2.5-1.5B as the re-\nward model, we generate 4, 8, 16, and 32 candidate outputs per question on MATH500 and OlympiadBench.\nTo assess the stability of reward rankings under input modifications, we compute Spearman's rank corre-\nlation coefficient (p) as our ranking consistency metric. Spearman's correlation quantifies the monotonic\nrelationship between two variables, providing insight into how well the reward model preserves the relative\nranking of candidate outputs despite perturbations in input structure. A high Spearman correlation indi-\ncates that the ranking of outputs remains stable regardless of modifications. Conversely, a low correlation\nsignifies that input modifications significantly alter the ranking behavior, revealing potential inconsistencies\nin the model's reward assignment process.\nFigure 4 presents the Spearman rank correlation coefficients for different truncation strategies across varying\nvalues of N. Removing the question consistently results in the highest rank correlation across all values of N,\nsuggesting that the reward model remains relatively stable in ranking outputs even when the problem state-\nment is absent. Additionally, the correlation increases as N grows, indicating that the ranking consistency\nimproves when selecting from a larger set of candidate outputs. In contrast, removing the first half of the\nsolution steps leads to a significantly lower correlation, implying that complete reasoning steps is important\nin determining rankings. Furthermore, disrupting the semantic coherence between the question and solution\nhas a noticeable impact on ranking correlation, demonstrating that maintaining a logically consistent input"}, {"title": "6.2 Best-of-N", "content": "Table 1 and Table 2 present the Best-of-N results under different input modifications for reward models. For\nthe Qwen2.5-Math-Instruct-7B models, removing the question has a relatively minor impact on performance,\nwith scores remaining stable across different values of N. For instance, when N=16, the model achieves\nan average score of 66.1, only 0.4 points lower than the vanilla Best-of-N setting. This finding aligns\nwith previous observations that reward models primarily rely on reasoning steps rather than the problem\nstatement itself. In contrast, truncating initial or final steps leads to a more noticeable performance drop,"}, {"title": "7 Discussion and Conclusion", "content": "In this paper, we investigate the impact of input modifications on reward models and uncover key insights\ninto their evaluation behavior. Our findings reveal that truncating the question has minimal impact on\nboth absolute reward values and ranking consistency, suggesting that reward models primarily evaluate the\nsolution trajectory rather than the problem statement itself. However, shuffling the question or modifying\nnumerical values significantly alters the reward model's output, indicating that semantic coherence and\nnumerical consistency play a crucial role in assessment. Additionally, incomplete reasoning steps lead to\nsubstantial changes in rankings, highlighting the model's strong reliance on a structured and complete\nreasoning trajectory rather than an explicit understanding of problem-solving steps.\nThe Consistency Bias in Reward Models. Our findings suggest that reward models are not truly\nevaluating the causal relationship between the question and its solution but rather the internal consistency\nof the reasoning process. Even when the question is removed, if the solution remains well-structured, the\nmodel continues to assign high scores. Conversely, when reasoning steps are truncated, the model struggles\nto maintain ranking consistency, indicating that it relies on pattern recognition rather than an actual causal\nunderstanding of the problem-solution relationship. This raises concerns about the model's ability to gener-\nalize beyond familiar solution structures and adapt to novel problem distributions. Future research should\nexplore techniques to mitigate this consistency bias and encourage a more causally grounded evaluation\nframework.\nTowards Causality-Aware Reward Models. To move beyond consistency-driven ranking, future reward\nmodels should incorporate causal reasoning techniques to better assess the logical validity of solutions.\nPotential directions include:\n\u2022 Causality-Augmented Training: Incorporating counterfactual reasoning tasks to train reward mod-\nels to recognize causal dependencies rather than relying solely on surface-level patterns.\n\u2022 Chain-of-Thought Awareness: Rewarding models not only for correct final answers but also for\ntheir adherence to logically structured reasoning chains, ensuring that each step contributes meaning-\nfully to the solution.\n\u2022 Human-in-the-Loop Refinement: Leveraging human preference data to penalize superficial pattern\nmatching and encourage robust causal reasoning, improving the model's ability to distinguish valid\nreasoning from plausible but incorrect trajectories.\nReconsidering Reward Model Objectives. Current reward models might be optimizing for ranking\nstability rather than true problem-solving ability. This raises the need to rethink selection strategies, such\nas:\n\u2022 Uncertainty-Aware Reward Models: Incorporating confidence-aware mechanisms to quantify the model's\nuncertainty in evaluating complex reasoning tasks.\n\u2022 Deeper Reasoning Signals: Designing reward functions that explicitly capture reasoning depth and\nlogical validity, rather than solely relying on surface-level agreement with high-ranked answers."}]}