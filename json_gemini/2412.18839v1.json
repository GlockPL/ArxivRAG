{"title": "Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM Dataset", "authors": ["Neil Shah", "Shirish Karande", "Vineet Gandhi"], "abstract": "Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning to simulate ground-truth speech from paired whispers. However, the simulated speech often lacks intelligibility and fails to generalize well across different speakers. To address this issue, we focus on learning phoneme-level alignments from paired whispers and text and employ a Text-to-Speech (TTS) system to simulate the ground-truth. To reduce dependence on whispers, we learn phoneme alignments directly from NAMs, though the quality is constrained by the available training data. To further mitigate reliance on NAM/whisper data for ground-truth simulation, we propose incorporating the lip modality to infer speech and introduce a novel diffusion-based method that leverages recent advancements in lip-to-speech technology. Additionally, we release the MultiNAM dataset with over 7.96 hours of paired NAM, whisper, video, and text data from two speakers and benchmark all methods on this dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Silent Speech Interfaces (SSIs) are devices that enable communication without audible speech [1]. SSIs capture physiological signals, such as muscle movements, associated with speech production and convert them into speech. SSIs are crucial for individuals who have lost their speaking abilities, such as those who have undergone a laryngectomy [2] and are also useful in acoustically harsh environments or where silence is necessary, like hospitals or quiet public spaces.\nSSI techniques capture articulator movements using various sensors and imaging techniques such as Ultrasound tongue imaging [3], Electromyography [4], real-time MRI [5], Electrolarynx [6]. However, these techniques are constrained by their invasiveness [4] or the need for highly sensitive tracking devices [3]. Nakajima et al. [7] introduced a method to capture NAMs (signals lacking acoustic intelligibility and incomprehensible even to nearby listeners) from tissues behind the ear using a specialized microphone. This SSI technique offers advantages like content privacy, availability in select markets, good performance in noisy environments, and cost effectiveness [1]. Subsequent efforts improved device design and usability [8] and studied the sensitivity and frequency characteristics of several NAM microphones [9], [10]. Yang et al. [11] also released a 40-minute corpus from a single speaker of paired NAM, whispered speech, and corresponding text to aid further research.\nGiven NAM vibrations and the corresponding ground-truth speech, the task can be framed as a direct sequence-to-sequence translation. However, the unavailability of ground truth speech remains the primary hurdle. By definition, since the subjects only murmur, at most, only the whisper sound can be captured. A simple approach to obtain ground-truth speech is to record clean speech in a studio and then use Dynamic Time Warping to align it with input NAMs [12]. However, warping techniques introduce unnatural distortions in the aligned signals, resulting in poor intelligibility of the converted speech. Shah et al. [13] leverage self-supervised methods to simulate ground-truth speech by converting whispers into speech. While promising, this approach has its limitations-specifically, the simulation method does not generalize well to whisper data from different speakers. Furthermore, the reliance on paired whisper data presents a significant hurdle, as such data may not always be accessible, especially from patients with speech difficulties. To address this dependency, one alternative involves deriving phoneme-level alignments between NAMs and text and feeding these durations into a TTS module [14]. However, within a resource-scarce setting where NAM data is limited, the resulting alignments tend to be noisy. Additionally, current methods and datasets overlook the potential of visual modalities, which could offer a fresh perspective.\nTo address the aforementioned limitations, we present a MultiNAM dataset, containing 7.96 hours of NAM vibrations with corresponding text, whispers, and facial videos. We benchmark existing methods alongside newly proposed methodologies in various scenarios where different input modalities are available, such as whisper and text, NAMs and text, whisper alone, or only facial video, among others. We study these methods in both high-resource and resource-scarce settings. To eliminate reliance on paired whispers and improve intelligibility in resource-scarce scenarios, we explore existing state-of-the-art (SOTA) lip-to-speech synthesis methods and introduce a novel diffusion model conditioned on simulated NAMs and video to generate ground-truth speech."}, {"title": "II. DATASETS", "content": "The proposed MultiNAM corpus was collected in a typical office-like environment."}, {"title": "III. METHOD", "content": "Our method follows a two-step approach (a) simulating aligned ground-truth speech corresponding to NAMs and (b) learning a Sequence-to-Sequence (Seq2Seq) model to convert NAMs into speech. In this section, we primarily focus on various methodologies for simulating aligned ground-truth speech."}, {"title": "A. Ground-truth speech simulation from whisper", "content": "Shah et al. [13] relied on HuBERT [17] as a speech encoder to obtain content-rich representations that focus on ignoring speaker and ambient information, rather than using Mel-cepstral features for encoding whisper. During training, a HiFi-GAN [18] speech decoder is trained on HuBERT representations derived from the LJSpeech dataset. During inference, HuBERT representations from the available whisper are processed through the trained speech decoder to generate an aligned ground-truth speech. Since whispers and NAMs are aligned, this process creates a paired corpus of input NAMs and simulated ground-truth speech using the available paired whisper. We refer to this method as HuBERT-HiFi."}, {"title": "B. Ground-truth simulation using forced alignment", "content": "Shah et al. [14] proposed StethoSpeech, an alternative approach to generating aligned ground-truth speech by learning phoneme-level alignments between the input NAM and the target text to estimate the duration of each phoneme. These durations are then explicitly fed into a TTS model aligned with the text to simulate ground-truth speech. In this work, we experiment with both NAMs and whispers in correspondence with the target text to synthesize ground-truth speech and validate the efficacy of the method. To achieve this, we train an acoustic model on the provided audio-text pairs to determine phoneme durations. We train the acoustic model using the Montreal Forced Aligner (MFA) [19] and employ FastSpeech2 [20] as the TTS model. We refer to this method as MFA-TTS."}, {"title": "C. Ground-truth simulation Using vision modality", "content": "We address the problem of simulating ground-truth speech as a lip-to-speech synthesis task, generating speech from silent video. In this section, we explore two recent methods: Cross-Attention [21] and Lipvoicer [22]. Since these methods require ground-truth speech for training, we cannot train them directly on our data. Therefore, we train the models using the LRS3 [23] dataset and then apply these trained models to our videos during inference.\nData Preprocessing: Following most TTS works [20], [24], we convert text into phonemes. For every video segment, we locate the 68 facial landmarks using dlib [25] and align each frame to a reference face frame with an affine transformation as described in [26]. We then crop an 88\u00d788 lip region centered on the mouth and convert each frame to grayscale. To extract lip representations, we employ AV-HuBERT [26] as our video encoder. Using all the available (video, text) pairs from our dataset, we fine-tune the entire AV-HuBERT model for visual speech recognition with Connectionist Temporal Classification (CTC) [27] loss and extract lip features from the final layer of the fine-tuned model.\nCross-Attention: We follow the architecture proposed by Sindhu et al. [21] but differ in preprocessing. The system takes text and video from the LRS3 dataset as input, with speech as the training target. The phonemes are processed by a text encoder, while the AV-HuBERT lip features are encoded into visual embeddings using a visual encoder. Both encoders are based on transformer layers from [28]. To achieve video-text temporal alignment, we use multi-head scaled-dot product attention [28], where visual embeddings serve as queries and text embeddings act as both keys and values. We use a pre-trained HuBERT speech encoder to obtain the target speech representation. This choice is motivated by the similarity in training methodologies between AV-HuBERT and HuBERT, which we believe will yield content-rich represen-tations conducive to improved intelligibility. The HuBERT model encodes speech at 50 Hz and AV-HuBERT at 25 Hz, indicating a temporal relationship. Thus, target HuBERT dura-tions can be obtained by upscaling AV-HuBERT durations by 2 before passing through a transformer decoder. The transformer decoder predicts HuBERT speech units, optimized with cross-entropy loss. These units are then passed through a pre-trained HiFi-GAN speech decoder to generate speech.\nDiffusion Process: Lipvoicer, a recent lip-to-speech method [22] trains a conditional denoising diffusion probabilistic model (DDPM) [29] on video to generate mel-spectrograms. The generation process during inference is guided by the video and either predicted transcriptions from a pre-trained lip reading network or, if available, ground-truth transcriptions. However, the speech lacks intelligibility and quality in our videos, whether using predicted text or available ground-truth text. We propose modifying the training approach to generate mel-spectrograms by conditioning the DDPM on both video and synthetic NAMs. Given a training data point sampled from the data distribution x0 ~ Pdata(x) and \u1e9et a pre-defined noise schedule, DDPM defines a forward process q(xt|Xt-1) = N(xt; \u221a(1-\u03b2t)xt-1, \u03b2tI) that iteratively turns input into Gaussion noise and a reverse process [30] is then learned by a network that approximates q(xt-1|Xt, xo).\nMany diffusion models use classifier guidance for condi-tional generation, but recent approaches [22] improve per-formance with classifier-free guidance [31], which we also employ in training our diffusion model. We extract 16 kHz speech signals from the training videos of the LRS3 dataset and use these to generate mel-spectrograms. During training, the DDPM generates 1-second mel-spectrograms conditioned on the video and synthetic NAM vibrations, using L\u2081 loss for diffusion noise prediction. To generate synthetic NAMs for speech samples from the LRS3 dataset, we train a HiFiGAN speech decoder using our recorded NAMs. After training, we input the HuBERT representations of LRS3 speech samples into the trained speech decoder to produce NAMs that reflect the content of the LRS3 speech. Unlike [22], which relies on pre-trained ShuffleNet v2, Temporal Convolutional Network, and ResNet-18 for video feature extraction, we use fine-tuned AV-HUBERT representations for lip video and NAMs to em-phasize content. We avoid conditioning on image embeddings to prevent the model from learning speaker-specific informa-"}, {"title": "D. Seq2Seq learning network and speech decoder", "content": "Utilizing ground-truth speech using the approaches above, we train a non-autoregressive transformer [20] to model the relationship between the latent spaces of NAMs and the ground truth. The 6-layer Seq2Seq network employs feed-forward transformer blocks with two multi-head self-attention layers and two 1D convolutions, similar to FastSpeech2 [20]. The encoder converts NAM vibrations into fixed-dimensional vectors, while the decoder generates ground-truth speech embeddings. The model optimizes the mean squared error loss, quantifying the difference between the decoded and ground-truth speech embeddings. Following [13], we add a fully connected layer after the transformer encoder to enhance text prediction by optimizing CTC loss. We train a modified HiFiGAN-v2 [32] with a batch size of 16, a learning rate of 2 \u00d7 10-4, 100 embeddings, an embedding dimension of 128, and an input dimension of 256. The HiFiGAN speech decoder synthesizes speech from embeddings predicted by the Seq2Seq network."}, {"title": "IV. RESULTS AND DISCUSSION", "content": ""}, {"title": "A. Ground-truth speech recognition with whisper", "content": "We compare the performance of the simulated ground-truth speech from HuBERT-Hifi [13] and MFA-TTS [14] methods trained with paired whisper data. The HuBERT-Hifi method achieves high intelligibility (WER: 23.77%) when trained on whisper data from the CSTR corpus, but its performance degrades with our whisper data (WER: 100.14% for speaker S1). This suggests poor generalization to diverse speakers, likely due to the HuBERT model's limited exposure to regional accents. The MFA-TTS method shows improved performance for speaker S1 compared to speaker S2 and those in the CSTR corpus, indicating that it requires more whisper-text pairs for training; performance drops when less data is available."}, {"title": "B. Ground-truth speech recognition without whisper", "content": "MFA-TTS performs exceptionally well when a large amount of whisper data is available (WER: 5.84% for speaker S1). However, in many practical scenarios (e.g., patients with voice disorders), whisper data is not always accessible. Therefore, in this section, we explore experiments where ground-truth speech was simulated without using whisper data. We observed that in this setting, MFA-TTS (all speaker) method, trained on NAM vibrations and text from both speakers, achieves the best performance. If the alignment is done on individual speakers (MFA-TTS (per-speaker)), we observe a drop in performance, with WER increasing from 12.37% to 23.81% for speaker S1. This strengthens our earlier observation (see Section IV-A) that increased data enhances forced alignment, resulting in more intelligible ground-truth speech.\nThe techniques explored (MFA-TTS and HuBERT-Hifi) re-quire the speaker's audio data (NAMs/whispers) for training. However, in extreme cases, such as patients with chronic obstructive pulmonary disease [33], where airflow is severely restricted and the muscles are weakened to the point of being unable to murmur or whisper, these approaches may fail to effectively simulate ground truth. Therefore, we investigate techniques for simulating ground-truth speech without relying on the audio modality and experiment with lip modality alone by using lip-to-speech models trained on large out-of-domain datasets. We used the Cross-Attention [21] method trained on the LRS3 dataset, but it resulted in the highest error rates, indicating its ineffectiveness for our in-the-wild videos. We then apply the pre-trained diffusion-based lip-to-speech model [22] to infer ground-truth speech from our recorded videos. When using lip-reader predicted text as a conditioning factor alongside video during inference, error rates ranged from 39.04% to 49.99% for speaker S1 and S2. In contrast, conditioning with ground-truth text available during our ground-truth simulation-reduced error rates to 27.01% for speaker S1 and 33.94% for speaker S2. Our proposed Diff-NAM method further enhanced performance, lowering error rates to 17.24% for S1 and 21.73% for S2. This improvement underscores the key role of content-specific preprocessing and DDPM conditioning with simulated NAMs in generating more accurate ground-truth speech."}, {"title": "NAM-to-speech conversion without whisper:", "content": "Table II also shows the error rates of converted speech from mapping input NAMs via Seq2Seq network. The term \u201cconverted speech\u201d refers to the final output of the NAM-to-Speech process. These results validate our hypothesis that the quality and intelligibility of the ground truth impact the intelligibility of the converted speech. The converted speech using the simulated ground-truth from the MFA-TTS baseline performs well with ample NAM data but struggles in resource-scarce scenarios, with WER increasing from 26.38% to 56.65% for speaker S1. Speech converted using simulated ground-truth from lip-to-speech methods (Cross-Attention and Lipvoicer) trained on out-of-domain datasets results in poor intelligibility, with the WER remaining above 50%. In contrast, speech converted using the ground truth simulated by our proposed Diff-NAM method achieves the lowest error rates (32.39% for S1 and 38.94% for S2) among all lip-to-speech methods. This demonstrates that improving task-specific preprocessing and conditioning the diffusion process with simulated NAMS yields significantly better results."}, {"title": "V. CONCLUSION", "content": "This study proposes to eliminate the reliance on paired whisper data for ground-truth speech simulation. Existing HuBERT-Hifi method struggle to generalize to new speakers even when using paired whisper data. The MFA-TTS base-line based on the StethoSpeech method achieves the lowest error rates with available paired whispers. Using HuBERT-Hifi and MFA-TTS with ample NAMs and corresponding text improves performance and eliminates the need for paired whisper data. However, these methods, when trained with limited samples, struggle with intelligibility. To eliminate reliance on NAMs/whispers for ground-truth simulation, we leverage advances in lip-to-speech synthesis and propose a novel diffusion model. We introduce a 7.96-hour multi-modal dataset with paired NAMs, whispers, facial videos, and texts from two speakers to support further research. Our future goal is to refine cross-modality alignments, crucial for boosting the intelligibility of simulated ground-truth speech and advancing Seq2Seq network training."}]}