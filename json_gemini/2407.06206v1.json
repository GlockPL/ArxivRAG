{"title": "The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data", "authors": ["Ximing Wen", "Rosina O. Weber", "Anik Sen", "Darryl Hannan", "Steven C. Nesbit", "Vincent Chan", "Alberto Goffi", "Michael Morris", "John C. Hunninghake", "Nicholas E. Villalobos", "Edward Kim", "Christopher J. MacLellan"], "abstract": "Point-of-Care Ultrasound (POCUS) is the practice of clinicians conducting and interpreting ultrasound scans right at the patient's bedside. However, the expertise needed to interpret these images is considerable and may not always be present in emergency situations. This reality makes algorithms such as machine learning classifiers extremely valuable to augment human decisions. POCUS devices are becoming available at a reasonable cost in the size of a mobile phone. The challenge of turning POCUS devices into life-saving tools is that interpretation of ultrasound images requires specialist training and experience. Unfortunately, the difficulty to obtain positive training images represents an important obstacle to building efficient and accurate classifiers. Hence, the problem we try to investigate is how to explore strategies to increase accuracy of classifiers trained with scarce data. We hypothesize that training with a few data instances may not suffice for classifiers to generalize causing them to overfit. Our approach uses an Explainable AI-Augmented approach to help the algorithm learn more from less and potentially help the classifier better generalize.", "sections": [{"title": "1 Introduction", "content": "In radiology imaging, the scarcity of positive data (i.e., illustrating each specific disease) poses a significant challenge for the development of effective diagnostic tools. Radiology relies heavily on machine learning (ML) algorithms to accurately detect and diagnose various conditions. However, without sufficient positive data, the classifiers struggle to learn the distinct features of each condition, leading to reduced accuracy and reliability.\nResearchers investigate various approaches to address the problem of limited training data in ML such as data augmentation [Perez and Wang, 2017; Heidari et al., 2020], single-class learning [Denis, 1998; De Comit\u00e9 et al., 1999; Liu et al., 2003], zero-, one-, or few-shot learning [Larochelle"}, {"title": "2 Background and Related Work", "content": "In this section, we describe the baseline classifiers and the related works."}, {"title": "2.1 The Baseline Classifiers", "content": "The baseline classifiers we utilize in the experiments herein are three variations of the architecture described in Hannan et al. [2023]. To the best of our knowledge, the architecture presented in Hannan et al. [2023] achieves the highest accuracy on the datasets used and thus we choose them as the baseline classifiers. Each variation is used for a different dataset, namely, PTX, ONSD, and COVID-19, adapting to their characteristics. Common to all is the use of a sparse coding model and a small data classifier with convolutional layers.\nThe task for PTX and COVID-19 is to classify whether a pleural line is moving shown in lung ultrasound videos. A pleural line is a terminology in radiology, which indicates where the lung comes into contact with the chest wall. Its movement is the most important feature for PTX diagnosis. We will provide more detail about the datasets in Section 5. The baseline model for PTX [Hannan et al., 2023] has three components. First, a YOLOv4 object detection model [Bochkovskiy et al., 2020] recognizes the region of interest around the pleural line and creates a bounding box around it. Second, a 3D convolutional sparse coding model [Olshausen and Field, 1997] is learned with a convolutional variant of the Locally Competitive Algorithm (LCA) [Paiton, 2019] to compute sparse features with an activation map. The representation of the activated sparse features is used in a convolutional neural network (CNN) classifier with two convolutional layers and two feed-forward layers with dropout. These classifiers are then trained with a binary cross-entropy loss (BCE) function. The classifiers for PTX and COVID-19 are designed to classify frames rather than videos. The process is to extract frames by striding over the video frames at a fixed interval. At each point, the model extracts the given frame along with the two previous and two subsequent frames, resulting in a five-frame block. When applying this binary classifier to the PTX dataset, it determines whether a frame block represents lung sliding. Sliding indicates normal and is thus negative for pneumothorax. This pipeline utilizes the output logits as confidence values, averaging these and rounding to the closest prediction.\nThe classifier for the COVID-19 data uses the same 3D convolutional sparse coding model as for PTX. The difference is that YOLO is not required because there are no object labels to be learned. The same small-data convolutional classifier is used.\nThe ONSD dataset consists of ultrasound videos of the optic nerve sheath. The goal of this task is to detect elevated intracranial pressure (ICP) by measuring the optic nerve sheath diameter (ONSD). The model is different from the PTX model in [Hannan et al., 2023] in a few aspects. The YOLO is run to detect both the eye and the nerve. A 300\u00d7100 region is cropped from the nerve, where the top of the region falls 3mm below the bottom of the eye and is centered on the middle of the nerve. This is then downsized to 150\u00d750 before being fed to a 2D sparse coding model. Once this region is acquired, the task has two steps: detecting the boundaries of the nerve and measuring the distance between the boundaries. The sparse coding model is constructed with both of these steps in mind, where the goal is to construct an activation map for a given image where the distance between the nerve boundaries is obvious, reducing the complexity of the task for the classifier. A standard 2D convolutional sparse coding approach would use a small filter size, such as 8\u00d78, and stride these filters over both the x and y dimensions to build an activation map (see PTX architecture in Hannan et al. [2023]). While this approach may yield vertical edge detectors that activate on the nerve boundaries, these filters would activate for other vertical edges as well. Additionally, striding over both the x and y dimensions is computationally expensive, where the cost scales quadratically with image size. Therefore, a 2D sparse coding model is trained on these regions with a filter size of 150\u00d710. This filter covers the entire width of the nerve region, resulting in the filters only sliding across the y-axis. The idea is that the edges of the nerve will still need to be represented in the sparse model and therefore it will learn filters that activate on both edges of a given nerve within the same filter. For nerves of different widths, different filters would be learned with the same overall pattern but the edges would be closer or farther based on the observed width of the nerve. The resulting sparse coding model produces activation maps that are 1\u00d7M\u00d7N, where N is the number of filters and M is the vertical dimension. They sum these activation maps over the vertical dimension to learn which of our learned, specialized nerve-width filters are most activated in a given image. If one of these filters fits the given nerve well, then it will be highly activated over the entire vertical dimension of the image. This results in a vector of size N, where each value corresponds to the activity level of a filter. They then learn a simple two-layer MLP that takes the N-dimensional vector as input and produces a single binary class prediction, corresponding to whether the width of the nerve exceeds the target threshold."}, {"title": "2.2 Related Work", "content": "Erion et al. [2021] introduced the formal expression of attribution priors as constraints imposed when training a classifier originating from feature attribution methods. They ex-"}, {"title": "3 Motivation to Use XAI to Augment Training", "content": "Given small data classifiers f, we have input features x \u2208 X,  XC Rd and binary labels y \u2208 Y, Y = {0,1} (i.e., y = 0 when class is negative for disease and y = 1 when class is positive for disease). These data represent the real-world data, which characterizes a ground-truth decision boundary that separates the classes 1 and 0. Using the set of n training instances x, we train a classifier f(x) that learns to assign labels f(x) for each instance. By producing these values for f(x), f(x) proposes its characterization of the decision boundary. In practice with the datasets studied herein, f(x) obtains an accuracy level describing the proportion of correct classifications over the total predictions af \u2260 1 hence producing an error Ef \u2260 0. Consequently, its decision boundary does not perfectly match the boundary in the real data. Now consider we build an additive feature attribute model g(z) such as SHAP [Lundberg and Lee, 2017] given by Equation 1:\n$g(z) = \\Phi_0 + \\sum_{j=1}^{d} \\Phi_j(z_j)$  (1)\nwhere z \u2208 {0, 1}d is the projection of the input features from the original input space into binary values. The explanation model g(z) models the classifier f(x), attributing an effect \u03c6j to each feature and summing the effects of all feature attributions approximates the output f(x). As per the additive model property local accuracy [Lundberg and Lee, 2017], the result of Equation 1 is g(z) \u2208 Y. Therefore, this model builds yet a third decision boundary, except that this is now local, i.e., it models each instance. We construct a new prior \u03a6(X, Y) through constraining this third decision boundary with the boundary indicated by the real data using cross entropy loss (Equation 2):\n$\\Phi(X,Y) = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log P(g(h_z(x_i)) = Y_i|h_z(x_i)),$ (2)\nwhere we use a projection function z = hz(x) to represent z. The intuition underlying our proposed approach is to use the XAI prediction for each training instance to provide a second loss as feedback to the training algorithm. The idea is that the new prior can help push the limits of data by learning more from less. With the original cross entropy loss (Equation 3):\n$\\mathcal{L}(\\theta; X,Y) = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log P(f(x_i) = Y_i|x_i; \\theta),$ (3)\nwhere \u03b8 are weights in the classifier. We construct the new objective function using the prior thus generating the new classifier we call the XAI-Augmented (XAIAUG) classifier (Equation 4):\n$\\mathcal{L}_{XAIAUG}(\\theta) = \\mathcal{L}(\\theta; X, Y) + \\Phi(X, Y)$ (4)\nThe expectation is that the classifications produced by the XAIAUG classifier, on average, have higher accuracy than the original classifier (which we will use as baseline). It is also our expectation that the local accuracy of the additive feature attribution model computed for XAIAUG classifier is higher on average compared to the Base. The local accuracy is computed using Gradient SHAP [Erion et al., 2021] for each classifier at the end of every training epoch.\nThe prior \u03a6(X, Y) needs to be differentiable so that we can backpropagate the error and update the weights \u03b8 of the classifier. In the next section, we will describe how we use Gradient SHAP to implement the prior."}, {"title": "4 XAI-Augmented Approach", "content": "In this section, we describe how we use Gradient SHAP and binary classification entropy (BCE) loss to implement the prior and then present the algorithm for training the XAIAUG classifier LXAIAUG (x). From [Erion et al., 2021], for each input feature xi, we first obtain the feature attribution using Integrated Gradients and Expected Gradients:\n$\\Phi_{(i)}(f,x) = ExpectedGradients_{(i)}(x)$\n$= \\int_{x'} IntegratedGradients_{(i)}(x, x')p_D(x')dx'$\n$= \\int_{x'} (x_i - x_i') \\int_{\\alpha=0}^{1} \\frac{\\delta f(x' + \\alpha \\times (x - x'))}{\\delta x_i} d\\alpha p_D(x') dx',$ (5)\nwhere x' is a baseline input feature used as a starting point for calculating feature importance, which is drawn from a background set constructed with all training instances. PD(x') represents the probability distribution of the background dataset. Intuitively, the baseline input feature has the explanatory role as in representing the impact of the absence of each input feature on the prediction to contrast with the impact of each feature on the prediction when present in the input. By sampling the baseline input feature from the background set, we can measure the average impact of each feature.\nIn our implementation, we solve the calculus by approximation, we have a dataset D, then we get:\n$\\Phi_{(i)}(f,x) = \\frac{1}{T} \\sum_{x'\\sim D} \\sum_{k=1}^{m} \\frac{\\delta f(x' + \\frac{k}{m} \\times (x - x'))}{\\delta x_i}$ (6)\nwhere D is the background set and T is the sampling number. In Gradient SHAP, we have the assumption that all attribution go into features and do not consider \u03c60. Thus we have:\n$g(h(x)) = \\sum_{j=1}^{d} \\Phi_{(i)}(f,x)$ (7)\nIn this way, we construct the differentiable prior and construct the new loss:\n$\\Phi(X, Y, \\theta) = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\log P(g(h_z(x_i)) = y_i|h_z(x_i); \\theta)$ (8)\n$\\mathcal{L}_{XAIAUG}(\\theta) = \\mathcal{L}(\\theta; X, Y) + \\Phi(X, Y, \\theta)$ (9)\nAs Algorithm 1 describes, we first randomly select 100 images from the training dataset as background for Gradient SHAP, for each iteration, we first apply gradient SHAP for each input image and get the sum of all SHAP values for each input image which is \u0177. Then we construct SHAP loss using BCE loss to measure the difference between \u0177 and their original labels. We update the classifier's weight by training with the sum of SHAP loss and classification loss."}, {"title": "5 Studies", "content": "In this section, we describe the methodology, metrics, and hypotheses for studies we conduct over three datasets. The"}, {"title": "6 Results and Discussion", "content": "We present results for performance along selected metrics comparing the proposed approach with the baseline."}, {"title": "6.1 Performance", "content": "Tables 1, 2, and 3 present the averages across all folds for AA, BA, and F1 for PTX, ONSD, and COVID-19. The results consistently show higher values for XAIAUG, particularly in the two metrics (i.e., BA and F1) that account for data imbalance. These results reject the null hypotheses for the three datasets for BA and F1. For AA, the null hypothesis is rejected for the COVID-19 and ONSD datasets. On the other hand, the results for the COVID-19 data seem to be where the proposed XAIAUG approach produces the least improvement in the metric values. We observe that the COVID-19 dataset contains the largest amount of data with 202 videos, compared to the PTX dataset with 62 videos, and the ONSD dataset with 60 videos. This motivates us to investigate the reason for the least improvement in the COVID-19 dataset by analyzing the sensitivity of these metrics with respect to the data amounts in Section 6.3."}, {"title": "6.2 Local Accuracy", "content": "Table 5 shows the results for LA for the three datasets. The average LA of the XAIAUG classifier across all folds improves over the baseline for all three datasets. These results demonstrate that interpretability is improved since the additive feature attribution method is better aligned with the XA-IAUG classifier than with the baseline."}, {"title": "6.3 Sensitivity on Data Amounts", "content": "We hypothesize that XAIAUG improves accuracy the least in the COVID-19 dataset because it is the one with the largest number of samples (i.e., 2423 frames). Recall that our motivation is to learn more from the data when instances are limited. Because instances are not as limited in the COVID-19 dataset, the baseline classifier may have been able to learn generalization from the training dataset, potentially constraining XAIAUG's ability to significantly enhance accuracy.\nTo verify that the COVID-19 dataset is large enough and this is why XAIAUG does not improve accuracy as much as in the two datasets, we broke down the COVID-19 data into three smaller subsets we refer to as Small COVID-19 Set 1, Small COVID-19 Set 2, and Small COVID-19 Set 3 with 722, 907, and 794 frames, respectively."}, {"title": "6.4 Comparison against Regularization", "content": "Tables 6, 7, and 8 show results for the previously defined performance metrics but now comparing the XAIAUG classifier with a different baseline \u2013 L2. The compared classifier is different from the original baseline in that the L2 norm is added to the loss function for regularization. As when against the baseline, in general, XAIAUG produces better performance in metrics that consider class imbalance. The exception is in BA for the ONSD dataset. We next look at the loss values plotted along training epochs."}, {"title": "7 Conclusions", "content": "This paper proposes an XAI-Augmented approach to train classifiers with scarce data, with the aim to learn more from less. To investigate and analyze the proposed approach in terms of accuracy, we utilize three ultrasound video datasets, PTX, ONSD, and COVID-19. We present studies to better comprehend why and when this approach improves performance. We found a consistent improvement in Balanced Accuracy and F1 for all three datasets. The study of subsets of COVID-19 datasets shows that our approach is sensitive to data amounts and has the potential for improving classifiers when training data is scarce. By comparing against regularization, we found that the improvements in accuracy are consistent with reduced overfitting. Overall, our results are consistent and conclusions are drawn based on results without major variations."}, {"title": "8 Limitations and Future Work", "content": "This work is limited in that it does not investigate multiple approaches that have been used to increase accuracy such as data augmentation, single-class learning, and zero-, one-, or few-shot learning. Future studies should demonstrate how the proposed XAIAUG approach compares to these other ways to improve accuracy when only small data amounts are available to train a classifier. In future work, synthetic datasets may be used to determine what data conditions, if any, cause these variations. The other alternative is to investigate the impact of the XAIAUG approach on aspects such as incremental learning [Shmelkov et al., 2017]."}, {"title": "Ethical Statement", "content": "The authors have conducted this work after receiving approval from IRB in their respective institutions and from the funding agency sponsoring this work."}]}