{"title": "MetricGold: Leveraging Text-To-Image Latent Diffusion Models for Metric Depth", "authors": ["Ansh Shah", "K. Madhava Krishna"], "abstract": "Recovering metric depth from a single image remains a fundamental challenge in computer vision, requiring both scene understanding and accurate scaling. While deep learning has advanced monocular depth estimation, current models often struggle with unfamiliar scenes and layouts, particularly in zero-shot scenarios and when predicting scale-ergodic metric depth. We present MetricGold, a novel approach that harnesses generative diffusion model's rich priors to improve met-\nric depth estimation. Building upon recent advances [6, 15, 17] in MariGold,\nDMD, and Depth Anything V2 respectively, our method combines latent diffusion,\nlog-scaled metric depth representation, and synthetic data training. MetricGold\nachieves efficient training on a single RTX 3090 within two days using photo-\nrealistic synthetic data from [12, 16, 2], HyperSIM, VirtualKitti, and TartanAir\nrespectively. Our experiments demonstrate robust generalization across diverse\ndatasets, producing sharper and higher quality metric depth estimates compared to\nexisting approaches.", "sections": [{"title": "1 Introduction", "content": "Monocular metric depth estimation seeks to transform a single photographic image into a metric depth\nmap, meaning it regresses a depth value for every pixel. This task is essential whenever understanding\nthe 3D structure of a scene is required, but direct range or stereo measurements are unavailable.\nHowever, recovering the 3D structure from a 2D image is a geometrically ill-posed problem, as\nmuch of the depth information is lost in the projection process. Solving this issue requires prior\nknowledge, such as typical object shapes, sizes, common scene layouts, and occlusion patterns. In\nother words, monocular metric depth estimation implicitly demands an understanding of both scene\ngeometry and scale. The emergence of deep learning has led to significant improvements in this field.\nDepth estimation is now treated as a neural image-to-image translation problem, typically learned in\na supervised or semi-supervised manner using collections of paired, aligned RGB images and depth\nmaps. Early approaches in this domain were limited by their training data, often focusing on specific\nenvironments like indoor scenes [12] or driving scenarios [2].\nMore recently, there has been a growing interest in developing general-purpose models, such as\nMetric Depth V2 [18] for metric depth and Marigold and Depth Anything V1 and V2 [6, 7, 17] for\nrelative depth, which can be applied to a wide range of scenes without retraining or fine-tuned to\nspecific applications with minimal data. These models often build on the strategy first introduced\nby MiDaS [11], which achieves generalization by training high-capacity models on data sourced\nfrom diverse RGB-D datasets across multiple domains. The latest advancements include a shift\nfrom convolutional encoder-decoder architectures [11] to larger, more powerful vision transformers\n[10], and incorporating surrogate tasks [3] to enhance the model's understanding of the visual world,\nleading to better depth predictions. For example, Depth Anything builds on this concept by training\na DPT-style transformer for relative depth prediction over DINOV2 embeddings [9], while Depth\nAnything V2 introduces training on photo-realistic simulation datasets, such as [16], [12], and [2],\ninstead of sensor-based datasets. They argue that sensor data introduces biases from sensor-specific\ncorruptions, which the model might learn, potentially leading to suboptimal performance. Their\nresults demonstrate that models trained on photo-realistic datasets produce sharper and more accurate\ndepth predictions. Additionally, visual depth cues depend not only on the scene content but also on\nthe (often unknown) camera intrinsics [19]. As shown in [17], relative depth can be predicted with\nstrong generalization and accuracy, pushing the frontier of metric depth estimation even further.\nThe intuition behind this work is highly derived from previous papers like [6, 17, 15] the following:\nModern image diffusion models have been trained on internet-scale image collections specifically\nto generate high-quality images across a wide array of domains [1, 13, 14]. If the cornerstone of\nmonocular depth estimation is indeed a comprehensive, encyclopedic representation of the visual\nworld, then it should be possible to derive a broadly applicable depth estimator from a pretrained\nimage diffusion model. In this paper, we set out to explore this option and develop MetricGold, a\nlatent diffusion model (LDM) based on Stable Diffusion [13], along with a fine-tuning protocol to\nadapt the model for depth estimation. The key to unlocking the potential of a pretrained diffusion\nmodel is to keep its latent space intact. We find this can be done efficiently by modifying and fine-\ntuning only the denoising U-Net. Turning Stable Diffusion into MetricGold requires only synthetic\nRGB-D data (in our case, the Hypersim [12] and Virtual KITTI [2] datasets) and a few GPU days on\na single consumer graphics card. Empowered by the underlying diffusion prior of natural images,\nMetricGold exhibits excellent zero-shot generalization: Without ever having seen real depth maps, it\nattains state-of-the-art performance on several real datasets. To summarize, our contributions are:"}, {"title": "2 Re-purposing Image Diffusion Models for Metric Depth", "content": "We present a method that leverages text-to-image diffusion models as an initialization for training\na diffusion model to predict metric depth from an image. Specifically, we reformulate monocular\nmetric depth estimation as a generative task, translating RGB images to depth maps using denoising\ndiffusion. Additionally, we demonstrate qualitative results and describe the training process for\nrepurposing components from the text-to-image diffusion model."}, {"title": "2.1 Latent Diffusion Models for Depth", "content": "Diffusion models are probabilistic models that assume a forward noising process, progressively\ntransforming a target distribution into a noise distribution. A neural denoiser is then trained to reverse\nthis process iteratively, converting noise samples back into samples from the target distribution. These\nmodels have demonstrated exceptional effectiveness with images, videos, and relative depth. They\nare well-suited for this task because they achieve strong performance on regression problems without"}, {"title": "2.2 Using Photo-realistic Synthetic Datasets", "content": "Building on the pioneering work of MiDaS [11] in zero-shot relative depth estimation, recent\napproaches have focused on creating large-scale training datasets to improve estimation accuracy and\ngeneralization. Notably, Depth Anything V1, Metric3D V2 [18], and ZeroDepth [4] have compiled\ndatasets ranging from 1M to 16M images sourced from various sensors. However, the data collected\nfrom different sensor suites inherently includes sensor-based noise, depth-related inaccuracies,\nillumination-induced uncertainty, and depth sensor pattern biases. Although preprocessing and\ncleaning are applied, these biases persist, acting as hidden contaminants that hinder learning a true\ndepth distribution. Depth Anything V2 [17] addresses this issue by exclusively training on photo-\nrealistic synthetic datasets, demonstrating sharper and qualitatively superior depth predictions. In our\nwork, we train our model using the Hypersim and Virtual Kitti 2 datasets."}, {"title": "2.3 Joint Depth scaling for Indoor-Outdoor", "content": "Training a unified model for both indoor and outdoor environments is challenging due to significant\nvariations in depth distribution, color, and scale. Indoor datasets typically contain depths under 10\nmeters, while outdoor datasets include ground truth depths up to 80 meters. Furthermore, learning a\nlatent space for the diffusion model's metric depth is crucial. To address these issues, we propose\nusing log depth as input to the VAE. Log depth directly addresses the indoor-outdoor depth distribution\nby applying a scaling factor early."}, {"title": "3 Method", "content": null}, {"title": "3.1 Diffusion Formulation", "content": "We frame monocular metric depth estimation as a conditional denoising generation task. MetricGold\nis trained to model the conditional distribution $D(d | x)$ over depth $d \\in R^{W\\times H}$ , where the condition\n$x \\in R^{W\\times H\\times 3}$ is an RGB image.\nIn the forward process, starting from the initial latent depth map $d_0 := d$ sampled from the con-\nditional distribution, Gaussian noise is incrementally added at each level $t \\in \\{1, ..., T\\}$, yielding\nprogressively noisier samples $d_t$ defined by:\n$d_t = \\sqrt{\\bar{a}_t}d_0 + \\sqrt{1 - \\bar{a}_t} \\epsilon$\nwhere $\\epsilon \\sim N(0, I)$, $\\bar{a}_t := \\Pi_{s=1}^t(1 - \\beta_s)$, and $\\{\\beta_1, . . . , \\beta_T\\}$ represents the variance schedule across\nthe T diffusion steps. This forward process essentially degrades the initial depth map by adding\nGaussian noise at each timestep t, eventually transforming $d_0$ into a nearly pure Gaussian noise $d_T$.\nIn the reverse process, the conditional denoising model $\\epsilon_\\theta(\\cdot)$, parameterized by learned weights $\\theta$,\niteratively removes noise from $d_t$ to obtain $d_{t-1}$, gradually reconstructing the original depth structure.\nDuring training, the model parameters $\\theta$ are updated by taking a pair $(x, d)$ from the training\ndata, adding sampled noise $\\epsilon$ at a randomly chosen timestep t, and computing the noise estimate\n$\\epsilon = \\epsilon_\\theta(d_t, x, t)$. The objective is to minimize one of the denoising diffusion loss functions. The\nstandard noise prediction objective L is defined as:\n$L = E_{d_0,\\epsilon\\sim N(0,I),t\\sim U(T)} [||\\epsilon - \\epsilon_\\theta||^2]$\nAt inference time, the depth map $d := d_0$ is reconstructed by starting from a normally distributed\nvariable $d_T$ and iteratively applying the learned denoiser $\\epsilon_\\theta(d_t, x, t)$.\nIn contrast to traditional diffusion models that operate directly on raw data, latent diffusion models\nexecute diffusion steps within a low-dimensional latent space. This approach enhances computational\nefficiency and is particularly advantageous for generating high-resolution images. The latent space"}, {"title": "3.2 Network Architecture", "content": "One of our main objectives is training efficiency and demonstration that generalizable models can\nbe trained in an academic setting by relying on quality pretrained models for other doamins. WIth\nminimal changes to the Stable Diffusion v2 model [13], we can add image conditioning. We also\nfinetune Image VAE to learn a Depth VAE to cover the depth distribution in a better manner. 2\ncontains the overview of the proposed training procedure."}, {"title": "Implementation", "content": "We implement MetricGold using PyTorch and utilize Stable Diffusion v2 as our backbone, adhering\nto the original pre-training setup with a v-parameterization objective. We disable text conditioning\nand follow the steps outlined in 3.2. During training, we employ the DDPM noise scheduler with\n1000 diffusion steps. At inference time, we switch to the DDIM scheduler and sample only 50 steps.\nFor the final prediction, we aggregate results from 10 inference runs with varying starting noise.\nTraining our method requires 16,000 iterations using a batch size of 32. To accommodate training\nwith a single GPU, we accumulate gradients over 16 steps with a batch size of 2. We use the Adam\noptimizer with a learning rate of 5 \u00d7 10-5. Additionally, we apply random horizontal flipping as an\naugmentation to the training data. Training our method to convergence takes approximately 2 days\non a single Nvidia RTX 3090 GPU."}, {"title": "5 Conclusion, Further possibilities & limitations", "content": "We have introduced a fine-tuning pipeline that effectively adapts latent diffusion models for metric\ndepth estimation in a computationally efficient manner. By incorporating incremental innovations\nsuch as diffusion for depth, log-normalized depth representation, latent diffusion priors, and synthetic\ndata training, we demonstrate that these strategies can be successfully extended to achieve accurate\nmetric depth predictions. Our approach highlights the versatility of latent diffusion models in solving\nthe challenging task of depth estimation while maintaining efficiency and scalability.\nAs our pipeline has been trained on v-parameterization, we can leverage Consistency Distillations [8]\nmethods, to distill the model into lesser number of steps. A paradigm of distilling diffusion model to\nconditional GANs [5] is also a promising prospect to reduce inference speeds and reduce compute\nrequirements."}, {"title": "Log Depth", "content": "Most neural networks usually model data distribution in [-1, 1]. One might naively\nconvert metric depth to this range with linear scaling, given we know $d_{min}$ and $d_{max}$, maximum and\nminimum range of depth, i.e.,\n$d_{linear} = clip(2 * d \u2013 1, -1, 1)$\n$d_{linear} = normalize((dr - d_{min})/(d_{max} - d_{min}))$\nWe find this does not adequately account for the indoor-outdoor depth distribution. To address this,\nwe model the depth using a logarithmic scale, shifting the uncertainty so that lower depth values\nreceive more attention, while larger distances are relatively less important. By using log-scaled depth\n($d_{log}$) as the target for inference, we allocate more representation capacity to indoor scenes.\n$d_{log} = normalize(log(dr/d_{min})/log(d_{max}/d_{min}))$\nThis adjustment not only improves the representation of depth across both indoor and outdoor\ndistributions, but also leads to enhanced performance of the Variational Autoencoder (VAE). However,\nwe identify that the reconstruction of depth maps from latent embeddings remains the primary\nperformance bottleneck in the pipeline."}, {"title": "Depth encoder and decoder", "content": "We utilize the Variational Autoencoder (VAE) from the Stable\nDiffusion pipeline and fine-tune it specifically for reconstructing log-normalized metric depth. Our\nanalysis has revealed that the reconstruction of the depth image serves as a critical bottleneck in\nthis pipeline, underscoring the importance of this step. Since the model receives a single-channel\ndepth map alongside the 3-channel RGB inputs, we replicate the depth map across the three channels\nto mimic an RGB image. During inference, the depth latent decoder is employed to generate a\nthree-channel depth image, which is then averaged across the channels to produce the final predicted\ndepth.\n$L_{VAE} = E_{q(d|x)} [logp(log d_{log}|d_{latent})] \u2013 D_{KL}(q(d_{latent}|x)||p(d_{latent}))$"}, {"title": "Adapted Denoising U-Net", "content": "To implement the conditioning of the latent denoiser $\\epsilon_\\theta(z(d)_t, z(x),t)$\non the input image x, we concatenate the image and depth latent codes into a single input $z_t =$\n$cat(z(d)_t, z(x))$ along the feature dimension. The input channels of the latent denoiser are then\ndoubled to accommodate the expanded input $z_t$. To prevent inflation of the activation magnitudes\nin the first layer and to preserve the pre-trained structure as faithfully as possible, we duplicate the\nweight tensor of the input layer and divide its values by two."}]}