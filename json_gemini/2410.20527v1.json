{"title": "CODEROSETTA: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming", "authors": ["Ali Tehrani Jamsaz", "Arijit Bhattacharjee", "Le Chen", "Nesreen K. Ahmed", "Amir Yazdanbakhsh", "Ali Jannesari"], "abstract": "Recent advancements in Large Language Models (LLMs) have renewed interest in automatic programming language translation. Encoder-decoder transformer models, in particular, have shown promise in translating between different programming languages. However, translating between a language and its high-performance computing (HPC) extensions remains underexplored due to challenges such as complex parallel semantics. In this paper, we introduce CODEROSETTA, an encoder-decoder transformer model designed specifically for translating between programming languages and their HPC extensions. CODEROSETTA is evaluated on C++ \u2192 CUDA and Fortran \u2192 C++ translation tasks. It uses a customized learning framework with tailored pretraining and training objectives to effectively capture both code semantics and parallel structural nuances, enabling bidirectional translation. Our results show that CODEROSETTA outperforms state-of-the-art baselines in C++ to CUDA translation by 2.9 BLEU and 1.72 CodeBLEU points while improving compilation accuracy by 6.05%. Compared to general closed-source LLMs, our method improves C++ to CUDA translation by 22.08 BLEU and 14.39 CodeBLEU, with 2.75% higher compilation accuracy. Finally, CODEROSETTA exhibits proficiency in Fortran to parallel C++ translation, marking it, to our knowledge, as the first encoder-decoder model for this complex task, improving CodeBLEU by at least 4.63 points compared to closed-source and open-code LLMs.", "sections": [{"title": "1 Introduction", "content": "Automatic code translation between programming languages offers numerous benefits, such as modernizing legacy systems, enabling cross-platform development, and refactoring sequential code into parallel high-performance versions. However, this task poses significant challenges, primarily due to the scarcity of parallel corpora-paired datasets of the same applications written in different languages (e.g., C++ \u2194 CUDA or Fortran \u2194 C++). This lack of data limits the effectiveness of supervised learning approaches. While recent advances in code LLMs have shown promise in general code translation, translating code that involves parallel programming paradigms (e.g., C++ to CUDA) remains largely unexplored. That is primarily due to the inherent complexities in capturing and correctly replicating parallel code semantics [28].\nTransCoder [36] and its follow-up works [37, 39] have demonstrated the potential of unsupervised learning for code translation. However, these methods often struggle with the complexities of"}, {"title": "2 Related Works", "content": "Automatic parallelization. Translating from C to CUDA poses a major challenge. Early efforts in this area primarily involved semi-automatic tools that required significant developer intervention. Noaje et al. [30] implemented an OpenMP C [11] to CUDA translation using the OMPi compiler. Other tools, such as CUDAfy.NET and GPUcc [48], provided annotations to assist the translation process. DawnCC [27] automatically annotates C and C++ code for parallelism, utilizing static analysis to identify opportunities for optimizing execution on multicore and GPU architectures with OpenMP/OpenACC directives. However, much of the responsibility for identifying parallelizable sections and optimizing memory usage remained with the developer. Efforts to translate between C/C++ and Fortran have been more limited. FABLE [15] is one of the few frameworks designed for this, facilitating automatic translation of Fortran to C++ while preserving the original code's semantics through advanced analysis and transformation techniques.\nNeural machine translation. Tournavitis et al. [42] proposed a framework that combines static analysis with machine learning to identify parallelizable code regions and determine the optimal parallelization scheme. This adaptive approach aims to reduce the overhead of manual parallelization while accommodating different architectures. TransCoder [36] pioneered the use of unsupervised learning techniques to translate code across various high-level languages, including Java, C++, and Python, without the need for parallel corpora. Building on TransCoder's architecture, BabelTower [46] extends its capabilities to perform parallel semantic conversion between C and CUDA.\nDenoising Auto-Encoding (DAE) has become a popular technique for training encoder-decoder models, as seen in methods like CodeT5 [45] and PLBART [2]. These models typically use common"}, {"title": "3 CODEROSETTA: Unsupervised Code Translation for Parallel Programming", "content": "This section presents the design and training methodology of CODEROSETTA, our proposed encoder-decoder transformer model for unsupervised code translation. We begin by outlining the overall architecture, followed by a detailed discussion of the pre-training and training objectives that enable CODEROSETTA to effectively capture the nuances of both general-purpose programming languages and their parallel extensions. We focus on the C++ CUDA and C++ Fortran translation tasks."}, {"title": "3.1 Cross Language Masked Language Modeling", "content": "Pre-training plays a crucial role in enabling transformer models to develop a foundational understanding of programming languages. We use Masked Language Modeling (MLM) [47], a widely adopted pre-training objective, to achieve this, as outlined in Figure 1. In MLM, the model receives input code with a portion of tokens randomly masked. The objective is to predict the masked tokens based on the surrounding context, thereby encouraging the model to learn both local syntactic patterns and broader semantic relationships within code.\nTo further challenge the model and better reflect code structure, we mask entire words rather than individual tokens. For instance, in the input code snippet \u201c int index\u201d, the entire word \u201c index\u201d would be masked, requiring the model to predict the missing identifier based on its type (\u201c int\u201d) and its usage in the surrounding code. This approach mirrors how code comprehension often relies on understanding the roles of variables and functions within their scope.\nAdditionally, while MLM is typically applied to monolingual datasets, we extend it to a cross-lingual setting by training on a combined dataset of both C++ and the target language (CUDA or Fortran). This cross-lingual exposure enables CODERosetta to learn shared programming concepts and syntactic structures across languages, such as control flow statements (if, else, while) and variable declarations. By recognizing these commonalities, the model can transfer knowledge across languages, improving its ability to translate even unseen code patterns."}, {"title": "3.2 Abstract Syntax Tree Entity Recognition", "content": "Following cross-lingual MLM pre-training, we introduce a new pre-training objective called Abstract Syntax Tree (AST) Entity Recognition (AER) to further improve CODEROSETTA'S understanding of code structure. This approach draws inspiration from Named Entity Recognition (NER) in natural language processing [20], where models learn to classify words or phrases into predefined categories (e.g., person, location, or organization). In AER, CODEROSETTA learns to recognize and categorize various syntactic components in code.\nThe process, illustrated in Figure 2, starts by using Tree-sitter\u00b2, a multi-language parsing library, to generate the Abstract Syntax Tree (AST) of a source code snippet. The AST representation provides a hierarchical, tree-structured view of the code, with each node corresponding to constructs such as function definitions, variable declarations, or arithmetic expressions. From this AST, we extract a set of entities and their corresponding categories. Examples of categories used in our implementation include function, variable, constant, pointer, and literal.\nDuring AER pre-training, CODEROSETTA tokenizes the input code and predicts the syntactic category of each token based on its role in the AST. Tokens that do not correspond to any specific category are labeled as \u201cO\u201d (Outside). This training enables CODEROSETTA to develop an understanding of the syntactic relationships between code elements, an essential step in accurately translating and generating code across different languages and extensions.\nA key strength of AER is its flexibility-the set of entity categories can be easily adapted for different languages or programming paradigms. For instance, when focusing on CUDA code, we can introduce specialized categories for parallel constructs such as threadIdx, blockIdx, and gridDim, enabling CODEROSETTA to learn the language-specific semantics of parallel programming.\nFurthermore, AER is highly adaptable. Even in cases where AST parsing is only partially available, CODEROSETTA can still leverage this pre-training, showcasing its applicability to diverse code translation tasks. The complete list of tags used in our implementation is provided in Appendix D.2."}, {"title": "3.3 Denoising Auto Encoding with Adaptive Noise Injection", "content": "While cross-lingual MLM and AST Entity Recognition effectively pre-train CODEROSETTA'S encoder to generate meaningful representations of source code, the decoder remains untrained at this stage. Consequently, attempting direct code translation would result in suboptimal performance due to the decoder's lack of exposure to the target language's syntax and semantics. To bridge this gap, we employ a Denoising Auto-Encoding (DAE) training strategy specifically tailored for code translation with adaptive noise injection mechanisms. In essence, DAE training involves corrupting the input source code with various types of noise and then training the model to reconstruct the original, noise-free code. This process compels the decoder to learn both the underlying syntactic rules of the target language and the ability to recover meaningful code from perturbed inputs, simulating the challenges of translating real-world code with potential variations and inconsistencies.\nTo initiate the DAE training phase, we first initialize the decoder using the pre-trained encoder's weights, providing it with a starting point for language understanding. Next, we apply a combination of common noise injection techniques, such as random token masking and shuffling, alongside our new noise strategies designed to emphasize the distinctions between programming languages and their extensions. Figure 3 illustrates the overall process of DAE training in CODEROSETTA. We now delve into the specifics of our customized noise injection methods, which distinguish CODEROSETTA from conventional DAE-based code translation models. These strategies are crucial for enabling the model to discern the subtle but significant differences between languages like C++ and their high-performance counterparts like CUDA."}, {"title": "Weighted token dropping", "content": "To encourage the model to learn the distinctive features of each language and its extensions, we introduce a weighted token dropping strategy during the noise injection phase. Unlike uniform random token removal, this approach assigns higher removal probabilities to language-specific keywords, encouraging the model to focus on critical syntactic elements.\nFor each programming language or extension, CODEROSETTA maintains a list of reserved keywords. During token dropping, these keywords are prioritized, making them more likely to be removed than other tokens. For example, when training on CUDA code, keywords like blockIdx, threadIdx, blockDim, _global__, and atomicSub are more frequently targeted for removal.\nThis weighted sampling creates a more challenging reconstruction task for the model, compelling the decoder to develop a deeper understanding of the language-specific semantics and parallel programming constructs. While the reserved keywords are given higher priority, the weighted random sampling still ensures that other tokens are occasionally dropped, preserving the overall balance of the noise injection process."}, {"title": "Language-specific token insertion", "content": "In addition to weighted token dropping, we implement a language-specific token insertion strategy to improve CODEROSETTA's ability to discern between languages and their extensions during code generation. This method strengthens the model's robustness against out-of-vocabulary tokens, preventing it from inadvertently blending elements from different languages.\nDuring DAE training, CODEROSETTA must distinguish between valid and invalid tokens within the target language. To facilitate this, we construct a vocabulary of unique tokens for each programming language in our training dataset, tracking their frequency of occurrence. Tokens from the vocabulary of other languages are then randomly inserted into the input code based on their probability from the frequency distribution. For example, in the C++ to CUDA translation task, we insert CUDA-specific tokens into C++ code inputs during DAE training. CODEROSETTA is then trained to recognize and disregard these foreign tokens while reconstructing the original C++ code. This process enables the model to develop an understanding of language boundaries, ensuring it generates syntactically and semantically valid code during translation."}, {"title": "Adaptive noise ratios", "content": "Additionally, we introduce an adaptive noise strategy. Instead of applying a fixed noise ratio, such as 10% for token dropping, we begin with an initial noise ratio and progressively increase it throughout the training process. This approach allows the model to gradually adapt to more challenging conditions as it learns to reconstruct the corrupted input sequences. As the training progresses, the input sequences become increasingly corrupted, making the reconstruction task more difficult and forcing the model to learn more robust representations.\nThere is a maximum corruption rate that, once reached, halts further increases in noise. This prevents over-corrupting the inputs, ensuring that the model can still derive meaningful patterns. The impact of adaptive noise ratios, along with the new noise strategies, is examined in our ablation study (Section 5.3).\nTo further support accurate code generation in the target language, we prepend a special <LANG> token to each input sequence. During DAE, this token indicates the language of the corrupted input, prompting the decoder to reconstruct the code in the same language. This mechanism ensures that the model remains focused on generating code within the correct language context."}, {"title": "3.4 Back Translation for Unsupervised Refinement", "content": "To further improve CODEROsetta's translation quality and its ability to capture complex code semantics, we employ back translation during the training process [36]. As illustrated in Figure 4,"}, {"title": "3.5 Finetuning with Synthetic Data from Language Models (Optional Step)", "content": "While CODEROSETTA demonstrates promising results through unsupervised training, we explore the potential of further enhancements by leveraging the capabilities of large language models (LLMs) such as GPT-4 [1] and Gemini Ultra [41]. These LLMs, trained on extensive text and code datasets, have exhibited impressive code generation abilities. However, directly employing such large models for code translation can be computationally expensive and impractical for many real-world applications.\nTo address this, we adopt a knowledge distillation approach [18], where these LLMs serve as teacher models to generate synthetic data for fine-tuning CODEROSETTA, a smaller student model. This method allows us to capture the expertise of the larger models while maintaining computational efficiency.\nSpecifically, we prompt GPT-4 and Gemini to translate C++ code into CUDA where feasible. After filtering out empty or invalid translations, natural text, and non-relevant data (i.e., instances lacking CUDA-specific keywords), we are left with approximately 5,000 high-quality translations from an initial set of 100,000. This significant reduction highlights the inherent challenges in C++ to CUDA translation.\nThe resulting synthetic dataset of C++\u2194CUDA pairs is then used to fine-tune CODEROSETTA. This process allows CODEROSETTA to incorporate the valuable knowledge embedded in the larger LLMs without incurring their high computational costs. It is important to note that this fine-tuning step is optional and can be omitted if access to powerful LLMs for synthetic data generation is not feasible."}, {"title": "4 Experimental Setup", "content": "Training hyperparameters. We implement CODEROSETTA using the HuggingFace Transformers library v4.40.1 [47]. The model is a 12-layer encoder-decoder transformer, with each layer having 12 attention heads and a hidden dimension of 1,536. We initialized the tokenizer with a pre-trained Byte Pair Encoding (BPE) tokenizer from UniXcoder [17], which was further trained on our specific training datasets. The training was conducted using the AdamW optimizer [24] and a batch size of 16, using gradient accumulation over two steps. The experiments were run on a single node with four Nvidia A100 SXM4 GPUs, each with 80GB of memory. To speed up the training process, mixed-precision training was enabled. The final model consists of ~0.8 billion parameters."}, {"title": "4.1 Datasets", "content": "We evaluate CODEROSETTA on two code translation tasks: C++ to CUDA and Fortran to C++. Table 8 provides an overview of the datasets used. For the C++ to CUDA translation task, we use the dataset from BabelTower [46], which consists of:\n\u2022 Unpaired training set: A collection of 243,008 C++ and CUDA source code files, meeaning there is no direct correspondance between the files in each language. To avoid any language bias, we ensure an equal number of C++ and CUDA files during training."}, {"title": "4.2 Data Preprocessing", "content": "To ensure the quality and consistency of training data, we applied task-specific preprocessing steps for each translation task. C++ to CUDA. Although the BabelTower dataset [46] was reportedly cleaned, we found noisy data within the CUDA files. To address this, we curated a list of CUDA-specific reserved keywords and filtered the dataset, retaining only those CUDA files that contained at least one such keyword. This step significantly reduced noise and resulted in a final training set of 243,008 C++ files, matched by an equal number of CUDA files. The validation and test sets remained unchanged, comprising 184 and 180 paired examples, respectively.\nC++ to Fortran. Preprocessing the Stack V2 dataset for C++ to Fortran translation involved managing the large imbalance between C++ and Fortran files, as well as filtering out low-quality or uninformative code snippets. We implemented the following steps:\n\u2022 Educational value filtering: Inspired by the phi-1 model data filtering approach [16], We randomly sampled 100,000 C++ files from Stack V2 and employed GPT-3.5 to assess their \"educational value\" for learning C++ coding concepts. We prompted GPT-3.5 (see Figure 5 to classify each snippet as either \"Yes\" or \"No\" based on its educational value. These labels were then used to fine-tune a binary classifier built on the CodeSage model [49], which we applied to the remaining C++ files in Stack V2. Only files deemed educationally valuable were retained.\n\u2022 Balancing language representation: From the filtered C++ files, we randomly selected a subset equal in size to the number of Fortran files to create a balanced training set.\n\u2022 Length-based filtering: To ensure training stability and avoid biases toward very short or long code snippets, we filtered out files containing fewer than ten tokens or more than 1,000 tokens in both languages.\nAfter these steps, the final training set for C++ to Fortran translation consisted of 474,856 files. For fine-tuning and validation, we used the small paired C++-Fortran dataset from Bin et al. [19], which contains 282 samples. The model was then evaluated on a test set of 33 paired samples."}, {"title": "4.3 Evaluation", "content": "To evaluate CODEROSETTA's translations, we use two widely used code translation metrics: BLEU [32] and CodeBLEU [34]. We benchmark CODEROSETTA against the following baselines. For C++ to"}, {"title": "5 Experimental Results", "content": "5.1 C++ to CUDA\nTable 1 presents the results of CODEROSETTA for C++ CUDA translation. For BabelTower and TransCoder, the results are directly quoted from BabelTower [46], as their models and implementations are not publicly available. Comparing the performance of CODEROSETTA to other models, it demonstrates superior translation capabilities for C++ to CUDA. Specifically, CODEROSETTA outperforms BabelTower by 2.9 BLEU points. Additionally, it achieves a CodeBLEU score of 78.84, which is 1.72 points higher than BabelTower. Although GPT4 and Gemini were not specifically trained on this dataset, they still reached CodeBLEU scores of 64.45 and 64.20, respectively. Evtikhiev et.al [14] indicate that ChrF and ROGUE-L metrics are better suited for code generation tasks than BLEU and CodeBLEU. Notably, CODERosetta also surpasses these models in both ChrF and ROUGE-L metrics.\nCODEROSETTA effectively learns the necessary semantics to generate CUDA code without relying on specific metrics for training, a departure from previous approaches. The compilation accuracy of CODEROSETTA is 98.85% after post-processing. For examples of the CUDA code generated by our model compared to other baselines, please refer to Appendix B. Furthermore, CODEROSETTA is bidirectional, allowing it to translate both C++ to CUDA and vice versa. Please refer to Appendix A for CUDA to C++ results."}, {"title": "5.1.1 Post-processing: Compilation Error Analysis", "content": "Our test set, consisting of 180 samples, provided diverse input scenarios to evaluate our model's performance. We observed that 23 samples generated compilation errors when processed through the NVCC compiler with the required flags. Upon manual investigation, we found that most errors were trivial and could be easily fixed with minor edits.\nSpecifically, 48% of the errors were attributed to the use\nof an undefined generic type T. Another 9% resulted\nfrom missing closing braces, while 26% were due to a\nsingle missing variable initialization. Additionally, 9%\nof the errors were caused by incorrect function calls.\nOnly 8% of the files contained no trivial errors. By\napplying quick fixes for the undefined generic type T,\nmissing variable initializations, and missing closing\nbraces, the overall compilation accuracy significantly\nimproved, with 98.85% of all generated code becoming\ncompilable. This indicates that most errors were simple\nand could be easily resolved by incorporating compiler\nfeedback, which will be a focus of our future work.\nSubsection F.1 and Figure 13 in the Appendix presents\nexamples of our findings."}, {"title": "5.2 Runtime Evaluation", "content": "Although CODEROSETTA demonstrates more accurate translations based on the aforementioned metrics compared to the reference code, these metrics are derived from static evaluations, leaving runtime performance uncertain. To address this, we randomly selected 30 translated CUDA kernels from the test set and created unique template programs to execute them. We ran the translated CUDA kernels using NVCC and found that the functional correctness of the generated code was preserved in the majority of samples (approximately 93%). For further details, see Appendix Section B."}, {"title": "5.3 Ablation Study", "content": "We conduct an ablation study to evaluate the impact of each training objective on the code translation results of CODEROSETTA. Specifically, we remove individual training objectives (e.g., AER) while keeping the other components intact and retraining the model. Table 3 presents the results of the ablation study for C++ to CUDA translation. As observed, removing any of the pertaining or training objectives negatively impacts translation results, with Masked Language Modeling having the most significant effect when omitted. This is expected, as Masked Language Modeling is the primary pretraining objective that enables the model to understand source code.\nAER training task. CODEROSETTA employs two pre-training tasks for training its encoder: Mask Language Modeling (MLM) and Abstract Syntax Tree Entity Recognition (AER). In this phase, we maintain consistent training setups except for the removal of the AER component."}, {"title": "5.4 Fortran to C++", "content": "We train and apply CODEROSETTA for translation between C++ and Fortran. Fortran has had a long-standing presence in the scientific computing community; however, its integration with modern HPC systems [38] can pose significant challenges for developers. Due to the complexities involved in translating Fortran to C++, there has been limited effort to address this issue. Bin et al. [19] were the first to make significant strides in this area, curating a small paired dataset specifically for this translation task and fine-tuning several open-code LLMs.\nThey found StarCoder (15B), when fine-tuned, benefited the most from their paired dataset. We compare CODEROSETTA with the fine-tuned StarCoder (15B), as well as with other general LLMs. The results are shown in Table 4. Fine-tuning CODEROSETTA on the dataset from Bin et al. [19] further enhances its performance, achieving a CodeBLEU score of 65.93. Notably, CODEROSETTA outperforms StarCoder, even though StarCoder is nearly 20 times larger, highlighting the efficiency of our model. It also surpasses state-of-the-art models like GPT-4 and Gemini by a substantial margin, achieving an improvement of at least 4.63 points in CodeBLEU."}, {"title": "6 Conclusion", "content": "In this paper, we introduced CodeROSETTA, an encoder-decoder transformer model designed for translating between programming languages and their high-performance computing (HPC) extensions. We proposed two novel learning objectives: Abstract Syntax Tree (AST) Entity Recognition (AER) and customized Denoising Auto-Encoding, which incorporates weighted token dropping and insertion. These contributions enable CODEROSETTA to capture both the general syntactic structure of code and the specific nuances of parallel programming constructs, without relying on language-specific metrics. Our experiments show that CODEROSETTA significantly outperforms state-of-the-art baselines on C++ to CUDA translation, achieving improvements up to 2.9 BLEU, 1.72 in CodeBLEU, and 6.05% in compilation accuracy. Furthermore, CODEROSETTA is, to the best of our knowledge, the first model to demonstrate proficiency in translating Fortran to its parallel counterpart in C++, highlighting its potential in handling diverse programming paradigms."}, {"title": "A CUDA to C++ Translation Results", "content": "CODEROSETTA is capable of bidirectional translation between languages. Once trained for C++ to CUDA translation, it can also translate CUDA back to C++, unlike previous approaches such as BabelTower [46]. In this section, we compare CODEROSETTA with GPT4 and Gemini on the task of translating CUDA back to C++. Table 5 summarizes the results. As shown, CODEROSETTA demonstrates higher accuracy in translating CUDA to C++. Moreover, we observed that Gemini struggles to clearly distinguish between CUDA and C++, frequently generating C++ translations that are nearly identical to the original CUDA input."}, {"title": "B Functional Correctness Analysis", "content": "The metrics and results shown in Table 1 may have limitations in capturing functional equivalence, as discussed by Evtikhiev et al. [14]. To address this, we evaluated the functional correctness of the translated code by compiling and executing the generated programs. For the C++ \u2192 CUDA translation task, we randomly selected 30 generated CUDA kernels and developed a template program for their execution. We then compared the runtime results of the translated CUDA code against the reference implementations. Our findings indicate that 93% of the translated CUDA code produced results consistent with the reference.\nWe analyzed three representative cases of CUDA translation in detail. In the first case, shown in Figure 6, the kernel is designed to be launched with a grid of thread blocks. Each thread calculates its global index i, and if i is within the array's bounds (i < N), it assigns the value ALPHA to the element at index i * INCX in the array X. CODEROSETTA successfully identified the optimal 2D grid structure with (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x, whereas other models defaulted to a less efficient 1D structure using blockIdx.x * blockDim.x + threadIdx.x. This choice of grid structure significantly impacts CUDA performance, and CODEROSETTA's selection mirrors that of the baseline implementation. Furthermore, CODEROSETTA employed the correct grid structure in four additional instances where other models did not.\nThe second case, illustrated in Figure 7, involves a kernel designed to initialize an array of offsets for sorting purposes. Each offset corresponds to the starting position of a column in a flattened 2D grid. This is often useful for parallel sorting algorithms or other operations requiring column-wise processing. The expression int tid = threadIdx.x + blockIdx.x * blockDim.x; assigns each thread a unique index across the entire grid of blocks, enabling access to distinct elements in a global array. In contrast, the expression int tid = threadIdx.x; provides an index that is only unique within a single block. Without proper offset calculations, threads across different blocks could access the same data, potentially leading to race conditions and negating the kernel's intended behavior. This issue was observed in several examples where Gemini-Ultra produced incorrect results due to this oversight.\nThe third case, depicted in Figure 8, processes 3D arrays in parallel. Each thread calculates its 3D position, checks bounds, and updates specific elements of the array vec based on values from vec1. The kernel averages and scales values from vec1, storing the results in vec while ensuring safe memory access within the array's limits. CODEROSETTA correctly handled large block and grid dimensions by using unsigned long, whereas both GPT-4 and Gemini-Ultra failed due to the use of int, leading to index overflow.\nWe also analyzed Fortran to C++ translations, shown in Figure 9. The translated code snippets maintained functional equivalence, specifically in the synchronization of shared variables between threads. OpenMP, used in the Fortran code, relies on directives such as #pragma omp critical, #pragma omp flush, and #pragma omp atomic to ensure synchronization and memory visibility. In the C++ translation, equivalent functionality was achieved using std::mutex, std::atomic, and std::atomic_thread_fence. Both approaches ensure that x is updated and visible to the second thread before it prints its value, effectively synchronizing the thread actions. CODEROSETTA, Gemini-Pro, and Gemini-Ultra correctly recognized the use of OpenMP in the original code, while GPT-4 did not and opted for a different approach. This highlights the limitations of metrics such as BLEU, which focus on syntax rather than functionality. Despite functional equivalence, GPT-4's translation would score lower due to its syntactic divergence. This underscores the necessity of human evaluation to ensure code correctness, as no single automated metric can fully capture functional behavior."}, {"title": "C Decontamination Analysis", "content": "The C++ to CUDA dataset was obtained from BabelTower [46], which has gone through deduplication and cleaning. Notably, there is no paired trained data available within the dataset, meaning the model does not encounter C++ code alongside the CUDA equivalent during training. As such, the model must rely solely on self-supervised training objectives to learn to embed source code from different"}, {"title": "D Unsupervised Training Parameters", "content": "D.1 Training Parameters\nFor Masked Language Modeling (MLM) training, we use a learning rate of 8 \u00d7 10^{-5} and train for 100 epochs with 15% masking. After each epoch, we measure the perplexity on the validation set and save the model if the perplexity is the lowest. For Abstract Syntax Tree (AST) entity recognition, we use a learning rate of 5 \u00d7 10^{-6} and train for ten epochs. We then create the encoder-decoder model by transferring the encoder's weights to initialize the decoder, so the decoder begins with some foundational knowledge.\nFor Denoising Auto-Encoding and Back Translation, we use a learning rate of 5 \u00d7 10^{-5} and train for 20 epochs. For Denoising Auto-Encoding, we set the masking to 15%, token dropping to 25%, and token insertion to 15%, with a denoising ratio increasing by 2.5% per epoch. Finally, for fine-tuning, we use a learning rate of 5 \u00d7 10^{-5} for ten epochs. At each training iteration, we save the model with the lowest validation loss. All the parameter values are determined empirically through detailed hyperparameter tuning."}, {"title": "D.2 AST Entity Recognition Tags", "content": "The AER tags used in pretraining are shown in Table 7."}, {"title": "D.3 Dataset Statistics", "content": "A detailed overview of the dataset is shown in Table 8."}, {"title": "E Impact of Beam Size", "content": "We conducted beam search decoding with varying beam sizes, returning the top candidate in each case. The results, shown in Table 9, indicate that CODEROSETTA consistently produces the same output, regardless of the beam size."}, {"title": "F Analysis of Generated Code from CODEROSETTA and Closed-Source LLMs", "content": "C++ \u2192 CUDA: In this part, we compare the code generated by CODEROSETTA, GPT4, and Gemini-Ultra. As the BabelTower model and its code are not publicly available, we were unable to access them. However, the BabelTower paper highlights a kernel where the model failed to generate CUDA code due to a syntax error when defining keyCharPtr, as shown in Figure 10. In contrast, CODEROSETTA successfully generates the correct CUDA code. It is interesting to note that CODEROSETTA also recognized the if condition and improved the readability of the code by inverting the if statement, similar to the approach taken by Gemini-Ultra and GPT4. Additionally, CODEROSETTA adheres to the preferred practice of declaring a variable or pointer before assigning a value, which is why first keyCharPtr is defined out of the if statement.\nWe demonstrate another example in Figure 11, where CODEROSETTA accurately reproduces the reference CUDA kernel without adding unnecessary lines of code, such as a host or main function, which is often seen in other models.\nFortran \u2192 C++: Figures 9, 12 show examples of C++ code generated by CODEROSETTA in comparison with other LLMs. Despite CODEROSETTA's smaller size, it effectively translates Fortran code into correct C++ code.\nMoreover, we also evaluated our model in terms of C++ \u2192 Fortran translation 10. The results indicate the capability of CODEROSETTA in translating to and from Fortran code."}, {"title": "F.1 Common Issues and Post-processing in CODEROSETTA-Generated Code", "content": "Code translated by large language models like GPT-4 often includes additional caller functions that extend beyond the scope of the original function. In contrast, code translated by CODEROSETTA may occasionally fail to compile despite being syntactically correct. We identified two common issues in the code generated by CODEROSETTA and applied a simple post-processing method to ensure a fair comparison across models.\nThe first issue involves the use of generic types, which can enhance code efficiency but require explicit type definitions at compile time. Figure 13a shows the use of a generic type, although the necessary definition is missing. Adding the type definition, as shown in Figure 13b, resolves the compilation issue. The second issue relates to misses variable initialization in the function definition, as shown in Figure 13c. By initializing the required variable, as demonstrated in Figure 13d, the compilation problem is resolved. Lastly, for longer code snippets, CODEROSETTA occasionally omits the closing curly bracket."}, {"title": "G Discussion on Unsupervised Training", "content": "G.1 Fine-tuning for Code Translation\nIn the context of code translation, paired data is scarce. However, our model benefits from a strong foundational understanding of code translation acquired through unsupervised and self-supervised pre-training on 243K training examples for C++ \u2194 CUDA. We demonstrate that fine-tuning, even with a small amount of synthetic data\u2014without verifying the one-to-one mapping between the generated samples and the input code in a supervised manner can further improve the model's performance. Specifically, fine-tuning with merely 5K paired samples (less than 2% of total data) generated by larger models still led to significant performance gains. While synthetic data may introduce some errors (as large models can make translation mistakes), the combination of this foundational pre-training and fine-tuning with a small synthetic dataset yields further improvements."}, {"title": "G.2 Back Translation", "content": "Back Translation (BT) has been extensively used in unsupervised translation tasks for"}]}