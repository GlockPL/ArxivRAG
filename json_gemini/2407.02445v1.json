{"title": "Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials", "authors": ["Yawar Siddiqui", "Tom Monnier", "Filippos Kokkinos", "Mahendra Kariya", "Yanir Kleiman", "Emilien Garreau", "Oran Gafni", "Natalia Neverova", "Andrea Vedaldi", "Roman Shapovalov", "David Novotny"], "abstract": "We present Meta 3D AssetGen (AssetGen), a significant advancement in text-to-3D generation which produces faithful, high-quality meshes with texture and material control. Compared to works that bake shading in the 3D object's appearance, AssetGen outputs physically-based rendering (PBR) materials, supporting realistic relighting. AssetGen generates first several views of the object with factored shaded and albedo appearance channels, and then reconstructs colours, metalness and roughness in 3D, using a deferred shading loss for efficient supervision. It also uses a sign-distance function to represent 3D shape more reliably and introduces a corresponding loss for direct shape supervision. This is implemented using fused kernels for high memory efficiency. After mesh extraction, a texture refinement transformer operating in UV space significantly improves sharpness and details. AssetGen achieves 17% improvement in Chamfer Distance and 40% in LPIPS over the best concurrent work for few-view reconstruction, and a human preference of 72% over the best industry competitors of comparable speed, including those that support PBR. Project page with generated assets: https://assetgen.github.io", "sections": [{"title": "1 Introduction", "content": "Generating 3D objects from textual prompts or images has enormous potential for 3D graphics, animation, gaming and AR/VR. However, despite the outstanding progress of image and video generation [66, 42, 53, 40, 95, 101], the quality of 3D generators is still insufficient for professional use. Typical limitations include slow generation speed and artifacts in the generated 3D meshes and textures. Many approaches, furthermore, still \u201cbake\u201d appearance as albedo, ignoring how the 3D model should respond to variable environmental illumination. This results in visually unattractive outputs, especially for reflective materials, which look out of place when put in novel environments.\nWe introduce Meta 3D AssetGen (AssetGen), a significant step-up in text-conditioned 3D generation. AssetGen generates assets in under 30s while outperforming prior works of a comparable speed in faithfulness, quality of the generated 3D meshes and, especially, quality and control of materials, supporting Physically-Based Rendering (PBR) [85]. AssetGen uses a two-stage design inspired by [40]. The first stage stochastically generates 4 images of the object from 4 canonical viewpoints, and the second stage deterministically reconstructs the 3D shape and appearance of the object from these views (Fig. 1). This approach is faster and more robust than SDS-based techniques [66] and produces more diverse and faithful results than single-stage 3D generators [36, 59, 92, 77].\nAssetGen explicitly models the interaction of light and object using PBR, which is essential for 3D computer graphics applications. Specifically, we accounts for albedo, metalness, and roughness to render scenes that accurately reflect environmental illumination. Furthermore, we focus on meshes as the output representation due to their prevalence in applications and compatibility with PBR.\nOur first contribution is to extend the two stages to generate PBR information. Since the image-to-3D stage is deterministic, it is the text-to-image stage, which is stochastic, that should resolve ambiguities in assigning materials. This could be done by fine tuning the text-to-image model to output directly the required PBR channels, but we found this to be problematic due to the large gap between PBR and natural images. Instead, we assign the text-to-image model the simpler task of outputting shaded and albedo (unshaded) versions of the appearance. This allows the image-to-3D component to accurately predict PBR materials by analyzing the differences between the albedo and shaded channels.\nOur second innovation is in the mesh generation step. Existing works typically output an opacity field to represent 3D shape, but the latter can lead to poorly-defined level sets, which result in meshing artifacts. To address this, AssetGen's image-to-3D stage, MetaILRM, directly predicts a signed-distance field (SDF) instead of an opacity field. This yields higher-quality meshes, as the zero level set of an SDF traces the object's surface more reliably. Furthermore, this representation can be easily supervised using ground-truth depth maps, which is not immediately possible for opacities. We use VolSDF [107] to render the SDF-based model differentiably, and we improve the training efficiency by implementing rendering within the memory-efficient Lightplane kernels [6]. This allows for larger batches and photometric loss supervision on high-resolution renders, leading to better texture quality.\nWhile MetaILRM produces high quality shapes and good textures, the latter can still be blurry, due to the limited resolution of the volumetric representation, limitations of the feed-forward 3D reconstruction network, and the mesh conversion process. We address this issue by training a new texture refiner transformer which upgrades the extracted albedo and materials by fusing information extracted from the original views, resolving at the same time possible conflicts between them.\nWe demonstrate the effectiveness of AssetGen on the image-to-3D and text-to-3D tasks. For image-to-3D, we attain state-of-the-art performance among existing few-view mesh-reconstruction methods when measuring the accuracy of the recover shaded and PBR texture maps. For text-to-3D, we conduct extensive user studies to compare the best methods from academia and industry that have comparable inference time, and outperform them in terms of visual quality and text alignment."}, {"title": "2 Related Work", "content": "Text-to-3D. Inspired by text-to-image models, early text-to-3D approaches [62, 33, 26, 109, 104] train 3D diffusion models on datasets of captioned 3D assets. Yet, the limited size and diversity of 3D data prevents generalization to open-vocabulary prompts. Recent works thus pivoted into basing such generators on text-to-image models that are trained on billions of captioned images."}, {"title": "3 Method", "content": "AssetGen is a two-stage pipeline (Fig. 2). First, text-to-image (Sec. 3.1), takes text as input and generates a 4-view grid of images with material information. Second, image-to-3D, comprises a novel PBR-based sparse-view reconstruction model (Sec. 3.2) and the texture refiner (Sec. 3.3). As such, AssetGen is applicable to two tasks: text-to-3D (stage 1+2) and image-to-3D (stage 2 only)."}, {"title": "3.1 Text-to-image: Generating shaded and albedo images from text", "content": "The goal of the text-to-image module is to generate several views of the generated 3D object. To this end, we employ an internal text-to-image diffusion model pre-trained on billions of text-annotated images, with an architecture similar to Emu [16]. Similar to [72, 40], we finetune the model to predict a grid of four images \\(I_i, i = 1, . . ., 4\\), each depicting the object from canonical viewpoints \\(\\pi_i\\). Note that I are RGB images of the shaded object. We tried deferring the PBR parameter extraction to the image-to-3D stage, but this led to suboptimal results. This is due to the determinism of the image-to-3D stage, which fails to model ambiguities when assigning materials to surfaces.\nA natural solution, then, is to predict the PBR parameters directly in the text-to-image stage. These consists of the albedo \\(p_0\\) (by which we mean the base color, which is the same as albedo only for zero metalness), the metalness \\(\\gamma\\), and the roughness \\(\\alpha\\). However, we found this to be ineffective too because the metalness and roughness maps deviate from the distribution of natural images making them a hard target for finetuning. Our novel solution is to train the model to generate instead a 4-view grid with 6 channels, 3 for the shaded appearance I and 3 more for the albedo \\(p_0\\). This reduces the finetuning gap, and removes enough ambiguity for accurate PBR prediction in the image-to-3D stage."}, {"title": "3.2 Image-to-3D: A PBR-based large reconstruction model", "content": "We now describe the image-to-3D stage, which solves the reconstruction tasks given either a small number of views \\(I_i\\) (few-view reconstruction), or the 4-view 6-channel grid of Sec. 3.1.\nAt the core of our method is a new PBR-aware reconstruction model, MetaILRM, that reconstructs the object given N > 1 posed images \\((I_i, \\pi_i)_{i=1}^{N}\\), where \\(I_i \\in \\mathbb{R}^{H \\times W \\times D}\\) and \\(\\pi_i \\in \\Pi\\) is the camera viewpoint. As noted in Sec. 3.1, we consider N = 4 canonical viewpoints \\(\\pi_1,..., \\pi_4\\) (fixed to 20\u00b0 elevation and 0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0 azimuths) and D = 6 input channels. The output is a 3D field representing the shape and PBR materials of the object as an SDF \\(s : \\mathbb{R}^3 \\rightarrow \\mathbb{R}\\), where s(x) is the signed distance from the 3D point x to the nearest object surface point, and a PBR function \\(k : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^5\\), where \\(k(x) = (\\rho_0, \\gamma, \\alpha)\\) are the albedo, metalness and roughness.\nThe key to learning the model is the differentiable rendering operator \\(\\mathcal{R}\\). This takes as input a field \\(l : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^D\\), the SDF s, the viewpoint \\(\\pi\\), and a pixel \\(u \\in U = [0, W) \\times [0, H)\\), and outputs the projection of the field on the pixel according to the rendering equation [56], which has the same number of channels D as the rendered field l:\n\\[\\mathcal{R}(u | l, s, \\pi) = \\int_0^\\infty l(x_t)o(x_t | s)e^{-\\int_0^t o(x_{t'} | s) dt'} dt.\\]\nHere \\(x_t = x_0 - t \\omega\\), \\(t \\in [0,\\infty)\\) is the ray that goes from the camera center \\(x_0\\) through the pixel u along direction \\(-\\omega \\in S^2\\). The function \\(o(x | s)\\) is the opacity of the 3D point x and is obtained"}, {"title": "3.3 Mesh extraction and texture refiner", "content": "The MetaILRM module of Sec. 3.2 outputs a sign distance function s, implicitly defining the object surface \\(\\Lambda = \\{x \\in \\mathbb{R}^3 | s(x) = 0\\}\\) as a level set of s. We use the Marching Tetrahedra algorithm [18] to trace the level set and output a mesh \\(\\mathcal{M} \\approx \\Lambda\\). Then, xAtlas [111] extracts a UV map \\(\\phi : [0, V]^2 \\rightarrow \\mathcal{M}\\), mapping each 2D UV-space point \\(v = \\phi(x)\\) to a point \\(x \\in \\mathcal{M}\\) on the mesh.\nNext, the goal is to extract a high-quality 5-channel PBR texture image \\(K \\in \\mathbb{R}^{V \\times V \\times 5}\\) capturing the albedo, metalness, and roughness of each mesh point. The texture image K can be defined directly by sampling the predicted PBR field k as \\(K(v) \\leftarrow k(\\phi(v))\\), but this often yields blurry results due to the limited resolution of MetaILRM. Instead, we design a texture refiner module which takes as input the coarse PBR-sampled texture image as well as the N views representing the object and outputs a much sharper texture K. In essence, this modules leverages the information from the different views to refine the coarse texture image. The right part of Fig. 2 illustrates this module.\nMore specifically, it relies on a network \\(\\Phi\\) which is fed N + 1 texture images \\(\\{K_i\\}_{i=0}^{N}\\). First, each pixel \\(v \\in [0, V]^2\\) of \\(K_0 \\in \\mathbb{R}^{V \\times V \\times 11}\\) is annotated with the concatenation of the normal, the 3D location, and the output of MetaILRM's PBR field \\(k(\\phi(v))\\) evaluated at v's 3D point \\(\\phi(v)\\). The remaining \\(K_1, ..., K_N\\) correspond to partial texture images with 6 channels (for the base and shaded colors) which are obtained by backprojecting the object views to the mesh surface. The network \\(\\Phi\\) utilises two U-Nets to fuse \\(\\{K_i\\}_{i=0}^{N}\\) into the enhanced texture K. \\(\\Phi\\)'s goal is to select, for each UV point v, which of the N input views provides the best information. Specifically, each partial texture image \\(K_i\\) is processed in parallel by a first U-Net, and the resulting information is communicated via cross attention to a second U-Net whose goal is to refine \\(K_0\\) into the enhanced texture K. Please refer to App. A.7 for further details.\nSuch a network is trained on the same dataset and supervised with the PBR and albedo rendering losses as MetaILRM. The only difference is meshes (whose geometry is fixed) are rendered differentiably using PyTorch3D's [69] mesh rasterizer instead of the Lightplane SDF renderer."}, {"title": "4 Experiments", "content": "Our training data consists of 140,000 meshes of diverse semantic categories created by 3D artists. For each asset, we render 36 views at random elevations within the range of [-30\u00b0, 50\u00b0] at uniform intervals of 30\u00b0 around the object, lit with a randomly selected environment map. We render the shaded images, albedo, metalness, roughness, depth maps, and foreground masks from each viewpoint. The text-to-image stage is based on an internal text-to-image model architecturally similar to Emu [16], fine-tuned on a subset of 10,000 high-quality 3D samples, captioned by a Cap3D-like pipeline [51] that uses Llama3 [86]. The other stage utilizes the entire 3D dataset instead.\nFor evaluation, following [103, 101, 40], we assess visual quality using PSNR and LPIPS [116] between the rendered and ground-truth images. PSNR is computed in the foreground region to avoid metric inflation due to the empty background. Geometric quality is measured by the L1 error between the rendered and ground-truth depth maps (of the foreground pixels), as well as the IoU of the object silhouette. We further report Chamfer Distance (CD) and Normal Correctness (NC) for 20,000"}, {"title": "5 Conclusions", "content": "We have introduced Meta 3D AssetGen, a significant advancement in sparse-view reconstruction and text-to-3D. Meta 3D AssetGen can generate 3D meshes with high-quality textures and PBR materials faithful to the input text. This uses several key innovations: generating multi-view grids with both shaded and albedo channels, introducing a new reconstruction network that predicts PBR materials from this information, using deferred shading to train this network, improving geometry via a new scalable SDF-based renderer and SDF loss, and introducing a new texture refinement network. Comprehensive evaluations and ablations demonstrate the effectiveness of these design choices and state-of-the-art performance."}, {"title": "A Appendix", "content": "A.1 Societal Impact\nSafeguards should be implemented to prevent abuse, such as filtering input text prompts and detecting unsafe content in generated 3D models. Additionally, our generation process may be vulnerable to biases present in the data and 3D models it relies on, potentially perpetuating these biases in the generated content. Despite these risks, our method can augment the work of artists and creative professionals by serving as a complementary tool to boost productivity. It also holds the potential to democratize 3D content creation, making it accessible to those without specialized knowledge or expensive proprietary software.\nA.2 Limitations\nMeta 3D AssetGen significantly advances shape generation but faces several limitations. Despite the fine-tuning of the multiview image grid generator for view consistency, it is not guaranteed, potentially impacting 3D reconstruction quality. Since we use an SDF as an underlying representation, the reconstructor may incorrectly model translucent objects or thin structures like hair or fur. Additionally, while our scalable Triton [65] implementation supports a triplane representation at a resolution of 128 \u00d7 128, this representation is inefficient, as much of its capacity is used for empty regions. Future work could explore scalable representations such as octrees, sparse voxel grids, and hash-based methods, which may remove the need for a separate texture enhancement model. We also only predict albedo, metalness and roughness, and not emissivity or ambient occlusions. Finally, our method has only been tested on object-level reconstructions, leaving scene-scale 3D generation for future research.\nA.3 Additional qualitative comparisons\nThis section describes additional qualitative comparisons that, due to limited space, could not be included in the main paper. Firstly, please refer to the video attached in the supplementary material which provides a holistic presentation of Meta 3D AssetGen's qualitative results. In Fig. 7, we highlight the contributions of MetaILRM in geometry, texture and material reconstruction. In Fig. 11, we visualize the control of materials provided by Meta 3D AssetGen, i.e., metalness and roughness, by changing the text prompt for the same concept. Fig. 8 visualizes the renders of the material maps extracted with MetaILRM given four input test views. In Fig. 9, we provide a more extensive qualitative comparison to MeshLRM, the strongest few-view reconstruction baseline. Finally, Fig. 6 provides a gallery of text-conditioned generations depicting Blender-shaded renders together with the rendered PBR maps.\nA.4 User-study details\nAs described in Sec. 4.2, we conducted an user study on 404 meshes generated using the DreamFu-sion [66] prompt-set on a standard crowdsourcing marketplace. In the study, users were shown 360\u00b0"}]}