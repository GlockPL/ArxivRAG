{"title": "Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials", "authors": ["Yawar Siddiqui", "Tom Monnier", "Filippos Kokkinos", "Mahendra Kariya", "Yanir Kleiman", "Emilien Garreau", "Oran Gafni", "Natalia Neverova", "Andrea Vedaldi", "Roman Shapovalov", "David Novotny"], "abstract": "We present Meta 3D AssetGen (AssetGen), a significant advancement in text-to-3D generation which produces faithful, high-quality meshes with texture and material control. Compared to works that bake shading in the 3D object's appearance, AssetGen outputs physically-based rendering (PBR) materials, supporting realistic relighting. AssetGen generates first several views of the object with factored shaded and albedo appearance channels, and then reconstructs colours, metalness and roughness in 3D, using a deferred shading loss for efficient supervision. It also uses a sign-distance function to represent 3D shape more reliably and introduces a corresponding loss for direct shape supervision. This is implemented using fused kernels for high memory efficiency. After mesh extraction, a texture refinement transformer operating in UV space significantly improves sharpness and details. AssetGen achieves 17% improvement in Chamfer Distance and 40% in LPIPS over the best concurrent work for few-view reconstruction, and a human preference of 72% over the best industry competitors of comparable speed, including those that support PBR. Project page with generated assets: https://assetgen.github.io", "sections": [{"title": "Introduction", "content": "Generating 3D objects from textual prompts or images has enormous potential for 3D graphics, animation, gaming and AR/VR. However, despite the outstanding progress of image and video generation [66, 42, 53, 40, 95, 101], the quality of 3D generators is still insufficient for professional use. Typical limitations include slow generation speed and artifacts in the generated 3D meshes and textures. Many approaches, furthermore, still \u201cbake\u201d appearance as albedo, ignoring how the 3D model should respond to variable environmental illumination. This results in visually unattractive outputs, especially for reflective materials, which look out of place when put in novel environments.\nWe introduce Meta 3D AssetGen (AssetGen), a significant step-up in text-conditioned 3D generation. AssetGen generates assets in under 30s while outperforming prior works of a comparable speed in faithfulness, quality of the generated 3D meshes and, especially, quality and control of materials, supporting Physically-Based Rendering (PBR) [85]. AssetGen uses a two-stage design inspired by [40]. The first stage stochastically generates 4 images of the object from 4 canonical viewpoints, and the second stage deterministically reconstructs the 3D shape and appearance of the object from these views (Fig. 1). This approach is faster and more robust than SDS-based techniques [66] and produces more diverse and faithful results than single-stage 3D generators [36, 59, 92, 77].\nAssetGen explicitly models the interaction of light and object using PBR, which is essential for 3D computer graphics applications. Specifically, we accounts for albedo, metalness, and roughness to render scenes that accurately reflect environmental illumination. Furthermore, we focus on meshes as the output representation due to their prevalence in applications and compatibility with PBR.\nOur first contribution is to extend the two stages to generate PBR information. Since the image-to-3D stage is deterministic, it is the text-to-image stage, which is stochastic, that should resolve ambiguities in assigning materials. This could be done by fine tuning the text-to-image model to output directly the required PBR channels, but we found this to be problematic due to the large gap between PBR and natural images. Instead, we assign the text-to-image model the simpler task of outputting shaded and albedo (unshaded) versions of the appearance. This allows the image-to-3D component to accurately predict PBR materials by analyzing the differences between the albedo and shaded channels.\nOur second innovation is in the mesh generation step. Existing works typically output an opacity field to represent 3D shape, but the latter can lead to poorly-defined level sets, which result in meshing artifacts. To address this, AssetGen's image-to-3D stage, MetaILRM, directly predicts a signed-distance field (SDF) instead of an opacity field. This yields higher-quality meshes, as the zero level set of an SDF traces the object's surface more reliably. Furthermore, this representation can be easily supervised using ground-truth depth maps, which is not immediately possible for opacities. We use VolSDF [107] to render the SDF-based model differentiably, and we improve the training efficiency by implementing rendering within the memory-efficient Lightplane kernels [6]. This allows for larger batches and photometric loss supervision on high-resolution renders, leading to better texture quality.\nWhile MetaILRM produces high quality shapes and good textures, the latter can still be blurry, due to the limited resolution of the volumetric representation, limitations of the feed-forward 3D reconstruction network, and the mesh conversion process. We address this issue by training a new texture refiner transformer which upgrades the extracted albedo and materials by fusing information extracted from the original views, resolving at the same time possible conflicts between them.\nWe demonstrate the effectiveness of AssetGen on the image-to-3D and text-to-3D tasks. For image-to-3D, we attain state-of-the-art performance among existing few-view mesh-reconstruction methods when measuring the accuracy of the recover shaded and PBR texture maps. For text-to-3D, we conduct extensive user studies to compare the best methods from academia and industry that have comparable inference time, and outperform them in terms of visual quality and text alignment."}, {"title": "Related Work", "content": "Text-to-3D. Inspired by text-to-image models, early text-to-3D approaches [62, 33, 26, 109, 104] train 3D diffusion models on datasets of captioned 3D assets. Yet, the limited size and diversity of 3D data prevents generalization to open-vocabulary prompts. Recent works thus pivoted into basing such generators on text-to-image models that are trained on billions of captioned images.\nAmong these, works like [73, 54] finetune 2D diffusion models to output 3D representations, but the quality is limited due to the large 2D-3D domain gap. Other approaches can be dived into two groups.\nThe first group contains methods that build on DreamFusion, a seminal work by [66], and distill 3D objects by optimizing NeRF via the SDS loss, matching its renders to the belief of a pre-trained text-to-image model. Extensions have considered: (i) other 3D representations like hash grids [42, 67], meshes [42] and 3D Gaussians (3DGS) [79, 110, 12]; (ii) improved SDS [89, 93, 118, 30]; (iii) monocular conditioning [67, 80, 112, 76]; (iv) predicting additional normals or depth for better geometry [68, 76]. Yet, distillation methods are prone to issues such as the Janus effect (duplicating object parts) and content drift [72]. A common solution is to incorporate view-consistency priors into the diffusion model, by either conditioning on cameras [45, 71, 31, 11, 67] or by generating multiple object views jointly [72, 96, 91, 38, 117]. Additionally, SDS optimization is slow and requires minutes to hours per assets; this issue is partly addressed in [50, 99] with amortized SDS.\nThe second group of methods includes faster two-stage approaches [44, 49, 47, 106, 105, 8, 81, 28, 23] that start by generating multiple views of the object using a text-to-image or -video model [52, 13] tuned to output multiple views of the object followed by per-scene optimization using NeRF [56] or 3DGS [37]. However, per-scene optimization requires several highly-consistent views which are difficult to generate reliably. Instant3D [40] improves speed and robustness by generating a grids of just four views followed by a feed-forward network (LRM [29]) that reconstructs the object from these. One-2-3-45++ [43] replaces the LRM with a 3D diffusion model. Our AssetGen builds on the Instant3D paradigm and upgrades the LRM to output PBR materials and an SDF-based representation of 3D shape. Furthermore, it starts from grids of four views with shaded and albedo channels, key to predicting accurate 3D shape and materials from images.\n3D reconstruction from images. 3D scene reconstruction, in its traditional multi-view stereo (MVS) sense, assumes access to a dense set of scene views. Recent reconstruction methods such as NeRF [56] optimize a 3D representation by minimizing multi-view rendering losses. There are two popular classes of 3D representation: (i) explicit representations like meshes [22, 113, 24, 61, 57, 74] or 3D points/Gaussians [37, 25], and (ii) implicit representations like occupancy fields [63], radiance fields [56, 60] and signed distance functions (SDF) [108]. Compared to occupancy fields, SDF [64, 107, 90, 17, 21] simplifies surface constraints integration, improving scene geometry. For this reason, we adopt an SDF formulation and demonstrate that it outperforms occupancy.\nSparse-view reconstruction instead assumes few input views (usually 1 to 8). An approach to mitigate the lack of dense multiple views is to leverage 2D diffusion priors in optimization [53, 97], but this is often slow and not very robust. More recently, authors have focused on training feed-forward reconstructors on large datasets [14, 35, 55, 46, 98, 58, 92]. In particular, the state-of-the-art LRM [29] trains a large Transformer [87] to predict NeRF using a triplane representation [7, 9]. LRM extensions study other 3D representations like meshes [101, 95] and 3DGS [119, 103, 78, 114], improved backbones [94, 95] and training protocols [84, 34]. Our approach also builds on LRM but introduces three key modifications: (i) an SDF formulation for improved geometry, (ii) PBR material prediction for relighting, and (iii) a texture refiner for better texture details.\n3D modeling with PBR materials. Most 3D generators output 3D objects with baked illumination, either view-dependent [56, 37] or view-independent [29]. Since baked lighting ignores the model's response to environmental illumination, it is unsuitable for graphics pipelines with controlled lighting. Physically-based rendering (PBR) defines material properties so that a suitable shader can account for illumination realistically. Several MVS works have considered estimating PBR materials using NeRF [4, 3, 100], SDF [115], differentiable meshes [61, 27] or 3DGS [32, 41]. In generative modelling, [10, 68, 48, 102] augment the text-to-3D SDS optimization [66] with a PBR model. Differently from them, we integrate PBR modeling in our feed-forward text-to-3D network, unlocking for the first time fast text-based generation of 3D assets with controllable PBR materials."}, {"title": "Method", "content": "AssetGen is a two-stage pipeline (Fig. 2). First, text-to-image (Sec. 3.1), takes text as input and generates a 4-view grid of images with material information. Second, image-to-3D, comprises a novel PBR-based sparse-view reconstruction model (Sec. 3.2) and the texture refiner (Sec. 3.3). As such, AssetGen is applicable to two tasks: text-to-3D (stage 1+2) and image-to-3D (stage 2 only)."}, {"title": "Text-to-image: Generating shaded and albedo images from text", "content": "The goal of the text-to-image module is to generate several views of the generated 3D object. To this end, we employ an internal text-to-image diffusion model pre-trained on billions of text-annotated images, with an architecture similar to Emu [16]. Similar to [72, 40], we finetune the model to predict a grid of four images \\(I_i, i = 1, ..., 4\\), each depicting the object from canonical viewpoints \\(\\pi_i\\). Note that \\(I\\) are RGB images of the shaded object. We tried deferring the PBR parameter extraction to the image-to-3D stage, but this led to suboptimal results. This is due to the determinism of the image-to-3D stage, which fails to model ambiguities when assigning materials to surfaces.\nA natural solution, then, is to predict the PBR parameters directly in the text-to-image stage. These consists of the albedo \\(\\rho_0\\) (by which we mean the base color, which is the same as albedo only for zero metalness), the metalness \\(\\gamma\\), and the roughness \\(\\alpha\\). However, we found this to be ineffective too because the metalness and roughness maps deviate from the distribution of natural images making them a hard target for finetuning. Our novel solution is to train the model to generate instead a 4-view grid with 6 channels, 3 for the shaded appearance \\(I\\) and 3 more for the albedo \\(\\rho_0\\). This reduces the finetuning gap, and removes enough ambiguity for accurate PBR prediction in the image-to-3D stage."}, {"title": "Image-to-3D: A PBR-based large reconstruction model", "content": "We now describe the image-to-3D stage, which solves the reconstruction tasks given either a small number of views \\(I_i\\) (few-view reconstruction), or the 4-view 6-channel grid of Sec. 3.1.\nAt the core of our method is a new PBR-aware reconstruction model, MetaILRM, that reconstructs the object given \\(N > 1\\) posed images \\((I_i, \\pi_i)_{i=1}^N\\), where \\(I_i \\in \\mathbb{R}^{H \\times W \\times D}\\) and \\(\\pi_i \\in \\Pi\\) is the camera viewpoint. As noted in Sec. 3.1, we consider \\(N = 4\\) canonical viewpoints \\(\\pi_1, ..., \\pi_4\\) (fixed to 20\u00b0 elevation and 0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0 azimuths) and \\(D = 6\\) input channels. The output is a 3D field representing the shape and PBR materials of the object as an SDF \\(s : \\mathbb{R}^3 \\rightarrow \\mathbb{R}\\), where \\(s(x)\\) is the signed distance from the 3D point \\(x\\) to the nearest object surface point, and a PBR function \\(k : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^5\\), where \\(k(x) = (\\rho_0, \\gamma, \\alpha)\\) are the albedo, metalness and roughness.\nThe key to learning the model is the differentiable rendering operator \\(\\mathcal{R}\\). This takes as input a field \\(l : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^D\\), the SDF \\(s\\), the viewpoint \\(\\pi\\), and a pixel \\(u \\in \\mathcal{U} = [0, W) \\times [0, H)\\), and outputs the projection of the field on the pixel according to the rendering equation [56], which has the same number of channels D as the rendered field \\(l\\):\n\\[\n\\mathcal{R}(u | l, s, \\pi) = \\int_0^\\infty l(x_t) o(x_t | s) e^{-\\int_0^t o(x_{\\tau} | s) d\\tau} dt.\n\\]\nHere \\(x_t = x_0 + t w\\), \\(t \\in [0, \\infty)\\) is the ray that goes from the camera center \\(x_0\\) through the pixel \\(u\\) along direction \\(-w \\in \\mathbb{S}^2\\). The function \\(o(x | s)\\) is the opacity of the 3D point \\(x\\) and is obtained from the SDF value \\(s(x)\\) using the VolSDF [107] formula\n\\[\no(x | s) = \\frac{a}{2} (1 + \\text{sign}(s(x)) (1 - e^{-|s(x)| / b})),\n\\]\nwhere \\(a, b\\) are the hyperparameters. We use Eq. (1) to render several different types of fields \\(l\\), the most important of which is the radiance field, introduced next along with the material model.\nReflectance model. The appearance of the object \\(\\hat{I}(u) = \\mathcal{R}(u | L, s, \\pi)\\) in a shaded RGB image \\(\\hat{I}\\) is obtained by rendering its radiance field \\(l(x) = L(x, w_o | k, n)\\), where \\(n\\) is the field of unit normals. The radiance is the light reflected by the object in the direction \\(w_o\\) of the observer (see App. A.8 for details), which in PBR is given by:\n\\[\nL(x, w_o | k, n) = \\int_{\\mathcal{H}(n)} f(w_i, w_o | k(x), n(x)) L(x, -w_i) (n(x) \\cdot w_i) d \\omega_i,\n\\]\nwhere \\(w_o, w_i \\in \\mathcal{H}(n) = \\{w \\in \\mathbb{S}^2 : n \\cdot w > 0\\}\\) are two unit vectors pointing outside the object and \\(L(x, w_i)\\) is the radiance incoming from the enviroment at \\(x\\) from direction \\(w_i\\) in the solid angle \\(d \\omega_i\\). The Bidirectional Reflectance Distribution Function (BRDF) \\(f\\) tells how light received from direction \\(-w\\) (incoming) is scattered into different directions \\(w_o\\) (outgoing) by the object [20].\nIn PBR, we consider a physically-inspired model for the BRDF, striking a balance between realism and complexity [2, 85, 75, 15, 88]; specifically, we use the Disney GGX model [88, 5], which depends on parameters \\(\\rho_0\\), \\(\\gamma\\), and \\(\\alpha\\) only (see App. A.12.1 for the parametric form of \\(f\\)). Hence, the Metal LRM predicts the triplet \\(k(x) = (\\rho_0, \\gamma, \\alpha)\\) at each 3D point x.\nDeferred shading. In practice, instead of computing \\(\\hat{I}(u) = \\mathcal{R}(u | L, s, \\pi)\\) using Eqs. (1) and (3), we use the process of deferred shading [20]:\n\\[\n\\hat{I}(u) = \\mathcal{R}_{def}(u | \\hat{k}, s, \\pi) = \\int_{\\mathcal{H}(n)} f(w_i, w_o | \\hat{k}, \\hat{n}) L_{env} (-w_i) (\\hat{n} \\cdot w_i) d \\omega_i,\n\\]\nwhere \\(L_{env}\\) is the environment radiance (assumed to be the same for all \\(x\\)), \\(\\hat{k} = \\mathcal{R}(u | k, s, \\pi)\\) and \\(\\hat{n} = \\mathcal{R}(u | n, s, \\pi)\\) are rendered versions of the material and normal fields. The advantage of Eq. (4) is that the BRDF \\(f\\) is evaluated only once per pixel, which is much faster and less memory intensive than doing so for each 3D point during the evaluation of Eq. (1), particularly for training/backpropagation. During training, furthermore, the environment light is assumed to be a single light source at infinity, so the integral (4) reduces to evaluating a single term.\nTraining formulation and losses. MetaILRM is thus a neural network that takes as input a set of images \\((I_i, \\pi_i)_{i=1}^N\\) and outputs estimates \\(\\hat{s}\\) and \\(\\hat{k}\\) for the SDF and PBR fields. We train it from a dataset of mesh surfaces \\(M \\subset \\mathbb{R}^3\\) with ground truth PBR materials \\(k : M \\rightarrow \\mathbb{R}^5\\).\nReconstruction models are typically trained via supervision on renders [29, 95]. However, physically accurate rendering via Eq. (1) is very expensive. We overcome this hurdle in two ways. First, we render the raw ground-truth PBR fields \\(k\\) and use them to supervise their predicted counterparts with the MSE loss, skipping Eq. (1). For the rendered albedo \\(\\rho_0\\) \u2013 which is similar enough to natural images \u2013 we also use the LPIPS [116] loss:\n\\[\n\\mathcal{L}_{pbr} = \\text{LPIPS} (\\mathcal{R}(\\rho_0, \\hat{s}, \\pi), \\mathcal{R}(\\cdot | \\rho_0, M, \\pi)) + ||\\mathcal{R}(\\cdot | k, \\hat{s}, \\pi) - \\mathcal{R}(\\cdot | k, M, \\pi)||_2.\n\\]\nWe further supervise the PBR field by adding a computationally-efficient deferred shading loss:\n\\[\n\\mathcal{L}_{def} = ||\\sqrt{w} (\\mathcal{R}_{def}(\\cdot | \\hat{k}, \\hat{s}, \\pi) - \\mathcal{R}_{def}(\\cdot | k, M, \\pi))||_2.\n\\]\nThe weight \\(w(u) = n(u) \\cdot n(u)\\) is the dot product of the predicted and ground-truth normals at pixel \\(u\\). It discounts the loss where the predicted geometry is not yet learnt. Fig. 13 (b) visualizes deferred shading and the rendering loss.\nFinally, we also supervise the SDF field with a direct loss \\(\\mathcal{L}_{sdf}\\) (implemented as in [1]), a depth-MSE loss \\(\\mathcal{L}_{depth}\\) between the depth renders and the ground truth, and with a binary cross-entropy \\(\\mathcal{L}_{mask}\\) between the alpha-mask renders and the ground-truth masks. Refer to App. A.6.2 for more details.\nLightPlane implementation. We base MetaILRM on LightplaneLRM [6], a variant of LRM [29] exploiting memory and compute-efficient Lightplane splatting and rendering kernels, offering better quality reconstructions. However, since LightplaneLRM uses density fields, which are suboptimal for mesh conversion [90, 64, 1], we extend the Lightplane renderering GPU kernel with a VolSDF [107] renderer using Eq. (2). Additionally, we also fuse into the kernel the direct SDF loss \\(\\mathcal{L}_{sdf}\\) since a naive autograd implementation is too memory-heavy."}, {"title": "Mesh extraction and texture refiner", "content": "The MetaILRM module of Sec. 3.2 outputs a sign distance function \\(s\\), implicitly defining the object surface \\(\\Lambda = \\{x \\in \\mathbb{R}^3 | s(x) = 0\\}\\) as a level set of \\(s\\). We use the Marching Tetrahedra algorithm [18] to trace the level set and output a mesh \\(\\mathcal{M} \\approx \\Lambda\\). Then, xAtlas [111] extracts a UV map \\(\\phi : [0, V]^2 \\rightarrow \\mathcal{M}\\), mapping each 2D UV-space point \\(v = \\phi(x)\\) to a point \\(x \\in \\mathcal{M}\\) on the mesh.\nNext, the goal is to extract a high-quality 5-channel PBR texture image \\(K \\in \\mathbb{R}^{V \\times V \\times 5}\\) capturing the albedo, metalness, and roughness of each mesh point. The texture image \\(K\\) can be defined directly by sampling the predicted PBR field \\(k\\) as \\(K(v) \\leftarrow k(\\phi(v))\\), but this often yields blurry results due to the limited resolution of MetaILRM. Instead, we design a texture refiner module which takes as input the coarse PBR-sampled texture image as well as the N views representing the object and outputs a much sharper texture \\(K\\). In essence, this modules leverages the information from the different views to refine the coarse texture image. The right part of Fig. 2 illustrates this module.\nMore specifically, it relies on a network \\(\\Phi\\) which is fed \\(N + 1\\) texture images \\(\\{K_i\\}_{i=0}^N\\). First, each pixel \\(v \\in [0, V]^2\\) of \\(K_0 \\in \\mathbb{R}^{V \\times V \\times 11}\\) is annotated with the concatenation of the normal, the 3D location, and the output of MetaILRM's PBR field \\(k(\\phi(v))\\) evaluated at \\(v\\)'s 3D point \\(\\phi(v)\\). The remaining \\(K_1, ..., K_N\\) correspond to partial texture images with 6 channels (for the base and shaded colors) which are obtained by backprojecting the object views to the mesh surface. The network \\(\\Phi\\) utilises two U-Nets to fuse \\(\\{K_i\\}_{i=0}^N\\) into the enhanced texture \\(K\\). \\(\\Phi\\)'s goal is to select, for each UV point \\(v\\), which of the N input views provides the best information. Specifically, each partial texture image \\(K_i\\) is processed in parallel by a first U-Net, and the resulting information is communicated via cross attention to a second U-Net whose goal is to refine \\(K_0\\) into the enhanced texture \\(K\\). Please refer to App. A.7 for further details.\nSuch a network is trained on the same dataset and supervised with the PBR and albedo rendering losses as MetaILRM. The only difference is meshes (whose geometry is fixed) are rendered differentiably using PyTorch3D's [69] mesh rasterizer instead of the Lightplane SDF renderer."}, {"title": "Experiments", "content": "Our training data consists of 140,000 meshes of diverse semantic categories created by 3D artists. For each asset, we render 36 views at random elevations within the range of [-30\u00b0, 50\u00b0] at uniform intervals of 30\u00b0 around the object, lit with a randomly selected environment map. We render the shaded images, albedo, metalness, roughness, depth maps, and foreground masks from each viewpoint. The text-to-image stage is based on an internal text-to-image model architecturally similar to Emu [16], fine-tuned on a subset of 10,000 high-quality 3D samples, captioned by a Cap3D-like pipeline [51] that uses Llama3 [86]. The other stage utilizes the entire 3D dataset instead.\nFor evaluation, following [103, 101, 40], we assess visual quality using PSNR and LPIPS [116] between the rendered and ground-truth images. PSNR is computed in the foreground region to avoid metric inflation due to the empty background. Geometric quality is measured by the L1 error between the rendered and ground-truth depth maps (of the foreground pixels), as well as the IoU of the object silhouette. We further report Chamfer Distance (CD) and Normal Correctness (NC) for 20,000"}, {"title": "Sparse-view reconstruction", "content": "We tackle the sparse-view reconstruction task of predicting a 3D mesh from 4 posed images of an object on a subset of 332 meshes from Google Scanned Objects (GSO) [19]. We compare against state-of-the-art Instant3D-LRM [40], GRM [103], InstantMesh [101], and MeshLRM [95]. We also include LightplaneLRM [6], an improved version of Instant3D-LRM, which serves as our base model. MeshLRM [95] has not been open-sourced so we compare only qualitatively to meshes from their webpage. All methods are evaluated using the same input views at 5122 resolution. Since none of the latter predict PBR materials and since GSO lacks ground-truth PBR materials, for fairness, we use a variant of our model that predicts shaded object textures.\nAs shown in Figs. 4 and 9 and Tab. 3, our method outperforms all baselines across all metrics. GRM captures texture detail well but struggles with fine geometric structures when meshified. InstantMesh and LightplaneLRM improve geometry but fall short on finer details and texture quality. Our approach excels in reconstructing shapes with detailed geometry and high-fidelity textures.\nAblations in Tab. 3 and Fig. 4 show that incorporating our scalable SDF-based rendering and direct SDF loss into the base LightplaneLRM model enhances geometric quality. Adding texture refinement further brings fine texture details.\nNext, we consider the task of sparse-view reconstruction with PBR materials, where the goal is to reconstruct the 3D geometry and texture properties (albedo, metalness, and roughness) from four posed shaded 2D views of an object. This is done on an internal dataset of 256 artist-created 3D meshes, curated for high-quality materials. Since there are no existing few-view feed-forward PBR reconstructors, we conduct an ablation study in Tab. 1 and Figs. 3 and 12.\nWhile adding material prediction with additional MLP heads provides some improvements, we observe that incorporating the deferred shading loss and texture refinement is essential for high-quality PBR decomposition. Example PBR predictions are shown in Fig. 8."}, {"title": "Text-to-3D generation", "content": "Finally, we evaluate text-to-3D with PBR materials. We compare against state-of-the-art feed-forward methods that generate assets at comparable speed (\u2248 10 to 30 sec per asset). This includes text-to-3D variants of GRM [103], InstantMesh [101], and LightplaneLRM [6]. GRM uses Instant3D's 4-view grid generator, InstantMesh receives the first view from our 2D diffusion model and subsequently generates 6 views, while LightplaneLRM accepts 4 views from our grid generator. Since these"}, {"title": "Conclusions", "content": "We have introduced Meta 3D AssetGen, a significant advancement in sparse-view reconstruction and text-to-3D. Meta 3D AssetGen can generate 3D meshes with high-quality textures and PBR materials faithful to the input text. This uses several key innovations: generating multi-view grids with both shaded and albedo channels, introducing a new reconstruction network that predicts PBR materials from this information, using deferred shading to train this network, improving geometry via a new scalable SDF-based renderer and SDF loss, and introducing a new texture refinement network. Comprehensive evaluations and ablations demonstrate the effectiveness of these design choices and state-of-the-art performance."}, {"title": "Societal Impact", "content": "Safeguards should be implemented to prevent abuse, such as filtering input text prompts and detecting unsafe content in generated 3D models. Additionally, our generation process may be vulnerable to biases present in the data and 3D models it relies on, potentially perpetuating these biases in the generated content. Despite these risks, our method can augment the work of artists and creative professionals by serving as a complementary tool to boost productivity. It also holds the potential to democratize 3D content creation, making it accessible to those without specialized knowledge or expensive proprietary software."}, {"title": "Limitations", "content": "Meta 3D AssetGen significantly advances shape generation but faces several limitations. Despite the fine-tuning of the multiview image grid generator for view consistency, it is not guaranteed, potentially impacting 3D reconstruction quality. Since we use an SDF as an underlying representation, the reconstructor may incorrectly model translucent objects or thin structures like hair or fur. Additionally, while our scalable Triton [65] implementation supports a triplane representation at a resolution of 128 \u00d7 128, this representation is inefficient, as much of its capacity is used for empty regions. Future work could explore scalable representations such as octrees, sparse voxel grids, and hash-based methods, which may remove the need for a separate texture enhancement model. We also only predict albedo, metalness and roughness, and not emissivity or ambient occlusions. Finally, our method has only been tested on object-level reconstructions, leaving scene-scale 3D generation for future research."}, {"title": "Additional qualitative comparisons", "content": "This section describes additional qualitative comparisons that, due to limited space, could not be included in the main paper. Firstly, please refer to the video attached in the supplementary material which provides a holistic presentation of Meta 3D AssetGen's qualitative results. In Fig. 7, we highlight the contributions of MetaILRM in geometry, texture and material reconstruction. In Fig. 11, we visualize the control of materials provided by Meta 3D AssetGen, i.e., metalness and roughness, by changing the text prompt for the same concept. Fig. 8 visualizes the renders of the material maps extracted with MetaILRM given four input test views. In Fig. 9, we provide a more extensive qualitative comparison to MeshLRM, the strongest few-view reconstruction baseline. Finally, Fig. 6 provides a gallery of text-conditioned generations depicting Blender-shaded renders together with the rendered PBR maps."}, {"title": "User-study details", "content": "As described in Sec. 4.2, we conducted an user study on 404 meshes generated using the DreamFusion [66] prompt-set on a standard crowdsourcing marketplace. In the study, users were shown 360\u00b0"}, {"title": "Additional text-to-3D comparisons", "content": "While Tab. 2 compared```json\n{\n        "}, {"title": "Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials", "authors": ["Yawar Siddiqui", "Tom Monnier", "Filippos Kokkinos", "Mahendra Kariya", "Yanir Kleiman", "Emilien Garreau", "Oran Gafni", "Natalia Neverova", "Andrea Vedaldi", "Roman Shapovalov", "David Novotny"], "abstract": "We present Meta 3D AssetGen (AssetGen), a significant advancement in text-to-3D generation which produces faithful, high-quality meshes with texture and material control. Compared to works that bake shading in the 3D object's appearance, AssetGen outputs physically-based rendering (PBR) materials, supporting realistic relighting. AssetGen generates first several views of the object with factored shaded and albedo appearance channels, and then reconstructs colours, metalness and roughness in 3D, using a deferred shading loss for efficient supervision. It also uses a sign-distance function to represent 3D shape more reliably and introduces a corresponding loss for direct shape supervision. This is implemented using fused kernels for high memory efficiency. After mesh extraction, a texture refinement transformer operating in UV space significantly improves sharpness and details. AssetGen achieves 17% improvement in Chamfer Distance and 40% in LPIPS over the best concurrent work for few-view reconstruction, and a human preference of 72% over the best industry competitors of comparable speed, including those that support PBR. Project page with generated assets: https://assetgen.github.io", "sections": [{"title": "Introduction", "content": "Generating 3D objects from textual prompts or images has enormous potential for 3D graphics, animation, gaming and AR/VR. However, despite the outstanding progress of image and video generation [66, 42, 53, 40, 95, 101], the quality of 3D generators is still insufficient for professional use. Typical limitations include slow generation speed and artifacts in the generated 3D meshes and textures. Many approaches, furthermore, still \u201cbake\u201d appearance as albedo, ignoring how the 3D model should respond to variable environmental illumination. This results in visually unattractive outputs, especially for reflective materials, which look out of place when put in novel environments.\nWe introduce Meta 3D AssetGen (AssetGen), a significant step-up in text-conditioned 3D generation. AssetGen generates assets in under 30s while outperforming prior works of a comparable speed in faithfulness, quality of the generated 3D meshes and, especially, quality and control of materials, supporting Physically-Based Rendering (PBR) [85]. AssetGen uses a two-stage design inspired by [40]. The first stage stochastically generates 4 images of the object from 4 canonical viewpoints, and the second stage deterministically reconstructs the 3D shape and appearance of the object from these views (Fig. 1). This approach is faster and more robust than SDS-based techniques [66] and produces more diverse and faithful results than single-stage 3D generators [36, 59, 92, 77].\nAssetGen explicitly models the interaction of light and object using PBR, which is essential for 3D computer graphics applications. Specifically, we accounts for albedo, metalness, and roughness to render scenes that accurately reflect environmental illumination. Furthermore, we focus on meshes as the output representation due to their prevalence in applications and compatibility with PBR.\nOur first contribution is to extend the two stages to generate PBR information. Since the image-to-3D stage is deterministic, it is the text-to-image stage, which is stochastic, that should resolve ambiguities in assigning materials. This could be done by fine tuning the text-to-image model to output directly the required PBR channels, but we found this to be problematic due to the large gap between PBR and natural images. Instead, we assign the text-to-image model the simpler task of outputting shaded and albedo (unshaded) versions of the appearance. This allows the image-to-3D component to accurately predict PBR materials by analyzing the differences between the albedo and shaded channels.\nOur second innovation is in the mesh generation step. Existing works typically output an opacity field to represent 3D shape, but the latter can lead to poorly-defined level sets, which result in meshing artifacts. To address this, AssetGen's image-to-3D stage, MetaILRM, directly predicts a signed-distance field (SDF) instead of an opacity field. This yields higher-quality meshes, as the zero level set of an SDF traces the object's surface more reliably. Furthermore, this representation can be easily supervised using ground-truth depth maps, which is not immediately possible for opacities. We use VolSDF [107] to render the SDF-based model differentiably, and we improve the training efficiency by implementing rendering within the memory-efficient Lightplane kernels [6]. This allows for larger batches and photometric loss supervision on high-resolution renders, leading to better texture quality.\nWhile MetaILRM produces high quality shapes and good textures, the latter can still be blurry, due to the limited resolution of the volumetric representation, limitations of the feed-forward 3D reconstruction network, and the mesh conversion process. We address this issue by training a new texture refiner transformer which upgrades the extracted albedo and materials by fusing information extracted from the original views, resolving at the same time possible conflicts between them.\nWe demonstrate the effectiveness of AssetGen on the image-to-3D and text-to-3D tasks. For image-to-3D, we attain state-of-the-art performance among existing few-view mesh-reconstruction methods when measuring the accuracy of the recover shaded and PBR texture maps. For text-to-3D, we conduct extensive user studies to compare the best methods from academia and industry that have comparable inference time, and outperform them in terms of visual quality and text alignment."}, {"title": "Related Work", "content": "Text-to-3D. Inspired by text-to-image models, early text-to-3D approaches [62, 33, 26, 109, 104] train 3D diffusion models on datasets of captioned 3D assets. Yet, the limited size and diversity of 3D data prevents generalization to open-vocabulary prompts. Recent works thus pivoted into basing such generators on text-to-image models that are trained on billions of captioned images.\nAmong these, works like [73, 54] finetune 2D diffusion models to output 3D representations, but the quality is limited due to the large 2D-3D domain gap. Other approaches can be dived into two groups.\nThe first group contains methods that build on DreamFusion, a seminal work by [66], and distill 3D objects by optimizing NeRF via the SDS loss, matching its renders to the belief of a pre-trained text-to-image model. Extensions have considered: (i) other 3D representations like hash grids [42, 67], meshes [42] and 3D Gaussians (3DGS) [79, 110, 12]; (ii) improved SDS [89, 93, 118, 30]; (iii) monocular conditioning [67, 80, 112, 76]; (iv) predicting additional normals or depth for better geometry [68, 76]. Yet, distillation methods are prone to issues such as the Janus effect (duplicating object parts) and content drift [72]. A common solution is to incorporate view-consistency priors into the diffusion model, by either conditioning on cameras [45, 71, 31, 11, 67] or by generating multiple object views jointly [72, 96, 91, 38, 117]. Additionally, SDS optimization is slow and requires minutes to hours per assets; this issue is partly addressed in [50, 99] with amortized SDS.\nThe second group of methods includes faster two-stage approaches [44, 49, 47, 106, 105, 8, 81, 28, 23] that start by generating multiple views of the object using a text-to-image or -video model [52, 13] tuned to output multiple views of the object followed by per-scene optimization using NeRF [56] or 3DGS [37]. However, per-scene optimization requires several highly-consistent views which are difficult to generate reliably. Instant3D [40] improves speed and robustness by generating a grids of just four views followed by a feed-forward network (LRM [29]) that reconstructs the object from these. One-2-3-45++ [43] replaces the LRM with a 3D diffusion model. Our AssetGen builds on the Instant3D paradigm and upgrades the LRM to output PBR materials and an SDF-based representation of 3D shape. Furthermore, it starts from grids of four views with shaded and albedo channels, key to predicting accurate 3D shape and materials from images.\n3D reconstruction from images. 3D scene reconstruction, in its traditional multi-view stereo (MVS) sense, assumes access to a dense set of scene views. Recent reconstruction methods such as NeRF [56] optimize a 3D representation by minimizing multi-view rendering losses. There are two popular classes of 3D representation: (i) explicit representations like meshes [22, 113, 24, 61, 57, 74] or 3D points/Gaussians [37, 25], and (ii) implicit representations like occupancy fields [63], radiance fields [56, 60] and signed distance functions (SDF) [108]. Compared to occupancy fields, SDF [64, 107, 90, 17, 21] simplifies surface constraints integration, improving scene geometry. For this reason, we adopt an SDF formulation and demonstrate that it outperforms occupancy.\nSparse-view reconstruction instead assumes few input views (usually 1 to 8). An approach to mitigate the lack of dense multiple views is to leverage 2D diffusion priors in optimization [53, 97], but this is often slow and not very robust. More recently, authors have focused on training feed-forward reconstructors on large datasets [14, 35, 55, 46, 98, 58, 92]. In particular, the state-of-the-art LRM [29] trains a large Transformer [87] to predict NeRF using a triplane representation [7, 9]. LRM extensions study other 3D representations like meshes [101, 95] and 3DGS [119, 103, 78, 114], improved backbones [94, 95] and training protocols [84, 34]. Our approach also builds on LRM but introduces three key modifications: (i) an SDF formulation for improved geometry, (ii) PBR material prediction for relighting, and (iii) a texture refiner for better texture details.\n3D modeling with PBR materials. Most 3D generators output 3D objects with baked illumination, either view-dependent [56, 37] or view-independent [29]. Since baked lighting ignores the model's response to environmental illumination, it is unsuitable for graphics pipelines with controlled lighting. Physically-based rendering (PBR) defines material properties so that a suitable shader can account for illumination realistically. Several MVS works have considered estimating PBR materials using NeRF [4, 3, 100], SDF [115], differentiable meshes [61, 27] or 3DGS [32, 41]. In generative modelling, [10, 68, 48, 102] augment the text-to-3D SDS optimization [66] with a PBR model. Differently from them, we integrate PBR modeling in our feed-forward text-to-3D network, unlocking for the first time fast text-based generation of 3D assets with controllable PBR materials."}, {"title": "Method", "content": "AssetGen is a two-stage pipeline (Fig. 2). First, text-to-image (Sec. 3.1), takes text as input and generates a 4-view grid of images with material information. Second, image-to-3D, comprises a novel PBR-based sparse-view reconstruction model (Sec. 3.2) and the texture refiner (Sec. 3.3). As such, AssetGen is applicable to two tasks: text-to-3D (stage 1+2) and image-to-3D (stage 2 only)."}, {"title": "Text-to-image: Generating shaded and albedo images from text", "content": "The goal of the text-to-image module is to generate several views of the generated 3D object. To this end, we employ an internal text-to-image diffusion model pre-trained on billions of text-annotated images, with an architecture similar to Emu [16]. Similar to [72, 40], we finetune the model to predict a grid of four images \\(I_i, i = 1, ..., 4\\), each depicting the object from canonical viewpoints \\(\\pi_i\\). Note that \\(I\\) are RGB images of the shaded object. We tried deferring the PBR parameter extraction to the image-to-3D stage, but this led to suboptimal results. This is due to the determinism of the image-to-3D stage, which fails to model ambiguities when assigning materials to surfaces.\nA natural solution, then, is to predict the PBR parameters directly in the text-to-image stage. These consists of the albedo \\(\\rho_0\\) (by which we mean the base color, which is the same as albedo only for zero metalness), the metalness \\(\\gamma\\), and the roughness \\(\\alpha\\). However, we found this to be ineffective too because the metalness and roughness maps deviate from the distribution of natural images making them a hard target for finetuning. Our novel solution is to train the model to generate instead a 4-view grid with 6 channels, 3 for the shaded appearance \\(I\\) and 3 more for the albedo \\(\\rho_0\\). This reduces the finetuning gap, and removes enough ambiguity for accurate PBR prediction in the image-to-3D stage."}, {"title": "Image-to-3D: A PBR-based large reconstruction model", "content": "We now describe the image-to-3D stage, which solves the reconstruction tasks given either a small number of views \\(I_i\\) (few-view reconstruction), or the 4-view 6-channel grid of Sec. 3.1.\nAt the core of our method is a new PBR-aware reconstruction model, MetaILRM, that reconstructs the object given \\(N > 1\\) posed images \\((I_i, \\pi_i)_{i=1}^N\\), where \\(I_i \\in \\mathbb{R}^{H \\times W \\times D}\\) and \\(\\pi_i \\in \\Pi\\) is the camera viewpoint. As noted in Sec. 3.1, we consider \\(N = 4\\) canonical viewpoints \\(\\pi_1, ..., \\pi_4\\) (fixed to 20\u00b0 elevation and 0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0 azimuths) and \\(D = 6\\) input channels. The output is a 3D field representing the shape and PBR materials of the object as an SDF \\(s : \\mathbb{R}^3 \\rightarrow \\mathbb{R}\\), where \\(s(x)\\) is the signed distance from the 3D point \\(x\\) to the nearest object surface point, and a PBR function \\(k : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^5\\), where \\(k(x) = (\\rho_0, \\gamma, \\alpha)\\) are the albedo, metalness and roughness.\nThe key to learning the model is the differentiable rendering operator \\(\\mathcal{R}\\). This takes as input a field \\(l : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^D\\), the SDF \\(s\\), the viewpoint \\(\\pi\\), and a pixel \\(u \\in \\mathcal{U} = [0, W) \\times [0, H)\\), and outputs the projection of the field on the pixel according to the rendering equation [56], which has the same number of channels D as the rendered field \\(l\\):\n\\[\n\\mathcal{R}(u | l, s, \\pi) = \\int_0^\\infty l(x_t) o(x_t | s) e^{-\\int_0^t o(x_{\\tau} | s) d\\tau} dt.\n\\]\nHere \\(x_t = x_0 + t w\\), \\(t \\in [0, \\infty)\\) is the ray that goes from the camera center \\(x_0\\) through the pixel \\(u\\) along direction \\(-w \\in \\mathbb{S}^2\\). The function \\(o(x | s)\\) is the opacity of the 3D point \\(x\\) and is obtained from the SDF value \\(s(x)\\) using the VolSDF [107] formula\n\\[\no(x | s) = \\frac{a}{2} (1 + \\text{sign}(s(x)) (1 - e^{-|s(x)| / b})),\n\\]\nwhere \\(a, b\\) are the hyperparameters. We use Eq. (1) to render several different types of fields \\(l\\), the most important of which is the radiance field, introduced next along with the material model.\nReflectance model. The appearance of the object \\(\\hat{I}(u) = \\mathcal{R}(u | L, s, \\pi)\\) in a shaded RGB image \\(\\hat{I}\\) is obtained by rendering its radiance field \\(l(x) = L(x, w_o | k, n)\\), where \\(n\\) is the field of unit normals. The radiance is the light reflected by the object in the direction \\(w_o\\) of the observer (see App. A.8 for details), which in PBR is given by:\n\\[\nL(x, w_o | k, n) = \\int_{\\mathcal{H}(n)} f(w_i, w_o | k(x), n(x)) L(x, -w_i) (n(x) \\cdot w_i) d \\omega_i,\n\\]\nwhere \\(w_o, w_i \\in \\mathcal{H}(n) = \\{w \\in \\mathbb{S}^2 : n \\cdot w > 0\\}\\) are two unit vectors pointing outside the object and \\(L(x, -w_i)\\) is the radiance incoming from the enviroment at \\(x\\) from direction \\(w_i\\) in the solid angle \\(d \\omega_i\\). The Bidirectional Reflectance Distribution Function (BRDF) \\(f\\) tells how light received from direction \\(-w\\) (incoming) is scattered into different directions \\(w_o\\) (outgoing) by the object [20].\nIn PBR, we consider a physically-inspired model for the BRDF, striking a balance between realism and complexity [2, 85, 75, 15, 88]; specifically, we use the Disney GGX model [88, 5], which depends on parameters \\(\\rho_0\\), \\(\\gamma\\), and \\(\\alpha\\) only (see App. A.12.1 for the parametric form of \\(f\\)). Hence, the Metal LRM predicts the triplet \\(k(x) = (\\rho_0, \\gamma, \\alpha)\\) at each 3D point x.\nDeferred shading. In practice, instead of computing \\(\\hat{I}(u) = \\mathcal{R}(u | L, s, \\pi)\\) using Eqs. (1) and (3), we use the process of deferred shading [20]:\n\\[\n\\hat{I}(u) = \\mathcal{R}_{def}(u | \\hat{k}, s, \\pi) = \\int_{\\mathcal{H}(n)} f(w_i, w_o | \\hat{k}, \\hat{n}) L_{env} (-w_i) (\\hat{n} \\cdot w_i) d \\omega_i,\n\\]\nwhere \\(L_{env}\\) is the environment radiance (assumed to be the same for all \\(x\\)), \\(\\hat{k} = \\mathcal{R}(u | k, s, \\pi)\\) and \\(\\hat{n} = \\mathcal{R}(u | n, s, \\pi)\\) are rendered versions of the material and normal fields. The advantage of Eq. (4) is that the BRDF \\(f\\) is evaluated only once per pixel, which is much faster and less memory intensive than doing so for each 3D point during the evaluation of Eq. (1), particularly for training/backpropagation. During training, furthermore, the environment light is assumed to be a single light source at infinity, so the integral (4) reduces to evaluating a single term.\nTraining formulation and losses. MetaILRM is thus a neural network that takes as input a set of images \\((I_i, \\pi_i)_{i=1}^N\\) and outputs estimates \\(\\hat{s}\\) and \\(\\hat{k}\\) for the SDF and PBR fields. We train it from a dataset of mesh surfaces \\(M \\subset \\mathbb{R}^3\\) with ground truth PBR materials \\(k : M \\rightarrow \\mathbb{R}^5\\).\nReconstruction models are typically trained via supervision on renders [29, 95]. However, physically accurate rendering via Eq. (1) is very expensive. We overcome this hurdle in two ways. First, we render the raw ground-truth PBR fields \\(k\\) and use them to supervise their predicted counterparts with the MSE loss, skipping Eq. (1). For the rendered albedo \\(\\rho_0\\) \u2013 which is similar enough to natural images \u2013 we also use the LPIPS [116] loss:\n\\[\n\\mathcal{L}_{pbr} = \\text{LPIPS} (\\mathcal{R}(\\rho_0, \\hat{s}, \\pi), \\mathcal{R}(\\cdot | \\rho_0, M, \\pi)) + ||\\mathcal{R}(\\cdot | k, \\hat{s}, \\pi) - \\mathcal{R}(\\cdot | k, M, \\pi)||_2.\n\\]\nWe further supervise the PBR field by adding a computationally-efficient deferred shading loss:\n\\[\n\\mathcal{L}_{def} = ||\\sqrt{w} (\\mathcal{R}_{def}(\\cdot | \\hat{k}, \\hat{s}, \\pi) - \\mathcal{R}_{def}(\\cdot | k, M, \\pi))||_2.\n\\]\nThe weight \\(w(u) = n(u) \\cdot n(u)\\) is the dot product of the predicted and ground-truth normals at pixel \\(u\\). It discounts the loss where the predicted geometry is not yet learnt. Fig. 13 (b) visualizes deferred shading and the rendering loss.\nFinally, we also supervise the SDF field with a direct loss \\(\\mathcal{L}_{sdf}\\) (implemented as in [1]), a depth-MSE loss \\(\\mathcal{L}_{depth}\\) between the depth renders and the ground truth, and with a binary cross-entropy \\(\\mathcal{L}_{mask}\\) between the alpha-mask renders and the ground-truth masks. Refer to App. A.6.2 for more details.\nLightPlane implementation. We base MetaILRM on LightplaneLRM [6], a variant of LRM [29] exploiting memory and compute-efficient Lightplane splatting and rendering kernels, offering better quality reconstructions. However, since LightplaneLRM uses density fields, which are suboptimal for mesh conversion [90, 64, 1], we extend the Lightplane renderering GPU kernel with a VolSDF [107] renderer using Eq. (2). Additionally, we also fuse into the kernel the direct SDF loss \\(\\mathcal{L}_{sdf}\\) since a naive autograd implementation is too memory-heavy."}, {"title": "Mesh extraction and texture refiner", "content": "The MetaILRM module of Sec. 3.2 outputs a sign distance function \\(s\\), implicitly defining the object surface \\(\\Lambda = \\{x \\in \\mathbb{R}^3 | s(x) = 0\\}\\) as a level set of \\(s\\). We use the Marching Tetrahedra algorithm [18] to trace the level set and output a mesh \\(\\mathcal{M} \\approx \\Lambda\\). Then, xAtlas [111] extracts a UV map \\(\\phi : [0, V]^2 \\rightarrow \\mathcal{M}\\), mapping each 2D UV-space point \\(v = \\phi(x)\\) to a point \\(x \\in \\mathcal{M}\\) on the mesh.\nNext, the goal is to extract a high-quality 5-channel PBR texture image \\(K \\in \\mathbb{R}^{V \\times V \\times 5}\\) capturing the albedo, metalness, and roughness of each mesh point. The texture image \\(K\\) can be defined directly by sampling the predicted PBR field \\(k\\) as \\(K(v) \\leftarrow k(\\phi(v))\\), but this often yields blurry results due to the limited resolution of MetaILRM. Instead, we design a texture refiner module which takes as input the coarse PBR-sampled texture image as well as the N views representing the object and outputs a much sharper texture \\(K\\). In essence, this modules leverages the information from the different views to refine the coarse texture image. The right part of Fig. 2 illustrates this module.\nMore specifically, it relies on a network \\(\\Phi\\) which is fed \\(N + 1\\) texture images \\(\\{K_i\\}_{i=0}^N\\). First, each pixel \\(v \\in [0, V]^2\\) of \\(K_0 \\in \\mathbb{R}^{V \\times V \\times 11}\\) is annotated with the concatenation of the normal, the 3D location, and the output of MetaILRM's PBR field \\(k(\\phi(v))\\) evaluated at \\(v\\)'s 3D point \\(\\phi(v)\\). The remaining \\(K_1, ..., K_N\\) correspond to partial texture images with 6 channels (for the base and shaded colors) which are obtained by backprojecting the object views to the mesh surface. The network \\(\\Phi\\) utilises two U-Nets to fuse \\(\\{K_i\\}_{i=0}^N\\) into the enhanced texture \\(K\\). \\(\\Phi\\)'s goal is to select, for each UV point \\(v\\), which of the N input views provides the best information. Specifically, each partial texture image \\(K_i\\) is processed in parallel by a first U-Net, and the resulting information is communicated via cross attention to a second U-Net whose goal is to refine \\(K_0\\) into the enhanced texture \\(K\\). Please refer to App. A.7 for further details.\nSuch a network is trained on the same dataset and supervised with the PBR and albedo rendering losses as MetaILRM. The only difference is meshes (whose geometry is fixed) are rendered differentiably using PyTorch3D's [69] mesh rasterizer instead of the Lightplane SDF renderer."}, {"title": "Experiments", "content": "Our training data consists of 140,000 meshes of diverse semantic categories created by 3D artists. For each asset, we render 36 views at random elevations within the range of [-30\u00b0, 50\u00b0] at uniform intervals of 30\u00b0 around the object, lit with a randomly selected environment map. We render the shaded images, albedo, metalness, roughness, depth maps, and foreground masks from each viewpoint. The text-to-image stage is based on an internal text-to-image model architecturally similar to Emu [16], fine-tuned on a subset of 10,000 high-quality 3D samples, captioned by a Cap3D-like pipeline [51] that uses Llama3 [86]. The other stage utilizes the entire 3D dataset instead.\nFor evaluation, following [103, 101, 40], we assess visual quality using PSNR and LPIPS [116] between the rendered and ground-truth images. PSNR is computed in the foreground region to avoid metric inflation due to the empty background. Geometric quality is measured by the L1 error between the rendered and ground-truth depth maps (of the foreground pixels), as well as the IoU of the object silhouette. We further report Chamfer Distance (CD) and Normal Correctness (NC) for 20,000"}, {"title": "Sparse-view reconstruction", "content": "We tackle the sparse-view reconstruction task of predicting a 3D mesh from 4 posed images of an object on a subset of 332 meshes from Google Scanned Objects (GSO) [19]. We compare against state-of-the-art Instant3D-LRM [40], GRM [103], InstantMesh [101], and MeshLRM [95]. We also include LightplaneLRM [6], an improved version of Instant3D-LRM, which serves as our base model. MeshLRM [95] has not been open-sourced so we compare only qualitatively to meshes from their webpage. All methods are evaluated using the same input views at 5122 resolution. Since none of the latter predict PBR materials and since GSO lacks ground-truth PBR materials, for fairness, we use a variant of our model that predicts shaded object textures.\nAs shown in Figs. 4 and 9 and Tab. 3, our method outperforms all baselines across all metrics. GRM captures texture detail well but struggles with fine geometric structures when meshified. InstantMesh and LightplaneLRM improve geometry but fall short on finer details and texture quality. Our approach excels in reconstructing shapes with detailed geometry and high-fidelity textures.\nAblations in Tab. 3 and Fig. 4 show that incorporating our scalable SDF-based rendering and direct SDF loss into the base LightplaneLRM model enhances geometric quality. Adding texture refinement further brings fine texture details.\nNext, we consider the task of sparse-view reconstruction with PBR materials, where the goal is to reconstruct the 3D geometry and texture properties (albedo, metalness, and roughness) from four posed shaded 2D views of an object. This is done on an internal dataset of 256 artist-created 3D meshes, curated for high-quality materials. Since there are no existing few-view feed-forward PBR reconstructors, we conduct an ablation study in Tab. 1 and Figs. 3 and 12.\nWhile adding material prediction with additional MLP heads provides some improvements, we observe that incorporating the deferred shading loss and texture refinement is essential for high-quality PBR decomposition. Example PBR predictions are shown in Fig. 8."}, {"title": "Text-to-3D generation", "content": "Finally, we evaluate text-to-3D with PBR materials. We compare against state-of-the-art feed-forward methods that generate assets at comparable speed (\u2248 10 to 30 sec per asset). This includes text-to-3D variants of GRM [103], InstantMesh [101], and LightplaneLRM [6]. GRM uses Instant3D's 4-view grid generator, InstantMesh receives the first view from our 2D diffusion model and subsequently generates 6 views, while LightplaneLRM accepts 4 views from our grid generator. Since these"}, {"title": "Conclusions", "content": "We have introduced Meta 3D AssetGen, a significant advancement in sparse-view reconstruction and text-to-3D. Meta 3D AssetGen can generate 3D meshes with high-quality textures and PBR materials faithful to the input text. This uses several key innovations: generating multi-view grids with both shaded and albedo channels, introducing a new reconstruction network that predicts PBR materials from this information, using deferred shading to train this network, improving geometry via a new scalable SDF-based renderer and SDF loss, and introducing a new texture refinement network. Comprehensive evaluations and ablations demonstrate the effectiveness of these design choices and state-of-the-art performance."}, {"title": "Societal Impact", "content": "Safeguards should be implemented to prevent abuse, such as filtering input text prompts and detecting unsafe content in generated 3D models. Additionally, our generation process may be vulnerable to biases present in the data and 3D models it relies on, potentially perpetuating these biases in the generated content. Despite these risks, our method can augment the work of artists and creative professionals by serving as a complementary tool to boost productivity. It also holds the potential to democratize 3D content creation, making it accessible to those without specialized knowledge or expensive proprietary software."}, {"title": "Limitations", "content": "Meta 3D AssetGen significantly advances shape generation but faces several limitations. Despite the fine-tuning of the multiview image grid generator for view consistency, it is not guaranteed, potentially impacting 3D reconstruction quality. Since we use an SDF as an underlying representation, the reconstructor may incorrectly model translucent objects or thin structures like hair or fur. Additionally, while our scalable Triton [65] implementation supports a triplane representation at a resolution of 128 \u00d7 128, this representation is inefficient, as much of its capacity is used for empty regions. Future work could explore scalable representations such as octrees, sparse voxel grids, and hash-based methods, which may remove the need for a separate texture enhancement model. We also only predict albedo, metalness and roughness, and not emissivity or ambient occlusions. Finally, our method has only been tested on object-level reconstructions, leaving scene-scale 3D generation for future research."}, {"title": "Additional qualitative comparisons", "content": "This section describes additional qualitative comparisons that, due to limited space, could not be included in the main paper. Firstly, please refer to the video attached in the supplementary material which provides a holistic presentation of Meta 3D AssetGen's qualitative results. In Fig. 7, we highlight the contributions of MetaILRM in geometry, texture and material reconstruction. In Fig. 11, we visualize the control of materials provided by Meta 3D AssetGen, i.e., metalness and roughness, by changing the text prompt for the same concept. Fig. 8 visualizes the renders of the material maps extracted with MetaILRM given four input test views. In Fig. 9, we provide a more extensive qualitative comparison to MeshLRM, the strongest few-view reconstruction baseline. Finally, Fig. 6 provides a gallery of text-conditioned generations depicting Blender-shaded renders together with the rendered PBR maps."}, {"title": "User-study details", "content": "As described in Sec. 4.2, we conducted an user study on 404 meshes generated using the DreamFusion [66] prompt-set on a standard crowdsourcing marketplace. In the study, users were shown 360\u00b0"}, {"title": "Additional text-to-3D comparisons", "content": "While Tab. 2"}]}]}