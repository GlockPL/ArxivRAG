{"title": "SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection", "authors": ["Tamara R. Lenhard", "Andreas Weinmann", "Kai Franke", "Tobias Koch"], "abstract": "Developing robust drone detection systems is often constrained by the limited availability of large-scale annotated training data and the high costs associated with real-world data collection. However, leveraging synthetic data generated via game engine-based simulations provides a promising and cost-effective solution to overcome this issue. Therefore, we present SynDroneVision, a synthetic dataset specifically designed for RGB-based drone detection in surveillance applications. Featuring diverse backgrounds, lighting conditions, and drone models, SynDroneVision offers a comprehensive training foundation for deep learning algorithms. To evaluate the dataset's effectiveness, we perform a comparative analysis across a selection of recent YOLO detection models. Our findings demonstrate that SynDroneVision is a valuable resource for real-world data enrichment, achieving notable enhancements in model performance and robustness, while significantly reducing the time and costs of real-world data acquisition. SynDroneVision will be publicly released upon paper acceptance.", "sections": [{"title": "1. Introduction", "content": "Unmanned aerial vehicles (UAVs), commonly known as drones, have become integral to a variety of sectors, including agriculture, logistics, surveillance, and recreation. However, their rapid proliferation introduces new challenges, particularly in terms of security and privacy protection [14]. Therefore, the implementation of effective drone detection systems is crucial to mitigate the risks associated with unauthorized or malicious drone activities. Combining optical sensors, specifically cameras, with advanced deep learning (DL) techniques represents a highly promising and economically efficient detection strategy [18]. Nevertheless, the effectiveness of DL models is heavily reliant on extensive and diverse training data [44, 52].\nIn practical applications, the acquisition of substantial amounts of annotated real-world data is both time-consuming and resource-intensive [28, 45]. Additional constraints, such as non-fly zones and adverse weather conditions, further complicate the data collection process. Leveraging synthetic data presents a viable alternative to circumvent environmental limitations and significantly reduce acquisition costs [16, 44] (not only in drone detection, but also in other domains [4, 29, 34, 47]). In particular, the application of game engine-based data generation techniques enables the efficient and physically precise simulation of diverse real-world conditions [7, 16, 30, 35]. It facilitates the seamless interchange of environmental configurations (e.g., from urban landscapes to rural terrains), offering the potential for comprehensive coverage of diverse scenarios, including those inadequately represented by real-world data. Furthermore, the ability to rapidly alter illumination, time of day, and weather conditions \u2013 from clear summer days to overcast skies within seconds \u2013 provides a time-efficient solution without compromising data diversity. A broad selection of interchangeable drone models, materials, and textures supports a high variability in drone appearances, in contrast to the often limited drone selection in practical applications [2, 55]. Moreover, a key advantage of synthetic data is the automated generation of pixel-precise annotations [31]. This capability accelerates both training and validation processes, enabling more rapid experimentation and iteration cycles. Furthermore, it significantly decreases resource requirements associated with traditional data collection methods [28], particularly in terms of annotation costs and recording time.\nDespite the substantial benefits of synthetic data, there is still a gap between simulated scenarios and real-world conditions [6, 35]. This discrepancy can negatively impact detection quality, especially when transferring drone detection"}, {"title": "2. Related Work", "content": "In the following sections, we briefly summarize recent advances in image-based drone detection and assess the characteristics of publicly available drone detection datasets, emphasizing their similarities and differences."}, {"title": "2.1. Drone Detection", "content": "State-of-the-art techniques for RGB-based drone detection primarily leverage single-stage DL algorithms, which offer an optimal balance between real-time performance and precision. A majority of methodologies employ variants of the You Only Look Once (YOLO) models, including YOLOv3, YOLOv5, and YOLOv8, either in their original configurations [6, 38] or with custom modifications [24, 31, 33]. Architectural innovations typically seek to resolve particular challenges encountered in drone detection, including the identification of small drones [24, 28, 32, 33], the differentiation between drones and other aerial entities (e.g., birds) [11, 33], and the mitigation of camouflage effects [31]. Transformer-based approaches offer an effective, albeit less frequently employed, alternative to traditional detection techniques [27].\nTraining drone detection models predominantly relies on (self-collected) application-specific real-world data. However, significant efforts are also directed towards the creation and utilization of synthetic data (e.g., see [6, 35]). Despite variations in generation techniques, prevailing research highlights the substantial potential of synthetic data, particularly in combination with real-world data. Prevalent training strategies for improving detection quality by integrating real and synthetic data include mixed-data training [9, 46] and fine-tuning models, initially trained on synthetic data, with real-world data [6, 35]. However, the optimal ratio of synthetic to real-world data is a controversial topic of ongoing research [17]."}, {"title": "2.2. Datasets", "content": "Publicly available datasets for drone detection can be divided into two primary categories. The first category includes datasets exclusively designed for detection tasks [2, 5, 6], typically featuring individual images. The second category comprises datasets that support both detection and tracking [9, 11, 51, 54], generally including sequential data or specialized subsets featuring both image and video files. Except for the dataset by Barisic et al. [6], the majority of datasets consists of real-world data sourced from Google images [2], YouTube videos [2], or self-recorded footage [9, 51, 53] using static, moving, handheld, or drone-mounted devices. Beyond the variability in data origins and collection techniques, available datasets exhibit further variations in the following attributes:\nDataset Size - The costly nature of real-world data acquisition leads to significant discrepancies in the sizes of publicly available datasets. For instance, the datasets UAV-Eagle [5] and Malicious Drones [27] are relatively small, with 510 and 776 images, respectively (see Table 1). Most datasets comprise 4,000 [2] to 40,232 images [41]. Exceptions include the Halmstad Data [45], featuring 203,328 annotated frames (IR + RGB), and the Drone-vs-Bird Detection Challenge dataset [11], with 85,904 annotated frames. (Notably, the Drone-vs-Bird Detection Challenge dataset [11] constitutes a comprehensive compilation of data, collected over time and continually enhanced by input from various contributors.)\nImage Resolution Publicly available drone detection datasets encompass a wide spectrum of resolutions, ranging from low-resolution (e.g., 224\u00d7224 pixels [27] and 608\u00d7608 pixels [6]) to high-resolution images (e.g., 5616\u00d73744 pixels [54]). The variability in resolution is observed both across different datasets and within individual datasets. While some datasets maintain a uniform resolution [1, 5, 6, 9, 27, 45, 51, 55], others offer a diverse range of resolutions [2, 11, 54]. For instance, the DUT Anti-UAV dataset provided by"}, {"title": "3. SynDroneVision Dataset", "content": "This section provides a comprehensive overview of the proposed SynDroneVision dataset, detailing the employed generation technique, simulation parameter variations, its composition, and inherent characteristic properties."}, {"title": "3.1. Data Generation Process", "content": "The synthetic RGB data of the proposed dataset is generated using an advanced iteration of the data generation pipeline introduced by Dieter et al. [16]. Unlike Dieter et al.'s pipeline reliant on Microsoft AirSim [36] and Unreal Engine 4.25 [22] the employed pipeline leverages Colosseum [10] (the successor to Microsoft AirSim) and Unreal Engine 5.0. Unreal Engine 5.0 introduces advanced capabilities for rendering dynamic global illumination and reflections through the implementation of the fully dynamic global illumination and reflections system Lumen [19]. This advancement significantly enhances the realism of depicted scenes (especially in terms of daytime-dependent light and shadow variations), thereby elevating the fidelity of synthetic RGB data. To ensure precise representation of Lumen-based lighting effects and reflections, we refine the data generation process by modifying the pipeline's data generation module (cf. [16]). In Dieter et al.'s pipeline, visual sensor data was acquired through Scene Capture 2D Actors. However, when integrated with Unreal Engine 5.0, these components lack the ability to accurately capture the intricate details of Lumen's dynamic global illumination and reflections. To address this limitation, we implement the capture of RGB data via high resolution screenshots. All other pipeline components remain consistent with [16]. For detailed information on individual components, refer to [16].\nThe data acquisition process itself is predicated on a strategic placement of stationary virtual camera sensors, whose position and orientation are pre-determined with respect to the underlying simulation environment (inspired by a typical surveillance setup, cf. Figure 1). Data collection is systematically performed from each designated camera in a sequential manner, adhering to a pre-defined recording duration. The recording duration is synchronized with the drone's flight time. The drone's flight trajectory is determined by a probabilistic selection of waypoints, randomly positioned within the camera's field of view (up to 30 meters from its vantage point)."}, {"title": "3.2. Simulation Parameter and Domain Variations", "content": "To foster a high level of diversity in the generated data, we introduce variations across multiple simulation components, including the environment, drone models, and light-"}, {"title": "3.2.1 Environments", "content": "To establish a foundation for simulating physically-realistic drone flights, we leverage a variety of three-dimensional environments, encompassing both commercially licensed and freely available options. The selection of environments is guided by the defining attributes of the real-world settings utilized in creating the DUT Anti-UAV dataset [54]. Particular emphasis is placed on incorporating environments with diverse complexity levels and substantial variations (cf. Figure 1), ensuring thorough data diversification. The"}, {"title": "3.2.2 Drone Models", "content": "To ensure a high degree of diversity in drone representation, we employ a selection of drone models from the commercially available Quadcopter Pack [37], Drone Pack [3], and Military Drone Pack [25] for synthetic data generation (see Figure 2). Our selection features a variety of widely-deployed drone models, including the DJI Phantom (Figure 2, second row, second model from the left) and the DJI Tello Ryze (Figure 2, last row, third model from the left). Each drone model is rendered with realistic textures sourced from the respective asset packages. This contrasts with the approach of Barisic et al. [6], characterized by texture randomization featuring unconventional drone textures."}, {"title": "3.2.3 Illumination", "content": "To incorporate a variety of realistic illumination conditions, we utilize the dynamic illumination and reflection system, Lumen [19], in conjunction with the Sun and Sky Actor [21] and the Post Process Volume [20] provided by Unreal Engine. The Sun and Sky Actor offers precise control over the sun's positioning based on geographic location, date, and time, while the Post Process Volume provides a comprehensive toolkit for regulating visual aesthetics and atmospheric properties (e.g., color grading, contrast, or bloom). By leveraging the dynamic nature of directional lights within the Sun and Sky Actor, lightmap baking becomes obsolete, thus eliminating the need for pre-computed lighting. Consequently, this configuration enables the creation of authentic renderings, accurately portraying the interplay of sunlight and shadow.\nDuring the data generation process, we systematically introduce variations in the intensity of the Directional Light Actor and the Rayleigh scattering properties of the Sky Atmosphere - both fundamental components of the Sun and Sky Actor. Additionally, adjustments are made to the color temperature parameter within the Post Process Volume to further refine atmospheric properties. Thus, SynDroneVision offers a broad spectrum of illumination conditions, ranging from dawn (Figure 1, first row, first image) to dusk"}, {"title": "3.3. Post-Processing", "content": "To further increase the data diversity, a subset of randomly selected images undergoes post-capture blurring. Considering a synthetically generated image $I \\in \\mathbb{R}^{W \\times H \\times 3}$, where $W\\in \\mathbb{N}$ denotes the width and $H \\in \\mathbb{N}$ denotes the height, the blurring procedure is defined by the following convolution operation:\n$I'(x, y) = \\sum_{i=0}^{2m} \\sum_{j=0}^{2n} I(x + i - m, y + j - n) \\cdot K(i,j)$ (1)\nwhere $x \\in \\{0, ..., W - 1\\}$ and $y \\in \\{0, ..., H - 1\\}$. The kernel $K \\in \\mathbb{R}^{M \\times N}$ is characterized by the dimensions $M = 2m - 1$ and $N = 2n - 1$ with $m, n \\in \\mathbb{N}$. In our application, the kernel is specified as either an average kernel or a Gaussian kernel. The average kernel is given by $K(i, j) = \\frac{1}{(2m - 1)(2n - 1)}$, with indices $i \\in \\{0, ..., 2m\\}$ and $j \\in \\{0,..., 2n\\}$ determining the kernel's spatial extent. Conversely, the Gaussian kernel is described by $K(i, j) = \\frac{1}{(2\\pi\\sigma^2)} \\exp\\left(-\\frac{(i-m)^2+(j-n)^2}{2\\sigma^2}\\right)$, where $\\sigma$ represents the standard deviation of the Gaussian distribution.\nFor the creation of SynDroneVision, square kernels with dimensions M, N \u2208 {13, 15, 17, 19, 21}, M = N are employed. The choice of kernel type and size is randomized independently for each image. Note that for kernel extensions beyond image boundaries (i.e., when $x \\notin \\{0, ..., W - 1\\}$ or $y \\notin \\{0, ..., H - 1\\}$), explicit boundary conditions are imposed. Specifically, replication padding is employed to extend the image boundaries by duplicating edge pixel values."}, {"title": "3.4. Composition", "content": "For the generation of SynDroneVision, four to thirteen distinct camera positions are established per environment (cf. Table 2). This environment-specific camera placement results in the capture of 72 annotated image sequences. Each sequence is characterized by a unique combination of drone model, lighting configurations, and background composition, providing a high degree of realism and variation."}, {"title": "3.5. Characteristics", "content": "The proposed SynDroneVision dataset exhibits the following characteristic properties:\nImage Resolution \u2013 An identical, consistently high resolution of 2560\u00d71489 pixels is maintained for all image sequences across all environments.\nObject Position \u2013 The spatial distribution of objects within the image frame, depicted in Figure 3, reveals"}, {"title": "4. Analysis", "content": "This section outlines the evaluation setup and presents key findings regarding the efficiency of SynDroneVision."}, {"title": "4.1. Experimental Setup", "content": "To assess the effectiveness of SynDroneVision, we conduct a comparative analysis of various YOLO models and associated training configurations. Building upon the latest developments in YOLO architectures, our analysis focuses on YOLOv8 [49] and YOLOv9 [50], with a detailed examination of their respective variants: YOLOv8m, YOLOv81, YOLOv9c, and YOLOv9e. Considering the significance of real-world data in validating SynDroneVision's practical value, we incorporate the DUT Anti-UAV dataset [54] (selected for its characteristic resemblance to SynDroneVision). To ensure a comprehensive evaluation, especially in terms of model robustness, we also include the UAV-Eagle dataset [5] and the Drone Dataset by [2] as out-of-distribution data.\nThe training procedure for each model involves three distinct strategies: (i) training exclusively on SynDroneVision, (ii) training solely on DUT Anti-UAV, and (iii) a hybrid approach combining both datasets. Each model is trained for 100 epochs with a batch size of 64. For YOLOv9e, a reduced batch size of 32 is employed due to memory limitations. Other hyperparameters follow their default configurations specified in [48]. Model performance is evaluated using the DUT Anti-UAV test set [54], UAV-Eagle [5], and the Drone Dataset by [2]. Key performance indicators include standard object detection metrics such as mean average precision (mAP) at an intersection over union (IoU) threshold of 0.5, and computed over a range of IoU thresholds from 0.5 to 0.95. To account for precision variations in manually generated annotations (cf. [31]), we also consider mAP values at an IoU threshold of 0.25, along with false negative rates (FNRs) and false discovery rates (FDRs). All experiments are performed using the Ultralytics repository [48] on a single NVIDIA Quadro RTX-8000 GPU."}, {"title": "4.2. Results", "content": "For evaluations on the SynDroneVision test set, refer to the supplementary material. In the following, we provide an assessment of all trained models on real-world data, highlighting SynDroneVision's practical applicability."}, {"title": "5. Conclusion", "content": "In this work, we introduced SynDroneVision, a novel and comprehensive synthetic RGB dataset designed to support the development of robust drone detection systems. By providing a detailed analysis of recent YOLO models trained on both SynDroneVision and real-world data, we demonstrated the effectiveness of SynDroneVision in enhancing model accuracy and robustness."}]}