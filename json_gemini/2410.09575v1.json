{"title": "RECONSTRUCTIVE VISUAL INSTRUCTION TUNING", "authors": ["Haochen Wang", "Anlin Zheng", "Yucheng Zhao", "Tiancai Wang", "Zheng Ge", "Xiangyu Zhang", "Zhaoxiang Zhang"], "abstract": "This paper introduces reconstructive visual instruction tuning (Ross), a family of Large Multimodal Models (LMMs) that exploit vision-centric supervision signals. In contrast to conventional visual instruction tuning approaches that exclusively supervise text outputs, Ross prompts LMMs to supervise visual outputs via reconstructing input images. By doing so, it capitalizes on the inherent richness and detail present within input images themselves, which are often lost in pure text supervision. However, producing meaningful feedback from natural images is challenging due to the heavy spatial redundancy of visual signals. To address this issue, ROSS employs a denoising objective to reconstruct latent representations of input images, avoiding directly regressing exact raw RGB values. This intrinsic activation design inherently encourages LMMs to maintain image detail, thereby enhancing their fine-grained comprehension capabilities and reducing hallucinations. Empirically, Ross consistently brings significant improvements across different visual encoders and language models. In comparison with extrinsic assistance state-of-the-art alternatives that aggregate multiple visual experts, Ross delivers competitive performance with a single SigLIP visual encoder, demonstrating the efficacy of our vision-centric supervision tailored for visual outputs.", "sections": [{"title": "INTRODUCTION", "content": "The success of GPT-style Large Language Models (LLMs) (Radford et al., 2018; 2019; Brown et al., 2020; OpenAI, 2023b; Yang et al., 2024a; Touvron et al., 2023; Chiang et al., 2023; Dubey et al., 2024) has motivated researchers to adapt LLMs to understand multimodal inputs (Liu et al., 2023a; 2024a; Dai et al., 2023; Bai et al., 2023). Notably, visual instruction tuning approaches (Liu et al., 2023a) demonstrate superior performance with cost-efficient training recipes. Some approaches (Chen et al., 2024b; Li et al., 2024c) even surpass GPT-4V(ision) (OpenAI, 2023a) on benchmark evaluations.\nTypically, these Large Multimodal Models (LMMs) based on visual instruction tuning adopt a plug-in architecture, as depicted in Figure 1a, where pre-trained vision-language foundation models such as CLIP (Radford et al., 2021) are responsible for projecting images into visual tokens. They serve as prefix tokens for multimodal comprehension. However, this type of design, i.e., visual encoder \u2192 connector \u2192 LLM \u2190 language instructions, where \u201c\u21d0\u201d indicates supervision, is primarily LLM-centric: (i) visual comprehension largely depends on vision-to-text alignment and the selected vision models, and (ii) supervision derives exclusively from text data. As a result, they exhibit systematic visual shortcomings such as recognizing specific visual patterns (Tong et al., 2024b).\nUntil very recently, some concurrent works proposed vision-centric solutions (Tong et al., 2024a;b). Illustrated in Figure 1b, their solutions leverage extrinsic assistance via aggregating several different visual experts. Inspired by the evolution in image recognition, from manually designed visual features (S\u00e1nchez & Perronnin, 2011) to learnable deep convolutional models (Krizhevsky et al.,"}, {"title": "RELATED WORK", "content": "Visual Instruction Tuning. Most visual instruction tuning-based LMMs adopt a plug-in architecture (Liu et al., 2023a; 2024a; Bai et al., 2023), where a language-supervised visual encoder (Radford et al., 2021; Zhai et al., 2023) is responsible for extracting visual tokens. A connector is used to map those visual representations into the LLM space, e.g., Resamplers (Alayrac et al., 2022), Q-Formers (Li et al., 2023b; Dai et al., 2023; Bai et al., 2023; Ge et al., 2024a), and MLPs (Liu et al., 2023a; 2024a; Li et al., 2024c; Liu et al., 2024b; Li et al., 2024a). These LMMs usually follow a two-stage training recipe. During the alignment stage, the connector is trained on high-quality caption data. Next, the full model is trained on single-image visual instruction tuning data. However, only text outputs are supervised. Ross, on the other hand, introduces novel vision-centric supervision via reconstructing fine-grained visual tokens conditioned on visual outputs.\nVisual Encoders for LMMs. As the original CLIP (Radford et al., 2021) adopted by conventional visual instruction tuning approaches is trained on noisy image-text pairs, it exhibits specific visual shortcomings, and thus stronger backbones (Fang et al., 2024; Zhai et al., 2023; Chen et al., 2024c) have been introduced to LMMs. Some concurrent works (Tong et al., 2024b;a) leverage extrinsic assistance, which further utilizes vision-only self-supervised models (Oquab et al., 2023; Wang et al., 2023a;b;c; He et al., 2022; Caron et al., 2021) and domain experts (Kirillov et al., 2023; Birkl et al., 2023; Rombach et al., 2022). Ross, from a new intrinsic activation perspective, aims to catalyze enhanced comprehension through reconstructing input images with no extra visual experts.\nGenerative Objectives for LMMs. Another line of work introduces pre-trained text-to-image diffusion models (Rombach et al., 2022) to make LMMs capable of both comprehension and generation (Dong et al., 2024; Ge et al., 2024a; Sun et al., 2024b; Ge et al., 2024b; Sun et al., 2023). Our Ross, with a totally different motivation, targets to catalyze multimodal comprehension via reconstruction. Specifically, conditions are different, where Dong et al. (2024) and Sun et al. (2024b) take outputs corresponding to learnable queries as conditions, while our Ross takes outputs corresponding to visual inputs. Those methods are generative while Ross is reconstructive. The detailed pipeline comparison can be found in Appendix C."}, {"title": "PRELIMINARIES", "content": "Large Multimodal Models. In the literature (Radford et al., 2018; 2019), a \u03b8-parameterized LLM models the canonical causal distribution of each text token xi as $p_{\\theta}(x) = \\prod_{i=1}^{T} p_{\\theta}(x_i | x_{<i})$, where ${x}_{i=1}^{T}$ represents a sequence of text tokens. To make LLMs understand visual contents, typical plug-in style LMMs (Liu et al., 2023a; 2024a) regard a sequence of visual tokens as prefix tokens. Specifically, an input image $I \\in \\mathbb{R}^{H \\times W \\times 3}$ is first projected into a sequence of visual tokens by a \u03be-parameterized visual encoder $G_{\\xi}$ such as CLIP (Radford et al., 2021) and SigLIP (Zhai et al., 2023), where (H, W) indicates the spatial resolution. Then, a \u03c6-parameterized multimodal projector $H_{\\phi}$ is utilized to project these visual tokens into the feature space of LLMs. As a result, the canonical causal distribution in a multimodal sentence containing an image I becomes\n$p_{\\theta}(x) = \\prod_{i=1}^{T} p_{\\theta}(x_i | x_{<i}, V)$, $v = H_{\\phi} \\circ G_{\\xi}(I)$,\nwhere $\u0398 = {\\theta,\u03be, \u03c6}$ is the parameters and $v \\in \\mathbb{R}^{N \\times D}$ indicates the projected visual tokens. N is the number of visual tokens and D indicates the feature channel. The visual encoder $G_{\\xi}$ could be either frozen (Liu et al., 2023a; 2024a; Tong et al., 2024a) or fine-tuned (Liu et al., 2024b; Bai et al., 2023; Li et al., 2024c; Wang et al., 2024b).\nTraining Recipes for LMMs. LMMs almost follow a two-stage training recipe (Liu et al., 2023a), i.e., the pre-training stage (or the alignment stage) and the supervised fine-tuning stage (or the instruction tuning stage). The instruction (supervision) comes from languages such as the answers to"}, {"title": "Ross: RECONSTRUCTIVE VISUAL INSTRUCTION TUNING", "content": "In this section, we first provide an overview of our reconstructive visual instruction tuning (Ross). Then, we discuss our explorations towards the optimal formulation in the following subsections, with the ultimate goal of handling spatial redundancy of visual signals to provide meaningful visual supervision. Our explorations mainly include reconstruction targets and the training objective.\nOverview. Illustrated in Figure 2, the overall philosophy of our Ross is to construct reconstructive visual supervision signals on visual outputs xi<N. The training objective includes (i) the original next-token prediction on xi>N shown in the right part of Figure 2, and (ii) another reconstructive term in the left part of Figure 2, i.e., LRoss = LUMM + Lisual. Specifically, this visual term could be any custom measurements M between xi<N and specific reconstruction targets of image I:\n$L_{Ross}^{visual}(\\Theta = {\\theta, \\xi, \\phi, \\pi}, x, I) = M(I_{\\pi}(x_{i<N}), F(I))$,\nwhere $I_{\\pi}$ indicates the \u03c0-parameterized post projection that maps the dimensions of visual tokens xi<N to be consistent with the teacher tokenizer F.\nVariants of Ross. Evidently, different choices of F and M contribute to different variants. F controls the reconstruction target while M defines the objective:\n1.  Towards the target, F can be the pachify operation (Dosovitskiy et al., 2021), resulting in pixel-level reconstruction, or pre-trained fine-grained visual tokenizers such as VAE (Kingma, 2013) and VQGAN (Esser et al., 2021), leading to latent-level reconstruction. F could even be vision-only models such as DINOv2 (Oquab et al., 2023), making LMMs learn specific visual patterns from F, which is also a type of latent-level reconstruction.\n2.  Towards the objective, the most straightforward choice of M is MSE or cosine similarity for regressing raw pixel values or latent features, respectively. We also explore the denoising objective (Ho et al., 2020) to avoid being overwhelmed by fitting exact values.\nWe introduce our explorations step by step in the following sections. The ultimate goal of our exploration is to design an appropriate self-supervised reconstructive pre-text task that provides meaningful vision-centric supervision signals to LMMs, where handling the spatial redundancy of visual signals (He et al., 2022) becomes the crux."}, {"title": "ROSSR: REGRESSING AS RECONSTRUCTIVE VISUAL INSTRUCTION", "content": "In this section, we introduce straightforward variants, i.e., regressing as reconstructive visual instruction. As shown in Figure 3, depending on the choice of F, it mainly has three variants: (a) RossR-Pixel, (b) RossR-Latent, and (c) ROSSR-Latent2Pixel.\nDirectly Regressing Raw RGB Values. The most straightforward variant is to directly regress raw RGB values illustrated in Figure 3a, called \u201cROSSR-Pixel\u201d. Under such a setting, F is the patchify"}, {"title": "ROSSD: DENOISING AS RECONSTRUCTIVE VISUAL INSTRUCTION", "content": "As an objective for handling heavy spatial redundancy to provide meaningful vision-centric supervision signals, denoising is better than vanilla regressing, since the introduction of noise into the training data acts as an implicit form of data augmentation and regularization. The denoising process encourages the model to focus on the underlying data manifold rather than memorizing specific instance values (Chen et al., 2023c; Song & Ermon, 2019; Karras et al., 2022; Yang et al., 2024b).\nTechinically, as illustrated in Figure 4a, our final RossD takes high-level visual outputs $x_{i<N}$ as conditions to recover clean fine-grained tokens $z_{0}$ from noisy tokens $z_{t}$. Specifically, clean tokens $z_{0} = F(I)$ are obtained from the teacher tokenizer F. By default, we utilize a continuous VAE (Kingma, 2013) regularized by Kullback\u2013Leibler (KL) divergence provided by Rombach et al. (2022), since it is believed to capture sufficient image details. The training procedure of the denoiser $I_{\\pi}$ follows a diffusion process (Ho et al., 2020):\n$L_{visual} (\\Theta = {\\theta, \\xi, \\phi, \\pi}, x, I) = E_{t, \\epsilon} [||I_{\\pi}(z_t; x_{i<N}, t) - \\epsilon||^2]$.\nThe denoiser $I_{\\pi}$ actually estimates the conditional expectation $E[\\epsilon ~ N(0, I)|z_t]$. More details about the background knowledge of diffusion models can be found in Appendix A.\nArchitecture of the Denoiser. As conditions $x_{i<N}$ are causal, we introduce a self-attention module to model the inter-token dependencies illustrated in Figure 4b. Specifically, the architecture of the"}, {"title": "EXPERIMENTS", "content": "Implementation Details. All ablation studies are implemented based on LLaVA-v1.5 (Liu et al., 2024a). The visual encoder $G_{\\xi}$ is CLIP-ViT-L/14@336 (Radford et al., 2021) and the base LLM is Qwen2-7B-Instruct (Yang et al., 2024a). The training data is LLaVA-558K (Liu et al., 2023a) and Cambrian-737K (Tong et al., 2024a) for the pre-training stage and the instruction tuning stage, respectively. We evaluate our each variant of Ross mainly on (i) hallucination: POPE (Li et al., 2023c) and HallusionBench (Guan et al., 2024), (ii) fine-grained comprehension: MMVP (Tong et al., 2024b) and ChartQA (Masry et al., 2022), and (iii) general comprehension: MMBench (Liu et al., 2023b) English dev split. All evaluations are conducted with VLMEvalKit (Duan et al., 2024). Evaluation prompts can be found in Appendix B.\nPixel Regression v.s. Latent Regression. Starting from the visual instruction tuning baseline (Liu et al., 2023a; 2024a), we first explore the effectiveness of using regression as the objective for our reconstructive visual instruction tuning. We utilize a continuous VAE (Kingma, 2013) with an encoder-decoder architecture provided by Rombach et al. (2022), where the encoder part serves as F for ROSSR-Latent while the decoder part is F-1 for ROSSR-Latent2Pixel. As illustrated in Figure 5, our vision-centric regression supervision outperforms the visual instruction tuning baseline in most cases. Moreover, latent regression performs the best since regressing raw RGB pixels fails to provide meaningful supervision signals, regardless of whether utilizing a decoder or not.\nChoices of F. We study the effectiveness across different latent teacher tokenizers F in Figure 6, including KL-16 provided by Rombach et al. (2022), which is a continuous VAE (Kingma, 2013) with Kullback-Leibler (KL) divergence, self-supervised DINOv2 (Oquab et al., 2023), fully-supervised DEIT-III (Touvron et al., 2022), and language-supervised EVA02CLIP (Fang et al., 2024). Among"}, {"title": "IN-DEPTH ANALYSIS", "content": "Attention Analysis. We compute the attention scores of the last token with respect to all visual tokens on MMVP (Tong et al., 2024b). Quantitative and qualitative comparisons between the visual instruction tuning baseline (LLaVA) (Liu et al., 2024a) and our Ross are provided in Table 1 and Figure 8, respectively. Table 1 reveals that the attention scores achieved by Ross are significantly higher than those of LLaVA, indicating that the inclusion of vision-centric reconstructive objective $L_{visual}$ effectively directs focus towards input images, thereby enhancing the comprehending visual signals. Similarly, Figure 8 demonstrate that the implementation of $L_{visual}$ enables the alignment of attention closely with the relevant visual elements corresponding to the text query.\nGenerative v.s. Reconstructive. We ablate the effectiveness of reconstruction over generation in Table 2. Similar to Sun et al. (2024b) and Dong et al. (2024), for generative cases, we adopt 576 learnable latent tokens to query the LMM and utilize the corresponding outputs as conditions to the denoiser. The detailed pipeline of these two methods can be found at Figure 11 in Appendix C."}, {"title": "TRANSFER LEARNING ON UNDERSTANDING DEPTH MAPS", "content": "We further evaluate the transfer learning capability of our Ross on SpatialBench (Cai et al., 2024), which requires the model to understand depth maps. We compare our Ross with the visual instruction tuning baseline, with the same training data and model architecture. Also, we compare the effectiveness of the extrinsic assistance solution, i.e., combining a depth expert MiDaS-3.0 (Birkl et al., 2023) to visual instruction tuning, with our intrinsic activation solution. Specifically, the pre-training data is LLaVA-558K (Liu et al., 2023a) and the fine-tuning data is SpatialQA-853K (Cai et al., 2024), where each conversation contains the RGB image and the depth maps extracted by ZoeDepth (Bhat et al., 2023). The visual encoder is CLIP-ViT-L/14@336 (Radford et al., 2021) and the base LLM is Qwen2-7B-Instruct (Yang et al., 2024a).\nAs demonstrated in Table 5, our Ross manages to make use of the extra depth map, as consistent and significant improvements are observed when taking \"RGB + D\" inputs for testing. Extrinsic assistance approaches cannot take advantage of extra depth maps when testing. Even GPT-40 cannot fully understand depth maps. Figure 9 provides qualitative results. The extra depth understanding visual expert, i.e., MiDaS (Birkl et al., 2023), fails to help LMMs understand depth maps both quantitatively in Table 5 and qualitatively in Figure 9."}, {"title": "CONCLUSION", "content": "This paper introduces reconstructive visual instruction tuning (Ross). Different from conventional LMMs that solely utilize text supervision, Ross leverages a vision-centric reconstructive objective to supervise visual outputs. To avoid being overwhelmed by heavily redundant raw RGB values, we train a denoiser to recover clean latent visual representations conditioning on visual outputs. Experimentally, the proposed objective indeed brings enhanced comprehension capabilities and reduced hallucinations. Ross outperforms the state-of-the-art under most cases with only a single SigLIP (Zhai et al., 2023) as the visual encoder. In-depth analysis demonstrates the effectiveness of Ross for directing focus towards visual elements and understanding depth maps.\nDiscussion. One limitation is that Ross does not have generation capabilities, since Ross is designed for enhanced multimodal comprehension, without the need to generate photorealistic aesthetic images. Furthermore, the gap in training data between comprehension and generation methods also matters. For instance, PixArt-a (Chen et al., 2023a), which is one of the most efficient text-to-image models, was trained on nearly 400M images to model the pixel discrepancy just in the first training stage. By contrast, our Ross is only trained on nearly 3M images for one epoch. Future topics include achieving photorealistic text-to-image generation via incorporating more training samples."}]}