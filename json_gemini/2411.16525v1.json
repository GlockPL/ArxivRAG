{"title": "Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency", "authors": ["Jerry Yao-Chieh Hu", "Wei-Po Wang", "Ammar Gilani", "Chenyang Li", "Zhao Song", "Han Liu"], "abstract": "We investigate the statistical and computational limits of prompt tuning for transformer-based foundation models. Our key contributions are prompt tuning on single-head transformers with only a single self-attention layer: (i) is universal, and (ii) supports efficient (even nearly-linear time) algorithms under the Strong Exponential Time Hypothesis (SETH). Statistically, we prove that prompt tuning on such simplest possible transformers are universal approximators for sequence-to-sequence Lipschitz functions. In addition, we provide an exponential-in-dL and -in-(1/\u20ac) lower bound on the required soft-prompt tokens for prompt tuning to memorize any dataset with 1-layer, 1-head transformers. Computationally, we identify a phase transition in the efficiency of prompt tuning, determined by the norm of the soft-prompt-induced keys and queries, and provide an upper bound criterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for prompt tuning exists under SETH. Within this criterion, we showcase our theory by proving the existence of almost-linear time prompt tuning inference algorithms. These fundamental limits provide important necessary conditions for designing expressive and efficient prompt tuning methods for practitioners.", "sections": [{"title": "1 Introduction", "content": "We investigate the statistical and computational limits of prompt tuning for transformer-based foundation models. These models are gigantic transformer-based architectures [Bommasani et al., 2021], pretrained on vast datasets, are pivotal across multiple fields [Touvron et al., 2023b,a, Brown et al., 2020, Floridi and Chiriatti, 2020, Yang et al., 2023, Wu et al., 2023, Nguyen et al., 2024, Zhou et al., 2024, 2023, Ji et al., 2021, Thirunavukarasu et al., 2023, Singhal et al., 2023, Moor et al., 2023]. Despite their power, the significant cost of pretraining these models often makes them prohibitive outside certain industrial labs. Thus, most practitioners resort to fine-tuning methods to tailor these models to specific needs [Zheng et al., 2024, Ding et al., 2022]. However, fine-tuning large models with billions or trillions of parameters is still often resource-intensive [Minaee et al., 2024]. Prompt tuning mitigates this by adapting a learnable prompt with a limited set of parameters (tokens), preserving the pretrained model weights and allowing adaptation to new tasks or data without any retraining [Lester et al., 2021, Liu et al., 2021]. It saves substantial computational resources and time. However, despite its empirical successes [Gao et al., 2024, Shi and Lipani, 2024, Fu et al., 2024, Chen et al., 2023, Wang et al., 2023b, Khattak et al., 2023, Jia et al., 2022, Liu et al., 2022, 2021], the theoretical aspects of prompt tuning are still underexplored, relatively [Wang et al., 2023a, Petrov et al., 2024]. This work provides a timely theoretical analysis of the statistical and computational limits of prompt tuning, aiming to explain its successes and offer principled guidance for future prompt tuning methods in terms of performance and computational cost.\nLet X, Y \u2208 Rd\u00d7L be the input and the corresponding label sequences, respectively. For i \u2208 [L], we denote X,i \u2208 Rd as the i-th token (column) of X. Let [\u00b7, \u00b7] denote sequential concatenation.\nDefinition 1.1 (Prompt Tuning). Let 7 be a pretrained transformer. Let P \u2208 Rd\u00d7Lp be a length-Lp prompt weight (termed soft-prompt) prepended to input prompt X such that Xp := [P, X] \u2208"}, {"title": "2 Statistical Limits of Prompt Tuning Transformers: Universality and Memory Capacity", "content": "To better understand the expressive power of prompt tuning, we explore its universality (Sections 2.3 and 2.4) and memory capacity (Section 2.5) on a transformer of simplest configurations.\nOverview of Our Results. Let Th,s,r denote transformers with h heads, s hidden size, and r MLP neurons, and let e represent the approximation error tolerance. Let X \u2208 Rd\u00d7L and P \u2208 RdxLp be the input and soft-prompt defined in Definition 1.1, respectively. We answer Question 1 affirmatively, and present three results for transformer models with 1-head, 1-layer attention layers:\nLemma 2.1 (1-Head, 1-Layer Attention with Any-Rank Weight Matrices Is Contextual Mapping, Informal Version of Lemma 2.2). A 1-head, 1-layer attention mechanism with weight matrices WK, WQ, Wv of any rank is able to associate each input sequence with a unique label sequence.\nTheorem 2.1 (Universality of Prompt Tuning T1,1,4 Transformers with O(e-d(Lp+L)) FFN Layers, Informal Version of Theorem 2.3). Prompt tuning transformers with 1 head, a hidden size of 1, and O(e-d(Lp+L)) FFN layers of width 4 are universal approximators for Lipschitz seq-to-seq functions.\nTheorem 2.2 (Universality of Prompt Tuning T1,1,r=O(e-d(Lp+L)) Transformers with 2 FFN Layers\nInformal Version of Theorem 2.4). Prompt tuning transformers with 1 head, a hidden size of 1, and 2 FFN layers of width O(e-d(Lp+L)) are universal approximators for Lipschitz seq-to-seq functions.\nComparing with Prior Works. Our results improve previous works in three aspects:\n\u2022 Any Weight Matrices. While [Kajitsuka and Sato, 2024] show that a self-attention layer with rank-1 weight matrices serves is a contextual map, we improve this to weight matrices of any rank.\n\u2022 Transformers with 1-Head, 1-Layer Attention. While [Wang et al., 2023a] shows that prompt tuning on transformers of O((Lp + L)(1/6)d) attention layers with at least 2 attention heads, we achieve the universality of prompt tuning transformers with only single-head-single-layer attention.\n\u2022 Only 2 FFN Layers. We identify a width-depth tradeoff of universality. While [Wang et al., 2023a] achieves prompt tuning universality with transformers of O((1/e)d(Lp+L)) FFN layers,"}, {"title": "2.1 Preliminaries and Problem Setup", "content": "We first present the ideas we build on.\nLet Z \u2208 Rd\u00d7L denote the input embeddings of attention layer and s denote the hidden dimension.\nTransformer Block. Let h-head self-attention layer as a function f(SA) : Rd\u00d7L \u2192 Rd\u00d7L,\n```latex\nf^{(SA)}(Z) = Z + \\sum_{i=1}^{h} W_o f^{(Att)}_i(Z, Z) \\in \\mathbb{R}^{d \\times L},\n```\nwhere Wo \u2208 Rdxs and f(Att) (Att) is the size-s self-attention mechanism for the i-th head\n```latex\nf_i^{(Att)}(Z_{:,k}, Z) = (W_i Q Z)\\text{Softmax} [(W_i K Z)^\\top (W_i Q Z_{:,k})] \\in \\mathbb{R}^{s}.\n```\nHere, f(Att): : Rd\u00d7Rd\u00d7L \u2192 Rs acts token-wise, and Wi, WK, WQ \u2208 Rsxd are the weight matrices. Next, we define the r-neuron feed-forward layer function as f(FF) \u2208 F(FF) : ]Rd\u00d7L \u2192 Rd\u00d7L and the output at k-th token is\n```latex\nf^{(FF)}(Z)_{:,k} = Z_{i,k} + W^{(2)}\\text{ReLU}(W^{(1)} Z_{:,k} + b^{(1)}) + b^{(2)},\n```\nwhere W(1) \u2208 Rr\u00d7d and W(2) \u2208 Rdxr are weight matrices, and b(1), b(2) \u2208 R are the bias terms.\nDefinition 2.1 (Transformer Block). We define a transformer block of h-head, s-size and r-neuron as f(Th,s,r) (Z) = f(FF) (f(SA) (Z)) : Rd\u00d7L \u2192 Rd\u00d7L.\nNow, we define the transformer networks as compositions of transformer blocks.\nDefinition 2.2 (Transformer Network Function Class). Let Th,s,r denote the transformer network function class where each function \u0442\u0435\u2208 Th,s,r consists of transformer blocks f(Th,s,r) with h heads of size s and r MLP hidden neurons: Th,s,r := {T : Rd\u00d7L \u2192 RdxL | T = f(Th,s,r) (f(Th,s,r) (...))}.\nPrompt Tuning Pretrained Transformer Models. In this work, we consider the prompt tuning problem Definition 1.1 with a pretrained transformer network T\u2208 Th,s,r.\nProblem Setup. To answer Question 1, we focus on the universal approximation of prompt tuning pretrained transformer models. We start from stating the target functions of our approximation.\nDefinition 2.3 (Target Function Class). Let Fc be the C-Lipschitz (under p-norm) target function class of continuous sequence-to-sequence. Let fseq2seq \u2208 Fc : [0,1]d\u00d7L \u2192 [0,1]d\u00d7L denote continuous sequence-to-sequence functions on a compact set of sequence.\nExplicitly, for any fseq2seq \u2208 Fc and two input sequences Z, Z' \u2208 Rd\u00d7L, we have || fseq2seq (Z) - fseq2seq (Z') || \u2264 C||Z - Z'||. In this work, we adopt fseq2seq as our approximation target function. Concretely, we investigate whether it is possible to approximate any C-Lipschitz sequence-to-sequence function fseq2seq through prompt tuning with a pretrained single-head, single-layer transformer model. Namely, we reformulate Question 1 into the following problem."}, {"title": "2.2 Any-Rank Single-Layer Attention is a Contextual Mapping Function", "content": "As stated in the previous technical overview, a key element of our proof is the concept of con-textual mapping in attention [Kajitsuka and Sato, 2024, Yun et al., 2020]. Contextual mapping enables transformers to move beyond simple token-wise manipulation and capture the full context of a sequence. Through this, identical tokens within different input sequences become distinguish-able. In this subsection, we present new results on the contextual mapping property of attention. These results allow us to use feed-forward neural networks to map each input sequence to its corresponding label sequence, thereby achieving universal approximation in Section 2.3.\nBackground: Contextual Mapping. Let Z, Y \u2208 RdxL be the input embeddings and output label sequences, respectively. Let Z:,i \u2208 Rd be the i-th token (column) of each Z embedding sequence.\nDefinition 2.4 (Vocabulary). We define the i-th vocabulary set for i \u2208 [N] by V(i) = Uk\u2208[L] ZC Rd, and the whole vocabulary set V is defined by V = Ui\u2208[N] V(i) CRd.\nNote that while \u201cvocabulary\u201d typically refers to the tokens' codomain, here it refers to the set of all tokens within a single sequence. To facilitate our analysis, we introduce the idea of input token separation following [Kajitsuka and Sato, 2024, Kim et al., 2022, Yun et al., 2020].\nDefinition 2.5 (Tokenwise Separateness). Let Z(1), ..., Z(N) \u2208 Rd\u00d7L be embeddings. Then, Z(1), ..., Z(N) are called tokenwise (min, max, 8)-separated if the following conditions hold.\n(i) For any i \u2208 [N] and k \u2208 [L], ||Z|| > Ymin holds.\n(ii) For any i \u2208 [N] and k \u2208 [L], ||Z|| <\n(iii) For any i, j \u2208 [N] and k,l \u2208 [L] if Z \u2260Z, then || - > \u03b4 holds.\nNote that when only conditions (ii) and (iii) hold, we denote this as (\u03b3, \u03b4)-separateness. Moreover, if only condition (iii) holds, we denote it as (d)-separateness.\nTo clarify condition (iii), we consider cases where there are repeated tokens between different input sequences. Next, we define contextual mapping. Contextual mapping describes a function's ability to capture the context of each input sequence as a whole and assign a unique ID to each input sequence.\nDefinition 2.6 (Contextual Mapping). A function q : Rd\u00d7L \u2192 Rd\u00d7L is said to be a (\u03b3, \u03b4)-contextual mapping for a set of embeddings Z(1), ..., Z(N) \u2208 Rd\u00d7L if the following conditions hold:"}, {"title": "2.3 Universality of Prompt Tuning T1,1,4 with O((1/6)d(Lp+L)) FFN Layers", "content": "In this section, we prove the universality of prompt tuning by showing that there exists a simple transformer of single-layer self-attention T\u2208 T1,1,4 such that for any fseq2seq \u2208 Fc, prompt tuning on approximates this function up to some error \u20ac > 0. Consider simple transformers TE T1,1,4 consisting of a single-head, single-layer, size-one self-attention function f(SA) \u2208 F(SA), and O((1/\u20ac)d(Lp+L)) feed-forward layers f(FF) \u2208 F(FF), each with 4 MLP hidden neurons:\n```latex\n\\mathcal{T}^{1,1,4} := \\{\\tau : \\mathbb{R}^{d\\times L} \\rightarrow \\mathbb{R}^{d\\times L} | \\tau =  f^{(FF)} \\circ ... \\circ f^{(FF)} \\circ f^{(SA)} \\circ f^{(FF)} \\circ ... \\circ f^{(FF)}\\}.\n```\nProof Strategy. We employs a chained reduction of piece-wise constant approximations:\n(A1) We start by quantizing the input and output domain of fseq2seq \u2208 Fc into a quantized function fseq2seq: G8,L \u2192 G8,L where G8,L = {0, \u03b4, 28, . . ., 1 \u2013 8}dxL. Here, f seq2seq, Fc denote the quantized function and function class. This is basically performing a piece-wise constant approximation with bounded error \u03b4.\n(A2) Next, we construct a surrogate quantized sequence-to-sequence function\nhseq2seq: G8,(Lp+L) \u2192 G8,(Lp+L), where G8,(Lp+L) = {0, \u03b4, 2\u03b4, ..., 1 \u2013 8}dx(Lp+L).\nHere hseq2seq takes prompts and embeddings Zp = [P, Z] as inputs. Crucially, its Lp-imputed output approximates any f seq2seq \u2208 Fc by using various soft prompts P.\n(A3) Finally, we show that there exist transformers \u03c4\u2208 T1,1,4 approximating hseq2seq to any pre-cision. By simple reduction from hseq2seq, fseq2seq and fseq2seq, we achieve the universality of prompt tuning on TA' T1,1,4 with O((1/e)d(Lp+L)) FFN layers, where e is the approximation error.\nRemark 2.2. We remark that while (A1) shares some similarity with [Wang et al., 2023a] by the nature of quantization approach to transformer's universality [Yun et al., 2020], (A2) and (A3) differs significantly in techniques and results. See the opening of this section for an overview.\nFor (A1) and (A2), we introduce the next lemma, showing the quantized f seq2seq is approximated by Lp-imputed version of some quantized sequence-to-sequence function\nhseq2seq: G8,(Lp+L) \u2192 G8,(Lp+L), where G8,(Lp+L) = {0, \u03b4, 2\u03b4, . . ., 1 \u2013 8}dx(Lp+L).\nLemma 2.3 (Universality of Prompt Tuning Surrogate Function hseq2seq). Consider a C-Lipschitz sequence-to-sequence function class Fc, where each function fseq2seq: [0,1]d\u00d7L \u2192 [0,1]d\u00d7L. There exists a sequence-to-sequence function hseq2seq: G8, (Lp+L) \u2192 G8, (Lp+L) with G8,(Lp+L) = {0, \u03b4, 2\u03b4, ..., 1 - 8}dx(Lp+L) such that, for any fseq2seq \u2208 Fc, we can find a prompt P\u2208 RdxLp"}, {"title": "2.4 Width-Depth Tradeoff: Universality of Prompt Tuning T1,1,r=0((1/e)d(Lp+L)) Only Needs 2 FFN Layers", "content": "In Section 2.3, we achieve the universality of prompt tuning simple transformers with many FFN layers. In this section, we explore the possibility of further simplify such transformer block by reducing the number of FFN layers. Surprisingly, we show that 2 FFN layers are enough.\nWe start with the required number of FFN layers for \u03a4\u0395 T1,1,4 transformers to achieve univer-sality through prompt tuning. For clarity, we denote transformer of 4 MLP neurons by T\u0104 (i.\u0435., (2.4)).\nLemma 2.5. (Required Number of FFN Layers) For a transformer \u03c4\u2208 T1,1,4, defined in (2.4), to be a universal approximator through prompt tuning, it requires O((1/\u20ac)d(Lp+L)) FFN layers.\nNow, we prove the universality of prompt tuning on another simple transformer block with sig-nificantly smaller FFN depth than T1,1,4 from Section 2.3. This suggests a trade-off between the depth and width of the transformer. Let transformers \u03c4\u2208 T\u00b9 consist of a single-head, single-layer, size-one self-attention f(SA) and 2 feed-forward layers, fi and f f(FF) each with r MLP hidden neurons: T1,1,r := {T : Rd\u00d7L \u2192 RdxL | T = f of(SA) of}.\nProof Strategy. We follow a similar proof strategy as in Section 2.3. However, this section differs as we use the construction technique from [Kajitsuka and Sato, 2024] to build a transformer with single-head, single-layer, size-one self-attention, and two FFN layers. This outcome is achieved by summing multiple shifted ReLU functions to map the inputs to the desired outputs with preci-sion guarantees. Additionally, this approach allows for a reduction in the number of FFN layers by compensating with an increase in the number of neurons in the MLP.\nTheorem 2.4 (Prompt Tuning Transformers with Single-Head, Single-Layer Attention and Two Feed-Forward Layers). Let 1 <p< < \u221e and \u20ac > 0. There exists a transformer 7 \u2208 T with a single self-attention layer and r = O ((1/\u20ac)d(Lp+L)) MLP neurons, such that for any fseq2seq \u2208 Fc, there exists a prompt P \u2208 Rd\u00d7Lp satisfying: dp (T([P, \u00b7]):,Lp, fseq2seq) \u2264 \u20ac."}, {"title": "2.5 Memory Capacity of Prompt Tuning", "content": "Based on our universality results, we show the memory capacity of prompt tuning on simple transformer networks with single head single layer self attention. We start with definition.\nDefinition 2.7 (Prompt Tuning Memorization). Given a dataset S = {(X(i), (i))}1 with X(i), Y(i) \u2208 Rd\u00d7L, a pretrained transformer \u03c4\u2208T memorizes S through prompt tuning if there exists a prompt P \u2208 RdxLp such that: maxi\u2208[N] ||T([P, X(i)]):,Lp - Y(i) || \u2264 e for all i \u2208 [N].\nWe now prove the existence of a transformer T\u2208 T,\" that memorizes any dataset S through prompt tuning. This result is easy to extend to transformers \u03c4 \u2208 T1,1,4.\nTheorem 2.5 (Memorization Capacity of Prompt Tuning). Consider a dataset S = {(X(i), (i))}1, where X(i), Y(i) \u2208 [0,1]dxL. Assume the coresponding embedding sequences Z(1),..., Z(N) are generated from a C-Lipschitz function. Then, there exists a single-layer, single-head attention transformer Te T with r = O ((1/\u20ac)d(Lp+L)) and a soft-prompt P\u2208 RdxLp such that, for any i \u2208 [N]: ||T([P, Z(i)]):,Lp - Y(i) || \u2264 6, where Lp > LA, with A = (2\u20ac-1C(dL)1/a)dL.\nProof Sketch. We first find the underlying sequence-to-sequence function of the dataset S, which is feq2seq: [0,1]dxL \u2192 [0,1]dxL, such that for any i \u2208 [N], fseq2seq (Z(i)) = Y(i). Next, we complete the proof by utilizing the results of Theorem 2.4 to construct a transformer 7 \u2208 T1, that is capable of approximating fseq2seq through prompt tuning.\nRemark 2.3. Theorem 2.5 shows that a carefully constructed simple transformer is capable of memorizing any dataset through prompt tuning. In contrast, [Wang et al., 2023a, Theorem 3] is limited to datasets with only two tokens per example and defines memorization as memoriz-ing only the last token. Additionally, we provide a lower bound on the prompt sequence length required to memorize any dataset, based on its dimensions and the desired accuracy.\nRemark 2.4. In [Wang et al., 2023a, Theorem 2], they construct a dataset and prove it to be unmemorizable by prompt tuning on a transformer with single-layer self-attention. However, their case differs as they require full-rank self-attention weight matrices and a specific form for the feed-forward layer. They design the dataset by exploiting the invertibility of the weight matrices and using a weak feed-forward layer, preventing the transformer from mapping contextual embeddings to the correct labels. We discuss these limitations in the expressive power of prompt tuning in Appendix I. In contrast, we prove that a transformer with single-layer self-attention and weight matrices of any rank is capable of achieving memorization through prompt tuning.\""}, {"title": "3 Computational Limits of Prompt Tuning Transformers", "content": "We analyze the computational limits of inference of prompt tuning Problem 1 using fine-grained complexity theory. Specifically, recall that Xp = [P,X] \u2208 Rd\u00d7(Lp+L) with Qp = WQXp \u2208 IRdx(Lp+L), Kp = WKXp \u2208 Rdx(Lp+L), and Vp = WKXp\u2208 Rd\u00d7(Lp+L). We study approximate prompt tuning inference with precision guarantees under df = 1/poly(Lp + L)."}, {"title": "3.1 Preliminaries: Strong Exponential Time Hypothesis (SETH)", "content": "Our hardness results are built on a common conjecture. Impagliazzo and Paturi [2001] introduce the Strong Exponential Time Hypothesis (SETH) as a stronger form of the P \u2260 NP conjecture. It suggests that our current best SAT algorithms are optimal and is a popular conjecture for prov-ing fine-grained lower bounds for a wide variety of algorithmic problems [Cygan et al., 2016, Williams, 2018].\nHypothesis 1 (SETH). For every \u20ac > 0, there is a positive integer k \u2265 3 such that k-SAT on formulas with n variables cannot be solved in O(2(1-6)n) time, even by a randomized algorithm.\nBelow, we rely on SETH to facilitate the fine-grained reduction for lower bound result (Theorem 3.1)."}, {"title": "3.2 Efficiency Criterion for Prompt Tuning Inference", "content": "We answer Question 2 affirmatively by identifying a phase transition behavior in the efficiency of all possible algorithms for Prompt Tuning Inference problem APTI (Problem 1), based on on the norm of Qp = WQXp, Kp = WKXp, and Vp = WvXp with X\u2081 = [P, X] \u2208 Rd\u00d7(Lp+L).\nTheorem 3.1 (Norm-Based Efficiency Phase Transition). Let ||QP||max \u2264 B, || Kp||, < B and ||Vp||max \u2264 B with B = O(\u221alog(Lp + L)). Assuming Hypothesis 1, for every q > 0, there are constants C, Ca, C\u266d > 0 such that: there is no O((Lp + L)2-9)-time (sub-quadratic) algorithm for the problem APTI(L, Lp, d = C log(Lp + L), B = Cblog(Lp + L), df = (Lp + L)-Ca)."}, {"title": "3.3 Prompt Tuning Can Be as Fast as Almost-Linear Time", "content": "We answer Question 3 affirmatively by proving the existence of almost-linear time efficient algo-rithms for Prompt Tuning Inference problem APTI (Problem 1) based on low-rank approximation.\nTheorem 3.2 (Almost-Linear Prompt Tuning Inference). The prompt tuning inference problem APTI(L, Lp, d = O(log(Lp + L)), B = o(\u221alog(Lp + L)), df = 1/poly(Lp + L)) can be solved in time Tmat((Lp + L), (Lp + L)\u00ba(1), d) = (Lp + L)1+0(1)."}, {"title": "4 Discussion and Conclusion", "content": "We study the fundamental limits of prompt tuning transformer-based pretrained models (i.e., foun-dation models) in two aspects: statistical and computational. Statistically, we show the universal-ity of prompt tuning transformer models with 1-head, 1-layer attention layers (Theorem 2.3 and Theorem 2.4). Recall that d is the token dimension, L is the input sequence length, Lp is the soft-prompt length, and e is the approximation error. Our results significantly relax previous re-quirements for thick layers, reducing from (Lp + L)(1/6)d layers to 1 attention layer, and from O((1/e)d(Lp+L)) layers to 2 FFN layers for prompt tuning universality. In addition, we prove the memorization capacity of prompt tuning and derive an exponential-in-dL and -in-1/6 lower bound on required soft-prompt tokens (Theorem 2.5). Different from [Wang et al., 2023a] where the analysis of capacity is solely on datasets of two-token sequences and focuses on only memorizing the last token, we demonstrate a complete memorization of prompt tuning on any general dataset. Computationally, we establish an efficient criterion of all possible prompt tuning inference for the norm of soft-prompt induced keys and queries (Theorem 3.1). In addition, we showcase our theory by proving the existence of nearly-linear time prompt tuning algorithms (Theorem 3.2).\nPractical Implications from Statistical Limits (Section 2). We analyze the universality of prompt tuning transformers with minimal structures, and its memorization capacity on general datasets.\n\u2022 Universality (Theorem 2.4). Our results show that the universality of prompt tuning pretrained transformer is achievable on as simple as a single-layer, single-head attention transformers. This demonstrates that universality in prompt-tuning isn't limited to large, complex foundation models.\n\u2022 Width-Depth Tradeoff (Section 2.4). Our results highlight a trade-off in the design choices for the depth and width of FFN (MLP) layers: (i) ((1/\u20ac)d(L+Lp) FFN layers of width 4 or (ii) 2 FFN layers of width O((1/\u20ac)d(L+Lp). In practice, (i) and (ii) differ in memory usage, parallelization, and optimization preferences, leading to distinct application scenarios.\n\u2022 Memorization (Section 2.5). Our memorization results apply to general datasets, whereas prior results are limited to specialized cases. This makes our results go beyond specialized theoretical analysis and align more with practical applications with a suggested long soft-prompt length.\nPractical Implications from Computational Limits (Section 3). We analyze the O(L2) bottle-neck of prompt tuning transformers and provides useful guidance for designing efficient prompt tuning (approximation) methods with precision guarantees. Let Qp = WQXp, Kp = WKXp, and Vp = WvXp with X\u2081 = [P, X] \u2208 Rd\u00d7(Lp+L). Here Land Lp are the input and soft-prompt length.\n\u2022 Self- and Cross-Attention. Our computational results apply to both self-attention and cross-attention prompt tuning. This is because the norm bound conditions depend on max{|Qp|, |Kp|, |Vp|}, which are valid for both self- and cross-attention inputs.\n\u2022 Necessary Conditions for Subquadratic Prompt Tuning (Theorem 3.1). Our result suggests proper normalization on soft-prompt and weight matrices are required to ensure subquadratic prompt tuning inference, i.e., max{||Qp||max, || Kp||max, ||Vp||max} \u2264 0(\u221alog(Lp + L)).\n\u2022 Necessary Conditions for Almost Linear Time Prompt Tuning (Theorem 3.2). Our re-sult suggests more strict normalization on soft-prompt and weight matrices are required to ensure almost linear time prompt tuning inference, i.e., max{||Qp||max, || Kp || max, ||Vp||max} < o(log(Lp + L))."}, {"title": "I Limitations of Prompt Tuning Transformers", "content": "In Section 2, we demonstrate that through prompt tuning, even a transformer with the simplest architecture can serve as a universal approximator. However, to achieve this, it is necessary to construct a specific transformer tailored for the task. In this section, we explore how prompts in-fluence the output of a pretrained transformer model. Additionally, we investigate the boundaries of prompt tuning on arbitrary pretrained transformer model by analyzing its underlying mecha-nisms."}, {"title": "I.1 Discussion on the Limitations of Prompt Tuning", "content": "For simplicity, consider a single-layer transformer function class with 1 head of size s and r MLP hidden neurons:\n```latex\n\\mathcal{T}^{h,s,r} := \\{\\tau : \\mathbb{R}^{d\\times L} \\rightarrow \\mathbb{R}^{d\\times L} | \\tau = f^{(FF)} (f^{(SA)} (.))\\}.\n```\nThe tokenwise output of the transformer 7 with input [P, X] \u2208 []Rd\u00d7(Lp+L) is\n```latex\n\\tau ([P, X])_{:,i} = f^{(FF)} (f^{(Att)} ([P, X]_{:,i}, [P, X]) + [P, X]_{:,i}),\n```\nwhere [P, X] is the concatenation of a prompt P\u2208 Rd\u00d7Lp and a data X \u2208 Rd\u00d7L. By taking the inverse of feed-forward function f(FF-1) : Rd \u2192 Rd, we have\n```latex\nf^{(Att)} (x, [P, X]) \\in f^{(FF)^{-1}} (y) - x,\n```\nwhere x = X.,i and y is the corresponding label token for x.\nNext, to better understand how the prompt P affect the output of the transformer, we focus on the output token of the attention layer corresponding to some data token x = X:,i,\n```latex\nf^{(Att)} (x, [P, X])\n&= W_o (W_v [P, X]) \\text{Softmax} [(W_K [P, X])^\\top (W_Q x)]\n&= W_o (W_v[P, X]) \\frac{\\begin{bmatrix} \\exp [(W_K [P, X]_{:,1})^\\top (W_Q x)] \\\\ : \\\\ \\exp [(W_K [P, X]_{:,(L+L_p)})^\\top (W_Q x)] \\end{bmatrix}}{\\sum_{j=1}^{L+L_p} \\exp [(W_K [P, X]_{:,j})^\\top (W_Q x)]}\n&= \\frac{\\sum_{j=1}^{L_p} W_o (W_v[P, X]_{:,j}) \\exp [(W_K [P, X]_{:,j})^\\top (W_Q x)]}{\\sum_{j=1}^{L+L_p} \\exp [(W_K [P, X]_{:,j})^\\top (W_Q x)]} + \\frac{\\sum_{j=1}^{L} W_o (W_v X_{:,j}) \\exp [(W_K X_{:,j})^\\top (W_Q x)]}{\\sum_{j=1}^{L+L_p} \\exp [(W_K [P, X]_{:,j})^\\top (W_Q x)]}\n&= \\frac{\\Psi (P, x)}{\\Psi ([P, X], x)} f^{(Att)} (x, P) + \\frac{\\Psi (X, x)}{\\Psi ([P, X], x)} f^{(Att)} (x, X),\n```\nwhere \u03a8(\u00b7,\u00b7,\u00b7) is a positive scalar and defined as\n```latex\n\\Psi (A, z) = \\sum_i \\exp ((W_K A_{:,i})^\\top (W_Q z)).\n```\nCombining (I.1) and (1.2), we have\n```latex\nf^{(Att)} (x, [P, X]) = \\frac{\\Psi (P, x)}{\\Psi ([P, X], x)} f^{(Att)} (x, P) + \\frac{\\Psi (X, x)}{\\Psi ([P, X], x)} f^{(Att)} (x, X) \\in f^{(FF)^{-1}} (y) - X.\n```\nEssentially, with all parameters for the feed-forward and self-attention layers fixed, prompt tun-ing finds the prompt P* such that (1.3) holds for each input-label pair (x, y). In (1.3), note that while (.,.,.) are positive scalars, the attention terms f(Att) (.) are vectors. The initial term depending entirely on P, highlighting the strong effect of prompt tuning on shaping the model's outputs by guiding the attention mechanism. In contrast, P's influence on the second term is limited to scaling, preserving the original attention pattern between x and X. Thus, prompt tuning biases the attention function's output but does not alter the intrinsic attention pattern between x and X."}]}