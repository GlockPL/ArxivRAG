{"title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling", "authors": ["Weijie Liu", "Zecheng Tang", "Juntao Li", "Kehai Chen", "Min Zhang"], "abstract": "Recent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation (MemLong, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ret-mem module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k\u00b9.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success in various fields. However, due to the quadratic time and space complexity of vanilla attention mechanisms (Vaswani et al., 2017), it is challenging to extend the context length considerably, which poses significant limitations for applications involving long-sequence tasks, such as long-document summarization (Koh et al., 2022) and multiple rounds of dialogue (Wang et al., 2024a). As a result, LLMs are often expected to maintain a long working capability (a.k.a. long context LLMs) to effectively handle these demanding scenarios.\nTo tackle the computational bottleneck, numerous efforts have been made. The first line of work focuses on reducing the computation of vanilla attention mechanisms (Vaswani et al., 2017) by employing sparse attention operations (Beltagy et al., 2020; Wang et al., 2020; Kitaev et al., 2020; Xiao et al., 2023a; Chen et al., 2023b; Lu et al., 2024). Although these types of works can reduce computational complexity to approximately O(n), it often comes with trade-offs in model capability. Therefore, Some works shift their focus to memory selection (Dai et al., 2019; Bertsch et al., 2024; Yu et al., 2023). These approaches, as token-level memory selection, can result in the truncation of semantic information. Another recent line of work is Retrieval-Augment Language Modeling (Wu et al., 2022; Wang et al., 2024b; Rubin and Berant, 2023). These works usually introduce a retrieval mechanism to enhance the model's ability to handle long texts. However, these methods have several drawbacks. Firstly, the information stored in memory may experience distribution shifts due to changes in model parameters during training. Secondly, these methods often require retraining, which is impractical in the era of large models. Finally, these models are often prone to processing long text inputs at the expense of the original capabilities of the pre-trained model. To address the limitations of previous research, we posed the following question: Can we utilize the explicit retrieval capabilities of a retriever to approximate the implicit retrieval processes within the model?\nIn this work, we propose MemLong, an efficient and lightweight method to extending the context window of LLMs. The key idea is to store past contexts and knowledge in a non-trainable memory bank and further leverages these stored embeddings to retrieve chunk-level key-value (K-V) pairs for input into the model.. MemLong is applicable to any decoder-only pretrained language models by incorporating (1) an additional ret-mem component for memory and retrieval, and (2) a retrieval causal attention module for integrating local and memory information. The memory and retrieval process of MemLong is illustrated in Figure 1(b). During generation,one text that exceeds the model's maximum processing length is stored as context information in a Memory Bank. Subsequently, given a recently generated text chunk in a long document, we use the retriever to explicitly retrieve past information, obtaining additional context information through index alignment.\nMemLong offers several benefits: (1) Distributional Consistency: Unlike previous models that experienced a distribution shift when information was stored in memory, MemLong ensures the distribution of cached information remains consistent. (2) Training Efficient: We freeze the lower layers of the model and only need to finetune the upper layers which greatly reduced computational cost. In our experiments, finetuning a 3B parameter version of MemLong on 0.5B tokens requires only eight 3090 GPUs for eight hours. (3) Extensive Context Window: Since only a single layer's K-V pairs need to be memorized, MemLong is capable of extending the context window up to 80k tokens easily on a single 3090 GPU."}, {"title": "Preliminary", "content": "2.1 Task Definition\nLanguage models are designed to define probability distributions over sequences of tokens, effectively predicting the likelihood of a sequence within a given language. Given such a sequence x1,..., Xn, the standard approach to modeling its probability is via the next-token prediction: \\(p(x_1,...,x_n) = \\sum_{i=0}^{n} P_\\theta(x_i|x_{<i})\\), where x<i := X1,..., Xi\u22121 is the sequence of tokens proceeding xi. Differently from the standard language modeling objective, we not only use the current context to make next-token predictions, but also utilize external retrieval to obtain relevant information and perform knowledge fusion at the upper layers of the model. Specifically, given a sequence consisting of l tokens and the size of each chunk \\(\\tau\\), we partition it into a long sequence of \\(v = \\frac{l}{\\tau}\\) non-overlapping chunks, which denoted as C = (C1, . . ., C\u2081). Correspondingly, its textual form is divided into v text chunks, which denoted as T = (t1, . . ., t\u2081). In each step, we perform causal language modeling on ci in the lower layers, while in the upper layers, we conduct fine-grained controllable retrieval on t\u012f for the fusion of additional information. After do this, our language modeling objective becomes\n\\[p(x_1,...,x_n) = \\sum_{i=0}^{n}P_\\theta(x_i|R(t_i), x_{<i})\\] (1)\nwhere R(ti) denotes the retrieval of neighboring chunks of ti where xi is located.\n2.2 Module and Operation Definitions\nAs shown in Figure 2, the Ret-Mem module comprises a Retriever and a Memory component for information exchange. Initially, we define the Memory component as M and the Retriever as R, and their corresponding operations M(\u00b7) and R(.). Furthermore, we specify the dimension of the model as dmodel, the dimension of the retriever"}, {"title": "MemLong", "content": "3.1 Overview\nAs illustrated in Figure 2, each step involves an input of a chunk ci, where the original text for that chunk is ti. In the lower layers where the model is frozen, the standard causal attention is applied to the entire ci. For the final layer of the lower layers, we refer to it as the memory layer. Following each traversal of the memory layer, two key operations are performed. The first operation is retrieval, depicted by the red line, where ti is utilized to fetch the most pertinent K-V pairs. The second operation, indicated by the blue line, involves caching the acquired K-V pairs along with their associated chunk representation. Within the model's upper layers, the retrieved K-V pairs are integrated with the current input context, subsequently tuning the model parameters to calibrate the retrieval reference. Subsequent sections will explore the various facets of the MemLong framework and their intricacies, encompassing Retriever and Dynamic Memory Management (\u00a7 3.2), Attention Reformulation (\u00a7 3.3), and Inference with MemLong (\u00a7 3.4).\n3.2 Retriever and Dynamic Memory Management\nWe offer a comprehensive explanation of the retrieval process and the dynamics of memory management.\nRetrieval Process. Given our objective to replace traditional KNN retrieval based on K-V pairs with explicit retrieval, we aim to pre-fetch the desired information when feasible before each model input. Specifically, for each potential query block cq = ci and its corresponding text block t\u00ba = ti, we first pass it through Retriever and then obtain a representation embedding rq = R(t\u00ba), where rq \u2208 Rdret. Subsequently, we use this representation embedding to perform retrieval against the embeddings in M to obtain the required k chunk-level indices. We compute the cosine similarity between the retrieval representation r\u00ba and the embeddings stored in Memory M. Finally, we get the top-k indices zq = TopK{Cos (rq)} for the c\u00ba, where zq \u2208 Rk. Due to the contiguous nature within the blocks, we can easily extend the obtained indices to cover the entire relevant range for retrieval. Finally, we retrieve the corresponding K-V pairs Zq \u2208 Rk\u00d7T\u00d7dmodel from Memory based on these indices and used for the upper layer. It is noteworthy that we have equipped the Memory with a counter mechanism to record the frequency of retrievals for each index contained therein. This frequency data will subsequently serve as a basis for dynamic memory updating, allowing for the prioritization of more frequently retrieved information.\nMemory Process. The memory process synchronously stores the K-V pairs from the memory layer and the representation embedding previous calculated for retrieval, ensuring that indices for K-V pairs correspond accurately to their representation embeddings (see Figure 2, right, blue line). For every possible chunk memory cm = ci, and its corresponding text chunk tm = ti, we divide the memory process into two parts: the first part details how to cache the K-V pairs, and the second part explains how to store the corresponding representations. Firstly, we input cm into the MemLong and get the output from the memory layer. It is worth noting that, since the lower layers are frozen during training, we can ensure that the distribution of the output K-V pairs is consistent. This consistency is crucial for avoiding the distribution shift issue, which was previously observed in models like MemTrm (Wu et al., 2022). Our memory operation is highly efficient because it only involves storing the representations needed for retrieval, rm = rq, thereby avoiding redundancy. After the retrieval for all chunk pairs is complete, the memory operation\u2014denoted as M(k, v; rm)\u2014synchronously updates the memory with both the Key-Value pairs and their corresponding representations.\nDynamic Memory Update. When memory overflows, we use the Counter to update memory intelligently. In our experiments, we keep the latest 10% of memory content due to its potential relevance, discard the oldest 10% as likely outdated, and prioritize the middle 80% based on retrieval frequency, deleting the least accessed entries until memory usage drops to 50%. This selective pruning balances recency and relevance, retaining valuable information and removing less pertinent data. Unlike traditional FIFO strategies, our method focuses on retrieval frequency to efficiently prune redundant information, maintaining a high-quality dataset. The decision to dynamically update the datastore is a trade-off between effectiveness and efficiency. For tasks requiring long-term dependencies, storing all information can enhance comprehensive processing, but for shorter-term tasks, dynamic updates are more suitable. Dynamic updates control memory size to prevent out-of-memory issues, discard stale information, and reduce retrieval overhead, ensuring efficiency without significantly compromising performance.\n3.3 Attention Reformulation\nIn the trainable upper layers of the model, we revised the attentions to fuse with long-term memory. As illustrated in Figure 3, unlike the traditional Transformer decoder layers that utilize Multi-Head Attention (Vaswani et al., 2017), we propose a Retrieval Causal Attention to extend it to a joint-attention mechanism and propose a long-term memory fusion process to enable each token to attend on both local contexts and chunk-level past contexts which have complete and continuous semantics. With the head-wise hidden state output from previous layer Hl\u22121 \u2208 R|x|xdmodel and the corresponding retrieved key-value pairs are q\n    {Ki, Vi}i=1 \u2208 RkxTXdmodel, the output hidden state for the next layer H\u00b9 is computed as:\n\\[S_m = Softmax(\\frac{Q K^T}{\\sqrt{d}})\\] (2)\n\\[S_a = Concat(Softmax(\\frac{QK^T}{\\sqrt{d}}))^{3}_{i=1}\\] (3)\nTo avoid the interference caused by the retrieval attention scores Sm at the initial stage of training, we adopt a multi-head attention mechanism following the approach of the LLaMA-adapter(Zhang et al., 2023b):\n\\[S^h_i = [(S_m) \\cdot g_i; (S_a)]^T\\] (4)\nFinally, we concatenate the V and V to obtain H\u00b9:\n\\[V_i = [V_i; V_i], \\quad H = S V_i\\] (5)\n3.4 Inference with MemLong\nWhen MemLong receives an input exceeding the length, we treat it as two segments: the prefix and the main. We will separately describe the encoding of long inputs and the generation of long outputs during the inference phase. When MemLong receives long inputs, it first divides the prefix into multiple non-overlapping chunks and computes the from its memory layer, which ensures that the number of tokens involved in the attention is equal to the chunk size, which is much smaller than the length of the input. It is important to note that each chunk is interrelated (e.g., the t-th chunk needs to process the of the previous t \u2212 1 chunks).\nThe second step is to select the k most relevant chunks for the main based on chunk-level retrieval representations and to obtain their key and value representations. After this, for the upper retrieval layers, the attention window for retrieval is equivalent to k * \\(\\tau\\), which is also smaller than the input length. Finally, both length-restricted causal attention and retrieval attention is performed efficiently."}, {"title": "Experiments", "content": "We evaluate our proposed MemLong model on various tasks that require in-memory long-context processing: (a) long-context language modeling and retrieval-augmented language modeling; (b) scalable in-context learning capable of handling a large number of demonstration examples in memory.\n4.1 Implementation Details\nTraining Details. We use OpenLLaMA-3B as the pre-trained backbone LLM with rotation position coding (Su et al., 2024). Due to hardware constraints, we opted to train our models using the LORA (Hu et al., 2021) technique. The backbone LLM holds a L = 26, H = 32, d = 100 architecture. Unless specified otherwise, we use the 13-th layer as the memory layer and the [14,18,22,26] layers as the retrieval-augment layers. The training for retrieval-augmented adaptation iterates only on 0.5B tokens with 1024 sequence length. MemLong's trainable parameters are from 14 to 26 layers. We utilized the slimpajama dataset sampled by (Fu et al., 2024) as our training corpus.\nPosition Remapping. There are several chunk-level K-V in the M retrieved for generation. Due to the uncertainty of retrieval at each step, we need to remap position embeddings to the retrieved chunks. Same as the previous work (Tworkowski et al., 2024), the local context (up to 2048 tokens) receives the standard rotary positional encoding, whereas memory keys are encoded as if they had position 0 in the local context window.\n4.2 Long-Context Language Modeling\nWe first evaluate MemLong on long-context language modeling benchmarks to assess basic language modeling abilities. Due to the K-V cache providing sinificant background and contextual information, MemLong can retrieve relevant K-V cache quickly and make full use of it, thereby enhancing the model's in long-context modeling tasks.\nDatasets. We conducte an evaluation of our model across four extensive text benchmark datasets: English-language books PG-19 (Rae et al., 2019) and BookCorpus (Zhu et al., 2015), Wikipedia articles Wikitext-103 (Merity et al., 2016), and mathematical papers Proof-Pile (Azerbayev et al., 2023). The experimental results indicate a significant perplexity improvement across all datasets. Our model is tested over various lengths ranging from 1024 to 32768 tokens. Across all datasets, our model demonstrated substantial performance gains with minimal memory overhead by leveraging an external retriever and memory.\nSetup. Following (Yen et al., 2024), we calculate the perplexity on the last 2048 tokens of each sequence. This experimental setup is designed to validate the influence of different retriever sizes on the overall performance of our model. For the implementation of the efficient fine-grained retrieval, we use the faiss (Johnson et al., 2019) toolkit to construct an exact-search index on GPU to store the Representation Embeddings of text chunks and perform efficient retrieval. For MemLong, we split and put the tokens over finetune-length = 1024 into the M used for further retrieval.\nBaselines. For our experiments, we employ the OpenLLaMA-3B model as our baseline. To ensure a fair comparison, we utilize an identical LoRA"}, {"title": "In Context Learning", "content": "Traditional in-context learning (ICL; Brown et al., 2020) inputs few-shot non-parameterized demonstration examples along with the query into the model. However, these methods are typically constrained by the model's input length. In this experiment, since MemLong can store examples in a parameterized form within its memory, we primarily investigate whether MemLong can effectively utilize the knowledge stored in its memory to enhance its emergent abilities. The results are shown in Table 2. Compared to OpenLLaMA,which rely solely on non-parametric knowledge, given the same number of in-context demonstrations, MemLong can utilize additional demonstrations stored in its memory. The performance further increases or remains consistent with more demonstrations in the memory. In our comparative analysis against LongLLaMA, it was observed that our model outperforms LongLLaMA across the majority of datasets under the same conditions of preserving In-Memory Demonstrations. It is important to highlight that our model operates with significantly lower training parameters (200M V. S. 0.3B) and fine-tuning data volume (0.5B V. S. 5B) compared to LongLLaMA. This underscores our model's efficiency in leveraging an external retriever for information acquisition, demonstrating a superior ability to synthesize and utilize knowledge effectively with substantially fewer resources."}, {"title": "Ablation Study", "content": "5.1 Training Setting\nDuring the training phase, we explore the effects of varying retrieval layers on the model and examine whether the distribution shift problem, as discussed in MemTrm (Wu et al., 2022), could be adequately resolved by our approach. As mentioned before, Our method proposes a low-cost solution for distribution shifts. As shown in Figure 4, the brown line (the line at the top of the picture; the training method is similar to MemTrm fine-tuning all parameters of the model and all layers after the memory layer are involved in the retrieval) is significantly worse than all other ours methods (even the most unreasonable settings) in terms of performance and fitting speed. We will analyze the performance of the reasoning stage later.\n5.2 Inference Performance\nQ1: Does the memory length affect the performance of the model ? As depicted in Figure 5, our examination of the same model's performance across various memory sizes demonstrates a clear correlation between memory capacity and model efficiency. The trend indicates that incremental increases in memory size yield gradual enhancements in performance. Moreover, a critical threshold is identified at a memory size of 65536, beyond which the model's capabilities undergo a substantial leap. This suggests that while expanding memory offers substantial benefits, there is a practical ceiling to its effectiveness, likely influenced by the nuances of the data's distribution.\nQ2: How many layers do we need to introduce extra memory information? As shown in Figure 4, (the pink line) and Table 3 (RPL+TH), the model performs best when the number of retrieval layers is set to [13,17,21,25]. It is empirically believed that if retrieval information is introduced into all upper layers of the model, it leads to a decrease in the model's attention to local context. Therefore, selecting retrieval layers at appropriate intervals can actually enhance the model's capabilities."}, {"title": "Related Work", "content": "6.1 Long Context Language Modeling\nLong context Language Modeling mainly concentrate on length extension and context window expansion. Length Extension studies typically target the popular RoPE encoding, aiming to scale unseen PE into the space of positions seen during pre-training. These works (Su et al., 2024; Press et al., 2021; Chen et al., 2023a; Peng et al., 2023) enable the model to generalize to unseen positional encodings during inference, thereby achieving extrapolation beyond the lengths encountered during training. In contrast, our method does not require modifying the PE, and only use one addition module to extend the context. Context Window Extension focuses on how to extend the context window that LLMs can handle the input at one time. Due to the quadratic time and space complexity of computing attention, extending the input length of language models is quite challenging. Sparse attention (Kitaev et al., 2020; Chen et al., 2023b; Tworkowski et al., 2024; Bertsch et al., 2024; Beltagy et al., 2020) techniques have made significant strides, but our focus is on improving long-range language modeling by enabling LLMs to access relevant information at shorter input lengths via a retrieval-enhanced method.\n6.2 Retrieval-Augmented Language Modeling\nMuch effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020; Izacard and Grave, 2020; Ram et al., 2023; Yu et al., 2022; Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2024b) mitigates this by training a sub-network, though this adds significant overhead. In contrast, our approach involves a fixed pre-trained model, enhancing it with a frozen retriever that aligns with the model's internal retrieval processes, thus avoiding distribution shifts and architectural changes."}, {"title": "Conclusion", "content": "We introduce MemLong, an innovative approach that significantly enhances the capability of language models to process long texts by leveraging an external retriever. MemLong utilizes a proficient retriever to swiftly and accurately access text relevant to the distant context with minimal memory overhead. MemLong successfully expands the model's context window from 2k to 80k tokens. We demonstrate that MemLong exhibits considerable competitive advantages in long-distance text modeling and comprehension tasks. MemLong can achieve up to a 10.4 percentage point improvement in performance compared to the full-context model."}, {"title": "Limitations", "content": "Our work primarily focuses on OpenLLaMA-3B. We hope that future research will explore and investigate the application of our methods to models of various sizes. At the same time, it has been found that while single-layer K-V Pairs can provide additional semantic information to the upper layers, this information is unstable. We hope that future work can provide a more rational framework to accommodate our methods. At the same time, we employ a retriever with fixed FlagEmbeddings (Xiao et al., 2023b; Zhang et al., 2023a), but studying a greater range of retrievers would be useful."}, {"title": "Ethics Statement", "content": "In the pursuit of advancing knowledge and developing innovative solutions, we are committed to upholding the highest ethical standards. Our work is guided by a steadfast dedication to integrity, transparency, and respect for all individuals and communities involved. Since pre-trained models may have some bias due to the unavoidable presence of harmful/offensive corpus during training, MemLong fine-tuning on Slimpajama will face this problem as well. Although solving this problem is out of our current work, we hope that there will be future work that addresses this type of problem well."}]}