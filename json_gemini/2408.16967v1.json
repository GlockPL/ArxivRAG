{"title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling", "authors": ["Weijie Liu", "Zecheng Tang", "Juntao Li", "Kehai Chen", "Min Zhang"], "abstract": "Recent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation (MemLong, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ret-mem module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k\u00b9.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success in various fields. However, due to the quadratic time and space complexity of vanilla attention mechanisms (Vaswani et al., 2017), it is challenging to extend the context length considerably, which poses significant limitations for applications involving long-sequence tasks, such as long-document summarization (Koh et al., 2022) and multiple rounds of dialogue (Wang et al., 2024a). As a result, LLMs are often expected to maintain a long working capability (a.k.a. long context LLMs) to effectively handle these demanding scenarios.\nTo tackle the computational bottleneck, numerous efforts have been made. The first line of work focuses on reducing the computation of vanilla attention mechanisms (Vaswani et al., 2017) by employing sparse attention operations (Beltagy et al., 2020; Wang et al., 2020; Kitaev et al., 2020; Xiao et al., 2023a; Chen et al., 2023b; Lu et al., 2024). Although these types of works can reduce computational complexity to approximately O(n), it often comes with trade-offs in model capability. Therefore, Some works shift their focus to memory selection (Dai et al., 2019; Bertsch et al., 2024; Yu et al., 2023). These approaches, as token-level memory selection, can result in the truncation of semantic information. Another recent line of work is"}, {"title": "Preliminary", "content": null}, {"title": "Task Definition", "content": "Language models are designed to define probability distributions over sequences of tokens, effectively predicting the likelihood of a sequence within a given language. Given such a sequence x1,..., Xn, the standard approach to modeling its probability is via the next-token prediction: p(x1,...,xn) = \u2211i=0 Po(xi|x<i), where x<i := x1,..., xi\u22121 is the sequence of tokens proceeding xi. Differently from the standard language modeling objective, we not only use the current context to make next-token predictions, but also utilize external retrieval to obtain relevant information and perform knowledge fusion at the upper layers of the model. Specifically, given a sequence consisting of l tokens and the size of each chunk \u03c4, we partition it into a long sequence of v =  non-overlapping chunks, which denoted as C = (c1, . . ., cv). Correspondingly, its textual form is divided into v text chunks, which denoted as T = (t1, . . ., tv). In each step, we perform causal language modeling on ci in the lower layers, while in the upper layers, we conduct fine-grained controllable retrieval on ti for the fusion of additional information. After do this, our language modeling objective becomes\np(x1,...,xn) = \u2211i=0 Po(xi|R(ti), x<i) (1)\nwhere R(ti) denotes the retrieval of neighboring chunks of ti where xi is located."}, {"title": "Module and Operation Definitions", "content": "As shown in Figure 2, the Ret-Mem module comprises a Retriever and a Memory component for information exchange. Initially, we define the Memory component as M and the Retriever as R, and their corresponding operations M(\u00b7) and R(\u00b7). Furthermore, we specify the dimension of the model as dmodel, the dimension of the retriever"}, {"title": "MemLong", "content": null}, {"title": "Overview", "content": "As illustrated in Figure 2, each step involves an input of a chunk ci, where the original text for that chunk is ti. In the lower layers where the model is frozen, the standard causal attention is applied to the entire ci. For the final layer of the lower layers, we refer to it as the memory layer. Following each traversal of the memory layer, two key operations are performed. The first operation is retrieval, depicted by the red line, where ti is utilized to fetch the most pertinent K-V pairs. The second operation, indicated by the blue line, involves caching the acquired K-V pairs along with their associated chunk representation. Within the model's upper layers, the retrieved K-V pairs are integrated with the current input context, subsequently tuning the model parameters to calibrate the retrieval reference. Subsequent sections will explore the various facets of the MemLong framework and their intricacies, encompassing Retriever and Dynamic Memory Management (\u00a7 3.2), Attention Reformulation (\u00a7 3.3), and Inference with MemLong (\u00a7 3.4)."}, {"title": "Retriever and Dynamic Memory Management", "content": "We offer a comprehensive explanation of the retrieval process and the dynamics of memory management.\nRetrieval Process. Given our objective to replace traditional KNN retrieval based on K-V pairs with explicit retrieval, we aim to pre-fetch the desired information when feasible before each model input. Specifically, for each potential query block cq = ci and its corresponding text block tq = ti, we first pass it through Retriever and then obtain a representation embedding rq = R(tq), where rq \u2208 Rdret. Subsequently, we use this representation embedding to perform retrieval against the embeddings in M to obtain the required k chunk-level indices. We compute the cosine similarity between the retrieval representation rq and the embeddings stored in Memory M. Finally, we get the top-k indices zq = TopK{Cos (rq)} for the cq, where"}, {"title": "Attention Reformulation", "content": "In the trainable upper layers of the model, we revised the attentions to fuse with long-term memory. As illustrated in Figure 3, unlike the traditional Transformer decoder layers that utilize Multi-Head Attention (Vaswani et al., 2017), we propose a Retrieval Causal Attention to extend it to a joint-attention mechanism and propose a long-term memory fusion process to enable each token to attend on both local contexts and chunk-level past contexts which have complete and continuous semantics. With the head-wise hidden state output from previous layer Hl\u22121 \u2208 R|x|\u00d7dmodel and the corresponding retrieved key-value pairs q = {Ki, Vi}ki=1 \u2208 Rk\u00d7\u03c4\u00d7dmodel, the output hidden state for the next layer H\u00b9 is computed as:\nSm = Softmax(QKT\u221ad)(2)\nSa = Concat(Softmax(S))ji=1(3)\nTo avoid the interference caused by the retrieval attention scores Sm at the initial stage of training, we adopt a multi-head attention mechanism following the approach of the LLaMA-adapter(Zhang et al.,"}, {"title": "Inference with MemLong", "content": "When MemLong receives an input exceeding the length, we treat it as two segments: the prefix and the main. We will separately describe the encoding of long inputs and the generation of long outputs during the inference phase. When MemLong receives long inputs, it first divides the prefix into multiple non-overlapping chunks and computes the  from its memory layer, which ensures that the number of tokens involved in the attention is equal to the chunk size, which is much smaller than the length of the input. It is important to note that each chunk is interrelated (e.g., the t-th chunk needs to process the  of the previous t \u2212 1 chunks).\nThe second step is to select the k most relevant chunks for the main based on chunk-level retrieval representations and to obtain their key and value representations. After this, for the upper retrieval layers, the attention window for retrieval is equivalent to k * \u03c4, which is also smaller than the input length. Finally, both length-restricted causal attention and retrieval attention is performed efficiently."}, {"title": "Experiments", "content": "We evaluate our proposed MemLong model on various tasks that require in-memory long-context processing: (a) long-context language modeling and retrieval-augmented language modeling; (b) scalable in-context learning capable of handling a large number of demonstration examples in memory."}, {"title": "Implementation Details", "content": "Training Details. We use OpenLLaMA-3B as the pre-trained backbone LLM with rotation position coding (Su et al., 2024). Due to hardware constraints, we opted to train our models using the LORA (Hu et al., 2021) technique. The backbone LLM holds a L = 26, H = 32, d = 100 architecture. Unless specified otherwise, we use the 13-th layer as the memory layer and the [14,18,22,26] layers as the retrieval-augment layers. The training for retrieval-augmented adaptation iterates only on 0.5B tokens with 1024 sequence length. MemLong's trainable parameters are from 14 to 26 layers. We utilized the slimpajama dataset sampled by (Fu et al., 2024) as our training corpus.\nPosition Remapping. There are several chunk-level K-V in the M retrieved for generation. Due to the uncertainty of retrieval at each step, we need to remap position embeddings to the retrieved chunks. Same as the previous work (Tworkowski et al., 2024), the local context (up to 2048 tokens) receives the standard rotary positional encoding, whereas memory keys are encoded as if they had position 0 in the local context window."}, {"title": "Long-Context Language Modeling", "content": "We first evaluate MemLong on long-context language modeling benchmarks to assess basic language modeling abilities. Due to the K-V cache providing sinificant background and contextual information, MemLong can retrieve relevant K-V cache quickly and make full use of it, thereby enhancing the model's in long-context modeling tasks.\nDatasets. We conducte an evaluation of our model across four extensive text benchmark datasets: English-language books PG-19 (Rae et al., 2019) and BookCorpus (Zhu et al., 2015), Wikipedia articles Wikitext-103 (Merity et al., 2016), and mathematical papers Proof-Pile (Azerbayev et al., 2023). The experimental results indicate a significant perplexity improvement across all datasets. Our model is tested over various lengths, ranging from 1024 to 32768 tokens. Across all datasets, our model demonstrated substantial performance gains with minimal memory overhead by leveraging an external retriever and memory.\nSetup. Following (Yen et al., 2024), we calculate the perplexity on the last 2048 tokens of each sequence. This experimental setup is designed to validate the influence of different retriever sizes on the overall performance of our model. For the implementation of the efficient fine-grained retrieval, we use the faiss (Johnson et al., 2019) toolkit to construct an exact-search index on GPU to store the Representation Embeddings of text chunks and perform efficient retrieval. For MemLong, we split and put the tokens over finetune-length = 1024 into the M used for further retrieval.\nBaselines. For our experiments, we employ the OpenLLaMA-3B model as our baseline. To ensure a fair comparison, we utilize an identical LoRA"}, {"title": "In Context Learning", "content": "Traditional in-context learning (ICL; Brown et al., 2020) inputs few-shot non-parameterized demonstration examples along with the query into the model. However, these methods are typically constrained by the model's input length. In this experiment, since MemLong can store examples in a parameterized form within its memory, we primarily investigate whether MemLong can effectively utilize the knowledge stored in its memory to enhance its emergent abilities. The results are shown in Table 2. Compared to OpenLLaMA,which rely solely on non-parametric knowledge, given the same number of in-context demonstrations, MemLong can utilize additional demonstrations"}, {"title": "Ablation Study", "content": null}, {"title": "Training Setting", "content": "During the training phase, we explore the effects of varying retrieval layers on the model and examine whether the distribution shift problem, as discussed in MemTrm (Wu et al., 2022), could be adequately resolved by our approach. As mentioned before, Our method proposes a low-cost solution for distribution shifts. As shown in Figure 4, the brown line (the line at the top of the picture; the training method is similar to MemTrm fine-tuning all parameters of the model and all layers after the memory layer are involved in the retrieval) is significantly worse than all other ours methods (even the most unreasonable settings) in terms of performance and fitting speed. We will analyze the performance of the reasoning stage later."}, {"title": "Inference Performance", "content": null}, {"title": "Related Work", "content": null}, {"title": "Long Context Language Modeling", "content": "Long context Language Modeling mainly concentrate on length extension and context window expansion. Length Extension studies typically target the popular RoPE encoding, aiming to scale unseen PE into the space of positions seen during pre-training. These works (Su et al., 2024; Press et al., 2021; Chen et al., 2023a; Peng et al., 2023) enable the model to generalize to unseen positional encodings during inference, thereby achieving extrapolation beyond the lengths encountered during training. In contrast, our method does not require modifying the PE, and only use one addition module to extend the context. Context Window Extension focuses on how to extend the context window that LLMs can handle the input at one time. Due to the quadratic time and space complexity of computing attention, extending the input length of language models is quite challenging. Sparse attention (Kitaev et al., 2020; Chen et al., 2023b; Tworkowski et al., 2024; Bertsch et al., 2024; Beltagy et al., 2020) techniques have made significant strides, but our focus is on improving long-range language modeling by enabling LLMs to access relevant information at shorter input lengths via a retrieval-enhanced method."}, {"title": "Retrieval-Augmented Language Modeling", "content": "Much effort has been made to enhance Retrieval-Augmented Language Modeling (Lewis et al., 2020; Izacard and Grave, 2020; Ram et al., 2023; Yu et al., 2022; Asai et al., 2023). While some approaches use external retrievers, non-parametric information fusion often falls short compared to parametric methods within the model. We concentrate on integrating retrieval concepts directly into the model. REALM (Guu et al., 2020) suggests that relying solely on internal model knowledge is inefficient and advocates for the model to learn to retrieve and comprehend. kNN-LM (Khandelwal et al., 2019) enhances language modeling by blending the LLM's next-word predictions with those from a retrieval-based mechanism. MemTrm (Wu et al., 2022) introduces a memory bank but risks shifting memory distributions due to parameter adjustments. LongMEM (Wang et al., 2024b) mitigates this by training a sub-network, though this adds significant overhead. In contrast, our approach involves a fixed pre-trained model, enhancing it with a frozen retriever that aligns with the model's internal retrieval processes, thus avoiding distribution shifts and architectural changes."}, {"title": "Conclusion", "content": "We introduce MemLong, an innovative approach that significantly enhances the capability of language models to process long texts by leveraging an external retriever. MemLong utilizes a proficient retriever to swiftly and accurately access text"}, {"title": "Ethics Statement", "content": "In the pursuit of advancing knowledge and developing innovative solutions, we are committed to upholding the highest ethical standards. Our work is guided by a steadfast dedication to integrity, transparency, and respect for all individuals and communities involved. Since pre-trained models may have some bias due to the unavoidable presence of harmful/offensive corpus during training, MemLong fine-tuning on Slimpajama will face this problem as well. Although solving this problem is out of our current work, we hope that there will be future work that addresses this type of problem well."}]}