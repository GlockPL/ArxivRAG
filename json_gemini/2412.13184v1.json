{"title": "Tilted Quantile Gradient Updates for Quantile-Constrained Reinforcement Learning", "authors": ["Chenglin Li", "Guangchun Ruan", "Hua Geng"], "abstract": "Safe reinforcement learning (RL) is a popular and versatile paradigm to learn reward-maximizing policies with safety guarantees. Previous works tend to express the safety constraints in an expectation form due to the ease of implementation, but this turns out to be ineffective in maintaining safety constraints with high probability. To this end, we move to the quantile-constrained RL that enables a higher level of safety without any expectation-form approximations. We directly estimate the quantile gradients through sampling and provide the theoretical proofs of convergence. Then a tilted update strategy for quantile gradients is implemented to compensate the asymmetric distributional density, with a direct benefit of return performance. Experiments demonstrate that the proposed model fully meets safety requirements (quantile constraints) while outperforming the state-of-the-art benchmarks with higher return.", "sections": [{"title": "Introduction", "content": "There has been rising attention on Safe reinforcement learning (RL) which seeks to develop a policy that maximizes the expected cumulative return while meeting all the safety constraints. A common option is the Constrained Markov Decision Process (CMDP) framework, in which the safety constraints are established to bound the expected cumulative cost $C = \\sum_t \\gamma^t c_t$ by:\n\n$\\mathbb{E}[C] \\leq d$ (1)\n\nMost previous works follow this framework to construct constraints, because this form is consistent with the expected cumulative return (objective function) such that the gradient calculation would become the same for both the safety constraints and the objective function. As for the solution, the Lagrange approach is common (Achiam et al. 2017; Liang, Que, and Modiano 2018; Tessler, Mankowitz, and Mannor 2018; Liu, Ding, and Liu 2020), but there are other options such as projection-based methods (Yang et al. 2020), shielding methods (Alshiekh et al. 2018; Carr et al. 2023), and barrier methods (Marvi and Kiumarsi 2021; Cheng et al. 2019).\nUnfortunately, the above expectation setup does not apply for many real-world safety-critical applications. Bounding expectations may still induce constraint violation in extreme cases, and the potential risk cannot be strictly limited. A better solution is to apply probability-related constraints, e.g., 95%-quantile, to impose a more accurate and robust requirement for safety. In this case, the probability $\\text{Pr}[C < d]$ can be more informative than the safety expectation $\\mathbb{E}[C] \\leq d$.\nA probabilistic constraint or a chance constraint is typically expressed as follows (Chow et al. 2018; Chen, Subramanian, and Paternain 2024):\n\n$\\text{Pr}[C < d] \\geq 1 - \\epsilon$ (2)\n\nEqn. (2) strictly limits the probability of constraint violation under a given level $\\epsilon$, but this constraint is computationally intractable and suffers from low sample efficiency and lack of distribution priors. A direct transformation of (2) needs to apply the quantile, or value-at-risk (VaR) metric. Mathematically, Eqn. (2) is equivalent to the following quantile constraint:\n\n$q_{1-\\epsilon} := \\inf\\{x | \\text{Pr}[C \\leq x] \\geq 1 - \\epsilon\\} \\leq d$ (3)\n\nAn optimization model with quantile constraints integrated is still computationally intensive for training. In the literature, quantile optimization has been widely studied. Estimating the gradient of quantile is a challenge, and most of the existing solutions such as the perturbation analysis (Jiang and Fu 2015), the likelihood ratio method (Glynn et al. 2021), and the kernel density estimation (Hong and Liu 2009) are heavily relied on the analytical model formulation. Also, these models are focused on unconstrained optimization problems with quantile-based objectives, but the focus of this paper is on the quantile constrains for Safe RL.\nQuantile-constrained RL was first studied in (Jung et al. 2022), and the idea was to supplement the expected cumulative cost $\\mathbb{E}[C]$ with an additive term to approximate the quantile $q_{1-\\epsilon}$. Within this setting, the quantile constraint could be converted into an expectation-type constraint at last, aligning with the CMDP framework. Note that this work required the cumulative cost distribution, and its empirical performances might be over-conservative (with relatively low return) because of the biased approximation of quantiles.\nOther related works have also been conducted to bound the probability of constraint violation. Yang et al. (2023) applied Conditional Value-at-Risk (CVaR) as an approximation of the quantile. Chow et al. (2018) proposed"}, {"title": "Preliminaries", "content": "A CMDP framework (Altman 2021) is characterized by a tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, r, c, d, \\gamma)$, where $\\mathcal{S}$ denotes the state space, $\\mathcal{A}$ denotes the action space, $\\mathcal{P}(\\cdot | s, a)$ is the state transition probability function, $r(s, a)$ is the reward function, $c(s, a)$ is the cost function, $d$ is a given threshold, and $\\gamma \\in (0, 1)$ is the discount factor.\nThe agent interacts with the environment at each time step $t$ by observing the current state $s_t \\in \\mathcal{S}$ and selecting an action $a_t \\in \\mathcal{A}$, then receives a reward $r(s_t, a_t)$ as well as a cost $c(s_t, a_t)$. This agent focuses on learning a policy $\\pi_\\theta(\\cdot | s)$ parameterized by $\\theta$. The next states are generated by the state transition probability function $\\mathcal{P}(\\cdot | s, a)$ and the policy $\\pi_\\theta(\\cdot | s)$. Given an initial state $s_0$, the cumulative return is defined as $R(s_0, \\pi_\\theta) = \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)$, and the cumulative cost is defined as $C(s_0, \\pi_\\theta) = \\sum_{t=0}^{\\infty} \\gamma^t c(s_t, a_t)$.\nBased on the previous definitions, CMDP framework is established to find a policy $\\pi_\\theta$ that maximizes the expected return while satisfying the expected cost under a given threshold $d$:\n$\\max_\\theta V(s, \\pi_\\theta) = \\mathbb{E}\\_{\\pi_\\theta} [R(s, \\pi_\\theta)] := \\mathbb{E}\\_{\\pi_\\theta} [\\sum_t \\gamma^t r(s_t, a_t)]$  \ns.t. $\\mathbb{E}\\_{\\pi_\\theta} [C(s, \\pi_\\theta)] = \\mathbb{E}\\_{\\pi_\\theta} [\\sum_t \\gamma^t c(s_t, a_t)] \\leq d$ (4)\n\nwhere $V(s, \\pi_\\theta)$ denotes the state value function. The expectation term of constraints matches the same term of the objective, therefore the gradient of the constraint can be calculated in a similar way, resulting in easy implementation"}, {"title": "Quantile-constrained RL", "content": "For a sequence of samples $\\{s_0, s_1, ..., s_N\\}$, the cumulative cost $C(s_i, \\pi_\\theta)$ of each state $s_i$ follows the empirical distribution $F(\\cdot; \\pi_\\theta)$. Given a probability level $\\epsilon \\in (0, 1)$, similar to Eqn. (3) with the cumulative cost $C(s, \\pi_\\theta)$ following the distribution $F(\\cdot; \\pi_\\theta)$, the $1 - \\epsilon$ quantile of $C(s, \\pi_\\theta)$ can be rewritten as:\n\n$q_{1-\\epsilon}(\\pi_\\theta) := \\inf\\{q | \\text{Pr}(C(s, \\pi_\\theta) \\leq q) = F(q; \\pi_\\theta) \\geq 1 - \\epsilon\\}$ (5)\n\nWhen the quantile of the distribution of $C$ satisfies $q_{1-\\epsilon}(\\pi_\\theta) \\leq d$, the probability of constraint violation is under $\\epsilon$. Therefore, the quantile-constrained RL problem can be formulated as follows:\n\n$\\max_\\theta V(s, \\pi_\\theta) = \\mathbb{E}\\_{\\pi_\\theta} [R(s, \\pi_\\theta)] = \\mathbb{E}\\_{\\pi_\\theta} [\\sum_t \\gamma^t r(s_t, a_t)]$\ns.t. $q_{1-\\epsilon} (\\pi_\\theta) \\leq d$ (6)\n\nSince the quantile $q_{1-\\epsilon} (\\pi_\\theta)$ is not of an expectation form, the conventional expectation Bellman equation cannot solve the gradient calculation of the quantile constraint. How to estimate quantile gradients become a challenge in this problem setting."}, {"title": "Methodology", "content": "In this section, we use a sample-based approach to estimate the gradient of the quantile constraint, and then construct a tilted quantile gradient update to accelerate the training process. Finally, we proposed a quantile-constrained RL algorithm based on the tilted quantile gradient update."}, {"title": "Estimating Quantile Gradients Through Sampling", "content": "Consider a quantile constraint as Eqn. (5). When $F(\\cdot; \\pi_\\theta)$ is continuous and differentiable (a minor assumption), the quantile $q_{1-\\epsilon} (\\pi_\\theta)$ is mathematically the inverse of $F(\\cdot; \\pi_\\theta)$. According to the inverse function theorem, the gradient of this quantile constraint can be calculated as follows:\n\n$\\nabla_\\theta q_{1-\\epsilon}(\\pi_\\theta) = \\nabla_\\theta F^{-1}(1-\\epsilon; \\pi_\\theta) =  \\frac{\\nabla_\\theta F(q; \\pi_\\theta)}{f(q; \\pi_\\theta)} |_{q=q_{1-\\epsilon}(\\pi_\\theta)}$ (7)\n\nwhere $f(\\cdot; \\pi_\\theta)$ is the probability density function (PDF) of the cumulative cost $C$. Eqn. (7) implies that the quantile gradient can be estimated by figuring out the above numerator and denominator.\nThe numerator $\\nabla_\\theta F(q; \\pi_\\theta)$ can be estimated by the likelihood ratio method in policy gradient algorithms. Given a batch of samples $\\{s_0, s_1, ..., s_N\\}$, the gradient of the CDF"}, {"title": "Tilted Quantile Gradient Update", "content": "However, a direct use of Eqn. (13) with a fixed $\\eta$ to update $\\lambda$ can be inefficient due to the overshooting of $\\lambda$ at the early stage of training. The reason is that the quantile $q_{1-\\epsilon}(\\pi_\\theta)$ may have an asymmetric distributional density around the threshold $d$. We illustrate this issue as follows.\nAssuming a given violation probability level $\\epsilon$, at the early stage of training, the initial policy may behave unsafe with violation probability much larger than $\\epsilon$, as well as $q_{1-\\epsilon}(\\pi_\\theta)$ several times larger than the threshold $d$, resulting in a large increase of $\\lambda$. This large increase makes $\\lambda$ overshoot to a large value at the early training, which may result in over-conservatism of the policy (Peng et al. 2022).\nLater when the policy satisfies the constraint with $0 < q_{1-\\epsilon}(\\pi_\\theta) \\leq d$, $\\lambda$ starts to decrease with a slower rate. Even for an absolute safe policy with $q_{1-\\epsilon}(\\pi_\\theta) = 0$ where $\\lambda$ decreases fastest, the decrease of $\\lambda$ is still slow as $\\Delta \\lambda = \\eta(q_{1-\\epsilon}(\\pi_\\theta) - d) = - \\eta d$, rather than the rapid increase earlier with $q_{1-\\epsilon} (\\pi_\\theta)$ several times larger than $d$. The slow decrease of $\\lambda$ from a large value may result in a slow recovery of the policy from over-conservatism. In general, this asymmetric distributional density of the quantile indicates that $q_{1-\\epsilon}(\\pi_\\theta) \\geq d$ is relatively large in the early stage of training, but not small enough at the later stage. This issue makes $\\lambda$ overshoot rapidly at the early unsafe training, while decrease slowly at the later over-conservatism stage, which may eventually result in slow convergence of the algorithm.\nTo address this issue, we propose a tilted quantile gradient update to compensate the asymmetric distributional density of $q_{1-\\epsilon} (\\pi_\\theta)$. Since we expect the distribution of $q_{1-\\epsilon} (\\pi_\\theta)$ to be symmetric around $d$, a tilted factor is designed to compensate the asymmetric distribution. Similar to the pinball loss in quantile regression (Steinwart and Christmann 2011), we revise the update rate $\\eta$ in Eqn. (13) with a tilted term defined as follows:\n\n$\\eta = \\begin{cases}\n\\eta^+ =  \\frac{F_q(d) + \\delta}{1 + \\delta}, & \\text{if } q_{1-\\epsilon}(\\pi_\\theta) \\geq d \\\\\n\\eta^- = \\frac{1 - F_q(d) + \\delta}{1 + \\delta}, & \\text{if } q_{1-\\epsilon} (\\pi_\\theta) < d\n\\end{cases}$ (14)\n\nwhere $F_q(d)$ denotes the CDF of the distribution of quantile $q_{1-\\epsilon}(\\pi_\\theta)$ at $d$, which is estimated by sampling per epoch. $\\eta^+$ and $\\eta^-$ are the update rates for the positive and negative tilted terms respectively, $\\delta \\in (0, 1)$ a small smoothing factor.\nThe tilted term in Eqn. (14) is utilized to update the Lagrangian multiplier $\\lambda$ in Eqn. (13). For example, in the early stage of training, the policy is always unsafe with $q_{1-\\epsilon}(\\pi_\\theta) \\geq d$, resulting in a small positive update rate $\\eta = \\eta^+ \\approx \\delta$. With the increase of $\\lambda$, the policy gradually satisfies the constraint with $q_{1-\\epsilon}(\\pi_\\theta) \\leq d$, then $\\lambda$ switches to decrease with a negative update rate $\\eta = \\eta^- \\approx 1 - \\delta$. Assuming $\\delta = 0.1$, the decrease update rate $\\eta \\approx 0.9$ will be about 9 times larger than the increase rate $\\eta^+ \\approx 0.1$. Therefore, the decrease of $\\lambda$ from a large value can be accelerated, with the tilted term compensating the asymmetric distributional density of $q_{1-\\epsilon}(\\pi_\\theta)$, which facilitates the recovery of the policy from over-conservatism.\nThe tilted term in Eqn. (14) is performed each epoch to update $\\lambda$ adaptively, boosts the decrease of $\\lambda$ from overshoot and tunes it to a more appropriate value range eventually, which can prevent the policy from over-conservatism and facilitate it to achieve higher return."}, {"title": "Tilted Quantile Policy Optimization", "content": "Based on the quantile gradient estimation with sampling and the tilted quantile gradient update, we propose an algorithm named Tilted Quantile Policy Optimization (TQPO) to solve the quantile-constrained RL problem in Eqn. (6). The algorithm is based on the classic RL algorithm Proximal Policy"}, {"title": "Convergence Analysis", "content": "In this section, we provide the theoretical proofs of the convergence of the TQPO algorithm. First, let's reconsider the update of the three main parameters in the TQPO algorithm, i.e., the estimated quantile $q$, the policy parameter $\\theta$ and the Lagrange multiplier $\\lambda$ in Eqn. (17). For the convenience of theoretical analysis, We modify Eqn. (17b) by replacing the implemented loss $L_\\theta$ to the Lagrange objective function $\\mathcal{L}(\\theta, \\lambda, q)$ in Eqn. (10). In order to prove the convergence, we first adopt the following assumptions:\nAssumption 1. For any probability level $\\epsilon \\in (0, 1)$, the objective function $\\mathcal{L}(\\theta, \\lambda, q)$ is continuous and differentiable with respect to $\\theta$.\nAssumption 2. $\\nabla_\\theta \\mathcal{L}(\\theta, \\lambda, q)$ is Lipschitz continuous w.r.t. $\\theta, \\lambda$ and $q$, i.e., $\\forall (\\theta_1, \\lambda_1, q_1), (\\theta_2, \\lambda_2, q_2) \\in \\Theta \\times \\mathbb{R}^+ \\times \\mathbb{R}$, there exists a constant $\\kappa$ such that $|\\| \\nabla_\\theta \\mathcal{L}(\\theta_1, \\lambda_1, q_1) - \\nabla_\\theta \\mathcal{L}(\\theta_2, \\lambda_2, q_2) |\\| \\leq \\kappa || (\\theta_1, \\lambda_1, q_1) - (\\theta_2, \\lambda_2, q_2) ||$, where $\\Theta$ is the parameter space of the policy network.\nAssumption 3. The update rates $\\alpha_k$, $\\beta_k$, and $\\eta_k$ are all positive, nonsummable, and square summable. In specific, this indicates that $\\alpha_k > 0, \\sum_{k=0}^{\\infty} \\alpha_k = \\infty, \\sum_{k=0}^{\\infty} \\alpha_k^2 < \\infty$. Moreover, the update rates satisfy: $\\eta_k = o(\\beta_k), \\beta_k = o(\\alpha_k)$.\nAssumption 1 is a common condition in continuous optimization, which ensures the continuity of the objective function w.r.t the policy parameter $\\theta$. Assumption 2 and 3 are standard conditions for stochastic approximation analysis (Borkar 2008; Gattami, Bai, and Aggarwal 2021).\nAssumption 3 indicates the update timescales of the quantile $q$, the policy parameter $\\theta$, and the Lagrange multiplier $\\lambda$, respectively, where $q$ is updated fastest, followed by $\\theta$, and $\\lambda$ is the slowest. The three parameters affect each other in the updating but with different timescales. Therefore, we can utilize the timescale separation to conduct the proof by two steps, first proving the convergence of $(\\theta, q)$, and then proving the convergence of $(\\theta, q, \\lambda)$.", "Convergence of ($\theta$, q)": "content\": \"First, we consider the convergence of the quantile $q$ and the policy parameter $\\theta$ in the TQPO algorithm. Since $\\lambda$ updates slower than $q$ and $\\theta$, we can regard $\\lambda$ as an arbitrary constant in the timescale of $q$ and $\\theta$.\nConsidering the updates of $q$ and $\\theta$ in Eqn. (17a) and Eqn. (17b), the two recursions are expected to track two coupled odinary differential equations (ODEs) with respect to $q$ and $\\theta$:\n$\\dot{q}(t) = g_1(\\theta, q) := \\hat{q}_{1-\\epsilon}(\\theta) - q$\n$\\dot{\\theta}(t) = g_2(\\theta, q) := \\nabla_\\theta \\mathcal{L}(\\theta, q)$ (18)\nSince the update rate of $\\theta$ is slower than $q$, we can regard $\\theta$ as a constant when updating $q$.\nLemma 1. For any $\\theta \\in \\Theta$, the ODE $\\dot{q}(t) = g_1(\\theta, q)$ has the unique global asymptotically stable equilibrium $q_\\theta^*$.\nWith the support of Lemma 1, $\\{q_k\\}$ converges to the equilibrium of the ODE $\\dot{q}(t) = g_1(\\theta, q)$ for any $\\theta \\in \\Theta$. Then we focus on the convergence of $\\theta$. The gradient update of $\\theta$ in Eqn. (17b) can be considered as tracking the right-hand side of the ODE in Eqn. (18). Therefore, we adopt the following theorem to prove $\\{\\theta_k\\}$ converge to the unique global asymptotically stable equilibrium.\nTheorem 1. For the two coupled iterations:(Borkar 1997)\n$q_{k+1} = q_k + \\alpha_k (g_1(\\theta_k, q_k) + m_k)$\n$\\theta_{k+1} = \\theta_k + \\beta_k (g_2(\\theta_k, q_k) + n_k)$ (19)\nfor $k \\geq 0$, where,\n(i): $g_1(\\theta, q)$ and $g_2(\\theta, q)$ are Lipschitz continuous\n(ii): $\\alpha_k$ and $\\beta_k$ satisfy Assumption 3\n(iii): $m_k$ and $n_k$ are noise sequences and satisfy $\\sum_{k=0}^{\\infty} \\alpha_k m_k, \\sum_{k=0}^{\\infty} \\beta_k n_k < \\infty$\nIf $\\forall \\theta \\in \\Theta$, ODE $\\dot{q}(t) = g_1(\\theta, q)$ has a unique global asymptotically stable equilibrium point $q_\\theta^*$, the iterations 19 converge to the unique global asymptotically stable equilibrium of the ODE $\\dot{\\theta}(t) = g_2(\\theta, q)$ a.s. on the set $\\sup_k |q_k| < \\infty$.\nTheorem 1 requires $\\{q_k\\}$ and the log gradient of $\\pi_\\theta$ to be bounded, which can be guaranteed by the following lemma and assumption respectively.\nLemma 2. If Assumptions 1, 3 hold, the sequence $\\{q_k\\}$ satisfies $\\sup_k |q_k| < \\infty$."}, {"title": "Experiments", "content": "We evaluate the proposed TQPO on three classic safe RL tasks: SimpleEnv, DynamicEnv and GremlinEnv from Mujoco and Safety Gym (Todorov, Erez, and Tassa 2012; Ray, Achiam, and Amodei 2019).\nIn these tasks, a robot (red) is required to reach a goal while avoiding collisions with obstacles. The complexity of the three tasks gradually increases due to the addition of randomness.\nThe reward is defined as the distance reduction from the robot to the goal over two time steps. When the robot reaches the goal, the environment provide an additional reward +1 and moves the goal to a fixed (Fig. 1a) or random position (Fig. 1b, 1c). When the robot collides with other objects, it receives a cost +1 at this time step, otherwise the cost is 0. This reward and cost shaping encourages the robot to reach the goal as many times as possible while avoiding collisions in an episode of 1000 steps.\nWe compare the proposed TQPO with the state-of-the-art quantile-constrained RL algorithm QCPO in (Jung et al. 2022) and a classic expectation-constrained RL method PPO-Lag in (Stooke and Abbeel 2019). Four metrics are used to evaluate the performance of the algorithms: Average episode return, safety probability, average episode cost and $1-\\epsilon$ quantile of cost. The cost threshold is set to $d = 15$. Since many safety-critical applications require a high safety probability above 90%, $1-\\epsilon = 90\\%, 95\\%$ are used in the experiments. All the experiments are conducted with five random seeds, with the solid line representing the mean and shaded area indicating the standard deviation. Implementation details can be found in Appendix B.\nFirst, we prove that compared to the quantile constraint, the expectation constraint is not suitable for safety-critical scenarios. Fig. 2 shows the average cost (Row 1) and $1 - \\epsilon$ quantile of cost (Row 2). From Row 1, we can observe that PPO-Lag (green) satisfies its expectation constraint $\\mathbb{E}[C] < d$, with the average cost around the threshold (black line). However, as shown in Row 2, the 90% cost quantile of PPO-Lag significantly exceeds the threshold. In contrast, both QCPO (blue and purple) and TQPO (red and orange) satisfy their quantile constraints $q_{1-\\epsilon}(\\pi_\\theta) \\leq d$, with their cost quantile around the threshold (black line) in Row 2, and their average cost is below that of PPO-Lag in Row 1. This difference in cost between the expectation-based method and the quantile-constrained methods results in the difference in safety probability. Fig. 3 Row 2 demonstrates that the safety probability of PPO-Lag is below 70%, which is significantly lower than that of QCPO and TQPO. As discussed in the introduction, the expectation constraint fails to bound the safety probability, resulting in the low safety probability of PPO-Lag. In comparison, QCPO and TQPO achieve higher safety probabilities closer to the given level (black line) in Fig. 3 Row 2. Therefore, in safety-critical scenarios with high safety probability requirements, the quantile constraint can achieve better safety performance and is more suitable than the expectation-based constraint.\nNext, we compare the two quantile-constrained methods.\nFor a more intuitive comparison, we evaluate the average performance of trained QCPO and TQPO algorithms, as shown in Table 1. Higher return and closer safety probability to the given level are preferred, as highlighted by the bolded values.\nFirst consider the safety probability of QCPO and TQPO. Fig. 3 Row 2 shows that both QCPO (blue for $1 - \\epsilon = 90\\%$, purple for $1 - \\epsilon = 95\\%$) and TQPO (red for $1 - \\epsilon = 90\\%$, orange for $1 - \\epsilon = 95\\%$) achieve safety probability close to the given level $1 - \\epsilon$ (black line). However, the curves of QCPO are more likely to be above $1 - \\epsilon$ rather than around the given probability level like TQPO. Table 1 (Pr Columns) also demonstrates that the safety probability of TQPO is closer to the given level. As mentioned before, QCPO assumes the cumulative cost follows a certain distribution and uses an additive form on $\\mathbb{E}[C]$ to approximate the quantile, which may lead to biased quantile estimation, resulting in higher safety probability. In contrast, TQPO avoids any distribution assumption and expectation-form approximation, directly estimates the quantile through a sampling technique. Results in Fig. 3 Row 2 and Table 1 demonstrate the safety probability of TQPO is closer to the given level, indicating the accuracy of our quantile gradient estimation method.\nNext we compare the return of QCPO and TQPO. Fig. 3 Row 1 shows that TQPO achieves higher return than QCPO in all tasks, with the return curves of TQPO above those of QCPO, especially in the case of high safety probability $1 - \\epsilon = 95\\%$ (orange for TQPO, purple for QCPO). Table 1 (R Columns) also demonstrates that TQPO outperforms QCPO with higher return. Notably, in SimpleEnv and DynamicEnv (Fig. 3 Row 1, Column 1&2), TQPO with a higher safety level $1 - \\epsilon = 95\\%$ (orange) may even outperform QCPO with a lower safety level $1 - \\epsilon = 90\\%$ (purple). The higher return of TQPO not only proves the effectiveness of the quantile estimation method but also demonstrates the advantage of the tilted quantile gradient update. Fig. 4 indicates that the tilted term compensates the asymmetric distributional density of the quantile, ensuring the tilted quantile symmetrically distributed around the threshold. Therefore the decrease of $\\lambda$ is boosted, facilitating its recovery from overshooting and avoiding the over-conservatism of the policy, which leads to higher return for TQPO within the same number of training epochs compared to QCPO.\nFurthermore, Directly estimating the quantile through sampling benefits TQPO with greater time efficiency compared to QCPO, which necessitates additional time for distribution fitting and quantile approximation. As shown in Table 1 (T Columns), the average training time for TQPO is approximately 10% shorter than that of QCPO. This indicates that TQPO not only surpasses QCPO in performance"}, {"title": "Ablation Study", "content": "Ablation studies are conducted on two tasks with three variants: QCPO with tilted update, TQPO w/o tilted update and TQPO with fixed tilted rates $\\eta^+ = 0.2, \\eta^- = 0.8$, as shown in Table 2. First, TQPO(w/o tilt) have higher return than QCPO, validating the effectiveness of the quantile gradient estimation. Second, QCPO(tilt) outperforms QCPO, while TQPO outperforms TQPO(w/o tilt) and TQPO(fixed tilt), indicating the benefit of proposed tilted update. The naive tilted method TQPO(fixed tilt) alleviates early overshooting of $\\lambda$, but leads to its undershooting later and a biased safety probability 92% eventually. Our tilted update calculates $\\eta$ each epoch to update $\\lambda$ adaptively, results in better safety probability 95%."}, {"title": "Conclusion", "content": "In this paper, we have developed a novel quantile-constrained RL model named Tilted Quantile Policy Optimization. This model applies sampling-based quantile gradient estimation for quantile constraints, and a tilted quantile gradient update strategy for higher return. We provide theoretical proofs of the convergence of this TQPO model, which converges to the optimal solution under certain conditions. Experiments on three classic safe RL tasks demonstrate the effectiveness of the proposed TQPO model, which satisfies all the quantile constraints and achieves higher return than the state-of-the-art benchmarks. Our future work will focus on extending this model to multi-agent RL applications and considering the application in real-world systems."}]}