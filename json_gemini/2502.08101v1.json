{"title": "Rethinking Tokenized Graph Transformers for Node Classification", "authors": ["Jinsong Chen", "Chenyang Li", "GaiChao Li", "John E. Hopcroft", "Kun He"], "abstract": "Node tokenized graph Transformers (GTs) have shown promising performance in node classification. The generation of token sequences is the key module in existing tokenized GTs which transforms the input graph into token sequences, facilitating the node representation learning via Transformer. In this paper, we observe that the generations of token sequences in existing GTs only focus on the first-order neighbors on the constructed similarity graphs, which leads to the limited usage of nodes to generate diverse token sequences, further restricting the potential of tokenized GTS for node classification. To this end, we propose a new method termed SwapGT. SwapGT first introduces a novel token swapping operation based on the characteristics of token sequences that fully leverages the semantic relevance of nodes to generate more informative token sequences. Then, SwapGT leverages a Transformer-based backbone to learn node representations from the generated token sequences. Moreover, SwapGT develops a center alignment loss to constrain the representation learning from multiple token sequences, further enhancing the model performance. Extensive empirical results on various datasets showcase the superiority of SwapGT for node classification.", "sections": [{"title": "1. Introduction", "content": "Node classification, the task of predicting node labels in a graph, is a fundamental problem in graph data mining with numerous real-world applications. Graph Neural Networks (GNNs) have traditionally been the dominant approaches. However, the message passing mechanism inherent to GNNs suffers from some limitations, such as over-smoothing, which prevents them from effectively capturing deep graph structural information and hinders their performance in downstream tasks.\nIn contrast, Graph Transformers (GTs), which adapt the Transformer framework for graph-based learning, have emerged as a promising alternative, demonstrating impressive performance in node classification. Existing GTs can be broadly classified into two categories based on their model architecture: hybrid GTs and tokenized GTs.\nHybrid GTs combine the strengths of GNN and Transformer, using GNNs to capture local graph topology and Transformers to model global semantic relationships. However, recent studies have highlighted a key issue with this approach: directly modeling semantic correlations among all node pairs using Transformers can lead to the over-globalization problem, which compromises model performance.\nTokenized GTs, on the other hand, generate independent token sequences for each node, which encapsulate both local topological and global semantic information. Transformer models are then utilized to learn node representations from these token sequences. The advantage of tokenized GTs is that they limit the token sequences to a small, manageable number of tokens, naturally avoiding the over-globalization issue. In tokenized GTs, the token sequences typically include two types of tokens: neighborhood tokens and node tokens. Neighborhood tokens aggregate multi-hop neighborhood information of a target node, while node tokens are sampled based on the similarity between nodes.\nHowever, recent studies have shown that neighborhood tokens often fail to preserve complex graph properties such as long-range dependencies and heterophily, limiting the richness of node representations. On the other hand, node tokens, generated through various sampling strategies, can better capture correlations between nodes in both feature and topological spaces, making them more effective in preserving complex graph information. As a result, this paper focuses on node token-based GTs.\nA recent study formalized the node token generation process as two key steps: similarity evaluation and top-k sampling. In the first step, similarity scores"}, {"title": "2. Relation Work", "content": ""}, {"title": "2.1. Graph Neural Networks", "content": "GNNs have shown remarkable performance in this task. Previous studies have primarily concentrated on the incorporation of diverse graph structural information into the message-passing framework. Classic deep learning techniques, such as the attention mechanism and residual connections, have been exploited to enhance the information aggregation on graphs. Moreover, aggregating information from high-order neighbors or nodes with high similarity across different feature spaces has been demonstrated to be efficacious in improving model performance.\nFollow-up GNNs have focused on the utilization of complex graph features to extract distinctive node representations. A prevalent strategy entails the utilization of signed aggregation weights to optimize the aggregation operation. In this way, positive and negative values are respectively associated with low- and high-frequency information, thereby enhancing the discriminative power of the learned node representations. Nevertheless, restricted by the inherent limitations of message-passing mechanism, the potential of GNNs for graph data mining has been inevitably weakened. Developing a new graph deep learning paradigm has attracted great attention in graph representation learning."}, {"title": "2.2. Graph Transformers", "content": "GTs have emerged as a novel architecture for graph representation learning and have exhibited substantial potential in node classification. A commonly adopted design paradigm for GTs is the combination of Transformer modules with GNN-style modules to construct hybrid neural network layers, called hybrid GTs. In this design, Transformer is employed to capture global information, while GNNs are utilized for local information extraction. Despite effectiveness, directly utilizing Transformer to model the interactions of all node pairs could occur the over-globalization issue , inevitably weakening the potential for graph representation learning.\nAn alternative yet effective design of GTs involves transforming the input graph into independent token sequences termed tokenized GTs, which are then fed into the Transformer layer for node representation learning. Neighborhood tokens and node tokens are two typical elements in existing tokenized GTs. The former, generally constructed by propagation approaches, such as random walk and personalized PageRank. The latter is generated by diverse sampling methods based different similarity measurements, such as PageRank score and attribute similarity. Since tokenized GTs only focus on the generated tokens, they naturally avoiding the over-globalization issue.\nAs pointed out in previous study, node token oriented GTs are more efficient in capturing various graph information, such as long-range dependencies and heterophily, compared to neighborhood token oriented GTs. However, we identify that previous methods only leverage a small subset of nodes as tokens for node representation learning, which could limit the model ability of deeply exploring graph information. In this paper, we develop a new method SwapGT that introduces a novel token swapping operation to produce more informative token sequences, further enhancing the model performance."}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Node Classification", "content": "Suppose an attributed graph is denoted as $G = (V, E, X)$ where V and E are the sets of nodes and edges in the graph. $X \\in \\mathbb{R}^{n \\times d}$ is the attribute feature matrix, where n and d are the number of nodes and the dimension of the attribute"}, {"title": "3.2. Transformer", "content": "Here, we introduce the design of the Transformer layer, which is the key module in most GTs. There are two core components of a Transformer layer, named multi-head self-attention (MSA) and feed-forward network (FFN). Given the model input $H^{n \\times d}$, the calculation of MSA is as follows:\n$MSA(H) = (||_{i=1}^{m} headi). W_o,$\n$headi = softmax(\\frac{[(HW_q) (HW_k)^T]}{\\sqrt{d_k}})(HW_v),$\nwhere $W_q$, $W_k$ and $W_v$ are the learnable parameter matrices of the i-th attention head. m is the number of attention heads. $||$ denotes the vector concatenation operation. $W_o$ denotes a projection layer to obtain the final output of MSA.\nFFN is constructed by two linear layers and one non-linear activation function:\n$FFN(H) = \\sigma(H. W^1) . W^2,$\nwhere $W^1$ and $W^2$ denote learnable parameters of the two linear layers and $\\sigma(\\cdot)$ denotes the GELU activation function."}, {"title": "4. Methodology", "content": "In this section, we detail our proposed SwapGT. Specifically, SwapGT conducts the token sampling operation based on different feature spaces to generate the initial token sequences. Then, SwapGT develops a novel token swapping operation that produces diverse token sequences based on the initial token sequences. Finally, SwapGT introduces a Transformer-based backbone with a center alignment loss to learn node representations from generated token sequences. The overall framework of SwapGT is shown in Figure 2."}, {"title": "4.1. Token Sampling", "content": "Token sampling, which aims to select relevant nodes from input graph to construct a token sequence for each target"}, {"title": "4.2. Token Swapping", "content": "As discussed in Figure 1, existing node token generators could be regarded as selecting the 1-hop neighborhood nodes in the constructed k-NN graph, which is inefficient in fully leveraging the semantic relevance between nodes\nTherefore, the selected node $v_{new}$ is within the 2-hop neighborhoods of $v_i$. Therefore, performing the swapping operation t times is equal to enlarge the sampling space from 1-hop neighborhood to (t + 1)-hop neighborhood. Hence, the swapping operation effectively utilize the semantic relevance between nodes to generate diverse token sequences. The overall algorithm of token swapping is summarized in Algorithm 1."}, {"title": "4.3. Transformer-based Backbone", "content": "The proposed Transformer-based backbone aims to learn node representations and predict the node labels according to the input token sequences. Take the input sequence $Z^A$ for example, we first utilize the projection layer to obtain the model input:\n$Z_i^{A,(0)} = \\rho(Z_i^A),$\nwhere $\\rho(.)$ denotes the projection layer and $Z_i^{A,(0)} \\in \\mathbb{R}^{(1+s)\\times k \\times d_0}$ denotes the model input of node $v_i$.\nThen, a Transformer layer-based encoder is applied to learn node representations from the model input:\n$Z_i^{A,(l)} = MSA(LN(Z_i^{A,(l-1)})) + Z_i^{A,(l-1)},$\n$Z_i^{A,(2)} = FFN(LN(Z_i^{A,(l)})) + Z_i^{A,(1)},$\nwhere l = 1,..., L indicates the l-th Transformer layer and LN(.) denotes the LayerNorm operation.\nThrough the encoder, we obtain the representations of input token sequences $Z_i^{A,(L)} \\in \\mathbb{R}^{(1+s)\\times (1+k) \\times d_1}$. Then, we take the representation of the first item in each token sequence as the final representation of the input sequence. This is because the first item in each token sequence is the target node itself, and the output representation has learned necessary information from other sampling tokens in the input sequence via the Transformer encoder.\nHence, the final output of the Transformer encoder is denoted as $Z_{A,i} \\in \\mathbb{R}^{(1+s)\\times d_L}$. Then, we utilize the following readout function to obtain the node representation learned from multi token sequences:\n$Z_i^{A} = ||_{j=1}^{s}(\\frac{1}{s} \\sum Z_{A,i}^{j}),$\nwhere $||$ denotes the concatenation operation, $Z_{A,i} \\in \\mathbb{R}^{1 \\times d_L}$ denotes the representation of $v_i$ learned from token sequences generated from the attribute feature space. Similarly, we can obtain $Z_i^{T}$ from the topology feature space.\nTo effectively utilize information of different feature spaces, we leverage the following strategy to fuse the learned representations:\n$Z_i = \\alpha \\cdot Z_i^{A}+ (1 - \\alpha) \\cdot Z_i^{T},$\nwhere $\\alpha \\in [0, 1]$ is a hyper-parameter to balance the contribution from attribute features and topology features on the final node representation.\nAt the end, we adopt Multi-Layer Perception-based predictor for label prediction and the cross-entropy loss for model training:\n$\\hat{Y} = MLP(Z_i),$\n$\\mathcal{L}_{ce} = - \\sum_{i \\in V_L} \\sum_{j=0}^{C} Y_{ij}\\ln \\hat{Y_{ij}}$"}, {"title": "4.4. Center Alignment Loss", "content": "To further enhance the model's generalization, we develop a center alignment loss to constrain the representations learned from different token sequences for each node"}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Dataset", "content": "We adopt eight widely used datasets, involving homophily and heterophily graphs: Photo , ACM, Computer, BlogCatalog, UAI2010, Flickr and Wiki-CS. The edge homophily ratio H(G) \u2208 [0,1] is adopted to evaluate the graph's homophily level. H(G) \u2192 1 means strong homophily, while H(G) \u2192 0 means strong heterophily. Statistics of datasets"}, {"title": "5.2. Baseline", "content": "We adopt eleven representative approaches as the baselines: SGC, APPNP, GPRGNN, FAGCN, BM-GCN, ACM-GCN, NAGphormer, SGFormer, Specformer, VCR-Graphormer and PolyFormer. The first six are mainstream GNNs and others are representative GTs."}, {"title": "5.3. Performance Comparison", "content": "To evaluate the model performance in node classification, we run each model ten times with random initializations. The results in terms of mean accuracy and standard deviation are reported in Table 1 and Table 2.\nFirst, we can observe that SwapGT achieves the best performance on all datasets with different data splitting strategies, demonstrating the effectiveness of SwapGT in node classification. Then, we can find that advanced GTs obtain more competitive performance than GNNs on over half datasets under dense splitting. But under sparse splitting, the situation reversed. An intuitive explanation is that Transformer has more learnable parameters than GNNs, which bring more powerful modeling capacity. However, it also requires more training data than GNNs in the training stage to ensure"}, {"title": "5.4. Study on the center alignment loss", "content": "The center alignment loss, proposed to constrain the representation learning from multiple token sequences, is a key design of SwapGT. Here, we validate the effectiveness of the center alignment loss in node classification. Specifically, we develop a variant of SwapGT by removing the center alignment loss, called SwapGT-O. Then, we evaluate the performance of SwapGT-O on all datasets under dense split-"}, {"title": "5.5. Study on the token sequence generation", "content": "The generation of token sequences is another key module of SwapGT, which develops a novel token swapping op-"}, {"title": "5.6. Analysis on the swapping times t", "content": "As discussed in Section 4.2, t determines the range of candidate tokens from the constructed k-NN graph, further affecting the model performance. To validate the influence of t on model performance, we vary t in {1,2,3,4} and observe the changes of model performance. Results are shown in Figure 6 and Appendix B.3. We can clearly observe that SwapGT can achieve satisfied performance on all datasets when t is no less than 2. This situation indicates that learning from tokens with semantic associations beyond the immediate neighbors can effectively enhancing the model performance. This phenomenon also reveals that reasonably enlarging the sampling space to seek more informative tokens is a promising way to improve the effect of node tokenized GTs."}, {"title": "5.7. Analysis on the augmentation times s", "content": "The augmentation times s determines how many token sequences are adopted for node representation learning. Similar to t, we vary s in {1,2,..., 8} and report the performance of SwapGT. Results are shown in Figure 7 and Appendix B.4. Generally speaking, sparse splitting requires a larger s to achieve the best performance, compared to dense splitting. This is because SwapGT needs more token sequences for model training in the sparse data scenario. This situation indicates that a tailored data augmentation strategy can effectively improve the performance of tokenized GTs when training data is sparse. Moreover, the optimal s varies on different graphs. This is because different graphs exhibit different topology features and attribute features, which affects the generation of token sequences, further influencing the model performance."}, {"title": "6. Conclusion", "content": "In this paper, we introduced a novel tokenized Graph Transformer SwapGT for node classification. In SwapGT, we developed a novel token swapping operation that flexibly swaps tokens in different token sequences, thereby generating diverse token sequences. This enhances the model's ability to capture rich node representations. Furthermore, SwapGT employs a tailored Transformer-based backbone with a center alignment loss to learn node representations from the generated multiple token sequences. The center alignment loss helps guide the learning process when nodes are associated with multiple token sequences, ensuring that the learned representations are consistent and informative. Experimental results demonstrate that SwapGT significantly improves node classification performance, outperforming several representative GT and GNN models."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of graph representation learning. There is none potential societal consequence of our work that must be specifically highlighted here."}]}