{"title": "Rolling in the deep of cognitive and Al biases", "authors": ["ATHENA VAKALI", "NICOLETA TANTALAKI"], "abstract": "Nowadays, we delegate many of our decisions to Artificial Intelligence (AI) that acts either in solo or as a human companion in decisions made to support several sensitive domains, like healthcare, financial services and law enforcement. Al systems, even carefully designed to be fair, are heavily criticized for delivering misjudged and discriminated outcomes against individuals and groups. Numerous work on AI algorithmic fairness is devoted on Machine Learning pipelines which address biases and quantify fairness under a pure computational view. However, the continuous unfair and unjust Al outcomes, indicate that there is urgent need to understand AI as a sociotechnical system, inseparable from the conditions in which it is designed, developed and deployed. Although, the synergy of humans and machines seems imperative to make AI work, the significant impact of human and societal factors on AI bias is currently overlooked. We address this critical issue by following a radical new methodology under which human cognitive biases become core entities in our Al fairness overview. Inspired by the cognitive science definition and taxonomy of human heuristics, we identify how harmful human actions influence the overall Al lifecycle, and reveal human to Al biases hidden pathways. We introduce a new mapping, which justifies the human heuristics to Al biases reflections and we detect relevant fairness intensities and inter-dependencies. We envision that this approach will contribute in revisiting Al fairness under deeper human-centric case studies, revealing hidden biases cause and effects.", "sections": [{"title": "INTRODUCTION", "content": "Al technology has invaded everyday life and activities, disrupting societal, economical, and policy making norms. AI acts either in solo or as a human companion in decisions made in so many domains: to provide health insights, approve loans, detect potential criminals and terrorists, screen University admissions and job applications, allo- cate digital content and so on [15, 21, 34]. The AI systems' ability to acquire cognitive skills such as perception, reasoning, and decision-making makes AI a valuable technology [15, 41]. However, the hypothesis that Al's mathematical and statistical nature will be more neutral, objective, and rational than humans, is not evident yet! In so many real-world cases there are several problems, associated with AI algorithms, based on the fact that they are often proven to be biased [36, 43, 46]. Up to now, bias and fairness terms are perplexed, studied under varying hypotheses and addressed with ad- hoc definitions and divergent goals. For example, in statistics, bias is a systematic error in the estimation of parameters or variables [64]. In this context, the effectiveness of AI systems was based on this definition and assessed based on how well their outputs correspond to their inputs. Thus, AI to be fair, meant that it had to deliver high accuracy based on the training data chosen and effectively generalize to unseen data. Apparently, the full spectrum of risks posed by bias in AI cannot be encompassed sufficiently by such a statistical perspective [55]. AI as a human mirroring technology, reflects both the fair and unfair patterns in society, and the perspectives of its creators, whether these perspectives are fair or biased [66]. In pursuit of Fairness in Artificial Intelligence (FairAI), a lot of research efforts explored the importance of data in mitigating bias and creating equitable outcomes. Conceptualizing \"fairness\" necessitated examining various data characteristics, like demographic information, from a technical perspective [16, 37]. Computational meth- ods, though, like data re-sampling or re-weighting prove to be not enough, are costly, and demand prerequisites"}, {"title": "HUMAN HEURISTICS AND BIASES BEHIND DECISION-MAKING", "content": "A recent NIST special publication about standards for bias and fairness, has positioned computational biases at the tip of a biases iceberg, emphasizing current research inefficiency to deeper study human and systemic (real world) biases [55]. It's now urgent to go deep in the bias iceberg, taking advantage of the long studied topics and the scientific evidence revealed by cognitive science about human cognition and biased opinion making. Identifying causes of cognitive biases, though, is too challenging. In the groundbreaking work of Tversky and Kahneman \"Judgment under Uncertainty\" back in 1974 researchers argued that a broad family of cognitive biases can be explainable in terms of three heuristics; representativeness, availability, and anchoring and adjustment [62] (also addressed in their collection of papers [12]). Later in 2002, in \"Heuristics and Biases: The Psychology if Intuitive Judgement\" [23], Thomas Gilovich, Dale Griffin, and Daniel Kahneman have pulled together a new collection of papers to revisit each of these three major general purpose heuristics, reinforcing their importance. The Nobel Prize in Economic Science awarded to Kahneman in 2002 is certainly one indicator of the tremendous impact of heuristics and biases research. Another major theme underlined in this collection ([23]) is the fact that intuition should also be explained considering the role of affect, mood, and emotion. The affect heuristic was introduced by Paul Slovic et al. [58] and later mentioned in more Kahneman's works [35, 36]. These heuristics are briefly summarized next, to inspire our new FairAI hupothesis testing base. Representativeness is about estimating the probability of an event by comparing it to a known situation or a prototype that already exists in someone's mind. People overweight certain information because they believe it's representative of reality. This may happen because this piece of information is more recent, or has been emphasized more in the media, or has been presented in a certain way. It's a mental shortcut that helps people group the world into simple categories like X and Y, but when a new experience or data rises, they cannot consider that it could be an entirely new category Z. The representativeness heuristic leads to the bias that makes people believe that a stereotype is true. A prominenet example of bias introduced by representativeness is the illusion of validity that refers to the overconfidence that is produced when we have a good fit between the predicted outcome and the input information [23, 35, 62]. Availability is a type of mental shortcut that involves assessing the frequency or plausibility of something based on how easily examples are brought to mind, without making an effort to look for other, potentially more significant information. Instances of large classes are usually recalled better and faster than instances of less frequent classes, likely occurrences are easier to imagine than unlikely ones, and the associative connections between events are strengthened when the events usually co-occur. People use this heuristic for estimating the number of elements in a class, the likelihood of an event, or the frequency of co-occurrences, by how easily the relevant mental operations of retrieval, construction, or association can be performed. A prominent bias that belongs in this category is the illusory correlation that makes people conclude that variables are correlated because their pairings come to mind easily (e.g. because they are quick to grasp, or because they seem likely) [23, 35, 62]. Anchoring and adjustment occurs when people base their initial ideas, estimates or predictions on a piece of information and make changes driven by this starting point. The anchoring bias refers exactly to this tendency of people to rely heavily on a reference point that is called \"anchor\" (and can be completely irrelevant e.g. an arbitrary number) and subsequently adjust that information until a final decision. Often, these adjustments are inadequate and remain close to the original anchor [62]. Automation bias is a well-known bias in the context of AI that belongs in this category. It occurs when individuals rely heavily on automated systems or technology to make a decision [35, 36]. Individuals may sometimes gravitate toward confirming their initial views which are the anchor, making insufficient adjustments, selectively gathering information that reinforces their starting point, giving rise to a well studied cognitive bias, known as confirmation bias [22, 39]. This type of bias is capable of involving both information processing and emotions as discussed below [14]. It should also be noted"}, {"title": "COMPUTATIONAL BIAS TYPES IN THE AI LIFECYCLE", "content": "Cognitive biases (we also refer to them as \"human biases\") described above, may enter the engineering and modeling processes and are ubiquitous in the decision making processes across the whole AI lifecycle, and in the use of AI applications once deployed [55]. In the context of AI, the typical approach of employing FairAI research and practices involves exploring specific bias types (known as \"computational bias\") throughout the Al lifecycle [18, 38, 42, 61, 63]. The following seven computational bias types are distinguished as the most prominent in the context of AI and the most studied in literature: \u2022 Historical bias results from practices that lead to a specific treatment towards certain social groups being advantaged or favored, while others may be disadvantaged or excluded. Biases in this category arise even if data is flawlessly sampled and can be present in the datasets used prior to the creation of the model. They are the result of the majority following existing rules or norms. Racial biases, gender biases, biases towards people with disabilities belong in this category [55, 62, 63]. \u2022 Representation bias occurs when building datasets to sample from a population. A non-representative dataset lacks the diversity of the population. The sample might be small, wrongly sampled, with a dis- tribution that differs from the true underlying distribution of the relevant population, and neglect un- derrepresented groups. Even with perfect a sampling technique, a model can have bad performance for the underrepresented groups dew to possible skewness of the underlying distribution. A model trained with fewer data points regarding a subgroup will not generalize well for it [42, 57, 61, 63]. Sampling bias mentioned in statistics belongs in this category and arises when the chosen sample represents a skewed subset of the target population [38, 61]. \u2022 Measurement bias arises from the choices made when choosing and computing particular features and labels. Usually, poor reflections are used as proxies of complex constructs. The method and accuracy of measurement may also vary across groups, found in the relevant population [38, 61]. As an example, using historical school performance as a proxy to predict the abilities of students, who could not take proper exams during COVID-19, introduced measurement bias that unfairly affected them. \u2022 Aggregation bias arises when false conclusions are drawn about individuals from observing the entire population. Such biases come up even when different subgroups are represented equally in the training data but are inappropriately combined. Any general assumptions about subgroups should be avoided [42, 61]. For example, models for diagnosing and monitoring diabetes have relied on Hemoglobin Alc (HbA1c) levels for predictions. However, a recent study revealed that HbA1c levels vary in complex ways across different ethnicities [7], making it likely that a single model for all populations would display aggregation bias."}, {"title": "SYSTEMATIC MAPPING BETWEEN HUMAN HEURISTICS AND COMPUTATIONAL BIASES", "content": "In our methodology, we explore specific human actions as sources of biases which are then spotted at the pre-processing, in-processing, and post-processing phases of the AI lifecycle [45]. The selected harmful actions re- sulted from our exploratory analysis of the most popular computational biases (Section 2) commonly highlighted in several recent surveys [18, 38, 42, 61, 63]. In tandem with the Information Commissioner's Office (ICO) Annex regarding \"Fairness in the AI lifecycle\" [45], that sets out potential sources of bias, we harvest the necessary in- formation around the decisions that may lead to unfair outcomes in the context of AI. Building on the grounded evidence that humans may take wrong or sub-optimal decisions, leading to problematic actions, based on the nec- essary but (often) biased four heuristics, we view computational biases as reflections of these human heuristics (Section 2). Then the explored human, i.e. cognitive biases are reflected to the prominent computational biases (Section 3), and an insightful mapping among them is provided. Our approach offers a solid scientific evidence coming from our rolling in the deep interdisciplinary study, which bridges cognitive and AI scientific knowledge, and reveals hidden human and AI ties which have been largely overlooked."}, {"title": "Pre-processing", "content": "In any typical AI pipeline, data must first be collected and a target population is identified, along with a set of features and labels chosen. Surely, the large scale of data, prohibits the inclusion of the entire data population, thus certain groups may be excluded or underrepresented, leading to inaccurate results that are harmful when making important decisions. Harmful action 1: Use of a non-appropriate (and non-representative) sample This action leads to results, applicable to a subset of the relevant population and not to the broader population itself. The chosen sample may seem to be representative of the relevant population, while it is not. The variability within the population may have not been taken into consideration, the size of the sample used may not be the appropriate or the method of selecting the sample may be uneven or limited [61]. Heuristics mapping: From a cognitive perspective, the representativeness and availability heuristic impact Al lifecycle at such a sampling step. For instance, it has been noted that during training, most AI face recognition algorithms, make use of pictures from people from developed countries, as people assume that these pictures are representative of the world population [41]. People consistently overestimate the proportion of the world that"}, {"title": "In-processing", "content": "This phase involves the model design, development, and evaluation. A model is trained to optimize a specified objective (e.g. minimize a loss function) and its quality is often measured on benchmarks i.e available datasets (different from the training data). What is needed is a model able to generalize on new, unseen data. The recent deployment of AI applications in social sensitive domains put forth a range of other important desiderata apart from accuracy, that models should be aiming for like fairness and privacy. Researchers use different benchmarks to estimate model's generalizability and select appropriate metrics and criteria that reflect the goals that had been initially set. Harmful action 1: Make inappropriate model design choices. Humans' model design choices regard aspects that range from the model's objective function to hyperparameter settings. What has to be understood is which model design choices disproportianately amplify error rates on features that may be underrepresented and possibly protected, raising fairness issues [29, 30]. Heuristics mapping: Decisions and who makes them at this phase can be driven by the representation, avail- ability and affect heuristic. Information may be flattened as analysts seek patterns and try to come up with easier problems to solve even without being aware of this substitution (guided by representativeness and avail- ability heuristics [11]. Certain objectives may rise as more important than others based on someone's existing beliefs and stereotypes that fill gaps in information. Consequently, choices give rise to harmful bias. Such choices are usually made in the absence of discriminatory intent. Simple models with fewer (and already available) pa- rameters may be preferred, as they tend to be less expensive to build, more explainable, and transparent. By approximating a real-life problem, which is complicated, by a much simpler model, makes outputs easier to understand and this might be tempting, but introduces bias [51, 55, 62]. The choice of models' desiderata and hyperparameters is another matter of crucial importance at this phase. An analyst may believe that it is im- perative to optimize for privacy guarantees in a health-care system based on their prior experience and inner beliefs, emphasizing the emotional importance of protecting personal information, but this can come at the cost of accuracy as discussed below [30]. Reflections: Several \u201cflattening\u201d techniques occur during inevitable mathematical abstractions leading to algorithmic bias. First of all, the choice of the model's objective function can lead to biased results for several sub-populations. Simple models fail to capture regularities in the data and tend to underfit the data. A model that does not generalize well for underrepresented groups causes harm and leads to discrimination for these underrepresented groups. In comparison, a model with high variance (and low statistical bias) may represent the data set accurately but could lead to overfitting to noisy or unrepresentative training data [30, 64]. Moreover, when it comes to the choice of a model's desiderata, there are several known trade-offs that have to be considered [19, 25, 30, 31, 68]. As an example, the loss function is used to evaluate how well the model fits the data. Loss on the training data has to be typically minimized, however, if the loss function is biased towards a specific group (e.g., white patients in a population), the relevant model will be better trained for this group. In this way, maximizing accuracy do not hold static other properties that are important like fairness [30, 67, 68]. As another example, differential privacy, which is the current state of the art in private machine learning, uses approaches like gradient clipping and noise injection. The cost of differential privacy, though, is a reduction in the model's accuracy and this can affect disproportionately underrepresented groups and model's fairness[2, 68]."}, {"title": "Post-processing", "content": "Post-processing steps regard the model's deployment and interpretation. There is also interaction of the AI model with the final users and this includes further learning and further development. The output of the model can be used as a new input to refine (e.g., through re-training) and (re)evaluate the algorithm creating what is known as \"feedback loop\". Without the feedback loops, AI systems would not be able to adapt to changing environments or improve their performance. However, AI algorithms can get influenced by their output, reinforcing and mag- nifying biases over time in several ways [47]. This self-perpetuating cycle of unfair decisions is resembled in literature to the butterfly effect [17]. Different types of biases are connected but understanding the dynamics"}, {"title": "TOWARDS A HUMAN AND AI BIASES TAXONOMY - CONCLUSIONS", "content": "In our technology-laden lives, AI systems even carefully designed to be fair, remain biased with human-societal harms spotted over the AI lifecycle. Current computational and accuracy-based fair AI solutions fail to encom- pass sufficiently the full spectrum of risks posed by biases. Recognizing the necessity of a future where humans and Al work in synergy, this work offers a systematic mapping among the ways humans' heuristics lead to wrong or sub-optimal decisions which then are reflected in the AI detected computational biases. Biases intertwine, causing cascading effects on the final AI system's performance. Biases in the collected datasets can lead to under-representation or exclusion of certain groups, resulting in discriminatory behaviors. An Al model, though, does not merely reflect existing biases in the data; humans' subjective choices regarding the act of selecting or computing features and model design choices during development can lead to measurement and algorithmic bias. Consequently, if a model is not assessed on a diverse dataset in real-world settings, some populations may be disproportionately impacted. Additionally, evaluation biases can emerge if inappropriate metrics are used. Finally, aggregation and deployment biases may arise if end-users are not adequately trained or supported regarding the right use of the model, further perpetuating a cycle of biased feedback loops. These harmful actions stem from humans' heuristics that result in computational biases. The representative- ness and availability heuristic seem to be responsible for the most types of computational biases like represen- tation, measurement, algorithmic and evaluation bias. The representativeness heuristic is also responsible for actions that result from people's bigotry and result in historical bias. The role of affect, mood and emotion should also not be neglected. This automatic response from the intuitive system can generate judgements able to gen- erate measurement, aggregation, deployment and even algorithmic bias. Anchoring and adjustment seems to be more manifest when a model is evaluated or deployed as an \u201canchor\u201d becomes available. Figure 1 illustrates a preliminary taxonomy of this heuristics-computational bias mapping, offering a new line of FairAI hypothesis setting and testing. With numerous cognitive biases that interfere with how people process data and think critically and several frameworks developed to identify computational biases in AI [38, 42, 61, 63], this work can't be exhaustive. Nonetheless, it constitutes a scientifically evidenced human and AI biases alignment summary, explanatory of the ways biases are perpetuated and amplified, and sheds light to prominent risks and vulnerabilities to consider when designing, developing, deploying, and evaluating FairAI solutions. Our analysis does not prove that other researchers' explanations or assumptions are wrong. Indeed, the biases in FairAI lifecycle may be a result of several factors, and even deeper study will need to explore the complicated and complex biases \"iceberg\". In summary, we claim that we offer a stepping stone to widen the study of human biases role in FairAI by revealing possible cognitive-computational biases intensities and inter-dependencies, largely uknown up to now. Our ongoing work on FairAI focuses on extending this mapping to a more holistic human-inclusive taxonomy which will be theoretically grounded and experimentatlly tested, under varying and bias-sensitive constraints posed in several critical FairAI domains."}]}