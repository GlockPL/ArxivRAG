{"title": "Rolling in the deep of cognitive and AI biases", "authors": ["ATHENA VAKALI", "NICOLETA TANTALAKI"], "abstract": "Nowadays, we delegate many of our decisions to Artificial Intelligence (AI) that acts either in solo or as a human companion in decisions made to support several sensitive domains, like healthcare, financial services and law enforcement. Al systems, even carefully designed to be fair, are heavily criticized for delivering misjudged and discriminated outcomes against individuals and groups. Numerous work on AI algorithmic fairness is devoted on Machine Learning pipelines which address biases and quantify fairness under a pure computational view. However, the continuous unfair and unjust Al outcomes, indicate that there is urgent need to understand AI as a sociotechnical system, inseparable from the conditions in which it is designed, developed and deployed. Although, the synergy of humans and machines seems imperative to make AI work, the significant impact of human and societal factors on AI bias is currently overlooked. We address this critical issue by following a radical new methodology under which human cognitive biases become core entities in our Al fairness overview. Inspired by the cognitive science definition and taxonomy of human heuristics, we identify how harmful human actions influence the overall Al lifecycle, and reveal human to Al biases hidden pathways. We introduce a new mapping, which justifies the human heuristics to Al biases reflections and we detect relevant fairness intensities and inter-dependencies. We envision that this approach will contribute in revisiting Al fairness under deeper human-centric case studies, revealing hidden biases cause and effects.", "sections": [{"title": "INTRODUCTION", "content": "Al technology has invaded everyday life and activities, disrupting societal, economical, and policy making norms.\nAI acts either in solo or as a human companion in decisions made in so many domains: to provide health insights,\napprove loans, detect potential criminals and terrorists, screen University admissions and job applications, allo-\ncate digital content and so on [15, 21, 34]. The AI systems' ability to acquire cognitive skills such as perception,\nreasoning, and decision-making makes AI a valuable technology [15, 41]. However, the hypothesis that Al's\nmathematical and statistical nature will be more neutral, objective, and rational than humans, is not evident yet!\nIn so many real-world cases there are several problems, associated with AI algorithms, based on the fact that\nthey are often proven to be biased [36, 43, 46].\nUp to now, bias and fairness terms are perplexed, studied under varying hypotheses and addressed with ad-\nhoc definitions and divergent goals. For example, in statistics, bias is a systematic error in the estimation of\nparameters or variables [64]. In this context, the effectiveness of AI systems was based on this definition and\nassessed based on how well their outputs correspond to their inputs. Thus, AI to be fair, meant that it had to\ndeliver high accuracy based on the training data chosen and effectively generalize to unseen data. Apparently, the\nfull spectrum of risks posed by bias in AI cannot be encompassed sufficiently by such a statistical perspective [55].\nAI as a human mirroring technology, reflects both the fair and unfair patterns in society, and the perspectives\nof its creators, whether these perspectives are fair or biased [66].\nIn pursuit of Fairness in Artificial Intelligence (FairAI), a lot of research efforts explored the importance of data\nin mitigating bias and creating equitable outcomes. Conceptualizing \"fairness\" necessitated examining various\ndata characteristics, like demographic information, from a technical perspective [16, 37]. Computational meth-\nods, though, like data re-sampling or re-weighting prove to be not enough, are costly, and demand prerequisites"}, {"title": "HUMAN HEURISTICS AND BIASES BEHIND DECISION-MAKING", "content": "A recent NIST special publication about standards for bias and fairness, has positioned computational biases at\nthe tip of a biases iceberg, emphasizing current research inefficiency to deeper study human and systemic (real\nworld) biases [55]. It's now urgent to go deep in the bias iceberg, taking advantage of the long studied topics\nand the scientific evidence revealed by cognitive science about human cognition and biased opinion making.\nIdentifying causes of cognitive biases, though, is too challenging. In the groundbreaking work of Tversky and\nKahneman \"Judgment under Uncertainty\" back in 1974 researchers argued that a broad family of cognitive biases\ncan be explainable in terms of three heuristics; representativeness, availability, and anchoring and adjustment\n[62] (also addressed in their collection of papers [12]). Later in 2002, in \"Heuristics and Biases: The Psychology\nif Intuitive Judgement\" [23], Thomas Gilovich, Dale Griffin, and Daniel Kahneman have pulled together a new\ncollection of papers to revisit each of these three major general purpose heuristics, reinforcing their importance.\nThe Nobel Prize in Economic Science awarded to Kahneman in 2002 is certainly one indicator of the tremendous\nimpact of heuristics and biases research. Another major theme underlined in this collection ([23]) is the fact that\nintuition should also be explained considering the role of affect, mood, and emotion. The affect heuristic was\nintroduced by Paul Slovic et al. [58] and later mentioned in more Kahneman's works [35, 36]. These heuristics\nare briefly summarized next, to inspire our new FairAI hupothesis testing base.\nRepresentativeness is about estimating the probability of an event by comparing it to a known situation or\na prototype that already exists in someone's mind. People overweight certain information because they believe\nit's representative of reality. This may happen because this piece of information is more recent, or has been\nemphasized more in the media, or has been presented in a certain way. It's a mental shortcut that helps people\ngroup the world into simple categories like X and Y, but when a new experience or data rises, they cannot\nconsider that it could be an entirely new category Z. The representativeness heuristic leads to the bias that\nmakes people believe that a stereotype is true. A prominenet example of bias introduced by representativeness\nis the illusion of validity that refers to the overconfidence that is produced when we have a good fit between the\npredicted outcome and the input information [23, 35, 62].\nAvailability is a type of mental shortcut that involves assessing the frequency or plausibility of something\nbased on how easily examples are brought to mind, without making an effort to look for other, potentially more\nsignificant information. Instances of large classes are usually recalled better and faster than instances of less\nfrequent classes, likely occurrences are easier to imagine than unlikely ones, and the associative connections\nbetween events are strengthened when the events usually co-occur. People use this heuristic for estimating the\nnumber of elements in a class, the likelihood of an event, or the frequency of co-occurrences, by how easily\nthe relevant mental operations of retrieval, construction, or association can be performed. A prominent bias\nthat belongs in this category is the illusory correlation that makes people conclude that variables are correlated\nbecause their pairings come to mind easily (e.g. because they are quick to grasp, or because they seem likely)\n[23, 35, 62].\nAnchoring and adjustment occurs when people base their initial ideas, estimates or predictions on a piece\nof information and make changes driven by this starting point. The anchoring bias refers exactly to this tendency\nof people to rely heavily on a reference point that is called \"anchor\" (and can be completely irrelevant e.g. an\narbitrary number) and subsequently adjust that information until a final decision. Often, these adjustments are\ninadequate and remain close to the original anchor [62]. Automation bias is a well-known bias in the context\nof AI that belongs in this category. It occurs when individuals rely heavily on automated systems or technology\nto make a decision [35, 36]. Individuals may sometimes gravitate toward confirming their initial views which\nare the anchor, making insufficient adjustments, selectively gathering information that reinforces their starting\npoint, giving rise to a well studied cognitive bias, known as confirmation bias [22, 39]. This type of bias is\ncapable of involving both information processing and emotions as discussed below [14]. It should also be noted"}, {"title": "COMPUTATIONAL BIAS TYPES IN THE AI LIFECYCLE", "content": "Cognitive biases (we also refer to them as \"human biases\") described above, may enter the engineering and\nmodeling processes and are ubiquitous in the decision making processes across the whole AI lifecycle, and in\nthe use of AI applications once deployed [55]. In the context of AI, the typical approach of employing FairAI\nresearch and practices involves exploring specific bias types (known as \"computational bias\") throughout the\nAl lifecycle [18, 38, 42, 61, 63]. The following seven computational bias types are distinguished as the most\nprominent in the context of AI and the most studied in literature:\n\u2022 Historical bias results from practices that lead to a specific treatment towards certain social groups\nbeing advantaged or favored, while others may be disadvantaged or excluded. Biases in this category\narise even if data is flawlessly sampled and can be present in the datasets used prior to the creation of\nthe model. They are the result of the majority following existing rules or norms. Racial biases, gender\nbiases, biases towards people with disabilities belong in this category [55, 62, 63].\n\u2022 Representation bias occurs when building datasets to sample from a population. A non-representative\ndataset lacks the diversity of the population. The sample might be small, wrongly sampled, with a dis-\ntribution that differs from the true underlying distribution of the relevant population, and neglect un-\nderrepresented groups. Even with perfect a sampling technique, a model can have bad performance for\nthe underrepresented groups dew to possible skewness of the underlying distribution. A model trained\nwith fewer data points regarding a subgroup will not generalize well for it [42, 57, 61, 63]. Sampling bias\nmentioned in statistics belongs in this category and arises when the chosen sample represents a skewed\nsubset of the target population [38, 61].\n\u2022 Measurement bias arises from the choices made when choosing and computing particular features and\nlabels. Usually, poor reflections are used as proxies of complex constructs. The method and accuracy of\nmeasurement may also vary across groups, found in the relevant population [38, 61]. As an example,\nusing historical school performance as a proxy to predict the abilities of students, who could not take\nproper exams during COVID-19, introduced measurement bias that unfairly affected them.\n\u2022 Aggregation bias arises when false conclusions are drawn about individuals from observing the entire\npopulation. Such biases come up even when different subgroups are represented equally in the training\ndata but are inappropriately combined. Any general assumptions about subgroups should be avoided\n[42, 61]. For example, models for diagnosing and monitoring diabetes have relied on Hemoglobin Alc\n(HbA1c) levels for predictions. However, a recent study revealed that HbA1c levels vary in complex ways\nacross different ethnicities [7], making it likely that a single model for all populations would display\naggregation bias."}, {"title": "SYSTEMATIC MAPPING BETWEEN HUMAN HEURISTICS AND COMPUTATIONAL BIASES", "content": "In our methodology, we explore specific human actions as sources of biases which are then spotted at the pre-\nprocessing, in-processing, and post-processing phases of the AI lifecycle [45]. The selected harmful actions re-\nsulted from our exploratory analysis of the most popular computational biases (Section 2) commonly highlighted\nin several recent surveys [18, 38, 42, 61, 63]. In tandem with the Information Commissioner's Office (ICO) Annex\nregarding \"Fairness in the AI lifecycle\" [45], that sets out potential sources of bias, we harvest the necessary in-\nformation around the decisions that may lead to unfair outcomes in the context of AI. Building on the grounded\nevidence that humans may take wrong or sub-optimal decisions, leading to problematic actions, based on the nec-\nessary but (often) biased four heuristics, we view computational biases as reflections of these human heuristics\n(Section 2). Then the explored human, i.e. cognitive biases are reflected to the prominent computational biases\n(Section 3), and an insightful mapping among them is provided. Our approach offers a solid scientific evidence\ncoming from our rolling in the deep interdisciplinary study, which bridges cognitive and AI scientific knowledge,\nand reveals hidden human and AI ties which have been largely overlooked.\n4.1 Pre-processing\nIn any typical AI pipeline, data must first be collected and a target population is identified, along with a set of\nfeatures and labels chosen. Surely, the large scale of data, prohibits the inclusion of the entire data population,\nthus certain groups may be excluded or underrepresented, leading to inaccurate results that are harmful when\nmaking important decisions.\nHarmful action 1: Use of a non-appropriate (and non-representative) sample\nThis action leads to results, applicable to a subset of the relevant population and not to the broader population\nitself. The chosen sample may seem to be representative of the relevant population, while it is not. The variability\nwithin the population may have not been taken into consideration, the size of the sample used may not be the\nappropriate or the method of selecting the sample may be uneven or limited [61].\nHeuristics mapping: From a cognitive perspective, the representativeness and availability heuristic impact\nAl lifecycle at such a sampling step. For instance, it has been noted that during training, most AI face recognition\nalgorithms, make use of pictures from people from developed countries, as people assume that these pictures are\nrepresentative of the world population [41]. People consistently overestimate the proportion of the world that"}, {"title": "In-processing", "content": "This phase involves the model design, development, and evaluation. A model is trained to optimize a specified\nobjective (e.g. minimize a loss function) and its quality is often measured on benchmarks i.e available datasets\n(different from the training data). What is needed is a model able to generalize on new, unseen data. The recent\ndeployment of AI applications in social sensitive domains put forth a range of other important desiderata apart\nfrom accuracy, that models should be aiming for like fairness and privacy. Researchers use different benchmarks\nto estimate model's generalizability and select appropriate metrics and criteria that reflect the goals that had\nbeen initially set.\nHarmful action 1: Make inappropriate model design choices.\nHarmans' model design choices regard aspects that range from the model's objective function to hyperparameter\nsettings. What has to be understood is which model design choices disproportianately amplify error rates on\nfeatures that may be underrepresented and possibly protected, raising fairness issues [29, 30].\nHeuristics mapping: Decisions and who makes them at this phase can be driven by the representation, avail-\nability and affect heuristic. Information may be flattened as analysts seek patterns and try to come up with\neasier problems to solve even without being aware of this substitution (guided by representativeness and avail-\nability heuristics [11]. Certain objectives may rise as more important than others based on someone's existing\nbeliefs and stereotypes that fill gaps in information. Consequently, choices give rise to harmful bias. Such choices\nare usually made in the absence of discriminatory intent. Simple models with fewer (and already available) pa-\nrameters may be preferred, as they tend to be less expensive to build, more explainable, and transparent. By\napproximating a real-life problem, which is complicated, by a much simpler model, makes outputs easier to\nunderstand and this might be tempting, but introduces bias [51, 55, 62]. The choice of models' desiderata and\nhyperparameters is another matter of crucial importance at this phase. An analyst may believe that it is im-\nperative to optimize for privacy guarantees in a health-care system based on their prior experience and inner\nbeliefs, emphasizing the emotional importance of protecting personal information, but this can come at the cost\nof accuracy as discussed below [30].\nReflections: Several \u201cflattening\u201d techniques occur during inevitable mathematical abstractions leading to\nalgorithmic bias. First of all, the choice of the model's objective function can lead to biased results for several\nsub-populations. Simple models fail to capture regularities in the data and tend to underfit the data. A model\nthat does not generalize well for underrepresented groups causes harm and leads to discrimination for these\nunderrepresented groups. In comparison, a model with high variance (and low statistical bias) may represent\nthe data set accurately but could lead to overfitting to noisy or unrepresentative training data [30, 64].\nMoreover, when it comes to the choice of a model's desiderata, there are several known trade-offs that have\nto be considered [19, 25, 30, 31, 68]. As an example, the loss function is used to evaluate how well the model fits\nthe data. Loss on the training data has to be typically minimized, however, if the loss function is biased towards\na specific group (e.g., white patients in a population), the relevant model will be better trained for this group.\nIn this way, maximizing accuracy do not hold static other properties that are important like fairness [30, 67, 68].\nAs another example, differential privacy, which is the current state of the art in private machine learning, uses\napproaches like gradient clipping and noise injection. The cost of differential privacy, though, is a reduction in\nthe model's accuracy and this can affect disproportionately underrepresented groups and model's fairness[2, 68]."}, {"title": "Post-processing", "content": "Post-processing steps regard the model's deployment and interpretation. There is also interaction of the AI model\nwith the final users and this includes further learning and further development. The output of the model can be\nused as a new input to refine (e.g., through re-training) and (re)evaluate the algorithm creating what is known as\n\"feedback loop\". Without the feedback loops, AI systems would not be able to adapt to changing environments\nor improve their performance. However, AI algorithms can get influenced by their output, reinforcing and mag-\nnifying biases over time in several ways [47]. This self-perpetuating cycle of unfair decisions is resembled in\nliterature to the butterfly effect [17]. Different types of biases are connected but understanding the dynamics"}, {"title": "TOWARDS A HUMAN AND AI BIASES TAXONOMY - CONCLUSIONS", "content": "In our technology-laden lives, AI systems even carefully designed to be fair, remain biased with human-societal\nharms spotted over the AI lifecycle. Current computational and accuracy-based fair AI solutions fail to encom-\npass sufficiently the full spectrum of risks posed by biases. Recognizing the necessity of a future where humans\nand Al work in synergy, this work offers a systematic mapping among the ways humans' heuristics lead to\nwrong or sub-optimal decisions which then are reflected in the AI detected computational biases.\nBiases intertwine, causing cascading effects on the final AI system's performance. Biases in the collected\ndatasets can lead to under-representation or exclusion of certain groups, resulting in discriminatory behaviors.\nAn Al model, though, does not merely reflect existing biases in the data; humans' subjective choices regarding the\nact of selecting or computing features and model design choices during development can lead to measurement\nand algorithmic bias. Consequently, if a model is not assessed on a diverse dataset in real-world settings, some\npopulations may be disproportionately impacted. Additionally, evaluation biases can emerge if inappropriate\nmetrics are used. Finally, aggregation and deployment biases may arise if end-users are not adequately trained\nor supported regarding the right use of the model, further perpetuating a cycle of biased feedback loops."}]}