{"title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning", "authors": ["Tian Xie", "Zitian Gao", "Qingnan Ren", "Haoming Luo", "Yuqian Hong", "Bryan Dai", "Joey Zhou", "Kai Qiu", "Zhirong Wu", "Chong Luo"], "abstract": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.", "sections": [{"title": "Introduction", "content": "The post-training phase of large language models (LLMs) has advanced rapidly [19], with models like DeepSeek-R1 [3], Kimi-K1.5 [15], and OpenAI-01 [10] demonstrating remarkable reasoning abilities. DeepSeek-R1, in particular, introduced a simple yet effective rule-based reinforcement learning (RL) approach, enabling emergent reasoning patterns without relying on traditional scaffolding techniques such as Monte Carlo Tree Search (MCTS) [4, 6, 18, 20] or Process Reward Models (PRM) [8].\nDespite these advancements, substantial gaps remain in translating these developments into reproducible research. While DeepSeek-R1 provides open-source model weights, it does not release the corresponding training code or dataset. This absence raises critical questions: (1) Can similar reasoning abilities emerge in smaller-scale models? (2) What is the optimal training data structure for fostering such capabilities? (3) What methodologies can reliably replicate these results?\nAddressing these questions requires controlled experimental frameworks that isolate key variables. While mathematics is often treated as the common testbed for reasoning, widely-used math datasets like GSM8K [2] and Omini-MATH [5] suffer as the training data due to its uncontrolled variance in problem complexity, which may span various logical induction depths. To overcome this limitation, we leverage a procedurally generated Knights and Knaves (K&K) logic puzzle dataset [17], which allows controllable difficulty levels and ease of rule-based reward verification, making it ideal for studying reasoning dynamics.\nIn this paper, we introduce Logic-RL, a rule-based reinforcement learning framework that acquires R1-like reasoning patterns through training on logic puzzles. The training framework adopts the REINFORCE++ algorithm [7] and the reward designs from DeepSeek-R1 for post-training. While naive training would lead to collapsed solutions, we propose a practical system prompt and a stringent format reward to avoid the reasoning model for taking shortcuts. We also incorporate a few modifications to the REINFORCE++ algorithm for improved performance.\nAs the RL training undergoes, we observe that the model naturally allocates more training steps to reason. This computational expansion scales from generating hundreds to thousands of tokens, enabling deeper exploration and refinement of its thought processes. We evaluate the model performance on the challenging math benchmarks for reasoning. With merely 5,000 procedurally generated logic puzzles, our 7B model improves by 125% on AIME and 38% on AMC against the base model. This cross-domain generalization capability suggests that RL-trained reasoning heuristics develop abstract problem-solving schemata rather than relying on domain-specific pattern matching.\nBesides the technical contributions mentioned above, our study also makes several interesting findings:\n\u2022 Longer responses don't guarantee better reasoning. Length alone is not a valid performance metric for training time evaluation. The most efficient reasoning comes from the shortest path.\n\u2022 Language mixing hinders reasoning. This observation underscores the need for a language consistency penalty in reward modeling.\n\u2022 Increasing 'thinking' tokens do help. RL training naturally boosts the frequency of reflection-related words, suggesting a correlation between certain tokens' frequency and performance.\n\u2022 SFT memorizes; RL generalizes. SFT relies heavily on memorization, often leading to superficial shortcut learning, whereas RL self-evolves with minimal dependence on dataset structure.\n\u2022 Cold start is a bonus, not a necessity. Training dynamics remain surprisingly similar whether starting from a base or instruct model, though the latter exhibits slightly better performance.\n\u2022 Curriculum Learning still matters. Under a fixed data curation ratio, a well-designed curriculum learning approach always outperforms random shuffle."}, {"title": "Method", "content": ""}, {"title": "Data Synthesis", "content": "The Knights and Knaves (K&K) puzzles [17] constitute an algorithmically generated reasoning dataset. In these puzzles, characters are either knights, who always tell the truth, or knaves, who always lie. The objective is to determine the nature of each character based on their statements. This dataset is distinguished by its high degree of controllability:\n1. Procedural Generation: Puzzles are systematically generated using logic templates, ensuring both consistency and infinite variability. Importantly, these puzzles represent unseen data for the original model, making them ideal for testing generalization capabilities.\n2. Controlled Difficulty Levels: The difficulty of the puzzles can be precisely adjusted, enabling the design of a curriculum learning strategy. Difficulty is modulated by varying the number of characters (2\u20138) and the complexity of logical operations (1\u20134 combinations of Boolean operators). Furthermore, more complex puzzles can serve as out-of-distribution tests for models trained on simpler cases, providing insights into their ability to generalize.\n3. Ease of Verification: Each puzzle has a single, unambiguous ground truth answer, with correctness guaranteed by the generation algorithm. Solutions require strict deductive reasoning, allowing for accurate evaluation of model responses and minimizing the risk of reward hacking."}, {"title": "Rule Based Reward Modeling", "content": "The reward serves as the primary training signal in reinforcement learning (RL), guiding the optimization process. We continuously monitored hacking behaviors in the model's outputs, refining our reward design iteratively. This led to a nearly unhackable, rule-based reward system that comprises only two types of rewards: Format Reward and Answer Reward."}, {"title": "System Prompt", "content": "You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to solve a logical reasoning problem. After thinking, when you finally reach a conclusion, clearly state the identity of each character within <answer> </answer> tags. i.e., <answer> (1) Zoey is a knight, (2) ... </answer>."}, {"title": "Format Reward:", "content": "We use regular expression extraction to enforce a structured response format. The model is required to put its reasoning process within <think></think> tags and provide the final conclusion inside <answer></answer> tags. Additionally, we recommend including a <think> tag directly at the end of the prompt, which significantly reduces the difficulty for the base model to follow our instructions.\nUnder our early imperfect rule design, we consistently observed reward hacking phenomena, some of which are listed below:\n\u2022 Skipping the <think></think> process and directly answering.\n\u2022 Placing reasoning inside the <answer></answer> tag.\n\u2022 Repeatedly guessing answers without proper reasoning.\n\u2022 Including irrelevant nonsense in addition to providing the answer.\n\u2022 Organizing correct answer in a wrong manner for extraction.\n\u2022 Revisiting the thinking phase after already outputting an <answer> due to insufficient reasoning.\n\u2022 Repeating the original question or using phrases like \"thinking process here\" to avoid true reasoning.\nAccordingly, we iteratively refine our rule design. For example, each tag should appear exactly once and in the correct sequential order, the thinking process must include genuine reasoning, and the conclusion should be presented in an extractable and readable manner. By enforcing these constraints, we ensure that different actions receive appropriate rewards based on their adherence to the format. The format score ($S_{format}$) is computed as follows:\n$S_{format} = \\begin{cases} 1, & \\text{if format is correct} \\\\ -1, & \\text{if format is incorrect} \\end{cases}$"}, {"title": "Answer Reward:", "content": "The second component evaluates the correctness of the content in the model's response. Once the format is validated, we check if the model's answer matches the ground truth. The answer score ($S_{answer}$) is computed as:\n$S_{answer} = \\begin{cases} 2, & \\text{if the answer fully matches the ground truth} \\\\ -1.5, & \\text{if the answer partially mismatches the ground truth} \\\\ -2, & \\text{if the answer cannot be parsed or is missing} \\end{cases}$"}, {"title": "RL Algorithm", "content": "We adopt a modified version of REINFORCE++ as our baseline algorithm, which has demonstrated superior performance compared to GRPO in our experimental setup. A detailed comparison of these algorithms is provided in Section 4.\nReinforce Return Calculation: The discounted cumulative rewards for each trajectory are computed as below, where \u03b3 is the discount factor, set to 1 in our experiments:\n$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t}r_k$\nFollowing recommendations from DeepSeek-Math [13], we incorporate several minor refinements into the implementation of REINFORCE++."}, {"title": "First modification: Use KL Loss", "content": "The KL-divergence between the response distributions of the RL model and the SFT model is calculated for each token. This divergence is incorporated as a penalty term in the reward function of PPO during training. The per-token reward is defined as follows:\nr(st, at) = I(st = [EOS])r(x, y) \u2013 \u03b2KL(t),\nwhere I(st = [EOS]) is an identity function that evaluates to 1 when the <eos> token is reached, and B controls the weight of the KL penalty.\nIn contrast, the GRPO implementation does not include the KL-divergence as part of the reward function. Instead, it directly incorporates the KL-divergence into the loss function, arguing that this approach simplifies the computation and avoids unnecessary complexity. Following this rationale, we also use KL loss like GRPO:\n$I_{GRPO}(\\theta) = E_{q \\sim P(Q), o_i \\sim \\pi_{\\theta_{old}}(O|q)}[-\\sum_{i=1}^{G} \\sum_{t=1}^{O_i} min(\\frac{\\pi_{\\theta}^{A_{i,t}}}{{\\pi_{\\theta_{old}}^{A_{i,t}}}}, clip(\\frac{\\pi_{\\theta}^{A_{i,t}}}{{\\pi_{\\theta_{old}}^{A_{i,t}}}}, 1-\\epsilon, 1+\\epsilon)) B_{DKL}[\\pi_{\\theta} || \\pi_{ref}]]$"}, {"title": "Second Modification: KL Estimation", "content": "Another key distinction lies in how the KL-divergence is estimated. The default KL estimator for PPO is defined as follows:\n$KL(t) = log(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)})$\nIn contrast, GRPO employs an unbiased estimator for the KL-divergence, formulated as:\n$D_{KL}[\\pi_{\\theta}|| \\pi_{ref}] =  log\\frac{\\pi_{ref}(O_i, q, O_{i,<t})}{\\pi_{\\theta} (O_{i,t}|q, O_{i,<t})}-\\frac{\\pi_{ref}(O_i, q, O_{i,<t})}{\\pi_{\\theta} (O_{i}| q, O_{i,<t})}-1$.\nThis approach ensures that the KL estimate is always non-negative, whereas the original formulation may yield negative values. GRPO's estimator provides a more stable and reliable measure of divergence during training.\nAfter implementing these modifications, we proceed with our experiments."}, {"title": "Training Schedule", "content": "We directly train the model for 3600 steps with a constant learning rate of 4 \u00d7 10-7 and temperature parameter of 0.7. During training, the model is directly exposed to mixed complexity logic puzzles ranging from 3 to 7 people. This straightforward training regimen achieves competitive performance as shown in the final results table. Through continuous training with these fixed hyperparameters, the model develops stable reasoning patterns characterized by logical exploration, intermediate verification, and systematic summarization before producing final answers. These emergent behaviors demonstrate the model's capacity to handle complex logical reasoning tasks effectively.\nOther key parameters used during training are summarized in Table 1"}, {"title": "Experiment", "content": "We began by experimenting with various models from the Qwen2.5 series as potential baseline candidates. For instance, Qwen2.5-Math-7B exhibited a strong tendency to generate Python code blocks, which often conflicted with our strict formatting requirements. Despite efforts to mitigate this behavior by removing system prompts and penalizing specific markdown styles, it remained challenging to fully address.\nAdditionally, we tested both Qwen2.5-7B-Base and Qwen2.5-7B-Instruct as starting points. Surprisingly, we found that the base and instruct models displayed nearly identical training metrics during RL training, including validation accuracy, response length growth curves, and reward curves. A detailed comparison between base & instruct model can be found in the appendix 7. However, the instruct model demonstrated slightly higher test accuracy, making it the preferred choice. Consequently, we selected Qwen2.5-7B-Instruct-1M [22] as our baseline. See more base & instruct model training dynamics comparision in Appendix 10\nNotably, despite the training dataset being limited to 3 to 7-person K&K logic puzzles with fewer than 5,000 synthetic samples\u2014the model demonstrates a remarkable ability to generalize to out-of-distribution (OOD) scenarios, such as 8-person puzzles."}, {"title": "Research Questions", "content": ""}, {"title": "RQ 1: How Does GRPO Compare to Other RL Algorithms?", "content": "Does GRPO [13] outperform other reinforcement learning algorithms, such as REINFORCE++ and PPO, in terms of training stability, speed, and performance accuracy?"}, {"title": "RQ 2. Do certain thinking tokens and language-mixing phenemona improve reasoning?", "content": "Does the inclusion of complex reasoning behaviours (such as exploration, verification, summarization, and backtracking) and language switching improve the model's reasoning ability?"}, {"title": "RQ 3: Does an 'Aha Moment' Emerge During Training?", "content": "Is there an observable 'Aha moment' where the model exhibits a significant leap in reasoning capability, such as the emergence of multi-step verification or reflection during the RL process?\nThe emergence of sophisticated behaviors becomes increasingly evident as performance grows. These behaviors include reflective actions\u2014where the model revisits and reevaluates prior steps\u2014and the spontaneous exploration of alternative problem-solving strategies. Such behaviors are not explicitly planted into training corpus but emerge organically through the model's interaction with the reinforcement learning environment, consistent with findings by Ye et al. [23].\nThe \"aha moment\" referenced in the R1 report [3] primarily refers to the model's sudden acquisition of complex reasoning behaviors. A secondary interpretation involves the model spontaneously verbalizing \"aha moment,\" such as in phrases like \"Wait, wait. Wait. That's an aha moment I can flag here.\" While our model did not exhibit this specific verbalization, Figure 4 shows that it displayed some complex reasoning behaviors (e.g., self-reflection, exploration, verification, summarization) even by step 10.\nThus, we conclude that the RL process likely lacks a sudden \"aha moment\"\u2014that is, complex reasoning behaviors do not abruptly emerge at a specific training step, aligned with Liu et al. [9]."}, {"title": "RQ 4: Can the Model Generalize to Out-of-Distribution (OOD) Tasks?", "content": "To what extent can the trained model handle tasks that differ from its training data, particularly those that are out-of-distribution?"}, {"title": "RQ 5: Which Generalizes Better, SFT or RL?", "content": "Can post-training methods achieve more than just superficial alignment, which just learns format patterns? Can SFT or RL actually learn to learn, effectively generalizing to other domains?\nWe investigate whether models merely memorize training data or truly learn reasoning skills. Follow-ing the setup in [17], we test this by comparing performance on familiar problems versus slightly altered ones. Two signs of memorization: High accuracy on seen problems, and low accuracy on slightly perturbed versions. So, we measure these using two metrics, denote model as f, dataset as D.\n1. Accuracy on Observed Problems: Acc(f; D), the model's accuracy on the training set (Tr).\n2. Consistency Ratio: CR(f; D), the ratio of correct solutions after small changes (perturbations) to those solved without changes. Perturbations preserve the puzzle's core principle and difficulty.\nThe Local Inconsistency-based Memorization Score is defined as:\nLiMem(f; D) = Acc(f; D) \u00b7 (1 \u2013 CR(f; D))\nThis score captures both memorization and sensitivity to changes. If a model's performance drops significantly when the problem format is altered, it likely hasn't learned the true reasoning skills required to solve similar but modified puzzles. We study two types of perturbations: (i) changing one person's statement to another bool logic expression, and (ii) reordering the statements between different people. Examples of perturbations are as follows:"}, {"title": "Perturbation Examples", "content": "Original Problem:\nZoey remarked, \"Oliver is not a knight\". Oliver stated, \"Oliver is a knight if and only if Zoey is a knave\". So who is a knight and who is a knave?\nStatement Perturbation:\nZoey remarked, \"Oliver is a knight or Zoey is a knight\". Oliver stated, \"Oliver is a knight if and only if Zoey is a knave\"\nReorder Perturbation:\nOliver stated, \"Oliver is a knight if and only if Zoey is a knave\". Zoey remarked, \"Oliver is not a knight\"\nWhile it is challenging to collect a suitable SFT dataset, we employ reject sampling to gather ground truth data, referred to as the RFT method.\nTo explore which training paradigm offers better generalization performance, we apply two types of disturbances on training dataset, then compare RFT and RL with Test acc - Mem Score curve. For RFT settings, we use Reject Sampling on origin model, then use a rule-based Best-of-N method to collect the correct yet the shortest response for further fine-tune. As a result, we obtained the curve shown in Fig. 6, which illustrates the changes in unseen test accuracy and training dataset memorization over training process."}, {"title": "RQ 6: Is Curriculum Learning Still Necessary in RL?", "content": "Does curriculum learning still matter in the RL paradigm? Specifically, is the sequential order of data important, or is it merely the curation ratio that matters?\nTo evaluate the necessity of curriculum learning, we compare its effectiveness to a mixed-difficulty approach. In curriculum learning, the model is trained sequentially on progressively more difficult datasets (3-7 people scenarios) for one epoch each. In contrast, the mixed-difficulty approach trains the model on all difficulty levels simultaneously within a single epoch. All other hyperparameters are kept constant between the two methods.\nWe analyze the test score trajectories using a rolling average (window size = 5) to mitigate stochastic fluctuations and highlight underlying trends. The results in Figure 7 indicate that curriculum learning yields slightly higher test scores during intermediate training phases. However, this advantage diminishes in practical significance, as the performance difference during early training stages remains statistically negligible, suggesting a limited impact on initial convergence. While curriculum learning may offer a marginal theoretical benefit in terms of sample efficiency, its practical necessity is not conclusively supported, given the minimal real-world performance difference and the added complexity of staged training."}, {"title": "RQ 7: Does Longer Response Length Guarantee Better Reasoning?", "content": "Does an increase in response length during training directly improve a model's reasoning perfor-mance, or are these trends merely correlated?\nThis experiment investigates whether an increase in response length during training causally enhances reasoning performance. We compared two models trained using the same algorithm and base model but with different hyperparameters and dataset difficulties.\n\u2022 Positive Example Model (Blue): Despite a slight decrease in response length over time, this model demonstrated significant improvements in both validation accuracy and reward, indicating stronger reasoning and generalization abilities.\n\u2022 Negative Example Model (Red): This model consistently increased its response length but showed no improvement in validation accuracy or reward. This suggests that increasing response length alone does not necessarily enhance reasoning capabilities.\nFigure 8 illustrates these findings: the positive model's reward and accuracy improved while its response length decreased, whereas the negative model's length increased without any corresponding performance gains. These divergent trends suggest that changes in response length are likely a byproduct of training dynamics rather than a causal driver of reasoning improvements.\nThe observed increase in response length is likely a side effect of reinforcement learning (RL) dynamics. While some studies report a natural tendency for output length to grow as models generate more complex responses, this growth should be viewed as a correlate rather than a direct cause of improved reasoning. Importantly, there is no statistically significant evidence that the magnitude of length increase reliably predicts proportional gains in reasoning performance.\nIn conclusion, longer responses do not always guarantee better reasoning. Enhanced reasoning capabilities may naturally lead to more detailed explanations, but artificially extending response length does not necessarily result in proportional performance improvements."}, {"title": "Discussion and Future Work", "content": "While our study demonstrates the potential of Logic-RL in developing complex reasoning skills, it is important to note that our findings are based on a small-scale logic dataset. The generalizability of our results to large-scale real-world mathematical or coding scenarios remains to be explored. Future work should focus on extending our approach to more diverse and complex datasets to further validate its effectiveness and robustness. Our work will remain an open research project to benefit the community.\nChain-of-Thought Long to Short Methods. Despite the fact that our initial prompt is concise and straightforward, the length of the responses can expand by up to four times after reinforcement learning training. In order to enhance token efficiency and accommodate a long-context friendly training paradigm, we find it particularly valuable to explore methods that transform long responses into shorter, more digestible formats. This investigation aims at improving overall efficiency and effectiveness in handling lengthy outputs, thereby optimizing the training process for better scalability and performance.\nStablize RL Training. We have found it beneficial in some cases to eliminate KL constraints, espe-cially when starting from a strong foundation model. Additionally, introducing a higher temperature at the beginning of training appears to provide the model with a more diverse starting point. We plan to further investigate how the SFT stage impacts the effectiveness and efficiency of RL training.\nMixed-Language Reasoning. A curious phenomenon is the model's frequent use of Chinese tokens in the <think> section, despite training data being fully in English. One hypothesis is that certain tokens in the Chinese vocabulary vector might produce hidden states that are \"favorable\" under our RL scheme. Investigating whether code-switching or even random token switching could systematically aid internal reasoning is an exciting avenue.\nRelaxing the Formatting Constraints. Although <think>...</think> effectively organizes the chain of thought, it remains an open question whether an entirely unconstrained or latent approach might yield better results. The model might eventually \u201cinvent\u201d its own internal representation for reasoning if given the right incentives."}, {"title": "Qualitative Analysis of Emergent Reasoning", "content": "We highlight four key emergent behaviors observed in the RL-trained model:\n(1) Hesitation and Self-Verification The model occasionally uses phrases like \"I am not entirely sure; let's re-check this step\" in the <think> section. This hesitation, absent in pre-training, emerges as the model is rewarded for correct answers and penalized for errors. Before providing the final <answer>, it systematically verifies all prior steps.\n(2) Multi-Path Exploration & Backtracking Encouraging the model to reason thoroughly, we see it propose multiple solutions (\"Let's test both possibilitie\") and backtrack to check for consistency, resembling human problem-solving in logic puzzles.\n(3) Formula Application After RL training, our model instinctively applied the \"If P, then Q\" implication formula when solving logical puzzles, like the Knights and Knaves problem. This formula asserts that the proposition is false only when P is true and Q is false. We were surprised to see that the model not only solved the puzzles through trial and error but also incorporated formal logical reasoning, resembling human problem-solving, despite no such data included in the training set.\n(4) Occasional Language Switching Interestingly, some <think> segments contain Chinese tokens (the base model is English-centric). The final <answer> remains in English, presumably to gain a format reward. In some demos, the model briefly interjects a line of Chinese while analyzing statements, then seamlessly switches back to English for the solution. This phenomenon may indicate that the model uses language mixing as a hidden fallback or distinct internal representation."}, {"title": "Related Work", "content": "Large Language Model Reasoning A key focus for LLMs is improving their reasoning abilities, particularly for complex tasks like code generation and math problem-solving. Chain-of-Thought (CoT) reasoning [16] has been crucial in breaking down problems into manageable steps, enhancing logical reasoning. Originally successful in AlphaGo's victory [14], MCTS has been adapted to guide model-based planning by balancing exploration and exploitation through tree-based search and random sampling, and later to large language model reasoning [6, 18, 20]. However, recent research suggests that the vast token generation space of LLMs may make improving reasoning capabilities inefficient [3]. Additionally, long-path reasoning is likely influenced by the model's working memory [24]."}]}