{"title": "Improved Fine-Tuning of Large Multimodal Models for Hateful Meme Detection", "authors": ["Jingbiao Mei", "Jinghong Chen", "Guangyu Yang", "Weizhe Lin", "Bill Byrne"], "abstract": "Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While large multimodal models have shown strong generalization across various tasks, they exhibit poor generalization to hateful meme detection due to the dynamic nature of memes tied to emerging social trends and breaking news. Recent work further highlights the limitations of conventional supervised fine-tuning for large multimodal models in this context. To address these challenges, we propose Large Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a novel two-stage fine-tuning framework designed to improve both in-domain accuracy and cross-domain generalization. Experimental results on six widely used meme classification datasets demonstrate that LMM-RGCL achieves state-of-the-art performance, outperforming agent-based systems such as VPD-PALI-X-55B. Furthermore, our method effectively generalizes to out-of-domain memes under low-resource settings, surpassing models like GPT-40.", "sections": [{"title": "1 Introduction", "content": "The rise of social media has led to a surge in hateful content, notably in the form of memes. Manual detection is infeasible due to the vast amount of content and psychological risks for human moderators. Consequently, hateful meme detection systems have attracted considerable research interest (Kiela et al., 2021; Liu et al., 2022; Prakash et al., 2023; Shah et al., 2024).\nPrevious studies focus on supervised settings, fine-tuning neural networks with in-domain training data (Pramanick et al., 2021b; Kumar and Nandakumar, 2022; Burbi et al., 2023; Lin et al., 2024). However, real-world challenges arise as memes continuously evolve with social trends and breaking news. This creates an out-of-domain generalization gap where these systems often fail to recognize new hate patterns (Cao et al., 2024). Frequent retraining becomes impractical given daily content generation rates and associated annotation requirements. These challenges highlight the need for generalizable detection methods that perform well on in-domain examples while maintaining robustness in low-resource, out-of-domain scenarios (Huang et al., 2024).\nWhile large language models (LLMs) have demonstrated strong generalization across various tasks (OpenAI, 2023), recent studies indicate that vanilla Large Multimodal Models (LMMs) struggle with hateful meme detection, both in-domain and out-of-domain (Mei et al., 2024; Huang et al., 2024; Hee et al., 2024; Cao et al., 2024). Notably, Mei et al. (2024) found that fine-tuned CLIP (Radford et al., 2021) outperforms supervised fine-tuned (SFT) LMMs such as LLaVA-13B (Liu et al., 2023b) and Flamingo-80B (Alayrac et al., 2022), thus exposing shortcomings in standard SFT approaches for LMMs. Additionally, they showed that LMMs exhibit poor cross-domain generalization: for instance, a LLaVA model fine-tuned on the HatefulMemes dataset (Kiela et al., 2021) (focusing on racist, sexist, and religion-based hate speech) fails to generalize to HarMeme (Pramanick et al., 2021a), which targets COVID-related political memes. In addition, Huang et al. (2024) observed that hateful meme detection remains challenging for in-context learning framework in LMMs, suggesting that innovative approaches are needed to make better use of few-shot meme examples.\nTo address these challenges, we propose Large Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a two-stage fine-tuning framework designed to improve both in-domain and out-of-domain hateful meme"}, {"title": "2 Related Work", "content": "2.1 Hateful Meme Detection\nMost existing approaches to hateful meme detection use supervised learning. Early systems (Zhu, 2020; Muennighoff, 2020) fine-tuned object detection-based vision-language models such as OSCAR (Li et al., 2020), and UNITER (Chen et al., 2020), which utilize Faster R-CNN (Ren et al., 2015)-based object detectors (Anderson et al., 2018; Zhang et al., 2021).\nMore recently, research has shifted toward CLIP (Radford et al., 2021) for its end-to-end simplicity and stronger multimodal alignment. Numerous studies have fine-tuned models based on CLIP using different modality fusion mechanisms (Pramanick et al., 2021b; Kumar and Nandakumar, 2022; Shah et al., 2024). Other works incorporate caption models into the CLIP-based feature fusion network to further enhance performance (Burbi et al., 2023; Cao et al., 2023; Ji et al., 2024). Additionally, contrastive learning techniques have been explored to address confounding factors in meme classification (Lippe et al., 2020; Mei et al., 2024).\nWhile LMMs such as Flamingo (Alayrac et al., 2022) have shown promise in hateful meme detection via SFT, fine-tuning strategies for LMMs remain underexplored relative to CLIP-based approaches. In fact, Mei et al. (2024) demonstrated that fine-tuned CLIP models can outperform much larger LMMs, highlighting the need for specialized fine-tuning methods. In this work, we propose a novel fine-tuning approach for LMMs to improve their effectiveness in hateful meme detection.\n2.2 Low resource hateful meme detection\nLow-resource hateful meme detection has received relatively little attention, despite its growing importance in real-world deployments that require out-of-domain generalization. In this setting, an initially trained model is deployed to a new domain without gradient updates, relying only on demonstration examples for inference (Huang et al., 2024). Hee et al. (2024) utilized text similarity-based few-shot examples to help LMMs generalize to unseen memes. Similarly, Hu et al. (2024) and Huang et al. (2024) explored agent-based LMM systems with few-shot learning for out-of-domain settings. However, Huang et al. (2024) observed that in-context learning is less effective for meme classification compared to other tasks, highlighting the need for more effective strategies to use demonstration examples. In contrast, we use a retrieval-based majority voting scheme for classifying unseen memes and find that it makes more effective use of demonstration examples than conventional"}, {"title": "3 LMM-RGCL Methodology", "content": "3.1 Preliminaries\nProblem Statement Hateful memes datasets are defined as ${(I_i, T_i, Y_i)}_1^N$, where $I_i \\in \\mathbb{R}^{C \\times H \\times W}$ is the image portion of the meme in pixels; $T_i$ is the caption overlaid on the meme; $y_i \\in \\{0, 1\\}$ is the label, where 0 stands for benign, 1 for hateful. Large Multimodal Models Some prior work in using LMMs for hateful meme detection has approached the problem via text generation, where the LMM takes a meme $(I_i, T_i)$ as an input to predict a single token label $\\hat{y}^{LMH}_i \\in$ {\\\"benign\\\", \\\"hateful\\\"} (Lin et al., 2024; Huang et al., 2024). We refer to the final linear layer of the LMM as the LM Head (LMH), which maps hidden representations to a probability distribution over the vocabulary via a softmax function. For meme classification, the LMH decodes the hidden state of the last token and generates the output label. This contrasts with approaches based on CLIP, which train Logistic Regression Classifiers (LRC) on encoder CLS tokens (Kumar and Nandakumar, 2022; Mei et al., 2024).\n3.2 LMM-RGCL Framework\nArchitecture As illustrated in Figure 1, LMM-RGCL integrates an LMM with two additional trainable components: a Multilayer Perceptron (MLP) that projects the LMM final hidden state $h_i$ into an embedding $g_i$ for use in classification and retrieval; and an LRC operating on $g_i$. Figure 1 shows how the architecture supports multiple fine-tuning and inference modes.\nRetrieval During training, FAISS-based (Johnson et al., 2021) nearest neighbor search retrieves pseudo-gold positive (Mei et al., 2024) and hard negative examples (Schroff et al., 2015) for contrastive learning from the encoded meme database G. At inference, FAISS is used to retrieve neighbors for the Retrieval-based KNN Classifier (RKC).\nInference modes Figure 1 shows three different classifiers: LMH, LRC, and RKC. For pre-trained and SFT LMMs, we report LMH results following prior work (Section 3.1). For LMM-RGCL models, we report the LRC results, unless otherwise specified. Section 4.5 presents a detailed comparison of the three inference modes."}, {"title": "3.3 Stage 1: Joint Multimodal Fine-tuning", "content": "In stage 1, the LMM is fine-tuned via Low-Rank Adaptation (Hu et al., 2022), which applies trainable low-rank matrices to the model while freezing its original weights. The MLP and LRC are updated simultaneously. We optimize the joint loss"}, {"title": "3.4 Stage 2: RGCL Fine-tuning", "content": "In stage 2, the LMM is frozen; only the MLP and LRC are fine-tuned to refine retrieval-aligned representations. Stage 2 jointly optimizes:\n$\\mathcal{L}^{Stage2} = \\mathcal{L}^{RGCL}_{RGCLL} + \\mathcal{L}^{LR}_{CLR}$,\nwhere $\\mathcal{L}^{LR}_{CLR}$ is defined in Eq. 4, and $\\mathcal{L}^{RGCL}_{RGCLL}$ is the Retrieval-Guided Contrastive Learning Loss.\nTo compute $\\mathcal{L}^{RGCL}_{RGCLL}$, we retrieve pseudo-gold positive and hard negative examples from the training set. Specifically, for a given sample i with embedding $g_i$, we use FAISS (Johnson et al., 2021) to perform the nearest neighbor search between $g_i$ and every other target embedding $g_j \\in G$ from the training set. The encoded meme database G is updated every 100 steps during fine-tuning.\nPseudo-gold positive examples are same-label examples that have high similarity scores with $g_i$, while hard negative examples are opposite-label examples that have high similarity scores. We denote the embedding of the pseudo-gold positive example and hard negative example as $g^+_i$ and $g^-_i$, respectively. $\\mathcal{L}^{RGCL}_{RGCLL}$ is then computed as:\n$\\mathcal{L}^{RGCL}_{RGCLL} = \\mathcal{L}(g_i, g^+_i, g^-_i)$\n$= -\\log(\\frac{e^{sim(g_i, g^+_i)}}{e^{sim(g_i, g^+_i)} + e^{sim(g_i, g^-_i)}})$,\nwhere $sim(., .)$ denotes the cosine similarity function. Stage 2 fine-tuning explicitly aligns the representations of semantically similar meme pairs, thereby improving the generalization of LMMs to distribution shifts in unseen datasets."}, {"title": "3.5 Retrieval Based KNN Classification", "content": "In addition to the LMH and LRC, RKC is used specifically for out-of-domain meme classification. For a test meme t, we retrieve K similar memes within the embedding space from the meme database G. We perform similarity-weighted majority voting to obtain the prediction:\n$\\hat{y}^{RKC}_t = \\sigma (\\sum_{k=1}^K Y_k sim(g_k, g_t))$,\nwhere $\\sigma(.)$ is the sigmoid function and\n$Y_k := \\begin{cases} 1 & \\text{if } y_k = 1\\\\ -1 & \\text{if } y_k = 0 \\end{cases}$"}, {"title": "4 Experiments", "content": "We evaluate the performance of our systems on six popular meme classification datasets: HatefulMemes (Kiela et al., 2021), HarMeme (Pramanick et al., 2021a), MAMI (Fersini et al., 2022), Harm-P (Pramanick et al., 2021b), MultiOFF (Suryawanshi et al., 2020) and PrideMM (Shah et al., 2024). These datasets encompass varying definitions of harmful content (hateful, offensive, or targeted harassment) across different sociopolitical contexts. A detailed description and statistics are in Appendix A.\nFor HatefulMemes, HarMeme, and MAMI, we report the Area Under the Receiver Operating Characteristic Curve (AUC) and Accuracy (Acc) in line with previous studies (Kumar and Nandakumar, 2022; Cao et al., 2023; Mei et al., 2024; Cao et al., 2024). For Harm-P, MultiOFF, and PrideMM, we report Accuracy and F1 score consistent with the literature (Pramanick et al., 2021b; Mei et al., 2024; Shah et al., 2024; Lin et al., 2024). Implementation details, including hyperparameters, and statistical significance test procedures are detailed in Appendix B."}, {"title": "4.1 Comparing LMM-RGCL to Baseline Systems under Supervised Settings", "content": "Table 1 presents the performance of baseline systems under supervised fine-tuning settings. We compare LMM-RGCL against a range of strong baselines: the best prior models for each dataset; supervised fine-tuned CLIP-based classifiers; and Large Multimodal Models (LMMs). All models are fine-tuned and evaluated for each dataset separately.\nCLIP-based Classifiers We compare the performance of fine-tuned CLIP (Radford et al., 2021) model with three other fine-tuning methods for CLIP-based systems: MOMENTA (Pramanick et al., 2021b), HateCLIPper (Kumar and Nandakumar, 2022) and RGCL (Mei et al., 2024).\nLarge Multimodal Models We experiment with three LMMs from two model families: LLaVA-1.5-7B (Liu et al., 2023a), Qwen2VL-2B and Qwen2VL-7B (Wang et al., 2024). We report the performance of these LMMs in the following settings: pre-trained models with zero-shot and few-shot prompts using the LMH; SFT LMMs using the LMH; and classification using LRC under the LMM-RGCL fine-tuning framework.\nBest Prior Models Visual Program Distillation (VPD) (Hu et al., 2024) and ExplainHM (Lin et al., 2024) are LLM agent-based systems. The remaining state-of-the-art models, including ISSUES (Burbi et al., 2023), Pro-Cap (Cao et al., 2023), RGCL (Mei et al., 2024) and MemeCLIP (Shah et al., 2024), are based on fine-tuning CLIP-based vision and language models. Detailed descriptions of these methods are provided in Appendix C.\nObservation 1: Fine-tuned CLIP-based classifiers outperform baseline LMMs.\nAs shown in Table 1, RGCL (#4) achieves the highest performance among CLIP-based classifiers, surpassing standard fine-tuned CLIP (#1) by approximately 10% across multiple datasets. On 5 out of 6 datasets, RGCL performs better than, or on par with, all three SFT LMMs (#7, #11, #15).\nObservation 2: In-context learning exhibits limited efficacy for meme classification.\nWe compare the zero-shot (#5, #9, #13) and few-shot (#6, #10, #14) performance of the pre-trained"}, {"title": "4.2 Comparing LMM-RGCL with Baseline Systems under Low-Resource Settings", "content": "Online hate speech is constantly evolving, posing a challenge to systems as the distribution of memes encountered in the wild departs from that of the training data. To simulate real-world deployment constraints, we evaluate systems on out-of-domain examples under low-resource settings where gradient updates are prohibited and only demonstration examples are available (Huang et al., 2024; Hee et al., 2024; Cao et al., 2024).\nWe adopt a cross-dataset evaluation protocol similar to Mei et al. (2024): models fine-tuned on HarMeme are evaluated on HatefulMemes, while models trained on HatefulMemes are evaluated on all other datasets. This protocol simulates a scenario in which a trained meme classification system is deployed to evaluate trending memes. Few-shot and RKC examples are drawn from the training split of each of the target evaluation datasets to avoid test set contamination.\nWe compare LMM-RGCL fine-tuned LMM with the RKC against the following systems: SFT LMMs with zero-shot and few-shot prompting us-"}, {"title": "4.3 Effects of Two-Stage Fine-tuning", "content": "We assess the impact of each stage in the two-stage LMM-RGCL fine-tuning process. As shown in Table 3a and Table 3b, omitting either stage results in performance losses under both supervised and cross-dataset settings. Excluding Stage 1 results in the largest performance drop, particularly because the Large Multimodal Model (LMM) backbone remains frozen during Stage 2.\nWhen stage 2 is omitted, the performance loss in supervised settings is less severe than in cross-dataset settings. We attribute this to the use of contrastive loss in Stage 2, which explicitly optimizes retrieval capabilities by aligning representations of semantically similar meme pairs. This alignment enhances robustness to distribution shifts in unseen datasets.\nFor 'combined stages', we jointly optimize the objectives for both stages:\n$\\mathcal{L}^{Combined} = \\mathcal{L}^{RGCL}_{RGCLL} + \\mathcal{L}^{LR}_{CLR} + \\mathcal{L}^{LM}.$"}, {"title": "4.4 Numbers of Shots and Neighbors", "content": "We ablate the effects of varying the number of shots for few-shot in-context learning and varying the number of top K nearest neighbors for RKC.\nFigure 2 demonstrates that increasing the number of in-context examples for LMMs does not consistently yield performance improvements over the zero-shot setting, and in some cases even causes loss. These findings suggest that merely adding more shots does not necessarily improve performance, which is consistent with findings from Huang et al. (2024).\nFigure 2 shows that as the number of nearest neighbors K for RKC increases, the performance continues to increase for both AUC and accuracy, plateauing at around K = 20. The consistent improvement in performance indicates that RKC trained with LMM-RGCL utilizes demonstration examples more effectively than the standard in-context learning framework."}, {"title": "4.5 Comparing Different Inference Modes", "content": "Table 4 compares Qwen2VL-7B fine-tuned with LMM-RGCL using the three classifiers. Our results indicate that under supervised settings, the differences among the three inference modes are minimal. However, under cross-dataset settings, there is a significant disparity in generalization performance. Notably, RKC outperforms both LMH and LRC, underscoring its superior effectiveness in handling out-of-domain examples."}, {"title": "5 Conclusion", "content": "We propose LMM-RGCL, a two-stage fine-tuning framework designed to improve LMM performance on hateful meme classification, addressing the ineffectiveness of standard SFT. Our approach effectively improves both in-domain accuracy and cross-domain generalization. State-of-the-art performance across six meme classification datasets demonstrates the effectiveness of LMM-RGCL."}, {"title": "Limitations", "content": "Hate speech is described using various terminologies, including online harassment, online aggression, cyberbullying, and harmful speech. The United Nations Strategy and Plan of Action on Hate Speech acknowledges that definitions of hate speech can be controversial and subject to debate (Nderitu, 2020). Similarly, the UK Online Harms White Paper highlights that certain harms may be insufficiently defined (Woodhouse, 2022).\nWe acknowledge that the definition of hate speech can be subjective and varies across different cultural and legal contexts. To this end, we evaluate our methods on six widely used meme classification datasets, allowing for generalization across different definitions of hate speech. As the discourse on defining hate speech evolves, we align our research with this ongoing process and plan to incorporate new datasets as they become available.\nIn our error analysis, we find that the system is unable to recognize subtle visual details in memes. Enhancing image understanding through a more powerful vision encoder could further improve performance, which we leave for future work."}, {"title": "Ethical Statement", "content": "Reproducibility. Detailed experimental setups, implementation specifics, and hyperparameter settings are provided in Appendix B to ensure reproducibility. The source code will be released upon publication.\nUsage of Datasets. The datasets used in this study\u2014HatefulMemes, HarMeme, MAMI, Harm-P, MultiOFF, and PrideMM\u2014were curated for research purposes to combat online hate speech. We strictly adhere to the terms of use established by the dataset authors.\nSocietal benefits. Hateful meme detection systems, like LMM-RGCL, can be used to automatically detect hateful content online, contributing significantly to reducing online hate speech. By reducing hate speech, fostering safer digital environments, and supporting human content moderators, these systems can make a significant impact on online communication and safety. We believe these benefits are both substantial and essential in the broader effort to create a more secure and respectful digital space."}, {"title": "Intended use", "content": "We intend to enforce strict access controls for model release. The model will be available only to researchers who agree to our terms of use, which explicitly state that the system is designed solely for the detection and prevention of hateful speech. Its use for any purposes that promote, condone, or encourage hate speech or harmful content is strictly prohibited."}, {"title": "Misuse Potential", "content": "Although our system is not inherently designed to induce bias, training on existing datasets such as HatefulMemes may inadvertently propagate existing biases towards certain individuals, groups, or entities (Pramanick et al., 2021b). To mitigate the risk of unfair moderation resulting from these dataset-induced biases, it is essential to incorporate human oversight into the moderation process if deployed."}, {"title": "Deployment consideration", "content": "Cultural differences and subjective topics introduce biases in moderating online hate speech. Expressions that may seem benign to some can be deeply offensive to others. Our RKC inference mode relies on retrieving examples that generalize well across various domains, allowing the creation of multiple retrieval sets tailored to diverse cultural sensitivities without requiring retraining. However, before deploying such systems, it is crucial to carefully evaluate dataset annotations, particularly when addressing cultural differences and subjective interpretations. Key factors include data curation guidelines, potential annotator biases, and the inherently context-dependent definitions of hate speech. These considerations are essential to ensuring the system is deployed responsibly and effectively across varied cultural contexts."}, {"title": "Environmental Impact", "content": "Training large-scale models is computationally intensive and contribute to global warming due to heavy GPU/TPU usage. However, our approach mitigates this issue by fine-tuning LMMs using quantized LoRA, a parameter-efficient method. As a result, our system can be trained in under four hours on a single GPU, significantly reducing both training time and computational cost compared to full-scale LMM fine-tuning. Furthermore, since our method generalizes across different domains without requiring retraining, it further minimizes computational overhead."}, {"title": "D Comparing RKC and In-Context Learning under different Fine-tuning Schemes", "content": "In this section, we compare the performance of the RKC inference mode against few-shot in-context learning for pre-trained LMMs, SFT LMMs, and LMMs fine-tuned using our proposed LMM-RGCL framework under the cross-dataset setting. As shown in Table 7, RKC consistently outperforms the few-shot in-context learning approach across all LMM variants, indicating that RKC makes better use of demonstration examples. Furthermore, LMM-RGCL fine-tuned LMMs with RKC outperform SFT LMMs with RKC, highlighting the advantages of our fine-tuning strategy."}, {"title": "E Ablation study on the loss function", "content": "Table 8 shows the results when each loss objective is removed from different stages of fine-tuning. Notably, when the cross-entropy loss is removed in stage 1 for the logistic regression component, the LRC fails to train properly via backpropagation, resulting in performance that is equivalent to random guessing. Consequently, we exclude this case from our comparison. Overall, we observe that removing any loss function from the fine-tuning objective leads to a significant drop in performance, highlighting the importance of each loss term in optimizing the model."}, {"title": "F Case Analysis", "content": "F.1 Comparing SFT and LMM-RGCL Predictions\nTable 9 presents examples where our LMM-RGCL method successfully corrects prediction errors"}, {"title": "F.2 Error Analysis", "content": "In Table 10, we present examples where LMM-RGCL was unable to correct errors made by the baseline SFT model. In the first case, the model struggles with the nuanced visual understanding required to interpret the disabled body of the swimmer. Additionally, these examples demand complex reasoning to assess the hatefulness of the memes. Interpreting such nuanced meanings remains a challenge for current models. However, we anticipate that the advanced reasoning capabilities of emerging systems like OpenAI-01 (OpenAI, 2024b) and DeepSeek-R1 (DeepSeek-AI, 2025) will help address these limitations."}, {"title": "G AI Assistance", "content": "Our coding work was assisted by Github Copilot. OpenAI ChatGPT was only used in proofreading and spell-checking. We claim that the content presented in this paper was fully original."}]}