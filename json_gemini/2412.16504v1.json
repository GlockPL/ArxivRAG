{"title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions", "authors": ["Hao Du", "Shang Liu", "Lele Zheng", "Yang Cao", "Atsuyoshi Nakamura", "Lei Chen"], "abstract": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing their effectiveness and limitations in addressing privacy risks and maintaining model utility. By identifying key gaps in existing research, we highlight challenges and propose directions to advance the development of privacy-preserving methods for fine-tuning LLMs, promoting their responsible use in diverse applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid advancement of large language models (LLMs) such as GPT-4, LLaMA, and PaLM has revolutionized natural language processing (NLP), enabling applications across diverse domains such as content generation and code completion. These models are trained on vast amounts of data, often sourced from publicly available text on the Internet, to achieve remarkable performance in understanding and generating human-like text. However, the widespread deployment of LLMs also raises significant privacy concerns, especially as they become increasingly integrated into sensitive applications, such as healthcare, finance, and customer service.\nFine-tuning has emerged a highly popular approach for adapting pre-trained large language models (LLMs) to specific downstream tasks. By leveraging pre-trained knowledge and tailoring it to domain-specific needs, fine-tuning enables"}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Fine-tuning Model", "content": "Fine-tuning is a fundamental process for adapting pre-trained models to downstream tasks, enabling them to leverage the vast knowledge captured during pre-training.\nFine-tuning involves training a pre-trained model on a smaller, task-specific dataset, which is closely aligned with the target objectives. The datasets used for fine-tuning often come from specialized sources, such as forums, professional communities, or curated data repositories in fields like medicine, law, and technology. Typically, fine-tuning datasets are composed of labeled demonstration data, including human-labeled datasets, machine-generated datasets, or domain-specific annotated examples. These datasets tend to be relatively small, ranging from a few hundred to several thousand samples, especially when compared to the massive datasets used in the pre-training phase. Through this targeted process, fine-tuning enables the model to learn nuanced characteristics and details pertinent to specific tasks, thereby enhancing its applicability and effectiveness.\nFine-tuning heavily relies on pre-trained models, which are often openly released by other individuals or organizations. These pre-trained models provide a foundational knowledge base that fine-tuning adapts for more specific use cases. In comparison to pre-training, fine-tuning typically involves more sensitive data and leverages task-specific strategies to optimize speed and reduce costs. The selection of fine-tuning methods often depends on the requirements of the target task, highlighting the flexibility and practicality of this process."}, {"title": "2.2 Parameter-Efficient Fine-Tuning Methods", "content": "While traditional full fine-tuning updates all parameters of a model, achieving high task-specific performance, it is computationally expensive and prone to overfitting, especially with limited data. To overcome these challenges, a variety of Parameter-Efficient Fine-Tuning (PEFT) methods [6,2,20,22,43,15] have been proposed, offering more efficient alternatives. These methods vary in their approach, striking different balances between parameter efficiency, adaptability, performance, and speed, expand the range of available fine-tuning methods.\nFull Fine-tuning [6] (FFT) updates all parameters of a model, providing maximum flexibility and allows the model to fully adapt to the task. However, it is computationally intensive and requires substantial data to avoid overfitting, making it less practical for resource-constrained scenarios. BitFit [2], which is Bias-term Fine-tuning, only updates bias terms while freezing the rest of the model. It is lightweight, with minimal computational cost but limited in capturing complex task-specific features. Prompt-tuning [20] introduces trainable prompt vectors to guide the pre-trained model without modifying its internal weights. Prompt-tuning is effective for small datasets, and leverages the full capacity of large pre-trained models but less effective for smaller models. Prefix-tuning [22] adds trainable prefix vectors at every Transformer layer, interacting with input through attention mechanisms. Compared to Prompt-tuning, it allows dynamic adjustment of intermediate representations, improving adaptability for complex tasks, but slightly increases computational cost due to its per-layer modifications. Adapters [14] inserts small task-specific trainable modules into Transformer layers while freezing the backbone. Adapters are well-suited for multi-task learning and are modular by design, enabling efficient transfer across tasks. However, introducing extra modules leads to extra computation, which may impact inference speed. LoRA [15] uses low-rank decomposition of weight matrices and fine-tunes only the low-rank components. It maintains a effective balance between parameter efficiency and task performance, making it highly applicable in large-scale models and complex tasks.\nDue to the extensive variety of fine-tuning methods for LLMs, this survey focuses on the representative fine-tuning methods introduced above. In the following tables, we use the following abbreviations: A for Adapter, L for LORA, B for BitFit, P1 for Prompt-tuning, P2 for Prefix-tuning, and F for Full fine-tuning."}, {"title": "3 Privacy Attacks in Fine-tuning LLMs", "content": "Fine-tuning is a process of adapting pre-trained models using specific datasets to make them more suitable for specialized downstream tasks. However, this process introduces unique privacy risks. This section focuses on privacy attacks specific to the fine-tuning phase, highlighting two major inherent risks:\nAttacks targeting sensitive data in fine-tuning datasets: These attacks, including membership inference attacks and data extraction attacks, aim to"}, {"title": "3.1 Membership Inference Attack", "content": "Membership inference attacks (MIAs) aim to identify whether specific data points were part of a model's training dataset, posing significant risks to sensitive information such as clinical records and preference datasets. Recent studies have examined the mechanisms, vulnerabilities, and impacts of MIAs on large language models (LLMs), shedding light on various attack strategies and the factors that influence model susceptibility. Among these, black-box access is the most commonly assumed scenario for MIAs, wherein attackers lack access to the model's internal parameters but infer the inclusion of data samples in the fine-tuning process based on the model's outputs.\nJagannatha et al. [18] conducted a black-box MIA on clinical language models, employing a Threshold-Based Attack to identify which samples were included in fine-tuning across models of varying sizes. Their findings revealed that model architecture and size significantly influence privacy risks. Specifically, Auto-Regressive Models (ARMs) like GPT-2 are more susceptible to MIAs compared to Masked Language Models (MLMs) such as BERT. Moreover, the success rate of these attacks increases with model size.\nMireshghallah et al. [31] further leveraged the fine-tuning scenario where attackers can access the pre-trained base model. They proposed an MIA based on Likelihood Ratios, using the original model as a reference. This approach was extended from masked language models to auto-regressive language models. Their study demonstrated that the fine-tuning method impacts privacy vulnerability, with head fine-tuning being more vulnerable to MIA compared to full fine-tuning or adapter fine-tuning. The easy accessibility of pre-trained models exacerbates the risks associated with MIAs. Fu et al. [11] introduced SPV-MIA, a novel technique that eliminates the need for attackers to access an external reference dataset. This approach uses a self-prompting method to generate a reference dataset directly from the target model. By incorporating a probabilistic variation metric, SPV-MIA focuses on measuring memorization rather than overfitting, offering a more robust indicator for identifying member records.\nWhen an attacker can modify the pre-trained model and publish it, the backdoor can be easily integrated into MIA, leading to a significant increase in attack success rate of MIA. Wen et al. [39] demonstrated that injecting a backdoor into a pre-trained model before fine-tuning substantially enhances the effectiveness of MIAs. Similarly, Liu et al. [25] proposed the PreCurious attack framework, where"}, {"title": "Remark 1.", "content": "Membership inference attacks exploit various characteristics of the fine-tuning process, with factors such as model architecture, size, and fine-tuning methods significantly influencing vulnerability. While traditional MIAs remain effective, emerging approaches introduce new attack vectors. Additionally, the integration of backdoors models further amplifies the risk, underscoring the pressing need for robust defenses tailored to fine-tuning scenarios."}, {"title": "3.2 Data Extraction Attack", "content": "Data extraction and reconstruction attacks expose a significant vulnerability in large language models (LLMs), allowing attackers to retrieve sensitive information, including personally identifiable information (PII), from model outputs or internal representations. These attacks highlight the critical need to assess the security implications of deploying LLMs in applications that handle sensitive or proprietary data, such as healthcare, finance, and customer service. Recent studies have investigated various data extraction techniques, demonstrating the increasing sophistication and effectiveness of these attacks, driven by advances in adversarial methods and model interpretability tools. Furthermore, the diversity of attack methodologies, including querying-based approaches and gradient-based approaches, underscores the multifaceted challenges in mitigating these risks for privacy preserving.\nLukas et al. [27] introduced three types of black-box data extraction attacks: PII Extraction, PII Reconstruction, and PII Inference. Their key innovation lies in PII reconstruction. Because of the unique characteristics of autoregressive models, previous reconstruction attacks struggled to fully utilize contextual information. To address this limitation, they introduced a novel approach using a pre-trained masked language model to generate potential candidates, significantly improving the success rate of reconstructing masked PII. Their evaluation also revealed that Differential Privacy (DP) mechanisms reduce the attack's effectiveness but do not fully eliminate the risk.\nAs previously mentioned, fine-tuning introduces new attack scenarios. For example, when attackers gain access to publicly released fine-tuned models and can perform further training or fine-tuning, new vulnerabilities arise. Using snapshots of the base model with cumulative change in the probabilities assigned to each token by the two models, Zanella-B\u00e9guelin et al. [49] have successfully extracted the data used to fine-tune and update the model. Ozdayi et al. [32] demonstrated that through prompt-tuning, attackers can deliberately control the memorization of specific data by the model, enabling sensitive information from the fine-tuning dataset to be more easily extracted. PreCurious, which adds a backdoor to the pre-trained models proposed by Liu et al. [25] also supports data extraction."}, {"title": "Remark 2.", "content": "The easy accessibility of pre-trained models has enabled adversaries to significantly enhance extraction success rates. In particular, the availability of publicly released base models provides attackers with a reliable reference point, creating nearly unavoidable risks. This poses substantial challenges for designing effective defense mechanisms."}, {"title": "3.3 Backdoor Attack", "content": "Backdoor attacks aim to inject malicious behaviors or vulnerabilities into models by manipulating the fine-tuning process, often with minimal effort. As discussed in the previous sections, backdoor attacks can be seamlessly integrated into various attack workflows. While most backdoor techniques are not explicitly designed for privacy attacks but rather to manipulate model outputs, their privacy risks are nonetheless significant. Moreover, since the base models used in fine-tuning are often derived from publicly available, open-source releases, attackers have ample opportunity to introduce malicious backdoors into these models. In this section, we review some of the latest backdoor attacks, focusing on how backdoors are inserted during fine-tuning. These methods typically involve modifications or fine-tuning of publicly available models to implant harmful backdoors, exposing fine-tuned models to increased privacy risks.\nNumerous studies have highlighted the vulnerability of instruction-tuned models to backdoor attacks. Wan et al. [38] demonstrated that instruction-tuned models, such as T5, are particularly susceptible to such attacks. By injecting carefully crafted malicious examples into the fine-tuning dataset, attackers can implant backdoors, causing the fine-tuned model to exhibit incorrect behavior in classification tasks when specific trigger phrases are encountered. Similarly, Xu et al. [42] found that backdoors can achieve high success rates by mixing malicious instructions with legitimate data. To investigate this vulnerability further, Yan et al. [46] proposed the Virtual Prompt Injection (VPI) attack. By poisoning instruction-tuning data, attackers can precisely control the model's output in response to specific triggers, thereby exposing more pronounced privacy risks.\nIn addition to manipulating model behavior, backdoor attacks have also been applied to the extraction of private information. For instance, PreCurious [25] pre-tuning the model on auxiliary data to enable rapid overfitting on private data during fine-tuning and adjusts model parameters to delay convergence, forcing prolonged exposure to private data and increasing memorization risks. Wen et al. [39] also proved the a pre-tained model with a backdoor poses a greater risk to the privacy of fine-tuning data. This backdoor design amplifies the original privacy risks by several magnitudes."}, {"title": "Remark 3.", "content": "Traditional backdoors can be implanted through fine-tuning to manipulate models into generating harmful outputs, which is particularly effective in instruction tuning. Additionally, backdoor attacks can serve as auxiliary methods to significantly enhance the success rates of MIAs and data extractions. The combination of multiple attack strategies in the context of fine-tuning is relatively straightforward to implement, further amplifying the associated risks."}, {"title": "4 Privacy Defenses in Fine-tuning LLMs", "content": "In this section, we investigate existing privacy defense methods that can be used in fine-tuning phase. Table 2 compares the common privacy defense methods: data anonymization, fine-tuning with DP, federated learning, knowledge unlearning, and off-site tuning."}, {"title": "4.1 Data Anonymization", "content": "Data anonymization protects privacy data security by removing or obfuscating sensitive information and employing techniques such as data masking, aggregation, generalization, or randomization to reduce the identifiability of sensitive data. The k-anonymity [35] was firstly introduced in 1997, which means ensuring that data cannot be re-identified to fewer than k individuals. This fundamental concept has driven significant research, resulting in the development of numerous anonymization techniques. In our survey, data anonymization is the process of protecting sensitive information in a text by removing or modifying personally identifiable information (PII) or other confidential details. Since this survey focuses on the fine-tuning stage, and data anonymization can be applied in both pre-training and fine-tuning without being specific to either stage, only a selection of representative methods is introduced in this paper.\nClassical Anonymization. The standard classical anonymization workflow involves preprocessing the text, using Named Entity Recognition (NER) to identify sensitive information, applying Coreference Resolution (CRR) to maintain consistency across references, and finally anonymizing the data through methods like suppression, tagging, random substitution, or generalization.\nMamede et al. [28] developed a four-module anonymization pipeline for Portuguese which closely resembles the general workflow for this task mentioned before. Their work demonstrates the importance of NER and CRR in this workflow which lays a foundation for the future. More recent anonymization methods leverage the power of LLMs. The LLMs are also used in the classical anonymization workflows by Vats et al. [37] Their framework find named entities with NER and mask them. After that, an LLM is used to impute previously masked tokens. Chen et al. [5] further advanced the utilization of LLMs. They employed two local LLMs: the first anonymizes the prompt before sending it to a black-box LLM, while the second restores the original content by de-anonymizing the output from the black-box LLM. Both of the utilization of LLMs achieve good privacy-utility trade-off.\nAnonymization with Differential Privacy. Differential Privacy (DP) [8] is a formal framework for privacy preservation that ensures the output of an algorithm remains statistically indistinguishable, whether or not a specific individual's data is included in the input dataset. The adoption of DP in text anonymization offers robust privacy guarantees. Typically, A neural model is"}, {"title": "4.2 Fine-tuning with Differential Privacy", "content": "Differential Privacy (DP) was introduced to model training through Differentially Private Stochastic Gradient Descent (DP-SGD), which is a modification of the standard SGD algorithm designed to ensure differential privacy during training. However, applying DP learning to large NLP models has faced significant challenges, including substantial performance degradation and high computational overhead when using DP-SGD, prompting researchers to propose more advanced DP-tuning methods.\nFull Fine-tuning with DP. Li et al. [10] addressed these issues by leveraging large pre-trained models, optimizing hyperparameters for DP, and aligning fine-tuning objectives with pretraining procedures. They introduced a memory-saving technique known as ghost clipping, which enables efficient DP-SGD for large Transformers. This approach maintains strong performance under the same privacy budget with minimal runtime overhead.\nBuilding on ghost clipping, numerous advancements have been proposed. For example, the Book-Keeping (BK) [3] technique requires only a single back-propagation round and eliminates the need to instantiate per-sample gradients. This method significantly improves throughput while maintaining similar memory usage, providing a more scalable solution for handling high-dimensional data. Similarly, Ding et al. [7] identified the attention distraction phenomenon in existing approaches, which affects training efficiency and performance in differential privacy settings. To address this, they proposed Phantom Clipping, further improving the training of large language models (LLMs) under DP constraints."}, {"title": "4.3 Federated Learning", "content": "Federated Learning (FL) is a distributed machine learning paradigm where multiple devices collaboratively train a shared model without sharing their raw data. Instead, each device computes updates locally and only exchanges model parameters or gradients with a central server, preserving data privacy while enabling large-scale model training across decentralized datasets.\nFwdLLM [44] introduces federated learning into LLM fine-tuning by proposing a backpropagation-free training method called perturbed inference. This approach replaces traditional gradient computation, significantly reducing memory and computational demands while seamlessly integrating with PEFT methods. FedIT [50] framework, which focuses directly on PEFT by leveraging LoRA for instruction-tuning in a federated learning setup. Similarly, the FedBPT [34]"}, {"title": "4.4 Knowledge Unlearning", "content": "Knowledge Unlearning, also known as machine unlearning [4], refers to the process by which a machine learning model is able to forget or remove knowledge about certain data points from its training. Unlike Differential Privacy, which cannot fully guarantee the right to be forgotten due to the inherent limitation of a non-zero privacy budget, Machine Unlearning focuses explicitly on eliminating the influence of specific data samples. In the fine-tuning phase, it can also be applied to remove the impact of specific data points in the fine-tuning datasets.\nMeng et al. [29] introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. [19] proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang [4] approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer."}, {"title": "4.5 Off-site Tuning", "content": "Off-site Tuning, detailed by Xiao et al. [41], is a novel framework for privacy preserving and efficient transfer learning, designed to address challenges in tuning large-scale pre-trained models without requiring full access to the model or the data. The key idea of Off-site Tuning is to enable transfer learning without sharing full models or sensitive data. It splits the pre-trained model into two components: the Adapter, a small, trainable module for task-specific tuning, and the Emulator, a compressed version of the model that provides gradients while concealing the complete model structure. The data owner fine-tunes the adapter locally with the emulator and sends back the adapter. The model owner integrates it with the full model to complete the tuning. This approach ensures"}, {"title": "5 Challenges and Future Directions", "content": null}, {"title": "5.1 Privacy Risks from Pre-trained Models", "content": "During the investigation, we found there is a particular reliance on pre-trained models in fine-tuning phase, which introduces new privacy challenges due to the open availability of many pre-trained models: Adversaries can release maliciously modified pre-trained models, embedding backdoors or vulnerabilities. Fine-tuning these models can amplify privacy risks and increase the likelihood of sensitive information in the fine-tuning dataset being exposed. Moreover, since most pre-trained models are publicly available, adversaries can easily acquire and use them as reference points or further fine-tune them to facilitate attacks on fine-tuning datasets, significantly increasing attack success rates. To counter malicious pre-trained models, we need to focus on developing robust verification methods to detect backdoors or malicious modifications in pre-trained models. Establishing trusted auditing frameworks, such as watermarking mechanisms, can also help the provenance and integrity of pre-trained models. Enhancing privacy protection during fine-tuning by obfuscating relationships between pre-trained and fine-tuned models is also a possible direction to reduce the effectiveness of reference model."}, {"title": "5.2 Risks and Opportunities from PEFTS", "content": "Parameter-Efficient Fine-Tuning (PEFT) methods significantly reduce computational demands and fine-tuning time, but they introduce novel attack and defense challenges at the same time. Privacy implications of different PEFTs remain underexplored, leaving critical questions unanswered: How do various fine-tuning methods (e.g., LoRA, Adapters, Prefix-tuning) impact privacy risks? Does PEFT offer better or worse privacy risks compared to full fine-tuning? What are the differences in privacy risks across PEFT methods, and what causes them? From an attack perspective, PEFT introduces new vulnerabilities, such as malicious attacks leveraging shared fine-tuning results (e.g., LoRA). From a defense perspective, addressing these risks remains challenge. While PEFT is being increasingly adopted, there is a lack of corresponding research on privacy-preserving methods specifically designed for PEFT. Therefore, we need to conduct more in-depth and comprehensive research on PEFT methods to understand their varying impacts on privacy. We can conduct in-depth researches of PEFT-specific attacks to identify new privacy risks and compare vulnerabilities across different PEFT methods. As for defense, we need develop targeted defenses for the unique attack surfaces introduced by PEFT. For the possibility of PEFTs, We can explore privacy-preserving methods specifically designed for PEFT or improve existing frameworks, to fully leverage PEFT's efficiency advantages and achieve a better utility-privacy balance."}, {"title": "5.3 Unique Characteristics of Fine-tuning Data", "content": "Fine-tuning datasets differ significantly from the general-purpose data used in pre-training, often exhibiting narrower domain coverage, task-specific distributions, and higher data sensitivity. These characteristics introduce unique privacy risks. Narrower and task-specific distributions make fine-tuning datasets more vulnerable to targeted attacks. Smaller dataset sizes exacerbate memorization issues, increasing the risk of sensitive information being leaked. However, there is currently a lack of privacy research specifically focused on the characteristics of fine-tuning data. To address this issue, future research can be conducted from both the attack and defense perspectives. From the perspective of attack, exploring domain-specific or task-specific attacks can help us understand the unique traits of fine-tuning datasets and design more effective defenses. From the perspective of defense, we need to explore task-specific or domain-specific privacy mechanisms to adapt to more specific tasks."}, {"title": "5.4 Limitations of Existing Defense Methods", "content": "Current defense methods struggle to comprehensively address diverse privacy attacks. For instance, Differential Privacy (DP) reduces but does not eliminate risks of data extraction. [27] Fine-tuning can also be used to enhance the memorization of LLMs [32], which may make unlearning ineffective. Therefore, there is a lack of unified metrics for evaluating effectiveness of defenses across diverse scenarios. To overcome these limitations, it is advocated to develop comprehensive privacy metrics that evaluate defenses against diverse attack types in fine-tuning settings. Furthermore, designing integrated defense frameworks that combine multiple privacy-preserving techniques to address a wider range of attacks is also crucial."}, {"title": "5.5 Balancing Utility and Privacy in Fine-tuning Defenses", "content": "The utility-privacy trade-off remains a longstanding challenge in fine-tuning large language models (LLMs). In fact, most defense methods also consider their utility-privacy trade-off as a critical factor. This trade-off, a well-known challenge in privacy protection, continues to be relevant in the fine-tuning stage. Moreover, given the high performance demands of fine-tuning for downstream tasks, maintaining utility becomes even more critical and cannot be overlooked. To address that, dynamic adjustment defense mechanisms or task-aware defense strategies can be potential directions."}, {"title": "6 Conclusion", "content": "In this survey, we first introduce essential background knowledge about fine-tuning LLMs, including the overview of fine-tuning and several representative fine-tuning methods. Subsequently, we investigate the existing privacy attacks"}]}