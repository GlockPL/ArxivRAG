{"title": "TOWARDS A CATEGORICAL\nFOUNDATION OF DEEP LEARNING:\nA SURVEY", "authors": ["FRANCESCO RICCARDO CRESCENZI"], "abstract": "The unprecedented pace of machine learning research has lead to incredible advances, but also\nposes hard challenges. At present, the field lacks strong theoretical underpinnings, and many impor-\ntant achievements stem from ad hoc design choices which are hard to justify in principle and whose\neffectiveness often goes unexplained. Research debt is increasing and many papers are found not to\nbe reproducible.\nThis thesis is a survey that covers some recent work attempting to study machine learning cate-\ngorically. Category theory is a branch of abstract mathematics that has found successful applications\nin many fields, both inside and outside mathematics. Acting as a lingua franca of mathematics and\nscience, category theory might be able to give a unifying structure to the field of machine learning.\nThis could solve some of the aforementioned problems.\nIn this work, we mainly focus on the application of category theory to deep learning. Namely, we\ndiscuss the use of categorical optics to model gradient-based learning, the use of categorical algebras\nand integral transforms to link classical computer science to neural networks, the use of functors to\nlink different layers of abstraction and preserve structure, and, finally, the use of string diagrams to\nprovide detailed representations of neural network architectures.", "sections": [{"title": "Introduction", "content": "In the last seventy years, machine learning has gone from being a curiosity to being one of the most\nexciting frontiers of engineering. The ever-increasing amount of research carried out in the field has\nlead to incredible advancements and, nowadays, machine learning models have a substantial impact\nmany areas of science ([HSK+23]) and society ([KM23]). Despite its unquestionable successes, the\nfield also faces significant challenges on many levels. Setting aside ethical and societal considerations,\nwhich are not discussed in this work, there are also significant scientific concerns. First of all, machine\nlearning, and deep learning in particular, lack strong theoretical underpinnings: at the moment, there\nis no general mathematical theory of machine learning, and the field looks more like alchemy than\nscience ([Gav24b], [Rah17]). Scientists and engineers discover and optimize models by trial and error\nwith no direction and no frame of reference. This leads to a lot wasted time and resources and clearly\nhampers future progress.\nAt the same time, bad incentives in the machine learning community lead to issues with the\nresearch process itself ([SGW21]). [OC17] points out the chaotic state of research and likens it to\ntechnical debt in computer engineering, coining the phrase \"research debt\": bad notation, unclear\nexplanations, and unproven conjectures clutter the machine learning research panorama and make it\ndifficult to actually produce new research. These factors, together with erroneous or missing statistical\nanalysis, also lead to widespread reproducibility issues ([GCKG22]). For instance, [Raf19] found that\nmore than half of the 255 machine learning papers they considered were not reproducible.\nThese issues are especially significant in deep learning, where models often reveal themselves\nto be brittle black boxes: minor changes in hyperparameters or architectures have large effects on\nthe performance of deep neural networks, and it is often impossible to explain their inner workings\nproactively ([Gav24b]). Despite this, deep learning models are being deployed at scale, with little\nconcern for their flaws.\nIn the most general terms possible, category theory is the branch of mathematics that deals with\nstructure. From its start in algebraic topology, category theory has risen to be one of the most\nfoundational areas of mathematics. Category theory can be seen as a lingua franca of mathematics\nthat unites an ever expanding number of mathematical fields under the same fundamental notions.\nMany have likened the rise of category theory as an extension of the Erlangen Programme that unified\ngeometry at the end of the nineteenth century and, from this point of view, categories might be seen\nas the pulsating heart of mathematics.\nMore recently, the field of applied category theory has developed, with the aim to applying cate-\ngory theory more generally as a unifying language of science and even engineering. Category theory\nhas found applications in resource theory ([CFS16]), database theory ([Spi12]), quantum mechanics\n([AC09]), and much more. Applied category theory can be seen as the study of compositionality\nand is thus useful wherever compositional structure arise, regardless of the nature of the objects or\nphenomena at hand. Compositionality here is defined as the property of systems and relationships\nthat \u201ccan be combined to form new systems or relationships\u201d ([FS18]).\nCompositionality is a good property that should be strived for because it offers insight into the\nfundamental structure of the systems at hand: large compositional systems can be broken up into\nsmaller systems that are easier to understand; conversely, small compositional systems can be as-\nsembled into larger systems without requiring any paradigm shift. Machine learning models such as\nBayesian networks and neural networks are inherently modular, and so it makes sense to investigate\ntheir compositionality using category theory. More generally, many authors think that category theory\ncould provide a unifying structure for machine learning, and thus could help solve or mitigate many\nof the problems discussed above. Hence, starting from the seminal paper [FST19], a large amount of\nresearch has explored the intersection between machine learning and category theory.\nThis thesis was originally meant to be a general survey of the intersection between machine learning\nand category theory, in the style of [SGW21]. However, we soon realized that the field has grown so\nmuch in the last few years that it has become impossible to do justice to every interesting approach\nin the space and time available to us. Thus, we opted to focus on categorical approaches to deep\nlearning and we decided to focus this thesis around four main ideas, described in the same number of\nchapters. Each chapter provides detailed descriptions of a few relevant approaches, and then compares\nsuch approaches with other related works, which are only briefly touched upon. Brief summaries of\nthe chapters are listed below.\n\u2022 Chapter 1: Parametric Optics for Gradient-Based Learning. Gradient-based learning\ncan be described and implemented within categories of adequately chosen parametric optics.\n\u2022 Chapter 2: From Classical Computer Science to Neural Networks. Category theory\ncan build a bridge between classical computer science and neural networks, offering insight into\ncurrently known neural network architectures and informing the design of novel ones.\n\u2022 Chapter 3: Functor Learning. Learning functors between categories instead of just mor-\nphisms between objects allows the models at hand to preserve structure, which can lead to better\nlearning outcomes.\n\u2022 Chapter 4: Detailed Representations of Neural Networks. Appropriate classes of string\ndiagrams can be used to represent neural network architectures with all the detail necessary for\nimplementation.\nWe emphasize that this work only covers a small amount of the available research: for instance,\nwe do not even touch upon the enormous and enormously relevant field of categorical probabilistic\nlearning, nor do we discuss categorical approaches to explainable artificial intelligence. We do not\neven cover every categorical approach to neural networks. Nevertheless, we believe that this thesis\ncan offer an interesting taste of the research that is going on in the field.\nThis work is aimed towards people who already know the basics of both category theory and\ndeep learning. The readers are not required to be extremely proficient in neither, but we will assume\nthat they already have some familiarity with neural network architectures, gradient-based learning,\nbasic categorical definitions, and the theory of symmetric monoidal categories. Regrettably, we do not\nhave enough space to provide our own supplementary material on these subjects, but we encourage\ninterested readers to consult the repository [Gav23], for an introduction to category theory, and the\nonline textbook [ZLLS21], for an introduction to deep learning. [FS18] is a particularly good starting"}, {"title": "Parametric Optics for Gradient-Based Learning", "content": "Despite the unquestionable success that gradient-based deep learning has enjoyed in recent years,\nthe field is still both young and poorly understood. As mentioned in the introduction, the lack of\ntheoretical underpinnings means that good performance is highly dependent on ad hoc choices and\nempirical heuristics leading to brittleness and poorly understood phenomena ([SGW21], [Gav24b]).\nThe ever-growing complexity of deep learning models poses significant challenges both in terms of\noptimization ([Ell18]) and architectural design ([GLD+24]), and, while there are a number of general\npurpose deep learning libraries that automatically implement backpropagation and provide tools for\ndesigning a wide variety of neural networks, these tools often rely on inelegant machinery difficult to\nparallelize ([Ell18]). Given the ever-increasing role gradient based learning plays in the sciences, in\nindustry, and in everyday life, solving these issues is of the utmost importance.\nHence, it would be desirable to develop a mathematically structured framework for gradient-\nbased learning able to act as a bridge between low-level automatic differentiation and high level\narchitectural specifications ([Gav24b]). The great number of architectures developed in recent years\nand the inherently modular structure of deep neural networks call for a model which is general (that is,\nnot dependent on a specific differentiation algorithm or a specific optimizer) and compositional (that\nis, we should be able to predict the behavior of the entire model if the behavior of each part is known).\n[CGG+22] and [Gav24b] propose a promising combination of differential categories, parametrization\nand optics as a full-featured gradient-based framework able to challenge established tools and attack\nopen problems. In this chapter, we illustrate such framework and part of its mathematical foundations."}, {"title": "Categorical toolkit", "content": "Learning neural networks have two important properties: they depend on parameters and infor-\nmation flows through them bidirectionally (forward propagation and back propagation). Any aspiring\ncategorical model of gradient-based learning must take these two aspects into consideration. A number\nof authors (among which [CGG+22] and [Gav24b]) propose the Para construction as a categorical\nmodel of parameter dependence and various categories of optics as the right categorical abstraction\nfor bidirectionality."}, {"title": "Actegories", "content": "Before we can deal with parametric maps, we need to find a way to glue input spaces to parameter\nspaces, so that such maps have well-defined domains. One common strategy is to provide the category"}, {"title": "The Para construction", "content": "Suppose we have an M-actegory (C, \u2022). We wish to study maps in C which are parametrized using\nobjects of M, that is, maps in the form P \u2022 A \u2192 B. We are not just interested in the maps by\nthemselves, but also in their compositional structure. Thus, we abstract away the details by defining\na new category Para. (C) (first introduced in simplified form by [FST19]). Since we also want to\nformalize the role of reparametrization, we actually construct Para. (C) as a bicategory, so that a\n0-cell A can serve as an input/output space, a 1-cell (P, f) can serve as a parametric map, and, finally,\na 2-cell r can serve as a reparametrization.\n\n\u2022 The 1-cell composition law is\n(P, f) ; (Q, g) = (Q \u2297 P, (Q \u2022 f) ; g).\nIt is quite handy to represent the cells of Para.(C) using the string diagram notation illustrated in Fig.\n1.1. The Para construction has a dual coPara construction whose 1-cells f : coPara. (C)(A, B) take\nthe form (P, f), where f : A \u2192 P\u2022 B. Cells in coPara.(C) can also be represented with appropriate\nstring diagrams. The reader can find a complete definition in [Gav24b].\n\nIt is shown in [Gav24b] that Para. (C) is actually a 2-category if the underlying actegory is strict.\nAssuming this is the case (as we do in this thesis), we can use a functor F : Cat \u2192 Set to quotient out\nthe 2-categorical structure and turn Para. (C) into a 1-category F\u2217(Para.(C)). Here, F\u2217 : 2Cat \u2192 Set\nis the change of enrichment basis functor induced by F. This meaningfully recovers the 1-categorical\nperspective of [FST19].\nBoth Para.(C) and coPara.(C) can be given a monoidal structure if (C, \u2022) is a monoidal acte-\ngory. This is extremely important because it allows us to compose (co)parametric morphisms both in\nsequence and in parallel. Once again, more detail can be found in [Gav24b].\nRemark 7. Another way to parametrize morphisms is the coKleisli construction. As noted by\n[Gav24b], the main difference between coKl and Para is that the parametrization offered by coKl is\nglobal, while the parametrization offered by Para is local: all morphisms in coKl(X \u00d7 \u2212) must take a\nparameter in X, while different morphisms of Para(C) admit different parameter spaces. Nevertheless,\nthe two constructions are related, and the former can be embedded into the latter."}, {"title": "Optics", "content": "If we take a parametrized category Para.(C) and we restrict our attention to morphisms parametrized\nwith the monoidal identity I, we get back the original category C. This is expressed by the following\nproposition ([Gav24b]).\nProposition 8. Let (C, \u2022) be an M-actegory. Then, there exists an identity-on-objects pseudofunctor\n\u03b3 : C \u2192 Para. (C) that maps f \u2192 (I, f). If M is strict, this is a 2-functor.\nModelling bidirectional flows of information is not only useful in machine learning, but also in\ngame theory, database theory, and more. As such, categorical tools for bidirectionality have been\nsought after for a long time: in particular, a great deal of effort has been devoted to the development\nof lens theory. Lenses have then been generalized into optics (see e.g. [Ril18]) to subsume other tools\nsuch as prisms and traversals into a single framework. Finally, there have also been various attempts\nto generalize optics (see e.g. [CEG+24] for a definition of mixed optics). We will introduce lenses and\noptics, and focus on the generalization of optics introduced by [Gav24b]: weighted optics.\nAs stated in [Gav24b], there is no standard definition of lens, and different authors opt for different\nad hoc definitions that best suit their purposes. We will borrow the perspective of [CGG+22].\n\nf': A \u00d7 B'  A'; f is known as the forward pass of the lenswhereas f' is known as the\nbackward pass;\n\n\n\u2022 the composition of \n\nLenses are a powerful tool, but they cannot be used to model all situations: for instance, lenses\ncannot be used if we wish to be able to choose not to interact with the environment depending on the"}, {"title": "Weighted optics", "content": "input, or if we would like to reuse values computed in the forward pass for further computation in the\nbackward pass.\nOptics generalize lenses by weakening the link between forward and backward passes, and by replac-\ning the Cartesian structure of the underlying category with a simpler symmetric monoidal structure.\nIn an optic over C, an object M: Cacts as an inaccessible residual space transferring information\nbetween the upper components and the lower component. We provide the definition given by [Ril18]1.\n\nf': M \u2297 B' \u2190 A', where M:Cis known as residual space; such pairs  are also\nquotiented by an equivalence relation that allows for reparametrization of the residual space and\neffectively makes it inaccessible;\nRefer to [Ril18] or [Gav24b] for the more information about the composition of optics and the repre-\nsentation of optics with string diagrams.\nLenses come up as a special case of optics ([Ril18]), and optics do solve some of the issues we\nhave with lenses. However, optics are not perfect either: for instance, [Gav24b] points out that optics\ncannot be used in cases where we ask that the forward pass and backward pass are different kind of\nmaps, as they are both forced to live in the same category. Thus, a further layer of generalization is\nuseful: namely, weighted optics.\nBefore we define weighted optics, we need to introduce a new tool to our toolbox: the category of\nelements of a functor.\n\n\u00b2Weighted optics also admit a coend definition. Refer to [Gav24b] for more information.\nwhere \u03c0o\u2217 is the enrichment base change functor generated by the connected component functor \u03c0o :\nCat \u2192 Set. More explicitly, \u03c0o\u2217 quotients out the connections provided by reparametrizations.\n\nThen, if S is the case, a  map is a triplet ((M),s, (f)),\nwhere M is the forward residual, M' is the backward residual, s : M \u2192 M' links the two residuals,\nf: X \u2192 MY is the forward pass, and f' : M' \u2022 Y' \u2192 X' is the backward pass. The triplets are also\nquotiented with respect to reparametrization, which makes the residual spaces effectively inaccessible\n(as it happens in the case of ordinary optics). We can get a clear \"operational\" understanding of how\na weighted optic works looking at an associated string diagram (see Fig. 1.3): data from X flows\nthrough the forward map, which computes an output in Y and a forward residual in M. Such forward\nresidual is then converted into a backward residual in M' by the map s, which is provided by the\nweight functor. Finally, the backward residual is used by f', together with input from Y', in order\nto compute a value in X'. A full account of the composition law for weighted optics can be found on\n[Gav24b]. As stated in [Gav24b], since coPara can be given a monoidal structure, we can also give\nOpticW :) one such structure as long as the underlying actegories are monoidal and the weight functor\nW is braided monoidal.\nThe advantages of weighted optics over ordinary optics are clear: when dealing with weighted\noptics, we are no longer forced to take reverse maps from the same category as the forward maps.\nThe action on the category of forward spaces is now separated from the action on the category of\nbackward spaces, and the link between the two actions is provided by an external functor. Such\nmodular approach provides a great deal of conceptual clarity and flexibility, more than regular optics\nor lenses can provide on their own. It is also shown in [Gav24b] that weighted optics are indeed a\ngeneralization of optics. In particular, it is shown that the lenses in Def. 9 are the specialized weighted\noptics obtained when C = D is Cartesian and the actegories are given by the Cartesian product. More"}, {"title": "Differential categories", "content": "generally, [Gav24b] claims that to the best of the author's knowledge - all definitions of lenses\ncurrently used in the literature are subsumed by the definition of mixed optics (see [CEG+24]), which\nare themselves a special case of weighted optics. Hence, all lenses are weighted optics.\n[Gav24b] goes on to apply the Para construction onto weighted optics, obtaining parametric\nweighted optics, which are proposed as a full-featured model for deep learning. The author conjectures\nthat \"weighted optics provide a good denotational and operational semantics for differentiation\". In its\nfull, generality, this is still an unproven conjecture. However, restricting our attention to a special class\nLens of lenses with an additive backward passes yields a formal theory of \"backpropagation through\nstructure\" ([Gav24b]), which will be illustrated in the rest of the chapter, after a short digression on\ndifferential categories.\nModelling gradient-based learning obviously requires a setting where differentiation can take place.\nAlthough it is tempting to directly employ smooth functions over Euclidean spaces, recent research\nhas shown that there are tangible advantages in working with generalized differential combinators\nthat extend the notion of derivative to polynomial circuits ([WZ22], [WZ21]), manifolds ([PVM+21]),\ncomplex spaces ([BQL21]), and so on. Thus, it makes sense to work with an abstract notion of\nderivative which can then be appropriately implemented depending on the requirements at hand.\nOne approach to this problem involves the explicit definition of two kinds of differential categories:\nCartesian differential categories (first introduced in [BCS06]) and Cartesian reverse differential cate-\ngories (first introduced by [CCG+19]). The former allow for forward differentiation, while the latter\nallow for reverse differentiation. We will omit the defining axioms for the sake of brevity, but the\nreader can find complete definitions in [CCG+19].\n\nR[f] : (x,y) \u2192 I f(x)Ty\ninduce well-defined combinators D and R. This is only a partial coincidence: as shown in [CCG+19]\nCRDCs are always CDCs under a canonical choice of differential combinator. The converse, however,\nis generally false.\nAs it turns out, forward differentiation tends to be less efficient when dealing with neural networks\nthat come up in practice ([Ell18]), so CDCs are not extremely useful when studying deep learning.\nCRDCs, on the other hand, have been applied to great success (see e.g. [CGG+22]). As shown in"}, {"title": "Parametric lenses", "content": "[WZ22], a large supply of CRDCs can be obtained by providing the generators of a finitely presented\nCartesian left-additive category with associated reverse derivatives (as long as the choices of reverse\nderivative are consistent). Moreover, CRDCs have been recently generalized by [Gav24b] to coalgebras\nassociated with copointed endofunctors, which could also increase the number of known CRDCs in\nthe future. The rest of this section is devoted to this generalization.\nIt is shown in [Gav24b] that there is a particular class of weighted optics which is useful for reverse\ndifferentiation, being able to represent both maps (through forward passes) and the associated reverse\nderivatives (through backward passes). Moreover, such weighted optics can be represented as lenses\nin the sense of Def 9, which means that their inner workings can be pictured in a simple, intuitive\nway.\nThen, we can define the category of lenses with backward passes additive in the second component.\n\nExplicitly, a colagebra for Lens\u0104 is a pair (C, Rc) such that C : CLACat and Rc : C \u2192 Lens\u0104 (C)\nsatisfies Rc; Ec  idc. The intuition behind such definition is that Re should map f = . \nwhere R is a generalized reverse derivative combinator. [Gav24b] shows that ordinary CRDCs are\ngeneralized CRDCs under this definition of Re.\nWe conclude this section discussing the relation between the Para construction and the Lens A\nendofunctor. [Gav24b] and [GLD+24] show that, under an appropriate definition, actegorical strong\nfunctors induce 2-functors between parametric 2-categories.\nProp 24 Para 2-functor\n\nAs a consequence, it can be shown that, if (C, Rc) is a generalized CRDC, Rc induces a 2-functor\nPara(Rc): Parax(C) \u2192 Para.(Lens\u2081(C)), which takes a parametric map f:P \u00d7 A \u2192 Band\naugments is with its reverse derivative R[f], forming a parametric lens. Parametric lenses behave very\nsimilarly to lenses, but we provide a separate stand-alone definition (which we take from [CGG+22])\nfor the reader's convenience."}, {"title": "Supervised learning with parametric lenses", "content": "In this section, we show how parametric lenses can be used to model supervised gradient-based\nlearning ([CGG+22], [Gav24b], [SGW21]). While lenses are not as general as weighted optics, it is\nshown in [CGG+22] that they are powerful enough for most purposes and that there is empirical\nevidence of their applicability and performance. The paper also discusses the use of parametric lenses\nin modeling unsupervised deep learning and deep dreaming, but we do not have enough space to\ndiscuss this topic."}, {"title": "Model, loss, optimizer, learning rate", "content": "Supervised gradient-based learning can be modeled using parametric lenses as follows:\n1. we can design an architecture (P, Model) as a parametric morphism in Para. (C) for some gen-\neralized CRDC (C, Rc);\n2. we can use the functor Rc to endow (P, Model) with its reverse derivative R[(P, Model)], yielding\na lens in Para.(Lens(C));\n3. we can use 2-categorical machinery of Para.(Lens\u0104(C)) to provide a loss function, a learning\nrate, and an optimizer, which can be assembled onto Rc (P, Model) to yield a supervised learning\nlens able to update parameters based on inputs and predictions;\n4. we can use copy maps from the Cartesian structure of C to create a learning iteration.\nThe theory of parametric optics and differential categories does not offer explicit insight with\nrespect to architecture design, so we will assume a good architecture has already been designed4. Given\nan architecture (P, Model), it can be embedded into Para.(Lens(C)) as a lens  by\nbreaking it up into its basic components (such as linear layers, convolutional layers, etc.), augmenting\nsuch components with their reverse derivatives, and the composing the resulting lenses. The backward\npass of the composition is the reverse derivative of its forward pass because Rc is a functor5. Many\nexamples can be found in [CGG+22].\nUpdating the parameters based on data requires a loss function, an optimizer and a learning\nrate. Loss functions can be implemented as parametric lenses which take in predictions as input and\nlabels as parameters. The output they produce can be considered the actual loss that needs to be\ndifferentiatied. Given a model parametric lens and a loss parametric\nlens R[Loss] : (BB)\u2192(1), the composition  takes in features as input and\ntakes model parameters and labels as parameters. Then, this information is used to compute the loss\nassociated with the predictions of the model.\nIt can be helpful to think about dangling wires in the diagrams as open slots where other com-\nponents can be plugged. For instance, the diagram of Fig. 1.5 (a) has dangling wires labeled with\nL on its right. We can use a learning rate lens \u03b1 to link these wires and allow forward-propagating\ninformation to \"change direction\" and go backwards. \u03b1 must have domain equal to (1) and codomain\nequal to (1), where 1 is the terminal object of C. For instance, if C = Smooth, \u03b1 might just mul-"}, {"title": "Weight tying, batching, and the learning iteration", "content": "tiply the loss by some , which is what machine learning practitioners would ordinarily call learning\nrate. Fig. 1.5 (b) shows how a learning rate can be linked to the loss function and the model using\npost-composition.\nThe final element needed for the model Model in Fig. 1.5 (b) to learn is an optimizer. It is\nshown in [CGG+22] that optimizers can be represented as reparametrisations in Para(Lens(C)).\nMore specifically, we might see an optimizer as a lens . We can plug such reparametrisation on top of the\nmodel, we can redirect the input wires of the model to convert them into parameters, and we can\nplug useless wires with delete maps taken from the Cartesian structure of C. We are then left with\na parametric lens with parameter space . This lens is pictured in Fig.\n1.5 (c).The diagram shows how the machinery hidden by the Para(Lens(C)) can take care of forward\npropagation, loss computation, backpropagation and parameter updating in a seamless fashion.\nBoth [CGG+22] and [Gav24b] emphasize the essential role played by weight tying in deep learning.\nWeight tying can be implemented within the parametric lens framework as a reparametrization that\ncopies a single parameter to many parameter slots (see Fig. 1.6 (a)): given (P \u00d7 P, f) : Para(C)(X, Y),\nwe can define (P, f\u25b3P) : Para(C)(X, Y) so that\nCopy maps can also be used for batching: batching is implemented by instantiating n different copies\nof our supervised learning lens (comprised of model, loss function, and learning rate) and tying the\nparameters to a unique value. Then, it suffices to feed the n data points to the n lenses, and we can\noptimize across a single parameter (see Fig. 1.6 (b))."}, {"title": "Empirical evidence", "content": "[CGG+22] introduces a possible representation for the whole learning iteration of a supervised\nlearning model as a single map. The paper suggests extracting the backward pass of the lens in Fig.\n1.5 (d) and reframing it as a P \u2192 P parametric map with parameters A\u00d7 B. Since this is an endomap,\nit can be composed n times with itself to obtain a P \u2192 P map, which is proposed as a model of the\nlearning iteration. While this approach requires breaking lenses apart, it is markedly simple.\nEmpirical evidence for the effectiveness of the parametric lens framework discussed in this section\ncan be found in [CGG+22], where the authors implement a Python library for gradient-based learning\nrooted in these ideas. They use the library to develop a MNIST classifier, obtaining comparable\naccuracy to models developed using traditional tools. The Python implementation of components\nof learning as parametric lenses is elegant and mathematically principled, as it mirrors an abstract\ncategorical structure. It is also insightful because it highlights possible generalizations, which manifest\nas simple modifications of existing lenses.\nThis kind of success story foreshadows a future where popular machine learning libraries also\nfollow elegant principled paradigms informed by category theory. Quoting [CGG+22] directly, \"[the]\nproposed algebraic structures naturally guide programming practice\"."}, {"title": "Future directions and related work", "content": "The parametric optic framework discussed in this chapter is very promising, but there is still a\nlot of work that needs to be done for it to reach its full potential. For instance, [Gav24b] conjectures\nthat weighted optics can be used in its full generality to model differentiation in cases which are not\ncovered by lenses. For instance, lenses cannot model automatic differentiation algorithms that do\nnot use gradient checkpointing, while weighted optics are conjectured to be able to do so. [Gav24b]\nsuggests investigating locally graded categories as potential replacements for actegories, and also\nsuggests investigating the applications of parametric optics to meta-learning, that is deep learning\nwhere the optimizers themselves are learned. Moreover, [CGG+22] conjectures that some of the\naxioms of CRDC may be used to model higher order optimization algorithms. Finally, as suggested\nby [CGG+22], future work might allow the parametric optic framework to encompass non-gradient\nbased optimizers such as the ones used in probabilistic learning. See [SGW21] for more on this topic."}, {"title": "Learners", "content": "We conclude this chapter by discussing three other directions of machine learning research that\nare closely related to the framework of parametric optics.\nOne of the first compositional approaches to training neural networks in the literature can be found\nin the seminal paper [FST19], which spurred a lot of research in the field, including what is presented\nin [Gav24b] and [CGG+22]. The authors introduce a category of learners, objects which are meant to\nrepresent neural network components and behave similarly to parametric lenses.\nLearners quotiented by an appropriate reparametrization relationship form a category Learn.\nA learner represents an instance of supervised learning: the implement function takes a param-\neter and implements a function and the update function updates the parameters using a data from\na dataset. The request function is necessary to implement backpropagation when optimizing a com-\nposition of learners. Suppose we select a learning rate and an error function e : R2 \u2192 R such\nthat y(xo, y) is invertible for all xo. It is argued in [FST19] that we can define a functor\nLe,e: Parax (Smooth) \u2192 Learn which takes a parametric map and yields an associated learner that\nimplements gradient descent.\nWe do not have the space to talk about learners at length, but we wish to draw a short compar-\nison between parametric weighted optics (and, in particular, parametric lenses) and the approach of\n[FST19], given the relevant position held by the latter in machine learning literature. The similarities\nbetween learner-based learning and lens-based learning are evident: every learner (P, I, U, r) looks\nlike a parametric lens, where I passes information forward, r passes information backwards and P\nis the parameter space. Moreover, the role of Lee is very similar to the role played by Para(Rc) in\noptic-based learning. Such similarities were even discussed in the original paper [FST19] and have\nbeen researched at length: it has been proved in [FJ19] that learners can be functorially embedded in\na special category of symmetric lenses (as opposed to the lenses of Def. 9, which are asymmetric).\nDespite the similarities, there is one fundamental difference between the lens-based approach and\nthe learner-based approach: each learner carries its own optimizer, whereas optimization of lenses is\nusually carried out separately. Moreover, if we compare parametric weighted optics with learners, the\nlatter clearly win in versatility, generality, and (at least from our point of view) conceptual clarity. It\nis argued in [SGW21] and [CGG+22] that the parametric lens framework largely subsumes the learner\napproach. More information regarding the comparison can also be found in [Gav24b]."}, {"title": "Exotic differential categories", "content": "We have presented the parametric weighted optic approach of [Gav24b] and [CGG+22] within the\ncontext of neural networks for the sake of simplicity, but the framework has been developed with\ngenerality in mind and applies to a much wider range of situations. For instance, we can easily replace\nSmooth with any other CRDC C, yielding a full-feature compositional framework for gradient-based\nlearning over C.\nSwitching to a different CRDC is useful because different differential categories can lead to different\nlearning outcomes, both in terms of accuracy of the model and in terms of computational costs\n([WZ22]). For instance, is argued in [WZ22] that polynomial circuits can be used to define and train\nintrinsically discrete machine learning models. Even 'radical'environments such as Boolean circuits\nwhere scalars reside in Z2 seem to be conductive to machine learning under the right choice\nof architecture and optimizer ([WZ21]). Using such exotic differential categories could be of great\nadvantage because they might be able to better reflect the intrinsic computational limits of computer\narithmetic, leading to more efficient learning ([WZ22])."}, {"title": "Functional reverse-mode automatic differentiation", "content": "Finally, we wish to highlight the similarities between the formal theory of differential categories\nillustrated here and the work in [Ell18"}]}