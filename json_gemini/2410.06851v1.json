{"title": "Understanding Model Ensemble in Transferable Adversarial Attack", "authors": ["Wei Yao", "Zeliang Zhang", "Huayi Tang", "Yong Liu"], "abstract": "Model ensemble adversarial attack has become a powerful method for generating transferable adversarial examples that can target even unknown models, but its theoretical foundation remains underexplored. To address this gap, we provide early theoretical insights that serve as a roadmap for advancing model ensemble adversarial attack. We first define transferability error to measure the error in adversarial transferability, alongside concepts of diversity and empirical model ensemble Rademacher complexity. We then decompose the transferability error into vulnerability, diversity, and a constant, which rigidly explains the origin of transferability error in model ensemble attack: the vulnerability of an adversarial example to ensemble components, and the diversity of ensemble components. Furthermore, we apply the latest mathematical tools in information theory to bound the transferability error using complexity and generalization terms, contributing to three practical guidelines for reducing transferability error: (1) incorporating more surrogate models, (2) increasing their diversity, and (3) reducing their complexity in cases of overfitting. Finally, extensive experiments with 54 models validate our theoretical framework, representing a significant step forward in understanding transferable model ensemble adversarial attacks.", "sections": [{"title": "1 Introduction", "content": "Neural networks are highly vulnerable to adversarial examples [Szegedy et al., 2013, Goodfellow et al., 2014] -perturbations that closely resemble the original data but can severely compromise safety-critical applications [Zhang and Li, 2019, Kong et al., 2020, Bortsova et al., 2021, Zhang et al., 2024a, Tang et al., 2024a]. Even more concerning is the phenomenon of adversarial transferability [Papernot et al., 2016, Liu et al., 2017]: adversarial examples crafted to deceive one model often succeed in attacking others. This property enables attacks without requiring any knowledge of the target model, significantly complicating efforts to ensure the robustness of neural networks [Dong et al., 2019, Silva and Najafirad, 2020].\nTo enhance adversarial transferability, researchers have proposed a range of algorithms that fall into three main categories: input transformation [Xie et al., 2019, Wang et al., 2021], gradient-based optimization [Gao et al., 2020, Xiong et al., 2022], and model ensemble attacks [Li et al., 2020, Chen et al., 2024]. Among these, model ensemble attacks have proven especially powerful, as they leverage multiple models to simultaneously generate adversarial examples that exploit the strengths of each individual model [Dong et al., 2018]. Moreover,"}, {"title": "2 Related Work", "content": "Researchers have developed various algorithms to enhance adversial transferability. Most of them fall into three categories: input transformation, gradient-based optimization, and model ensemble attack. Input transformation techniques apply data augmentation strategies to prevent overfitting to the surrogate model. For instance, random resizing and padding [Xie et al., 2019], downscaling [Lin et al., 2019], and mixing [Wang et al., 2021]. Gradient-based optimization optimizes the generation of adversarial examples to achieve better transferability. Some popular ideas include applying momentum [Dong et al., 2018], Nesterov accelerated gradient [Lin et al., 2019], scheduled step size [Gao et al., 2020] and gradient variance reduction [Xiong et al., 2022]. Model ensemble attack combine outputs from surrogate models to create an ensemble loss, increasing the likelihood to deceive various models simultaneously. It can be applied collectively with both input transformation and gradient-based optimization algorithms [Tang et al., 2024b]. Some popular ensemble paradigms include loss-based ensemble [Dong et al., 2018], prediction-based [Liu et al., 2017], logit-based ensemble [Dong et al., 2018], and longitudinal strategy [Li et al., 2020]. Moreover, advanced ensemble algorithms have been created to ensure better adversarial transferability [Li et al., 2023, Wu et al., 2024, Chen et al., 2024]. An extended and detailed summary of related work is in Appendix C.\nWithin the extensive body of research on model ensemble attacks, two notable observations stand out. First, increasing the number of models in an ensemble improves adversarial transferability [Liu et al., 2017, Dong et al., 2018, Lin et al., 2019, Gubri et al., 2022a]. Second, using more diverse surrogate models with varying architectures and back-propagated gradients [Tang et al., 2024b] further enhances transferability. However, to our best knowledge, these intriguing phenomena have yet to be fully understood from a theoretical perspective. In this paper, we are the first to provide a theoretical explanation for them, offering insights that can guide future algorithm design."}, {"title": "2.1 Transferable Adversarial Attack", "content": "Researchers have developed various algorithms to enhance adversial transferability. Most of them fall into three categories: input transformation, gradient-based optimization, and model ensemble attack. Input transformation techniques apply data augmentation strategies to prevent overfitting to the surrogate model. For instance, random resizing and padding [Xie et al., 2019], downscaling [Lin et al., 2019], and mixing [Wang et al., 2021]. Gradient-based optimization optimizes the generation of adversarial examples to achieve better transferability. Some popular ideas include applying momentum [Dong et al., 2018], Nesterov accelerated gradient [Lin et al., 2019], scheduled step size [Gao et al., 2020] and gradient variance reduction [Xiong et al., 2022]. Model ensemble attack combine outputs from surrogate models to create an ensemble loss, increasing the likelihood to deceive various models simultaneously. It can be applied collectively with both input transformation and gradient-based optimization algorithms [Tang et al., 2024b]. Some popular ensemble paradigms include loss-based ensemble [Dong et al., 2018], prediction-based [Liu et al., 2017], logit-based ensemble [Dong et al., 2018], and longitudinal strategy [Li et al., 2020]. Moreover, advanced ensemble algorithms have been created to ensure better adversarial transferability [Li et al., 2023, Wu et al., 2024, Chen et al., 2024]. An extended and detailed summary of related work is in Appendix C.\nWithin the extensive body of research on model ensemble attacks, two notable observations stand out. First, increasing the number of models in an ensemble improves adversarial transferability [Liu et al., 2017, Dong et al., 2018, Lin et al., 2019, Gubri et al., 2022a]. Second, using more diverse surrogate models with varying architectures and back-propagated gradients [Tang et al., 2024b] further enhances transferability. However, to our best knowledge, these intriguing phenomena have yet to be fully understood from a theoretical perspective. In this paper, we are the first to provide a theoretical explanation for them, offering insights that can guide future algorithm design."}, {"title": "2.2 Theoretical Understanding of Adversarial Transferability", "content": "In contrast to the wealth of empirical and intuitive studies, research on the theoretical understanding of adversarial transferability remains limited. Recent efforts have primarily focused on aspects such as data [Tram\u00e8r et al., 2017], surrogate model [Wang and Farnia, 2023], optimization [Yang et al., 2021, Zhang et al., 2024b, Chen et al., 2024] and target model [Zhao et al., 2023]. Tram\u00e8r et al. [2017] investigates the space of transferable adversarial examples and establishes conditions on the data distribution that suggest transferability for some basic models. In terms of the surrogate model generalization, Wang and Farnia [2023] builds the generalization gap to show that a surrogate model with a smaller generalization error leads to more transferable adversarial examples. From an optimization perspective, Yang et al. [2021], Zhang et al. [2024b] establish upper and lower bounds on adversarial transferability, linking it to model smoothness and gradient similarity, while Chen et al. [2024] provides theoretical evidence connecting transferability to loss landscape flatness and closeness to local optima. Regarding the target model, Zhao et al. [2023] theoretically reveals that reducing the discrepancy between the surrogate and target models can limit adversarial transferability.\nDespite these theoretical advances, to the best of our knowledge, transferable model ensemble adversarial attacks remain unexplored. To address this gap, we take a pioneering step by presenting the first theoretical analysis of such attacks. Our work not only offers theoretical insights into these attacks but also incorporates recent advancements in learning theory, laying the groundwork for future theoretical investigations into adversarial transferability."}, {"title": "3 Key Definitions: Transferability Error, Diversity, and Ensemble Complexity", "content": "In this section, we first highlight the fundamental goal of model ensemble adversarial attack (Section 3.1). Then we define the transferability error (Section 3.2), diversity in transferable model ensemble attack (Section 3.3) and empirical model ensemble Rademacher complexity (Section 3.4)."}, {"title": "3.1 Model Ensemble Adversarial Attack", "content": "Given a feature space X C Rd and a label space Y = {\u22121,1}, we have a joint distribution Pz over the input space Z = X\u00d7Y. The training set Ztrain = {zizi = (Xi, Yi) \u2208 Z, i = 1,\u2026\u2026 , K}, which consists of K examples drawn independently from Pz. We denote the hypothesis space by H : X \u2192 Y and the parameter space by \u0398. Let f(0; \u00b7) \u2208 H be a classifier parameterized by \u03b8\u0395\u0398, trained for a classification task using a loss function l : Y X Y\u2194 R. Let Pe represent the distribution over the parameter space O. Define Pen as the joint distribution over the product space ON, which denotes the space of N such sets of parameters. We use Ztrain to train N surrogate models f(01;\u00b7),\u2026\u2026\u2026, f(0N;) for model ensemble. The training process of these N classifiers can be viewed as sampling the parameter sets (01,..., 0v) from the distribution Pen. For a clean data 2 = (x, y) \u2208 Z, an adversarial example z = (x, y) \u2208 Z, and N classifiers for model ensemble attack, define the population risk Lp(z) and the empirical risk LE(z) of the adversarial example z as\n$L_p(z) = E_{\\theta \\sim P_{\\Theta}}[l(f(\\theta; x), y)],$ (1)\nand $L_E(z) = \\frac{1}{N} \\sum_{i=1}^{N} l(f(\\theta_i;x), y).$ (2)\nIntuitively, a transferable adversarial example leads to a large Lp(z) because it can attack many classifiers with parameter \u03b8\u2208 \u0398. Therefore, the most transferable adversarial example z* = (x*, y) around z is defined as\n$x^* = \\arg \\max_{x\\in B_{\\epsilon}(\\hat{x})} L_p(\\hat{z}),$ (3)\nwhere $B_{\\epsilon}(\\hat{x}) = \\{x : ||x - \\hat{x}||_2 < \\epsilon\\}$ is an adversarial region centered at \u00ee with radius \u20ac > 0. However, the expectation in Lp(z) cannot be computed directly. Thus, when generating adversarial examples, the empirical version Eq. (2) is used in practice, such as loss-based ensemble attack [Dong et al., 2018]. So the adversarial example z = (x, y) is obtained from\n$x = \\arg \\max_{x\\in B_{\\epsilon}(\\hat{x})} L_E(\\hat{z}).$ (4)\nThere is a gap between the adversarial example z we find and the most transferable one z*. It is due to the fact that the ensemble classifiers cannot cover the whole parameter space of the classifier, i.e., there is a difference between Lp(z) and LE(z). Accordingly, the core objective of transferable model ensemble attack is to design approaches that approximate LE(z) to Lp(z), thereby increasing the transferability of adversarial examples."}, {"title": "3.2 Transferability Error", "content": "Considering the difference between z and z*, the transferability of an adversarial example z can be characterized as the difference in population risk between it and the optimal one.\nDefinition 1 (Transferability Error). The transferability error of z with radius e is defined as:\n$TE(z,\\epsilon) = L_p(z^*) \u2013 L_p(z).$ (5)\nThere always holds TE(z, \u0454) \u2265 0 as Lp(z*) > Lp(z). The closer TE(z, e) is to 0, the better the transferability of z. Therefore, in principle, the essential goal of various model ensemble attack algorithms is to make transferability error TE(z, e) as small as possible. Moreover, if the distribution over the parameter space Pe, adversarial region Be(x) and loss function l"}, {"title": "3.3 Quantifying Diversity in Model Ensemble Attack", "content": "Before the advent of model ensemble attacks, the formal definition of diversity in ensemble learning had remained a long-standing challenge for decades [Wood et al., 2023]. Unfortunately, this challenge continues to persist in the context of transferable model ensemble attacks. Despite various intuitive approaches [Li et al., 2020, Tang et al., 2024b], there is still no widely accepted method for rigorously quantifying diversity. In the following definition, we propose measuring diversity among ensemble attack classifiers through prediction variance, building on recent advances in ensemble learning theory [Ortega et al., 2022, Wood et al., 2023].\nDefinition 2 (Diversity of Model Ensemble Attack). The diversity of model ensemble attack across 0 ~ Pe for a specific adversarial example z = (x,y) is defined as the variance of model prediction:\n$Var_{\\theta \\sim P_{\\Theta}} (f(\\theta;x)) = E_{\\theta\\sim P_{\\Theta}} [f(\\theta;x) \u2013 E_{\\theta\\sim P_{\\Theta}}f(\\theta; x)]^2 .$ (7)\nIt indicates the degree of dispersion in the predictions of different ensemble classifiers for the same adversarial example. The diversity of model ensemble attack is a measure of ensemble member disagreement, independent of the label. From an intuitive perspective, the disagreement among the ensemble components helps prevent the adversarial example from overfitting to the classifiers in the ensemble, thereby enhancing adversarial transferability to some extent.\nTo calculate the diversity explicitly as a metric, we consider a dataset of adversarial examples Zattack = {ZiZi = (Xi, Yi), i = 1,\u2026, M} and N classifiers in the ensemble. The diversity is computed as the average sample variance of predictions for all adversarial examples in the dataset:\n$Diversity = \\frac{1}{M} \\sum_{i=1}^{M} [\\frac{1}{N} \\sum_{j=1}^{N} f(\\theta_j; x_i) \u2013 \\frac{1}{N} \\sum_{j=1}^{N} f(\\theta_j; x_i)]^2$ (8)\nRemark. For multi-class classification problems, f(0;x) is replaced with the logit corresponding to the correct class prediction made by the classifier."}, {"title": "3.4 Empirical Model Ensemble Rademacher Complexity", "content": "We define the empirical Rademacher complexity for model ensemble by analogy to the original empirical Rademacher complexity [Koltchinskii and Panchenko, 2000, Bartlett and Mendelson, 2002].\nDefinition 3 (Empirical Model Ensemble Rademacher Complexity). Given the input space Z = X \u00d7 Y and N classifiers f(01;\u00b7),\u2026\u2026, f(0v;\u00b7). Let \u03c3 = {i}i\u2208[N] be a collection of independent Rademacher variables, which are random variables taking values uniformly"}, {"title": "4 Theoretically Reduce Transferability Error", "content": "4.1 Vulnerability-diversity Decomposition of Transferability Error\nInspired by the bias-variance decomposition [Geman et al., 1992, Domingos, 2000] in learning theory, we provide the corresponding theoretical support for prediction variance by decomposing the transferability error into vulnerability, diversity and constants.\nTheorem 1 (Vulnerability-diversity Decomposition). Consider the squared error loss l(f(0;x),y) = [f(0;x) - y]\u00b2 for a data point z = (x,y). Let f(0;x) = Eo\u223cPof(0;x) be the expectation of prediction over the distribution on the parameter space. Then there holds\n$TE(z,\\epsilon) = L_p(z^*) \u2013 l(\\bar{f(\\theta; x)}, y) \u2013 Var_{\\theta \\sim P_{\\Theta}} f(\\theta; x).$ (10)"}, {"title": "4.1 Vulnerability-diversity Decomposition of Transferability Error", "content": "Inspired by the bias-variance decomposition [Geman et al., 1992, Domingos, 2000] in learning theory, we provide the corresponding theoretical support for prediction variance by decomposing the transferability error into vulnerability, diversity and constants.\nTheorem 1 (Vulnerability-diversity Decomposition). Consider the squared error loss l(f(0;x),y) = [f(0;x) - y]\u00b2 for a data point z = (x,y). Let f(0;x) = Eo\u223cPof(0;x) be the expectation of prediction over the distribution on the parameter space. Then there holds\n$TE(z,\\epsilon) = L_p(z^*) \u2013 l(\\bar{f(\\theta; x)}, y) \u2013 Var_{\\theta \\sim P_{\\Theta}} f(\\theta; x).$ (10)\nThe proof and the empirical version of it is in Appendix B.2. The \"Vulnerability\" term measures the risk of a data point z being compromised by the model ensemble. If the model ensemble is sufficiently strong to fit the direction opposite to the target label, the resulting high loss theoretically reduces the transferability error. This insight suggests that selecting strong attackers as ensemble components leads to lower transferability error. The \"Diversity\" term implies that selecting diverse attackers in a model ensemble attack theoretically contributing to a reduction in transferability error. In conclusion, Theorem 1 provides the following guideline for reducing transferability error in model ensemble attack: we are supposed to choose ensemble components that are both strong and diverse.\nRemark. This theoretical result connects the existing body of work and clarifies how each algorithm strengthens adversarial transferability. For instance, some approaches tend to optimizing the attack process [Xiong et al., 2022, Chen et al., 2023] to improve \u201cVulnerability\u201d, while others aim to diversify surrogate models [Li et al., 2020, 2023, Wang et al., 2024] to enhance \u201cDiversity\u201d. Further discussion of these algorithms is provided in the Appendix D.4.\nHowever, due to the mathematical nature of Eq. (10), there remains a vulnerability-diversity trade-off in model ensemble attacks, similar to the well-known bias-variance trade-off [Geman et al., 1992]. This means that, in practice, it is not feasible to maximize both \"Vulnerability\" and \"Diversity\" simultaneously. Recognizing this limitation, we proceed with further theoretical analysis to propose more guidelines for practitioners in the following section."}, {"title": "4.2 Upper Bound of Transferability Error", "content": "We develop an upper bound of transferability error in this section. We begin by taking Multi-Layer Perceptron (MLP) as an example of deep neural network and derive the upper bound of RN (Z). The proof is in Appendix A.4.\nLemma 2. Let $H = \\{x \\rightarrow W_l\\phi_{l-1} (W_{l-1}\\phi_{l-2} (... \\phi_1 (W_1x)))\\}$ be the class of real-valued networks of depth l, where $x \\in \\mathbb{R}^{d_1}, W_i \\in \\mathbb{R}^{d_{i+1}\\times d_i}$. Given N classifiers from H, where the"}, {"title": "4.3 Generalization and Adversarial Transferability", "content": "We offer new insights into a foundational and popular analogy in the literature: The transferability of an adversarial example is an analogue to the generalizability of the model [Dong et al., 2018, Lin et al., 2019, Wang et al., 2021, Wang and He, 2021, Xiong et al., 2022, Chen et al., 2024]. Interestingly, our theory also sheds light on this insight in several ways.\nFirst, the mathematical formulations in Lemma 1 is similar to generalization error [Vapnik, 1998, Bousquet and Elisseeff, 2002]. Also, Lemma 2 is similar to the bound of the original Rademacher complexity [Golowich et al., 2018]. More importantly, recall that in the conventional framework of learning theory: (1) increasing the size of training set typically leads to a better generalization of the model [Bousquet and Elisseeff, 2002]; (2) improving the diversity among ensemble classifiers makes it more advantageous for better generalization [Ortega et al., 2022]; and (3) reducing the model complexity [Cherkassky, 2002] may mitigate overfitting and benefit the generalization ability. These ideas correspond to each of our theoretical understanding in Section 4. Overall, we support the analogy from a theoretical perspective. And further detailed discussion is in Appendix D.3."}, {"title": "5 Experiments", "content": "We conduct our experiments on four datasets, including the MNIST [LeCun, 1998], Fashion-MNIST [Xiao et al., 2017], CIFAR-10 [Krizhevsky et al., 2009], and ImageNet-1K [Russakovsky et al., 2015] datasets. We use the first three datasets to empirically validate our theoretical contribution, and leave the experiments on ImageNet to build a powerful ensemble adversarial attack in practice.\nWe build six deep neural networks for image classification, including three MLPs with one to three hidden layers followed by a linear classification layer, and three convolutional neural networks (CNNs) with one to three convolutional layers followed by a linear classification layer. To ensure diversity among the models, we apply three different types of transformations during training. Additionally, we set the weight decay under the L2 norm to 10-4,10-3,10-2, respectively. This results in a total of 6 \u00d7 3 \u00d7 3 = 54 models. To establish a gold standard for adversarial transferability evaluation, we additionally train a ResNet-18 [He et al., 2016] from scratch on three datasets (MNIST, Fashion-MNIST, and CIFAR-10), respectively. We will leverage the models at hand to attack this ResNet-18 for a reliable evaluation."}, {"title": "5.1 Evaluation on the Attack Dynamics", "content": "For each dataset (MNIST & Fashion-MNIST & CIFAR-10), we record the attack success rate (ASR), loss value, and the variance of model predictions. The first metric exhibits an inverse relationship with transferability error. And the latter two metrics correspond to the"}, {"title": "5.2 Evaluation on the Ensemble Framework", "content": "We further validate the effectiveness of the vulnerability-diversity decomposition within the ensemble framework. Specifically, instead of focusing solely on the training dynamics, we progressively increase the number of models in the ensemble attack to evaluate the decomposition's impact. We begin by incorporating MLPs with different architectures and regularization terms, followed by CNNs. In total, up to 18 models are included in a single attack. We depicted the results in fig. 5."}, {"title": "6 Conclusion", "content": "In this paper, we address the underdeveloped theoretical foundation of transferable model ensemble adversarial attacks. We introduce three key definitions: transferability error, prediction variance, and empirical model ensemble Rademacher complexity. Through the vulnerability-diversity decomposition of transferability error, we identify a crucial trade-off between vulnerability and diversity in ensemble components, presenting the challenge of optimizing both simultaneously. To overcome this, we introduce recent mathematical tools and derive an upper bound on transferability error, offering practical guidelines for improving adversarial transferability. Our extensive experiments validate these insights, marking a significant advancement in the understanding and development of transferable model ensemble adversarial attacks."}, {"title": "Ethics Statement", "content": "We recognize the potential societal impact of our work on transferable adversarial attacks and emphasize its contribution to improving the robustness and security of machine learning models. We have been careful to ensure that our findings advance the public good by providing insights that can enhance defenses against malicious use of adversarial attacks, rather than contributing to harmful applications. Our study upholds high standards of scientific excellence through transparency, rigor, and reproducibility. No human subjects were involved, and no privacy or confidentiality concerns arise from the data used. We have"}, {"title": "A Proof of Generalized Rademacher Complexity", "content": "For simplicity, denote f(0i; x) as fi(x). For 1-Lipschitz loss function l(yf(x)) (for example, hinge loss l(f(x), y) = max (0,1 \u2013 yf(x)), there holds:\n$R_N(\\mathcal{Z}) = E_{\\sigma} \\sup_{\\mathcal{Z}\\in \\mathcal{Z}} [\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_i f_i(x)]$\n$< E_{\\sigma} \\sup_{\\mathcal{Z}\\in \\mathcal{Z}} [\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_i f_i(x)]$\n$= E_{\\sigma} \\sup_{\\mathcal{Z}\\in \\mathcal{Z}} [\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_i f_i(x)] := R_N(\\mathcal{Z}).$\nSo we can bound $R_N(\\mathcal{Z})$ instead of $R_N(\\mathcal{Z}).$\nGiven Section A.1, we provide the bound below.\nLemma 3 (Linear Model). Let $H = \\{x \\rightarrow w^Tx\\}$, where $x,w \\in \\mathbb{R}^d$. Given N classifiers from H, assume that $||x||_2 < B$ and $||w||_2 <C$. Then\n$R_N(\\mathcal{Z}) \\le \\frac{BC}{\\sqrt{N}}$\nProof. We have"}, {"title": "A.1 Preliminary", "content": "For simplicity, denote f(0i; x) as fi(x). For 1-Lipschitz loss function l(yf(x)) (for example, hinge loss l(f(x), y) = max (0,1 \u2013 yf(x)), there holds:\n$R_N(\\mathcal{Z}) = E_{\\sigma} \\sup_{\\mathcal{Z}\\in \\mathcal{Z}} [\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_i f_i(x)]$\n$< E_{\\sigma} \\sup_{\\mathcal{Z}\\in \\mathcal{Z}} [\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_i f_i(x)]$\n$= E_{\\sigma} \\sup_{\\mathcal{Z}\\in \\mathcal{Z}} [\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_i f_i(x)] := R_N(\\mathcal{Z}).$\nSo we can bound $R_N(\\mathcal{Z})$ instead of $R_N(\\mathcal{Z}).$"}, {"title": "A.2 Linear Model", "content": "Given Section A.1, we provide the bound below.\nLemma 3 (Linear Model). Let $H = \\{x \\rightarrow w^Tx\\}$, where $x,w \\in \\mathbb{R}^d$. Given N classifiers from H, assume that $||x||_2 < B$ and $||w||_2 <C$. Then\n$R_N(\\mathcal{Z}) \\le \\frac{BC}{\\sqrt{N}}$\nProof. We have"}, {"title": "A.3 Two-layer Neural Network", "content": "Given Section A.1, we provide the bound below.\nLemma 4 (Two-layer Neural Network). Let $H = \\{x \\rightarrow w\\phi(Ux)\\}$, where $x \\in \\mathbb{R}^d, U \\in \\mathbb{R}^{m\\times d}, w\\in \\mathbb{R}^m, m$ is the number of the hidden layer, and $\\phi(x) = max (0,x)$ is the element-wise ReLU function. Given N classifiers from H, assume that $||x||_2 \\le B, ||w||_2 \\le B'$, and $||U_i||_2 \\le C$, where Uj is the j-th row of U. Then\n$R_N(\\mathcal{Z}) \\le \\frac{\\sqrt{m}BB'C}{\\sqrt{N}}$\nProof. We have"}, {"title": "A.4 Proof of Lemma 2", "content": "For simplicity, denote f(0i; x) as fi(x) and i \u2208 {1,\u2026\u2026, N} as i\u2208 [N].\nFirst, we begin with a lemma, which is a similar version of Lemma 1 from [Golowich et al., 2018].\nLemma 5. Let & be a 1-Lipschitz, positive-homogeneous activation function which is applied element-wise (such as the ReLU). Then for any class of vector-valued functions F and any convex and monotonically increasing function g : R \u2192 [0,\u221e), there holds:\n$E_{\\sigma} \\sup_{f\\in F,W:||W||_F\\le R} g(\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_i \\phi (Wf_i (x))) \\le 2. E_{\\sigma} \\sup_{f\\in F} g(\\frac{R}{N} \\sum_{i=1}^{N} \\sigma_i f_i (x))$ (13)\nProof. Let w\u2081,\u2026\u2026,wh be the rows of W, we have"}, {"title": "B Proof of Transferability Error", "content": "Transferability Error and Generalization Error\nFor z = (x, y), there holds\n$TE(z) = L_p(z^*) \u2013 L_p(z) < L_p(z^*) \u2013 L_p(z) + (L_E(z) - L_E(z^*))$\n$= (L_p(z^*) \u2013 L_E(z^*)) + (L_E(z) \u2013 L_p(z))$\n$<sup (L_p(z) - L_E(z)) + sup (L_E(z) - L_p(z))$\nB.1\nTransferability Error and Generalization Error\nFor z = (x, y), there holds\n$TE(z) = L_p(z^*) \u2013 L_p(z) < L_p(z^*) \u2013 L_p(z) + (L_E(z) - L_E(z^*))$\n$= (L_p(z^*) \u2013 L_E(z^*)) + (L_E(z) \u2013 L_p(z))$\n$<sup (L_p(z) - L_E(z)) + sup (L_E(z) - L_p(z))$\nWe prove a general version of the theorem as follows:\nTheorem 3. Consider the squared error loss $l(\\theta,x,y) = [f(\\theta;x) \u2212 y]^2$ for a data point $z = (x,y)$. Assume that the data is generated by a function g(x) such that $y = g(x) + \u03c1$, where the zero-mean noise \u03c1 has a variance of $\u03b7^2$ and is independent of x. Then there holds\n$TE(z, \\epsilon) = L_p(z^*) \u2013 \u03b7^2 \u2013 Var_{\\theta\\sim P_{\\Theta}} f(\\theta; x) \u2013 [g(x) - E_{\\theta\\sim P_{\\Theta}} f(\\theta; x)]^2.$ (14)"}, {"title": "B.2 Proof of Theorem 1", "content": "We prove a general version of the theorem as follows:\nTheorem 3. Consider the squared error loss $l(\\theta,x,y) = [f(\\theta;x) \u2212 y]^2$ for a data point $z = (x,y)$. Assume that the data is generated by a function g(x) such that $y = g(x) + \u03c1$, where the zero-mean noise \u03c1 has a variance of $\u03b7^2$ and is independent of x. Then there holds\n$TE(z, \\epsilon) = L_p(z^*) \u2013 \u03b7^2 \u2013 Var_{\\theta\\sim P_{\\Theta}} f(\\theta; x) \u2013 [g(x) - E_{\\theta\\sim P_{\\Theta}} f(\\theta; x)]^2.$ (14)\nNow we start our proof of it.\nProof. Given Eq. (5), it is equivalent to prove\nNote that\n\\theta\\sim P_{\\Theta}} [f(\\theta; x) \u2013 y]^2\n\\theta\\sim P_{\\Theta}} [f(\\theta; x) \u2013 g(x) + g(x) \u2013 y]^2\n\\theta\\sim P_{\\Theta}} [(f(\\theta;x) \u2013 g(x))^2 + (g(x) \u2013 y)^2 + 2(g(x) \u2013 y)(f(\\theta; x) \u2013 g(x))].\nRecall that y = g(x) + p with E(p) = 0 and Var(p) = $\u03b7^2$, we have"}, {"title": "B.3 Proof of Theorem 2", "content": "We first define a divergence measure taken into account. Given a measurable space and two measures \u03bc", "below": "nDefinition 4 (Hellinger integrals [Hellinger", "1909": ".", "ya": "R+ \u2192 R be defined as \u03c6\u03b1(x) = x\u00ba. Then the Hellinger integral of order a is given by\n$H_a(v||u) = \u222b (\\frac{dv}{du})du.$\nIt can be seen as a 4-Divergence with a specific parametr"}]}