{"title": "Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities", "authors": ["Zora Che", "Stephen Casper", "Robert Kirk", "Anirudh Satheesh", "Stewart Slocum", "Lev McKinney", "Rohit Gandikota", "Aidan Ewart", "Domenic Rosati", "Zichu Wu", "Zikui Cai", "Bilal Chughtai", "Yarin Gal", "Furong Huang", "Dylan Hadfield-Menell"], "abstract": "Evaluations of large language model (LLM) risks and capabilities are increasingly being incorporated into AI risk management and governance frameworks. Currently, most risk evaluations are conducted by designing inputs that elicit harmful behaviors from the system. However, a fundamental limitation of this approach is that the harmfulness of the behaviors identified during any particular evaluation can only lower bound the model's worst-possible-case behavior. As a complementary method for eliciting harmful behaviors, we propose evaluating LLMs with model tampering attacks which allow for modifications to latent activations or weights. We pit state-of-the-art techniques for removing harmful LLM capabilities against a suite of 5 input-space and 6 model tampering attacks. In addition to benchmarking these methods against each other, we show that (1) model resilience to capability elicitation attacks lies on a low-dimensional robustness subspace; (2) the attack success rate of model tampering attacks can empirically predict and offer conservative estimates for the success of held-out input-space attacks; and (3) state-of-the-art unlearning methods can easily be undone within 16 steps of fine-tuning. Together these results highlight the difficulty of removing harmful LLM capabilities and show that model tampering attacks enable substantially more rigorous evaluations than input-space attacks alone.", "sections": [{"title": "1. Introduction: Limitations of Input-Output Evaluations", "content": "Rigorous evaluations of large language models (LLMs) are widely recognized as key for risk mitigation and are being incorporated into AI governance frameworks. However, despite their efforts, developers often fail to identify overtly harmful LLM behaviors pre-deployment. Current methods primarily rely on automated input-space attacks, where evaluators search for prompts that elicit harmful behaviors. These are useful but often leave unidentified vulnerabilities. A difficulty with input-space attacks is that they are poorly equipped to cover the attack surface. This happens for two reasons. First, it is intractable to exhaustively search the input space. Second, attackers can sometimes manipulate more than just model inputs (e.g., if a model is open-source). This highlights a fundamental limitation of input-space evaluations: the worst behaviors identified during an assessment can only offer a lower bound of the model's overall worst-case behavior.To address this challenge, we draw inspiration from a safety engineering principle: that safety-critical systems should be tested under stresses at least as extreme-if not more than-those expected in deployment. For example, buildings are designed to withstand loads multiple times greater than their intended use. Here, we take an analogous approach to evaluating LLMs: stress-testing them under attacks that go beyond input-space manipulations.We propose using model tampering attacks, which allow for adversarial modifications to the model's weights or latent activations, in addition to evaluating systems under input-space attacks . We attempt to answer two questions, each corresponding to a different threat model:\n\u2022 Question 1: How vulnerable are LLMs to model tampering attacks? Answering this helps us understand how model tampering attacks can be used to study risks from models that are open-source, have fine-tuning APIs, or may be leaked.\n\u2022 Question 2: Can model tampering attacks inform evaluators about LLM vulnerabilities to unforeseen input-space attacks? Answering this will help us understand how model tampering attacks can help assess risks from both open- and closed-source models.To answer these questions, we pit state-of-the-art methods for unlearning and safety fine-tuning in LLMs against a suite of input-space and model tampering attacks. We make four contributions:\n1. Benchmarking: We benchmark 8 unlearning methods and 9 safety fine-tuned LLMs, each against 11 capability elicitation attacks.\n2. Science of robustness: We show that LLM resilience to a variety of capability elicitation attacks lies on a low-dimensional robustness subspace.\n3. Evaluation methodology: We show that the success of some model tampering attacks correlates with that of held-out input-space attacks. We also find that few-shot fine-tuning attacks can empirically be used to conservatively over-estimate a model's robustness to held-out input-space threats."}, {"title": "2. Related Work", "content": "Latent-space attacks: During a latent-space attack, an adversary can make modifications to a model's hidden activations. Adversarial training under these attacks can improve the generality of a model's robustness. In particular, , and use latent adversarial training to improve defenses against held-out types of adversarial attacks. Other work on activation engineering has involved making modifications to a model's behavior via simple transformations to their latent states. Zhang et al. (2025) also showed that unlearning methods can be brittle to quantization methods.\nWeight-space (fine-tuning) attacks: During a few-shot fine-tuning attack , an adversary can modify model weights via fine-tuning on a limited number of samples. For example, Qi et al. (2023) showed that fine-tuning on as few as 10 samples could jailbreak GPT-3.5. Many works have used few-shot fine-tuning attacks to elicit LLM capabilities that were previously suppressed by fine-tuning or unlearning.Capability elicitation and evaluation: Research on adversarial capability elicitation in LLMs has primarily been done in the context of machine unlearning and jailbreaking. Here, we experiment in these two domains. However, capability elicitation has also been researched in the context of backdoors/trojans , \"password-locked models\" , and \"sandbagging\" . In the unlearning field, several recent works have used adversarial methods to evaluate the robustness of unlearning algorithms. Here, we build off of Li et al. (2024b) who introduce WMDP-Bio, a benchmark for unlearning dual-use biotechnology knowledge from LLMs. In the jailbreaking field, many techniques have been developed to make LLMs comply with harmful requests."}, {"title": "3. Methods", "content": "Our approach: pitting capability suppression defenses against capability elicitation attacks. For unlearning experiments, we experiment with 65 models trained using 8 different unlearning methods. For jailbreaking experiments, we experiment with 9 models off the shelf from prior works. In both cases, we pit these defenses against a set of 11 input-space and model tampering attacks to either elicit 'unlearned' knowledge or jailbreak the model. In Table 1, we list all unlearning methods, off-the-shelf models, and attacks we use. Since the input-space attacks that we use are held out, we treat them as proxies for 'unforeseen' input-space attacks in our evaluations.\nUnlearned models for unlearning experiments: We un-learn dual-use bio-hazardous knowledge on Llama-3-8B-Instruct with the unlearning methods listed in Table 1 and outlined in Appendix A.2.1. For all methods, we train on 1,600 examples of max length 512 from the bio-remove-split of the WMDP \u2018forget set' , and up to 1,600 examples of max length 512 from Wikitext as the 'retain set'. For the 8 unlearning methods listed in Table 1, we take 8 checkpoints evenly spaced across training. Finally, we also use the public release of the \"TAR-v2\" model from Tamirisa et al. (2024) as a 9th TAR model. In total, the 8 checkpoints each from the 8 methods we implemented plus the TAR model from Tamirisa et al. (2024) resulted in 65 models.\nFine-tuned models for jailbreaking experiments: For jailbreaking experiments, we use the 9 fine-tuned Llama3-8B-Instruct models off the shelf listed in Table 1. The first 8 are all fine-tuned for robust refusal of harmful requests. Of these, 'RR' and 'LAT' are state-of-the-art for open-weight jailbreak robust models. The final 'Orenguteng' model was fine-tuned to be \u2018helpful-only' and comply even with harmful requests. We discuss these models in more detail in Appendix A.3.\nCapability elicitation attacks: We use 5 input-space attacks and 6 model tampering attacks on our unlearned models. We use these attacks (single-turn) to increase dual-use bio knowledge (as measured by WMDP-Bio performance ) for unlearning experiments and to elicit compliance with harmful requests (as measured by the StrongReject AutoGrader ) for jailbreaking experiments. We selected attacks based on algorithmic diversity and prominence in the state of the art. We list all"}, {"title": "4. Experiments", "content": "As discussed in Section 1, we have two motivations, each corresponding to a different threat model. First, we want to directly evaluate robustness to model tampering attacks to better understand the risks of open-source, leaked, or API fine-tuneable LLMs. Second, we want to understand what model tampering attacks can tell us about unforeseen input-space attacks in order to study risks from all types of LLMs. Unfortunately, unforeseen attacks are, by definition, ones that we do not have access to. Instead, since the input-space attacks that we use are held out during fine-tuning, we treat them as proxies for \u2018unforeseen' input-space attacks.\n4.1. Unlearning\nWe first experiment with the unlearning of dual-use biology knowledge in LLMs by pitting unlearning methods against capability elicitation methods (see Table 1)."}, {"title": "4.1.1. \u0392\u0395NCHMARKING UNLEARNING METHODS", "content": "The ability to robustly remove harmful knowledge has been proposed as a key goal for designing safer AI. However, our results show that state-of-the-art un-learning methods fail to achieve this goal. Even the best-performing unlearning approaches leave models vulnerable to few-shot fine-tuning.\nUnlearning evaluation: In our models, we evaluate un-learning efficacy on the WMDP-Bio QA evaluation task . We evaluate general utility using three bench-marks. First, we use MMLU and AGIEval , which are based on asking LLMs multiple-choice questions. We then use MT-Bench which is based on long answer questions and thus measures both knowledge and fluency. Because the goal of unlearning is to differentially decrease capabilities in a target domain, we calculate an \"unlearning score\" based on both unlearning efficacy and utility degradation. Given an original model M and an unlearned model M', we calculate $S_{unlearn}(M')$ with the formula:\n$S_{unlearn}(M') =\\frac{([S_{WMDP}(M) - S_{WMDP}(M')] - [S_{utility}(M) - S_{utility}(M')])}{\\frac{[S_{WMDP}(M) - S_{WMDP}(rand)]}{\\Delta_{Random chance (for normalization)}}}$ (1)\nHere, $S_{WMDP}(.)$ is the accuracy on the WMDP-Bio QA Evaluation and $S_{utility}(.)$ is an aggregated utility measure. $S_{utility}(.)$ is calculated by taking a weighted average of MMLU, AGIEval, and MT-Bench. We use weights of 1/4, 1/4, and 1/2 respectively because MT-Bench uniquely mea-sures model fluency. Finally, \"rand\" refers to a random policy. An unlearning score of 1.0 indicates theoretically optimal unlearning \u2013 random performance on the unlearned domain and unaffected performance on others. Meanwhile, the unlearning score of the original model M is 0.0. Table 2 reports results from the best-performing checkpoint (deter-mined by unlearning score) from each of the 8 methods.\nRepresentation rerouting (RR) achieves the highest un-learning score. GradDiff and TAR struggle due to dys-fluency. We find different levels of unlearning success. Representation rerouting (RR) performs the best overall, achieving an unlearning score of 0.96. In contrast, GradDiff and TAR have limited success with the lowest unlearning scores. Poor MT-Bench scores and our manual assessment of these models suggest that GradDiff and TAR struggle principally due to poor fluency.\nNo method is robust to all attacks. We plot the increase in WMDP-Bio performance for the best checkpoint from each unlearning method after each attack in Figure 2 and show that all models, even those with the lowest unlearning scores, exhibit a worst-case performance increase of 8 percentage points or more when attacked."}, {"title": "4.1.2. MODEL ROBUSTNESS EXISTS ON A LOW-DIMENSIONAL SUBSPACE", "content": "Weighted PCA: First, to understand the extent to which some attacks offer information about others, we analyze the geometry of attack successes across our 65 models. We perform weighted principal component analysis on the WMDP-Bio improvements achieved by all 11 attacks on all 65 model checkpoints. We first constructed a matrix A with one row per model and one column per attack. Each Aij corresponds to the increase in WMDP-Bio performance in model i under attack j. We then centered the data and multiplied each row $A_i$ by the square root of the unlearning score: $\\sqrt{S_{unlearn}(A_i)}$. This allowed for models to influence the analysis in proportion to their unlearning score.\nThree principal components explain 89% of the varia-tion in attack success. Figure 3 displays the eigenvalues from PCA and the top three principal components (weighted by eigenvalues). This suggests that different capability elicitation attacks exploit models via related mechanisms.\nHierarchical clustering reveals distinct attack families. In Figure 4, we perform agglomerative clustering on attack success correlations. Algorithmically similar attacks tend to cluster together. However, adversarial finetuning attacks exhibit significant variation, even amongst each other. Finally we see that benign model tampering methods behave similarly to gradient-free input-space attacks."}, {"title": "4.1.3. MODEL TAMPERING ATTACKS EMPIRICALLY HELP TO PREDICT AND UPPER BOUND THE SUCCESS OF INPUT-SPACE ATTACKS", "content": "Embedding-space attacks, latent-space attacks, pruning, and benign fine-tuning empirically correlate with input-space attack successes. In Figure 5 these three model tampering attacks tend to have positive correlations with input-space attack successes with p values near zero. In these plots, we size points by their unlearning score and display the Pearson correlation weighted by unlearning score. Full results are in Appendix B. This suggests that embedding-space attacks, latent-space attacks, pruning, and benign fine-tuning are particularly able to predict the successes of held-out input-space attacks.\nFine-tuning attack successes empirically upper bound input-space attack successes. LoRA and Full fine-tuning performed differently on different attacks. However, together, the max of the two did as well or better than the best performing input-space attack on 64 of 65 models. This suggests that model tampering attacks could be used to develop more cautious estimates of a model's worst-case behaviors than other attacks.\nModel tamperings are predictive of the success of propri-etary attacks from the UK AI Safety Institute (UK AISI). To more rigorously test what model tampering attacks can reveal about unforeseen input-space attacks, we analyze their predictiveness on proprietary attacks from the UK AI Safety Institute. These attacks were known to the 'red team' authors (UK AISI affiliates) but were not accessible to all other 'blue team' authors. We conducted these attacks with the same data and approach as all of our other input-space attacks. Results are summarized in Figure 6 with full results in the Appendix. Correlations are weaker than before, but pruning and benign fine-tuning still correlate with attack success with a p value near zero. Also as before, fine-tuning attack successes often tend to be as strong or stronger than input-space attacks. However, this trend was weaker, only occurring for 60 of the 65 models. See Appendix B.2 for full results and further analysis of UK AISI evaluations.\nModel tampering attacks improve worst-case vulner-ability estimation. Finally, we test if model tampering attacks offer novel information that can be used to predict worst-case behaviors better than input-space attacks alone. We train linear regression models to predict worst-case input-space attack success rates with information from either (1) only input-space at-tacks, or (2) both input-space and model tampering attacks. We find that including model tampering attacks improves predictiveness (e.g. from 0.77 to 0.82 with three predic-tors). The best-performing combinations typically include attacks from multiple families, suggesting diverse attacks provide complementary signals by probing different aspects of model robustness.\nState-of-the-art unlearning can reliably be reversed within 16 fine-tuning steps \u2013 sometimes in a single step. We show the results of multiple fine-tuning attacks against the best-performing model from each unlearning method in Figure 7. All finetuning experiments, as detailed in Appendix A.4, are performed within 16 gradient steps and with 128 or fewer examples. The only method that was resistant to few-shot fine-tuning attacks was TAR, in which only 1 out of the 9 fine-tuning attacks were able to increase the WMDP-Bio performance by over 10 percentage points. However, TAR models had low unlearning scores due to poor general utility, which renders their robustness to fine-tuning unremarkable. All utility-preserving state-of-the-art unlearning methods can be attacked successfully to recover more than 30 percentage points of WMDP performance. Moreover, the 'Full-4' attack does only a single gradient step (with a batch size of 64) and still increases the WMDP performance on 6 of the 8 methods by over 25 percentage points."}, {"title": "4.2. Jailbreaking", "content": "We repeat analogous experiments with similar results in the jailbreaking setting. Finally, to test the generality of our findings outside the unlearning paradigm, we ask whether they extend to jailbreaking. Using the 9 off-the-shelf models and 11 attacks from Table 1, we conducted parallel experiments as in Section 4.1 but with attacks that jailbreak models for compliance with harmful requests. We plot all results in Appendix D.\nOur benchmark results demonstrate that all safety-tuning methods are vulnerable to model tampering. Principal component analysis of attack success rates in Figure 18 show that three principal components explain 96% of the variation in jailbreak success across the nine models.\nWe then reproduced our analysis of whether the success of model tampering jailbreaks correlates with and/or upper-bound the success of input-space jailbreaks (Figure 17). Like before, we find that fine-tuning attack success tends to empirically upper bound the success of input-space attacks. However, unlike before, we do not find clear evidence of a reliable correlation between tampering and input-space attacks due to only having 9 samples.\nFinally, to evaluate how helpful model tampering attacks can be for characterizing a model's vulnerability to unique, held-out attacks, we use Cascade, a multi-turn, state-of-the-art, proprietary attack algorithm from Haize Labs . In Figure 19, we see that single-turn model tampering attacks correlate well with multi-turn Cascade attacks."}, {"title": "5. Discussion", "content": "Implications for evaluations: Our findings have direct implications for AI risk evaluations and governance. Current evaluation frameworks rely heavily on input-space attacks, but our results show that these methods systematically underestimate worst-case behavior. Model tampering attacks provide a powerful tool for identifying hidden risks. By modifying a model's internals\u2014either through activation perturbations or fine-tuning\u2014we can reveal failure modes that input-space attacks miss. This is particularly critical for open-weight models, where safety mitigations can be undone post-release. Moreover, our findings on the fragility of unlearning reversibility highlight another governance concern. If harmful capabilities can be reactivated with minimal tampering, then regulators and developers must reconsider claims that unlearning provides robust safety assurances.\nLimitations: Our work focuses only on Llama-3-8B-Instruct derived models. This allows for considerable experimental depth, but other models may have different dynamics. The science of evaluations is still evolving, and it is not yet clear how to best translate the outcome of evaluations into actionable recommendations. Overall, we find that model tampering attacks can help with more rigorous evaluations\u2014even for models deployed as black boxes. However, there may be limitations in the mechanistic similarity of input-space and tampering attacks.\nFuture work:\n\u2022 Models and tasks: Experiments with larger models and more tasks would offer a broader understanding of model tampering attacks' potential to aid in evaluations.\n\u2022 What mechanisms underlie robust capability re-moval? We are interested in future work to mechanistically characterize weak vs. robust capability suppression. We hope that the 64 models we release help to lay the groundwork for this.\n\u2022 Bridging research and practice: Model tampering attacks can be further studied and used in practice to assess risks in consequential models pre-deployment."}]}