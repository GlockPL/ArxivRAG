{"title": "Counteracting temporal attacks in Video Copy Detection", "authors": ["Katarzyna Fojcik", "Piotr Syga"], "abstract": "Video Copy Detection (VCD) plays a crucial role in copy-\nright protection and content verification by identifying duplicates and\nnear-duplicates in large-scale video databases. The META AI Challenge\non video copy detection provided a benchmark for evaluating state-of-\nthe-art methods, with the Dual-level detection approach emerging as a\nwinning solution. This method integrates Video Editing Detection and\nFrame Scene Detection to handle adversarial transformations and large\ndatasets efficiently. However, our analysis reveals significant limitations\nin the VED component, particularly in its ability to handle exact copies.\nMoreover, Dual-level detection shows vulnerability to temporal attacks.\nTo address it, we propose an improved frame selection strategy based\non local maxima of interframe differences, which enhances robustness\nagainst adversarial temporal modifications while significantly reducing\ncomputational overhead. Our method achieves an increase of 1.4 to 5.8\ntimes in efficiency over the standard 1 FPS approach. Compared to\nDual-level detection method, our approach maintains comparable micro-\naverage precision (\u00b5AP) while also demonstrating improved robustness\nagainst temporal attacks. Given 56% reduced representation size and the\ninference time of more than 2 times faster, our approach is more suitable\nto real-world resource restriction.", "sections": [{"title": "1 Introduction", "content": "Video content has become ubiquitous in the digital age, serving as a primary\nmedium for entertainment, education, and communication. As video-sharing\nplatforms grow in popularity, the need for effective copyright protection mech-\nanisms becomes increasingly important. Video copy detection systems aim to\nidentify duplicates or near-duplicates of videos, enabling rights holders to enforce\nintellectual property laws and combat unauthorized usage. Aside from copyright\nprotection, copy detection is an important tool in reducing database redundan-\ncies or localizing the original recordings of DeepFake attacks.\nThe dual-level detection method, as described in [13,16] was recognized as\nthe winning solution in the prestigious META AI Challenge on video copy detec-\ntion [13]. This approach combines Video Editing Detection (VED) with Frame"}, {"title": "2 Related work", "content": "Video copy detection has been an active area of research for more than a decade,\nwith early methods focusing on descriptors such as histograms and block-based\nmatching techniques. As computational power and storage capabilities improved,\nresearchers shifted towards feature-based approaches, utilizing local descriptors\nlike Scale-Invariant Feature Transform (SIFT) and Speeded-Up Robust Features\n(SURF). These methods enabled more accurate matching of video fragments,\neven under transformations such as scaling, cropping, and rotation [17,10].\nWith the advent of deep learning, video copy detection has undergone a\nparadigm shift. Convolutional Neural Networks (CNNs) like VGG [15], ResNet [7],\nand models such as ViSiL [9] have significantly advanced frame-level feature ex-\ntraction for video similarity learning. Additionally, architectures like 3D-CNNs [11]\nand encoder-decoder ConvLSTM models [3] capture spatiotemporal dynam-\nics, improving robustness to temporal edits. Advanced methods now incorpo-\nrate transformer-based architectures, such as Video ViT [1], and self-supervised\nlearning frameworks like 3D-CSL [4] to further enhance the system's ability to\ndetect heavily edited and transformed videos [4,5,2]. Lightweight approaches,\nincluding multi-teacher distillation frameworks [12] and compact Siamese neu-\nral networks [6], have recently gained attention for achieving high efficiency and\nscalability on large datasets.\nThe importance of dynamic frame selection based on interframe differences\nto create highly compact video representations was shown in [6]. This approach"}, {"title": "Counteracting temporal attacks in Video Copy Detection", "content": "reduces computational complexity while preserving the core structure of the\nvideo, enabling efficient storage and retrieval even under severe distortions such\nas extreme compression or resizing. Such strategies demonstrate the potential for\nreal-time applications, where memory and computational constraints are critical.\nWe extend this approach to show robustness against temporal attacks.\nOn the other hand, [18] proposes a CNN-LSTM hybrid model for human\naction recognition, evaluated on UCF101 and HMDB51 datasets. Their method\nimproves accuracy by up to 3.5% over CNN-only baselines but suffers from high\ncomputational complexity and sensitivity to viewpoint variations, making the\nsolution unusable in real-world applications. The authors of [8] also indicate\ntime and memory limitations and propose Relational Self-supervised Distillation\nwith Compact Descriptors (RDCD) for image copy detection, using a lightweight\nnetwork and compact descriptors to improve efficiency. Their method employs\nrelational self-supervised distillation to transfer knowledge from a large teacher\nnetwork (ResNet-50) to a smaller student network (EfficientNet-B0) and intro-\nduces contrastive learning with a hard negative loss to mitigate dimensional\ncollapse. Evaluated on the DISC2021, Copydays (CD10K), and NDEC datasets,\nRDCD improves micro average precision (\u03bcAP) by about 5.0%, depending on\nthe descriptor sizes over the baseline while maintaining competitive performance\nwith significantly smaller descriptor sizes. However, the reliance on knowledge\ndistillation from a large network and additional computational costs during train-\ning remain limitations. Despite these advances, challenges persist. Many systems\nstruggle with efficiency when processing large-scale datasets, as feature extrac-\ntion and matching remain computationally expensive. For example, as demon-\nstrated in [6], widely used models like ViSiL generate descriptors of 2025kB for\na typical 30-second video, with an inference speed of approximately 6.5 sam-\nples per second (sps), where one sample corresponds to a 30-second video. In\ncontrast, the method proposed in [6] achieves a descriptor size of just 1.875kB\nwhile delivering an inference speed of up to 178.6 sps with satisfactory accuracy.\nSimilarly, the authors of [4] highlight that prevalent models such as VRL-F and\nTCA-F (both frame-based matching) require over 10 seconds to generate a sin-\ngle descriptor for a sample from the FIVR-200K dataset, with descriptor sizes\nspanning several megabytes. ViSiL, by comparison, is even less efficient, taking\nabout 10 times longer and producing descriptors approximately 10 times larger.\nRobustness against adversarial transformations such as frame manipula-\ntion, extreme compression, or rotation also varies widely among methods. The\ndual-level detection method, incorporating Video Embedding Descriptors (VED)\nfor unedited video identification and Fragment Similarity Detection (FSD) for\nedited content, addresses some of these issues. However, as highlighted in our\nanalysis, these components require further refinement to ensure robustness and\nscalability in real-world scenarios."}, {"title": "3 The META AI Challenge Dataset and Baseline Model", "content": "The META AI Challenge on video copy detection, held in 2023, provided a rigor-\nous platform for evaluating state-of-the-art methods in video similarity analysis.\nThe challenge was designed to push the boundaries of video copy detection and\nlocalization, attracting top research teams from around the world. It included\ntwo tracks: video copy detection (VCD) and video copy localization (VCL). The\nVCD track focused on identifying whether two videos shared copied content,\nwhile the VCL track required participants to localize the temporal segments of\nshared content within video pairs. In this paper we focus on VCD track.\nThe participants were constrained by computational resource limits to en-\nsure solutions were scalable to real-world scenarios. The competition empha-\nsized practical applications, such as content moderation, copyright protection,\nand misinformation detection, highlighting the importance of efficient and accu-\nrate solutions in this domain. The challenge introduced a benchmark dataset-\nDVSC2023 and a strong baseline model to facilitate evaluation and comparison\nof methods. Although the official competition has ended, Meta AI Video Simi-\nlarity Challenge and DVSC2023 are still available in the form of Open Arena12."}, {"title": "3.1 Dataset", "content": "The DVSC2023 dataset was created using videos from the YFCC100M collec-\ntion, filtered to ensure Creative Commons licenses and to exclude videos that\nwere too short or low-resolution. It contains a mixture of reference and query\nvideos, with query videos being transformed versions of reference videos or dis-\ntractors. The videos were modified using various augmentations, such as spatial\nupdates (cropping, resizing), temporal edits (speed changes, frame alternation),\nand complex transformations like screen capture simulation. These transforma-\ntions were applied to create challenging scenarios for detecting copies. Examples\nof video frames copied with applied transformations are presented in Fig. 1. The\ndata is partitioned into Training Split, that contains 8,404 query videos and\n40,311 reference videos, with 2,708 queries containing copied segments, Valida-\ntion Split, consisting of 8,295 query videos and 40,318 reference videos, with\n2,641 queries containing copied segments, and Test Split of 8,015 query videos\nevaluated against 2,519 reference videos, with 1,840 queries containing copied\nsegments. Additionally, 6,475, 6,369, and 6,175 distractor queries are included in\nthe training, validation, and test splits, respectively. These queries ensure that\nmost queries contain no copied segments, replicating real-world conditions.\nThe contestants received only the training and validation splits before submit-\nting their methods. Training data came with match results for all query-reference\npairs, while validation match results were hidden and known only to the orga-\nnizers. After submission, participants received feedback on validation split per-\nformance based on the ground truth. In Phase 2 of the challenge, the unseen"}, {"title": "3.2 Baseline Model", "content": "The baseline model leverages the Self-Supervised Copy Detection (SSCD) de-\nscriptor [14], which extracts frame descriptors at a rate of one frame per second\nafter resizing and cropping. These descriptors undergo score normalization to\nmitigate false matches, enhancing reliability. The matching process is evaluated\nusing the micro-average precision-recall (\u03bc\u0391\u03a1) metric, which jointly assesses the"}, {"title": "4 Limitations of the Original Method", "content": "It is worth noting, that DVSC2023 dataset for the challenge does not include\nqueries with exact duplicate fragments (unedited) of reference videos. However,\nit contains video pairs that are largely similar but exhibit subtle differences,\ndistinguishing them as distinct videos rather than identical copies, e.g., videos\nmay share the same background but feature different people.\nThe dual-level method uses VED to identify unedited videos, assigning ran-\ndom descriptors with small norms and negative bias terms to those that are\nunedited. Hence, it is able to handle these challenging video pairs, reporting a\n5% improvement in matching accuracy. However, when tested under real-world\nconditions, the method reveals a significant limitation. Our experiments show\nthat VED consistently misclassifies exact video copies from the reference set as\nnon-copies. We tested dual-level performance on 100 queries, which were exact\ncopies of chosen 100 references video. None of them was recognized as a copy.\nThis highlights a fundamental flaw in the current implementation of VED, mak-\ning it unsuitable for practical applications.\nAdditionally, the authors deterministically extract one frame from each sec-\nond of the video, which is a common approach used also in the baseline model\nof challenge organizers. Although straightforward, this approach is vulnerable to\ntargeted temporal attacks, as discussed in Sect. 6."}, {"title": "5 Proposed Method", "content": ""}, {"title": "5.1 Improved Frame Extraction", "content": "To reduce computation and memory costs, we reduced the number of video\nframes while maintaining key data representation. Instead of the standard method\nof selecting one frame per second (fps), we focused on scene changes using the\ninterframe difference curve. This was calculated as the sum of absolute pixel-wise\ndifferences between consecutive frames, averaged by the number of pixels.\nWe tested two frame selection approaches. In first, we select frames which\nare local maxima of the interframe difference curve. The second approach used\nframes in the middle between two local maxima of the interframe difference\ncurve. Both methods used Hanning window smoothing to reduce noise and high-\nlight significant changes."}, {"title": "Counteracting temporal attacks in Video Copy Detection", "content": "The motivation is that the first method, while more time efficient, targets\nexact moments of scene change, making it vulnerable to temporal attacks (e.g.,\ninsertion of random frames). In contrast, the second method may offer better\nresistance, yet requires more time to identify the frame. Using either method, we\nreduced the number of frames by 40 to over 150 times, which is 1.4 to 5.8 times\nmore efficient than the standard 1 fps approach shown in [16]. For a sample video\nwith an fps of 24 and a total of 719 frames, we select 17 frames when using a\nHanning window of size 30 (Fig. 2), 13 frames with a Hanning window of size\n50 (Fig. 3), and only 5 frames with a Hanning window of size 100 (Fig. 4). This\ncorresponds to a reduction in the number of frames for a video by factors of\napproximately 42, 55, and 144, respectively. In contrast, a simple one-frame-per-\nsecond approach reduces the frames by a factor of only 24. However, it should be\nnoted that smoothing with larger Hanning windows may result in the omission\nof frames from shorter scenes, as the larger the window, the greater the reduction\nin frames, which can be also observed in Fig. 5, where we present selected frames\nfrom the first 10 seconds of the sample video obtained using different methods.\nThis phenomenon is investigated during our experiments (cf. Tab. 1)."}, {"title": "5.2 Resistance to Temporal Attacks", "content": "To evaluate the robustness of both frame extraction methods, we conducted\nexperiments with three types of temporal attacks:\nRandom Frame Blackouts: The frames were blacked out in the original\nvideo with probabilities of $p = 1/25$ and $p = 1/10$. Naturally, the included\nblack frames modify the related frames during video compression, affecting\nrelative P- and B- frames, hence after video decoding more than 1 frame is\naffected. The number of affected frames depends on the blacked out frame,\nas it influences the I-, P-, and B-frame selection.\nTargeted Frame Blackouts: In this attack, the middle frame of each second\nof the video was blacked out in the same manner as in the first attack. Note\nthat, such precise attacks are visually imperceptible to users, making them\na significant threat to video copy detection systems.\nSpeed modification We used ffmeg to modify the tempo at which the video is\nplayed. In technical terms, it keeps all the frames but saves them, as a video\nwith changed fps, so that we get the effect of acceleration or deceleration.\nNaturally, it may influence the I-, P-, and B-frame selection."}, {"title": "6 Experimental Results", "content": "The winning team extracted a smaller validation set from the original training\ndataset for their experiments, reducing its size by a factor of four (1681 queries).\nWe conduct our experiments using the same data."}, {"title": "6.1 Parameter selection", "content": "In order to analyze the efficacy of each frame selection method as well as the\nsize of the smoothing window, we measured \u00b5AP in a scenario reflecting the\nchallenge. Moreover, we analyze the influence of the temporal attacks, described\nin Sect. 5.2."}, {"title": "6.2 Efficiency", "content": "Due to the ubiquitous nature of videos, the VCD system used in copyright man-\nagement or DeepFake detection should be time efficient. Additionally, the more"}, {"title": "6.3 Robustness against temporal attacks", "content": "Since the analysis in Sect 6.2 includes tests on the dataset with image processing\nattacks, we want to investigate of the influence of the modifications typical for\n3D data (a video in contrast to an image). For that purpose we modified the\ntest dataset with attacks described in Sect. 5.2."}, {"title": "7 Conclusion", "content": "This paper focuses on the problem of Video Copy Detection and addresses the\nlimitations of the Dual-level detection method [16] that was successful in Meta\nAI Challange. We propose an improved method of video representation by more\nadaptive frame selection. Additionally, we analyze the performance of the de-\ntection method against three proposed temporal attacks. Using local maxima of\ninterframe differences, the proposed method reduces computational costs while\nkeeping comparable efficiency measured as micro-average precision. The perfor-\nmance difference on the DVSC2023 dataset is less than 1%, while increasing\nresilience against temporal attacks. Our experiments show that our method re-\nduces the number of frames required to properly represent a video by 1.4 to 5.8\ntimes when compared to the standard one-frame-per-second approach. Moreover,\nin the performed tests, our method achieved 2 times faster inference, which is\nimportant in real-world applications when processing massive video databases.\nMoreover, the proposed approach efficacy showed to be invariant on video speed\nmanipulations, whereas the previous method suffered a 7% \u03bc\u0391\u03a1 drop. Similar\nresults were obtained for frame blackout, both random and targeted, showing\nthe resistance of our method.\nFuture research will focus on developing adaptive temporal alignment tech-\nniques to enhance robustness against significant frame modifications (like black-\nouts), as well as integrating feature matching techniques that allow reducing the\nsize of representation for even better performance enhancement. Additionally,\nexpanding the approach to handle localized transformations and cross-modal\nattacks (e.g., temporal modifications, room attack and overlays as shown in\nFig. 1) will further strengthen its effectiveness. Another branch of research is\nfurther improvement of inference time, so that large-scale video databases may\nbe checked for copies. Finally, the VCD models should be investigated for their\ninterpretability, so that they follow XAI research trend and be compliant with\njurisdical restrictions for real-world legal usage."}]}