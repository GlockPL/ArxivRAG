{"title": "Discovering emergent connections in quantum physics research via dynamic word embeddings", "authors": ["Felix Frohnert", "Xuemei Gu", "Mario Krenn", "Evert van Nieuwenburg"], "abstract": "As the field of quantum physics evolves, researchers naturally form subgroups focusing on specialized problems. While this encourages in-depth exploration, it can limit the exchange of ideas across structurally similar problems in different subfields. To encourage cross-talk among these different specialized areas, data-driven approaches using machine learning have recently shown promise to uncover meaningful connections between research concepts, promoting cross-disciplinary innovation. Current state-of-the-art approaches represent concepts using knowledge graphs and frame the task as a link prediction problem, where connections between concepts are explicitly modeled. In this work, we introduce a novel approach based on dynamic word embeddings for concept combination prediction. Unlike knowledge graphs, our method captures implicit relationships between concepts, can be learned in a fully unsupervised manner, and encodes a broader spectrum of information. We demonstrate that this representation enables accurate predictions about the co-occurrence of concepts within research abstracts over time. To validate the effectiveness of our approach, we provide a comprehensive benchmark against existing methods and offer insights into the interpretability of these embeddings, particularly in the context of quantum physics research. Our findings suggest that this representation offers a more flexible and informative way of modeling conceptual relationships in scientific literature.", "sections": [{"title": "I. INTRODUCTION", "content": "The corpus of scientific literature is expanding at an ever-increasing rate. In particular, the field of quantum physics is witnessing a steady growth in published papers each year, growing at an increasing pace.\u00b9 Thus, it is becoming increasingly challenging for individual researchers to gain a comprehensive understanding of the diverse domains within quantum physics. Naturally, this leads to the formation of various specialized subgroups within the community [1]. While specialization enables researchers to focus on specific areas, it also creates isolated knowledge silos where valuable insights may not be shared across different fields. This compartmentalization can result in parallel research trajectories, where solutions to structurally similar problems could remain confined within particular subfields. Overcoming this isolation can help researchers from leveraging advancements in adjacent fields that could accelerate progress in their own field [2\u20134].\nRecent efforts to bridge the knowledge gaps between specialized subfields have led some researchers to leverage large language models (LLMs) trained on vast amounts of scientific literature. By feeding millions of papers into these models, the aim is to generate or rank novel research ideas directly [5\u20138]. While this approach holds significant promise, it poses substantial challenges in evaluation - it inherently requires human expertise to assess the quality and feasibility of the generated ideas.\nAn alternative strategy involves forecasting the future trajectory of scientific research by predicting what scientists might work on next or identifying areas poised for high impact [9]. A common way to solve meta-scientific forecast tasks is to model the scientific progress as an evolving knowledge graph constructed from millions of publications, closely aligning with link prediction problems in network theory [10]. Pioneered in the context of biochemistry, it has been demonstrated that knowledge graphs can be used to find more efficient research strategies to accelerate collective scientific discovery [11]. This idea has been expanded to the forecast of research directions in quantum physics [12] and artificial intelligence [13], the prediction of future impact of new scientific connections [14] and to the predictions of interesting research directions which human researchers might not discover [7]. However, representing scientific literature as a knowledge graph reduces the information down to binary connections between concepts, which may not fully capture the intricate dynamics of scientific advancement.\nOne way to address this issue is by using unsupervised word embeddings derived directly from scientific literature, analyzing how these embeddings evolve to predict future research trends and dynamics. This approach fundamentally differs from knowledge graphs by capturing information from scientific literature without human bias, representing words solely based on contextual similarities and mapping semantically related terms to closely related points in the embedding. Prominent examples involve the automated encoded of scientific literature in material science. Here, when the names and properties of materials are encoded, the word embedding can be used to find new materials with specific properties [15\u201317]. Another work demonstrated how automated embeddings can relate the surprise and impact of research ideas [18, 19].\nIn our work, we demonstrate how an unsupervised dynamic word embedding can efficiently predict future research directions in quantum physics. Without relying on any hand-crafted features, a machine learning model that uses only the coordinates of the embedded latent space can predict the likelihood that previously unstudied research combinations will be explored in the future. Our method surpasses other approaches that do not rely on human-crafted features, showcasing a pathway for fully end-to-end prediction tasks within the science of science.\nThe rest of the paper is structured as follows: Sec. II discusses the methodology, including embedding generation process and the machine learning pipeline. Sec. III presents the benchmarking results of the trained ML models and interpretation of the learned embedding. Finally, Sec. IV discusses the findings and provides an outlook for future research."}, {"title": "II. METHODOLOGY", "content": "The text corpus D consists of 66,839 abstracts from arXiv\u2019s quant-ph category, covering the period AT from January 1994 to December 2023. The vocabulary of all words in the corpus is denoted as V. We define the set of quantum physics concepts C as multi-word tokens, each comprising one or more words v \u2208 V that represent distinct ideas or topics within the field. To identify these concepts, we process the corpus by matching words v to a predefined concept list as outlined in [7]. When a set of words matches a known quantum physics concept, it is replaced in the corpus with a single token, and the year of its appearance is recorded. This approach results in a final list of 10,235 distinct quantum physics concepts and a modified text corpus D with an updated vocabulary V containing the replaced tokens, where C \u2286 V. Fig. 1a illustrates how an abstract may contain specific quantum physics concepts, such as entanglement witnesses, which are tracked as unique tokens."}, {"title": "B. Dynamic Embedding Model", "content": "The goal of the dynamic embedding model is to obtain a vector $w_{c,t} \\in R^n$ for each token c\u2208 C at each time step t \u2208 \u0394\u03a4, encoding the semantic relationships between concepts in quantum physics literature and how these relationships change over time. To achieve this, we use the Word2Vec model with the Skip-gram approach [20, 21]. Word2Vec is a neural network-based model that learns continuous vector representations of tokens (embeddings) from the text corpus D. The Skip-gram model is designed to predict the context tokens surrounding a given target word within a window size k, learning embeddings that reflect semantic similarities based on context. Conceptually, Skip-gram is a two-layer neural network trained to maximize the conditional probability $P(W_{i+j,t}|W_{i,t})$ of observing context token $W_{i+j,t}$ given a target word $W_{i,t}$ [20]:\n$L_{Skip} = - \\sum_{i \\in D} \\sum_{\\substack{ -k < j < k \\\\ j \\neq 0 }} log P(W_{i+j,t}|W_{i,t})$\nThe architecture consists of an input layer, a projection (hidden) layer, and an output layer. To generate dynamically evolving embeddings, we train the model sequentially for each year in AT, initializing each step with the trained model from the previous year, as illustrated in Fig. 1b. This approach has proven to reveal semantic shifts over time [22]. This process generates embeddings for the set of concepts C from their first appearance in the text corpus up to the cutoff date. Since concepts emerge at different times in the corpus (see Fig. A1 inset), we use backtracking to maintain a consistent set of embeddings across all years. For concepts that first appear after the corpus's starting year, we fill in the missing embeddings by using the embedding of their initial occurrence. This approach ensures that all concepts are trackable from the same initial year, providing a consistent basis for analysis over time."}, {"title": "C. Predictive Model", "content": "Given that the embedding features capture detailed information about relationships between various concepts and how these relationships evolve over time, we investigate training a machine learning model to predict whether concept pairs will eventually become connected based on their embedding data. Our approach aims to predict if pairs of concepts, which have not yet appeared together in the same abstract, will co-occur in future research. In our classification task, illustrated in Fig. 1c, we categorize quantum physics concept pairs into two groups. For both groups, the concept pairs have not co-occurred in the same abstract during a training window At. We assign the label y = 1 if the concepts co-occur in the same abstract within a subsequent label window At, and y = 0 if they remain unconnected during this period. To perform this classification, we use a neural network-based classifier. The network is trained to differentiate between the two categories based on embeddings in the dataset X, minimizing the binary cross-entropy loss [23] between true labels y and predicted labels \u0177:\n$L_{BCE} = - \\frac{1}{|X|} \\sum_{(x,y) \\in X} [y log(\\hat{y}) + (1 - y) log(1 - \\hat{y})]$"}, {"title": "III. RESULTS", "content": "We begin by analyzing the quality of the proposed dynamic embedding method to assess its effectiveness in capturing relationships among quantum physics concepts. To do this, we train the dynamic embedding model, as described in Sec. IIB, over two time frames: At = [1994, 2012] and \u2206t = [1994, 2022] with an embedding size of n = 128. To explore the distribution of the embeddings, we examine those from the final years, 2012 and 2022, each informed by data dating back to 1994. We apply Uniform Manifold Approximation and Projection (UMAP) [24] to reduce the 128-dimensional embeddings to two dimensions, allowing us to observe relationships among quantum physics concepts in this reduced space. In this, UMAP aims to preserve a notion of both local and global structures from the original feature space [24]. Additionally, we use k-means clustering [25] to identify nine distinct clusters within this reduced space, helping us analyze neighborhoods of related quantum physics concepts. Both UMAP and k-means hyperparameters were optimized empirically for the clearest visualization. The results of this clustering analysis are shown in Fig. 2, providing a visual representation of concept relationships and groupings based on their embeddings. Below the figure, we include tables listing key concepts in each cluster, selected by proximity to the cluster center and their frequency in abstracts from 2012 (2022).\nTo evaluate embedding quality, we interpret the structural patterns within the clusters. Each cluster contains thematically related quantum physics concepts; for instance, cluster A0 in 2012 focuses on quantum computing, while A6 contains concepts about stochastic noise. Concept distances within clusters are also meaningful. For example, clusters A5 and A8, which are close, represent concepts related to experimental quantum physics, whereas A5 and A7, which are farther apart, include distinct topics. This clustering shows that the embeddings capture structured information about central quantum physics topics. To illustrate how research focus has shifted over time, we compare the clusters from 2012 to those from 2022. We observe emerging topics, such as gravitational waves and quantum advantage. While these clusters allow us to interpret the structure of the embeddings within a single year, the independent initialization of dimensionality reduction and evolving embeddings over time mean that clusters from different years (e.g., A0 and B0) are not directly comparable.\nThe observed structure suggests that the embeddings encode meaningful information about relationships among quantum physics concepts and their evolution over time."}, {"title": "B. Analyzing the Performance of Embedding Methods", "content": "The primary objective of this study is to evaluate the effectiveness of the proposed dynamic embedding technique for classifying quantum physics concept pairs and to compare its performance against existing methods. All results are presented in Tab. I, in which the performance of each model is evaluated using the area under the curve (AUC) metric [26]. The underlying receiver operating characteristic (ROC) curves are presented in Appx. A2. To ensure an equal comparison, all embedding models whether utilizing our proposed embeddings or baseline methods were trained and evaluated using the same predictive (neural network) architecture, as outlined in Appx. C. The only variable across experiments was the input representation, with the number of input neurons reflecting the dimensionality of the respective embeddings.\nWe trained and validated the models using embeddings generated from abstracts spanning the years At = [1994, 2017], with label windows corresponding to the years At = [2018, 2020]. For testing, we utilized embeddings up to At = [1994, 2020], validating predictions against the label window covering At = [2021, 2023].\nThe results demonstrate the proposed dynamic embedding method consistently surpassing the baseline approaches. Since both features were derived from the same underlying dataset and the predictive models were trained with identical structures, the performance disparity can be attributed solely to the input embeddings.\nWe conclude that the model's capacity to utilize the temporal structure of these embeddings enriches the contextual information, leading to more accurate and robust predictions. In the following, we briefly outline the distinctions between each embedding technique, with a more detailed description provided in Appx. A.\n(Ours) Dynamic Word Embedding (Word-Dynamic) Our proposed dynamic embedding approach represents each concept as a vector $w_{c,t} \\in R^n$ for each token c\u2208 C at each time step t\u2208 AT with n = 128. It captures the semantic relationships between concepts in quantum physics literature and how these relationships evolve over time. While this technique does not explicitly encode the connections between concepts, it provides the predictive model with comprehensive information about the structure within the embedding space.\nThe Static Word Embedding (Word-Static) The static word embedding approach can be viewed as the standard Word2Vec method for generating embeddings $w_c \\in R^n$ with n = 128. Like the dynamic embedding, the training is performed in an unsupervised manner; however, it uses a single corpus of abstracts from a specified time frame, At. Thus, it does not incorporate the dynamic training introduced in Sec. II B. The resulting structure encodes the relationships between concepts implicitly within an embedding space, but does not account for temporal changes in these relationships.\nStatic Word Embedding and Handcrafted Feature (Word-Hand) This method utilizes the same static word embeddings but derives a handcrafted feature: the cosine similarity between word embedding vectors. In [15], this metric is used to predict potential undiscovered connections in material science, effectively identifying novel relationships by assessing the proximity of concepts in the embedding space. While this method also implicitly encodes relationships between concepts, it simplifies the information into a single handcrafted feature.\nKnowledge Graph and Handcrafted Feature (Knowledge-Hand) \u2013 This method does not use word embeddings but instead creates an evolving knowledge graph from the abstracts. In this graph, concepts are represented as nodes, and connections between concepts i.e., if they co-occur in the same abstract are modeled as edges. In [12], 15 handcrafted features are extracted from the knowledge graph to predict the connection between concepts. The approach formulates the task as a link prediction problem, training a machine learning model to classify whether two nodes, representing distinct concepts, will form a link within a specified timeframe. In general, the knowledge graph representation abstracts away all information aside from the connections, focusing explicitly on modeling the relationships between concepts.\nKnowledge Graph and Machine Learning Feature (Knowledge-Node) \u2013 Instead of selecting handcrafted features from knowledge graphs for the classification task, machine learning-based features can also be used [13]. This model is based on Node2Vec embeddings [27], which eliminates the need for handcrafted features in link prediction within knowledge graphs.\nKnowledge Graph and Machine Learning Feature (Knowledge-ProNE) \u2013 This model also uses machine-learned embeddings from the knowledge graph, in this case the ProNE method [28]. Thus, it also incorporates these automated embeddings into a neural network framework for link prediction between concepts, without the need for explicit feature engineering."}, {"title": "C. Analyzing the Model Calibration", "content": "In the previous section, we trained the predictive model with a logistic output by minimizing cross-entropy loss with binary labels, as introduced in Eq. 2, allowing the output to be interpreted as the probability that two quantum physics concepts will be linked in the future. This probability is generally well-calibrated but shows increased uncertainty around predicted probabilities near 0.5, as illustrated in Fig. 3a. For example, among samples with an output value of 0.9, approximately 90% have a true label of 1. Predictions closer to 0 or 1 indicate higher confidence, while probabilities near 0.5 signal greater uncertainty and an increased likelihood of misclassification. Therefore, this probabilistic output thus enables selection of test samples, allowing uncertain samples to be filtered out. In Fig. 3b, we see that removing low-confidence predictions, such as the 20% most uncertain pairs, increases the AUC score beyond 0.9 a threshold indicating strong predictive performance [29]. For studying interesting concept combinations, discarding a substantial portion of samples is not inherently problematic. Focusing on a small subset of high-confidence predictions makes investigating samples that warrant further analysis more feasible. This approach makes confidence-based selection an effective strategy for identifying this promising subset."}, {"title": "D. Analyzing the Model Predictions", "content": "An added benefit of dynamic word embeddings paired with a well-calibrated predictive model is their ability to capture and interpret the evolution of research concepts over time. Thus, as a final step, we evaluate whether our model - trained on quantum physics abstracts from prior years could predict concept combinations that would later emerge in the literature.\nIn Fig. 4, we visualize the predicted probability shown to be well-calibrated for confident cases in the previous section of a model trained on dynamic embeddings from At = [1994, 2017] and then evaluated on the embeddings for each year through 2023. We illustrate three pairs of concepts, showing how the model's predictions evolved over time. The star marker in each line indicates the year when the concept pair first appeared together in a published abstract. The selected pairs highlight recent popular combinations in the field: (1) The predicted link between quantum computing and active space methods first appears in Ref. [30] given our text corpus and concept list. (2) The predicted connection of using machine learning for constructing optimized circuits for noise-resilience first appears in Ref. [31]. (3) The technique of using tensor network approaches in investigating local quantum circuits first appears in Ref. [32]. Importantly, the concept set C is highly granular and does not yet account for synonyms; we discuss the implications of this limitation in Sec. IV. Nonetheless, this visualization shows that the model's increasing probability predictions align well with the first recorded uses of these concept pairs in the corpus. While the pairs were chosen based on rising topics of interest, we note that high predicted probabilities also corresponded to these papers having (1) 88, (2) 118, and (3) 9 citations, respectively, as of this manuscript's writing."}, {"title": "IV. DISCUSSION AND OUTLOOK", "content": "In this work, we demonstrated the application of dynamic word embeddings for predicting research trends using neural networks, with a focus on quantum physics concepts. Our results show that features generated from an unsupervised embedding model without the need for direct human input show promising performance, leading to improved prediction accuracy compared to the baseline embeddings. Thus, the presented embedding method offers an interesting research direction for leveraging machine learning to investigate how experts across diverse fields approach or solve structurally similar problems.\nBenchmark Additional Dynamic Embedding Approaches - For this initial study, we adopted the dynamic embedding method from Ref. [22]. A natural extension of this work would be to compare the effectiveness of various other dynamic embedding techniques for the specific prediction task at hand [33, 34].\nBenchmark LLM Embeddings \u2013 Additionally, exploring the potential of more modern embedding models like LLAMA [35], BERT [36] or GPT [37] could further enhance the performance of our method. We opted not to incorporate these advanced models in the present study for the following reasons. Modern language models generate contextual embeddings, which vary based on the surrounding text in which a word appears. Unlike traditional models like Word2Vec, which produce a static embedding for each word, contextual embeddings can capture subtle changes in meaning depending on the context. While this greatly enhances the representational power of such models, applying them to our scenario would introduce additional complexity. In quantum physics research, where concepts can be used in different contexts, contextual embeddings would result in multiple representations of the same concept. This would require further processing, such as averaging the representations to generate a consistent embedding for each concept, adding a layer of complexity to the analysis. We leave this refinement for future research.\nExtended Model Selection A valuable future research direction involves comprehensive model selection and optimization, customizing the neural network architecture to suit the specific characteristics of the dynamic embedding features. This approach could further improve predictive performance and underscore the potential of dynamic embeddings. We focused on using the embedding from the final year of the training window, as empirical investigations showed that incorporating multiple time slices did not significantly impact the final AUC score. A future study could explore whether a deep recurrent neural network architecture can leverage information from multiple time slices for enhanced predictive performance.\nApplication for Quantum Physics Research \u2013 The ultimate goal of the proposed method is to forecast the future trajectory of scientific research by predicting potential directions that scientists might explore. Our method represents a step towards achieving high-quality predictions in this domain. In Fig. 4, we demonstrate that our method assigns high probabilities to fruitful concept combinations.\nThe predictions depend on the specific concepts extracted from each abstract, especially for very prominent concepts that have many synonyms (different phrases with the same semantic meaning). An interesting next step would hierarchical grouping of related concepts (synonyms), meaning that although terms like active space and quantum algorithm may have co-occured before, the model only flagged the exact combination of quantum computing and active space in the displayed year. This would reduce the granularity of the set of concepts but could enhance the model's ability to track when distinct concept areas begin to intersect."}, {"title": "Appendix A: Details on Embedding Methods", "content": "For both (Word-Static) and (Word-Hand) methods, we utilized a Word2Vec model configured in the same way as described in Appx. B. The primary distinction between the proposed dynamic embedding model and the static embedding approach lies in their training processes. In the case of static embeddings, the training is conducted in a single run using the complete text corpus over the entire time period At. This approach captures the overall context and relationships within the data, producing a fixed set of word vectors that do not change over time. Conversely, the dynamic embedding model is designed to be updated iteratively. It is trained and initialized anew after each year within the specified interval At. This allows the model to adapt to shifts in language usage and context that may occur over time, resulting in a more nuanced representation of word meanings that reflects temporal changes in the corpus."}, {"title": "Appendix C: Classifier Training Details", "content": "The predictive model employed a neural network designed to forecast future connections between quantum physics concepts and was implemented in PyTorch [39]. The model is a multi-layer perceptron (MLP) consisting of a series of three layers. The network uses Parametric ReLU activations, batch normalization, and dropout to mitigate overfitting. The final output layer applies ReLU and sigmoid activations to produce a binary classification probability. The training process incorporates early stopping to avoid overfitting. Additionally, a learning rate scheduler is employed to dynamically adjust the learning rate during training, enhancing convergence and model performance.\nTo address class imbalance, where the number of negative (non co-occurring) concepts substantially exceeds the number of positive (occurring) concepts, we applied simple uniform sampling on the negative examples to balance the class distribution. This approach ensured that our model was trained on a more representative dataset and improved the reliability of our evaluation metrics."}]}