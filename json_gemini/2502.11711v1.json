{"title": "Knowledge-aware contrastive heterogeneous molecular graph learning", "authors": ["Mukun C", "Jia Wu", "Shirui Pan", "Fu Lin", "Bo Du", "Xiuwen Gong", "Wenbin Hu"], "abstract": "Molecular representation learning is pivotal in predicting molecular properties and advancing drug design. Traditional methodologies, which predominantly rely on homogeneous graph encoding, are limited by their inability to integrate external knowledge and represent molecular structures across different levels of granularity. To address these limitations, we propose a paradigm shift by encoding molecular graphs into heterogeneous structures, introducing a novel framework: Knowledge-aware Contrastive Heterogeneous Molecular Graph Learning (KCHML). This approach leverages contrastive learning to enrich molecular representations with embedded external knowledge. KCHML conceptualizes molecules through three distinct graph views-molecular, elemental, and pharmacological-enhanced by heterogeneous molecular graphs and a dual message-passing mechanism. This design offers a comprehensive representation for property prediction, as well as for downstream tasks such as drug-drug interaction (DDI) prediction. Extensive benchmarking demonstrates KCHML's superiority over state-of-the-art molecular property prediction models, underscoring its ability to capture intricate molecular features.", "sections": [{"title": "Introduction", "content": "At the core of computational drug discovery lies Molecular Representation Learning (MRL), a field that integrates state-of-the-art machine learning techniques with biomedical applications. MRL not only enhances our understanding of molecular interactions but also refines predictive models that are critical for both drug property and drug-drug interaction (DDI) prediction, two key tasks in biological research. By transforming extensive molecular datasets into actionable insights, MRL empowers researchers to explore novel therapeutic avenues and tailor treatments to specific biological markers, while also predicting potential drug interactions that could affect treatment outcomes. The accuracy of these predictions is crucial for the evaluation and selection of molecules across a wide range of applications, from therapeutic interventions to industrial chemicals. This precision facilitates the early identification of promising candidates, streamlining the drug development process, mitigating the risk of costly late-stage failures, and ensuring the safe combination of drugs.\nMRL involves the rigorous study of molecular structures and encoding strategies, with advanced models adept at capturing the complexities of molecular geometries, bond types, and functional groups-key factors in both the precise prediction of chemical properties and the prediction of drug interactions [1,2]. Various graph neural network (GNN) architectures, such as GCN [3], GIN [4], GAT [5], GGNN [6], and GraphSage [7], offer distinct approaches to molecular structure learning. Increasingly, MRL has been aligned with the Message Passing Neural Network (MPNN) framework, as established by Gilmer et al. [8], which has emerged as a fundamental paradigm in the field. These models emphasize the graph-based topologies of molecular structures, with advanced variants such as DMPNN [9], CMPNN [10], and COMPT [11] leveraging both node and edge attributes to enhance message-passing efficiency and accuracy for tasks such as molecular property prediction and DDI prediction.\nRecent advancements in self-supervised learning, exemplified by context prediction and attribute masking in PreGNN [12] and GROVER [13], have shown remarkable potential in both molecular property and DDI prediction. These approaches introduce advanced methodologies for learning molecular representations but remain primarily focused on local structural properties, often neglecting the integration of external knowledge such as drug-target interactions or therapeutic outcomes. To address this gap, knowledge graph (KG)-based methods, including KGNN [14] and MDNN [15], have emerged, framing molecules as interconnected nodes to incorporate external pharmacological insights. These models provide a more comprehensive perspective, blending molecular and therapeutic views. However, a critical limitation of current KG-based frameworks lies in their inability to seamlessly integrate the detailed microscopic features of drug molecules with their broader biological roles, particularly when predicting interactions between drugs.\nDespite significant strides, molecular representation learning continues to face profound challenges. The inherent complexity and heterogeneity of molecular structures frequently hinder the formation of robust embedded representations. Furthermore, traditional methods relying on homogeneous graph encoding are constrained by their limited capacity to incorporate external knowledge and often fail to capture the multi-granular intricacies of molecular structures. Consequently, integrating structural and pharmacological data into a cohesive model remains a complex yet essential task.\nAs the field progresses, contrastive learning has emerged as a powerful technique, enhancing the generalizability and resilience of graph encoders. However, significant challenges persist, as illustrated in Figure 1. Graph augmentation strategies\u2014such as node dropping, edge perturbation, attribute masking, and subgraph generation [16]\u2014can unintentionally compromise the fidelity of molecular structures, particularly when external knowledge is integrated [17]. While these augmentations offer new views of molecular configurations, they often overlook the profound impact of minor structural perturbations on pharmacological properties and drug interactions. For example, perturbing edges within a benzene ring, a fundamental structural motif known for its stability and distinctive chemical reactivity, can misrepresent the molecule's"}, {"title": null, "content": "aromatic characteristics, potentially misleading the model. Similarly, removing specific nodes, such as a chlorine atom, risks eliminating crucial information regarding the molecule's reactivity and solubility, given chlorine's critical role in the biological activity of many pharmaceutical compounds. Most models fall short in accounting for these subtleties during contrastive learning sample generation, especially when predicting drug interactions."}, {"title": "Author summary", "content": "In the field of drug discovery, predicting molecular properties and drug interactions is crucial for developing new medications and ensuring patient safety. Traditional methods for representing molecular structures often fail to incorporate external knowledge and struggle to capture complex interactions at different levels of detail. To address these limitations, we developed a new framework called Knowledge-aware Contrastive Heterogeneous Molecular Graph Learning (KCHML).\nOur approach integrates information from three perspectives\u2014molecular structure, elemental relationships, and pharmacological data using advanced machine learning techniques. This combination allows for a more detailed and accurate representation of molecules, leading to better predictions of molecular properties and drug interactions. By improving how we model and understand molecules, our work has the potential to streamline drug development and reduce the risk of harmful drug interactions, contributing to safer and more effective treatments."}, {"title": "Materials and methods", "content": "In this paper, scalars are denoted using lowercase letters (e.g., x), vectors by bold lowercase (e.g., x), and matrices with bold uppercase letters (e.g., X). Sets are represented in uppercase italics (e.g., X)."}, {"title": "Elemental Knowledge Graphs", "content": "The elemental KG, denoted as GE = {(h,r,t) | h, t \u2208 VE, r \u2208 RE}, is structured hierarchically to represent various levels of chemical knowledge. In this context, VE comprises entities, while RE captures the relationships between them. The KG is divided into three distinct levels, each offering progressively finer granularity of information.\nAt the highest level, \"class\" nodes convey broad categorical concepts, defining high-level chemical classifications and relationships. For example, the triple (\"ReactiveNonmetal\", \"isSubClassOf\u201d, \u201cNonmetals\") establishes a hierarchical link between \"ReactiveNonmetal\" and its parent class \"Nonmetals,\u201d encapsulating abstract chemical groupings.\nThe intermediate level encompasses core chemical entities such as elements (e.g., \"C\", \"N\", \"O\") and functional groups (e.g., \"Nitrile\", \"Nitro\", \"Acetal\"). Triples at this level, like (\u201cC\u201d, \"isPartOf\", \"Acetal\"), indicate that carbon is part of the functional group \"Acetal,\" denoted in SMARTS notation as \u201cO[CH1][OX2H0]\u201d. These relationships define how elements combine to form higher-order chemical structures.\nAt the most granular level, property nodes capture specific attributes of chemical entities, such as atomic weight or periodicity. An example triple (\u201cC\u201d, \u201chasWeight\u201d, \u201cWeight2\") denotes that carbon's atomic weight falls within the \"Weight2\" category, while (\u201cO\u201d, \u201cisInPeriod", "Period2": "places oxygen within Period 2 of the periodic table.\nThis hierarchical framework enables the elemental KG to integrate conceptual, structural, and property-level information, offering a comprehensive representation of chemical knowledge from broad classifications to specific properties."}, {"title": "Drug Knowledge Graph", "content": "The drug KG, denoted as GD = {\u27e8h,r,t\u27e9 | h, t \u2208 VD, r \u2208 RD}, is an integrated biological network that encompasses entities categorized as drugs, along with associated concepts such as genes, compounds, diseases, biological processes, side effects, and symptoms. This expansive structure organizes these entities while intricately mapping the complex interactions and relationships that exist among them.\nIn GD, VD represents the entities, which range from drug molecules to biological markers, and RD defines the various types of relationships, such as drug-gene interactions, drug-disease"}, {"title": null, "content": "associations, and drug-side effect linkages. The drug KG thus provides a comprehensive knowledge framework, capturing the diverse roles that drugs play within biological and medical systems, and offering deep insights into their interactions with biological pathways and molecular targets."}, {"title": "Heterogeneous Molecular Graph", "content": "Based on the integrated information sources, we categorize the Heterogeneous Molecular Graph (HMG) into three distinct views:\n\u2022 Molecule View GM : Generated solely by RDKit using the Simplified Molecular Input Line Entry System (SMILES), this view provides a foundational representation of molecular structures, focusing on atoms, bonds, and connectivity. It does not require external knowledge, capturing basic molecular details.\n\u2022 Element View GEM : This view enhances the molecule view GM by integrating nodes from the element knowledge graph (KG) GE. It incorporates chemical domain knowledge, such as elemental properties and functional groups, thereby enriching the graph's connectivity. This added information helps model molecular interactions with greater detail by incorporating atomic-level chemical data.\n\u2022 Drug View GDM: Augmented by the drug KG GD, this view includes a Drug Node (DNode) that acts as a central hub linking various nodes related to drugs, including genes, biological processes, and diseases. Initialized with embeddings from the drug KG, it integrates extensive pharmacological knowledge, such as drug efficacy, side effects, and biological mechanisms. This view is essential for guiding molecular pre-training by providing external insights and established drug properties, thus facilitating more accurate downstream predictions, sunch as DDI."}, {"title": "Problem Formulation", "content": "This study aims to develop a self-supervised graph encoder, denoted as h = f(GM) \u2208 Rd, that transforms molecular graphs into high-dimensional vectors without relying on external labels. The training of this encoder leverages knowledge graphs GE and GD to enrich the encoding process with contextual information."}, {"title": "Framework Overview", "content": "As illustrated in Figure 2, the cross-view Knowledge-aware Contrastive Heterogeneous Molecular Graph Learning (KCHML) approach consists of three key components: (1) multi-view augmented graph generation, (2) knowledge-enhanced molecular representation, and (3) cross-view contrastive objectives. This section provides an overview of each component."}, {"title": "Multi-view Augmented Graph Generation", "content": "Molecule View Within the molecule view, we delineate two node types and three edge types:\n\u2022 Atom-Bond-Atom: These edges denote the chemical bonds linking atoms within the molecule, representing its foundational chemical structure.\n\u2022 Fragment-Reaction-Fragment: Generated through the BRICS molecular fragmentation algorithm [18], where \"Fragment\u201d signifies molecular segments and \"Reaction\" denotes the breakpoints between these fragments.\n\u2022 Atom-Join-Fragment: Represents the association between an atom and its corresponding fragment within the molecule."}, {"title": "Element View", "content": "Element View : Expanding from the molecule view GM, we incorporate two additional node types and introduce five new edge types:\n\u2022 Atom-AE-Element: Links an atom node in GM to an element node in GE based on shared chemical symbols (e.g., linking a \"C\" atom to a \"C\" element).\n\u2022 Fragment-FrFu-Functional Group: Formed by detecting functional groups within molecular fragments in GM that correspond to entries in GE.\n\u2022 Element-EE-Element: Represents multi-hop connections between element nodes in GE , retaining only 2-hop connections in the Heterogeneous Molecular Graph (HMG). Edge features are determined by the attributes of intermediate nodes traversed.\n\u2022 Functional Group-FuFu-Functional Group: Depicts multi-hop connections between functional group nodes in GE, with 2-hop connections preserved in the HMG. Edge features are defined by intermediate nodes traversed.\n\u2022 Element-EFu-Functional Group: Transfers relationships from GE between elements and functional groups into the HMG.\nNote that multiple common attributes between elements and functional groups in GE result in the addition of new edges representing EE, FuFu, or EFu. This allows for the presence of multiple edges between any pair of nodes in GEM ."}, {"title": "Drug View", "content": "Drug View: Integrating the drug KG GD into the molecular view introduces a new type of node and two types of edges, linking each node to the DNode:\n\u2022 Atom-AD-DNode: Connects each atom node in GM to DNode.\n\u2022 Fragment-FrD-DNode: Connects each fragment node in GM to DNode."}, {"title": null, "content": "Notably, not all molecules have corresponding drug IDs. Therefore, we have devised batch generation strategies and cross-view contrastive learning techniques to handle these scenarios effectively."}, {"title": "Knowledge-Enhanced Molecular Representation", "content": "The encoding of the Heterogeneous Molecular Graph (HMG) focuses on implementing sophisticated message-passing mechanisms crucial for propagating node features across the network. Inspired by the Graph Transformer architecture [19], our approach adeptly navigates through diverse node and edge types within the HMG. This iterative process involves updating node states by aggregating neighborhood features, thereby capturing both local details and global molecular characteristics comprehensively.\nAlgorithm 1 outlines the detailed procedure for encoding the HMG, emphasizing the adaptability of our model to incorporate various node and edge types effectively. Within the element view, a dual message-passing mechanism manages multiple edge connections efficiently, ensuring each type contributes distinct information that enhances the semantic robustness of the molecular representation."}, {"title": "Initialize Input", "content": "For a given node vi \u2208 V with features ai \u2208 Rdv and an edge eij \u2208 E with features Bij \u2208 Rde, these input features ai and Bij undergo linear projection to be embedded into d-dimensional hidden features hand he, respectively. When encoding the edge eij, we integrate the encoding of the source node vi into its initial representation. It's important to note that heij \u2260 heji, indicating that distinct representations are maintained for each direction of the edge between connected nodes.\nhvi = Init(ai) = (aiAs(vi) + a) + (1\u00bfC\u00ba + c\u00ba); \nheij = Init(Bij) = BijBs(eiz) + b\u00b0 + hoi                                                                                                                                (1)"}, {"title": "Dual Message Passing Mechanisms", "content": "The KCHML framework incorporates a dual message-passing mechanism that operates on the principle of simultaneous propagation between nodes and edges. This approach enables nodes to receive messages from their adjacent edges while allowing edges to aggregate information from both their source nodes and neighboring edges. This bi-directional exchange of information across the HMGs enhances the interconnectedness and richness of the representations. The dual message-passing mechanism is pivotal in facilitating the simultaneous propagation of information between nodes and edges, thereby creating more comprehensive and interconnected representations within the KCHML framework.\nOur approach diverges from the standard Transformer architecture in two fundamental ways. Firstly, when computing queries, keys, and values (Q, K, V), we employ linear mappings that project heterogeneous node and edge types into a unified feature space. This transformation ensures that despite their inherent differences, all nodes and edges can be processed within a shared space. This unified space facilitates more effective aggregation and comparison of features across different types. Specifically:\nql,k = hl-1WW$(v);\nkl,k = hl-1WkW(v);\n(2)\nvl,k = hl-1WkW (2)\nHere, W , W, and We are linear projection matrices designed to map the input embeddings into multiple attention heads. The matrix W8(v) represents a type-specific transformation that ensures all node types are projected consistently into a unified feature space. This process enables the model to generate uniform attention representations across diverse node types. The term l = 1\u00b7\u00b7\u00b7 L denotes the current layer index in the stack, while k = 1\u00b7\u00b7\u00b7 K indicates the number of attention heads. Similarly, edge features undergo a similar transformation, where the edge type detection function (e) is used to project edge types into a shared feature space."}, {"title": null, "content": "Secondly, rather than using a conventional self-attention mechanism, we introduce a specialized attention mechanism. In this structure, the query (Q) denotes the recipient of the"}, {"title": null, "content": "message (i.e., the node or edge receiving the message), while the key (K) and value (V) are derived from the message sender (i.e., the node or edge transmitting the message). This design emphasizes the most pertinent message sources, enhancing the effectiveness of information dissemination across the graph. The attention mechanism is defined as follows:\nAttention(Q, K, V) = Softmax  QKT/\u221ad/K V                                                                                                               (3)\nCompared to existing MPNN structures, our HMG Encoder performs simultaneous Node Aggregation and Edge Aggregation, thereby synchronizing information propagation between nodes and edges more effectively.\nNode Aggregation : **: For each node vi, our model gathers messages from connected edges epi using multi-head attention. This process aggregates message sources to update the representation of node vi:\nmvi = AGG(1)({ep1|vp \u2208 N(vi)})\n1\u2211K1k=1 Attention(qek, kek, vek)Wv;        (4)\nHere, || denotes concatenation in multi-head attention mechanisms, and Wv \u2208 Rd\u00d7d.  m\u2208 Rd represents the aggregated messages at node vi during the l-th iteration of message passing. This aggregation incorporates messages from all incoming edges epi directed towards Vi.\nEdge Aggregation : Similarly, for each edge eij, our model collects information from both the source node vi and neighboring edges epi. This mechanism captures relationships between nodes and edges, ensuring comprehensive edge representations:\nmeij = AGG(1) (\u03c8 \u2208 {v} \u222a {el : vp \u2208 N(vi)})\nK\u22111k=1 Attention(qek, kek, vek))WE                 (5)\nHere, WE \u2208 Rd\u00d7d. m\u00b2 \u2208 Rd denotes the messages aggregated for edge eij, incorporating contributions from the source node vi and all edges epi converging at vi. Importantly, the contribution from the reverse edge eji is inherently considered within the set {epi}, ensuring a comprehensive collection of edge-based information flows.\nIn each iteration, our model updates both nodes and edges simultaneously, ensuring the dynamic evolution of the global molecular representation. This approach enables continuous refinement of the graph understanding, capturing both micro-level interactions (e.g., atomic bonds) and macro-level properties (e.g., pharmacological attributes). This iterative process facilitates the model's ability to adapt and enhance its representation of complex molecular structures comprehensively.\nUpdate Function\nFollowing the message-passing step, the update function integrates the incoming message vectors with the previous node or edge embeddings using a Multi-Layer Perceptron (MLP). This process ensures the seamless incorporation of new information into the existing representation, enabling the model to learn complex patterns while mitigating the risk of gradient vanishing or exploding:\nh = UPDATE(h1, m)\nVi = LeakyReLU((h1||m\u00b2)W())                                                                                                                               (6)"}, {"title": null, "content": "Here, Wl \u2208 R2d\u00d7d denotes the update matrix, and the LeakyReLU activation function ensures consistency in the update process across nodes and edges, while accommodating the distinct attributes of different types."}, {"title": "Graph Readout", "content": "In the final stage, we aggregate the learned node and edge representations into a unified graph representation using Self-Attention Graph Pooling (SAGPooling):\nhg = READOUT({h}, {h}))\n= SAGPooling(\u0397\u03bd, \u0397\u03b5)                                                                                                                            (7)\nThis final aggregation step ensures that both node-level and edge-level information is preserved in the global graph representation, leading to a more comprehensive understanding of molecular properties."}, {"title": "Cross-view Contrastive Objective", "content": "To utilize data from approved drug molecules effectively, we have devised a method to generate batch data for incorporation into our training procedures. This methodology guarantees that each batch is both balanced and representative, essential for training reliable predictive models in drug discovery and cheminformatics. Algorithm 2 outlines the approach employed for generating training batch data."}, {"title": "Batch Generation Strategy", "content": "To utilize data from approved drug molecules effectively, we have devised a method to generate batch data for incorporation into our training procedures. This methodology guarantees that each batch is both balanced and representative, essential for training reliable predictive models in drug discovery and cheminformatics. Algorithm 2 outlines the approach employed for generating training batch data."}, {"title": "Cross-view Contrastive Loss", "content": "Based on our batch generation strategy, for a mini-batch of size N, we generate three sets of views as follows:\n\u2022 SM = {GM,...,GM}: This set includes the Molecule Views for all N molecules in the mini-batch.\n\u2022 SEM = {GFM, . . ., GEM }: This set comprises the Element Views for all N molecules in the mini-batch.\n\u2022 SDM = {GPM,...,GPM}: This set includes the Drug Views for the last 0.3N molecules in the mini-batch that have Drug IDs.\nThe loss function L(G1,92,i) for molecule i, focused on views G\u00b9 and G2, is formulated as:\nL(G1,92,i) = -log  esim(zgos)/T\u03a3\u03b9esim(2)/ + \u03a3\u03b9 sim(2)/                                                (8)\nwhere zg\u0142 and zga are the embeddings of molecule i in G\u00b9 and its positive pair in G2. zgn and zga are embeddings of negative samples for molecule i in G\u00b9 and G2. sim(\u00b7, \u00b7) denotes a similarity function measuring the similarity between embeddings. 7 is a temperature parameter that scales the logits for better convergence. The vector zg is derived from hg through the Projector head, expressed as:\nzg = fprojector (hg)                                                                                                                                               (9)\nIn the loss function provided, the terms and their roles are clarified as follows:\n\u2022 Positive Sample (Inter-Positive Contrastive) esim(ZZ)/T:\n\n/T: This term measures the similarity between the embedding z\u00e7\u0131 from G\u00b9 and its corresponding positive pair zgos from G2.\n\u2022 First Term in the Denominator (Intra-Negative Contrastive) esim(z\u00e7\u0131,z9g)/7: This term sums over negative samples zgh, within G1.\n\u2022 Second Term in the Denominator (Inter-Negative Contrastive) esim(zg\u0142 zgneg)/7:  /:: This term sums over negative samples zg2 within G2.\nDue to the inherent sparsity of drug molecules, the size of SDM is invariably less than that of SM and SEM. Let us consider a view G\u00b9 encompassing M samples and a view G2 comprising N samples, where M < N. In this context, the number of inter-positive contrastive pairs between the two views is M, the number of inter-negative contrastive pairs associated with any given positive pair amounts to M \u2013 1, and the number of intra-negative contrastive pairs is N - 1. It is important to observe that L(G1,G2,i) \u2260 L(G2,G1,i). Consequently, the loss function between G\u00b9 and G2 can be expressed as:\nC1,2 = 1M\u03a3Mi=1L(G1,92,2) + 1M\u03a3Mi=1L(G2,G1,i)                                                                                                                                   (10)\nWhen either G\u00b9 or G2 is the drug view, the distribution of positive and negative samples within the loss function is depicted in Figure 4(a). In this figure, each row illustrates a sampled positive pair alongside its corresponding N + M - 2 negative sample pairs. Conversely, when neither G\u00b9 nor G2 is the drug view, the configuration of positive and negative samples is presented in Figure 4(b). In this scenario, the arrangement adheres to conventional practices in contrastive learning."}, {"title": null, "content": "Thus, the comprehensive loss function encompassing all three views is expressed as:\nLtotal = LM,EM + CM,DM + LEM,DM                                                                                                                                                                                   (11)\nwhere LM,EMCM,DM and LEM,DM denote the losses associated with the molecular-element, molecular-drug, and element-drug view pairings, respectively."}, {"title": "Fine-tuning on Molecular Property and DDI Prediction Tasks", "content": "The pre-trained molecular encoder is adaptable to a wide range of downstream molecular prediction tasks. As depicted in Figure 5, the fine-tuning process demonstrates the application of the HMG Encoder for predicting both molecular properties and DDIs."}, {"title": "Molecular Property Prediction", "content": "In the molecular property prediction task, every molecular graph G is encoded by a HMG encoder. The overall graph representation hg is then passed through a nonlinear predictor composed of fully connected layers, mapping the graph embedding to the target property:\n\u0177mp = fpredictor(hg)                                                                                                                                                                                      (12)"}, {"title": "DDI Prediction", "content": "DDI Prediction In the DDI Prediction task, a pair of molecular graphs G1 andG2 are independently encoded using the same HMG encoder, resulting in the graph embeddings hg\u2081 and hg2. This pair of embeddings are then concatenated into a joint representation:\nhpair = [hg1, hg2]                                                                                                                                                                                             (13)"}, {"title": null, "content": "This concatenated embedding is fed into a nonlinear predictor, which outputs the prediction of the interaction between the two drugs:\n\u0177ddi = fpredictor(hpair)                                                                                                                                                                                               (14)"}, {"title": "Results and Discussion", "content": "In the pre-training phase, the KCHML model was initially trained on a comprehensive dataset comprising 250,000 unlabeled molecules sourced from ZINC15 [20], along with 8,358 organic molecules from DRKG [21]."}, {"title": "Experimental Setup", "content": "In the pre-training phase, the KCHML model was initially trained on a comprehensive dataset comprising 250,000 unlabeled molecules sourced from ZINC15 [20], along with 8,358 organic molecules from DRKG [21]."}, {"title": "Pre-training Phase", "content": "In the pre-training phase, the KCHML model was initially trained on a comprehensive dataset comprising 250,000 unlabeled molecules sourced from ZINC15 [20], along with 8,358 organic molecules from DRKG [21]."}, {"title": "Fine-tuning", "content": "For the molecule property prediction tasks, we optimized computational efficiency by focusing solely on the molecule view encoder for downstream tasks, based on the premise that this encoder proficiently incorporates information from both the elemental and drug views.\nAdditionally, we enhanced the SAGPooling graph readout layer by integrating newly designed SAGPooling and MLP layers specifically tailored to the tasks at hand. We evaluated the model using 13 benchmark datasets from MoleculeNet [22], which encompass a wide array of molecular data across various scientific disciplines. To ensure robust and reliable performance, we adhered to standard practices by employing 5-fold cross-validation with an 8:1:1 train/validation/test split, and conducted three independent training runs.\nFor the DDI prediction task, we follow the transductive setting and negative sample generation strategy from GMPNN [23], a method specifically designed for DDI prediction. Given that the DRKG database already includes some data from DrugBank, we opted to exclude DrugBank from our experiments and instead evaluated the model performance solely on the TwoSide dataset."}, {"title": "Baselines", "content": "Supervised Learning Baselines : We benchmarked KCHML against several well-established graph neural network architectures, including GCN [3], GIN [4], and AttentiveFP [24]. Additionally, we compared it with two variants of message-passing neural networks (MPNNs) specifically tailored for molecular data: DMPNN [9] and CMPNN [10]. CoMPT [11] was also included for its consideration of edge features and its enhancement of message interactions between bonds and atoms.\nPre-trained Methods: In the realm of predictive-based self-supervised learning, we incorporated several pre-training models for comparison: N-GRAM [25], which constructs node embeddings through short walks and employs Random Forest or XGBoost for property prediction; Hu et al. [12] and GROVER [13], which integrate both node-level and graph-level knowledge in their pretext tasks.\nGraph Contrastive Learning Baselines: We evaluated KCHML against existing graph contrastive learning frameworks. MoCL [17] leverages domain knowledge at two distinct levels to enhance representation learning. MolCLR [26] applies general graph augmentation techniques to molecular data. KCL [27] utilizes a chemical element knowledge graph (KG) to augment the original molecular graph."}, {"title": "Overall Performance", "content": "Table 1 and 2 display the performance of various models across 13 datasets for molecule property prediction. The following conclusions can be drawn regarding the performance of various models:\n\u2022 KCHML's Superior Performance: Across both classification (Table 1) and regression tasks (Table 2), the KCHML model consistently outperformed all others. It achieved the highest ROC-AUC scores for classification and the lowest RMSE for regression tasks, demonstrating its strong predictive capabilities for molecular properties. This highlights the effectiveness of KCHML's architecture in integrating advanced learning algorithms with complex molecular data.\n\u2022 Leverage of KGs: Models like KCHML and KCL, which incorporate knowledge graphs, showed a clear advantage over methods that do not. This is attributed to the rich semantic information provided by KGs, enabling these models to capture intricate molecular interactions and properties more effectively. The integration of structured data through KGs significantly enhanced their prediction accuracy.\n\u2022 Comparison with Contrastive Learning Models: KCHML demonstrated marked improvements over models such as MoCL and MolCLR, both of which employ contrastive learning but do not utilize domain knowledge. Unlike MoCL's augmentation strategies, which may disrupt molecular integrity and introduce noise, KCHML uses more refined augmentation techniques that preserve the structure and function of the molecules. This approach leads to more accurate and stable learning, reducing the risk of misleading training signals."}, {"title": null, "content": "In the DDI prediction task, we retained only the models that performed well in molecular property prediction and compared them against two baseline models specifically designed for DDI prediction: DeepDDI [28] and GMPNN [23]. Table 3 summarizes the performance of each model across various metrics on TwoSide dataset, from which we drew the following conclusions.\n\u2022 In the DDI prediction task, by concatenating the molecular representations and feeding them directly into the Predictor, we achieved results that outperformed all previously existing molecular representation approaches for general tasks."}, {"title": null, "content": "\u2022 Our model, KCHML, significantly improved upon the KCL method, which leverages knowledge graphs of chemical elements. KCHML's hierarchical molecular graph (HMG) approach captures not only elemental relationships but also substructural and biological information, providing a more nuanced view of molecular interactions.\n\u2022 KCHML showed highly competitive performance against GMPNN, a model focused on learning shared substructures between molecule pairs. In several key metrics, KCHML matched or outperformed GMPNN, benefiting from its comprehensive HMG framework that integrates both elemental and substructural data."}, {"title": "Ablation Experiments", "content": "To explore the impact of different encoders, we replaced our HMG encoder with other encoders such as RGCN, MPNN, DMPNN, and CMPNN to form KCHMLR, KCHMLM, KCHMLD, and KCHMLC, respectively. Table 4 shows the performance values on the classification task."}, {"title": null, "content": "The following conclusions can be observed:\n\u2022 The HMG encoder within our contrastive learning framework consistently outperforms others, as demonstrated in Table 4. Meanwhile, KCL and KCHML with the CMPNN encoder displayed sub-optimal results.\n\u2022 Comparing Tables 1 and 4 reveals that all models (MPNN, DMPNN, and CMPNN) significantly benefit from the multi-view contrastive learning framework, suggesting the framework's effectiveness in providing a more comprehensive molecular representation.\nTo examine the impact of each view in model pre-training, we removed the element view and the drug view to assess the performance, denoted as KCHMLw/oeE and KCHMLw/oD, respectively."}, {"title": null, "content": "The summarized results, displayed in Table 5, indicate that removing the element view significantly reduces the model's performance. This suggests that the element view is crucial for accurately capturing the fundamental chemical properties and interactions within the molecules. Conversely, while eliminating the drug view does affect the model's effectiveness, it does not prevent the model from still outperforming KCL, indicating that the drug view, while important, is less critical than the element view. The absence of the drug view still allows the model to perform robustly, likely due to the retention of basic molecular structure and function information from the remaining views."}, {"title": "Case Study", "content": "The multi-view encoder approach provides multiple perspectives for interpreting molecular property predictions. This allows us not only to identify key structural elements but also to deepen our understanding of chemical properties and their impact on molecular behavior. We illustrated the interpretability of the model by visualizing the molecule Ancitabine from the HIV database. The following conclusions can be derived:\n\u2022 At the fragment level, our analysis identified a crucial segment in the molecule's structure, a \"Cytarabine\" derivative, which was assigned a higher attention weight due to its significance."}, {"title": null, "content": "\u2022 At the atomic level, we discerned varying importance levels for identical atoms within \"Ancitabine\". Particularly, the 11th oxygen atom, which eventually undergoes hydrolysis in the body, forms \"Cytarabine\" and prevents \u201cThymidine\u201d from doping into the DNA, so the proportion in weight allocation is relatively small.\n\u2022 From a functional group perspective, the functional group \u201cPhenyl\u201d, was allocated the highest weight. In contrast, the common \"Alkyl\" structure was assigned a significantly lower weight during the learning process."}, {"title": "Conclusion", "content": "In this study, we introduced the cross-view KCHML method, which is a significant advancement in molecular property prediction. We introduced and utilized HMG to integrate more external knowledge and capture finer molecular structure details. KHCML's innovative multi-view framework and dual message passing mechanism provide a comprehensive molecular property prediction method that improves result accuracy and robustness. Furthermore, the method's effectiveness is demonstrated through extensive experiments, outperforming existing state-of-the-art methods.\nIn future work, we plan to further explore and expand on the use of heterogeneous graph-based methods for drug encoding. This will involve delving into how different types of data, including three-dimensional molecular structures and descriptive textual information, can be effectively integrated into the model. Such advancements could greatly enhance our understanding of drug properties and interactions, paving the way for more nuanced and precise drug discovery processes."}, {"title": "Author contributions statement", "content": "Chen and Hu conceived the experiments and analyzed the results, Chen experimented and wrote the manuscript. All authors guided the writing of the manuscript and reviewed the manuscript."}, {"title": "Competing interests", "content": "No competing interest is declared."}]}