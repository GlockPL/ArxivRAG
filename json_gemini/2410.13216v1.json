{"title": "ANCHORED ALIGNMENT FOR SELF-EXPLANATIONS ENHANCEMENT", "authors": ["Luis Felipe Villa-Arenas", "Ata Nizamoglu", "Qianli Wang", "Sebastian M\u00f6ller", "Vera Schmitt"], "abstract": "In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning-self- explanation-even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, fine-tuning these models for specific applications often leads to a critical trade-off: improvements in one area may compromise the model's generalization capabilities (Yang et al., 2024; Kirk et al., 2024). In our study, we aim to enhance a secondary task-specifically, the model's ability to articulate reasoning processes in natural language, a skill known as self-explanation (Madsen et al., 2024a)-in parallel with the primary task, despite the constraint of not having human- annotated rationales.\nThe lack of annotated data of both high- and low-quality explanations can be framed in the context of model aligning without human preference data. Recent research has explored ways to align LLMs without direct human input. Some approaches generate self-instruct data to fine-tune models (Wang et al., 2023; Chen et al., 2023; Gulcehre et al., 2023), while others, like Bai et al. (2022); Yuan et al. (2024); Wu et al. (2024), use LLM-generated feedback to train reward models. Building on these advancements, we propose an end-to-end approach to align LLMs on classification tasks while also ensuring the generation of high-quality self-explanations, even without annotated data for this secondary task. Our approach integrates three core components: evaluating generated explanations, creating self-instruct datasets, and aligning the model. Additionally, we introduce Alignment with Anchor Preference Pairs, a method that improves preference pair selection by categorizing model responses into three groups: consistently correct, consistently incorrect, and variable. For each category, we apply tailored strategies to construct preference pairs, which are then used in the Direct Preference Optimization (DPO) phase (Rafailov et al., 2023). Our results demonstrate that this method consistently improves explanation quality, mitigating the degradation caused by SFT. Moreover, we show that using anchor preference pairs outperforms self-alignment strategies that rely solely on judge-based evaluations for preference pair selection.\nOur contributions are summarized as follows:"}, {"title": "2 A FRAMEWORK FOR QUALITATIVE ASSESSMENT OF SELF-EXPLANATIONS", "content": "2.1 QUALITY CRITERIA FOR EFFECTIVE SELF-EXPLANATIONS\nTo assess self-explanation quality, we focus on the model's ability to effectively communicate its reasoning. This approach differs from previous work that emphasized trustworthiness metrics such as faithfulness (Madsen et al., 2024b;a; Lanham et al., 2023; Lyu et al., 2023; Turpin et al., 2023; Parcalabescu & Frank, 2024) and truthfulness (Zhang et al., 2024; Sharma et al., 2023; Burns et al., 2022; Joshi et al., 2024). We evaluate self-explanations based on the following criteria:\n1. Logical coherence: The explanation should follow a clear and logical reasoning process, with all components cohesively connected to form a unified, non-contradictory narrative.\n2. Clarity: The explanation must present ideas clearly and precisely, using appropriate terminology to effectively communicate complex concepts without unnecessary complexity.\n3. Relevance: The explanation should comprehensively address the task at hand, directly answering the specific context or requirements without omitting critical information.\n4. Depth of argumentation: The explanation must provide strong reasoning and credible evidence to support its conclusions, reflecting a deep understanding of the task.\n5. Factual accuracy: This criterion assesses the correctness of individual claims within the explanation. While related to truthfulness, factual accuracy focuses on whether specific statements align with established knowledge.\n2.2 SELF-EXPLANATIONS EVALUATION METHODOLOGY\nLet M represent a large language model tasked with generating responses for a classification problem. Each response consists of two components: a self-explanation, denoted as $\u025b_i$, and a predicted classification label, $\u0177_i$, corresponding to an input prompt $x_i$. The self-explanation $\u025b_i$ is produced by prompting the model to articulate its reasoning before providing a final prediction, following the Chain-of-Thought prompting strategy (Wei et al., 2022).\nOur methodology is inspired by recent approaches that utilize LLMs as evaluators of other models' outputs (Dubois et al., 2023; Li et al., 2024; Fernandes et al., 2023; Bai et al., 2023; Saha et al., 2024). This approach has shown versatility, extending beyond simple evaluation to various applications in model improvement and self-alignment strategies. For instance, researchers have employed this framework to generate self-instruct data for fine-tuning models (Wang et al., 2023; Chen et al., 2023; Gulcehre et al., 2023) and to create feedback for training reward models (Bai et al., 2022; Yuan et al., 2024; Wu et al., 2024).\nFor our evaluation, we employ a more capable model, $M_{Judge}$, to assess the quality of self- explanations $\u025b_i$ based on predefined criteria (detailed in Section 2.1). The evaluation process pro- ceeds as follows:"}, {"title": "2.3 PAIRWISE MODEL EVALUATION", "content": "To assess the quality of self-explanations generated by different models, we adopt a pairwise evaluation strategy consistent with previous work (Chen et al., 2023; Yuan et al., 2024; Wu et al., 2024). For each input prompt $x_i$, we generate N sample self-explanations, capturing the inherent variability in model outputs. The score for the n-th response is denoted as $s_i^n$, with the corresponding explanation and prediction represented by the pair ($\u025b_i^n$, $\u0177_i^n$).\nFor a given prompt $x_i$, we conduct $N^2$ pairwise comparisons between the explanations generated by two models, $M_1$ and $M_2$. A win for model $M_1$ is defined when:\n$s_i^n(M_1) > s_i^m(M_2)$\nwhere n, m \u2208 {1, . . ., N}. The overall win rate W($M_1$, $M_2$) is then calculated as follows:\nW(M1,M2) = $\\frac{1}{|X|} \\sum_{x_i \\in X} (\\frac{1}{N^2}\\sum_{n=1}^{N} \\sum_{m=1}^{N} [s_i^n(M_1) > s_i^m(M_2)])$\nHere, X denotes the set of all prompts, while [\u00b7] represents the indicator function that returns 1 if the condition is true and 0 otherwise. This approach facilitates a nuanced comparison of model performance by taking into account the distribution of explanation qualities, rather than relying solely on single-point estimates. The rates for ties, defined as $s_i^n(M_1) = s_i^m(M_2)$, and losses, defined as $s_i^n(M_1) < s_i^m(M_2)$, are computed in a similar manner (see Appendix A). Throughout the evaluations presented in this work, $M_2$ refers to the baseline model $M_{base}$."}, {"title": "3 SELF-EXPLANATION ALIGNMENT WITH ANCHOR PREFERENCE PAIRS", "content": "In this section, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning-self-explanation\u2014even in the absence of annotated rationale explanations. However, we assume access to human-annotated data in the form of classification datasets for domain-specific adaptation, reflecting a common constraint in real- world applications, where comprehensive explanation data is often scarce or prohibitively expensive compared to classification datasets.\nBuilding on prior work (Bai et al., 2022; Wang et al., 2023; Yuan et al., 2024; Wu et al., 2024), our alignment methodology incorporates familiar components such as self-instruction dataset genera- tion, human-free evaluation of candidate responses using LLM-as-Judge, preference pair selection, and model alignment.\nHowever, our approach differs from previous methods in two key ways: First, for the assessment of candidate responses, we use the evaluation explanation quality framework introduced in Sections 2.1 and 2.2. Second, we propose a novel technique, Alignment with Anchor Preference Pairs, which improves preference pair selection by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of DPO.\nThe steps of the methodology are as follows:\n1. Supervised fine-tuning of the base model $M_{base}$ specifically on a target classification task, resulting in $M_{SFT}$."}, {"title": "3.1 SUPERVISED FINE-TUNING WITHOUT ANNOTATED EXPLANATIONS", "content": "We fine-tuned the base model, $M_{Base}$, on classification datasets (the primary task) to obtain $M_{SFT}$, simulating scenarios where explanation annotations are unavailable. To replicate typical domain- specific adaptations and avoid potential gains from multi-task learning, we fine-tuned a separate model for each task. During fine-tuning, loss was calculated only on the target tokens corresponding to the correct choice sentence, excluding the system instruction and question. We generated the full text of the selected option to provide richer context and preserve the model's text generation capabilities. Details on datasets and training setups are provided in Section 4.1."}, {"title": "3.2 SELF-INSTRUCTION CREATION", "content": "We generate self-instruct data for alignment as follows:\n1. Generate candidate responses: We sample N diverse pairs of explanations and predic- tions from $M_{SFT}$, denoted as {($\u025b_i^n$, $\u0177_i^n$)}$_{n=1}^N$, where $\u025b_i^n$ represents the explanation for the n-th prediction $\u0177_i^n$ corresponding to the prompt $x_i$.\n2. Evaluate candidate responses: We use the methodology described in Section 2.2 to eval- uate the self-explanations generated from the candidate responses, assigning a score $s_i^n$ to each explanation $\u025b_i^n$. During the creation of the self-instruct dataset, we employ $M_{base}$ as the judge ($M_{Judge}$). This ensures that the model alignment process remains self-contained, without the need for external models, except for evaluation purposes."}, {"title": "3.3 PREFERENCE PAIRS VIA ANCHOR SELECTION", "content": "We introduce a method to enhance the selection of preference pairs by categorizing model responses into three groups: consistently correct, consistently incorrect, and variable. For each category, we apply specific strategies to construct preference pairs, which are then used in during the DPO phase. To evaluate the model's consistency on a given input prompt, a ground truth reference, or anchor, is required. We use a classification task as the probing mechanism.\nPreference Pairs for Consistently Correct Prompts: For input prompts $x_i$ where $M_{SFT}$ con- sistently produces correct answers (i.e., $\u0177_i^n$ = $y_i$ for all n \u2208 {1,..., N}), preference pairs are constructed based on the quality of the explanations. Let $s_i^n$ denote the score assigned by the judge $M_{Judge}$ to the n-th explanation $\u025b_i^n$ for prompt $x_i$. We define two sets: $A_i^+$ = {$\u025b_i^n$ : $s_i^n$ = $\\max_{j\\in{1,...,N}} s_i^j$}, which contains all explanations that achieve the highest score for prompt $x_i$, and $A_i^-$ = {$\u025b_i^n$ : $s_i^n$ < $\\max_{j\\in{1,...,N}} s_i^j$}, which includes all explanations with scores lower than the maximum for prompt $x_i$.\nPreference Pairs for Variable Performance: For input prompts $x_i$ where $M_{SFT}$ produces a mix of correct and incorrect predictions (i.e., $\u0177_i^n$ \u2260 $y_i$ for some n \u2208 {1, ..., N}), preference pairs are constructed contrastively. We define the set $B_i$ = {$\u025b_i^n$ : $\u0177_i^n$ = $y_i$}, which contains explanations associated with correct predictions. From this set, we extract $A_i^+$ \u2286 $B_i$, the subset of explanations with the highest scores assigned by $M_{Judge}$, i.e., $A_i^+$ = {$\u025b_i^n$ \u2208 $B_i$ : $s_i^n$ = $\\max_{j\\in B_i} s_i^j$}. The set $A_i^-$ = {$\u025b_i^n$ : $\u0177_i^n$ \u2260 $y_i$ and $s_i^n$ < $\\max_{j\\in A_i^+} s_i^j$} contains explanations corresponding to incorrect predictions, with scores lower than the maximum score in $A_i^+$.\nPreference Pairs for Consistently Incorrect Prompts: For prompts where all predictions from $M_{SFT}$ are incorrect (i.e., $\u0177_i^n$ \u2260 $y_i$ for all n \u2208 {1, . . ., N }), all corresponding explanations are placed"}, {"title": "4 EXPERIMENTS", "content": "In all experiments, we utilized Llama-3-8B-Instruct as our base model. Our study involved comparing four distinct model configurations:\n1. $M_{Base}$: The base model, which remains unmodified.\n2. $M_{SFT}$: This model was obtained by performing supervised fine-tuning on the $M_{Base}$ model using only classification tasks, simulating scenarios where explanation annotations are not available.\n3. $M_{Rank}$: The $M_{SFT}$ model was further refined using DPO, employing a self-instruct dataset constructed from rank-ordered preference pairs derived solely from judge-based evalua- tions of the explanations. This approach aligns with methodologies described in Bai et al. (2022); Wang et al. (2023); Yuan et al. (2024); Wu et al. (2024).\n4. $M_{Anchor}$ (ours): Similar to $M_{Rank}$, this model was refined using DPO but utilized a self- instruct dataset created with our proposed anchored method for selecting preference pairs.\nSince both $M_{Rank}$ and $M_{Anchor}$ undergo an additional stage of DPO alignment with the self-instruct dataset, we will collectively refer to them as self-aligned models in comparison to $M_{SFT}$."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Datasets: We selected four datasets for our experiments: AQUA-Rat (Ling et al., 2017), ARC-Challenge (Clark et al., 2018), LogiQA (Liu et al., 2020), and OpenbookQA (Mihaylov et al., 2018). These datasets are established benchmarks for reasoning tasks, requiring a challenging reasoning process, which makes them an ideal fit for evaluating the quality of self-explanations. A key factor in their selection was the size of their training sets, which provided a sufficient number of input prompts to support the creation of the self-instruction dataset. In the case of AQUA-Rat, we sampled 5,000 examples due to computational constraints. For evaluation, we used the test split of each dataset.\nSFT Training Details: For $M_{SFT}$, we used the AdamW optimizer with a learning rate of 5 \u00d7 $10^{-5}$ for one epoch, following a cosine schedule with 10% warmup steps. Gradient clipping was set to 0.3, and we used an effective batch size of 12. Loss was computed only on the assistant's completions. Instead of fine-tuning the entire model, we applied a LORA adapter (a = 128, dropout = 0.05, rank r = 256) to all linear layers. LoRA adapters were used to accelerate training and to act as a regularization method (Biderman et al., 2024), addressing the overfitting tendencies of DPO (Thakkar et al., 2024), which is applied during the later alignment phase.\nSelf-Instruct Dataset: To ensure the integrity of our evaluation process, we created separate self- instruct datasets for each benchmark. These datasets were built using input prompts specific to each task, ensuring that the DPO alignment data remained uncontaminated by exposure to multiple tasks. This approach prevents the artificial inflation of results that could occur if models were aligned across diverse tasks, unlike SFT models, which are trained on one classification task at a time. To create the self-instruct dataset for aligning $M_{Rank}$ and $M_{Anchor}$, we sampled N 4 responses from $M_{SFT}$ for each input prompt (settings: temperature T = 0.6 and top-k value of 0.9). This sample size provided a reasonable assessment of the model's consistency and variability. The prompt used is presented in Appendix D. In cases of consistently incorrect responses, $M_{Base}$ was employed as the debater model (Llama-3-8B-Instruct) with adjusted parameters (T = 0.5, top-k 0.9). The responses were scored by $M_{Judge}$, which was the same base model, ensuring a self-contained alignment process. This setup differs from the evaluation phase, where a more capable model serves as the judge. The scoring of explanations followed the methodology outlined in Section 2.2, with $M_{Judge}$ using fixed inference parameters (T = 0). For $M_{Rank}$, preference pairs were chosen based on the assigned scores, with the highest-scoring explanation designated as the winner, and the losing explanation randomly selected from the remaining candidates. The preference pairs for $M_{Anchor}$ were selected using the methodology outlined in Section 3.3, and these pairs were then used to align the models through DPO.\nDPO Training Details: For DPO-aligned models ($M_{Rank}$, $M_{Anchor}$), we used similar hyperparam- eters as in the SFT phase but reduced the learning rate to 5 \u00d7 $10^{-7}$ and trained for 2.6k steps with an effective batch size of 6. The DPO process used a $\u03b2$ value of 0.1 and updated the LoRA weights obtained during SFT.\nEvaluation: We evaluated our models along two main dimensions: prediction accuracy and self- explanation quality. To account for variability in model outputs, we generated N = 16 explanation- prediction pairs per input prompt. The inference settings mirrored those used to create the self- instruction dataset, with a temperature of T = 0.6 and top-k set to 0.9 and the same prompt used during the creation of the self-instruct dataset (see Appendix D.3). Average prediction accuracy was used to measure performance on downstream tasks. To assess self-explanation quality, we performed head-to-head comparisons between the aligned models ($M_{SFT}$, $M_{Rank}$,$M_{Anchor}$) and the base model ($M_{Base}$). These comparisons followed the methodology outlined in Section 2.2, employing Llama-3-70B-Instruct as $M_{Judge}$."}, {"title": "4.2 RESULTS", "content": "Table 1 reports the average classification accuracy for each model, along with pairwise comparisons of self-explanation quality across multiple benchmark datasets. The win, tie, and loss rates are calculated by comparing the aligned models against $M_{Base}$.\n4.2.1 IMPACT OF SUPERVISED FINE-TUNING ON SELF-EXPLANATIONS\nWe observed a significant trade-off in evaluation results before and after applying supervised fine- tuning on a classification task (see Table 1). While SFT notably improved classification accuracy, it resulted in a substantial decline in the quality of self-explanations compared to the base model. The decline in explanation quality, as measured by the win-loss rate difference ($\u0394_{W-L}$), ranged from 15.6% to 30.9% across benchmarks.\nBuilding on evidence that supervised fine-tuning can improve performance on specific tasks at the expense of a model's generalization abilities (Yang et al., 2024; Kirk et al., 2024), we hypothesize that this decline occurs because the task of selecting predefined answers does not inherently encour- age the model to articulate its reasoning, leading to a specialization that diminishes the quality of the generated explanations.\nThese findings highlight the necessity for alignment techniques that can preserve high-quality ex- planations in situations where datasets with annotated explanations are unavailable for fine-tuning.\n4.2.2 ANALYSIS OF SELF-ALIGNED MODELS\nPrediction Accuracy: Our experiments demonstrate that the self-aligned models, $M_{Rank}$ and $M_{Anchor}$, maintain the classification accuracy improvements achieved by the seed model, $M_{SFT}$, over the base model, $M_{Base}$ (see Table 1). Notably, $M_{Anchor}$ consistently achieves the highest, or at least comparable, accuracy across all datasets. We believe that this improvement can be attributed to the fact that the model's predictions ($\u0177_i^n$) are compared to the ground truth ($y_i$) to determine how to treat the self-explanation ($\u025b_i^n$) during the selection of preference pairs while employing the anchor strategy, thereby providing a more informative learning signal.\nSelf-Explanation Quality: Pairwise evaluations of self-explanation quality (see Table 1) indicate that the initial decline in explanation performance observed in $M_{SFT}$ is partially inherited by both $M_{Rank}$ and $M_{Anchor}$, as they use $M_{SFT}$ as the seed model during the DPO alignment phase. Never- theless, both $M_{Rank}$ and $M_{Anchor}$ demonstrate significant improvements in explanation quality com- pared to $M_{SFT}$, with $M_{Anchor}$ exhibiting the strongest performance. Compared to the base model, $M_{Anchor}$ also shows positive $\u0394_{W-L}$ margins on ARC-Challenge (3.3%) and LogiQA (6.9%), and it significantly narrows the gap in explanation quality introduced by SFT across the remaining bench- mark datasets.\n4.2.4 IMPACT OF PREFERENCE PAIRS CATEGORY DISTRIBUTION\nWe define $\u03bb$ as the proportion of the self-instruct dataset used to align $M_{Anchor}$, which corresponds to preference pairs selected under the consistently-incorrect or variable strategies (see Section 3.3)."}, {"title": "5 RELATED WORK", "content": "LLM-as-Evaluator: This concept refers to the ability of large language models (LLMs) to evaluate the outputs of other LLMs, a technique commonly referred to as LLM-as-a-Judge. This approach has gained considerable traction in recent years (Dubois et al., 2023; Li et al., 2024; Fernandes et al., 2023; Bai et al., 2023) and is frequently used to assess LLM performance across various downstream tasks (Zheng et al., 2023). It has proven particularly effective in automating evaluations, as demon- strated on platforms like LMSys Chatbot Arena. Key implementations include direct scoring based on specific criteria, pairwise comparisons (Liu et al., 2024), reference-based evaluations, and ensem- ble methods (Verga et al., 2024). While LLM-as-a-Judge offers scalability and consistency, it can also inherit biases from the evaluation model, potentially amplifying problematic outputs (Huang et al., 2024). Despite these challenges, it remains a valuable tool due to its efficiency and cost- effectiveness in evaluating LLM systems. In our work, we introduce a framework for the qualitative assessment of self-explanations using the LLM-as-a-Judge technique, designed to evaluate how ef- fectively a model conveys its reasoning.\nSelf-Alignment: Several approaches have been developed to improve LLMs without requiring human-annotated feedback. One method involves fine-tuning models using high-quality, self- generated input-output pairs (Wang et al., 2023; Chen et al., 2023; Gulcehre et al., 2023), though this can perpetuate biases in example selection without a clear mechanism for improving selection quality. Another influential approach is Constitutional AI (Bai et al., 2022), where an LLM pro- vides feedback and refines responses, which are then used to train a separate, static reward model. Building on this concept, Yuan et al. (2024) and Wu et al. (2024) proposed using the LLM itself as a dynamic reward model, eliminating the need for a static one. This allows for continuous im- provement in both generation and evaluation capabilities through iterative training processes. In our work, we introduce a novel method for creating a self-instruct dataset. Our approach, called Alignment with Anchor Preference Pairs, enhances preference pair selection by categorizing model behavior in response to each input prompt and applying tailored strategies for each category. To eval- uate a model's consistency for a given input prompt, an anchor-i.e., a ground-truth reference-is required, which we derive from a classification task used as a probing mechanism."}, {"title": "LLM-as-a-Debater", "content": "This adversarial approach aims to improve model performance through argu- mentation. In Perez et al. (2019), debaters are limited to extracting relevant statements from a source text, rather than generating original arguments. Du et al. (2023) extended this concept by involving multiple LLM instances to debate their individual responses over several rounds, eventually converg- ing on a shared final answer. Khan et al. (2024) further developed this approach by using debate-like scenarios to challenge and refine model outputs through simulated arguments. In our work, we adopt the LLM-as-a-Debater approach in the role of a consultant, specifically following Khan et al. (2024), for cases where the model's response to certain input prompts is consistently incorrect. This strategy enables the creation of self-instruct examples that avoid reinforcing problematic behavior."}, {"title": "6 LIMITATIONS", "content": "We acknowledge some limitations in our approach. First, evaluating the model's consistency on a given input prompt requires a anchor-ground truth reference. Consequently, the selection of preference pairs via the anchor strategy relies on a classification task as the probing mechanism, which restricts its applicability. Second, when ranking the quality of self-explanations, we assign equal weights across all evaluation dimensions. This uniform weighting may not accurately reflect the varying significance of different aspects of explanation quality, which can differ depending on the user or specific application. Moreover, this approach may overlook instances where individual explanations degrade in separate criteria, potentially leading to preference pairs where score differ- ences arise from unrelated factors.\nFinally, using the base model as the judge during the creation of the self-instruct dataset eliminates the need for a more capable model but introduces a static evaluation process. As the model improves, the judge may fail to capture important evaluation nuances. Iteratively enhancing the judge's capa- bilities, similar to the approaches in Yuan et al. (2024) and Wu et al. (2024), could help mitigate this issue."}, {"title": "7 CONCLUSION", "content": "In this work, we introduced a methodology for alignment that enhances LLMs' ability to generate high-quality self-explanations, even in the absence of annotated rationale explanations. Our ap- proach provides an end-to-end solution for aligning LLMs on classification tasks, ensuring that they not only produce accurate predictions but also articulate coherent explanations for their decisions. This is achieved through three core components: evaluating the quality of generated explanations, creating self-instruct datasets, and aligning the model. Central to our approach is Alignment with Anchor Preference Pairs, a novel method that refines preference pair selection by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. For each cate- gory, we apply tailored strategies to construct preference pairs, which are then used in DPO. Our empirical results demonstrate that this method consistently improves explanation quality, reducing the degradation caused by task specialization. Furthermore, we show that using anchor preference pairs outperforms self-alignment methods that rely solely on judge-based evaluations for preference pair selection."}]}