{"title": "SOLVING A RUBIK'S CUBE USING ITS LOCAL GRAPH STRUCTURE", "authors": ["Shunyu Yao", "Mitchy Lee"], "abstract": "The Rubik's Cube is a 3 \u00d7 3 \u00d7 3 single-player combination puzzle attracting attention in the reinforce-\nment learning community. A Rubik's Cube has six faces and twelve possible actions, leading to a\nsmall and unconstrained action space and a very large state space with only one goal state. Modeling\nsuch a large state space and storing the information of each state requires exceptional computational\nresources, which makes it challenging to find the shortest solution to a scrambled Rubik's cube with\nlimited resources. The Rubik's Cube can be represented as a graph, where states of the cube are nodes\nand actions are edges. Drawing on graph convolutional networks, we design a new heuristic, weighted\nconvolutional distance, for A* search algorithm to find the solution to a scrambled Rubik's Cube.\nThis heuristic utilizes the information of neighboring nodes and convolves them with attention-like\nweights, which creates a deeper search for the shortest path to the solved state.", "sections": [{"title": "1 Introduction", "content": "The Rubik's Cube is a 3-dimensional single-player combination puzzle attracting attention in the reinforcement learning\ncommunity. A 3 \u00d7 3 \u00d7 3 Rubik's Cube has six faces and twelve possible actions. These features lead to a small and\nunconstrained action space and a very large state space (around 4.325 \u00d7 10^{19} states) with only one goal state. Modeling\nsuch a large state space and storing the information for each state requires exceptional computational resources. In this\nsituation, it is challenging to find the shortest solution to a scrambled Rubik's Cube with limited resources.\nPrevious research in the cube-solving community has developed methods to overcome this problem. One such method,\nDeepCubeA, is a high performance Rubik's Cube solver developed by Agostinelli et al. that utilizes the A* search\nalgorithm and neural networks to find a shortest path to the solved state [1, 2]. DeepCubeA trains a neural network\nusing distance as the heuristic of the search algorithm, and yields good results on finding the shortest path to the solved\ncube with limited computational resources. The success of DeepCubeA indicates that using neural networks to represent\ncertain properties of cube states is efficient for such a large state space.\nThe Rubik's Cube can be represented as a graph, where states of the cube are nodes and actions are edges [3]. This\nrepresentation can provide us with some insights from graph theory. Graph convolutional networks (GCNs) are a\npowerful convolution method for handling data by using graph structures. GCNs have a mechanism called message\npassing, which can pass information contained in nodes and edges to adjacent nodes, thus making use of the graph\nstructure of the data [4, 5, 6]. Inspired by this mechanism, we propose adopting a similar method to convolve states\nwith their distance from the solved state in order to find a solution. However, since the complete representation graph\nof the Rubik's Cube has such a large number of nodes, it is difficult to store the whole structure. Due to limited\ncomputational power and resources, considering only the local structure of each node is a more reasonable way to\nutilize the information stored in the graph.\nIn this work, we propose using the A* search algorithm to find the solution to a scrambled cube with a new heuristic:\nweighted convolutional distance. We use the following formula to compute the weighted convolutional distance:\n$d^{(k+1)}(s) = \\mu d^{(k)} (s) + (1-\\mu) f_p(s)d^{(k)}_{adj} (s)$\nwhere\n$f_p(s) = (P_{SR},P_{Sr}, P_{SL}, P_{Sl}, P_{SU}, P_{Su},P_{SD},P_{Sd},P_{SF},P_{Sf},P_{SB},P_{Sb})^T$"}, {"title": "2 Basic Concepts of the Rubik's Cube", "content": null}, {"title": "2.1 Notations of the Rubik's Cube", "content": "The Rubik's Cube has six faces. In this paper, we will use the following notations for faces:"}, {"title": "2.2 Large State Space of the Rubik's Cube", "content": "The Rubik's Cube has 26 pieces. These pieces can be classified into three classes: fixed pieces, corner pieces, and edge\npieces. Fixed pieces (or center pieces) are the central pieces of each face, which have no influence on the states of the\nRubik's Cube. Corner pieces are the eight pieces at the corners of the cube, each with three face colors. Edge pieces are\nthe twelve pieces between two corner pieces, each with two face colors.\nThere are eight corner pieces, so there are 8! possible arrangements of the corner pieces. Additionally, corner pieces\nhave three face colors, so there are three different orientations of each corner. Thus, there are $3^8$ possible orientations of\nthe corner pieces. Although there are three possible orientations for each corner piece, only one of those orientations is\ncorrect. So, we say that $\\frac{1}{3}$ of the orientations of the corner pieces are correct. Therefore, there are $8! \u00d7 3^8 \u00d7 \\frac{1}{3}$ valid\narrangements of the corner pieces.\nThere are twelve edge pieces, so there are 12! possible arrangements of the edge pieces. Additionally, edge pieces\nhave two face colors, so there are two different orientations of each edge. Thus, there are $2^{12}$ possible orientations of\nthe edge pieces. Although there are two possible orientations for each edge piece, only one of those orientations is\ncorrect. So, we say that $\\frac{1}{2}$ of the orientations of the edge pieces are correct. Therefore, there are $12! \u00d7 2^{12} \u00d7 \\frac{1}{2}$ valid\narrangements of the edge pieces.\nDue to the construction of the Rubik's Cube, every permutation of the cube is even. So, $\\frac{1}{2}$ of all of the possible\npermutations of the Rubik's Cube, $\\frac{1}{2}$ of them are valid [10].\nUsing the above three calculations, we conclude that the total number N of valid permutations of the Rubik's Cube, i.e.,\nthe total number of states in the state space, is:\n$N = \\frac{8! \\times 3^8}{3} \\times \\frac{12! \\times 2^{12}}{2} \\times \\frac{1}{2}$\n$N = \\frac{43252003274489860000}$ approximately 4.325 \u00d7 10^{19}."}, {"title": "3 Graph Convolution on the Rubik's Cube", "content": null}, {"title": "3.1 Graph Representation of the Rubik's Cube", "content": "The state space of the Rubik's Cube can be represented as a graph, where nodes are states of the cube and edges are\nactions [3]. The representation graph of the state space is a directed graph. In this paper, we focus on the distance of a\nnode from the solved state. Since our target property is not dependent on the direction of edges, we only consider an\nundirected representation graph of the state space."}, {"title": "3.2 Representation of Nodes' Property", "content": "The Rubik's Cube has a very large state space, and thus has a very large representation graph. Storing every possible\nstate of the cube and computing all of their distances from the solved state requires exceptional computational power\nand resources. One way to overcome this problem is to use neural networks to represent the value of the target property\n[1, 2]. We propose using two neural networks to solve the Rubik's Cube: one to compute the distance from the current\nstate to the solved state and one to predict the next optimal move to make.\nDistance is a crucial heuristic for searching for the shortest solution to the Rubik's Cube [2, 8, 9]. In this paper, we\ndefine the distance of a state of the cube as the smallest number of moves required to reach the solved state. We\nutilize the pre-trained model from the solver DeepCubeA\u00b9 as our distance representation [2] and denote it as $f_a$. We\nalso consider the next action that should be taken at each state. The probability of each possible action offers natural\nattention-like weights for convolving the distance. To obtain the probability of taking each action, we train a multilayer\nperceptron $f_p$\u00b2 with states s as input and a 12-dimensional probability vector p as output [11, 12]. The output vector is\nof the form\n$f_p(s) = (P_{SR},P_{Sr}, P_{SL}, P_{Sl}, P_{SU}, P_{Su},P_{SD},P_{Sd},P_{SF},P_{Sf},P_{SB},P_{Sb})^T$\nwhere $P_{SA}$ denotes the probability of taking action A at the input state s."}, {"title": "3.3 Weighted Convolutional Distance", "content": "Graph convolutional networks are a powerful method to make predictions about the properties of graphs. They utilize\nthe information stored in neighboring nodes and edges to make these predictions [4, 13, 14]. These predictions are\nmade through a mechanism called message passing. Message passing allows the information i stored in a node to be\npassed to its neighboring nodes, along with a weight w, so that neighboring nodes can use i and w to update their own\ninformation accordingly [5, 6]."}, {"title": "4 Path Search Algorithm", "content": null}, {"title": "4.1 A* Search Algorithm for the Rubik's Cube", "content": "A* search algorithm is a popular heuristic for path finding in a graph. It classifies the nodes into two classes: closed\nnodes and frontier nodes. Closed nodes are nodes that have already been explored by the algorithm. They are stored in"}, {"title": "5 Result", "content": null}, {"title": "5.1 Performance", "content": "We test the performance of a 1-layer weighted convolutional distance (WCD) and a 2-layer WCD, and compare them\nwith the heuristic used in DeepCubeA [2]. Higher layer WCDs are not included, as when the number of layers k is at"}, {"title": "5.2 Analysis", "content": "The test result shows that our weighted convolutional distance heuristic with more convolution layers is more precise\nin finding a solution of the Rubik's Cube. As the number of convolution layers increases, the average length of the\nsolution and the number of searched nodes decreases, speaking to the efficacy of this method for finding a solution to\nthe Rubik's Cube. This will decrease the memory usage required by the searching algorithm while allowing us to solve\ncubes that are further from the solved state. However, the average processing time significantly increases for deeper\nconvolution. This is because that we cannot handle the convolution process in matrix form and take advantage of GPUs.\nThe searching efficiency will be significantly enhanced after converting to matrix form and applying fast convolution\ntechniques in GCNs [4, 13, 14]."}, {"title": "6 Conclusion", "content": "Weighted convolutional distance can give a more precise search direction than DeepCubeA for A* search algorithm\nby convolving the distance representation with the action probability as an attention-like weight. It can find a shorter\nsolution of a scrambled Rubik's Cube while searching significantly fewer nodes. This will decrease the memory usage\nwhile allowing us to solve cubes that are further from the solved state.\nHowever, our algorithm is not very efficient, due to the inability to convert the convolution process into matrix form and\napply fast convolution techniques in GCNs [4, 13, 14]. Moreover, weighted convolutional distance can be generalized\nto similar types of combinatorial puzzles with graph-like state spaces, such as n-dimensional Rubik's Cubes, Sokoban,\nLights Out, and sliding tile puzzles. Our future work will focus on applying fast convolution techniques to the weighted\nconvolutional distance and generalizing this heuristic to other combinatorial puzzles."}]}