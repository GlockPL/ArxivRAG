{"title": "We're Different, We're the Same: Creative Homogeneity Across LLMs", "authors": ["EMILY WENGER", "YOED KENETT"], "abstract": "Numerous powerful large language models (LLMs) are now available for use as writing support tools, idea generators, and beyond. Although these LLMs are marketed as helpful creative assistants, several works have shown that using an LLM as a creative partner results in a narrower set of creative outputs. However, these studies only consider the effects of interacting with a single LLM, begging the question of whether such narrowed creativity stems from using a particular LLM-which arguably has a limited range of outputs-or from using LLMs in general as creative assistants. To study this question, we elicit creative responses from humans and a broad set of LLMs using standardized creativity tests and compare the population-level diversity of responses. We find that LLM responses are much more similar to other LLM responses than human responses are to each other, even after controlling for response structure and other key variables. This finding of significant homogeneity in creative outputs across the LLMs we evaluate adds a new dimension to the ongoing conversation about creativity and LLMs. If today's LLMs behave similarly, using them as a creative partners-regardless of the model used-may drive all users towards a limited set of \"creative\" outputs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have moved out of research labs and into our everyday lives. Given their advanced abilities to generate text and respond to prompts, LLMs are often marketed as creativity support tools that allow users to write drafts, edit documents, and generate novel ideas with ease [2, 4, 20, 21]. Consumers have responded eagerly to these suggestions. According to a 2024 survey by Adobe, over half of Americans have used generative AI tools like LLMs as creative partners for brainstorming, drafting written content, creating images, or writing code. An overwhelming majority of LLM users surveyed believe these models will help them be more creative [39].\nWhile appealing, outsourcing our creative thinking to LLMs could have unintended consequences and demands further scrutiny. For example, recent work has unearthed complications around the use of LLMs as creativity support tools. Researchers found that LLM-aided creative outputs look individually creative but are often quite similar to other LLM-aided outputs. Such \"homogeneity\" in LLM-aided creative outputs has been observed in a variety of settings, from creative writing to online survey responses to research idea generation and beyond [7, 16, 37, 43, 53].\nWhile concerning, these works typically only look at a single LLM and it's effect on downstream creative content. In a prototypical example, Doshi and Hauser [16] compared the individual and collective creativity of two groups of writers-humans alone and humans aided by ChatGPT-and found that stories produced by the ChatGPT-aided group were more homogeneous. Related work from Moon, Green, and Kushlev [37] compared college essays written by humans and GPT models and found that LLM-authored essays contributed fewer new ideas and were more homogeneous than human-authored essays. However, such work begs the question: does the observed homogeneity occur because only a single type of LLM (GPT variants) is studied? It could be reasonably argued that a single LLM must have a limited range of outputs, causing the homogeneity. Perhaps if writers all used different LLMs, creativity would be restored.\nRecent work studying feature space alignment in LLMs suggests otherwise. There is a long line of work measuring feature space similarity in machine learning models, since this is believed to indicate overall model similarity [8, 31, 33,\n34, 46]. Some initial work has applied these techniques to large-scale LLMs and found evidence of \"feature universality\" in these models [26, 29, 32]. We postulate that such feature space alignment in LLMs may result in homogeneous"}, {"title": "2 RELATED WORK", "content": "Creativity, Homogeneity, and LLMs. Prior work has explored issues of creativity and homogeneity related to specific LLMs. Several works have compared human and LLM performance on standard creativity tests, typically using GPT models, and found that LLMs often outperform humans on these tests [12, 25, 45]. Despite LLMs' displays of individual creativity, numerous studies have shown that using LLMs to support creative tasks tends to homogenize creative outputs. For example, Doshi and Hauser [16] found that writers who used GPT-4 as a creativity support tool produced more creative stories than humans working alone, but the stories from writers who collaborated with GPT-4 were more similar to each other than were stories from human writers. This phenomenon of LLM-drive content homogenization appears across domains-in research idea generation [43], essay writing [37], survey responses [53], creative ideation [7], and art [55]. Recent work also showed that when GPT models are evaluated multiple times on creativity tests like the DAT, their responses tend to overlap, even if each individual response achieves a high \"creativity\" score [14]. Such findings further motivate our study of whether it is the use of specific models in these studies-often ChatGPT-that causes observed homogeneity, or if such homogeneity would be observed regardless of the model used.\nFinally, a few works have considered issues of monoculture related to machine learning algorithms. Several works demonstrate suboptimal outcomes when multiple firms employ the same algorithm for decision-making [11, 30]. [52] proposed the term \"generative monoculture\" to describe the narrow distribution of LLM outputs relative to that of their"}, {"title": "3 METHODOLOGY", "content": "Our goal is to measure whether LLMs produce more, less, or equally diverse creative outputs as a group of humans. We measure this diversity (or variability) in responses by computing the semantic similarity among responses of humans and LLMs to prompts designed to elicit creativity. This section describes the creativity prompts we use, humans and LLMs tested, and evaluation metrics."}, {"title": "3.1 How do we elicit creative responses from LLMs?", "content": "The American Psychological Association defines creativity as \"the ability to produce or develop original work, theories, techniques, or thoughts\" [1]. Since our goal is to compare the diversity of creative responses from LLMs and humans, we sought out methods to elicit and compare creative outputs. Given the novelty of this field, no standard benchmarks exist for comparing LLM and human creativity. However, prior work has applied tests of divergent thinking in humans, which elicit qualities psychologists view as important to creativity, to LLMs and found that LLMs like ChatGPT scored similarly to humans [12, 25, 45, 54].\nCreativity tests for humans. One of the original divergent thinking tests was Guilford's Alternative Uses Test (AUT) [23], which presents subjects with an object and asks them to describe creative uses for it. AUT responses are scored by measuring the number of different uses presented (\u201cfluency\"), the originality of those ideas (\u201coriginality\"), how different they are from each other (\"flexibility\"), and the level of detail provided (\u201celaboration\"). While effective, the AUT evaluation process is onerous, so researchers have developed more lightweight divergent thinking tests in recent years. One popular test, Forward Flow [22] (FF), measures the divergence of a user's chain of thought from a fixed starting point. Another, the Divergent Association Test (DAT) [38], asks subjects to list 10 unrelated words. Both capture similar characteristics to the AUT but with less burden on participants and evaluators.\nShould we run human creativity tests on LLMs? Given our goals, it seems reasonable to test humans and LLMs using the AUT, FF, and DAT and then compare the population-level variability of their responses. However, it is an active"}, {"title": "3.2 Tests We Use", "content": "Based on the reasoning of \u00a73.1, we compare the variability of human and LLM responses to the AUT, FF, and DAT tests. Exact test wording is in Appendix A.\nGuilford's Alternative Uses Test (AUT) [23] presents people with an object and asks them to write down as many creative uses for it as they can think of. Following established best practices [9, 18], we test users with five common objects-book, fork, table, hammer, and pants. Using multiple starting objects reduces the effect of a particular object (e.g. book) on participant responses, ensuring results generalize [19]. It also allows us to collect more data, given the limited number of LLMs we can evaluate relative to the number of possible human subjects.\nForward Flow [22] measures how much a person's thoughts diverge from a given starting point. It provides a starting word and asks people to write down the next word that follows in their mind from the previous word for up to 20 words. We follow the original Forward Flow paper and run our study using five different start words: candle, table, bear, snow, and toaster. As in the AUT, providing multiple creative stimuli ensures results generalize and gives us more data.\nThe Divergent Association Task (DAT) [38] asks subjects to list 10 words that are as unrelated as possible. These are subject to certain constraints: only nouns, no proper nouns, only single words in English, and the task must be completed in less than four minutes. The DAT provides a limited amount of information compared to the other tests, since the creative stimulus cannot be varied."}, {"title": "3.3 Test subjects", "content": "We administer these tests to a set of LLMs and a set of humans, following IRB-approved user study protocol.\nLarge Language Models. As a baseline, we test 22 large language models with public APIs\u00b9: AI21-Jamba-Instruct, Cohere Command R, Cohere Command R Plus, Meta Llama 3 70B Instruct, Meta Llama 3 8B Instruct, Meta Llama 3.1 405B Instruct, Meta Llama 31 70B Instruct, Meta Llama 3.1 8B Instruct, Mistral large, Mistral large 2407, Mistral Nemo, Mistral"}, {"title": "3.4 Evaluation Metrics", "content": "The primary goal of this study is to evaluate the variability of LLM responses to creative prompts relative to that of humans. To do this, we compute the semantic similarity of responses in different populations (LLMs vs. humans) and compute distributional differences in similarity scores between populations. As a baseline, we also compare the originality of individual LLM responses to the tests relative to that of humans."}, {"title": "3.4.1 Scoring individual originality.", "content": "Although divergent thinking tests can be measured using multiple metrics, it has long been argued the originality of responses is the strongest indicator of creativity [36]. Originality-how novel tests responses are relative to the given prompt(s)-can also easily be measured in an automated fashion by embedding prompts and responses in a mathematical feature space and measuring the cosine distance between the feature vectors [10]. Prior work confirms that such automated analysis closely matches originality rankings of human scorers [19].\nOur metrics for individual originality follow the guidelines of the original studies but use the automated evaluation methods of [19], including the use of the GloVe 840B model [41] to compute word embeddings. The format of each test necessitates different originality scoring procedures, described in detail in Appendix \u00a7B. Originality scores are denoted as $O_t(P)$, where t = AUT, FF, or DAT and P is a population, either humans or LLMs.\nDistributional differences. After computing originality scores, we can then compare the distributions of $O_t(LLM)$ and $O_t(Humans)$ to measure differences in originality between the two groups. We do this by testing for statistically significant differences in $\u00b5(O_t(LLM))$ and $\u03bc(O_t(Human))$ using Welch's t-test to compare differences in means, since the populations typically do not exhibit equal variance. We use a statistical significance threshold of p = 0.01. For all tests, the null hypothesis is that $\u00b5(O_t(LLM)) = \u00b5(O_t(Human))$, and the alternative is that $\u00b5(O_t(LLM)) > \u00b5(O_t(Human))$."}, {"title": "3.4.2 Scoring population-level variability.", "content": "We measure the variability in responses to the creativity tests from a given population by computing the semantic distances between sets of responses from individuals in the population (e.g. comparing the set of AUT uses produced by an LLM to that of another LLM). If many population members given semantically similar sets of answers, this indicates that the response variability of the population is low, and vice versa if it is high. We denote the variability of a population P on test t as $V_t(P)$, the set of all similarity scores between all responses from all population members. As before, P refers to either LLMs or humans.\nWe use a sentence embedding model S to measure semantic similarity between responses. Sentence embedding models map sentences or short paragraphs to feature vectors and, similar to the word embedding model, map similar content to similar feature vectors. We compute elements of $V_t(P)$ by representing an individual's responses to a certain test condition (e.g. all their AUT responses to a certain prompt) as a single, space-separated word string R and embedding this into a mathematical space via S, producing S(R). We then take the cosine similarity between this vector and those of other population members to form $V_t(P)$:\n$V_t(P) = {1 - cos(S(R), S(R)), \\forall (R^i, R^j), p_{ij} \\in P}$                                                       (1)\nwhere $R^i, R^j$ denote the responses of two different population members to prompt p. In our experiments, we use all-MiniLM-L6-v2 from the sentence_transformers Python library [42], a high-performing and widely used model, to compute sentence embeddings. We remove punctuation and stopwords from responses before computing embeddings.\n$V_t(P)$ is composed of cosine distance scores, so if it skews towards 0, responses in the population are similar to each other. If it skews towards 1, they are more different, and therefore the population exhibits higher variability. Note that $V_t(P)$ only contains similarity scores of responses from different LLM/human subjects."}, {"title": "4 KEY RESULTS", "content": "When reporting results of statistical t-tests, we use the standard APA format, reporting the degrees of freedom (DOF), test statistic X, and significance level y: t(DOF) = X, p = y. For context, we also report the effect size, which is the difference between the means of the two populations divided by their pooled standard deviation. Cohen [13] defines small, medium, and large effect sizes as 0.2, 0.5, and 0.8, respectively. Finally, we report test power, which is the probability of correctly rejecting the null hypothesis (or 1 minus the probability of a false negative)."}, {"title": "4.1 Baseline measurement: individual originality in LLMs vs. humans", "content": "LLMs score slightly higher than humans on the AUT and DAT tasks, mirroring prior work [25], but perform worse on FF. Overall, these results show that LLMs and humans exhibit roughly equal levels of measured originality on these tests on average, removing this as a possible confounding variable in our study of response variability."}, {"title": "4.2 Population-level Response Variability-LLMs vs. Humans", "content": "Now, we explore the main question: whether LLMs and humans exhibit different population-level variability in creative outputs. For statistical analysis and $V_t$ distribution plots in this setting, we only consider responses from 7 distinct LLMs: AI21 Jamba 1.5 Large, Google Gemini 1.5, Cohere Command R Plus, Meta Llama 3 70B Instruct, Mistral Large, gpt 40, and Phi 3 medium 128k Instruct, a subset of our original 22 models. As discussed previously, this choice removes model family as a possible confounding variable in our analysis.\nOur key finding is that LLM responses exhibit much less variability, as measured by semantic distance between pairs of embedded responses, than do human responses. Both these views of the data confirm that LLM test responses are much more similar to each other than human responses are to each other. From this, we conclude that a population of LLMs produces more homogeneous outputs in response to creative prompts than does a population of humans.\nVisualizing embedded responses. To further understand the overlap in LLM responses as compared to humans, we visualize the sentence embeddings of AUT responses in Visualizations for FF and DAT confirms the behavior observed statistically: LLM responses \"cluster\" together in the embedded feature space, providing further evidence of low LLM response variability.\nOne explanation: word overlap in LLM responses. The low response variability of LLMs can be partially explained through analysis of lexical patterns in LLM and human responses. We remove stopwords from responses, then count the number of word overlaps between sets of responses from LLMs and humans-all AUT uses from a human/LLM, all words in a FF response, etc. As Figure 4 shows, LLM responses tend to have many more words in common than human responses, across all tests. This overlap at least partially accounts for the high semantic similarity between LLM responses, as the sentence embedding model will map responses with overlapping words to similar feature vectors.\nFurther exploration of differences in lexical patterns between LLMs and humans is important future work."}, {"title": "5 ADDITIONAL ANALYSIS", "content": "Having established that LLMs produce more homogeneous creative outputs than humans, we now explore several additional dimensions of this key finding. First, we demonstrate that this cross-LLM response homogeneity remains even after strictly controlling for structural differences in human and LLM responses. Next, we measure if homogeneity increases when LLMs all come from the same \"family.\" Then, we explore a possible mechanism to counteract LLM creative homogeneity through the use of creative system prompts. Finally, we confirm that our human user study results are similar to prior results, ensuring that the choice to conduct our survey online does not skew results. Throughout this section, we consider only responses to the AUT to avoid a combinatorial explosion of experiments."}, {"title": "5.1 Controlling for AUT Response Structure", "content": "For both the DAT and FF tests, the response structure is fixed, making comparison of population-level variability straightforward. However, the AUT is more open-ended, so confounding variables such as differences in response structure (e.g. number of words, tense, etc.) between LLMs and humans may impact measurements of response variability."}, {"title": "5.2 Creativity within LLM \"families\"", "content": "Next, we inspect whether models in the same \"family\" produce more homogenous responses than a baseline set of otherwise unrelated models. To do this, we measure the population-level variability of AUT responses from Llama model family: Meta Llama 3 70B Instruct, Meta Llama 3 8B Instruct, Meta Llama 3.1 405B Instruct, Meta Llama 31 70B"}, {"title": "5.3 Effect of LLM system prompt.", "content": "Next, we consider ways to make LLMs produce more variable outputs. As a baseline, we explore whether varying the LLM system prompt to strictly request creative outputs will induce higher variability. We experiment with prompts designed to elicit different levels of creativity:\n\u2022 Baseline: \"You are a helpful assistant.\"\n\u2022 More creative: \"You are a creative assistant that always provides answers that demonstrate imaginative, outside-the-box thinking.\"\n\u2022 Very creative: \"You are a creative assistant that always provides answers that demonstrate imaginative, outside-the-box thinking. You are about to take a creativity assessment, and your answers should be as novel, original, and bold as possible. If you receive the highest score on this creativity assessment, you will receive $200.\"\n\u2022 Not creative: \"You are a robot assistant that always provides answers that are unoriginal, bland, and soulless. You are about to take a creativity assessment, and your answers should be as generic and unoriginal as possible.\""}, {"title": "5.4 Validation with preexisting survey data", "content": "Finally, we compare responses in our user study to prior user studies to ensure that our human subject pool is reliable and not unduly skewed by possible use of LLMs. We test both the individual originality of our human responses and population-level variability and find that while respondents in prior studies score better individually on the tests, respondents to our study exhibit equal or greater population-level variability (the more important metric for our study) on the more-informative AUT and FF tests."}, {"title": "6 DISCUSSION", "content": "Motivated by measured homogeneity in creative outputs produced by specific LLMs and observed feature space overlap in LLMs, we study whether responses to creative prompts produced by a group of LLMs exhibit more, less, or equal variance as a set of human responses to the same creative prompts. We find that LLMs exhibit much lower population-level output variability than humans, even after controlling for potential model similarities and structural differences between LLM and human responses. Our work upholds prior work showing that LLMs perform well on tests of divergent thinking but adds the nuance that such performance is homogeneous-LLMs return a narrower range of responses to creative prompts than humans. This result enhances prior observations of LLM-induced homogeneity, which only considered the effect of specific LLMs on creative outputs, and suggests that the use of LLMs in general may homogenize creative outputs.\nImplications. These results have significant implications if LLMs are widely adopted as creativity support tools for writing, idea generation, or similar tasks. If all LLMs respond similarly to specific creative requests, then the population of users leveraging to LLMs to aid creativity will converge towards a limited set of creative outputs. In other words, LLM users may be self-limited from being exhibiting the divergent creativity that defined well-recognized artistic geniuses like Tolkein, Mozart, and Picasso because their LLM \"creative\" partners may collectively drive them towards a mean.\nLimitations. Our work has several limitations. First, while we have demonstrated LLM homogeneity in response to certain creativity tests, this does not prove that LLMs in general produce homogeneous outputs when asked to behave creatively. It merely provides a indication that future work should explore this subject. Additionally, we measure a single metric of divergent thinking or creativity-originality, as measured by semantic similarity between responses-and finds that LLms are homogeneous along this dimension. However, there are other well-known metrics of divergent thinking, such as flexibility, fluency, and elaboration (see \u00a73.1), and LLMs may demonstrate more or less homogeneity along these dimensions. Future work should consider these alternatives."}, {"title": "7 ETHICAL CONSIDERATIONS", "content": "We took care to ensure the user study in this paper was conducted in accordance with ethical standards. IRB approval for the study was obtained, and participants signed a clearly written consent form before completing our survey. To ensure privacy, participant data was anonymized and stored on secure servers. Other ethical risks from this paper are minimal, as our LLM experiments do not involve sensitive data and elicit only benign model responses."}, {"title": "A DIVERGENT THINKING TEST WORDING", "content": "Here, we report the exact wording for the tests given to humans and LLMs. The wording differs slightly between the two groups because the LLM models are prompted to output their work in a particular format for easier processing, while human prompts refer to text boxes in the survey UI. Without formatting instructions in the prompt, LLMs often discussed the reasoning behind their word choices. While mildly interesting, this muddied the data."}, {"title": "A.1 AUT prompts.", "content": "For original experiments, we use the following start words for AUT: WORD = {book, fork, table, hammer, pants}. For the expanded LLM evaluation of \u00a75.2, we use WORD = {book, bottle, brick, fork, hammer, pants, shoe, shovel, table, tire}.\nHuman prompt. Imagine that someone gives you WORD. In the blanks below, write down as many creative uses you can think of for this object, up to 10 uses."}, {"title": "A.2 Forward Flow prompts.", "content": "We use the following start words for Forward Flow: WORD = {candle, table, bear, snow, toaster}.\nHuman prompt. (From the original Flow paper) Starting with the word WORD, in each of the following blanks, write down the next word that follows in your mind from the previous word. Please put down only single words, and do not use proper nouns (such as names, brands, etc.). Start by writing WORD in the first space below.\nLLM prompt. Starting with the word WORD, your job is to write down the next word that follows in your mind from the previous word. Please put down only single words, and do not use proper nouns (such as names, brands, etc.). Stop after you listed at least 22 words. Print just the list of words, separated by commas, and do not add anything else to your response. The first word in the list should be 'candle'."}, {"title": "A.3 DAT Prompts.", "content": "Human prompt. (From the original DAT paper) In the spaces below, please enter 10 words that are as different from each other as possible, in all meanings and uses of the words. You must follow the following rules: 1. Only single words in English. 2. Only nouns (e.g., things, objects, concepts). 3. No proper nouns (e.g., no specific people or places). 4. No specialised vocabulary (e.g., no technical terms). 5. Think of the words on your own (e.g., do not just look at objects in your surroundings). 6. Complete this task in less than four minutes.\nLLM prompt. Instructions: Please enter 10 words that are as different from each other as possible, in all meanings and uses of the words. Rules: 1. Only single words in English. 2. Only nouns (e.g., things, objects, concepts). 3. No proper nouns (e.g., no specific people or places). 4. No specialised vocabulary (e.g., no technical terms). 5. Think of the words on your own (e.g., do not just look at objects in your surroundings). 6. Complete this task in less than four minutes. 7. Return just the list of words, separated by commas, and do not include any other content."}, {"title": "A.1 AUT prompts.", "content": "For original experiments, we use the following start words for AUT: WORD = {book, fork, table, hammer, pants}. For the expanded LLM evaluation of \u00a75.2, we use WORD = {book, bottle, brick, fork, hammer, pants, shoe, shovel, table, tire}.\nHuman prompt. Imagine that someone gives you WORD. In the blanks below, write down as many creative uses you can think of for this object, up to 10 uses."}, {"title": "A.2 Forward Flow prompts.", "content": "We use the following start words for Forward Flow: WORD = {candle, table, bear, snow, toaster}.\nHuman prompt. (From the original Flow paper) Starting with the word WORD, in each of the following blanks, write down the next word that follows in your mind from the previous word. Please put down only single words, and do not use proper nouns (such as names, brands, etc.). Start by writing WORD in the first space below.\nLLM prompt. Starting with the word WORD, your job is to write down the next word that follows in your mind from the previous word. Please put down only single words, and do not use proper nouns (such as names, brands, etc.). Stop after you listed at least 22 words. Print just the list of words, separated by commas, and do not add anything else to your response. The first word in the list should be 'candle'."}, {"title": "A.3 DAT Prompts.", "content": "Human prompt. (From the original DAT paper) In the spaces below, please enter 10 words that are as different from each other as possible, in all meanings and uses of the words. You must follow the following rules: 1. Only single words in English. 2. Only nouns (e.g., things, objects, concepts). 3. No proper nouns (e.g., no specific people or places). 4. No specialised vocabulary (e.g., no technical terms). 5. Think of the words on your own (e.g., do not just look at objects in your surroundings). 6. Complete this task in less than four minutes.\nLLM prompt. Instructions: Please enter 10 words that are as different from each other as possible, in all meanings and uses of the words. Rules: 1. Only single words in English. 2. Only nouns (e.g., things, objects, concepts). 3. No proper nouns (e.g., no specific people or places). 4. No specialised vocabulary (e.g., no technical terms). 5. Think of the words on your own (e.g., do not just look at objects in your surroundings). 6. Complete this task in less than four minutes. 7. Return just the list of words, separated by commas, and do not include any other content."}, {"title": "B ORIGINALITY SCORES FOR AUT, FF, AND DAT", "content": "Here, we describe our methods of computing originality scores for each test. Originality scores are denoted as $O_t(P)$, where t = AUT, FF, or DAT and P is a population, either humans or LLMs."}, {"title": "C TSNE OF FF AND DAT", "content": "Figure 10 visualizes the TSNE of sentence embeddings for DAT and FF responses, similar to Figure 3. This confirms the trend observed in the AUT TSNE: LLM responses cluster closer in feature space than human responses, resulting in lower population-level originality measurements. We perform k-means clustering of TNSE of FF sentence embeddings to demonstrate clusters of LLM response for each start word. Since the DAT since the test does not involve varying start words, we simply visualize all LLM and human responses."}]}