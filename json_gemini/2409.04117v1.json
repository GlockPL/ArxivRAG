{"title": "Confidence-Aware Document OCR Error Detection", "authors": ["Arthur Hemmer", "Micka\u00ebl Coustaty", "Nicola Bartolo", "Jean-Marc Ogier"], "abstract": "Optical Character Recognition (OCR) continues to face accuracy challenges that impact subsequent applications. To address these errors, we explore the utility of OCR confidence scores for enhancing post-OCR error detection. Our study involves analyzing the correlation between confidence scores and error rates across different OCR systems. We develop ConfBERT, a BERT-based model that incorporates OCR confidence scores into token embeddings and offers an optional pre-training phase for noise adjustment. Our experimental results demonstrate that integrating OCR confidence scores can enhance error detection capabilities. This work underscores the importance of OCR confidence scores in improving detection accuracy and reveals substantial disparities in performance between commercial and open-source OCR technologies.", "sections": [{"title": "Introduction", "content": "Optical Character Recognition (OCR) on scanned documents has significantly advanced due to developments in deep learning and computer vision. Despite these advancements, OCR errors persist and negatively impact performance on downstream NLP tasks, especially on low-quality documents such as historical documents [1,9,11,13,18], but also on information retrieval [31], event detection [5], named entity recognition [15,16], topic modeling [25] and others [40,42]. Some OCR errors are \"genuine\" errors that even humans would struggle with based solely on the visual information. Most OCR systems attempt to transcribe even illegible text, often resulting in gibberish. In contrast, humans utilize optical uncertainty and contextual cues to infer corrections or recognize text as unreadable. Recent advancements in OCR technology incorporate more contextual information through language models, though challenges remain, particularly with numerically dense documents [17], which comes from the lack of numeracy"}, {"title": "Related Work", "content": "The post-OCR processing framework typically adheres to the noisy channel model as introduced by Shannon [36]. This model attempts to recover the original sequence or \"word\" w from a noisy sequence of observed symbols o, with the goal to find w as\nw(o) = arg maxp(w|o).\nHistorically, directly estimating p(wo) is challenging due to limitations in data volume and training methods. Instead, the noisy channel model applies Bayesian inversion to decompose the probability into the likelihood and the prior, often referred to as the \"error model\":"}, {"title": "Data Acquisition", "content": "This section outlines the data acquisition process employed to assess the utility of OCR confidence scores in post-OCR processing. We test a variety of OCR systems, both open-source and commercial, and align their outputs with the ground-truth transcriptions from multiple public datasets as well as one private dataset. The alignment of these outputs presents significant challenges due to the diverse formats and results produced by different OCR technologies, which will be detailed further. Additionally, we present statistics related to the datasets obtained through this process."}, {"title": "OCR", "content": "Several OCR systems are evaluated on the datasets. We choose a mix of open-source and commercial, cloud-based OCRs for comparison: Microsoft Document"}, {"title": "OCR Statistics", "content": "Following the alignment and processing steps described above, we present various statistics for the different OCRs and datasets in table 2. In addition to the CER, we also include the Box Error Rate (BER), which is the percentage of connected components whose text is not equal to the GT text. We furthermore compute the Expected Calibration Error (ECE) [12], which is the average absolute deviation from the optimal calibration for each bin B:\nECE = \\sum_{i=1}^{m} \\frac{|B_i|}{n} |Y_i(B_i) - P_i(B_i)|,\nWhere each bin B\u2081 contains all predictions with confidence (i\u22121)/10 < Bi < i/10, yi(Bi) is the actual proportion of correct predictions in bin Bi and \u017fpi(Bi)"}, {"title": "Confidence-Aware Error Detection", "content": "To integrate OCR confidence scores into a post-OCR error detection model, we build on top of current state-of-the-art OCR error detection work using BERT [14,29] and make minimal modifications so that we would need to finetune the model as little as possible. BERT can be used for error detection by doing binary classification at the OCR box level, by labeling a box as erroneous if it contains an error."}, {"title": "Architecture", "content": "The confidence scores are integrated into the model by applying them directly to the initial token embeddings, much like positional encoding used in many transformer architectures. Given a list of tokens t = {t1, t2,..., tn} and an embedding function Emb: t \u2192 Rd where d is the hidden dimension of the model, the confidence-aware embedding ef for token ti is obtained using\ne_i = (1 - \\alpha)Emb(t_i) + \\alpha(1 \u2013 p_{ocr}(t_i)).\nWhere pocr is the OCR confidence in the box containing token ti, broadcast across Rd. The function Emb here refers to the standard BERT embedding function which converts a token id (integer) into a vector in Rd, before it is passed through the transformer part of the model.\nThe parameter a is a trainable parameter that controls how much the noise should be used in the model. As OCRs are calibrated differently (see Sec. 3.3), a was added to give the model more flexibility and potentially act as a knob to regulate the reliance on confidence scores or on the token embeddings. We investigate the impact of different values of a further in Sec. 4.4."}, {"title": "Additional Pre-training", "content": "In addition to integrating the confidence into the model, we also test an additional pre-training step to help the model learning how to integrate confidence scores. To do this, we continue pre-training a BERT model using the original Masked Language-Modeling (MLM) objective as well as a secondary binary noise prediction head. The pre-training uses data augmentation techniques where OCR noise is simulated on existing, non-noisy datasets.\nThe noise prediction head is used to predict whether a token was noised or not. We modify the MLM algorithm by sampling pocr ~ Beta(4, 1) and modify token ti for a random other token if pocr < Uniform(0, 1). The sampled pocr is then used in the model to represent the confidence in the token. We choose to sample following a Beta(4, 1) as it skews the samples towards high probability which corresponds most to distribution of OCR confidence scores. For the final loss objective, the MLM and binary noise prediction cross-entropy losses are summed together.\nWe continue pre-training the BERT on the original datasets BookCorpus [44] and English Wikipedia for an additional 2.5k steps using the AdamW optimizer with a linearly declining learning rate of 5e - 5."}, {"title": "Experiments", "content": "We evaluate the proposed methods with the data acquired and aligned as described in section 3. To compare with current state-of-the-art in error detection, we use an unmodified BERT model to classify boxes into \"error\" and \"no error\" as described in 4.\nThe BERT and ConfBERT models are trained for a maximum of 16 epochs on the training split for each dataset using an AdamW optimizer with a learning rate of 5e 5. Early stopping is used with a patience of 5 epochs and the checkpoint with the highest validation F\u2081-score is used for the final evaluation on the test set. Each training is repeated 10 times to measure the stability and significance of the obtained F\u2081-scores. We use the micro F\u2081-score and test statistical significance using the Kolmogorov-Smirnov test.\nA baseline is calculated by relying solely on the OCR confidences. It is computed by taking the percentiles of the confidences on the training set and then picking the threshold that results in the highest F\u2081 score on the validation set. The final score is evaluated on the test set for at the previously determined threshold. The results for the baseline and the other models can be found in Tab. 3.\nOverall we observe that the error detection F\u2081 scores are higher for the open-source datasets, although this can mostly be attributed to the higher BER (see Tab. 2) as this means lower class-imbalance and thus more positive-class training data, leading to an easier error detection overall. Due to these differences and the nature of the F\u2081 score, it is difficult to draw further conclusions from the F\u2081 scores between OCRs on the same dataset."}, {"title": "Impact of a", "content": "Although we set a to be a trainable parameter, the low learning rate and few training steps that the model goes through means the parameter can not change by much. As such, we investigate the impact of different values for a on the F\u2081 score. For this experiment, we fix a at a specific value in intervals of 0.1 and make it non-trainable. The measured metric is the relative improvement of the F\u2081 score with respect to a = 0. The results are shown in Fig. 2.\nAs observed in the results of the main experiment, integrating confidence scores generally tend to improve or at least maintain the F\u2081 base scores. However, we note that starting from a = 0.9, the model starts to be impacted negatively by the confidence scores as the information of the text itself \"disappears\".\nBesides these larger values of a, the relative improvement of the F\u2081 score (averaged over all OCRs) is positively correlated with the value of a, except for SROIE (p-value = 0.09 using Pearson correlation)."}, {"title": "Limitations", "content": "While this work contains various metrics of several OCR systems, it is not intended to serve as an overall OCR benchmark. As detailed in Sec. 3.2, the OCR systems were chosen according to ease-of-use, popularity and reported performance. but many other OCR methods could have been considered and might have shown better results. The intention of this work was to study the informativeness of confidence scores among a variety of OCR systems.\nSimilarly, although we have shown that proprietary OCR systems achieve better CER, an important benefit of open-source OCR systems is transparency and be able to adapt and tune a solution to specific needs. Simply changing the default parameters to better match the specific use-case can go a long way.\nAs for the confidence integration, we present a single way of integrating the confidence in the model. While we experimented briefly with some other ways, we found this to be performing the best. However, as the simple confidence-only baseline sometimes outperforms the other models, this suggests that there may be more effective ways to integrate this information into a model."}, {"title": "Conclusion", "content": "In this paper we investigated the possibility of using OCR confidences for improving OCR error detection. We created several datasets by running multiple commercial and open-source OCRs on three public and one private dataset, and computed several metrics such as the error rate and calibration error by using a two-stage alignment algorithm. For multiple metrics, we notice an important gap between the commercial and open-source solutions which, however, can be partly attributed to the different sizes of bounding boxes the different OCR systems produce."}]}