{"title": "Trust and Resilience in Federated Learning Through Smart Contracts Enabled Decentralized Systems", "authors": ["Lorenzo Cassano", "Jacopo D'Abramo", "Siraj Munir", "Stefano Ferretti"], "abstract": "In this paper, we present a study of a Federated Learning (FL) system, based on the use of decentralized architectures to ensure trust and increase reliability. The system is based on the idea that the FL collaborators upload the (ciphered) model parameters on the Inter-Planetary File System (IPFS) and interact with a dedicated smart contract to track their behavior. Thank to this smart contract, the phases of parameter updates are managed efficiently, thereby strengthening data security. We have carried out an experimental study that exploits two different methods of weight aggregation, i.e., a classic averaging scheme and a federated proximal aggregation. The results confirm the feasibility of the proposal.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) is a Machine Learning (ML) framework that enables multiple parties to collaboratively train a shared model without directly sharing their individual data [1]. FL holds the promise of enhancing data-driven learning models while safeguarding data owners' privacy. This makes it an appealing approach for developing ML models in several application domains. A prominent example is clinical diagnosis, where patient data privacy is critical, yet data aggregation from diverse sources is necessary. However, a significant challenge revolves around ensuring FL collaborators actively participate in the protocol while securely contributing their data. To this extent, recently several studies have explored the integration of blockchain technology within FL systems [2]-[6]. While existing research often emphasizes on participation incentives, traceability, and security aspects, there is an under-explored dimension that deserves attention, i.e., the impact of delays, number of collaborators, and collaborator failures on system performance and model training accuracy.\nIn our work, we investigate the performance of our FL system and the accuracy of ML training when one or more collaborators experience failures.\nWe propose a decentralized FL system that combines FL with two powerful components: the Inter-Planetary File System (IPFS) and smart contracts (executed over a permissioned Ethereum-like blockchain). Together, they create a tamper-proof storage mechanism for sharing encrypted model parameters. IPFS provides decentralized and fault-tolerant data storage, while smart contracts enforce rules and streamline interactions among participants [7]. The presence of a smart contract forces Collaborators to adhere to a protocol organized into distinct phases, ensuring orderly updates of model parameters. Additionally, the smart contract automates compliance checks, maintaining the integrity of the FL process [6].\nIn the paper, we present the overall decentralized system and we perform an experimental evaluation under varying failure scenarios among Collaborators. As concerns the FL model assessment, two weight update methods are used, FedAvg and FedProx [8]. We investigate the FL system's performance in terms of classification accuracy, using a dataset coming from the healthcare image classification domain, i.e., brain tumor image dataset. Furthermore, we measure the gas consumption of the smart contract, a critical aspect in blockchain-based FL systems. Finally, we also evaluate the performance of the data retrieval and update, by both FL Manager and Collaborators, from/to IPFS. Results confirm the viability of using decentralized systems in Federated Learning.\nThe remainder of this paper is organized as follows. Section II describes the proposed framework. Section III describes the methodology, while Section IV outlines the experimental results. Section V provides some concluding remarks."}, {"title": "II. SYSTEM ARCHITECTURE", "content": "Our system architecture comprises four key actors. The first two are the classic actors involved in a typical FL system, while the last two ones are those of typical decentralized systems [7]:\n\u2022 FL Manager: The Manager acts as the aggregator for the classification model parameters used in FL. It triggers requests, collects ML parameters from other Collaborators, and monitors parameter updates.\n\u2022 FL Collaborators: these are the nodes participating to FL model training. They retrieve data, locally train their models, and upload the trained parameters to the system.\n\u2022 Permissioned Blockchain: This blockchain executes a Federated Learning Smart Contract (FLSC). The FLSC autonomously regulates parameter exchanges among participants, ensuring adherence to the protocol. Additionally, the use of blockchain digests prevents tampering with parameters, enhancing security [9], [10].\n\u2022 Decentralized File Storage: The Inter-Planetary File System (IPFS) stores the ML model parameters."}, {"title": "A. System Interaction Overview", "content": "The diagram of the interactions among system components is shown in Figure 1. The protocol works as follows:\n1) The Manager deploys and initiates the Federated Learning Smart Contract (FLSC), inviting multiple Collaborators (for simplicity, only one is depicted in the figure).\n2) The Manager publishes the model details required for compilation within FLSC.\n3) Collaborators retrieve and adopt the model from the FL Smart Contract.\nFL Loop\n4) Each Collaborator trains its local model using its dataset. By keeping data decentralized, FL avoids the need for centralizing sensitive information. This brings positive aspects possibly related to enhanced privacy, data protection, regulatory compliance, and scalability.\n5) Periodically, Collaborators publish updated local parameters on IPFS and send their parameter hashes (along with retrieval information) to FLSC. Parameters are encrypted sequentially using the Collaborator's private key and the Manager's public key. This ensures confidentiality and verifiability \u2014 only the Manager can read the data, and their origin is guaranteed.\n6) The Manager decrypts these parameters using its private key and the Collaborator's public key. The Manager further validates parameters by comparing their digest to the hashed value stored in the smart contract.\n7) Based on an aggregation function, the Manager updates the ML model parameters, considering results obtained during training.\n8) The Manager securely uploads the updated ML model parameters to IPFS, using the same encryption approach as previously described. These parameters represent the refined knowledge gained during the training process.\n9) The Manager sends the digests (hashes) of these parameters to the smart contract. These digests serve as verifiable references for the stored parameters.\n10) Collaborators retrieve the updated model parameters from IPFS. They decrypt the parameters using their private keys. Collaborators then verify the validity of these parameters through the FL Smart Contract, following the same process as before. This validation ensures that the parameters have not been tampered with and align with the agreed-upon protocol. Once validated, Collaborators incorporate these updated model parameters into their local training during the subsequent iteration.\nIn our current implementation, Collaborators receive no rewards for parameter updates, assuming protocol compliance. However, our framework allows for alternative security-enhancing or incentive-based approaches. For example, participants might be automatically rewarded based on their contributions [2], [3], [11], [12]."}, {"title": "B. The Federated Learning Smart Contract", "content": "The FLSC has been implemented in Solidity. The contract structure incorporates methods that align with the state phases depicted in Figure 1, i.e, all messages involving the FLSC. It establishes four states for the FL process: OPEN, START, LEARNING, and CLOSE. These states guide the progression of the FL process, ensuring it adheres to predefined rules. This structured approach forces Collaborators to engage with the FLSC only when necessary. Furthermore, the phase definitions facilitate synchronization among all parties involved in FL, effectively addressing issues related to node failures. This implies that if a node fails or is late in transmitting the hash of its weights, the training process is not hindered, and weight updates proceed without considering the contribution of that particular node.\nCollaborators are added to the contract during the initialization phase, allowing only authorized users to participate. Collaborators can be added by the contract owner, and their interactions are monitored throughout the FL process. In Figure 1, we highlight the view functions, which correspond to methods within the smart contract that do not alter the contract state. These functions do not involve gas consumption for their execution and provide read-only access to specific data or parameters. The contract includes security measures to restrict unauthorized access and ensure that actions are carried out only by authorized users. During each round of the FL, Collaborators can perform specific actions only once, and the contract owner (Manager) has control over the FL process. Finally, the contract emits events to signal state transitions during the FL process, providing transparency and auditability."}, {"title": "C. On the Security and Reliability of the Protocol", "content": "The protocol is designed to handle the possibility of Collaborators failures. However, it clearly assumes that the Manager cannot fail. The Manager is responsible for coordinating all"}, {"title": "III. EXPERIMENTAL EVALUATION", "content": "We performed an experimental analysis to evaluate, on one side, the performance of the FL system, based on different aggregation methods and system setting. We also varied the amount of total nodes involved in the FL, with the possibility of having node failures during the training process, so as to assess how these node failures impact the overall performance. On the other hand, we measured the gas fees of the smart contract system, based on the amount of involved nodes."}, {"title": "A. The Federated Learning Model and Weight Update Methods", "content": "Collaborators used the same ML approach for classification, i.e., a three-layered, two-dimensional Convolutional Neural Network (CNN) model, coupled with a final fully connected layer at the end for the final classification of each image. The models are trained with the same setup: an Adam optimizer, a learning rate set to 10e-3, batch size equal to 32, softmax as the last activation function.\nIn this work, we used two specific FL weight update methods, inspired by [8]. In few words, we will denote with FedAvg the classic approach that assumes that all Collaborators contribute equally and that the dataset is evenly distributed across all nodes. Thus, when weights need to be updated at the end of a loop iteration, the novel weight is just the average value of weights received from the Collaborators. In this case, the classic cross-entropy is used as the loss function.\nFederated Proximal (FexProx), instead, removes the uniformity assumption. Proximal optimization involves adding a regularization term to the objective function to encourage specific properties (e.g., sparsity). At the end of the FL iteration happened at timestep t, weights are updated locally in order to minimize a loss function of this form\n\\(arg \\min_{w} F(w) + \\frac{\\mu}{2}||w \u2013 w^t||^2,\\)"}, {"title": "B. Brain Tumor Dataset", "content": "The dataset we employed during the tests is Brain Tumor MRI Dataset: https://www.kaggle.com/datasets/iashiqul/brain-tumor-mri-image-classification. This dataset contains 7,022 human brain MRI images classified into four classes: glioma, meningioma, no tumor, and pituitary."}, {"title": "IV. RESULTS", "content": "The implemented system involves various technologies, i.e., i) FL techniques for the decentralized analysis of datasets, which are partitioned across multiple sources; ii) blockchain technologies, particularly the FLSC, to coordinate the training phases, iii) IPFS, used for the decentralized yet secure exchange of information, thanks to data encryption. To evaluate this type of system, it is thus necessary to consider its various aspects. In this section, we show results obtained by using the FL system on the mentioned dataset related to image classification in healthcare, which is a typical use-case application for FL. Then, we will show the performance of the FLSC used to orchestrate the interaction among FL nodes [14]-[16]. Finally, we show also experimental results obtained for the weights retrieval and transmission to IPFS."}, {"title": "A. FL performance", "content": "Table I shows the averaged accuracy and F1 score obtained by the two FL aggregation techniques, against a centralized approach, i.e., the one that uses a classic ML, where the whole dataset is stored at a central node. In this case, er show results obtained in the best configuration, i.e., when the amount of employed Collaborators was equal to 5, with no failing nodes, together with the Manager (i.e., 6 nodes in total). Without any surprise, best results are obtained with the centralized approach. Having all the instances in the same data-store eases the training, and this leads to best results. However, we discussed already that in certain scenarios this is not possible. Thus, the centralized approach should be taken as an upper bound for the FL methods. In this respect, it is interesting to observe that the FL schemes perform quite well, especially FedAvg that is only 0.02 scores below centralized."}, {"title": "B. Federated Learning Smart Contract Gas Consumption", "content": "In this section, we show the gas consumption analysis for the smart contract that governs the interactions in the FL system. Due to space limitations, we will show only results related to system settings with no node failures."}, {"title": "C. IPFS Delays", "content": "Figures 6 and 7 show the average times required for the Manager and the Collaborators respectively, to perform essential actions related to updating weights on the Inter-Planetary File System (IPFS). Specifically, we examine the time taken to send (referred to as \"Add Operation\") and retrieve (referred to as \"Cat Operation\") the updated weights. The figures display both the averages and standard deviations of the measurements collected. Each individual configuration was repeated 10 times. In each run, the total number of participating nodes was the sum of the Manager and the Collaborators, the number of which is indicated in the chart. The execution spanned 10 rounds of the FL protocol. In this context, since the metrics measured were related to the transmission of weights to IPFS, all nodes were run on a single server with the following technical specifications: Intel(R) Core(TM) i7-8565U CPU (1.80 GHz - 1.99 GHz), 16.0 GB RAM, and a symmetrical bandwidth connection of 433/433 Mbps.\nFigures show that, as expected, delays are comparable for both the Manager and Collaborators. This consistency arises since the amount of data to be sent or received is the same, i.e., the data comprises a set of numerical values representing the weights of the employed Convolutional Neural Network (CNN). Moreover, the time to retrieve the novel weights was slightly higher than sending them. Finally, average times do not significantly vary based on the amount of involved Collaborators, since the amount of data to be retrieved for each Collaborator remains constant. There are differences in average values and standard deviations, that are emphasized in the charts only because of the limited scale of the y-axis. But in the end, results do not show significant differences.\nIn summary, however, our measurements confirm that there is a non-negligible time overhead associated with coordinating FL activities. Indeed, at the end of each round every Collaborator has to upload its weights to IPFS, compute and send the related hash value to the smart contract, and wait for an smart contract event, so as to retrieve the hash from the smart contract and the weights from IPFS. In the middle of this round step, the Manager has to retrieve all the hashes related to each Collaborator from the smart contract, retrieve all the weights of all the Collaborators, compute the average and upload the updated weights on IPFS. Given the measured times, depending on the number of Collaborators this upload phase requires some tens of seconds.\nIt is important to notice that we observed consistent performance on IPFS even in the event of node failures. Clearly enough, this process remains independent of other nodes, relying solely on the interaction with IPFS. For this reason and for the sake of brevity, we thus omit these results."}, {"title": "V. CONCLUSIONS", "content": "In this study, we presented a Federated Learning (FL) system that is based on the use of blockchain technology, aiming for a secure and coordinated approach to data management. The design of the framework is centered around data privacy protection, where it exchanges encrypted model parameters instead of sensitive data. This system integrates the Inter-Planetary File System (IPFS) and smart contracts to"}]}