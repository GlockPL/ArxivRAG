{"title": "Trust and Resilience in Federated Learning Through Smart Contracts Enabled Decentralized Systems", "authors": ["Lorenzo Cassano", "Jacopo D'Abramo", "Siraj Munir", "Stefano Ferretti"], "abstract": "In this paper, we present a study of a Federated Learning (FL) system, based on the use of decentralized archi-tectures to ensure trust and increase reliability. The system is based on the idea that the FL collaborators upload the (ciphered) model parameters on the Inter-Planetary File System (IPFS) and interact with a dedicated smart contract to track their behavior. Thank to this smart contract, the phases of parameter updates are managed efficiently, thereby strengthening data security. We have carried out an experimental study that exploits two different methods of weight aggregation, i.e., a classic averaging scheme and a federated proximal aggregation. The results confirm the feasibility of the proposal.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) is a Machine Learning (ML) framework that enables multiple parties to collaboratively train a shared model without directly sharing their individual data [1]. FL holds the promise of enhancing data-driven learning models while safeguarding data owners' privacy. This makes it an appealing approach for developing ML models in several application domains. A prominent example is clinical diagno-sis, where patient data privacy is critical, yet data aggregation from diverse sources is necessary. However, a significant challenge revolves around ensuring FL collaborators actively participate in the protocol while securely contributing their data. To this extent, recently several studies have explored the integration of blockchain technology within FL systems [2]- [6]. While existing research often emphasizes on participation incentives, traceability, and security aspects, there is an under-explored dimension that deserves attention, i.e., the impact of delays, number of collaborators, and collaborator failures on system performance and model training accuracy.\nIn our work, we investigate the performance of our FL system and the accuracy of ML training when one or more collaborators experience failures.\nWe propose a decentralized FL system that combines FL with two powerful components: the Inter-Planetary File Sys-tem (IPFS) and smart contracts (executed over a permissioned Ethereum-like blockchain). Together, they create a tamper-proof storage mechanism for sharing encrypted model pa-rameters. IPFS provides decentralized and fault-tolerant data storage, while smart contracts enforce rules and streamline interactions among participants [7]. The presence of a smart contract forces Collaborators to adhere to a protocol organized into distinct phases, ensuring orderly updates of model param-eters. Additionally, the smart contract automates compliance checks, maintaining the integrity of the FL process [6].\nIn the paper, we present the overall decentralized system and we perform an experimental evaluation under varying failure scenarios among Collaborators. As concerns the FL model assessment, two weight update methods are used, FedAvg and FedProx [8]. We investigate the FL system's performance in terms of classification accuracy, using a dataset coming from the healthcare image classification domain, i.e., brain tumor image dataset. Furthermore, we measure the gas consumption of the smart contract, a critical aspect in blockchain-based FL systems. Finally, we also evaluate the performance of the data retrieval and update, by both FL Manager and Collaborators, from/to IPFS. Results confirm the viability of using decentral-ized systems in Federated Learning.\nThe remainder of this paper is organized as follows. Section II describes the proposed framework. Section III describes the methodology, while Section IV outlines the experimental results. Section V provides some concluding remarks."}, {"title": "II. SYSTEM ARCHITECTURE", "content": "Our system architecture comprises four key actors. The first two are the classic actors involved in a typical FL system, while the last two ones are those of typical decentralized systems [7]:\n\u2022 FL Manager: The Manager acts as the aggregator for the classification model parameters used in FL. It triggers re-quests, collects ML parameters from other Collaborators, and monitors parameter updates.\n\u2022 FL Collaborators: these are the nodes participating to FL model training. They retrieve data, locally train their models, and upload the trained parameters to the system.\n\u2022 Permissioned Blockchain: This blockchain executes a Federated Learning Smart Contract (FLSC). The FLSC autonomously regulates parameter exchanges among par-ticipants, ensuring adherence to the protocol. Addition-ally, the use of blockchain digests prevents tampering with parameters, enhancing security [9], [10].\n\u2022 Decentralized File Storage: The Inter-Planetary File System (IPFS) stores the ML model parameters."}, {"title": "B. The Federated Learning Smart Contract", "content": "The FLSC has been implemented in Solidity. The contract structure incorporates methods that align with the state phases depicted in Figure 1, i.e, all messages involving the FLSC. It establishes four states for the FL process: OPEN, START, LEARNING, and CLOSE. These states guide the progression of the FL process, ensuring it adheres to predefined rules. This structured approach forces Collaborators to engage with the FLSC only when necessary. Furthermore, the phase definitions facilitate synchronization among all parties involved in FL, effectively addressing issues related to node failures. This implies that if a node fails or is late in transmitting the hash of its weights, the training process is not hindered, and weight updates proceed without considering the contribution of that particular node.\nCollaborators are added to the contract during the initial-ization phase, allowing only authorized users to participate. Collaborators can be added by the contract owner, and their interactions are monitored throughout the FL process. In Figure 1, we highlight the view functions, which correspond to methods within the smart contract that do not alter the contract state. These functions do not involve gas consumption for their execution and provide read-only access to specific data or parameters. The contract includes security measures to restrict unauthorized access and ensure that actions are carried out only by authorized users. During each round of the FL, Collaborators can perform specific actions only once, and the contract owner (Manager) has control over the FL process. Finally, the contract emits events to signal state transitions during the FL process, providing transparency and auditability."}, {"title": "C. On the Security and Reliability of the Protocol", "content": "The protocol is designed to handle the possibility of Collab-orators failures. However, it clearly assumes that the Manager cannot fail. The Manager is responsible for coordinating all activities, including sending messages to the FLSC, computing the updated weights, uploading the new values to IPFS, and notifying the Collaborators via smart contract calls, which in turn will emit events that the Collaborators can wait for.\nThe security of the protocol also relies on the Manager. Collaborators encrypt their weights using the Manager's public key and their own private key. This ensures verifiability (i.e., anyone can verify that the message was generated by a specific Collaborator) and confidentiality (i.e., only the Manager can decrypt the values).\nThese aspects make the Manager a possible single point of failure for the system. If the Manager fails, no one can carry out the training phase, as no one can invoke the appropriate functions of the FLSC smart contract or decrypt the values.\nThis limitation can be addressed by adopting one of the classic consensus protocols in an asynchronous network for managing a primary node election [13]. Any problems related to data encryption could be solved by using mechanisms that facilitate the use of encrypted decentralized file storage while enabling data sharing, such as key re-distribution techniques and multi-party computation [12]."}, {"title": "III. EXPERIMENTAL EVALUATION", "content": "We performed an experimental analysis to evaluate, on one side, the performance of the FL system, based on different aggregation methods and system setting. We also varied the amount of total nodes involved in the FL, with the possibility of having node failures during the training process, so as to assess how these node failures impact the overall performance.\nOn the other hand, we measured the gas fees of the smart contract system, based on the amount of involved nodes."}, {"title": "A. The Federated Learning Model and Weight Update Meth-ods", "content": "Collaborators used the same ML approach for classification, i.e., a three-layered, two-dimensional Convolutional Neural Network (CNN) model, coupled with a final fully connected layer at the end for the final classification of each image. The models are trained with the same setup: an Adam optimizer, a learning rate set to 10e-3, batch size equal to 32, softmax as the last activation function.\nIn this work, we used two specific FL weight update methods, inspired by [8]. In few words, we will denote with FedAvg the classic approach that assumes that all Collaborators contribute equally and that the dataset is evenly distributed across all nodes. Thus, when weights need to be updated at the end of a loop iteration, the novel weight is just the average value of weights received from the Collaborators. In this case, the classic cross-entropy is used as the loss function.\nFederated Proximal (FexProx), instead, removes the uni-formity assumption. Proximal optimization involves adding a regularization term to the objective function to encourage specific properties (e.g., sparsity). At the end of the FL iteration happened at timestep t, weights are updated locally in order to minimize a loss function of this form\n$\\arg \\min_{w} F(w) + \\frac{\\mu}{2}||w \u2013 w^t||^2,$ where F(w) represents the local loss function, w denotes the local weights to be identified, and wt signifies the global weights sent by the Manager at the beginning of timestep t. The Manager collects these weights coming from Collabora-tors, and then computes a novel version of these weights wt+1 by taking the average of the received values.\nDuring our experiments, we tried with different values for the \u00b5 hyper-parameter, finding that after the tuning the best results were obtained with \u00b5 = 0.001. Thus, we show results obtained according to this setting."}, {"title": "B. Brain Tumor Dataset", "content": "The dataset we employed during the tests is Brain Tumor MRI Dataset: https://www.kaggle.com/datasets/iashiqul/brain-tumor-mri-image-classification. This dataset contains 7,022 human brain MRI images classified into four classes: glioma, meningioma, no tumor, and pituitary."}, {"title": "IV. RESULTS", "content": "The implemented system involves various technologies, i.e., i) FL techniques for the decentralized analysis of datasets, which are partitioned across multiple sources; ii) blockchain technologies, particularly the FLSC, to coordinate the training phases, iii) IPFS, used for the decentralized yet secure ex-change of information, thanks to data encryption. To evaluate this type of system, it is thus necessary to consider its various aspects. In this section, we show results obtained by using the FL system on the mentioned dataset related to image classification in healthcare, which is a typical use-case application for FL. Then, we will show the performance of the FLSC used to orchestrate the interaction among FL nodes [14]-[16]. Finally, we show also experimental results obtained for the weights retrieval and transmission to IPFS."}, {"title": "A. FL performance", "content": "Table I shows the averaged accuracy and F1 score obtained by the two FL aggregation techniques, against a centralized approach, i.e., the one that uses a classic ML, where the whole dataset is stored at a central node. In this case, er show results obtained in the best configuration, i.e., when the amount of employed Collaborators was equal to 5, with no failing nodes, together with the Manager (i.e., 6 nodes in total). Without any surprise, best results are obtained with the centralized approach. Having all the instances in the same data-store eases the training, and this leads to best results. However, we discussed already that in certain scenarios this is not possible. Thus, the centralized approach should be taken as an upper bound for the FL methods. In this respect, it is interesting to observe that the FL schemes perform quite well, especially FedAvg that is only 0.02 scores below centralized."}, {"title": "B. Federated Learning Smart Contract Gas Consumption", "content": "In this section, we show the gas consumption analysis for the smart contract that governs the interactions in the FL system. Due to space limitations, we will show only results related to system settings with no node failures."}, {"title": "C. IPFS Delays", "content": "Figures 6 and 7 show the average times required for the Manager and the Collaborators respectively, to perform essen-tial actions related to updating weights on the Inter-Planetary File System (IPFS). Specifically, we examine the time taken to send (referred to as \"Add Operation\") and retrieve (referred to as \"Cat Operation\") the updated weights. The figures display both the averages and standard deviations of the measurements collected. Each individual configuration was repeated 10 times. In each run, the total number of participating nodes was the sum of the Manager and the Collaborators, the number of which is indicated in the chart. The execution spanned 10 rounds of the FL protocol. In this context, since the metrics measured were related to the transmission of weights to IPFS, all nodes were run on a single server with the following technical specifications: Intel(R) Core(TM) i7-8565U CPU (1.80 GHz - 1.99 GHz), 16.0 GB RAM, and a symmetrical bandwidth connection of 433/433 Mbps.\nFigures show that, as expected, delays are comparable for both the Manager and Collaborators. This consistency arises since the amount of data to be sent or received is the same, i.e., the data comprises a set of numerical values representing the weights of the employed Convolutional Neural Network (CNN). Moreover, the time to retrieve the novel weights was slightly higher than sending them. Finally, average times do not significantly vary based on the amount of involved Collaborators, since the amount of data to be retrieved for each Collaborator remains constant. There are differences in average values and standard deviations, that are emphasized in the charts only because of the limited scale of the y-axis. But in the end, results do not show significant differences.\nIn summary, however, our measurements confirm that there is a non-negligible time overhead associated with coordinating FL activities. Indeed, at the end of each round every Collabo-rator has to upload its weights to IPFS, compute and send the related hash value to the smart contract, and wait for an smart contract event, so as to retrieve the hash from the smart contract and the weights from IPFS. In the middle of this round step, the Manager has to retrieve all the hashes related to each Collaborator from the smart contract, retrieve all the weights of all the Collaborators, compute the average and upload the updated weights on IPFS. Given the measured times, depending on the number of Collaborators this upload phase requires some tens of seconds.\nIt is important to notice that we observed consistent perfor-mance on IPFS even in the event of node failures. Clearly enough, this process remains independent of other nodes, relying solely on the interaction with IPFS. For this reason and for the sake of brevity, we thus omit these results."}, {"title": "V. CONCLUSIONS", "content": "In this study, we presented a Federated Learning (FL) system that is based on the use of blockchain technology, aiming for a secure and coordinated approach to data man-agement. The design of the framework is centered around data privacy protection, where it exchanges encrypted model parameters instead of sensitive data. This system integrates the Inter-Planetary File System (IPFS) and smart contracts to create a secure, unalterable data storage infrastructure, thereby enhancing data security during FL operations. The viability of our proposal is confirmed by the results from our validation and tests.\nThe complete code for the project can be accessed here: https://github.com/LorenzoCassano/Blockchain-FederatedLearning/tree/main."}]}