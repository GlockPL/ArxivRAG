{"title": "DIAL: Distribution-Informed Adaptive Learning of Multi-Task Constraints for Safety-Critical Systems", "authors": ["Se-Wook Yoo", "Seung-Woo Seo"], "abstract": "Safe reinforcement learning has traditionally relied on predefined constraint functions to ensure safety in complex real-world tasks, such as autonomous driving. However, defining these functions accurately for varied tasks is a persistent challenge. Recent research highlights the potential of leveraging pre-acquired task-agnostic knowledge to enhance both safety and sample efficiency in related tasks. Building on this insight, we propose a novel method to learn shared constraint distributions across multiple tasks. Our approach identifies the shared constraints through imitation learning and then adapts to new tasks by adjusting risk levels within these learned distributions. This adaptability addresses variations in risk sensitivity stemming from expert-specific biases, ensuring consistent adherence to general safety principles even with imperfect demonstrations. Our method can be applied to control and navigation domains, including multi-task and meta-task scenarios, accommodating constraints such as maintaining safe distances or adhering to speed limits. Experimental results validate the efficacy of our approach, demonstrating superior safety performance and success rates compared to baselines, all without requiring task-specific constraint definitions. These findings underscore the versatility and practicality of our method across a wide range of real-world tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "ACQUIRING comprehensive knowledge [1]\u2013[3] has been a central focus in the fields of deep reinforcement learning (RL) [4] and imitation learning (IL) [5]\u2013[7], as it enables solving complex real-world problems. Recent advances in task-agnostic exploration [8]\u2013[11] demonstrate that leveraging shared knowledge across diverse tasks can significantly enhance performance in downstream tasks. However, in safety-critical domains such as autonomous driving, unrestricted exploration is infeasible [12]\u2013[14]. Safe RL [15]\u2013[17] addresses this by defining safely explorable regions as constraints. Learning a safe exploration policy [18] not only ensures adherence to safety requirements but also promotes effective transferability to novel tasks, enabling the agent to explore states within the boundaries of predefined constraints. Nevertheless, crafting accurate constraint functions across diverse tasks presents a burden. Moreover, relying solely on the acquired policy without constraints during transfer learning (TL) risks losing essential safety information previously learned.\nThe inverse constraint RL (ICRL) framework [19], [20] offers a solution by alternatively learning constraints to ensure safety while simultaneously developing the policy that achieves goals safely under the defined reward function. However, when these constraints are restored solely from single-task demonstrations, unexplored areas in the policy space are considered unsafe. This can lead to discrepancies between inferred and actual constraints, resulting in conservative policies with large regret bounds. The issue is further exacerbated in downstream tasks, where overly conservative constraints impede finding feasible solutions. To reduce regret bounds, incorporating multi-task demonstrations to learn constraints is preferable [21]. Nonetheless, collecting demonstrations across diverse tasks poses practical challenges because it requires meeting multiple safety requirements and addressing biases that arise from experts' risk tendencies, which remain significant hurdles in existing ICRL methods.\nTo overcome these limitations, we propose distribution-informed adaptive learning (DIAL), a novel method that leverages shared knowledge across diverse tasks to enable safe and effective adaptation to novel tasks. DIAL extends the ICRL framework by incorporating a distributional understanding of risks inherent in multi-task demonstrations. This risk-awareness is implemented through distorted criteria such as conditional value at risk (CVaR), allowing dynamic adjustment of risk levels to facilitate safe adaptation to changing environments [22]\u2013[24]. The core idea of DIAL is to design the constraint function to capture the distribution of risk across tasks while encouraging task-agnostic safe exploration (TASE) by maximizing entropy across diverse risk levels. Similar to standard ICRL, DIAL alternatively learns both the constraint function and the policy from demonstrations. However, DIAL introduces two critical innovations: 1) In the inverse step, DIAL learns the constraint distribution from the multi-task demonstrations, providing rich supervision of safety requirements. 2) In the forward step, DIAL maximizes task entropy within the learned risk bounds, encouraging safe exploration across a broader range of tasks. These innovations bring several benefits. The learned constraint distribution facilitates risk adjustment through distorted criteria, enabling adaptation to changed safety conditions. TASE policy also enables agents to effectively find feasible solutions for TL to meta-task scenarios."}, {"title": "II. RELATED WORK", "content": "Building on recent advancements in RL and IL, this research is inspired by various methodologies that enhance task adaptability and safety in autonomous systems.\nLearning Safe Exploration Policy: Task-agnostic exploration [25]\u2013[27] has become essential in RL to build generalized knowledge across diverse tasks without reliance on specific rewards or environmental dynamics. Early efforts [1]\u2013[3] established frameworks for learning exploration policy that adapts effectively to new tasks. Most techniques in this area focus on improving exploration efficiency through deep RL or IL [10], [28], [29]. While these methods excel in adaptability, they typically do not address the safety constraints critical to real-world applications. On the contrary, our study aims to address through a safety-focused exploration strategy. Safe exploration policy balances the need for broad state exploration with constraints that maintain system safety. Previous works [30], [31] have proposed state density maximization as a means to enable exploration but often limited its application to discrete state spaces. Some studies [18], [32] on maximizing constrained entropy illustrate how safety constraints can be incorporated into RL frameworks to ensure TASE while enabling TL across different domains. Our study builds upon these ideas by proposing a model that uses IL rather than RL to learn safe exploration policy, reducing the need for precise cost functions and improving sample efficiency.\nLearning Reward Function: Traditional IL methods [33]\u2013[35], which align policy with expert behaviors through supervised learning, often encounter compounding errors, especially in dynamic settings. Advanced hybrid methods [36]\u2013[38] based on maximum entropy IRL (MaxEnt IRL) [39] have mitigated these issues by unifying IL and RL approaches [40], [41]. However, adversarial IRL approaches such as GAIL [36] and AIRL [37] present stability and interpretability issues in reward function learning. To address these issues, assuming that the reward function is known, we modify the ICRL framework [20] that avoids adversarial training and focuses on interpretable constraint learning that can generalize across safety-critical tasks.\nLearning Constraint Function: The shift from reward learning to constraint learning arises from the need to ensure safety rather than merely replicating expert behaviors. Approaches [20], [42] that use maximum likelihood inference, a variant of IRL, aim to learn constraint conditions from expert demonstrations without predefined functions. However, these methods can be sensitive to data quality and may struggle to generalize constraint conditions across multiple tasks. Other approaches [19], [43], [44] use grid-based or probabilistic parameters to define safety boundaries, guiding robots to avoid dangerous areas through model-based exploration. These methods show potential for maintaining constraint conditions across various tasks and enabling real-world robotic applications. However, high-resolution grid representations come with increased computational costs, which limit scalability. To address this, efforts have focused on developing flexible models capable of applying constraint functions to new environments. For instance, some studies [45]\u2013[47] incorporate uncertainty into constraint inference from a distributional perspective, allowing learned constraints to adapt reliably to changed dynamics. Meanwhile, a reward-decomposition approach [48] facilitates the safe transfer of constraints to new reward structures. While these studies improve the generalizability of learned constraint functions, they often remain limited to specific tasks. Prior works [21], [26] address this limitation by exploring multi-task learning, but they still fail to account for distributional shifts. In contrast, our proposed approach learns an adaptive policy and constraint function that adjusts risk to ensure safety without strict assumptions tied to any specific task, providing a more flexible and broadly applicable solution.\nPrevious studies highlight the need for constraint learning that can be flexibly applied across diverse tasks. Our proposed approach, DIAL, emphasizes the derivation of a risk-sensitive constraint function and an adaptive policy from multi-task demonstrations, allowing for safety without being bound to specific tasks. DIAL effectively adjusts risk bias and supports policy adaptation across various scenarios. This design ensures scalability and safety, positioning it as a promising solution for applications such as autonomous driving and other safety-critical fields."}, {"title": "III. PRELIMINARIES", "content": "The CRL approach considers the environment as a constrained Markov decision process (CMDP) [49] to learn an optimal policy that maximizes the discounted cumulative rewards while ensuring the agent adheres to a set of safety constraints. To maintain consistency in notation, we redefine the CMDP from a distribution perspective as a following tuple (S, A, PT,R, \u03bc,\u03b3,C). Here S \u2208 RISI and A \u2208 RIA are state and action space, respectively, where s \u2208 S,a \u2208 \u0410 are the elements for each space. Pr(s'|s,a) indicates the state transition dynamics. R represents the set of all reward functions, where r(s,a) \u2208 R :S \u00d7 A \u2192 R is a reward function. Let \u03bc denote the initial state distribution, \u03b3\u2208 (0, 1) the discount factor, and C = {(PC,i)}N{i=1} the set of K constraints. For the i-th constraint, Pc\u2081 (c|s, a) gives the probability of incurring cost c in state s and action a, and e\u00bf \u2265 0 is a budget, which is an upper bound on expected cumulative costs. A cost function ci(s, a) \u2208 C\u00a1 : S \u00d7 A \u2192 {0,1} is represented as an indicator function for unsafe conditions, where Ci is the set of all cost functions. We denote a policy as \u03c0(\u03b1\u03c2) \u2208 \u03a0 : S \u00d7 A \u2192 [0,1], which maps states to probability distributions over actions, where II is the set of all policies. Let us refer to the y-discounted cumulative sum over an infinite time horizon as the return. To be specific, we denote reward-return as r(t) = \u2211t=0 \u03b3tr(st,at) and cost-return as ci(t) = \u2211t=0 \u03b3tci(st, at), respectively. Here T = (80,A0, 81,01,...) is a trajectory of state-action pair and a set of trajectories is D = {T}N{i=1}. When \u03c0(\u03c4) = \u03bc(so) \u03a0\u03c4=\u03bf \u03c0(at|st)PT(St+1|st,at) is defined as the probability that policy yield trajectory with 80 ~ \u03bc, \u03b1\u03c4 ~ \u03c0(\u00b7|St), and st+1 ~ PT(St, at) for t \u2265 0, we can represent the expected reward-return as E\u30f6\u223c\u03c0(\u00b7)[r(7)] and a constraint with expected cost-return as E\u30f6\u223c\u03c0(:)[Ci(T)] \u2264 \u20aci, respectively. Note that this type of constraint is typically referred to as \"soft\", meaning that it allows some trajectories to exceed the cost-return threshold ei, as long as the expected cost-return stays within ei. In contrast, a \"hard\" constraint demands that each trajectory individually remains within the cost bound of ei. To enforce a hard constraint, one could set \u20ac\u2081 = 0 to ensure all costs remain non-negative. In this context, the objective of maximum entropy (MaxEnt) CRL is to determine an optimal policy that maximizes the expected entropy-regularized reward-return while satisfying the given constraints. This is represented by the following formulation [15]:\n\u03c0* = arg max \u0395\u03c0 [r(\u03c4)] + \u03b2\u0397(\u03c0) subject to \u0395\u03c0 [Ci(T)] \u2264 \u20aci Vi.   (1)\nwhere augmented term H(\u03c0) = - f\u0375 \u03c0(\u03c4) log \u03c0(\u03c4)dt with coefficient \u1e9e \u2192 \u221e promotes randomness in action selection, supporting a certain level of exploration to prevent getting stuck in local optima while satisfying constraints."}, {"title": "IV. DISTRIBUTION-INFORMED ADAPTIVE LEARNING", "content": "In this paper, we aim to enable agents to safely adapt to diverse environments in navigation and control domains, particularly in safety-critical systems. To achieve this objective, we propose the DIAL method, a two-stage approach designed for environments represented by a CMDP. This method first performs safe IL and then shifts its focus to safe TL. The core idea of DIAL is to simultaneously learn a constraint function that is aware of the risk distribution across various tasks and a policy encouraging safe exploration of new tasks. These components are then used to facilitate safe adaptation. In the first stage, safe IL, we use multi-task demonstrations, as illustrated on the right side of Fig. 1b, to learn both the constraint distribution and a TASE policy. Since the true constraints of the environment are unknown in this stage, we rely on expert demonstrations that accomplish multiple tasks. We assume that while these demonstrations meet all safety requirements in the original environment, they may be suboptimal in adapted environments due to inherent biases in the expert's risk preference. In the second stage, safe TL, as shown in Fig. 2, we utilize the distorted criterion and TASE policy. Both are flexibly adjusted based on the risk level to enable safe and efficient adaptation to meta-tasks. To illustrate the benefits of DIAL in this stage, we configure a changed environment with the same safety requirements but a new target task. The underlying hypothesis of DIAL is that the TASE strategy with awareness of the constraint distribution is crucial for managing potential risks arising from limited data while facilitating adaptation to new tasks. Notably, DIAL can also be effectively coupled with a stable linear model-based controller [51] for autonomous driving, enabling structured exploration while maintaining lane alignment. The remainder of this section provides a detailed explanation of the design of DIAL. We begin by introducing methods to infer the constraint distribution from multi-task demonstrations that allow for flexible adjustment of the risk level. We also explain how to derive a policy that supports TASE. We then describe the process of carefully adjusting the learned constraint distribution by selecting an appropriate risk level to ensure safety in the changed environment and utilizing the TASE policy to facilitate meta-task resolution.\nWhen designing the distribution-aware constraint in Fig. 1b, we aim to capture the diverse risk preferences in expert demonstrations that meet safety requirements, even for randomly assigned tasks. To achieve this, we employ a learning approach that infers distorted constraint distributions with properly selected risk levels. Instead of maximum likelihood estimation for \u03da(T), Bayesian methods [45], [46], [52] can estimate a posterior distribution p($(+)|D) by combining a prior p(\u03b6(\u03c4)) with the trajectory likelihood p(D|\u03b6(\u03c4)) \u2248 \u03c0(\u03c4), as specified in Eq. 2. Previous methods calculate the constraint \u0395\u03c4~\u3160(\u00b7),5~p(:\\D)[[(T)] \u2264 \u20ac to ensure the mean of the estimated distribution remains below the threshold. However, this approach is vulnerable to long or heavy-tailed distributions, rendering it insufficiently stringent for handling rare but extreme situations. The mean constraint ultimately lacks the flexibility to adjust risk levels for changes in the unsafe set, shown as the green polygon in Fig. 1b. This limitation becomes even more pronounced in multi-task settings, where the attraction region, marked by the blue boundary, expands to cover more tasks. Although a previous study [46] addresses this issue by employing a risk-sensitive criterion like CVaR, it focuses solely on optimizing the policy for a single task during the forward step. In contrast, we emphasize using CVaR in the inverse step to learn constraints capable of handling scalable novel tasks.\nWhen estimating statistical metrics like the mean or CVaR of a posterior distribution, it is often challenging because arbitrary distributions lack a precise closed form. Therefore, it is necessary to approximate them with more manageable distributions. In our case, we would like to model the probability that a trajectory is safe as a random variable that falls within a finite interval between 0 and 1. Consequently, we choose an ideal Beta distribution as the posterior distribution and set p(C(T)|D) \u2248 q($(\u03c4)|\u03b1). The Beta distribution is particularly useful as it can represent asymmetric or heavy-tailed distributions. This distribution reflects the ratio of binary outcomes, such as successes and failures, of safe paths and is characterized by two parameters, a = [a1, a2]. For simplicity in notation, we omit i and assume a single constraint case. To handle multiple constraints, we can factorize q(\u03b6(7)|\u03b1) = \u03a0\u2081\u03b1(\u03b6\u03b5(\u03c4)\u03b1\u00b2), expressing it as a product of independent components.\nWe use CVaR to measure distorted risk, taking into account the probability of a path being safe at a specified risk level within the Beta distribution. Fig. 3 illustrates how CVaR is calculated in contrast to the mean. First, we determine the value at risk (VaR), which represents the threshold probability of safety at a given risk level A. This value is the A-percentile $F^{-1}_{F(\u03c4)}(\u03bb; \u03b1)$, with VaRx defined as the lower x-quantile of the distribution:\n$VaRx = inf{x \u2208 (0,1) : F\u03c2(\u03c4)(x; \u03b1) \u2265 \u03bb}$,   (5)\nwhere $F\u03b6(\u03c4) (x; a)$ represents the cumulative distribution function (CDF) of the $q(\u03b6(\u03c4)\u03b1)$ distribution. Next, CVaR is obtained by calculating the expected value over the region below VaRx as follows:\n$CVaRx = \u0395\u03c2(\u03c4)~q(\u00b7\u00a6a)[$(t)|$(t) \u2264 VaRx]$.   (6)\nThis represents the average of extreme values at a given risk level. Note that if x = 1, CVaR is equal to the mean. We employ a neural network $f\u2084(\u03b1|\u03c4)$, which takes a trajectory 7 as input and outputs the parameter a of a Beta distribution. This approach facilitates constraint learning by capturing the risk distribution and incorporating CVaR. Since the two parameters of the Beta distribution are positive, the final layer is implemented with a softplus activation. We interpret the network's output as variables sampled from two Gamma distributions, treating the approximated network as a prior model for the Beta distribution. This approach allows the shape of the Beta distribution to be determined by random variables generated by the Gamma distributions, rather than fixed parameters, thereby capturing uncertainty in the data. This flexibility makes probabilistic modeling more adaptable and relevant to the context. Therefore, we update the constraint model by applying the risk-sensitive criterion $\u0393(\u03c4) = Ea~f\u3041(\u00b7|r)[CVaRx]$ in place of $\u03b6(\u03c4)$ in Eq. 4. Consequently, the gradient of the loss \u2207LC(, ) is formulated as:\n$ETE\u2208DE [\u2207\u00a2log \u221a(TE)] \u2013 \u0395\u03c4\u223c\u03c0[\u03c9(\u03c4)\u2207log (7)]$.   (7)\nIn addition, considering diverse risk levels is known to be beneficial for acquiring risk-sensitive knowledge [53]. For this reason, we update the network using risk level A sampled from the uniform distribution U(0,1) in the safe IL stage where multi-task learning is conducted. In the later safe TL stage, where achieving high performance on the target task is crucial, fine-tuning is performed using grid search to determine \u03bb. Following this, we include a regularization term when optimizing the evidence lower bound (ELBO) for the approximate posterior as follows:\n$Lp($) = ET~{DE,D} [DKL[q($\\f\u2084(a|t)) || p(\u03b6)]] (8)\nGiven that both distributions q() and p(.) are Beta distributions, the Kullback-Leibler (KL) divergence can be computed in closed form, as shown in [54]."}, {"title": "V. EXPERIMENTS", "content": "We organize the experimental analysis using the following two aspects to evaluate our proposed method. First, we demonstrate our method that encourages safe exploration at varying risk levels by using multi-task demonstrations instead of explicit constraints in safe IL. Second, we reveal that leveraging our recovered risk-sensitive constraints and safe exploration policy can accelerate learning of the target task and show benefit safety assurance in safe TL.\nSafe IL and TL are evaluated within the navigation and control domain, encompassing scenarios with risky situations involving static or dynamic obstacles across state spaces ranging from low to high dimensions. In safe IL, the aim is to perform exploration that navigates through as many safe states as feasible for arbitrarily given tasks. In safe TL, the objective is to conduct safe exploitation that effectively tackles a given target task while ensuring it remains within the constraints. To demonstrate the advantages of these two components, we have composed urban driving tasks that address real-world navigation and robot control tasks to validate performance across diverse environments, further details are described in the following paragraphs.\nUrban Driving: To compare environments characterized by dynamic risks in high-dimensional states, we use a modified version of the intersection environment provided by HighwayEnv [59], referring [26]. Fig. 4 illustrates safe IL, where the agent learns shared constraints (4b) from demonstrations in the multi-task scenario (4a), and safe TL, where the agent applies this learned knowledge to adapt in the meta-task scenario (4c). The observation space includes 7 types of kinematic information, such as position and speed, for the agent and 15 surrounding vehicles. To achieve a permutation-invariant representation of the R15\u00d77 input independently of the order of surrounding vehicles, we use an attention-based encoder as the backbone network for the constraint model f\u00f8(\u03b1|\u03c4). This encoder captures the relative importance of each surrounding vehicle and calculates a weighted sum to represent the input as a latent variable in R32. We adopt the same controllers, reward function, and constraints as specified in [26] to ensure fair comparisons with the baselines. Each vehicle follows a fixed path by adjusting acceleration and steering angle through a linearized controller parameterized in R5 [51]. Surrounding vehicles maintain fixed parameters, while the agent's parameters are optimized using a constrained cross-entropy method (CEM) [32] rather than the PPO Lagrangian in Eq.11. This approach addresses multiple constraints by first ranking parameters according to the number of constraint violations, then assessing them based on violation magnitude and reward. The reward function is designed as a linear combination of random variables and features to reflect various driving preferences, including reaching the target, driving speed, and lane changes:\n$10 \u00d7 Goal + \u03be\u03c5Vt + \u00a7<\\\u2220t]$,   (12)\nwhere vt represents the vehicle's speed, \u2220t indicates the difference angle between the heading of the vehicle and the target lane, and \u03be ~ N(0.1, 0.1) and g\u2220 ~ N(-0.2, 0.1) are random variables. The target lane is the path extending to the given target location along the road network provided by the environment. The constraint function limits dangerous events by defining features composed of four metrics, (st, at) : S \u00d7 A \u2192 {0,1}4. These features include cases where the speed exceeds 15, the distance to the vehicle ahead is less than 10, a collision occurs, and the vehicle departs from the road. A safe situation is defined as one where the probability of each event occurring in an episode remains below the ground truth constraint thresholds \u20ac = [0.2,0.2, 0.05, 0.1]. In this problem, we consider 1 - max(0,\u2211t (st, at) \u2013 \u00ee) as the feasibility function and learn to infer thresholds \u00ea for the four defined features.\nRobot Control: To compare environments with static risks in low-dimensional states, we use a modified version of OpenAI Gym [60], shown in Fig. 5a-c. Each environment has a safety area marked with a red line. For MountainCar, the goal is to reach the point where the right flag is located without the car going to the left of the red line. However, the agent is penalized so that a large action at is not performed. For CartPole, the goal is to raise the pole angle \u03b8t vertically while keeping the cart inside both red lines. For BasicNav, the goal is to reduce the goal distance dt = Sgoal - St while avoiding the circular hazard region in the center. For evaluating responses to risks involving randomness in high-dimensional states, we use Safety Gym [50], illustrated in Fig. 5d. For PointGoal, we aim to control the point robot to reach a random goal described as a green cylinder. In this environment, the agent is restricted from entering the blue circles depicted on the ground or pushing the vase marked with a cyan block. A vase moving upon contact and stationary blue circles are randomly generated within a certain range around the target point.The rewards and costs used to train the RL agent are given the prefix \"extrinsic\" for those provided by the environment and \"intrinsic\u201d for those recovered through IL. Each environment provides an extrinsic cost whenever unsafe interactions occur, but this is used only for performance evaluation and not for training. However, the extrinsic rewards from the environment and the recovered intrinsic costs are both used for training."}, {"title": "VI. CONCLUSION", "content": "This paper proposes a novel approach called DIAL for safe RL in autonomous driving. DIAL leverages multi-task demonstrations to reconstruct the distribution of shared safety constraints and flexibly adjusts the required risk levels to address new tasks, demonstrating superior safety and efficiency compared to existing methods in experimental results. This approach offers a promising solution for safe exploration in safety-critical autonomous systems by enabling safe adaptation to new environments without relying on explicitly defined constraints. However, DIAL requires sufficient demonstrations to learn scalable constraints across multiple tasks, which can be challenging and costly in complex environments. Additionally, the two-stage learning structure, in which constraints are first learned from data and then used to safely adapt based on the given reward functions, can reduce learning efficiency. Moreover, selecting inappropriate risk levels during the distortion of constraint distributions may lead to overly conservative or overly optimistic behaviors, potentially hindering task performance or compromising safety. To overcome these limitations, it is necessary to develop methods that effectively utilize suboptimal demonstrations. Furthermore, integrating the two-stage learning structure into a single stage can enhance learning efficiency. Additionally, incorporating techniques that automatically optimize or dynamically adjust risk levels could achieve a more effective balance between safety and performance, presenting a promising research direction. Extending these approaches to other safety-critical domains, such as healthcare, would also allow for the validation of their scalability and broad applicability. These research directions are expected to overcome the limitations of DIAL and contribute to the development of more robust and efficient safe RL methods."}, {"title": "APPENDIX A ENVIRONMENTAL SETTINGS", "content": "Tab. III provides details about each environment used in the experiments. Although we assume infinite-horizon settings, the experimental environment is finite-horizon, which requires calculating the discounted approximation of d in a finite horizon Tas e = (1-7)d. The parameter d represents the threshold for the cost-return. The value e ranges between 0 and 1. It indicates the probability that the agent violates the constraint in a single episode. For the robot control tasks shown in Fig. 5(a-d), d is set to 0.5, 5, 10, and 25, respectively. These values are consistent with the configurations in [18]. In high-dimensional environments such as Intersection and PointGoal, the states are not discretized for visualization because selecting two main dimensions that clearly represent the state space is difficult. To aid understanding, Fig. 13 presents the visualization map of the reward function based on discretized states for each environment depicted in Fig. 5(a-c). In the main text, the axis information for these visualization maps is consistent with that in the figure and is omitted for simplicity. To implement the PointGoal environment using the SafetyGym engine, the configuration dictionary is as follows:"}, {"title": "APPENDIX B IMPLEMENTATION DETAILS", "content": "Expert Demonstrations: To collect the expert trajectories DE for urban driving tasks, the agent is trained to maximize true rewards while satisfying multiple constraints defined by the designed features in Fig. 4b and the ground-truth budgets. The trained agent from [32] is then deployed in the environment shown in Fig. 4a, targeting randomly selected goal positions that require either a left or right turn. For robot control tasks, we deploy a trained agent based on [18] in an environment with known true costs but no assigned target task.\nHyperparameters: This section briefly describes the hyperparameters required to reproduce our experiments. As shown in Tab. IV, several hyperparameters are unified across all methods to ensure a fair comparison. The parameters Nsamp, Nelite, and Niter correspond to the hyperparameters of CEM. While these are not explicitly mentioned in the main text, the pseudocode for CEM is provided in Algo. 4 of [26]. Tab. V and VI present the hyperparameters used for each environment during the safe IL and safe TL stages, respectively.\nTechnique for Learning Stability: In general, the learning rate of k is set to a small value. At this point, the following issues arise. When the policy is unsafe, k cannot quickly adjust to a larger value needed to ensure safety. Conversely, when the policy is safe, k does not swiftly revert to a smaller value. This fact destabilizes the learning process. To address the instability, we introduce a damping weight in place of Kin Eq. 11. This adjustment directly tackles the issue by allowing the algorithm to dynamically respond to safety considerations while stabilizing the learning process. The damping weight is defined as \u00f1 = \u03ba-\u03ba\u03b1(\u03b5-\u0395\u03c4~\u03c0\u03bf(\u00b7) [\u0403\u00a6(7)]) based on methods [62], [63], where k\u0430 is a damp scaling factor. This formulation adjusts K based on the difference between the safety threshold e and the expected safety risk Er\u223c\u03c0\u03bf(\u00b7)[\u0393\u2642(\u03c4)], providing a more nuanced response to environmental conditions."}]}