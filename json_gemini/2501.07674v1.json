{"title": "CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory", "authors": ["Haokun Zhao", "Jinyi Han", "Jiaqing Liang", "Yanghua Xiao"], "abstract": "Large Language Models (LLMs) have demonstrated outstanding capabilities across various domains, but the increasing complexity of new challenges demands enhanced performance and adaptability. Traditional benchmarks, although comprehensive, often lack the granularity needed for detailed capability analysis. This study introduces the Cognitive Diagnostic Synthesis (CDS) method, which employs Cognitive Diagnosis Theory (CDT) for precise evaluation and targeted enhancement of LLMs. By decomposing complex tasks into discrete knowledge points, CDS accurately identifies and synthesizes data targeting model weaknesses, thereby enhancing the model's performance. This framework proposes a comprehensive pipeline driven by knowledge point evaluation, synthesis, data augmentation, and filtering, which significantly improves the model's mathematical and coding capabilities, achieving up to an 11.12% improvement in optimal scenarios.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, the increasing complexity of emerging challenges has raised higher demands on model performance, and the shortcomings exposed in practical applications also highlight the need for new methods to further enhance their capabilities.\nTo achieve continuous improvement during training, developers commonly analyze model responses using various benchmarks and adjust the training corpus accordingly. For example, when LLMs underperform on mathematical benchmarks such as GSM8k, targeted math data is incorporated into subsequent training cycles to address specific weaknesses. Recent studies have utilized advanced LLMs like GPT-4 as synthesizers(Dai\net al., 2023) or gathers their response as a new instruction-tuning dataset by prompting them (Sun\net al., 2023). These approaches leverage distilled data from advanced models to supplement the training corpus and improve the fine-tuning process.\nIn this context, we identify two primary concerns: (1) Existing benchmarks typically rely on coarse-grained metrics such as overall accuracy to summarize model capabilities. As shown in Figure 1, while these metrics provide a dataset-level overview, they fail to pinpoint specific strengths or weaknesses, particularly at the level of individual knowledge points or sub-skills. This lack of fine-grained analysis restricts actionable insights into model deficiencies and impedes targeted improvements. (2) The traditional method of indiscriminately generating data to expand the training corpus is inefficient. It does not fully leverage the advanced capabilities of LLMs. Beyond data generation, advanced language models have demonstrated exceptional analytical and evaluative capabilities. Guiding these models to perform analysis prior to data synthesis can significantly enhance the quality of the generated data. (Bai et al., 2023; Dai\net al., 2023). We are thus inspired by the cognitive diagnosis theory (CDT) in educational psychology, which, unlike traditional assessment methods that provide only an overall score or general evaluation, emphasizes assessing an individual's mastery of specific knowledge points or skills and identifying their strengths and weaknesses. This approach informs the design of strategies for enhancing LLMs' further training by mirroring the personalized and targeted methodologies employed in human education.\nSpecifically, we introduce the Cognitive Diagnostic Synthesis (CDS) method, utilizing CDT at both macro and micro levels to optimize model evaluation and guide data synthesis. From a macro perspective, we decompose problems into multiple knowledge points and statistically evaluate the model's mastery of each knowledge point. From a micro perspective, we guide advanced LLMs to perform cognitive diagnosis based on the specific answering records of the student model, identifying the mastery of knowledge points revealed during the answering process. This diagnosis is then used as the context for Chain-of-Thought (CoT) reasoning to facilitate the synthesis of high-quality data focused on weakly mastered knowledge points. Furthermore, the macro-level knowledge point diagnosis results can also be used to guide the synthesis of diverse data and serve as a reference for subsequent data filtering.\nOur contribution can be summarized as follows: (1) We apply CDT to introduce a more fine-grained knowledge point-level evaluation process into the model assessment, enabling a deeper understanding of model capabilities.\n(2) We design two data synthesis strategies: one based on statistically derived cognitive results to target the synthesis of data that addresses weakly mastered knowledge points, and the other based on specific error diagnoses from the model's incorrect responses which leverages the capabilities of advanced LLMs to diagnose the student model, analyze the causes of errors, and generate targeted supplemental data. Based on these synthetic data, we apply knowledge point-constrained rewriting and multi-knowledge point fusion data augmentation methods.\n(3) We propose a two-stage data filtering approach. In the first stage, we use advanced LLMs"}, {"title": "Related Work", "content": ""}, {"title": "Cognitive Diagnosis Theory", "content": "Cognitive Diagnosis Theory (CDT) provides fine-grained assessments by diagnosing an individual's mastery of specific knowledge points, offering actionable insights for targeted interventions (Junker\nand Sijtsma, 2001; Rupp et al., 2010). CDT focuses\non identifying strengths and weaknesses through\nmodels such as DINA (De La Torre, 2009) and\nG-DINA (de la Torre, 2011). These models lever-\nage Q-Matrix Theory (Tatsuoka, 1983) to link test\nitems with underlying knowledge points and pro-\nvide probabilistic mastery estimates. While CDT\nintegrated with AI has been widely applied in ed-\nucational assessments (Minn, 2022; Wang et al.,\n2019; Liu, 2021), its application in data synthesis\nand model improvement is highly underexplored."}, {"title": "Synthetic Data for Improving Model", "content": "Leveraging advanced LLMs to generate training\ndata has become a widely adopted strategy for im-\nproving open-source models (Dai et al., 2023; Xu\net al., 2023; Mitra et al., 2024; Wang et al., 2023;\nIvison et al., 2023; Chen et al., 2023b; Mitra et al.,\n2023; Fu et al., 2023; Kumar et al., 2020; Li et al.,\n2024a, 2023). Concurrently, researchers have in-\nvestigated generating corrective data through error\nanalysis of target models (An et al., 2023; Lee\net al., 2024) and enhancing learning via compar-\native analysis of positive and negative examples\n(Ying et al., 2024). Zhang et al. (2024) optimized\nprompts by extracting reasoning principles from\nerrors, while Liao et al. (2024) analyzed errors in\nsmaller LMs, storing derived knowledge and sum-\nmaries in specialized knowledge bases to enhance\nreasoning performance.\nSome studies begin with knowledge-based syn-\nthesis, generating knowledge concepts from online\ncourse platforms (Huang et al., 2024b), GPT-4 (Li\net al., 2024b), and seed instruction analysis and\nclustering (Huang et al., 2024a), thereby guiding\nadvanced LLMs in data synthesis. However, these\napproaches have several limitations: simple nom-"}, {"title": "Methods", "content": ""}, {"title": "CDS Method", "content": "We propose the CDS method, as illustrated in Figure 2. The main steps are as follows:\nStep 1: Tagging the Benchmark with knowledge points. We employ an advanced LLM to annotate each sample in the benchmark with relevant knowledge points through a two-round tagging process. (1) In the first round, the model performs unrestricted coarse annotations, after which all identified knowledge points are retrieved to construct a comprehensive set. This set is then merged and refined to eliminate redundancies, ensuring that the knowledge points are mutually independent, comprehensively covered, and possess appropriate granularity. (2) In the second round, the refined set of knowledge points is utilized by the advanced LLM to conduct fine-grained tagging, guaranteeing that these points originate from a limited and meticulously curated collection. The Question-Knowledge Point Matrix in the Figure2 illustrates the annotation results, where 0 indicates that a knowledge point was not examined and 1 indicates that it was examined.\nStep 2: Evaluating the student LLM and globally diagnosing knowledge point mastery. We assess the student Large Language Model (LLM) using the benchmark annotated in Step 1, leveraging the Deterministic Inputs, Noisy \"And\" gate (DINA) cognitive diagnosis model. This model operates under the assumption that mastery of each knowledge point associated with a question is binary and all-or-nothing: a correct response indicates mastery of all relevant knowledge points, whereas an incorrect response signifies a lack of mastery. Based on the student LLM's accuracy in answering each question, we construct a Knowledge Point Diagnostic Matrix. This matrix evaluates the global diagnostic performance of the student model using two metrics: the frequency of knowledge point assessments and the accuracy of knowledge point mastery, with the calculation formulas provided as follow.\n$Acc(kp) = \\frac{\\sum_{i=1}^{N} Correctness_i \\cdot Q-KPi(kp)}{\\sum_{i=1}^{N}Q-KP_i(kp)}$\n$Freq(kp) = \\frac{\\sum_{i=1}^{N}Q-KPi(kp)}{N}$\nwhere Correctness, is a binary indicator specifying whether\nquestion i was answered correctly (1 for correct, 0 other-\nwise), Q-KPi(kp) is a binary indicator from the Question-\nKnowledge Point Matrix indicating whether knowledge point\nkp is associated with question i (1 for associated, 0 otherwise),\nand N is the total number of questions.\nWe focus on rarely tested knowledge points and low-accuracy knowledge points, which are identified as the weakly mastered knowledge points of the student LLM. These points will also be prioritized in the subsequent data synthesis process.\nStep 3: Data synthesis based on cognitive diagnosis. We propose two synthetic strategies, one from a global perspective and the other from a fine-grained perspective, to synthesize targeted data. (1) Global Strategy: Based on the global knowledge point mastery identified in Step 2, the Advanced LLM synthesizes data by focusing on weakly mastered knowledge points. Unlike approaches that directly expose the model to original questions during prompt-based generation, this strategy abstracts away from the original questions and operates exclusively at the knowledge point level. This effectively avoids the phenomenon where the model, when prompted with the original questions, unconsciously rewrites or rephrases them, resulting in biased or redundant outputs. By decoupling data synthesis from direct exposure to the original questions, this method ensures that the generated data is both novel and independent, addressing the limitations of prompt-based approaches that may inadvertently overfit to the original dataset. (2) Fine-Grained Strategy: Advanced LLMs have demonstrated robust capabilities in error analysis and in identifying both weakly and well-mastered knowledge points. By diagnosing the specific causes of incorrect answers in the student LLM's responses, we can accurately identify insufficiently mastered knowledge points, which are then prioritized in subsequent data synthesis. In contrast, well-mastered knowledge points are deprioritized to avoid redundancy and optimize training efficiency.\nTo further improve the relevance and quality of the synthesized data, the diagnostic results from the Advanced LLM are incorporated into the data synthesis process as a Chain-of-Thought (CoT) con-"}, {"title": "Data Augmentation", "content": "To further enhance\nthe diversity and volume of the synthesized data,\nwe adopt two data augmentation strategies: (1)\nKnowledge Point-Constrained Instruction Rewrite:\nBuilding upon existing approaches that utilize seed\ninstructions with constraints and diverse prompt"}, {"title": "Data Filtering", "content": "After synthetic data augmentation, the subsequent filtering phase discards\nsamples that fail to meet the required quality stan-\ndards. This process ensures that only high-quality\nentries are retained in the dataset, thereby improv-\ning the overall reliability and usability of the gener-\nated data. We design a two-stage filtering pipeline:\n(1) First, an advanced LLM assigns scores to the\nsynthetic data based on multiple criteria, including\ncorrectness and knowledge point relevance. Only\nsamples with scores exceeding the threshold @ are\nretained. (2) Second, we introduce a novel met-\nric, Qscore, which integrates the initial accuracy of\nthe student model on each knowledge point and\nthe frequency with which knowledge points are\nassessed in the synthetic dataset. This metric pri-\noritizes problems that: (a) assess a greater number\nof knowledge points (i.e., more complex and com-\nprehensive), (b) involve knowledge points that are\nunderrepresented in the synthetic dataset, and (c)\ntarget knowledge points where the student model\ninitially performs poorly. The Qscore is calculated\nas follows:\n$CDS Score (di) = \\sum_{kpj\\in K di} KP Score (kpj), di \\in D_s$\n$KPScore (kpj) = Norm (w\\cdotlog(\\frac{Acc_{kpj} + \\epsilon}{1}) + (1 - w) \\cdot log(\\frac{1}{Freq_{kp} + \\epsilon}))$\nwhere $CDS Score (d_i)$ is the overall score of instruction $d_i$, with\n$D_s$ as the synthetic dataset, $K_{d_i}$ as the set of knowledge points\nassessed by $d_i$, and $KPScore (kpj)$ as the score of the j-th\nknowledge point in $d_i$. Here, $Acc_{kp_j}$ denotes the student\nLLM's initial accuracy on $kpj$, $Freq_{kp_j}$ is the frequency of\n$kpj$ in $D_s$, $w$ is a balancing weight, and $Norm(\\cdot)$ is a normal-\nization function.\nWe apply a 1-0 principle, retaining samples with\n$CDS Score (di) > \u03bc-\u03c3$, to construct the final supple-\nmentary training dataset for fine-tuning the student\nLLM."}, {"title": "Experimental Setup", "content": ""}, {"title": "Datasets", "content": "We assess two primary capabilities: mathematical reasoning and coding. For each, we designate a training dataset Dtrain and an evaluation dataset Deval, comprising both In-Domain (ID) and Out-of-Domain (OOD) subsets to evaluate generalization beyond the training distribution. Specifically, GSM8k (Cobbe et al., 2021) serves as the ID dataset for mathematical reasoning, while GSM8k-PLUS (Li et al., 2024c), augmented with mathematical perturbations, is used as the OOD dataset. For coding, MBPP(Austin et al., 2021) is the ID dataset, and HumanEval(Chen et al., 2021) is employed for OOD evaluation."}, {"title": "Language Models", "content": "We use Qwen1.5-7B-Chat(Bai et al., 2023) and Llama3-8B-Instruct(AI@Meta, 2024) as the student LLMs, with Qwen2-72B-Instruct(Yang et al., 2024) serving as the advanced LLM."}, {"title": "Training Setup", "content": "We train the models on 1 NVIDIA A800 GPUS, with ZeRO Stage 1 (Rajbhandari et al., 2020) from DeepSpeed (Rasley et al., 2020), using AdamW (Kingma and Ba, 2015) as the optimizer and LoRA (Hu et al., 2021) with a rank of r = 8 for parameter-efficient fine-tuning. The batch size is 32, with a maximum sequence length of 2,048 and 1 training epoch."}, {"title": "Inference Setup", "content": "We tailor sampling strategies to specific tasks. For code generation, we use sampling with temperature 0.3, top-p 0.8, and top-k 10. For mathematical reasoning, we adopt greedy decoding. Both tasks apply O-shot inference and limit the output to a maximum of 512 tokens."}, {"title": "Baselines", "content": ""}, {"title": "Synthetic Baselines", "content": "We consider several baselines for comparison with our method as follows: (1) Prompt: Directly prompting the model to answer. (2) IFT: Using previously unused instructions from the in-domain training set for fine-tuning. (3) Learning from Errors by Contrast (LEC) (Ying et al., 2024): Using SentenceBERT (Reimers and Gurevych, 2019) to embed erroneous cases, selecting the most similar positive examples based on the L2 distance, and utilizing both positive and negative cases to prompt the advance LLM for synthesis. (4) AugGPT (Dai\net al., 2023): Previously unused instructions from\nthe in-domain training set are sampled as seed data\nto generate synthetic data using the advanced LLM.\n(5) LLM2LLM (Lee et al., 2024): Generate addi-\ntional data using incorrect examples from the eval-\nuation set with the advanced LLM. (6) MUSTARD\n(Huang et al., 2024b): Questions are generated\nbased on randomly selected seed concepts from on-\nline course platforms, with corresponding answers\nproduced by an advanced LLM and subsequently\nfiltered for correctness using a sophisticated de-\ntector. We employ Qwen2-72B-Instruct as the ad-\nvanced LLM and detector, ensuring consistent in-\nstruction quantity across all baselines."}, {"title": "Filtering Baselines", "content": "We evaluate our filtering algorithm against several baselines as follows: (1) Cluster-Based Selection (CBS) (Chen et al., 2023a): Instructions are embedded using SentenceBERT, clustered with HDBSCAN (Campello et al., 2013), and samples are selected using the K-Center-Greedy algorithm. (2) Coreset (Sener and Savarese, 2017): Similar to CBS, instructions are embedded with SentenceBERT, and samples are selected using the K-Center-Greedy algorithm. (3) Diversity (Wang\net al., 2022): For each instruction, the ROUGE\nscore is computed against a randomly selected sub-\nset of n samples (n \u226a M). We select k sam-\nples with the lowest ROUGE scores. (4) Length:\nSamples are selected based on the length of the in-\nput instructions, focusing on the longest instances\n(Lengthlong) to assess their impact. (5) Perplexity\n(Marion et al., 2023): Samples are selected based\non low per-token perplexity, indicating high model\ncertainty and fluency. (6) AlpaGasus (Chen et al.,\n2024): Each data point is scored using an advanced\nLLM, such as ChatGPT, based on dimensions such\nas helpfulness and accuracy. Instances with low\nscores are filtered out. (7) Random: Instances are\nselected purely at random from the dataset."}, {"title": "Experiments", "content": ""}, {"title": "Main Results", "content": "The main experimental results of our methods and baseline approaches across various tasks are presented in Table 1."}, {"title": "Evaluation of Filtering Strategies", "content": "he main experimental results of our filtering strategy and baseline approaches across various tasks are presented in Table 2."}, {"title": "Conclusion", "content": "In this paper, we introduced the Cognitive Diagnostic Synthesis (CDS) method, which leverages Cognitive Diagnosis Theory (CDT) to perform fine-grained, knowledge point-level evaluations of Large Language Models (LLMs). CDS enables targeted data synthesis by decomposing problems into specific knowledge points and utilizing advanced LLMs for detailed cognitive diagnosis, effectively addressing identified model weaknesses. We developed two data synthesis strategies\u2014cognitive-based synthesis and error diagnosis-guided synthesis and implemented a two-stage data filtering approach featuring the novel CDSScore metric. Our experiments demonstrate that CDS significantly enhances LLM performance by generating high-"}]}