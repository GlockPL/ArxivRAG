{"title": "Al Mismatches: Identifying Potential Algorithmic Harms Before Al Development", "authors": ["Devansh Saxena", "Ji-Youn Jung", "Jodi Forlizzi", "Kenneth Holstein", "John Zimmerman"], "abstract": "Al systems are often introduced with high expectations, yet many fail to deliver, resulting in unintended harm and missed opportunities for benefit. We frequently observe significant \"AI Mismatches\", where the system's actual performance falls short of what is needed to ensure safety and co-create value. These mismatches are particularly difficult to address once development is underway, highlighting the need for early-stage intervention. Navigating complex, multi-dimensional risk factors that contribute to AI Mismatches is a persistent challenge. To address it, we propose an AI Mismatch approach to anticipate and mitigate risks early on, focusing on the gap between realistic model performance and required task performance. Through an analysis of 774 AI cases, we extracted a set of critical factors, which informed the development of seven matrices that map the relationships between these factors and highlight high-risk areas. Through case studies, we demonstrate how our approach can help reduce risks in AI development.", "sections": [{"title": "1 INTRODUCTION", "content": "There has been growing interest and an increasing amount of hype about how AI might revolutionize different industries. The recent emergence of large, pre-trained models (e.g. Large Language Models, Generative AI, Foundational Models) has only accelerated this interest, investment, and hype. Al now impacts numerous sectors, including manufacturing, pharmaceuticals, agriculture, and more. Currently, 90% of CEOs plan to prioritize AI [63]. AI can offer value through improved insights, decision-making, and resource optimization.\nWhat sometimes gets lost in the hype is the tremendously high failure rate for AI initiatives within organizations (85%) [4], and one of the main causes of failure is the unintended harms these systems can create [118]. For instance, after OpenAI's ChatGPT launch in November 2022, Meta's Galactica was quickly shut down due to poor quality [46]. Similarly, a demo error in Google's Bard caused a $100 billion drop in market value in one day [83]. The rapid pace of innovation and hype often lead to the release of AI systems that unintentionally cause significant harm.\nOver the last decade, the Responsible AI (RAI) and HCI communities developed various approaches to reduce Al's unintended harms. These approaches include fairness toolkits, auditing methods, algorithmic \u201cde-biasing\u201d techniques, and documentation protocols [29, 32, 41, 51, 58, 79, 80, 120]. Interestingly, most of these efforts focus on fixing harmful Al systems after they have been deployed or after models have been developed. Recently, researchers have recognized that some AI concepts\u00b9 are fundamentally flawed doomed before the first line of code is written. These failures often result from AI Mismatches: mismatches between the required model performance needed to deliver the intended value and minimize harm and the model performance that can be realistically achieved given factors such as data quality and the complexity of the inference [22, 87, 116]. Moreover, ethical problems can be exceedingly challenging to address once a model is created. Post-hoc fixes can sometimes even exacerbate biases and introduce new ethical harms [21, 32, 58]. Thus, there is a growing consensus that RAI concerns are often best addressed at the earliest stages of AI innovation; during the ideation and problem formulation stages of Al development [22, 60, 85, 87, 100, 116, 126].\nRecent RAI research maps out various types of AI-related harms and has developed taxonomies to help practitioners anticipate and minimize these harms (e.g., [7, 102, 115]). Similarly, recent work in HCI indicates that many successful AI systems create value for users by focusing on simple tasks where it is realistic to expect excellent model performance, or by addressing situations where even moderate model performance creates value for users [126]. For example, moderate performance in automatic speech recognition is sufficient for generating voicemail transcripts, as users can generally understand the text despite some errors. By contrast, moderate performance is inadequate for transcribing court cases, where precise wording is crucial.\nIn making the case for thinking about unintended harm at the earliest innovation stages, researchers note that algorithmic harms often arise from the interplay of multiple factors [76, 116]. This complicates efforts to address harms using one-dimensional frameworks. Navigating and predicting the factors that increase the risk of AI-related harm, especially during the early stages of development, remains a major challenge for innovation teams.\nTo address this, we developed an approach for thinking through AI Mismatches in early-stage Al concepts to identify the potential for ethical harm. Following this approach, we envision how Al innovators are sensitized to notice mismatches between (i) the model performance we can realistically expect, and (ii) the performance required to provide actual benefit and avoid unintended harm [87, 116]. We adopt a human-centric definition of model performance [70, 84, 124]. Rather than understanding performance solely in terms of standard ML metrics such as predictive accuracy, we define model performance holistically in terms of the model's ability to perform a task that fulfills a human need.\nOur approach leverages Yildirim et al.'s Task-Expertise x AI-Performance matrix as a building block [126]. It helps to explore several perspectives around unintended harms for a given concept. Our approach surfaces mismatches, increasing the ability to foresee potential harms and consider ways to reduce these risks before system building begins. To develop our approach, we reviewed 774 case studies of real-world algorithmic harm, identifying critical underlying risk factors and their interplay with model performance. We then created a set of seven matrices to expose the interrelationships and identify high- and low-risk areas. Finally, we conducted a preliminary assessment of our process by analyzing six case studies involving automated decision-making, generative AI, and machine learning applications. This provides a preview of how our approach might help surface key risk factors during the concept selection phase of an innovation process.\nThis paper makes four contributions. First, it offers an initial step towards understanding the multi-dimensional factors that influence AI Mismatch. Our matrices serve as an example to help uncover discrepancies that indicate an Al concept might be infeasible to implement in a way that both creates value and minimizes harm. Second, our approach provides explanatory power to articulate inherent risk factors for an Al concept. We believe this can support teams in steering their concepts toward 'safer' zones, where a more balanced approach can be taken between what a concept aims to achieve and what is realistically possible. Third, this paper contributes a set of comparative case studies where we used our approach to uncover key AI Mismatch factors that influence the gap between Al capabilities and task requirements. Finally, this paper identifies areas for future study related to AI Mismatch, particularly around design for moderate model performance."}, {"title": "2 RELATED WORK", "content": "Over the past decade, some HCI research has explored the concept of AI as a Design Material [9, 31, 50, 125]. Researchers reasoned that integrating design thinking into AI innovation would open up the space for innovation by bringing more creative thinking to envisioning what might be built [31]. In reaction to the high failure rate of AI initiatives, these researchers suggested the problem might be what innovation teams chose to build in the first place. They noted that in many cases, there seemed to be a lack of ideation and time devoted to exploring many different concepts early on (e.g., sketching, brainstorming, and ideation). They noted that most AI-focused resources and guidebooks only address challenges that happen during the prototyping phase, well after problem formulation and project selection.\nIn response to this challenge, researchers developed various resources, tools, and processes that enable designers to engage with Al's potential, envision how AI capabilities can address specific problems, and create value for users and service providers (e.g., [39, 57, 126]). Despite these efforts, findings show that design teams often struggle to identify situations where AI capabilities can realistically create value. Prior work examined why AI is \"uniquely difficult to design for\" [123], with Al's inherent uncertainty identified as a core challenge. Additionally, some scholars have noted that Al is often metaphorically described as 'magical' or 'enchanting' because it seems to perform tasks previously thought impossible for computers [75]. The positioning of AI as magic may partly explain why design and HCI seem to overestimate what AI can do and underestimate its costs and harms.\nTo address the issue of designers overestimating Al's capabilities, Yildirim et al. [126] introduced the Task-Expertise x AI-Performance matrix. This matrix helps innovation teams more easily recognize low-risk and high-value concepts when dealing with many possible things to build. The matrix consists of three rows representing levels of task expertise. For example, a step counter (the task of noticing and counting steps) requires minimal task expertise, while recognizing cancer from a pathology image requires significant expertise. The matrix also has three columns indicating the minimum level of AI model performance needed for a user to experience the system as valuable. For example, automatic speech recognition of a voicemail only needs moderate performance to be useful, while automatic transcription of a court case would need excellent model performance to be useful.\nWhile this framework holds promise for helping teams generate more feasible Al concepts during ideation, it does not consider the ethical risks embedded in the concepts. Many ethical failures in Al stem from unrealistic problem formulations, so balancing performance requirements with feasibility presents an important area for further research. Our paper builds on this work by identifying"}, {"title": "2.1 AI as a Design Material", "content": "Over the past decade, some HCI research has explored the concept of AI as a Design Material [9, 31, 50, 125]. Researchers reasoned that integrating design thinking into AI innovation would open up the space for innovation by bringing more creative thinking to envisioning what might be built [31]. In reaction to the high failure rate of AI initiatives, these researchers suggested the problem might be what innovation teams chose to build in the first place. They noted that in many cases, there seemed to be a lack of ideation and time devoted to exploring many different concepts early on (e.g., sketching, brainstorming, and ideation). They noted that most AI-focused resources and guidebooks only address challenges that happen during the prototyping phase, well after problem formulation and project selection.\nIn response to this challenge, researchers developed various resources, tools, and processes that enable designers to engage with Al's potential, envision how AI capabilities can address specific problems, and create value for users and service providers (e.g., [39, 57, 126]). Despite these efforts, findings show that design teams often struggle to identify situations where AI capabilities can realistically create value. Prior work examined why AI is \"uniquely difficult to design for\" [123], with Al's inherent uncertainty identified as a core challenge. Additionally, some scholars have noted that Al is often metaphorically described as 'magical' or 'enchanting' because it seems to perform tasks previously thought impossible for computers [75]. The positioning of AI as magic may partly explain why design and HCI seem to overestimate what AI can do and underestimate its costs and harms.\nTo address the issue of designers overestimating Al's capabilities, Yildirim et al. [126] introduced the Task-Expertise x AI-Performance matrix. This matrix helps innovation teams more easily recognize low-risk and high-value concepts when dealing with many possible things to build. The matrix consists of three rows representing levels of task expertise. For example, a step counter (the task of noticing and counting steps) requires minimal task expertise, while recognizing cancer from a pathology image requires significant expertise. The matrix also has three columns indicating the minimum level of AI model performance needed for a user to experience the system as valuable. For example, automatic speech recognition of a voicemail only needs moderate performance to be useful, while automatic transcription of a court case would need excellent model performance to be useful.\nWhile this framework holds promise for helping teams generate more feasible Al concepts during ideation, it does not consider the ethical risks embedded in the concepts. Many ethical failures in Al stem from unrealistic problem formulations, so balancing performance requirements with feasibility presents an important area for further research. Our paper builds on this work by identifying"}, {"title": "2.2 Responsible AI and FATE Research", "content": "Fairness, Accountability, Transparency, and Ethics (FATE) in sociotechnical systems has seen significant growth over the past decade with the emergence of new research communities and conferences (e.g., FAccT and AIES). Given that these research communities were initially dominated by machine learning researchers and legal scholars, much of the early FATE research focused on formalizing specific mathematical definitions of 'fairness', along with creating algorithmic techniques that attempt to align existing datasets or AI models to comply with those definitions [100]. For instance, algorithmic fairness mitigation or 'de-biasing' techniques typically rely upon existing datasets both to correct for unfairness and bias and to assess whether such corrections have been 'successful' [21, 32, 58]. They assume flawed AI systems should be fixed while never asking what developers should choose not to build.\nHowever, recent findings reveal a paradox: when datasets are severely biased, de-biasing methods can sometimes inadvertently amplify the very biases they aim to correct [21, 32, 58]. Consequently, researchers have drawn attention to the fact that FATE concerns can be inherent to a specific problem formulation, requiring a fundamental AI system redesign rather than post hoc adjustments to models or datasets [14, 51, 85, 87]. Moreover, some applications-particularly in high-stakes public sector contexts like child welfare-pose unavoidable risks. For example, early AI innovations aimed at predicting child maltreatment risk faced criticism due to the high costs of errors, prompting a shift toward lower-risk, preventive systems [35, 62, 96, 105].\nIn sum, most RAI tools and processes for Al practitioners have largely mirrored the focus on \"making the thing right\" rather than \"making the right thing\" [17]. These efforts focus on refining existing systems or documenting their limitations, as opposed to ideating and choosing better things to make [51, 120]. Moreover, research investigating industry product teams' current practices and challenges around Al fairness, found that teams were most interested in finding ways to avoid FATE challenges in the first place [29, 51, 72]. To address this, several recent calls to action urge FATE researchers to turn their attention toward the earliest stages of the Al innovation process (e.g., [14, 51, 85, 87, 117]). Our study contributes to this dialogue by showing how a design perspective can help teams preemptively unpack risk factors, identify high-risk regions, and refocus AI practitioners on designing the right thing in the first place.\nAn emerging dialogue within the FATE community on \"AI functionality\" highlights how harms often arise when systems underperform, when they make unexpected errors. For instance, Raji et al. [87] highlight that current AI FATE discussions frequently assume systems function as intended, focusing on \"bias\" and \"fairness\" without first addressing whether the system performs adequately. Many real-world harms stem from Al systems that underperform on their given tasks, they do not achieve an acceptable level of performance. This leads to situations where Al systems not only fail to deliver their promised value but also cause harm. Currently, there is a gap in how we should systematically address this type of Al performance failure. Our paper builds on this discourse by framing the issue as an AI Mismatch, making this discrepancy our central focus.\nFATE researchers developed taxonomies of downstream algorithmic harms to help AI practitioners better understand and anticipate potential harms [7, 11, 102, 115]. Proactively anticipating harm for Al systems deployed in heterogeneous social contexts is inherently challenging because of the interplay between technologies and social and cultural dynamics [12, 102]. Here, a design approach to AI harm taxonomies can help create actionable resources that help AI practitioners systematically uncover potential sources of harm before committing to building a system. There is emerging interest in the FATE community to employ HCI and design methods to address FATE concerns at earlier stages (e.g., [67, 103, 105, 107]). However, concrete processes or actionable guidance for early-stage Al development are still limited [22, 51, 87]. Our work addresses this gap by proposing an approach that supports early AI concept analysis, systematically examining risk factors and revealing critical tradeoffs between risks and benefits."}, {"title": "3 METHOD", "content": "Our primary focus was to suggest an approach for examining early-stage Al concepts, specifically aiming to identify potential AI Mismatches. We sought to explore how these mismatches occur between (i) the performance we can realistically expect from AI models and (ii) the performance required for a given task before development begins. As an initial example of how this approach could be applied, we developed a descriptive framework that provides HCI researchers with a valuable lens for identifying and disentangling the complex factors that indicate the likelihood of harm. We envisioned how visualizing the placement of the AI concepts on our framework may help teams more systematically identify which refinements to a design concept might reduce the chance of harm, while maintaining intended benefits. This framework is a preliminary step, demonstrating how our approach can help anticipate scenarios where AI applications might be unintentionally misused."}, {"title": "3.1 Requirements", "content": "Research shows a strong link between AI failure and tasks requiring high expertise and near-perfect performance, such as recommending treatment for septic shock [124, 126]. Focusing on tasks where moderate performance still delivers value could reduce failure risks. To investigate this, we used Yildirim et al.'s Task-Expertise x AI-Performance matrix [126] to explore whether algorithmic harms are more likely in areas where tasks demand higher expertise or performance. In developing the framework, we were guided by three criteria:\n\u2022 Explanatory power: The framework should help teams articulate and analyze underlying factors in an early-stage AI concept that increase the risks that a concept will cause harm or produce limited value.\n\u2022 Analytical leverage: This framework should support teams in identifying factors that cause the AI to be fundamentally inadequate for the intended task.\n\u2022 Flexibility: The framework should be flexible enough to accommodate different types of AI applications, industry domains, and contexts."}, {"title": "3.2 Overview of the Process", "content": "We employed an iterative Research through Design (RtD) approach [128] in our study. Our process consisted of four activities. First, we collected and curated approximately 774 AI application cases. Second, we analyzed these cases using both deductive and inductive approaches. Third, we extracted and synthesized key factors contributing to the mismatch between required and feasible model performance, which led to potential harm. This process resulted in the development of seven key matrices (see Section \u00a7 5).\nFinally, both our internal research team and external researchers stress-tested these matrices against real-world counterexamples. Our team of five researchers, with expertise in HCI, design, responsible AI, machine learning, psychology, and data science, worked closely with external researchers and practitioners from academia and industry. This included feedback from approximately 20 H\u0421\u0406 and AI Fairness academic researchers, 5 data scientists, and 3 designers, ensuring that the most critical factors were thoroughly considered in practice."}, {"title": "3.2.1 Collecting Al Examples.", "content": "We began by reviewing the literature to identify various ways AI can cause harm. We then searched three primary sources for real-world AI incidents: 1) the AIAAIC (AI, Algorithmic, and Automation Incidents and Controversies) database, curated and maintained by journalism professionals, 2) the AI Incident database developed by the Partnership on AI, and 3) cases of algorithmic harms discussed in FAccT taxonomies (e.g., [87, 102, 116]). Collectively, these three sources offer a rich set of examples where Al has harmed individuals, communities, or society. We also analyzed approximately 64 widely used AI applications, such as Siri, Alexa, fraud detection in finance, and medical imaging in healthcare, as well as a set of 449 industry case studies curated by Evidently AI [36].\nGiven our focus on harms due to AI Mismatches, we excluded cases of deliberate misuse such as deepfake pornography or identity theft. Although these issues are important for understanding the broader impact of AI harm, they fall outside the scope of our study. Instead, we focused on cases where the system was used as intended but led to harm due to a fundamental mismatch between the model's required performance and achievable capabilities.\nOur initial collection yielded 478 cases. To avoid redundancy, we grouped similar incidents-those with comparable causes and outcomes-into single cases, even when they occurred in different organizations. For instance, if two cases involved the use of large language models (LLMs) to produce online articles, both leading to misinformation, we grouped them as one case. As a result, we ended up with 261 AI harm cases. Combined with Al examples outside the Al harm sources, our final set resulted in 774 AI cases."}, {"title": "3.2.2 Al Examples Analysis and Extracting Risk Factors.", "content": "To begin with, we deductively analyzed 774 AI cases using the Task-Expertise x AI-Performance matrix, rating each case based on these two criteria. Next, we inductively analyzed AI cases to unpack these underlying risk factors. Our internal team of five researchers iteratively brainstormed potential factors contributing to AI Mismatches. For example, if a case lacked critical data, we labeled it under \"quality of data.\" At this stage, our team encouraged the generation of as many potential labels as possible for each case, deliberately avoiding premature convergence on a single label to minimize the risk of overlooking any potential causes. Any disagreements regarding specific labels were informally discussed and documented, but final decisions were deferred to the subsequent step of synthesizing these labels. This process was iteratively discussed and cross-checked to ensure that the identified causes accurately represented the reasons for failure. These brainstormed codes were transferred to Post-it notes, ready to be categorized and synthesized."}, {"title": "3.2.3 Synthesizing Risk Factors.", "content": "From the initial set of post-it notes with AI Mismatch factors, we grouped and synthesized the factors that contributed to AI harms, separating them from related but distinct factors. The two co-first authors conducted an initial round of categorizing the factors into emerging themes. When conflicting factors were grouped within a single concept, they carefully deliberated to identify the most appropriate ones to include. Following this initial categorization, the internal research team (n=5) engaged in a series of weekly discussions over 12 months. Between meetings, team members shared opinions, concerns, and suggestions asynchronously using collaborative web applications. These comments were consolidated by the first authors and incorporated into updated drafts for discussion in subsequent meetings. Throughout this iterative process, some factors were decoupled or synthesized to strike a balance between achieving a suitable level of abstraction and ensuring conceptually distinct categories. For instance, we decoupled users' expectations of errors from the need for error detection and mitigation. Some Al examples, like creative AI tools or smartwatch step counters, showed that users expect errors but have a higher tolerance for them. These users are less concerned with accuracy and more interested in patterns or inspiration. On the other hand, in cases like AI for financial fraud detection, users expect errors but also require accurate error detection and mitigation. This highlighted that, beyond error expectation, the tolerance for error in Al output is a distinct factor. This distinct factor was documented and added to our set of post-it notes.\nWe then explored additional risk factors that could influence the initial risk factor, placing these on the horizontal and vertical axes of a matrix to identify high-risk areas. For instance, the severity of errors correlates with the prevalence of errors: a higher prevalence of low-severity errors could be acceptable (e.g., language translation), whereas high-severity errors should be rare (e.g., industrial automation). An AI system with frequent severe errors is problematic. Similarly, we recognized that ease of mitigation is related to ease of detection for high-severity errors; errors that are hard to detect and mitigate would render the AI concept infeasible (e.g., LLMs for medical diagnosis [44]). The interplay between these risk factors - severity, expectation, detection, and mitigation of errors indicates the need to systematically unpack them. Drawing upon the examples of creative AI tools and the smartwatch step counter, since the severity of individual errors is negligible and users expect these errors to occur with minimal impact on value creation, it is not necessary to be able to detect these errors or mitigate them."}, {"title": "4 OVERVIEW OF THE MATRICES", "content": "Our synthesis identified seven key factors that impact AI Mismatches: (i) Data Quality, (ii) Model Unobservables, (iii) Expected Performance, (iv) Cost of Errors, (v) Disparate Performance, (vi) Expectation of Errors, and (vii) Error Detection and Mitigation. To help communicate, we visualized the factors as 3x3 matrices with two axes that represent different variables of interest, focusing on areas of potential risk. Table 1 offers a comprehensive summary, including titles, explanations, axes, and section references for further exploration. While Section \u00a7 5 provides detailed visualizations and explanations for each matrix, this section provides a single, integrated view of the framework and explains how the matrices connect, inform one another, and guide decision-making.\nAmong seven AI Mismatch matrices, the framework consists of three high-level matrices aimed at helping teams reflect on potential mismatches between expected model performance and expected real-world value and risks. These matrices help determine whether a concept's expected performance is sufficient to deliver the intended value (i.e., Required Performance Matrix), whether there are expected performance disparities that matter (i.e., Disparate Performance Matrix), and whether its expected performance is sufficient to minimize harm due to errors (i.e., Cost of Errors Matrix). In addition to these three core matrices, we present four examples of lower-level matrices that can help inform these higher-level judgments, such as assessments of expected performance or expected consequences of errors. Figure 1 presents a simplified diagram illustrating how these matrices interconnect and support one another in guiding decision-making. The left side of the figure presents three motivating questions that lead to the three core matrices. On the right, four supporting lower-level matrices with two prompting questions, inform one axis of the core matrices. Additional details are provided in a step-by-step workflow (Section \u00a7 A) and a one-page flyer (Section \u00a7 B) in the appendix."}, {"title": "4.1 Performance Mismatch Matrices", "content": "These are the core matrices that highlight the critical high-level judgments teams must make.\nRequired Performance Matrix (Expected vs. Required Performance) - This matrix asks, \"Is the expected model performance sufficient to deliver the intended value?\" It encourages teams to move beyond technical metrics to consider what is the minimum level of performance required to create actual value for people or organizations. Then it asks teams to reflect on what level of performance is likely to be achievable in practice, to anticipate potential performance mismatches.\nDisparate Performance Matrix (Expected Disparity in Performance vs. Importance of Avoiding Disparities). This matrix introduces an equity lens and asks, \u201cCan we ensure equitable performance where it matters?\" Even if a model meets minimum performance for most users, are there significant performance gaps across different demographic or user groups? How critical is it for a given AI concept to ensure equitable outcomes?\nCost of Errors Matrix (Expected Performance vs. Severity of Error Consequences) - This matrix complements the Required Performance matrix by looking beyond benefits and asking, \"Is the expected model performance sufficient to minimize harm due to errors?\". It is not enough for a model to perform well on average; we must understand what happens when it fails. Are these failures merely inconvenient, or could they cause real harm?\nThese three matrices address technical feasibility, practical value, risk tolerance, and equity considerations to support reflection on potential mismatches in AI concepts. They offer high-level checks that help practitioners understand whether the AI concept meets the thresholds of real-world value and acceptable risk."}, {"title": "4.2 Examples of Lower-Level, Supporting Matrices", "content": "Below, we present examples of lower-level matrices that can help support the high-level judgments required by the core matrices (e.g., assessments of expected performance)."}, {"title": "4.2.1 Scaffolding Reflection on Expected Performance.", "content": "The Data Quality Matrix asks teams to reflect upon the extent to which limitations of the underlying data may impact model performance in ways that matter. As a second example, the Model Unobservables Matrix encourages reflection on what information the model may be missing, and the implications of these information gaps for model performance. Taken together, these two matrices support reflection on what level of performance can be realistically expected."}, {"title": "4.2.2 Scaffolding Reflection on the Severity of Error Consequences.", "content": "The Expectation of Errors Matrix encourages reflection on user expectations, which can amplify or reduce the perceived severity of errors. If users are likely to expect errors, the impact might feel less severe; if they expect near-perfection, even mild errors can be perceived as catastrophic. As a second example, the Error Detection & Mitigation Matrix draws attention to the fact that knowing how easily errors can be detected and fixed can change how severe they feel operationally. On the other hand, difficult-to-detect errors, even if rare, have the potential to lead to more severe consequences. Taken together, these two matrices can help support reflection on an Al concept's potential for real-world harm that may be due to errors.\nIn summary, Required Performance, Cost of Errors, and Disparate Performance Matrices are the central \"decision dashboards\" for high-level viability judgments, while lower-level matrices like the examples above may act as diagnostic or \"drill down\" tools and support the use of core matrices. This layered approach may allow teams to move from a broad, human-centered perspective (i.e., core matrices) down to the root causes and potential opportunities for idea refinement (supporting matrices)."}, {"title": "5 AI MISMATCH MATRICES", "content": "The previous section (Section \u00a7 4) provided an overview of the seven matrices within the AI Mismatch Matrices framework, highlighting their relationships and interdependencies. Building on that foundation, this section delves deeper into each matrix, offering a detailed explanation of their structure and purpose. These matrices are: (i) Data Quality, (ii) Model Unobservables, (iii) Expected Performance, (iv) Cost of Errors, (v) Disparate Performance, (vi) Expectation of Errors, and (vii) Error Detection and Mitigation. The AI Mismatch approach illustrates how conceptually flawed AI systems can inadvertently cause harm. Our framework uses 3x3 matrices to visualize and highlight these areas of risk. Each matrix is structured with two axes, representing different and often opposing variables of interest, to clarify the underlying tensions or trade-offs. This detailed exploration aims to illuminate how each matrix contributes to identifying and addressing potential mismatches in AI systems.\nThe matrices use a color gradient to indicate risk levels: red represents the highest risk, pink portrays an intermediate risk, yellow indicates moderate risk, and uncolored areas represent the low risk. Red zones highlight significant gap between required and actual performance, posing substantial risk. We encourage teams to aim for Al concepts in low-risk, uncolored zones wherever possible. However, if work in higher-risk zones is unavoidable, careful consideration should be given to mitigating potential negative impacts.\nThese matrices are meant to foster interdisciplinary collaboration, bringing together teams with technical, ethical, and design expertise to make informed judgments about Al concepts. By collectively assessing where an Al concept falls within the matrix, teams can identify and discuss potential concerns or uncertainties early in the development process. This initial assessment is a dynamic process, evolving over time, and aims to highlight AI Mismatches early, allowing teams to address issues before development advances. The hypothetical nature of this process highlights the importance of interdisciplinary expertise in providing comprehensive evaluations and ensuring AI Mismatches are detected as early as possible."}, {"title": "5.1 Performance Mismatch Matrices", "content": "In the following subsections, we provide a walkthrough of our three core matrices."}, {"title": "5.1.1 Required Performance Matrix.", "content": "The Required Performance matrix draws attention to the contrast between the AI model's expected performance under current constraints (i.e., data quality and model unobservables) and the minimum performance needed to create value. Figure 2 represents the matrix as a 3x3 grid. The Expected Model Performance is plotted on the X-axis, and the Minimum Required Performance is on the Y-axis. Minimum Required Performance to Create Value defines the baseline at which the AI must operate to be effective, avoid harm, and provide value. For example, AI used to help content moderation on social media typically requires moderate performance. False positives or missed content can be reviewed by human moderators. However, in K-12 online learning, excellent performance is essential to protect children from harmful content. Small errors that expose children at school to harmful content can harm their well-being and damage the school's reputation. Expected Model Performance reflects how well the AI is expected to perform given current technological constraints. For instance, GitHub Copilot's coding assistance performs well due to the structured nature of code, while mobile device autocorrect performs only moderately well due to the complexity of language and the messiness of texting communication.\nWhen the expected model performance on a given task is lower than the required performance, it raises an important contradiction that makes the AI concept infeasible. For instance, facial scanning for biometric security on personal devices performs exceptionally well because the task is straightforward: matching a face to saved data, similar to showing an ID to a bartender. In contrast, facial recognition for surveillance is much more complex, involving unobserved factors (e.g., lighting, weather, inconsistent angles of image capture) that lead to a higher error rate and lower model performance. It is akin to asking a bartender to identify a person from numerous profile pictures. Consequently, facial recognition used by police departments has led to wrongful arrests and caused significant harm to communities [16].\nRationale. Through our case studies, we decoupled the goals of AI applications from their actual model performance, uncovering the discrepancy highlighted by this matrix. In the deductive analysis, we observed that many AI harm cases required high performance to create value. However, when we inductively analyzed the causes of harm, we found that these AI systems often only achieved moderate or good performance. This performance gap is often due to factors like poor data quality, unrealistic problem formulation, or missing unobserved factors that are critical to task success."}, {"title": "5.1.2 Disparate Performance Matrix.", "content": "The Disparate Performance matrix highlights performance disparities and their ethical implications. Figure 3 represents the matrix as a 3x3 grid. The Expected Disparity in Performance is plotted on the X-axis, and the Importance of Avoiding Disparities is on the Y-axis. Expected Disparity in Performance refers to how well AI models perform across different groups (e.g., racial, socio-economic, gender). High disparity means the Al is expected to perform significantly better for some groups than others, often due to biased data or structural issues. Low disparity indicates that performance is consistent across groups, with minimal variation, typically achieved through balanced or bias-reducing training data. For instance, facial recognition AI may exhibit low disparity in controlled settings like office security, but disparities increase in public spaces like airports, where environmental variability impacts performance. Importance of Avoiding Disparities assesses how critical it is to avoid performance gaps, based on the ethical, social, or financial consequences of unequal outcomes. In less critical contexts, disparities might be considered \"not very important,\" leading only to minor user dissatisfaction. However, in high-stakes settings, avoiding disparities becomes crucial, as unequal Al performance can reinforce bias and discrimination, raising significant ethical and legal concerns.\nThis matrix is crucial in early AI development, helping teams assess and mitigate performance disparities. For instance, in language translation (e.g., Google Translate), disparate performance is often anticipated when translating between less commonly used languages due to the limited availability of parallel text for training [99", "104": ".", "82": "leading to a high ethical cost of disparate performance. This further helps assess the effort needed to prevent or mitigate disparities, and whether that effort creates enough value that outweighs the risks. For instance, Holstein et al. [51", "medical professionals": "nd avoid using 'doctors' and 'nurses' as labels; a simple redesign that mitigates disparate classification.\nRationale. Disparate performance from data imbalances was observed in Al systems such as facial recognition and speech recognition, where some demographic groups might be underrepresented in the training data. Disparate performance due to differing base rates for different demographic groups means that"}]}