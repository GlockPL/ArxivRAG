{"title": "Saarthi: The First AI Formal Verification Engineer", "authors": ["Aman Kumar", "Deepak Narayan Gadde", "Keerthan Kopparam Radhakrishna", "Djones Lettnin"], "abstract": "Recently, Devin has made a significant buzz in the Artificial Intelligence (AI) community as the world's first fully autonomous AI software engineer, capable of independently developing software code [1] [2]. Devin uses the concept of agentic workflow in Generative AI (GenAI), which empowers AI agents to engage in a more dynamic, iterative, and self-reflective process. In this paper, we present a similar fully autonomous AI formal verification engineer, Saarthi, capable of verifying a given RTL design end-to-end using an agentic workflow. With Saarthi, verification engineers can focus on more complex problems, and verification teams can strive for more ambitious goals. The domain-agnostic implementation of Saarthi makes it scalable for use across various domains such as RTL design, UVM-based verification, and others.", "sections": [{"title": "I. INTRODUCTION", "content": "Hardware design verification, especially formal verification, entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. Formal verification uses mathematical methods to prove the correctness of hardware designs against their specifications, ensuring that all possible states and inputs are considered, which complements traditional simulation-based verification techniques that might only cover a subset of possible scenarios due to practical constraints [3]. The formal verification process encompasses several key roles, such as organizational coordination, task allocation, code development, property proving, analyzing Counter Examples (CEXs), debugging, coverage closure, and documentation preparation. These roles are crucial for managing the complexity and ensuring the thoroughness of the verification process. For instance, analyzing counterexamples involves identifying specific scenarios where the design might fail to meet its specifications, which is critical for debugging and refining the design. This highly intricate activity demands meticulous attention to detail, given its long development cycles and the critical nature of ensuring hardware functionality and reliability [4].\nThe field of Natural Language Processing (NLP) has undergone a significant transformation with the advent of Large Language Models (LLMs) [5]. These powerful models, often referred to as GenAI, have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications [6]. Through extensive training on large datasets using the \u201cnext word prediction\" approach, LLMs have demonstrated remarkable capabilities in various downstream tasks such as context-sensitive question answering, machine translation, and code generation [7]. Interestingly, the primary components of formal verification \u2013 specifically code (assertions as properties) and specification documents can be considered as forms of \"language\" or sequences of characters [8]. Various surveys [9] have discussed techniques for improving conventional formal verification; however, we aim to enhance it further using AI. This paper introduces an end-to-end formal verification process driven by LLMs. This process encompasses design specification analysis, code development, verification, and document generation, aiming to establish a unified, efficient, and cost-effective paradigm for hardware design verification. By leveraging the advanced capabilities of LLMs, it is possible to streamline the formal verification process, enhancing both accuracy and productivity.\nLike every other semiconductor company, we wanted to investigate the possibilities of using GenAI for dedicated use cases. However, there are known challenges that prevent precise use case definitions [10]. GenAI operates as a stochastic process, meaning it generates non-deterministic output with each regeneration. This is in contrast to the requirements of hardware design verification, which demands precise, deterministic answers, particularly in formal verification where engineers need to make clear pass/fail decisions based on exact criteria. Due to this fundamental mismatch, the non-deterministic output of LLMs is not always suitable for hardware design verification. Additionally, LLMs can suffer from artificial Attention Deficit Hyperactivity Disorder (ADHD), characterized by a tendency to lose focus on the task at hand, and hallucination, where the model generates incorrect or nonsensical information confidently [11]. These issues are very prominent and can lead Generative Pre-trained Transformer (GPT) users to get stuck in iterative loops, repeatedly seeking accurate results without success. Given these challenges, the current state of GenAI may not be well-suited for applications that require the high precision and determinism essential in hardware design verification."}, {"title": "II. BACKGROUND", "content": "Recent work in [11] found that LLMs tend to generate incorrect designs and are vulnerable to security flaws as the authors observed around 60% failure rate for the generated RTL designs. Our findings indicated that the expectations of authors from AI were too high in terms of achieving first-time-right designs. Additionally, their approach of benchmarking the LLM-generated designs may have been overly pessimistic, possibly not accounting for the iterative improvement potential of these models. Moving forward, we want to focus on using GenAI as a problem-solving tool and to use the existing capabilities of LLMs to generate better results. There are two types of AI-based workflows:\n\u2022 Non-agentic workflow (zero-shot)\n\u2022 Agentic workflow (few-shot)"}, {"title": "A. Non-Agentic Workflow", "content": "The first productive uses of LLMs involved non-agentic workflows, where we type a prompt and the model generates an answer in one go. This is akin to asking a person to write an essay on a topic and saying \"please sit down to the keyboard and type the essay from start to finish without ever using backspace\". Despite how hard this is, LLMs do it remarkably well; however, the quality of the generated content is often relatively lower due to the lack of iterative refinement. This approach is termed a zero-shot or non-agentic workflow. In a zero-shot scenario, the model attempts to generate a response without any prior specific examples or iterations tailored to the task at hand.\nIn the ReFormAI dataset paper [11], the authors used a similar non-agentic workflow and benchmarked the LLM generated RTL codes that resulted in a relatively higher failure rate. The results suggested that the failure rate was significant due to the one-pass, zero-shot nature of the generation process. The results would likely have been better if a feedback loop had been added to the generation part. A feedback loop would allow for iterative refinement, where the model could receive feedback on its initial outputs and make adjustments to improve accuracy and quality. This approach would enable the LLM to correct errors, incorporate additional context, and ultimately produce higher-quality RTL designs."}, {"title": "B. Agentic Workflow", "content": "In contrast to the zero-shot workflow, the agentic or few-shot workflow uses iterative loops and feedback to produce better results. This approach is very similar to how humans think and approach a given task. For the task of writing an essay, a human would typically start by outlining the essay on topic X, conducting web research, preparing a first draft, considering what parts of the essay need revision, revising the draft, and finally producing the final version. Similarly, if LLMs employ this iterative approach to address a prompt, they deliver remarkably better results. In a few-shot workflow, the model is initially provided with a few examples to guide its responses. As it generates outputs, it receives feedback, which it uses to refine and improve its responses iteratively. This process allows for error correction and the incorporation of additional context, leading to higher quality and more accurate results compared to the zero-shot approach.\nBased on open-source benchmarks, researchers found that using OpenAI models [12] such as GPT-3.5 with zero-shot prompting, the LLM yields 48% correct results. With GPT-4, this accuracy improves to 67%. However, when using an"}, {"title": "C. Reflection", "content": "Reflection uses the concept of a coder agent and a critic agent. For any given task, there would be a coder agent that generates code-in our case, SystemVerilog Assertion (SVA) \u2013 and a critic agent that critically analyzes and reviews the output of the coder agent, providing feedback. This feedback loop is iterative, allowing the coder agent to refine its code based on the critic agent's insights, leading to progressively better results.\nFig. 3 shows a case where a human asked the coder AI agent to write SVA code for a given specification. Once the SVA is generated, the critic agent analyzes the code and provides feedback, identifying a bug in line X. The coder agent then uses this feedback to fix the code and generate version 1 (v1) of the SVA. Next, the critic agent attempts to compile the generated code using a formal verification tool and reports a compilation issue, including the error message from the tool, to the coder agent. The coder agent analyzes this feedback and produces a corrected SVA as version 2 (v2). Using this iterative approach, the human is able to obtain a significantly better SVA by leveraging the capabilities of existing LLMs. The human's role includes initiating the process, reviewing the iterations, and making use of the final, refined SVA code.\nLLM agents are increasingly being used to interact with external environments as goal-driven agents. However, these language agents face difficulties in rapidly and effectively learning through trial-and-error, since conventional reinforcement learning techniques necessitate a large number of training samples and expensive model fine-tuning. The authors in [15] propose a novel framework, Reflexion, that uses verbal reinforcement to help agents learn from previous failures. Creating valuable reflective feedback is difficult because it involves accurately identifying where the model went wrong (known as the credit assignment problem [21]) and being able to produce a summary that offers actionable recommendations for improvement. The authors also propose several mitigation techniques such as simple binary environment feedback, pre-defined heuristics for"}, {"title": "D. Tool Use", "content": "Tool use is another agentic reasoning design pattern that leverages AI-based tools to generate meaningful results. Web search tools such as Copilot, which assists with coding by providing intelligent code suggestions, and code execution tools such as [22] and [17], which help automate and streamline the coding process, significantly increase productivity gains. These tools enable users to accomplish tasks more efficiently by harnessing the power of AI to provide relevant information, automate repetitive processes, and enhance overall workflow. Some examples of areas of application for tool use are mentioned in Table II."}, {"title": "E. Planning", "content": "Planning is one of the key workflows to generate quality output from LLMs using agentic reasoning. Even though LLMs are quite good at generating responses for a given prompt, they are often criticized for their lack of commonsense reasoning. This deficiency can impact the accuracy and reliability of the generated responses. Fig. 7 illustrates this issue through a standard prompting example where the model's output is incorrect for a simple reasoning question. By incorporating planning into the workflow, LLMs can be guided to consider broader context and logical steps, which helps mitigate the lack of commonsense reasoning and leads to higher quality outputs.\nThe authors in [18] explore generating a Chain-of-Thought (CoT) \u2013 a series of intermediate reasoning steps that enable LLMS to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. CoT, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps. For many reasoning tasks where standard prompting has a flat scaling curve, CoT prompting leads to dramatically increasing scaling curves. An example of CoT prompting is shown in Fig. 7 that elicits reasoning in LLMs.\nAlthough CoT emulates the thought process of human reasoners, this does not necessarily indicate that the neural network is actually \"reasoning\". CoT typically involves few-shot prompting, where the model is provided with a few examples to guide its responses. This approach can be expensive, especially when using paid LLMs. In contrast, using zero-shot prompting with a more generalized prompt could be more cost-effective. Furthermore, there is no guarantee that CoT will follow correct reasoning paths, which can lead to both correct and incorrect answers. The variability and uncertainty in the reasoning process mean that while CoT can help generate more logically structured responses, it can also propagate errors if the initial reasoning path is flawed."}, {"title": "F. Multi-Agent Collaboration", "content": "Al agents can collaborate to solve tasks given by a human. These agents can leverage several LLMs to handle different responsibilities within a complex task. The authors in [7] introduce communicative agents for software development, which are designed to interact and share information to improve task-solving efficiency, and present an open-source alternative to Devin [1]. Research done in [24] supports the notion that a multi-agent system performs better than a single agent when solving complex tasks. Table III summarizes the results of a multi-agent debate for different tasks, such as the Massive Multitask Language Understanding (MMLU) benchmark and chess moves, demonstrating the improved performance of multi-agent systems in these diverse scenarios.\nA classic example of multi-agent collaboration is depicted in Fig. 3, where the coder and critic agents work together to solve a given task that includes reasoning and feedback. In this scenario, the coder agent generates the code, while the critic"}, {"title": "III. SAARTHI: AGENTIC AI-BASED FORMAL VERIFICATION", "content": "To realize our contributions and conduct our experiments, we prepared a flow as illustrated in Fig. 9 where AI agents are in the driver's seat as soon as a task is given to solve. Saarthi is designed to facilitate formal verification through a sophisticated agentic AI-based approach that leverages multi-agent collaboration. Saarthi integrates several design patterns, including agentic reasoning and techniques to mitigate issues such as attention deficits, hallucinations, and repetitive looping. It is built using"}, {"title": "A. Flow Architecture", "content": "The AutoGen implementation features a sequential processing pipeline that manages inter-agent communication through structured message-passing protocols. The system implements comprehensive logging mechanisms that capture detailed interactions and logs. The CrewAI implementation utilizes decorator patterns for task definition, implementing a structured approach to workflow management. The system incorporates file-based tool integration mechanisms and implements a comprehensive logging system that captures execution details at multiple granularity levels.\nThe framework also implements an error management system incorporating multiple feedback loops for continuous improvement. The system utilizes critic agents that perform property evaluations, providing detailed feedback for assertion improvement. The iteration control system uses threshold-based monitoring to prevent infinite loops, automatically triggering human intervention when resolution cannot be achieved autonomously. The error-handling system implements structured exception management through try-catch hierarchies. Saarthi also generates verification artifacts through template engines and formatting systems. The logging system implements timestamp-based organization and multi-level capture, ensuring complete traceability of the verification process. Fig. 10 shows an example usage of Saarthi to accomplish a task using multi-agent collaboration.\nFig. 9 is the high-level overview of the flow we have defined for formal verification where AI agents are in the driver's seat as soon as a task is given to solve. Every block in the flow chart is executed by an LLM, except the first, the design specification that a human provides. The first LLM agent is the so-called \"formal verification lead\" who is responsible for generating a Verification Plan (vPlan) (i.e., a list of the properties necessary to verify the given design written in the natural English language) based on the given specification. The verification lead divides the follow-up tasks to other LLM agents. An example of such an agent in the AutoGen framework with its role, goal and backstory is highlighted in Listing 1. The subsequent steps in the flow involve several LLMs acting as formal verification engineers to analyze the vPlan and generate SVA for each corresponding element. An example of the tasks defined in the AutoGen framework for vPlan generation and SVA generation is highlighted in Listing 2. The generated properties are evaluated for correctness by the critic agents and feedback is provided to improve SVA. This iterative process continues until a threshold is reached without a conclusion on the SVA. This is when human intervention is required to decide the correct SVA and continue the overall process. The generated properties are then proven in a formal tool, and the CEXs are analyzed if any and fixed by the LLM agents. Once all properties are proven, another LLM agent takes over to analyze the formal coverage, an important verification sign-off criteria. Based on missing coverage, feedback is provided to the formal verification lead to add missing properties."}, {"title": "B. Agent Orchestration", "content": "Once the agents are configured, they are passed as keys to an orchestration setup. This orchestration mechanism is capable of managing agents in both a sequential and hierarchical manner. For this formal verification process, the agents are arranged in a sequential order. After orchestration, the selected framework's main module initiates the verification process, invoking the agents sequentially.\nDuring this process, the agents generate key artifacts such as vPlan and properties, logging their interactions and collaborations as they proceed. The generated properties are also subjected to evaluation by critic agents, who provide feedback to improve the quality and correctness of SVAs. This iterative process continues until a stable threshold is achieved. If the agents are unable to finalize the SVAs within a predefined maximum number of iterations, human intervention is triggered for further assessment. Once the SVAs are finalized, they undergo formal verification, with any CEXs identified and resolved by the agents."}, {"title": "IV. BENCHMARKING AND RESULTS", "content": "To evaluate the performance and benchmark its capabilities, we used Saarthi to verify RTL designs of varied complexity. Tables IV, V and VI underline the performance of Saarthi on basic, intermediate, and advanced design complexity levels. We chose three Key Performance Indicators (KPIs) to benchmark the results with different LLMs. The first is \"success rate\" which determines the number of successful runs (i.e., end-to-end formal verification) out of the total runs. The second KPI is the coverage rate (formal coverage after end-to-end formal verification). The third KPI indicates the pass rate of the assertions generated from Saarthi with each LLM. The results are highlighted in Fig. 13.\nSaarthi performs the best with the GPT-40 model and the worst with the Llama3-70B model. GPT-40 has a consistent proof rate and the most consistent coverage metrics. Even though the proven rate of the assertions is lower for GPT-40, it yields a higher overall coverage. GPT-4-Turbo has the highest proven rate variability (i.e., less consistent results with varied design complexity). It has a higher average assertion proven rate but lower overall coverage. Llama3-70B has consistently lower success rates with the highest number of attempts needed to complete the overall end-to-end formal verification. It is worth noting that this model often generates more assertions per run compared to the other two.\nLLMs have context length and input token restrictions that can lead to truncated critical information in large RTL designs, resulting in inaccurate assertions. The quality of LLM-generated outputs depends on the prompts, yet models may still deviate from guidelines, producing incorrect assertions. Selecting the optimal temperature parameter for assertions varies across designs; incorrect settings cause overly deterministic or random outputs. Robust models like GPT-40 can err by introducing implicit"}, {"title": "V. CONCLUSION", "content": "The paper outlines a fully autonomous AI-based formal verification engineer, Saarthi. Saarthi understands design specifications, creates a verification plan, assigns tasks to several AI verification engineers, and communicates with formal verification tools such as Cadence Jasper to prove properties. It also analyzes CEXs, assesses formal coverage, and reacts to improve it by adding missing properties for the final sign-off. Although the results for end-to-end formal verification do not yield a 100% guarantee with every run, Saarthi performs significantly well most of the time, with an overall efficacy of around 40%. The quality of the results also depends on the LLM used, with GPT-40 outperforming other models. As predicted in [27] and [28], achieving Artificial General Intelligence (AGI) by 2027 is strikingly plausible, and we believe that Saarthi could be pivotal in reaching that milestone within the hardware design verification domain. Saarthi is based on the agentic workflow, which makes it highly scalable and adaptable to other domains such as RTL design, UVM testbench generation, and more. To expand its capabilities, we need to define the agents and their responsibilities to ensure they work together to solve given tasks. In the future, we will continue our experiments to extend Saarthi's applications beyond formal verification, exploring its potential in additional usecases."}]}