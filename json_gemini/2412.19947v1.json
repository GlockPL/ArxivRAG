{"title": "Standard-Deviation-Inspired Regularization for Improving Adversarial Robustness", "authors": ["Olukorede Fakorede", "Modeste Atsague", "Jin Tian"], "abstract": "Adversarial Training (AT) has been demonstrated to improve the robustness of deep neural networks (DNNs) to adversarial attacks. AT is a min-max optimization procedure wherein adversarial examples are generated to train a robust DNN. The inner maximization step of AT maximizes the losses of inputs w.r.t their actual classes. The outer minimization involves minimizing the losses on the adversarial examples obtained from the inner maximization. This work proposes a standard-deviation-inspired (SDI) regularization term for improving adversarial robustness and generalization. We argue that the inner maximization is akin to minimizing a modified standard deviation of a model's output probabilities. Moreover, we argue that maximizing the modified standard deviation measure may complement the outer minimization of the AT framework. To corroborate our argument, we experimentally show that the SDI measure may be utilized to craft adversarial examples. Furthermore, we show that combining the proposed SDI regularization term with existing AT variants improves the robustness of DNNs to stronger attacks (e.g., CW and Auto-attack) and improves robust generalization.", "sections": [{"title": "INTRODUCTION", "content": "The vulnerability of deep neural networks (DNNs) to adversarial perturbations is well documented in machine learning literature (Goodfellow et al., 2014; Moosavi-Dezfooli et al., 2016; Papernot et al., 2016; Szegedy et al., 2013), prompting concerns about the deployment of DNNs into safety-critical domains. Hence, for the safe deployment of DNNs, improving their robustness to adversarial perturbations is imperative.\nAdversarial Training (AT) Goodfellow et al. (2014); Madry et al. (2018) has been demonstrated to be effective in improving the robustness of DNNs to adversarial attacks. AT is a min-max optimization procedure, where the inner maximization step corresponds to finding adversarial examples in the direction of worst-case loss. The outer minimization minimizes the loss on the crafted adversarial examples. The success of AT in improving the robustness of DNNs to adversarial perturbations has inspired a myriad of variants that have yielded better robustness or computational efficiency (Zhang et al., 2019; Wang et al., 2019; Li et al., 2019; Andriushchenko & Flammarion, 2020; Wong et al., 2020; Shafahi et al., 2019b). Furthermore, recent works have employed methods such as adversarial weight perturbation Wu et al. (2020), integration of hypersphere embedding into AT (Pang et al., 2020; Fakorede et al., 2023a), and loss re-weighting Zhang et al. (2020);\nLiu et al. (2021); Fakorede et al. (2023b); Zhang et al. (2023) to improve the performance of existing AT variants.\nIn this work, we delve into a standard-deviation-inspired (SDI) measure proposed in a recent study in (Fakorede et al., 2024) for estimating the vulnerability of an input example. Drawing inspiration from the concept of standard deviation, which quantifies the dispersion of data points around the mean of a distribution, the SDI measure aims to capture the dispersion of logits associated with incorrect classes in relation to the logits corresponding to the true class.\nWe draw a parallel between the inner maximization step of the AT process and minimizing the SDI loss function. Moreover, we argue that the outer minimization step of the AT process, which seeks model param-eters minimizing the loss on adversarial examples, is conceptually similar to maximizing the SDI measure. Both endeavors aim to enhance the likelihood of correctly classifying individual adversarial examples.\nUnlike prevalent information-theoretic losses utilized in min-max AT optimization, the SDI measure operates independently from concepts like cross-entropy, entropy, or Kullback-Leibler divergence. Consequently, integrating the SDI measure into existing AT variants could complement information-theory-inspired losses and potentially enhance the performance of these variants. Therefore, we propose adding the SDI loss as a regularization term to prominent AT variants such as the standard AT (Madry et al., 2018) and TRADES (Zhang et al., 2019). The proposed regularization term is applied to outer minimization of the respective AT variants to maximize the SDI measure.\nOur contributions are summarized as follows:\n1.  We propose utilizing the SDI measure as a regularization term to existing AT variants. Our exten-sive experiments show that our proposed approach further improves the robustness of existing ATvariants on strong attacks Auto attacks and CW attack and strong query-based black-box attackSPSA.\n2.  We experimentally show that the proposed SDI regularization on existing AT variants improves thegeneralization to other attacks not seen during adversarial training.\n3.  In addition, we establish a connection between minimizing the SDI measure and the inner maximiza-tion of the min-max AT procedure. Specifically, we experimentally show that adversarial examplesmay be obtained from adversarial perturbations that minimize the SDI metric. Furthermore, wecompare the success rates of adversarial examples obtained using the SDI metric with those obtainedusing cross-entropy loss and KL divergence on adversarial trained models."}, {"title": "BACKGROUND AND RELATED WORK", "content": null}, {"title": "\u039d\u039f\u03a4ATION", "content": "We use bold letters to represent vectors. We denote D = {xi, Yi}=1 a data set of input feature vectors x \u2208 XC Rd and labels yi \u2208 Y, where X and Y represent a feature space and a label set, respectively.\nLet fo : X \u2192 RC denote a deep neural network (DNN) classifier with parameters 0, and |C| represents the number of output classes. For any x \u2208 X, let the class label predicted by fo be Fo(x) = arg maxk fo(x)k, where fo(x)k denotes the k-th component of fo(x). fo(x)y is the probability of x having label y.\nWe denote || || as the lp- norm over Rd, that is, for a vector x \u2208 Rd, $||x||_p = (\\sum_{i=1}^{d} |x_i|^p)^{\\frac{1}{p}}$. An \u03f5-neighborhood for x is defined as B\u025b(x) : {x' \u2208 X : ||x\u2032 \u2212 x||p \u2264 6}. An adversarial example corresponding to a natural input x is denoted as x'. We often refer to the loss resulting from the adversarial attack (inner maximization) as adversarial loss."}, {"title": "ADVERSARIAL ROBUSTNESS", "content": "Adversarial robustness is a machine learning model's capability to resist adversarial attacks. Over the past years, many methods (Guo et al., 2018; Buckman et al., 2018; Dhillon et al., 2018; Madry et al., 2018;\nGoodfellow et al., 2014; Zhang et al., 2019) have been proposed to improve adversarial robustness of neural networks. However, some of these defenses have been shown to provide a false sense of defense because they intentionally or inadvertently used obfuscated gradients in their defenses (Athalye et al., 2018).\nIn a seminal work, Madry et al. (2018) proposed Adversarial training (AT), which involves training the model with adversarial examples obtained under worst-case loss to improve robustness. Formally, AT involves solving a min-max optimization as follows:\n$\\min_\\theta E_{(x,y)\\sim D} \\max_{x' \\in B_{\\epsilon}(x)} L(f_\\theta(x'), y)$  (1)\nwhere L() represents the loss function, y is the true label of input feature x, and e are the model parameters. The inner maximization in Eq. (1) aims to obtain a worst-case adversarial version of the input x that increases the loss. The outer minimization then tries to find model parameters that would minimize this worst-case adversarial loss. The efficacy of AT has spurred the development of numerous variants (Zhang et al., 2019; Wang et al., 2019; Wu et al., 2020; Pang et al., 2020).\nA prominent variant TRADES Zhang et al. (2019) proposed a principled regularization term that trades off adversarial robustness against natural accuracy. Wang et al. (2019) proposed MART, an AT variant that differentiates between naturally misclassified examples that are used in the inner maximization of the AT process, using this information to improve adversarial robustness. Wu et al. (2020) proposed adversarial weight perturbation, a double perturbation mechanism that employs the perturbation of inputs and weights to improve adversarial robustness. More recent AT methods improve existing AT variants by employing reweighting (Zhang et al., 2021; Liu et al., 2021; Fakorede et al., 2023b) or incorporating hypersphere embedding (Pang et al., 2020; Fakorede et al., 2023a).\nThe adversarial examples obtained in the inner maximization step of adversarial training methods are typi-cally crafted using the projected gradient descent (PGD), maximizing the probability estimates of incorrect classes at the expense of the ground truth. Training on these specific adversarial types often leads to models performing well on the PGD adversarial attacks, on which the models are trained but generalizing relatively poorly to others. To address this, we propose a standard-deviation-inspired regularization term that explicitly maximizes the probability gap between incorrect classes and the ground truth while boosting the ground-truth probability. This proposed regularization operates directly on the model output logits, categorizing it as a form of logit regularization.\nMost existing logit regularization variants (Mosbach et al., 2018; Kannan et al., 2018; Shafahi et al., 2019b; Summers & Dinneen, 2019; Kanai et al., 2021) involve utilizing techniques such as label smoothing and logit squeezing for improving adversarial robustness. These methods typically encourage smaller logit norms before softmax, which studies such as Shafahi et al. (2019b;a) associate with reduced overconfidence in predictions and improved adversarial robustness. However, the robustness achieved through these logit regularization methods has been criticized as potentially attributed to gradient obfuscation (Athalye et al., 2018; Engstrom et al., 2018; Lee et al., 2020; Raina et al., 2024). In contrast, our method operates on the post-softmax logits. It explicitly maximizes the probability gap between actual classes and the probability of incorrect classes, maximizing the confidence in the true classes of individual training samples. Our extensive experiments show the broad effectiveness of our approach in improving adversarial robustness to various adversarial attacks."}, {"title": "STANDARD DEVIATION AS A RISK MEASURE", "content": "The standard deviation measures the spread of a distribution around the mean of that distribution. The standard deviation of a distribution is given as:\n$SD = \\sqrt{\\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N-1}}$  (2)\nwhere xi is a data-point, \u03bc is the population mean, and N is the number of data-points in the distribution. A smaller SD value suggests that data points are more clustered, whereas a larger SD value indicates that data points are farther from the mean. The properties of standard deviation have made it a useful measure\nof risks in various domains. For example, the standard deviation is used as a risk measure in finance to measure market volatility and risk of assets and portfolios by indicating how much the returns of an asset deviate from the mean return (Artzner et al., 1999; Hull, 2012; Ross et al., 2019).\nDrawing inspiration from the widely used standard deviation statistic, a recent work by Fakorede et al. (2024) proposes a modified standard deviation measure for scoring and characterizing the vulnerability of individual natural examples. Inspired by this work, our paper further argues a standard-deviation-inspired measure to be utilized to capture the risk of misclassification of training samples."}, {"title": "PROPOSED METHOD", "content": "Here, we justify the introduction of a Standard-Deviation-Inspired (SDI) measure as a regularization term into existing adversarial training approaches.\nThe SDI measure was originally proposed in Fakorede et al. (2024) for the purpose of estimating the vul-nerability of natural examples. In this paper, we connect the SDI measure to the min-max optimization concept in adversarial training and use it as a regularization term."}, {"title": "THE SDI MEASURE", "content": "The SDI measure adopts an idea similar to standard deviation to characterize the spread of output prob-ability vectors of DNN models for individual training examples. Specifically, the approach measures the variation of a model's estimated probabilities for incorrect classes around the model's estimated probability for the true label of individual input examples x. Formally, given an input-label pair (xi, Yi) and the output probabilities of a DNN model on input sample x\u2081 denoted as fo(xi), the SDI measure is given as:\n$MSDI(x_i, y_i, \\theta) = \\sqrt{\\frac{\\sum_{k=1}^{|C|}(f_\\theta(x_i)_k - f_\\theta(x_i)_{y_i})^2}{|C|-1}}$ (3)\nwhere C is the number of output classes, fo(xi)k is the model's estimated probability corresponding to class k, fo(xi)y; is the model's estimated probability of the true class, and e denote the model parameters.\nUnder the condition where fo(xi)yi \u2265 maxk,k\u2260yi fo(xi)k, the MSDI(xi, Yi, 0) measure serves to capture the vulnerability and risk of misclassification of individual examples xi. A smaller value of MSDI(Xi, Yi, 0) suggests that the output probabilities returned for sample x are more evenly distributed among classes, indicating a higher likelihood of misclassification as the model may misclassify it into any of the k-1 incorrect classes."}, {"title": "AN SDI-ORIENTED PERSPECTIVE TO ADVERSARIAL TRAINING", "content": "In this section, we provide an explaination of the MSDI measure from the perspective of the min-max optimization framework of adversarial training.\nAT methods are typically formulated as min-max optimization procedures. The inner maximization step of the AT approach involves generating adversarial examples x from natural examples x\u2081 by iteratively adjusting the input data in directions that maximize the loss, using projected gradient descent (PGD) algorithm as follows:\nx+1 \u2190 \u03a0xB\u025b(xi) (x't + a sign(\u221ax L(xt, Yi))).\n(4)\nwhere II is the projection operator and L is a loss function.\nEssentially, the adversarial examples produced during the inner maximization process are tailored to reduce the model's confidence in correctly classifying them into their true classes. Moreover, the resulting adver-sarial examples are untargeted, i.e., the inner maximization misclassifies the adversarial examples without prioritizing any particular incorrect class.\nThe MSDI(Xi, Yi, 0) measure estimates the vulnerability of individual inputs into a DNN model, using the spread of the model's estimated probabilities w.r.t. the model's estimated probability of the actual class\nof each input. Smaller values of MSDI(Xi, Yi, 0) for the output probability vector of a model indicate that the predicted probabilities are more concentrated or similar, reflecting lower confidence in the true class of the input. Therefore, the magnitude of MSDI(xi, Yi, 0) value for an input-label pair (xi, yi) is indicative of the degree of risk in misclassifying xi. In contrast, a large value of MSDI(Xi, Yi, 0) indicates that the model assigns a high probability to class yi for xi, suggesting strong confidence in the prediction and a low risk of misclassification.\nThis observation suggests that adversarial examples may be generated simply by finding adversarial pertur-bation along the gradient direction that minimizes the MSDI metric. We might use MSDI for generating adversarial examples as follows:\nx+1 \u2190 \u041f\u0445\u0454\u0432\u0454(x) (x't - a sign(\u2207xt MSDI(xt, Yi, 0))).  (5)\nThe above adversarial example generation is achieved using the widely adopted PGD algorithm (Madry et al., 2018), with the notable difference that the sign of the gradient is inverted to move in the opposite direction. For most AT variants, adversarial examples in the inner maximization step are obtained by finding perturbations that maximize a cross-entropy loss function or a Kullback-Leibler divergence. The SDI measure does not rely on information-theoretic measures. Therefore, it offers a complementary approach for finding adversarial examples. We provide experimental evidence for our claim in Sec. 4.5.\nThe outer minimization seeks model parameters that minimize the loss on the adversarial examples generated during the inner maximization step. Essentially, the outer minimization process aims to maximize the likelihood of correctly classifying individual adversarial training examples. Invariably, the outer minimization minimizes the likelihood of incorrect classification by increasing the probability gap between the example belonging to the label and belonging to incorrect classes. This conceptually aligns with the goal of maximizing the SDI measure. Maximizing the SDI metric encourages the model to correctly classify the input to its true class by widening the probability gap between the estimated probability for the true class and the estimated probabilities for other incorrect classes. Moreover, when fo(xi)y \u2265 maxk,k\u2260y fe(xi)k, maximizing the MSDI measure maximizes fo(xi)y."}, {"title": "SDI REGULARIZATION", "content": "Here, we propose the SDI regularization term for improving adversarial training.\nTypically, adversarial training techniques involve training models using adversarial examples generated by various forms of PGD attacks. However, this approach may lead to overly specialized models defending against PGD attacks, potentially causing poor generalization to different attack types. As discussed earlier, the MSDI metric introduced in the previous section has beneficial characteristics, particularly its ability to maximize the probability gap between the true class and the other classes. This property aligns well with the objectives of adversarial training, enhancing its effectiveness. Hence, to improve the robust generalization and performance of existing AT methods, we propose adding a regularization term that maximizes the MSDI measure on each training example.\nMaximizing the MSDI metric as a regularization term encourages the model to maximize the output prob-ability of a training example belonging to its actual class, thus improving training. Moreover, since existing AT variants depend on information-theoretic measures for both the inner maximization step and the outer minimization step, applying the MSDI metric as a regularization term offers a complementary addition to AT methods that does not depend on the information-theoretic measures that these AT methods are based. Lastly, maximizing the MSDI metric facilitates the widening of the probability gaps between the probability of the actual class of individual adversarial examples and the probabilities corresponding to incorrect classes, thus improving the discriminability of the model.\nNote that maximizing the MSDI measure to improve fo(xi)y is only valid when fo(xi)y \u2265 maxk,k\u2260y fo(xi)k\u00b7 When fo(xi)y < maxk,k\u2260y fo(xi)k, maximizing MSDI may further minimize fo(xi)y, since the probability gap between each fo(xi)k,k\u2260y and fo(xi)y is further increased to maximize the MSDI measure. Therefore, we propose a regularization term LSDI that selectively maximizes the MSDI measure on samples whose output probabilities satisfies fe(xi)y \u2265 maxk,k\u2260y fo(Xi)k\u00b7"}, {"title": null, "content": "We utilize the multi-class margin from (Koltchinskii & Panchenko, 2002) to determine input samples satisfy-ing the desired conditions. For a DNN denoted by fe and the input-label pair (xi, yi), the margin dm (xi, Yi; 0) is given as follows:\ndm (xi, yi; 0) = fo(xi)yi - $\\max_{k,k\\neq yi} f_\\theta(x_i)_k$ (6)\nwhere fo(xi)y is the model's predicted probability of the correct label yi, and maxk,k\u2260y; fo(xi)k is the maximum prediction of the remaining classes.\nThe proposed SDI regularization term is formally described as follows:\n$L_{SDI} (x_i, y_i; \\theta) = \\begin{cases}  MSDI(x_i, y_i; \\theta), & \\text{if } d_m (x_i, y_i; \\theta) \\geq 0 \\\\  0, & \\text{otherwise} \\end{cases}$  (7)\nIn this paper, we apply the LSDI(xi, yi; 0) regularization term to two prominent adversarial training methods: standard AT (Madry et al., 2018) and TRADES (Zhang et al., 2019). We refer to the SDI-regularized standard AT and TRADES as AT-SDI and TRADES-SDI respectively. The regularized training objectives are stated as follows:\nAT-SDI:\n$\\sum L_{CE}(f_\\theta(x), y_i) - \\beta \\cdot L_{SDI} (x, y_i; \\theta)$  (8)\nTRADES-SDI:\n$\\sum L_{CE}(f_\\theta(x_i), y) + \\frac{1}{M} KL(f_\\theta(x_i)||f_\\theta(x)) - \\beta L_{SDI} (x, y_i; \\theta)$ (9)\nwhere \u1e9e in Eq. (8) or (9) represents the regularization hyperparameter for controlling the weight of the SDI regularization term, and KL in Eq. (9) represents Kullback-Leibler divergence. In the proposed AT-SDI and TRADES-SDI, the LSDI regularization term is selectively applied. The regularization term is only applied to adversarial training instances satisfying: f(x)y \u2265 maxk,k\u2260y fo(x)k. If fo(x)y < maxk,k\u2260y fo(x)k on a sample x, the normal AT or TRADES adversarial training is applied on x.\nAs an example, the proposed AT-SDI algorithm for adversarial training is presented in the following."}, {"title": "EXPERIMENTAL SETUP", "content": "In this section, we conduct an extensive evaluation of the proposed method. To assess its versatility, we test it on various datasets, including CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and Tiny ImageNet Deng et al. (2009). We apply simple data augmentations, such as 4-pixel padding with 32 \u00d7 32 random crop and random horizontal flip, to each of the datasets. Additionally, we employ ResNet-18 (He et al., 2016) and WideResNet-34-10 (He et al., 2016) as the backbone models."}, {"title": "Training Parameters.", "content": "We train the backbone networks using mini-batch gradient descent for 110 epochs, with a momentum of 0.9 and a batch size of 128. For training CIFAR-10, we used a weight decay of 5e-4, and for CIFAR-100, SVHN, and TinyImageNet, we used a weight decay of 3.5e-3. The initial learning rate was set to 0.1 (0.01 for CIFAR-100, SVHN, and TinyImageNet), and it was divided by 10 at the 75th epoch and then again at the 90th epoch."}, {"title": "Hyperparameters.", "content": "We set the value of \u1e9e to 3.0 for training AT-SDI and TRADES-SDI on CIFAR-10, SVHN, and TinyImagenet. For CIFAR-100 using AT-SDI and TRADES-SDI, we set \u1e9e to 3.0. When incorporating AWP (Wu et al., 2020) into AT-SDI and TRADES-SDI, we respectively set \u03b2 to 3.0 and 1.0. The hyperparameters are tuned using a validation set. We provide the sensitivity analysis of \u1e9e hyperparameter on AT-SDI and TRADES-SDI for CIFAR-10 using Wideresnet-34-10 in Tables 9 and 10."}, {"title": "BASELINES", "content": "We use prominent methods Standard AT (Madry et al., 2018) and TRADES (Zhang et al., 2019) as our baselines. In addition, we compare our results to other popular works MART (Wang et al., 2019), AWP (Wu et al., 2020), MAIL (Liu et al., 2021), and ST-AT (Li et al., 2023). All hyperparameters of the baseline methods remain consistent with those in their original papers. Nevertheless, we maintain consistency by using the same learning rate, batch size, and weight decay values as those utilized during the training of our proposed method."}, {"title": "THREAT MODELS", "content": "We evaluate the performance of the proposed method against strong attacks under white-box and black-box settings, as well as the Auto attack.\nWhite-box attacks. These attacks have access to model parameters. To assess robustness on CIFAR-10 using Resnet-18 and Wideresnet-34-10, we employ the PGD attack with e = 8/255, step size \u043a = 1/255, and K = 20 iterations (PGD-20). Additionally, we utilize the CW attack (CW loss (Carlini & Wagner, 2017) optimized by PGD-20) with \u0454 = 8/255 and step size 1/255. On SVHN and TinyImageNet, we use the PGD attack with e = 8/255, step size \u043a = 1/255, and K = 20 iterations.\nBlack-box attacks. In black-box settings, the adversarial attack method does not have access to the model parameters. We evaluate robust models trained on CIFAR-10 against strong black-box attack, SPSA (Uesato et al., 2018), with 100 iterations. These attacks use a perturbation size of 0.001 for gradient estimation, a learning rate of 0.01, and 256 samples for each gradient estimation. All black-box evaluations are conducted on trained Wideresnet-34-10.\nAuto attacks (AA). Lastly, we assess the robustly trained models using Autoattack (1 and 12) (Croce & Hein, 2020b), which is a powerful ensemble of attacks consisting of APGD-CE (Croce & Hein, 2020b), APGD-\nT (Croce & Hein, 2020b), FAB-T (Croce & Hein, 2020a), and Square (a black-box attack) (Andriushchenko et al., 2020)."}, {"title": "PERFORMANCE EVALUATION", "content": "We present our experimental results and comparisons on various datasets using ResNet-18 and WideResNet-34-10 architectures. Specifically, results for CIFAR-10 on ResNet-18 and WideResNet-34-10 are summarized in Tables 1 and 2, respectively, while results for CIFAR-100, SVHN, and Tiny ImageNet using ResNet-18 are presented in Tables 3, 4, and 5, respectively. To further explore the versatility of the proposed method, we evaluate it using a lightweight backbone, VGG-16 architecture (Simonyan & Zisserman, 2014), on the CIFAR-10 dataset. The results are presented in Table 6.\nAdditionally, comparisons with other prominent baselines are provided in Table 7. Finally, we compare the perfomance of adversarial examples generated using the SDI metric approach described in Eq. (5) to adversarial examples crafted using cross-entropy and KL-divergence losses.\nThe experiments were carried out three times using different random seeds. The mean and standard deviation were then calculated, with the results presented as mean \u00b1 std."}, {"title": "Comparing AT and TRADES with their SDI-regularized variants.", "content": "In this comparison, we evaluate the performance of AT and TRADES against their respective variants with the SDI regularization term, AT-SDI and TRADES-SDI. Experimental findings indicate that the proposed regularization term enhances robustness against various adversarial attacks, including Autoattacks and CW. Specifically, when applied to ResNet-18 and WideResNet-34-10 architectures on CIFAR-10, AT-SDI demonstrates improvements over AT across all evaluated attacks (see Tables 1 and 2). For example, on WideResNet-34-10, AT-SDI outperforms AT in robustness against PGD-20 (+0.45 %), CW (+2.54 %), and Autoattacks (+1.65%). The improvement in robustness are achieved without a significant reduction in the natural accuracy.\nSimilarly, TRADES-SDI exhibits superior performance compared to TRADES on PGD-20 (+1.19 %), CW (+2.06%), and Autoattacks (+1.14%). Training with TRADES-SDI also exhibit a noticeable improvement of 0.67% on the natural accuracy. Overall, AT-SDI achieves greater improvement in robustness against CW attacks compared to TRADES-SDI, while TRADES-SDI demonstrates better enhancement against PGD-20 attacks compared to AT across ResNet-18 and WideResNet-34-10 architectures.\nThe proposed SDI regularization term also enhances robustness on CIFAR-100 when applied to ResNet-18 across all evaluated adversarial attacks (see Table (3)). The margin of improvement in robustness against adversarial attacks on CIFAR-100 appears to be larger than that observed on CIFAR-10 for both AT-SDI and TRADES-SDI. Similar improvements in robustness are observed when AT-SDI and TRADES-SDI are utilized to train Resnet-18 on SVHN dataset. Results in Table (4) show that AT-SDI outperforms AT on CW (+5.23%), Autoattack (+ 1.20%), and PGD-20 (+2.43%).\nTable 5 also clearly shows that the proposed training objective improves the robustness of Resnet-18 against all the evaluated attacks on Tiny Imagenet. The consistent improvement of performance across all the datasets tested validates the efficacy of the proposed SDI regularization term.\nFinally, Table 6 demonstrates the effectiveness of the proposed training objectives on VGG-16 using the CIFAR-10 dataset. Incorporating the proposed LSDI regularization term into AT results in a marginal improvement in robustness to PGD-20, with a significant gain of 3.14% against CW and a 2.7% improvement against Autoattack. Similarly, TRADES-SDI surpasses TRADES with a 1.3% increase in robustness against PGD-20, a 1.71% gain against CW, and a 2.55% improvement against Autoattack."}, {"title": "Comparison with other prominent baselines.", "content": "Here, we compare our approach with other prominent and state-of-the-art methods from existing works, including MART (Wang et al., 2019), adversarial weight perturbation (AWP) (Wu et al., 2020), ST-AT (Li et al., 2023), LAS AT (Jia et al., 2022), LOAT (Yin & Ruan, 2024) and Randomize-AT (Jin et al., 2023). Additionally, for a fair comparison with AWP, we combine AT-SDI and TRADES-SDI with AWP and denote them as AT-SDI + AWP and TRADES-SDI + AWP, respectively. In both AT-SDI + AWP and TRADES-SDI + AWP, the SDI regularization term is employed for perturbing the network weights.\nExperimental results displayed in Table 7 show that AT-SDI outperforms all existing baselines in robustness against CW attacks. Additionally, AT-SDI outperforms TRADES and MART against Autoattacks. However, AT-SDI slightly underperforms compared to MART against PGD-20. Furthermore, AT + AWP also marginally outperforms AT-SDI against Autoattacks. On the other hand, TRADES-SDI achieves better performance than all baselines on CW and Autoattacks.\nWhen compared to recent state-of-the-art methods, AT-SDI and TRADES-SDI demonstrate superior performance on CW and SPSA attacks. TRADES-SDI outperforms LAS AT on CW (+0.82%), Auto-attacks\n(+0.66%), and SPSA (+1.1%). Although LOAT moderately performs better than AT-SDI and TRADES-SDI on PGD-20, both methods show significantly better than LOAT against CW, AA, and SPSA. While Randomize AT and CAT slightly surpass AT-SDI and TRADES-SDI on PGD-20, AT-SDI and TRADES-SDI perform better on CW and SPSA attacks. Moreover, Randomize AT and CAT take roughly twice as long to train, making AT-SDI and TRADES-SDI significantly more efficient.\nCombining our approach with AWP further improves robustness against the evaluated attacks. Specifically, AT-SDI + AWP and TRADES-SDI + AWP outperform AWP + AT and TRADES + AWP against every adversarial attack. AT-SDI + AWP and TRADES-SDI + AWP demonstrate improved performance across all attacks. AT-SDI + AWP enhances robustness to PGD-20, CW attacks, and Autoattacks over AWP by 2.11%, 4.38%, and 2.79%, respectively. AT-SDI+AWP also considerably outperforms AWP robustness against SPSA, a strong query-based blackbox attack, by 2.99%. Additionally, AT-SDI + AWP achieves superior performance on natural samples. TRADES-SDI + AWP improves performance over AWP on all the attacks but dips by 0.15% in performance on natural examples."}, {"title": "SDI-regularization Improves Generalization of Adversarial Training.", "content": "Most AT methods involve training with a specific type of adversarial examples crafted by maximizing either the cross-entropy or KL-divergence measure using PGD. Therefore, the adversarial examples utilized for adversarial training do not entirely reflect the universe of all possible adversarial attacks that a robust model may encounter. This limitation can lead to poor generalization of adversarially trained models to other types of adversarial examples (Song et al., 2018).\nTypically, adversarial training methods exhibit significantly higher performance on PGD attacks, as evident from the experimental results tables. However, when subjected to other types of attacks, the performance of robust models tends to diminish. For example, it can be observed from the tables that the robust accuracy on CW and AA attacks are notably lower compared to PGD-20.\nThe introduction of the LSDI regularization term to the standard AT and TRADES improves their per-formances against other attacks. Unlike other AT methods, AT-SDI considerably improves the robustness against CW and AA on all the datasets evaluated. In fact, training a model using the proposed AT-SDI consistently improves the performance of the resulting robust model to CW attack and achieve better per-formance over PGD-20 on CIFAR-10 dataset, as may be observed in Tables 1, 2, and 7. Note that CW adversarial examples are not used for training, yet better robust accuracies are recorded compared to PGD adversarial examples, which are typically used for adversarial training. Significant improvement in robustness against CW and AA can also be observed on other datasets, CIFAR-100, SVHN, and Tiny Imagenet. The SDI regularization"}]}