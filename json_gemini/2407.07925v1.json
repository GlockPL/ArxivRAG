{"title": "Enhancing Social Media Personalization: Dynamic User Profile Embeddings and Multimodal Contextual Analysis Using Transformer Models", "authors": ["Pranav Vacharajani", "Pritam Ranjan"], "abstract": "This study investigated the impact of dynamic user profile embedding on personalized context-aware experiences on social networks. A comparative analysis of multilingual and English transformer models was performed by analyzing a dataset of over twenty million data points It involved analyzing a wide range of metrics and performance indicators to compare dynamic profile embeddings vs. non-embeddings. effectively static profile embeddings. A comparative study was conducted using degradation functions. Extensive testing and extensive research confirmed that dynamic embedding successfully tracks users' changing tastes and preferences, provides more accurate recommendations and user engagement Thus these results are important for social media platforms that aim to improve user experience through relevant features and sophisticated recommendation engines.", "sections": [{"title": "1 KEYWORDS:", "content": "DYNAMIC EMBEDDINGS, DECAY FUNCTIONS, TRANSFORMER Models, USER PROFILE PERSONALIZATION, SOCIAL MEDIA, MULTIMODAL DATA, RECOMMENDATION SYSTEMS, COSINE SIMILARITY."}, {"title": "2 INTRODUCTION:", "content": "In the rapidly evolving digital age, social media platforms are constantly looking for ways to increase user engagement and satisfaction.. Personalized content recommendations are central to this pursuit, [1]. requiring the development of sophisticated analytical techniques that can adapt to user behavior and preferences.. Transformer models known for their effectiveness in a variety of natural language processing tasks offer promising approaches has improved recommendation processes through embeddings of dynamic user profiles. [2]"}, {"title": "2.2. Problem Statement:", "content": "Traditional static user profile embeddings often fail to capture the temporal evolution of user interactions.. This results in inadequate recommendations. This limitation presents a challenge in maintaining user engagement, as recommendations do not reflect recent changes in user preference or behavior. [3]"}, {"title": "2.3. Purpose and Significance of the Study:", "content": "The main purpose of this study is to investigate the effectiveness of dynamic user profile embedding, using different decay functions, to accurately track and display the changes in user interest over time. The objectives of this study are:\n\u2022 Evaluate how different the decay functions influence the relevance and accuracy of recommendations.\n\u2022 Compare the performance of multiple transformer-based models in handling dynamic embeddings..\n\u2022 Explore the implication of these finding for enhancing personalized user experiences on SM platforms."}, {"title": "3 LITERATURE REVIEW:", "content": "In order to deal with the dynamic nature of user interests and behaviours on the social media platforms like Twitter, Facebook, Instagram.. researchers have looked into dynamic user profile embeddings for better personalised recommendation system. In order to capture the temporal changes in user profiles, Liang et al. (2018) [4] suggested dynamic user and word embeddings, with the aim to capture the temporal changes in user profiles and activities. This can be instrumental in enhancing personalization.\nSimilar to this, Kerin et al. (2019) [5] highlighted the significance of temporal word embeddings for creating temporal user profiles, with the main driving force for their research being the dynamism present in user profiles. Furthermore, Zheng et al. (2018)[6] created a technique called user profile embedding that allows to capture similarities among users in social networks through user profile embedding (Zheng et al., 2018)[6]"}, {"title": "3.2 Multimodal Contextual Analysis Using Transformer Models", "content": "Transformer models [7] ,especially those based on self-attention mechanisms, have revolutionised natural language processing and have been applied for recommendation systems. They are perfect for analysing noisy and unstructured social media data because of their capacity to handle sequential data and capture long-range dependencies. Wu et al.'s survey from 2023 [8] examines the different ways that LLMs can be used in recommendation systems and emphasises how well they work to produce personalised content using advanced pattern recognition and predictive analytics.\nA major area of recent study has been the promise of Transformer models in multimodal learning environments. The main elements and difficulties of using Transformers to integrate multimodal data were covered by Xu et al. (2023) (Xu et al., 2023) [9]. To analyse unaligned multimodal language sequences, Tsai et al. ((Tsai et al., 2019).) [9] presented the Multimodal Transformer (MulT), which makes use of directional paired cross-modal attention mechanisms. Bartolomeu et al. (2022) [10] developed a context-enriched Multimodal Transformer model that enhances textual and visual contexts to better align images with news content, thus expanding on this idea (Bartolomeu et al., 2022).[10]"}, {"title": "3.3 Social Media Recommender Systems Approaches", "content": "Recommender systems (RS) are (Eirinaki et al., 2018) [11] used by a variety of social media platforms to improve user experience through engagement and content personalisation. Anandhan et al. (Alamdari et al., 2020)[12] examined methods in social media RS, emphasising the use of collaborative filtering and content-based filtering, frequently in hybrid forms, and concentrating on context-awareness, scalability issues, and decision analysis. In their 2018 study, Eirinaki et al. (Eirinaki et al., 2018) [11] concentrated on the handling of user-generated content and volatile social interactions by RS in large-scale networks, highlighting the necessity of scalable solutions that can adjust to changing user preferences. Zhou et al. (Zhou et al., 2012) [13] talked about the development of personalised RS in social networking, including its history, technological advances, and social settings improvements."}, {"title": "3.4 E-commerce Recommender Systems Approaches", "content": "Recommendation systems (RS) improve the e-commerce shopping experience and boost sales. An overview of RS techniques, such as collaborative filtering, content-based filtering, knowledge-based systems, and hybrid systems, were given by Alamdari et al. (Alamdari et al., 2020) [12] . Sivapalan et al. (Sivapalan et al., 2014) [14] investigated how RS increases the effectiveness of e-commerce by recognising patterns in user behaviour and making product recommendations in line with those patterns. Furthermore, Schafer et al. (Schafer et al., 1999) [15]. described how RS helps e-commerce sites by making product recommendations based on user ratings, browsing history, and past purchases."}, {"title": "3.5 Main Approaches for Recommender Systems", "content": "In the digital world, recommender systems are essential because they direct user experiences by making relevant suggestions for products, content, and social interactions based on the interests and actions of the user. These systems use a range of approaches, each with special advantages and modifications for distinct platforms, such as social media (SM) websites and e-commerce platforms. The main approaches and their applications across notable platforms are mentioned below. [3], [12]"}, {"title": "3.5.1 Collaborative Filtering (CF)", "content": "In recommender systems, one of the most popular methods is collaborative filtering. It bases its recommendations on user preferences as a whole, presuming that users who have previously agreed will do so in the future.\n\u2022 User-Based CF: This technique is used by eBay and Amazon to make product recommendations by comparing users based on past purchases. [16]\n\u2022 Item-Based CF: YouTube uses this method to make video recommendations by analyzing the similarities in user interactions with different content [17]."}, {"title": "3.5.2 Content-Based Filtering (CBF)", "content": "Based on feature similarity, content-based filtering suggests products that are similar to those a user has previously favoured. One of the best examples of this is Netflix, which provides recommendations to users based on the genres, actors, and directors of films and TV shows they have already seen[18]."}, {"title": "3.5.3 Hybrid Approaches", "content": "Hybrid systems combine content-based filtering and collaborative filtering to avoid certain limitations of each approach. Spotify employs this technique to provide personalised music recommendations by combining user behaviour (collaborative data) and song audio features (content-based data).[16]"}, {"title": "3.5.4 Knowledge-Based Systems", "content": "These systems recommend products based on explicit knowledge about the item assortment, user preferences, and recommendation criteria. This approach is often used in situations where items are not purchased frequently, such as on real estate or job sites like LinkedIn[19] ."}, {"title": "3.5.5 Demographic-Based Recommendations", "content": "This approach makes product recommendations based on user demographic data. Facebook and Instagram match recommendations with users' age, gender, location, and interests by using demographic data to target advertising and content[20]."}, {"title": "3.5.6 Utility-Based Systems", "content": "Products are recommended by utility-based recommender systems based on an assessment of their value to a specific user. This approach is used by Amazon's Alexa to make product recommendations based on the usefulness or practicality determined from user interactions and queries[21]."}, {"title": "3.5.7 Session-Based Recommendations", "content": "These systems provide recommendations based on what was done during a session. This method is common in e-commerce settings for brief sessions; it can be observed on platforms such as Alibaba, where real-time product recommendations are provided based on user interactions within a session[22]."}, {"title": "3.5.8 Deep Learning Approaches", "content": "Deep learning techniques have have gained popularity because of their capacity to represent intricate nonlinear relationships in data. Convolutional neural networks (CNNs) are used by Pinterest to evaluate visual content and suggest related pins. In order to recommend apps in its Play store, Google leverages deep neural networks in conjunction with user data and app features [23] .\nIndustry Applications\nFacebook and Instagram: Leverage a mixture of demographic-based and utility-based systems to optimize ad placements and content feeds[24].\n- Amazon: Primarily uses collaborative filtering for product recommendations but also incorporates content-based methods and utility systems, especially in its voice-activated tool, Alexa[25].\n- eBay: Uses a hybrid strategy that combines elements of item-based collaborative filtering and user-based filtering to customise product recommendations based on user browsing and purchase history [26]"}, {"title": "3.6 Identification of Gaps in the Literature", "content": "The emphasis on text in current models often obscures the dynamic user behaviours and multimodal data that are crucial for comprehending user preferences. (Recommender Systems Handbook.pdf; Social Media Recommendation Algorithms Tech Primer.pdf) [16], [27]. Furthermore, a lot of models don't make full use of the real-time data processing that's required to capture ephemeral social media trends. The embedding which is very important aspect is needs to be addressed."}, {"title": "3.7 How the Current Research Addresses These Gaps", "content": "This study uses transformer models to create a model that combines multimodal data with dynamic user profile embeddings. By updating user profiles dynamically in realtime, it overcomes the drawbacks of static profiling. The model seeks to improve contextual relationship understanding by utilising transformer technology, which will allow for more precise user preference predictions and greatly improved personalised recommendations."}, {"title": "4 METHODOLOGY", "content": "The study adopted a quantitative research design to systematically check the effectiveness of dynamic user profile embeddings as compared to static ones.. Various transformer models, [28] such as multilingual and English-specific ones, have been analyzed the use of special decay capabilities [29] to understand their effect on the accuracy and relevancy of content recommendations on social networks."}, {"title": "4.2 Tools and Techniques for Data Analysis", "content": "The analysis was conducted using several advanced data processing and machine learning tools:\n\u2022 Python: Used for scripting and automating the data analysis process, including data cleaning and transformation tasks.\n\u2022 Pandas and NumPy: Employed for efficient data manipulation and numerical analysis.\n\u2022 Scikit-learn: Utilized for implementing machine learning models and conducting statistical tests to compare the effectiveness of different embedding techniques.\n\u2022 TensorFlow and PyTorch: Applied for training and evaluating deep learning models, particularly for developing and testing the multimodal profile embeddings.\n\u2022 Sentence Transformers Library: Used to generate and manipulate embeddings from the transformer models.\n\u2022 GPU: Kaggle's NVIDIA TESLA P100 GPU was used as the process required high computing power.\n\u2022 GitHub: GitHub repositories were used for keeping the codes [30]"}, {"title": "5 DATA COLLECTION:", "content": "To start with data collection exercise, we identified the top 100 most followed personalities on the Twitter (now its X), from various fields & regions globally, reflecting diverse interests. This selection aimed to cover a broad spectrum of user engagement."}, {"title": "5.2 User Data Compilation:", "content": "Utilizing the Twitter API, we extracted data from these 100 influential users along with their 1,000 most active followers each, culminating in a dataset comprising 100,100 individual user profiles.."}, {"title": "5.3 Tweet Data Acquisition:", "content": "We proceeded to gather timeline data for these users, resulting in approx 20 million data points. Post-processing, which included the removal of duplicates, refined the dataset to the final count of 14,765,661 (14M) data points."}, {"title": "5.4 User Activity Data:", "content": "To analyze user interactions such as... likes, Quo Tweets, and ReTweets, we collected user activity data. For evaluative purposes, we randomly selected 30,000 instances from this dataset. The total of all liked tweets from these selected instances amounted to 2,107,054 (2M) data points.\nThis extensive data collection phase spanned approximately three weeks, constrained by the rate limits by the Twitter API. [31]"}, {"title": "5.5 Concatenation of the data", "content": "After downloading the Twitter activity data, the next step involves processing the data using a comprehensive approach that includes concatenation, parsing, cleaning, and storage. Multiple JSON and CSV files containing Twitter activity timelines and user tweets are imported from various sources. Data from CSV files are read into Pandas DataFrames, with columns parsed and relevant fields extracted, followed by filtering to include specific information such as timestamps, user names, user descriptions, and tweet texts. Parsed data from individual files are appended into a list of DataFrames, which are then concatenated into a single DataFrame to create a unified dataset. Memory management techniques, such as garbage collection are utilized to handle large datasets efficiently, and dtype warnings are managed to ensure smooth data import despite mixed data types. The final concatenated DataFrame is saved to a CSV file for further analysis, providing a robust foundation for subsequent analysis and modeling efforts, such as creating dynamic user profile embeddings."}, {"title": "6 ALGORITHM FLOW CHART", "content": "A dynamic profile embedding method for social media data is shown in the flow chart. The first step in the process is gathering user information from Twitter (Now X), such as their bio and tweets. Hybrid tweet embeddings are produced by processing each tweet along with the user bio through an encoding model. A decay function is applied to these embeddings to adjust their weights based on their temporal relevance and recentness. A dynamic profile embedding that reflects the user's changing interests and preferences is the result. The most advantageous aspect of this strategy is its capacity to dynamically update the user profile with the most recent data, guaranteeing that recommendations and personalisations are current and pertinent. By utilising the most recent user activity data, this dynamic feature improves the accuracy of predictions pertaining to user engagement, such as predicting likes or follows."}, {"title": "7 TIME DECAY FUNCTIONS AND DYNAMIC E\u043c-BEDDINGS ANALYSIS", "content": "Time Decay Functions and Dynamic Embeddings Analysis leverages various pre-trained models from the Sentence Transformers library (https://sbert.net/docs/sentence_transformer/pretrained_models.html ) to create embeddings from the textual data and than applies different time decay functions on these embeddings. Main goal is to know that how time influences the representation of textual data in a dynamic context.\nWith the application of these time decay functions, the code aims to simulate real-world scenarios where the relevance of information changes over time.."}, {"title": "7.2 Approach", "content": "The approach to analyzing the impact of various time decay functions, various models and similarities and diversities on dynamic embeddings covers many steps.. These steps are explained as below:"}, {"title": "7.2.1 Data Loading and Data Preparation", "content": "Data Acquisition and Preprocessing: Data is been collected, now its necessary to convert and preprocess these data in the format which can be analysed."}, {"title": "7.2.2 Model Selection and Embedding Calculation", "content": "Model Selection: Based on the requirement, and existing resources, computing power, select appropriate pre-trained models. requirements."}, {"title": "7.2.3 Application of Time Decay Functions", "content": "Decay Function Selection: Analyse the application and relevance of various decay functions and choose a variety of decay functions (e.g., exponential, logarithmic) to model to catch the relevance of data over time."}, {"title": "7.2.4 Integration and Assessment of Similarity Measures", "content": "Similarity Metric Integration: Employ Basic, Cosine, and Cos-time similarities to evaluate changes in embeddings."}, {"title": "7.3 Uniqueness of the Approach", "content": "Diverse Model Usage: Utilisation of various pre-trained models enables a comprehensive evaluation across different data types and model architectures, illustrating how each model captures and represents data dynamics.\nVariety of Time Decay Functions: By experimenting with multiple decay functions, the analysis gains depth in understanding the differential impact of time on data relevance, catering to diverse application needs where the freshness of data varies in importance.\nIntegration of Multiple Similarity Measures: The approach not only applies traditional similarity metrics but also explores their integration into the decay functions. Incorporating a range of similarity measures, specifically Basic, Cosine, and Cos-time similarities, into the decay functions introduces a novel way to adjust embeddings based on temporal and semantic changes. This varied approach not only assesses the immediate similarity but also considers the evolution of these similarities over time, offering a multi-dimensional analysis of data dynamics."}, {"title": "7.4 Model Selection and Embedding Calculation", "content": "Following table gives and overview of the existing selective sentence transformer models.\nFour different pre-trained models from the Sentence Transformers library are selected for our research:\n\u2022 all-MiniLM-L6-v2: Small and fast.\n\u2022 distiluse-base-multilingual-cased-v2: Multilingual.\n\u2022 all-mpnet-base-v2: Large with extensive knowledge.\n\u2022 jinaai /jina-embeddings-v2-base-en: State-of-the-art (SOTA).\nAlgorithm: Generate Text Embeddings with Multiple Models"}, {"title": "Compute Embeddings:", "content": "main_embeddings1 <- Encode text with model1 using CUDA\nmain_embeddings2 <- Encode text with model2 using CUDA\nmain_embeddings3 <- Encode text with model3 using CUDA\nmain_embeddings4 <- Encode text with model4 using CUDA"}, {"title": "Initialize Models:", "content": "model1 <- Load SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') with CUDA\nmodel2 <- Load SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2') with CUDA\nmodel3 <- Load SentenceTransformer('sentence-transformers/all-mpnet-base-v2') with CUDA\nmodel4 <- Load SentenceTransformer('jinaai/jina-embeddings-v2-base-en') with CUDA"}, {"title": "Prepare Text Data:", "content": "text <- Concatenate DataFrame columns 'bio' and 'tweet-text' into a list"}, {"title": "Save Embeddings:", "content": "Save main_embeddings1 to 'minilml6v2.npy'\nSave main_embeddings2 to 'distiluse-base-multilingual.npy'\nSave main_embeddings3 to 'all-mpnet-base-v2.npy'\nSave main_embeddings4 to 'jina-v2-en.npy'"}, {"title": "End Procedure", "content": "The decay functions play a crucial role in dynamic embeddings by assigning weights to embeddings based on their temporal distance. Here are the key decay functions :\nVarious time decay functions are implemented to adjust embeddings over time. Each function takes a timeline, embeddings, and a decay constant \u2018k\u02bb as inputs, and returns adjusted embeddings."}, {"title": "Algorithm: Apply Time Decay Functions to Embeddings", "content": "Define exponential_decay(timeline, embeddings, k):\na <- Create an array from 1 to the length of the timeline\ndecay <- Calculate $e^{-k * a}$\nResult <- Multiply each decay value with corresponding embeddings\nReturn result\nDefine inverse_linear_decay(timeline, embeddings, k):\na <- Create an array from 1 to the length of the timeline\ndecay <- Calculate $1 / (1 + k * a)$\nResult <- Multiply each decay value with corresponding embeddings\nReturn result\nDefine inverse_square_root_decay(timeline, embeddings, k):\na <- Create an array from 1 to the length of the timeline\ndecay <- Calculate $1 / \\sqrt{1 + k * a}$\nResult <- Multiply each decay value with corresponding embeddings\nReturn result\nDefine hyperbolic_decay(timeline, embeddings, k):\na <- Create an array from 1 to the length of the timeline\ndecay <- Calculate $1 / (1 + k * a^2)$\nResult <- Multiply each decay value with corresponding embeddings\nReturn result\nDefine logarithmic_decay(timeline, embeddings, k):\na <- Create an array from 1 to the length of the timeline\ndecay <- Calculate $1 / log(1 + k * a + 1)$\nResult <- Multiply each decay value with corresponding embeddings\nReturn result\nDefine gaussian_decay(timeline, embeddings, k):\na <- Create an array from 1 to the length of the timeline\ndecay <- Calculate $e^{-k* a^2}$\nResult <- Multiply each decay value with corresponding embeddings\nReturn result\nEnd Procedure"}, {"title": "7.6. Similarities Calculation:", "content": "To measure the effectiveness of the time decay functions, specifically Basic, Cosine, and Cos-time similarities between consecutive pairs of embeddings is computed.\nAlgorithm: Apply Time and Cosine-Similarity Adjusted Decay Functions to Embeddings"}, {"title": "Define exponential decay function", "content": "Compute decay as $exp(-k * a * delta_t / cos_sim)$\nInverse Linear Decay:\nCompute decay as $1 / (1 + k * a * delta_t / cos_sim)$\nInverse Square Root Decay:\nCompute decay as $1 / sqrt(1 + k * a * delta_t / cos_sim)$\nHyperbolic Decay:\nCompute decay as $1 / (1 + (k* a^2 * delta_t) / cos_sim)$\nLogarithmic Decay:"}, {"title": "and At)", "content": "Exponential Decay : Decays rapidly at first and then slows down. Formula: $f(t) = e^{-At}$ and Formula: $f(t, Vt, Ve-1) =  e^{-At-\u0394v}$ and Formula: $f(t, Ve, Vt-1) = e^{-At-\u0394t/\u0394v}$\nInverse Linear : Decays at a constant rate. Formula: $f(t) = 1/(1+At)$ and Formula: $f(t, Vt, Ve-1) = 1/(1+At)-\u0394v$ and Formula: $f(t, Ve, Vt-1) = 1/(1+At-\u0394t/\u0394v)$\nInverse Square Root : Decays slowly, providing a more gradual reduction. Formula: $f(t) = 1/\\sqrt{1+At}$ and Formula: $f(t, Vt, Ve-1) = 1/\\sqrt{1+At}-\u0394v$ and Formula: $f(t, Ve, Vt-1) = 1/\\sqrt{1+At-\u0394t/\u0394v}$\nHyperbolic : Decays very slowly, useful for long-term processes. Formula: $f(t) = 1/(1+A^2t)$ and Formula: $f(t, Vt, Ve-1) = 1/(1+A^2t)-\u0394v$ and Formula: $f(t, Ve, Vt-1) = 1/(1+A^2t-\u0394t/\u0394v)$\nLogarithmic: Provides a very slow decay, suitable for scenarios where recency fades gradually. Formula: $f(t) = 1/log(1+At+1)$ and Formula: $f(t, Vt, Ve-1) = 1/log(1+At+1)-\u0394v$ and Formula: $f(t, Ve, Vt-1) = 1/log(1+At-\u0394t/\u0394v+1)$\nGaussian : Decays rapidly at first and then flattens out, forming a bell-shaped curve. Formula: $f(t) = e^{-A^2t}$ and Formula: $f(t, Vt, Ve-1) = e^{-A^2t}.\u0394v$ and Formula: $f(t, Ve, Vt-1) = e^{-A^2t-\u0394t/\u0394v}$"}, {"title": "Calculate Decay Values:", "content": "Initialize lists to store decay values for each decay function and user:\nBasic decay functions: exp_dpe, inv_lin_dpe, inv_sqrt_dpe, hyperbolic_dpe, logarithmic_dpe, gaussian_dpe\nDecay with cosine similarity: exp_cos_dpe, inv_lin_cos_dpe, inv_sqrt_cos_dpe, hyperbolic_cos_dpe, logarithmic_cos_dpe, gaussian_cos_dpe\nDecay with cosine similarity and time: exp_cos_time_dpe, inv_lin_cos_time_dpe, inv_sqrt_cos_time_dpe, hyperbolic_cos_time_dpe, logarithmic_cos_time_dpe, gaussian_cos_time_dpe\nProcess each user's data:\nExtract timestamps and embeddings for the user\nCompute decay values for each decay type using embeddings and timestamps:\nApply basic decay functions\nApply decay functions adjusted by cosine similarity\nApply decay functions adjusted by cosine similarity and time differences\nAppend computed decay values to respective lists\nSum and Save Function:"}, {"title": "Define a function", "content": "Define a function to sum decay values for all users and save the results as numpy files\nFor each type of decay data list:\nSum across all users' decayed embeddings\nSave the summed array to a numpy file\nDiversity Analysis:\nCalculate diversity metrics for each decay adjustment type using a custom diversity calculation function\nOrganize the diversity metrics into a DataFrame for analysis and visualization"}, {"title": "8 COMPREHENSIVE ANALYSIS OF FINDINGS ACROSS DECAY FUNCTIONS, DIVERSITY METRICS, AND MODEL PERFORMANCE", "content": "This analysis integrates various pre-trained models-MiniLM, DistilUSE Multilingual, MPNet, and Jina-to examine their performance across different decay functions and manage diversity metrics: basic, cosine, and cosine-time similarities.\nAnalysis of Output Matrix and Graph\nExponential Decay (exp): Shows moderate diversity in basic metrics, high diversity in cosine similarity, and excellent temporal consistency. This balance makes it suitable for environments where both recent and historical data influences are essential.\nInverse Linear (inv_lin) and Square Root (inv_sqrt ): These decays show a graduated reduction in diversity metrics, with inv_sqrt slightly outperforming inv_lin in handling outliers. This suggests their use in applications where historical data gradually diminishes in influence.\nHyperbolic Decay: Achieves the highest basic diversity, indicating its effectiveness in distinguishing between data points over a shorter term. However, its lower consistency over time may limit its application in long-term predictive modeling.\nLogarithmic Decay: Similar to inverse functions but offers the slowest reduction in relevance, ideal for scenarios requiring long-term data retention without significant decay.\nGaussian Decay: Stands out with the highest cosine similarity, pointing to its efficiency in emphasizing recent data trends, making it highly suitable for real-time analytical applications."}, {"title": "8.2. Summary of Model Performances Across Decay Functions", "content": "MiniLM : Consistently high performance in maintaining diversity across all metrics. Excels in cosine similarity across all decay functions, suggesting its effectiveness in capturing semantic relationships in multilingual contexts. Best under Gaussian decay, indicating robustness in adapting to recent data changes.\nDistilUSE Multilingual: Exhibits lower basic and cosine similarities compared to MiniLM, indicating a slight reduction in its ability to maintain diversity over time. Performs best under Gaussian decay for cosine similarity, aligning with MiniLM in terms of emphasizing recent data.\nMPNet: Shows a good balance across all diversity metrics, slightly outperforming DistilUSE in cosine similarity. This reflects its deep semantic understanding capabilities, especially under exponential and Gaussian decays. Best performance noted under Gaussian decay for both basic and cosine similarities.\nJina: Registers the lowest diversity scores among the models, particularly in basic similarity, which might be due to its focus on similarity rather than diversity, as it is optimized for relevance-focused tasks like RAG models. Despite its lower scores, it shows a respectable performance in cosine similarity under Gaussian decay."}, {"title": "8.3 Comparative Analysis and Diversity Metrics", "content": "Cosine-Time Decay's Superiority: Especially effective in capturing the maximum diversity across models, suggesting its utility in analyzing the evolution of data relationships over time.\nDecay Function Effectiveness: Gaussian decay emerges as the top performer, especially when recent data relevance is paramount. In contrast, logarithmic and hyper-"}, {"title": "9 SUPERVISED EVALUATION USING ACTIVITY DATA", "content": "Supervised Evaluation Using Activity Data aimed at analyzing User Activity Data to recommend activities based on user profiles and post embeddings. The first part involved time decay analysis with various decay functions and different models on User Data, including 100,100 individual user profiles and Tweet Data, resulting in approximately 20 million data points. In this second part, we analyze User Activity Data, such as likes, quote tweets, and retweets, by randomly selecting 30,000 instances, resulting in a total of 2,107,054 (2M) data points for liked tweets."}, {"title": "9.2 Unique Approach", "content": "The unique approach in this research involves several key elements that enhance the recommendation system:\nUsing Activity Data: By incorporating user activity data such as likes, quote tweets, and retweets, on top of user profile data and timeline data, the model gains an additional dimension to compute similarities. This comprehensive approach captures both static user characteristics and dynamic interactions, improving the relevance and accuracy of the recommendations."}, {"title": "Pipeline Creation:", "content": "A well-structured pipeline was created for loading data, preprocessing, encoding using multiple embedding models, training, and evaluation. This pipeline ensures a systematic and reproducible process for analyzing and modeling the data."}, {"title": "Multiple Embedding Models:", "content": "The use of multiple embedding models (MiniLM, DistilUse Multilingual, MPNET, and JINA) captures different aspects of the user activity data. Each model brings unique strengths, and their comparative analysis helps in identifying the best performer."}, {"title": "Cosine Similarity Metric:", "content": "Utilizing cosine similarity as a metric for recommendation enhances the relevance of the suggestions by measuring the cosine of the angle between two non-zero vectors. This metric is particularly effective in high-dimensional spaces typical of embeddings."}, {"title": "Neural Network Model with Normalization:", "content": "A neural network (ANN) model with normalization layers and mean squared error loss was developed. This model includes dense layers with ReLU activation functions to capture complex patterns in the data. The normalization step ensures that the input vectors are on the same scale, which is crucial for accurate similarity calculations."}, {"title": "Explanation of the steps adopted:", "content": "Data Loading and Preprocessing : The initial step involves loading and preprocessing the Twitter activity data. This data comprises user interactions such as likes, quote tweets, and retweets. JSON files containing this activity data are loaded into a list called activity_data. For each entry in the activity_data, the text from each post is extracted and stored in a separate list called posts. This preprocessing step ensures that the data is structured and ready for further analysis.\nModel Setup and Encoding: Once the data is preprocessed, the next step is to set up and initialize multiple embedding models using the Sentence Transformers library. Four models are used in this research: MiniLM, DistilUse Multilingual, MPNET, and JINA. These models are chosen for their varying capabilities in handling different languages and computational efficiencies. The text data in posts is then encoded into embeddings using each of these models. This step transforms the textual data into numerical vectors that can be used for similarity calculations and other machine learning tasks.\nModel Training and Evaluation : To train and evaluate the models, a neural network (ANN) is created with input layers for the embeddings, followed by dense layers with ReLU activation functions. A custom cosine similarity function is used to measure the similarity between the embeddings. The model is compiled using the Adam optimizer and mean squared error loss. This ANN model helps in capturing the complex patterns within the embeddings and normalizes the input vectors to ensure accurate similarity calculations. The training involves loading .npy files containing the embeddings and fitting the model on these embeddings for evaluation.\nPipeline Setup for Evaluating Similarity : The approach involved setting up a pipeline to process the results obtained from the time decay analysis conducted on various models using user data and their tweet timelines. This pipeline was designed to incorporate activity data, such as likes, quote tweets, and retweets, to further evaluate the similarity between user profiles and their interactions. By systematically processing the embeddings through this pipeline, the additional activity data provided an enhanced dimension for accurately assessing similarities and making more precise activity recommendations.\nResult Compilation : The results are compiled into a DataFrame for further analysis. This involves iterating through the directory containing the .npy files, training the model on each file, and storing the evaluation results in a dictionary. These results are then transformed into a DataFrame format, which facilitates easy comparison and visualization."}, {"title": "Model Creation and Training", "content": "Algorithm: Neural Network Model Creation and Training\nImport necessary modules from tensorflow.keras.\nDefine function create_model():\na. Initialize input layers input1 and input2 with shape (512", "function": "nNormalize input x and y.\nReturn sum of element-wise multiplication of x and y.\nd. Compile model with inputs [input1", "input2": "and output cos_sim", "adam": "loss 'mean_squared_error'.\ne. Return model.\nDefine function train_evaluate(file", "e)": "na. Load input_1 from file.\nb. Set input_2 as e.\nc. Initialize model using create_model"}]}