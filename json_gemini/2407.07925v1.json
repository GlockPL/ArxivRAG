{"title": "Enhancing Social Media Personalization: Dynamic User Profile Embeddings and Multimodal Contextual Analysis Using Transformer Models", "authors": ["Pranav Vacharajani", "Pritam Ranjan"], "abstract": "This study investigated the impact of dynamic user profile embedding on personalized context-aware experiences on social networks. A comparative analysis of multilingual and English transformer models was performed by analyzing a dataset of over twenty million data points It involved analyzing a wide range of metrics and performance indicators to compare dynamic profile embeddings vs. non-embeddings. effectively static profile embeddings. A comparative study was conducted using degradation functions. Extensive testing and extensive research confirmed that dynamic embedding successfully tracks users' changing tastes and preferences,\nprovides more accurate recommendations and user engagement Thus these results are important for social media platforms that aim to improve user experience through relevant features and sophisticated recommendation engines.", "sections": [{"title": "INTRODUCTION:", "content": "2.1. Background information and context: In the rapidly evolving digital age, social media platforms are constantly looking for ways to increase user engagement and satisfaction.. Personalized content recommendations are central to this pursuit, [1]. requiring the development of sophisticated analytical techniques that can adapt to user behavior and preferences.. Transformer models known for their effectiveness in a variety of natural language processing tasks offer promising approaches has improved recommendation processes through embeddings of dynamic user profiles. [2]\n2.2. Problem Statement: Traditional static user profile embeddings often fail to capture the temporal evolution of user interactions.. This results in inadequate recommendations. This limitation presents a challenge in maintaining user engagement, as recommendations do not reflect recent changes in user preference or behavior. [3]\n2.3. Purpose and Significance of the Study: The main purpose of this study is to investigate the effectiveness of dynamic user profile embedding, using different decay functions, to accurately track and display the changes in user interest over time. The objectives of this study are:\n\u2022 Evaluate how different the decay functions influence the relevance and accuracy of recommendations.\n\u2022 Compare the performance of multiple transformer-based models in handling dynamic embeddings..\n\u2022 Explore the implication of these finding for enhancing personalized user experiences on SM platforms."}, {"title": "LITERATURE REVIEW:", "content": "3.1 Dynamic User Profile Embeddings\nIn order to deal with the dynamic nature of user interests and behaviours on the social media platforms like Twitter, Facebook, Instagram.. researchers have looked into dynamic user profile embeddings for better personalised recommendation system. In order to capture the temporal changes in user profiles, Liang et al. (2018) [4] suggested dynamic user and word embeddings, with the aim to capture the temporal changes in user profiles and activities. This can be instrumental in enhancing personalization.\nSimilar to this, Kerin et al. (2019) [5] highlighted the significance of temporal word embeddings for creating temporal user profiles, with the main driving force for their research being the dynamism present in user profiles. Furthermore, Zheng et al. (2018)[6] created a technique called user profile embedding that allows to capture similarities among users in social networks through user profile embedding (Zheng et al., 2018)[6]\n3.2 Multimodal Contextual Analysis Using Transformer Models\nTransformer models [7] ,especially those based on self-attention mechanisms, have revolutionised natural lan-"}, {"title": "Social Media Recommender Systems Approaches", "content": "Recommender systems (RS) are (Eirinaki et al., 2018) [11] used by a variety of social media platforms to improve user experience through engagement and content personalisation. Anandhan et al. (Alamdari et al., 2020)[12] examined methods in social media RS, emphasising the use of collaborative filtering and content-based filtering, frequently in hybrid forms, and concentrating on context-awareness, scalability issues, and decision analysis. In their 2018 study, Eirinaki et al. (Eirinaki et al., 2018) [11] concentrated on the handling of user-generated content and volatile social interactions by RS in large-scale networks, highlighting the necessity of scalable solutions that can adjust to changing user preferences. Zhou et al. (Zhou et al., 2012) [13] talked about the development of personalised RS in social networking, including its history, technological advances, and social settings improvements."}, {"title": "E-commerce Recommender Systems Approaches", "content": "Recommendation systems (RS) improve the e-commerce shopping experience and boost sales. An overview of RS techniques, such as collaborative filtering, content-based filtering, knowledge-based systems, and hybrid systems, were given by Alamdari et al. (Alamdari et al., 2020) [12] . Sivapalan et al. (Sivapalan et al., 2014) [14] investigated how RS increases the effectiveness of e-commerce by recognising patterns in user behaviour and making product recommendations in line with those patterns. Furthermore, Schafer et al. (Schafer et al., 1999) [15] . described how RS helps e-commerce sites by making product recommendations based on user ratings, browsing history, and past purchases."}, {"title": "Main Approaches for Recommender Systems", "content": "In the digital world, recommender systems are essential because they direct user experiences by making relevant suggestions for products, content, and social interactions based on the interests and actions of the user. These systems\nuse a range of approaches, each with special advantages and modifications for distinct platforms, such as social media (SM) websites and e-commerce platforms. The main approaches and their applications across notable platforms are mentioned below. [3], [12]\n3.5.1 Collaborative Filtering (CF)\nIn recommender systems, one of the most popular methods is collaborative filtering. It bases its recommendations on user preferences as a whole, presuming that users who have previously agreed will do so in the future.\n\u2022 User-Based CF: This technique is used by eBay and Amazon to make product recommendations by comparing users based on past purchases. [16]\n\u2022 Item-Based CF: YouTube uses this method to make video recommendations by analyzing the similarities in user interactions with different content [17].\n3.5.2 Content-Based Filtering (CBF)\nBased on feature similarity, content-based filtering suggests products that are similar to those a user has previously favoured. One of the best examples of this is Netflix, which provides recommendations to users based on the genres, actors, and directors of films and TV shows they have already seen[18].\n3.5.3 Hybrid Approaches\nHybrid systems combine content-based filtering and collaborative filtering to avoid certain limitations of each approach. Spotify employs this technique to provide personalised music recommendations by combining user behaviour (collaborative data) and song audio features (content-based data).[16]\n3.5.4 Knowledge-Based Systems\nThese systems recommend products based on explicit knowledge about the item assortment, user preferences, and recommendation criteria. This approach is often used in situations where items are not purchased frequently, such as on real estate or job sites like LinkedIn[19] .\n3.5.5 Demographic-Based Recommendations\nThis approach makes product recommendations based on user demographic data. Facebook and Instagram match recommendations with users' age, gender, location, and interests by using demographic data to target advertising and content[20].\n3.5.6 Utility-Based Systems\nProducts are recommended by utility-based recommender systems based on an assessment of their value to a specific user. This approach is used by Amazon's Alexa to make product recommendations based on the usefulness or practicality determined from user interactions and queries[21].\n3.5.7 Session-Based Recommendations\nThese systems provide recommendations based on what was done during a session. This method is common in ecommerce settings for brief sessions; it can be observed on platforms such as Alibaba, where real-time product recommendations are provided based on user interactions within a session[22]."}, {"title": "Identification of Gaps in the Literature", "content": "The emphasis on text in current models often obscures the dynamic user behaviours and multimodal data that are crucial for comprehending user preferences. (Recommender Systems Handbook.pdf; Social Media Recommendation Algorithms Tech Primer.pdf) [16], [27]. Furthermore, a lot of models don't make full use of the real-time data processing that's required to capture ephemeral social media trends. The embedding which is very important aspect is needs to be addressed."}, {"title": "How the Current Research Addresses These Gaps", "content": "This study uses transformer models to create a model that combines multimodal data with dynamic user profile embeddings. By updating user profiles dynamically in realtime, it overcomes the drawbacks of static profiling. The model seeks to improve contextual relationship understanding by utilising transformer technology, which will allow for more precise user preference predictions and greatly improved personalised recommendations."}, {"title": "METHODOLOGY", "content": "4.1 Research Design and Methods\nThe study adopted a quantitative research design to systematically check the effectiveness of dynamic user profile embeddings as compared to static ones.. Various transformer models, [28] such as multilingual and English-specific ones, have been analyzed the use of special decay capabilities [29] to understand their effect on the accuracy and relevancy of content recommendations on social networks.\n4.2 Tools and Techniques for Data Analysis\nThe analysis was conducted using several advanced data processing and machine learning tools:\n\u2022 Python: Used for scripting and automating the data analysis process, including data cleaning and transformation tasks.\n\u2022 Pandas and NumPy: Employed for efficient data manipulation and numerical analysis.\n\u2022 Scikit-learn: Utilized for implementing machine learning models and conducting statistical tests to compare the effectiveness of different embedding techniques.\n\u2022 TensorFlow and PyTorch: Applied for training and evaluating deep learning models, particularly for developing and testing the multimodal profile embeddings.\n\u2022 Sentence Transformers Library: Used to generate and manipulate embeddings from the transformer models.\n\u2022 GPU: Kaggle's NVIDIA TESLA P100 GPU was used as the process required high computing power.\n\u2022 GitHub: GitHub repositories were used for keeping the codes [30]"}, {"title": "DATA COLLECTION:", "content": "5.1 Identifying Influential Personalities:\nTo start with data collection exercise, we identified the top 100 most followed personalities on the Twitter (now its X), from various fields & regions globally, reflecting diverse interests. This selection aimed to cover a broad spectrum of user engagement.\n5.2 User Data Compilation:\nUtilizing the Twitter API, we extracted data from these 100 influential users along with their 1,000 most active followers each, culminating in a dataset comprising 100,100 individual user profiles..\n5.3 Tweet Data Acquisition:\nWe proceeded to gather timeline data for these users, resulting in approx 20 million data points. Post-processing, which included the removal of duplicates, refined the dataset to the final count of 14,765,661 (14M) data points.\n5.4 User Activity Data:\nTo analyze user interactions such as... likes, Quo Tweets, and ReTweets, we collected user activity data. For evaluative purposes, we randomly selected 30,000 instances from this dataset. The total of all liked tweets from these selected instances amounted to 2,107,054 (2M) data points.\nThis extensive data collection phase spanned approximately three weeks, constrained by the rate limits by the Twitter API. [31]\n5.5 Concatenation of the data\nAfter downloading the Twitter activity data, the next step involves processing the data using a comprehensive approach that includes concatenation, parsing, cleaning, and storage. Multiple JSON and CSV files containing Twitter activity timelines and user tweets are imported from various sources. Data from CSV files are read into Pandas DataFrames, with columns parsed and relevant fields extracted, followed by filtering to include specific information such as timestamps, user names, user descriptions, and tweet texts. Parsed data from individual files are appended into a list of DataFrames, which are then concatenated into a"}, {"title": "ALGORITHM FLOW CHART", "content": "A dynamic profile embedding method for social media data is shown in the flow chart. The first step in the process is gathering user information from Twitter (Now X), such as their bio and tweets. Hybrid tweet embeddings are produced by processing each tweet along with the user bio through an encoding model. A decay function is applied to these embeddings to adjust their weights based on their temporal relevance and recentness. A dynamic profile embedding that reflects the user's changing interests and preferences is the result. The most advantageous aspect of this strategy is its capacity to dynamically update the user profile with the most recent data, guaranteeing that recommendations and personalisations are current and pertinent. By utilising the most recent user activity data, this dynamic feature improves the accuracy of predictions pertaining to user engagement, such as predicting likes or follows."}, {"title": "TIME DECAY FUNCTIONS AND DYNAMIC E\u043cBEDDINGS ANALYSIS", "content": "7.1 Overview\nTime Decay Functions and Dynamic Embeddings Analysis leverages various pre-trained models from the Sentence Transformers library (https://sbert.net/docs/sentence_transformer/pretrained_models.html ) to create embeddings from the textual data and than applies different time decay functions on these embeddings. Main goal is to know that how time influences the representation of textual data in a dynamic context.\nWith the application of these time decay functions, the code aims to simulate real-world scenarios where the relevance of information changes over time..\n7.2 Approach\nThe approach to analyzing the impact of various time decay functions, various models and similarities and diversities on dynamic embeddings covers many steps.. These steps are explained as below:\n7.2.1 Data Loading and Data Preparation\nData Acquisition and Preprocessing: Data is been collected, now its necessary to convert and preprocess these data in the format which can be analysed."}, {"title": "Model Selection and Embedding Calculation", "content": "7.2.2 Model Selection and Embedding Calculation\nModel Selection: Based on the requirement, and existing resources, computing power, select appropriate pre-trained models. requirements.\n7.2.3 Application of Time Decay Functions\nDecay Function Selection: Analyse the application and relevance of various decay functions and choose a variety of decay functions (e.g., exponential, logarithmic) to model to catch the relevance of data over time.\n7.2.4 Integration and Assessment of Similarity Measures\nSimilarity Metric Integration: Employ Basic, Cosine, and Cos-time similarities to evaluate changes in embeddings.\n7.3 Uniqueness of the Approach\n7.3.1. Diverse Model Usage: Utilisation of various pre-trained models enables a comprehensive evaluation across different data types and model architectures, illustrating how each model captures and represents data dynamics.\n7.3.2. Variety of Time Decay Functions: By experimenting with multiple decay functions, the analysis gains depth in understanding the differential impact of time on data relevance, catering to diverse application needs where the freshness of data varies in importance.\n7.3.3. Integration of Multiple Similarity Measures: The approach not only applies traditional similarity metrics but also explores their integration into the decay functions. Incorporating a range of similarity measures, specifically Basic, Cosine, and Cos-time similarities, into the decay functions introduces a novel way to adjust embeddings based on temporal and semantic changes. This varied approach not only assesses the immediate similarity but also considers the evolution of these similarities over time, offering a multi-dimensional analysis of data dynamics.\n7.4 Model Selection and Embedding Calculation\nFollowing table gives and overview of the existing selective sentence transformer models.\nFour different pre-trained models from the Sentence Transformers library are selected for our research:\n\u2022 all-MiniLM-L6-v2: Small and fast. (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 )\n\u2022 distiluse-base-multilingual-cased-v2: Multilingual. (https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2 )\n\u2022 all-mpnet-base-v2: Large with extensive knowledge.(https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n\u2022 jinaai /jina-embeddings-v2-base-en: State-of-the-art (SOTA). (https://huggingface.co/jinaai/jina-embeddings-v2-base-en)\nAlgorithm: Generate Text Embeddings with Multiple Models\nInput: DataFrame columns 'bio' and 'tweettext'\nOutput: Embeddings files for each model\nProcedure:"}, {"title": "Similarities Calculation: To measure the effective-ness of the time decay functions, specifically Basic, Cosine, and Cos-time similarities between consecutive pairs of embeddings is computed.", "content": "Algorithm: Apply Time and Cosine-Similarity Adjusted Decay Functions to Embeddings\nInput:\ntimeline: Array of time points (can be datetime objects)\nembeddings: Matrix of embeddings corresponding to each time point\nk: Decay parameter\nOutput:\nAdjusted embeddings after applying decay\nProcedure:\nDefine compute_cosine_similarity(embeddings):\nCompute pairwise cosine similarity of the embeddings\nExtract similarities for consecutive embedding pairs\nReturn the consecutive pair similarities\nDefine compute_time_differences (timeline):\nCalculate the time differences between consecutive timestamps in the timeline\n- Convert time differences to seconds if using datetime objects\n- Return the time differences\nDefine decay_functions(timeline, embeddings, k) for each decay type:\nFetch delta_t from compute_time_differences (timeline)\nFetch cos_sim from compute_cosine_similarity(embeddings)\nFor each decay function:\nExponential Decay:\nCompute decay as $\\exp(-k * a * \\delta\\_t / cos\\_sim)$\nInverse Linear Decay:\nCompute decay as $1 / (1 + k * a * \\delta\\_t / cos\\_sim)$\nInverse Square Root Decay:\nCompute decay as $1 / \\sqrt{1 + k * a * \\delta\\_t / cos\\_sim}$\nHyperbolic Decay:\nCompute decay as $1 / (1 + (k* a^2 * \\delta\\_t) / cos\\_sim)$\nLogarithmic Decay:"}, {"title": "Diversity Measurement:", "content": "The diversity of the embeddings is measured using different similarities, assessing how similar or diverse the embeddings are from each other after the decay functions have been applied.\nCombined Diversity Matrix Across Models and Decay Functions\n7.10 Model based performance matrix\n7.11 Similarities based performance matrix"}, {"title": "OF COMPREHENSIVE ANALYSIS FINDINGS ACROSS DECAY FUNCTIONS, DIVERSITY METRICS, AND MODEL PERFORMANCE", "content": "8.1 Overview This analysis integrates various pre-trained models-MiniLM, DistilUSE Multilingual, MPNet, and Jina-to examine their performance across different decay functions and manage diversity metrics: basic, cosine, and cosine-time similarities.\nAnalysis of Output Matrix and Graph\n8.1.1 Exponential Decay (exp): Shows moderate diversity in basic metrics, high diversity in cosine similarity, and excellent temporal consistency. This balance makes it suitable for environments where both recent and historical data influences are essential.\n8.1.2Inverse Linear (inv_lin) and Square Root (inv_sqrt ): These decays show a graduated reduction in diversity metrics, with inv_sqrt slightly outperforming inv_lin in handling outliers. This suggests their use in applications where historical data gradually diminishes in influence.\n8.1.3 Hyperbolic Decay: Achieves the highest basic diversity, indicating its effectiveness in distinguishing between data points over a shorter term. However, its lower consistency over time may limit its application in long-term predictive modeling.\n8.1.4 Logarithmic Decay: Similar to inverse functions but offers the slowest reduction in relevance, ideal for scenarios requiring long-term data retention without significant decay.\n8.1.5 Gaussian Decay: Stands out with the highest cosine similarity, pointing to its efficiency in emphasizing recent data trends, making it highly suitable for real-time analytical applications.\n8.2. Summary of Model Performances Across Decay Functions\n\u2022 MiniLM : Consistently high performance in maintaining diversity across all metrics. Excels in cosine similarity across all decay functions, suggesting its effectiveness in capturing semantic relationships in multilingual contexts. Best under Gaussian decay, indicating robustness in adapting to recent data changes.\n\u2022 DistilUSE Multilingual: Exhibits lower basic and cosine similarities compared to MiniLM, indicating a slight reduction in its ability to maintain diversity over time. Performs best under Gaussian decay for cosine similarity, aligning with MiniLM in terms of emphasizing recent data.\n\u2022 MPNet: Shows a good balance across all diversity metrics, slightly outperforming DistilUSE in cosine similarity. This reflects its deep semantic understanding capabilities, especially under exponential and Gaussian decays. Best performance noted under Gaussian decay for both basic and cosine similarities.\n\u2022 Jina: Registers the lowest diversity scores among the models, particularly in basic similarity, which might be due to its focus on similarity rather than diversity, as it is optimized for relevance-focused tasks like RAG models. Despite its lower scores, it shows a respectable performance in cosine similarity under Gaussian decay."}, {"title": "Comparative Analysis and Diversity Metrics", "content": "\u2022 Cosine-Time Decay's Superiority: Especially effective in capturing the maximum diversity across models, suggesting its utility in analyzing the evolution of data relationships over time.\n\u2022 Decay Function Effectiveness: Gaussian decay emerges as the top performer, especially when recent data relevance is paramount. In contrast, logarithmic and hyper-"}, {"title": "SUPERVISED EVALUATION USING ACTIVITY DATA", "content": "9.1 Overview\nSupervised Evaluation Using Activity Data aimed at analyzing User Activity Data to recommend activities based on user profiles and post embeddings. The first part involved time decay analysis with various decay functions and different models on User Data, including 100,100 individual user profiles and Tweet Data, resulting in approximately 20 million data points. In this second part, we analyze User Activity Data, such as likes, quote tweets, and retweets, by randomly selecting 30,000 instances, resulting in a total of 2,107,054 (2M) data points for liked tweets.\n9.2 Unique Approach\nThe unique approach in this research involves several key elements that enhance the recommendation system:\n9.2.1 Using Activity Data: By incorporating user activity data such as likes, quote tweets, and retweets, on top of user profile data and timeline data, the model gains an additional dimension to compute similarities. This comprehensive approach captures both static user characteristics and dynamic interactions, improving the relevance and accuracy of the recommendations.\n9.2.1. Pipeline Creation: A well-structured pipeline was created for loading data, preprocessing, encoding using multiple embedding models, training, and evaluation. This pipeline ensures a systematic and reproducible process for analyzing and modeling the data.\n9.2.2. Multiple Embedding Models: The use of multiple embedding models (MiniLM, DistilUse Multilingual, MPNET, and JINA) captures different aspects of the user activity data. Each model brings unique strengths, and their comparative analysis helps in identifying the best performer.\n9.2.3. Cosine Similarity Metric: Utilizing cosine similarity as a metric for recommendation enhances the relevance of the suggestions by measuring the cosine of the angle between two non-zero vectors. This metric is particularly effective in high-dimensional spaces typical of embeddings.\n9.2.4. Neural Network Model with Normalization: A neural network (ANN) model with normalization layers and mean squared error loss was developed. This model includes dense layers with ReLU activation functions to capture complex patterns in the data. The normalization step ensures that the input vectors are on the same scale, which is crucial for accurate similarity calculations.\n9.3. Explanation of the steps adopted:\n9.3.1. Data Loading and Preprocessing : The initial step"}, {"title": "Algorithm: Neural Network Model Creation and Training", "content": "Input:\ne1, e2, e3, e4: Encoded embeddings for each post using four different models\ndirectory: Directory path containing .npy files\nOutput:\nResults: Dictionary containing evaluation results\nProcedure:\nImport necessary modules from tensorflow.keras.\nDefine function create_model(): a. Initialize input layers input1 and input2 with shape (512,). b. Define dense layers densel and dense2 with ReLU activation. c. Define custom cosine similarity function:\nNormalize input x and y.\nReturn sum of element-wise multiplication of x and y.\nd. Compile model with inputs [input1, input2] and output cos_sim, optimizer 'adam', loss 'mean squared_error'. e. Return model.\nDefine function train_evaluate(file, e): a. Load input_1 from file. b. Set input_2 as e. c. Initialize model using create_model(). d. Fit model on input_1 and input_2 for 1 epoch. e. Return mean of sigmoid predictions on input_1 and input_2.\nInitialize an empty dictionary results.\nEvaluate Models: a. For each file in directory:\nIf file ends with '.npy':\nIf file starts with '1':\nStore result of train_evaluate (file, e1) in results[file].\nElse if file starts with '2':\nStore result of train_evaluate (file, e2) in results[file].\nElse if file starts with '3':\nStore result of train_evaluate(file, e3) in results[file].\nElse if file starts with '4':\nStore result of train_evaluate(file, e4) in results[file].\nEnd Procedure\n9.3.7. Combined Supervised Evaluation matrix Using Activity Data:\nThis table summarizes the evaluation results for each model (MiniLM, DistilUse, MPNET, JINA) across different decay functions (Gaussian, Inv_Sqrt, Logarithmic, Exp, Inv_Lin, Hyperbolic) and metrics (Basic, Cos, Cos_Time).\n9.3.8. Findings from the Combined Supervised Evaluation Using Activity Data\n9.3.8.1 Overall Performance: The Jina model consistently outperformed the other models across most metrics and decay functions, achieving the highest scores in both Cosine and Cos-Time metrics. Jina performs the best for recommending activities accurately, effectively matching the profile embeddings to the post embeddings.\n9.3.8.2 Model Ranking: The overall ranking of models based on their performance is: Jina > DistilUSE Multilingual > MPNet > MiniLM.\n9.3.8.3 Gaussian Decay Function: For the Gaussian decay function, Jina achieved a perfect score in the Cos-Time metric (1.000000), indicating its superior ability to capture activity data dynamics. DistilUSE Multilingual and MiniLM also performed well, but Jina was the clear leader.\n9.3.8.4 Inverse Square Root Decay Function: Jina again performed exceptionally well, with perfect scores in the CosTime metric. DistilUSE Multilingual followed closely, while MiniLM and MPNet showed lower performance compared to Jina.\n9.3.8.5 Logarithmic Decay Function: The Jina model led in the Cos-Time metric with a score of 0.965639. DistilUSE Multilingual and MPNet showed good performance, but MiniLM lagged behind.\n9.3.8.6 Exponential Decay Function: Jina scored highest in the Cos-Time metric, followed by DistilUSE Multilingual and MPNet. MiniLM had the lowest scores among the models for this decay function.\n9.3.8.7 Inverse Linear Decay Function: Jina maintained the lead with the highest scores across all metrics, partic-"}, {"title": "Findings: Static vs Dynamic Embeddings", "content": "The graph illustrates that dynamic embeddings generally exhibit higher diversity scores compared to static embeddings across all models. This supports the observation that dynamic embeddings are better at capturing a broader and more up-to-date range of user behaviors and preferences.\nThis visualization clearly shows that dynamic embeddings significantly outperform static embeddings in accuracy across all models. This supports the conclusion that dynamic embeddings are more effective in applications\nrequiring precise and up-to-date personalization, such as recommendation systems on social media platforms.\n10.5.1 Adaptability and Relevance:\nDynamic embeddings demonstrate high adaptability with scores approaching 1.0 in Cos-Time metrics (e.g., MiniLM: 0.9995, MPNet: 0.9999), showing their ability to stay current with evolving user profiles.\nStatic embeddings, in contrast, maintain a consistent diversity measurement (e.g., MiniLM: 0.238, MPNet: 0.362) which does not reflect changes over time."}, {"title": "Diversity and Accuracy:", "content": "Dynamic embeddings often achieve higher diversity scores (e.g., MPNet in Cosine metric: 0.843 under Gaussian decay) compared to static embeddings, indicating a better capture of a broad range of user behaviors.\nAccuracy of dynamic embeddings in real-time applications also tends to be higher. For example, Jina shows a Cos-Time metric score of 1.000 in the Gaussian decay setting, highlighting its precision in capturing user preferences dynamically.\nnotably suited for real-time applications, shown by high Cos-Time scores, indicating they adjust quickly to new user data (Jina: 1.000, DistilUSE Multilingual: 0.933).\n10.5.6. Computational Efficiency:\nWhile dynamic embeddings offer updated insights, they require ongoing computation as new data arrives, contrasting with static embeddings which are computed once (e.g., static diversity for MiniLM: 0.238 vs dynamic diversity under Gaussian: 0.741)."}, {"title": "Performance Across Decay Functions:", "content": "Exponential and Gaussian decays are especially effective in prioritizing recent data. Gaussian decay shows high scores across all models in the Cos-Time metric (e.g., Jina: 1.000, MiniLM: 0.9995).\n10.5.4. Logarithmic and hyperbolic decays maintain relevance of older data better, with slower reduction rates (e.g., Logarithmic Cos-Time for MiniLM: 0.9999), useful in applications needing long-term behavioral analysis.\nSuitability for Real-Time Applications:\n10.5.5. Dynamic embeddings using Gaussian decay are"}, {"title": "Application Specificity:", "content": "Dynamic embeddings are recommended for environments with frequent user interaction changes, as shown by their superior performance in dynamic settings (e.g., Jina's consistent high scores in dynamic settings).\nStatic embeddings may still be useful in stable, less change-intensive environments, offering a simpler computational approach without the need for frequent updates."}, {"title": "INTERPRETATION OF THE RESULTS IN THE CONTEXT OF THE RESEARCH QUESTION", "content": "The study explores the efficacy of different decay functions and embedding models in capturing user behavior dynamics for recommendation systems. The findings suggest that Gaussian and exponential decay functions, when combined with models like Jina, MiniLM and MPNet, effectively prioritize recent interactions while still maintaining historical context, thereby enhancing recommendation systems' accuracy and relevance. This addresses the research question concerning how different temporal decay functions influence the adaptability of dynamic embeddings in social media environments."}, {"title": "COMPARISON WITH PREVIOUS STUDIES", "content": "12.1 Decay Function Effectiveness:\n\u2022 Similarity with Existing Work: The study confirms the effectiveness of Gaussian decay in capturing recent data trends, which is consistent with the findings of Covington et al. (2016) [23] in their study on deep neural networks for YouTube recommendations.\n\u2022 Differences: The study also highlights the importance of logarithmic and hyperbolic decays in maintaining longer historical contexts, which is not explicitly discussed in Covington et al. (2016). [23]\n12.2 Model Performance:\nSimilarity with Existing Work: The study shows that MiniLM consistently performs well across all metrics, which is consistent with the findings of Chen et al. (2018) in their survey on session-based recommender systems. [22]\nDifferences: The study also highlights the strengths of MPNet in balancing diversity metrics and its ability to adapt to recent data changes, which is not explicitly discussed in Chen et al. (2018).\n12.3 Diversity Metrics:\nSimilarity with Existing Work: The study emphasizes the importance of cosine similarity in capturing semantic relationships, which is consistent with the findings of He and McAuley (2016) [25] in their study on modeling the visual evolution of fashion trends with one-class collaborative filtering.\nDifferences: The study also highlights the effectiveness of cosine-time decay in capturing the maximum diversity across models, which is not explicitly discussed in He and McAuley (2016). [25]"}, {"title": "RECOMMENDATIONS:", "content": "13.1. Similarity with Existing Work: The study recommends a balanced approach to decay functions and model selection based on application requirements, which is consistent with the findings of Sarwar et al. (2001)[32] in their study on item-based collaborative filtering recommendation algorithms.\n13.2. Differences: The study provides more specific recommendations for real-time systems and historical data analysis, which are not explicitly discussed in Sarwar et al. (2001). [32]"}, {"title": "REAL-WORLD DATA:", "content": "The study's use of real-world data from Twitter, including user profiles and tweet activity, provides a more practical and applicable context for the evaluation of recommender systems. This aligns with the focus on real-world data in other studies, such as Archak (2010) [24], which analyzed strategic behavior in crowdsourcing contests on TopCoder.com."}, {"title": "SUGGESTIONS FOR FUTURE RESEARCH", "content": "\u2022 Exploring Additional Models: Future research could explore the integration of newer or less common machine learning models that might offer improved performance or efficiency.\n\u2022 Cross-platform Validation: Validating the findings across different social media platforms or even in different contexts (like e-commerce or content streaming services) could help in understanding the broader applicability of the proposed methods.\n\u2022 Real-time Implementation: Investigating the real-time implementation of these models and decay functions in live environments could provide insights into practical challenges and performance issues.\n\u2022 User Feedback Incorporation: Incorporating user feedback mechanisms to dynamically adjust the decay rates and model parameters could enhance the adaptability and accuracy of recommender systems.\n\u2022 Ethical and Privacy Considerations: Further research could also explore the ethical implications and privacy concerns related to the use of dynamic embeddings in user profiling."}, {"title": "SUMMARY OF KEY FINDINGS", "content": "\u2022 Decay Function Effectiveness : The study validates the effectiveness of Gaussian decay for emphasizing recent interactions,aligning with existing literature that underscores the importance of recency in recommender systems. Also it highlights the unique value of logarithmic and hyperbolic decays in keeping historical context, offering a more nuanced approach to temporal data analysis in dynamic embeddings.\n\u2022 Model Performance: From the various models tested, Jina, MiniLM and MPNet demonstrated robust performance, excelling in balancing recent and historical data influences. This suggests their suitability for complex recommendation tasks that require a nuanced understanding of user behaviors over time..\n\u2022 Diversity Metrics: The use of cosine and cosine-time similarity metrics were particularly effective in capturing semantic relationships and maximizing diversity across models, pointing towards their potential in enhancing the accuracy and relevance of recommendations. ."}, {"title": "FINAL REMARKS ON THE STUDY'S CONTRIBUTIONS", "content": "This research contributes significantly to the field of recommender systems by offering a comprehensive evaluation of how different decay functions affect the performance of dynamic embeddings. It bridges a gap in existing research by systematically analyzing the impact of less commonly used decay functions and by exploring their implications in real-world social media data. The study's methodological rigor and detailed analysis provide valuable benchmarks for future research and practical applications in dynamic user profiling."}, {"title": "POTENTIAL APPLICATIONS OF THE RESEARCH", "content": "\u2022 Social Media Platforms: Adopting the approach and the findings of the study can help various social-media platforms including multilingual platforms to refine their content recommendation algorithms for better alignments with user preferences", "E-commerce": "Online retailers", "Services": "OTT platforms like Netflix or Spotify", "recommendations": "keeping them fresh and relevant to user choice that change```json\n{"}, {"title": "Enhancing Social Media Personalization: Dynamic User Profile Embeddings and Multimodal Contextual Analysis Using Transformer Models", "authors": ["Pranav Vacharajani", "Pritam Ranjan"], "abstract": "This study investigated the impact of dynamic user profile embedding on personalized context-aware experiences on social networks. A comparative analysis of multilingual and English transformer models was performed by analyzing a dataset of over twenty million data points It involved analyzing a wide range of metrics and performance indicators to compare dynamic profile embeddings vs. non-embeddings. effectively static profile embeddings. A comparative study was conducted using degradation functions. Extensive testing and extensive research confirmed that dynamic embedding successfully tracks users' changing tastes and preferences,\nprovides more accurate recommendations and user engagement Thus these results are important for social media platforms that aim to improve user experience through relevant features and sophisticated recommendation engines.", "sections": [{"title": "INTRODUCTION:", "content": "2.1. Background information and context: In the rapidly evolving digital age, social media platforms are constantly looking for ways to increase user engagement and satisfaction.. Personalized content recommendations are central to this pursuit, [1]. requiring the development of sophisticated analytical techniques that can adapt to user behavior and preferences.. Transformer models known for their effectiveness in a variety of natural language processing tasks offer promising approaches has improved recommendation processes through embeddings of dynamic user profiles. [2]\n2.2. Problem Statement: Traditional static user profile embeddings often fail to capture the temporal evolution of user interactions.. This results in inadequate recommendations. This limitation presents a challenge in maintaining user engagement, as recommendations do not reflect recent changes in user preference or behavior. [3]\n2.3. Purpose and Significance of the Study: The main purpose of this study is to investigate the effectiveness of dynamic user profile embedding, using different decay functions, to accurately track and display the changes in user interest over time. The objectives of this study are:\n\u2022 Evaluate how different the decay functions influence the relevance and accuracy of recommendations.\n\u2022 Compare the performance of multiple transformer-based models in handling dynamic embeddings..\n\u2022 Explore the implication of these finding for enhancing personalized user experiences on SM platforms."}, {"title": "LITERATURE REVIEW:", "content": "3.1 Dynamic User Profile Embeddings\nIn order to deal with the dynamic nature of user interests and behaviours on the social media platforms like Twitter, Facebook, Instagram.. researchers have looked into dynamic user profile embeddings for better personalised recommendation system. In order to capture the temporal changes in user profiles, Liang et al. (2018) [4] suggested dynamic user and word embeddings, with the aim to capture the temporal changes in user profiles and activities. This can be instrumental in enhancing personalization.\nSimilar to this, Kerin et al. (2019) [5] highlighted the significance of temporal word embeddings for creating temporal user profiles, with the main driving force for their research being the dynamism present in user profiles. Furthermore, Zheng et al. (2018)[6] created a technique called user profile embedding that allows to capture similarities among users in social networks through user profile embedding (Zheng et al., 2018)[6]\n3.2 Multimodal Contextual Analysis Using Transformer Models\nTransformer models [7] ,especially those based on self-attention mechanisms, have revolutionised natural lan-"}, {"title": "Social Media Recommender Systems Approaches", "content": "Recommender systems (RS) are (Eirinaki et al., 2018) [11] used by a variety of social media platforms to improve user experience through engagement and content personalisation. Anandhan et al. (Alamdari et al., 2020)[12] examined methods in social media RS, emphasising the use of collaborative filtering and content-based filtering, frequently in hybrid forms, and concentrating on context-awareness, scalability issues, and decision analysis. In their 2018 study, Eirinaki et al. (Eirinaki et al., 2018) [11] concentrated on the handling of user-generated content and volatile social interactions by RS in large-scale networks, highlighting the necessity of scalable solutions that can adjust to changing user preferences. Zhou et al. (Zhou et al., 2012) [13] talked about the development of personalised RS in social networking, including its history, technological advances, and social settings improvements."}, {"title": "E-commerce Recommender Systems Approaches", "content": "Recommendation systems (RS) improve the e-commerce shopping experience and boost sales. An overview of RS techniques, such as collaborative filtering, content-based filtering, knowledge-based systems, and hybrid systems, were given by Alamdari et al. (Alamdari et al., 2020) [12] . Sivapalan et al. (Sivapalan et al., 2014) [14] investigated how RS increases the effectiveness of e-commerce by recognising patterns in user behaviour and making product recommendations in line with those patterns. Furthermore, Schafer et al. (Schafer et al., 1999) [15] . described how RS helps e-commerce sites by making product recommendations based on user ratings, browsing history, and past purchases."}, {"title": "Main Approaches for Recommender Systems", "content": "In the digital world, recommender systems are essential because they direct user experiences by making relevant suggestions for products, content, and social interactions based on the interests and actions of the user. These systems\nuse a range of approaches, each with special advantages and modifications for distinct platforms, such as social media (SM) websites and e-commerce platforms. The main approaches and their applications across notable platforms are mentioned below. [3], [12]\n3.5.1 Collaborative Filtering (CF)\nIn recommender systems, one of the most popular methods is collaborative filtering. It bases its recommendations on user preferences as a whole, presuming that users who have previously agreed will do so in the future.\n\u2022 User-Based CF: This technique is used by eBay and Amazon to make product recommendations by comparing users based on past purchases. [16]\n\u2022 Item-Based CF: YouTube uses this method to make video recommendations by analyzing the similarities in user interactions with different content [17].\n3.5.2 Content-Based Filtering (CBF)\nBased on feature similarity, content-based filtering suggests products that are similar to those a user has previously favoured. One of the best examples of this is Netflix, which provides recommendations to users based on the genres, actors, and directors of films and TV shows they have already seen[18].\n3.5.3 Hybrid Approaches\nHybrid systems combine content-based filtering and collaborative filtering to avoid certain limitations of each approach. Spotify employs this technique to provide personalised music recommendations by combining user behaviour (collaborative data) and song audio features (content-based data).[16]\n3.5.4 Knowledge-Based Systems\nThese systems recommend products based on explicit knowledge about the item assortment, user preferences, and recommendation criteria. This approach is often used in situations where items are not purchased frequently, such as on real estate or job sites like LinkedIn[19] .\n3.5.5 Demographic-Based Recommendations\nThis approach makes product recommendations based on user demographic data. Facebook and Instagram match recommendations with users' age, gender, location, and interests by using demographic data to target advertising and content[20].\n3.5.6 Utility-Based Systems\nProducts are recommended by utility-based recommender systems based on an assessment of their value to a specific user. This approach is used by Amazon's Alexa to make product recommendations based on the usefulness or practicality determined from user interactions and queries[21].\n3.5.7 Session-Based Recommendations\nThese systems provide recommendations based on what was done during a session. This method is common in ecommerce settings for brief sessions; it can be observed on platforms such as Alibaba, where real-time product recommendations are provided based on user interactions within a session[22]."}, {"title": "Identification of Gaps in the Literature", "content": "The emphasis on text in current models often obscures the dynamic user behaviours and multimodal data that are crucial for comprehending user preferences. (Recommender Systems Handbook.pdf; Social Media Recommendation Algorithms Tech Primer.pdf) [16], [27]. Furthermore, a lot of models don't make full use of the real-time data processing that's required to capture ephemeral social media trends. The embedding which is very important aspect is needs to be addressed."}, {"title": "How the Current Research Addresses These Gaps", "content": "This study uses transformer models to create a model that combines multimodal data with dynamic user profile embeddings. By updating user profiles dynamically in realtime, it overcomes the drawbacks of static profiling. The model seeks to improve contextual relationship understanding by utilising transformer technology, which will allow for more precise user preference predictions and greatly improved personalised recommendations."}, {"title": "METHODOLOGY", "content": "4.1 Research Design and Methods\nThe study adopted a quantitative research design to systematically check the effectiveness of dynamic user profile embeddings as compared to static ones.. Various transformer models, [28] such as multilingual and English-specific ones, have been analyzed the use of special decay capabilities [29] to understand their effect on the accuracy and relevancy of content recommendations on social networks.\n4.2 Tools and Techniques for Data Analysis\nThe analysis was conducted using several advanced data processing and machine learning tools:\n\u2022 Python: Used for scripting and automating the data analysis process, including data cleaning and transformation tasks.\n\u2022 Pandas and NumPy: Employed for efficient data manipulation and numerical analysis.\n\u2022 Scikit-learn: Utilized for implementing machine learning models and conducting statistical tests to compare the effectiveness of different embedding techniques.\n\u2022 TensorFlow and PyTorch: Applied for training and evaluating deep learning models, particularly for developing and testing the multimodal profile embeddings.\n\u2022 Sentence Transformers Library: Used to generate and manipulate embeddings from the transformer models.\n\u2022 GPU: Kaggle's NVIDIA TESLA P100 GPU was used as the process required high computing power.\n\u2022 GitHub: GitHub repositories were used for keeping the codes [30]"}, {"title": "DATA COLLECTION:", "content": "5.1 Identifying Influential Personalities:\nTo start with data collection exercise, we identified the top 100 most followed personalities on the Twitter (now its X), from various fields & regions globally, reflecting diverse interests. This selection aimed to cover a broad spectrum of user engagement.\n5.2 User Data Compilation:\nUtilizing the Twitter API, we extracted data from these 100 influential users along with their 1,000 most active followers each, culminating in a dataset comprising 100,100 individual user profiles..\n5.3 Tweet Data Acquisition:\nWe proceeded to gather timeline data for these users, resulting in approx 20 million data points. Post-processing, which included the removal of duplicates, refined the dataset to the final count of 14,765,661 (14M) data points.\n5.4 User Activity Data:\nTo analyze user interactions such as... likes, Quo Tweets, and ReTweets, we collected user activity data. For evaluative purposes, we randomly selected 30,000 instances from this dataset. The total of all liked tweets from these selected instances amounted to 2,107,054 (2M) data points.\nThis extensive data collection phase spanned approximately three weeks, constrained by the rate limits by the Twitter API. [31]\n5.5 Concatenation of the data\nAfter downloading the Twitter activity data, the next step involves processing the data using a comprehensive approach that includes concatenation, parsing, cleaning, and storage. Multiple JSON and CSV files containing Twitter activity timelines and user tweets are imported from various sources. Data from CSV files are read into Pandas DataFrames, with columns parsed and relevant fields extracted, followed by filtering to include specific information such as timestamps, user names, user descriptions, and tweet texts. Parsed data from individual files are appended into a list of DataFrames, which are then concatenated into a"}, {"title": "ALGORITHM FLOW CHART", "content": "A dynamic profile embedding method for social media data is shown in the flow chart. The first step in the process is gathering user information from Twitter (Now X), such as their bio and tweets. Hybrid tweet embeddings are produced by processing each tweet along with the user bio through an encoding model. A decay function is applied to these embeddings to adjust their weights based on their temporal relevance and recentness. A dynamic profile embedding that reflects the user's changing interests and preferences is the result. The most advantageous aspect of this strategy is its capacity to dynamically update the user profile with the most recent data, guaranteeing that recommendations and personalisations are current and pertinent. By utilising the most recent user activity data, this dynamic feature improves the accuracy of predictions pertaining to user engagement, such as predicting likes or follows."}, {"title": "TIME DECAY FUNCTIONS AND DYNAMIC E\u043cBEDDINGS ANALYSIS", "content": "7.1 Overview\nTime Decay Functions and Dynamic Embeddings Analysis leverages various pre-trained models from the Sentence Transformers library (https://sbert.net/docs/sentence_transformer/pretrained_models.html ) to create embeddings from the textual data and than applies different time decay functions on these embeddings. Main goal is to know that how time influences the representation of textual data in a dynamic context.\nWith the application of these time decay functions, the code aims to simulate real-world scenarios where the relevance of information changes over time..\n7.2 Approach\nThe approach to analyzing the impact of various time decay functions, various models and similarities and diversities on dynamic embeddings covers many steps.. These steps are explained as below:\n7.2.1 Data Loading and Data Preparation\nData Acquisition and Preprocessing: Data is been collected, now its necessary to convert and preprocess these data in the format which can be analysed."}, {"title": "Model Selection and Embedding Calculation", "content": "7.2.2 Model Selection and Embedding Calculation\nModel Selection: Based on the requirement, and existing resources, computing power, select appropriate pre-trained models. requirements.\n7.2.3 Application of Time Decay Functions\nDecay Function Selection: Analyse the application and relevance of various decay functions and choose a variety of decay functions (e.g., exponential, logarithmic) to model to catch the relevance of data over time.\n7.2.4 Integration and Assessment of Similarity Measures\nSimilarity Metric Integration: Employ Basic, Cosine, and Cos-time similarities to evaluate changes in embeddings.\n7.3 Uniqueness of the Approach\n7.3.1. Diverse Model Usage: Utilisation of various pre-trained models enables a comprehensive evaluation across different data types and model architectures, illustrating how each model captures and represents data dynamics.\n7.3.2. Variety of Time Decay Functions: By experimenting with multiple decay functions, the analysis gains depth in understanding the differential impact of time on data relevance, catering to diverse application needs where the freshness of data varies in importance.\n7.3.3. Integration of Multiple Similarity Measures: The approach not only applies traditional similarity metrics but also explores their integration into the decay functions. Incorporating a range of similarity measures, specifically Basic, Cosine, and Cos-time similarities, into the decay functions introduces a novel way to adjust embeddings based on temporal and semantic changes. This varied approach not only assesses the immediate similarity but also considers the evolution of these similarities over time, offering a multi-dimensional analysis of data dynamics.\n7.4 Model Selection and Embedding Calculation\nFollowing table gives and overview of the existing selective sentence transformer models.\nFour different pre-trained models from the Sentence Transformers library are selected for our research:\n\u2022 all-MiniLM-L6-v2: Small and fast. (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 )\n\u2022 distiluse-base-multilingual-cased-v2: Multilingual. (https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2 )\n\u2022 all-mpnet-base-v2: Large with extensive knowledge.(https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n\u2022 jinaai /jina-embeddings-v2-base-en: State-of-the-art (SOTA). (https://huggingface.co/jinaai/jina-embeddings-v2-base-en)\nAlgorithm: Generate Text Embeddings with Multiple Models\nInput: DataFrame columns 'bio' and 'tweettext'\nOutput: Embeddings files for each model\nProcedure:"}, {"title": "Similarities Calculation: To measure the effective-ness of the time decay functions, specifically Basic, Cosine, and Cos-time similarities between consecutive pairs of embeddings is computed.", "content": "Algorithm: Apply Time and Cosine-Similarity Adjusted Decay Functions to Embeddings\nInput:\ntimeline: Array of time points (can be datetime objects)\nembeddings: Matrix of embeddings corresponding to each time point\nk: Decay parameter\nOutput:\nAdjusted embeddings after applying decay\nProcedure:\nDefine compute_cosine_similarity(embeddings):\nCompute pairwise cosine similarity of the embeddings\nExtract similarities for consecutive embedding pairs\nReturn the consecutive pair similarities\nDefine compute_time_differences (timeline):\nCalculate the time differences between consecutive timestamps in the timeline\n- Convert time differences to seconds if using datetime objects\n- Return the time differences\nDefine decay_functions(timeline, embeddings, k) for each decay type:\nFetch delta_t from compute_time_differences (timeline)\nFetch cos_sim from compute_cosine_similarity(embeddings)\nFor each decay function:\nExponential Decay:\nCompute decay as $\\exp(-k * a * \\delta_t / cos_sim)$\nInverse Linear Decay:\nCompute decay as $1 / (1 + k * a * \\delta_t / cos_sim)$\nInverse Square Root Decay:\nCompute decay as $1 / \\sqrt{1 + k * a * \\delta_t / cos_sim}$\nHyperbolic Decay:\nCompute decay as $1 / (1 + (k* a^2 * \\delta_t) / cos_sim)$\nLogarithmic Decay:"}, {"title": "Diversity Measurement:", "content": "The diversity of the embeddings is measured using different similarities, assessing how similar or diverse the embeddings are from each other after the decay functions have been applied.\nCombined Diversity Matrix Across Models and Decay Functions\n7.10 Model based performance matrix\n7.11 Similarities based performance matrix"}, {"title": "OF COMPREHENSIVE ANALYSIS FINDINGS ACROSS DECAY FUNCTIONS, DIVERSITY METRICS, AND MODEL PERFORMANCE", "content": "8.1 Overview This analysis integrates various pre-trained models-MiniLM, DistilUSE Multilingual, MPNet, and Jina-to examine their performance across different decay functions and manage diversity metrics: basic, cosine, and cosine-time similarities.\nAnalysis of Output Matrix and Graph\n8.1.1 Exponential Decay (exp): Shows moderate diversity in basic metrics, high diversity in cosine similarity, and excellent temporal consistency. This balance makes it suitable for environments where both recent and historical data influences are essential.\n8.1.2Inverse Linear (inv_lin) and Square Root (inv_sqrt ): These decays show a graduated reduction in diversity metrics, with inv_sqrt slightly outperforming inv_lin in handling outliers. This suggests their use in applications where historical data gradually diminishes in influence.\n8.1.3 Hyperbolic Decay: Achieves the highest basic diversity, indicating its effectiveness in distinguishing between data points over a shorter term. However, its lower consistency over time may limit its application in long-term predictive modeling.\n8.1.4 Logarithmic Decay: Similar to inverse functions but offers the slowest reduction in relevance, ideal for scenarios requiring long-term data retention without significant decay.\n8.1.5 Gaussian Decay: Stands out with the highest cosine similarity, pointing to its efficiency in emphasizing recent data trends, making it highly suitable for real-time analytical applications.\n8.2. Summary of Model Performances Across Decay Functions\n\u2022 MiniLM : Consistently high performance in maintaining diversity across all metrics. Excels in cosine similarity across all decay functions, suggesting its effectiveness in capturing semantic relationships in multilingual contexts. Best under Gaussian decay, indicating robustness in adapting to recent data changes.\n\u2022 DistilUSE Multilingual: Exhibits lower basic and cosine similarities compared to MiniLM, indicating a slight reduction in its ability to maintain diversity over time. Performs best under Gaussian decay for cosine similarity, aligning with MiniLM in terms of emphasizing recent data.\n\u2022 MPNet: Shows a good balance across all diversity metrics, slightly outperforming DistilUSE in cosine similarity. This reflects its deep semantic understanding capabilities, especially under exponential and Gaussian decays. Best performance noted under Gaussian decay for both basic and cosine similarities.\n\u2022 Jina: Registers the lowest diversity scores among the models, particularly in basic similarity, which might be due to its focus on similarity rather than diversity, as it is optimized for relevance-focused tasks like RAG models. Despite its lower scores, it shows a respectable performance in cosine similarity under Gaussian decay."}, {"title": "Comparative Analysis and Diversity Metrics", "content": "\u2022 Cosine-Time Decay's Superiority: Especially effective in capturing the maximum diversity across models, suggesting its utility in analyzing the evolution of data relationships over time.\n\u2022 Decay Function Effectiveness: Gaussian decay emerges as the top performer, especially when recent data relevance is paramount. In contrast, logarithmic and hyper-"}, {"title": "SUPERVISED EVALUATION USING ACTIVITY DATA", "content": "9.1 Overview\nSupervised Evaluation Using Activity Data aimed at analyzing User Activity Data to recommend activities based on user profiles and post embeddings. The first part involved time decay analysis with various decay functions and different models on User Data, including 100,100 individual user profiles and Tweet Data, resulting in approximately 20 million data points. In this second part, we analyze User Activity Data, such as likes, quote tweets, and retweets, by randomly selecting 30,000 instances, resulting in a total of 2,107,054 (2M) data points for liked tweets.\n9.2 Unique Approach\nThe unique approach in this research involves several key elements that enhance the recommendation system:\n9.2.1 Using Activity Data: By incorporating user activity data such as likes, quote tweets, and retweets, on top of user profile data and timeline data, the model gains an additional dimension to compute similarities. This comprehensive approach captures both static user characteristics and dynamic interactions, improving the relevance and accuracy of the recommendations.\n9.2.1. Pipeline Creation: A well-structured pipeline was created for loading data, preprocessing, encoding using multiple embedding models, training, and evaluation. This pipeline ensures a systematic and reproducible process for analyzing and modeling the data.\n9.2.2. Multiple Embedding Models: The use of multiple embedding models (MiniLM, DistilUse Multilingual, MPNET, and JINA) captures different aspects of the user activity data. Each model brings unique strengths, and their comparative analysis helps in identifying the best performer.\n9.2.3. Cosine Similarity Metric: Utilizing cosine similarity as a metric for recommendation enhances the relevance of the suggestions by measuring the cosine of the angle between two non-zero vectors. This metric is particularly effective in high-dimensional spaces typical of embeddings.\n9.2.4. Neural Network Model with Normalization: A neural network (ANN) model with normalization layers and mean squared error loss was developed. This model includes dense layers with ReLU activation functions to capture complex patterns in the data. The normalization step ensures that the input vectors are on the same scale, which is crucial for accurate similarity calculations.\n9.3. Explanation of the steps adopted:\n9.3.1. Data Loading and Preprocessing : The initial step"}, {"title": "Algorithm: Neural Network Model Creation and Training", "content": "Input:\ne1, e2, e3, e4: Encoded embeddings for each post using four different models\ndirectory: Directory path containing .npy files\nOutput:\nResults: Dictionary containing evaluation results\nProcedure:\nImport necessary modules from tensorflow.keras.\nDefine function create_model(): a. Initialize input layers input1 and input2 with shape (512,). b. Define dense layers densel and dense2 with ReLU activation. c. Define custom cosine similarity function:\nNormalize input x and y.\nReturn sum of element-wise multiplication of x and y.\nd. Compile model with inputs [input1, input2] and output cos_sim, optimizer 'adam', loss 'mean_squared_error'. e. Return model.\nDefine function train_evaluate(file, e): a. Load input_1 from file. b. Set input_2 as e. c. Initialize model using create_model(). d. Fit model on input_1 and input_2 for 1 epoch. e. Return mean of sigmoid predictions on input_1 and input_2.\nInitialize an empty dictionary results.\nEvaluate Models: a. For each file in directory:\nIf file ends with '.npy':\nIf file starts with '1':\nStore result of train_evaluate (file, e1) in results[file].\nElse if file starts with '2':\nStore result of train_evaluate(file, e2) in results[file].\nElse if file starts with '3':\nStore result of train_evaluate(file, e3) in results[file].\nElse if file starts with '4':\nStore result of train_evaluate(file, e4) in results[file].\nEnd Procedure\n9.3.7. Combined Supervised Evaluation matrix Using Activity Data:\nThis table summarizes the evaluation results for each model (MiniLM, DistilUse, MPNET, JINA) across different decay functions (Gaussian, Inv_Sqrt, Logarithmic, Exp, Inv_Lin, Hyperbolic) and metrics (Basic, Cos, Cos_Time).\n9.3.8. Findings from the Combined Supervised Evaluation Using Activity Data\n9.3.8.1 Overall Performance: The Jina model consistently outperformed the other models across most metrics and decay functions, achieving the highest scores in both Cosine and Cos-Time metrics. Jina performs the best for recommending activities accurately, effectively matching the profile embeddings to the post embeddings.\n9.3.8.2 Model Ranking: The overall ranking of models based on their performance is: Jina > DistilUSE Multilingual > MPNet > MiniLM.\n9.3.8.3 Gaussian Decay Function: For the Gaussian decay function, Jina achieved a perfect score in the Cos-Time metric (1.000000), indicating its superior ability to capture activity data dynamics. DistilUSE Multilingual and MiniLM also performed well, but Jina was the clear leader.\n9.3.8.4 Inverse Square Root Decay Function: Jina again performed exceptionally well, with perfect scores in the CosTime metric. DistilUSE Multilingual followed closely, while MiniLM and MPNet showed lower performance compared to Jina.\n9.3.8.5 Logarithmic Decay Function: The Jina model led in the Cos-Time metric with a score of 0.965639. DistilUSE Multilingual and MPNet showed good performance, but MiniLM lagged behind.\n9.3.8.6 Exponential Decay Function: Jina scored highest in the Cos-Time metric, followed by DistilUSE Multilingual and MPNet. MiniLM had the lowest scores among the models for this decay function.\n9.3.8.7 Inverse Linear Decay Function: Jina maintained the lead with the highest scores across all metrics, partic-"}, {"title": "Findings: Static vs Dynamic Embeddings", "content": "The graph illustrates that dynamic embeddings generally exhibit higher diversity scores compared to static embeddings across all models. This supports the observation that dynamic embeddings are better at capturing a broader and more up-to-date range of user behaviors and preferences.\nThis visualization clearly shows that dynamic embeddings significantly outperform static embeddings in accuracy across all models. This supports the conclusion that dynamic embeddings are more effective in applications\nrequiring precise and up-to-date personalization, such as recommendation systems on social media platforms.\n10.5.1 Adaptability and Relevance:\nDynamic embeddings demonstrate high adaptability with scores approaching 1.0 in Cos-Time metrics (e.g., MiniLM: 0.9995, MPNet: 0.9999), showing their ability to stay current with evolving user profiles.\nStatic embeddings, in contrast, maintain a consistent diversity measurement (e.g., MiniLM: 0.238, MPNet: 0.362) which does not reflect changes over time."}, {"title": "Diversity and Accuracy:", "content": "Dynamic embeddings often achieve higher diversity scores (e.g., MPNet in Cosine metric: 0.843 under Gaussian decay) compared to static embeddings, indicating a better capture of a broad range of user behaviors.\nAccuracy of dynamic embeddings in real-time applications also tends to be higher. For example, Jina shows a Cos-Time metric score of 1.000 in the Gaussian decay setting, highlighting its precision in capturing user preferences dynamically.\nnotably suited for real-time applications, shown by high Cos-Time scores, indicating they adjust quickly to new user data (Jina: 1.000, DistilUSE Multilingual: 0.933).\n10.5.6. Computational Efficiency:\nWhile dynamic embeddings offer updated insights, they require ongoing computation as new data arrives, contrasting with static embeddings which are computed once (e.g., static diversity for MiniLM: 0.238 vs dynamic diversity under Gaussian: 0.741)."}, {"title": "Performance Across Decay Functions:", "content": "Exponential and Gaussian decays are especially effective in prioritizing recent data. Gaussian decay shows high scores across all models in the Cos-Time metric (e.g., Jina: 1.000, MiniLM: 0.9995).\n10.5.4. Logarithmic and hyperbolic decays maintain relevance of older data better, with slower reduction rates (e.g., Logarithmic Cos-Time for MiniLM: 0.9999), useful in applications needing long-term behavioral analysis.\nSuitability for Real-Time Applications:\n10.5.5. Dynamic embeddings using Gaussian decay are"}, {"title": "Application Specificity:", "content": "Dynamic embeddings are recommended for environments with frequent user interaction changes, as shown by their superior performance in dynamic settings (e.g., Jina's consistent high scores in dynamic settings).\nStatic embeddings may still be useful in stable, less change-intensive environments, offering a simpler computational approach without the need for frequent updates."}, {"title": "INTERPRETATION OF THE RESULTS IN THE CONTEXT OF THE RESEARCH QUESTION", "content": "The study explores the efficacy of different decay functions and embedding models in capturing user behavior dynamics for recommendation systems. The findings suggest that Gaussian and exponential decay functions, when combined with models like Jina, MiniLM and MPNet, effectively prioritize recent interactions while still maintaining historical context, thereby enhancing recommendation systems' accuracy and relevance. This addresses the research question concerning how different temporal decay functions influence the adaptability of dynamic embeddings in social media environments."}, {"title": "COMPARISON WITH PREVIOUS STUDIES", "content": "12.1 Decay Function Effectiveness:\n\u2022 Similarity with Existing Work: The study confirms the effectiveness of Gaussian decay in capturing recent data trends, which is consistent with the findings of Covington et al. (2016) [23] in their study on deep neural networks for YouTube recommendations.\n\u2022 Differences: The study also highlights the importance of logarithmic and hyperbolic decays in maintaining longer historical contexts, which is not explicitly discussed in Covington et al. (2016). [23]\n12.2 Model Performance:\nSimilarity with Existing Work: The study shows that MiniLM consistently performs well across all metrics, which is consistent with the findings of Chen et al. (2018) in their survey on session-based recommender systems. [22]\nDifferences: The study also highlights the strengths of MPNet in balancing diversity metrics and its ability to adapt to recent data changes, which is not explicitly discussed in Chen et al. (2018)."}, {"title": "Diversity Metrics:", "content": "Similarity with Existing Work: The study emphasizes the importance of cosine similarity in capturing semantic relationships, which is consistent with the findings of He and McAuley (2016) [25] in their study on modeling the visual evolution of fashion trends with one-class collaborative filtering.\nDifferences: The study also highlights the effectiveness of cosine-time decay in capturing the maximum diversity across models, which is not explicitly discussed in He and McAuley (2016). [25]"}, {"title": "RECOMMENDATIONS:", "content": "13.1. Similarity with Existing Work: The study recommends a balanced approach to decay functions and model selection based on application requirements, which is consistent with the findings of Sarwar et al. (2001)[32] in their study on item-based collaborative filtering recommendation algorithms.\n13.2. Differences: The study provides more specific recommendations for real-time systems and historical data analysis, which are not explicitly discussed in Sarwar et al. (2001). [32]"}, {"title": "REAL-WORLD DATA:", "content": "The study's use of real-world data from Twitter, including user profiles and tweet activity, provides a more practical and applicable context for the evaluation of recommender systems. This aligns with the focus on real-world data in other studies, such as Archak (2010) [24], which analyzed strategic behavior in crowdsourcing contests on TopCoder.com."}, {"title": "SUGGESTIONS FOR FUTURE RESEARCH", "content": "\u2022 Exploring Additional Models: Future research could explore the integration of newer or less common machine learning models that might offer improved performance or efficiency.\n\u2022 Cross-platform Validation: Validating the findings across different social media platforms or even in different contexts (like e-commerce or content streaming services) could help in understanding the broader applicability of the proposed methods.\n\u2022 Real-time Implementation: Investigating the real-time implementation of these models and decay functions in live environments could provide insights into practical challenges and performance issues.\n\u2022 User Feedback Incorporation: Incorporating user feedback mechanisms to dynamically adjust the decay rates and model parameters could enhance the adaptability and accuracy of recommender systems.\n\u2022 Ethical and Privacy Considerations: Further research could also explore the ethical implications and privacy concerns related to the use of dynamic embeddings in user profiling."}, {"title": "SUMMARY OF KEY FINDINGS", "content": "\u2022 Decay Function Effectiveness : The study validates the effectiveness of Gaussian decay for emphasizing recent interactions,aligning with existing literature that underscores the importance of recency in recommender systems. Also it highlights the unique value of logarithmic and hyperbolic decays in keeping historical context, offering a more nuanced approach to temporal data analysis in dynamic embeddings.\n\u2022 Model Performance: From the various models tested, Jina, MiniLM and MPNet demonstrated robust performance, excelling in balancing recent and historical data influences. This suggests their suitability for complex recommendation tasks that require a nuanced understanding of user behaviors over time..\n\u2022 Diversity Metrics: The use of cosine and cosine-time similarity metrics were particularly effective in capturing semantic relationships and maximizing diversity across models, pointing towards their potential in enhancing the accuracy and relevance of recommendations. ."}, {"title": "FINAL REMARKS ON THE STUDY'S CONTRIBUTIONS", "content": "This research contributes significantly to the field of recommender systems by offering a comprehensive evaluation of how different decay functions affect the performance of dynamic embeddings. It bridges a gap in existing research by systematically analyzing the impact of less commonly used decay functions and by exploring their implications in real-world social media data. The study's methodological rigor and detailed analysis provide valuable benchmarks for future research and practical applications in dynamic user profiling."}, {"title": "POTENTIAL APPLICATIONS OF THE RESEARCH", "content": "\u2022 Social Media Platforms: Adopting the approach and the findings of the study can help various social-media platforms including multilingual platforms to refine their content recommendation algorithms for better alignments with user preferences", "E-commerce": "Online retailers", "Services": "OTT platforms like Netflix or Spotify, applying dynamic embeddings with appropriate decay functions can improve"}]}]}