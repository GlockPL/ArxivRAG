{"title": "Spatio-Temporal Graphical Counterfactuals: An Overview", "authors": ["Mingyu Kang", "Duxin Chen", "Ziyuan Pu", "Jianxi Gao", "Wenwu Yu"], "abstract": "Counterfactual thinking is a critical yet challenging topic for artificial intelligence to learn knowledge from data and ultimately improve their performances for new scenarios. Many research works, including Potential Outcome Model and Structural Causal Model, have been proposed to realize it. However, their modelings, theoretical foundations and application approaches are usually different. Moreover, there is a lack of graphical approach to infer spatio-temporal counterfactuals, that considers spatial and temporal interactions between multiple units. Thus, in this work, our aim is to investigate a survey to compare and discuss different counterfactual models, theories and approaches, and further build a unified graphical causal frameworks to infer the spatio-temporal counterfactuals.", "sections": [{"title": "I. INTRODUCTION", "content": "How to enable an intelligent machine to think and answer a counterfactual question? E.g., sometimes we want to know, if a different investment strategy had been implemented, would we obtain higher returns? Or, in computer networks, what changes would occur in network load if a node's configuration had never been changed? Or, would someone still purchase the product even if they had never been shown the advertisement? These questions generally have the counterfactual pattern, that is, \"Observed..., if had done..., how would go?\". Thus, according to the pattern, firstly, time is necessary to consider, because the questions are queried now, but the counterfactual actions are supposed to be done in the past. Secondly, counterfactual outcomes are unobservable. The outcomes of some actions are observed if the actions have been done to the real-world system, thus these outcomes are factual. But the counterfactual outcomes are not observed, and they are imaged through supposing different actions had been done.\nTo enable an intelligent machine to imitate counterfactual thinking like human, there are mainly three approaches, as shown in Fig. 1. The first is system identification, which aims to identify the system dynamics, and then solve the evolution trajectories with different initial values [1]. This process may be data-driven and adaptive, but the physical mechanisms and models are necessary to be known as knowledge in prior [2]\u2013[5]. E.g., in [6], [7], functional basis or network structure are necessary to be known at least one to identify the network dynamics sparsely. However, the knowledge of system mechanism is usually hard to obtain in prior.\nThe second approach is generative modeling, which aims to model and learn the system uncertainty, and then generate scenarios with different random inputs [8], [9]. Thus, compared with system identification, generative approach is not necessary to know the determined part, that is the knowledge of system mechanism, but on contrary, it focuses on the uncertain modeling, especially with the deep learning [10]\u2013[15]. However, the generated scenarios would not be out of the distribution of the observational data [9], while the counterfactual scenarios are usually not in the observational factuals. Thus, the mechanism knowledge is still needed to correct the bias between the distributions of factuals and counterfactuals.\nThus, the third approach is causal inference, which aims to discover causality from the observational data, and then infer factuals and counterfactuals directly with as little knowledge as possible [16], [17]. Causal counterfactual inference enables artificial intelligence to learn the knowledge from data, and ultimately improve their performances for new scenarios.\nTo realize it, many research works have been proposed to infer counterfactuals, including the temporal cases [18]-[23]. However, there are still two challenging but critical issues to address: (i) Most current inference approaches are built on the frameworks of Potential Outcome Model (POM) [17] or Structural Causal Model (SCM) [16], [24]-[26]. The goals of them are the same, but their foundations are different. This induces difficulty in discussing them in a unified framework. (ii) The two frameworks both cannot infer spatio-temporal counterfactuals, that considers spatial and temporal interactions between system units.\nThus, the main contributions of this work are as follows:\n1.  A survey is investigated to discuss the frameworks from POM to SCM, including their theoretical foundations and application approaches. This part is organized from Section II to Section IV.\n2.  A Forward Counterfactual Inference algorithm is designed here to recursively and autonomously infer counterfactuals through causal graphical languages. This part is presented as Algorithm 1 in Section IV-C.\n3.  An overview of the spatio-temporal graphical counterfactual framework is proposed to discuss Spatio-Temporal Bayesian Networks, its nonstationarity, and the relationship with complex networks. This part is organized in Section V."}, {"title": "II. POTENTIAL OUTCOME MODEL", "content": "The counterfactuals are defined as the unobserved potential outcomes in POM, as shown in TABLE I. Here, Y is the potential outcomes of the target variable. T is treatment variable. T = 1 represents being treated and T = 0 represents being controlled, with a group of randomized control trials on N units. X represents covariates that have relationship with Y and T, and are controlled for randomized trials. POM models the general process from causal estimands (T, Y and X) to \"Science\" [27]. In this case, for different treatments T, only one potential outcome, Y(1) or Y(0), can be observed, and another one is unobserved, namely the counterfactual."}, {"title": "A. Inferring Counterfactuals through POM", "content": "To infer the counterfactuals through POM, one common approach is matching, e.g., exact matching. For a unit i, one can search the samples with exactly matching covariates X but treatment T is different, and then estimate the counterfactual outcome Y(1 \u2013 Ti). But it is difficult to conduct if the dimension of X is high, that requires large number of samples to support the matching. Thus, approximate matching is commonly better, e.g., caliper matching [28]-[31] and propensity score matching [32]\u2013[35]. They approximately search a group of similar samples for the target unit with one or multiple measurements, and finally calculate the counterfactuals.\nAnother common approach is data-driven imputation, that views the counterfactuals as missing values, and interpolates them by fitting the observational data. To realize it, linear or nonlinear regressions [36]-[39] are commonly used to interpolate the missing values from the trained regressive models on the observational data. Different from this, tensor decomposition approaches [40]-[42] recover the missing values by decomposing and reconstructing the sparse data tensor. Moreover, deep generative models [10]-[15] are also used to model the uncertainty in data, and generate the missing values by randomly sampling. Note that, the generated missing values are not unique, but a group of values conforming to a random distribution. This is different compared to the regressive approaches and tensor decomposition approaches."}, {"title": "B. Foundational Assumptions of POM", "content": "Before using the above approaches on the framework of POM, four assumptions are necessary to be accepted. The first is the stable unit treatment value assumption (SUTVA), defined as\nAssumption 1 (SUTVA [43]). In TABLE I,\n1.  there is no interference between units, that is, neither Y(1) nor Y\u00bf(0) is affected by what action any other unit received.\n2.  there is no hidden versions of treatments, that is, no matter how unit i received treatment T = 1, the outcome that would be observed would be Yi(1), and similarly for treatment T = 0.\nSUTVA guarantees the identical independence between the experimental units, and for unit i, the outcome Yi is only up to its treatment T\u2081, not the others'. However, this assumption is not always satisfied, e.g., in the case of social network, in which there are frequent interactions between units with varying time.\nThe second assumption is consistency, defined as\nAssumption 2 (Consistency [44], [45]). In TABLE I, if the i-th unit is selected for a treatment Ti, the observed value of Yi, neither Yi(1) nor Y\u00bf(0), is the same for all assignments of treatments to the other experimental units.\nAnother form to describe the consistency assumption through expectation E[] is\n\\begin{equation}\nE[Y(1)|T = 1] = \\sum_{i=1}^{N} Y_i \\times t_i = E[Y|T = 1],\n\\end{equation}\n\\begin{equation}\nE[Y(0)|T = 0] = \\sum_{i=1}^{N} Y_i \\times (1 - t_i) = E[Y|T = 0],\n\\end{equation}\nwhere y1,..., YN are the samples for N units, and t\u2081,..., tN \u2208 {0,1} are the values of treatment variable T, as shown in TABLE I. This is the same to the case of conditional expectation with X. Thus, the consistency assumption guarantees the missing counterfactual values in Section II-A can be interpolated by the other observational values actually.\nThe third assumption is positivity, defined as\nAssumption 3 (Positivity [32]). In TABLE I, 0 < P(T = 1|X = x) < 1.\nThis means for X = x, there are always some treatments randomly assigned with T = 0 and T = 1. Otherwise, the counterfactuals for T = 1 could not be obtained if all units with X = x are assigned with T = 0, and this is the same to the counterfactuals for T = 0.\nThe forth assumption is ignorability, defined as\nAssumption 4 (Ignorability [44], [45]). In TABLE I, T\u315bY(1), Y(0)|X for each unit.\nThus, the ignorability assumption implies that all the confounders have been observed in X, that is, no hidden confounder would interfere the observations of Y(1), Y(0). Thus, the ignorability assumption is also called as unconfoundedness. This assumption are usually made to avoid the preference on experimental manipulation, that would guarantee the randomization in the experiments [46]."}, {"title": "III. STRUCTURAL CAUSAL MODEL", "content": "An SCM is built on a set of variables, including endogenous variables V = {X1,...,Xn} and exogenous variables U = {U1,..., Un}. Note that, endogenous variables are observable from data, but exogenous variables are not. Moreover, a set of functions F = {f1,..., fn} are also set to describe the functional relationship between these variables.\nFor example, as shown in Fig. 2, the SCM can be functionally described as\n\\begin{equation}\nf_1: X_1 = U_1,\nf_2: X_2 = aX_1 + U_2,\nf_3: X_3 = bX_1 + cX_2 + U_3,\n\\end{equation}\nwhere a = 0.5, b = 0.7, c = 0.4. U1, U2, U3 are all additive noise, and they are mutually independent.\nNote that, in an SCM, the functional relationship must conform to a directed-acyclic graphical constraint. The graph is early originated from the concept of Bayesian network [47]-[51]. And latter, it is also called as causal graphical model to highlight the causality [16], [24], [52]-[54]. And recently, it is also called as causal network in the field of causal discovery [55]\u2013[59]. But actually, they are the same in the framework of SCM, and they are all directed acyclic graph (DAG) that represents Markovian knowledge. Thus, to avoid misunderstanding, we here use the name of Bayesian network uniformly, due to the fact that causal graphical model and causal network are not always a Bayesian network in Fig. 2. For example, Direct Cyclic Graph [60]\u2013[63], Markov network [64], [65], and Full Time Graph [25], they are also causal graph or causal network."}, {"title": "A. Pearl's Causal Ladder [16], [24], [26]", "content": "To infer the counterfactuals through SCM, there are three steps:\n1.  Abduction: Infer the values of exogenous variables U from the observational data;\n2.  Action: Perform an intervention on SCM, e.g., do(X2 = 2) on SCM in Fig. 2, and then, X2 would be assigned with value 2, and the arrows to X2 would be modified, as shown in Fig. 3;\n3.  Prediction: Use the modified SCM to recalculate the counterfactuals of the target variable.\nAs shown in Fig. 3, we query \u201cobserved X1 = 0.5, X2 = 1, X3 = 1.5, if had done X2 = 2, what would X3 =?\u201d. Obviously, this is a counterfactual question. Thus, follow the Pearl's causal ladder, as shown in Fig. 3, we first infer the values of exogenous variables U = {U1,U2, U3} from the observational data X1 = 0.5, X2 = 1, X3 = 1.5, as follows:\n\\begin{equation}\nU_1 = 0.5,\n\\end{equation}\n\\begin{equation}\nU_2 = 1 - 0.5 \\times 0.5 = 0.75,\n\\end{equation}\n\\begin{equation}\nU_3 = 1.5 - 0.7 \\times 0.5 - 0.4 \\times 1 = 0.75.\n\\end{equation}\nThen, do(X2 = 2) to obtain a modified SCM. And finally, recalculate the counterfactual of X3, that is,\n\\begin{equation}\nX_3(do(X_2 = 2)) = 0.5 \\times 0.7 + 2 \\times 0.4 + 0.75 = 1.9,\n\\end{equation}\nwhere X3(do(X2 = 2)) is different compared to the observational X3.\nThe intervention, also named as do-operator [16], can be generalized into the form of probability, if modularity assumption is introduced as follow:\nAssumption 5 (Modularity [16]). If a set of variables {Xj1,...,Xjp} CV is intervened, then for each variable X \u2208 V, it is obtained that\n1.  if X \u2209 {Xj1,..., X.jp}, then P(X|Pa(X)) remains unchanged. Here, Pa(X) is the predecessors of X in Bayesian network, that is also called as causal parents.\n2.  if X \u2208 {Xj1,..., X.jp}, then P(X = x|Pa(X)) = 1 if x is the value set by intervention to X, otherwise, P(X = x|Pa(X)) = 0.\nThis means, if a group of interventions are conducted on X1,..., X.jp, the values of them would be fixed. The connections between them and their causal parents would be broken, and their causal parents would not affect them anymore in the modified SCM, just like in Fig. 3. Moreover, this also means that the probability distributions of the other variables that are not be intervened would not change."}, {"title": "B. Foundational Assumptions of Bayesian Network", "content": "As presented above, to clearly define the causality, every SCM is associate to a Bayesian network with directed-acyclic graphical constraint. Thus, the causality in Bayesian network determines the counterfactuals foundationally. Thus, we discuss the foundational assumptions of Bayesian network in the following contents.\nFirst of all, a concept of d-separation can be defined on a Bayesian network Gas\nDefinition 1 (d-separation [47]\u2013[51]). Let X, Y and Z be three disjoint subsets of endogenous variables V = {X1,..., Xn}, and let p be any path from a node in X to a node in Y regardless of direction. Z is said to block p if and only if there is a node v \u2208 p satisfying one of the following items:\n1.  v has v-structure (two nodes a,b \u2208 p pointing to v, namely a \u2192 v \u2190 b), and neither v nor its any descendants are in Z;\n2.  v in Z and v does not have v-structure.\nThen, Z d-separate X and Y, denoted as XYZ, if Z blocks any p.\nThen, two assumptions, causal Markov (or Markov property) and faithfulness, are introduced as follows:\nAssumption 6 (Causal Markov [47]\u2013[51]). Probability distribution P is Markovian to a Bayesian network G if\n\\begin{equation}\nXYZ \u21d2 X_Y|Z,\n\\end{equation}\nwhere X, Y and Z are three disjoint subsets.\nAssumption 7 (Faithfulness [47]\u2013[51]). Probability distribution P is faithful to a Bayesian network G if\n\\begin{equation}\nXYZ \u21d2 XYZ\n\\end{equation}\nfor all disjoint subsets X, Y and Z.\nIt is intuitive to understand the two assumptions, that is, the Bayesian network G has one-to-one correspondence to the independence of probability distribution P in observational data. Thus, if they are satisfied, the joint distribution can be factorized according to the Bayesian network, as follow:\n\\begin{equation}\nP(X_1,..., X_n) = \\prod_{i=1}^{n} P(X_i|Pa(X_i)),\n\\end{equation}\nwhere Pa(Xi) is the predecessors of Xi in G, that is also called as causal parents.\nMoreover, a Bayesian network is also assumed to be causally sufficient, that is\nAssumption 8 (Causal Sufficiency [47]\u2013[51]). Variables V are said to satisfy causal sufficiency if there is no hidden variable that is a common cause of two or more variables in V.\nThis means, if a Bayesian network (or an SCM) is causally sufficient, there is no hidden path connecting two endogenous variables in V, and all variable information is collected sufficiently to support the network. Actually, the ignorability assumption in POM (see Assumption 4) also implies the causal sufficiency, and this assumption is weaker than ignorability. But note that, the causal sufficiency is hard to satisfy practically, because many real-world systems (e.g., economic system and climate system) are complex, thus, we could not collect all the information to describe the systems usually. Thus, some research works focus on this issue. For example, FCI (or Fast Causal Inference) algorithm and its variants [66]\u2013[71] are proposed to discover causality in the presence of hidden variables. And some other related works [72]\u2013[74] investigate the causal discovery with soft intervention, that intervenes the SCM but does not change the network structure. There are also some related works [75]-[78] proposed approaches to search the optimal and efficient adjustment set, that all its variables are observable, minimal and valid, and the removal of any of its variables would destroys the validity."}, {"title": "IV. DIFFERENCES BETWEEN POM AND SCM", "content": "Through the above comparisons between POM and SCM, one can find many differences intuitively. For example, they have different statements for counterfactuals, even if some of them can be equally transformed for each other. Moreover, SCM has a causal graph to represent the causations, and the interventions are also introduced to modify the graph to obtain the counterfactuals, but POM does not use it. However, although they have made many distinctions to distinguish each other, as stated in Rubin's and Pearl's books [17], [26], there are still some fundamental differences and relationship that need to be discussed."}, {"title": "A. Ignorability and Back-door Criterion", "content": "In the view of statistics, the ignorability assumption (see Assumption 4) is equivalent in calculation to the causal sufficiency assumption (see Assumption 8) with a back-door criterion. The accurate definition of back-door criterion can be found in these papers and books [16], [24], [53], [54]. Here we present another from with respect to the TABLE I, defined as\nDefinition 2 (Back-door Criterion [16], [24], [53], [54]). In TABLE I, the covariate X is said to satisfy back-door criterion, if a Bayesian network is built on all the variables relative to X,Y,T, and there is an ordered pair T \u2192 Y that satisfy\n1.  no variable in X is a descendant of T;\n2.  X blocks every path between T and Y that contains an arrow into \u03a4.\nThus, for X, Y, T, only one structure can satisfy the back-door criterion, as shown in Fig. 4. And in the back-door path, it is obtained that\n\\begin{equation}\nP(Y|do(T = 1))\n\\end{equation}\n\\begin{equation}\n= \\sum_{X} P(Y|do(T = 1), X = x)P(X = x|do(T = 1))\n\\end{equation}\n\\begin{equation}\n= \\sum_{X} P(Y|T = 1, X = x)P(X = x),\n\\end{equation}\nwhich is the same in case of do(T = 0) [16], [24], [53], [54]. Then, if the causal sufficiency is satisfied, that is, no hidden variable can interfere X, Y, T, the expectation of potential outcomes in TABLE I can be calculated by\n\\begin{equation}\nE[Y|do(T = 1)]\n\\end{equation}\n\\begin{equation}\n= \\sum_{y} y \\times P(Y = y|do(T = 1))\n\\end{equation}\n\\begin{equation}\n= \\sum_{y} \\sum_{X} y \\times P(Y = y|T = 1, X = x)P(X = x)\n\\end{equation}\n\\begin{equation}\n= \\sum_{X} P(X = x) \\sum_{y} y \\times P(Y = y|T = 1, X = x)\n\\end{equation}\n\\begin{equation}\n=E[E[Y|T = 1, X]].\n\\end{equation}\nThis is the same in POM, as follow:\n\\begin{equation}\nE[Y(1)]\n=E[E[Y(1)|X]] \\qquad /* Law of full expectation */\n=E[E[Y(1)|T = 1, X]] \\qquad /* Ignorability */\n=E[E[Y|T = 1, X]], \\qquad /* Consistency */\n\\end{equation}\nwhich is the same in case of do(T = 0). Thus, with comparing Eq. (9) and Eq. (10), E[Y|do(T = 1)] = E[Y(1)], and E[Y|do(T = 0)] = E[Y(0)], in the two frameworks, but they start from different assumptions. Thus, the ignorability assumption can be decomposed as the causal sufficiency assumption and the back-door criterion, and the causal sufficiency is relatively trivial to satisfy. Actually, the ignorability is more likely to be a technical assumption, that needs many controlled trials to deconfound, while the back-door path is clear to search in a Bayesian network. A potential and more difficult technical issue is to discover the network structure accurately."}, {"title": "B. Counterfactual Falsifiability", "content": "Back to the example in Fig. 3, if we query to a POM, \u201cObserved X1 = 0.5, X2 = 1, X3 = 1.5, if had done X2 = 2, what would X3 =?\u201d, we must ensure there are at least two observational samples, and they have the same values X1 = 0.5 but different values in X2. That is, one is X2 = 1, and another one is X2 = 2, to satisfy the positivity assumption (see Assumption 3). And then, due to the consistency assumption (see Assumption 2), we can infer the counterfactuals through matching the two samples exactly. But this is different in SCM, we can infer the counterfactuals through the Pearl's causal ladder, even if we only have a single sample, as shown in Section III-A. Thus, how to falsify the counterfactuals?\nIt is a difficult question, because the counterfactuals are unobserved according to the definition (see \u201c?\u201d in TABLE I), and we need at least two samples for comparison. In general, we cannot back to the past to falsify it with a different treatment, and we cannot reproduce the same experiments completely at different times. Thus, the inferred counterfactuals in SCM cannot be falsified without the assumptions of positivity and consistency, and this view is rejected ambiguously in Pearl's book [26].\nAnother explanation is that, without the positivity, but with the consistency only in POM, the counterfactuals can be extrapolated by regressive approaches, and they are falsifiable. For example, as shown in Fig. 5, if there are T and Y only, and the observational data (T, Y) = {(0, 0.5), (1, 1), (2, 2.5), (3, 2)} is collected, the linear regressive model can be fitted as Y = 0.5T + 0.5. Then, we query \u201cif had done T = 4, what would Y =?\u201d, the counterfactual answer is Y = 2.5, and the answer can be falsified by the regressive function because it does not beyond the observation. Thus, the consistency assumption actually claims that the counterfactuals can be falsified by the observations. This is also true for SCM, because SCM also uses the regressive approaches to infer counterfactuals.\nMoreover, there may be some confusions regarding the preset Bayesian network (see Fig. 2). To build an SCM, a Bayesian network is necessary to be known first. However, how to discover a probabilistic network from a single sample? This is usually impossible. A reasonable explanation is that, the network structure can be discovered from the system mechanisms (e.g., physical mechanisms, communication connections), or graphical knowledge learned from other domains, instead of probability. However, if it is accepted, the definition of causality could be broader and more ambiguous. This is why probabilistic and causal Bayesian network are often indistinguishable, as stated in Pearl's book [26].\nThus, actually, SCM provides a way to think the counterfactuals, and it emphasizes the thinking way is counterfactual, but not the thinking result, because the result cannot always be falsified without the positivity and consistency assumptions. And on the contrary, POM emphasizes the thinking result is counterfactual. This is a fundamental difference in the problem of counterfactual falsifiability."}, {"title": "C. Direct and Indirect Causation", "content": "Suppose that there is a causal ordered pathway from one treatment variable to another outcome variable. Then, if there are one or more variables between the two endpoints, the pathway is called as indirect causation, otherwise, it is called as directed causation. Actually, the indirect causation is also called as mediating effect, that has been discussed deeply in Rubin's papers [79]\u2013[81]. Here, Baron-Kenny model [82], [83] is introduced to present mediating analysis for potential outcome with graph. As shown in Fig. 6, T \u2192 Y is direct causation, and T \u2192 X \u2192 Y is indirect causation.\nTo analyse indirect causation, causal-steps approach [82] is proposed, through building three linear regressive models, as shown in Fig. 6. Suppose that the regressive models are fitted sufficiently, the indirect causation would be detected if satisfy\n1.  coefficients a, b and care significant;\n2.  b < b'.\nAnother approaches are to test the significance of Ho : b'-b [84]-[86]. Or, to test the significance of Ho : ab = 0 [87], [88]. If the correct regressive models are built, the counterfactuals can be calculated, just like SCM in Fig. 2. Moreover, they are all regression-based approaches, that restrict the usage in large multivariate datasets, because one needs to test whether a covariate is a confounder or a mediator, and the test process would be time-consuming seriously.\nBut on the contrary, the test process can be faster with Bayesian network in an SCM. Here, we provide an algorithm to realize forward counterfactual inference as shown in Algorithm 1, and provide an example to show the inference process as shown in Fig. 7."}, {"title": "Algorithm 1: Forward Counterfactual Inference", "content": "Input: A Bayesian network G built on\nV = {X1, ..., Xn} with one target Xi and\nmultiple sources Xj1, ..., X.jp.\nOutput: Distribution P(Xi|do(Xj\u2081),..., do(Xjp)).\n1  Search one or multiple ordered causal pathways in G\nfrom the sources Xj1,..., X.jp to the target Xi;\n2  if Xj1,..., Xjp are all the direct causation for Xi\n3  then\n4  Search back-door paths (common causes) starting\nfrom Xj1,..., Xjp and Xi reversely in G.\n5  if no back-door path within them then\n6  return P(XiXj1, ..., X.jp);\n7  else\n8  For every back-door path, select a group of\ndirect predecessors, W CV, and then,\naccording to back-door criterion (see\nDefinition 2),\nreturn \u2211w P(Xi|Xj1, ..., X.jp, W)P(W);\n9  Otherwise, for q pathways starting from Xjp, select q\nfirst mediators X.jp1,..., X.jpq. Then, a group of\nsource-mediator pairs, e.g., (Xjp, X.jpg), can be\nobtained;\n10 Factorize P(Xi|do(Xj\u2081), ..., do(Xjp)) as\nP(Xi|do(Xj\u2081),..., do(Xjp))\n= \u2211 P(Xildo(Xj1\u2081),..., do(Xj2\u2081),..., do(X.jpq))\nXj11:jpq\n\\times \\prod_{p} \\prod_{q} P(X_{jpq}|do(X), X \\in Pa(X_{jpq}) \\subseteq {X_{j1:jp}}\n\\begin{equation}\n,\n\\end{equation}\nwhere Pa(Xjpg) is the causal parents of Xjpq;\n11 Solve each distribution P(\u00b7|do(\u00b7), . . ., do(\u00b7)) in\nEq. (11) by Algorithm 1 recursively.\n12 return P(Xi|do(Xj\u2081), ..., do(Xjp)).\nThe approaches to realize the codes in lines 1 and 3 in Algorithm 1 can be various, e.g., some variants of depth-first-search algorithm and breadth-first-search algorithm [89]. Thus, their time complexities are both O(|V| + |E|), if a Bayesian network G = (V, E) is given, where V is the node set and E is the edge set. Meanwhile, if the computational cost of factorization and adjustment are overlooked, the time complexity of the code in line 9 in Algorithm 1 is O(|V|), because we only need to traverse all mediators in each pathway forward, and this process can be performed in parallel. Thus, the total time complexity of Algorithm 1 is O(|V|\u00b2+|V||E|). Thus, if the network structure of the Bayesian network is large, and it can be discovered accurately, the efficiency of SCM to infer counterfactuals would be better with polynomial time complexity."}, {"title": "V. SPATIO-TEMPORAL GRAPHICAL COUNTERFACTUALS", "content": "Back to the examples of counterfactual questions at the beginning of this article, that is, if a different investment strategy had been implemented, would we obtain higher returns? And, in computer networks, what changes would occur in network load if a node's configuration had never been changed? And, would someone still purchase the product even if they had never been shown the advertisement? In these real-world scenarios, interactive behaviors with lagged causal effects are widely-existing. POM cannot answer these questions, because POM does not allow the interactive behaviors between experimental units (see SUTVA in Assumption 1), even with some temporal quasi-experimental approaches (e.g., Regression Discontinuity Design [90], [91] and Differences in Differences [92], [93]). And this is the same to SCM, because the foundational Bayesian network is a DAG that also does not allow mutual connections between network nodes. Thus, to answer these counterfactual queries for interactive units and lagged causal effects, a concept of spatio-temporal graphical counterfactuals is proposed here."}, {"title": "A. Spatio-Temporal Bayesian Networks", "content": "Before the buildings of SCM, actually, various causal graphical models have been proposed to model the spatio-temporal causality. As shown in Fig. 8, Temporal Bayesian Network [64", "94": "are proposed to model the momentary causal dependency in a first-order Markov process. Further, Dynamical Bayesian Networks [64", "25": "is proposed to summarize the temporal Bayesian dependency at full timestamps. Note that, Full Time Graph allows the instantaneous causal effects between two units (or variables) in a high-order Markov process, but it only allows strictly stationary causality, that is, the causal dependencies do not change with time. Moreover, Full Time Graph is proved to uniquely decomposed into multiple high-order Temporal Bayesian Networks, named UCN, if the instantaneous causal effects is not allowed [95"}, {"25": [95], "96": "."}]}