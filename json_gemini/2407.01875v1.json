{"title": "Spatio-Temporal Graphical Counterfactuals: An Overview", "authors": ["Mingyu Kang", "Duxin Chen", "Ziyuan Pu", "Jianxi Gao", "Wenwu Yu"], "abstract": "Counterfactual thinking is a critical yet challenging topic for artificial intelligence to learn knowledge from data and ultimately improve their performances for new scenarios. Many research works, including Potential Outcome Model and Structural Causal Model, have been proposed to realize it. However, their modelings, theoretical foundations and application approaches are usually different. Moreover, there is a lack of graphical approach to infer spatio-temporal counterfactuals, that considers spatial and temporal interactions between multiple units. Thus, in this work, our aim is to investigate a survey to compare and discuss different counterfactual models, theories and approaches, and further build a unified graphical causal frameworks to infer the spatio-temporal counterfactuals.", "sections": [{"title": "I. INTRODUCTION", "content": "How to enable an intelligent machine to think and answer a counterfactual question? E.g., sometimes we want to know, if a different investment strategy had been implemented, would we obtain higher returns? Or, in computer networks, what changes would occur in network load if a node's configuration had never been changed? Or, would someone still purchase the product even if they had never been shown the advertisement? These questions generally have the counterfactual pattern, that is, \"Observed..., if had done..., how would go?\". Thus, according to the pattern, firstly, time is necessary to consider, because the questions are queried now, but the counterfactual actions are supposed to be done in the past. Secondly, counterfactual outcomes are unobservable. The outcomes of some actions are observed if the actions have been done to the real-world system, thus these outcomes are factual. But the counterfactual outcomes are not observed, and they are imaged through supposing different actions had been done.\nTo enable an intelligent machine to imitate counterfactual thinking like human, there are mainly three approaches, as shown in Fig. 1. The first is system identification, which aims to identify the system dynamics, and then solve the evolution trajectories with different initial values [1]. This process may be data-driven and adaptive, but the physical mechanisms and models are necessary to be known as knowledge in prior [2]\u2013[5]. E.g., in [6], [7], functional basis or network structure are necessary to be known at least one to identify the network dynamics sparsely. However, the knowledge of system mechanism is usually hard to obtain in prior.\nThe second approach is generative modeling, which aims to model and learn the system uncertainty, and then generate scenarios with different random inputs [8], [9]. Thus, compared with system identification, generative approach is not necessary to know the determined part, that is the knowledge of system mechanism, but on contrary, it focuses on the uncertain modeling, especially with the deep learning [10]\u2013[15]. However, the generated scenarios would not be out of the distribution of the observational data [9], while the counterfactual scenarios are usually not in the observational factuals. Thus, the mechanism knowledge is still needed to correct the bias between the distributions of factuals and counterfactuals.\nThus, the third approach is causal inference, which aims to discover causality from the observational data, and then infer factuals and counterfactuals directly with as little knowledge as possible [16], [17]. Causal counterfactual inference enables artificial intelligence to learn the knowledge from data, and ultimately improve their performances for new scenarios. To realize it, many research works have been proposed to infer counterfactuals, including the temporal cases [18]-[23]. However, there are still two challenging but critical issues to address: (i) Most current inference approaches are built on the frameworks of Potential Outcome Model (POM) [17] or Structural Causal Model (SCM) [16], [24]\u2013[26]. The goals of"}, {"title": "II. POTENTIAL OUTCOME MODEL", "content": "The counterfactuals are defined as the unobserved potential outcomes in POM, as shown in TABLE I. Here, Y is the potential outcomes of the target variable. T is treatment variable. T = 1 represents being treated and T = 0 represents being controlled, with a group of randomized control trials on N units. X represents covariates that have relationship with Y and T, and are controlled for randomized trials. POM models the general process from causal estimands (T, Y and X) to \"Science\" [27]. In this case, for different treatments T, only one potential outcome, Y(1) or Y(0), can be observed, and another one is unobserved, namely the counterfactual."}, {"title": "A. Inferring Counterfactuals through POM", "content": "To infer the counterfactuals through POM, one common approach is matching, e.g., exact matching. For a unit i, one can search the samples with exactly matching covariates X but treatment T is different, and then estimate the counterfactual outcome Y(1 \u2013 Ti). But it is difficult to conduct if the dimension of X is high, that requires large number of samples to support the matching. Thus, approximate matching is commonly better, e.g., caliper matching [28]-[31] and propensity score matching [32]\u2013[35]. They approximately search a group of similar samples for the target unit with one or multiple measurements, and finally calculate the counterfactuals.\nAnother common approach is data-driven imputation, that views the counterfactuals as missing values, and interpolates them by fitting the observational data. To realize it, linear or nonlinear regressions [36]-[39] are commonly used to interpolate the missing values from the trained regressive models on the observational data. Different from this, tensor decomposition approaches [40]-[42] recover the missing values by decomposing and reconstructing the sparse data tensor. Moreover, deep generative models [10]-[15] are also used to model the uncertainty in data, and generate the missing values by randomly sampling. Note that, the generated missing values are not unique, but a group of values conforming to a random distribution. This is different compared to the regressive approaches and tensor decomposition approaches."}, {"title": "B. Foundational Assumptions of POM", "content": "Before using the above approaches on the framework of POM, four assumptions are necessary to be accepted. The first is the stable unit treatment value assumption (SUTVA), defined as\nAssumption 1 (SUTVA [43]). In TABLE I,\n1. there is no interference between units, that is, neither Y(1) nor Yi(0) is affected by what action any other unit received.\n2. there is no hidden versions of treatments, that is, no matter how unit i received treatment T = 1, the outcome that would be observed would be Yi(1), and similarly for treatment T = 0.\nSUTVA guarantees the identical independence between the experimental units, and for unit i, the outcome Yi is only up to its treatment Ti, not the others'. However, this assumption is not always satisfied, e.g., in the case of social network, in which there are frequent interactions between units with varying time.\nThe second assumption is consistency, defined as\nAssumption 2 (Consistency [44], [45]). In TABLE I, if the i-th unit is selected for a treatment Ti, the observed value of Yi, neither Yi(1) nor Yi(0), is the same for all assignments of treatments to the other experimental units.\nAnother form to describe the consistency assumption through expectation E[] is\n$E[Y(1)|T = 1] = \\sum_{i=1}^{N} Y_i \\times t_i = E[Y|T = 1],$\n$E[Y(0)|T = 0] = \\sum_{i=1}^{N} Y_i \\times (1 - t_i) = E[Y|T = 0],$\nwhere y1,..., yN are the samples for N units, and t1,..., tN \u2208 {0,1} are the values of treatment variable T, as shown in TABLE I. This is the same to the case of conditional expectation with X. Thus, the consistency assumption guarantees the missing counterfactual values in Section II-A can be interpolated by the other observational values actually."}, {"title": "III. STRUCTURAL CAUSAL MODEL", "content": "An SCM is built on a set of variables, including endogenous variables V = {X1,...,Xn} and exogenous variables U = {U1,..., Un}. Note that, endogenous variables are observable from data, but exogenous variables are not. Moreover, a set of functions F = {f1,..., fn} are also set to describe the functional relationship between these variables.\nFor example, as shown in Fig. 2, the SCM can be functionally described as\n$f_1: X_1 = U_1,$\n$f_2: X_2 = aX_1 + U_2,$\n$f_3: X_3 = bX_1 + cX_2 + U_3,$\nwhere a = 0.5, b = 0.7, c = 0.4. U1, U2, U3 are all additive noise, and they are mutually independent.\nNote that, in an SCM, the functional relationship must conform to a directed-acyclic graphical constraint. The graph is early originated from the concept of Bayesian network [47]\u2013[51]. And latter, it is also called as causal graphical model to highlight the causality [16], [24], [52]\u2013[54]. And recently, it is also called as causal network in the field of causal discovery [55]\u2013[59]. But actually, they are the same in the framework of SCM, and they are all directed acyclic graph (DAG) that represents Markovian knowledge. Thus, to avoid misunderstanding, we here use the name of Bayesian network uniformly, due to the fact that causal graphical model and causal network are not always a Bayesian network in Fig. 2. For example, Direct Cyclic Graph [60]\u2013[63], Markov network [64], [65], and Full Time Graph [25], they are also causal graph or causal network."}, {"title": "A. Pearl's Causal Ladder", "content": "To infer the counterfactuals through SCM, there are three steps:\n1. Abduction: Infer the values of exogenous variables U from the observational data;\n2. Action: Perform an intervention on SCM, e.g., do(X2 = 2) on SCM in Fig. 2, and then, X2 would be assigned with value 2, and the arrows to X2 would be modified, as shown in Fig. 3;\n3. Prediction: Use the modified SCM to recalculate the counterfactuals of the target variable.\nAs shown in Fig. 3, we query \u201cobserved X1 = 0.5, X2 = 1, X3 = 1.5, if had done X2 = 2, what would X3 =?\". Obviously, this is a counterfactual question. Thus, follow the Pearl's causal ladder, as shown in Fig. 3, we first infer the values of exogenous variables U = {U1,U2, U3} from the observational data X1 = 0.5, X2 = 1, X3 = 1.5, as follows:\n$U_1 = 0.5,$\n$U_2 = 1 - 0.5 \\times 0.5 = 0.75,$\n$U_3 = 1.5 - 0.7 \\times 0.5 - 0.4 \\times 1 = 0.75.$\nThen, do(X2 = 2) to obtain a modified SCM. And finally, recalculate the counterfactual of X3, that is,\n$X_3(do(X_2 = 2)) = 0.5 \\times 0.7 + 2 \\times 0.4 + 0.75 = 1.9,$\nwhere X3(do(X2 = 2)) is different compared to the observational X3.\nThe intervention, also named as do-operator [16], can be generalized into the form of probability, if modularity assumption is introduced as follow:\nAssumption 5 (Modularity [16]). If a set of variables {Xj1,...,Xjp} CV is intervened, then for each variable X \u2208 V, it is obtained that\n1. if X \u2209 {Xj1,..., Xjp}, then P(X|Pa(X)) remains unchanged. Here, Pa(X) is the predecessors of X in\""}, {"title": "IV. DIFFERENCES BETWEEN POM AND SCM", "content": "Through the above comparisons between POM and SCM, one can find many differences intuitively. For example, they have different statements for counterfactuals, even if some of them can be equally transformed for each other. Moreover, SCM has a causal graph to represent the causations, and the interventions are also introduced to modify the graph to obtain the counterfactuals, but POM does not use it. However, although they have made many distinctions to distinguish each other, as stated in Rubin's and Pearl's books [17], [26], there are still some fundamental differences and relationship that need to be discussed."}, {"title": "A. Ignorability and Back-door Criterion", "content": "In the view of statistics, the ignorability assumption (see Assumption 4) is equivalent in calculation to the causal sufficiency assumption (see Assumption 8) with a back-door criterion. The accurate definition of back-door criterion can be found in these papers and books [16], [24], [53], [54]. Here we present another from with respect to the TABLE I, defined as\nDefinition 2 (Back-door Criterion [16], [24], [53], [54]). In TABLE I, the covariate X is said to satisfy back-door criterion, if a Bayesian network is built on all the variables relative to X,Y,T, and there is an ordered pair T \u2192 Y that satisfy\n1. no variable in X is a descendant of T;"}, {"title": "B. Counterfactual Falsifiability", "content": "Back to the example in Fig. 3, if we query to a POM, \"Observed X1 = 0.5, X2 = 1, X3 = 1.5, if had done X2 = 2, what would X3 =?"}, {"content": "In this work, we mainly focus on the spatio-temporal graphical counterfactuals, and organize an overview for it to discuss its theoretical foundations and application approaches. To discuss the theoretical foundations, a survey is investigated, and the definition of counterfactuals is defined based on"}]}