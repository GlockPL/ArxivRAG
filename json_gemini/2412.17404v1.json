{"title": "BrainMAP: Learning Multiple Activation Pathways in Brain Networks", "authors": ["Song Wang", "Zhenyu Lei", "Zhen Tan", "Jiaqi Ding", "Xinyu Zhao", "Yushun Dong", "Guorong Wu", "Tianlong Chen", "Chen Chen", "Aiying Zhang", "Jundong Li"], "abstract": "Functional Magnetic Resonance Image (fMRI) is commonly employed to study human brain activity, since it offers insight into the relationship between functional fluctuations and human behavior. To enhance analysis and comprehension of brain activity, Graph Neural Networks (GNNs) have been widely applied to the analysis of functional connectivities (FC) derived from fMRI data, due to their ability to capture the synergistic interactions among brain regions. However, in the human brain, performing complex tasks typically involves the activation of certain pathways, which could be represented as paths across graphs. As such, conventional GNNs struggle to learn from these pathways due to the long-range dependencies of multiple pathways. To address these challenges, we introduce a novel framework BrainMAP to learn Multiple Activation Pathways in Brain networks. BrainMAP leverages sequential models to identify long-range correlations among sequentialized brain regions and incorporates an aggregation module based on Mixture of Experts (MoE) to learn from multiple pathways. Our comprehensive experiments highlight BrainMAP's superior performance. Furthermore, our framework enables explanatory analyses of crucial brain regions involved in tasks. Our code is provided at https://github.com/LzyFischer/Graph-Mamba.", "sections": [{"title": "1 Introduction", "content": "Recently, significant research has focused on learning complex patterns in brain activities, which has promoted tasks such as cognitive process decoding (Li and Fan 2019; Thomas, R\u00e9, and Poldrack 2022; Finn, Poldrack, and Shine 2023) and the diagnosis of mental health disorders (Jo, Nho, and Saykin 2019; Eslami et al. 2019). Generally, brain activities could be represented as functional magnetic resonance imaging (fMRI) data (Fox and Raichle 2007; Zhang, Ji, and Liu 2024), which measures blood-oxygen-level-dependent (BOLD) responses and reflects changes in metabolic demand associated with neural activity (Kohoutov\u00e1 et al. 2020; Davis et al. 2020). By leveraging fMRI's unique blend of spatial and temporal characteristics, researchers can delve into the complexities of cognitive processes in the human brain (Bassett and Sporns 2017). More specifically, BOLD signals are commonly used to construct networks of brain regions from fMRI data, where the functional connectivities (FC) among distinct brain regions are associated with various normal and pathological states (Kawahara et al. 2017), as shown in Fig. 1. Studying the FC features that correspond to the different brain states enables the identification of specific behavioral traits and neurological disorders linked to particular FC patterns (Morris et al. 2019).\nTo extract patterns in FC features, they are generally modeled as FC graphs, where nodes represent brain Regions of Interest (ROIs), and edges represent their relationships (Cui et al. 2022b,a). In this way, the correlations among brain regions could be explicitly represented (Said et al. 2023). With the development of Graph Machine Learning (GML) techniques, Graph Neural Networks (GNNs) are widely applied to FC graphs (Wang et al. 2022; Zhou et al. 2020). By capitalizing on the structured nature of the FC graphs and integrating local information, GNNs facilitate learning from patterns in functional connectivities and informative features (Li et al. 2021). While FC graphs offer valuable connectivity insights by depicting correlations among brain regions, existing works often overlook the activation pathways that are inherently present in these graphs. Specifically, in the human brain, performing tasks typically involves the activation of certain pathways (Sporns 2011), which could be represented as paths across the FC graphs (Sankar et al. 2018), as shown in Fig. 1. These pathways indicate the transmission of neural signals to a particular brain region. By incorporating these pathways into analysis, we could capture the complex interactions that might be overlooked when only considering pairwise correlations. Furthermore, these pathways provide insights into how different brain regions segregate into functional modules and collaborate to perform complex tasks.\nHowever, despite the benefits of considering activation pathways on FC graphs, learning from these pathways is challenging. Since they are not explicitly represented in the graphs, without ground truth, models struggle to accurately learn and interpret them. Generally, pathways exhibit two crucial properties as shown in Fig. 1: (1) Sequential dependency is a fundamental feature of brain networks, where multiple regions co-activate and interact over long distances (Dahan et al. 2021). For example, in emotional memory processing, the hippocampus encodes memories, the amygdala assesses their emotional significance, and the prefrontal cortex uses this information for decision-making, exemplifying the long-range dependencies across multiple brain regions (Said et al. 2023). Nevertheless, while capturing these sequential dependencies is crucial for understanding the information flow in the brain, the structural nature of FC graphs makes it challenging to effectively model such dependencies. (2) Multiple pathways are generally necessary for the brain to process different behaviors and perform complex tasks. For example, in visual processing, the brain utilizes two parallel pathways: one along the dorsal visual cortex, which handles fast but coarse information, and the \"what\" stream along the ventral visual cortex, which processes slower but more detailed information (Lee et al. 2016). These distinct pathways correspond to different aspects of visual stimuli, emphasizing the need for multiple pathways in visual processing. Nevertheless, it is especially challenging to capture multiple pathways with existing GNN architectures. Due to the inherent limitations of the message-passing mechanism (Kipf and Welling 2017; Veli\u010dkovi\u0107 et al. 2018; Tan et al. 2022b), which focuses on aggregating information from neighboring nodes, GNNs struggle to effectively model the complex, long-range interactions in multiple pathways (Kim, Ye, and Kim 2021). Moreover, the interpretability of functional connectivity patterns is underexplored in current GNN-based approaches. Existing interpretable GNN models (Ying et al. 2019; Luo et al. 2020), which are typically designed to explain the importance of individual nodes and edges rather than considering their relationships within an activation path, struggle to provide explanations for interactions in long-range paths.\nIn this work, we propose BrainMAP to effectively learn from and interpret Multiple Activation Pathways present in FC (functional connectivities) graphs while tackling the challenges posed by long-range dependencies and pathway correlations. To achieve this: (1) We propose an Adaptive Graph Sequentialization module to transform each FC graph into a node sequence that reflects the order of information flow, which enables the extraction of the hidden pathways that are crucial for modeling long-range interactions. (2) We design a Hierarchical Pathway Integration strategy that analyzes correlations among multiple pathways. Inspired by the human brain's use of parallel pathways in complex tasks, we propose to integrate insights from diverse pathways, which captures complementary information contributed by each pathway. More importantly, our design improves interpretability by identifying the crucial brain regions in pathways that work together to support brain functions. Such interpretability offers deeper insights into the functional co-activated pattern of the brain. To evaluate our framework, We conduct experiments on five real-world fMRI datasets. The results demonstrate that our framework outperforms existing models in various prediction tasks on FC graphs while also offering comprehensive explanations for pathways. In summary, our contributions are as follows:\n\u2022 Innovation. We present a novel framework for predictive tasks on FC graphs while providing comprehensive explanations to identify crucial brain regions\u2014an area that has been underexplored in prior research.\n\u2022 Architecture. We design an Adaptive Sequentialization module to transform FC graphs into node sequences for pathway learning, and a Pathway Integration module to aggregate and analyze correlations across multiple pathways on FC graphs.\n\u2022 Validation. We conduct extensive experiments on various real-world FC datasets, and the results demonstrate the superior performance of our framework in both predictions and explanations."}, {"title": "2 Related Work", "content": "Brain network analysis aims to understand the intricate patterns of connectivity within the brain (Cui et al. 2022a; Kan et al. 2022; Zhang et al. 2022; Hsu et al. 2024; Zhang, Ji, and Liu 2024; Gao et al. 2024), which has gained increasing popularity recently due to its various applications, including identifying biomarkers for neurological diseases (Chang, Lin, and Lane 2021; Yang et al. 2022), understanding cognitive processes (Liu et al. 2023; Chen et al. 2024), and distinguishing different types of brain networks (Liao, Wan, and Du 2024). Among these, one of the most important tasks is the prediction of brain-related attributes, such as demographics and task states (Said et al. 2023; He et al. 2020). Recently, GNNs have significantly evolved as a major field of exploration for these tasks (Li et al. 2022; Cui et al. 2022a), due to their extraordinary ability to leverage the structured data (Li et al. 2021; Xu et al. 2024; Wang, Chen, and Li 2022). Nevertheless, GNN-based approaches often struggle to fully exploit the useful knowledge in brain networks, particularly the activation pathways that are inherently present in brains (Keller, Taube, and Lauber 2018) are neglected. To address this limitation, we propose to extract multiple underlying activation pathways with adaptive structure sequentialization and Mixture of Experts (MoE), which thus enables a more comprehensive understanding of the brain connection."}, {"title": "2.1 Brain Network Analysis", "content": "Brain network analysis aims to understand the intricate patterns of connectivity within the brain (Cui et al. 2022a; Kan et al. 2022; Zhang et al. 2022; Hsu et al. 2024; Zhang, Ji, and Liu 2024; Gao et al. 2024), which has gained increasing popularity recently due to its various applications, including identifying biomarkers for neurological diseases (Chang, Lin, and Lane 2021; Yang et al. 2022), understanding cognitive processes (Liu et al. 2023; Chen et al. 2024), and distinguishing different types of brain networks (Liao, Wan, and Du 2024). Among these, one of the most important tasks is the prediction of brain-related attributes, such as demographics and task states (Said et al. 2023; He et al. 2020). Recently, GNNs have significantly evolved as a major field of exploration for these tasks (Li et al. 2022; Cui et al. 2022a), due to their extraordinary ability to leverage the structured data (Li et al. 2021; Xu et al. 2024; Wang, Chen, and Li 2022). Nevertheless, GNN-based approaches often struggle to fully exploit the useful knowledge in brain networks, particularly the activation pathways that are inherently present in brains (Keller, Taube, and Lauber 2018) are neglected. To address this limitation, we propose to extract multiple underlying activation pathways with adaptive structure sequentialization and Mixture of Experts (MoE), which thus enables a more comprehensive understanding of the brain connection."}, {"title": "2.2 Mixture of Experts", "content": "The Mixture of Experts (MoE) approach involves deploying a collection of expert networks, each designed to specialize in a particular task or a subset of the input space (Shazeer et al. 2017; Wang et al. 2024b). Originally derived from traditional machine learning models (Jacobs et al. 1991; Jordan and Jacobs 1994), MoE has since been adapted for deep learning, significantly enhancing its ability to handle complex vision and language tasks (Jiang et al. 2024). In addition to the strategy of interesting MoE layers with conventional neural networks (Vaswani et al. 2017; Dauphin et al. 2017), the concept of MoE is also extended to large and independent modules, e.g., language models as agents (Wang et al. 2024). In this work, we extend the MoE framework to address the challenge of multiple pathways in brain networks, focusing on learning the correlations across pathways. As a result, our framework is able to extract multiple pathways within and across different orders while learning from them."}, {"title": "3 Preliminary", "content": "In this work, we define an FC graph G as G = (V, E, A, X), where V represents the set of nodes that indicate brain regions, & denotes the edges that illustrate functional connections between these regions, A \u2208 RN\u00d7N is the adjacency matrix capturing the connectivity structure, and X \u2208 RN\u00d7d denotes node features that may include various biological markers or other relevant attributes. The total number of vertices in the graph is represented by N, such that |V| = N, and let d be the number of dimensions in the input feature of each node. We use Y to denote the prediction target of each graph in classification or regression tasks."}, {"title": "4 Methodology", "content": "An overview of BrainMAP is presented in Fig 2. Specifically, BrainMAP is composed of two components: (1) Adaptive Graph Sequentialization, which learns the optimal sequence of brain regions by transforming the FC graph structure into a meaningful order that captures key dependencies, and (2) Hierarchical Pathways Aggregation, which utilizes multiple experts to extract diverse pathways from different orders and then aggregates them to capture complex interactions across multiple pathways. Each expert is instantiated as a sequential model such as Transformer (Vaswani et al. 2017) or Mamba (Gu and Dao 2023), in order to extract long-range dependencies within potential pathways."}, {"title": "4.1 Adaptive Graph Sequentialization", "content": "When performing tasks such as visual or motor activities, research has shown that multiple brain regions often collaborate over long distances rather than functioning in isolation, which means cognitive processes emerge from the sequential activation of these regions (Thiebaut de Schotten and Forkel 2022). Consequently, capturing the order of sequential activation paths is crucial for accurate prediction in brain networks. Nevertheless, due to the complex structure of FC graphs, it is difficult to identify and extract such pathways. To address this, we propose an adaptive sequentialization strategy that transforms each FC graph into a sequence, in order to preserve key pathway information and facilitate more effective modeling of the brain's dynamic processes.\nLearning Orders for FC Graphs. A significant obstacle in converting FC graphs into node sequences lies in the permutation invariance of brain network regions (i.e., nodes) (Said et al. 2023). This invariance contrasts with the inherently sequential nature of activation pathways, which do not naturally account for such invariance.\nTo tackle this, we introduce a learning-based strategy that utilizes an order-learning GNN to adaptively determine the node order for each input FC graph. With the order-learning GNN, we aim to learn the optimal sequence of nodes by arranging them based on their learned ordering scores in an ascending order. The benefit of using the learned scores to describe the order is that it avoids the massive search space of possible node orders (i.e., N! for a graph of size N), which would otherwise make exhaustive search infeasible. In the following, we describe the process of learning the ordering scores with order-learning GNN. Given an input graph G, the ordering score $s_i \\in \\mathbb{R}$ of node $v_i$ in G is learned as:\n$S_i = GNN_l(V_i, E_i, X_i)$, where $V_i = N_i \\cup \\{v_i\\}$.\nHere $X_i$ is the feature matrix of $V_i$, which is the set of neighboring nodes of $v_i$. $E_i$ is the set of edges for nodes in $V_i$. $GNN_l$ is the order-learning GNN.\nWith the ordering scores $\\{s_1, s_2, ..., s_N \\}$ of nodes in G, calculated in Eq. (1), we obtain the order $ \\phi $ of N nodes $\\{v_1, v_2, ..., v_N\\}$ as follows:\n$\\phi = (v_{\\pi(1)}, v_{\\pi(2)}, \u00b7\u00b7\u00b7, v_{\\pi(N)})$, where $\\pi(i) = \\underset{j \\notin \\{\\pi(1), \\pi(2),...,\\pi(i-1)\\}}{\\text{argmin}}  s_j$.\nHere $\\pi$ is a permutation of indices that sorts the scores $\\{s_1, s_2,...,s_N\\}$ in an ascending order. $ \\phi $ denotes the obtained order of N nodes in the input graph.\nOptimization of Order-Learning GNNs. To optimize the order-learning GNN, an obvious challenge is the lack of ground-truth orders. That being said, the optimal node order that consists of sufficient pathway information remains unavailable. Therefore, we propose to use the loss of BrainMAP output to select good and bad orders as the supervision signal. Intuitively, the orders that could provide smaller losses regarding the correct label (or ground-truth values in regression tasks) should be more similar to the optimal orders. In concrete, within each training step, we first randomly sample a batch of orders and compute their corresponding output. Then we select $N_p$ orders with the smallest losses as positive samples (denoted as $ \\Phi_p$), and select $N_n$ orders with the largest losses as negative samples (denoted as $ \\Phi_n$). Based on the concept of contrastive learning (You et al. 2020; Tan et al. 2022a; Xu et al. 2023, 2024; Wang et al. 2023), our optimization aims to increase the similarity between the learned order and the positive orders, while decreasing the similarity between the learned order and the negative orders. In this manner, we manage to gradually make the learned order approach the better orders during training."}, {"title": "4.2 Hierarchical Pathway Integration", "content": "Although sequential models can extract long-range pathways, they are inherently limited to identifying a single pathway at a time. In contrast, the human brain typically relies on multiple pathways to process various behaviors and perform complex tasks, as different pathways often contribute unique and complementary information (Morris et al. 2019). For instance, in visual processing, the brain employs two parallel pathways: one along the dorsal visual cortex, which quickly processes broad, less detailed information, and another one along the ventral visual cortex, which handles slower but more detailed information (Lee et al. 2016).\nTo deal with the challenge of multiple pathways, we propose to learn numerous activation pathways from each order of brain regions with multiple sequential models. Moreover, the activation pathways can be present in different orders. To effectively learn from these diverse pathways, we propose a two-level hierarchical integration approach, across and within different orders. (1) We first utilize the Mixture of Experts (MoE) strategy to integrate multiple pathways within each sequential order of brain regions. (2) Next, we aggregate the representations across different orders to obtain a comprehensive representation of brain activity.\n\u25ba Step 1: Pathway Aggregation within Each Order. Within each order, multiple sub-sequences may connect different sets of brain regions that appear as activation pathways, while they can be hard to extract with only one sequential model, due to the potential heterogeneity among pathways. Thus, we propose to learn multiple pathways simultaneously based on the MoE architecture, with each expert capturing different underlying pathways. To be specific, BrainMAP consists of multiple experts, each utilizing a different sequential model. To dynamically determine which experts are most suitable for a specific order, we design a gating function that ensures the similar pathways are consistently assigned to the same expert. In this manner, each expert specializes in capturing a specific type of pathway.\nFormally, considering an input order $ \\phi $ and $P$ experts, the aggregation is performed as follows:\n$\\textbf{z}^{\\'}= \\sigma(\\sum_{i=1}^{P} G_i(\\phi)F_i(\\phi))$\nwhere $F_i$ is the sequential model of the $i$-th expert. G is the gating function that generates multiple decision scores with the input as sequentialized brain regions $ \\phi $, and $G(\\phi) \\in \\mathbb{R}^P$ denotes the scores to choose $P$ experts for the graph. We employ an attention-based top-k gating design for G, which can be formalized with\n$G(\\phi) = Softmax(TopK(Q(\\phi), K))$,\n$Q(\\phi) = MLP(Attention(Q, K, V))$,\n$Q = W_Qh, K=W_Kh, V=W_Vh$,\n$h = W_1\\phi + PE(\\phi)$,\nwhere $PE$ denotes the sinusoidal positional encoding, which is utilized to inform the gating function with the order information of sequentialized graph representations. $W_1, W_Q, W_K$, and $W_V$ are learnable parameters, and $Attention$ denotes self-attention mechanism. Besides, $K$ denotes the number of selected experts ($K < P$). $TopK(Q(\\phi), K)$ denotes that we keep the top $K$ values in $Q(\\phi)$, i.e.,\n$TopK(Q(\\phi), K)_j =  \\begin{cases}\nQ(\\phi)_j, & \\text{if } Q(\\phi)_j \\text{ is in the top K values of } Q(\\phi), \\\\ -\\infty, & \\text{otherwise}. \n\\end{cases}$\n\u25ba Step 2: Pathway Aggregation across Different Orders. After the aggregation in Step 1, we obtain an output representation from each order. To aggregate the pathway information across different orders, we compute the weighted sum over representations learned from these orders, and the weights are the maximum value of Q($ \\phi $). In this manner, we achieve a final embedding for the input FC graph, i.e.,\n$ \\textbf{z} = \\sum_{i=1}^{M} Max(Q_i(\\phi)) \\cdot  \\textbf{z}^{\\'}_i$\nwhere $ \\textbf{z}^{\\'}_i$ is the representation learned from the $i$-th order. For the training of gating functions and experts in BrainMAP, we adopt the cross-entropy (CE) loss for classification and the mean absolute error (MAE) for regression tasks."}, {"title": "5 Experiments", "content": "In this section, we aim to answer the following research questions (RQs). RQ1. How well can BrainMAP perform on brain-related tasks compared to other alternatives? RQ2. How does each component contribute to the overall predictive performance? RQ3. How effectively can BrainMAP elucidate the rationale behind its predictive outcomes? RQ4. What impact does the design of MoE have on performance?"}, {"title": "5.1 Experimental Settings", "content": "We provide a brief introduction to the experimental settings. For the sequential model in our framework, we utilize the Mamba (Gu and Dao 2023), which is particularly effective in capturing long-range dependencies. The implementation details are explained in Appendix C.\nDatasets. In our experiments, we consider the Human Connectome Project (HCP) dataset (Van Essen et al. 2013), which is a comprehensive publicly available neuroimaging dataset that includes both imaging data and a wide range of behavioral and cognitive data. We process the HCP-Task dataset by parcellating it into 360 distinct brain regions. With respect to other datasets, we use the processed ones from the NeuroGraph benchmark (Said et al. 2023).\nBaselines. We compare our framework with baselines leveraged by the NeuroGraph benchmark and two state-of-the-art models GraphGPS (Ramp\u00e1\u0161ek et al. 2022) and Graph-Mamba (Wang et al. 2024a) that can extract long-range dependencies within the graph data."}, {"title": "5.2 Main Results", "content": "To answer RQ1, we first evaluate the performance of BrainMAP in comparison to all baselines on the HCP datasets. We make the following observations from empirical results in Table 5. (i) BrainMAP outperforms all baselines across various benchmarks with improvements up to 4.09% over the state-of-the-art, which demonstrates its ability to extract long-range dependencies between brain regions along multiple pathways. (ii) BrainMAP and Graph-Mamba surpass traditional GNN models by a large margin, with BrainMAP showing improvements of up to 12.13% on HCP-Task. The observation corroborates the benefit of extracting activation pathways for brain-related tasks. (iii) BrainMAP consistently outperforms Graph-Mamba, which demonstrates the effectiveness of learning multiple pathways."}, {"title": "5.3 Ablation Studies", "content": "To address RQ2, we conduct ablation studies on BrainMAP by removing different components, where w/o LR refers to the removal of the structure sequentializer and w/o LB indicates the exclusion of the load balancing loss for the MoE. The empirical results in Table 3 lead to the following observations. (i) Both the MoE and the structure sequentializer contribute to the overall performance, which suggests the importance of effective structure sequentialization and the extraction of multiple pathways. (ii) The MoE appears to be the most critical component for overall performance, indicating the significant role of learning multiple pathways. (iii) The removal of load balancing loss results in a reduction in the overall performance, which illustrates the importance of broad and balanced activation of experts."}, {"title": "5.4 Explanation Study", "content": "To answer RQ3 and better comprehend the prediction decisions made by different models, we aim to identify the salient brain regions that contribute the most to the predictions. To be more specific, we seek to identify the activated brain regions during a specific task MOTOR from the HCP-Task dataset. We first adopt explanation models to calculate the importance scores of brain regions during the task MOTOR of several random samples from the HCP-Task dataset, where the scores are then averaged to assess the interpretation ability. We adopt the commonly used GNNExplainer (Ying et al. 2019) for ResGCN and GraphGPS, and a Mamba-specific explanation method for Graph-Mamba and BrainMAP to calculate the importance scores. We select the salient brain regions, which are then compared to the ground-truth activated brain region of the HCP-Task given by domain experts, where the correspondence is measured with Hit@10, Hit@30, and Mean Reciprocal Rank (MRR). The results in Table 4 showcase that BrainMAP achieves higher precision in locating the activated brain regions for the MOTOR task, which demonstrates its reliability and effectiveness. Apart from the quantitative analysis of the interpretation ability of the BrainMAP, we also visualize the interpretation results in Fig. 3, where top-ranked brain regions of BrainMAP are highlighted with different colors, and ground-truth activated brain regions given by domain experts are circled. We could make the observation that BrainMAP is able to identify several ground-truth regions, which further demonstrates its effectiveness."}, {"title": "5.5 MoE Analysis", "content": "The MoE is critical in extracting multiple pathways. To answer RQ4, we evaluate the impact of varying the number of experts in the MoE on the model's performance. We could make the following observations from Fig. 4. (i) The performance of BrainMAP improves as the number of experts increases up to 3. It can be attributed to the fact that experts might be insufficient to extract diverse pathways necessary for comprehensive prediction. (ii) The accuracy remains nearly constant once the number of experts exceeds 4, which can be attributed to the limited number of potential pathways in the brain. To gain a deeper understanding of the MoE component, we analyze the activation distribution across different layers of BrainMAP, as shown in Fig. 5. The results illustrated that BrainMAP consistently maintains high activation rates, with a minimum of 66.7% activation rate for HCP-Gender. The findings further suggest that BrainMAP effectively extracts multiple pathways, as evidenced by the activation of diverse experts."}, {"title": "6 Conclusion", "content": "Despite significant progress has been made in understanding brain activity through functional connectivity (FC) graphs, challenges persist in effectively capturing and interpreting the complex, long-range dependencies and multiple pathways that are inherent in these graphs. In this work, we introduce BrainMAP, a novel framework designed to extract multiple long-range activation pathways with adaptive sequentialization and pathway aggregation. Experiments demonstrate the effectiveness of BrainMAP in extracting underlying activation pathways for predictions tasks."}, {"title": "A Proof of Theorem 4.1", "content": "Theorem 4.1. The mean and standard deviation of $S_i$ are the same as those of any real rank variable R for a sample size of N, i.e.,\n$\\mu(S_i) = \\mu(R), \\sigma(S_i) = \\sigma(R)$.\nProof. We start by showing that $\\widetilde{S_i} = \\frac{S_i - E[s]}{\\sqrt{E[s^2]-(E[s])^2}}$ is the standardized value of $s_i$, i.e., $\\mu(\\widetilde{S_i}) = 0$ and $\\sigma(\\widetilde{S_i}) = 1$.\n$\\mu(\\widetilde{S_i}) = E[\\frac{S_i - E[s]}{\\sqrt{E[s^2]-(E[s])^2}}] = \\frac{E[S_i] - E[s]}{\\sqrt{E[s^2]-(E[s])^2}} = \\frac{E[s] - E[s]}{\\sqrt{E[s^2]-(E[s])^2}} = 0$\n$\\sigma^2(\\widetilde{S_i}) = \\mu((\\widetilde{S_i})^2) - (\\mu(\\widetilde{S_i}))^2 = \\mu((\\widetilde{S_i})^2)$\n$=\\frac{E[(S_i - E[s])^2]}{E[s^2] - (E[s])^2}$\n$=\\frac{E[(s_i - E[s])^2]}{E[s^2] - (E[s])^2} = 1$\nFor a real rank variable R of sample size N, we denote $R_i$ as the corresponding rank of the i-th sample. By definition of rankings, we know $1 < R_i < N, i = 1, 2, ..., N$, and all $R_i$ are distinct integers. We calculate the mean and standard deviation of R as follows. Firstly, by definition of rankings, we can consider R as a random variable that is uniformly distributed on $\\{1, 2, . . ., N\\}$. Thus, we can obtain\n$\\mu(R) = E[R] = \\frac{1}{N} \\sum_{i=1}^{N} i = \\frac{N(N+1)}{2N} = \\frac{N+1}{2}$\n$E[R^2] = \\frac{1}{N} \\sum_{i=1}^{N} i^2 = \\frac{(N+1)(2N+1)}{6}$\nTherefore,\n$\\sigma^2(R) = E[R^2] - (E[R])^2 = \\frac{(N+1)(2N+1)}{6} - (\\frac{N+1}{2})^2 = \\frac{N^2-1}{12}$\nTherefore, with the calculated mean and variance of R, we can rewrite Eq. (4) as follows:\n$S_i = \\widetilde{S_i} \\cdot \\sigma(R) + \\mu(R)$.\nSince $\\mu(\\widetilde{S_i}) = 0$ and $\\sigma(\\widetilde{S_i}) = 1$, we know the linear transformation of $\\widetilde{S_i}$ will accordingly change the mean and variance. Hence, we have $\\mu(S_i) = \\mu(R)$ and $\\sigma(S_i) = \\sigma(R)$."}, {"title": "B Proof of Theorem 4. 2", "content": "Theorem 4.2. Minimizing the loss L described in Eq. (6) equals maximizing the Spearman's rank correlation coefficient (Spearman 1904) between learned orders and good orders", "as": "n$r(\\phi", "follows": "n$cov(S", "E[SR": "E[S", "E[R": "n$= \\frac{1"}, {"follows": "n$cov(S, R) = \\frac{1}{2N} \\sum_{i=1}^{N} (S_i^2 + R_i^2) - \\bar{S} \\bar{R} - \\frac{1}{2N} \\sum_{i=1"}]}