{"title": "Beyond Detection: Leveraging Large Language Models for Cyber Attack Prediction in IoT Networks", "authors": ["Alaeddine Diaf", "Abdelaziz Amara Korba", "Nour Elislem Karabadji", "Yacine Ghamri-Doudane"], "abstract": "In recent years, numerous large-scale cyberattacks have exploited Internet of Things (IoT) devices, a phenomenon that is expected to escalate with the continuing proliferation of IoT technology. Despite considerable efforts in attack detection, intrusion detection systems remain mostly reactive, responding to specific patterns or observed anomalies. This work proposes a proactive approach to anticipate and mitigate malicious activities before they cause damage. This paper proposes a novel network intrusion prediction framework that combines Large Language \u2611 Models (LLMs) with Long Short Term Memory (LSTM) net-works. The framework incorporates two LLMs in a feedback loop: a fine-tuned Generative Pre-trained Transformer (GPT) model for predicting network traffic and a fine-tuned Bidi-rectional Encoder Representations from Transformers (BERT) for evaluating the predicted traffic. The LSTM classifier model then identifies malicious packets among these predictions. Our framework, evaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant improvement in predictive capabilities, achieving an overall accuracy of 98%, offering a robust solution to IoT cybersecurity challenges.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's interconnected world, the Internet of Things (IoT) plays a pivotal role in shaping and optimizing various aspects of our daily lives, revolutionizing the way devices, systems, and data seamlessly interact. The large-scale adoption of IoT devices across diverse fields has resulted in the emergence of novel cyberattacks specifically designed for IoT, with the intent of exploiting the vulnerabilities present in these interconnected devices. Sophisticated cyberattacks, allows unauthorized or malicious activities that compromise the security of IoT net-work which is referred as a network intrusion [1]. Network in-trusions can take various forms, including unauthorized access to sensitive information, denial-of-service attacks, or the intro-duction of malware into the network. Detecting and preventing network intrusions are critical aspects of network security to ensure the integrity, confidentiality, and availability of network resources. To maintain these security requirements, various techniques have been implemented to counter intrusions in IoT networks. Among the existing techniques, Intrusion Detection Systems (IDSs) play a crucial role in identifying potential security threats within IoT networks. In recent years, Artificial Intelligence-based IDSs have gained significant attention from the research community due to their capability to achieve real-time intrusion detection. Nevertheless, their efficacy in antic-ipating and preemptively mitigating malicious events before they occur is constrained, they are often reactive in nature [2]\u2013[4]. Given this limitation of intrusion detection systems, it is essential to prioritize intrusion prediction for enhancing the strength and effectiveness of cybersecurity measures. In-trusion prediction enhances a security posture by introducing a forward-looking dimension, enabling organizations to stay one step ahead of cyber adversaries and significantly reducing the likelihood and impact of successful intrusions. In this context, using Pre-trained Large Language Models (LLMS) presents a cutting-edge approach in the field of cybersecurity. Adapting LLMs for various threat detection has been studied [5]\u2013[10]. Leveraging the inherent linguistic capabilities of these models, such as Generative Pre-trained Transformer (GPT) [11] and Bidirectional Encoder Representations from the Transformers (BERT) [12] models, for analyzing network patterns and anomalies opens new avenues for proactive threat detection. By harnessing the contextual understanding and pattern recognition abilities embedded in LLMs, intrusion prediction systems can be enhanced, providing a more ro-bust defense against evolving cyber threats. In this paper, we present a novel intrusion prediction framework based on network packets, employing a combination of Fine-tuned Pre-trained LLMs and LSTM model. The proposed framework is designed to predict potential intrusions in IoT networks by predicting next network packets giving current ones using a generative pre-trained LLM and classifying them through the LSTM model. The assessment of the predicted network packets leverages the bidirectional contextual understanding provided by the BERT model. This framework is required to grasp the fundamental features of network packets, accurately predict their subsequent packets, and efficiently predict intru-sions. As far as we know, this is the first attempt to propose a pretrained large language models for network intrusion prediction. We performed fine-tuning of GPT on both normal and malicious network traffic, aiming to predict the next network packets. Additionaly, BERT was fine-tuned for a packet-pair classification task to assess the predicted packets from the fine-tuned GPT. Furthermore, predicting intrusions using a trained LSTM model. We evaluate the effectiveness of our proposed framework using a recent IoT attack dataset."}, {"title": "II. RELATED WORK", "content": "Several recent LLMs-based IDSs have been proposed for detecting attacks in IoT networks. BERT is widely fine-tuned for this purpose. The authors of [5] proposed SeMalBERT model for identifying malicious software in Windows systems. Where BERT was fine-tuned to learn behavioral features of API call sequences for semantic-based malware detection, enabling more detailed and context-aware malware detection. However, this model is complex faces challenges related to computational resources. In [7], BERT's bidirectional contex-tual understanding has been leveraged for effectively identi-fying anomalies in system log data, it shows an impressive F1-score of 99.3% in anomalies detection. A similar approach to detect anomalous HTTP requests was proposed in [6]. In another effort, the authors in [13] introduces a lightweight intrusion detection model tailored for IoT, leveraging an enhanced version of BERT-of-Theseus. This model aims to enhance the efficiency of intrusion detection in the context of IoT devices, presenting a notable advancement in lightweight security solutions. Other LLMs have also been fine-tuned for cybersecurity aims, a fine-tuned version of Falcon [14] was proposed in [9], called SecureFalcon to identify vulnerabilities in C codes. Additionally, GPT has been fine-tuned to develop VulDetect [15], a vulnerability detection framework. With an accuracy rate of up to 92.65%, VulDetect effectively identifies software vulnerabilities.\nOverall, existing LLMs-based cybersecurity solutions are focusing only on intrusion detection, aiming at preventing known attacks and stopping ongoing threats. However, Stop-ping multistage attacks in its earlier stages and predicting the ultimate attack to avoid its catastrophic damages, have been ignored. Considering these gaps, we propose a novel network packet-based intrusion prediction framework designed using LLMs and LSTM model. Our framework can predict intru-sions based on current network packets, predicting multistage and unseen attacks."}, {"title": "III. PROPOSED SOLUTION", "content": "This section outlines the crucial steps of our proposed framework, consisting of three essential elements: packet pars-ing and preparing, fine-tuning pre-trained LLMs for predicting the next packets, and training LSTM to classify network packets.\nDuring the development phase of our intrusion prediction framework (see figure 1), we pass through three key elements: fine-tuning GPT for next packets prediction, fine-tuning BERT to asses next packets prediction and training an LSTM packet classifier.\nRegarding temporal dependence between consecutive pack-ets, the network packets transmitted within an IoT network can be represented as sequence or a time series P = {p(1), p(2),..., p(n)}, where each packet p(t) is an m-dimensional vector {f(t), p(t)....., p(t)}\nFirstly, we leverage the capabilities of GPT in capturing previous contextual informa-tion and dealing with long-range dependencies in predicting next packet. Evaluating GPT's predicted next packets given current packets requires an understanding of the context on both sides of the two packets to make predictions about their relationship. Next, exploiting BERT's bidirectional contextual understanding, we fine-tune it for a packet-pair classification task that aims to predict whether the predicted packet is the next of a given packet. Finally, we train LSTM as a packet classifier, exploiting its proficiency in learning long-term dependencies, which aids in identifying normal and malicious traffic.\nIn the deployment phase illustrated in figure 1, after col-lecting and parsing packets, the packet predictor predicts next packets. The LSTM packet classifier then classifies these pack-ets as either normal or malicious. Notably, this deployment is strategically executed at the Multi-Access Edge Computing (MEC) server level. This positioning enhances the overall effectiveness and responsiveness of our framework."}, {"title": "A. Packet parsing and Preparing", "content": "The proposed framework is centered around intrusion pre-diction based on network packets, focusing on packet headers. Before feeding data into the framework, we initiate the process by calculating a set of packet features for each packet. This involves parsing various application protocols and extracting pertinent information from network packet capture files. These features characterize packet header field values corresponding to the layer 2, 3, and 4 protocol fields of the TCP/IP protocol stack. Notably, a flow index feature facilitates tracking each packet back to its corresponding flow. Concluding the packet parsing phase, we obtain a valid input for our models."}, {"title": "B. Pre-trained Large Language Models for Predicting Next Packets", "content": "In the realm of network traffic, where data is inherently structured as a sequence of packets, we harness the power of transformer architecture [16], specifically in sequence-to-sequence tasks, to craft our next packet predictor. This neural network architecture employs the self-attention mech-anism, facilitating parallel processing of input sequences and demonstrating remarkable effectiveness in handling sequence-to-sequence tasks. The mathematical expression of this self-attention mechanism in the transformer is as follows:\n\\(Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\\)\nHere, Q, K, and V represent the query, key, and value matrices, respectively, while \\(d_k\\) signifies the dimensionality of the keys vector. We integrate two LLMs in a feedback loop, fine-tuned GPT model for predicting network traffic and a fine-tuned BERT for evaluating the predicted traffic."}, {"title": "1) Developing an LLM-based next packet predictor", "content": "In the initial stage of our Intrusion Prediction framework, we fine-tune the GPT model using a network packet dataset for the purpose of generating high-quality next network packets given current packets while maintaining contextual coherence. GPT operates in a decoder-only configuration within the trans-former architecture and pre-trained using a causal language modeling objective (CLM), which means it predicts the next token in a sequence given the preceding context. Initially, the network packet undergoes tokenization as a continuous sequence, breaking it into smaller units or tokens. Subse-quently, these tokenized packets are converted into numerical values, accompanied by positional encoding, facilitating GPT's processing while preserving the relative positions of tokens within the input. GPT model applies a multi-headed self-attention operation over the input tokens followed by position-wise feedforward layers to produce an output distribution over target tokens as follows:\n\\(h_0 = TW_e + W_p\\)\n\\(h_l = tf\\_block(h_{l-1})\\forall l \\in [1, L]\\)\nWhere T is a matrix of one-hot row vectors of the token indices in the sentence, \\(W_e\\) is the token embedding matrix, and \\(W_p\\) is the position embedding matrix, L is the number of Transformer blocks, and \\(h_l\\) is the state at layer 1. This intricate process empowers GPT to capture contextual information and intricate relationships between tokens, considering only previous tokens to generate the next ones. Ultimately, GPT generates a numer-ical output representing the next network packet based on its comprehensive understanding of the preceding input tokens. Specifically, given an input network packet consisting of N tokens, denoted as P ={P1, P2,...,pN}, GPT calculates the probability PN of token tk based on the preceding k-1 tokens:\n\\(P_N(t_k/t_1,..., t_{k-1}) = softmax(W_vh_{k-1})\\)\nWhere \\(h_{k\u22121}\\) denotes the representation encoded by Transformer with the previous tokens {t1,...,tk \u2212 1} as input. Wv represents the learnable parameters. Thus, the objective is to maximize the likelihood of predicting the next token in a sequence given the preceding context. In mathematical terms, it involves finding the parameters of the model that maximize the probability of the training data:\n\\(L_{GPT}(\\theta) = argmax_{\\theta} \\sum_{i=1}^N log P(x_i | x_{<i}; \\theta)\\)\nHere, \u03b8 represents the model parameters, N is the number of tokens in the training dataset, xi is the i th token, and \\(x_{<i}\\) represents the context of tokens before xi. The objective is to maximize the log-likelihood of the observed data. This numerical output is subsequently decoded back into the network packet format."}, {"title": "2) Developing an LLM-based next packet prediction evalua-tor", "content": "We fine-tune BERT for packet-pair classification task that to evaluate GPT's output. In contrast to GPT, BERT randomly selects a portion of input tokens and replaces them with a \"[MASK]\" token. The model is subsequently trained to predict the original identities of these masked tokens, leveraging contextual information. The objective function is designed to"}, {"title": "3) Training a next packet classifier", "content": "We train an LSTM model to classify network packets as normal or malicious as shown in the devlopement phase in Figure 1. Network intrusions often manifest as deviations from normal network behavior. The LSTM model consists of two key components: the encoder and the decoder. Working collaboratively, these components aim to learn a compressed representation of the input data and then faithfully reconstruct it. The encoder processes the input sequence, utilizing LSTM cells to compress it into a latent representation. Sequentially processing each element of the input sequence, the encoder captures pertinent information in hidden states. Subsequently, the decoder employs LSTM cells to iteratively generate each element of the reconstructed sequence from the compressed representation generated by the encoder. The operations within an LSTM cell are governed by equations that control the flow of information. With our input vector xt, the implementation of the LSTM's cell for the hidden state ht at time t can be represented by the following equations:\n\\(i_t = \\sigma(W_ix_t + U_ih_{t-1} + b_i)\\)\n\\(f_t = \\sigma(W_fx_t + U_fh_{t-1} + b_f)\\)\n\\(o_t = \\sigma(W_ox_t + U_oh_{t-1} + b_o)\\)\n\\(g_t = \\pi(W_gx_t + U_gh_{t-1} + b_g)\\)\n\\(C_t = f_t \\cdot C_{t-1} + i_t \\cdot g_t\\)\n\\(h_t = o_t \\cdot \\pi(C_t)\\)\nHere, \u03c3 denotes the logistic sigmoid function, while \u03c0 represents the non-linear activation functions for cell input and output, typically using the hyperbolic tangent function. Vectors it, ft, ot, gt, and ct correspond to the input gate, forget gate, output gate, cell input activation, and cell state. The matrices and vectors Wa, Ua, and ba, where a \u2208 {i, f, o, g}, are the parameters to be learned and \u00b7 signifies element-wise product. With their ability to retain and utilize information over extended sequences, LSTM can effectively capture the temporal context of network activities given previously seen packet traffic. This includes recognizing normal patterns and identifying anomalies or suspicious deviations."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this section, we initiate with an overview of the dataset utilized in our research. Then, we pass through the conducted experiments to present used configurations for our models. Finally, we present the obtained results and a discussion of the performance metrics related to our novel intrusion prediction framework."}, {"title": "A. Dataset pre-processing", "content": "We used a realistic IoT attack dataset developed by [17] called CICIoT2023, to fine-tune the pre-trained large language models, GPT and BERT, as well as to train the LSTM model. This dataset, developed in a lab environment with 105 devices, encompasses 33 attacks categorized into DDoS, DoS, Recon, Web-based, brute force, spoofing, and Mirai. We selected five attack types, considering their higher frequency in real-world scenarios. We utilized Tranalyzer [18], a packet exporter to extract packet features from raw network traffic (PCAP files), focusing on layer 2, 3, and 4 protocol fields of the TCP/IP protocol stack. This extraction yielded a total of 71 features, from which 26 were selected after applying feature selection techniques. Constant and quasi-constant fea-tures were removed using a minimum Variance Threshold of 25%. Additionally, highly correlated features (> 90%) were discarded through a Pearson correlation filter, ensuring a refined set of distinctive and relevant features for analysis. Data is prepared differently for each model."}, {"title": "B. Experiments", "content": "We conduct our experiment on Google Colab cloud envi-ronment [19] using python 3 with the freely available GPU computing to implement the different parts of our intrusion prediction framework. In our experiment, CICIoT2023 dataset is partitioned between GPT-2, BERT and LSTM models."}, {"title": "1) Fine-tuning GPT-2 for next packet prediction", "content": "To con-struct fine-tuning dataset for GPT2 model, we transform unla-beled input network packets into textual representation where each line represents a network packet. We add two special tokens that mark the begin and the end of a network flow to the Byte-level BPE (BBPE) Tokenizer (the GPT2's tokenizer) vocabulary, in order to help the model learn the concept of flow boundaries, thus ensuring prediction of packets of the same network flow and helping GPT-2 model representing and understanding the context and patterns of a network traffic. GPT2's output has a fixed-length representation of the network packet, since in its input there is the same number of extracted features for each network packets which helps in generating coherent, contextually relevant output and avoiding randomness. We fine-tuned the the small version of GPT-2 model which has 117 million parameters from the HuggingFace transformer Python package [20]."}, {"title": "2) Fine-tuning BERT for Packet-Pair Classification", "content": "To fine-tune BERT for pair-packet classification task that predict whether one packet is next to another, we constructed a dataset that contains a pair of network packets per each line that rep-resents consecutive or non consecutive class. This customized dataset is constructed from CICIoT2023 normal and attack network traffic and a binary classification will be performed on it. Available data were split into training, validation, and test sets. We leverage the HuggingFace transformer Python libraries to fine-tune distilbert-base-uncased model [21], a small and fast version of the BERT base model. This model has 6 layers, 768 dimension, 12 heads and 66 million parameters. Our model has a sequence classification head on top of its outputs, in order to be able to classify the input pair-packet as positive (non consecutive) or negative (consecutive). An Early stopping mechanism is employed to prevent overfitting. The well fine-tuned BERT model is considered as an evaluator of the GPT2's generated network packets, it classifies the pair of current packets with their corresponding GPT2's generated next packets."}, {"title": "3) LSTM training for packet classification", "content": "The training process of the LSTM packet classifier, utilizing labeled CI-CIoT2023 network packets, involves several key steps. Ini-tially, we categorize the data into benign or specific attack types, creating a binary classification copy. To facilitate numerical processing, we convert categorical data into numerical format using an ordinal encoder, and then we normalize the data using a min-max normalization technique following the formula:\n\\(X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}\\)\nwhere x, Xscaled represent the original input, and scaled re-spectively. Similarly, Xmin and Xmax are the minimum and the maximum values of the input respectively. After that, we split the data into training, validation, and test sets and reshaping it into three-dimensional input data (number of network packets, size of time step, number of input features) to ensure the data is appropriately formatted for input into the LSTM. During training, the model learns from the training dataset across multiple epochs, with progress monitored on a validation set. An early stopping mechanism is utilized to prevent overfitting. Post-training, the model undergoes evaluation on the test set, involving the analysis of metrics such as accuracy, precision, recall, and F1-score. Hyperparameters tuning is carried out to optimize the model's configuration. The trained model is then employed to classify GPT's predicted network packets."}, {"title": "C. Results", "content": "To evaluate our framework, we used the following perfor-mance metrics:\n\\(Accuracy = \\frac{TP+TN}{TP+TN+FP+ FN}\\)\n\\(Precision = \\frac{TP}{TP+FP}\\)\n\\(Recall = \\frac{TP}{TP+FN}\\)\n\\(F1-Score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\\)\nwhere TP (True Positive) denotes the count of instances accurately classified as attacks, while TN (True Negative) sig-nifies the instances correctly identified as normal. Conversely, FP (False Positive) indicates instances incorrectly classified as an attack, and FN (False Negative) represents instances incorrectly classified as normal.\nTable III presents the obtained results for the packet-pair classification task. With an accuracy of 93.4%, fine-tuned BERT model demonstrates a high level of overall correctness in predicting whether one packet is next to another. Notably, the model achieves a well-balanced combination of precision"}, {"title": "V. CONCLUSION", "content": "The heightened connectivity of IoT devices exposes them to various threats, emphasizing the insufficiency of relying solely on IDSs, as damage often occurs before effective mitigation measures can be applied. Recognizing this, our paper introduces a network packet-based intrusion prediction"}]}