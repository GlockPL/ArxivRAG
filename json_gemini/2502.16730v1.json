{"title": "RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents", "authors": ["Sho Nakatani"], "abstract": "We present RapidPen, a fully automated penetration testing (pentesting) framework that addresses the challenge of achieving an initial foothold (IP-to-Shell) without human intervention. Unlike prior approaches that focus primarily on post-exploitation or require a human-in-the-loop, RapidPen leverages large language models (LLMs) to autonomously discover and exploit vulnerabilities, starting from a single IP address. By integrating advanced ReAct-style task planning (Re) with retrieval-augmented knowledge bases of successful exploits, along with a command-generation and direct execution feedback loop (Act), RapidPen systematically scans services, identifies viable attack vectors, and executes targeted exploits in a fully automated manner.\nIn our evaluation against a vulnerable target from the Hack The Box platform, RapidPen achieved shell access within 200-400 seconds at a per-run cost of approximately $0.3-$0.6, demonstrating a 60% success rate when reusing prior \"success-case\u201d data. These results underscore the potential of truly autonomous pentesting for both security novices and seasoned professionals. Organizations without dedicated security teams can leverage RapidPen to quickly identify critical vulnerabilities, while expert pentesters can offload repetitive tasks and focus on complex challenges. Ultimately, our work aims to make penetration testing more accessible and cost-efficient, thereby enhancing the overall security posture of modern software ecosystems.", "sections": [{"title": "1 Introduction", "content": "Penetration testing (pentesting) typically begins with its most critical and challenging phase: initial infiltration of a target system. Once an attacker-or in this case, a testing platform-gains an initial foothold, subsequent post-exploitation tasks such as privilege escalation, credential theft, lateral movement, and data exfiltration become significantly more feasible. Although the initial foothold phase in penetration testing is challenging, preventing all infiltration attempts is equally daunting, especially given the risks posed by zero-day exploits and social engineering. Consequently, it is essential to assess post-exploitation risks under the realistic assumption that a compromise may occur. The faster a testing process can confirm initial access, the more effectively it can allocate time to deeper post-exploitation stages before \"running out of clock.\"\nDespite advances in automation, fully autonomous solutions for identifying initial compromise vectors remain elusive. In many cases, sophisticated pentesting still demands substantial human expertise, time, and cost. Recent advancements in large language models (LLMs) have driven progress in automating pentesting tasks, such as vulnerability scanning and post-exploitation. However, the initial-access phase has received comparatively less attention. Existing approaches that incorporate LLMs often rely on a human-in-the-loop to validate generated scans and exploits or to guide testing when ambiguities arise [6]. While this approach may suit seasoned pentesters, it presents a significant barrier for software developers and system operators with limited security expertise, who may struggle to evaluate or refine the LLM's recommendations. Moreover, prior research has identified two key challenges to full automation [26]: the vast search space of potential entry points and the highly target-specific nature of exploits.\nIn this work, we focus on IP-to-Shell testing: given only a target IP address, an autonomous system must obtain a shell without human intervention. Our goal is to develop a high-speed, low-cost solution that significantly simplifies penetration testing for both security professionals and non-specialists alike.\nResearch Questions (RQs)\nComparing highly skilled human penetration testers with existing human-in-the-loop systems [6], we hypothesize that two key design choices can facilitate autonomous, robust, and efficient initial infiltration:\nRQ1: Can reusing \"success cases\" (i.e., past experiences with successful scans and exploit paths) enhance the speed and reliability of initial-access automation?\nRQ2: Does iterative command refinement\u2014where the system analyzes failures and regenerates commands until successful-result in a higher probability of exploitation success?\nBuilding on these ideas, we propose an LLM-based pentesting agent, RapidPen, which requires no human intervention beyond specifying a single target IP [15]. We further broaden the scope of our investigation by posing the following research questions:\nRQ3: How does the time-to-compromise achieved by RapidPen compare to that of a skilled human pentester?\nRQ4: How do the automation costs (in dollars per test) compare to manual penetration testing? Are they low enough to make automated solutions widely practical?\nContributions and Scope\nWhile our system is still in the early stages of development, we validate the feasibility of fully automated IP-to-Shell exploitation on a vulnerable target from the Hack The Box (HTB) platform [1]. Specifically, we achieve:\n\u2022 A 60% success rate when leveraging past \"success-case\" data for the same class of vulnerability;\n\u2022 Typical end-to-end compromise in 200\u2013400 seconds;\n\u2022 A per-run cost of only $0.3\u2013$0.6 to conduct a fully automated test.\nThese promising results support our hypotheses regarding the benefits of success-case knowledge (RQ1) and iterative command refinement (RQ2). We also provide a preliminary analysis relevant to RQ3 and RQ4, comparing automated testing speed and cost with expert-driven testing.\nWhile RapidPen is designed to support organizations with limited security expertise, it is not exclusively intended for teams without dedicated security staff. We also envision its adoption by security teams and professional penetration testers, enabling them to offload straightforward assessments to RapidPen and focus their efforts on more complex, high-value testing scenarios. Like many other LLM-based penetration testing automation tools, RapidPen primarily achieves its objectives by flexibly leveraging existing knowledge. However, human expertise remains crucial for identifying novel vulnerabilities that are not yet documented or well understood.\nBy providing a solution that benefits security-conscious organizations and industries with minimal prior expertise in penetration testing, we aim to enhance the overall security posture of modern software ecosystems.\nPaper Outline. This paper is structured as follows: Section 2 reviews the fundamentals of penetration testing and the evolving landscape of AI-driven automation. Section 3 defines our threat model, scope, and assumptions, clarifying RapidPen's operational boundaries. Section 4 presents the high-level architecture of RapidPen, including its ReAct-based modules and retrieval-augmented workflow. Next, Section 5 details our prototype implementation and core technical choices. Section 6 describes our experimental setup and discusses the results of testing RapidPen on a vulnerable target. Finally, Section 9 summarizes the key findings and outlines future directions for enhancing RapidPen's capabilities and impact."}, {"title": "2 Background and Motivation", "content": "2.1 Overview of Penetration Testing\nPenetration testing (pentesting) is a structured process for identifying and validating vulnerabilities in systems and networks before malicious actors can exploit them. It typically involves multiple phases, which align with well-known frameworks such as the Penetration Testing Execution Standard (PTES) [21] or the MITRE ATT&CK model [5]. Although different organizations may use slightly varying terminology, a common workflow includes:\n\u2022 Reconnaissance (Recon) \u2013 Gathering preliminary information about the target, such as domain names, IP address ranges, and publicly available data. Effective reconnaissance can guide subsequent actions by identifying potential entry points.\n\u2022 Scanning (Enumeration) \u2013 Conducting deeper, often automated probes on discovered services, ports, and configurations. Tools like nmap can identify vulnerabilities or anomalies.\n\u2022 Exploitation (Initial Access) \u2013 Leveraging discovered weaknesses to gain unauthorized access. This phase is often the most challenging and high-stakes, as it determines whether an attacker can successfully compromise the system.\n\u2022 Post-Exploitation \u2013 Once an initial foothold is obtained, security testers (or adversaries) may escalate privileges, move laterally, and explore deeper layers of the environment to assess the impact of a breach.\nPenetration testing plays a crucial role in cybersecurity: rigorous simulated attacks can expose complex weaknesses that static code analysis or automated scanners might overlook. By emulating real-world threats, pentesters help organizations prioritize remediation efforts and improve their overall security posture. overall security posture of modern software ecosystems."}, {"title": "2.2 LLM-Driven Automation in General", "content": "In recent years, large language models (LLMs) have rapidly advanced in both capability and scope, enabling significant progress in automating a wide range of tasks, including natural language processing, programming assistance, and more. Early breakthroughs include transformer-based architectures such as BERT [7], GPT-2/3 [18, 2], and T5 [19], which collectively demonstrated how pre-trained models could perform text classification, summarization, and translation with minimal fine-tuning. Subsequent models like LaMDA [25] and GPT-4 [16] have further increased parameter counts and the sophistication of emergent behaviors, allowing for more complex and context-aware interactions.\nThese advances have driven adoption across various application domains:\n\u2022 Text Summarization and Translation. LLMs trained on large corpora can generate concise summaries of lengthy documents and translate text between multiple languages, often surpassing traditional systems [2, 19].\n\u2022 Code Generation and Debugging. Models such as Codex [3] can generate scaffolding code, unit tests, or entire functions from natural language descriptions, accelerating software development and improving productivity. Research also explores the use of LLMs for debugging and static analysis to identify potential software vulnerabilities.\n\u2022 Task Planning and Reasoning. Recent advancements integrate symbolic and factual reasoning with language models, facilitating tasks such as chain-of-thought prompting [12] and multi-step planning [29]. These improvements enable structured decision-making in scenarios requiring multi-step execution and complex logic.\nSince LLMs essentially learn a broad \"prior\" from large-scale text corpora, they can be adapted for novel tasks through well-crafted prompts. This prompt engineering paradigm significantly lowers the barrier to automating domain-specific workflows, including cybersecurity-related tasks. Notably, LLMs can parse tool outputs, synthesize commands, and adjust actions based on prior responses, making them particularly well-suited for penetration testing scenarios requiring multi-step, context-aware orchestration."}, {"title": "2.3 Existing Research and Opportunities for Improvement", "content": "Recent research has explored the application of LLMs to automate various penetration testing tasks, from initial access to remediation. For example, PentestGPT [6] introduces an LLM-based framework for guided exploitation using a task-tree architecture, while PenHeal [11] focuses on vulnerability discovery and mitigation strategies. Tools such as BLADE [24] and AutoAttacker [28] extend automation into post-exploitation, and Wintermute [9] highlights autonomous Linux privilege escalation.\nDespite these advancements, a key gap remains in achieving fast, fully automated initial infiltration. To date, most approaches still rely on human-in-the-loop validation or focus primarily on post-exploitation rather than providing a high-speed, end-to-end framework for breaching a target. From a software development and operations perspective, the critical questions are often, \u201cCan my system be infiltrated, and how quickly can that happen?\" Delivering an IP-to-Shell workflow at practical speed and cost could provide significant value to a broader audience, including security-conscious organizations and industries lacking dedicated security teams.\nIn the following sections, we introduce an approach to address this need. By focusing on the initial-access phase and aiming for fully automated, low-cost, high-speed penetration testing, our work seeks to enhance overall cybersecurity and enable a wider range of users to incorporate real-world adversarial testing into their development processes."}, {"title": "3 Threat Model and Problem Definition", "content": "3.1 Threat Model\nThe RapidPen agent is assumed to have minimal prior knowledge of the target system:\n\u2022 Target IP Only. The attacker (i.e., RapidPen) is provided only with the IP address of the machine under test, without additional configuration details or vulnerability disclosures.\n\u2022 Shell Acquisition. In its current prototype, RapidPen exploits vulnerabilities using the Metasploit Framework [20] (msfconsole) and considers a shell \"obtained\" once logs confirm that a reverse shell has been successfully established.\n3.2 Assumptions\nRapidPen operates under the assumption that it can establish TCP connections to the target system's IP address. If necessary, an OpenVPN configuration file can be deployed within the RapidPen environment to enable VPN-based connectivity. Beyond these basic networking requirements, no additional external services or credentials are assumed.\n3.3 Scope and Limitations\n\u2022 Pre-Scanning Recon Excluded. Passive reconnaissance steps, such as searching domain records or metadata leaks, are beyond the scope of this study. Instead, we focus on active port scanning as the starting point.\n\u2022 No Post-Exploitation. RapidPen does not attempt privilege escalation or lateral movement once a shell is acquired.\n\u2022 No Web-Based Attacks. Although web vulnerabilities can serve as entry points, the current system does not address them. Future work will explore extending RapidPen to support web exploits.\n\u2022 No UDP-Based Attacks. This implementation is limited to TCP-based targeting. UDP-based exploits and scans are not considered in this study."}, {"title": "4 Design Overview of RapidPen", "content": "In this section, we describe the overall architecture of our fully automated penetration testing framework, referred to as RapidPen. We adopt the ReAct [29] paradigm, which consists of a Re (task planning) module and an Act (command execution) module, both supported by specialized retrieval-augmented generation (RAG) [13] repositories. Below, we detail the system architecture, how each module interacts, and how failures are handled.\n4.1 System Architecture\nFigure 1 provides a high-level overview of RapidPen's core components.\n\u2022 Input: The user provides the target IP address.\n\u2022 Output: RapidPen-vis displays penetration test progress (e.g., logs, discovered vulnerabilities) and generates the final reports, including the command used to obtain a shell.\n\u2022 Re and Act Modules: These modules jointly implement the ReAct loop, coordinating tasks and executing commands.\n\u2022 RapidPen-vis: A separate visualization tool for monitoring intermediate processes and final reports. [15]\n4.2 PTT as a Core Data Model in the Re Module\nIn prior work on PentestGPT [6], the concept of a Pentesting Task Tree (PTT) was introduced to structure the entire penetration testing process as an attributed tree, where each node represents a task (e.g., port scanning, vulnerability testing, exploitation), and edges define the flow of reasoning or dependencies between tasks. The tree evolves dynamically as new tasks are generated, completed, or require backtracking due to partial failures.\nPTT Definition (from PentestGPT). A PTT is essentially a labeled tree (or attributed polytree) with the following key elements:\n1. Nodes (tasks) with unique identifiers and optional child nodes.\n2. Attributes assigned to each node, such as task descriptions, current statuses, and relevant parameters.\n3. Edges representing parent-child relationships (e.g., subtask expansions) that structure the penetration testing workflow at multiple levels of detail.\nOur Extensions. We integrate the PTT as the core data model in the Re (reasoning) module to structure and coordinate tasks. In addition to the standard PentestGPT functionality, we introduce the following enhancements:\n1. Environment Metadata. Our PTT includes a dedicated metadata block capturing details about the penetration testing environment (e.g., attacker and target IP addresses, time stamps, test status).\n2. Act Results in Nodes. Each task node maintains a history of command executions, including the executed command string, exit_code, exit_class, and a brief log summary. This allows for a clearer interplay between the Act module outputs and the Re module's reasoning state.\n3. JSON-based I/O. We consistently store and exchange the PTT in JSON format, ensuring that the LLM operates within a strict schema. This prevents ambiguity or \"hallucination\" when the LLM appends new tasks or updates existing nodes. A simplified JSON schema is provided in Listing 1."}, {"title": "4.3 Layered ReAct Modules in RapidPen", "content": "RapidPen's execution logic follows the ReAct [29] paradigm, where:\n1. Re (Task Planning): Monitors current logs, prior task outcomes, and \"success-case\" data to propose new tasks or exploit paths.\n2. Act (Command Execution): Issues commands to gather information or launch attacks. Upon receiving logs, the system refines or regenerates commands before feeding outcomes back to Re."}, {"title": "4.4 RAG for Offensive Security", "content": "While ReAct provides a general \u201creasoning-acting\" pattern, RapidPen enhances this approach with two specialized Retrieval-Augmented Generation (RAG) repositories for domain-specific commands and proven exploit steps:\n1. Act (L1) Command Generation RAG: A curated collection of 148 Markdown files from HackTricks [4], primarily focused on \"Network Services Pentesting\" (e.g., SMB, FTP, SSH). These documents provide typical scan commands, exploit techniques, and enumeration strategies relevant to the initial-access phase. The Command Generation module references these documents to generate commands via the LLM.\n2. Re (L2) New Tasks (Success Cases) RAG: PTTs in JSON format capturing successful pentesting sequences. Currently, this dataset includes two PTTs for the Blue machine in Hack The Box [1]. Each file outlines step-by-step instructions, from scanning to obtaining a shell. The New Tasks (Success Cases) module generates a search query for the RAG based on the results of the most recent task execution. It then analyzes the retrieved PTT output to generate effective subtasks."}, {"title": "4.5 Feedback Cycle in Act Module", "content": "Figure 5 illustrates the feedback loop within the Act module, where command generation and execution are tightly coupled with log analysis and error handling. After executing each command, the system interprets the outcome (e.g., SUCCESS, TIMEOUT, COMMAND_NOT_FOUND) and determines whether to retry or escalate. The feedback loop follows these key policies:\n1. Three-Strike Retry Limit. When commands are generated and executed in a cycle, the Log Analysis module evaluates the logs to determine if the result is conclusive. If the command fails or does not produce sufficient evidence for further progress, the Act module refines or regenerates the command and re-executes it. This cycle repeats up to three times. If no success is achieved after three attempts, RapidPen marks the corresponding task as failed and reports this outcome to the Re module.\n2. Handling Timeouts. In some cases, command execution may hang indefinitely if the target server is unresponsive. To prevent this, each command is assigned an initial timeout (e.g., 30 seconds). When a TIMEOUT occurs, the next execution cycle begins with Act (L1) Command Generation searching for a faster alternative command. For example, it may replace an nmap port scan command with rustscan for quicker execution. If no faster alternative exists, the system doubles the timeout threshold to allow more time for execution.\n3. Handling Missing Commands or Files. Since Act (L1) Command Generation references Hack Tricks and other sources, it may propose commands or reference files that are not available in the Act (L1) Command Executor environment. In such scenarios, COMMAND_NOT_FOUND or FILE_NOT_FOUND errors occur. Upon detecting these, RapidPen employs a fail-fast strategy: it terminates the current penetration test session and notifies the developer. The rationale is that an external installation or environment fix is required before continuing, and automated retries would be ineffective."}, {"title": "5 Implementation", "content": "This section describes the prototype implementation of Rapid-Pen. While Chapter 4 presented the overall design, here we focus on the specific tools, infrastructure, and configurations used to realize our fully automated pentesting workflow.\n5.1 Prototype Setup and LLM Usage\nCurrently, RapidPen exists as a prototype implementation built on top of Dify\u00b9. We run Dify locally to manage interactions with multiple Large Language Model (LLM) endpoints. Additionally, we integrate LangSmith\u00b2 with Dify to precisely measure and monitor LLM invocation costs. This setup enables tracking of API calls, token usage, and associated costs under realistic testing conditions.\nOur system exclusively employs OpenAI's gpt-4o [17] as the underlying language model. Internally, we maintain 10 LLM instances dedicated to the Re module (task planning and reasoning) and 8 LLM instances for the Act module (command generation and log analysis). Initially, some prompts were adapted from PentestGPT [6]; however, all prompts have since been replaced with original designs.\n5.2 RapidPen-vis\nFor visualization and reporting, we provide RapidPen-vis, consisting of:\n\u2022 Server-Side: A Python Flask application responsible for rendering real-time test logs and final pentest summaries.\n\u2022 Client-Side: A lightweight vanilla JavaScript frontend that communicates with the Flask API to fetch and display pentesting progress graphically.\nThis interface allows operators to observe the automated exploit process, review execution logs, and track the overall state of penetration testing tasks.\n5.3 Custom Dify-Sandbox\nDify provides a secure Python execution environment called Dify-Sandbox, which restricts system calls and external network access within a controlled Docker container. However, our Act (L1) Command Executor requires broader system access to execute real-world pentesting commands. To address this limitation, we implemented a custom Docker image that maintains the same REST API interface as Dify-Sandbox, but without restrictive sandbox policies. This customized container is integrated into our docker compose setup as a direct replacement for the official Dify-Sandbox. It processes the same API calls for command execution while permitting the necessary system calls and network interactions required for pentesting.\nBy leveraging this custom sandbox implementation, we maintain compatibility with Dify's workflow and Python execution mechanism while removing constraints that would otherwise prevent valid pentesting operations. This dual approach ensures that our local environment remains modular and extensible, allowing for future experiments and improvements in pentest automation."}, {"title": "6 Evaluation", "content": "This section presents preliminary experiments on the Legacy machine from Hack The Box [1], designed to validate Rapid-Pen's ability to establish an initial foothold (IP-to-Shell) in an early-stage prototype. Future work will extend these experiments to a broader set of targets.\n6.1 Objectives and Questions\nOur evaluation seeks to answer the following key questions:\n1. Success Rate: How often does RapidPen successfully achieve initial access (shell) on a known vulnerable machine?\n2. Time and Bottlenecks: How long does an average run take, and which modules in RapidPen consume the most time?\n3. LLM Cost: What is the cost of an automated penetration test in terms of LLM usage?\n4. Behavioral Insights: How does the feedback mechanism in the Act module (cf. Section 4.5) function in practice, and what role does the Re (L2) New Tasks (Success Cases) RAG (cf. Section 4.4) play in generating effective exploit paths?\n6.2 Experimental Setup\nTarget Machine (HTB Legacy). We selected the Hack The Box \"Legacy\" machine as our primary target. This machine features an older SMB server exposed on tcp/445 with the MS17-010 (EternalBlue) vulnerability, enabling remote code execution (RCE).\nAttacker Environment. We executed the RapidPen orchestrator and RapidPen-vis on a local MacBook Pro (13-inch M2, 24 GB RAM, macOS Sequoia 15.3.1). The Dify-based RapidPen orchestration runs in Docker containers, including our custom sandbox for actual command execution.\n6.2.1 Testing Two Configurations\nTo evaluate the impact of Re (L2) New Tasks (Success Cases) RAG, we conducted two sets of experiments:\n\u2022 With Success Cases Enabled (Runs #1-10): The system had access to a stored PTT reflecting successful exploitation steps on HTB \"Blue\" (which shares the MS17-010 vulnerability).\n\u2022 Without Success Cases (Runs #11-20): The system relied solely on scanning and standard exploit references, without leveraging pre-recorded successful sequences.\nFor each run, we reset the environment, then launched RapidPen with a single target IP. We recorded the following metrics:\n\u2022 Outcome: Success (obtained a shell) or Failure.\n\u2022 #Steps: Number of PTT expansions initiated by the Re (L1) PTT Planner, from start to success/failure.\n\u2022 Elapsed Time: Total wall-clock time from test initiation to termination.\n6.3 Results\n6.3.1 Overall Success Rates and Timings\nWith Success Cases (#1-10). In the left column of Figure 6, we observe that 6 out of 10 runs successfully achieved a shell on the Legacy machine. Runs that failed tended to get stuck in repeated enumerations or timed out when nmap scanning did not produce conclusive results quickly. When the test succeeded, execution time ranged from 200-400 seconds, with a moderate correlation between the number of steps and elapsed time.\nWithout Success Cases (#11-20). In contrast, the right column of Figure 6 presents a less favorable outcome. The system succeeded in only 3 out of 10 runs. We also observed more outlier runs that either timed out after multiple scanning attempts or executed redundant exploit attempts. For instance, Run #13 followed an excessively long sequence of unsuccessful attack vectors. Consequently, the average failure time was significantly higher, occasionally exceeding 400 seconds. When an exploit succeeded, execution time was typically below 350 seconds.\n6.3.2 Observations and Discussion\nThe presence of Success Cases significantly improved the success rate, as the Legacy machine shares the same SMBv1 vulnerability exploited by HTB \u201cBlue.\u201d Although the exact environment differs slightly, the fundamental MS17-010 exploit steps stored in the PTT closely align with the real target's requirements. For more diverse vulnerabilities, we expect a lower direct transferability of RAG data; however, we anticipate that it will still accelerate the identification of effective enumeration and exploitation paths.\n\u2022 Task Steps vs. Time: Runs with fewer steps generally completed faster.\n\u2022 Failure Causes:\n1. Runs #1, #16, #18, and #20: Act (L1) Command Generation produced smbclient commands with incorrect parameters, resulting in COMMAND_NOT_FOUND, FILE_NOT_FOUND, and OTHERS errors.\n2. Runs #4, #6, and #17: Act (L1) Command Generation generated inappropriate commands for the given tasks, leading to OTHERS, FILE_NOT_FOUND, and COMMAND_NOT_FOUND errors.\n3. Run #9: Act (L1) Command Execution failed to execute the enum4linux command on port 139. Act (L1) Log Analysis classified it as an OTHERS error, triggering the fail-fast mechanism.\n4. Run #13: The system continuously attempted to exploit port 139. After exceeding 1200 seconds, execution in Dify stalled.\n5. Run #14: Re (L1) PTT Prioritizer generated a hallucinated non-leaf task in the PTT, causing a validation error in the Act module.\n\u2022 Future Generalization: We plan to extend testing to machines with partially overlapping but not identical vulnerabilities to assess how well the success-case RAG generalizes.\n6.4 Module-Wise Time and Cost Breakdown\nTo gain deeper insights, we instrumented the runs (particularly in the with Success Cases scenario) to measure each module's contribution to the total runtime and LLM costs.\nThe runtime breakdown (see Figure 7) indicates that Act (L1) Command Execution contributes the most to total execution time, followed by Re (L1) PTT Planner.\nNext, we analyze the LLM cost distribution (see Figure 8). Re (L1) PTT Planner dominates the cost due to frequent PTT expansions and the overhead of merging newly generated subtasks from Re (L2) New Tasks (Success Cases).\nThe current cost and execution time are practical for targeted penetration testing scenarios. However, further optimization is possible by reducing large PTT inputs (which can sometimes exceed 14KB) to the LLM and improving error-handling mechanisms.\n6.5 Behavioral Insights\nAct Feedback Examples. As described in Section 4.5, the Act module attempts to recover from command failures by either adjusting parameters or switching to alternative tools. In multiple runs, we observed that when an nmap scan timed out, it was immediately replaced with rustscan. Additionally, when an exploit attempt using msfconsole timed out, the system generally did not find an alternative command and instead increased the timeout from 30 to 60 seconds before re-executing the command.\nRole of Re (L2) New Tasks (Success Cases). Section 4.4 introduced the New Tasks (Success Cases) RAG, where RapidPen references a stored success PTT from HTB \"Blue.\u201d"}, {"title": "7 Discussion", "content": "7.1 Benefits and Future Directions of the Act Feedback Mechanism\nThe self-reliant feedback cycle implemented in the Act module (Section 4.5) significantly reduces the need for human intervention. As long as the tasks assigned by the Re module are appropriate, the Act module persistently re-generates and refines commands, interprets resulting logs, and explores alternative strategies when errors occur. This design choice allows RapidPen to continue progressing without manual oversight, enhancing its ability to achieve fully automated penetration testing.\nHowever, the current fail-fast mechanism employed by the Act module causes the entire process to terminate upon encountering specific errors, such as COMMAND_NOT_FOUND, FILE_NOT_FOUND, and OTHERS. While this approach prevents unnecessary retries and repeated failures, it can also abruptly halt the penetration test in cases where partial remediation such as installing missing packages or updating outdated command syntax-would suffice.\nFuture improvements should modify both Command Generation and Command Execution to address these errors dynamically. A more nuanced error-handling strategy should categorize failures, apply targeted retries or fixes, and reserve immediate termination for cases where it is strictly necessary. Such refinements would further enhance the system's robustness and adaptability in real-world scenarios.\n7.2 Advantages and Limitations of Using Success Cases\nOur experiments indicate that RapidPen's use of Success Cases accelerates exploit discovery when the target vulnerability closely matches those in previously recorded penetration tests. For example, referencing the MS17-010 exploit path from the \"Blue\" machine on Hack The Box (HTB) was effective against the \"Legacy\u201d machine, which shares a similarly vulnerable SMBv1 service. This demonstrates that reusing existing exploit sequences can streamline scanning and exploitation, leading to faster and more reliable outcomes.\nHowever, handling scenarios where no relevant Success Cases exist remains an open problem. Zero-day vulnerabilities or configurations that have never been encountered may require more advanced reasoning beyond merely \"copying\" from past success. While our current approach leverages the LLM's internal knowledge and a RAG-based repository, a more powerful framework for abstracting exploit techniques-enabling RapidPen to discover novel attack strategies-will be essential for addressing unknown threats. Designing and evaluating such a next-generation system is a critical step toward making automated pentesting broadly effective against new or rare vulnerabilities.\n7.3 Expanding the Attack Surface\nAlthough RapidPen currently achieves fully automated IP-to-Shell compromises, it does not yet address the post-exploitation phase. Privilege escalation, lateral movement, and deeper analysis of the compromised environment represent logical extensions for future work. In particular, tools like BLADE [24] and AUTOATTACKER [28] already explore AI-assisted post-exploitation. Extending RapidPen to integrate with such frameworks could broaden its applicability, enabling more comprehensive, end-to-end assessments.\nAnother important direction involves web exploits, which are currently absent from the system. Web-based vulnerabilities often require specialized knowledge-ranging from injection techniques to authentication bypass methods and may involve GUI-based testing beyond simple command-line interactions. Incorporating these capabilities would likely require RAG expansions to include relevant web exploitation knowledge bases and potentially adapt the Act module to handle browser automation. Achieving the same degree of autonomy for web exploits poses additional research and engineering challenges.\n7.4 Ethical and Safety Considerations\nAlthough the user explicitly provides a target IP address to RapidPen, reducing the risk of scanning unrelated systems, the possibility of misuse cannot be ignored. Any automated exploit tool can be leveraged for malicious purposes if placed in the wrong hands or configured improperly. Future developments should focus on access control, rate-limiting, and formal usage policies especially if the system transitions from a research prototype to a commercial or open-source deployment. Additionally, practical safeguards like monitoring logs, validating the legitimacy of the target environment, and enforcing strict network boundaries are pivotal for preventing inadvertent attacks against unauthorized hosts.\nOverall, while RapidPen lowers the barrier for automated security testing, it underscores the need for responsible deployment practices. Addressing legal and ethical ramifications is essential to ensuring that the benefits of fully automated pentesting do not come at the expense of broader cybersecurity risks."}, {"title": "8 Related Work", "content": "8.1 LLM-Based Penetration Testing\n8.1.1 PentestGPT \u2013 Task Tree-Driven AI Pentesting\nRecent research has explored using large language models (LLMs) to automate penetration testing. PentestGPT [6] is a notable example: it leverages an LLM (GPT-3.5/GPT-4) to guide the pentest process via a Pentesting Task Tree (PTT) structure. Inspired by attack trees, the PTT decomposes engagements into sub-tasks (e.g., port scanning, service enumeration, exploitation), allowing the LLM to maintain context throughout testing. PentestGPT operates using three coordinated modules: a Reasoning module (the \"lead tester\") that updates the task tree and determines next steps, a Generation module (the \"junior tester\") that proposes specific commands, and a Parsing module to summarize tool output.\nWhile PentestGPT automates attack planning, it requires a human-in-the-loop to execute suggested commands and correct errors. Users must review and refine commands before execution, limiting its autonomy. Thus, PentestGPT functions more as a guided assistant rather than a fully autonomous pentesting tool.\n8.1.2 Other LLM-Driven Pentesting Tools\nBeyond PentestGPT, several emerging tools utilize LLMs for penetration testing, each focusing on different aspects of the workflow. These tools can be categorized as follows:\nTools that Automate Initial Access but Focus on Broad Vulnerability Scanning Rather than Speed\n\u2022 PenHeal [11] \u2013 an AI agent that operates without direct human involvement, designed to identify a broad range of vulnerabilities and propose mitigation strategies. Although the paper does not explicitly confirm automation of initial foothold attacks, it is possible that PenHeal's capabilities overlap with RapidPen in terms of initial access. However, no evaluation is provided regarding the time and cost required to achieve initial access. In contrast, RapidPen focuses on demonstrating the most immediate security risk-namely, gaining unauthorized shell access as quickly as possible-before handing over control to established tools designed for post-exploitation. While RapidPen does not yet provide broad vulnerability coverage or automated remediation, incorporating such features remains an area for future exploration.\nTools That Focus on Post-Exploitation and Are Complementary to RapidPen\n\u2022 BLADE [24] \u2013 Breaking Limits, Automate Deep Exploitation \u2013 an AI-driven pentesting agent built on an autonomous agent framework (Microsoft's Auto-Gen [27]). BLADE autonomously orchestrates exploitation tasks by leveraging external tools and dynamic script generation. For example, it uses pre-configured tools like LinPEAS for privilege escalation and John the Ripper for credential cracking to achieve deeper system compromise. Additionally, it includes agents for network scanning and lateral movement, showcasing how multi-agent Al systems can enhance penetration testing workflows."}, {"title": "8.2 Reinforcement Learning-Based Penetration Testing Approaches", "content": "Deep reinforcement learning (RL) has also been explored for autonomous pentesting. RL-based systems learn attack sequences by interacting with an environment and optimizing for successful exploits. Key contributions include:\n\u2022 Hu et al. [10", "8": "applied RL to vehicular ad-hoc network (VANET) penetration testing", "14": "proposed a hierarchical RL agent for large-scale network penetration", "23": "an early RL-powered pentesting tool integrated with Metasploit", "Automation": "RapidPen is designed for full automation of initial access", "Techniques": "RapidPen focuses on achieving unauthorized shell access as quickly as possible, covering a broad range of network and system-level exploitation techniques. Unlike PentestGPT, which primarily provides recommendations, RapidPen directly executes exploits. Meanwhile, tools like BLADE and AutoAttacker specialize in post-exploitation rather than"}]}