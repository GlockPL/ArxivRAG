{"title": "ZERO LOSS GUARANTEES AND EXPLICIT MINIMIZERS FOR GENERIC OVERPARAMETRIZED DEEP LEARNING NETWORKS", "authors": ["THOMAS CHEN", "ANDREW G. MOORE"], "abstract": "We determine sufficient conditions for overparametrized deep learning (DL) networks to guarantee the attainability of zero loss in the context of supervised learning, for the L2 cost and generic training data. We present an explicit construction of the zero loss minimizers without invoking gradient descent. On the other hand, we point out that increase of depth can deteriorate the efficiency of cost minimization using a gradient descent algorithm by analyzing the conditions for rank loss of the training Jacobian. Our results clarify key aspects on the dichotomy between zero loss reachability in underparametrized versus overparametrized DL.", "sections": [{"title": "1. INTRODUCTION", "content": "Sufficiently overparameterized deep feed-forward neural networks are capable of fitting arbitrary generic training data with zero L\u00b2 loss. Later in this paper, we prove that this is true by an explicit construction of the zero loss minimizers without invoking gradient descent. On the other hand, analysis of this case shows something surprising: when the width is large enough, increasing the depth of the network can introduce obstructions to efficient cost minimization using gradient descent. This phenomenon is caused by rank loss of the training Jacobian, violating the assumptions of the simplicity guarantees in [6]. The rank of this object is very poorly studied, despite its relevance to the analysis of gradient descent trajectories.\nWhen the width is great enough, depth is not necessary at all to achieve zero loss, as linearly generic data may be fit by linear regression. However, this is an advantage from the point of view of qualitative analysis: by focusing on such a simple situation, the challenges to gradient flows introduced by depth become clearer by contrast. We prove sufficient conditions under various fairly general assumptions that generic training data ensures that the Jacobian has full rank for sufficiently overparameterized models, thus ensuring relative simplicity of gradient descent in these regions. Together with previous papers by one of the authors, our results shed light on key aspects of the dichotomy between zero loss reachability in underparametrized versus overparametrized DL."}, {"title": "2. DEEP NEURAL NETWORKS", "content": "We define the DL network as follows. The input space is given by $R^M$, with training inputs $x^{(0)}_j \u2208 R^M$, $j = 1,..., N$. The reference output vectors are given by a linearly independent family {ye \u2208 R?|l = 1,..., Q} with Q < M, which label Q classes. We introduce the map \u03c9 : {1, ..., N} \u2192 {1, ..., Q}, which assigns the output label w(j) to the j-th input label, so that $x^{(0)}_j$ corresponds to $y_{\u03c9(j)}$.\nWe assume that the DL network contains L hidden layers. The l-th layer is defined on $R^{Me}$, and recursively determines\n$x^{(l)}_j = \u03c3(W_l x^{(l-1)}_j + b_l) \u2208 R^{M_e}$\nwith weight matrix $W_l \u2208 R^{M_e\u00d7M_{e-1}}$, bias vector $b_l \u2208 R^{M_e}$. The map \u03c3: $R^{M\u00d7N}, A = [a_{ij}] \u2192 [\u03c3(a_{ij})]$ acts component-wise by way of the scalar activation function \u03c3: R \u2192 I \u2286 R where I is a connected interval. We assume that o has a Lipschitz continuous derivative, and that the output layer\n$x^{(L+1)}_j = W_{L+1}x^{(L)}_j + b_{L+1} \u2208 R^?$\ncontains no activation function."}, {"title": null, "content": "Let \u2208 RK enlist the components of all weights We and biases be, l = 1, . . ., L + 1, including those in the output layer. Then,\n$K = \\sum_{l=1}^{L+1} (M_eM_{e-1} + M_e)$\nwhere Mo = M for the input layer.\nIn the output layer, we denote $x^{(L+1)}_j \u2208 R^Q$ by $x_j[@]$ for brevity, and obtain the $L^2$ cost as\n$C[x[@]] = \\frac{1}{2N} ||x-Y_{\\omega}||^2_{R^{QN}} = \\frac{1}{2N} \\sum_j ||x_j[@]-Y_{\\omega(j)}||^2$\nusing the notation $x := (x_1,...,x_n)^T \u2208 R^{QN}$, and $|\\cdot|_{R^n}$ for the Euclidean norm.\nTraining of the networks corresponds to finding the minimizer @ \u2208 RK of the cost, and we say that zero loss is achievable if there exists @ so that C[x[0]] = 0.\nIn matrix notation, we let\n$X^{(l)} := [x^{(l)}_1 ... x^{(l)}_N] \u2208 R^{M_e \u00d7 N}$\n$Y_\u03c9 := [\u2026Y_{w(j)}\u2026] \u2208 R^{Q\u00d7N}$.\nThen, we have that\n$X^{(l)} = \u03c3(W_l X^{(l-1)} + B_l)$\nwhere $B_l := b_l u_N$ with $u_N := (1, 1, ..., 1)^T \u2208 R^N$. Then, the solvability of\n$W_{L+1}X^{L} + B_{L+1} = Y_\u03c9$\nis equivalent to achieving zero loss."}, {"title": "2.1. Underparametrized DL networks.", "content": "If K < QN, the DL network is underparametrized, and the map\n$f_{X^{(0)}} : R^K \u2192 R^{QN}$\n$0 \u2192 x[@]$\nis an embedding. Accordingly, for generic training data X (0) the zero loss minimizer of the cost is not contained in the submanifold $f_{X^{(0)}} (R^K) CR^{QN}$.\nHowever, if the training data is non-generic, zero loss can be achieved. As proven in [3, 5], sufficient clustering and sufficient depth (L> Q) allows the explicit construction of global zero loss minimizers."}, {"title": "2.2. Paradigmatic dichotomy.", "content": "In combination with Theorem 2.2, below, and results in subsequent sections of this paper, we establish the following paradigmatic dichotomy between underparametrized and overparametrized networks:\n\u2022 Underparametrized DL networks with M > M\u2081 > \u2026 > ML > Q layer dimensions, and (locally mollified) ReLU activation function\ncannot in general achieve zero loss for generic training data,\nbut with sufficient depth, they are capable of achieving zero loss for non-generic training data with sufficient structure.\nFor sufficiently clustered or linearly sequentially separable training data, the zero loss minimizers are explicitly constructible without gradient descent, [3, 4].\n\u2022 Overparametrized networks with layer dimensions M = M\u2081 = ML > Q and activations acting as diffeomorphisms \u03c3:RM \u2192 R\u00a5\nare capable of achieving zero loss if the training data are generic; the minimizers are explicitly constructible, without gradient descent.\nHowever, increase of depth can decrease the efficiency of cost minimization via gradient descent algorithm."}, {"title": "3. \u039d\u039f\u03a4\u0391\u03a4\u0399ONS", "content": "We introduce the following notations, which are streamlined for our subsequent discussion.\nDefinition 3.1. The following notations will be used for the context of supervised learning.\n(1) Let X \u2208 RM\u00d7N be the matrix of data. This is a matrix with M rows and N columns, consisting of N data points, each of which is a vector in RM. The data are represented by the columns of X, where the ith column is denoted by $X_i$.\n(2) Let O be a parameterized function realization map, considered as a map RK \u2192 C\u00b0(RM, R), where K is the number of parameters. We will use the notations (\u03b8), fe, or f interchangeably depending on the context. We can extend f to a map g : RMXN \u2192 RN by defining $g\u00b2(Y) = f(Y_i)$, where Y is any data matrix."}, {"title": "4. OVERPARAMETERIZED NETWORKS", "content": "Recent results from [6, 1] have shown that in the overparameterized setting, the dynamics of gradient descent are deformable to linear interpolation in output space, at least away from the 'bad regions' in parameter space where the Jacobian matrix of the network outputs with respect to the parameters is not full rank. More precisely, there is a continuous deformation of output space which converts every gradient descent path which does not encounter such a region into a straight line [6]. This setting also grants convergence speed estimates.\nJacobian rank loss presents a problem for the interpretation of gradient descent: continuous or 'true' gradient descent considered as a solution to the gradient flow ODE is redirected by rank-loss regions and changes direction unpredictably. However, practical implementations of gradient descent such as the ever-popular forward Euler stochastic gradient descent will almost surely 'miss' or 'tunnel through' the measure-zero rank-deficient regions. However, this does not mean that Jacobian rank loss is irrelevant. Rather, it implies that practical gradient descent is not necessarily well modeled by the ideal gradient flow at any point after the trajectory has crossed a rank-deficient barrier.\nIt has long been known in the literature [15] that in the infinite-width limit of a shallow neural network, the Jacobian is constant through gradient descent. This heuristically suggests that in the large parameter limit the Jacobian is generically always full rank. In this section, we will describe some ways in which this inference may be extended (or not) to the case of large numbers of parameters, i.e. K, M > N. This allows us to better understand the qualitative training behavior of networks of arbitrary depth at large (but still finite) width."}, {"title": "4.1. Other Work.", "content": "To our knowledge, little work has been done on the rank of the output-parameter Jacobian. Some related works are as follows:\n\u2022 Some analysis of the clean behavior of gradient descent was performed in [16]. Their work assumes that 'no two inputs are parallel', i.e. that X is full rank the in language of our work, and they work with shallow networks only. We believe that this work contributes towards extending such analysis to the more general case of deep learning, and to more general activation functions.\n\u2022 The papers [17, 18] also investigate the Jacobians of neural networks as they relate to gradient descent, but they are not the same ones discussed here. Note that the Jacobian discussed here is 'df/d0' not 'df /dx'.\n\u2022 The relation of the Jacobian [in the sense of this paper] rank and generalization performance is investigated in detail in [14], but the setting differs greatly from this work."}, {"title": "4.2. Preliminaries.", "content": "We will only deal with networks that are strongly overparameterized. We will show shortly that, as expected, strongly overparameterized networks are usually solvable.\nDefinition 4.1 (Strongly Overparameterized). We say that a template model (\u0398, L, X) is strongly over-parameterized if M > N and K > N. Note that the former implies the latter for essentially every neural network model.\nIntuitively, it is sensible that wider matrices will fail to be full rank less frequently. However, there is an important wrinkle: if X itself is rank deficient, then Jacobian rank deficiency may be more common than expected. The following results (regarding several common feed-forward models) can be summarized as follows:\n(1) If X is full rank then the model is solvable and D is almost always full rank."}, {"title": null, "content": "(3) Define the Jacobian matrix D as follows. Let the element of D at the ith row and jth column be equal to dg/80i (recall that g depends on @ because it is defined in terms of f = (0)). It follows that $DERN\u00d7K$\n(4) Let y \u2208 RN be the vector of labels, i.e. the intended outputs for the datapoints.\n(5) We define the loss as a function of \u03b8 as $C(0) := (2N)^{-1}||g(X) \u2013 y||^2$, where the norm denotes the countable (2 norm. This is the standard mean squared error loss.\n(6) If necessary for convenience, define a template model as a pair (\u0398, \u03a7) and an instantiated model as a triple (\u0398, \u03a7, 0). Each of these objects carries implied values of N, M, and K.\n(7) We call a template model (\u0398, X) solvable if for all y there exists 0 such that C(0) = 0."}, {"title": "4.3. Linear Model.", "content": "It is most natural to begin by recalling classical underdetermined linear regression. Consider the optimization problem min ||w||32 s.t. XTw-y = 0. By introducing the Lagrangian L(w, 1) = ||w|| \u2013 (\u03bb, \u03a7\u03a4w \u2013 y) and differentiating, we obtain the saddle point conditions w = XX and XTw - y = 0. Substituting obtains XT XX = y. If we assume that XTX is invertible, i.e. that X is full rank (recall these are real matrices), then X = (XTX)-\u00b9y. Finally, another substitution obtains w = X(XTX)\u2212\u00b9y given by the Moore-Penrose inverse of XT. This process is quite familiar and will be used in the later arguments as a step.\nHowever, note that XT is surjective as a linear transformation if and only if it is full rank. Therefore, if X is not full rank, the problem has no solution unless y is in the span of XT, which is measure zero in RN. Note that the critical step involves the assumption that X is full rank: if the data of X are not linearly generic, we cannot fit arbitrary labels y using a linear model. We next turn to the quite simple answer for the Jacobian rank:\nProposition 1. Let be linear regression. Then the following are equivalent:\n\u2022 D is full rank.\n\u2022 X is full rank.\n\u2022 (\u0398, X) is solvable.\nProof. In the language we have developed for supervised learning, we can express fo(x) = (x,0), and go(X) = XT0. It follows that D = XT. The first equivalence is then trivial. The second equivalence follows from the above discussion."}, {"title": null, "content": "We will need to use the following technical lemma. Please observe that in this paper we will use the Einstein summation convention for repeated indices, but we intend to suppress the summation over a particular index if the index is repeated on one side of an equation, but not on the other side. For example, we write the definition of the Hadamard product of vectors as $(u \u2299 v)^i = u^i v^i$, and no summations are implied.\nDefinition 4.2 (Broadcast Vectorized Tensor Product). Let $A \u2208 R^{n\u00d7s}, B \u2208 R^{n\u00d7t}$. Define a matrix $AB E R^{n\u00d7st}$ by letting $(A&B)^i = Vec[A^i \u00d0 B^i]$ (where the tensor product is represented by the Kronecker product). We will refer to the column indices of AB with a 'pair index' (\u03b1, \u03b2). The above definition can also be stated as follows:\n$(\u0391\u0392)_{(\u03b1,\u03b2)}^i = \u0391^i\u0392^i_\u03b2$\nObserve that the operation is commutative up to column permutation.\nLemma 1. Let $A \u2208 R^{n\u00d7s}, B \u2208 R^{n\u00d7t}$. Assume that s, t > n and B has no zero rows. Then if A is full rank, it follows that AB is full rank.\nProof. For brevity, denote Z = AB. We proceed by contrapositive. Assume that Z is not full rank.\nThen, since s,t \u2265 n, st \u2265 n, so the rows of Z are linearly dependent. By definition, there must exist c\u2208 Rn such that c\u2260 0 and c\u00a1Z\u00b2 = 0. Therefore there must exist\u00ee such that c\u00ee \u2260 0. Since B has no zero rows, there must exists \u1e9e such that B\u2260 0. Define \u03b7 \u2208 Rm by nj := c; B. Since R has no zero divisors, we know that a = c\u2081B\n0 = CiZa,B) = CiBA =\u03b2\u03b2\u2260 0, so \u03b7 \u2260 0. Now, pick any a \u2208 [s]. Since c\u00a1Z\u00b2 = 0, in particular we have\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2A. Since a was arbitrary, it follows that A\u00b2 = 0, so A has linearly dependent rows. Since s > n, A is not full rank.\nRemark 3. It is easy to see that if either A or B has a zero row, then AB is not full rank. It is also the case that if $A^i = a A$ and $B^i = b B^i$, then AB has nullity at least 1. However, in general the linear dependence relations of A and B interact unpredictably, and AB is usually, though not always, full rank."}, {"title": "4.4. Abstract Nonlinearity.", "content": "In the case where our network is composed of a single affine transform followed by some abstract fixed non-linearity, we have essentially the same properties as in the linear case. First, the technicalities:\nDefinition 4.3 (Fixed Nonlinearity Affine Network). Let \u03c3 : RM\u2192 R be a surjective C\u00b9 submersion and \u0398(\u03b8)(x) = \u03c3\u03bf\u0391(x) where A(x) = Wx+b, 0 = (W, b), and $W \u2208 R^{M\u00d7M}$. We say that \u0472 is a fixed nonlinearity affine network on \u03c3.\nProposition 2. If X is full rank, then is solvable.\nProof. Consider any y \u2208 RN. For any i \u2208 [n], surjectivity of o implies that o\u00af\u00b9(y\u00b2) \u2286 RM is nonempty. Therefore, we may pick \u1ef9 \u2208 \u03c3\u22121(y\u00b2) for each i. The \u1ef9 assemble into the rows of a matrix \u1ef8 \u2208 RN\u00d7M. Define Z = X (XTX)\u00af\u00b9Y. It follows that XTZ = Y. Let W = ZT and b = 0. Then \u03c3(A(X\u2081)) = \u03c3(ZT X\u2081) = \u03c3(\u0178\u00bf) = \u03c3(\u1ef9') = y\u00b2.\nWe can now calculate derivatives of the network with respect to the weights:\n$\\frac{\u2202}{\u2202W_{\u03b1\u03b2}} fo(Xi) = \\frac{\u2202}{\u2202W_{\u03b1\u03b2}} \u03c3(A(Xi) = \\frac{\u2202}{\u2202W_{\u03b1\u03b2}} \u03c3(WX_i+b) = (\u2202\u03b1\u03c3)_{|A(X_{i}) X_i^\u03b2$\nThe derivatives with respect to the biases are much simpler:\n$ \\frac{\u2202}{\u2202B_\u03b1} fo(Xi) = \\frac{\u2202}{\u2202B_\u03b1} \u03c3(A(Xi) = \\frac{\u2202}{\u2202B_\u03b1} \u03c3(WX_i+b) = (\u2202\u03b1\u03c3)_{|A(X_{i})$\nDefine \u2207\u03c3|A(x) \u2208 RN\u00d7M as the matrix whose ith row is \u2207o|4(x\u2081). Then we have the result that\nD = [\u221a6|A(X)&XT \u22076A(X)]\nIf we work with a slightly modified model that has no local bias, the result is even simpler: D = \u221a6|A(X)&XT.\nWe may now turn to making claims about the rank of D.\nProposition 3. If X is full rank, then D is full rank.\nProof. Since is a submersion, \u2207o \u2260 0 everywhere. In particular, \u2207o|A(x) has no zero rows. The result then follows by the Lemma 1.\nThis shows in particular that continuous gradient descent drives the system to the zero-loss minimizer and that numerical gradient descent does not significantly diverge from this behavior. If X is not full rank, then the situation becomes more complicated. However, we can state two general facts:\nProposition 4. Assume that A(X) has no duplicate rows. Then there exists o such that D is full rank. If A(X) also has no zero rows, then D has full rank in the bias-omitted model.\nProof. Pick an arbitrary full rank matrix G\u2208 RN\u00d7M. Then by multivariate Hermite interpolation, there exists a polynomial o such that \u2207o|A(x) = G. The first result is trivial, the latter follows by Lemma 1."}, {"title": "4.5. Feed-Forward Neural Networks.", "content": "Regarding the question of solvability, we will supply the proof of Theorem 2.1. First, we prove a lemma:\nLemma 2. Take any map \u03c3 : RM \u2192 RM which is a local diffeomorphism at at least one point. Then for any dataset X, there exists an affine map A = (W, B) and a dataset Y such that Wo(Y\u00bf) + B = X\u00bf for all i, where Y is a smooth function of W, B, and X.\nProof. Let z \u2208 RM be a point at which is a local diffeomorphism. It follows that there exists an open box set R = \u03a0j\u2208[N] (aj, bj) where aj < bjj such that o(z) \u2208 R and o\u00af\u00b9|r is a diffeomorphism. Since all nondegenerate open boxes are affine equivalent, there exists an invertible affine transform A = (W, B) such that X C WR+ B. Therefore, there exists a unique Y\u00bf = \u03c3\u00af\u00b9(A\u22121(X\u2081)) for each i. Since A is diffeomorphic and o is a diffeomorphism on this region, Y is a smooth function of W, B, X."}, {"title": null, "content": "Then recursively let W, B', and S(5-1) be defined by Lemma 2 such that Wo(S-1))+B = S() for all k, for all j such that 2 \u2264 j \u2264 L+1. Finally, we must solve the linear regression W\u2081X (0) +B\u2081 = S(1), which has a closed-form solution for X(0) full rank, i.e. generic data: pick B\u2081 arbitrarily and set\n$W\u2081 = (S(1) \u2013 B\u2081)((x(0))TX(0))\u22121(x(0))T$\nLet Wj, Bj = W, B' for 2 \u2264 j \u2264 L, but let WL+1 = PW1+1 and BL+1 = PBL+1. This set of weights and biases is constructed to give exactly zero loss, which proves the claim.\nIt should be clear that the model is extremely redundant: we need to jump through a variety of hoops to pull the data back through the layers, during which many arbitrary choices are made, and in the end we just end up using a single-layer linear regression anyway. This reflects the fact that a deep neural network is massive overkill in the high-width tail, i.e. strongly overparameterized regime.\nNow, we turn to characterizing the rank of D. We would like to understand the geometry of the rank-deficient set. Consider the two-layer one-output network f(x) = \u03c9\u03c3(Wx + B), where o is a submersion (such as a mollified ReLU, for example). Then\n$\\frac{\u2202}{\u2202W_{\u03b1\u03b2}} f(x) = \\frac{\u2202}{\u2202W_{\u03b1\u03b2}}\u03c3(W x + B)$\nIt follows that\nD = [\u03c3(WX + B) w\u00bf\u2207o\u00b2/wX+B\u00aeXT w\u00bf\u2207o\u00b2/wX+B]\nWhere the order of the variables is w, W, B. Therefore, for D to be full rank (assuming that X is full rank) it is sufficient that (dowxj+B)w \u2260 0 for all j. Since o is a submersion, this is equivalent to w \u2260 0. This set of parameters has codimension M, and therefore is very unlikely to be encountered by a gradient flow path since M is large we leave a detailed quantitative analysis of this to future work.\nIt is clear that there are many cases where D may be rank deficient independently of the rank of X. For example, if the activation is ReLU, and all the data is mapped into RM in any layer, then D is rank zero. On the other hand, perhaps o is a diffeomorphism, but all the weight matrices and biases are zero. Then D has rank exactly 1, from the output layer bias. In this sense, the extra depth has, strictly speaking, only made the situation worse, since the addition of the redundant layers has also introduced many opportunities for rank loss. However, if we relax the situation slightly, we can apply our earlier result to obtain a guarantee:\nProposition 5. Suppose is a submersion and W2,..., WL+1 are full rank. Then if X is full rank, D is full rank.\nProof. Since W; is full rank for j > 1, X(L+1) is equal to a sequence of submersions applied to W\u2081X(0) + B1. In other words the network may be written as 0(W1X(0) + B1) for \u03b8 a submersion. The result follows by Proposition 3 (note that the assumption of surjectivity was not necessary in that proof)."}, {"title": "5. PROOF OF THEOREM 2.2", "content": "Proof. To begin with, we consider the linear network obtained from L = 0, that is, the output layer is the only layer. Then, zero loss minimization of the cost is equivalent to solving\n$W\u2081X(0) + B\u2081 = Y \u2208 RQ\u00d7N .\nThis slightly generalizes the problem covered in Section 4.3. Let us write W\u2081 = A(X(0))T \u2208 RQ\u00d7M where A \u2208 RQ\u00d7N. Then, generically, X(0) has full rank N < M, so that (X(0))TX(0) \u2208 RN\u00d7N is invertible, and we obtain from\nA(X(0))TX (0) = Y\u025b \u2013 B1,\nthat\nW\u2081 = (Y \u2212 B\u2081)((X(0))TX(0))\u22121(x(0))T.\nIt follows that W\u2081X(0) = Y\u0ed6 \u2013 B\u2081, and we have found the explicit zero loss minimizer.\nNext, we assume L hidden layers. In the output layer, we have Q < M. Then, zero loss minimization requires one to solve\nWL+1X(L) + BL+1 = Yw \u2208RM\u00d7N.\nWe may choose WL+1 \u2208 RQXM to have full rank Q, so that WL+1W1+1 \u2208 R is invertible, and determine bL+1 so that\nX(L) = W+1(WL+1W1+1)\u00af\u00b9(Yw \u2013 BL+1) \u2208RM\u00d7N\nFor instance, one can choose bL+1 to satisfy (WL+1W1+1)-1bL+1 = duq where uq = (1,1,\u2026\u2026,1)T \u2208R? is parallel to the diagonal. Then, for \u5165 > 0 sufficiently large, all column vectors of (WL+1W1+1)-1Yw are translated into the positive sector R. R. Application of WI+1 then maps all resulting vectors into R because the components of WL+1 \u2208 R\u2640\u00d7 are non-negative. This construction is similar as in [2].\nIn particular, we thereby obtain that every column vector of X(L) \u2208 RM\u00d7N is contained in the domain of \u03c3\u00af\u00b9 : R\u00a5 \u2192 RM. To pass from the layer L to L \u2212 1, we then find\nX(L\u22121) = W\u208116\u22121(X(L)) \u2013 W\u00af\u00b9BL\nwhere by can be chosen to translate all column vectors of W\u2081\u00b9\u03c3\u22121(X(L)) into the positive sector R along the diagonal, with \u2212W\u012b\u00b9b\u2081 = \u03bb\u0438\u043c and > > 0 sufficiently large.\nBy recursion, we obtain\nX(e-1) = W-10-1(X(e)) \u2013 We Be ER-1\nfor l = 2,..., L, and\nW\u2081X(0) = \u03c3\u22121(X(1)) \u2013 B1\nwhere X(1) is locally a smooth function of Yw, (Wj, bj). For generic training data, X(0) \u2208 RM\u00d7N has full rank N < M, and in the same manner as in (18), we obtain that\nW\u2081 = (\u03c3\u00af\u00b9(X(1)))((x(0))TX(0))\u22121(x(0))T\nwhere we chose b\u2081 0 without any loss of generality. Hereby, we have constructed an explicit zero loss minimizer.\nThe arbitrariness in the choice of the weights W; and biases bj, j = 2, . . ., L + 1, implies that the global zero loss minimum of the cost is degenerate."}]}