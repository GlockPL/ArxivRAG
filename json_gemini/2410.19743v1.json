{"title": "AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction", "authors": ["Hongru Wang", "Rui Wang", "Boyang Xue", "Heming Xia", "Jingtao Cao", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "abstract": "Large Language Models (LLMs) can interact with the real world by connecting with versatile external APIs, resulting in better problem-solving and task automation capabilities. Previous research primarily focuses on APIs with limited arguments from a single source or overlooks the complex dependency relationship between different APIs. However, it is essential to utilize multiple APIs collaboratively from various sources (e.g., different Apps in the iPhone), especially for complex user instructions. In this paper, we introduce AppBench, the first benchmark to evaluate LLMs' ability to plan and execute multiple APIs from various sources in order to complete the user's task. Specifically, we consider two significant challenges in multiple APIs: 1) graph structures: some APIs can be executed independently while others need to be executed one by one, resulting in graph-like execution order; and 2) permission constraints: which source is authorized to execute the API call. We have experimental results on 9 distinct LLMs; e.g., GPT-40 achieves only a 2.0% success rate at the most complex instruction, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning and finetuning. Our code and data are publicly available at https://github.com/ruleGreen/AppBench.", "sections": [{"title": "1 Introduction", "content": "Empowering Large Language Models (LLMs) (Zhao et al., 2023) with versatile tools such as retrievers (Wang et al., 2023a, 2024b), models (Shen et al., 2023), and even physical robots (Liang et al., 2023), holds significant promise in overcoming inherent limitations, such as hallucination (Ji et al., 2023) and outdated information (Nakano et al., 2021; Liu et al., 2023a), and unveils the immense potential for LLMs to tackle increasingly complex and interactive real-world tasks (Li et al., 2023; Lu et al., 2023). Over the past several months, lots of new benchmarks and datasets have been proposed to evaluate the performance of different LLMs to adeptly select and execute various tools (Li et al., 2023; Shen et al., 2023; Huang et al., 2024a), marking a pivotal milestone in their evolution. Out of plentiful tools in practice, APIs have become one of the fundamental and promising tools in today's digital world, due to greater flexibility and customizability with well-defined format and ease of execution (Qin et al., 2023).\nPrevious works have attempted to evaluate LLMs on their ability to call the correct API in multiple turn dialogues, such as API-Bank (Li et al., 2023) and ToolBench (Qin et al., 2024), or single turn instructions, like APIBench (Patil et al., 2023). However, most existing benchmarks focus either on a single API call in a single turn or on APIs with limited arguments. For instance, API-Bank mainly evaluate one API call per turn in multi-turn dialogues, while APIBench and ToolBench considers APIs only with one or two arguments (e.g., only"}, {"title": null, "content": "one output with one or two inputs). Furthermore, the small number of arguments makes it difficult to fully explore the complex dependency relationships between multiple APIs. For instance, the input arguments for a current API may depend on the return arguments of several previous APIs. These limitations highlight a gap in addressing complex user instructions when it is necessary to utilize multiple APIs in practice, underscoring the need for more comprehensive and practical evaluation benchmarks.\nTo bridge the gap, we introduce a new evaluation benchmark: AppBench, representing the first effort to assess the aptitude of LLMs to function as the meta planner for multiple APIs from various sources for complex user instruction. Specifically, we simulate a situation in which the user instruction can be fulfilled through collaboratively API calls from various APPs in the mobile device.  shows one typical example. Given the complex user instruction, the meta LLM, such as Apple's Siri and Google Assistant, need to plan an executable path according to user instruction and corresponding API descriptions. To fulfill this requirement, it is necessary not only to indicate which APP will distribute and execute each API but also to specify the execution order of the APIs, including all necessary inputs and returned arguments. We consider this setting aligns well with the complexity and practical limitations in the real world, and presents a great opportunity for advanced AI assistants like Apple's Siri to showcase their intelligence and capability in orchestrating collaborative API executions across multiple Apps.\nIn this way, two significant challenges are identified: graph structure and permission isolation. Firstly, the inter-dependency between multiple APIs creates a more complex execution structure. Some APIs can be executed independently, while others are dependent and must be executed sequentially, resulting in a graph-like structure. Secondly, these APIs may originate from different sources, and the LLM might not have permission to call them directly. This necessitates identifying the authorized source for each API. For instance, APIs from one company may only be executed by an LLM within the same company. In doing so, we aim to chart a path towards realizing the vision of an intelligent assistant capable of seamlessly navigating and interfacing with the myriad APPs and APIs pervasive in contemporary digital ecosystems.\nTo conclude, our contribution can be summarized in three folds:\n\u2022 To the best of our knowledge, we are the first to identify graph structure and permission isolation issues of multiple API calls when addressing complex user instructions.\n\u2022 We propose AppBench, serving as an important complementary evaluation benchmark to assess the planning capabilities of different LLMs as meta planner for these APIs. Additionally, we introduce an automatic data collection pipeline, which can be used to gather data efficiently and effectively.\n\u2022 Our experimental results on 9 distinct LLMs demonstrate almost all models, including the latest GPT-40, fall short in this setting, particularly when dealing with complex graph planning structures. Further analysis shows that simple in-context learning and fine-tuning do not significantly improve performance."}, {"title": "2 Related Work", "content": "Tool Benchmarks. The complexity of real-world tasks necessitates the integration of diverse tools and services, consisting of three types of tools (Qin et al., 2023): 1) physical interaction-based tools (Liang et al., 2023); 2) GUI-based tools (Wang et al., 2024d); and 3) program-based tools (Wang et al., 2023a; Li et al., 2023). On the one hand, some work focuses on models, retrievers, or calculators to address the intrinsic limitations of LLMs, such as ToolQA (Zhuang et al., 2023) and ToolBench (Qin et al., 2024). On the other hand, another line of work targets APIs since they are particularly crucial for bridging smooth interaction between humans and the digital realm (Li et al., 2023; Qin et al., 2024; Huang et al., 2024a). Most previous works formulate this as an API selection task given all related information about each API and current input, which overlooks the nuanced dependencies and permission constraints between different APIs, such as APIBench (Patil et al., 2023) and API-Bank (Li et al., 2023). Nevertheless, the successful execution of APIs in the real world necessitates meeting requirements fulfilled (either the value is provided by the user or previous APIs) and obtaining permission from trusted agents beyond just knowing API names and a few arguments. More details can refer to latest survey (Qu et al., 2024) and tutorial (Wang et al., 2024a)."}, {"title": null, "content": "Language Agent. Existing frameworks for language agents have made notable strides in facilitating interaction with external tools (Shen et al., 2023; Li et al., 2023; Huang et al., 2024a) and environment (Puig et al., 2018a; Wang et al., 2022). They usually follow the single-agent paradigm to access different tools or services sequentially (Lu et al., 2023; Li et al., 2023), or multi-agent framework by assigning different agents different roles to call different cognitive tools (Wang et al., 2023b) . For example, Lu et al. (2023) propose Chameleon which utilizes one agent to plan the execution order of different services by outputs a sequence of names of tools, which assume that the agents to call these tools are already known, and lots of works follow (Xu et al., 2023; Huang et al., 2024a). Furthermore, various benchmarks are proposed to evaluate the abilities of LLMs serving as agents in different situations (Li et al., 2023; Liu et al., 2023b; Ma et al., 2024). For instance, Yao et al. (2023) proposes WebShop to evaluate whether LLMs are capable of interacting with the Web. Similarly, (Puig et al., 2018b) simulates household activities through programs, and many works use this as a testbed for embodied agents (Hao et al., 2024). Latest work focus on using APIs or functions to control the whole planning processing of agents (Wang et al., 2024c)."}, {"title": "3 AppBench Construction", "content": "In this section, we start with a formal task definition and then provide a detailed explanation of how we efficiently and effectively built our AppBench by leveraging existing datasets."}, {"title": "3.1 Task Definition", "content": "Given the user instruction u and a virtual mobile environment with an APP family, $\\mathcal{E}$ = {APP1, APP2, ..., APPn} where each APP contains several APIs {p}, ..p} where i stands for ith APP and j means jth API inside this APP, the meta agent need to decide an executable path to call different APIs from various APPs to fulfill the instruction in the format of the list which each item in the list is {APPi: r1,r2,\u2026,rm = p(k1 = U1, ..., kn = Un)}. The APP; and po denote the name of the APP and corresponding API of this APP, and the ri and ki mean the ith returned and input arguments respectively. The vi can be the actual value provided by the user or a returned argument by previous APIs."}, {"title": "3.2 Data Categorization", "content": "Based on the number of APPs and APIs utilized in each user instruction, the data can be categorized into four distinct types. Each category represents a typical use case in practical scenarios, creating a comprehensive benchmark for evaluating real-world applications when combined.\n\u2022 Single APP Single API (SS) The instructions of the users only need to utilize one API from one APP.\n\u2022 Single APP Multiple API (SM) The instructions of the users need to utilize multiple API from one APP. It is important to note that these APIs can be called either sequentially or concurrently, depending on whether there is a dependency between their arguments.\n\u2022 Multiple APPs Single API (MS) The instructions of the users need to utilize multiple APIs and each of them belongs to one different APP. Also, there may exist dependency between APIs across different APPS.\n\u2022 Multiple APPs Multiple API (MM) The instructions of the users need to utilize multiple APIs and multiple APPs. The difference with MS is there may exist multiple APIs come from the"}, {"title": null, "content": "same APP. Furthermore, the dependency relationship between arguments can be the most complex when dealing with APIs from the same APP or from different APPs."}, {"title": "3.3 Data Collection", "content": "To maximize the authenticity of user instructions and minimize human efforts, we prioritize using existing task-oriented dialogue datasets (Rastogi et al., 2020; Budzianowski et al., 2018). These datasets are typically collected through human-to-human interactions in real-world scenarios and contain a wide range of APIs across numerous domains and services. Specifically, we selected the SGD (Rastogi et al., 2020) dataset as the seed dataset because it encompasses most of the domains and APIs. We then utilized LLMs and Python scripts to generate the desired inputs and outputs, respectively.\nInstruction Acquisition. Firstly, we extract the utterances of the user and system in the task-oriented dialogue and feed it into the LLM\u00b9 to summarize the user's requirements in one instruction. For example, the user may want to know the city and date of EMNLP 2024, and book a hotel according to the city and date. In the previous task-oriented dialogue, this is achieved by multi-turn interactions. In contrast, we summarize the whole dialogue into one complex user instruction to mimic more natural and complex cases in practice."}, {"title": "3.4 Data Statistic", "content": "illustrates the statistics of AppBench. Specifically, there are approximately 10 different APPs for each type and over 20 various APIs in both MS and MM. We provide the list of all APP and API in Thirdly, the average number of APIs increases from SS, SM to MS, MM, revealing the complex relationship. We also emphasize that the higher number of arguments for each API aligns with the complicated nature of tool execution in practice, as there may be multiple input and returned arguments for one API. Furthermore, we provide statistics about sequential and parallel relationships in each category (Seq. and Para.), revealing the complex graph structure in the dataset.  presents one example for each category for better understanding. More analysis can be found in the Appendix A.2."}, {"title": "4 Experiments", "content": "Models. We choose several LLMs from both open- and closed-source models, aiming to provide"}, {"title": "4.1 Setup", "content": "a comprehensive evaluation, following (Huang et al., 2024a; Zhuang et al., 2023). Specifically, we choose Mistral-7B (Mistral-7B-v0.2)(Jiang et al., 2023), the LLaMa3 series (AI@Meta, 2024) (Meta-Llama-3-8B/70B-Instruct), and the Qwen series (Bai et al., 2023) (Qwen1.5-7B/14B/72B-Chat) from open-source LLMs. Besides that, we also select GPT3.5 (gpt-3.5-turbo) and GPT4 (gpt-40) from closed-source LLMs. We also tried other models such as LLaMA2-7B or Vicuna but we find it difficult for them to output in the required format.\nImplementation Details. We set the temperature and top p as 0.1 to reduce randomness. The experiments of open-source models are run on NVIDIA A100 GPUs and those of closed-source models are fulfilled by APIs of OpenAI. To address the limitations imposed by the varying context windows of different LLMs, we adopt a hierarchical prompting approach. First, we prompt the LLMs to identify the relevant APP. Once the appropriate APP is determined, we then provide the LLMs with only the API descriptions of these specific APPs."}, {"title": "4.2 Evaluation Metrics", "content": "In order to evaluate the LLMs' capabilities of selecting proper APPs, choosing APIs, and fulfilling all arguments to execute the API based on the users'"}, {"title": null, "content": "instruction, we carefully design two F1 scores for APP and API, and one overall success rate considering the complexity of the task. We also provide the results of EM metrics in the Appendix.\nF1 of App. We first get the precision $P_{app}$ as the number of correctly predicted APPs divided by the total number of APPs predicted by the model:\n$P_{app} = \\frac{app\\_hit\\_num}{app\\_pred\\_num}$     (1)\nand recall $R_{app}$ as the number of correctly predicted APPs divided by the total number of APPs that are in the ground truth as follows.\n$R_{app} = \\frac{app\\_hit\\_num}{app\\_ground\\_truth\\_num}$    (2)\nThe F1 of App score is 2PR / (P+R), as usual.\nF1 of API. Similarly, the metrics of API predictions can be evaluated using $F1_{api}$. Note that we only consider the name of the API here to determine LLM whether or not to choose the right API, and the performance of arguments of APIs is evaluated in the next metric.\nSuccess Rate (Succ): This metric evaluates whether the LLMs can fully execute the user's instruction by correctly identifying all required APPS, APIs, and arguments. It is defined as the proportion of instances where all elements\u2014APP, API, and arguments\u2014are in perfect alignment with the ground truth, considering the complex dependency relationship between different APIs across APPs, resulting in a direct measure of model capability in full instruction fulfillment. Since there may exist different output orders, we calculate this at the structure level since the execution structure is unique."}, {"title": "4.3 Main Results", "content": "Table 3 shows the results of different LLMS for different types of user instructions on AppBench, respectively. Several conclusions can be drawn from the results 3.\nOverall, GPT-4o achieves the best overall performance, while LLaMA3-70B sometimes outperforms GPT-3.5, mostly in scenarios only involving single APP. In general, other models significantly lag behind GPT-40 in all types of instructions, and only QWen1.5-72B or LLaMA3-70B achieves better or competitive performance compared with GPT-40. Despite significant advancements in LLMs, the existing models still fall short in addressing the complexities of planning cases such as multiple APPs and multiple APIs. One fact is that all LLMs only get less than 3% Succ in MM situations.\nAs the size of the model increases, the performance can get further improved regardless of the type of instructions and the improvement becomes less significant with multiple APPs. As evidenced by LLaMA3 and QWen1.5 series models, we can find that large models mostly lead to better performance. However, when the instruction requires coordination between multiple APPs, most models show a significant drop in performance and some models even get 0 at Succ, such as QWen1.5-7B and 14B. Moreover, the $F1_{app}$ can get around 10% improvement in a single APP while only less than 5% in LLaMA3 series models.\nThe complexity of planning highly impacts the performance of these models. From the varying scores of different LLMs across different scenarios,"}, {"title": "5 Analysis", "content": "In this section, we conduct a comprehensive analysis, aiming to answer three research questions. RQ1: How do the parallel and sequential dependencies influence the model performance? (Sec 5.1) RQ2: Is it necessary to identify APP first to reduce the context window? (Sec 5.2) and RQ3: What is the major bottleneck of current LLMs (Sec 5.3), and can fine-tuning or in-context learning alleviate it? (Sec 5.4, 5.5)."}, {"title": "5.1 The Effects of Dependency Structures", "content": "We classify the dependency structures among APIs as twofold: parallel execution and sequential execution. For each data sample, we measure the parallel execution scale by the number of connected components of APIs and use the average size of these API-connected components as the sequential execution scale. The data sample with a sequential scale of 1 means no sequential dependencies among APIs. All of the APIs can be finished in a parallel way. Then, we classify the data samples of AppBench based on the above criteria and discard the categories with less than 10 samples."}, {"title": "5.2 The Effects of Different Prompting", "content": "In the main experiments, we initially required LLMs to select candidate APPs based on user input and the APP's descriptions, and then generate API calls, resulting in hierarchical prompting. Recently, many studies have expanded the context of LLMs to 200K or more (Huang et al., 2024b). Many of these works proposed LLMs with a context window that is sufficient to accommodate all the descriptions of APPs and APIs at once (flat prompting). Therefore, this section explores how the model would perform if we directly provided all apps and APIs to the model. We test GPT-3.5 and GPT-40 and compare the results in .  We can observe that flat prompting has impacted the performance of the GPT-3.5, with obvious declines in metrics such as $F1_{app}$ scores across data types. We attribute this to the introduction of a large amount of irrelevant information, which affects the model's understanding and extraction of useful APPs and APIs. Surprisingly, the GPT-40 model achieved better performance using flat"}, {"title": "5.3 Error Analysis", "content": "We further conduct error analysis at the argument level since it is directly related to different relationships between multiple APIs, to identify potential bottlenecks of the current best model: GPT-40. Specifically, there are two main categories of errors to consider: 1) key error. It occurs when the model predicts fewer keys than expected to successfully execute the API call, and it can be further divided into two types: Independent: The missing or incorrect keys are from the independent variables or arguments and Dependent: The missing or incorrect keys are from the dependent variables or arguments; and 2) value error: it occurs when the model predicts values that do not match the ground truth values, given the name of the key. Value errors can also be divided into I and D types."}, {"title": "5.4 The Effects of Fine-tuning", "content": "We additionally collected around 1,000 samples for each category from the training dataset of SGD, resulting in 4,000 samples total. We then used this mixed dataset to fine-tune the LLaMA3-8B model. Figure 6 shows the final results. Further fine-tuning on in-domain data did bring some improvement in the F1 score of the APP and API, but can not boost the performance for the Succ. Upon closer inspection, we found that the major reason for the lower performance on Succ was due to issues with recognizing or matching the keys and values in the input arguments. The model sometimes failed to recognize all the necessary input keys and values, or mistakenly used keys from other APIs. This appears to be strongly related to the complexity of the task. Factors like the dependency relationships between multiple APIs, as well as the lengthy API descriptions, made it challenging for the LMs to fully capture the necessary patterns and logic."}, {"title": "5.5 The Effects of In-context Learning", "content": "Table 5 shows the in-context performance of GPT-40 when using different shots at the demonstrations. Specifically, we randomly sample (instruction, outputs) from the same training set created during fine-tuning according to the used APP in the current instruction. For example, if the used APP in current instruction is Hotel, we sample the first 3 appeared samples with the same APP in the training set to form 3-shot demonstrations, aiming to save the space of additional API descriptions and make the agent familiar the utilization of current API. From the table, we find that in-context learning"}, {"title": null, "content": "shows some improvement in simpler cases, such as SS (\u2248 10 points increase on Succ). However, the performance does not improve further as situations become more complex and even decreases in scenarios like SM or MS, highlighting the challenges of complex planning. The worst performance in SM may be strongly related to our sampling strategy, as we only consider the APP level rather than different APIs within the same APP. More effective in-context learning for complex planning is desired and warrants further exploration and attention."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a new benchmark, AppBench, addressing the challenge of complex user instructions that require the involvement of multiple APIs. These scenarios demand advanced planning capabilities from LLMs to effectively handle graph structures and ensure permission isolation in practical applications. We left the self-evolving or more effective fine-tuning framework in our future work."}, {"title": "9 Ethical Considerations", "content": "In conducting our research, we have thoroughly reviewed and ensured compliance with ethical standards. Our study utilizes existing datasets, which have been publicly available and previously vetted for ethical use. These datasets have been carefully selected to avoid any form of offensive or biased content. Therefore, we consider that our research does not present any ethical issues. The data used is ethically sourced, the analysis is unbiased, and all procedures align with established ethical guidelines."}, {"title": "A Data Collection", "content": ""}, {"title": "A.1 Prompt Details", "content": "Your task is to generate a complex instruction in one sentence which exactly reflect what user want to do during the dialogue with the dialogue system as follows.\nPlease give all specific values of user requirements in user aware arguments {user_aware_arguments}.\nYou should not know any values of other arguments specified by the system side."}, {"title": "A.2 Data Statistics", "content": "Definition of Parallel and Sequential In this section, we delve into the execution logical structure inherent in each data sample to have a better understanding of task complexity. We conceptualize the APIs used within a single data instance as nodes within a directed graph and the dependency among them as the directed edges. Consequently, we analyze the interrelations among all APIs within the data sample and construct a corresponding graph for each of them. It is important to notice that not all APIs within a sample are interdependent, and some may operate independently. As a result, the APIs within the same data sample generally form several distinct components. We treat each component as a unit to perform topological sorting. The execution process of each unit can be parallel, while the procedure within the component is sequential.\nAs shown in , the illustrated MM sample needs to leverage 2 APIs from APP-1 and 3 APIs"}, {"title": "B Experimental Details", "content": ""}]}