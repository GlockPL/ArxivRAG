{"title": "A-VL: Adaptive Attention for Large Vision-Language Models", "authors": ["Junyang Zhang", "Mu Yuan", "Ruiguang Zhong", "Puhan Luo", "Huiyou Zhan", "Ningkang Zhang", "Chengchen Hu", "Xiangyang Li"], "abstract": "The Large Vision-Language Model (LVLM) integrates computer vision and natural language processing techniques, offering substantial application potential. However, these models demand extensive resources during inference. Adaptive attention techniques can dynamically reduce computational redundancy and thus improve efficiency. Although current adaptive attention methods significantly reduce the memory requirements of Transformer-based language models, they are not tailored for LVLMs. We observe that LVLMs generate responses from both remote image tokens and local text tokens, and different modalities have different attention patterns. This observation inspires us to manage the attention for each modality separately. Specifically, for visual input, we store the cache of potentially useful information but only compute the most critical parts. For language input, we care more about local information. Based on our observation and analysis of vision-language attention patterns, we develop A-VL, a plug-and-play adaptive attention tailored for LVLM inference. Extensive evaluations on three vision-language tasks and five datasets show the effectiveness of our designs. Our approach A-VL outperforms existing adaptive attention methods in reducing memory usage and computational load without compromising performance.", "sections": [{"title": "Introduction", "content": "In recent years, large vision-language models (LVLMs) have exhibited exceptional capabilities in vision-language understanding and reasoning (Liu et al. 2024a; Bai et al. 2023; Wang et al. 2023). LVLMs are increasingly integrated into many applications, such as personal intelligent assistants and vehicle cockpit systems. Despite their impressive performance, deploying LVLMs in real-world systems still faces practical challenges due to expensive resource overheads. Consequently, reducing computational overheads and improving inference speed have become critical problems.\nIn LVLM inference, visual and text inputs are separately encoded and fed into the pretrained large language model (LLM). As each token generated through LLM autoregression depends on all preceding tokens, this process consumes considerable time and memory resources. This issue is exacerbated in LVLMs since high-resolution images lead to rapidly expanding token sequences. Recent studies explored adaptive attention and KV cache compression techniques for single-modal language models, achieving significant advancements.\nAdaptive attention methods must address two critical aspects: identifying redundancy and maintaining performance. To develop an effective adaptive attention method for LVLMs, we must first examine the specific characteristics of attention in LVLMs. Subsequently, we can design a strategy based on these characteristics to reduce memory usage and computational demands while maintaining performance. In this paper, we reveal that different modalities of LVLMs exhibit different attention patterns, necessitating separate analysis and processing for each modality within LVLM's adaptive attention mechanism. Vision inputs, though positioned at the beginning of the sequence, consistently receive a substantial proportion of attention during generation. In contrast, text inputs at the sequence's end exhibit rapid attention decay. Vision inputs also show patterns of attention sparsity and drift. Specially, the attention at the token level for vision inputs presents a surprisingly sparse distribution. Additionally, we find that some useful vision caches are not required at every step. We observe a gradual shift in attention among these useful vision caches.\nTo address this issue, we develop distinct methods for different modalities. In response to the sparsity and attention drift observed in vision attention, we categorize attention importance and dynamically compute the most critical parts of the KV cache. Specifically, we first preserve caches likely to be useful in subsequent inference and evict those deemed useless. We then select the most critical caches in the current step from these useful caches for computation. Simultaneously, we regularly update this critical cache to ensure accurate computations. For text attention, which rapidly loses attention, we concentrate on the local cache and retain only the most essential remote text caches. Notably, our approach does not require fine-tuning the model and offers plug-and-play compatibility with LVLMs."}, {"title": "Background and Related Work", "content": "Large Vision-Language Modal.\nLarge vision-language models (LVLMs) integrate the capabilities of visual encoders and large language models to understand multimodal information and generate textual responses related to images. LVLMs such as LLaVA (Liu et al. 2024b), Qwen-VL (Bai et al. 2023) and CogVLM (Wang et al. 2023) have demonstrated impressive performance across various vision-language tasks. The LVLM typically consists of a visual encoder, an adapter, and a pre-trained LLM. The visual encoder processes and converts vision inputs into hidden features. Then the adapter transforms features into tokens for LLM. Text inputs are also converted into tokens through the embedding model. The LLM generates responses based on the combined visual and textual tokens. The inference of an LLM is divided into two phases: the prefill phase and the decode phase. During the prefill phase, all input tokens are processed and KV caches are simultaneously generated. The KV caches store the intermediate results of the processed tokens. In the decode phase, the model computes using the newly generated token and the KV cache from all previous tokens. As the sequence of tokens extends, the size of the KV cache grows linearly, directly affecting the memory usage and computational demands during inference. However, the vision tokens from higher-resolution images output by the visual encoder significantly increases the length of token sequences. For instance, LLaVA 1.5 processes 576 image tokens, while LLaVA 1.6 increases this number to over 2000. This increase presents challenges to the deployment and utilization of LVLMs.\nInference Optimization for LLM and LVLM.\nThe decoder-only transformer-based LLM necessitates autoregressive iteration for output generation, where the generation of each token depends on all preceding tokens. This dependency significantly increases memory requirements and computational overhead. To address these challenges, existing methods are categorized into two main approaches: inference process optimization and computation reduction. Inference process optimization includes strategies such as page attention (Kwon et al. 2023) and speculative sampling (Xia et al. 2024) to achieve flexible and efficiency management of the entire inference process. Computation reduction involves methods like adaptive attention, early exit (Bae et al. 2023), and structured pruning (An et al. 2024). Our primary focus is on adaptive attention technology, which dynamically reduces computational redundancy according to inputs and has produced remarkable results in LLMs. StreamingLLM (Xiao et al. 2023) is designed to retain a fixed-position cache, which can train LLMs with long contexts on a limited cache window. Anagnostidis et al. (Anagnostidis et al. 2024) propose a new sigmoid function to dynamically sparse attention, but the model needs to be fine-tuned. FastGen (Ge et al. 2023) combines multiple strategies to retain recent caches, special characters, and caches that have been historically assigned heavier attention. H2O (Zhang et al. 2024b) leverages historical accumulated attention and combined with the local cache for KV cache reduction, and has achieved remarkable results. Although these methods improve the inference efficiency of LLM, they are designed for single-modal text inputs. In addition, there are some studies focus on enhancing the efficiency of transformer-based visual models. SPViT (Kong et al. 2022) prunes tokens in Vision Transformers (ViT), and PuMer (Cao, Paranjape, and Hajishirzi 2023) prunes and merge tokens in cross attention; however, these methods are designed for encoder models. FastV (Chen et al. 2024) is an optimization solution designed for autoregressive LVLMs, revealing that without using KV cache, LVLM can potentially use only 50% of tokens after the second layer. Nevertheless, KV cache is an essential optimization tool for autoregressive inference. FastV has not been verified in inference with KV cache, because token pruning after the second layer may lead to the loss of some kv cache, making it impossible to select tokens in subsequent steps. And the pruning in FastV with KV cache may lead to the loss of potentially important tokens in the subsequent steps."}, {"title": "Observations and Insights", "content": "In this section, we employ experiments to uncover key insights into the inference of large vision-language models, facilitating optimization of these processes."}, {"title": "Attention of Different Modalities", "content": "The large language model in LVLMs processes three types of input tokens: system input, image tokens and user instruction, as shown in Figure 2. System input typically comprises control information, such as role settings and task modes. Image tokens represent vision information encoded by the visual encoder. User instruction is the query posed by users about the image. Attention in LLM quantifies the relevance between two tokens by assigning weights. The attention directed toward image tokens in user queries implies the importance of these image tokens to the current question (Cao, Paranjape, and Hajishirzi 2023), which we call text-aware vision attention."}, {"title": "Heterogeneity of Vision Attention", "content": "We subsequently explore how vision attention changes across different decoder layers. For an intuitive visualization, we visualize the attention scores across various layers for image tokens during the inference with the LLaVA-1.5 7B model, as illustrated in Figure 3. The visualization shows that most image tokens are allocated little attention after first decoder and the vision attention is highly sparse. To quantitatively illustrate the ubiquity of this sparsity, we use the LLaVA-1.5 7B model to evaluate the OCRVQA dataset and find that 22.88% of the tokens receive over 80% of the total attention in all layers. This indicates that image information is highly concentrated in a few tokens, resulting in inefficient inference. It is similar to the Pareto principle. Based on the observation that a small number of image tokens are allocated a large amount of attention, we define a metric termed the p-percentile concordance index (PPCI). This metric compares the correlation between two attention weights on the tokens, specifically focusing on key tokens. The PPCI measures the proportion of the top p% of tokens that are identical in two different attention weights. Specifically, for two sequences of attention scores, $S_1$ and $S_2$, on the N tokens, the sets $T_1$ and $T_2$ represent the sets of top p% tokens in $S_1$ and $S_2$ respectively, sorted by their attention scores. The p-percentile concordance index (PPCI) is defined as follows:\n$p\\% PPCI = \\frac{| T_1 \\cap T_2 |}{p\\%. N}$"}, {"title": "Continuity of Vision Attention", "content": "We subsequently analyze the attention from the temporal dimension with the LLaVA-1.5 7B model and the OCRVQA dataset. During the autoregressive generation of tokens, we observe a high correlation in attention within the same decoder layer across different steps, as depicted in Figure 5. The figure illustrates that during the generation of 20 new tokens (20 steps) in the same decoder layer, the p% PPCI in image tokens between each step and first step. As depicted in the figure, each layer consistently allocates high attention to the top 50% of image tokens. For the top 30% image tokens, each layer allocates high attention in the short term, although this may shift after three steps. We refer to this short-term correlation of attention among image tokens within the same layer as the continuity of vision attention. This indicates that attention may gradually drift within partial image tokens. Given the gradual nature of these changes, we can reasonably assume that the set of core tokens remains stable in the short term.\nBased on the heterogeneity and continuity of vision attention, we can select important image tokens for each layer according to the attention scores, thereby reducing computational load. In fact, we retain only the top 30% of image tokens for each decoder layer using the LLaVA-1.5 7B model and OCRVQA dataset to ensure that 99% of the samples generate three tokens identical to the original model output. However, for longer outputs, because attention drifts, relying on only 30% image tokens is not enough to predict. Thus, we can preserve potentially useful image tokens for future use, but compute only the most critical token at each step. This specific design will be elaborated in the following section."}, {"title": "Locality of Text Attention", "content": "As illustrated in Figure 1, attention of text tokens decays rapidly. Therefore, it is advisable to focus on local text attention and a limited number of critical remote text attention. The H2O method can be precisely applied to achieve this objective. Specially, we maintain local text cache caches and evict useless text caches based on accumulated historical attention. We also analyze the attention patterns of Llama 2 (Touvron et al. 2023), a large language model. We observe that the attention patterns in Llama 2 are similar to those in the text token of LVLMs, both characterized by rapid attention decay. This similarity further underscores the suitability of employing an LLM's adaptive attention method to the text tokens of LVLMs."}, {"title": "A-VL Design", "content": "Based on the above insights, we propose A-VL, a plug-and-play adaptive attention method tailored for LVLMs."}, {"title": "Adaptive Text-Aware Vision Attention", "content": "Decode phase. We design a hierarchical adaptive attention mechanism for vision attention based on the attention scores from the perspective of text tokens, as shown in Figure 6. In the prefill phase, we categorize image tokens into three sets: core, secondary, and minor, according to the attention score of the last text token. This classification uses the attention score from the text token to emphasize the semantic relationship between text and images. Specifically, after computing the attention score in each decoder layer, tokens within the top C% of scores are designated as core tokens, those within the top S% as secondary tokens, and the remainder as minor tokens. We ensure that S > C, thereby making core tokens a subset of secondary tokens. Minor tokens are immediately evicted to minimize KV cache usage. Secondary tokens, which may be useful in future, are retained in memory; however, they are not all used in computations. Only core tokens are involved in the computations of subsequent attention modules. However, our observations suggest that vision attention may shift gradually among secondary tokens, potentially altering the set of core tokens. Thus, every K steps, we utilize all secondary tokens for inference and compute the attention scores to update the core token set, ensuring that attention remains focused on the most significant tokens. We refer to this step as the update decode phase. This design is based on our insights into the continuity of vision attention, allowing the model to minimize cache usage and further reduce computational load of attention. It should be noted that our method is applied independently to each decoder layer, and the results are not utilized until running to the corresponding layer in the next step, allowing for parallel processing with the original model's inference.\nPrefill phase. The related work, FastV, has shown that inference without using KV cache may result in the deletion of half image tokens based on attention scores at each step. However, employing KV cache with this method leads to performance degradation, as potentially useful tokens are evicted, causing cache misses. Our experiments illustrate this phenomenon in evaluation section. Inspired by this study, we propose simplifying this technology for only use in the prefill phase. Specifically, we only retain P% image tokens based on attention score after the second decoder layer in the prefill phase. And then we use only the kv cache of these retained image tokens for inference in the decode phase. We can reduce the parameter P to prevent the performance from decreasing. This technology reduces computational load in the prefill stage and has been experimentally verified to be compatible with the above adaptive vision attention technology."}, {"title": "Adaptive Text Attention", "content": "Based on the insight of text attention, when generating new text tokens, both remote image tokens and the local text tokens are used to predict. Since image tokens are processed separately, we use H2O's accumulated attention method for text cache eviction. Specially, we set a text cache window size of T% of the maximum text token length. After computing attention weights at each layer, we separate the attention of vision and text, and store the accumulated text attention scores. If the text cache size exceed the set text cache window size, we evict the redundant caches from the first half of the window based on these scores. This method, utilized by H2O, has proven effective in LLM. We incorporate this approach into the text attention of LVLMs and verify its effectiveness through experiments. We list all the parameters involved in our method and their description in Table 1."}, {"title": "Implementation of A-VL", "content": "Our method is plug-and-play, requiring no fine-tuning of the original model. Cache selection and eviction can be processed in parallel with the original model inference, and the results are not utilized until the next step. Besides, in our adaptive vision attention method, core caches are selected from secondary caches for computation, as depicted in Figure 7. Slicing the cache before matrix multiplication introduces a performance bottleneck, because the core caches are not stored contiguously in memory (Liu et al. 2023b). Sometimes increasing latency of slicing beyond original matrix multiplication. To address this, we develop a specialized CUDA operator that allows direct multiplication with selected rows or columns of the second matrix, eliminating the need for slicing. This CUDA operator employs block processing and shared memory to enhance speed further. We measure the latency during the process in Figure 7 with PyTorch on an NVIDIA A40 GPU under the configure of LLaVA-1.6 7B. Latency variations across different batch sizes are illustrated in Figure 8. It is evident that slicing is slow and negates the benefits of reduced computation. Our CUDA operator demonstrates effective acceleration."}, {"title": "Evaluation", "content": "In this section, we demonstrate the effectiveness of our method through experiments.\nEvaluation Setup\nWe implement our evaluation system based on the multimodal evaluation suite, LMMs-Eval (Zhang et al. 2024a).\nEvaluation Tasks We employ multiple vision-language tasks to evaluate our method. Since our method primarily optimizes the KV cache in the decode phase, we do not choice any multiple-choice datasets that generate only one token during the prefill phase. The tasks and datasets used in our evaluation are as follows:\n\u2022 Image Caption. This task involves automatically generating textual descriptions for visual content. We employ two datasets, Nocaps (Agrawal et al. 2019) and Flickr30k (Plummer et al. 2015). The metric is the CIDEr score (Vedantam, Lawrence Zitnick, and Parikh 2015).\n\u2022 Visual Question Answering (VQA). This task requires models to answer questions based on visual information from images. We use two datasets, DocVQA (Mathew, Karatzas, and Jawahar 2021) and TextVQA (Singh et al. 2019), measuring performance with the ANLS (Biten et al. 2019) and accuracy metrics respectively.\n\u2022 Optical Character Recognition (OCR). OCR involves recognizing text content within images. We select the OCRBench dataset (Liu et al. 2023a), which is specifically designed for LVLMs. The metric is the score designed in OCRBench.\nModels. We use mainstream LVLMs and their different sizes, including LLaVA 1.5 7B, LLaVA 1.5 13B, LLaVA 1.6 7B and LLaVA 1.6 13B.\nBaselines. We compare the output of the original model, FastV designed for LVLMs, and H2O designed for LLMs. For the H2O method, set the cache window to 75% recent and 25% heavy hitter. For FastV, in order to compare performance with KV cache, we delete redundant tokens according to the FastV method in the prefill phase, and use the generated KV cache for inference in the decode phase.\nPerformance Guarantee\nWe evaluate the performance of various models across different tasks, as shown in Table 2. FastV, H2O, and our method retains 50% of the KV cache. Our method achieves nearly lossless performance using only 50% of the KV cache. And our average performance is better than the other two baselines. FastV shows reduced performance when utilizing the KV cache, due to the loss of potentially useful tokens during token pruning. The H2O method designed for LLMs shows effectiveness, but its average performance is still inferior to our method. Our method demonstrates good results across various models, confirming its effectiveness."}, {"title": "Decoder Inference Latency", "content": "Our method utilizes fixed eviction ratios for each layer, facilitating batch inference. We measure the inference latency of the Transformer Decoder in LLaVA-1.6 7B under various batch sizes on NVIDIA A40. By compressing the KV cache to 50%, we achieve a 1.8x increase in decoder inference. Furthermore, by utilizing our CUDA operator to only calculate the core cache, we can further enhance the speed by an additional 1.1x. Consequently, the total decoder latency required is only 50.5% of the original. It is evident that our method can substantially reduce inference latency. While our custom CUDA operator does increase speed, the improvement is less pronounced than in isolated tests due to the influence of other operations in PyTorch on its performance. We plan to further improve our CUDA operator in future work."}, {"title": "Conclusion", "content": "In this paper, we propose A-VL, a plug-and-play adaptive attention method tailored specifically for large vision-language models. Our analysis of attention patterns across different modalities indicates that remote vision tokens consistently receive high attention, while textual tokens exhibit rapid attention decay. Moreover, vision attention patterns are characterized by sparsity and drift, enabling dynamic selection of the most critical cache for computation. Consequently, we design the adaptive attention method tailored to each modality separately, which reduce KV cache stored and concentrates computational load on the most critical cache. Our experiments confirm the efficacy of A-VL, demonstrating substantial reductions in memory usage and computation load without compromising performance."}]}