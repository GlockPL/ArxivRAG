{"title": "UNRAVELING AND MITIGATING Safety ALIGNMENT DEGRADATION OF VISION-LANGUAGE MODELS", "authors": ["Qin Liu", "Chao Shang", "Ling Liu", "Nikolaos Pappas", "Jie Ma", "Neha Anna John", "Srikanth Doss", "Llu\u00eds M\u00e0rquez", "Miguel Ballesteros", "Yassine Benajiba"], "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as \u201csafety alignment degradation\u201d in this paper, and show that the challenge arises from the representation gap that emerges when introducing vision modality to VLMs. In particular, we show that the representations of multi-modal inputs shift away from that of text-only inputs which represent the distribution that the LLM backbone is optimized for. At the same time, the safety alignment capabilities, initially developed within the textual embedding space, do not successfully transfer to this new multi-modal representation space. To reduce safety alignment degradation, we introduce Cross-Modality Representation Manipulation (CMRM), an inference time representation intervention method for recovering the safety alignment ability that is inherent in the LLM backbone of VLMs, while simultaneously preserving the functional capabilities of VLMs. The empirical results show that our framework significantly recovers the alignment ability that is inherited from the LLM backbone with minimal impact on the fluency and linguistic capabilities of pre-trained VLMs even without additional training. Specifically, the unsafe rate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as 3.15% with only inference-time intervention.", "sections": [{"title": "INTRODUCTION", "content": "The development of Vision Language Models (VLMs) has marked a significant advancement, enabling models to process information from both visual and textual modalities and have shown promising capabilities across various applications (Liu et al., 2024b; Zhu et al., 2024). However, the integration of the vision module (as is widely adopted as the default architecture of VLMs (Liu et al., 2024b;a; Dai et al., 2023; Chen et al., 2023a)) degrades the overall alignment ability of a VLM compared to its LLM backbone, and we refer to this phenomenon as safety alignment degradation. For instance, LLaVA, built on the Vicuna-13b LLM, demonstrated a decline in MT-Bench (Zheng et al., 2023) performance from 6.57 to 5.92 (scored by GPT-4), even faring worse than the smaller Vicuna-7B model (Li et al., 2024c). The alignment degradation is even more crucial when it comes to safety-related queries. For example, even the incorporation of a blank image, which may not carry any semantics in most contexts, can break the safety alignment and trigger harmful responses from the VLM (Gou et al., 2024; Li et al., 2024d).\nSeveral existing works have explored the phenomenon of safety alignment degradation. For example, Gou et al. (2024) attempt to transform unsafe images into texts to activate the intrinsic safety mechanism of pre-aligned LLMs in VLMs. However, images often contain fine-grained information that could not be fully captured by texts. On the other hand, Li et al. (2024d) leverage the safety risks posed by the visual modality and propose a jailbreak method that conceals and amplifies the malicious intent within text inputs using carefully crafted images. However, the underlying mechanisms"}, {"title": "HOW VISION MODALITY AFFECTS MODEL BEHAVIOR?", "content": "We first analyze the alignment degradation challenge by investigating the following question: how does the integration of the vision modality intrinsically affect model behavior? We hypothesize that since the vision and language modules within a VLM are trained independently, the resulting representations tend to cluster in distinct regions of the latent space, leading to a distribution shift that reduces alignment ability when processing multi-modal inputs, as it deviates from the representation space that the LLM backbone is optimized for. To verify this hypothesis, we investigate how representations of different types of inputs exist in model's representation space, and how the distinction correlates with the alignment degradation of VLMs."}, {"title": "EXPERIMENTAL SETUP", "content": "To investigate the influence of vision modality on the safety align-ment of a VLM, we employ 5 variations on the model input: (1) Original Input, where the images and textual queries remain unchanged as model input; (2) Blank Image, where the original image is substituted with a blank image that does not carry any semantic meaning; (3) Gaussian Noise, where we perturb the original images with Gaussian noise in an attempt to destroy their semantic meaning; (4) Text + Caption, where the image is substituted by its caption; and (5) Text Query Only, where only the textual queries are used as input, and the images are discarded. The first three input variations are multi-modal input, investigating the alignment ability of VLMs under various levels of image toxicity. The last two variations are pure text inputs, evaluating the backbone LLMs of VLMs on harmful instructions.\nWe use two safety-related multi-modality datasets for model behavior analysis: VLSafe (Chen et al., 2024b) and JailbreakLLMs (Shen et al., 2023) paired with images from the COCO dataset (Lin et al., 2014). VLSafe (Vision-Language Safety) dataset contains 1,110 pairs of malicious queries and benign images where the input images are auxiliary and the queries can be answered without referencing the images. JailbreakLLMs dataset includes 390 jailbreak prompts that are collected and filtered from Reddit, Discord, websites, and open-source datasets covering 14 topics, including illegal activity, malware, physical harm, etc. We filter out two safety-irrelevant topics and pair each remaining jailbreak prompt with a related image retrieved from the COCO dataset to construct a multi-modal dataset composed of 330 test samples.\nWe analyze three VLMs of different scales: LLaVA-1.5-7B, LLaVA-1.5-13B (Liu et al., 2024b), and ShareGPT4V (Chen et al., 2023a) with Vicuna (Chiang et al., 2023) as their LLM backbone. To investigate the safety alignment ability of VLMs and their"}, {"title": "METHODS", "content": "Building on insights from \u00a72, we attempt to formalize the affected hidden states of current VLMs with vision input incorporated (\u00a73.1), based on which we propose two variations of CMRM to intervene model representations during inference time to prevent the alignment degradation (\u00a73.2)."}, {"title": "FORMALIZATION OF VLM REPRESENTATION: SHIFTED FROM OPTIMAL DISTRIBUTION", "content": "Inspired by Favero et al. (2024), we propose to formalize the hidden states of multi-modal input to VLMs as being shifted from the ideal representation that remains within the distribution of the LLM backbone while capturing extra visual information from the incorporated image in the input. Under the shifting assumption, we model the representations of a vanilla VLM h(x, img) as an interpolation between two scenarios: (1) the VLM with only text query as input, without any visual information or the impact of vision modality; (2) an ideal VLM that benefits from visual information based on the image input while not being affected by the visual modality. Accordingly, we propose the following formalization:\n$h(x, img) = h^{*}(x, img) + \\alpha[h(x, img') \u2013 h(x)],$\nwhere img and x stand for the image input and textual instruction respectively, and img' is a meaningless image that does not carry any visual information. $\\alpha \\in [0,1]$ is a mixing coefficient that indicates the level of representation shift. When \u03b1 is small, the shifting effect is mild and the representation is not pulled much from the backbone LLM's representation distribution. For higher \u03b1, the VLM suffers from severe representation shifting.\nGiven the modality affected original representation $h_{o} \\equiv h(x, img)$, the representation of corrupted image as input $h_{c} \\equiv h(x, img')$, and the one of pure text input $h_{t} \\equiv h(x)$, our goal is to find an estimate of the ideal representation distribution h* that is not affected by vision modality and"}, {"title": "CMRM: RECOVER ALIGNMENT ABILITY AT INFERENCE TIME", "content": "Based on the formalization of VLMs under the affect of representation shifting (Eq. 1), we further propose the hypothesis that alignment degradation can be mitigated by eliminating representation shift when an image is incorporated as input, based on which we introduce CMRM (Cross-Modality Representation Manipulation) to calibrate the shifted representation of multi-modal inputs according to Eq. 3. Its core idea is to pull multi-modality representations back or closer to the distribution that the LLM backbone is optimized for, where the safety alignment capability was initially developed and fine-tuned for processing purely textual inputs.\nCMRM first extracts the shifting direction caused by the incorporation of visual input, which is, according to our assumption, correlated with the alignment degradation phenomenon. As defined in the second term of Eq. 3, these shifting vectors are obtained by contrasting the representations of two types of variations on the input: text query only ht and text query with corrupted image hc. To systematically analyze the shifting direction caused by visual input incorporation, we propose two variations for shifting vector extraction: dataset-level extraction and sample-level extraction. Each method offers unique insights into how the model's internal representations are affected by the introduction of corrupted visual data.\nThe dataset-level extraction. aims to capture the overall trend of shifting directions across the entire dataset. This approach is particularly useful for understanding the general impact of visual corruption on the model's safety alignment performance. Mathematically, the dataset-level shifting vector for layer l is computed by performing a down-projection using PCA on the differences between the representations of all samples in the target dataset, with and without corrupted visual input. The formal definition is given by:\n$V_{data} = PCA(\\frac{\\sum_{i=1}^{N} (h_t(i) - h_c(i))}{N})_{first component},$\nwhere N represents the total number of samples in the dataset, $h^{l}(i)$ denotes the representation of the last token for the i-th input in the l-th layer. $V_{data}$ represents the principal direction of the shift in the model's hidden states due to the introduction of corrupted visual input, as captured across the entire dataset. By performing PCA on the collection of these difference vectors across all N samples, the first principal component, which is the direction in space along which the data points have the highest or most variance, captures the most significant direction of variation in these shifts, indicating the predominant trend in which the model's internal representations are influenced by the visual input.\nAs an alternative, the sample-level extraction focuses on capturing the shifting direction at the granularity of individual samples. This approach is crucial for identifying specific instances where the alignment degradation is particularly pronounced or where the visual corruption has an unexpectedly minimal or even beneficial effect. For each individual sample i, the shifting vector for layer l is calculated as:\n$V_{sample}^{l(i)} = h_t^{l(i)} - h_c^{l(i)},$\nwhich investigates and captures the nuances of alignment degradation on a case-by-case basis."}, {"title": "EXPERIMENTS AND EVALUATION", "content": "In this section, we empirically evaluate the proposed CMRM. First, we introduce the experimental settings in \u00a74.1. Then, we assess CMRM from the following perspectives: (i) How well can CMRM improve the safety of VLMs and recover the alignment ability of the LLM backbone? (\u00a74.2) (ii) Does CMRM harm general performance of VLMs? (\u00a74.2) (iii) How does the hyperparameters affect CMRM? (\u00a74.3) (iv) Does the extracted shifting direction based on one anchor dataset generalize to other datasets? (\u00a74.4) (v) What is the impact of CMRM on VLMs' hidden states? (\u00a74.5)"}, {"title": "MAIN RESULTS", "content": "As shown in Tab. 1, both dataset- and sample-level CMRM greatly boost the safety of evaluated models, decreasing the unsafe rate from 61.53% to as low as 5.41% and 3.15% for LLaVA 7B on VLSafe dataset. It is noteworthy that the performance of CMRM approximates the unsafe rate of pure text inputs, demonstrating its effectiveness in mitigating the safety risks introduced by multi-modal inputs and easing the alignment degradation phenomenon to recover the alignment ability of the LLM backbone. Further, the application of CMRM does not cause much decrease in terms of model utility with reasonable computational overhead (Appx. \u00a7A.1), and we can even spot an increase in some cases. Intuitively, CMRMsample consistently outperforms CMRM dataset since it comes with fine-grained shifting vector extraction, although at the cost of significantly higher computation consumption. Compared to the training-time baseline method VLGuard, which is supposed to outperform CMRM for deliberately curated safety dataset and additional computational cost, our proposed method surprisingly achieves a lower unsafe rate under several scenarios. For example, CMRM sample for LLaVA 13B on VLSafe dataset exceeds VLGuard Mixed in Orig. setting. This indicates the potential of improving the safety alignment of VLMs by recovering the ability that is inherent in the LLM backbone, which could save much effort of tedious fine-tuning."}, {"title": "SENSITIVITY TO ALPHA VALUE AND MANIPULATED LAYERS", "content": "As shown in Fig. 2, the unsafe rate of models w.r.t. different alpha values reveals that the alpha value of 1.0 consistently performs well across different models and tasks for dataset-level CMRM. The unsafe rate tends to decrease as the alpha value approaches 1.0, with a significant reduction in unsafe outputs. For instance, VLsafe with ShareGPT shows a sharp decline in unsafe rate as the alpha value increases from 0.7 to 0.9, stabilizing around 2% at alpha 1.0. It is important to note that although higher alpha values (e.g., above 1.0) may further reduce unsafe rates in some cases, they tend to compromise the model's utility (refer to Tab. 5 for case study). Excessively high alpha values can overly suppress model expressiveness, leading to a drop in overall performance, as models become too conservative and fail to generate useful or informative outputs. Therefore, an alpha value of 1.0 strikes a balance between minimizing unsafe content and maintaining model utility for the involved settings. However, the optimal alpha value may vary for other models and datasets, which requires a case-by-case investigation.\nTab. 2 indicates that manipulating all layers is essential to achieve the best results. For instance, manipulating 32 encoder layers in both LLaVA and ShareGPT achieves the lowest unsafe rates of 4.32% and 1.91%, respectively. As the number of manipulated layers decreases, the unsafe rate increases significantly, as evidenced by the unsafe rate jumping to 16.94% for LLaVA. This suggests that full manipulation of all layers is necessary for optimal model safety, particularly in models like LLaVA, where incomplete layer manipulation can drastically compromise performance."}, {"title": "TRANSFERABILITY OF EXTRACTED SHIFTING DIRECTION AND ANCHOR DATASET", "content": "We investigate the transferability of the anchor dataset to demonstrate the generalization capability of our proposed method. Specifically, we aim to evaluate whether using VLsafe as the anchor dataset can improve model safety not only on the VLsafe dataset itself but also when applied to other datasets, such as JailbreakLLMs. To achieve this, we use the entire VLsafe dataset as the anchor dataset and fix the alpha value at 1.0, ensuring consistency across all tests without hyperparameter tuning. As shown in Tab. 3, when VLsafe is used as the anchor dataset, we observe notable improvements in safety performance compared to the results shown in Tab. 1. Furthermore, models also perform well in JailbreakLLMs dataset, demonstrating the transferability of the extracted shifting direction based on the anchor dataset. In summary, using VLsafe as the anchor dataset not only improves safety performance on the original VLsafe data but also generalizes effectively, reducing unsafe rates on entirely different datasets such as JailbreakLLMs."}, {"title": "IMPACT OF CMRM ON HIDDEN STATES", "content": "In Fig. 3, we visualize the hidden states of the model under various input settings and intervention coefficients. The plot illustrates that our proposed CMRM successfully shifts the hidden states of multi-modal inputs closer to those generated from pure text inputs. This alignment is crucial for reducing the unsafe behaviors typically observed when models handle multi-modality data. Notably, as demonstrated by the hidden states under different intervention coefficients (shown by different colored triangles), there is a risk when the intervention coefficient is set too high. For instance, at an intervention coefficient of 2.0, the hidden states are pulled too far from the original hidden state distribution, deviating significantly from both the cluster center and the model's typical distribution under normal conditions. This excessive shift causes the hidden states to move outside the distribution of the VLM's language model backbone, leading to malfunction and degraded performance (Tab. 5). In contrast, when the intervention coefficient is set to 1.0, the hidden states remain much closer to the pure text inputs, reflecting an optimal balance between reducing unsafe behaviors and maintaining the model's general utility. The results suggest that moderate intervention levels can effectively align hidden states without compromising the model's ability to function properly."}, {"title": "RELATED WORK", "content": "Alignment refers to the process of fine-tuning pre-trained models with annotations based on human preferences, to ensure that the generated responses of the models are Helpful, Honest, and Harmless, i.e., the 3H principle (Askell et al., 2021). This practice first thrives for LLMs where RLHF (Reinforcement Learning from Human Feedback) (Christiano et al., 2017; Ouyang et al., 2022) has proven to be an effective approach for aligning LLMs with human values. DPO (Direct Preference Optimization) (Rafailov et al., 2023) further enhances the efficacy and efficiency of RLHF by directly optimizing LLMs based on human preferences so that the constrained reward maximization problem can be optimized exactly with a single stage of policy training. In multi-modal scenarios, efforts have been made to adapt these alignment methods to VLMs either on creating multi-modal preference data (Li et al., 2023; Zhao et al., 2023; Zhou et al., 2024; Deng et al., 2024; Pi et al., 2024b; Helff et al., 2024) or specifically designed learning objectives (Wang et al., 2024a; Li et al., 2023; Zhao et al., 2023; Zhou et al., 2024; Yu et al., 2024; Liu et al., 2024c).\nSpecifically, to enhance safety alignment for VLMs, a straightforward approach involves aligning VLMs with specially-constructed red-teaming data (Chen et al., 2024a; Li et al., 2024b; Zong et al., 2024). However, red-teaming is labor-intensive and may not encompass all potential failure cases. At the same time, these attempts overlook the fact that the LLM backbone within VLMs may already have undergone substantial safety alignment. Consequently, retraining the entire VLM with additional red-teaming data can be an inefficient use of computational resources. Another approach focuses on protecting VLMs during inference time (Wang et al., 2024b; Chen et al., 2023b; Pi et al., 2024a; Gou et al., 2024), among which the most relevant to ours is the work by Wang et al. (2024b). They employ safety steering vectors to adjust VLM activation in response to unsafe inputs. However, this may overlook unsafe intents in images that are not detectable by text-centric safety vectors. Instead, our proposed CMRM inspects the shifting of internal representation caused by multi-modal input and calibrates the representations to be closer to the distribution of the LLM backbone to activate its intrinsic alignment ability."}, {"title": "REPRESENTATION ENGINEERING", "content": "Representation engineering is a set of alignment techniques that work by making targeted perturbations to a model's hidden states (Subramani et al., 2022; Hernandez et al., 2023; Turner et al., 2023). Li et al. (2024a) propose inference-time intervention (ITI), to shift representations along the directions identified by linear probes within those truthfulness-related attention heads to elicit more truthful outputs. Zou et al. (2023) develop RepE to identify and extract representations corresponding to high-level concepts such as honesty and safety in LLMs. They use a \"reading vector\" generated from the activations on datasets related to the specific concepts to steer model behavior. Directed Representation Optimization (DRO) Zheng et al. (2024) treats safety prompts as trainable embeddings and learns to move the queries' representations along or opposite the refusal direction, depending on their harmfulness. Similarly, InferAligner Wang et al. (2024b) extracts safety-related vectors from safety-aligned LLMs to indicate the direction of safe input and intervene on inputs with harmful intents for safe responses. These works focus on identifying differences in representations of LLMs based on specific features, such as safety or truthfulness. In contrast, we propose that VLMs exhibit distinct representations depending on the input type (pure textual or multi-modal), and this shifting in representation can lead to a decline in alignment ability. Our proposed CMRM attempts to recover the inherent alignment ability of the LLM module for VLMs by reversing the representation shift."}, {"title": "CONCLUSION, LIMITATION, AND FUTURE WORK", "content": "We investigate the impact of incorporating visual input on VLMs. We find that multi-modal input drastically degrades the safety alignment mechanism of the LLM backbone in VMLs, while intrinsically shifting the hidden states away from the distribution that the LLM module is optimized for. Drawing this inspiration, our proposed CMRM method intervenes the representations of VLMs upon multi-modal inputs by moving hidden states closer to the trained distribution of its LLM"}, {"title": "COMPUTATIONAL OVERHEAD", "content": "As is shown in Tab. 4, our method causes a minimal increase in computational overhead during model inference, even for sample-level CMRM."}, {"title": "CASE STUDY", "content": "In Tab. 5, we showcase models' responses with the input prompt asking How to use skis to cause harm to others while riding down a snow-covered slope? and an image of a man in red riding skis down a snow covered slope."}]}