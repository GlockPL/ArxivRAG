{"title": "FEDERATED DOMAIN GENERALIZATION WITH LABEL SMOOTHING AND BALANCED\nDECENTRALIZED TRAINING", "authors": ["Milad Soltany", "Farhad Pourpanah", "Mahdiyar Molahasani", "Michael Greenspan", "Ali Etemad"], "abstract": "In this paper, we propose a novel approach, Federated Do-\nmain Generalization with Label Smoothing and Balanced\nDecentralized Training (FedSB), to address the challenges\nof data heterogeneity within a federated learning framework.\nFedSB utilizes label smoothing at the client level to prevent\noverfitting to domain-specific features, thereby enhancing\ngeneralization capabilities across diverse domains when ag-\ngregating local models into a global model. Additionally,\nFedSB incorporates a decentralized budgeting mechanism\nwhich balances training among clients, which is shown to\nimprove the performance of the aggregated global model.\nExtensive experiments on four commonly used multi-domain\ndatasets, PACS, VLCS, OfficeHome, and TerraInc, demon-\nstrate that FedSB outperforms competing methods, achieving\nstate-of-the-art results on three out of four datasets, indicating\nthe effectiveness of FedSB in addressing data heterogeneity.", "sections": [{"title": "1. INTRODUCTION", "content": "Federated learning (FL) [1] has recently gained interest in ma-\nchine learning (ML) as an alternative to centralized training\nin scenarios where there are either data privacy concerns or\nresource-sharing limitations. Unlike centralized ML meth-\nods, which requires data from multiple sources to be accessed\nin one global model [2, 3], FL allows for collaborative train-\ning of local models without sharing data or resources among\nthem, thus preserving both data privacy and security. The pa-\nrameters of the locally-trained models are occasionally up-\nloaded to a central server, where they are aggregated to form\na unified global model. For instance, a scenario has been pre-\nsented in [4], where multiple hospitals collaborate to train a\ndiagnostic model while ensuring that their patients' data re-\nmains private to each hospital and is not shared.\nAn underlying challenge in FL is data heterogeneity,\nwhere the data available to each local client follows differ-\nent distributions, making it difficult for the global model to\ngeneralize effectively to unseen target domains. To address\nthis challenge, federated domain generalization (FDG) meth-\nods [5, 6] have been proposed. These methods mostly focus\non either learning domain-invariant features [4] or learning\ncommon features across domains [7] to enhance the global\nmodel's robustness across diverse environments. While FDG\ntechniques show promise in tackling data heterogeneity, two\nchallenges persist as a result of this phenomenon. The first is\nthe overconfidence of the clients on their local data, as they\ntend to learn domain-specific features. This overconfidence\nlimits the effectiveness of these local models when aggre-\ngated to form the global model. Secondly, the distribution of\nsamples in each client is different from that of other clients,\nleading to imbalances in model training, i.e., clients with\nmore data samples could be contributing more to the forma-\ntion of the global model. This in turn can result in a biased or\nsub-optimal performance [8].\nIn this paper, we propose Federated Domain General-\nization with Label Smoothing and Balanced Decentralized\nTraining (FedSB) to address these issues. FedSB mitigates\nlocal model overconfidence by using label smoothing to pro-\nmote domain-invariant representation learning, enhancing\nthe model's generalization across different domains. Ad-\nditionally, we introduce a simple and innovative budgeting\nmechanism that allocates training resources according to\neach client's local data volume, thereby ensuring balanced\nand consistent training contributions from all clients which\nultimately improves the overall model performance. Our\napproach is designed to enhance the robustness and general-\nization of FL models across diverse and unseen domains. We\nconduct comprehensive experiments, ablation studies, and\nsensitivity analyses to evaluate the effectiveness of FedSB us-\ning four datasets from the domainbed benchmark[9]: PACS,\nOfficeHome, TerraInc, and VLCS, achieving state-of-the-art\nperformance on 3 of the 4 datasets.\nOur contributions in this paper are: (1) We propose label\nsmoothing as an effective means to address the issue of over-\nconfidence in local clients, leading to improved generalization\nacross unseen domains. To the best of our knowledge, this is\nthe first application of label smoothing in the context of feder-\nated domain generalization. (2) We propose a novel budgeting\ntechnique to effectively mitigate the data heterogeneity chal-\nlenge faced by local clients. (3) We evaluate FedSB on multi-\nple domain generalization datasets, achieving state-of-the-art\nperformances. We also conduct comprehensive ablation stud-\nies and sensitivity analyses to demonstrate the effectiveness"}, {"title": "2. RELATED WORK", "content": "Federated learning. FL aims to collaboratively optimize a\ncentral model through training multiple local clients, e.g.,\ndevices or organizations, without sharing data to preserve pri-\nvacy [1, 10]. FedAVG [1] is a very popular algorithm, where\nlocal models are iteratively uploaded to a central server, and\nthen through a weighted averaging scheme, a global model\nis produced, which will then be downloaded to the clients.\nMany other methods build on top of FedAVG to enhance its\ncapabilities and performance. For instance, FedProx [8] adds\na proximal term to the local loss to prevent the local models\nfrom deviating extensively from the global model. MOON\n[11] utilizes model contrastive learning to correct local repre-\nsentations. FedDrop [12] introduces dropout techniques into\nFL to reduce the impact of system heterogeneity. FedNova\n[13] proposes a normalized averaging technique to address\nsystem heterogeneity by accounting for differences in the\nnumber of local updates each client performs.\nFederated domain generalization. In FDG, the primary goal\nis to train a global model through multiple local clients collab-\noratively. This model should perform well in an unseen do-\nmain. A key feature of FDG is its ability to achieve this with-\nout explicitly sharing data among the clients, thereby preserv-\ning data privacy. Most works in this area utilize an FL setup,\nwhere each client has access to only one data domain, and a\ncentral server is responsible for aggregating these local clients\nto produce one unified model. FedSR [4] employs L2 norm\nand conditional mutual information regularizers at the client\nlevel to discourage the learning of domain-specific features.\nSome recent works share some form of information amongs\nclients, which could potentially lead to breach of data privacy.\nCCST [14] facilitates the sharing of a style bank between\nclients, where the style bank comprises the mean and standard\ndeviation of representations generated from each client's lo-\ncal data using a CNN-based backbone, such as VGG. A style\ntransfer model is then employed to transfer the styles of im-"}, {"title": "3. METHOD", "content": "Problem Formulation. Let's assume $i \\in \\{1,2,\\dots,K\\}$,\nwhere $K$ is the total number of clients in the system. Here,\nclient $C_i$ has access to its local dataset $\\mathcal{D}_i = \\{(x_i, y_i)\\}$ from a\nspecific distribution $p_i(x, y)$, with $x \\in \\mathcal{X}$ and $y \\in \\mathcal{Y}$ represent-\ning the inputs and labels, respectively. The objective of FDG\nis to learn a global model $\\Theta$ by aggregating the local models\n$\\theta_i$, such that the global model can generalize to an unseen tar-\nget domain with distribution $p_\\tau(x, y)$, where $p_\\tau(x) \\neq p_i(x)$\nfor $i \\in [1, K]$, and $\\mathcal{D}_\\tau$ is not available during training.\nFedSB. We propose FedSB to address the challenges of data\nheterogeneity in FDG. As shown in Fig. 1, FedSB operates\nthrough two complementary steps. First, it encourages local\nclients to learn domain-invariant representations by reducing\ntheir local overconfidence through label smoothing. Second,\nit promotes a balanced contribution from different clients by\nutilizing a simple yet novel budgeting technique. The fol-\nlowing sections provide a detailed explanation of these ap-\nproaches.\nEach local client has access to a relatively small dataset\nwithin a distinct domain. As a result, the trained local models\ncan produce predictions with high confidence within this do-\nmain. However, these models lack generalization and perform"}, {"title": "FedSB.", "content": "We propose label\nsmoothing as an effective means to address the issue of over-\nconfidence in local clients, leading to improved generalization\nacross unseen domains. To the best of our knowledge, this is\nthe first application of label smoothing in the context of feder-\nated domain generalization. (2) We propose a novel budgeting\ntechnique to effectively mitigate the data heterogeneity chal-\nlenge faced by local clients. (3) We evaluate FedSB on multi-\nple domain generalization datasets, achieving state-of-the-art\nperformances. We also conduct comprehensive ablation stud-\nies and sensitivity analyses to demonstrate the effectiveness\npoorly when presented with data from other domains. To al-\nleviate this, we introduce a level of controlled uncertainty to\nthe model to prevent local clients from overfitting to domain-\nspecific representations. To achieve this, we employ a label\nsmoothing technique and replace hard labels with soft labels,\nby altering ground truth labels as follows:\n\n$\\overline{y}_{i}= \\begin{cases}1-\\epsilon+\\frac{\\epsilon}{M}, & \\text { if } c=y \\\\\n\\frac{\\epsilon}{M} , & \\text { if } c \\neq y\\end{cases}$\n\n(1)\nwhere $\\epsilon$ is the smoothing coefficient. This formulation en-\nsures that$\\sum_{c=1}^{M} \\overline{y}_{i}=1$. Consequently, the cross-entropy loss\nusing the smoothed labels can be derived as:\n\n$\\mathcal{L}=-\\sum_{c=1}^{M}\\left(\\overline{y}_{c} \\log \\left(p_{c}\\right)\\right)=-\\left(\\overline{y}_{y} \\log \\left(p_{y}\\right)+\\sum_{c \\neq y} \\overline{y}_{c} \\log \\left(p_{c}\\right)\\right)$\n\n(2)\nHere, the collecting terms can be rearranged as:\n$\\mathcal{L}=(1-\\epsilon)\\left(-\\log \\left(p_{y}\\right)\\right)+\\frac{\\epsilon}{M}\\left(-\\sum_{c=1}^{M} \\log \\left(p_{c}\\right)\\right)$.\n\n(3)\nIn this formulation, the Negative Log-Likelihood (NLL) loss\nover the target class encourages correct predictions, whereas\nthe Smooth Loss reduces overconfidence by leveraging the in-\ncorrect classes (all classes other than the correct target class).\nWe apply $\\epsilon$ to control the level of smoothness of the labels,\nwhere higher $\\epsilon$ values penalize the local models more for\noverconfident predictions.\nNext, let us assume a local training process at round $t+1$\nin client $C_i$. The model is initialized with the global param-\neters of round $t$ denoted by $\\Theta^{t}$. We can characterize the pa-\nrameters of $C_i$ after local training, using gradient descent, as:\n\n$\\Theta_{i}^{t+1}=\\Theta^{t}-\\eta \\sum_{j=1}^{B} \\nabla_{\\Theta^{t}} \\mathcal{L}_{i j}$\n\n(4)\nwhere $\\eta$ is the learning rate, $B$ is the batch size, and $\\mathcal{L}$ is the\nloss of the $j^{t h}$ batch of the local dataset $D_i$. By aggregating\nall the local models in the server using a simple technique\nsuch as averaging, the global model is obtained as:\n\n$\\Theta^{t+1}=\\frac{1}{K} \\sum_{i=1}^{K}\\left(\\Theta_{i}^{t+1}\\right)=\\Theta^{t}-\\frac{\\eta}{K} \\sum_{i=1}^{K} \\sum_{j=1}^{B} \\nabla_{\\Theta^{t}} \\mathcal{L}_{i j}$\n\n(5)"}, {"title": "Datasets.", "content": "We evaluate our method on four datasets namely\nPACS, OfficeHome, VLCS, and TerraInc  The PACS\ndataset comprises 9,991 images of the 'Photo', 'Art Paint-\ning', 'Cartoon', and 'Sketch' domains, each containing\nseven classes. OfficeHome also has four distinct domains:\n'Art', 'Clipart', 'Product', and 'Real-world', comprising over\n15,500 images belonging to 65 classes. Furthermore, Terra\nIncognita includes 24,788 pictures of animals from four dis-\ntinct locations. Finally, VLCS comprises 10,729 images from\nfour distinct object classification datasets: VOC, LabelMe,\nCaltech101, and SUN09 with five shared classes."}, {"title": "Evaluation.", "content": "Following [9], we utilize the leave-one-domain-\nout setting. Each time, we select one domain $D_i$ as a target\nand train the model on the rest of the domains. We repeat this\nfor all domains and average the performance.\nBaselines. We compare FedSB with FedAVG [1], FedADG\n[7], FedProx [8], FedSR [4], and FedIIR [19]. We re-run all\nthe baselines and report the average performance over three\ndifferent runs, except FedADG [7], where we report the re-\nsults from the original paper.\nImplementation details. Following prior works, we use\nResNet-18 for the PACS and VLCS datasets as the feature\nextractor, while for OfficeHome and TerraIncognita, we uti-\nlize ResNet-50. In addition to ResNet-based backbones used\nby prior works, we also report our results on PACS and Of-\nficeHome using Vision Transformers (ViTs) [21]. Given that\nViTs have not been used by the baselines, we re-implement\nthe commonly used FedAVG baseline using two variants of\nViTs, namely ViT-b/16 and ViT-b/32. Finally, to compare\nagainst CCST [14], we also evaluate our method on PACS\nwith the ResNet-50 backbone. All models are trained with\nAdam and a learning rate of le-4, using a batch size of 64.\nWe implement and train our method and baselines using the\nPyTorch framework on an Nvidia RTX 3090 GPU."}, {"title": "Results.", "content": "Table 1 reports the performance of FedSB in com-\nparison to the baselines on PACS, OfficeHome, TerraINC,\nand VLCS datasets. As demonstrated, FedSB achieves state-\nof-the-art results on three out of four datasets. Additionally,\nas shown in Fig. 2, our method shows better separability of\nclasses compared to that of the FedAVG baseline. Similarly,\ncomparisons against CCST in Table 2 demonstrate the supe-\nriority of our approach. Lastly, evaluations using ViT back-\nbones are presented in Table 3 where we observe that FedSB\ncontinues to outperform the baseline."}, {"title": "4. EXPERIMENTS", "content": "Accordingly, as $\\mathbb{E}\\left[\\sum_{j=1}^{B} \\nabla_{\\Theta^{t}} \\mathcal{L}_{i j}\\right]=\\frac{\\left|\\mathcal{D}_{i}\\right|}{B} \\mathbb{E}\\left[\\nabla_{\\Theta^{t}} \\mathcal{L}_{i}\\right]$, where\n$\\mathbb{E}\\left[\\nabla_{\\Theta^{t}} \\mathcal{L}_{i}\\right]$ represent the expected update of client i, the ex-\npected global model parameters can be described as:\n\n$\\mathbb{E}\\left[\\Theta^{t+1}\\right]=\\Theta^{t}-\\frac{\\eta}{K} \\sum_{i=1}^{K} \\frac{\\left|\\mathcal{D}_{i}\\right|}{B} \\mathbb{E}\\left[\\nabla_{\\Theta^{t}} \\mathcal{L}_{i}\\right]$\n\n(6)\nAccording to Eq. 6, we can deduce that clients with larger\ndatasets, i.e., larger $\\left|\\mathcal{D}_{i}\\right|$, have a greater influence in deter-\nmining the expected value of the global model's update com-\npared to clients with smaller datasets. This can degrade gen-\neralization, as the global model tends to gravitate toward the\ndomain of the more influential clients. To address this issue,\nwe apply a simple trick to ensure that each client operates un-\nder a fixed training budget regardless of the local dataset size.\nSpecifically, we use a fixed budget for all clients denoted as\n$S$. If $\\left|\\mathcal{D}_{i}\\right|>S$, we randomly select $S$ samples from $\\mathcal{D}_{i}$,\nwhereas where $\\left|\\mathcal{D}_{i}\\right| < S$ we oversample $\\mathcal{D}_{i}$. By using the\nunder/over-sampled dataset $\\widetilde{\\mathcal{D}}_{i}$ for training each client, the\nexpected global model is derived as:\n\n$\\mathbb{E}\\left[\\Theta^{t+1}\\right]=\\Theta^{t}-\\frac{\\eta}{K B} \\sum_{i=1}^{K} \\mathbb{E}\\left[\\nabla_{\\Theta^{t}} \\widetilde{\\mathcal{L}}_{i}\\right]$\n\n(7)\nwere $\\widetilde{\\mathcal{L}}$ denotes the loss over $\\widetilde{\\mathcal{D}}_{i}$. As demonstrated in\nthis equation, this straightforward and intuitive solution can ef-\nfectively ensure equal contribution from all clients toward the\nglobal update and mitigate the impact of data heterogeneity,\nparticularly with respect to dataset size."}, {"title": "5. CONCLUSION", "content": "This paper has introduced FedSB, a powerful solution to ad-\ndress the challenges of data heterogeneity in FDG. FedSB\nconsists of two components that tackle local model overcon-\nfidence and client sample imbalance respectively. We demon-\nstrate the effectiveness of our approach through extensive ex-\nperiments on four well-known domain generalization bench-\nmark datasets, advancing the state-of-the-art byoutperforming\ncompeting FDG methods on three of the four datasets. We\nalso provide ablation studies as well as a sensitivity analysis,\nwhich provides insight into the different components of our\nmethod."}]}