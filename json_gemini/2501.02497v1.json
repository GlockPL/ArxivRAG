{"title": "Test-time Computing: from System-1 Thinking to System-2 Thinking", "authors": ["Yixin Ji", "Juntao Li", "Hai Ye", "Kaixin Wu", "Jia Xu", "Linjian Mo", "Min Zhang"], "abstract": "The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.", "sections": [{"title": "Introduction", "content": "Over the past decades, deep learning with its scaling effects has been the driving engine behind the AI revolution. Particularly in the text modality, large language models (LLMs) represented by the GPT series (Radford et al., 2018, 2019; Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023) have demonstrated that larger models and more training data lead to better performance on downstream tasks. However, on the one hand, further scaling in the training phase becomes difficult due to the scarcity of data and computational resources (Villalobos et al., 2024); on the other hand, existing models still perform far below expectations in terms of robustness and handling complex tasks. These shortcomings are attributed to\nthe model's reliance on fast, intuitive System-1 thinking, rather than slow, deep System-2 thinking (Weston and Sukhbaatar, 2023). Recently, the o1 model (OpenAI, 2024), equipped with System-2 thinking, has gained attention for its outstanding performance in complex reasoning tasks. It demonstrates a test-time computing scaling effect: the greater the computational effort in the inference, the better the model's performance.\nThe concept of test-time computing emerged before the rise of LLMs and was initially applied to System-1 models (illustrated in Figure 1). These System-1 models can only perform limited perceptual tasks, relying on patterns learned during training for predictions. As a result, they are constrained by the assumption that training and testing are identically distributed and lack robustness and generalization to distribution shifts (Zhuang et al., 2020). Many works (Wang et al., 2021; Ye et al., 2023) have attempted to improve model robustness with test-time computing, also known as test-time adaptation (TTA). In the era of small models, the mainstream method of TTA is to update model parameters at test-time, so that the model's representation gradually adapts to the test distribution. For LLMs, due to the high cost of updating parameters, TTA leverages external information to steer model behavior, including modifying inputs, editing representations, and calibrating outputs. By introducing TTA, the System-1 model slows down its thinking process and gradually evolves into a weak System-2 model.\nCurrently, advanced LLMs with chain-of-thought (CoT) prompting have enabled language models to reach preliminary System-2 thinking, demonstrating the human-like cognitive ability to decompose problems and reason step by step (Wei et al., 2022). However, they still struggle with complex tasks like reasoning and planning (Stechly et al., 2024; Sprague et al., 2024). To achieve stronger System-2 models, researchers employ"}, {"title": "2 Background", "content": "System-1 and System-2 thinking are psychological concepts (Kahneman, 2011). When recognizing familiar patterns or handling simple problems, humans often respond intuitively. This automatic, fast thinking is called System-1 thinking. In contrast, when dealing with complex problems like mathematical proofs or logical reasoning, deep and deliberate thought is required, referred as System-2 thinking-slow and reflective. In the field of artificial intelligence, researchers also use these terms to describe different types of models (LeCun, 2022). System-1 models respond directly based on internally encoded perceptual information and world knowledge without showing any intermediate decision-making process. In contrast, System-2 models explicitly generate reasoning processes and solve tasks incrementally. Before the rise of"}, {"title": "3 Test-time Adaptation for System-1 Thinking", "content": "Model updating utilizes test sample information to further finetune model parameters during the infer-"}, {"title": "3.1 Updating the Model", "content": "Model updating utilizes test sample information to further finetune model parameters during the inference stage, enabling the model to adapt to the test distribution. The key lies in how to obtain information about the test samples to provide learning signals and how to select appropriate parameters and optimization algorithms to achieve efficient and stable updates.\nLearning signal In the inference stage, the ground-truth of test samples is unavailable. Thus many works attempt to design unsupervised or self-supervised objectives as learning signals. Existing learning signals can be classified into two categories based on whether the training process can be modified: test-time training (TTT) and fully test-time adaptation (FTTA). TTT assumes users can modify the training process by incorporating distribution-shift-aware auxiliary tasks. During test-time adaptation, the auxiliary task loss serves as the learning signal for optimization. Many self-supervised tasks have been shown to be effective as auxiliary tasks in image modality, such as rotation prediction (Sun et al., 2020), meta learning (Bartler et al., 2022), masked autoencoding (Gandelsman et al., 2022) and contrastive learning (Liu et al., 2021; Chen et al., 2022). Among them, contrast learning has been successfully applied to test-time adaptation for visual-language tasks due to its generalization of self-supervised learning within and across modalities (Zhu et al., 2024).\nIn contrast, FTTA is free from accessing the training process and instead uses internal or external feedback on test samples as learning signals. Uncertainty is the most commonly learned signal, driven by the motivation that when test samples shift from the training distribution, the model's confidence in its predictions is lower, resulting in higher uncertainty. Tent (Wang et al., 2021) uses the entropy of model predictions as a measure of"}, {"title": "3.2 Modifying the Input", "content": "When it comes to LLM, the large number of parameters makes model update-based TTA methods face a tougher dilemma of efficiency and stability. As a result, input-modification-based methods, which do not rely on parameter updates, have become the mainstream method for TTA in LLMs. The effectiveness of input-modified TTA stems from the in-context learning (ICL) capability of LLM, which can significantly improve the performance by adding some demonstrations before the test sample. ICL is highly sensitive to the selection and order of demonstrations. Therefore, the core objective of input-modification TTA is to select appropriate demonstrations for the test samples and arrange them in the optimal order to maximize the effectiveness of ICL.\nFirst, empirical studies (Liu et al., 2022) show that the closer the demonstrations are to the test sample, the better the ICL performance. Therefore, retrieval models like BM25 and SentenceBERT are used to retrieve demonstrations semantically closest to the test sample and rank them in descending order of similarity (Qin et al., 2024a; Luo et al., 2023a). To improve the accuracy of demonstration retrieval, Rubin et al. (2022) and Li et al. (2023b) specifically train the demonstration retriever by contrastive learning. Then, as researchers delve deeper into the mechanisms of ICL, ICL is considered to conduct implicit gradient descent on the demonstrations (Dai et al., 2023). Therefore, from the perspective of training data, demonstrations also need to be informative and diverse (Su et al., 2022; Li and Qiu, 2023). Wang et al. (2023c) view language models as topic models and formulate"}, {"title": "3.3 Editing the Representation", "content": "For generative LLMs, some works have found that the performance bottleneck is not in encoding world knowledge, but in the large gap between the information in intermediate layers and the output. During the inference phase, editing the representation can help externalize the intermediate knowledge into the output. PPLM (Dathathri et al., 2020) performs gradient-based representation editing under the guidance of a small language model to control the style of outputs. ActAdd (Turner et al., 2024) selects two semantically contrastive prompts and calculates the difference between their representations as a steering vector, which is then added to the residual stream. Representation editing based on contrastive prompts has demonstrated its effectiveness in broader scenarios, including instruction following (Stolfo et al., 2024), alleviating hallucinations (Li et al., 2023a; Arditi et al., 2024), reducing toxicity (Liu et al., 2024a; Lu and Rimsky, 2024) and personality (Cao et al., 2024). SEA (Qiu et al., 2024b) projects representations onto directions with maximum covariance with positive prompts and minimum covariance with negative prompts. They also introduce nonlinear feature transformations, allowing representation editing"}, {"title": "3.4 Calibrating the Output", "content": "Using external information to calibrate the model's output distribution is also an efficient yet effective test-time adaptation method (Khandelwal et al., 2020). AdaNPC (Zhang et al., 2023a) designs a memory pool to store training data. During inference, given a test sample, AdaNPC recalls k samples from the memory pool and uses a kNN classifier to predict the test sample. It then stores the test sample and its predicted label in the memory pool. Over time, the sample distribution in the memory pool gradually aligns with the test distribution. In NLP, the most representative application of such methods is kNN machine translation (kNN-MT). kNN-MT (Khandelwal et al., 2021) constructs a datastore to store contextual representations and their corresponding target tokens. During translation inference, it retrieves the k-nearest candidate tokens from the datastore based on the decoded context and processes them into probabilities. Finally, it calibrates the translation model's probability distribution by performing a weighted fusion of the model's probabilities and the retrieved probabilities. kNN-MT has demonstrated superior transferability and generalization compared to traditional models in cross-domain and multilingual MT tasks. Subsequent studies have focused on improving its performance and efficiency (Wang et al., 2022a; Zhu et al., 2023b; You et al., 2024) or applying its methods to other NLP tasks (Wang et al., 2022b; Bhardwaj et al., 2023)."}, {"title": "4 Test-time Reasoning for System-2 Thinking", "content": "Test-time reasoning aims to spend more inference time to search for the most human-like reasoning process within the vast decoding search space. In this section, we introduce the two core components of test-time reasoning: feedback modeling and search strategies (as shown in Figure 3)."}, {"title": "4.1 Feedback Modeling", "content": "Score-based Feedback Score-based feedback, also known as the verifier, aims to score generated results, evaluating their alignment with ground truth or human cognitive processes. Its training process is typically similar to the reward model in RLHF, using various forms of feedback signals and modeling it as a classification (Cobbe et al., 2021) or rank task (Bradley and Terry, 1952; Yuan et al., 2024a; Hosseini et al., 2024). In reasoning tasks, verifiers are mainly divided into two categories: outcome-based (ORMs) and process-based verifiers (PRMs). ORMs (Cobbe et al., 2021) use the correctness of the final CoT result as training feedback, while PRMs (Uesato et al., 2022; Lightman et al., 2024) are trained based on feedback from each reasoning step. PRM not only evaluates intermediate reasoning steps but also evaluates the entire reasoning process more accurately than ORM. However, PRM requires more human effort to annotate feedback for the intermediate steps. Math-Shepherd (Wang et al., 2024e) and OmegaPRM (Luo et al., 2024) utilize MCTS algorithm to collect high-quality process supervision data automatically. Setlur et al. (2024) argue that PRM should evaluate the advantage of each step for subsequent reasoning rather than focusing solely on its correctness. They propose process advantage verifiers (PAVs) and efficiently construct training data through Monte Carlo simulations. Score-based feedback modeling overlooks the generative capabilities of LLMs, making it difficult to detect fine-grained errors. Thus, recent works propose generative score-based verifiers (Ankner et al., 2024; Ye et al., 2024). GenRM (Zhang et al., 2024d) leverages instruction tuning to enable the verifier to answer \u2018Is the answer correct (Yes/No)?' and uses the probability of generated 'Yes' token as the score. GenRM can also incorporate CoT, allowing the verifier to generate the corresponding rationale before answering '\u2018Yes' or 'No'. Critic-RM (Yu et al., 2024) jointly trains the critique model and the verifier. During inference, the verifier scores according to answers and verbal-based feedback generated by the critique model.\nVerbal-based Feedback Although the verifier can accurately evaluate the correctness of generated answers or steps, it lacks interpretability, making it unable to locate the specific cause of errors or provide correction suggestions. Verbal-based feedback, also referred to critic, fully leverages the LLM's instruction-following ability. By designing specific instructions, it can perform pairwise comparisons, evaluate answers from multiple dimensions, and even provide suggestions for revision in natural language. Powerful closed-source LLMs, such as GPT-4 and Claude, are effective critics. They can perform detailed and controlled assessments of generated texts, such as factuality, logical errors, coherence, and alignment, with high consistency with human evaluations (Wang et al., 2023a; Luo et al., 2023b; Liu et al., 2023; Chiang and Lee, 2023). However, they still face biases such as length, position, and perplexity (Bavaresco et al., 2024; Wang et al., 2024d; Stureborg et al., 2024). LLM-as-a-Judge (Zheng et al., 2023) carefully designs system instructions to mitigate the interference of biases.\nTo obtain cheaper verbal-based feedback, open-source LLMs can also serve as competitive alternatives through supervised fine-tuning (SFT) (Wang et al., 2024f; Zhu et al., 2023a; Liang et al., 2024c; Paul et al., 2024). Shepherd (Wang et al., 2023b) collects high-quality training data from human annotation and online communities to fine-tune an evaluation model. Auto-J (Li et al., 2024a) collects queries and responses from various scenarios and designs evaluation criteria for each scenario. GPT-4 then generates critiques of the responses based on these criteria and distills its critique ability to open-source LLMs. Prometheus (Kim et al., 2024b,c) designs more fine-grained evaluation dimensions."}, {"title": "4.2 Search Strategies", "content": "4.2.1 Repeated Sampling\nSampling strategies such as top-p and top-k are commonly used decoding algorithms in LLM inference. They introduce randomness during decoding to enhance text diversity, allowing for parallelly sampling multiple generated texts. Through repeated sampling, we have more opportunities to find the correct answer. Repeated sampling is particularly suitable for tasks that can be automatically verified, such as code generation, where we can easily identify the correct solution from multiple samples using unit tests (Li et al., 2022a; Rozi\u00e8re et al., 2024). For tasks that are difficult to verify, like math word problems, the key to the effectiveness of repeated sampling is the verification strategy.\nVerification strategy Verification strategies include two types: majority voting and best-of-N (BON) sampling. Majority voting (Li et al., 2024b; Lin et al., 2024a) selects the most frequently occurring answer in the samples as the final answer, which is motivated by ensemble learning. Majority voting is simple yet effective. For instance, self-consistency CoT (Wang et al., 2023d) can improve accuracy by 18% over vanilla CoT in math reasoning tasks. However, the majority does not always hold the truth, as they may make similar mistakes. Therefore, some studies perform validation and filtering before voting. For example, the PROVE framework (Toh et al., 2024) converts CoT into executable programs, filtering out samples if the program's results are inconsistent with the reasoning chain's outcomes.\nBest-of-N sampling uses a verifier to score each generated result and selects the one with the highest score as the final answer (Stiennon et al., 2020; Cobbe et al., 2021; Nakano et al., 2022). Li et al. (2023c) propose a voting-based BoN variant, which performs weighted voting on all answers based on the verifier's scores and selects the answer with the highest weight. In addition, some works aim to"}, {"title": "4.2.2 Self-correction", "content": "Self-correction is a sequential test-time computation method that enables LLMs to iteratively revise and refine generated results based on external or internal feedback (Shinn et al., 2023).\nFeedback sources The feedback used for self-correction is typically presented in natural language and comes from various sources, including human evaluation, tool checking, external model evaluation, and intrinsic feedback. Human evaluation is the gold standard for feedback, but due to its high cost and limited scalability, it is mainly used in early research to explore the upper limits of self-correction capabilities (Tandon et al., 2021; Elgohary et al., 2021; Tandon et al., 2022). For certain domain-specific tasks, external tool checking provides accurate and efficient feedback (Gou et al., 2024; Chen et al., 2024c; Gao et al., 2023b). For example, Yasunaga and Liang (2020) propose to"}, {"title": "4.2.3 Tree Searching", "content": "Repeated sampling and self-correction scale test-time computation in parallel and sequentially, respectively. Human thinking is a tree search that combines brainstorming in parallel with backtracking to find other paths to solutions when it encounters a dead end. Search algorithms and value functions are two critical components in tree searching.\nSearch algorithm In LLM reasoning, current search algorithms include uninformed search and heuristic search. Uninformed search does not rely on specific heuristic information but explores the search space according to a fixed rule. For example, tree-of-thought (ToT) (Yao et al., 2023) adopts the BFS or DFS to search, while Xie et al. (2023) use beam search. Uninformed search is usually less efficient for problems with large search spaces, so heuristic search represented by MCTS is widely used in reasoning tasks (Hao et al., 2023; Zhang et al., 2024b; Bi et al., 2024). MCTS gradually optimizes search results through four steps: selection, expansion, simulation, and backpropagation, approaching the optimal solution. Long (2023) trains an LLM controller using reinforcement learning to guide the LLM reasoner's search path.\nValue function The value function evaluates the value of each action and guides the tree to expand towards branches with higher values in MCTS. RAP (Hao et al., 2023) designs a series of heuristic value functions, including the likelihood of the action, the confidence of the state, self-evaluation results, and task-specific reward, and combines them according to task requirements. Reliable and generalized value functions facilitate the application of MCTS to more complex problems with deeper search spaces. AlphaMath (Chen et al., 2024a) and TS-LLM (Feng et al., 2024b) replace the hand-crafted value function with a learned LLM value function, automatically generating reasoning process and step-level evaluation signals in MCTS. Traditional MCTS methods expand only one trajectory, while rStar (Qi et al., 2024) argues that the current value function struggles to guide the selection of the optimal path accurately. Therefore, rStar retains multiple candidate paths and performs"}, {"title": "5 Future Directions", "content": "5.1 Generalizable System-2 Model\nCurrently, most o1-like models exhibit strong deep reasoning abilities only in specific domains such as math and code and struggle to adapt to cross-domain or general tasks. The key to addressing this issue lies in enhancing the generalization ability of verifiers or critics. Currently, some works utilize model ensemble (Lin et al., 2024b) or regularization constraints (Yang et al., 2024; Jia, 2024) to make verifiers more generalizable. Nevertheless, there is still significant room for improvement in the generalization of the verifier. Additionally, weak-to-strong generalization (Burns et al., 2023) is a topic worth further exploration. People are no longer satisfied with solving mathematical problems with standard answers; they hope System-2 models can assist in scientific discovery and the proofs of mathematical conjectures. In such cases, even human experts struggle to provide accurate feedback, while weak-to-strong generalization offers a promising direction to address this issue.\n5.2 Multimodal Reasoning\nIn System-1 thinking, TTA has been successfully applied to multimodal LLMs, improving performance in tasks such as zero-shot image classification, image-text retrieval, and image caption-"}, {"title": "5.3 Efficiency and Performance Trade-off", "content": "The successful application of test-time computing shows that sacrificing reasoning efficiency can lead to better reasoning performance. However, researchers continue to seek a balance between performance and efficiency, aiming to achieve optimal performance under a fixed reasoning latency budget. This requires adaptively allocating computational resources for each sample. Damani et al. (2024) train a lightweight module to predict the difficulty of a question, and allocate computational resources according to its difficulty. Zhang et al. (2024c) further extend the allocation targets to more hyperparameters. There are still many open questions worth exploring, such as how to integrate inference acceleration strategies, e.g. KV cache compression, token pruning, and speculative decoding with test-time computing and how to predict problem difficulty more accurately."}, {"title": "5.4 Scaling Law", "content": "Unlike training-time computation scaling, test-time computing still lacks a universal scaling law. Some works have attempted to derive scaling laws for specific test-time computing strategies (Wu et al., 2024c; Chen et al., 2024d). Brown et al. (2024)"}, {"title": "5.5 Strategy Combination", "content": "Different test-time computing strategies are suited to various tasks and scenarios, so combining multiple strategies is one way to achieve better System-2 thinking. For example, Marco-o1 (Zhao et al., 2024b) combines the MCTS and self-correction, using MCTS to plan reasoning processes, and self-correction to improve the accuracy of each step. Moreover, test-time adaptation strategies in System-1 models can also be combined with test-time reasoning strategies. Aky\u00fcrek et al. (2024) combine test-time training with repeated sampling. They further optimize the language modeling loss on test samples, then generate multiple candidate answers through data augmentation, and finally determine the answer by majority voting. They demonstrate the potential of test-time training in reasoning tasks, surpassing the human average on the ARC challenge. Therefore, we think that for LLM reasoning, it is crucial to focus not only on emerging test-time strategies but also on test-time adaptation methods. By effectively combining these strategies, we can develop System-2 models that achieve or surpass ol-level performance."}, {"title": "6 Conclusion", "content": "In this paper, we conduct a comprehensive survey of existing works on test-time computing. We introduce various test-time computing methods in System-1 and System-2 models, and look forward to future directions for this field. We believe test-time computing can help models handle complex real-world distributions and tasks better, making it a promising path for advancing LLMs toward cognitive intelligence. We hope this paper will"}, {"title": "Limitations", "content": "Test-time computing, especially the strategies in System-2, is evolving rapidly. While we have made efforts to provide a comprehensive survey of existing research, it is challenging to cover all the latest developments. This review includes papers up to November 2024, with more recent advancements to be updated in future versions. TTA has seen many successful applications and task-specific strategies in CV tasks. Since the primary audience of our paper is researchers in NLP, we do not systematically present these works, and interested readers can refer to Liang et al. (2024a) for details."}]}