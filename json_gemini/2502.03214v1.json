{"title": "\u00a1VISPAR An Interactive Visual-Spatial Reasoning Benchmark for VLMs", "authors": ["Julius Mayer", "Mohamad Ballout", "Serwan Jassim", "Farbod Nosrat Nezami", "Elia Bruni"], "abstract": "Vision-Language Models (VLMs) are known to struggle with spatial reasoning and visual alignment. To help overcome these limitations, we introduce iVISPAR, an interactive multi-modal benchmark designed to evaluate the spatial reasoning capabilities of VLMs acting as agents. iVISPAR is based on a variant of the sliding tile puzzle\u2014a classic problem that demands logical planning, spatial awareness, and multi-step reasoning. The benchmark supports visual 2D, 3D, and text-based input modalities, enabling comprehensive assessments of VLMs' planning and reasoning skills. We evaluate a broad suite of state-of-the-art open-source and closed-source VLMs, comparing their performance while also providing optimal path solutions and a human baseline to assess the task's complexity and feasibility for humans. Results indicate that while some VLMs perform well on simple spatial tasks, they encounter difficulties with more complex configurations and problem properties. Notably, while VLMs generally perform better in 2D vision compared to 3D or text-based representations, they consistently fall short of human performance, illustrating the persistent challenge of visual alignment. This highlights critical gaps in current VLM capabilities, highlighting their limitations in achieving human-level cognition.", "sections": [{"title": "1. Introduction", "content": "The rapid advancement of Large Vision-Language Models (VLMs) has spurred significant debate regarding their capacity to achieve human-level cognition. These models are increasingly deployed as general reasoning systems capable of addressing complex problems across diverse domains, with applications extending into dynamic, real-world scenarios such as physical agent-based tasks and planning (Wang et al., 2024a; Xi et al., 2023; Zeng et al., 2023). However, critical gaps persist in their spatial reasoning and visual alignment capabilities, areas essential for understanding, interpreting, and manipulating objects and their spatial relationships (Kamath et al., 2023; Bordes et al., 2024; Campbell et al., 2024).\nSpatial reasoning, a foundational aspect of problem-solving, navigation, and interaction with the physical world, requires models to bridge vision and cognition by interpreting visual information to understand spatial arrangements. Tasks such as mentally rotating shapes, predicting object movement, and recognizing patterns exemplify the importance of visual-spatial reasoning. Despite these critical requirements, progress in VLMs has been hampered by evaluation benchmarks that fail to capture the dynamic and multi-step complexity of real-world spatial reasoning. Existing benchmarks predominantly rely on static, text- or image-based setups that often oversimplify spatial contexts, focusing on 2D environments without interactivity or dynamic problem-"}, {"title": "2. Related work", "content": "Recent advancements in Vision-Language Models (VLMs) have prompted a surge in evaluations, yet many lack comprehensive benchmarking approaches. Existing studies primarily rely on question-answering tests or visual-spatial reasoning tasks, highlighting the need for more systematic evaluations tailored to the unique strengths of VLMs. For instance, Wang et al. (2024a) proposed SpatialEval, a benchmark encompassing diverse spatial reasoning tasks such as relationship understanding, navigation, and counting. Their findings reveal significant challenges for both language and vision-language models, with VLMs often underperforming compared to LLMs when textual clues are sufficient. Similarly, Tang et al. (2024) introduced Sparkle, a dataset focusing on core 2D spatial capabilities\u2014direction comprehension, distance estimation, and localization\u2014with fine-tuning experiments showing improved performance on composite spatial reasoning tasks. However, these works are primarily constrained to 2D environments and do not incorporate agency or explore advanced multi-modal comparisons in 3D scenarios.\nSeveral benchmarks in non-VLM domains focus on physical simulation or interactive environments, offering insights into related tasks but presenting challenges for VLM evaluation. For example, ThreeDWorld (Aghzal et al., 2023) provides a high-fidelity physics environment with tasks emphasizing spatial-temporal reasoning, such as path planning. However, its interaction framework is overly complex, and the lack of a dedicated language API limits its suitability for VLM evaluation. Rajabi & Kosecka (2023) proposed GSR-BENCH, a benchmark evaluating spatial relationships using multimodal models, but their work primarily focuses on grounded relationships and does not extend to agent-based tasks or dynamic reasoning.\nBenchmarks targeting VLMs often prioritize tasks that do not explicitly focus on spatial reasoning. For instance, Yamada et al. (2024) evaluated LLMs on textual representations of spatial relationships and navigation tasks across various grid structures, revealing gaps in their implicit understanding of spatial structures. Similarly, Rizvi et al. (2024) introduced SpaRC and SpaRP, which focus on textual spatial reasoning chains and path generation. While these works highlight interesting patterns in reasoning, they lack the integration of visual modalities necessary for real-world multimodal tasks.\nPath planning benchmarks, such as those presented in Aghzal et al. (2023), test spatial-temporal reasoning using textual inputs, with visual components relegated to supplementary material. Other works, such as Zhang et al. (2024), leverage synthetic abstract images for visual reasoning but fail to explore spatial agency or complex multi-step planning. Earlier multimodal benchmarks, like ShapeWorld (Kuhnle & Copestake, 2017), demonstrated the value of synthetic data for controlled evaluations but were limited to abstract tasks and did not address dynamic interactions or real-world spatial reasoning.\nDespite these contributions, key gaps remain. Few studies systematically investigate the interplay between textual and visual reasoning in dynamic environments, and evaluations of detailed, multi-step planning are rare. While benchmarks"}, {"title": "3. The iVISPAR Benchmark", "content": "iVISPAR is an interactive and multi-modal puzzle simulator that provides agents with an image or text representation of the board state. By rendering in 3D space, iVISPAR offers a more realistic depiction of spatial scenes compared to traditional 2D grid puzzles. Additionally, it supports a 2D top-down view and a text-based representation. Both the 3D and 2D renderings and text-based representations are illustrated in Figure 2.\nA key feature of iVISPAR is the Sliding Geom Puzzle (SGP), which reimagines the classic sliding tile puzzle by replacing numbered tiles with geometric objects defined by their unique combinations of color and shape (see Section 3.2). Agents interact with the board by issuing natural language commands through a text-based API to apply actions to the board. The objective is to rearrange pieces on the board to match a goal configuration. iVISPAR allows for a fine scaling of complexity, customizable random puzzle generation, and benchmarking performance with multiple baseline models."}, {"title": "3.1. Sliding Tile Puzzle", "content": "The sequential generalized sliding-tile puzzle (SGSTP) is a generalization of the classic 15-Tile Sliding Tile Puzzle (Appendix A.3: Figure 9). In the SGSTP, a set of $n < m_1 \\times m_2$ tiles, each uniquely labeled 1, ..., n, are placed on a rectangular grid of size $m_1 \\times m_2$, denoted by $G = (V, E)$. The grid has $M_1 \\times M_2 - n$ empty positions that allow tile movement.\nA configuration of tiles is represented as an injective mapping from the set ${1, ..., n}$ to positions $V = {(V_x, V_y) : 1 \\leq x \\leq m_2, 1 \\leq y \\leq m_1}$. Each tile must be repositioned from an arbitrary initial configuration $S = {s_1, ..., s_n}$ to a specified goal configuration $G = {g_1, ..., g_n}$, such as an ordered row-major layout.\nLet the movement path of tile i, where $1 \\leq i \\leq n$, be expressed as $p_i: N_0 \\rightarrow V$. The puzzle seeks a set of feasible paths $P = {P_1, ..., P_n}$ that satisfy the following conditions for all $1 < i, j < n$ with $i \\neq j$, and for all time steps $t \\geq 0$:\nIncremental Movement: $p_i(t + 1) = p_i(t)$ or $(p_i(t + 1), p_i(t)) \\in E$. Tiles move to adjacent, unoccupied positions or stay still.\nGoal Achievement: $p_i(0) = s_i$ and $p_i(T) = g_i$ for some $T > 0$. Each tile must start at $s_i$ and reach $g_i$\nExclusive Occupancy: $p_i(t) \\neq p_j(t)$ for all $i \\neq j$. Two tiles cannot occupy the same position at the same time.\nIn this sequential version, tiles move one at a time. Therefore, the head-on collision and corner-following constraints found in the generalized sliding-tile puzzle are omitted, as simultaneous tile movements are not permitted."}, {"title": "3.2. Sliding Geom Puzzle", "content": "The Sliding Geom Puzzle (SGP) replaces traditional numbered tiles with geometric objects (geoms) that are uniquely defined by color and shape attributes, increasing visual-spatial complexity and enhancing task scalability. This design shift requires models to interpret object features rather than follow simple numerical sequences, mirroring real-world spatial reasoning, where objects are distinguished by appearance, size, or structure. This approach aligns with physical tasks such as organizing items, assembling structures, or packing, promoting a more authentic evaluation of real-world spatial capabilities.\nGame dynamics In each episode, agents receive observations of the start and goal states, accompanied by task instructions. Agents apply move actions to geoms by referencing their unique color and shape combination and specifying the direction of intended movement. Geoms can be moved in cardinal directions (LEFT, RIGHT, UP, DOWN), with actions formatted as \"move <color> <shape> <direction>\":\n\"move blue sphere right\"\nActions are validated and applied if legal, with agents receiving updated board states regardless of the action's success after each move command. Effective and ineffective actions both result in valid new board states but, respectively, decrease or increase the path length to the goal state. Invalid moves, such as occupied destination and out-of-bounds actions, fail to alter the board state, as do illegal commands, which violate the instructed action format. This action-perception loop repeats until the goal state is achieved or a step limit is reached. Due to limited context windows, VLM agents receive task instructions at each time step. System prompt instructions and a sample agent-environment"}, {"title": "4. Experiments", "content": "Performance of VLMs is tested for the SGP to assess their capabilities in scene understanding, problem-solving, and multi-step planning within constrained environments."}, {"title": "4.1. Dataset Generation", "content": "Experiments were conducted on a dataset of SGPs on a fixed 4x4 board, with performance assessed by varying complexity across two parameters: the number of objects (2-11) and the shortest path length (2-11). Configurations maintain a geom interference factor of 0, ensuring the shortest path equals the cumulative Manhattan distance. Initial experiments indicated that VLM agents faced significant challenges at higher task complexities. Three episodes are sampled for each complexity level, producing a dataset of 300 diverse board configurations. The set of geom properties consists of four shapes-sphere, pyramid, cube, and cylinder\u2014and four colors-red, green, blue, and yellow-resulting in 16 unique combinations. A detailed list of the dataset generation parameter is provided in Appendix B.2. VLM agents are tested on the same dataset for each modality, resulting in 900 episodes for each model."}, {"title": "4.2. Baselines", "content": "To contextualize agent performance and provide upper and lower bounds, we establish four baselines encompassing human and AI agents.\nHuman performance was evaluated with 30 participants using a web app GUI of the SGP, where participants interacted by prompting text commands over a command line, mirroring the interaction method of VLM agents. Baselines were provided for 3D vision modality on the same dataset as the VLM agents.\nAI baselines were introduced for two agents: an optimal agent executing shortest path solutions computed by A* (Hart et al., 1968), and a random agent performing uninformed but valid actions uniformly sampled from those leading to new board states. Algorithms for the AI agents are detailed in Appendix E."}, {"title": "4.3. Models", "content": "We evaluate a selection of open- and closed-source VLMs that scored high on OpenCompass\u00b3 and which support multi-image inputs and a minimum context length of 800 tokens. Selected models are: Sonnet-3.5 (Claude Team, 2024), Gemini-2.0-flash (Gemini Team, 2024), GPT-40 (OpenAI, 2024), InternVL2.5-78B (Chen et al., 2024), LLaVA-OneVision-72B (Li et al., 2024), Qwen2-72B (Wang et al., 2024b). We use a temperature of 1.0, top-p of 0.95, and top-k of 50 for all open-source models. An overview of all models and their details can be found in the Appendix A.2.2."}, {"title": "4.4. Context-Aware Zero-Shot Reasoning", "content": "The models employ Chain-of-Thought (CoT) reasoning (Wei et al., 2022) to break down complex problems into smaller sub-tasks, enhancing accuracy and interpretability (Appendix D). We constrain VLMs' context windows to the past two steps, incorporating state representations alongside the model's action responses. This approach prioritizes extracting maximum value from limited experience to preserve the models' sequential coherence and minimize computational overhead (Appendix B.1). Operating within this context-aware zero-shot reasoning framework, the models interpret task requirements without examples, drawing exclusively from pretrained knowledge, task instructions, and limited past interactions."}, {"title": "4.5. Evaluation", "content": "Agent performance is evaluated through two primary metrics: the fraction of solved environments and mean step-deviation from the optimal path\nMean step-deviation from optimal path measures the deviation from optimal behavior during problem-solving. At each step t, the shortest path solution from the current board state to the goal, computed by A*, is used to assess how efficiently the agent progresses. Formally, step-deviation from optimal path is expressed as: $R(t) = d(s_t, s^*) \u2013 d(s_0, s^*)$ where $d(s, s^*)$ represents the shortest path from state s to the goal s*. This metric quantifies how much further the agent remains from the goal compared to the initial state. A"}, {"title": "5. Results", "content": "We evaluated the spatial reasoning capabilities of VLMs in our SGP environment on 3D vision and compared it to 2D vision and text-based modalities across 300 episodes each (Table 1). To standardize gameplay, the number of actions per episode was capped at 20.\nSuccess rates: The percentage of episodes completed and the mean deviations of steps from the optimal path were measured for each modality and compared to human performance as well as random actions (Figure 4).\nAction classification: We classified actions based on their effects on the board and calculated their average occurrence per episode to provide insights into the challenges VLMs face in efficiently completing episodes (Figure 7). Effective and ineffective actions both result in valid new board states"}, {"title": "6. Discussion", "content": "Our results show that VLMs perform worst on 3D vision, with performance generally close to the random baseline, highlighting the heightened difficulty of processing spatial relationships in three dimensions (Figure 4). Environments with 2D vision are more manageable for VLMs compared to both 3D vision and text-based spatial reasoning, likely due to finetuning on 2D spatial reasoning tasks during training. Sonnet-3.5 consistently outperforms other VLMs across"}, {"title": "7. Conclusion", "content": "We have introduced iVISPAR, a novel interactive multi-modal benchmark designed to evaluate the spatial reasoning capabilities in 3D vision of VLMs acting as agents. The benchmark, centered on the Sliding Geom Puzzle, evaluates VLMs' abilities in logical planning, spatial awareness, and multi-step problem-solving, aiming to reflect real-world spatial reasoning. Our evaluation tested a suite of state-of-the-art open-source and closed-source VLMs on a dataset of board configurations, scaled across two levels of complexity. We compared them to baselines for human capabilities, optimal and random agents, providing insight into their performance under varying conditions.\nOur findings demonstrate that VLMs struggle with spatial reasoning in 3D vision and that there are significant performance differences between the tested VLMs. While they understand the task and manage to outperform random agents in simple spatial tasks, they struggle with more complex configurations and intricate problem properties. Interestingly, VLMs demonstrate stronger performance in 2D vision representations compared to text-based tasks. This suggests that visual alignment for 3D spatial reasoning continues to pose a significant challenge, underscoring persistent gaps in VLM capabilities and highlighting barriers to achieving human-level cognitive performance."}, {"title": "C. System Prompt", "content": "You are a highly intelligent AI with exceptional spatial reasoning, tasked with solving a shape puzzle game on a 4 by 4 grid board.\n## Game Overview: The game consists of a grid board with two states: a current active state and a goal state. Your objective is to generate valid actions to move objects on the board, step by step on the shortest path, until the current state matches the goal state. 1. Analyze the current state. 2. Compare with the goal state. 3. Check past actions. 4. Generate a new valid action\n## Key Rules: ### Object Movement: 1. Each object occupies exactly one tile on the board. 2. Objects cannot move beyond the 4 by 4 grid boundaries or occupy the same tile as another object.\n## Action Format: - Your actions must follow this exact format: move <object color><object shape><direction>- Replace <object color>, <object shape>, and <direction>with appropriate options from the following list. - Do not use quotation marks or angle brackets <>in your action.\n### Valid Options: - Object Colors: green, red, blue, yellow - Object Shapes: cube, sphere, pyramid, cylinder - Directions: up, down, left, right\n### Example Actions: move green cube down - move blue sphere up - move red pyramid left - move yellow cylinder right\n## Important Notes: - No Coordinates: Your actions should only specify the color, shape, and direction of movement. Do not include grid coordinates. - Ensure every action adheres to the specified format and rules to progress effectively toward the goal state. Think carefully about the correct action and action format. - Invalid Actions: If an action does not change the board state, it is invalid (e.g., blocked by another object or out of bounds). If an action is invalid, you must try a different one.\n## Explain Your Reasoning - Before every action, explain your reasoning clearly. - At the end of every response, include this line exactly: action: <your action>- Replace <your action>with the valid move action based on your reasoning. - Do not add any characters after the word action.\n## Analyze the Images You can view your current active board state in the last image {text_snippet_active}. Study this image and the objects with their positions carefully. Your goal is to match the goal state, shown in the image {text_snippet_goal}. Study this image and the objects with their positions carefully.\n## Additionally, you are provided with: - The previous state image(s) {text_snippet_past}. - Your previous suggested action - Use this information by comparing it to your current active state to determine your next action.\n## Invalid Actions: - No Overlap: You are not allowed to position two objects in the same tile. - If the suggested action does not move any objects, it is invalid (e.g., blocked by another object or out of bounds). - Use the previous image(s) and action to understand why it failed and suggest a different action.\nIt is of most importance you always end your response with this exact format:\naction: move <object color><object shape><direction>where you replace <object color><object shape><direction>with the valid move action based on your reasoning and do not add any characters after your action.\nPlease explain your reasoning, then end with description: <your object coordinate list >, no matter what always end with description: <your object coordinate list >(dont add additional character after the word description)"}, {"title": "D. Example of an Interaction", "content": "Example of an Input\n## Analyze the Images\nYou can view your current active board state in the last image {text_snippet_active}.\nStudy this image and the objects with their positions carefully.\nYour goal is to match the goal state, shown in the image {text_snippet_goal}.\nStudy this image and the objects with their positions carefully.\n## Additionally, you are provided with:\n- The previous state image(s) {text_snippet_past}.\n- Your previous suggested action\n- Use this information by comparing it to your current active state to determine your next action.\n## Invalid Actions:\n- No Overlap: You are not allowed to position two objects in the same tile.\n- If the suggested action does not move any objects, it is invalid (e.g., blocked by another object or out of bounds).\n- Use the previous image(s) and action to understand why it failed and suggest a different action.\nIt is of most importance you always end your response with this exact format:\naction: move <object color> <object shape> <direction>\nwhere you replace <object color> <object shape> <direction> with the valid move action based on your reasoning\nand do not add any characters after your action.\nExample of an Output:\nlet me analyze the current situation\n1. current state green cylinder is at position d2 green cube is at position a3.\n2. goal state green cylinder needs to be at c2 green cube needs to be at a4.\n3. next move analysis the green cylinder needs to move left to reach c2 its currently at d2 and we can\nmove it left. looking at the previous moves and current position i can see the green cylinder\nhas been moving around to reach its goal position at c2 we should move it to the left.\nAction: \"move green cylinder left\""}, {"title": "E. Algorithms", "content": "E.1. Optimal Agent\nAlgorithm 1 A* Algorithm for Pathfinding\nRequire: start, goal\nEnsure: Path from start to goal or failure\nopenSet \u2190 {start}\ncameFrom \u2190 empty map\ngScore[start] \u21900\nfScore[start] \u2190 heuristic(start, goal)\nwhile openSet not empty do\ncurrent \u2190 node in openSet with lowest fScore\nif current = goal then\nreturn ReconstructPath(cameFrom, current)\nend if\nRemove current from openSet\nfor each neighbor of current do\ntentativeGScore \u2190 gScore[current] + d(current, neighbor)\nif tentativeGScore \u00a1 gScore[neighbor] or neighbor not in gScore then\ncameFrom[neighbor] \u2190 current\ngScore[neighbor] \u2190 tentativeGScore\nfScore[neighbor] \u2190 gScore[neighbor] + heuristic(neighbor, goal)\nif neighbor not in openSet then\nAdd neighbor to openSet\nend if\nend if\nend for\nend while\nreturn failure\nE.2. Random Agent\nAlgorithm 2 Generate Random Valid Path for Sliding Tile Puzzle\nRequire: n (board size), initial_state, max_steps\nEnsure: path from initial to final state\npath [initial_state]\ncurrent_state initial_state\nfor step = 1 to max_steps do\nneighbors get_neighbors(current_state, n)\ncurrent_state \u2190 random choice from neighbors\nAppend current_state to path\nend for\nreturn path"}]}