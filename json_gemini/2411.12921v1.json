{"title": "A Comparative Study of Text Retrieval Models on DaReCzech", "authors": ["Jakub Stetina", "Martin Fajcik", "Michal \u0160tef\u00e1nik", "Michal Hradis"], "abstract": "This article presents a comprehensive evaluation of 7 off-the-shelf document retrieval models: Splade, Plaid, Plaid-X, SimCSE, Contriever, OpenAI ADA and Gemma2 chosen to determine their performance on the Czech retrieval dataset DaReCzech. The primary objective of our experiments is to estimate the quality of modern retrieval approaches in the Czech language. Our analyses include retrieval quality, speed, and memory footprint. Secondly, we analyze whether it is better to use the model directly in Czech text, or to use machine translation into English, followed by retrieval in English. Our experiments identify the most effective option for Czech information retrieval. The findings revealed notable performance differences among the models, with Gemma2 achieving the highest precision and recall, while Contriever performing poorly. Conclusively, SPLADE and PLAID models offered a balance of efficiency and performance.", "sections": [{"title": "1 Introduction", "content": "Information retrieval (IR) is used in areas such as search engines and question-answering systems. Lately, we've seen advancements in IR models [12,16,32, inter alia], but picking the right one for a non-English document collection can be challenging. We address this gap for Czech language by doing a comprehensive comparison in our study. In particular, we utilize DaReCzech, a Czech retrieval and ranking dataset [14], for testing IR models to evaluate different IR models on Czech documents and queries. Our contributions are: (1) we analyze the index sizes to understand the storage requirements of various models, (2) we analyze the retrieval speed of such methods, to estimate how these models scale to large corpora in their default implementation, (3) we conduct ranking performance testing using multiple metrics on off-the-shelf models, and (4) we compare different model types, including those tested directly on the Czech dataset as well as on an English translation of the Czech dataset, keeping in mind the respective model's training data language, to provide insights into different approaches for indexing and retrieving czech. To the best of our knowledge, this is the first comparison study of existing state-of-the-art retrieval methods in the Czech language."}, {"title": "2 Related Work", "content": "Several well-known benchmarks have been used for evaluating information retrieval (IR) and text embedding models. MS MARCO [2] is widely used for passage and document retrieval, offering real-world web queries and answers. MIRACL [33] is a multilingual benchmark designed for retrieval across different languages. MTEB [18] provides a comprehensive evaluation across diverse tasks, including clustering, classification, and re-ranking.\nBeyond English-language benchmarks, several datasets focus on IR evaluation within specific linguistic contexts. For Czech, the CWRCzech dataset [28] includes 100M query-document pairs based on Czech click data from Seznam.cz search logs. German-language IR is explored through DPR German [19] and German LEGAL IR [29], which assess retrieval in general and legal domains. The SKQuad [17] dataset provides an IR benchmark specifically for the Slovak language and the Scandinavian Embedding Benchmark (SEB) [7] provides a comprehensive evaluation framework for text embeddings in Scandinavian languages. Within MTEB, Polish [23] and Chinese [30] datasets extend the evaluation to language-specific IR tasks.\nThe dataset utilized in our study is DaReCzech (see Subsection 4.1), introduced in [14], which is specifically tailored for the Czech language and consists of manually annotated query-document pairs. The relevance annotations in DaReCzech are not binary, allowing for a more nuanced evaluation of relevance ranking models and enabling the use of various evaluation metrics."}, {"title": "3 Model Descriptions", "content": "BM25 [24]. BM25 is a traditional lexical approach which has been widely used and had been the standard before the rise of neural models. It ranks documents based on a query's term frequency, inverse document frequency, and document length, meaning the importance of each term in the query and document is considered along with the document's length normalization, to produce relevance scores for each document.\nIn our study, we employed a BM25 baseline to assess the effectiveness of the other models. This model stands out as the only model with lemmatization applied to the query and document content, a distinction arising from the nature of BM25, which is not a neural model and relies on the precise lexical form of terms within the corpus.\nFor the Czech language, which features word inflection, lemmatization is essential for precise term matching and relevance ranking. Therefore, the lemmatized version of the corpus for BM25 is required.\nsplade-cocondenser-ensembledistil (SPLADE) [9]. (Sparse Lexical and Expansion Model) leverages sparse vocabulary-sized representations to leverage the advantages of BOW (bag-of-words). Splade operates by first applying a linear transformation to the BERT [6] output embeddings, then performing a dot product with the token embeddings from the whole vocabulary, resulting in a matrix of scores where each input token has a score for every token in the vocabulary\u00b9. While its predecessor, SparTerm [1], used a learned binary mask to select relevant scores from this matrix, Splade induces sparsity through a combination with a FLOPS regularizer and a logarithmic function during the representation computation. The final representation is obtained by summing the weights along the sequence tokens, producing a sparse embedding with a dimensionality equal to the vocabulary size. The second version of Splade improves this pooling mechanism to instead use the max for each token from the vocabulary. The model used in this comparison is the highest performing distilled version of splade as described in [8].\nColbertV2.0 (PLAID) [25]. The PLAID model represents a multi-vector approach. It extends the late interaction mechanism used in ColBERT [13] to enhance efficiency in information retrieval. In the original version of ColBERT, the comparison of query-document embeddings was performed by matching every token in the document embedding with every token in the query embedding, calculating scores using a maximum similarity function, where the highest similarity value for each query token across all document tokens is retained. ColBERTv2 [26] improved upon this approach by clustering the document embeddings into centroid clusters, thereby enabling a more efficient retrieval process. At search time, a fixed number of candidate clusters are selected, and their embeddings are decompressed to compute the final similarity scores.\nIn addition, PLAID, further enhances efficiency and performance by introducing a multi-stage candidate generation process. This approach includes steps for pruning and centroid-based interactions, progressively narrowing down the set of candidate passages. The final, smaller set of potential passages is then scored, resulting in a more streamlined and scalable retrieval pipeline. The model used in this study was trained on the English MS MARCO.\nPlaidx-xlmr-large-mlir-neuclir (PLAID-X) [20,31]. Multilingual version of PLAID, PLAID-X builds upon the ColBERT architecture and employs a multilingual encoder XLM-ROBERTa (XLM-R) for multilingual and cross-lingual encodings. The model used in this study was trained using the translate-train approach on Chinese, Persian, and Russian data, relying on the XLM-R encoder for cross-language mappings."}, {"title": "Text-embedding-ada-002 (OpenAI ADA) [22]", "content": "A closed-source model, that uses cosine similarity to compare two embeddings to calculate the resulting score.\nContriever-msmarco (Contriever) [10]. Contriever is a dense retrieval model that leverages self-supervised contrastive learning to effectively learn representations for information retrieval tasks. The model distinguishes between positive and negative passage pairs, where positive pairs are generated through independent window cropping from the original context (a document), and random token deletion. These approaches ensure that positive pairs share semantic content while exhibiting variation in phrasing, but also occasionally retaining lexical overlap. Negative pairs, on the other hand, are mined using MoCo [11], a method that builds a queue and uses a slowly changing encoder to generate negative samples. The model updates its document encoder by incorporating an online average of past parameters, ensuring that representations remain consistent across nearby training steps, hence making old representations stored in the queue compatible. This self-supervised training approach enables Contriever to produce dense vector representations for queries and documents, facilitating efficient retrieval and robust generalization across various retrieval tasks without the need for labeled datasets. The model used in this study was further fine-tuned on the English MS MARCO [2].\nSimcse-dist-mpnet-paracrawl-cs-en (SimCSE) [4, 10]. SimCSE employs contrastive learning to generate sentence embeddings, using simple dropout-based noise to create positive pairs from the same sentence while drawing negative pairs from other sentences within the batch. This approach trains the model to capture semantic similarities and differences between sentences without relying on large supervised datasets. Positive pairs are formed through augmentation techniques, such as random token deletion, replacement and masking, which introduce variability while preserving the original meaning. We chose to specifically test a model trained on the ParaCrawl [3] dataset (SimCSE-Dist-MPNet-ParaCrawl) as it achieved the highest DaReCzech performance at P@10 in the original work. The model used in this study was pre-trained using an undisclosed Czech dataset from Seznam.cz and distilled on czeng20-csmono [15] and Paracrawler v9 [3].\nBGE Multilingual Gemma2 (Gemma2) [5]. BGE-Multilingual-Gemma2 is a large-language model based multilingual embedding model. It is directly fine-tuned using contrastive objective on an undisclosed diverse set of languages and tasks based on google/gemma-2-9b model [27]. During evaluation, the prompt used was: \"Given a web search query, retrieve relevant passages that answer the query,\""}, {"title": "4 Experimental Setup", "content": "4.1 DaReCzech Dataset\nDaReCzech is a Czech dataset designed for text relevance ranking, comprising over 1.6 million query-document pairs. It is divided into Train-big (1.4M pairs for model training), Train-small (97K pairs for model training), Dev (41K pairs), and Test (64K pairs), with no overlap between splits. Each record includes a query, URL, document title, document body text extract (BTE), and a relevance label. Queries are real user inputs, with minor corrections, and the documents are preprocessed to exclude irrelevant sections, ensuring a cleaner representation of content for ranking tasks.\nWe utilized DaReCzech by selecting test queries along with their associated relevant documents and additional documents to create a 100,000-document sample for indexing. This approach allowed us to maintain a representative document pool without indexing the entire dataset, primarily due to computational and economical overhead. Specifically, for the OpenAI Ada model, embedding generation incurs a cost. This balanced approach enabled a comprehensive evaluation while managing resource expenditure effectively. For relevance scores, we classified documents with scores above 0 as relevant and those with a score of 0 as non-relevant, aligning with binary metrics like precision and recall. \n4.2 BM25 grid search\nFor fair comparison, we ran a grid search on the development set within our corpus to find the most optimal setting of the BM25's hyperparameters. The performance of the BM25 model was most optimal when the document length normalization parameter B was set to its maximum value of 1.0. This adjustment highlighted the importance of document length normalization in our particular case. The K\u2081 parameter, saturation of term frequency, showed minimal impact on our corpus, suggesting that its tuning had little to no effect on the performance. Based on this experiment, the BM25 hyperparameters were set to [K1, B] = [2, 1].\n4.3 Dataset Translation\nSome of the models tested were primarily or exclusively trained on English data. To achieve optimal performance and ensure a fair comparison, we applied document-level translation to the DaReCzech corpus, translating it into English using OPUS-MT, a multilingual translation model based on the OPUS corpora 3. This approach allowed us to evaluate all models in their supported language setting.\n4.4 Segmentation\nFor the purpose of our evaluation, we employed a common indexing methodology across all models. For an initial experiment, we indexed documents in two ways: using only a truncated section up to each model's maximum input limit, and as multiple overlapping segments for longer documents, thus running the evaluation with two separate indices for each model4. However, since the overlapping approach did not yield any significant improvement, as can be seen in Figures 1 and 2, the later tested models were evaluated using non-overlapping segments only. The overlapping segments revealed an inherent bias of DaReCzech, as all the important data were usually concentrated at the beginning of the documents. This was also indicated by our extra analysis in Appendix C, making the cutoff method with no overlap sufficient. The cutoff lengths for each model were derived from the respective model papers, and a stride (if used) was selected to be roughly one-third of the maximum token length for each model"}, {"title": "5 Results and Analysis", "content": "The precision and recall values for all models across different k values exhibit distinct patterns, with Contriever performing the worst, even below BM25. The stricly top-performing model is Gemma2. Notably, both the Czech and English versions of Gemma2 rank highly, with the Czech model showing a slight advantage in performance. Beneath Gemma2, the best results come from the PLAID models. However, segmenting the documents with these models demonstrates a decline in both precision and recall as k increases, possibly due to an accumulation of irrelevant information from segment-level retrievals impacting the overall ranking quality5.\nThese observed trends are even more evident in the full MRR and NDCG metrics, where the differences among models are more pronounced. In the MRR graphs, nearly consistent performance across different k values indicates that while the general retrieval ability remains stable, the ranking quality of results varies significantly between models. The superior performance of Gemma2 and the relative weaknesses of Contriever are reflected here, reinforcing the patterns observed in the precision and recall figures. This alignment suggests that models with higher precision and recall also exhibit better ordering and ranking capabilities, as demonstrated by their MRR and NDCG scores.\nFigure 3a demonstrates the trade-off between document size (estimated by averaging the size of each index by the number of indexed documents) and retrieval precision. As anticipated, BM25 maintains a compact document representation but exhibits a low Precision@5 performance, with Contriever faring even worse. PLAID-X achieves modest gains over BM25, with a smaller index size per document due to a restrictive 180-token limit. SPLADE, while comparable to PLAID-X in precision, maintains a much smaller index thanks to its sparse nature 6. The original PLAID model, without cross-lingual settings, slightly outperforms both PLAID-X and SPLADE, though it incurs a larger index size due to a higher token limit of 300. OpenAI's Ada model struggles to compete, hindered by its large embedding dimension, resulting in a substantial index size that does not justify its middling performance. The Gemma2 model emerges as the top performer, albeit with the largest embedding size, indicating a trade-off between high retrieval accuracy and storage requirements. Such result is aligned with observations in [21], where authors demonstrate that embedding performance tends to scale with model size and embedding dimension.\nAn analysis of query latency in Figure 3b shows that BM25 achieves the fastest query times, which aligns with its straightforward term-matching approach. Contriever and SimCSE, both using single-vector embeddings and cosine similarity, follow closely. The PLAID-X and PLAID models exhibit slightly longer latencies, likely due to their multi-stage retrieval process, which involves candidate selection and more complex ranking steps, contributing to a moderate increase in query time. SPLADE and Gemma2 are slower still; SPLADE's sparse representation requires additional computation to dynamically calculate sparse scores, while Gemma2's high-dimensional embeddings impose added processing overhead. These patterns suggest that models with multi-stage or complex scoring mechanisms naturally incur higher latency compared to more direct embedding or term-based approaches.\nRegarding the overlap among the models, as depicted in Figure 4, Contriever consistently exhibits the lowest overlap scores across comparisons with other models, a finding that aligns well with its previously observed underperformance in Figures la and 1b. Notably, PLAID and PLAID-X display a high degree of overlap and strong Kendall correlation, likely attributable to their shared architecture and training approach, with PLAID-X being a multilingual adaptation of PLAID. Interestingly, we also observe a notably high overlap value between PLAID and GEMMA, which could be attributed to GEMMA's training on diverse multilingual data that likely includes features common to PLAID's retrieval methodology."}, {"title": "6 Conclusion", "content": "In this paper, we evaluated 7 off-the-shelf information retrieval models on the DaReCzech corpus, comparing their performance against the traditional BM25 approach. The goal was to identify the most effective model for information retrieval in Czech.\nOur findings showed that Gemma2 consistently delivered the best precision and recall metrics across various k values, with the Czech version slightly outperforming the English one. However, its high retrieval accuracy came with a large index size due to high-dimensional embeddings exceeding even the multi-vector models. In contrast, BM25 and Contriever exhibited the poorest performance, with Contriever notably underperforming and struggling to match BM25's baseline.\nSPLADE and the PLAID models offered a balance between performance and efficiency. SPLADE's sparse representation resulted in the smallest index size, making it suitable for resource-constrained applications, while the PLAID models, especially the original, provided higher precision with modest increases in index size. The ColBERT-based models performed well unsegmented, but segmenting for long documents led to a decrease in performance as k increased.\nFor Czech-language IR tasks, Gemma2 is recommended if accuracy is the top priority and storage is manageable. SPLADE is a practical choice when memory efficiency is crucial, and PLAID/PLAID-X offer a middle ground, particularly with token limit adjustments. This study underscores the trade-offs between model complexity, storage, and retrieval quality, guiding suitable model selection for Czech-language IR."}, {"title": "A.1 Evaluation Metrics", "content": "To assess the performance of these IR models, we employ a range of standard evaluation metrics:\nPrecision Precision quantifies the accuracy of relevant documents in the retrieved set and is defined as:\nPrecision = $\\frac{{\\mid {relevant documents} \\cap {retrieved documents}\\mid}}{{\\mid {retrieved documents}\\mid}}$ (1)\nRecall Recall measures the ability of the model to retrieve all relevant documents from the corpus and is given by:\nRecall = $\\frac{{\\mid {relevant documents} \\cap {retrieved documents}\\mid}}{{\\mid {relevant documents in corpus}\\mid}}$ (2)\nMRR (Mean Reciprocal Rank) Mean Reciprocal Rank (MRR) evaluates the ranking quality by taking the mean of the reciprocal ranks of the first relevant document for each query:\nMRR = $\\frac{1}{\\mid Q\\mid} \\sum_{i=1}^{Q} \\frac{1}{rank_i}$ (3)\nwhere rank, is the position of the first relevant document for the i-th query and |Q| is the total number of queries.\nMAP (Mean Average Precision) Mean Average Precision (MAP) calculates the average precision for each query and averages these scores across all queries, thereby reflecting the model's ranking consistency. For a given query q, the average precision is:\nAPq = $\\frac{1}{{|{relevant documents for q}|}} \\sum_{k=1}^{N} Precision(k) \\cdot rel(k)$ (4)\nwhere N is the total number of documents, Precision(k) is the precision at rank k, and rel(k) is a binary indicator of relevance at rank k. MAP is then:\nMAP = $\\frac{1}{\\mid Q\\mid} \\sum_{q=1}^{Q} AP_q$ (5)\nnDCG (Normalized Discounted Cumulative Gain) Normalized Discounted Cumulative Gain (nDCG) evaluates the ranked list's quality by considering the position of relevant documents in the ranking. For a query q, nDCG at rank p is calculated as:\nDCGp = $\\sum_{k=1}^{p} \\frac{2^{rel(k)} - 1}{log_2(k+1)}$ (6)\nnDCGp = $\\frac{DCG_p}{IDCG_p}$ (7)\nwhere rel(k) is the relevance score of the document at rank k, and IDCGp is the ideal DCG, obtained by sorting documents in the perfect order of relevance."}, {"title": "A.2 Additional Evaluation Criteria", "content": "In addition to the standard metrics, we also consider two specific criteria:\nRepresentation Size Examining the memory footprint required by each model's document representations (measured per document as kB/doc),\nQuery Latency Query latency refers to the duration taken by an information retrieval system to retrieve and present relevant documents in response to a given query.\nKendall's T Rank Correlation and Lexical Overlap assessing the consistency of ranking across the models on the top 100 retrieved results for each query helps understanding how well the models agree on the most relevant documents"}, {"title": "\u0412 BM25 Hyperparameter Tuning", "content": "We perform a BM25 grid search to tune the K1 and B parameters for optimal results on the corpus. The results from the grid search are visualized in Figure 5."}, {"title": "C ColBERT's Token-level Focus", "content": "To estimate which parts of the document are important, the study analyzed ColBERTv2, a model that uses a multi-vector approach, where each token in a document is represented by a separate vector. By examining the vectors of the retrieved documents, tokens with the most interaction with the query were identified. This might indicate which specific parts of the documents were most relevant to the query and contributed to the retrieval process for the given document.\nThis experiment examined two modes for the document-query scores (samples with scores aggregated from Colbert's similarity matrix through max-pooling over query token representations (in contrast with the original Colbert's MaxSim operation which computes max-pooling over document token representations) can be seen below in Figure 6) visualized as a probability distribution using the softmax function with the brighter color denoting higher similarity score):\nMaxSim operation: particularly chosen as it reflects how the model selects positive document-query pairs, and identifies the best score for each query token with the highest scoring document token, highlighting the most significant interactions."}]}