{"title": "KOALA : Knowledge Conflict Augmentations for Robustness in Vision Language Models", "authors": ["Peter Carragher", "Nikitha Rao", "Abhinand Jha", "R Raghav", "Kathleen M. Carley"], "abstract": "The robustness of large language models (LLMs) against knowledge conflicts in unimodal question answering systems has been well studied. However, the effect of conflicts in information sources on vision language models (VLMs) in multimodal settings has not yet been explored. In this work, we propose KOALA, a framework that applies targeted perturbations to image sources to study and improve the robustness of VLMs against three different types of knowledge conflicts, namely parametric, source, and counterfactual conflicts. Contrary to prior findings that showed that LLMs are sensitive to parametric conflicts arising from textual perturbations, we find VLMs are largely robust to image perturbation. On the other hand, VLMs perform poorly on counterfactual examples (< 30% accuracy) and fail to reason over source conflicts (< 1% accuracy). We also find a link between hallucinations and image context, with GPT-40 prone to hallucination when presented with highly contextualized counterfactual examples. While challenges persist with source conflicts, finetuning models significantly improves reasoning over counterfactual samples. Our findings highlight the need for VLM training methodologies that enhance their reasoning capabilities, particularly in addressing complex knowledge conflicts between multimodal sources.", "sections": [{"title": "Introduction", "content": "Recent advancements in vision language models (VLMs) have led to AI assistants capable of Visual Question Answering (VQA). Given few image sources and a text-based question, a VQA system generates a relevant response by interpreting the content in the images, and understanding the intent of the question. Prior work has found that unimodal question answering (QA) models are not robust to knowledge conflicts that arise between parametric knowledge (encoded in the model weights during training) and contextual knowledge (external knowledge sources given to the model) (Neeman et al., 2022). While a body of research improves the robustness of unimodal LLMs to conflicts (Longpre et al., 2022), multimodal robustness studies (Liu et al., 2024b) have not addressed multimodal conflicts (Xu et al., 2024). We aim to address this gap and investigate three different types of multimodal knowledge conflict in the VQA setting, namely, parametric conflicts (arising between the encoded knowledge and external input information source), source conflicts (between two input information sources) and counterfactual conflicts (such that a query cannot be answered with the given input information source), see Section 3.2. We propose KOALA, a framework to enhance the reasoning abilities of vision-language models (VLMs) over knowledge conflicts through constrained dataset augmentation. KOALA extends existing VQA datasets by introducing augmentations for each type of knowledge conflict. First, we generate parametric conflicts, where image perturbations alter attributes like the shape or color of the object in question, therefore changing the expected response (for example, replacing the color of the horse, as demonstrated in Figure 1). Next, we generate counterfactual conflicts where image perturbations remove the object in question therein making it impossible to answer the question using the new image (for example, removing the bat from the child's hand and asking what the child is holding as demonstrated in Figure 7a). Lastly, we generate source conflicts where one of two image sources is modified to create a conflict that makes the image source inconclusive (for example, presenting the model with 2 images of the same room, where one of them was altered and asking the model for the color of the ceiling, as shown in Figure 3). We apply KOALA on three datasets, We-"}, {"title": "Related Work", "content": "Prior work on addressing parametric conflicts falls into two broad categories; the construction of evaluation datasets to quantify where and when conflicts occur, and method-based contributions to train QA models to overcome their reasoning limitations. Along these lines, our work extends diffusion models for conditional image generation to investigate knowledge conflicts in the multimodal setting.\nKnowledge Conflict Evaluation Recent work on evaluation has shown that LLMs are not robust to perturbations in text-based reasoning tasks (Zhang et al., 2024b; Mirzadeh et al., 2024; Zhu et al., 2023; Wang et al., 2024c) and that LLM performance degrades when conflicts exist in the source data for QA tasks (Xu et al., 2024; Wang et al., 2023). Longpre et al. (Longpre et al., 2022) introduced an entity-based knowledge conflict framework for evaluating how models handle conflicting information between learned parametric knowledge and contextual (non-parametric) data. Chen et al. (Chen et al., 2022) evaluate QA model on source conflicts. Hong et al. (Hong et al., 2024) induce hallucinations in retrieval-augmented models by introducing counterfactual noise, which they define as conflicting but contextually relevant information. They also find that retrieval-augmented models ignore conflicting sources.\nKnowledge Conflict Fine-tuning Attempts to address this reasoning gap in LLMs include finetuning on both human annotated (Hsu et al., 2021; Ko et al., 2023) and LLM generated (Pan et al., 2023; Li et al., 2024; Wan et al., 2024) datasets. Generative approaches involve extending a base dataset like SQUAD (Rajpurkar et al., 2016) to include sources with conflicting information (Li et al., 2022). Neeman et al. adopt a combination of prompting and entity-substitution techniques for data augmentation on textual QA datasets, producing the DisentQA(Neeman et al., 2022). Recent work demonstrates that LLMs can be trained to retrieve more relevant context when the parametric information and provided sources are insufficient (Labruna et al., 2024; Wang et al., 2024a). However, these methods do not focus on multimodal QA tasks (Xu et al., 2024) and our work builds on these foundations by fine-tuning VLMs with knowledge conflicts to recognize when visual evidence is insufficient to complete the VQA task."}, {"title": "Methodology", "content": "KOALA is a framework designed to enhance the robustness of VLMs by augmenting existing VQA datasets with the intention of introducing knowledge conflicts using perturbed images. Quality checks ensure that noisy perturbations are filtered out before we finetune models on the generations. Model performance is then evaluated on both the original and perturbed datasets. Finally, we analyze the effect of image-question contextualization on hallucination rate for counterfactual conflicts."}, {"title": "The KOALA Framework", "content": "Figure 1 gives an overview of the framework. First, given a QA pair with image sources $i_1, ..., i_n$, we prompt Gemini-1.5-flash to extract the noun that functions as the object of the question. We then prompt the Segment Anything Model v2 (SAMv2) (Ravi et al., 2024; Liu et al., 2024c) to segment the object of the question in each of the images $i_1,..., i_n$. Finally, we apply a perturbation to the segmented regions by either removing the object from the image using Large Mask Inpainting (LaMa) (Suvorov et al., 2022) or changing the color or shape of the object using Stable Diffusion (Rombach et al., 2022). These perturbations are used to generate different kinds of augmentations that enable us to study the reasoning ability of the models on the three types of knowledge conflict."}, {"title": "Knowledge Conflict Types", "content": "We look at three main types of conflicts between different sources of information, and study the reasoning abilities of different models on them.\n(i) Counterfactual conflicts: We introduce conflicts between the query and image source. We do so by removing the object in question from the image source to invalidate the premise of the question. As a result, any answer except for requests for more information, or statements about lacking information (IRET) are incorrect (Figure 2).\n(ii) Parametric conflicts: Here we introduce conflicts between the encoded knowledge (embedded in the learned weights) and an input information source, in this case the perturbed image. To study this effect, we alter attributes like the shape or color of the object under consideration in the image, therefore changing the expected response to the new label, $I_{new}$. This requires the model to rely on the new image and ignore any learned knowledge it may have about the image to answer the question correctly (for example, Figure 3).\n(iii) Source conflicts: We introduce conflicts between the sources of information, in this case between multiple image sources, such that the question becomes unanswerable. For multihop questions (i.e. questions with two image sources), we augment that dataset by combining the perturbed variant of one of the two images with the original version of the other i.e. (image 1, perturbed image 2) and vice versa, therein introducing a conflict that makes the question unanswerable and therefore making retrieval token $IRET$ the only correct response (see Figure 3).\nNote, we adopt the concept of the retrieval token $IRET$ from Labruna et. al.(Labruna et al., 2024)."}, {"title": "The Knowledge Conflicts Dataset", "content": "Existing VQA datasets do not include examples with conflicting sources of information. To address this gap, we take three popular VQA datasets, WebQA (Chang et al., 2022), VQAv2 (Goyal et al., 2017), and OK-VQA (Marino et al., 2019) (see Table 1), and augment them with knowledge conflicts by perturbing the image sources and updating the expected answers using the KOALA framework.\nUnlike WebQA, where questions fall into specific categories (color, shape, yesno, number), VQAv2 on OK-VQA are open-domain tasks. As a result, we can use feature modifications to generate parametric conflicts only for the WebQA dataset (as in Figure 1, Figure 3). In addition, since source conflicts require two images, we only generate them for the multihop portion of the WebQA dataset. We cannot generate source conflicts for VQAv2 and OK-VQA as they are single-image VQA tasks. Lastly, we generate samples with counterfactual conflicts for all three datasets.\nTable 2 gives a breakdown of the samples generated for each dataset along with the method used. Note that for every perturbed sample, we also keep the corresponding original, unperturbed samples from each of the constituent datasets. This ensures that models finetuned on the generated knowledge conflicts dataset learn to discriminate between conflicting and counterfactual sources, while also learning to answer questions on the original image samples. 38% of the resulting generations have the answer $IRET$."}, {"title": "Quality Checks", "content": "The generative methods used for perturbing images are imperfect. We therefore apply quality checks to filter out the noisy generations before finetuning VQA models. We present each generated sample to a quantized Qwen2-VL-7b-Instruct VLM and ask whether the modified feature is the same (or for object removal, whether the object exists), in both the original and perturbed images. Framing the question in this way eliminates bias towards affirmative responses. Manual evaluation of the quality-checked images finds that they are indeed high quality (Table 2). Quality checks prompts are listed in the supplementary (Appendix B)."}, {"title": "Finetuning on knowledge conflicts data", "content": "To evaluate the KOALA frameworks efficacy in developing VLM robustness, we finetune three VLMs on the generated knowledge conflicts data-Llava-1.5-7b (Liu et al., 2024a), Phi3-vision-128k-instruct (Abdin et al., 2024), and Qwen2-VL-7B-Instruct (Wang et al., 2024b). All models are finetuned on the training set (Table 2) for 1 epoch on 2x NVIDIA RTX A6000 GPUs using SWIFT (Zhao et al., 2024), with convergence shown in the appendix (Figure 8). Subject to resource limitations, we apply LoRA (Hu et al., 2021) to reduce GPU memory requirements and use Distributed Data Parallel methods DeepSpeed (Rasley et al., 2020) and ZeRO (Rajbhandari et al., 2020) to train across multiple GPUs. Refer to Table 3 for hyperparameters."}, {"title": "Evaluation", "content": "We compare performance of the finetuned versions of the VLMs against their base versions on the KOALA validation set (Table 2). We also evaluate on-Llava-1.5-13b (Liu et al., 2024a) and GPT-40-mini (Achiam et al., 2023).\nEvaluation on KOALA Generations We measure the VLM's reasoning ability over conflicting sources of information with the following accuracy scores (see Appendix E for details)\u2014\nParametric response rate: % of model responses that incorrectly predict the original label when a color or shape attribute has been changed. Therefore, highlighting the effect of parametric conflicts on model performance by showcasing the model's over reliance on the encoded parametric knowledge instead of adapting to the modified image source.\nAccuracy for counterfactual conflicts: % of model responses that correctly generate IRET or any response which acknowledges the models failure to answer on the set of counterfactual samples.\nAccuracy for source conflicts: % of model responses that correctly generate IRET or any response which acknowledges the models failure to answer on the set of source conflicts. See Table 5 in the supplementary for the 'acknowledgment' phrases we parse from model responses.\nEvaluation on Original Samples We evaluate model accuracy on original samples to check for performance regressions on the original VQAv2, OK-VQA, and WebQA validation sets that may occur as a result of finetuning. Accuracy scores on the original samples are simply the % of model responses that generate the original labels in each dataset when presented with the original, unperturbed images. These results are reported alongside accuracy scores for the knowledge conflict tasks.\nRobustness on Counterfactuals Counterfactual conflicts are generated using LaMa. To ensure that our finetuned models do not learn to predict"}, {"title": "Qualitative Results", "content": "After generating a large number of samples (>200,000), we apply quality checks to remove noisy generations, resulting in approximately 35, 225 samples. See Figure 2 for examples of counterfactual image generations and Figure 3 for parametric and source conflicts.\nTwo raters independently labeled a subset of 100 quality-checked generations for each category of conflicts to determine if the new label (IRET OR Inew) matches the perturbed image\u2014see label quality ratings in Table 2. Counterfactuals have a higher quality rating (>90%). Parametric (76%) and source conflicts (82%) produce more noisy generations which we attribute to the increased difficulty in replacing an object versus removing it. Raters only disagreed on a small fraction of samples (30/300), while a Cohen's Kappa of 0.45 reflects that disagreements happened only on lower quality generations (Delgado and Tibau, 2019)."}, {"title": "Quantitative Results", "content": "In Figure 5 we find that baseline VLMs fail to acknowledge counterfactual conflicts (Counter) and source conflicts (Source). Finetuning mitigates this across every dataset. The resulting finetuned models (-Ft) outperform the baseline models (-Base) on perturbed samples. Finetuning has some benefit on the original samples (Original) for VQA and WebQA counterfactual sources, but a large performance regression is apparent for samples with source conflicts in WebQA.\nSource Conflicts For WebQA samples with source conflicts, the finetuned models have extremely low accuracy on original samples. This is a result of the finetuned models failing to predict the old label and instead overpredicting the IRET when presented with two images. Interestingly, instead of generating an 'acknowledgement' response, baseline models tend to predict one of the two incorrect answers\u2014either the original label (for the unperturbed image) or Inew (for the perturbed image)\u2014uniformly at random.\nCounterfactual Conflicts Baseline models perform poorly on counterfactual conflicts, with no model achieving more than 30% accuracy. Since these models are not trained to return the IRET, we consider any admission of failure by the model as a IRET. These baseline models are sometimes able to determine when an image lacks the information required to answer a question, they are not robust to these samples. Finetuning on enables these models to identify counterfactual conflicts with high accuracy, without degrading performance on the original datasets. Additionally, finetuning provides a 5-10% performance gain on the original samples from WebQA and VQA datasets.\nRobustness of Counterfactual Conflicts We find that finetuned models are robust in detecting randomized counterfactual samples. They are not simply detecting images that have been modified by LaMa to remove objects. The finetuned Qwen2 model predicts IRET for 80% of the randomized counterfactuals sampled from the WebQA dataset.\nParameter Size We find that performance improvements on the evaluation metrics derived from increasing model size have diminishing returns. There exists a gap in performance between SOTA models (i.e. GPT-40-mini) and the finetuned models (see Figure 9 in the supplementary).\nImage-Question Contextualization Intuitively, image-question contextualization relates to contextual cues within an image that provides the models with clues to answer the question, as in Figure 7. We find evidence for a link between image-question contextualization, as approximated by GPT-40-mini, and accuracy on counterfactual samples. Figure 6 reveals that models perform poorly in identifying a sample as counterfactual (i.e. lower accuracy of predicting IRET) and is more likely to hallucinate on heavily contextualized image question pairs. Interestingly, GPT-40-mini hallucinates for all of the counterfactual examples given in Figure 2.\nFor a concrete example, see Figure 7, where both counterfactual examples were generated by removing a baseball bat. Here, a poorly contextualized image question pair features a child standing in a field with the question \"what is he holding?\" (Figure 7a). The only contextual cues as to what the child might have been holding are the generic outdoor setting, and the child's body positioning. Contrasting this in the adjoining sample is a baseball player, adorned in a jersey with his player number printed on the back, in a stadium filled with sporting fans (Figure 7b). ChatGPT recognizes that the child is holding nothing, but hallucinates a bat in the hands of the batsman. Alongside previous works that show a relationship between image context and object detection (Beery et al., 2018), these results indicate that contextual cues have a priming effect that induces hallucinations in VLMs for highly contextualized counterfactuals."}, {"title": "Discussion", "content": "The KOALA framework extends research on reasoning with knowledge conflicts to the multimodal domain. The framework builds on the unimodal text-based Entity Replacement Framework (Longpre et al., 2022) and extends it to VQA by segmenting and modifying relevant entities and objects in images. Our perturbations are inspired from prior work on knowledge conflicts (Chen et al., 2022; Longpre et al., 2022) and counterfactual reasoning (Neeman et al., 2022; Hong et al., 2024) in LLMs.\nVLMs, like LLMs, may internalize statistical and factual knowledge from large-scale training data. This includes details such as the typical colors of specific bird and flower species, (Figure 3), or even historical facts such as the color of the horse that Eli Bremer rode in the 2008 Summer Olympics (Figure 1). We measure the degree to which VQA models prioritize these parametric facts over the information contained in input sources. Whereas LLMs have been shown to exhibit strong parametric tendencies, we find that this is not the case for VLMs. As seen, parametric response rates are low, ~20% across all models tested (Figure 4).\nOur core contributions lie in our analysis of model robustness to different types of knowledge conflict (Figure 5). Without finetuning, models such as GPT-4o ignore the counterfactual sources and instead hallucinate (Figure 6). While the counterfactual reasoning task may seem unreasonable as hallucinations could represent the correct answer for common-sense questions, we highlight that the utility of counterfactual samples is that they reveal a significant gap in understanding between human and machine performance. For instance, it is immediately obvious to a human that examples in Figure 2 are unusual. The fact that this is not obvious to VLMs motivates our framework and dataset.\nThe ease of construction and availability of paired image-caption data has made it vital for image summarization tasks. As such, our framework is also motivated by a broader challenge: an over reliance on paired image-caption data and contrastive loss functions for training VLMs. While these image-caption helps models learn to reason about what is in the image, we find that models struggle with reasoning about what is not in the image. Our work aims to correct the counterfactual reasoning gap by paving the way for counterfactual samples to be integrated into the training process.\nWe demonstrate that counterfactual reasoning in VLMs is conditional on the sources presented. Reasoning over \u2018randomized negatively sampled counterfactuals\u2019 (i.e. a question and an unrelated image) appears trivial for both base and finetuned models (Table 4). However, cases with high image-question contextualization present interesting insights as they trigger hallucinations in even the most advanced VLMs. This link between hallucinations and highly contextualized counterfactual samples underlines the value of our framework and dataset for multimodal reasoning.\nWithout our framework, such samples are difficult and costly to collect. Our methodology provides a systematic way for future work to build on counterfactual reasoning, source conflicts, and hallucinations in the multimodal setting. Future work may center around developing more sophisticated sets of generative constraints, extending the KOALA framework and dataset to tackle aspects of visual reasoning that continue to be underrepresented in VQA datasets."}, {"title": "Conclusion", "content": "We introduce KOALA, a framework designed to improve the robustness of visual reasoning in VLMs. Through the application of image segmentation and inpainting techniques, we augment VQA datasets with parametric, source and counterfactual conflicts. These samples test LLMs' abilities to recognize and respond to various types of image-based reasoning challenges. While our experiments demonstrate VLM resilience to perturbations that lie within their training distribution (i.e. feature modifications that induce parametric conflicts), they struggle with counterfactual cases and conflicts across multiple image sources, especially in multi-hop scenarios. Our findings highlight the need for VQA models that are robust to knowledge conflicts and we hope that our contribution will inspire future research in advancing visual reasoning."}, {"title": "Limitations", "content": "Our framework effectively generates and evaluates parametric, source, and counterfactual conflicts across VQA datasets. However, three key limitations may affect its generalizability: reliance on VLMs for quality checks, residual and generative artifacts, and image-question contextualization.\nFirst, we rely on smaller quantized VLMs for quality assurance which may introduce an additional source of error. A fine-grained visual and semantic understanding in the VLM could lead to overlooked errors in perturbation or segmentation that affect the dataset's overall quality. Although we manually review a subset of outputs from each perturbation type to gauge quality, the effectiveness of quality control could be enhanced by leveraging more powerful models or ensemble-based methods. We also note the possibility of the quality-check ruling out high quality generations. However, this is less of a concern as we wish to minimize false positives in the dataset, and we can compensate simply by generating more samples.\nSecond, handling residual artifacts left after object removal, like shadows or reflections, is challenging. These artifacts can indicate the previous presence of objects, introducing noise and inconsistencies that may mislead models that are sensitive to visual details. While we mitigate this partially through manual evaluation and quality checks, future work could explore advanced inpainting or shadow removal for cleaner counterfactuals.\nCurrent generative methods suffer from quality issues, with artifacts like blurred infilled regions and excessive noise in segmented areas, despite high quality ratings across perturbation categories. Emerging text-to-image editing models (Hui et al., 2024; Bodur et al., 2023; Zhang et al., 2024a) may help address these issues. While we employ a rule-based segmentation approach, these models dynamically infer infill regions from input prompts. Given the lower quality ratings for knowledge conflict perturbations, future work should explore new generative methods to improve this aspect.\nFinally, our analysis of these hallucinations follows a naive approach where image-question contextualization is determined by GPT-4o-mini. Alternatively, generating question sets for each image and computing text similarity with dataset questions could enhance contextualization. Informed by our findings on VLM hallucinations, future work is needed to refine this approach (Figure 6)."}, {"title": "Model Finetuning", "content": "Hyperparameters for the finetuned models are given in Table 3. Note: Clip-vit refers to openai/clip-vit-large-patch14-336. Convergence of training loss within one epoch for Qwen2 and Phi3 is shown in Figure 8."}, {"title": "Prompts", "content": "Prompts for QA checks and image-question context evaluation are listed here\u2014namely the counterfactual QA check, the feature modification QA check, and the image-question contextualization prompt."}, {"title": "Larger VLMs", "content": "Finally, we include the accuracy of two additional baseline models, Llava-1.5-13b and GPT-4o-mini, on both the original VQA tasks and the various tasks in the KOALA dataset (Figure 9). As previously discussed, performance improvements from larger baseline VLMs are limited (Llava-7b vs Llava-13b). None of the baseline models are capable of matching the performance of KOALA finetuned models."}, {"title": "Robustness Checks", "content": "As models are not trained on irrelevant images, randomly sampling negative image query pairs from across our three datasets is an out-of-distribution task. This evaluates the robustness of our finetuning process on the more trivial cases where the image and query are irrelevant. Table 4 shows the full set of results, which as previously discussed reveal that finetuned models have improved performance compared with baseline models. The list of 'acknowledgment' terms we consider as admissions of failure to reason over an image query pair due to incomplete information are given in Table 5."}, {"title": "WebQA Accuracy", "content": "Accuracy on the WebQA task is determined by comparing a restricted bag of words (bow) vector between the expected (E) and generated (G) answers;\n$Acc = \\frac{1}{n} \\sum_1^n [\\frac{bow_E \\cap bow_G}{bow_E} \\ge 1] $ (1)\nThe vectors' vocabulary is limited to a domain determined by the question type. Questions are classified into domains such as yes/no, color, shape, or number, and each domain uses a predefined vocabulary list (see Figure 10)."}]}