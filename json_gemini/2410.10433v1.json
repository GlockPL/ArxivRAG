{"title": "LKASeg:Remote-Sensing Image Semantic Segmentation with Large Kernel Attention and Full-Scale Skip Connections", "authors": ["Xuezhi Xiang", "Yibo Ning", "Lei Zhang", "Denis Ombati", "Himaloy Himu", "Xiantong Zhen"], "abstract": "Semantic segmentation of remote sensing images is a fundamental task in geospatial research. However, widely used Convolutional Neural Networks (CNNs) and Transformers have notable drawbacks: CNNs may be limited by insufficient remote sensing modeling capability, while Transformers face challenges due to computational complexity. In this paper, we propose a remote-sensing image semantic segmentation network named LKASeg, which combines Large Kernel Attention(LSKA) and Full-Scale Skip Connections(FSC). Specifically, we propose a decoder based on Large Kernel Attention (LKA), which extract global features while avoiding the computational overhead of self-attention and providing channel adaptability. To achieve full-scale feature learning and fusion, we apply Full-Scale Skip Connections (FSC) between the encoder and decoder. We conducted experiments by combining the LKA-based decoder with FSC. On the ISPRS Vaihingen dataset, the mF1 and mIoU scores achieved 90.33% and 82.77%.", "sections": [{"title": "I. INTRODUCTION", "content": "With the widespread application of deep neural networks in computer vision, significant advancements have been made in semantic segmentation of natural images, achieving remarkable results. Notably, Long et al. [1] introduced Fully Convolutional Networks (FCN), which enable end-to-end dense pixel prediction. However, the overly simplistic upsampling in FCN leads to less precise segmentation results. Subsequently, Ronneberger et al. [2] proposed UNet, an encoder-decoder network with a symmetric structure. UNet improves segmentation accuracy by using skip connections to compensate for the loss of feature information during downsampling. However, UNet merely concatenates low-level features generated by the encoder with high-level features from the decoder, without further refining these features, which limits the network's ability to differentiate between low-level and high-level features.\nMethods based on convolution, such as FCN and UNet, lack the ability to capture global information. In high-resolution remote-sensing image, where complex objects frequently appear, the absence of global semantic information makes precise segmentation challenging. To capture global semantic information, one solution is to incorporate transformer-based self-attention mechanisms. Existing transformer-based semantic segmentation methods for remote-sensing image can be broadly categorized into two types. The first type consists of encoder-decoder architectures composed entirely of transformers, such as SegFormer [3]. The second type is hybrid encoder-decoder architectures combining CNNs and transformers. For example, Wang et al. [4] proposed a Bidirectional Attention Network (BANet) with dependency and texture pathways, where the dependency pathway uses transformers to capture long-range dependencies, and the texture pathway uses CNNs to extract local texture features. Wang et al. [5] introduced UNetFormer, a transformer-like architecture that innovatively employs a hybrid design, combining a CNN-based encoder with a decoder based on Global-Local Transformer Blocks (GLTB). Another approach involves using large kernel convolutions [6] to build correlations and generate attention maps to capture global information.\nHowever, large kernel convolutions and self-attention still have their drawbacks. Large kernel convolutions incur significant computational overhead and parameter costs, and the static nature of convolutional weights lacks adaptability. Guo et al. [7] noted that self-attention was originally designed for one-dimensional NLP tasks, treating two-dimensional images as one-dimensional sequences, which disrupts the critical two-dimensional structure of images. Additionally, the quadratic computation and memory overhead of self-attention make it challenging to handle high-resolution images. Furthermore, self-attention is a mechanism that only considers spatial adaptability while neglecting channel-wise adaptability. We address these issues by introducing Large Kernel Attention (LKA), which leverages the advantages of both self-attention and convolution. LKA combines the benefits of convolution and self-attention, including local structural information, long-range dependencies, and adaptability, while also overcoming the limitation of self-attention's lack of channel-wise adaptability.\nIn practical applications, remote sensing images often contain numerous objects, with similar objects arranged densely, differing greatly in size, color, and texture. The issue of scale variation in remote sensing images is thus a significant concern. Spatial detail information is essential for accurate semantic segmentation. Convolutional Neural Networks (CNNs) can easily lose spatial and boundary information during the feature extraction phase of semantic segmentation. Research on segmentation shows that feature maps at different scales provide different types of information. Huang et al. [8] noted that lower-level detailed feature maps capture rich spatial information, while higher-level semantic feature maps contain positional information. TransUNet adopts hybrid visual transformers [9] as encoders to enhance feature extraction and achieves state-of-the-art results in medical image segmentation. Li et al. [5] proposed UNetFormer, which uses skip connections of the same scale between the encoder and decoder to mitigate the loss of spatial details. Wu et al. [10] introduced CMTFNet, which employs multi-scale transformers for multiscale feature learning and fusion. However, these methods do not directly integrate feature maps of different scales from a full-size perspective. In this paper, we use full-size skip connections to address the issues of scale variation and spatial information loss in remote sensing image segmentation tasks.\nThe contributions of this work can be summarized as follows:\n1.  We propose a remote-sensing image semantic segmentation network named LKASeg. We introduce a decoder based on Large Kernel Attention (LKA), which extract global features and provide channel adaptability while avoiding the computational overhead of self-attention.\n2.  We apply Full-Scale Skip Connections (FSC) between the encoder and decoder in LKASeg to achieve full-scale feature learning and fusion, addressing issues of scale variation and spatial information loss in remote sensing image semantic segmentation tasks.\n3.  We conducted experiments by combining the LKA-based decoder with FSC. On the ISPRS Vaihingen dataset, the mF1 and mIoU scores achieved 90.33% and 82.77%."}, {"title": "II. METHODOLOGY", "content": ""}, {"title": "A. Overall Architecture", "content": "The overall architecture of LKASeg is shown in Fig. 1. LKASeg consists of three main components: an encoder based on ResNet-18, a decoder based on Large Kernel Attention (LKA), and full-size skip connections (FSC). We chose the pretrained ResNet-18 as the encoder to extract multiscale semantic features at very low computational costs. ResNet-18 consists of four stages of Resblocks, each downsampling the feature maps by a factor of 2. We employed the same Feature Refinement Head (FRH) design as the baseline. Additionally, we aggregate the semantic features generated by each layer of Resblocks with those generated by the LKA-based decoder below using a weighted sum operation. This operation selectively weights the contributions of both features to segmentation accuracy, thereby learning more generalized fused features. The weighted sum operation is as:\n$F_f = \\alpha \\times F_R + (1 - \\alpha) \\times F_L,$\nwhere $F_f$ is the fused features, $F_R$ is the features generated by Resblocks, and $F_L$ is the feature generated by the decoder blocks composed of LKA. A detailed description of the decoder, which is based LKA and FSC will show in the following sections."}, {"title": "B. LKA-based decoder", "content": "LKA is derived by decomposing a large kernel convolution. As shown in Fig. 2, a K \u00d7 K convolution can be decomposed into three parts: depthwise local convolution, depthwise dilated convolution with expansion factor d, and channel convolution 1 \u00d7 1 convolution. Specifically, a K \u00d7 K convolution can be decomposed into a depthwise expansion convolution, a (2d -1) \u00d7 (2d -1) depthwise convolution, and a 1 \u00d7 1 convolution. Through this decomposition, we can capture long-range dependencies with slight computational cost and parameters, and channel adaptability is achieved through 1\u00d71 convolution. After obtaining long-range dependencies, we estimate the importance of each point and generate attention maps. The operations of LKA module can be expressed as:\n$Attention = Conv1\\times1(DW-D-Conv(DW-Conv(F_i))),$\n$Output = Attention \\cdot F_i,$\nhere, $F_i \\in C \\times H \\times W$ represents the input features, Conv1\u00d71 represents 1 \u00d7 1 convolution, DW \u2013 D \u2013 Conv represents depth-wise dilation convolution, DW \u2013 Conv represents depth-wise convolution, denotes element-wise product, and $Attention \\in C \\times H \\times W$ denotes the attention map. The value in attention map indicates the importance of each feature. LKA combines the benefits of convolution and self-attention, considering local context information, large receptive fields, linear complexity, and dynamic processes. Moreover, LKA achieves adaptability not only in the spatial dimension but also in the channel dimension."}, {"title": "C. Full-Scale Skip Connections", "content": "To fully utilize feature information at all scales, we use FSC between features output by the encoder and those output by the decoder units. As shown in Fig. 1. Adjacent decoder units are connected via FSC pathways. First, features at four different scales are converted to a common scale for fusion. In our method, feature maps generated at each encoder stage are combined with the corresponding feature maps from the decoder through a weighted sum, and then processed through the LKA module, thus aggregating features from both the encoder and decoder. We use max pooling operations to reduce the dimensionality of the features and bilinear interpolation to scale up the feature dimensions. We employ linear layers to unify the number of feature channels, which helps in reducing redundant information and model parameters. Finally, four feature maps, harmonized in scale and channel count, are concatenated along the channel direction and then fed into the decoding unit for further feature fusion. Our FSC method leverages full-scale feature information, thoroughly integrating relationships between different levels of features, and preserving more spatial details."}, {"title": "III. EXPERIMENTS AND DISCUSSION", "content": ""}, {"title": "A. Datasets", "content": "ISPRS Vaihingen: The ISPRS Vaihingen dataset consists of 16 very high-resolution true orthophotos, averaging 2500 \u00d7 2000 pixels each. Each orthophoto includes three channels: near-infrared, red, and green (NIRG), with a ground sampling distance of 9 centimeters. The dataset comprises five foreground classes: impervious surfaces, buildings, low vegetation, trees, cars, and one background class (clutter). The 16 orthophotos are divided into a training set of 12 patches and a test set of 4 patches. The training set includes orthophotos indexed as 1, 3, 23, 26, 7, 11, 13, 28, 17, 32, 34, and 37, while the test set includes orthophotos indexed as 5, 21, 15, and 30."}, {"title": "B. Implementation Details", "content": "We use ResNet18-based UNetformer [5] as the baseline model. The proposed LSKASeg is benchmarked against several prominent methods, including ABCNet [11], TransUNet [9], UNetformer [5], CMTFNet [10], BANet [4], and MARes-Unet [12]. Experiments are conducted using PyTorch on a single NVIDIA GeForce RTX 4090 GPU with 24GB RAM. All models are optimized and trained using the Stochastic Gradient Descent (SGD) algorithm with a learning rate of 0.01, momentum of 0.9, weight decay of 0.0005, and a batch size of 10. The total number of epochs is set to 50, with testing conducted after each epoch. To quantitatively evaluate the proposed method, two widely used metrics are recorded: mean F1 score (mF1) and mean Intersection over Union (mIoU)."}, {"title": "C. Performance Comparison", "content": "Performance comparison on the Vaihingen dataset: As shown in Table 1, the proposed LKASeg shows significant improvements in mF1 and mIoU compared to the baseline UNetformer. This confirms the effectiveness of the LKA-based decoder structure and full-scale skip connections. Compared to several models included in the comparison, LKASeg performs excellently across most categories. Notably, LKASeg's F1 and IoU increased by 0.52% and 1.01% for the building category compared to BANet. The IoU for impervious surface increased by 0.60% and for car by 0.29% compared to the baseline UNetformer. Overall, LKASeg achieved mF1 and mIoU scores of 90.33% and 82.77%, respectively, mF1 and mIoU increased by 0.53% and 0.96% to the baseline UNetformer. These improvements are mainly attributed to the global semantic information from LKA-based decoder and full-scale features from full-scale skip connections. Fig. 3 shows visual examples of results obtained by all eight methods. Clearly, LKASeg can more accurately segment objects, with smoother boundaries and fewer noise points."}, {"title": "D. Ablation Study", "content": "In this section, we evaluate the model performance under different configurations and compare our method with the baseline [5]. First, without the LKA module and FSC structure, the model's ability to obtain global information and full-scale information is insufficient, leading to decreased performance. Next, we evaluated networks using only the LKA module, only the FSC structure, and using both LKA and FSC structures. All four configurations outperformed the baseline network. This is because the LKA modules learn long-range dependencies between pixels without disrupting the 2D structure of the context features, maintaining channel adaptivity and significantly improving semantic segmentation results for remote sensing images. As shown in the table 2, replacing the baseline transformer-based decoder with LKA-based decoder not only improves mF1 and mIoU but also reduces the number of parameters and computational cost, demonstrating the effectiveness of our structure and its simplified computational complexity. The FSC effectively handles large variations in shape and size through dense encoder-decoder connections. Although it results in a slight increase in the number of parameters and computational cost, it improves segmentation accuracy. Compared to the baseline, our method's mF1 and mIoU on the Vaihingen dataset increased by 0.53% and 0.96%, respectively."}, {"title": "IV. CONCLUSION", "content": "This paper proposes a new semantic segmentation network, LKASeg, which features two decoders based on Large Kernel Attention (LKA), respectively. The LKA-based decoder extract global features and offer channel adaptability while avoiding the computational overhead of self-attention. To achieve full-scale feature learning and fusion, we apply Full-Scale Skip Connections (FSC) between the encoder and decoder. We conducted experiments combining LKA-based decoder with FSC. Evaluations on a remote sensing datasets demonstrate that LKASeg outperforms existing CNN and Transformer-based semantic segmentation methods. Specifically, on the ISPRS Vaihingen dataset, the mF1 and mIoU scores increased by 0.53% and 0.96%, respectively, compared to the baseline. These results highlight the potential of our method in the field of remote sensing semantic segmentation."}]}