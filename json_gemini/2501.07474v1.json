{"title": "Estimating Musical Surprisal in Audio", "authors": ["Mathias Rose Bjare", "Giorgia Cantisani", "Stefan Lattner", "Gerhard Widmer"], "abstract": "In modeling musical surprisal expectancy with computational methods, it has been proposed to use the information content (IC) of one-step predictions from an autoregressive model as a proxy for surprisal in symbolic music. With an appropriately chosen model, the IC of musical events has been shown to correlate with human perception of surprise and complexity aspects, including tonal and rhythmic complexity. This work investigates whether an analogous methodology can be applied to music audio. We train an autoregressive Transformer model to predict compressed latent audio representations of a pretrained autoencoder network. We verify learning effects by estimating the decrease in IC with repetitions. We investigate the mean IC of musical segment types (e.g., A or B) and find that segment types appearing later in a piece have a higher IC than earlier ones on average. We investigate the IC's relation to audio and musical features and find it correlated with timbral variations and loudness and, to a lesser extent, dissonance, rhythmic complexity, and onset density related to audio and musical features. Finally, we investigate if the IC can predict EEG responses to songs and thus model humans' surprisal in music. We provide code for our method on github.com/sonycslparis/audioic.", "sections": [{"title": "I. INTRODUCTION", "content": "Surprisal, as estimated by the information content (IC) or negative log-likelihood of an autoregressive sequence model, has been proposed as a proxy for estimating perceived musical surprise as experienced by human listeners [1]\u2013[4]. With an appropriately chosen model, the IC of musical events has been shown to correlate with human perception of surprise and with complexity aspects, including tonal and rhythmic complexity [5], [6]. The analysis of music with IC offers a way to quantify information-theoretic hypotheses about music and music perception [7] and has been shown to work as an interesting conditioning signal for generative models [8]\u2013[10]. With a few exceptions [11]\u2013[13], most work on computing surprisal in music data has been limited to the discrete symbolic domain (e.g., sheet-music or MIDI), for which music can be serialized as condensed, interpretable sequences of musical events where the surprisal can be identified with specific musical events, such as an unexpected pitch breaking with the expectation of an established tonality, or sudden change of rhythm. The condensed discrete nature of symbolic encodings makes them suitable for studying suprisal with language models [6], [10], [14], [15] like Transformers [16]. However, as opposed to audio representations, symbolic music has limited availability and fails to capture all musical aspects, including acoustic properties like timbre and dynamics, that are critical to many musical genres, including popular music, and might influence the experience of surprisal. Therefore, estimating surprisal using audio would be more general than using symbolic data.\nAnalyzing surprisal in full-length music audio has until recently [17] been impractical with invertible audio representations that faithfully preserve the audio content. This has been due to the low compression rates of such representations, causing the sequences to be restrictively long. As such, previous works have calculated IC using preselected audio features [11], [13], chosen using domain expertise of the studied music that potentially bias facets of the investigation.\nIn this work, we analyze surprisal aspects of full-length music recordings without using preselected audio features. More specifically, we train a GIVT model [18] and estimate IC from its one-step predictions on audio sequences encoded into the invertible audio representations of Music2Latent [19]. We use the mean IC (information density) of music segments to show the effect of learning by analyzing the reduction in IC on repeated segments. Furthermore, we find that segment types that appear later in pieces have higher IC, suggesting that they have been composed to contrast earlier segments musically. We correlate the IC with audio signal features and musical features typically associated with surprisal and complexity. Finally, using conventional cortical tracking methodologies for validating computational models of human expectations, we found that our IC significantly predicts EEG brain responses to songs, supporting our methodology's potential for modeling human surprisal in music."}, {"title": "II. RELATED WORK", "content": "In the symbolic music domain, IC, as a proxy for musical surprisal, has most notably been studied with the variable order Markov-model IDyOM [2]. Several behavioral and neural studies already support IDyOM modeling of human melodic expectation [3], [20]\u2013[24], and its predictions were shown to correlate with tonal and rhythmic complexity [5], [6], typically associated with musical surprisal. The model is, however, limited to monophonic symbolic music stimuli. In the work of [10], the authors propose an IC-based technique for estimating surprisal in polyphonic symbolic music and show the IC to correlate with tonal and rhymic complexity using solo piano performances. In the audio domain, suprisal has been calculated using human-selected audio features. The Audio Oracle [11] analyzes surprisal using information rate calculated from self-similarities of audio features. The method was shown to identify high surprisal with segment boundaries. In [13], IC of a D-REX [25], [26] model is related to the MEG brain response of human participants. IC is calculated using 15 different audio features and Bayesian inference. Although not related to temporal suprisal, [12] uses the KL-divergence of a diffusion model to approximate the likelihood of 5-second clips. This is used to reproduce the inverted U-shape relation between the total IC of music clips and listener preference presented in [7]. The model does not rely on audio features; however, it ignores causality and memory aspects of surprisal."}, {"title": "III. METHOD", "content": "Instead of modeling surprisal directly in the audio domain, we encode our data into the invertible 64-dimensional latent representations of Music2Latent [19], an autoencoder with a continuous latent bottleneck, based on consistency models [27]. Thus, the audio signal is encoded in a 64-channel sequence of ~11Hz. We choose an autoencoder representation over a contrastively learned representation [28], [29] to ensure that the representation preserves the signal information. Furthermore, we chose Music2Latent over discrete representations [30]\u2013[33], due to its low frame rate allowing us to train autoregressive models on full-length music pieces."}, {"title": "B. Modelling Suprisal", "content": "When dealing with symbolic music or discrete audio representations, surprisal can be obtained directly from the (learned) next-step prediction probability mass function. In the context of neural networks, this means having a finite vocabulary of tokens and typically implies training a Transformer to do masked prediction [16], [34] with a softmax cross-entropy loss. More specifically, given a sequence of discrete tokens $x_{1,2,...,T}$ from a finite vocabulary, the IC of sequence element $x_{t}$ is given by\n$IC(x_t = i|x_{<t}) = -log(Softmax(f_\\theta(x_{<t}))_i),$ (1)\nwhere $f_\\theta$ is a Transformer model, outputting the softmax logits. As such, IC is bounded by the interval $[0,\\infty[$. Working with continuous representations, the vocabulary is infinite, and we cannot estimate a probability mass function. Instead, we model the IC using the probability density of the next frame. Similarly to [18], we replace the softmax function in (1) with a Gaussian mixture model (GMM) parameterized by the logits of the Transformer $f_\\theta$, i.e.,\n$IC(x_t = v|x_{<t}) = -log(P_{GMM}(x_t = v; f_\\theta(x_{<t}))),$ (2)\nwhere $P_{GMM}$ is the differentiable probability density function of a GMM with parameters $f_\\theta(x_{<t})$ (Parameterization in section IV-B). We note that the IC of our model is unbounded."}, {"title": "IV. TRAINING DETAILS", "content": "For training our model, we use a proprietary multi-stem dataset (PR) consisting of 19,701 pieces of popular music. The multi-stem aspect will be useful for our experiment on correlating the surprisal estimations with EEG responses to singing voices in section V-D since it contains vocal stems. The dataset is split into 17,730 and 1,969 pieces for training and evaluation, respectively. In addition, we evaluate our model on the multi-stem datasets of MedleyDB 2.0 (ME) [35] and MUSDB18 (MU) [36]. For the former, we use the 69 pieces annotated with Pop, Rock, or that have genre annotation. For the latter, we use the 74 pieces tagged with Pop, Country, Reggae, and Rock that are absent in ME. For all datasets, we preprocess the audio by averaging the stems, converting to mono, downsampling to 22050Hz, and encoding to MP3 before embedding to Music2Latent representations. Finally, we use Deep12 [37] to automatically extract the following metadata: 1) segment boundaries 2) ordered segment labelings 3) beat positions 4) measures 5) chord labels."}, {"title": "B. Model and Training", "content": "Our model follows a standard 12-layer causal Transformer with the additions: For the input, we \u201cembed\u201d latent sequences with a maximum length of 4600, corresponding to approximately 7 minutes of audio, using a linear projection layer that upscales the latents' dimensions. For the output, we use a linear layer to upscale the output dimension to the number of parameters of a 32-component GMM model. Similarly to [18], we use a diagonal covariance matrix and ensure non-negativity by using a softplus function. We ensure the mixing coefficients are non-negative and sum to one by using a softmax function. The model is trained by minimizing (2). We use Pre-Layer Normalization similar to [34], rotary positional embeddings [38], and FlashAttention [39].\nWe train our model with a batch size of 16 sequences corresponding to a maximum of ~ 2 hours of music. We use 16-bit automatic mixed precision and automatic gradient scaling. We use a linear learning rate schedule, which increases the learning rate from 0 to 10-4 over 80 warmup epochs and decreases again to 0 at epoch 500. The system is terminated with early stopping after converging at epoch 90. We use an Adam optimizer [40] with a weight decay of 3\u00d710-7.\nFor our experiment in section V-D that uses IC of singing voices, we additionally fine-tune our model on the vocal stems of our train dataset until convergence for another 69 epochs."}, {"title": "V. EXPERIMENTS", "content": "We investigate if repetition lowers our IC as seen in symbolic music IC modeling [10], [14]. In the following, a repetition is a pair of segment indices where the corresponding segments have the same label (extracted with Deep12). For example, a piece with musical form Intro,A,A,B,A,Outro has two repetitions: (2,3) and (3,5), stemming from the segments labeled A. We quantify the IC decrease of a repetition pair by calculating the difference between its corresponding segments' average frame IC. We collect the differences for all segment types within all pieces, calculate the mean difference (\u03bc), and a one-tailed t-test for positivity. The average differences are then reported in Table I, column Repetition. For PR, the decrease is negative, and the t-test is significant; for MU and ME, the results are not significant. In the following, we use 5% significance levels unless otherwise stated. Therefore, repetition lowers IC on average for the largest dataset, which follows the training distribution most closely. We emphasize that our results are obtained for repetitions that do not correspond to exact copies since the data is recorded music."}, {"title": "B. Contrasting Segments Hypothesis", "content": "We are interested in the following hypothesis: segment types, as identified by their labels, contrast their predecessor segment types in the sense of having a higher IC on average. Outros, however, serve to close a piece and, therefore, have lower IC than predecessor segments. As for repetitions, our experiments involve calculating the difference in segment ICs. However, instead of extracting repetition index pairs, we extract index pairs corresponding to subsequent sequence types on their first occurrence. For the previous example, we, therefore, obtain index pairs (1,2), (2,4) and (4,6). We calculate an average across all pieces and report this in Table I, column Seg. Contrast by: $\u03bc_s$ considering all segment types not including outros, and $\u03bc_o$ only segment pairs that involve outros. For segment types not involving outros, we find that $\u03bc_s$ is positive for all datasets, and the results are significant using a one-tailed t-test. For segments involving outros, we find all $\u03bc_o$ to be negative and significant except for MU. The results, therefore, support our hypothesis."}, {"title": "C. Relation to Complexity", "content": "We are interested in the relationship between the IC and descriptive features of the music, which can be associated with complexity. As such, we investigated to what extent IC is correlated with dissonance (d), as indicated by TIV features [41], [42], rhythmic complexity (r) as indicated by the normalized entropy of interonset interval (IOI) histogram [43], onset density (o) and spectral flux (f) associated with timbre variations [44]. Additionally, we investigate IC's relation to the signal's loudness (l). We do so by extracting windows of metric-dependent lengths and calculating Pearson's correlation between the metric and the mean IC of the segment's frames. We calculated the TIV dissonance of 2-second windows centered around the chord changes within the pieces, because we hypothesized that the greatest dissonance variations would be found there. The IOI-entropies are calculated by quantizing onsets calculated with the detection algorithm of [45] to 12 subdivisions per beat and calculating the normalized entropy of IOI histograms within each measure. The onset densities are calculated using 4-second windows. Loudness is calculated on 1-second windows according to [46]. Average spectral flux is calculated using 1-second bin-normalized log-mel spectrogram windows. The results are presented in Table I, column Complexity, where $r_d$, $r_r$, $r_o$, $r_l$ and $r_f$ are the correlations corresponding to TIV dissonance, IOI-entropy, onset density, loudness and spectral flux respectively. All correlations are significant. Loudness and spectral flux are mostly correlated with the IC across all datasets. For loudness, possibly due to quieter parts corresponding to segments with fewer simultaneously sounding stems with less information than louder segments. For spectral flux, it indicates that timbre variations influence surprisal. Dissonance is the third highest correlated metric, with a correlation of 0.2 on ME and PR. The rest of the correlations are neglectable. As shown in section V-A, repetition decreases the IC values. We, therefore, investigate how the correlations between IC and the metrics develop over time. To that end, we bin the timeline in 4-second intervals and associate the previously mentioned windows with a particular bin if its start point is within that window. For each window bin, we calculated the correlations for the dataset PR and report it in Fig. 1. We present the results for PR since it contains most pieces, but the other datasets showed similar trends. The correlations are noticeably higher at the beginning of pieces than the average correlations of Table I for all metrics; however, the correlations decrease over time. This is consistent with IC complexity correlations in the symbolic domain [10]. Since the data is popular music, it is reasonable to believe that many pieces' ICs are dominated by repetitions after the first minute, which decreases the IC to some lower bound, resulting in a decrease in the correlation."}, {"title": "D. Prediction of EEG Responses", "content": "To validate whether the proposed modeling correlates with human perception, we test if the IC can predict neural responses to monophonic sung music. To do so, 64-channel EEG responses to songs (20 adult participants, 18 different song stimuli; for details, see [47]) are modeled as a linear combination of two predictor variables, the IC and the energy envelope of the waveform (E), so that these two compete to explain EEG variance. E is extracted using the Hilbert transform and used as a nuisance regressor, absorbing EEG variance relating to lower-level acoustic features and enabling us to isolate the encoding of the higher-level processes related to expectations. Brain responses are sensitive to onsets and variations, and since the IC is correlated with loudness (see section V-C), controlling for acoustic changes is crucial. To further control that, we do a constant-value interpolation of unvoiced segments using the last IC value before the silence. The channel-specific mapping between predictors and the neural data is estimated for each participant by solving a regularised linear regression problem [48]. The model accounts for non-instantaneous interactions as it is learned considering multiple stimulus-response time lags. We set a [-100,700]ms time window with a margin of 50ms to avoid border artifacts. The resulting forward models are then evaluated in terms of how well they predict unseen EEG data using a leave-one-out cross-validation procedure across trials. The quality of a prediction is then quantified by calculating the z-transformed Pearson's correlation (r) between the estimated signals and the corresponding predictions at each scalp electrode.\nThe significance of the IC contribution was assessed by comparing the predictive power of the full model to the average of $N=100$ baseline models consistent with the null hypothesis that there is no causal relationship between IC and brain response. The baseline models are identical to the full model except for the IC, which is randomly shuffled. This procedure enabled us to evaluate any incremental improvement in the model's performance due to a significant relationship between IC and responses without introducing bias (e.g., by altering the model's complexity) and has been widely employed in cortical tracking experiments, among which those testing IDyOM [21]. Statistical analyses use two-tailed t-tests or non-parametric Wilcoxon signed-rank tests and corresponding effect size measures (Cohen's d and Wilcoxon's Z score respectively) depending on the normality of the data, assessed via the Anderson-Darling test. False discovery rate (FDR) is used to correct for multiple comparisons.\nAs can be seen in Fig. 2, including IC among the predictors significantly improves the average model predictions across channels (Wilcoxon signed-rank test, p = 0.025, Z = 2.22) and almost all individual channel predictions (FDR corrected t-test with significance threshold set at p = 0.05), meaning IC explains a distinct portion of the variance in the data. Note that the encoding of the control variable E is already significant per se on the average over all electrodes (t-test, p < 0.001, d = 1.41). E and IC topographies are consistent with those of [49], where right lateralization was observed in the same data for melodic expectations computed with IDyOM and not for lower-level acoustics. Here, however, we found no significant effect of right-lateralization across subjects using the same test."}, {"title": "VI. CONCLUSION", "content": "We presented a general model for estimating musical surprisal in full-length music via the IC of a GIVT model's next-step predictions on invertible audio representations. This is opposed to models of previous works that operate on preselected audio features capable of capturing only selected aspects of the audio signal. We showed that the model's IC, like symbolic music analogous models, decreases with repetition. We showed that the IC of segment types appearing later in a piece is higher than those appearing earlier on average but that the opposite is true for outros. We showed correlations between the IC and descriptive features of the music and found these to decrease with time, similar to findings in the symbolic domain. Finally, we validated our model's capabilities as a perceptual model of human musical surprisal by showing that the IC significantly predicts EEG brain responses to songs."}]}