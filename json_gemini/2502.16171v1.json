{"title": "EPERM: An Evidence Path Enhanced Reasoning Model\nfor Knowledge Graph Question and Answering", "authors": ["Xiao Long", "Liansheng Zhuang", "Aodi Li", "Minghong Yao", "Shafei Wang"], "abstract": "Due to the remarkable reasoning ability, Large language mod-\nels (LLMs) have demonstrated impressive performance in\nknowledge graph question answering (KGQA) tasks, which\nfind answers to natural language questions over knowledge\ngraphs (KGs). To alleviate the hallucinations and lack of\nknowledge issues of LLMs, existing methods often retrieve the\nquestion-related information from KGs to enrich the input con-\ntext. However, most methods focus on retrieving the relevant\ninformation while ignoring the importance of different types\nof knowledge in reasoning, which degrades their performance.\nTo this end, this paper reformulates the KGQA problem as a\ngraphical model and proposes a three-stage framework named\nthe Evidence Path Enhanced Reasoning Model (EPERM) for\nKGQA. In the first stage, EPERM uses the fine-tuned LLM\nto retrieve a subgraph related to the question from the origi-\nnal knowledge graph. In the second stage, EPERM filters out\nthe evidence paths that faithfully support the reasoning of the\nquestions, and score their importance in reasoning. Finally,\nEPERM uses the weighted evidence paths to reason the final\nanswer. Since considering the importance of different struc-\ntural information in KGs for reasoning, EPERM can improve\nthe reasoning ability of LLMs in KGQA tasks. Extensive ex-\nperiments on benchmark datasets demonstrate that EPERM\nachieves superior performances in KGQA tasks.", "sections": [{"title": "Introduction", "content": "Question answering over knowledge graph (KGQA) has gar-\nnered significant attention in recent years. It aims to find\nanswers for natural language questions based on knowledge\ngraphs (KGs), such as Freebase (Bollacker et al. 2008) and\nWikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch 2014), which are built\nfrom a large number of triplets consisting of (head entity, re-\nlation, tail entity). Since the natural language questions often\ncontain compositional semantics (Lan et al. 2022), exactly\nunderstanding the semantic information in the question and\nidentifying the structured knowledge in KGs is very impor-\ntant for KGQA tasks.\nRecently, as large language models (LLMs) (OpenAI 2023;\nHadi et al. 2023) have demonstrated impressive ability to un-\nderstand natural language and reasoning abilities in many\nNLP tasks, LLMs have also shown impressive performance\nin knowledge graph question answering tasks (Jiang et al."}, {"title": "Related Work", "content": "Knowledge Graph Question Answering. Conventional\nKBQA solutions can be categorized into three types: Seman-\ntic Parsing-based (SP-based) methods, Information Retrieval-\nbased (IR-based) methods, and Embedding-based methods.\nSP-based methods parse the question into a structural query\n(e.g., SPARQL) which can be executed by a query engine\nto get answers (Lan et al. 2022). ArcaneQA (Gu and Su\n2022) dynamically generates the query based on results from\nprevious steps. RnG-KBQA (Ye et al. 2021) first enumerate\nall possible queries and then rank them to get the final out-\nput. These methods heavily rely on the quality of generated\nqueries. If the query is not executable, no answers will be\ngenerated. DECAF (Donahue et al. 2014) combines seman-\ntic parsing and LLMs reasoning to jointly generate answers,\nwhich also reach salient performance on KGQA tasks. How-\never, these methods need to annotate expensive logic forms\nas supervision or are limited to narrow domains with a few\nlogical predicates (Lan et al. 2022). KG embedding, which\naims to encode entities and relations into a continuous vec-\ntor space (Bordes et al. 2013; Long et al. 2022; Sun et al.\n2019; Long et al. 2024a), and its effectiveness has been vali-\ndated in knowledge graph question answering (KGQA) tasks.\nEmbedding-based methods model the entities and relations\nin embedding space and design special model architectures to\nreason answers. KV-Mem (Miller et al. 2016) adopts a Key-\nValue memory network to store triples for reasoning. Embed-\nKGQA (Saxena, Tripathi, and Talukdar 2020) and NSM (He\net al. 2021) utilize the sequential model to mimic the multi-\nhop reasoning process. IR-based methods primarily retrieve\nrelevant factual triples or text from Knowledge Graphs (KGs)\nbased on natural language questions and then design special\nmodel architectures to reason answers. Early works adopt the\npage rank or random walk algorithm to retrieve subgraphs\nfrom KGs for reasoning (Sun et al. 2018). Recently, to inte-\ngrate LLMs for KGQA, retrieval augmented methods (Jiang\net al. 2022; Luo et al. 2023) aim to leverage the LLMs to\nreason on the retrieved facts from the KGs to improve the\nreasoning performance. For example, UniKGQA (Jiang et al.\n2022) unifies the graph retrieval and reasoning process into a\nsingle model with LLMs. ToG (Sun et al. 2023) uses LLM\nas an agent to iteratively perform beam search on knowledge\ngraphs to find answers. RoG (Luo et al. 2023) uses LLM to\ngenerate relation plans, which are used to retrieve the relative\nfacts from raw KGs for LLMs to conduct faithful reasoning.\nHowever, these methods treat the different retrieval informa-\ntion equally to reason the answer, ignoring the differences\nbetween retrieved information. EPERM proposes to retrieve\nand score the evidence paths, which consider the different\nimportance of the structural information for better reasoning\nthe answers.\nLarge Language Models. With the launch of ChatGPT and\nGPT-4 (OpenAI 2023), displaying the prowess of decoder-\nonly large language models (LLMs) with a vast number of\nparameters that exhibit emergent phenomena, many tradi-\ntional NLP tasks are becoming simplified (Hadi et al. 2023).\nSubsequently, open-source models like Llama-2-7B (Touvron\net al. 2023), ChatGLM2-6B (Zeng et al. 2022) and Qwen-\nChat (Bai et al. 2023) emerged and can be supervised fine-\ntuned (SFT) using instruction-tuning technologies (Zhang\net al. 2023) such as LoRA (Hu et al. 2021), QLORA (Dettmers\net al. 2024), P-Tuning v2 (Liu et al. 2021), and Freeze (Geva\net al. 2020), enhancing the capabilities of LLMs for spe-\ncific tasks. Additionally, Chain-of-Thought (CoT) (Wei et al.\n2022) has been shown to be effective in enhancing LLM\nreasoning. It creates a series of prompt instances according\nto reasoning logic under a few-shot learning paradigm in\norder to improve LLM's performance on complex tasks. In\nthis paper, EPERM employs the instruction-tuning technique\nto fine-tune open-source LLMs, which consists of the sub-\ngraph retriever, evidence path finder, and answer predictor.\nAll the modules in EPERM are joint fine-tuning to learn the\nparameters."}, {"title": "Methodology", "content": "Overall, the framework of the Evidence Path Enhanced Rea-\nsoning Model (EPERM) is shown in Figure 2, which con-\ntains the subgraph retriever module, the evidence path finder\nmodule and the answer predictor module. In the first stage,\nEPERM first uses the fine-tuned LLM to retrieve a subgraph\nrelated to the question from the original knowledge graph.\nIn the second stage, the proposed evidence path finder first\ngenerates a series of weighted plans that faithfully support\nthe reasoning of the questions. Then it scores and filters out\nthe weighted evidence path in the subgraph based on the\nweighted plans. Finally, EPERM uses the evidence paths\nwith their importance score to reason the final answer. In the\nfollowing subsections, we first formally define the KGQA\ntask. Then, we introduce the details of the proposed method."}, {"title": "Problem Definition", "content": "A knowledge graph typically consists of a set of triples,\ndenoted by $G = \\{(e,r,e')|e,e' \\in E,r \\in R\\}$, where E\nand R denote the entity set and relation set, respectively.\nKnowledge Graph Question Answering (KGQA) is a typi-"}, {"title": "The EPERM Framework", "content": "We start by formalizing the knowledge graph question an-\nswering in a probabilistic way, Given a question Qn and its\nanswers An, we formalize the KBQA problem as to model\nthe probabilistic distribution $P(A_n|G, Q_n)$. We introduce\ntwo latent variables: a question-related subgraph $G$ and a\nseries of evidence paths $R_n$ help to reason the question $Q_n$.\nThen the KGQA task can be reformulated as a probabilis-\ntic graphical model in Figure 5. Based on the independence\namong the variables in the directed graph and following the\nd-separation principle (Pearl 2009), the proposed model (ob-\njective distribution) $P_o(A_n|G, Q_n)$ can be reformulated as\nbelow (we include these details in the Appendix):\n$P_o(A_n|G, Q_n) = \\Sigma_{R_n}\\Sigma_{G} P_o(A_n|R_n, Q_n)$\n$P_o(R_n|G, Q_n)P_o(G|G, Q_n)$.\nIn the above equation, the proposed EPERM can be divided\ninto three parts. The first part is the subgraph retriever module,\nwhich is described by $P_o(G|G, Q_n)$. It aims to retrieve a\nquestion-related subgraph from the KGs. The second part\nis the evidence path finder module, which is described by\n$P_o(R_n|G, Q_n)$. Its aim is to find and evaluate evidence paths\nfor the subsequent reasoning. Finally, the answer predictor\nmodule is formulated by $P_o(A_n|R_n, Q_n)$, which leverages\nthe weighted evidence paths to reason the final answer. The\nfollowing sections will provide a detailed introduction to the\nthree modules of EPERM and its training objectives.\nSubgraph retriever module. The subgraph retriever module\naims to calculate $P_o(G|G, Q_n)$ for any $G$, which is intractable"}, {"title": "Optimization Framework", "content": "Next, we introduce how to optimize the EPERM framework.\nSince the LLMs have zero knowledge of the relations con-"}, {"title": "Experiment", "content": "In this section, we first introduce the experiment settings\nincluding datasets, baselines, and evaluation protocols. Sec-\nondly, we compare EPERM with competitive models and\ndemonstrate its superiority. Thirdly, we conduct a series of\nablation studies to analyze the importance of the three mod-\nules in the EPERM. Then, we analyze the impact of two\nimportant parameters on the proposed model. Finally, we do\nthe case study to exploit how the EPERM finds the evidence\npaths and reasons the answers based on them.\nExperiment Setup\nDatasets. We evaluate the proposed EPERM on two bench-\nmarks, WebQuestionSP (WebQSP) (Yih et al. 2016) and\nComplex WebQuestion (CWQ) (Talmor and Berant 2018),\nwhich contain up to 4-hop questions. The statistics of the\ndatasets are given in Table 5. Freebase (Bollacker et al. 2008)\nis the background knowledge graph for both datasets, which\ncontains around 88 million entities, 20 thousand relations,\nand 126 million triples.\nEvaluation Metrics. Following previous works (Luo et al.\n2023), we use Hits@1 and F1 as the evaluation metrics.\nHits@1 measures the proportion of questions whose top-1\npredicted answer is correct. Since a question may correspond\nto multiple answers, F1 considers the coverage of all answers,"}, {"title": "Case Study", "content": "Finally, we explore how the EPERM reasoning the an-\nswers based on the weighted evidence paths. We illustrate\na case study in Figure 4. We can see that for the ques-\ntion \"Where are the NATO headquarters located?\", EPERM\ncan generate a series of weighted plans and then it scores\nand filters out the weighted evidence paths in the sub-\ngraph based on the weighted plans. Although these paths\nare related to the problem, they still have different con-\nfidence scores to reason the questions. If we treat each\npath equally, it will degrade the reasoning performance.\nFor example, the first path \u201cNATO$\\\\\\\\\\\\_organization.headquarters\n\\\\\\\\\\\\\\_mailingaddress.citytown$\\\\\\\\\\\\_m.04300hm Brussels\u201d is more\nlikely to reason the final result. Because the question\nemphasizes where NATO's headquarters is located. The\nother evidence paths e.g., \u201cNATO$\\\\\\\\\\\\_organizations\\\\\\\\\\\\\\_founded\nNorway$\\\\\\\\\\\\_administrative\\\\\\\\\\\\\\_divisions$, Oslo\u201d focus on where\nNATO's various departments are located, which are supposed\nto have lower confidence in reasoning the answer. In this way,\nthe Answer Predictor in the EPERM can better make the final\nchoice in the reasoning stage."}, {"title": "Conclusion", "content": "In this paper, we propose a novel framework called the Evi-\ndence Path Enhanced Reasoning Model (EPERM) to address\nRAG-based knowledge graph question answering tasks. This\nframework explores the integration of the generative and rea-\nsoning capabilities of large language models (LLMs) with\nprior knowledge in knowledge graphs for faithful reason-\ning. We reformulate the KGQA task as a graphical model\ncomprising three stages. In the first stage, EPERM utilizes a\nfine-tuned LLM to retrieve a subgraph related to the question\nfrom the original knowledge graph. In the second stage, the\nevidence path finder generates a series of weighted plans\nthat reliably support the reasoning process. It then scores and\nfilters the weighted evidence paths within the subgraph based\non these plans. Finally, in the third stage, the answer predic-\ntor leverages the weighted evidence path to reason the final\nanswer. Since the weight of each evidence path indicates the\ndifferent importance of the structural information for reason-\ning the question, EPERM can better leverage them to reason\nthe answer. Extensive experiments on benchmark datasets\ndemonstrate that EPERM achieves superior performances in\nKGQA tasks."}]}