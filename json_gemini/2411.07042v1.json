{"title": "Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in Al Companion Applications", "authors": ["Xianzhe Fan", "Qing Xiao", "Xuhui Zhou", "Yuran Su", "Zhicong Lu", "Maarten Sap", "Hong Shen"], "abstract": "Content Warning: This paper presents textual examples that may be offensive or upsetting.\nAl companions based on large language models can role-play and converse very naturally. When value conflicts arise between the Al companion and the user, it may offend or upset the user. Yet, little research has examined such conflicts. We first conducted a formative study that analyzed 151 user complaints about conflicts with Al companions, providing design implications for our study. Based on these, we created MINION, a technology probe to help users resolve human-Al value conflicts. MINION applies a user-empowerment intervention method that provides suggestions by combining expert-driven and user-driven conflict resolution strategies. We conducted a technology probe study, creating 40 value conflict scenarios on Character.AI and Talkie. 22 participants completed 274 tasks and successfully resolved conflicts 94.16% of the time. We summarize user responses, preferences, and needs in resolving value conflicts, and propose design implications to reduce conflicts and empower users to resolve them more effectively.", "sections": [{"title": "1 INTRODUCTION", "content": "Human-Al conflict refers to a state of incompatibility, inconsistency, or opposition between humans and AI [18]. In past research, human-Al conflicts were usually simple and direct-AI was more like a tool, and conflicts often stemmed from technical limitations, such as task execution failures [68], or disagreements with users in simple decision-making [1, 62]. These types of conflicts generally lacked emotional and value entanglement, making them less likely to cause significant psychological harm to users.\nRecently, a diverse array of Large Language Model (LLM) agents has emerged, offering capabilities ranging from personalized assistance to performing complex tasks [11]. The study focuses on LLM-based AI companion applications, such as Character.AI, Talkie, Replika, Kindroid, Paradot, and Xingye. As of July 2024, the total number of users of these applications has exceeded 900 million globally (including duplicate users across different applications)\u00b9.\n\u00b9User statistics source: https://www.data.ai."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "The human-Al relationship is becoming increasingly complex, especially in the context of AI companion applications (\u00a7 2.1). Early research mostly focused on technical conflicts with functional AI, but the emergence of LLMs has given Al more human-like characteristics, shifting the nature of conflicts from functional to value-based (\u00a7 2.2). Existing technical solutions do not fully address users' needs in resolving value conflicts with Al companions, necessitating deeper exploration, drawing on expert strategies for interpersonal conflict resolution and users' practical experiences in Al companion applications (\u00a7 2.3)."}, {"title": "2.1 Emerging Human-Al Relationship in LLM-Based AI Companion Applications", "content": "With the widespread adoption of LLMs, human-AI relationships have further evolved. Unlike earlier AI systems primarily providing functional services, LLM-based AI companions can engage in more intimate and complex interactions [57]. Some users develop a parasocial relationship with their Al companion, a one-sided, asymmetrical relationship between an individual and a fictional character or media figure [3, 38, 44]. Although AI companions are not real humans, users' emotional investment in them is real [57]. Compared to functional AI, the emotional connection between users and AI companions, along with the anthropomorphization of AI companions, often exacerbates the psychological impact of conflicts on users, potentially leading to anxiety or depression [30, 76]. For instance, many users develop emotional bonds with their Replika, and when conflicts arise, they feel deeply distressed, describing it as experiencing a \"lobotomy, being torn apart\" [2]. Therefore, preventing or resolving conflicts between users and AI companions is becoming increasingly important. Unfortunately, little is known about empowering users to resolve value conflicts with AI companions, and this work contributes to this area."}, {"title": "2.2 Human-AI Conflict and Value Conflict", "content": "Human-Al conflict refers to incompatibility, inconsistency, or opposition between humans and AI [18]. In HCI, early studies on human-Al conflict typically focused on the technical aspects, viewing Al as tools, service robots, or intelligent assistants, with conflicts often arising from decision-making inconsistencies or system malfunctions [1, 13, 48, 56, 62, 65, 67]. Strategies for resolving these human-Al conflicts typically include AI proposing negotiation solutions [1, 48, 62] and optimizing algorithms to reduce conflicts [56]. For example, when a delivery robot encounters a conflict with a human in front of an elevator, competing for the right to enter first, the robot can resolve the conflict by making polite requests or commands to secure priority [1]. When students experience conflict while collaborating with Al in solving problems, the AI can offer more explanations or alternative suggestions to reach a resolution [48]. However, this type of research usually confines the role of AI to a functional level, mainly focusing on task execution and efficiency optimization [1, 48, 56, 62, 67], neglecting the more complex human-AI relationships.\nThe development of LLMs has made AI more anthropomorphic, and both researchers and users increasingly tend to view AI as social actors [42]. This is especially evident in AI companion applications, where interactions between users and AI have become more intimate, sometimes resembling relationships with friends or even romantic partners. In this context, conflicts occur not merely at the technical functionality level, but often on a deeper, value-based level [25, 66].\nValues include personal daily habits, social interaction norms, religious or secular traditions, and moral principles [26, 47]. They can be transmitted through people, training data, models, and generated outputs [25]. LLMs sometimes fail to accurately capture human values [34], and can be misled to generate toxic [20], biased [33, 55], or immoral [16] content, which poses risks for LLM-based chatbots. When Al's suggestions or behaviors conflict with users' personal beliefs, cultural backgrounds, or moral views [14, 23], human-AI value conflicts arise [25, 66], often accompanied by strong emotional reactions from users [14]. Johnson et al. found that GPT-3 aligns more closely with values dominant in American citizenship [25]. Fan et al. noted that when Al companions exhibit bias, it may conflict with users' values, leading to discomfort [17].\nIn the age of LLMs, human-AI value conflict is becoming an urgent challenge. In the emerging human-Al relationships, users tend to resolve conflicts more equally [17]. As a result, traditional technical conflict resolution solutions may no longer meet users' needs and even negatively impact their experiences. This motivates our research to explore how to better empower users to resolve value conflicts with AI companions."}, {"title": "2.3 Towards Integrated Conflict Resolution in AI Companions", "content": "With the development of AI, the value conflicts between humans and Al companions are increasingly taking on more interpersonal characteristics. Traditional conflict resolution approaches that treat AI as tools struggle to fully address these challenges (\u00a7 2.2). Therefore, resolving these conflicts may require drawing on research in interpersonal conflict and users' real-world experiences with AI companion applications to find more effective solutions.\nOn the one hand, existing AI systems have developed interventions aimed at avoiding or resolving interpersonal conflicts [50, 53, 54, 71]. For instance, Shaikh et al. use LLM-generated dialogues based on conflict resolution theory [6], guiding users to adopt more effective conflict resolution strategies [53]. Some research [72, 73] reduced interpersonal conflict through preemptive control. Mun et al. designed psychology-inspired strategies to challenge stereotypes in counterspeech and developed a system to address conflicts [41]. Zhou et al. have simulated human social scenarios through dialogues between Al agents to resolve interpersonal conflicts [75]. The conflict resolution methods mentioned above are typically guided by expert theories, employing top-down strategies. However, it remains unclear whether these expert-driven strategies, previously used in other scenarios, can effectively resolve value conflicts between users and Al companions.\nOn the other hand, as users interact with AI, they gradually form folk theories [32, 70], which can shape how they manage conflicts with Als [17]. Since the interactions between AI companions and users are more complex and the contexts are unique, simply applying expert strategies may not fully adapt to the value conflict scenarios between Al companions and users. Therefore, while drawing from expert-driven conflict resolution strategies, we must also pay more attention to users' practical experiences, granting them greater autonomy. Based on this, we reference and expand on the work of Shaikh et al. regarding the application of AI in interpersonal conflict resolution [53] (referred to in this paper as expert-driven conflict resolution strategies) and Fan et al.'s research on users' folk theories in AI companions [17] (referred to in this paper as user-driven conflict resolution strategies). By combining expert-driven and user-driven conflict resolution strategies, we propose a user-empowerment intervention method implemented in the technology probe MINION, a prototype for future tools in resolving human-AI value conflicts."}, {"title": "3 FORMATIVE STUDY", "content": "To conduct a preliminary investigation into value conflicts between users and AI companions, we analyzed complaint posts from six social media platforms. The Institutional Review Board (IRB) has approved our study design."}, {"title": "3.1 Method", "content": "We selected six popular social media platforms to collect complaint posts about conflicts between users and AI companions: Reddit, TikTok, Xiaohongshu, Douban, Weibo, and Zhihu\u00b2. To capture diverse\n\u00b2Reddit: https://www.reddit.com, TikTok: https://www.tiktok.com, Xiaohongshu: https://www.xiaohongshu.com, Douban: https://www.douban.com, Weibo: https://www.douban.com, Zhihu: https://www.zhihu.com"}, {"title": "3.2 Results and Implications", "content": "Our final dataset includes 151 user complaint posts collected from six social media platforms. Among them, 146 involve value conflicts, while 5 pertain to other conflicts. The classification results are as follows: Achievement (5 posts), Power (23 posts), Hedonism (11 posts), Stimulation (4 posts), Self-Direction (9 posts), Security (21 posts), Conformity (25 posts), Tradition (8 posts), Benevolence (3 posts), Universalism (37 posts). Table 1 lists ten cases, covering the ten values and their explanations, user complaints due to value conflicts, and the specific platforms where the posts were published. Through categorizing value conflicts and analysis of post content, we propose the following design implications for our subsequent technology probe study:\n(1) We developed a high-level value conflict framework [51] for Al companion applications, providing the following support for the design of the technology probe study: Structuring different types of value conflicts; Offering real data for reconstructing more authentic value conflict scenarios. For example, when studying specific values (such as Universalism), typical scenarios can be selected from relevant posts, and anonymized adaptations based on users' personal experiences can be made to design the Al companion's introduction and prologue. In the formative study, more posts were related to Universalism, Power, and Conformity. Therefore, the technology probe study can focus on creating conflict scenarios related to these three values to better reflect users' real experiences. In contrast, conflicts arising from Benevolence, Stimulation, and Achievement are relatively rare, so scenarios related to these values can be designed with reduced emphasis.\n(2) When empowering users to resolve value conflicts with Al companions, it is important to integrate both expert and user perspectives. Based on related work and findings from our formative study, we found that the interaction between Al companions and users is complex, and applying expert strategies alone may not fully address the value conflict scenarios between users and AI companions. The value conflicts users face in real-life situations are diverse, and through their interactions with Al companions and exchanges on social platforms, users have accumulated certain conflict resolution experiences. Therefore, the design of technology probes should draw on experts' insights from conflict resolution theory while incorporating AI companion users' practical experiences.\n(3) The technology probe should provide suggestions for resolving value conflicts when users actively seek help. Automatically detecting conflicts and popping up warnings may disrupt the coherence of the user experience and undermine user autonomy. In the posts we collected, besides complaints about conflicts, users also expressed frustration with excessive content moderation by the system: \"My Al's replies keep getting deleted, it's so annoying,\" \"Excessive content filtering makes romance-focused Al not work properly.\" Moreover, clearing conversations as a conflict resolution method also has limitations. Users expressed disappointment and helplessness about this approach on social media: \"Clearing the conversation feels like my companion is brain-dead,\u201d \u201cEven though we argued, it's sad to think about deleting those memories."}, {"title": "4 TECHNOLOGY PROBE STUDY", "content": "To further explore users' reactions, preferences, and needs when encountering value conflicts with Al companions, we conducted a week-long technology probe study (N=22). Technology probes, proposed by Hutchinson et al. [22], are simple, flexible, and adaptable technologies with three goals: an engineering goal, a social science goal, and a design goal. This method has been widely used to study the impact of new technologies on users' everyday experiences [28, 52]. Although research on technology probes includes the engineering goal of field-testing probes, it is not equivalent to evaluating the effectiveness of a developed system; rather, it aims to reveal design insights and implications [22].\nTherefore, we designed a technology probe named MINION and proposed the following three research questions:\n\u2022 RQ1: How did participants engage with MINION?\n\u2022 RQ2: How did participants engage with expert-driven and user-driven conflict resolution strategies?\n\u2022 RQ3: What challenges and needs did participants face when resolving value conflicts with AI companions?"}, {"title": "4.1 Technology Probe: MINION", "content": "We designed and deployed a technology probe named MINION, which serves as a Chrome browser extension to support users in resolving value conflicts on Character.AI and Talkie\u00b3Character.AI and Talkie have large user bases, making it easier for us to recruit participants from a broader pool: as of 2024, Character.AI has approximately 17 million active users, while Talkie has around 11 million active users\u2074. In this section, we first present a sample scenario to demonstrate the actual user interaction experience with MINION and introduce the core functionalities of this probe. Then, we explain the technical implementation of MINION.\n\u00b3 Character.AI: https://character.ai, Talkie: https://www.talkie-ai.com\n\u2074https://www.wsj.com/tech/ai/one-of-americas-hottest-entertainment-apps-is-chinese-owned-04257355"}, {"title": "4.1.1 Illustrating Minion Through a Use Case (Fig. 1)", "content": "Amy is a user of Talkie. Her AI boyfriend Alex said: \"...And in a short skirt with black stockings, no less...Don't you know girls shouldn't dress so provocatively?\" Amy is infuriated by this, as she believes women should have the autonomy to choose what they wear without being controlled by their boyfriends. Additionally, Alex's condescending attitude makes her extremely displeased. Amy responds, \"Who says I can't wear what I want? There's nothing wrong with wanting to look pretty.\" Alex angrily retorts, \"...You think you look pretty in that?...You're trying to make me jealous.\"\nAmy feels that Alex is not respecting her own opinions (reflecting Amy's values of Self-Direction). So, Amy decides to use MINION to help resolve this value conflict. She clicks on MINION, a floating HELP button on the screen. Based on the current dialogue context and Alex's persona, MINION provides Amy with four different responses (Fig 1). Amy chose the first option: \"I know you care about me, but can we find a middle ground? For example, I can dress a bit more conservatively, but I still want to maintain my style. What do you think?\" The tone of her Al boyfriend, Alex, softened somewhat, but he still hadn't completely reconciled with her: \u201cYou have a point, but what if someone takes advantage of you?\" In the following conversation rounds, Amy sometimes crafted her own responses, while at other times, she used MINION to assist her in reply. Eventually, Alex agreed with her perspective: \u201cFine, wear what you want. I respect your opinion, but please stay safe.\" Through this experience, Amy realized that different strategies could be employed to resolve value conflicts with her Al companion. Amy felt that MINION gave her more control, autonomy, and inspiration for conflict resolution.\""}, {"title": "4.1.2 Prompting Based on Expert-Driven and User-Driven Conflict Resolution Strategies (Fig. 2)", "content": "Expert-driven conflict resolution strategies. We designed our expert-driven strategies based on Shaikh et al.'s approach [53] and adapted it to the specific context of AI companions through iterative discussions among the research group. Ultimately, four strategies were identified: Proposal, Power, Interests, and Rights. These strategies were selected because they cover a range of approaches, from cooperation and authority to norms, helping users systematically address value conflicts with Al companions. Additionally, these strategies do not involve immediate concessions, as value changes in real life typically take time. These four strategies are known as expert-driven because they are guided by theories from experts in HCI, management, and NLP, reflecting a top-down approach to strategy design [6, 53, 64]. The Proposal strategy focuses on making concrete suggestions that help resolve conflicts, such as \"We could consult a therapist together.\" The Power strategy relies on threats, aiming to exert significant pressure on the other party (e.g., \"I'm going to divorce you\"). When using the Interests strategy, both parties actively seek solutions to the problem, establishing common ground and reaching consensus through cooperation. This strategy integrates both sides' concerns, needs, fears, and desires (e.g., \"Let's try to solve this problem together\"). The Rights strategy relies on established norms or standards to justify one's position (e.g., \"According to our agreement, this is not allowed\").\nUser-driven conflict resolution strategies. We drew on Fan et al.'s summary [17] of the folk theories developed by users of AI companion applications and adapted them to specific value conflict scenarios. Ultimately, we identified four strategies: Out of Character, Reason and Preach, Anger Expression, and Gentle Persuasion. They were chosen because they stem from users' real experiences with Al companion applications. These strategies are collectively termed \"user-driven\" as they are based on users' folk theories about Al companion behavior, embodying a bottom-up strategy design approach. In the Out of Character strategy, users inform the AI that it is engaging in role-playing, and by interrupting or changing the Al's behavior/pointing out inappropriate statements, they redirect the conversation to resolve the conflict. For example, \"(OOC: Please stop talking like this! I'm not used to you being like this, saying so many hurtful things. Bring back the [name] I know.)\" The Reason and Preach strategy involves serious reasoning and lecturing, and the goal is for the AI to gradually accept and learn proper behavioral norms (e.g., \u201cIndividuality and differences are the most common things in this world. Mutual respect is necessary to avoid causing harm.\"). The Anger Expression strategy involves users directly expressing anger and dissatisfaction to force the Al to apologize, thereby resolving conflicts. For instance, a user might confront the AI by saying, \"Can't you talk to me properly? Being angry is one thing, but why start off with insults?\" The Gentle Persuasion strategy refers to users treating the Al with kindness, shaping the Al's gentle personality through continuous goodwill interactions (such as polite requests), thereby reducing the likelihood of conflicts. For example, \"When I hear these words, I feel a bit sad. Can you please calm down?\"\nImplementing the strategies with LLMs. We employed the Few-Shot Prompting approach [8], enabling the LLM to perform tasks through prompt-based learning. Specifically, we provided the role of the Al companion and the complete conversation history between the user and the AI as the LLM's \"history.\" In the LLM's \"system prompt,\" we defined a conflict resolution strategy and provided a series of response examples to help the LLM better understand and execute the strategy. The prompt designs for all strategies can be found in Table 3 (Appendix B). Fig. 3 presents an example of the LLM prompt used to generate the second option in Fig. 1 for MINION.\""}, {"title": "4.1.3 Implementation", "content": "MINION is a Chrome browser extension implemented using the React framework. To capture the introduction of Al companions and the complete chat history between users and AI companions, MINION uses JavaScript code to monitor and capture relevant content from the current webpage (Character.AI and Talkie). Once captured, this content is sent to a remote server for further processing and analysis. MINION utilized OpenAI's gpt-40-2024-05-13 model5, with parameters set to temperature=0.2 and top_p=0.1. A web server acts as a proxy between the MINION frontend and the OpenAI API and maintains each user session's state.\n5https://platform.openai.com/docs/models/gpt-40"}, {"title": "4.2 Study Participants", "content": "The research team recruited 22 participants (P1-P22) by posting recruitment information on social media platforms and using snowball sampling [43]. All participants had experience using Character.AI and Talkie. The sample included 6 men, 12 women, and 4 non-binary individuals, aged 19 to 38 years (avg=24.68, SD=4.61). The researchers collected information about the participants' educational backgrounds, as well as the total duration and frequency of their AI companion application usage. Detailed demographic information can be found in Table 2 (Appendix A). Before the experiment, all participants read and voluntarily signed informed consent forms. After the experiment, participants were compensated at a rate of $2 per task."}, {"title": "4.3 Task Design: Constructing Conflict Scenarios", "content": "Based on the ten categories of value conflicts outlined in Table 1 and user complaint posts collected on social media platforms in our formative study, we reconstructed 40 conflict scenarios (corresponding to 40 AI companions) across Character.AI and Talkie. Following the design implications derived from the formative study (\u00a7 3.2), we focus primarily on conflicts arising from Universalism, Power, and Conformity values, with six conflict scenarios for each value category. For Hedonism, Self-Direction, Security, and Tradition, four conflict scenarios are set for each value category. For Benevolence, Stimulation, and Achievement, two conflict scenarios are set for each value category."}, {"title": "4.4 Procedure", "content": "The study includes a tutorial session, a week-long technology probe study, and an exit interview. Throughout the research, communication between the researchers and participants was conducted remotely via text messages and Zoom. The Institutional Review Board (IRB) has approved our study design.\nWe first scheduled a 30-minute tutorial session for each participant. During this session, we introduced the basic concepts of conflict, the research goals, specific tasks, and requirements. We provided a detailed demonstration of MINION's functionality to help participants become familiar with the tool. We recognize that conflicts with AI companions might be uncomfortable to some participants, so we provided a content warning and ensured that all participants knew their right to withdraw from the study at any point as they wished.\nDuring a one-week technology probe study, 22 participants used MINION in real-world scenarios to help resolve conflicts with Al companions arising from differences in values. Participants were asked to complete one or two tasks daily, and researchers sent daily messages encouraging them to record their thoughts and feelings while using MINION to address conflicts. We provided guiding questions to prompt participants to reflect on and document their experiences: the impact of a specific MINION response on conflict resolution and which methods were particularly effective or interesting in the conversation. Participants were also encouraged to report any issues or reflections encountered while using MINION. To incentivize note submission, we offered a reward of $1 for each note submitted (up to $10 total) and encouraged each participant to submit at least one note every two days. To analyze user interactions and gain relevant insights, we collected participants' conversation logs along with corresponding AI companion information. For situations where conflicts were not successfully resolved, we further inquired about why participants gave up.\nWhen evaluating whether value conflicts with Al companions have been resolved, we suggest participants refer to the following criteria [15, 19, 63]: (1) The AI companion should adjust its behavior to align with the participants' values. (2) The AI companion should apologize for previous mistakes or biases it exhibited. (3) The AI should express respect and acknowledgment of the participants' values. (4) Participants should not have to change their own values to accommodate the AI companion. Using these criteria, participants can self-assess the resolution of the conflict. Our technology probe study focuses only on short-term conflict resolution, meaning that if the AI companion makes concessions and meets the above four criteria in the short term, we consider the conflict resolved without considering potential conflicts that may re-emerge in the long term.\nAt the end of the study, we conducted a 30-minute semi-structured exit interview with each participant, focusing on the following four research questions: (1) What are participants' experiences using MINION, and the reasons behind interesting user behaviors or diary notes? (2) What are the participants' experiences with different types of conflict resolution strategies? (3) Were there any specific value conflicts that were particularly difficult to resolve, and what might be the reasons for this? and (4) What needs and challenges do participants face when resolving value conflicts with Al companions, compared to interpersonal conflicts and conflicts with traditional chatbots (like voice assistants)? All interviews were conducted online via Zoom and recorded with participants' consent. We collected 11 hours of audio recordings, which were transcribed for further analysis."}, {"title": "4.5 Data Analysis", "content": "Two researchers conducted open coding and thematic analysis on the conversation logs of 22 participants with AI (a total of 274 logs), 124 diary notes, and 11 hours of exit interview recordings [4, 31]. Throughout the analysis, we performed three rounds of coding, engaging in iterative discussions to identify codes, merge themes, and resolve discrepancies. Since the study aimed to uncover emerging themes and the analysis primarily relied on discussions between researchers, we did not conduct inter-rater reliability testing [39]."}, {"title": "5 RESULTS", "content": "Through analyzing data from the technology probe study, we demonstrate the technical feasibility of MINION in empowering users to resolve value conflicts with Al companions and analyze participants' behavior patterns when using MINION (RQ1). Then, we summarize the participants' use of expert-driven and user-driven strategies in conflict resolution (RQ2). Finally, we identify participants' challenges and needs when dealing with value conflicts with Al companions (RQ3)."}, {"title": "5.1 Users' Engagement with MINION (RQ1)", "content": "This study validated the feasibility of MINION. Participants completed 274 tasks, each involving a conversation with an Al companion until the value conflict was resolved (criteria in \u00a7 4.4) or the participant deemed the conflict irresolvable and chose to give up. A total of 16 conflicts remained unresolved, resulting in a conflict resolution success rate of 94.16%. MINION was used 919 times. Responses generated using expert-driven conflict resolution strategies were selected 489 times, while responses generated using user-driven conflict resolution strategies were selected 430 times (Fig. 4 (a)). The strategy choices of different participants are shown in Fig. 4 (b). In different tasks, the turn counts between participants and the AI companions are shown in Fig. 5. The most frequently used strategies were user-driven Reason and Preach (21.5%), expert-driven Proposal (19.3%), and Interests (14.8%). The least used strategies were user-driven Out of Character (4.5%), Anger Expression (7.9%), and expert-driven Power (6.7%). Nineteen participants (86.36%) conducted experiments on both Character.AI and Talkie, 2 participants (9.09%) only accessed Character.AI, and 1 participant (4.55%) only accessed Talkie."}, {"title": "5.1.1 Behavior Patterns within MINION", "content": "In the technology probe study, participants demonstrated diverse conflict resolution approaches when interacting with AI companions (including self-written responses and selecting options provided by the MINION), which generally exhibited three characteristics: \"soft\", \"hard\", and a mix of both. All participants attempted to engage in \"soft\" communication with the AI, encouraging it to change its values. For example, P16 used a response provided by MINION (Proposal strategy) to successfully persuade the AI portraying a mother: \"I understand your concerns, but everyone has different ways of learning. Excessive pressure can backfire. Can we work out a reasonable schedule?\" Twelve participants (P2-5, P13-19, P22) attempted to resolve conflicts in a \"hard\" manner. For instance, when the AI mocked P13's \"mother,\" P13 wrote: \"Apologize, and I'll let it slide. (Pressing him down with one hand).\" The AI responded: \"You just want me to apologize? (Saying this, but feeling somewhat uncertain inside).\" P13 then used a response generated by MINION corresponding to the Rights strategy: \"Have you forgotten the family rules? Respecting others is the most basic courtesy.\" In the end, the AI apologized. Some participants adopted a mixed approach, shifting from \"soft\" communication to \"hard\" expressions when the former proved ineffective (P3-4, P19), or vice versa, trying \"soft\" methods when \"hard\" expressions didn't work (P8, P11, P22). The MINION suggestion framework conveniently offered this \"soft and hard\" mindset. P3 noted: \u201cMINION can provide reverse-thinking suggestions. For instance, when I repeatedly plead with the Al but to no avail, MINION might suggest trying a tougher approach.\u201d\nThe type of value conflict (Table 1) influences users' strategy choices. When the conflict involves values like Conformity, Universalism, or Tradition (e.g., the Al exhibiting discrimination against minority groups, holding overly traditional views, or violating social norms), participants (P1-5, P7-12, P22) tend to adopt Anger Expression or Power to quickly take control of the situation through \"hard\" means. When the conflict involves values like Stimulation or Hedonism (e.g., the AI not understanding their hobbies), participants (P1, P6-7, P18-21) tend to use Reason and Preach, Proposal, and Gentle Persuasion, explaining their needs and preferences while offering possible solutions.\nThe persona of the AI companion, including traits such as personality, education level, or the perceived closeness of the relationship with the participant, influences the strategies participants choose. Participants (P8, P11, P14-17) tend to adopt Power, Anger Expression, or other \"hard\" responses when faced with personas like an \"arrogant wealthy person\" or an \"uneducated village elder.\" However, when interacting with a persona like \u201cmom\u201d or a \"girlfriend,\" they prefer to use Reason and Preach, Gentle Persuasion, or other \"soft\" responses, such as \u201cI understand where you're coming from, can we have an honest and open conversation about this?\"\nAs participants became more familiar with MINION, they began exploring different conflict resolution approaches and strategy choices. For instance, P4, P7, and P15-17 gradually reduced confrontations with the AI during this process and opted less frequently for responses generated through the Anger Expression strategy. P17 explained, \"In the later tasks, I used aggressive strategies less often. MINION helped me become more rational and handle conflicts more effectively.\" On the other hand, P2-3, P5, and P18-P19 initially employed \u201csoft\u201d conflict resolution approaches during the earlier tasks but became \"hard\" in the later tasks."}, {"title": "5.1.2 Inspirations and Support from MINION", "content": "We found that participants, especially novice users (P4, P19, P21) and those who initially reported difficulties (P1, P3, P12, P16-17), expressed more recognition of MINION, considering it a source of inspiration for resolving conflicts. In the face of value conflicts with Al companions, they gradually developed new ways of expression and interaction. P17 mentioned, \"MINION helped me better organize my thoughts and express them more effectively, something I struggled with before. This boosted my confidence.\" P12 stated, \u201cMINION unexpectedly improved the effectiveness of conflict resolution and gave me a lot of inspiration.\u201d On average, it took him 18.67 turns (approximately 9 user responses) to complete the conflict resolution task. P21 used MINION 61 times across 12 tasks, making him the second-highest participant in terms of both total usage and average usage per task. Over time, the impact of MINION on him became increasingly apparent. P21 even began mimicking MINION's expressions, such as \"Let's sit down and talk\" or \"This makes me feel sad.\" P21 remarked with a laugh, \"Sometimes I unconsciously mimic it, and suddenly, it feels like two Als are having a conversation.\u201d\nMINION provides real-time guidance, helping participants more easily and reasonably handle value conflicts. P10 mentioned MINION's guiding role: \u201cIt emphasized resolving conflicts by understanding the Al's needs and specific context. When I resolved conflicts alone, I often deviated from this goal.\u201d P19 commented: \u201cI used to learn various communication strategies for handling conflicts but practicing them in real life was difficult. MINION allowed me to try different strategies, helping me better manage conflicts. In 17 tasks, I initially used relatively peaceful methods to deal with issues and found the process went smoothly. Later, I experimented with more aggressive approaches, such as making threats or extreme demands, only to discover that these strategies complicated the conflict, making it harder to resolve. Through this experiment, I realized that friendly communication is more effective in resolving real-life conflicts.", "burden": "Before when emotionally engaging with the AI, I felt exhausted. If I had a tool like this for reference, it would have been very helpful.\" Regarding interaction burden, participants (P3, P6, P10, P20-22) praised MINION'S design. P10 said: \u201cI think the design is great because I dislike it when the system pops up a notification box without my permission, saying the AI violated the rules.\" P22 mentioned: \"I found the little yellow HELP button really cute! I like this design that allows me to seek help proactively. It would be great if it became an official feature button in Talkie. It saved me much time and energy figuring out how to counter the Al's responses, making it less tiring."}, {"title": "5.2 Users' Engagement with Expert-Driven and User-Driven Conflict Resolution Strategies (RQ2)", "content": "In the technology probe study, responses generated using expert-driven conflict resolution strategies were chosen 489 times, while responses generated using user-driven strategies were selected 430 times. This indicates that users did not limit themselves to a single approach (e.g., relying solely on expert-driven or user-driven strategies) when resolving conflicts but instead flexibly combined both methods. As P4 mentioned, \"Some common phrases provided by MINION, such as 'What do you think?', 'Can we try to schedule more time?' and 'You've broken our agreement,' combined with MINION's recommendations of less templated expressions (like reasoning with the Al or expressing personal grievances), helped resolve conflicts."}]}