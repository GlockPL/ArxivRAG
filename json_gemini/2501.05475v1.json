{"title": "Retrieval-Augmented Generation by Evidence Retroactivity in LLMS", "authors": ["Liang Xiao", "Wen Dai", "Shuai Chen", "Bin Qin", "Chongyang Shi", "Haopeng Jing", "Tianyu Guo"], "abstract": "Retrieval-augmented generation has gained significant attention due to its ability to integrate relevant external knowledge, enhancing the accuracy and reliability of the LLMs' responses. Most of the existing methods apply a dynamic multiple retrieval-generating process, to address multi-hop complex questions by decomposing them into sub-problems. However, these methods rely on an unidirectional forward reasoning paradigm, where errors from insufficient reasoning steps or inherent flaws in current retrieval systems are irreversible, potentially derailing the entire reasoning chain. For the first time, this work introduces Retroactive Retrieval-Augmented Generation (RetroRAG), a novel framework to build a retroactive reasoning paradigm. RetroRAG revises and updates the evidence, redirecting the reasoning chain to the correct direction. RetroRAG constructs an evidence-collation-discovery framework to search, generate, and refine credible evidence. It synthesizes inferential evidence related to the key entities in the question from the existing source knowledge and formulates search queries to uncover additional information. As new evidence is found, RetroRAG continually updates and organizes this information, enhancing its ability to locate further necessary evidence. Paired with an Answerer to generate and evaluate outputs, RetroRAG is capable of refining its reasoning process iteratively until a reliable answer is obtained. Empirical evaluations show that RetroRAG significantly outperforms existing methods.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), such as ChatGPT(OpenAI 2022) and ChatGLM(THUDM 2024), have demonstrated outstanding performance across a wide range of natural language processing tasks. However, despite the vast amount of knowledge stored during training, these models still exhibit a tendency to generate hallucinatory content, resulting in unverified or factually incorrect answers(Huang et al. 2023a; Wu, Wu, and Zou 2024). To address this issue, the Retrieval-Augmented Generation (RAG) framework is leveraged to acquire and subsequently inject relevant external source knowledge into the LLM's prompt, significantly enhancing the accuracy and reliability of LLM's responses(Lewis et al. 2020; Mao et al. 2021; Chen et al. 2024).\nTraditional retrieval-augmented models typically retrieve and extract knowledge documents once based on the initial query, these approaches struggle with addressing multi-hop complex questions due to insufficient knowledge. To tackle this issue, recent studies have transformed the single retrieval-generating into a dynamic multiple retrieval-generating process. These approaches decompose the complex question into several sub-questions, and obtain the final output by answering all these sub-questions(Shao et al. 2023; Trivedi et al. 2023; Press et al. 2023; Yao et al. 2023). Even though the latest approaches have been proposed to improve the effectiveness of knowledge documents retrieval(Asai et al. 2024; Xu et al. 2024), current RAG frameworks are susceptible to the threat of insufficient reasoning from the documents which are factual, and related, but irrelevant due to the inherent flaws of current retrieval systems(Wu et al. 2024), and cause external hallucination. As illustrated in Figure 1, the excessive focus on the local sub-question of who recruited Beckham? obtaining the incorrect answer of Eric Harrison, the youth coach of Manchester United who recruited Beckham in the youth team but never as the manager, rather than the manager at time Alex Ferguson, and mislead the following reasoning steps.\nWe argue that this flaw originates from the Unidirectional Forwards reasoning paradigm inherent in traditional RAG methods. In this paradigm, any errors produced during reasoning steps are irreversible for the whole reasoning chain. Although the paradigm can be altered by enabling LLMs to continuously reason from scratch through the cumulative retrieval of documents(Trivedi et al. 2023; Zhou et al. 2024), due to the limited useful information in documents, excessive retrieval would introduce much more noise information which distracts LLMs to engage in over-reasoning and build erroneous correlation, thereby generates hallucinatory content(Yu et al. 2023; Chiang and Lee 2024; Wu et al. 2024; Liu et al. 2024).\nTo address these issues, we refer the investigative process of the detective, that iteratively collates evidence to gather all related factual information, and through evidence discovery process to validate the relevance of evidence then update them, while uncovering unresolved issues along the way, to ultimately reach a valid conclusion. By continuously revising and reconstructing the reasoning chain through the evidence collation and discovery process, a comprehensive and definitive conclusion can be obtained(Findley 2011; Fahsing 2022). This evidence-collation-and-discovery structure allows LLMs to rethink and revise the reasoning chain through a Retroactive paradigm, correcting previous errors resulting from insufficient information by utilizing newly discovered evidence. As illustrated in Figure 1, LLMs can collate the evidence that Eric Harrison was the youth coach of the manager Alex Ferguson since 1986, and Ferguson was actually the manager, then update the reasoning process and answer correctly. Inspired by this detective-like approach, we introduce the Retroactive Retrieval-Augmented Generation framework (RetroRAG).\nTo effectively generate and utilize evidence, there are two aspects need to be considered: (1)The Effectiveness: the evidence should align with external inherent knowledge(Flores and Woodard 2023), while being attributable(Gao et al. 2023) and relevant(Liu et al. 2024), to avoid the irrelevant noise; (2)Dynamic Updating: the evidence should be continuously updated based on newly discovered information and overarching question, rather than being confined to local sub-questions. Hence, RetroRAG constructs Evidence-coLLation-and-discovERY framework (ELLERY) to retrieve, generate, and update the evidence, which involves two major components: (1)Evidence Collation retrieves relevant documents from the retrieval corpus as the source evidence, which would be utilized as doubtless material to generate inferential evidence, besides serving as the primary reference for the answering. The source evidence will be continuously updated as the question-solving process progresses, to mitigate the influence of retrieved irrelevant information; (2)Evidence Discovery first generates as much inferential evidence related to the key entities in question as possible from the source evidence. Then, inferential evidence will be filtered from the perspectives of relevance to the question and its attribution to the source evidence, to ensure the effectiveness. Inferential evidence would also be updated to only remain the most relevant parts. While both evidence would be used to help generate answers, the gap between the stored evidence and the initial question would be simultaneously analyzed, and the search-query would be proposed for retrieving more information in need.\nAlong with the Answerer to generate and evaluate the answer, RetroRAG provides an approach for refining effective reasoning chains through credible evidence. The experimental results on two multi-hop question answering (QA) datasets verify the effectiveness and state-of-the-art performance of RetroRAG, while also demonstrating the explainability in the reasoning process.\nThe contributions of this paper are summarized as:\n\u2022 We introduce RetroRAG approach, an innovative retrieval-augmented generation framework. Unlike existing RAG approaches uses an unidirectional forwards reasoning paradigm that cannot reverse the error in preceding reasoning steps, RetroRAG uses a retroactive reasoning paradigm that can revise and reconstruct the reasoning chain through two types of evidence, provides effective answers with less hallucination.\n\u2022 To the best of our knowledge, this is the first time an evidence-collation-and-discovery framework has been proposed and used in a retrieval-augmented framework, which generates and updates the evidence to support the reasoning process, significantly enhances knowledge retrieval performance on question-answering tasks."}, {"title": "RELATED WORK", "content": "Hallucination in Large Language Model\nCurrently, hallucination is referred as generated content that either does not align with real-world facts or deviates from the source material and self-consistency(Huang et al. 2023b; Ye et al. 2023). In the context of question-answering tasks, hallucination specifically manifests as the generation of arbitrary, and incorrect answers. This phenomenon occurs because, in cases of hallucination, the internal consistency of the generation process in LLMs is unstable. (Manakul, Liusie, and Gales 2023; M\u00fcndler et al. 2024; Farquhar et al. 2024). Some studies consider addressing the hallucination problem based on the tendencies of generation from LLMs, they generate multiple outputs and then employ a majority voting strategy to obtain relatively reliable answers(Wang et al. 2022; Huang et al. 2022). More studies consider that the inconsistency generation of LLMs stems from the lack of knowledge, therefore, they introduce reliable external knowledge through the Retrieval-Augmented Generation (RAG) framework, to enhance the factual or specific domain knowledge of LLMs(He, Zhang, and Roth 2022; Gao et al. 2023; Siriwardhana et al. 2023; Ram et al. 2023). Besides, some methods enhance the LLMs to better perceive factual information by fine-tuning the model with the external knowledge (Lee et al. 2022; Tian et al. 2023).\nRetrieval Augmented Language Model\nMany studies have demonstrated the impressive performance of the retrieval-augmented language model (RALM) in various natural language tasks, which is enhanced by the provision of detailed and specific external knowledge to supplement LLMs(Lewis et al. 2020; Guu et al. 2020; Shi et al. 2024). These models typically employ a retriever to obtain a set of relevant documents from a knowledge corpus, such as Wikipedia, to enhance the effectiveness of the answers. While initial RALM performs the single-time retrieval strategy, which extracts knowledge once based on the user's initial query(He, Neubig, and Berg-Kirkpatrick 2021; Izacard and Grave 2021; Ram et al. 2023), recent studies have focused on multi-time retrieval models to overcome the issues"}, {"title": "Methodology", "content": "Existing retrieval augmented methods, due to their unidirectional forward reasoning paradigm, are prone to the risk of external hallucination from factually related irrelevant documents. Since the process of answering decomposed sub-questions can be equivalently regarded as the process of obtaining sub-evidence to answer the initial question, as illustrated in Figure 2, previous approaches have employed a linear paradigm of progressive sub-evidence generation, where the generation of each node is highly depends on the previous nodes. Although the verification of knowledge can prevent the emergence of subsequent unreliable node B, it is incapable of correcting erroneous validation node caused from the inherent flaws of current retrieval systems, like Eric Harrison in Figure 1 when it has reached node D. The LLMs will propagate the erroneous information as definitive knowledge, leading to inaccuracies in the following output.\nBy generating and updating of evidence, RetroRAG enables LLMs to refine their knowledge by integrating newly evidence D, J, K, with the previous evidence A, B,\n based on the relevance to the question, retaining only the most pertinent evidence. This process allows for a more comprehensive understanding as new evidence is integrated(for instance, Eric Harrison is the youth coach of Alex Ferguson), while erroneous nodes at any stage are discarded (for instance, Harrison recruited Beckham as manager will be correct). The correct nodes A, D, K will be preserved to the next stage. Essentially, it can be considered that RetroRAG constructs a graph-based thinking structure.\nOverview\nWe propose Retroactive Retrieval-Augmented Generation (RetroRAG) framework to tackle the issue of insufficient reasoning and over-reasoning, which applies Answerer to generate and evaluate credible answers, and Evidence-coLLation-and-discovERY (ELLERY) framework retrieves, generates, and updates the evidence.\nIn the QA scenario, the target of RetroRAG is to generate an answer $a$ to the given question $q$ with key entities $k$. As illustrated in Figure 3, RetroRAG utilizes the iteration application of two processes: (1) Answerer generates an answer based on the current knowledge context, and determines if a consistent response can be generated within the current knowledge context. (2) ELLERY obtains documents $D_Q$ from the retrieval corpus $D = \\{d_i\\}_{i=1}^{|D|}$ (with Wikipedia dumps served as the primary data source in this study) related to the question, as the source evidence, and by which generates credible evidence $E$, and re-queries $q_r$ based on $q$, $k$, $D_Q$, and the last reasoning chain $r$. ELLERY continues this process of collating and discovering evidence until a definitive answer is obtained by Answerer. The details of the prompts we designed and used will be introduced in Section A of the Appendix.\nAnswerer\nThe main target of Answerer is to generate a reliable answer to the question. To achieve this, the Answerer first generates pseudo-answer $a$ and corresponding reason $r$ with the current quantities of information $E$ through an answer-generator, which utilizes COT-prompt $M_C$ with a low-temperature parameter to obtain a more fixed output.\nTo assess whether the current knowledge context is sufficient, we refer Self-Consistency (SC) concept that outputs of LLMs should converge towards the correct answer under a strong knowledge context, with which LLMs are capable of constructing reliable reasoning and answers. To this end, we employ an SC-generator with a direct-answering-prompt $M_D$ and high-temperature parameter to obtain monitoring answer $a_{SC}$ with more divergent thinking pattern and the same knowledge content of $a$, and the similarity between these two outputs can assess the self-consistency score and the degree of hallucination. We designed an LLM-based evaluator $S$ to convert scores, e.g., similarity or relevance, into the probability of generating indicative tokens (e.g., 'yes' or 'no'). We calculate the similarity scores $S_{SC}$ through LLM-based evaluator $S_{SC}$ as the following formulation:"}, {"title": null, "content": "$S_{SC} = S_{SC}([a, a_{SC}, q], M_{SC})$\n$S_{sc} = \\frac{P(x ='yes'|([a, a_{sc}, q], M_{sc}))}{\\sum_{i \\in \\['yes', 'no'\\]} P(x = i|([a, a_{sc}, q], M_{sc}))}$\n$S_{sc} = S_{sc}([a, a_{sc}, q], M_{sc})$"}, {"title": null, "content": "Where $M_{sc}$ is a customized prompt, and a threshold $t$ is utilized to govern the model's output, only when $s_{sc} > t$, the iteration process is stop, and output the answer as the final answer. A higher value of $t$ implies that a more stringent requirement of knowledge context. And follow the research from (Zhou et al. 2024), a declarative assessor is implemented to ensure the standardization of the answer.\nEvidence Collation and Discovery\nIn each round of iteration, the initial query $q$ and its key entities $k$ will be fixed as constant input for ELLERY structure. The ELLERY structure obtains and updates source evidence through Evidence Collation, while based on which generating inferential evidence and proposing re-querie to obtain the required knowledge through Evidence Discovery.\nEvidence Collation In the L-th iteration, we use the search query generated from end of the last round $q_r^{(L-1)}$,which is designed to specifically target missing information, to retrieve relevant passages $D_Q^{(L)}$. And concatenate $q_r^{(L-1)}$ with $q$ to obtain the matching query $q_m^{(L)} = [q, q_r^{(L-1)}]$, which is designed to match the most relevant evidence within the current knowledge context, while avoiding the issue of deviating from the initial question by focusing too much on the generated search query. Specifically, we set $q_r^{(0)} = q_m^{(0)} = q$ as the initialization. After obtaining $D_Q^{(L)}$, we merge them with the last stored source evidence $E^{(L-1)}$, and apply the customized prompt $M_E$ with LLM to individually score each source evidence candidate, ranging from 0 to 1, to assess the contribution of source evidence to answering the question. Since source evidence should be updated to ensure the progression of current answering process, the scoring process of evaluators $S_E$ can be formulated as:"}, {"title": null, "content": "$S_E^{(L)} = S_E ([E^{(L-1)} \\cup D_Q^{(L)}], q^{(L)}, M_E))$"}, {"title": null, "content": "Based on $S_E^{(L)}$, we can extract the top-N source evidence as the current source evidence $E^{(L)}$, which would be sent to Answerer to help the answering process in the L-th round, and be used to generate the inferential evidence and the re-query in the failure answering case.\nEvidence Discovery Follow the actual detective, we uses Deductive method to generates inferential evidence, which contains two steps: (1)Deductive Reasoning: we utilize an LLM-based evidence generating prompt $M_{IE}$ to generate inferential evidence candidates $e_{ic}^{(L)}$ related to $k$ from $E^{(L)}$, to obtain as much deductive inference to the initial question as possible, from the current retrieval documents. (2)Hypothesis Testing: we design two specified LLM-based gated prompt $M_{qr}$ and $M_{ra}$ to calculate the Question-Relevance (QR) score and the Reference-Attribution (RA) score with their LLM-based evaluator $S_{qr}$ and $S_{ra}$, to ensure the effectiveness of each inference. The specific functions of these two evaluation scores are as follow:\n\u2022 Question-Relevance (QR): On knowing the last inferential evidence $E_i^{(L-1)}$, if $e_{ic}^{(L)}$ could be directly related to answering the matching query $q_m^{(L)}$:\n\u2022 Reference-Attribution (RA): If the claim of $e_{ic}^{(L)}$ can be directly found in any claims of $E^{(L)}$.\nThrough these step, we only reserve the useful and confirmed inference as evidence, and this process can be formulated as:"}, {"title": null, "content": "$S_{qr}^{(L)} = S_{qr}([e_{ic}^{(L)}, E_i^{(L-1)}, q_m^{(L)}], M_{qr})$\n$S_{ra} = S_{ra}([e_{ic}^{(L)}, E^{(L)}], M_{ra})$\n$e_{ic}^{(L)} = ((s_{qr} > 0.5) \\cap (S_{ra} > 0.5))e_{ic}^{(L)}$"}, {"title": null, "content": "Next, we merge $e_{ic}^{(L)}$ and last inferential evidence $E_i^{(L-1)}$. Following the updating process of source evidence, we apply the same evaluator $S_E$ to score each inferential evidence candidate. Since inferential evidence should align with the initial question for a long term memory, the scoring process of evaluators $S_E$ can be formulated as:\n$S_E^{(L)} = S_E ([E_i^{(L-1)} \\cup e_{ic}^{(L)}, q], M_E)$\nWe select the top-K inferential evidence as current inferential evidence $E_i^{(L)}$ in the same way, through which achieving the revising and reconstructing of reasoning nodes. And, $E_i^{(L)}$ would be sent to Answerer as referenced evidence in the (L+1)-th round.\nIt is important to note that since $E^{(L)} = E_i^{(L)} \\cup E_i^{(L-1)}$, and current quantities of information $E^{(L)} = E_i^{(L)} \\cup E_i^{(L-1)}$, no new information is brought in after the updating of $E_i$. If Answerer fails to provide an effective answer with the knowledge context $E^{(L)}$, it is necessary to retrieve more source evidence to fill the knowledge gap. To achieve this, we should first know the information $E^{(L)}$ LLM already have right now, and the reason $r$ why LLM can't (or wrongly) answer the question based on these information, then generate a new query to further retrieve information from the corpus to answer the initial question $q$. Hence, we construct a LLM-based re-query generator $G_R$ with customized prompt $M_R$, to generate search query $q_r^{(L)}$ to deduce what information LLM needs to answer the question:\n$q_r^{(L)} = LLM([E^{(L)}, E_i^{(L)}, r, q], M_R)$\nExperiments"}, {"title": null, "content": "In this section, we evaluate the effectiveness of our proposed model on two multi-hop question answering (QA) datasets.\nExperimental Setup\nDatasets and Evaluation Metrics We conduct experiments on two multi-hop question answering datasets: HotpotQA(Yang et al. 2018) and 2WikiMQA(Ho et al. 2020). Since both of the datasets are constructed based on Wikipedia documents, we use the same document corpus and retrievers to provide external references for LLMs. Due to the constraints of experimental costs, following(Zhou et al. 2024), we sub-sample 500 questions from the validation set of each dataset for experiments.\nFor evaluation metrics, we use exact match (EM) as our standard metrics at answer-level, to measure whether the predicted answer is completely consistent with the standard answer. And we use token-level F1, precision (Pre) and recall (Rec) for comprehensive evaluation at token-level, to evaluate the proportion of correct answer tokens in the overall tokens.\nBaselines We compare our RetroRAG to recent baseline approaches: Standard Prompting(Brown et al. 2020), Chain-of-Thought(Wei et al. 2023), Standard RAG(Lewis et al. 2020), ReAct(Yao et al. 2023), Self-Ask(Press et al. 2023), IR-COT(Trivedi et al. 2023), SearChain(Xu et al. 2024), and MetaRAG(Zhou et al. 2024). We comprehensively describe each baseline in Appendix B.1 and explain the rationale behind selecting these specific baselines.\nSettings We choose GLM4-9B-chat(THUDM 2024) LLM as the base LLM for all baseline and our RetroRAG approach with the temperature setting of 0.01, except the SC-generator of our RetroRAG whose temperature is set to 1.00. We utilize the Wikipedia dump(Karpukhin et al. 2020) as the document corpus for both datasets, where articles are segmented into passages of 100 tokens. We employ the BM25 algorithm(Robertson, Zaragoza et al. 2009) and SimLM retriever(Wang et al. 2023) to retrieve the top 5 relevant passages to be the external knowledge for all approaches. And we set a default judgment threshold for our answering evaluation mechanism at 0.7 to ensure consistency of answers. The maximum number of both iterations and the size of the evidence repository are set to 5.\nMain Results\nPerformance on multi-hop question answering datasets is shown in Table 1. It can be observed that:\n(1) Our proposed RetroRAG consistently surpasses all baseline methods across two datasets. At answer-level, the performance improvement on EM is +8.8 on HotpotQA and +5.2 on 2WikiMQA compared to the best baseline results; At token-level, the performance improvement on F1 is +10.6 on HotpotQA and +4.0 on 2WikiMQA compared to the best baseline results. This suggests that when directly employing LLM, without additional pre-training or fine-tuning, our approach exhibits optimal performance.\n(2) When compared to SearChain, which adapts the unidirectional forward reasoning paradigm but verifies each node in COT and outperforms other methods using the same paradigm, such as COT, ReACT, Self-Ask, etc., RetroRAG shows an improvement of +11.6 on HotpotQA and +5.2 on 2WikiMQA. This reflects that the retroactive reasoning paradigm RetroRAG uses can solve the issue of local insufficient reasoning, and provides more comprehensive reasoning, thereby improving the performance markedly.\n(3) When compared to IR-COT and MetaRAG, which also do not adhere to the paradigm of linear reasoning, but increase the quantity of retrieved documents to re-generate the answer, RetroRAG shows an improvement of +8.8 on HotpotQA and +9.2 on 2WikiMQA. This reflects that the evidence-collation-and-discovery framework RetroRAG uses can address the issue of over-reasoning, and mitigate the irrelevant and noisy information from knowledge documents, thereby improving the performance significantly.\n(4) Compared with Standard Prompting and COT approaches, both the idea of decomposing the initial query into multiple sub-questions and iteratively increasing the amount of knowledge retrieved can doubtlessly improve the ability of reasoning of LLMs. When multi-hop questions"}, {"title": null, "content": "Ablation Study\nTo verify the effectiveness of different components in RetroRAG, we conduct a comparative analysis respectively focus on frameworks, including Answerer (solely on the Answer Evaluation (AE) mechanism since Answerer needs to generate answer output) and ELLERY; Besides, we also focus on the designs of evidence utilization, including Source Evidence (SE), Inferential Evidence (IE), Updating of Evidence (UoE) and Evaluation of Evidence (EoE).\nAblation of structure As shown in Table 2, it is evident that removing either of the frameworks adversely affects the performance of both datasets. Specifically, the removal of AE makes the LLM consistently set to a state of knowledge deficiency, thereby posing the risk of over-reasoning that turns the correct reasoning path built, into the incorrect one. Meanwhile, the removal of ELLERY would result in a complete absence of external knowledge, thereby leading to a significant decline in performance. This emphasizes the notion that many hallucinations stem from an insufficiency in external knowledge.\nAblation of evidence utilization To conduct a more detailed analysis of the mechanisms of ELLERY, we performed ablation studies based on the characteristics of the evidence and the methods of evidence processing. Experimental results show that due to the complementary design, both source evidence and inferential evidence are crucial. Inferential evidence provides a summary of past effective information, thereby helping enhance the performance of LLMs. However, since it contains much less information than source evidence, source evidence has a greater impact on the quality of responses. Besides, the updating and evaluating of evidence can alleviate the introduction of irrelevant information, ensuring the progression of the current answering process while aligning with the initial question, thereby enhancing performance.\nQualitative Analysis\nTo better delve into the impact of the numbers of iterations and evidence, as well as similarity thresholds, we have embarked on a series of qualitative experiments.The results are shown in Figure 4. It can be observed that:\nDifferent maximum numbers of iterations Although the Answer Evaluation mechanism plays a significant role in the performance of RetroRAG, the maximum number of iterations also has a substantial impact on the final results.As depicted in Figure 4, we can find that on both datasets, the accuracy of RetroRAG improves progressively before the iterations reach 3, and then grows relatively stable, peaking when the iterations reach 5 or 6. After the peak, we observed a risk of decline in the performance with the increase of iterations. We think these results suggest that, by increasing the number of iterations, RetroRAG can extract more effective evidence, thereby improving performance. However, excessively increasing may cause over-reasoning and introduce noise information, which in turn to a decline in performance.\nDifferent numbers of evidence For the convenience of the experiment, we set the numbers of both source evidence and inferential evidence to the same.Based on this setting, we design a series of experiments to investigate the impact of the number of evidence.As depicted in Figure 4, we find that on both datasets, increasing the numbers of evidence from 1 to 3 can significantly improve the performance of RetroRAG, which reflects that under the condition of insufficient information, it is difficult for LLMs to perform effective reasoning. The accuracy of RetroRAG reaches the peak at the number of 5 pieces of evidence, and then slowly decreases as the increase of the evidence. This suggests that although the filtering and updating mechanism can suppress irrelevant information to some extent, an excessively large evidence window can still introduce noise and impair performance.\nDifferent similarity thresholds Although threshold t > 0.5 signifies that the LLM evaluator considers the result to be valid, a higher t represents the LLM has more confidence in the determination.This could lead to a more reliable monitor but might also result in overly stringent requirements for the results, causing misjudgments and excessive iterations.As illustrated in Figure 4, we find that for HotpotQA dataset, t = 0.5 can provide a good discriminative effect, but for 2WikiMQA, LLMs need a higher threshold which is about 0.7 to ensure the effectiveness.And for both datasets, excessively high thresholds lead to a decline in performance. This is also related to the over-reasoning issue.\nCase Study\nDue to factually related irrelevant documents from the inherent flaws of the current retrieval system, previous approaches cause issues of insufficient reasoning and over-reasoning, respectively.To address these issues, we introduce inferential evidence as a form of knowledge caching.By summarizing and updating past relevant information, this approach enables retroactive awareness when new knowledge is introduced, while simultaneously reducing interference from irrelevant information. The case in Figure 5 provides an intuitive demonstration of the differences between our RetroRAG and previous approaches, traditional RAG stops reasoning due to insufficient retrieval information; SearChain makes the correct decomposing process but answers wrongly due to the related irrelevant knowledge; MetaRAG is interfered with excessive retrieval documents and makes the wrong reasoning process. Being aware of the evidence, RetroRAG makes the correct reasoning process that leads to an accurate answer.\nEffectiveness of evidence\nTo better explore the effectiveness of Inferential Evidence, we divided the data into two categories based on the presence of evidence generated through Question-Relevance and Reference-Attribution evaluations during the iterative process. We then analyzed the differences in performance on the main baselines. As shown in Figure 6, we find in scenarios where Inferential Evidence is generated, iteratively increasing the amount of knowledge performs better compared to decomposing sub-questions. We believe this because the retrieval knowledge is extensively relevant and mutually corroborative, enhancing the LLM's performance. Furthermore, our work enhances the LLM's understanding of knowledge by generating Inferential Evidence, thereby further improving the reliability of its responses. Additionally, the absence of Inferential Evidence generation may be due to the sparse distribution of required knowledge within individual documents. In such cases, decomposing sub-questions allows the LLM to better understand and answer the questions. Moreover, our work ensures the accumulation of effective information through Source Evidence updating, thereby also enhancing the LLM's performance."}, {"title": "Conclusion", "content": "In this paper, we point out the threat from the unidirectional forward reasoning paradigm inherent in traditional RAG methods, within which any errors produced during reasoning steps are irreversible and affect the whole reasoning chain. We then introduce RetroRAG, a novel framework that uses a detective-like retroactive reasoning paradigm that can revise and reconstruct the reasoning chain, ensuring it on the correct direction. Through the evidence-collation-discovery framework, RetroRAG can search, generate, and update credible evidence, empower the model to perceive existing information, and seek out more necessary evidence to complete the reasoning process. Experimental results on two multi-hop QA datasets demonstrated that RetroRAG performs better than all baselines. In the future, we aspire to explore the possibility of allowing LLMs to independently learn the aforementioned evidence-collation-discovery process through methods such as fine-tuning or pre-training."}, {"title": "A. Prompt Detail", "content": "We show the prompt used in experiment on both datasets in Figure 7 to Figure 13. In this work, we constructed the few-shot CoT prompt, and the declarative assessor prompt by referencing (Zhou et al. 2024), and design different prompt for the corresponding functions. For all calculations as detailed in the Methodology section, we quantified the results by generating probability distributions of the 'yes' and 'no' tokens."}, {"title": "B. Experimental Details", "content": "Baselines\nWe compare our proposed model with several state-of-theart baselines listed as follows:\n\u2022 Standard Prompting(Brown et al. 2020): Standard Prompting directs LLM to answer the queries with a simple question-answer prompt.\n\u2022 Chain-of-Thought(Wei et al. 2023): Chain-of-Thought provides a few-shot prompt to LLM, make it can answer with a reasoning process.\n\u2022 Standard RAG(Lewis et al. 2020): Standard RAG first retrieves multiple documents by query, then inject these documents into prompts to LLM for answering.\n\u2022 ReAct(Yao et al. 2023): ReAct introduces a reasoning and acting paradigm, alternately executing reasoning and task-specific actions to complete the QA task.\n\u2022 Self-Ask(Press et al. 2023): Self-Ask introduces a paradigm that decomposes questions into sub-questions, continuously engaging in self-questioning until the final answer is obtained.\n\u2022 IR-COT(Trivedi et al. 2023): IR-COT iteratively alternates between using COT reasoning to guide retrieval, and utilizing retrieval results to enhance CoT reasoning, continuing executing until the final answer is obtained.\n\u2022 SearChain(Xu et al. 2024): SearChain introduces the concept of search-in-chain,\" which corrects the reasoning process through the interaction between Information Retrieval (IR) and Chain-of-Query (COQ), let IR provides the knowledge that LLM really needs.\n\u2022 MetaRAG(Zhou et al. 2024): MetaRAG combines the RAG process with metacognition, allowing LLM to execute different actions based on the reliability of internal and external knowledge, identify the sufficiency of knowledge and potential errors during reasoning.\""}, {"title": "Additional Case Studies", "content": "We present additional cases to further demonstrate the effectiveness of our proposed RetroRAG approach. As shown in Figure 14, we find that RetroRAG not only leverages the effective information from inferential evidence to make accurate reasoning, but can also compensates for insufficient inferential evidence by retrieving complementary source evidence through re-query, thereby enabling correct reasoning, which also highlights the necessity of leveraging both inferential evidence and source evidence."}, {"title": "C. Assessment using LLM-judge across Various Scenarios", "content": "To validate the transferability of our approach across different linguistic contexts, and its reliability in simple question-answering scenarios, we conducted performance evaluations on the knowledge question-answering segments of a Chinese hallucination evaluation dataset HalluQA(Cheng et al. 2023), which contain single-hop question build from Baidu Baike. We utilize the Baidu Baike dump as the document corpus for HalluQA datasets, where articles are segmented into passages of 100 tokens, and we employ the BM25 algorithm to retrieve the top 5 relevant documents to be the external knowledge. Given that the golden truth in HalluQA contains detailed descriptions, making it challenging to quantify performance using token-level metrics, we employed the GPT4 as the LLM-Judge, with the same setting of (Cheng et al. 2023), to assess the answer semantic accuracy.Additionally, we simultaneously applied the same LLM-Judge to evaluate the performance on HotpotQA and 2WikiMQA dataset from the semantic perspective."}]}