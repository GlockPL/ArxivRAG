{"title": "TAPVid-3D:\nA Benchmark for Tracking Any Point in 3D", "authors": ["Skanda Koppula", "Ignacio Rocco", "Yi Yang", "Joe Heyward", "Jo\u00e3o Carreira", "Andrew Zisserman", "Gabriel Brostow", "Carl Doersch"], "abstract": "We introduce a new benchmark, TAPVid-3D, for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). While point tracking in two dimensions (TAP) has many benchmarks measuring performance on real-world videos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To this end, leveraging existing footage, we build a new benchmark for 3D point tracking featuring 4,000+ real-world videos, composed of three different data sources spanning a variety of object types, motion patterns, and indoor and outdoor environments. To measure performance on the TAP-3D task, we formulate a collection of metrics that extend the Jaccard-based metric used in TAP to handle the complexities of ambiguous depth scales across models, occlusions, and multi-track spatio-temporal smoothness. We manually verify a large sample of trajectories to ensure correct video annotations, and assess the current state of the TAP-3D task by constructing competitive baselines using existing tracking models. We anticipate this benchmark will serve as a guidepost to improve our ability to understand precise 3D motion and surface deformation from monocular video. Code for dataset download, generation, and model evaluation is available at https://tapvid3d.github.io/.", "sections": [{"title": "1 Introduction", "content": "For robots, humans, and other agents to effectively interact with the physical 3D world, it is necessary to understand a scene's structure and dynamics. This is a key ingredient to any general embodied intelligence: the ability to learn a world model to understand and predict the structure and motion of arbitrary scenes. It is attractive to leverage the vast amounts of monocular video data that is available cheaply, and use such signals to understand the geometry and 3D motion in real-world footage. But how well can current perception algorithms actually do this?\n\nThe field has seen many efforts to measure 3D and motion understanding from videos, each contributing a part of the overall goal. For example, monocular depth estimation is a widely recognized task [7, 36, 63]. However, success in depth estimation alone does not reveal whether the model understands how surfaces move from one frame to the next, and may not respect temporal continuity. For instance, a basketball spinning on its axis is often not visible in a sequence of depth maps. On the other end of the generality spectrum, 3D pose tracking, e.g. for rigid objects [14, 34] and people [12, 49], evaluates precise motion, but requires a known 3D model of each object with articulations. Building parametric models and pose estimation methods for all animal classes is infeasible, much less for all the objects that a robot might encounter in, for example, an unseen, busy construction site.\n\nAn alternative and more general approach for dynamic scene understanding observes that the world consists of particles, each of which individually follows a 3D trajectory through space and time [58, 62]. Measuring the motion of these points provides a way to measure 3D motion without requiring any"}, {"title": "2 Related work", "content": "Tracking Any Point. In recent years, long-range tracking of local image points has been formalized as the Tracking Any Point task (TAP) and evaluated using the, now standard, TAPVid benchmark [9], among others. Success on TAP consists of tracking the 2D trajectory of any given (x, y) query point, defined at a particular frame t, throughout the rest of a given video. By definition, the query point is considered visible at the query frame, and associated to the material point of the scene which is observed at (x, y, t). However, this material point may become occluded or go out of the image boundaries. To handle this, TAP models also must predict a binary visibility flag v for each timestamp of the video. The tracked (x, y) positions and visibility estimates are scored jointly using an all-encompassing average Jaccard metric. Most current TAP models [9\u201311, 16, 21] are limited to tracking in 2D pixel space. Recently, some works have started exploring the extension of the TAP problem to 3D (TAP-3D) [57, 62]. However, most TAP benchmarks containing real-world videos, such as TAPVid-DAVIS [9], Perception Test [43], CroHD [52] and BADJA [4], don't have 3D annotations, and therefore evaluations are still performed on the 2D tracking task. Concurrent to our work, Wang et al. [57] introduced both a synthetic (LSFOdyssey) and a real benchmark (LSFDriving) for evaluating TAP-3D. However, their real benchmark is limited to the driving domain and only contains 180 test clips with 40 frames each. Our proposed TAPVid-3D benchmark is substantially larger and more diverse, containing 4000+ clips with durations between 25 and 300 frames.\n\nScene flow estimation. The scene flow estimation problem, introduced by Vedula et al. [55], seeks to obtain a dense, instantaneous, 3D motion field of a 3D scene, analogously to the way optical flow estimates a dense 2D motion field across consecutive frame pairs of a 2D video. The TAP-3D task is related to the scene flow problem in a similar way in which the TAP task is related to the optical flow problem. While scene flow seeks to obtain dense instantaneous motion estimation, TAP-3D seeks to obtain longer-range tracking, spanning tens or hundreds of frames. Furthermore, TAP-3D does not seek to produce dense fields of tracks, but is rather interested in tracking a sparse set of query points, which is more computationally tractable. From work in TAP, we have observed that having motion representations with longer temporal context is useful for downstream tasks such as robotic manipulation, while having sparser spatial coverage is sufficient for many tracking applications.\n\nPose estimation. Closely related to point tracking is pose estimation and keypoint tracking. Many benchmarks have been proposed for 2D and 3D pose estimation [15, 26, 56, 68]. 3D pose estimation tasks and benchmarks largely focus on specific categories of moving objects, and for objects that are articulated: e.g. humans [19, 56], hands [53], animals [38, 67], and even jointed furniture [31]. For general motion and scene understanding, we aim to learn motion estimation across any object or scene pixel, expanding the generality of the task.\n\nStatic scene reconstruction. Static scene reconstruction, a fundamental problem in computer vision, has been advanced through techniques like Structure-from-Motion (SfM) and monocular depth estimation. Significant contributions include COLMAP [45] for state-of-the-art reconstructions and MVSNet [65], which enhances multi-view stereo depth estimation with deep learning. These studies collectively advance robust and precise static scene reconstruction. Evaluation of the local features and depth are crucial for static scene reconstruction methods. [46] provided a comparative evaluation of hand-crafted and learned features, while MegaDepth [27] improved single-view depth prediction using large scale multi-view Internet photo collections. Despite significant progress, static"}, {"title": "3 TAPVid-3D", "content": "We build a real-world benchmark for evaluating Tracking Any Point in 3D (TAP-3D) models. To do this, we leveraged three publicly available datasets: (i) Aria Digital Twin [42], (ii) DriveTrack [2, 51] and (iii) Panoptic Studio [20]. These data sources span different application domains, environments, and video characteristics, deriving ground truth tracking trajectories from different sensor types. For instance, Aria Digital Twin is a dataset of egocentric video, and is more close to bimanual robotic manipulation problems, where the camera is robot mounted and sees the actions from a first person view. DriveTrack features footage captured from a Waymo car navigating outdoor scenes, akin to tasks in robotic navigation and outdoor, rigid-body scene understanding tasks. Finally, Panoptic Studio captures third-person views of people performing diverse actions within an instrumented dome. It presents complex human movement which aligns more closely with NRSfM-adjacent tasks. We believe that, combined, these data sources present a diverse and comprehensive benchmark of TAP-3D capabilities for many potential downstream tasks. We describe our pipeline to extract ground truth metric 3D point trajectories from each source in the next sections, with samples in Figure 1."}, {"title": "3.1 Aria Digital Twin", "content": "This split employs real videos captured with the Aria glasses [41] inside different recording studios, which mimic household environments. Accurate digital replicas of these studios, created in 3D modeling software, are used to obtain pseudo-ground truth annotations of the original footage. This includes annotations such as segmentation masks, 3D object bounding boxes and depth maps. We leverage these annotations to extract 3D trajectories from given 3D query points. In particular, given a video $V = \\{I_t\\}_{t=1,...,T}$ with T frames and $W \\times H$ spatial resolution, with corresponding object segmentation masks $\\{S_t\\} \\subset \\mathbb{Z}^{W \\times H}$, and depth maps $\\{D_t\\} \\subset \\mathbb{R}^{W \\times H}$, extrinsic world-to-camera poses* $\\{(P_{cam}^w)_t\\} \\subset \\mathbb{R}^{3\\times 4}$, camera intrinsics $K \\in \\mathbb{R}^{3\\times 3}$, and a query point $q = (x_q, y_q, t_q)$, we first extract the query point's 3D position $Q_{cam}^\\dagger$ in the camera coordinate frame, with\n\n$(Q_{cam})_{t_q} = K^{-1}(x_q, y_q, 1)^T \\cdot D_{t_q}(x_q, y_q).$\n\nAdditionally, we obtain the object ID of the query point from the segmentation mask $qid = S_{t_q}(x_q, y_q)$, and use it to retrieve the 3D object pose of the query object $(P_{obj})_{t_q}$, which converts points from world coordinate frame to the object coordinate frame. This allows us to compute the query point position in the object's frame of reference as\n\n$Q_{obj} = (P_{obj})_{t_q} (P_{cam}^w)_{t_q} (Q_{cam})_{t_q}.$\n\nIn this way, we fix the query point to the corresponding object, and then obtain its track across the whole video by leveraging the object's pose annotation. For any timestamp t, the position of the query point can be then obtained by\n\n$(Q_{cam})_t = (P_{cam}^w)_t (P_{obj})^w Q_{obj}.$\n\nTo compute the visibility v of the query point at any time, we first employ a pretrained semantic segmentation model to obtain the semantic masks of the operator's hands $\\{H_t\\}$, as these are not modelled in the digital replica. Then, we compute the visibility v by verifying that the query point's depth is close to the observed depth, and it does not lie on the hands segmentation mask H, so\n\n$v_t = 1(|(Z(Q_{cam})_t - D_t(u, v)| < \\delta) \\cdot (1 - H_t(u, v)),$\n\nwhere $(u, v) = \\Pi_K((Q_{cam})_t)$ is the projection of the query point $(Q_{cam})_t$ to the image plane according to the given camera intrinsics K, and $Z((x, y, z)) = z$ is the function that extracts the z-component of a 3D point. This approach allows us to compute the 3D trajectory $\\{(Q_{cam})_t\\}$ and visibility flag $\\{v_t\\}$ of the query point across the whole video."}, {"title": "3.2 DriveTrack", "content": "The DriveTrack split is based on videos from the Waymo Open dataset [51], and the 2D point trajectory pipeline used in DriveTrack [2]. In particular, each frame It in a video sequence V has a corresponding, time-synchronized point cloud $\\{C_t\\}$ from the Waymo car's LIDAR. The subset of points that correspond to a randomly selected, single, tracked vehicle $(Q_{cam})_{t_s}$ are subselected from the entire point cloud $(Q_{cam})_{t_s} \\subset C_{t_s}$ at a certain sampling time ts using a manually-annotated 3D bounding box around the chosen object. These object-specific points are then tracked across the whole video using: (i) a vehicle rigidity assumption, and, (ii) the pose and position of the object's annotated 3D bounding box through the entire video sequence. Specifically, the tracked points in object coordinate frame $Q_{obj}$ are first computed using (2), and then the trajectories in camera coordinate frame $\\{(Q_{cam})_t\\}$ are obtained using (3)."}, {"title": "3.3 Panoptic Studio", "content": "The original Panoptic Studio dataset [20] consists in video sequences captured inside a recording dome using stationary cameras, and depicting different actors performing various actions such as passing a ball or swinging a tennis racket, featuring complex non-rigid motions. To obtain 3D trajectories, we leverage the pretrained dynamic 3D reconstructions from Luiten et al. [35]. These have been obtained by first performing a rigid 3D reconstruction through 3D Gaussian Splatting [22], fitting a set of 3D Gaussians $\\{(\\mu_i, \\Sigma_i)_{t_0}\\}_{i=1,...,n}$ to the first timestamp $t_0$ of each sequence using the multiple cameras available in the dome. Then, these Gaussians are displaced and rotated in a as-rigid-as-possible manner to model the motion occurring in the subsequent frames of the video. For more details, please refer to [35]. Using these pretrained splatting models, we render pseudo-ground-truth depth maps $\\{D_t\\}$ for each sequence. Then, given an query point $q = (x_q, y_q, t_q)$, we unproject the point to 3D following (1), obtaining $(Q_{cam})_{t_q}$, and retrieve the index i* of the closest Gaussian center at that time using the poses $\\{(P_{cam}^w)_t\\}$, so that $i^* = \\underset{i=1,...,N}{\\text{argmin}} ||(Q_{cam})_{t_q} - (P_{cam}^w)_{t_q}(\\mu_i)_{t_0} ||_2$.\n\nNote that due to the distance between $(Q_{cam})_{t_q}$ and $(P_{cam}^w)_{t_q}(\\mu_{i^*})_{t_q}$, their projections onto the image plane will not match exactly. To account for this difference, we adjust the query point's 2D position as $(x_q, y_q) = \\Pi_K(P_{cam}^w)_{t_q}(\\mu_{i^*})_{t_q}$. Then, the query point's 3D track come by following the motion of the i* Gaussian center, so $(Q_{cam})_t = (P_{cam}^w)_t(\\mu_{i^*})_t$.\n\nVisibility $\\{v\\}$ is estimated as in (4) (omitting the second term), by comparing the depth of our 3D query point with the observed depth from Dt. We only track points across the foreground deforming characters, as tracking background points is trivial given that the cameras in this dataset are stationary."}, {"title": "3.4 Data Cleanup and Validation", "content": "While the aforementioned procedures generally produce high quality trajectories, small amounts of noise from the underlying dataset sources can cause issues in a small fraction of sequences. These minor inaccuracies, for example, can be caused by small misalignement between Aria Digital Twin synthetic annotations and the real world video, LIDAR sensor noise, insufficiently constrained Gaussian splats, or numerical error. We minimized these errors through automated methods, and then manually checked a sampling of the videos to ensure accuracy.\n\nFirstly, since trajectories are descriptors of surface motion, their motion should be localized to their associated object. We use instance segmentation models to generate object masks for each frame [24], filtering out errant trajectories that exceed these boundaries when not occluded. In DriveTrack specifically, the tracked bounding box is an approximation to the true object mesh, but such errors are fixed with tight segmentation masks (trimming 2-3% of initial trajectories).\n\nSecondly, we observed that some trajectories have a \u2018flickering' visibility flag. This is not unique to TAPVid-3D, as we notice this in the widely used Kubric [13] and DriveTrack [2]. However, to mitigate this in our dataset, we oversample trajectories in the annotation generation pipeline, and filter trajectories whose visibility state changes more times than 10% of the total number of video frames. More details can be found in the appendix."}, {"title": "3.5 Metrics", "content": "To accompany the TAPVid-3D dataset, we adopt and extend the metrics used in TAP [9] to the 3D tracking domain. These metrics measure the quality of the predicted 3D point trajectories (APD), the ability to predict point visibility (OA), or both simultaneously (AJ)."}, {"title": "8 Metrics using Median, Per-Trajectory, and Local Neighborhood Rescaling", "content": "In the results included in the main paper, we compute the 3D Average Jaccard and APD metrics using a global median rescaling procedure (L277). To get a good score, the entire scene must be reconstructed up to scale, and dynamic objects must be placed precisely. This is useful for many applications, such as navigation, but for others it may be overly stringent. If there is little camera motion, or if some of the objects have unclear size, it may also be very difficult for models to infer global scene shape.\n\nHowever, not all applications require such strong global scene shape capabilities. For example, for imitation learning, we may want an agent to simply approach an object. For such applications, measuring the relative depth of estimated 3D locations along a single trajectory may be sufficient, and it may be substantially easier, as the (2D) scaling of local textures may provide enough information to solve the problem. More generally, for robotic imitation of an assembly task, it is the local consistency that's most important: as long as points that are near each other in 3D have the correct depth relative"}, {"title": "9 Evaluations with Median, Per-Trajectory, and Local Neighborhood Scaling", "content": "Tables 5, 6, 3 present additional experimental results on the full_eval set, on all our baselines. To avoid biasing the results to the TAPVid-3D splits with higher number of videos, these tables present averaged results across the three constituent data sources (weighing each source equally)."}, {"title": "10 Evaluations using Fixed Metric Distance Thresholds", "content": "In the Average Jaccard formulation in Section 3.5, we describe how we use determine correctly predicted points along a trajectory, using a depth-adaptive radius threshold denoted $\\delta^{3D}(P_i)$. We also explored using a fixed metric threshold. Specifically, instead of the $\\{1, 2, 4, 8, 16\\}$ pixel thresholds (projected into 3D), we use a the fixed metric radius thresholds of 1 centimeter, 4 centimeter, 16 centimeters, 64 centimeters, and 2.56 meters. If the predicted point is within this distance to the ground truth point, it is marked as position correct within that threshold."}, {"title": "11 Baselines Details and Compute Resources", "content": "CoTracker. We use the pretrained model and PyTorch code from the official CoTracker codebase and run inference enabling the bi-directional tracking mode, with no other modifications to the default parameters. Internally, inference is performed in $512 \\times 384$ resolution, and the output predictions are rescaled back to the original clip resolutions. Inference is performed using A100 GPUs, and"}, {"title": "12 Filtering Incorrect Trajectories", "content": "We apply different automatic filters for removing problematic tracks. Tracks can present three type of issues: (i) issues with visibility flags, (ii) queries which are outside the moving objects, and (iii) noisy 3D trajectories.\n\nWe found visibility issues (i) to be present in all dataset splits, and we remove it simply by oversam-pling the number of query points and discarding those whose visibility flag changes state more than a 10% of the number of frames in the video.\n\nIssue (ii) was present mostly in the DriveTrack split, where trajectories in a video are localized and describe the motion of exactly one moving object in the scene. In some cases the 3D point-clouds associated with vehicles also contain points that are within the object bounding box, but outside of the object itself, such as in the road. To filter out errant trajectories, we use the Segment Anything"}, {"title": "13 Dataset Specifications, Metadata, and other Details", "content": "For this dataset release, we preserve the licenses for the constituent original data sources, which are non-commercial. For our additions, to the extent that we can, we release under a standard Apache 2.0 license. A full amalgamated license will be available in the open-sourced repository during complete release of the work, after the review period is finished.\n\nWe will publicly host the dataset for wide consumption by researchers on Google Cloud Storage indefinitely. Part of the dataset is already hosted in this way (and how the Colab link linked above is able to run). We also intend to open-source code for computing the new 3D-AJ metrics after the camera ready. We anticipate the release will require little maintenance (and the TAPVid-2D dataset release that the team released two years ago is similarly low maintanence), but we are happy to address any emergent issues raised by users.\n\nSpecific implementation details on how the dataset can be read are found in the Colab link provided. Each dataset example is provided in a * . npy file, containing the fields: tracks_xyz of shape [T, Q, 3] (containing the Q ground truth point tracks for the corresponding video of T frames, with (x, y, z)-coordinates in meters), query_xyt of shape [Q, 3] (containing each track's query point position in format (x, y, t), in (x,y)-pixel space and t as the query frame index), the ground truth visibility flags with shape [Q, T], and the camera_intrinsics (as [fx, fy, Cr, Cy]). Each * . npy file is named after its corresponding video in the original data source, which can be loaded by downloading from the original hosting sites [2, 42, 51], respecting their corresponding licenses."}, {"title": "14 Visualized Samples", "content": "See Figures 4, 5, 6, 7, 8, 9, 10, 11, and 12 below."}]}