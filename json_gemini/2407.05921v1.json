{"title": "TAPVid-3D:\nA Benchmark for Tracking Any Point in 3D", "authors": ["Skanda Koppula", "Ignacio Rocco", "Yi Yang", "Joe Heyward", "Jo\u00e3o Carreira", "Andrew Zisserman", "Gabriel Brostow", "Carl Doersch"], "abstract": "We introduce a new benchmark, TAPVid-3D, for evaluating the task of long-range\nTracking Any Point in 3D (TAP-3D). While point tracking in two dimensions\n(TAP) has many benchmarks measuring performance on real-world videos, such as\nTAPVid-DAVIS, three-dimensional point tracking has none. To this end, leverag-\ning existing footage, we build a new benchmark for 3D point tracking featuring\n4,000+ real-world videos, composed of three different data sources spanning a\nvariety of object types, motion patterns, and indoor and outdoor environments. To\nmeasure performance on the TAP-3D task, we formulate a collection of metrics\nthat extend the Jaccard-based metric used in TAP to handle the complexities of\nambiguous depth scales across models, occlusions, and multi-track spatio-temporal\nsmoothness. We manually verify a large sample of trajectories to ensure correct\nvideo annotations, and assess the current state of the TAP-3D task by constructing\ncompetitive baselines using existing tracking models. We anticipate this benchmark\nwill serve as a guidepost to improve our ability to understand precise 3D motion and\nsurface deformation from monocular video. Code for dataset download, generation,\nand model evaluation is available at https://tapvid3d.github.io/.", "sections": [{"title": "1 Introduction", "content": "For robots, humans, and other agents to effectively interact with the physical 3D world, it is necessary\nto understand a scene's structure and dynamics. This is a key ingredient to any general embodied\nintelligence: the ability to learn a world model to understand and predict the structure and motion of\narbitrary scenes. It is attractive to leverage the vast amounts of monocular video data that is available\ncheaply, and use such signals to understand the geometry and 3D motion in real-world footage. But\nhow well can current perception algorithms actually do this?\nThe field has seen many efforts to measure 3D and motion understanding from videos, each con-\ntributing a part of the overall goal. For example, monocular depth estimation is a widely recognized\ntask [7, 36, 63]. However, success in depth estimation alone does not reveal whether the model\nunderstands how surfaces move from one frame to the next, and may not respect temporal conti-\nnuity. For instance, a basketball spinning on its axis is often not visible in a sequence of depth\nmaps. On the other end of the generality spectrum, 3D pose tracking, e.g. for rigid objects [14, 34]\nand people [12, 49], evaluates precise motion, but requires a known 3D model of each object with\narticulations. Building parametric models and pose estimation methods for all animal classes is\ninfeasible, much less for all the objects that a robot might encounter in, for example, an unseen, busy\nconstruction site.\nAn alternative and more general approach for dynamic scene understanding observes that the world\nconsists of particles, each of which individually follows a 3D trajectory through space and time [58,\n62]. Measuring the motion of these points provides a way to measure 3D motion without requiring any"}, {"title": "2 Related work", "content": "Tracking Any Point. In recent years, long-range tracking of local image points has been formalized\nas the Tracking Any Point task (TAP) and evaluated using the, now standard, TAPVid benchmark [9],\namong others. Success on TAP consists of tracking the 2D trajectory of any given (x, y) query\npoint, defined at a particular frame t, throughout the rest of a given video. By definition, the query\npoint is considered visible at the query frame, and associated to the material point of the scene\nwhich is observed at (x, y, t). However, this material point may become occluded or go out of the\nimage boundaries. To handle this, TAP models also must predict a binary visibility flag v for each\ntimestamp of the video. The tracked (x, y) positions and visibility estimates are scored jointly using\nan all-encompassing average Jaccard metric. Most current TAP models [9\u201311, 16, 21] are limited\nto tracking in 2D pixel space. Recently, some works have started exploring the extension of the\nTAP problem to 3D (TAP-3D) [57, 62]. However, most TAP benchmarks containing real-world\nvideos, such as TAPVid-DAVIS [9], Perception Test [43], CroHD [52] and BADJA [4], don't have 3D\nannotations, and therefore evaluations are still performed on the 2D tracking task. Concurrent to our\nwork, Wang et al. [57] introduced both a synthetic (LSFOdyssey) and a real benchmark (LSFDriving)\nfor evaluating TAP-3D. However, their real benchmark is limited to the driving domain and only\ncontains 180 test clips with 40 frames each. Our proposed TAPVid-3D benchmark is substantially\nlarger and more diverse, containing 4000+ clips with durations between 25 and 300 frames.\nScene flow estimation. The scene flow estimation problem, introduced by Vedula et al. [55], seeks\nto obtain a dense, instantaneous, 3D motion field of a 3D scene, analogously to the way optical flow\nestimates a dense 2D motion field across consecutive frame pairs of a 2D video. The TAP-3D task is\nrelated to the scene flow problem in a similar way in which the TAP task is related to the optical flow\nproblem. While scene flow seeks to obtain dense instantaneous motion estimation, TAP-3D seeks\nto obtain longer-range tracking, spanning tens or hundreds of frames. Furthermore, TAP-3D does\nnot seek to produce dense fields of tracks, but is rather interested in tracking a sparse set of query\npoints, which is more computationally tractable. From work in TAP, we have observed that having\nmotion representations with longer temporal context is useful for downstream tasks such as robotic\nmanipulation, while having sparser spatial coverage is sufficient for many tracking applications.\nPose estimation. Closely related to point tracking is pose estimation and keypoint tracking. Many\nbenchmarks have been proposed for 2D and 3D pose estimation [15, 26, 56, 68]. 3D pose estimation\ntasks and benchmarks largely focus on specific categories of moving objects, and for objects that\nare articulated: e.g. humans [19, 56], hands [53], animals [38, 67], and even jointed furniture [31].\nFor general motion and scene understanding, we aim to learn motion estimation across any object or\nscene pixel, expanding the generality of the task.\nStatic scene reconstruction. Static scene reconstruction, a fundamental problem in computer\nvision, has been advanced through techniques like Structure-from-Motion (SfM) and monocular\ndepth estimation. Significant contributions include COLMAP [45] for state-of-the-art reconstructions"}, {"title": "3 TAPVid-3D", "content": "We build a real-world benchmark for evaluating Tracking Any Point in 3D (TAP-3D) models. To do\nthis, we leveraged three publicly available datasets: (i) Aria Digital Twin [42], (ii) DriveTrack [2, 51]\nand (iii) Panoptic Studio [20]. These data sources span different application domains, environments,\nand video characteristics, deriving ground truth tracking trajectories from different sensor types. For\ninstance, Aria Digital Twin is a dataset of egocentric video, and is more close to bimanual robotic\nmanipulation problems, where the camera is robot mounted and sees the actions from a first person\nview. DriveTrack features footage captured from a Waymo car navigating outdoor scenes, akin to\ntasks in robotic navigation and outdoor, rigid-body scene understanding tasks. Finally, Panoptic\nStudio captures third-person views of people performing diverse actions within an instrumented\ndome. It presents complex human movement which aligns more closely with NRSfM-adjacent tasks.\nWe believe that, combined, these data sources present a diverse and comprehensive benchmark of\nTAP-3D capabilities for many potential downstream tasks. We describe our pipeline to extract ground\ntruth metric 3D point trajectories from each source in the next sections, with samples in Figure 1."}, {"title": "3.1 Aria Digital Twin", "content": "This split employs real videos captured with the Aria glasses [41] inside different recording studios,\nwhich mimic household environments. Accurate digital replicas of these studios, created in 3D\nmodeling software, are used to obtain pseudo-ground truth annotations of the original footage. This\nincludes annotations such as segmentation masks, 3D object bounding boxes and depth maps. We\nleverage these annotations to extract 3D trajectories from given 3D query points. In particular, given\na video V = {It}t=1,...,T with T frames and W \u00d7 H spatial resolution, with corresponding object\nsegmentation masks {St} C ZW\u00d7H, and depth maps {D\u2081} CRW\u00d7H, extrinsic world-to-camera\nposes {(Pam)t} CR3\u00d74, camera intrinsics K \u2208 R3\u00d73, and a query point q = (xq, yq, tq), we first\nextract the query point's 3D position Qcam\u2020 in the camera coordinate frame, with\n(Qcam)tq = K-1(xq, yq, 1)T . Dtq (xq, Yq).\n(1)\nAdditionally, we obtain the object ID of the query point from the segmentation mask qid =\nStq(xq, Yq), and use it to retrieve the 3D object pose of the query object (Pobj)ta, which con-\nverts points from world coordinate frame to the object coordinate frame. This allows us to compute\nthe query point position in the object's frame of reference as\nQobj = (Pobj)ta (Puam)ta (Qcam)ta.\n(2)\nIn this way, we fix the query point to the corresponding object, and then obtain its track across the\nwhole video by leveraging the object's pose annotation. For any timestamp t, the position of the\nquery point can be then obtained by\n(Qcam)t = (Pcam)t(Pobj)tQobj.\n(3)\nTo compute the visibility v of the query point at any time, we first employ a pretrained semantic\nsegmentation model to obtain the semantic masks of the operator's hands {Ht}, as these are not\nmodelled in the digital replica. Then, we compute the visibility v by verifying that the query point's\ndepth is close to the observed depth, and it does not lie on the hands segmentation mask H, so\nUt = 1(|(Z(Qcam) \u2013 Dt(u, v)| < \u03b4) \u00b7 (1 \u2212 H\u2081(u, v)),\n(4)\nwhere (u, v) = \u041f\u043a((Qcam)t) is the projection of the query point (Qcam)t to the image plane\naccording to the given camera intrinsics K, and Z((x, y, z)) = z is the function that extracts the\nz-component of a 3D point. This approach allows us to compute the 3D trajectory {(Qcam)t} and\nvisibility flag {vt} of the query point across the whole video."}, {"title": "3.2 DriveTrack", "content": "The DriveTrack split is based on videos from the Waymo Open dataset [51], and the 2D point\ntrajectory pipeline used in DriveTrack [2]. In particular, each frame It in a video sequence V has\na corresponding, time-synchronized point cloud {Ct} from the Waymo car's LIDAR. The subset\nof points that correspond to a randomly selected, single, tracked vehicle (Qcam)ts are subselected\nfrom the entire point cloud (Qcam)ts CCts at a certain sampling time ts using a manually-annotated\n3D bounding box around the chosen object. These object-specific points are then tracked across the\nwhole video using: (i) a vehicle rigidity assumption, and, (ii) the pose and position of the object's\nannotated 3D bounding box through the entire video sequence. Specifically, the tracked points\nin object coordinate frame Qobj are first computed using (2), and then the trajectories in camera\ncoordinate frame {(Qcam)t} are obtained using (3)."}, {"title": "3.3 Panoptic Studio", "content": "The original Panoptic Studio dataset [20] consists in video sequences captured inside a recording\ndome using stationary cameras, and depicting different actors performing various actions such as\npassing a ball or swinging a tennis racket, featuring complex non-rigid motions. To obtain 3D\ntrajectories, we leverage the pretrained dynamic 3D reconstructions from Luiten et al. [35]. These\nhave been obtained by first performing a rigid 3D reconstruction through 3D Gaussian Splatting [22],\nfitting a set of 3D Gaussians {(\u03bc\u2081, \u03a3i)to }i=1,...,n to the first timestamp to of each sequence using\nthe multiple cameras available in the dome. Then, these Gaussians are displaced and rotated in a\nas-rigid-as-possible manner to model the motion occurring in the subsequent frames of the video. For\nmore details, please refer to [35]. Using these pretrained splatting models, we render pseudo-ground-\ntruth depth maps {Dt} for each sequence. Then, given an query point q = (xq, yq, tq), we unproject\nthe point to 3D following (1), obtaining (Qcam)tq, and retrieve the index i* of the closest Gaussian\ncenter at that time using the poses {(Pam)t}, so that i* = argmin ||(Qcam)t\u0105 \u2013 (Peam)ta(Hi)tg ||2.\ni=1,...,N\nNote that due to the distance between (Qcam)tq and (Peam)ta (pi*)tq, their projections onto the\nimage plane will not match exactly. To account for this difference, we adjust the query point's 2D\nposition as (xq, Yq) = \u041f\u043a(Pam)tq(i*)tq. Then, the query point's 3D track come by following the\nmotion of the i* Gaussian center, so (Qcam)t = (Peam)t(pi*)t.\nVisibility {v} is estimated as in (4) (omitting the second term), by comparing the depth of our 3D\nquery point with the observed depth from Dt. We only track points across the foreground deforming\ncharacters, as tracking background points is trivial given that the cameras in this dataset are stationary."}, {"title": "3.4 Data Cleanup and Validation", "content": "While the aforementioned procedures generally produce high quality trajectories, small amounts\nof noise from the underlying dataset sources can cause issues in a small fraction of sequences.\nThese minor inaccuracies, for example, can be caused by small misalignement between Aria Digital\nTwin synthetic annotations and the real world video, LIDAR sensor noise, insufficiently constrained\nGaussian splats, or numerical error. We minimized these errors through automated methods, and then\nmanually checked a sampling of the videos to ensure accuracy.\nFirstly, since trajectories are descriptors of surface motion, their motion should be localized to their\nassociated object. We use instance segmentation models to generate object masks for each frame [24],\nfiltering out errant trajectories that exceed these boundaries when not occluded. In DriveTrack\nspecifically, the tracked bounding box is an approximation to the true object mesh, but such errors are\nfixed with tight segmentation masks (trimming 2-3% of initial trajectories).\nSecondly, we observed that some trajectories have a \u2018flickering' visibility flag. This is not unique\nto TAPVid-3D, as we notice this in the widely used Kubric [13] and DriveTrack [2]. However, to\nmitigate this in our dataset, we oversample trajectories in the annotation generation pipeline, and\nfilter trajectories whose visibility state changes more times than 10% of the total number of video\nframes. More details can be found in the appendix."}, {"title": "3.5 Metrics", "content": "To accompany the TAPVid-3D dataset, we adopt and extend the metrics used in TAP [9] to the 3D\ntracking domain. These metrics measure the quality of the predicted 3D point trajectories (APD), the\nability to predict point visibility (OA), or both simultaneously (AJ).\nThe APD (<dang) metric measures the average percent of points within 8 error. If Pi is the i'th\npoint prediction at time t, Pi is the corresponding ground truth 3D point, and v\u012f is the ground-truth\npoint visibility flag, then:\nAPD3D = 1/V \u2211i,t vi\u00b7 1(||P \u2013 P|| < 3D(P)),\n(5)\nwhere 1(\u00b7) is the indicator function, || \u00b7 || is the Euclidean norm, V = \u2211i,t vi is the total number of\nvisibles, and 3D(Pi) is the threshold value.\nThe value of this threshold is relative to ground-truth depth; and is defined by unprojecting a pixel\nthreshold 82D C {1,2,4,8,16} to 3D space using the camera intrinsic parameters: 83D(Pt) =\nZ(Pi) 82D/f, where f is the camera focal length. We argue that points that are closer to the camera\nare of higher importance than those that are far away, which is enforced by the definition of 83D+.\nNote that by using this definition, the APD3D metric defined in (5) is numerically equivalent to the\nAPD2D metric from [9] when the point estimations P all have correct depths.\n\u2021\nIn addition, it is important to distinguish between occluded and visible points, because downstream\nalgorithms often want to rely exclusively on predictions which are based on visual evidence, rather\nthan on the guesses that might be very wrong. To this end, we adopt the occlusion accuracy metric\n(OA) from [9], which computes the fraction of points on each trajectory where v = v, where vi is\nthe model's (binary) visibility prediction.\nFinally, we define 3D-AJ, 3D Average Jaccard, following TAP, which combines OA and APD3D.\nThe AJ metric calculates the number of true positives (number of points within the 83D threshold,\npredicted correctly to be visible), divided by the sum of true positives and false positives (predicted\nvisible, but are occluded or farther than the threshold) and false negatives (visible points, predicted\noccluded or predicted to exceed the threshold). Mathematically, it is defined as:\nAJ3D = \u03a3i,t vi \u03b1\u0390 /(\u2211i, vi + \u2211i,t ((1 \u2013 v\u1ec1) v\u1ec1) + \u2211i,t (v\u1ec1 v\u1ec1 (1 \u2212 \u03b1\u0390))),\n(6)\nwhere a = 1(||P \u2013 P|| < 3D(P)) indicates whether the point prediction is below the distance\nthreshold. Note the relationship between this and prior metrics: if the depth estimates for a given\npoint are perfect, then this metric reduces to 2D-AJ, as there will be no depth errors. It is also related\nto a common Monodepth's metric \u03b4 < 1.25k, which also places a hard threshold on the relative\ndepth of the estimated point w.r.t. the ground truth; if the video is a single frame (occlusion-free)\nand the query points are dense, then there should be no 2D errors, and our metric will behave like a\nMonodepth metric. For general sequences, however, the algorithm must output correct tracking and\ncorrect depth-i.e., a full 3D trajectory-in order to be considered correct by our metric.\nFinally, one additional complication is scale ambiguity. As in monocular depth literature [44], we\nglobally re-scale predictions to match ground truth, before computing the metrics. We do this by\nmultiplying predictions P by the median of the depth ratios ||P|||||Pi || over all points and frames,\nwhich we call global median rescaling. Note, however, that some algorithms may be more adept at"}, {"title": "4 Baselines on TAPVid-3D", "content": "We construct one set of baselines by combining state-of-art 2D point trackers and monocular depth\nestimators. In particular, we use for 2D tracking, several state-of-the-art models such as CoTracker\n[21], BootsTAPIR model [11], and TAPIR [10]; and for depth regression, both the monocular depth\nestimation model ZoeDepth [3] and the SfM pipeline COLMAP [47, 48]. To convert the frame pixel\nspace predictions into metric x, y-position coordinates, we unproject using the camera intrinsics\nand the z-estimate provided by ZoeDepth. In the case of COLMAP, we import the 2D trajectories\nproduced by the TAP methods before running the SfM reconstruction pipeline.\nWe include the recently released SpatialTracker [62], one of the first models designed specifically\nfor 3D point tracking. We also provide results for a new variant of TAPIR [10] that we built for 3D\ntracking, that we call TAPIR-3D (more details in the appendix). Uniquely, this is the only baseline\nthat is trained only on synthetic videos. Finally, we include a static baseline, which projects the pixel\nquery point into 3D, and assumes no motion of that 3D point. Results for all these baseline methods\nare shown in Table 3, for the full_eval split, and three major methods on minival split. Results in\nTable 3 uses median depth scaling; results for other scaling options are included in the appendix.\nFirstly, comparing Tables 3 and 4, we find that the 3D tracking performance of our baselines are\nsignificantly lower compared to their 2D tracking abilities. We show examples in Figure 2, illustrating\ncommon failure modes regressing 3D trajectories, noting that while the 2D trajectories look accurate,\ntheir understanding of total scene geometry and correct 3D motion is poor.\nSecondly, we find that having the three different data sources provides a video diversity to the\nbenchmark that enables better assessment of the strengths and weakeness of the video model under\ntest. For example, ZoeDepth-based methods perform comparatively well on the PStudio subset, but\nsignficantly underperforms COLMAP on the DriveTrack subset (where monocular depthing might be\nharder, with the outdoor complex scenes, in which scale-consistent depthing may be difficult). On the\nother hand, COLMAP struggles with the PStudio subset, where nearly all tracks and majority of the\nscene is occupied by the main moving objects. This underperformance is likely because COLMAP\nfails to reconstruct moving content. SpatialTracker uses ZoeDepth under the hood, so its pattern of\nresults across the subsets are similar to 2D Tracking + ZoeDepth. Our TAPIR-3D experimental model,"}, {"title": "5 Conclusion", "content": "We introduce TAPVid-3D, a new benchmark for evaluating the nascent Tracking Any Point in 3D\n(TAP-3D) task. We contributed (1) a pipeline to annotate three distinct real-world video datasets\nto produce 3D correspondence annotations per video, (2) the first metrics for the TAP-3D task,\nwhich measure multiple axis of accuracy of 3D track estimates, and (3) an assessment of the current\nstate of TAP-3D, by evaluating commonly used tracking models. We believe this benchmark will\naccelerate research on the TAP-3D task, allowing the development of models with greater dynamic\nscene understanding from monocular video."}, {"title": "Appendix for TAPVid-3D", "content": "Table of Contents\n1. More Dataset Samples\n2. Dataset Statistics\n3. Metrics using Median, Per-Trajectory, and Local Neighborhood Scaling\n4. Evaluations using Median, Per-Trajectory, and Local Neighborhood Scaling\n5. Evaluations using Fixed Metric Distance Thresholds\n6. Baselines Details and Compute Resources\n7. Filtering Incorrect Trajectories\n8. Dataset Specifications, Metadata, and other Details\n9. Visualized Samples"}, {"title": "6 More Dataset Samples", "content": "More dataset samples are provided on our website, https://tapvid3d.github.io, including\ninteractive 3D visualizations.\nFinally, we include static visualizations of trajectories in the figures included in the Visualized\nSamples section at the end of this PDF."}, {"title": "7 Dataset Statistics", "content": "Figure 3 showcases various summary statistics about the TAPVid-3D datasets and its 3D point\ntrajectories. In the top left, we have the distribution of the number of frames in each video. The\nADT-sourced videos contain the longest videos, and clips of 300 frames were extracted. Similarly\nPanoptic Studio contains clips of 150 frames, while DriveTrack contains clips of varying duration.\nIn the top right, we have the number of point tracks annotated in each clip. In the bottom right, we\ncount the number of 'static' trajectories in each video, marking a trajectory as static if the distance\nbetween all pairwise locations within a single point's trajectory is less than 1 centimeter. The roughly\n10 DriveTrack videos consisting of static trajectories are usually cars stopped at stoplights. These\n'static' videos are a small minority of the 4000+ clips in TAPVid-3D. In the bottom right, we show the\naverage velocity of each trajectory in the dataset, noting that trajectories in DriveTrack are the fastest.\nThese histograms convey that there is a diversity of overall trajectory lengths, video lengths, and\npoint velocities in the TAPVid-3D dataset. Additionally, this dataset is larger than two widely used\n2D point tracking real-world-video datasets: TAPVid-Kinetics (1,189 videos) and TAPVid-DAVIS\n(30 videos)."}, {"title": "8 Metrics using Median, Per-Trajectory, and Local Neighborhood Rescaling", "content": "In the results included in the main paper, we compute the 3D Average Jaccard and APD metrics\nusing a global median rescaling procedure (L277). To get a good score, the entire scene must be\nreconstructed up to scale, and dynamic objects must be placed precisely. This is useful for many\napplications, such as navigation, but for others it may be overly stringent. If there is little camera\nmotion, or if some of the objects have unclear size, it may also be very difficult for models to infer\nglobal scene shape.\nHowever, not all applications require such strong global scene shape capabilities. For example,\nfor imitation learning, we may want an agent to simply approach an object. For such applications,\nmeasuring the relative depth of estimated 3D locations along a single trajectory may be sufficient, and\nit may be substantially easier, as the (2D) scaling of local textures may provide enough information to\nsolve the problem. More generally, for robotic imitation of an assembly task, it is the local consistency\nthat's most important: as long as points that are near each other in 3D have the correct depth relative"}, {"title": "10 Evaluations using Fixed Metric Distance Thresholds", "content": "In the Average Jaccard formulation in Section 3.5, we describe how we use determine correctly\npredicted points along a trajectory, using a depth-adaptive radius threshold denoted 83D (Pi). We also\nexplored using a fixed metric threshold. Specifically, instead of the {1, 2, 4, 8, 16} pixel thresholds\n(projected into 3D), we use a the fixed metric radius thresholds of 1 centimeter, 4 centimeter, 16\ncentimeters, 64 centimeters, and 2.56 meters. If the predicted point is within this distance to the\nground truth point, it is marked as position correct within that threshold."}, {"title": "11 Baselines Details and Compute Resources", "content": "CoTracker. We use the pretrained model and PyTorch code from the official CoTracker codebase\nand run inference enabling the bi-directional tracking mode, with no other modifications to the default\nparameters. Internally, inference is performed in 512 \u00d7 384 resolution, and the output predictions\nare rescaled back to the original clip resolutions. Inference is performed using A100 GPUs, and"}, {"title": "12 Filtering Incorrect Trajectories", "content": "We apply different automatic filters for removing problematic tracks. Tracks can present three type of\nissues: (i) issues with visibility flags, (ii) queries which are outside the moving objects, and (iii) noisy\n3D trajectories.\nWe found visibility issues (i) to be present in all dataset splits, and we remove it simply by oversam-\npling the number of query points and discarding those whose visibility flag changes state more than a\n10% of the number of frames in the video.\nIssue (ii) was present mostly in the DriveTrack split, where trajectories in a video are localized and\ndescribe the motion of exactly one moving object in the scene. In some cases the 3D point-clouds\nassociated with vehicles also contain points that are within the object bounding box, but outside of\nthe object itself, such as in the road. To filter out errant trajectories, we use the Segment Anything"}, {"title": "13 Dataset Specifications, Metadata, and other Details", "content": "For this dataset release, we preserve the licenses for the constituent original data sources, which are\nnon-commercial. For our additions, to the extent that we can, we release under a standard Apache 2.0\nlicense. A full amalgamated license will be available in the open-sourced repository during complete\nrelease of the work, after the review period is finished.\nWe will publicly host the dataset for wide consumption by researchers on Google Cloud Storage\nindefinitely. Part of the dataset is already hosted in this way (and how the Colab link linked above\nis able to run). We also intend to open-source code for computing the new 3D-AJ metrics after the\ncamera ready. We anticipate the release will require little maintenance (and the TAPVid-2D dataset\nrelease that the team released two years ago is similarly low maintanence), but we are happy to\naddress any emergent issues raised by users.\nSpecific implementation details on how the dataset can be read are found in the Colab link provided.\nEach dataset example is provided in a * . npy file, containing the fields: tracks_xyz of shape [T, Q,\n3] (containing the Q ground truth point tracks for the corresponding video of T frames, with (x, y, z)-\ncoordinates in meters), query_xyt of shape [Q, 3] (containing each track's query point position in\nformat (x, y, t), in (x,y)-pixel space and t as the query frame index), the ground truth visibility\nflags with shape [Q, T], and the camera_intrinsics (as [fx, fy, Cr, Cy]). Each * . npy file is named\nafter its corresponding video in the original data source, which can be loaded by downloading from"}, {"title": "14 Visualized Samples", "content": "See Figures 4, 5, 6, 7, 8, 9, 10, 11, and 12 below."}]}