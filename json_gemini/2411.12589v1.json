{"title": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based Understanding", "authors": ["Hesam Hosseini", "Ghazal Hosseini Mighan", "Amirabbas Afzali", "Sajjad Amini", "Amir Houmansadr"], "abstract": "Transformers have revolutionized Computer Vision (CV) and Natural Language Processing (NLP) through self-attention mechanisms. However, due to their complexity, their latent token representations are often difficult to interpret. We introduce a novel framework that interprets Transformer embeddings, uncovering meaningful semantic patterns within them. Based on this framework, we demonstrate that zero-shot unsupervised semantic segmentation can be performed effectively without any fine-tuning using a model pre-trained for tasks other than segmentation. Our method reveals the inherent capacity of Transformer models for understanding input semantics and achieves state-of-the-art performance in semantic segmentation, outperforming traditional segmentation models. Specifically, our approach achieves an accuracy of 67.2 % and an mIoU of 32.9 % on the COCO-Stuff dataset, as well as an mIoU of 51.9% on the PASCAL VOC dataset. Additionally, we validate our interpretability framework on LLMs for text summarization, demonstrating its broad applicability and robustness.", "sections": [{"title": "1. Introduction", "content": "In recent years, the Transformer architecture and foundation models, leveraging self-attention mechanisms to capture complex dependencies in text, have transformed Natural Language Processing (NLP) benchmarks [2, 51, 54, 58]. Similarly, Vision Transformers (ViTs) [13] have been adapted in Computer Vision (CV) and now serve as the backbone for various tasks such as segmentation and object detection [30, 52]. Despite their success, understanding the interpretability of Transformers remains a challenge due to the complexity of their latent token representations.\nSeveral methods have been developed to enhance the interpretability of CNN-based models [44, 47, 63]. While some of these can be extended to Transformer architectures, they do not fully leverage the unique attention mechanisms inherent to Transformers. Recent research has introduced interpretability methods specifically designed for Transformers [1, 6, 59]. However, these approaches primarily focus on explaining final model outputs, providing limited insight into the intermediate processes that lead to predictions. For instance, [7] maps latent tokens into CLIP's [37] multi-modal space to find corresponding text descriptions, relying on an external text encoder for interpretability. In contrast, our approach directly interprets the latent space of ViTs, elucidating the role and function of each token within the high-dimensional space without relying on external models.\nThis paper introduces a framework to interpret latent tokens, offering a deeper understanding of the internal workings of Transformers. This understanding enables users to perform image semantic segmentation using pre-trained Transformer-based vision models in an unsupervised, zero-shot manner, without any additional training. We demonstrate that applying semantic segmentation based on our interpretability framework achieves state-of-the-art performance on benchmark image segmentation datasets.\nDrawing inspiration from [6], our method analyzes the semantic information retained by latent tokens, enabling tasks such as object selection and semantic segmentation without additional training. We demonstrate that Transformers inherently understand the semantic structure of their input, viewing it as a collection of distinct concepts. Each latent token identifies a specific concept with semantic significance, thereby shedding light on the decision-making process of these models.\nAs shown in Section 4, our framework proves effective in a range of tasks, including semantic segmentation and model interpretation. Most recent unsupervised semantic segmentation methods involve an additional training phase to learn feature representations [18, 27, 46]. Our approach, however, utilizes the understanding embedded in pre-trained models to achieve zero-shot segmentation, leveraging their inherent knowledge of images. The stronger a model's comprehension of image content, the more accurately it performs segmentation. We further demonstrate that our method is capable of interpreting large language models (LLMs) at the token level, validating its application in tasks such as text summarization. The main contributions of this paper are as follows:\n\u2022 We propose a framework for interpreting latent tokens in Transformers, providing valuable insights into model decision-making processes.\n\u2022 By aggregating relevance maps generated by tokens using hierarchical clustering, we achieve zero-shot unsupervised semantic segmentation on pre-trained models, outperforming SOTA methods that require additional training.\n\u2022 We demonstrate the capability of our method to interpret LLMs at the token level, showcasing its practical applicability to textual data. Specifically, we demonstrate the interpretation of LLM operations in the text summarization task.\nThis paper is structured as follows: Section 2 reviews interpretability frameworks and previous work on semantic segmentation, highlighting our SOTA results. Section 3 presents our interpretability framework. In Section 4, we showcase its effectiveness on image and text tasks. Finally, Section 5 concludes the paper."}, {"title": "2. Related Work", "content": "The interpretability of deep learning architectures has become a central focus in AI research [16]. As models grow in complexity, understanding their decision-making processes is essential for ensuring transparency, reliability, and fairness [53]. Interpretability not only aids in debugging and performance improvement but also builds trust in AI systems, particularly in fields like healthcare, finance, and autonomous driving. Opaque models can perpetuate biases and generate unforeseen outcomes, making interpretability crucial for bridging high performance with safe, practical AI deployment [26]. In Transformer models, tokens are key to interpreting behavior, with their relationship to spatial locations in images or sequence order adding an important layer to interpretation. This relationship enhances semantic segmentation as a measure for evaluating token-based interpretation frameworks, which this section will explore."}, {"title": "2.1. Model Interpretation", "content": "Model interpretability is a critical area of research in deep learning, especially for complex models like transformers. Traditional deep learning models, such as CNNs, have been interpreted using various techniques like saliency maps [47], deconvolutional networks [63], Guided Backpropagation [48], Feature Visualization [34], Local Interpretable Model-Agnostic Explanations (LIME) [40], SHapley Additive exPlanations (SHAP) [31], Class Activation Mapping (CAM) [65], Feature Attribution with Integrated Gradients [50], and Grad-CAM [44], which highlight the most important regions of the input that influence model predictions. While effective for CNNs, these techniques are less suitable for transformer architectures, as they fail to account for the unique self-attention mechanisms that transformers rely on. Anchors is a model-agnostic interpretability method that provides high-precision, locally faithful explanations by generating if-then rules that sufficiently explain a model's prediction for specific inputs [41].\nRecent interpretability research on transformers has focused on understanding how these models allocate attention and propagate information. A seminal contribution was made by examining self-attention patterns in transformer-based language models, revealing how attention is distributed across tokens [59]. Abnar et al. [1] advanced this by proposing methods to visualize attention flow across layers, aiding in the understanding of information propagation. Chefer et al. [6] introduced a relevance-based approach, computing relevance scores for each token to provide deeper insights into the model's decision-making process. Additionally, the sensitivity of model predictions to input tokens has been explored as another way to interpret transformer behavior [20].\nInterpretability methods for reflecting semantic relations within input sequences have been investigated in NLP tasks,"}, {"title": "2.2. Unsupervised Semantic Segmentation", "content": "Unsupervised semantic segmentation has advanced significantly due to self-supervised learning and clustering-based techniques, which reduce reliance on labeled datasets. Early approaches, such as Invariant Information Clustering (IIC) [21], employed mutual information to group similar pixels without using labels. Building on this foundation, PiCIE introduced consistency by integrating photometric and geometric invariances, setting an important precedent for subsequent methods [9].\nThe emergence of Vision Transformers (ViTs) and self-supervised learning marked a major shift in the field. DINO became a pioneering method, extracting rich, meaningful features without the need for labeled data [5]. Its effectiveness in unsupervised segmentation laid the groundwork for more recent advancements. For instance, TransFGU [62] performs semantic segmentation in a top-down manner by deriving class activation maps from DINO models. STEGO [18] leverages DINO's features, employing contrastive learning to group similar regions and achieving notable improvements in segmentation accuracy.\nSubsequent methods further refined these concepts. MaskContrast incorporated clustering techniques to ensure region consistency across different views, enhancing feature representations for unsupervised segmentation [56]. Leopart utilized self-supervised ViTs to improve pixel grouping, particularly in complex scenes [66]. ACSeg introduced adaptive conceptualization for pixel-level semantic grouping, using an Adaptive Concept Generator (ACG) to dynamically align learnable prototypes with relevant concepts, thereby addressing over- and under-clustering challenges in varied images [27].\nLeveraging hidden positives for unsupervised semantic segmentation enhances pixel grouping by identifying and utilizing implicit positive relationships within the data, boosting segmentation performance without the need for labeled examples [45]. Using self-supervised learning and adaptive feature representations to enable models to discover and segment novel, unseen object categories without labeled data have also been investigated [55]. Smooseg introduces a smoothness prior to enhancing unsupervised semantic segmentation by promoting coherent region grouping [25]. Unsupervised semantic segmentation is also improved by leveraging depth-guided feature correlation and targeted sampling to enhance region consistency and accuracy [46]. [17].\nUnlike approaches that depend on self-training, pseudo-labeling, or complex setups, our model ULTra introduces a zero-shot method for unsupervised semantic segmentation. ULTra achieves strong segmentation performance by directly leveraging the semantic information embedded in the latent tokens of pre-trained Transformer models, without the need for additional training or fine-tuning. Furthermore, ULTra emphasizes explainability within Transformers by illustrating how the model's latent tokens contribute to segmentation, in contrast to the common practice of using only the CLS token to represent the entire image. This zero-shot, explainability-driven approach offers a novel direction for unsupervised segmentation, demonstrating that Transformer-based architectures can achieve SOTA performance without labeled data or extensive additional processing."}, {"title": "3. Methodology", "content": "In this section, we present our approach for interpreting latent representations in Transformers. We begin with essential preliminaries and notation for clarity, followed by a detailed explanation of our method for interpreting latent tokens and its application to semantic segmentation."}, {"title": "3.1. Preliminaries and Notation", "content": "The architecture of a typical Transformer can be formulated as follows: the input X is split into n tokens {x_i}_{i=1}^n. After tokenization, token embeddings {e_i}_{i=0}^n are computed, where $e_0$ corresponds to the CLS token. Positional encodings $PE_i$ are added to the i-th token embedding to incorporate spatial information, resulting in the latent token representation $z_i^{(1)} = z_i = e_i + PE_i$. Here, $z_i^{(l)}$ represents a latent token, where l denotes the layer index with $l \\in {1, ..., L}$ and L is the total number of layers in the Transformer, and i represents the i-th token within the l-th layer.\nFor each head $h \\in {1, ..., H}$ in the multi-head attention mechanism, the queries, keys, and values corresponding to the i-th token are obtained via linear transformations, projecting the latent token of dimension d into dimension k:\n$Q_i^{(h)}(z_i^{(l-1)}) = (W_q^{(h)})^Tz_i^{(l-1)}, K_i^{(h)}(z_i^{(l-1)}) = (W_k^{(h)})^Tz_i^{(l-1)}, V_i^{(h)}(z_i^{(l-1)}) = (W_v^{(h)})^Tz_i^{(l-1)}, \\forall l \\in {2,..., L} \\qquad (1)$\nwhere $W_q^{(h)}, W_k^{(h)}, W_v^{(h)} \\in \\mathbb{R}^{d \\times k}$. The attention weights for each token pair (i, j) at layer l and head h are computed as:\n$\\alpha_{h,i,j}^{(l)} = softmax_j(\\frac{(Q_i^{(h)}(z_i^{(l-1)}), K_j^{(h)}(z_j^{(l-1)}))}{\\sqrt{k}})$.  \\qquad (2)$\nThen, i-th token is updated by summing over the weighted values across all heads:\n$\\overline{u_i} = (W_c^{(h)})^T \\sum_{h=1}^H \\left( \\sum_{j=1}^n \\alpha_{h,i,j}^{(l)} V_j^{(h)}(z_j^{(l-1)})\\right), \\qquad (3)$\nwhere $W_{c,h} \\in \\mathbb{R}^{k \\times d}$. The updated token representation $u_i$ after the attention layer is computed as:\n$u_i^{(l)} = LayerNorm(z_i^{(l-1)} + \\overline{u_i}). \\qquad (4)$\nEach token then passes through a feed-forward network:\n$z_i^{(1)} = (W_2^{(1)})^T ReLU((W_1^{(1)})^Tu_i), \\qquad (5)$\n$z_i^{(l)} = LayerNorm(u_i + z_i^{(1)}). \\qquad (6)$\nHere, $W_1^{(1)} \\in \\mathbb{R}^{d \\times m}, W_2^{(1)} \\in \\mathbb{R}^{m \\times d}$"}, {"title": "3.2. Interpreting Latent Tokens", "content": "A straightforward approach for interpreting a model involves analyzing the semantic flow from the input to the corresponding logits. This can be achieved by adding the attention probability matrix to an identity matrix I, which incorporates skip connections, and then multiplying the result across layers [1]. However, a notable challenge arises from multiple attention heads in each Transformer layer. To address this, [6] proposes performing a weighted average across attention heads, with the weights determined by the gradient of the logits with respect to the attention weights. We employed a modified, simpler version of this method, utilizing only the attention weights rather than the layer's relevance.\nIn our framework, latent tokens $z_i^{(l)}$ are represented as vectors rather than direct class correspondences, introducing an additional layer of complexity. To manage this, an appropriate transformation, such as the energy or euclidean norm of the latent token, is employed as a surrogate. Consequently, we compute the gradient of $||z_i^{(l)}||$ with respect to the attention weights to facilitate the analysis.\nIn the following equations, we define the relevance map $S^{(l)} \\in \\mathbb{R}^n$, where the j-th element of this vector represents the importance of the j-th input token to the targeted latent token $z_i^{(l)}$. The relevance map is computed as:\n$A^{(b,l)} = I + E_h\\left( \\nabla_A ||z_i^{(l)}||\\right),$\n$S^{(l)} = A^{(1,l)} \\cdot A^{(2,l)} ..... A^{(l-1,l)},$\n$S_i^{(l)} = S^{(l)}[i, 1:], \\qquad (7)$\nwhere $(.)_+$ means considering only positive values, $S, A^{(b,l)} \\in \\mathbb{R}^{(n+1)x(n+1)}$, $\\odot$ denotes the Hadamard product, $E_h$ represents the mean across the heads dimension, and $A_h^{(b,l)} \\in \\mathbb{R}^{hx(n+1)x(n+1)}$ is the attention score matrix in the b-th layer and $A_{h,i,j}^{(b)}$ = $\\alpha_{h,i,j}^{(l)}$. Due to the skip connections in the transformer, most of the contribution of $S_i^{(l)}$ is concentrated on $S_i^{(l)}[i - 1]$ which makes it hard to analyze other tokens' contribution. To address this issue, we replace this element with the maximum value of other elements, thereby capturing the contributions of additional tokens to the selected token.\nFor vision tasks, we first reshape the relevance map and then upsample it using bilinear or cubic interpolation to match the resolution of the model's input. The resulting higher-dimensional matrix is denoted as $\\check{S}$. This upsampling step is essential for enabling accurate object selection and semantic segmentation tasks."}, {"title": "3.3. ULTra in Unsupervised Tasks", "content": "In this section, we aim to examine ULTra's capability to adapt to various tasks involving semantic knowledge. Importantly, it requires no additional training, leveraging the inherent understanding within transformers rather than relying on loss functions objective, final layer outputs, or fine-tuning.\nUnsupervised Semantic Segmentation. As previously discussed, a relevance map can be defined for each latent token at a fixed layer, with the total number of relevance maps equal to the number of latent tokens. In the context of segmentation, the goal is to assign a class label to each pixel within an image. To achieve this, we employ clustering techniques, such as hierarchical clustering, that do not require a predefined number of classes. These techniques group the relevance maps into k distinct clusters, where k is unknown. Ideally, we aim for k to approximate the actual number of classes present in the image.\nOur approach provides flexibility in adjusting the value of k by modifying the cutoff distance threshold $\\zeta$ within the clustering algorithm. Increasing $\\zeta$ produces fewer, broader clusters that capture general categories, such as background and foreground. Conversely, reducing $\\zeta$ allows for finer segmentation, distinguishing more specific features, such as an object's head or hands. To prevent the method from disproportionately favoring larger objects, given that the number of elements in each cluster may vary, we apply min-max scaling to each cluster independently.\nAfter clustering, we define k distinct concepts by aggregating the relevance maps within each cluster. The aggregated relevance map $\\check{S}_{c,v}[x, y]$ for a cluster c is computed as:\n$\\check{S}_{c,v}[x, y] = \\sum_{i \\in \\varphi_S(c)} \\check{S}_i^{(l)}[x, y], \\qquad (8)$\nwhere $\\varphi_S(c) = {i | Class(\\check{S}_i^{(l)}) = c}$ represents the set of label assignments determined by the clustering algorithm. For each pixel at position [x, y], the class label is determined by identifying the cluster c with the highest relevance value at that pixel. Mathematically, the class assignment for a pixel is expressed as:\n$Class[x, y] = argmax_{c \\in {1,...,k}} \\check{S}_{c,v}[x, y]. \\qquad (9)$"}, {"title": "4. Experiment", "content": "Datasets. In our experiments, we evaluate model performance on several semantic segmentation benchmarks, focusing on vision-related tasks. We conducted experiments on three datasets: COCO-Stuff 27 [4], PASCAL VOC 2012 [15], and Potsdam-3 [19]. This combination of datasets provides a diverse testing ground to rigorously evaluate our unsupervised zero-shot approach across both standard and challenging perspectives in semantic segmentation.\nCOCO-Stuff 27, a subset of the COCO dataset [28], includes complex, real-world scenes with pixel-level annotations across various object categories. Similarly, PASCAL VOC 2012 serves as a classic benchmark with pixel-level annotations, while the Potsdam-3 dataset offers a unique aerial, top-down perspective of urban scenes. The latter adds an additional challenge with its high-resolution images of buildings, roads, and natural elements captured over the city of Potsdam.\nFor our qualitative analysis of LLM interpretation in the task of text summarization, as described in Section 4.3, we utilized the TL;DR dataset [49]. The TL;DR dataset contains summary comparisons with human feedback collected by OpenAI. Each entry consists of a Reddit post, including its title, original content, and a human-generated TL;DR.\nModels. For all experiments in the vision tasks, we used CLIP's image encoder ViT-B/32 [37]. For interpreting text summarization, as described in Section 4.3, we used the Llama-2-7B language model [54]. All experiments were run on 8 NVIDIA RTX 4090-24GB GPUs."}, {"title": "4.1. Zero-shot Unsupervised Object Selection", "content": "The upsampled relevance map $\\check{S}_i^{(l)}$ can be converted into a binary segmentation mask using a threshold $\\tau$, where the binary mask $M_i^{(l)}$ is defined as:\n$M_i^{(l)}[x, y] =\\begin{cases}\n0, &\\text{if } \\check{S}_i^{(l)}[x, y] < \\tau, \\\\\n1, &\\text{otherwise}.\n\\end{cases} \\qquad (10)$\nHere, $\\check{S}_i^{(l)}[x, y]$ represents the relevance value at position [x, y] in $\\check{S}_i^{(l)}$, with $\\tau$ as the threshold. When $\\check{S}_i^{(l)}[x, y]$ is below $\\tau$, $M_i^{(l)}[x, y]$ is set to 0; otherwise, it is set to 1, marking the object region.\nOur findings indicate that as tokens propagate through the network, they refine their object representation while retaining the semantic meaning of their associated image patches performing Object Selection. Figure 5 visually illustrates this process. For a given patch token $x_i$, the object it most strongly represents is denoted as class $k_i$, indicating that $x_i$ predominantly corresponds to a region of the object belonging to class $k_i$ in the image. The latent token $z_i^{(l)}$ generates a relevance map that highlights areas with higher values associated with class $k_i$. After applying a threshold, this map becomes a binary segmentation mask expected to exhibit a high Intersection over Union (IoU) with the corresponding class $k_i$ region in the image. An illustrative example is shown in Figure 2.\nTo quantify alignment, we compute the IoU by converting the relevance map $\\check{S}_i^{(l)}$ into a binary mask $M_D$ and comparing it with the ground-truth mask. We propose the Initial Token IoU (ITIoU) metric, which measures how well the relevance maps of input tokens align with their respective class masks. The ITIoU is calculated as:\n$ITIoU_i^{(l)}(X) = \\frac{1}{C} \\sum_{i=1}^C \\frac{1}{|P_i|} \\sum_{x_j \\in P_i} IoU(M_i^{(l)}, G_i), \\qquad (11)$\nwhere C denotes the number of classes, $P_i$ represents the set of tokens associated with class i, $M_i^{(l)}$ is the binary segmentation mask for token $x_i$ within class i, and $G_i$ is the ground-truth mask for class i in image X. The inner sum averages the IoU for tokens in $P_i$ for each class, and the outer sum then averages across all classes. Using a threshold of 0.2, our ITIoU metric achieves an average score of 37.84 % on the COCO validation dataset and 39.51 % on the VOC dataset.\nThe ITIoU metric provides a comprehensive evaluation by incorporating a weighted average across tokens in each class, enhancing the assessment of token alignment with their respective ground-truth labels."}, {"title": "4.2. Zero-shot Unsupervised Semantic Segmentation", "content": "We benchmarked the segmentation capability of our method against several approaches in the literature on unsupervised segmentation. Notably, unlike other methods, our approach requires no additional training. Instead, it relies solely on a pre-trained vision transformer, which may have been trained on tasks unrelated to unsupervised segmentation.\nTo evaluate our method, we used the Unsupervised mean Intersection over Union (U. mIoU) and Unsupervised pixel Accuracy (U. Accuracy) metrics, following standard practices in semantic segmentation research. In all experiments across datasets, as shown in Tables 1, 2, and 3, we set the cutoff distance threshold $\\zeta = 0.4."}, {"title": "4.3. Interpreting LLMs in Text Summarization", "content": "In this section, we examine how our interpretability framework can be applied to text summarization tasks to uncover the underlying mechanisms and intent of LLMs. By visualizing the regions of the input context that an LLM prioritizes while interpreting a given TL;DR summary, we gain deeper insights into the model's behavior and decision-making processes. As shown in Figure 4, these visualizations allow us to pinpoint the most influential regions of a textual input prompt in generating concise and relevant summaries.\nFor the experiments, we used a Supervised Fine-Tuned (SFT) version of Llama-2-7B trained on the UltraFeedback Binarized (UFB) dataset [12]. Additionally, we aligned the model to the text summarization task on the TL;DR dataset using the Direct Preference Optimization (DPO) method [38] for 1,000 iterations, with a learning rate of 5 \u00d7 10-6 and $\\beta$ = 0.5. To validate our framework, we selected the preferred response (TL;DR) of each sample in the dataset, denoted it as y, and used it as the summary of the context x.\nIn this task, we concatenate the context x and the summary y with a separator token. After feeding this input to the model, we compute the relevance scores of the TL;DR tokens with respect to the context tokens. We then average these scores for each token in x to obtain a scalar value, referred to as the Token Contribution Score, $\\Lambda_i^{(l)} \\in \\mathbb{R}^+$, which highlights the contribution of each context token in interpreting the summary y with respect to the context. This provides visual evidence of how the model processes the context text and identifies key semantic elements relevant to producing the summary y. Accordingly, $\\Lambda_i^{(l)}$ is computed as:\n$\\Lambda_i^{(l)} = \\frac{1}{|y|} \\sum_{j=1}^{|y|} S_{j+x}^{(l)}[i], \\forall i \\in {1,...,|x|}, \\qquad (12)$\nwhere $|y|$ denotes the number of tokens in the text.\nIn example (a): semantically significant words such as 'relationship', 'experience', 'rejection', and 'never' are prominently highlighted, reflecting the model's interpretation of the person's struggles with relationships and feelings of rejection. Additionally, the highlighting of the question at the beginning of the context 'How do I stop feeling bad...' suggests the model recognizes the presence of uncertainty and a request for guidance, which is encapsulated in the summary as 'I don't know.'\nIn example (b): $\\Lambda_i^{(l)}$ scores reveals the model's focus on words such as 'feelings', 'hate', 'disappoint', 'love', and 'like', which correspond to the person's mixed emotions toward their girlfriend, as described in the summary. The apparent contradiction between 'love' and 'trashness' in the summary seems to be derived from these highlighted terms, suggesting the model understands the conflicting emotions present in the text. Furthermore, the emphasis on 'character' aligns with the summary's judgmental tone, suggesting that the model interprets this word as indicative of an assessment of personality traits and behaviors.\nThis token-level analysis can serve as a valuable tool for effective supervision in future research, particularly for developing interpretable fine-tuning and alignment methods like Reinforcement Learning from Human Feedback (RLHF) [10, 35, 49] and Preference Optimization [3, 14, 38], where understanding model behavior and intent is essential. It may also provide useful insights in related areas, such as Chain-of-Thought [8, 24, 43] and Theory-of-Mind [23, 60, 61], which seek to make the reasoning processes and intentions of LLMs more transparent. Ultimately, our framework offers insights into fundamental questions in NLP, such as: \"How is the model thinking?\" or \"What is the underlying intent behind the model's generated response?\u201d."}, {"title": "4.4. Ablation Study on the Effect of Layer Depth in ViT Token Understanding", "content": "In this section, we analyze the impact of depth on our model's interpretability and segmentation performance, providing insights into the contribution of each layer. As anticipated and observed in Figure 6, deeper layers generally carry more semantic significance. However, the contribution diminishes in the final layers, suggesting that a depth of around 13 layers might be more than sufficient for the ViT to effectively comprehend image content. This finding implies that even fewer layers might achieve comparable results, potentially reducing computational costs without compromising performance.\nWe observe an intriguing behavior in the initial layers, where performance initially declines before improving. This phenomenon is also visually evident in Figure 5, where the attention maps in the first layer appear to focus on the entire image. This suggests that, initially, the token examines the image as a whole before selectively gathering information from tokens with similar characteristics."}, {"title": "5. Conclusion", "content": "This paper introduces a novel interpretability framework that provides valuable insights into the decision-making processes of Transformer models. The framework is based on the input interpretation provided by each token in an arbitrary layer. By aggregating the interpretation of all tokens in the same layer, the framework enhances understanding of the Transformer's behavior at the layer level. The richness of this understanding enables zero-shot unsupervised semantic segmentation, where our method achieves state-of-the-art performance with an accuracy of 67.2% and an mIoU of 32.9% on the COCO-Stuff dataset, and an mIoU of 51.9% on the PASCAL VOC dataset for the unsupervised zero-shot version. Beyond vision tasks, the framework is not limited to vision models and can also be applied to interpret the behavior of large language models (LLMs) in tasks such as text summarization. This highlights the versatility and broad applicability of the proposed framework across various domains, offering potential for future work to simultaneously demystify the multimodal understanding of Transformers."}]}