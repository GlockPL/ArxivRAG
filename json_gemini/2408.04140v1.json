{"title": "UNLEARN\nEfficient Removal of Knowledge in Large Language Models", "authors": ["Tyler Lizzo", "Larry Heck"], "abstract": "Given the prevalence of large language mod-\nels (LLMs) and the prohibitive cost of train-\ning these models from scratch, dynamically\nforgetting specific knowledge e.g., private or\nproprietary, without retraining the model has\nbecome an important capability. This paper pro-\nposes a novel method to achieve this objective\ncalled UNLEARN. The approach builds upon\nsubspace methods to identify and specifically\ntarget the removal of knowledge without ad-\nversely affecting other knowledge in the LLM.\nResults demonstrate 96% of targeted knowl-\nedge can be forgotten while maintaining per-\nformance on other knowledge within 2.5% of\nthe original model, significantly outperform-\ning the discriminatory abilities of the previous\nstate-of-the-art. A dual method called LEARN\nis also proposed for targeted knowledge ad-\ndition. Results show LEARN can match the\nfine-tuning accuracy of Low-Rank Adaptation\n(LORA) without adversely affecting similar\ntasks.", "sections": [{"title": "Introduction", "content": "The swift advancement and widespread deploy-\nment of large language models (LLMs) have\nbrought many challenges including the inability\nto remove knowledge from the LLMs at will. Ef-\nficient removal of knowledge has become increas-\ningly important with 'Right to be Forgotten' laws\n(Goldman, 2020) and Europe's General Data Pro-\ntection Regulation (Goddard, 2017). Traditional\ntraining methodologies often lack the flexibility\nand efficiency required to address both tasks, es-\npecially when rapid model adaptation is needed\nwithout comprehensive retraining.\nThis paper introduces UNLEARN, a novel algo-\nrithm that can forget or unlearn knowledge within\nan LLM without adversely affecting related knowl-\nedge. UNLEARN leverages subspace techniques to\nidentify the subspaces spanned by particular knowl-\nedge (tasks) and discrimination methods to separate\nthat subspace from subspaces of similar tasks. This\nallows the algorithm to prevent performance degra-\ndation when there are similar tasks, a common\nissue with traditional methods and of particular im-\nportance to data privacy regulations. Further, this\ntechnique uses a unified set of operators, where\nthe task matrices are identical and used to either\nenhance or reduce the model's performance for a\ngiven task.\nUNLEARN achieves 96% forgetting on the\ntask of interest while maintaining performance\non dissimilar tasks within 2.5% of the original\nmodel. When the tasks are similar, UNLEARN\nstill achieves nearly 80% forgetting on the task of\ninterest while preserving performance on similar\ntasks within 10%. These results significantly out-\nperform the state-of-the-art, which achieves similar\nforgetting but is accompanied by significant degra-\ndation on similar tasks.\nThe forgetting of UNLEARN can easily be con-\nverted to add knowledge to the LLM. This new\nmethod LEARN matches the fine-tuning accuracy\nof the LORA method (Hu et al., 2021) without\naffecting related tasks, demonstrating its dual na-\nture across both knowledge unlearning and fine-\ntuning scenarios.\nThe contributions of this work are as follows:\n\u2022 An efficient method to identify the subspace\nof specific knowledge within an LLM.\n\u2022 A novel approach called subspace discrimina-\ntion and task removal to selectively target and\nremove specific knowledge without adversely\naffecting other knowledge in the LLM.\n\u2022 The introduction of LEARN, a dual algorithm\nto UNLEARN that provides a new approach\nto adding new knowledge to the LLM without\naffecting its other knowledge.\nThis paper presents the UNLEARN algorithm and\ndemonstrates its performance in removing knowl-\nedge represented as tasks. Section 2 reviews the"}, {"title": "Related Works", "content": "literature on Parameter Efficient Fine-Tuning, Ma-\nchine Unlearning, and LLM Unlearning. Section\n3 describes the three main parts of UNLEARN:\nsubspace identification, subspace discrimination,\nand task removal. In Section 4, the performance\nof UNLEARN is tested over a large set of metrics\nand settings and compared to the current state-of-\nthe-art. Section 4.5 introduces LEARN, a dual\napplication of the UNLEARN algorithm for adding\nknowledge to the LLM. A comparison to traditional\nfine-tuning methods is made in Section 5. Future\nworks are discussed in Section 6. Finally, Section\n7 concludes the paper and outlines potential direc-\ntions for future research."}, {"title": "Parameter Efficient Fine-Tuning", "content": "Parameter Efficient Fine-Tuning (PEFT) is used to\nfine-tune large models without modifying most of\nthe original pre-trained weights, resulting in signif-\nicant computational and storage savings.\nOne of the most significant PEFT methods is\nLow-Rank Adaptation (LoRA; Hu et al., 2021),\nwhich decomposes weight updates into two low-\nrank matrices. While reducing trainable parame-\nters by 10,000 times and GPU memory usage by 3\ntimes, LoRA is still able to maintain the fine-tuning\nperformance of a systems. Quantized Low-Rank\nAdapation would build upon LoRA's performance\ngains by quantizing model weights (Dettmers et al.,\n2023).\nOther notable PEFT methods include prompt\ntuning (Lester et al., 2021; Qin and Eisner, 2021),\ntuning hidden states (IA\u00b3; Liu et al., 2022a), adding\nlayers (Houlsby et al., 2019), tuning the embed-\nding layer inputs (An et al., 2022), and hybrid ap-\nproaches (Mahabadi et al., 2021). These extend\nprior work on domain adaptation of deep neural\nnetworks for Natural Language Processing (Jaech\net al., 2016)."}, {"title": "Machine Unlearning", "content": "Machine unlearning is the process of removing the\ninfluence of data on an already trained model, cre-\nating a model that behaves as if it was never trained\non that data (Xu et al., 2023). Its origins are in\ndata protection regulations, such as the California\nConsumer Privacy Act (CCPA; Goldman, 2020)\nand the European Union's General Data Protec-\ntion Regulation (GDPR; Goddard, 2017), which\nassert a user's 'Right to be Forgotten,' the right to\nhave their personal data erased upon request.\nMachine unlearning has since been extended to\nmyriad areas: federated learning (Liu et al., 2022b;\nZhang et al., 2023b), image classification (Bour-\ntoule et al., 2021; Gupta et al., 2021; Liu et al.,\n2024a), and image generation (Gandikota et al.,\n2023; Kumari et al., 2023; Fan et al., 2024).\nThe most rigorous method for machine unlearn-\ning is 'exact' unlearning, completely retraining a\nmodel with the data points of interest removed (Yan\net al., 2022; Nguyen et al., 2022; Fan et al., 2024).\nAlthough exact unlearning guarantees the removal\nof data, it is impractical for models of any signif-\nicant size due to the high computation cost. For\ninstance, training Llama 2 70B took ~ 1.7 million\nGPU-hours on Nvidia A100 GPUs (Touvron et al.,\n2023)."}, {"title": "LLM Unlearning", "content": "There is an increasing interest in machine unlearn-\ning in the context of LLMs (Jang et al., 2022;\nMeng et al., 2023; Liu et al., 2024c). Important\nworks have demonstrated the need for machine un-\nlearning within LLMs, showing clear motivations\nfrom both regulatory and application-specific stand-\npoints (Zhang et al., 2023a; Liu et al., 2024b).\nExisting methods for LLM unlearning include\ngradient ascent to reascend the learning curve (Jang\net al., 2022; Chen and Yang, 2023; Yao et al.,\n2024), preference optimization using alternative\nresponses (Eldan and Russinovich, 2023; Maini\net al., 2024), and input-based approaches (Pawel-\nczyk et al., 2024; Thaker et al., 2024).\nHowever, these methods face significant chal-\nlenges. There are the aforementioned cost and time\nrestraints. The vast amounts of training data used\nfor LLM training adds to the complexity, as iden-\ntifying and isolating the specific data points to be\nunlearned is a non-trivial task (Eldan and Russi-\nnovich, 2023; Ilharco et al., 2023). The scope of\nunlearning is generally underspecified; unlearning\nshould remove knowledge within the scope of the\ntargeted data while maintaining performance on\nother data (Mitchell et al., 2022). Finally, there is\na lack of comprehensive evaluation methods to as-\nsess the effectiveness of unlearning in LLMs (Patil\net al., 2023; Shi et al., 2024)."}, {"title": "UNLEARN Method", "content": "The method proposed in this paper consists of three\nmain tasks: subspace identification, discrimina-"}, {"title": "Subspace Identification", "content": "tion, and removal. Subspace identification trains\na knowledge (task)-dependent matrix for a speci-\nfied layer while freezing all other layers. This se-\nquential, layer-by-layer training starts with the first\nlayer and progresses through the entire network\nto yield a set of matrices that represent the task-\ndependent subspace (Section 3.1). Once identified,\nsubspace discrimination removes the information\nunique to the task of interest while preventing any\ndegradation of other tasks. This is achieved using\na variation of the Gram-Schmidt process to orthog-\nonalize subspaces, allowing mutual information to\nbe preserved (Section 3.2). The final step is sub-\nspace removal, where the modified task matrix, T,\nis subtracted (Section 3.3)."}, {"title": "Subspace Identification", "content": "This step identifies the subspace of a specific task\nwithin the LLM weight space. The method utilizes\na general training that is implemented layer-by-\nlayer, starting with the first layer (l = 1). All\ntraining is performed with a train/validation/test\nsplit of 0.6/0.2/0.2: The train set is used for training\nthe network, the validation set determines when to\nstop training for a specific layer in our sequential\nprocess, and all evaluations are performed on the\nfinal test set:\n0. Model: The original pretrained weights of\nthe LLM are removed and the weights for all\nlayers are randomly initialized.\n1. Layer Freezing: Except for the weights at\nlayer l, all other weights for the subsequent\nlayers of an LLM are frozen to isolate the\ntraining to one layer at a time.\n2. Training: Training is completed on the task\ndataset with the l-th layer unfrozen. This is\nachieved by maximizing the conditional lan-"}, {"title": "Sequential Training", "content": "guage modeling objective:\n$\\max\\limits_T\\sum\\limits_{(x,y)\\in Z}\\sum\\limits_{t=1}^nlog(P_T(y_t|x, y_{<t}))\\hspace{0.2cm}(1)$\nwhere $x_i$ and $y_i$ are sequences of tokens and\n$T\\in R^{nxn}$ is the matrix for task i at the l-th\nlayer and n x n the dimensions of the original\npre-trained weight matrix.\nGiven the matrix T is trained on a specific\ntask, the matrix is likely rank deficient. To\nfacilitate training, we alter each layer using a\nbottleneck architecture as shown in Figure 2\nwith interior dimension k, where $T = FG$.\n3. Sequential Training: Once the training at\nlayer l is complete, that layer is frozen and the\nnext layer is unfrozen. For our experiments,\ntraining concluded once loss on the validation\nset had stopped decreasing (i.e. potential over-\nfitting of the training set was starting). Similar\ntraining is then performed on the next layer.\nThis process is repeated across all layers, re-\nsulting in weight matrices for each layer.\nBy the end of this sequential training and freez-\ning process, shown in Figure 1, the set of weight\nmatrices captures an accurate representation of the\ntask-dependent subspace within the weights of the"}, {"title": "Subspace Discrimination", "content": "Transformer model. This method is lightweight,\nmaintaining the computational efficiency of low\nrank training. The layer-by-layer approach was\ntaken because the early layers contain higher-level\nsemantic information, while the later layers con-\ntain more task/fact-specific information. Training\nin this method ensures the most reliable identifica-\ntion of the tasks."}, {"title": "Subspace Discrimination", "content": "Once a task-dependent subspace has been identi-\nfied, it could be removed by subtracting it from\nthe entire weight space (layer-by-layer). While\nthis may be effective at removing the task of in-\nterest, it leads to performance loss when similar\ntasks are also evaluated, i.e. ones that occupy sim-\nilar subspaces. Therefore, a method is required\nthat maintains the mutual information between\nthese two subspaces, only removing the informa-\ntion unique to the task of interest. We call this\nsubspace discrimination.\nTo achieve subspace discrimination, we utilize\na variation of the Gram-Schmidt process. Gram-\nSchmidt is used to orthogonalize a set of vectors\nin an inner product space. Given the subspace\nU spanned by vectors $u_1,\\ldots,u_n$, we can find\nthe orthogonal subspace to a vector $v_k$ with the\nfollowing:\n$v'_k = v_k - \\sum\\limits_{j=1}^N\\frac{(v_k, u_j)}{(u_j, u_j)}u_j$.\nA proof that $v'_k$ is orthogonal to all $u_j$ is offered in\nAppendix A. For our application, we compute:\n$SV(T_i) = SV(T_i) - \\sum\\limits_{j=1}^N\\frac{SV(T_0) \\cdot SV(T_i)}{SV_j(T_i) \\cdot SV_j(T_0)}SV_j(T_0)$\nwhere $T_i$ represents the identified subspace to be\nremoved, $T_0$ represents a similar task, and $SV_k(T_i)$\nrepresents the k-th singular vector of matrix $T_i$ for\none of the Transformer layers l. When applied to\ntwo tasks, every pair of weight matrices is decom-\nposed and separated in this manner. For three or\nmore tasks, the other task matrices; $T_{0,1}, T_{0,2},\\ldots,$\n$T_{0n}$, are added into one $T_0$ matrix, then the above\nequation is applied. We chose to use Euclidean in-\nner products, inspired by the original LORA paper\n(Hu et al., 2021), which demonstrated that effi-\ncient training could be achieved with linear rank\ndecompositions. While neural network parameter\nspaces are non-Euclidean, the practical success of\nthe LORA method justified our approach.\nInitially, the similarity of tasks was determined\nsubjectively. However, this subspace discrimina-\ntion method allows us to quantify task similarity,\nas there will be more overlap in the weight space\nof two similar sets of matrices. For two dissimi-\nlar tasks, the discrimination process will have no\neffect, as they are already orthogonal.\nSubspace discrimination is essential to the\nUNLEARN algorithm, allowing for the precise sep-\naration of task-specific information within shared\nweight spaces and ensuring that the removal of one\ntask does not undesirably impact the performance\non similar tasks. Consequently, subspace discrim-\nination enhances the algorithm's adaptability and\nrobustness."}, {"title": "Task Removal", "content": "The final step removes the task subspace. To\nachieve this, our approach uses SVD reconstruc-\ntion to reconstitute the modified task matrix, T\nfrom the singular values of $T_i$ and singular vectors\nSV(T\u00bf) above. Once $T'$ is computed, we subtract\nit from W' for each matrix in the LLM:\n$W' = W' \u2013 T'$\n4 Experiments\nAll experiments in this section use the same setup,\nwith Llama 2 70b serving as the LLM. For the\ntraining step in the subspace identification method\n(Section 3.1) as illustrated in Figure 2, we used\nthe Python package LORALIB (Hu et al., 2021)\nbut, rather than training a fine-tuning adapter, we\nmodified it to train the bottleneck in Figure 2 from\nscratch. We used a rank of k = 16. Only the\nattention matrices were modified during training.\nThis was inspired by the original LORA paper (Hu\net al., 2021), where they only adapted the attention\nweights."}, {"title": "Datasets", "content": "A diverse selection of benchmarks is essential\nto evaluate performance degradation across sim-\nilar tasks when modifying task-specific subspaces\nwithin LLMs. This study used two signification\ncollections of benchmarks: Holistic Evaluation\nof Language Models (HELM; et al., 2023c) and\nthe Beyond-the-Imitation-Game Benchmark (BIG-\nBench; et al., 2023a).\nHELM evaluates a wide range of use cases and\nmetrics, encompassing general language abilities"}, {"title": "Single Task Removal", "content": "to simple question-answering settings. This bench-\nmark evaluates models across multiple metrics-\naccuracy, fairness, robustness, efficiency, and more-\nproviding a detailed view into the general language\ncapabilities of models.\nComplementing HELM, BIG-Bench focuses on\nmore specific and niche tasks that probe the bound-\naries of current LLM capabilities. With 204 tasks\ncontributed from experts across fields, BIG-Bench\nwas invaluable for testing specific tasks that were\nbeyond the domain of HELM. Importantly, BIG-\nBench provided niche tasks that have little overlap\nwith other tasks, offering an unbiased perspective\non subspace removal.\nTogether, these datasets facilitate a comprehen-\nsive analysis of the influence of subspace removal\non LLM performance across a spectrum of tasks.\nBy integrating the thorough evaluation of HELM\nfor general language abilities with the specialized\ntasks from BIG-Bench, this study explores how ma-\nnipulation of tasks affects both broad and targeted\nmodel capabilities. This sheds light on the ability\nof UNLEARN to remove a task without affecting\nadjacent tasks."}, {"title": "Single Task Removal", "content": "The first trio of experiments evaluated the\nUNLEARN method using only subspace identi-\nfication (Section 3.1) and task removal (Section\n3.3) without the subspace discrimination method\n(Section 3.2). In these experiments, a single task\nwas removed and performance across a set of tasks\nwas observed. We will refer to these experiments\nas 'UNLEARN w/o D', where 'w/o D' refers to\nthe absence of subspace discrimination.\nIn the first experiment, the math word prob-\nlem dataset GSM8K (Cobbe et al., 2021) was\nremoved using the \u2018UNLEARN w/o D'method.\nThis is the first Targeted Task in Table 1. The\nfirst six columns under Evaluation Tasks were cho-\nsen because they are very different tasks from\nGSM8K, ranging from question-answering (Nar-\nrativeQA; Kocisky et al., 2017) to more general\nbenchmarks (MMLU; Hendrycks et al., 2021). Be-\ncause these tasks are dissimilar, they theoretically\nhave little overlap in their weight subspaces. Eval-\nuating the six chosen benchmarks on both the\nbase model and 'UNLEARN w/o D' model shows\nour approach successfully forgets (dropped per-\nformance) by 96.5% on the desired GSM8K task,\nwhile all other tasks had minimal degradation (less"}, {"title": "Task Discrimination", "content": "than 2.5%).\nFor the second experiment, an additional bench-\nmark was added, the arithmetic benchmark from\nBIG-Bench. The base model performs exception-\nally well on this task, as do most off-the-shelf\nLLMs; however, it was important for demonstrat-\ning what happens when there are two similar tasks,\nas arithmetic is quite similar to GSM8K. Again,\nthe 'UNLEARN w/o D' algorithm was applied to\nGSM8K. This time, the GSM8K benchmark was\nnot the only metric affected; the arithmetic metric\nwas also affected (down 33%), shown in Table 1.\nThe final experiment was similar to the second;\nhowever, the arithmetic benchmark was used to\ntrain our $T_i$ matrices. Table 1 shows that the arith-\nmetic benchmark performance degrades by 97%\nwhile the GSM8K metric worsens by 82%. Com-\npared these last two experiments, the removal of\na simpler task leads to greater degradation of the\nmore complex task compared to the reverse.\nThis outcome underscores the challenges with\ntask-specific subspace removal when dealing with\nclosely aligned tasks. The performance decline on\nthe second task suggests that the extracted subspace\non the first task contains features shared by the\nsecond's subspace, highlighting the need for the\nsubspace discrimination technique of Section 3.2."}, {"title": "Task Discrimination", "content": "Prompted by the shared degradation seen on arith-\nmetic and GSM8K in Section 4.2, these experi-\nments explore the connection between closely re-\nlated tasks and evaluate the efficacy of the subspace\ndiscrimination method proposed in Section 3.2.\nThese experiments aimed to orthongonally separate\nthe subspaces corresponding to two tasks, allow-\ning us to manipulate one subspace while preserv-\ning the integrity of the other. These experiments\nfocus on two sets of overlapping tasks: Narra-\ntiveQA/NaturalQuestions and arithmetic/GSM8K.\nThe first pair of tasks both involve question an-\nswering: NarrativeQA (Kocisky et al., 2017) an-\nswers questions over books or movie scripts, while\nNaturalQuestions (aet al., 2019) answers questions\nfrom Google search queries. Two separate experi-\nments were run: one with NarrativeQA as the task\nof interest and one with NaturalQuestions as the\ntask of interest. As seen in Table 1, when Nar-\nrativeQA is the task of interest, UNLEARN suc-\ncessfully reduces its performance while the perfor-\nmance on NaturalQuestions is relatively unaffected"}, {"title": "Optimal Rank", "content": "(down 7.5%). Similarly, when NaturalQuestions is\nthe task of interest, NarrativeQA's performance is\nmostly preserved while NaturalQuestions's perfor-\nmance is successfully reduced.\nThe second pair of tasks is arithmetic and\nGSM8K. When the subspace discrimination\nmethod is applied to GSM8K, performance on\nGSM8K successfully decreases while performance\non arithmetic is preserved. However, when the\nsubspace discrimination method is applied to arith-\nmetic as the task of interest, there is no degradation\nin the performance of either metric. This behav-\nior can be explained by the relative simplicity of\nthe arithmetic benchmark; its subspace is likely\nencapsulated within the subspace of the GSM8K\nmetric."}, {"title": "Optimal Rank", "content": "We explore the impact of varying the rank\nof the rank-deficient matrices during subspace\nidentification, as shown in Figure 2. For the\nNaturalQuestions vs NarrativeQA experiment of\nthe UNLEARN approach, the rank was varied:\nk =1,2,4,8,16,32. We found that the performance\nis not hindered for k values above 4 as seen in\nTable 2. However, there is a slight degradation of\nperformance on the tasks of interest for the lower-\nrank experiments; the task of interest was not for-\ngotten as effectively, and the similar task experi-\nenced greater performance degradation. This result\ncan be attributed to the subspace identification step\nnot capturing the subspaces for those tasks as accu-\nrately when the rank is lower.\nThese results suggest that the rank can be sig-\nnificantly reduced with minimal performance loss.\nThis is reasonable given that the subspaces of inter-\nest were quite small compared to the overall dimen-\nsions of the weight matrices. We hypothesize that\nthe minimum rank required for full performance\nwould vary slightly with the complexity of the task.\nThese insights provide a valuable direction for op-\ntimizing the efficiency of the UNLEARN method,"}, {"title": "Using UNLEARN to LEARN", "content": "especially in resource-constrained environments."}, {"title": "LEARN methodology", "content": "The UNLEARN methodology, initially designed\nfor the selective removal of task-specific informa-\ntion from LLMs, also presents a versatile frame-\nwork that can be adapted for the enhancement of\nmodel performance on particular tasks. This sec-\ntion explores 'LEARN,\u201d the application of our ear-\nlier UNLEARN algorithm for training on new in-\nformation. This method aims to add knowledge\nand/or amplify the representation of a given task\nwithin the model, leading to improved performance\non that task.\nThe LEARN approach uses the same principles\nas UNLEARN but inverts the application to focus\non task enhancement. Specifically, the method\ninvolves identifying the subspace associated with\na desired task using the approach in Section 3.1;\nthis step is identical to UNLEARN. The difference\ncomes with task addition instead of task removal;\nthe only necessary change is flipping the equation\nfor task removal from Section 3.3:\n$W' = W + T'$\nThis addition should bolster performance on a new\ntask, as the $T'$ sits on top of the existing weight ma-\ntrix, similar to the function of most LLM adapters.\nIn addtion, due to subspace discrimination (Section\n3.2), adding the new knowledge should have mini-\nmal adverse effects on other knowledge already in\nthe LLM."}, {"title": "LEARN evaluation", "content": "To evaluate the effect of the LEARN method, exper-\niments were conducted on tasks where pre-trained\nmodels showed suboptimal performance but had\nthe potential to perform well if fine-tuned. Identify-\ning tasks that meets these criteria for larger LLMs\n(50 B+ parameter) is challenging because they are\ntrained on such extensive datasets that it is more\ndifficult to find data not included in the training\nset. Therefore, by restricting the size of the LLM,\nwe limit the total learning capacity of the model,\nallowing us to squeeze out additional learning that\nthe LLM should be able to handle.\nThese experiments used a similar setting to be-\nfore, with the exception of using Llama 2 7b. The\ndataset of interest is LegalBench, a benchmark built\nby a collaboration between lawyers and ML engi-\nneers to measure legal reasoning in LLMs (et al.,"}, {"title": "Comparison to Existing Methods", "content": "2023b). Llama 2 7b performs between 30-50%\nacross all tasks, leaving room for improvement.\nWhen the LEARN algorithm was applied to the\nmodel for LegalBench, it showed marked improve-\nment across all tasks. Table 3 shows the consistent\nimprovement across tasks and a 40% boost to the\naverage performance of the system compared to the\nbase LLM. Training with LEARN is shown relative\nto traditional LoRA fine-tuning. Only the two tasks\nof interest were shown in Table 3 because there was\na similar lack of impact on the other tasks. LEARN\nmatches the performance of LoRA. By systemati-\ncally adding task-specific subspaces, LEARN fine-\ntunes the model's performance on a selected task\nand minimizes any degradation of other capabil-\nities due to the subspace discrimination method.\nThe dual capability of UNLEARN/LEARN under-\nscores its main value: the ability to use the same\ntraining runs for both forgetting and learning."}, {"title": "Comparison to Existing Methods", "content": "This section presents a comparative analysis of the\nUNLEARN/LEARN methodology against existing\nmethods, with a focus on generality and task per-\nformance."}, {"title": "Generality and Efficiency", "content": "A key advantage of UNLEARN/LEARN is its oper-\national flexibility. It offers a generalized framework\nthat can be applied to full fine-tuning or any PEFT\nmethod for fine-tuning. UNLEARN/LEARN ap-\nplies the same underlying principles in any setting-\neither adding or subtracting task-specific matrices\nfrom the model's weight matrices-to both enhance\n(LEARN) and diminish (UNLEARN) the model's\nperformance on specific tasks. Because the same\nset of matrices are being used regardless of algo-\nrithm, this simplifies model management and re-\nduces the computational and storage overhead."}, {"title": "Task Performance", "content": "In scenarios involving similar tasks, the differences\nbetween UNLEARN/LEARN and existing meth-\nods become even more pronounced. In the LEARN\nsetting of Table 3, both methods show comparable\nimprovements in task performance, demonstrating\ntheir efficacy for bolstering model performance. In\nthe forgetting setting, the UNLEARN algorithm\nis able to successfully discriminate between two\nsimilar tasks and only remove the task of interest."}, {"title": "Future Works", "content": "seek to remove without unwanted performance ef-\nfects on secondary tasks.\nConversely, with its precise subspace manipu-\nlation, the UNLEARN method allows for the se-\nlective removal of task influences without nega-\ntively impacting the performance of related tasks.\nThis specificity is particularly beneficial in multi-\ntask learning/unlearning environments where tasks\nshare overlapping features (similar weight sub-\nspaces). As such, UNLEARN is better suited for\nforgetting tasks while preserving similar tasks."}, {"title": "Future Works", "content": "This paper has laid the groundwork for several in-\ntriguing avenues for future research. First, while\nour initial work focused on removing broad domain\nknowledge, future efforts will extend this method-\nology to the removal of specific knowledge and\nfacts. We are currently collecting datasets that will\nfacilitate this extension, particularly in scenarios\ninvolving private or harmful information.\nThere are some scalability concerns if UN-\nLEARN is applied to a large number of tasks.\nWhile the current work targets the selective re-\nmoval of a small number of unwanted tasks, future\nresearch will investigate strategies to efficiently\nhandle discrimination between larger sets of simi-\nlar tasks.\nOur current approach was largely inspired by the\noriginal LORA paper (Hu et al., 2021), which was"}, {"title": "Conclusion", "content": "our motivation for only manipulating the attention\nweights. Subsequent research into LORA revealed\nthe effectiveness of manipulating the other layers\nwithin an LLM. Future works will explore the adap-\ntion of other layers to enhance the flexibility and\nperformance of UNLEARN."}, {"title": "Conclusion", "content": "This paper introduces UNLEARN, a novel ap-\nproach for forgetting selected knowledge in Large\nLanguage Models. This method relies on subspace\nidentification for tasks and subspace discrimina-\ntion between similar tasks. The experimental re-\nsults demonstrate significant performance gains,\nhighlighting the effect of UNLEARN on removing\nunwanted knowledge without having deleterious\neffects on related tasks. The method's ability to\nisolate and remove specific subspaces within the\nmodel ensures precise unlearning, making it a valu-\nable tool for managing the complexities of task\nforgetting.\nCompared to state-of-the-art methods like Gra-\ndient Ascent, UNLEARN offers substantial advan-\ntages in terms of generality, efficiency, and pre-\ncision. UNLEARN achieves 96% forgetting on\nthe task of interest while maintaining performance\non other tasks within 2.5% of the original model.\nWhen similar tasks are considered, UNLEARN\nachieves nearly 80% forgetting on the task of in-\nterest while preserving performance on the similar\ntask within 10% of the original model. The dis-\ncriminative ability of UNLEARN far outpaces that\nof the existing state-of-the-art, ensuring targeted\nunlearning without compromising the performance\non related tasks."}, {"title": "Limitations", "content": "Although UNLEARN enhances the abilities of\nLLMs to forget knowledge, certain limitations still\nneed to be addressed. One limitation is when tasks\ncompletely overlap, as observed with arithmetic\nand GSM8K. When a subspace is entirely con-\ntained within another, as arithmetic was within\nGSM8K, it becomes challenging to discriminate\nbetween these two tasks. This highlights the dis-\ntinction between knowledge and the metrics that\nmeasure knowledge, which we will explore this\ndistinction in future works.\nAnother limitation of this paper that will be ad-\ndressed in future work is to more fully leverage the\nexperimental insights to optimize the efficiency of"}, {"title": "Ethics Statement", "content": "the UNLEARN method."}, {"title": "Ethics Statement", "content": "While UNLEARN has significant potential bene-\nfits, such as improving model flexibility and effi-\nciency, we are also mindful of the ethical implica-\\"}]}