{"title": "MOLE: Enhancing Human-centric Text-to-image\nDiffusion via Mixture of Low-rank Experts", "authors": ["Jie Zhu", "Yixiong Chen", "Mingyu Ding", "Ping Luo", "Leye Wang", "Jingdong Wang"], "abstract": "Text-to-image diffusion has attracted vast attention due to its impressive image-\ngeneration capabilities. However, when it comes to human-centric text-to-image\ngeneration, particularly in the context of faces and hands, the results often fall\nshort of naturalness due to insufficient training priors. We alleviate the issue\nin this work from two perspectives. 1) From the data aspect, we carefully col-\nlect a human-centric dataset comprising over one million high-quality human-\nin-the-scene images and two specific sets of close-up images of faces and hands.\nThese datasets collectively provide a rich prior knowledge base to enhance the\nhuman-centric image generation capabilities of the diffusion model. 2) On the\nmethodological front, we propose a simple yet effective method called Mixture of\nLow-rank Experts (MoLE) by considering low-rank modules trained on close-up\nhand and face images respectively as experts. This concept draws inspiration from\nour observation of low-rank refinement, where a low-rank module trained by a\ncustomized close-up dataset has the potential to enhance the corresponding image\npart when applied at an appropriate scale. To validate the superiority of MoLE in\nthe context of human-centric image generation compared to state-of-the-art, we", "sections": [{"title": "1 Introduction", "content": "Human-centric text-to-image generation is an important orientation for realistic applications, e.g.,\nposter design, virtual reality, etc. However, current models encounter issues with producing natural-\nlooking results, particularly in the context of faces and hands 2. To address this issue, we delve into\nthe matter and identify two factors that may contribute to this issue. Firstly, the absence of high-\nquality human-centric data makes diffusion models lack sufficient human-centric prior 3; Secondly,\nin the human-centric context, faces and hands represent the two most complicated parts due to high\nvariability, making them challenging to be generated naturally.\nHence, we alleviate this problem from two perspectives. On the one hand, we collect a human-in-\nthe-scene dataset of high quality and high resolution from the Internet. Basically, the resolution of\neach image is various and over 1024 \u00d7 2048. The dataset contains approximately one million images,\nand covers different races, various gestures, and activities, thereby providing diffusion models with\nsufficient knowledge to improve the performance of human-centric generation. However, for the\nsecond factor, our experiment in Sec 5.3 demonstrates though fine-tuning on above dataset brings an\noverall enhancement in human quality, human face and hand still exhibit unnatural outcomes, possibly\nbecause diffusion models focus more on overall performance during fine-tuning while struggling to\naccurately capture highly variable parts like face and hand gestures.\nTo address the second factor, an interesting low-rank refinement phenomenon inspires us. As shown\nin Fig 2, when combined with a customized low-rank module [19] and using a proper scale weight,\nStable Diffusion v1.5 (SD v1.5) [40] has the potential to refine the corresponding part of a person,\ne.g., for hand, s = 0.4 subtly refines the appearance of the woman drinking milk's hand. Thus,\ninspired by this, to refine the face and hand, we can first gather two customized high-quality datasets\n(one for face close-ups and one for hand close-ups) to train two low-rank modules, respectively. Then,\nfor the two specialized low-rank modules, we could add a certain assignment to adaptively select\nwhich low-rank module to use for a given input and Mixture of Experts (MoE) naturally stands out.\nMoreover, as face and hand often appear simultaneously in an image for a person, motivated by Soft\nMoE [35], we could adopt a soft assignment to produce adaptive scale weights, activating multiple\nexperts to handle the input at the same time. We refer to all mentioned three datasets above together\nas human-centric dataset for convenience as shown in Fig 3."}, {"title": "3 Human-centric Dataset", "content": "Overview. Our human-centric dataset involves over one million high-quality images, containing\nthree parts (See Sec 3.1). As shown in Fig 3, these images are diverse w.r.t. occasions, activities,\ngestures, ages, genders, and racial backgrounds. Specifically, approximately 57.33% individuals\nidentify as White, 14.68% as Asian, 9.98% as Black, 5.11% as Indian, 5.52% as Latino Hispanic,\nand 7.38% as Middle Eastern 5. Approximately 58.18% are male and 41.82% are female. For age,\napproximately 0.93% are babies (0-1 years old), 3.55% are kids (2-11 years old), 4.60% are teenagers\n(12-18 years old), 84.86% are adults (18-60 years old), and 6.06% are elderly (over 60 years old).\nEthical & legal compliance. Our collection is in compliance with the ethics and law as all images are\ncollected from websites under Public Domain CC0 1.06 license that allows free use, redistribution,\nand adaptation for non-commercial purposes. To avoid concerns, please see our license and privacy\nstatement in Appendix A.2. Note that this dataset is allowed for academic purposes only. When using\nit, the users are requested to ensure compliance with ethical and legal regulations."}, {"title": "3.1 Human-centric Dataset Constitution", "content": "Human-in-the-scene images. We primarily collect high-resolution human-centric images from\nInternet and the image resolution is basically over 1024 \u00d7 2048, providing sufficient priors for\ndiffusion models. To enable training, we use a sliding window (1024 \u00d7 1024) to crop the image to\nmaintain as much information as possible. However, for an image, high resolution does not mean\nhigh quality. Therefore, we train a VGG19 [45] to filter out blurred images. Additionally, considering\nthe crop operation could generate images that are full of background or contain little information\nabout people, we train a VGG19 [45] to filter out such bad cases 7. To ensure the quality, we repeat\nthe two processes multiple times until we do not find any case mentioned above in three times of\nrandom sampling. By employing these strategies, we can remove amounts of noise and useless\nimages, thereby guaranteeing the image quality.\nClose-up of face images. The face dataset contains two sources: the first is from Celeb-HQ [22] in\nwhich we choose images of high quality with size 1024 \u00d7 1024; The second is from Flickr-Faces-HQ\n(FFHQ) [23]. We sample images covering different skin color, age, sex, and race. There are around\n6.4k face images. We do not sample more face images as it is sufficient for low-rank expert training.\nClose-up of hand images. The hand dataset contains three sources: the first is from 11k Hands [1]\nwhere we randomly sample around 1k high-quality images and manually crop them to square; The\nsecond is from the Internet where we collect hand images of high quality and resolution with simple\nbackgrounds and use YOLOv5 [9] to detect hands and crop them with details maintained; The third\nis from human-in-the-scene images (before processing) where we sample 8k images. We check every\nimage and manually crop the hand of the image to square if the image is appropriate and the hand\nis clear. In this close-up hand dataset, there are abundant hand gestures and scenarios shown in\nFig 3, e.g., holding a flower, writing, etc. There are 7k high-quality hand images. To the best of our\nknowledge, such a high quality close-up hand dataset is absent in prior related studies."}, {"title": "3.2 Image Caption Generation", "content": "When collecting the dataset, we primarily consider image quality and resolution, neglecting whether\nit is text paired so as to increase image amount. Thus, producing a caption for each image is required.\nWe investigate four recently proposed SOTA models including BLIP-2 [26], ClipCap [32], MiniGPT-\n4 [53], and LLaVA [29]. We show several cases in Fig 4. One can see that BLIP-2 usually produces\nsimple descriptions and ignores details. ClipCap has a better performance but still lacks sufficient\ndetails along with the wrong description. MiniGPT4, although gives detailed descriptions, is inclined\nto spend a long time (17s on average) generating long and inaccurate captions that exceed the input\nlimit (77 tokens) of the Stable Diffusion CLIP text encoder [36]. In contrast, LLaVA produces neat\ndescriptions in one sentence with accurate details in a short period (3-5s). Afterward, we manually\nstreamline long LLaVA caption with a new shorter caption by ourselves while aligning with the\ncontent of the image. We also remove unrelated and uninformative text patterns, e.g., \u201cThe image\nfeatures that...", "showcasing...": "creating...\", \"demonstrating . . . \", etc. To further ensure the\ncaption alignment of LLaVA, we use CLIP to filter image-text pairs with lower scores.\""}, {"title": "4 Method", "content": null}, {"title": "4.1 Preliminary", "content": null}, {"title": "Low-rank Adaptation (LoRA)", "content": "Given a customized dataset, instead of training the entire model,\nLORA [19] is designed to fine-tune the \u201cresidual\" of the model, i.e., \u25b3W:\nW' = W + AW \nwhere AW is decomposed into low-rank matrices: \u25b3W = ABT (A\u2208Rnxd, B\u2208 Rm\u00d7d, d < n,\nand d < m). During training, we can simply fine-tune A and B instead of W, making fine-tuning on\ncustomized dataset memory-efficient. In the end, we get a small model as A and B are much smaller\nthan W."}, {"title": "Mixture-of-Experts (MoE)", "content": "MoE [20, 44, 24] is designed to enhance the predictive power of models\nby combining the expertise of multiple specialized models. Usually, a central \"gating\" model G(.)\nselects which specialized model to use for a given input:\ny = \u2211G(x)Ei(x).\nWhen G(x) = 0, the corresponding expert E\u2081 will not be activated."}, {"title": "4.2 Mixture of Low-rank Experts", "content": "Motivated by the two potential factors discussed in Sec 1, our method contains three stages as shown\nin Fig 5. We describe each stage below and put the training details in Appendix A.1.\nStage 1: Fine-tuning on Human-centric Dataset. The overall poor performance of human-centric\ngeneration could be attributed to the absence of large-scale high-quality datasets. Considering such\na pressing need, our work bridges this gap by providing a carefully collected dataset that contains\naround one million human-centric images of high quality. To learn as much prior as possible, we\nadopt SD v1.5 as a baseline and leverage the whole human-centric datasets to fine-tune. Concretely,\nwe fine-tuning the UNet modules [41] (and text encoder) while fixing the rest parameters. The\nwell-trained model is then sent to the next stage.\nStage 2: Low-rank Expert Generation. To construct MoE, in this stage, our goal is to prepare two\nexperts that are supposed to contain abundant knowledge about the corresponding part. To achieve\nthis, we train two low-rank modules using two customized datasets. One is the close-up face dataset.\nThe other is the close-up hand dataset that contains abundant hand gestures, full details with simple\nbackgrounds, and interactions with other objects. We then use the two datasets to train two low-rank\nexperts with SD v1.5 trained in stage 1 as the base model. The low-rank experts are expected to focus\non the generation of face and hand and learn useful context.\nStage 3: Soft Mixture Assignment. This stage is motivated by the low-rank refinement phenomenon\nin Fig 2 where a specialized low-rank module using a proper scale weight is able to refine the\ncorresponding part of a person. Hence, the key is to activate different low-rank modules with suitable\nweights. From this view, MoE naturally stands out and we novelly regard a low-rank module trained"}, {"title": "5 Experiment", "content": null}, {"title": "5.1 Evaluation Benchmarks and Metrics", "content": "Considering that our work primarily focuses on human-centric image generation, before presenting\nour experiment, we introduce two customized evaluation benchmarks. Additionally, since our"}, {"title": "6 Discussion", "content": "To further highlight our method contribution, below we present a comprehensive discussion on\ndistinctions between MoLE and conventional MoE methods from three aspects. We also discuss\nthe contribution of our curated human-centric dataset and analysis about ratios of different races in\nAppendix A.12. Firstly, from the aspect of training, MoLE independently trains two experts to learn\ncompletely different knowledge using two customized close-up datasets. In contrast, conventional\nMoE methods simultaneously train experts and base models using the same dataset. Secondly, from\nthe aspect of expert structure and assignment manner, MoLE simply uses two low-rank matrices\nwhile conventional MoE methods use MLP or convolutional layers. Moreover, MoLE combines local"}, {"title": "7 Conclusion", "content": "In this work, we primarily focus on the human-centric text-to-image generation that has important\nreal-world applications but often suffers from producing unnatural results due to insufficient prior,\nespecially the face and hand. To mitigate this issue, we carefully collect and process one million high-\nquality human-centric images, aiming to provide sufficient prior. Besides, we observe that a low-rank\nmodule trained on a customized dataset, e.g., face, has the capability to refine the corresponding part.\nInspired by it, we propose a simple yet effective method called Mixture of Low-rank Experts (MOLE)\nthat effectively allows diffusion models to adaptively select experts to enhance the generation quality\nof corresponding parts. We also construct two customized human-centric benchmarks from COCO\nCaption and DiffusionDB to verify the superiority of MoLE."}, {"title": "8 Limitation & Future Work", "content": "Honestly, although our experiments confirm MoLE's effectiveness in enhancing human-centric\nimage generation, there is still considerable room for improvement. Our method struggles with\nscenarios involving multiple individuals, likely due to our dataset being primarily single-person\nimages and uncertainty about the applicability of observations in Fig 2 to such cases. Additionally,\nin generating images using identical prompts, we observe that only about 25% of MoLE's results\nare of high quality, a remarkable improvement over SDXL's 10%, but still below practical standards.\nPotential reasons include insufficient close-up data for hands and faces and the need for further tuning\nof hyperparameters. Future work will focus on model optimization, improving data quality, and\nenhancing dataset diversity to better represent various demographics and real-world scenarios."}, {"title": "9 Broader Impact", "content": "MOLE mainly focuses on enhancing human-centric text-to-image generation in diffusion models.\nIt refrains from introducing any harmful content to the community and society. However, though\nMOLE may not introduce more biases on race, it also inherits the biases in the training data like\npervious methods. Hence it will be more meaningful to enhance the diversity of our collected dataset\nto represent different demographics and real-world scenarios better. As for other impacts such as\nfake faces, it also inevitably generates fake faces like other generative models, which requires users\nto leverage these generated images carefully and legally. We highlight that these issues also warrant\nfurther research and consideration. We maintain transparency in our methods with open-source code\nand dataset composition, allowing for continuous improvement based on community feedback."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Implementation Details", "content": null}, {"title": "Stage 1: Fine-tuning on human-centric Dataset", "content": "We use Stable Diffusion v1.5 as base model and\nfine-tune the UNet (and text encoder) with a constant learning rate 2e \u2013 6. We set batch size to 64\nand train with the Min-SNR weighting strategy [14]. The clip skip is 1 and we train the model for\n300k steps using Lion optimizer [5]. We use this stage on SD v1.5. For SD v2.1 and SDXL, we do\nnot use this stage to fine-tune the base models as their overall human-centric generation is relatively\nsatisfying but only looks poor on the details of face and hand."}, {"title": "Stage 2: Low-rank Expert Generation", "content": "For face expert, we set batch size to 64 and train it 30k\nsteps with a constant learning rate 2e - 5. The rank is set to 256 and AdamW optimizer is used. For\nhand expert, we set batch size to 64. Since hand is more sophisticated than face to generate, we train\nit 60k steps with a smaller learning rate 1e - 5. The rank is also set to 256 and AdamW optimizer is\nused. For both experts, we only add low-rank module to UNet. And the two experts are both built on\nthe fine-tuned base model in Stage 1."}, {"title": "Stage 3: Mixture Adaptation", "content": "In this stage, we use the batch size 64 and employ AdamW optimizer.\nWe use a constant learning rate le - 5 and train for 50k steps."}, {"title": "A.2 License and Privacy Statement", "content": "The human-centric dataset is collected from websites including seeprettyface.com, unsplash.com,\ngratisography.com, morguefile.com, pexels.com, etc. We use web crawler to download images\nonly when it is allowed. Most images in these websites are published by their respective authors\nunder Public Domain CC0 1.0 11 license that allows free use, redistribution, and adaptation for non-\ncommercial purposes. Seeprettyface.com require giving appropriate credit to the author by adding\nthe sentence (# Thanks to dataset provider:Copyright(c) 2018, seeprettyface.com, BUPT_GWY\ncontributes the dataset.) to the open-source code when using the images. When collecting and filtering\nthe data, we are careful to only include images that, to the best of our knowledge, are intended for\nfree use and redistribution by their respective authors. That said, we are committed to protecting the\nprivacy of individuals who do not wish their images to be included. Besides, for images fetched from\nother datasets, e.g., Flickr-Faces-HQ (FFHQ) [23], Celeb-HQ [22], and 11k Hands [1], we strictly\nfollow their licenses and privacy. Note that this dataset is allowed for academic purposes only. When\nusing it, the users are requested to ensure compliance with ethical and legal regulations. For the\napplication for the usage of generated images and the dataset, we will carefully review the applicant's\nqualifications, purpose of use, possible risks [56, 54, 55], etc. Finally, we only allow authorized\npersonnel to interact with the data."}, {"title": "A.3 Filter Training and Illustrations of Negative Samples", "content": "In both case, to train the VGG19, we manually collect around 300 positive samples and 300 negative\nsamples as training set, and we also collect around 100 positive samples and 100 negative samples as\nval set. When training the VGG19, we set the batch size to 128, set the learning rate to 0.001, and\nuse random flip as the data augmentation method. We train the model for 200 epochs and use the\nbest-performing model for subsequent classification. In Fig 9, we present the illustrations of negative\nsamples during refining human-in-the scene subset."}, {"title": "A.4 Comparison with Existing Related Datasets", "content": "We give a comparison of the differences between existing datasets like CosmicMan [28] and our\nnewly collected dataset, which primarily lie in four aspects:\n\u2022 From the aspect of image diversity, due to different motivations, CosmicMan only contains\nhuman-in-the-scene images while our dataset also involves two close-up datasets for face and hand,\nrespectively. Moreover, to the best of our knowledge, the high-quality close-up hand dataset is absent\nin prior related studies."}, {"title": "A.5 More Quantitative Comparisons", "content": "We perform four kinds of comparisons. We first evaluate MoLE and SD v1.5 on CLIP-T [25],\nFID [16], and Aesthetic Score [43] to present a comprehensive comparison. Specifically, We follow\n[25] and [21] using COCO Human Prompts and Human-centric datasets to evaluate the overall\nquality of generated images on CLIP-T and FID. We also use an aesthetic predictor 12 to generate\nthe Aesthetic Score. All the results are reported in Tab 4. We find MoLE is also superior in CLIP-T\nand FID. We also find that MoLE is slightly inferior to SD v1.5 in aesthetic score. We deem that\nit is reasonable as SD v1.5 is especially fine-tuned on laion-aesthetics v2 5+ dataset in which each\nimage's aesthetic score is evaluated with high aesthetics score by exactly the aesthetic predictor we\nused in this comparison."}, {"title": "A.6 Weight Distribution of Local Assignment", "content": "We provide the distribution of the local weight sent to each expert in Fig 12. To obtain this, we\ngenerate 10 samples for close-up images and normal human images, respectively, and collect local\nweights for each expert. In Fig 12, one can see that for close-up images, e.g., face, the corresponding\nexpert receives more weights of high value. We think this effectively demonstrates the efficacy of the\nsoft assignment mechanism in MoLE, which adaptively activates the relevant expert to contribute\nmore to the generation of close-up images. When generating normal human images involving face\nand hand, the two experts contribute equally, and generally, the face expert receives relatively more\nweights of high value as the area of face is typically larger than that of hand."}, {"title": "A.7 Ablation Study on Experts", "content": "To understand the importance of using two experts on the model's performance, we train only one\nexpert using all close-up images and compare the performance with that of two experts. We find that\none expert achieves 20.19 \u00b10.03 HPS(%), inferior to that of two experts (20.27 \u00b1 0.07 HPS), which\ndemonstrates the necessity of using one expert for face and hand, respectively."}, {"title": "A.8 More Visualization", "content": "We present more generated images and compare with other diffusion models in Fig 13. We provide\nmore full-body images in Fig 14. Additionally, we illustrate more images generated by MoLE in\nFig 17, Fig 18, Fig 19, and Fig 20. We also compare MoLE (build on SDXL) with SDXL in Fig 21,\nFig 22, and Fig 23 where MoLE further enhances SDXL by generating more natural face/hand."}, {"title": "A.9 Generic Image Generation", "content": "As shown in Fig 15, MoLE (fine-tuned SD v1.5) can also generate non-human-centric images, e.g.,\nanimals, scenery, etc. The main reason could be that the human-centric dataset also contains these\nentities that interact with humans in an image. As a result, the generative model learns these concepts.\nHowever, intuitively, MoLE may not be better at generic image generation than the generic generative\nmodels as MoLE is trained on a human-centric dataset."}, {"title": "A.10 Failure Case Analysis", "content": "We observe several failure cases involving unrealistic content. For example, we find a generated\nunrealistic images with a half-man in Fig 16 (a) whose prompt is \"a young man skateboarding while\nlistening to music\". To figure out the reason, we sample 10 bad and normal cases, calculate their L2\nnorm of outputs from face and hand expert respectively, and visualize the averaged L2 norm across\ntimestep as shown in Fig 16 (b). One can see that the bad cases generally have larger L2 norm for\nboth experts, which indicates that the output from linear layer in Eq 7 is strongly influenced by the\ntwo experts. As a result, the generated images may be uncoordinated. We leave this as feature work."}, {"title": "A.11 More Related Work", "content": "Text-to-image generation. Diffusion model [17, 46] has been widely used in image generation since\nits proposal. Afterward, a vast effort has been devoted to exploring the applications, especially in\ntext-to-image generation. GLIDE [34] leverages two different kinds of guidance, i.e., classifier-free"}, {"title": "A.12 More Discussion", "content": "Dataset Contribution. To the best of our knowledge, we newly collect and propose a very high-\nquality and comprehensive human-centric dataset simultaneously meeting legal compliance.\n\u2022 High-quality. Our human-centric dataset consists of images of high resolution (basically over\n1024 x 2048) and large file size, e.g., 1M, 3M, 5M, and 10M, collected from websites (e.g., https:\n//www.pexels.com/search/people/ and https://unsplash.com/s/photos/people). The\nreaders can simply click these links to have a look. Such quality is missing in current widely used\nimage-text datasets, e.g., LAION2b-en. To support it, we sample 350K human-centric images from\nLAION2b-en. The averaged height and width are approximately 455 and 415 respectively. Most of\nthem are between 320 and 357. Compared to our human-centric dataset, they fail to provide sufficient\nprior and high-quality details.\n\u2022Comprehensive. As the natural face and hand are relatively hard to generate compared to other\nparts as discussed in the community, in addition of the collected human-in-the-scene subset, we also"}]}