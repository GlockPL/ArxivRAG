{"title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences", "authors": ["Weijian Luo"], "abstract": "One-step text-to-image generator models offer advantages such as swift inference efficiency, flexible architectures, and state-of-the-art generation performance. In this paper, we study the problem of aligning one-step generator models with human preferences for the first time. Inspired by the success of reinforcement learning using human feedback (RLHF), we formulate the alignment problem as maximizing expected human reward functions while adding an Integral Kullback-Leibler divergence term to prevent the generator from diverging. By overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the first, fast-converging and image data-free human preference alignment method for one-step text-to-image generators. We also introduce novel theoretical insights, showing that using CFG for diffusion distillation is secretly doing RLHF with DI++. Such an interesting finding brings understanding and potential contributions to future research involving CFG. In the experiment sections, we align both UNet-based and DiT-based one-step generators using DI++, which use the Stable Diffusion 1.5 and the PixelArt-a as the reference diffusion processes. The resulting DiT-based one-step text-to-image model achieves a strong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO validation prompt dataset. It also achieves a leading Human preference Score (HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-a. Both theoretical contributions and empirical evidence indicate that DI++ is a strong human-preference alignment approach for one-step text-to-image models.", "sections": [{"title": "Introductions", "content": "In recent years, deep generative models have achieved remarkable successes across various data generation and manipulation applications (Karras et al., 2020; 2022; Nichol & Dhariwal, 2021; Oord et al., 2016; Ho et al., 2022; Poole et al., 2022; Hoogeboom et al., 2022; Kim et al., 2022; Tashiro et al., 2021; Kingma & Dhariwal, 2018; Chen et al., 2019; Meng et al., 2021; Couairon et al., 2022; Zhang et al., 2023a; Luo & Zhang, 2024; Xue et al., 2023; Luo et al., 2023b; Zhang et al., 2023b; Feng et al., 2023; Deng et al., 2024; Luo et al., 2024c; Geng et al., 2024; Wang et al., 2024; Pokle et al., 2022). These models have notably excelled in producing high-resolution, text-conditional models such as images (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022; 2021; Luo et al., 2024b) and other modalities with different applications(Brooks et al., 2024; Kondratyuk et al., 2023; Evans et al., 2024; Luo et al., 2024c), pushing the boundaries of Artificial Intelligence Generated Content.\nAmong the spectrum of deep generative models, one-step generators have emerged as a particularly efficient and possibly best performing (Zheng & Yang, 2024; Kim et al., 2024; Kang et al., 2023; Sauer et al., 2023a) generative model. Briefly speaking, a one-step generator uses a neural network to directly transport some latent variable to generate an output sample. Recently, there have been many fruitful successes in training one-step generator models by distilling from pre-trained diffusion models (aka, Diffusion Distillation (Luo, 2023)) in domains of image generations (Salimans & Ho, 2022; Luo et al., 2024a; Geng et al., 2023; Song et al., 2023; Kim et al., 2023; Song & Dhariwal, 2023; Zhou et al., 2024b), text-to-image synthesis (Meng"}, {"title": "Preliminary", "content": "Diffusion Models. In this section, we introduce preliminary knowledge and notations about diffusion models. Assume we observe data from the underlying distribution $q_d(x)$. The goal of generative modeling is to train models to generate new samples $x \\sim q_d(x)$. Under mild conditions, the forward diffusion process of a diffusion model can transform any initial distribution $q_0 = q_d$ towards some simple noise distribution,\n$dx_t = F(x, t) dt + G(t) dw_t,$\\newline where $F$ is a pre-defined vector-valued drift function, $G(t)$ is a pre-defined scalar-value diffusion coefficient, and $w_t$ denotes an independent Wiener process. A continuous-indexed score network $s_{\\theta}(x, t)$ is employed to approximate marginal score functions of the forward diffusion process (2.1). The learning of score networks"}, {"title": "Human-preference Alignment of One-step Text-to-image Models", "content": "In this section, we introduce how to align one-step text-to-image generator models and human preferences using Diff-Instruct++. In Section 3.1, we introduce the formulation of the alignment problem and then identify the alignment objective. After that, we address technical challenges and propose the Theorem 3.1 and Theorem 3.2, which set the theoretical foundation of DI++ through the lens of reward maximization. Interestingly, we also find that the diffusion distillation using classifier-free guidance is secretly doing RLHF with DI++. Based on the theoretical arguments in Section 3.1, we formally introduce the practical algorithm of DI++ in Section 3.3, and give an intuitive understanding of the algorithm as an education process that includes a teacher diffusion model, a teaching assistant diffusion model, and a student one-step generation."}, {"title": "The Alignment Objective", "content": "The Problem Formulation. We consider the text-to-image generation task. Other conditional generation applications share a similar spirit. Assume x is an image and c is a text prompt that is sampled from some prompt dataset C. The basic setting is that we have a one-step generator $g_{\\theta}(\\cdot)$, which can transport a prior latent vector $z \\sim p_z$ to generate an image based on input text prompt c: $x = g_{\\theta}(z|c)$. We use the notation $p_{\\theta}(x|c)$ to denote the distribution induced by the generator. We also have a reward model $r(x, c)$ which represents the human preference on a given image-prompt pair $(x, c)$.\nInspired by the success of reinforcement learning using human feedback (Ouyang et al., 2022) in fine-tuning large language models such as ChatGPT (Achiam et al., 2023), we first set our alignment objective to maximize the expected human reward function with an additional Kullback-Leibler divergence regularization"}, {"title": "Classifier-free Guidance is Secretly Doing RLHF.", "content": "In previous sections, we have shown in theory that with available reward models, we can readily do RLHF for one-step generators. However, in this part, we additionally find that the classifier-free guidance is secretly doing RLHF, and therefore we will show that using CFG for reference diffusion models when distilling them using Diff-Instruct is secretly doing Diff-Instruct++ with an implicitly defined reward.\nThe classifier-free guidance (Ho & Salimans, 2022) (CFG) uses a score function with a form\n$s_{ref}(x_t, t|c) := s_{ref}(x_t, t|\\emptyset) + w\\{s_{ref}(x_t, t|C) - s_{ref}(x_t, t|\\emptyset)\\}$\nto replace the original conditions score function $s_{ref}(x_t, t|c)$. This empirically leads to better sampling quality for diffusion models. In this part, we show how diffusion distillation using Diff-Instruct with CFG is secretly doing RLHF with DI++. If we consider a reward function as:\n$r(x_0, c) = \\int_{t=0}^{T} w(t) log \\frac{p_{ref}(x_t|t, c)}{p_{ref}(x_t|t)} dt$\nThis reward function will put a higher reward to those samples that have higher conditional probability than unconditional probability. Therefore, it can encourage samples with high conditional probability. We show that the gradient formula (3.4) in Theorem 3.2 will have an explicit solution:\n$Grad(\\theta) =E_{c,t,z\\sim p_z, q=g_{\\theta}(z|c)} \\beta w(t)\\left\\{s_{\\theta}(x_t|t, c) - s_{ref}(x_t|t, c)\\right\\}\\frac{d x_t}{d \\theta}$\nwhere $s_{ref}(x_t|t, c) = s_{ref}(x_t|t) + (1 + \\frac{1}{\\beta}) [s_{ref}(x_t|t, c) - s_{ref}(x_t|t)]$\nWe will give the proof in Appendix B.4. This gradient formula (3.6) recovers the case that uses the CFG for diffusion distillation using the Diff-Instruct algorithm to train a one-step generator. The parameter $(1+\\frac{1}{\\beta})$ is the so-called classifier-free guidance scale. In our Algorithm 1 and 2, we use the coefficient $a_{cfg}$ to represent the CFG scale. In the following section, we formally introduce the practical algorithm of Diff-Instruct++.\nRemark 3.5. Theorem 3.4 has revealed a new perspective that understands classifier-free guidance as a training-free and inference-time RLHF. This helps to understand why humans prefer samples generated by using CFG. Besides, Theorem 3.4 also shows that using Diff-Instruct with CFG to distill text-to-image diffusion models is secretly doing DI++. Therefore we can use both CFG and the human reward to strengthen the one-step generator models."}, {"title": "A Practical Algorithm", "content": "Though the gradient formula (3.2) gives an intuitive way to compute the parameter gradient to update the generator, it would be better to have an easy-to-implement pseudo loss function instead of the gradient"}, {"title": "Related Works", "content": "RLHF for Large Language Models. Reinforcement learning using human feedback (RLHF) has won great success in aligning large language models (LLMs). Ouyang et al. (2022) formulates the LLM alignment problem as maximization of human reward with a KL regularization to some reference LLM, resulting in the InstructGPT model. The Diff-Instruct++ draws inspiration from Ouyang et al. (2022). However, DI++ differs from RLHF for LLMs in several aspects: the introduction of IKL regularization, the novel gradient formula, and the overall algorithms. Many variants of RLHF for LLMs have also been intensively studied in Tian et al. (2023); Christiano et al. (2017); Rafailov et al. (2024); Ethayarajh et al. (2024), etc.\nDiffusion Distillation Through Divergence Minimization. Diffusion distillation (Luo, 2023) is a research area that aims to reduce generation costs using teacher diffusion models. Among all existing methods, one important line is to distill a one-step generator model by minimizing certain divergences between one-step generator models and some pre-trained diffusion models. (Luo et al., 2024a) first study the diffusion distillation by minimizing the Integral KL divergence. Yin et al. (2023) generalize such a concept and add a data regression loss to distill pre-trained Stable Diffusion Models. Many other works have introduced additional techniques and improved the distillation performance (Geng et al., 2023; Kim et al., 2023; Song et al., 2023; Song & Dhariwal; Nguyen & Tran, 2023; Song et al., 2024; Yin et al., 2024; Zhou et al., 2024a; Heek et al., 2024; Xie et al., 2024; Salimans et al., 2024). Different from IKL divergence, Zhou et al. (2024b) study the distillation through a variant of Fisher divergence. Other methods, such as Xiao et al. (2021); Xu et al. (2024), have used the generative adversarial training (Goodfellow et al., 2014) techniques in order to minimize certain divergences. The Diff-Instruct++ is motivated by Diff-Instruct. However, to the best of our knowledge, we are the first to study the problem of aligning one-step generator models with human preferences.\nPreference Alignment for Diffusion Models. In recent years, many works have emerged trying to align diffusion models with human preferences. There are three main lines of alignment methods for diffusion models. 1) The first kind of method fine-tunes the diffusion model over a specifically curated image-prompt dataset (Dai et al., 2023; Podell et al., 2023). 2) the second line of methods tries to maximize some reward functions either through the multi-step diffusion generation output (Prabhudesai et al., 2023; Clark et al., 2023; Lee et al., 2023) or through policy gradient-based RL approaches (Fan et al., 2024; Black et al., 2023). Though this approach shares goals similar to those of DI++ and RLHF for LLMs, the problems and challenges are essentially different. Our Diff-Instruct++ is the first work to study the alignment of one-step generator models, instead of diffusion models. Besides, backpropagating through the multi-step diffusion generation output is expensive and hard to scale. 3) the third line, such as Diffusion-DPO (Wallace et al., 2024), Diffusion-KTO (Yang et al., 2024), tries to directly improve the diffusion model's human preference property with raw collected data instead of reward functions."}, {"title": "Experiments", "content": "In previous sections, we have established the theoretical foundations for DI++. In this section, we consider one-step text-to-image models with two kinds of neural network architectures: the UNet-based one-step text-to-image model with the well-known Stable Diffusion 1.5 (SD1.5)(Rombach et al., 2022) as the reference diffusion model, and the diffusion-transformer-based one-step model with the PixelArt-a as the reference"}, {"title": "The Experiment Setup", "content": "As we have shown in Figure 2, our experiment workflow consists of three modeling stages: the pre-training stage, the reward modeling stage (not necessary), and the alignment stage."}, {"title": "Quantitative Evaluations with Standard Scores", "content": "In this section, we quantitatively evaluate one-step models aligned with DI++ together with other text-to-image generative models."}, {"title": "Qualitative Evaluations", "content": "In this section, we evaluate all one-step text-to-image generator models, showing that the DI++-aligned model shows improved human preference performances. Before the quantitative evaluations, we first give a qualitative comparison of the models with and without alignment."}, {"title": "Limitations of Diff-Instruct++", "content": "Despite the alignment training stage, the generator model still makes simple mistakes occasionally. Figure 5 shows some bad generation cases that we picked with multiple generation trails. (1) The Aligned Model Still Misunderstands the Input Prompt. As the leftmost three images of Figure 5 show, the generated images ignore the concept of pear earrings, the battling in a coffee cup, and the car playing football. However, we find such a mistake happens only occasionally. (2) The Generator Sometimes Generates Bad Human Faces and Hands. Please see the fourth and fifth images of Figure 5. The face of the generated lady in the fourth image is not satisfying with blurred eyes and mouth. In the fifth image, the generated Iron Man character has multiple hands. (3) Sometimes The aligned model Still Can not Count Correctly. For instance, in the rightmost image, the prompt asks the model to generate a birthday cake, however, the model generates two cakes with one near and another lying far away."}, {"title": "Conclusion and Future Works", "content": "In this paper, we have presented the Diff-Instruct++ method, the first attempt to align one-step text-to-image generator models with human preference. By formulating the problem as a maximization of expected human reward functions with an IKL divergence regularization, we have developed practical loss functions and a fast-converging yet image data-free alignment algorithm. We also establish theoretical connections of Diff-Instruct++ with previous methods, pointing out that the commonly used classifier-free guidance is secretly doing Diff-Instruct++. Besides, we also introduce a three-stage workflow to develop one-step text-to-image generator models: the pre-training, the reward modeling, and the alignment stage. We train one-step generator models with different alignment configurations and demonstrate the superior advantage of Diff-Instruct++ with a human reward that improves the sample quality and better prompt alignment.\nWhile Diff-Instruct++ does not completely eliminate the occurrence of simple mistakes in image generation, our findings strongly suggest that this approach represents a promising direction for aligning one-step"}, {"title": "Broader Impact Statement", "content": "This work is motivated by our aim to increase the positive impact of one-step text-to-image generative models by training them to follow human preferences. By default, one-step generators are either trained over large-scale image-caption pair datasets or distilled from pre-trained diffusion models, which convey only subjective knowledge without human instructions.\nOur results indicate that the proposed approach is promising for making one-step generative models more aesthetic, and more preferred by human users. In the longer term, alignment failures could lead to more severe consequences, particularly if these models are deployed in safety-critical situations. For instance, if alignment failures occur, the one-step text-to-image model may generate toxic images with misleading information, and horrible images that can potentially be scary to users. We strongly recommend using our human preference alignment techniques together with AI safety checkers for text-to-image generation to prevent undesirable negative impacts."}, {"title": "Important Materials for Main Content", "content": null}, {"title": "Meanings of Hyper-parameters.", "content": "Meanings of Hyper-parameters. As we can see in Algorithm 1 (as well as Algorithm 2). Each hyperparameter has its intuitive meaning. The reward scale parameter $a_{rew}$ controls the strength of human preference alignment. The larger the $a_{rew}$ is, the stronger the generator is aligned with human preferences. However, the drawback for a too large $a_{rew}$ might be the loss of diversity and reality. Besides, we empirically find that larger $a_{rew}$ leads to richer generation details and better generation layouts. The CFG scale controls the strength of CFG when computing score functions for the reference diffusion model. As we have shown in Theorem 3.4, the $a_{cfg}$ represents the strength of the implicit reward function (3.5). We empirically find that the best CFG scale for Diff-Instruct++ is the same as the best CFG scale for sampling from the reference diffusion model. The diffusion model weighting $\\lambda(t)$ and the generator loss weighting $w(t)$ controls the strengths put on each time level of updating TA diffusion and the student generator. We empirically find that it is decent to set $\\lambda(t)$ to be the same as the default training weighting function for the reference diffusion. And it is decent to set the $w(t) = 1$ for all time levels in practice. In the following section, we give more discussions on Diff-Instruct++."}, {"title": "Experiment Details for Pre-training and Alignment", "content": "Experiment Details for Pre-training and Alignment We follow the setting of Diff-Instruct (Luo et al., 2024a) to use the same neural network architecture as the reference diffusion model for the one-step generator. The PixelArt-a model is trained using so-called VP diffusion(Song et al., 2020), which first scales the data in the latent space, then adds noise to the scaled latent data. We reformulate the VP diffusion as the form of so-called data-prediction proposed in EDM paper (Karras et al., 2022) by re-scaling the noisy data with the inverse of the scale that has been applied to data with VP diffusion. Under the data-prediction formulation, we select a fixed noise $\\sigma_{init}$ level to be $\\sigma_{init}$ = 2.5 following the Diff-Instruct and SiD (Zhou et al., 2024b). For generation, we first generate a Gaussian vector $z \\sim p_z = N(0, \\sigma_{init}I)$. Then we input z into the generator to generate the latent. The latent vector can then be decoded by the VAE decoder to turn into an image if needed.\nThe Training Setup and Costs. We train the model with the PyTorch framework. In the pre-training stage, we use the official checkpoint of off-the-shelf PixelArt-a-512\u00d7512 model as weights of the reference diffusion. We initialize the TA diffusion model with the same weights as the reference diffusion model. We use Diff-Instruct to pre-train the generator. We use the Adam optimizer for both TA diffusion and generation at all stages. For the reference diffusion model, we use a fixed classifier-free guidance scale of 4.5, while for TA diffusion, we do not use classifier-free guidance (i.e., the CFG scale is set to 1.0). We set the Adam optimizer's beta parameters to be $\\beta_1 = 0.0$ and $\\beta_2 = 0.999$ for both the pre-training and alignment"}, {"title": "More Discussions on Experiment Results", "content": "Low Alignment Costs. Besides the top performance, the training cost with DI++ is surprisingly cheap. Our best model is pre-trained with 4 A100-80G GPUs for 2 days and aligned using the same computation costs. while other industry models in Table 2 require hundreds of A100 GPU days. We summarize the distillation costs in Table 2, marking that DI++ is an efficient yet powerful alignment method with astonishing scaling ability. We believe such efficiency comes from the image-data-free property of DI++. The DI++ does not require image data when aligning, this distinguishes the DI++ from other methods that fine-tune models on highly curated image datasets, which potentially is inefficient."}, {"title": "Pytorch style pseudo-code of Score Implicit Matching", "content": "In this section, we give a PyTorch style pseudo-code for algorithm 3."}, {"title": "Prompts", "content": null}, {"title": "Prompts for Figure 1", "content": "The prompts are listed from the first row to the second row; from left to right.\n\u2022\tA small cactus with a happy face in the Sahara desert.\n\u2022\tA dog that has been meditating all the time.\n\u2022\tA alpaca made of colorful building blocks, cyberpunk.\n\u2022\tA dog is reading a thick book.\n\u2022\tA delicate apple(universe of stars inside the apple) made of opal hung on a branch in the early morning light, adorned with glistening dewdrops. in the background beautiful valleys, divine iridescent glowing, opalescent textures, volumetric light, ethereal, sparkling, light inside body, bioluminescence, studio photo, highly detailed, sharp focus, photorealism, photorealism, 8k, best quality, ultra detail, hyper detail, hdr, hyper detail.\n\u2022\tDrone view of waves crashing against the rugged cliffs along Big Sur's Garay Point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff's edge. The steep drop from the road down to the beach is a dramatic feat, with the cliff's edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway.\n\u2022\tImage of a jade green and gold coloured Faberg\u00e9 egg, 16k resolution, highly detailed, product photography, trending on artstation, sharp focus, studio photo, intricate details, fairly dark background, perfect lighting, perfect composition, sharp features, Miki Asai Macro photography, close-up, hyper detailed, trending on artstation, sharp focus, studio photo, intricate details, highly detailed, by greg rutkowski."}, {"title": "Prompts of Figure 3", "content": "Prompts of Figure 3, from left to right:\n\u2022\tA small cactus with a happy face in the Sahara desert;\n\u2022\tA delicate apple(universe of stars inside the apple) made of opal hung on a branch in the early morning light, adorned with glistening dewdrops. in the background beautiful valleys, divine iridescent glowing, opalescent textures, volumetric light, ethereal, sparkling, light inside body, bioluminescence, studio photo, highly detailed, sharp focus, photorealism, photorealism, 8k, best quality, ultra detail, hyper detail, hdr, hyper detail;\n\u2022\tDrone view of waves crashing against the rugged cliffs along Big Sur's Garay Point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff's edge. The steep drop from the road down to the beach is a dramatic feat, with the cliff's edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway;\n\u2022\tAstronaut in a jungle, cold color palette, muted colors, detailed, 8k;\n\u2022\tA parrot with a pearl earring, Vermeer style;"}, {"title": "Prompts for Figure 3", "content": "prompt for first row of Figure 3: A small cactus with a happy face in the Sahara desert.\n\u2022\tprompt for second row of Figure 3: An image of a jade green and gold coloured Faberg\u00e9 egg, 16k resolution, highly detailed, product photography, trending on artstation, sharp focus, studio photo, intricate details, fairly dark background, perfect lighting, perfect composition, sharp features, Miki Asai Macro photography, close-up, hyper detailed, trending on artstation, sharp focus, studio photo, intricate details, highly detailed, by greg rutkowski.\n\u2022\tprompt for third row of Figure 3: Baby playing with toys in the snow."}, {"title": "Prompts for Figure 4", "content": "The answer for the left three columns:\n\u2022\tthe first row from left to right is the one-step model (4.5 CFG and 1.0 reward); the PixelArt-a diffusion with 30 generation steps; the one-step model (4.5 CFG and 10.0 reward);\n\u2022\tthe PixelArt-a diffusion with 30 generation steps; the PixelArt-a diffusion with 30 generation steps; the one-step model (4.5 CFG and 10.0 reward); the first row from left to right is the one-step model (4.5 CFG and 1.0 reward);\n\u2022\tthe PixelArt-a diffusion with 30 generation steps; the first row from left to right is the one-step model (4.5 CFG and 1.0 reward); the first row from left to right is the one-step model (4.5 CFG and 10.0 reward);\nThe prompts from up to down are:\n\u2022\tA dog that has been meditating all the time;"}]}