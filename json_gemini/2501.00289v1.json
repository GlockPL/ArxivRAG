{"title": "Dual Diffusion for Unified Image Generation and Understanding", "authors": ["Zijie Li", "Henry Li", "Yichun Shi", "Amir Barati Farimani", "Yuval Kluger", "Linjie Yang", "Peng Wang"], "abstract": "Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.", "sections": [{"title": "1. Introduction", "content": "We are currently in the midst of a multimodal generative modeling revolution. Large scale diffusion models such as Stable Diffusion [19], Dall-E [62], FLUX, and Imagen [64] have become indisputable industry leaders for generating high fidelity images from text descriptions, enabling the accurate modeling and sampling of complex and high dimensional distributions of images given text. Conversely, autoregressive next-token prediction models have achieved groundbreaking performance both in pure text generation and reasoning such as in ChatGPT [1], Gemini [73], and Llama [17] and in visually-grounded text generation with large language models (LLMs), as seen with LLaVA [47] or BLIP-2 [39].\nGiven these developments, a natural question comes to mind: Can these existing image-to-text (I2T) or text-to-image (T21) systems be modified to reason with and generate data in the reverse direction? A positive answer would suggest the possibility of producing a fully multi-modal model that is able to understand and sample from conditional distributions between modalities in an omni-directional manner. Moreover, unifying these generative frameworks under a single model with shared parameters can confer a multitude of downstream benefits including improved reasoning, simplified implementation, and may be a natural next step towards artificial general intelligence [31, 75].\nWith autoregressive next-token prediction models, this query has already been answered resoundingly in the affirmative, as evidenced by a multitude of studies [16, 22, 24, 70, 72, 78, 83] demonstrating T2I capabilities of finetuned LLMs. This is in part due to the known next-token generative capability of autoregressive models with visual tokens [40, 74, 82].\nOn the contrary, with diffusion models there has been surprisingly little evidence of a similar reverse capacity. Until recently, generative diffusion models have struggled with language modeling due to the lack of an empirically performant discrete diffusion process on text tokens, in spite of continued research in this area [4, 15, 42]. At present, multimodal diffusion models either exhibit limited text reasoning capabilities and partial text diffusion [7, 81], which require an autoregressive model such as GPT2 [59] to decode denoised text latents, or emerge as add-ons to pretrained LLMs fine-tuned in conjunction with a diffusion loss [80, 85], and ultimately still rely entirely on next-token prediction for text generation.\nWe leverage the novel progress in this domain to revisit the above-mentioned question and propose a dual-branch diffusion model based on the multimodal diffusion transformer (MM-DiT) architecture [19], which we modify to"}, {"title": "2. Background", "content": "In this section, we review the basic concepts that underpin our proposed model. Generally, diffusion models [29, 68] are inspired by non-equilibrium thermodynamics [67] designed to evaluate a likelihood $p_e(x) = \\int p_e(x_{0:T})dx_{1:T}$ where data $x_0 := x$ are related to a set of latent variables $x_{1:T}$ by a diffusion process that gradually corrupts the original data."}, {"title": "2.1. Continuous Diffusion", "content": "Continuous diffusion models operate on continuous vectors by learning to reverse the noise-corruption forward process\n$x_t = \\alpha_t x + \\sigma_t \\epsilon,$\nparameterized by time-dependent scalar $\\alpha_t$ and $\\sigma_t$, where $\\alpha_t, \\sigma_t > 0$, $\\alpha_t / \\sigma_t$ decreases monotonically, and $\\epsilon$ is an appropriately selected i.i.d. noise variable. In score-based diffusion models [29, 68], $\\alpha_t, \\sigma_t$ are determined by a forward stochastic differential equation (SDE) that pushes $x_t$ towards $\\mathcal{N}(0, I)$ as $t \\rightarrow \\infty$. New samples can be generated by learning the reverse process through estimating the score function [3, 68, 77] $\\nabla_{x_t} \\log p_t(x_t)$. Alternatively, from (1), the following ordinary differential equation (ODE) can be derived:\n$\\dot{x_t} = v(x_t, t),$\nwhere velocity field $v(x_t, t) = \\dot{x_t} + \\sigma_t \\epsilon$. The ODE in (2) pushes the distribution of $x_t$ from $p_0$ to $p_T$. To generate new samples, we can use neural networks to approximate the velocity field $v$ and then integrate ODE (2) backward in time starting from $x_T \\sim \\mathcal{N}(0, I)$. A common choice of $\\alpha_t, \\sigma_t$ in flow matching model is $\\alpha_t = 1 - t, \\sigma = t$ and therefore $v = \\epsilon - x$, which corresponds to the optimal transport interpolant between two distribution $p_0$ and $p_1$ [45, 50]. The neural network for regressing the velocity field $v$ in (2) is trained by optimizing the flow matching loss\n$\\mathcal{L}_{FM} = \\mathbb{E}_{t,q(x_t|x)} || V_{\\theta}(x_t, t) - (\\epsilon - x) ||^2.$\nRecent work such as Stable Diffusion 3 [19] has demonstrated the superiority of flow matching model on text-to-image generation, thus in this work we adopt flow matching objective for modeling the distribution of images."}, {"title": "2.2. Discrete Diffusion", "content": "In discrete diffusion, the variate $x \\in \\mathcal{X} \\times \\cdots \\times \\mathcal{X}$ has finite support over the product space of $\\mathcal{X} = \\{1, ..., N\\}$, where in language models $N$ is the vocabulary size of the token embedding. Generally, there are two ways to approach this modeling task. The first line of works [12, 15, 27, 41, 52] apply a continuous relaxation to the discrete variable and proceed with a continuous reformulation of the framework, allowing the application of the equations in Section 2.1. This greatly simplifies the diffusion modeling itself, but introduces a significant source of error in the mapping between discrete and relaxed continuous states. Conversely, the diffusion process is extended to the discrete token space [4, 51, 54, 65], which removes the need for the aforementioned mapping via a specialized discrete diffusion formulation. In our work, we will focus on this latter perspective given its empirical potential as validated in recent work"}, {"title": "3. Method", "content": "We propose an end-to-end multi-modal diffusion model named Dual Diffusion Transformer (D-DiT) with a unified backbone that jointly models image and text distribution. More specifically, given image $x^{(img)}$ and text $x^{(txt)}$, we are interested at modeling the conditional distribution $p(x^{(img)}|x^{(txt)})$ and $p(x^{(txt)}|x^{(img)})$. The former is usually referred to as text-to-image generation and the latter is closely related to various image understanding tasks such as captioning and visual question answering."}, {"title": "3.1. Architecture", "content": "Inspired by the MM-DiT in SD3 [19], our proposed D-DiT is a Transformer-based model comprising two branches one for processing image tokens and another for processing text tokens. The image and text tokens attend with each other in every attention layer. In D-DiT, the output of the image branch is the prediction of velocity defined in (2) with text conditioning, while the output for the text branch is the $x^{(txt)}$ prediction with image conditioning. The scalar timestep embedding modulates every layer's feature map via AdaLN (adaptive layernorm) [56]. We only input the timestep $t$ to the model during image generation, as $x^{(txt)}$ has already implicitly carried the information of signal-to-noise ratio (the number of mask tokens in the sequence). In addition, we add a text encoder with bi-directional attention on top of the text branch of the diffusion model. Despite an asymmetric design of the model is not strictly required, having a text encoder on top of a DiT model allows us to easily adapt many existing text-to-to-image model such as SD3 and FLUX as pretrained backone for our D-DiT model. Note that the text encoder should not have causal mask as this will violate the masked diffusion process.\nTo reduce the computational cost associated with high-resolution images, we follow prior works on latent-space (image) diffusion [63] which compresses images from raw pixel space to a spatially compressed latent space obtained from a variational-autoencoder (VAE) trained with discriminator loss [18] and KL-divergence regularization [34]."}, {"title": "3.2. Training", "content": "We propose a joint training objective for image-text joint modeling, which is essentially a joint denoising target that combines continuous and discrete diffusion. Formally, we use flow matching introduced in Section 2.1 to learn the conditional distribution of images and masked diffusion introduced in Section 2.2 to learn the conditional distribution of texts. During training, corrupted samples $x_t^{(img)}, x_t^{(txt)}$ are sampled from the corresponding forward corruption processes $q(x_t|x)$ defined in (1) and (6) respectively and then we can calculate the diffusion loss for each modality\n$\\begin{aligned}\n\\mathcal{L}_{image} &= \\mathbb{E}_{t,q(x^{(img)}|x^{(txt)})} || V_{\\theta}(x^{(img)}, t, x^{(txt)}) - (\\epsilon - x^{(img)} ||^2, \\\\\n\\mathcal{L}_{text} &= \\mathbb{E}_{q(x^{(txt)})} [\\frac{1}{K} \\sum_{i=1}^{K}  \\log[x_{\\theta}(x_t^{(txt)}, x^{(img)})\\cdot x]/t_i ],\n\\end{aligned}$\nwhere we use antithetic sampling for text diffusion timesteps $t_i$ by discretizing $(\\delta, 1]$ into $K$ points uniformly with $\\delta$ being a small number to avoid numerical instability,"}, {"title": "3.3. Inference", "content": "We introduce three types of sampling-based inference which can be used for different vision-language tasks, which we detail below.\nText-to-image Generation To perform text-guided image generation, i.e. $x \\sim p(x^{(img)}|x^{(txt)})$, we use the commonly adopted classifier-free guidance (CFG) technique [28] to sample from the conditional distribution $p(x^{(img)}|x^{(txt)})$, which amounts to a re-weighting of the velocity prediction\n$V_t = s V_{\\theta} (x^{(img)}, t, x^{(txt)} ))+(1-s)V_{\\theta} (x^{(img)}, t,\\emptyset),$\nwhere $s$ is a hyperparameter that controls the scale of guidance and $\\emptyset$ is a suitable null embedding (e.g. the embedding of an empty text) .\nImage-to-text Generation To sample images from the conditional distribution, we can use ancestral sampling to draw from the posterior distribution $q(x_s|x_t, x)$ in (5) by plugging in prediction $x \\approx x_{\\theta} (x^{(txt)}, x^{(img)}; t = 0)$.\nImage-to-text In-filling In certain tasks, both text conditioning information and image conditioning information are available, such as in a visual question answering"}, {"title": "4. Experiments", "content": "Implementation details We implement our proposed framework based on the open-sourced SD3-medium model [19]. We initialize the model weight of the DiT from the pretrained checkpoint and add a linear head on top of the text branch to do text denoising. Following SD3, we adopt the existing T5 encoder/tokenizer [61], and SD3's image VAE, whose weights remain unchanged throughout all"}, {"title": "4.2. Multi-modal Understanding", "content": "Existing multi-modal diffusion models such as UniDiffuser [7] and Versatile Diffusion [81] performed text diffusion in a CLIP latent space, which hampered their ability to perform text completion, a necessary feature for general question answering and conversation-based tasks. This is no longer a limitation with our proposed D-DiT due to its discrete masked diffusion branch, allowing us to leave question tokens unmasked throughout sampling. We are thus able to evaluate our fine-tuned model on a full suite of image-to-text generation tasks, including image captioning and visual question answering benchmarks, as well as long-form visual assistance responses.\nWe first evaluate the visual understanding capabilities of D-DiT via the academic question answering benchmarks VQAv2 [26], VizWiz [8], OKVQA [53], GQA [30], POPE [44], as well as MME [20]. Due to the short-form nature of the questions, we perform sampling with 16 diffusion steps, and compare against a selection of multi-modal models, including I2T only and I2T + T2I models. Our results are summarized in Table 3. We note that our D-DiT as the only diffusion-only multi-modal model capable of visual question answering tasks, already boosts performance that is competitive with recent I2T + T2I models. Our model at 512 resolution outperforms Show-O on MME, GQA, and POPE, approaching performances of auto-regressive VLMs such as QWEN-VL and BLIP-2.\nNext, we provide qualitative examples of the D-DiT, providing images and gauging the model's visual language assistance capabilities via image-related queries. Given the longer format of the responses, we sample D-DiT responses with 256 diffusion steps. Our model provides answers to human queries in a manner that suggests a fine-grained multi-modal understanding of the image and text conditioning"}, {"title": "4.3. Text-to-image Generation", "content": "Besides the image-conditioned text generation, we also test model's text-to-image generation capability. Following previous works, we evaluate our 512x512 model after the second training stage on the GenEval benchmark, which measures model's prompt following capability [25]. We follow the default setting in the open-sourced SD3 checkpoint where we use a Euler solver with 28 sampling steps and a CFG scale of 7.0. We observe that the joint diffusion training does not cause catastrophic forgetting on the model, the fine-tuned D-DiT preserves the performance of original SD3 model and slightly improves on some of metrics such as colors after joint training. Qualitative evaluation samples are shown in Figure 3, where we observe the ability to generate highly aesthetic images is preserved."}, {"title": "4.4. Ablation Studies", "content": "As text-to-image diffusion models are trained on a large number of text-image pairs, one may raise the question, whether the representation learned throughout this process can be transferred to multi-modal understanding tasks? To answer this question, we perform an ablation study on the"}, {"title": "5. Related Works", "content": "Diffusion models [29, 67, 68] generate data by gradually converting noise into signal via a reverse diffusion process. They are the de facto standard for image generation [14, 32] and likelihood modeling [33, 38, 55, 69]. Conditional diffusion models [28] have also been shown to be powerful interfaces bridging text and images, particularly for their ability to generate highly realistic and aesthetic images from textual descriptions [10, 22, 57, 62-64]. Their exceptional performance in the image domain has also inspired numerous extensions to the language generation [12, 15, 23, 27, 41, 51, 52, 65, 66], and is an attractive alternative as its sampling is not constrained by a specified"}, {"title": "5.2. Vision Language Models", "content": "The success of large language models (LLMs) [9, 76] and vision-language pretraining [60] has given rise to a series of multi-modal language models. The visual signal is projected to the text embedding space via vision encoders supervised by text labels [60, 84] and then connected to a pretrained language models through further instruction tuning [2, 13, 47, 75, 86]. While these models have shown promising capabilities in image understanding and few-shot generalization, their predictive targets are inherently language-centric, limiting their ability to model the image distributions directly."}, {"title": "5.3. Multimodal Text and Image Generative Models", "content": "Rather than simply connecting visual encoders to language models, recently there has been an active line of inquiry focused on exploring a unified generative model for joint vision and language generation. Inspired by autoregressive language models, many of the unified multi-modal generative models extend the next-token prediction to both image and text tokens [16, 24, 70, 72, 78]. More recently, Transfusion [85] and Show-O [80] demonstrate that bi-directional image diffusion can be integrated with autoregressive text prediction in the same framework. On the other hand, Versatile Diffusion [81] and Uni-diffuser [7] explore applying a continuous diffusion process to text and image modalities, where text generation is broken into two stages - first, continuous diffusion is used to generate latent embeddings which are then decoded into text by another LLM (e.g. GPT2 [58]). While these works hint at the potential of diffusion models as efficient multi-modal models, their text generation capability is restricted to simple tasks like generating short captions from images."}, {"title": "Conclusion and Discussion", "content": "In this work, we introduced an end-to-end multi-modal diffusion model that bridges the gap between text and image diffusion by enabling both text-to-image (T2I) and image-to-text (I2T) tasks through a unified diffusion model. We demonstrated that a bi-directional transformer trained with a joint diffusion target is an effective multi-modal learner capable of competing with the autoregressive models that have long dominated the field. Additionally, the bi-directional attention mechanism is equivariant to the order of input tokens, enabling the prediction of conditional distributions without requiring a specific arrangement of different modalities or special handling of the attention mask."}]}