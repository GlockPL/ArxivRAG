{"title": "Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency", "authors": ["Michael Somma", "Thomas Gallien", "Branka Stojanovi\u0107"], "abstract": "Anomaly detection in complex dynamical systems is essential for ensuring reliability, safety, and efficiency in industrial and cyber-physical infrastructures. Predictive maintenance helps prevent costly failures, while cybersecurity monitoring has become critical as digitized systems face growing threats. Many of these systems exhibit oscillatory behaviors and bounded motion, requiring anomaly detection methods that capture structured temporal dependencies while adhering to physical consistency principles. In this work, we propose a system-theoretic approach to anomaly detection, grounded in classical embedding theory and physics-inspired consistency principles. We build upon the Fractal Whitney Embedding Prevalence Theorem, extending traditional embedding techniques to complex system dynamics. Additionally, we introduce state-derivative pairs as an embedding strategy to capture system evolution. To enforce temporal coherence, we develop a Temporal Differential Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the approximated derivatives of latent variables with their dynamic representations. We evaluate our method on the C-MAPSS dataset, a benchmark for turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers while achieving a 200x reduction in MAC operations, making it particularly suited for lightweight edge computing. Our findings support the hypothesis that anomalies disrupt stable system dynamics, providing a robust, interpretable signal for anomaly detection.", "sections": [{"title": "I. INTRODUCTION", "content": "Anomaly detection in complex physical dynamical systems is a critical research area as industrial and engineered systems become more sophisticated. Identifying deviations from expected behavior is essential for ensuring reliability, safety, and efficiency. This is particularly relevant in predictive maintenance and cybersecurity monitoring. In industrial systems and rotating machinery, early fault detection helps prevent costly failures and downtime [1]. Meanwhile, as critical infrastructures such as power grids and water distribution systems become increasingly digitized, the risk of cyber threats has grown significantly [2], [3]. Recent attacks [4], [5] on cyber-physical systems highlight the need for robust monitoring techniques to detect both malicious intrusions and system failures. Ensuring the security and stability of these dynamical systems requires adaptive anomaly detection methods capable of addressing evolving threats and operational challenges.\nMany complex dynamical systems exhibit oscillatory behaviors and bounded motion, fundamental characteristics of both natural and engineered processes. The study of such systems intersects with two key fields: time-series modeling and the incorporation of physical laws. Given that many physical systems display structured temporal dependencies, effective modeling requires methods that capture correlations across time. Simultaneously, classical physics, which governs tangible objects and engineered systems, is largely defined by causal and deterministic principles, often described through differential equations and conservation laws. This study aims to bridge time-series modeling with physics-inspired approaches to develop more effective and sustainable anomaly detection methods."}, {"title": "II. RELATED WORK", "content": "Models like LSTMs and Transformers with attention mechanisms have been successfully applied to time-series anomaly detection in fields such as mechanical enginered system, aerospace, and industrial monitoring [6], [7], [8]. While these methods effectively capture long-range dependencies and irregular temporal patterns, they are computationally expensive. For example, common benchmark datasets require time steps ranging from 30 to 500 with hidden dimensions between 20 and 50, illustrating the complexity of modeling system dynamics efficiently [9]. Their high memory requirements often exceed the constraints of typical MCU-level devices, making real-time deployment infeasible For example, a typical LSTM with 500 time steps and 50 hidden units already surpasses the available memory of common low-power devices, limiting its practical applicability. Additionally, the sequential nature of RNNs restricts parallelization, further increasing computational cost [10]. These challenges highlight the need for alternative approaches that balance computational efficiency with robust anomaly detection, aligning with broader goals of sustainability and practical deployability.\nIn many applications, incorporating domain knowledge can help reduce computational demands. Physics-informed neural networks (PINNs) embed physical laws directly into neural networks, enabling them to leverage known system dynamics [11], [12], [13]. However, applying PINNs to complex dynamical physical systems remains challenging. These systems involve numerous interdependent physical principles, and explicitly modeling them within a neural network would require extensive computational resources and domain expertise, making large-scale applications impractical [14], [15].\nWe use the C-MAPSS dataset as a benchmark for studying complex dynamical systems due to its realistic representation of turbofan aeroengine degradation. Aeroengines are complex dynamical systems governed by nonlinear, time-dependent interactions of physical processes, making them an ideal test case for evaluating anomaly detection methods in real-world settings.\nIn the literature, anomaly detection for the C-MAPSS dataset generally follows two main approaches. The first focuses on fleet-level anomaly detection, where engines with shorter lifetimes are classified as abnormal based on their total life cycles [16], [17]. This method aims to distinguish early failures from normal operating conditions at a system-wide level.\nThe second approach considers individual engine degradation, defining anomalies based on a 60/40 time-based split [18], [19]. In this setup, the first 60% of an engine's life is labeled as normal, while the final 40% is considered abnormal. The dataset is then divided into train/test subsets, and models are evaluated based on their ability to classify each time step accordingly.\nSince we aim to develop methods that capture system dynamics, we consider the second approach a more suitable benchmark, as it focuses on time-dependent degradation rather than static fleet-level classification.\nPrevious work on anomaly detection in this setting has explored various deep learning models. One study employed an LSTM-based approach [19], leveraging recurrent structures to model time dependencies. Another approach used a standard autoencoder (AE) without explicit temporal modeling [18]. More recent research has investigated Transformer-based models, which excel at capturing long-range dependencies but introduce high computational costs and require extensive training data due to their large parameter space [20], [7]. Moreover, both Transformer studies formulated the problem as a multiclass classification task, with one using a slightly different dataset and the other applying the method directly to C-MAPSS. However, defining well-separated fault categories in a complex dynamical system is challenging in practice, as non-trivial interactions between multiple physical components create highly unpredictable behaviors. As system dynamics grow more intricate, these interactions become even less predictable, further complicating precise fault categorization.\nInstead of relying on overly complex models that attempt to directly differentiate between normal and abnormal behavior, we propose a system-theoretic approach inspired by classical embedding theory to study the dimensionality of the system's latent representation. Our approach introduces physics-inspired consistency principles that approximate the underlying causal mechanisms governing the system dynamics, without explicitly enforcing physical laws. We hypothesize that complex systems in a stable regime exhibit predictable behavior, allowing for well-approximated lower-dimensional embeddings. In contrast, anomalies introduce additional complexity, disrupting these stable relationships. We aim to leverage this property to detect anomalous states of the system when the learned embedding no longer adequately captures the system dynamics."}, {"title": "III. THEORETICAL FOUNDATION & METHODS", "content": "In this work, we investigate the mathematical foundations of embedding a bounded dynamical system, such as a rotary machine, into a latent space using autoencoders. Given a system governed by well-defined dynamical laws and a set of measurement values recorded as a time series, our objective is to establish conditions under which the system's dynamics can be reliably mapped to the latent space. By leveraging the learned manifold structure, we aim to detect deviations from the normal operating state within the latent space and subsequently infer corresponding deviations in the measurement domain and, ultimately, in the physical system. This formulation aims to provide a rigorous framework for anomaly detection and system health monitoring based on latent space representations.\nA general oscillatory dynamical system is governed by a set of differential equations describing periodic motion. In its most general form, the system evolves according to\n$\\dot{x} = f(x, t),$ (1)\nwhere $x \\in \\mathbb{R}^n$ represents the state variables, and $f(x,t)$ defines the governing dynamics. A common class of oscillatory systems follows a second-order differential equation of the form\n$\\ddot{\\theta} + g(\\theta,\\dot{\\theta}) + h(\\theta, t) = 0,$ (2)\nwhere $g(\\theta,\\dot{\\theta})$ represents dissipative forces, such as damping, and $h(\\theta,t)$ accounts for external periodic forcing. A widely studied case is the damped driven oscillator, as shown in Fig. 1, which satisfies the equation\n$\\ddot{\\theta} + \\gamma \\dot{\\theta} + \\omega_0^2 \\sin \\theta = A \\cos(\\omega_{drive} t),$ (3)\nwhere $\\theta$ denotes the angular displacement and $\\dot{\\theta}$ its corresponding angular velocity. The parameters $\\gamma$ and $\\omega_0$ represent the damping coefficient and the natural frequency of the system, respectively, while $A$ and $\\omega_{drive}$ define the amplitude and frequency of an external periodic driving force [21].\nTo analyze deviations from the nominal oscillatory state, we introduce a slow drift in both the angular displacement and velocity components by modifying the evolution equations as\n$\\theta \\leftarrow \\theta + \\alpha t, \\quad \\dot{\\theta} \\leftarrow \\dot{\\theta} + \\beta t.$ (4)\nFor illustration, we consider a system with parameters $\\gamma = 0.2$, $\\omega_0 = 1.0$, $A = 0.8$, $\\omega_{drive} = 1.2$, and drift rates $\\alpha = 0.005$ and $\\beta = 0.002$. Under these conditions, the system initially remains within a bounded oscillatory regime but gradually exhibits deviations due to the drift terms, leading to an eventual departure from the confined state space. The effect of this perturbation is visualized in Figure 2, where the phase space trajectory initially remains within a bounded region but progressively transitions into an unbounded state.\nThis formulation illustrates that the state variables of an oscillatory system, the angular displacement $\\theta$ and its derivative $\\dot{\\theta}$, define a phase space in which the system's behavior can be analyzed. In this phase space, it is possible to identify bounded regions corresponding to normal oscillations, where the system remains confined to a predictable, periodic trajectory. When the system leaves this bounded region, it indicates the onset of unbounded oscillations or deviations from normal behavior.\nThe key question is whether this concept extends to complex dynamical systems and what constraints must be addressed for a successful transition. Specifically, we seek the mathematical and structural conditions for mapping system dynamics to a reduced representation while preserving phase space properties.\nHigh-dimensional, nonlinear systems introduce challenges such as maintaining topological consistency and preserving dynamical coherence. The goal is to identify the fundamental conditions and assumptions governing this embedding.\nWe systematically examine the mathematical foundation and computational requirements for such mappings, informing the design of effective latent representations in an autoencoder architecture for detecting deviations in high-dimensional systems\nWe begin by formalizing the mathematical foundation for embedding high-dimensional dynamical systems into a reduced representation. Consider a complex bounded dynamical system with state variables $x \\in \\mathbb{R}^n$, where $n$ represents the dimension of the physical system and is typically much larger than the number of available measurements ($n \\gg k$)."}, {"title": null, "content": "At the measurement level, we access observables through a measurement function:\n$\\mu : \\mathbb{R}^{\\tilde{n}} \\to \\mathbb{R}^k,$ (5)\nwhere $\\mu$ extracts a lower-dimensional set of measurements $y = \\mu(x)$, with $y \\in \\mathbb{R}^k$, forming the input to the autoencoder. The encoding level is defined by:\n$\\mathcal{E} : \\mathbb{R}^k \\to \\mathbb{R}^n,$ (6)\nwhere $\\mathcal{E}$ maps the measurement space to an embedding space of dimension $n$.\nIn the normal state, system trajectories should be mapped onto a compact manifold in the latent space. The goal is to leverage the embedded representation so that a deviation in the latent space reliably indicates a departure from the stable state at the physical level. We first analyze the latent space encoding of a dynamical system from a system theory perspective. While system dynamics in a stable regime often evolve on a structured manifold, measurements are not necessarily taken from a smooth manifold. These manifolds, in the mathematical sense, locally resemble Euclidean space $\\mathbb{R}^n$ and allow for the application of calculus [22]. However, real-world measurements may be noisy or lie on fractal-like sets, requiring a more general mathematical framework.\nTherefore, we turn to the Fractal Whitney Embedding Prevalence Theorem, which extends the classical Whitney and Takens embedding results to settings where the underlying structure is not a smooth manifold. A key aspect of this generalization is the notion of dimensionality for compact subsets, which leads to the definition of box-counting dimension.\nFor a positive number $\\epsilon$, let $A_{\\epsilon}$ be the set of all points within $\\epsilon$ of $A$, i.e.,\n$A_{\\epsilon} = \\{ x \\in \\mathbb{R}^n : ||x - a|| \\le \\epsilon \\text{ for some } a \\in A \\}.$Let $vol(A_{\\epsilon})$ denote the $n$-dimensional outer volume of $A_{\\epsilon}$. The box-counting dimension of $A$ is then defined as\n$\\text{boxdim}(A) = n - \\lim_{\\epsilon \\to 0} \\frac{\\log \\text{vol}(A_{\\epsilon})}{\\log \\epsilon}.$Equipped with this notion of dimensionality, we now state the embedding theorem that generalizes classical results to fractal structures.\nLet $A$ be a compact subset of $\\mathbb{R}^k$ with box-counting dimension $d$, and let $n$ be an integer greater than $2d$. For almost every smooth map $F : \\mathbb{R}^k \\to \\mathbb{R}^n$,\nIf the measurement function is well-behaved, we can approximate the box-counting dimension using the dimension of bounded physical quantities that govern the real system dynamics in a stable regime. This allows us to estimate the required embedding dimension $n$ to achieve a faithful description of the system's dynamics.\nAfter analyzing the necessary embedding dimension, we now turn to the type of embedding that best captures the dynamics of the system. We draw inspiration from simple dynamical systems and propose using state-derivative pairs as an embedding strategy. This choice is motivated by two key principles. Physical laws typically encode causal relationships between a system's state and its derivatives, making this a natural way to describe a dynamical process. When chosen properly, state-derivative pairs help reduce redundancy by maintaining linear independence between embedding dimensions.\nWe hypothesize that leveraging this type of embedding allows for two key insights:\nDeviations beyond a certain region associated with normal variability indicate anomalous behavior, as suggested by phase space considerations.\nAs the dynamics become more complex, this leads to an increase in the box-counting dimension. By Th. 1, this increase suggests that the chosen embedding dimension is insufficient to fully describe the system. Consequently, we hypothesize that this insufficiency results in a collapse of the dynamical description, leading to the loss of the causal approximation between the state and its derivative.\nTo effectively utilize this type of embedding, we require more than just a one-to-one mapping-we also need consistent differential information. That is, small changes in the latent space should correspond to meaningful changes in the measurement space. A fundamental tool for analyzing smooth mappings between differentiable manifolds is the Jacobian matrix, which captures the local behavior of the map in terms of its partial derivatives. The immersion theorem ensures that local neighborhoods in the latent space retain the differential structure of the original system. This guarantees that the mapping not only remains locally invertible but also that variations in the system state are meaningfully preserved in the latent representation.\nSuppose $F:M \\to N$ is a smooth map and $p \\in M$. If $dF_p$ is injective, then there exists a neighborhood $U$ of $p$ such that the restriction $F|_U$ is an immersion.\nA sufficient condition for the immersion property is that the Jacobian of $F$ has full column rank, ensuring that $dF_p$ is injective [22]. By establishing that our encoder is a one-to-one map and an immersion within the considered subspace of the system's stable regime, we create a meaningful latent representation. From a theoretical perspective, this allows us to associate deviations in the latent space with deviations in the physical system.\nHaving established the theoretical framework, we now seek to apply it to a practical use case. This requires two key"}, {"title": null, "content": "components. First, we need a bounded dynamical system that operates in a stable regime with accessible measurement values. For this, we use the NASA C-MAPSS dataset, which provides time-series sensor data from aircraft engines\u2014a complex yet stable dynamical system suitable for our approach.\nSecond, we need to develop an algorithm that trains an autoencoder to learn embeddings aligned with our theoretical framework. This involves constructing a latent space representation based on state-derivative pairs while ensuring a one-to-one mapping and an immersion within the system's normal operating regime.\nIn the NASA C-MAPSS dataset, the measurement space has a dimension of $k = 24$, representing sensor readings from the turbine system. To determine a suitable embedding dimension consistent with Th. 1, we make the following assumptions about the system's essential dynamics.\nWe hypothesize that the turbine's core behavior is captured by three one-dimensional attractors, which could correspond to three state-derivative pairs, such as:\nGiven that the measurement function provides a structured representation of the underlying physical dynamics, we assume that it maps the real attractor dimension 3 to a box-counting dimension below 4, Based on Th. 1, this allows us to select an embedding dimension of $n = 8$, ensuring a one-to-one mapping.\nBased on our approach, we select four state-derivative pairs as the embedding. To ensure that this representation is at least an immersion in the subspace corresponding to the system's normal or stable regime, we apply a numerical method to verify whether the Jacobian has full rank in the normal state of the test data. While this does not constitute a strict mathematical proof, it serves as a reliable guideline for assessing the validity of the embedding.\nSingular Value Decomposition (SVD) is a fundamental matrix factorization technique that can be applied to any complex-valued matrix $X \\in \\mathbb{C}^{n\\times m}$. The decomposition expresses $X$ in the following form:\n$X = U \\Sigma V^*,$\nwhere:\n$U \\in \\mathbb{C}^{n \\times n}$ is a unitary matrix with orthonormal columns.\n$\\Sigma \\in \\mathbb{R}^{n \\times m}$ is a diagonal matrix with nonnegative real numbers on the diagonal and zeros elsewhere. These diagonal entries are known as the singular values of $X$.\n$V^* \\in \\mathbb{C}^{m \\times m}$ is the conjugate transpose of a unitary matrix $V \\in \\mathbb{C}^{m \\times m}$.\nThis decomposition exists for any complex matrix and provides significant insights into the structure of the matrix. The singular values in $\\Sigma$ are ordered such that:\n$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(n,m)} \\ge 0.$\nSVD is particularly powerful for analyzing the rank of a matrix. The rank of $X$ can be determined by counting the"}, {"title": null, "content": "number of non-zero singular values in $\\Sigma$. If all singular values are non-zero, the matrix is full rank [28].\nAs a foundational step, we require a method to approximate the first-time derivative. A widely used approach is the central difference method, which estimates the derivative of a function by computing the slope between two points symmetrically positioned around the point of interest. The central difference formula for the first derivative of a function $f(t)$ is:\n$\\dot{f}(t) \\approx \\frac{f(t + \\Delta t) - f(t - \\Delta t)}{2 \\Delta t},$where $\\Delta t$ represents a small step size. This method is often preferred over forward or backward differences due to its higher accuracy ($O(\\Delta t^2)$), meaning that the approximation error decreases quadratically as $\\Delta t$ decreases. Moreover, evaluating points symmetrically around the target reduces truncation errors and improves numerical stability, making the central difference method a reliable choice for derivative estimation [29].\nTo accurately capture temporal dynamics, we incorporate the central difference method into the training framework of the latent space. The latent representations at the previous (t-1) and next (t + 1) time steps are used to approximate the first-time derivative of the static latent variables (z) using the central difference formula. This derivative is then used as a target for the dynamic latent variables ($\\dot{z}$), ensuring consistency with the central difference approximation, scaled by the time interval $\\Delta t$. To incorporate temporal dynamics into the training process, we introduce a temporal differential consistency loss (TDC-Loss). This loss enforces consistency between the approximated time derivative of the static latent variables (z) and the output of the corresponding dynamic latent variables ($\\dot{z}$). During training, the central difference method is used to estimate the time derivative of z, which is then compared with $\\dot{z}$. The TDC-Loss is combined with the standard reconstruction loss, ensuring that the autoencoder learns a latent representation that captures both state and derivative information. A compact version of the pseudocode for TDC-informed training is presented in Algo. III-C3.\nWe employ an embedding based on state-derivative pairs, where the derivative is approximated using the central difference method. While numerical differentiation is generally ill-posed, particularly in the presence of noise [30], our approach does not seek a fully precise description of the system's dynamics. Instead, we aim for a causal approximation that reliably indicates when the system deviates from normal variability. By choosing a sufficiently large embedding dimension, we ensure that the system operates on a simple attractor geometry. This allows us to expect a nearly constant trendline for both the state and its derivative.\nTo evaluate this, we introduce two key performance indicators (KPIs).\nTo leverage the causal approximation between state and derivative, the variation of both quantities must remain in a comparable range. We quantify this using the ratio of their z-score normalized variances:\n$\\eta = \\frac{\\sigma^2_{\\mathcal{Z}}}{\\sigma^2_{\\dot{\\mathcal{Z}}}},$ (7)\nwhere $\\sigma^2$ represents the variance of the z-score normalized values. The interpretation is as follows:\nThe second KPI evaluates how well the approximated derivative maintains consistency with the state transitions. Specifically, we compute the mean squared error (MSE) between the integrated derivative and the actual state difference over small time intervals:\n$\\rho = \\frac{1}{N} \\sum_{i=1}^N \\sqrt{\\sum_{j=1}^{\\delta_i} (X_i - \\int_{t_i}^{t_i + j \\Delta t} \\dot{X}_i dt)^2},$ (8)\nThis metric does not require knowledge of the true derivative but serves as a self-consistency check. By integrating the derivative, short-term fluctuations are smoothed, making long-term trends more apparent. A significant increase in this error in anomalous conditions suggests a breakdown in the causal approximation, indicating that the embedding dimension is insufficient to describe the system dynamics adequately.\nWe aim to incorporate the developed mathematical foundations into anomaly detection logic. By leveraging state-derivative pairs in the latent space, which approximate the causal underlying process, we assume that normal states can be enclosed within a confined region. Deviations from this region indicate abnormal behavior in the physical system.\nTraining is conducted using the first 50% of the dataset, while validation is performed on data from 50% to 60%. A threshold for each latent node is determined based on validation data, set as a percentile of the validation split. This percentile value is optimized to maximize a classification performance metric, such as the F1-score. The threshold remains constant across all engines rather than being individually adapted. In testing, only the current time step is used, and the model infers the derivative representation from this single time step. This approach is motivated by the fact that the measurement at time t inherently contains values that correspond to rate changes, effectively carrying this derivative information.\nThe detection logic follows these principles:"}, {"title": null, "content": "IV. RESULTS"}, {"content": "First, we analyze the consistency of the proposed approach. Fig. 5 presents the evolution of the loss terms over 50 training epochs across five independent attempts. The plot shows both the standard Mean Squared Error (MSE) loss and the newly introduced TDC loss term. We observe consistent convergence in the TDC loss term, indicating stable training behavior.\nFurthermore, we conducted a Jacobian rank analysis using Singular Value Decomposition (SVD). Applying a zero-threshold of $\\epsilon = 10^{-9}$, all samples in the test dataset exhibit a full-rank Jacobian. This result suggests that the mapping $\\mathcal{E}$, which projects the measurement space $\\mathbb{R}^k$ to an embedding space $\\mathbb{R}^n$, behaves as an immersion, preserving the differential structure.\nNext, we evaluate two key performance indicators (KPIs) introduced in this study. The first KPI, denoted as $o$, assesses whether the central difference method provides an appropriate approximation of the first time derivative. This metric was computed individually for each engine in the training split (first 60% of the timesteps). The results indicate values close to 1, with deviations in the order of $10^{-7}$, confirming the validity of the approximation.\nThe second KPI evaluates the relationship between state derivatives, reflecting how well the causal approximation holds across latent nodes in both normal and abnormal conditions.\nThe results indicate that in the normal regime, the correlation values remain low with minimal variation. However, in the anomalous regime, we observe a slight increase in mean values and a broader standard deviation range, suggesting a stronger deviation in state-derivative relationships under anomaly conditions.\nFrom a phase-space perspective, the qualitative relationship between states remains intact. Anomalous states continue to map to regions outside the bounded domain of normal states, ensuring that the essential structural distinction is preserved, even if exact angles and scales are distorted. Our objective is to develop methods that integrate physics-inspired consistency principles while imposing only the necessary constraints. This approach maintains the model's flexibility and ability to learn effectively. Therefore, we do not enforce an explicit orthonormality condition in the AE design.\nLet's have a more detailed view on the phase space evolution. Fig. 6a illustrates a clear drift toward the anomalous region, highlighting a gradual deviation in system behavior. Fig. 6b presents the evolution of $p$, which quantifies the causal approximation of state-derivative pairs. In the normal range, $p$ remains stable on average, indicating a consistent dynamical description. However, as the system enters the anomalous regime, a significant increase in $p$ is observed, suggesting a loss in the dynamical consistency of the system. The theoretical derivation showed that when system dynamics deviate from a stable state, the increasing complexity requires a higher-dimensional embedding to adequately capture the system's behavior. We hypothesized that this effect would lead to the collapse of state-derivative approximations, serving as an indicator of anomalous behavior. Our observations now confirm this, as we see a loss of dynamical consistency in the latent space, reinforcing our hypothesis that anomalies emerge when the system transitions beyond the representational capacity of the learned embedding.\nIn Fig. 6c, we observe the evolution of the phase space representation over time, transitioning from normal to abnormal behavior. Initially, the phase space exhibits an attractor-like structure around which the dynamics evolve in a stable manner. As the system progresses, we observe increased dispersion and a gradual drift away from the attractor toward the anomalous region. Even while still within the range characterized as normal, the state-derivative approximation remains stable, as seen in Fig. 6a. However, subtle deviations from the attractor's dynamics can already be detected in the earliest phase of the transition. This aligns with our second hypothesized effect, which suggests that even minor disruptions in the system's stability could serve as an early indicator of anomalies.\nThese findings reinforce that our model is not merely a data-driven classifier for anomaly detection but instead captures meaningful insights into the underlying system dynamics.", "title": "C. Comparison with Literature Benchmarks"}, {"title": null, "content": "We have discussed the consistency of our method and analyzed how the TDC-AE aligns with the mathematical foundation developed in this paper. Now, we evaluate its detection performance in comparison to literature benchmarks.\nTab. II presents a comparison across different architectures. Our approach outperforms all reported models across all detection metrics, demonstrating consistent results across repeated attempts. The latent node threshold in all attempts was set using the 75th percentile of the validation set.\nEven complex models, such as LSTM-AEs, Transformer with attention mechanism designed to capture long-range temporal correlations, do not achieve superior performance. In [19], the exact LSTM architecture is not explicitly provided, so we base our assumptions on similar studies of complex physical systems [9], considering the following configuration: sequence length $L = 48$, hidden dimension $d_{hidden} = 16$, and input dimension $d_{input} = 24$. This results in a Multiply-Accumulate Operations (MAC) count of 245,760.\nIn contrast, our approach achieves superior detection performance with significantly lower computational complexity, requiring only 2,688 MACs."}, {"title": "V. CONCLUSION"}, {"title": null, "content": "We introduced an unsupervised framework for anomaly detection in complex physical systems, grounded in classical embedding theory and physics-inspired consistency principles, particularly state-derivative relations in the embedding space. To translate this theoretical foundation into practice, we developed TDC-AE, an algorithm designed to capture system dynamics efficiently.\nOur results demonstrate that time correlation can be effectively preserved in a 1D embedding, enabling robust anomaly detection. TDC-AE outperforms benchmarks on the C-MAPSS dataset, surpassing computationally intensive models like LSTMs and Transformers, while achieving a 200x reduction in MAC operations, keeping them below 5,000 MACS."}, {"title": null}, {"title": null, "content": "This efficiency makes our approach particularly suitable for lightweight edge computing applications.\nFuture work will focus on further leveraging the concept of state-derivative pairs by expanding on system-theoretic perspectives and physical modeling principles to make the method accessible for further use cases and other types of dynamical systems."}, {"title": null}]}