{"title": "FastAdaSP: Multitask-Adapted Efficient Inference for Large Speech Language Model", "authors": ["Yichen Lu", "Jiaqi Song", "Chao-Han Huck Yang", "Shinji Watanabe"], "abstract": "In this study, we aim to explore Multitask Speech Language Model (SpeechLM) efficient inference via token reduction. Unlike other modalities such as vision or text, speech has unique temporal dependencies, making previous efficient inference works on other modalities not directly applicable. Furthermore, methods for efficient SpeechLM inference on long sequence and sparse signals remain largely unexplored. Then we propose FastAdaSP, a weighted token merging framework specifically designed for various speech-related tasks to improve the trade-off between efficiency and performance. Experimental results on WavLLM and Qwen-Audio show that our method achieves the state-of-the-art (SOTA) efficiency-performance trade-off compared with other baseline methods. Specifically, FastAdaSP achieved 7x memory efficiency and 1.83x decoding throughput without any degradation on tasks like Emotion Recognition (ER) and Spoken Question Answering (SQA). The code will be available at https://github.com/yichen14/FastAdaSP", "sections": [{"title": "Introduction", "content": "Speech Language Models (SpeechLMs) have been an important role in the field of natural language processing and speech technology. Recent advancements (Hu et al., 2024; Chu et al., 2023; Sun et al., 2024b) have demonstrated significant capabilities in voice processing and audio understanding. Furthermore, GPT4-0 (OpenAI, 2024) showcases conversational speech processing abilities, advancing the capability of LLMs toward various voice-interface applications. However, challenges related to inference latency and memory efficiency remain major bottlenecks, especially as multitask SpeechLMs grow larger, reaching up to 7 billion parameters. These challenges necessitate the development of more efficient inference methods.\nSpeechLMs are often capable of performing a wide range of speech or audio-related tasks. As shown in Figure 1, in our study, we categorize and define these tasks into two distinct classes: Dense Tasks: Nearly all input audio tokens are useful, such as in Automatic Speech Recognition (ASR) and Speech Translation (ST); Sparse Tasks: Tasks like Emotion Recognition (ER) and Speaker Verification (SV), where only a few tokens within the entire audio input contain the crucial information needed to perform the task.\nThe temporal dependencies in speech signals require efficient handling of long sequences, while the sparsity of relevant information demands precise extraction of crucial audio features. These unique properties make SpeechLM tasks distinct from other modalities like vision or text, especially when implementing token reduction techniques.\nTo address these issues and improve the efficiency of SpeechLM inference, we introduce FastAdaSP, a unified SpeechLM fast inference framework that incorporates multiple audio token reduction methods during the pre-filling stage tailored to different types of tasks. FastAdaSP does not require any additional training, making the entire framework more practical and easy to use. Our main contributions are as follows:\n1. We introduce a new plug-and-play method for effectively selecting layers for audio token reduction operations on sparse tasks.\n2. We study efficient inference methods specifically designed for both dense and sparse tasks on SpeechLMs and validate the effectiveness of our methods across multiple tasks.\n3. To benchmark the task, previous token reduction methods, started from other modalities, have been investigated and analyzed in this emerging context of SpeechLM settings."}, {"title": "Related Work", "content": "Large Speech Language Models: SpeechLMs (Borsos et al., 2023; et al., 2023; Radhakrishnan et al., 2023; Sun et al., 2024b; Chu et al., 2023; Hu et al., 2024; Gong et al., 2024; Maiti et al., 2024; Lu et al., 2024) adopt a large pretrained language model (Touvron et al., 2023) as their base model and use audio encoder(s) (Radford et al., 2023; Chen et al., 2022; Hsu et al., 2021) to process raw audio input. Leveraging the language understanding and reasoning abilities of LLMs, SpeechLMs can perform various speech-related tasks. However, as SpeechLMs grow in size, inference latency and memory efficiency become problematic. Thus, research on cost-saving techniques is essential to address these challenges.\nEfficient Inference in ASR: Recent studies (Zhu et al., 2024; Kim et al., 2022; Burchi and Vielzeuf, 2021) have focused on efficient inference for ASR models (Gulati et al., 2020; Kim et al., 2023) by progressively down-sampling the audio features in the audio encoder to reduce sequence length. However, these methods are specifically designed for the ASR task and do not generalize well to multitask settings for SpeechLMs.\nKey-Value (KV) Cache Compression: In addition to the efficient inference methods for ASR, some of other works are focusing on compressing KV Cache to speed-up LLMs inference. Previous works such as StreamLLM (Xiao et al., 2024), H2O (Zhang et al., 2023), LESS (Dong et al., 2024), LOOK-M (Wan et al., 2024) were designed to compress the text or vision KV cache during inference to overcome the limited KV cache size and accelerate the inference speed. However, KV cache compression techniques do not actually reduce the number of input tokens during the pre-filling stage. When a long video is input to a multimodal LLM, the extensive sequence of vision and audio tokens can exceed the context length limit of the backbone LLM, causing several issues. Moreover, this technique does not improve the latency of the pre-filling stage.\nToken Reduction: To address these issues, extensive research has been conducted on token pruning techniques within Vision Language Models (VLMs). Recently, lots of token reduction works such as FastV (Chen et al., 2024), ToMe (Bolya et al., 2023), LLava-PruneMerge (Shang et al., 2024) focus on reducing the vision tokens to lower the computational costs through token eviction or merge. Besides the vision modality, A-ToMe (Li et al., 2023) applied the ToMe (Bolya et al., 2023) method to the audio modality in a Transformer-transducer model (Zhang et al., 2020) for ASR tasks only. However, token reduction methods for the audio modality in multitask SpeechLMs remain unexplored. Inspired by these previous works, our study primarily develops token reduction techniques that combine token merging and eviction for the audio modality in SpeechLMs during the inference process. We also explore the applicability of these methods to various speech-related tasks."}, {"title": "Methodology", "content": "In this section, we introduce the motivation and formulation of FastAdaSP, followed by our layer selection and task-specific design strategies for Multitask SpeechLMs. Note that, in our work, audio tokens refers to the audio features output by the multi-head attention block."}, {"title": "Preliminary", "content": "Speech Modality in Multitask SpeechLMs: During inference, VLMs often use only a small portion of visual information for reasoning and context understanding. However, SpeechLMs are capable of performing multiple tasks within a single model. For sequence-to-sequence dense tasks like ASR, it is crucial to consider \u201call audio tokens\" to generate accurate transcriptions. In addition to dense tasks, SpeechLMs also need to perform sparse tasks such as ER and SQA, where only a few tokens in the input hold critical information for generating accurate predictions. Therefore, a more careful token reduction policy is necessary for SpeechLMs.\nPre-filling Phase of SpeechLMs: During the pre-filling phase of SpeechLMs, the raw audio sequence is usually processed by pre-trained audio"}, {"title": "FastAdaSP: Method", "content": "To accommodate both sparse and dense tasks in SpeechLMs, we designed a novel token reduction method with different strategies for each.\nWeighted Token Merge: Dense tasks like ASR require most of the token information during inference, making direct token dropping from the attention output too aggressive and likely to result in the loss of critical information. Instead, merging similar audio tokens can eliminate redundant audio information while preserving essential content.\nToken merge techniques in the vision modality require calculating the similarity between numerous pairs of image patches in the spatial domain to identify the most similar pairs for merging (Bolya et al., 2023). For audio signals, however, token merge in audio processing needs to operate in the temporal domain. This involves calculating the similarity along adjacent audio tokens pairs and merge a cluster of adjacent audio tokens for a sequence of audio features $A = (a_i \\in R^D|i = 1, ..., L)$. For the audio features from 1 to L - 1, we use the cosine similarity score between the adjacent audio token key state to determine their similarity:\n$p_i = \\frac{K_i^T K_{i+1}}{||K_i||||K_{i+1}||}$"}, {"title": "FastAdaSP: Strategies", "content": "Based above method, we designed two similar but slightly different strategies for dense and sparse tasks to achieve better performance:\nDense Task Strategy: For dense tasks, we designed an operation scheduler that smoothly merges tokens layer by layer to prevent aggressive token dropping in SpeechLM. We implemented a constant schedule to maintain a consistent merge ratio and a decay schedule that linearly decreases the merge ratio to zero at the final layer. Please refer to 4.3 for the ablation study of schedulers.\nSparse Task Strategy: For sparse tasks, a more aggressive token reduction method can be applied by merging tokens within a single layer. However, layer selection needs to be approached carefully as it significantly affects task performance. Therefore, we incorporate a Transfer Entropy(TE)-based layer selection method (Section 3.4) specially designed for sparse tasks."}, {"title": "Addtional Studies on Layer Selection", "content": "Recent token reduction works (Chen et al., 2024; Shang et al., 2024; Bolya et al., 2023; Li et al., 2023) often struggle with selecting appropriate layers for token reduction. Due to the difficulties in interpreting current auto-regressive transformer models, understanding the exact properties of different layers during inference is challenging. Consequently, previous works have relied on empirical studies to test various layers and reduction ratios. This approach is impractical and lacks generalization for actual deployment. Therefore, we aim to explore a justification to serve as a theoretical attempt of token reduction layer selection.\nBy definition, entropy can reflect the information carried out by each layer. Here, we take $F$ as the feature output by the attention block which contains both audio and text features. Inspired by (Sun et al., 2022; Lin et al., 2024), we use the Gaussian distribution as the probability distribution to approximate the distribution of each channel in $F$. Thus, the entropy measurement of a single layer $H(F)$ can be defined as (for a more detail derivation, please refer to A.4):\n$H(F) \\propto H_\\tau (F) = \\sum_i log[\\sigma(F_i)]$\nHere, we calculate the entropy of each layer by summing the logarithm of the standard deviation($\\sigma$) of the each channels (audio tokens) in $F$. To assess the impact of weighted merge on a specific layer's contribution to the final output distribution, we calculate the Transfer Entropy to measure the information difference at the final layer based on the operation layer of our method. We define Transfer Entropy ($TE_i$) for layer i. $TE_i$ is equal to:\n$|H (\\Phi (F_{final} ; W_{final} )) \u2013 H (F_{final} | \\Phi (F_i; W_i))|$"}, {"title": "Experiments", "content": "4.1 Experiment Setting\nBasic Settings: We use 1\u00d7V100 32GB GPU to conduct the task performance experiment. We also use 1x A100 80GB GPU and 1\u00d7H100 80GB GPU for long sequence system metric experiment. We choose WavLLM 7B (Hu et al., 2024) and Qwen-Audio 7B (Chu et al., 2023) for all the experiments.\nFor each SpeechLM, we choose two dense tasks and two sparse tasks for experiments. Specifically, both models choose ASR and ST as dense task. For sparse task, we choose Emotion Recognition (ER) and Audio Caption (AC) on Qwen-Audio; ER and SQA on WavLLM. The full details of the dataset information and the evaluation metrics can be found in Table 1.\nSystem Metrics: We use Theoretical FLOPs, Real Time Factor (RTF), Pre-filling and Decoding Latency (seconds per sentence), and Throughput (tokens per second) to measure the efficiency of our method under different token reduction rates. We calculate the RTF by:\n$RTF = \\frac{T_{Pre-filling} + T_{Decoding}}{T_{audio}}$"}, {"title": "Results and Discussion", "content": "In this section, we compare our method with other SOTA methods. Then, we demonstrate the impact of token reduction on system metrics. For the full experiments results, please refer to Appendix A.1.\nBaselines: We selected several token reduction methods as our baselines. FastV (Chen et al., 2024) is a token eviction method based on attention scores for VLM. A-ToMe (Li et al., 2023) incorporates pair-wise merging techniques on the Transducer Model for ASR. We also test two other baselines method which randomly merge or evict tokens as the additional reference. Additionally, we applied our layer selection method to FastV and the two other random baselines since they do not have a clear layer selection strategy for speech tasks. Randomly choosing layers for these methods could result in completely failed decoding. Lastly, we evaluate the performance of the KV cache eviction method (H2O) (Zhang et al., 2023) on SpeechLMs for reference. However, this method is primarily designed to accelerate multi-round generation, focusing on a different set of challenges and applications compared to our work.\nEfficient Inference for Dense Tasks: We selected"}, {"title": "Ablation Study", "content": "Effectiveness of Layer Selection: We analyze the effectiveness of our TE-based layer selection method in Table 6 as an ablation study. Several operation layers before layer 15 were selected to analyze the relationship between the TE and their actual performance. The results indicate that selecting the operational layer based on the TE rank (layer 3) can achieve the best performance on the ER task at most of the time. While the rank of TE may not be strictly proportional to the actual performance, in our study, TE serves as a theoretical reference for layer selection. A more comprehensive study on layer selection for token reduction is left for future research.\nEffectiveness of Weighted Merge: Table 7 clearly illustrates the effectiveness of the weighted merge method. Compared to the normal average merge used in ToMe (Bolya et al., 2023) and A-ToMe (Li et al., 2023), our weighted merge algorithm consistently improves both ASR and ER in all the 10% to 50% FLOPs reduction ratio.\nEffectiveness of Scheduling: For the dense tasks ASR and ST, we utilize the decay or constant scheduler to smoothly merge audio tokens which can prevent aggressive token dropping. As shown in Table 8, layer scheduler can greatly improve the performance of the dense task when the token reduction rate is very high. However, due to multiple operations across many layers, the pre-filling latency will increase. Therefore, a more careful design of the overall strategies is needed in the future to better manage the trade-off between performance and efficiency."}, {"title": "Conclusion", "content": "In this study, we propose FastAdaSP, an efficient inference framework that incorporates multiple stages in SpeechLMs. This preliminary study explores token reduction methods for SpeechLMs. We investigated various properties of different types of SpeechLM tasks and proposed novel methods for both dense and sparse tasks. Our method achieved a 1.84x throughput increase with 7x memory efficiency, setting a new benchmark for the efficiency-performance trade-off across various tasks."}, {"title": "Appendix", "content": "A.1 Full Experiments Results\nWe also conduct the performance experiments on Qwen-Audio for both dense and sparse tasks and compare the baseline methods with our method. For the dense tasks ASR and ST, the results are presented in Table 9, demonstrating the effectiveness of our scheduling weighted token merge methods on another SpeechLM. The results for the sparse tasks ER and AC are shown in Table 10, which suggest our sparse setting method also performs well. These results on Qwen-Audio shows the effectiveness and generalization of our method across different SpeechLM.\nAdditionally, for the computation cost experiment, we also evaluated the Speech Summarization task on WavLLM using a subset of the How2 test set (Sanabria et al., 2018). As shown in Table 11, our method can effectively reduce the computation cost on a real dataset.\nFurther, we use one A100 80G GPU and one H100 80G GPU to conduct the long sequence experiments, which is shown in Table 12 and Table 13. The results indicate that increasing the audio length and beam size makes the acceleration of our method more noticeable."}, {"title": "Computation Reduction Theoretical Analysis", "content": "To analyze the computation reduction effect of our method, we use the theoretical FLOPs reduction rate. For simplicity, we just analysis the effective theoretical FLOPs reduction based on the token reduction rate and input sequence length on one layer. In the real situation, we can use the same methods to analyse all the decoder layers. Given the input sequence length n, the hidden dimension d and the Feed Forward Layer hidden dimension m. We can define the theoretical FLOPs in one transformer decoder layer as:\n$FLOPs = 2n^2d + 4nd^2 + 2ndm.$\nWhere the first term represents the attention operation in equation 2; The second term represents the calculation of query, key, value and output tensors; The third term represents the calculation of the operation in Feed Forward Layer. Given the reduction ratio k, after the token reduction, we obtain the reduced sequence length $n = n(1 - k)$. Then the theoretical FLOPs reduction rate at the next layer can be calculated as:\n$Rate = 1 - \\frac{2(1 - k)^2n^2d + nd(1 \u2013 k)(4d + 2m)}{2n^2d + 4nd^2 + 2ndm} = k + \\frac{(k-k^2)(2d+m)}{1 + (2d+m) \\cdot n}$\nAs a result, the longer the input sequence length, the higher the FLOPs reduction rate that can be achieved. As demonstrated in A.1 long sequence speed test, the acceleration is more pronounced for a 240-second audio sample compared to a 120-second audio sample.\nThis theoretical computation cost analysis suggests that our method will result in greater computational reduction for longer audio sequence input, highlighting the effectiveness of this technique in real world applications where the input audio is often very long."}, {"title": "FastAdaSP: Algorithm Details", "content": "Here we show the full implementation details of the FastAdaSP algorithm, which was brifely mentioned in Section 3.2. Given the audio feature sequence $A = (a_i \\in R^D|i = 1,..., L)$, the merge index list $M = (m_i \\in R|i = 1, ..., T)$ and merge weights $W_{merge} = (w_i \\in R|i = 1, ..., L)$. Then we can use Algorithm 1 to obtain the merged audio feature sequence $H = (h_i \\in R^D|i = 1, ..., N)$, where N is the length of the merged audio feature sequence.\nAdditionally, if there are B batches in the hidden states, we currently need to perform the algorithm B times to reduce the audio tokens for each audio sequence separately. In the future, this process may be improved by executing the algorithm for each batch in parallel."}, {"title": "Derivation of Transfer Entropy", "content": "In this section, we recall the derivation of transfer entropy from (Lin et al., 2024). We also did a slight modification on the final definition based on our settings. As mentioned in section 3.4, given $F\\in R^{LXD}$ as the feature output after attention block, the entropy was defined as:\n$H(F) = -\\int p(f) log p(f) df, f \\in F.$\nFollowing the (Lin et al., 2024; Sirignano and Spiliopoulos, 2020), we regard the feature $F$'s probability distribution as a Gaussian distribution $F ~ \\Nu(\u00b5, \u03c3^2)$. Therefore, the equation 11 can be derived into:\n$H(F) = -E[logN(\u03bc, \u03c3^2)] = -E [log ((2\\pi \\sigma^2)^{-1/2} exp(-(x-\u00b5)\u00b2 / 2\\sigma^2))] = \\frac{1}{2} log(\\sigma) + \\frac{1}{2}log(2\\pi e)$\nWhere $\u03c3_i$ is the standard deviation of i-th hidden state in F. The H(F) is proportional to the log(\u03c3) since $1/2log(2\u03c0) + 1/2e$ is constant term. Thus we could get the equation 7 in Sec 3.4."}, {"title": "Applications in the Real World and Future Perspective", "content": "In this study, we propose a efficient inference framework which designed for audio modality reduction in Multitask SpeechLM. In the context of long audio sequences, it is observed that only a small part of tokens carries critical information, while others may be not relevant (e.g. periods of noisy or blank audio). Our proposed plug-and-play methodology aims to efficiently identify and prioritize significant audio tokens during the pre-filling stage, which can offers substantial benefits for long-form audio comprehension.\nIn addition, in practical deployments of SpeechLM products, batch decoding is often a necessity, with batch sizes potentially reaching up to 128 or more. Within these batch decoding settings, our proposed methods are designed to reduce the memory footprint associated with many long audio inputs while simultaneously accelerating the decoding process. This optimization is crucial for enhancing the efficiency and scalability of SpeechLM systems in real world applications.\nIn the future, we may extend the current efficient inference framework to multi-round decoding scenarios, which can handle the dense task and sparse task at the same time. This improvement will make the whole system more applicable to real world use cases. Moving forward, this pioneering study on audio token reduction techniques in Multimodal Large Language Models (MLLM) paves the way for future research to explore the general behavior of audio and other modalities such as vision. The next stage of this study is to investigate the unified methodology to accelerate both audio and vision modalities simultaneously in Audio-Visual LLMs (e.g., video-SALMONN (Sun et al., 2024a)), which enable more efficient inference for long video understanding."}]}