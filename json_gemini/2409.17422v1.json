{"title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction", "authors": ["Zhenmei Shi", "Yifei Ming", "Xuan-Phi Nguyen", "Yingyu Liang", "Shafiq Joty"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long context inputs, but this comes at the cost of increased computational resources and latency. Our research introduces a novel approach for the long context bottleneck to accelerate LLM inference and reduce GPU memory consumption. Our research demonstrates that LLMs can identify relevant tokens in the early layers before generating answers to a query. Leveraging this insight, we propose an algorithm that uses early layers of an LLM as filters to select and compress input tokens, significantly reducing the context length for subsequent processing. Our method, Gem-Filter, demonstrates substantial improvements in both speed and memory efficiency compared to existing techniques, such as standard attention and SnapKV/H2O. Notably, it achieves a 2.4\u00d7 speedup and 30% reduction in GPU memory usage compared to SOTA methods. Evaluation on the Needle in a Haystack task shows that GemFilter significantly outperforms standard attention, SnapKV and demonstrates comparable performance on the LongBench challenge. GemFilter is simple, training-free, and broadly applicable across different LLMs. Crucially, it provides interpretability by allowing humans to inspect the selected input sequence. These findings not only offer practical benefits for LLM deployment, but also enhance our understanding of LLM internal mechanisms, paving the way for further optimizations in LLM design and inference. Our code is available at https://github.com/SalesforceAIResearch/GemFilter.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive abilities [WTB+22, BCE+23] and found widespread application in various AI systems, such as ChatGPT [SZK+22], Gemini [ABW+23], and Claude [Ant24], and so on. They are also a fundamental component in building language-based AI agents that can orchestrate plans and execute complex tasks through interaction with external tools. A key requirement for many of these applications is the ability to process long-context inputs. This ability can also potentially eliminate the need of a retriever in retrieval augmented generation (RAG) [XPW+24] or enhance its performance [JMC24]. Therefore, significant efforts have been made recently to build LLMs that support long context inputs. For instance, LLaMA 3.1 [DJP+24], Mistral [JSM+23], and Phi 3.5 [AJA+24] now support input sequences of up to 128K tokens, while Gemini can handle inputs of up to 1M tokens. However, processing such lengthy inputs comes at a substantial cost in terms of computational resources and time. Therefore, accelerating the LLM generation speed while simultaneously reducing GPU memory consumption for long-context inputs is essential to minimize response latency and increase throughput for LLM API calls.\nOne prominent optimization for fast text generation in decoder-only LLMs (i.e., using a causal attention mask) is the KV cache. Specifically, there are two phases involved in auto-regressive generation. Given a long context input, the first is the prompt computation phase, when the LLM computes the KV cache for all layers, storing the intermediate attention keys and values of the input tokens. Next, in the iterative generation phase, the LLM generates tokens iteratively using the pre-computed KV cache, avoiding redundant computations. GPU memory usage and running time scale linearly with the KV cache size, meaning that the computational is high for long inputs. To reduce GPU memory usage and running time during the iterative generation phase, H2O [ZSZ+23] and SnapKV [LHY+24] introduce static methods to compress/evict the KV cache. These techniques can shrink the KV cache size from 128K to 1024 with negligible performance loss, resulting in faster speeds and lower GPU memory consumption during the iterative generation phase. However, these methods do not improve the efficiency of the prompt computation phase, which becomes the dominant bottleneck as the input context lengthens. Thus, we ask:\nCan we accelerate the speed and reduce memory usage during the prompt computation phase?\nWe observe that when serving a query, LLMs often find the necessary information in the early layers, even before generating the answer. Specifically, the relevant tokens can be identified using the attention matrix from these early layers (Figure 2), which we refer to as filter layers. Figure 1 provides a real example from the Needle in a Haystack task, where LLMs must find a small piece of information within a large context. For LLaMA 3.1 8B, we observe that the information needed to answer the query can be distilled from the attention matrix in any of the 13th-19th layers. Furthermore, LLMs explicitly summarize the required information in these filter layers. As a consequence, we only need to perform the prompt computation on a long context input for the filter layers, allowing us to compress the input tokens into a smaller subset (e.g., reducing from 128K tokens to 100), saving both time and GPU memory. We then feed the selected tokens for full model inference and proceed with a standard generation function. Algorithm 1 in Section 3 presents our method GemFilter."}, {"title": "2 Related Works", "content": "Generation Speed-up with Long Context Input. One effective technique to accelerate auto-regressive generation is KV cache compression/eviction. During generation, LLMs store the previous key and value matrices to reduce computational complexity. However, when the input context is long (e.g., 128K tokens), the memory consumption and running time associated with the KV cache dominate iterative generation. Many studies have focused on KV cache eviction. For instance, [GZL+23] evict long-range contexts on attention heads to prioritize local contexts, using the KV cache only for heads that broadly attend to all tokens. Streaming LLM [XTC+23] introduces an attention sink that retains only the first few tokens and the latest k tokens in the KV cache to enable fast streaming generation. LOOK-M [WWL+24] applies KV eviction in the multimodality so that the model only needs to look once for the image. LongWriter [BZL+24] uses KV eviction to enable LLMs to generate coherent outputs exceeding 20,000 words. MInference 1.0 [JLZ+24] determines the optimal KV cache pattern for each attention head offline and dynamically builds sparse indices based on the assigned query during inference. QuickLLaMA [LSJ+24] classifies the KV cache to many subsets, e.g., query tokens, context tokens, global tokens, and local tokens, and only preserves some types of tokens in the KV cache. ThinK [XJD+24] proposes a query-dependent KV cache pruning method by pruning the least significant channel dimensions of the KV cache. H2O [ZSZ+23] retains only tokens contributing to cumulative attention. SnapKV [LHY+24] evicts non-essential KV positions for each attention head based on observation windows. While the aforementioned studies focus on eviction and compression of the KV cache during the prompt computation phase to optimize the iterative generation phase, they do not reduce the running time or GPU memory usage during the prompt computation phase. In contrast, our method, GemFilter, achieves both reduced running time and GPU memory usage in the prompt computation phase, as well as during the iterative generation phase. We provide a more detailed comparison in Section 3.4."}, {"title": "3 Method", "content": "3.1 Notations and Preliminary\nWhile the Transformer and self-attention architecture [VSP+17] have already become overwhelmingly popular, we first introduce certain preliminary definitions to provide a better methodological connection to our proposed GemFilter method in Section 3.2.\nFor any positive integer n, we use [n] to denote the set {1, 2,\u2026,n}. We use \u2218 to denote function composition and \u2299 to denote the Hardamard product. Let n be the input token/prompt length, d the hidden feature dimension, and V the vocabulary set. We now introduce the key concept of attention and transformers. We first define the query, key, and value matrices. It is important to note that during text generation, the key and value matrices are also referred to as the KV cache, as they are stored in GPU memory to reduce running time during the iterative prediction of the next token.\nDefinition 3.1 (Single layer self-attention). Let Q \u2208 \u211d^{n\u00d7d} be the query matrix, K \u2208 \u211d^{n\u00d7d} the key cache, and V \u2208 \u211d^{n\u00d7d} the value cache. Let M_c \u2208 {0,1}^{n\u00d7n} be the causal attention mask, where (M_c)_{i,j} is 1 if i > j and 0 otherwise. The self-attention function Attn is defined as:\nAttn(Q, K, V) = M_c \u2299 \\text{Softmax}(\\frac{QK^T}{\\sqrt{d}})\u22c5 V\nDefinition 3.2 (Multi-layer transformer). Let T \u2208 V^n represent the input tokens, and let m denote the number of transformer layers. Let g_i represent components in the i-th transformer layer other than self-attention, such as layer normalization, residual connections, and the MLP block, where g_i : \u211d^{n\u00d7d} \u2192 \u211d^{n\u00d7d} for any i \u2208 {0,1,...,m}. Let Attn_i denote the self-attention module in the i-th transformer layer. We define an m-layer transformer F_{1:m} : V^n \u2192 \u211d^{n\u00d7d} as\nF_{1:m}(T) := g_m \u2218 Attn_m \u2218 g_{m\u22121}\u2218\u22ef \u2218 g\u2081 \u2218 Attn\u2081 \u2218 g_0 \u2218E(T) \u2208 \u211d^{n\u00d7d},\nwhere E is the input embedding function mapping the input tokens to hidden features using the vocabulary dictionary, i.e., E(T) \u2208 \u211d^{n\u00d7d}.\nNote that the above definitions use a single attention head for simplicity, but in practice, multi-head attention is used [VSP+17].\n3.2 Our Algorithm: GemFilter\nWe present our method, GemFilter, in Algorithm 1. We also present PyTorch code in Appendix C.1 for the reader's interests. The high-level idea is to run the LLM twice. In the first pass, we run only the early layers of the LLM to select the key input tokens. This corresponds to the prompt computation phase (Line 4-7 of Algorithm 1). This process selects the top k tokens that receive the most attention from the last query token. In the second pass, we feed the selected tokens to the full LLM and run the generation function, corresponding to the iterative generation phase (Line 8). Below, we explain Algorithm 1 step by step."}, {"title": "3.3 Running Time and Memory Complexity Analysis", "content": "The results of our analysis on time complexity and GPU memory consumption are presented in Theorem 3.3 below, with the proof deferred to Appendix B.\nTheorem 3.3 (Complexity analysis). Let n be the input sequence (prompt) length and d the hidden feature dimensions. In our Algorithm 1, GemFilter uses the r-th layer as a filter to select k input tokens. Let SnapKV and H2O also use k as their cache size. Assume the LLM has m attention layers, each with h attention heads, and each transformer layer's parameters consume w GPU memory. Assuming that we generate t tokens with the GEN function and n \u2265 max{d, k,t}, the following table summarizes the complexity for standard attention, SnapKV and H2O, and GemFilter:"}, {"title": "3.4 Comparison with Other Methods", "content": "GemFilter reduces both running time and GPU memory usage in both the prompt computation and iterative generation phases, whereas SnapKV [LHY+24] and H2O [ZSZ+23] focus only on the iterative generation phase. During the prompt computation phase, standard attention computes and stores the entire KV cache for all layers in GPU memory, which is used during the generation phase. SnapKV and H2O, on the other hand, compute the entire KV cache for all layers but only store a portion of it in GPU memory (e.g., k = 1024). They use the selected KV cache for memory-efficient generation. SnapKV selects important clustered positions of the KV cache from an 'observation' window located at the end of the prompt, while H2O greedily drops tokens based on cumulative attention scores to retain only a small portion of the KV cache. In contrast, GemFilter avoids computing the KV cache for all layers during the prompt computation phase.\nCompared to SnapKV and H2O, there are two additional differences. First, SnapKV and H2O maintain separate index sets for each layer and attention head, resulting in mh index sets in total. This leads to different behaviors across attention heads, making their intermediate mechanisms more difficult to interpret. On the other hand, GemFilter uses a single index set, J, allowing for easier interpretability by enabling the printing of the selected sequence for human review before the second run (see a real example in Figure 1). Another distinction lies in how positional embeddings are handled. In SnapKV and H2O, the maximum positional embedding distance is n + t, as the same positional embedding is used in both the prompt computation and iterative generation phases. However, in GemFilter's second run, the maximum positional embedding distance is reduced to k+t because the input token length is reduced from n to k, and the RoPE function\u00b9 is re-computed. This reduction makes GemFilter more efficient, as the model can better handle shorter input sequences, as demonstrated in Figure 4 (a)."}, {"title": "4 Experiments", "content": "Model and Datasets. We evaluated our approach using three popular long-context models: LLaMA 3.1 8B Instruct\u00b2 [DJP+24], Mistral Nemo 12B Instruct\u00b3 [JSM+23], and Phi 3.5 Mini 3.8B Instruct\u2074 [AJA+24], all of which support an input token length of 128K. We compared our method, GemFilter, against standard attention and two state-of-the-art methods, SnapKV [LHY+24] and H2O [ZSZ+23]5. For our experiments, we used two popular datasets: Needle in a Haystack [Kam24] (Section 4.1) and LongBench [BLZ+23] (Section 4.2). More implementation details are provided in Appendix C.2.\nFilter Layer. Except Section 4.3, for context selection, we always use the index of 13 out of 32, 19 out of 40, and 19 out of 32 layers as the input filter for LLaMA 3.1, Mistral Nemo and Phi 3.5, respectively. In Section 4.3, we provide an ablation study for the filter layer choice."}, {"title": "4.1 Needle in a Haystack", "content": "The Needle in a Haystack [Kam24] benchmark serves as a pressure test, challenging LLMs to retrieve accurate information from a specific sentence (the 'needle') hidden within an extensive document"}, {"title": "4.2 LongBench", "content": "LongBench [BLZ+23] is a multi-task benchmark designed to rigorously evaluate long-context understanding capabilities across various datasets, including single- and multi-document Question Answering (QA), summarization, few-shot learning, and synthetic tasks. We evaluate on the English-only dataset, following [LHY+24, XJD+24].\nFor each LLM, we evaluate GemFilter and SnapKV with selected tokens/KV caches of 1024, 2048, and 4096. We also evaluated standard attention (all KV cache) and H2O with a KV cache size of 4096 on the LongBench dataset to further demonstrate the performance of GemFilter, following [LHY+24]. Table 1 shows a negligible performance drop in LLMs using GemFilter compared to standard attention, even with only 1024 selected tokens. In some cases, GemFilter even outperforms standard attention, such as GemFilter-2048 for Mistral Nemo 12B Instruct. It demonstrates significantly better performance than H2O and comparable performance with SnapKV. Furthermore, GemFilter effectively filters key information in long contexts, provides interpretable summaries,"}, {"title": "4.3 Filter Layer Choice", "content": "In this section, we explore which layer should be chosen as the input filter. First, we aim to determine which layer of the LLM can best identify the position of the needle information. In Figure 5, we plot the distance between the needle's position and the selected token index across all layers in the LLM. The results reveal three stages in the prompt computation of LLMs. In the first stage, the initial layers preprocess the input context and search for the 'needle'. In the second stage, some early to middle layers identify the needle information. Finally, in the third stage, the LLM prepares to generate the output based on the selected tokens."}, {"title": "4.4 Running Time and GPU Memory Consumption", "content": "In this section, we compare the running time and GPU memory consumption of different methods with FlashAttention [DFE+22, Dao23, SBZ+24] support. As shown in Figure 3, our method, GemFilter, achieves a 2.4\u00d7 speedup compared to SnapKV and standard attention, with 30% and 70% reductions in GPU memory usage, respectively. It saves both running time and GPU memory by processing the long input context only during the first stage, as described in Section 4.3. For the latter two stages, the LLMs only need to handle compressed inputs. In Figure 6, we present a comparison of running time and GPU memory consumption for Mistral Nemo 12B Instruct and Phi 3.5 Mini 3.8B Instruct using various methods. GemFilter runs faster and uses less GPU memory than the state-of-the-art methods, as discussed above. Additionally, Figure 3 and Figure 6 further support our Theorem 3.3 in Section 3.3."}, {"title": "5 Conclusion", "content": "In this work, we presented a novel approach, GemFilter, to accelerate LLM inference and reduce memory consumption for long context inputs. By leveraging the ability of early LLM layers to identify relevant information, GemFilter achieves significant improvements over existing techniques. It demonstrates a 2.4\u00d7 speedup and 30% reduction in GPU memory usage compared to SOTA methods, while also showing superior performance on the Needle in a Haystack benchmark. Our approach is simple, training-free, applicable to various LLMs, and offers enhanced interpretability by directly inspecting selected tokens. These results not only provide practical benefits for LLM deployment, but also provide insight into a better understanding of LLM internal mechanisms."}, {"title": "A More Preliminary", "content": "In this section, we introduce some key definitions of language modeling modules. We begin with the input embedding function and the output embedding function. They are functions that bridge between the input token space and the real vector space.\nDefinition A.1 (Input embedding function and input tokens). The input embedding function E : V^n \u2192 \u211d^{n\u00d7d} maps the input tokens to hidden features using the vocabulary dictionary D_{voc} \u2208 \u211d^{|V|\u00d7d}. Let T \u2208 V^n be input tokens. Then, we have E(T) \u2208 \u211d^{n\u00d7d} and E(T)_i = D_{voc} \u2208 \u211d^d for any i \u2208 [n].\nDefinition A.2 (Output embedding function). The output embedding function G : \u211d^d \u2192 \u211d^{|V|} maps hidden features to the probability logits of the vocabulary dictionary.\nWe introduce Softmax, which allows self-attention to learn the probability distribution rather than function anymore.\nDefinition A.3 (Softmax). Let z \u2208 \u211d^n. We define Softmax : \u211d^n \u2192 \u211d^n satisfying\nSoftmax(z) := exp(z) / (\\text{exp}(z), 1_n)."}, {"title": "B Proof of Time Complexity", "content": "Theorem B.1 (Complexity analysis. Restatement of Theorem 3.3). Let n be the input sequence (prompt) length and d the hidden feature dimensions. In our Algorithm 1, GemFilter uses the r-th layer as a filter to select k input tokens. Let SnapKV and H2O also use k as their cache size. Assume the LLM has m attention layers, each with h attention heads, and each transformer layer's parameters consume w GPU memory. Assuming that we generate t tokens with the GEN function and n \u2265 max{d, k,t}, the following table summarizes the complexity for standard attention, SnapKV and H2O, and GemFilter:"}, {"title": "C More Details about Experiments", "content": "C.1 PyTorch Code\nWe provide the PyTorch code of Algorithm 1 GemFilter below, where our method only needs a few lines of adaptation based on standard attention8."}, {"title": "C.2 Implementation Details", "content": "All the Needle in a Haystack and LongBench experiments run on A100-40GB GPUs. All the experiments of running time and memory complexity are evaluated on H100-80GB GPUs. We use"}, {"title": "C.3 More Needle in a Haystack", "content": "We provide more results of Section 4.1 here. In Figure 7, GemFilter outperforms All KV (standard attention) and SnapKV by a large margin with Phi 3.5 Mini 3.8B Instruct. In Figure 8, we use layer 14 of LLama 3.1 as the input filter layer, which is an empirical support of the ablation study in Section 4.3, as it can also obtain good performance on the Needle in a Haystack benchmark."}]}