{"title": "LARGE LANGUAGE MODELS AS USER-AGENTS FOR EVALUATING\nTASK-ORIENTED-DIALOGUE SYSTEMS", "authors": ["Taaha Kazi", "Ruiliang Lyu", "Sizhe Zhou", "Dilek Hakkani-T\u00fcr", "Gokhan Tur"], "abstract": "Traditionally, offline datasets have been used to evaluate\ntask-oriented dialogue (TOD) models. These datasets lack\ncontext awareness, making them suboptimal benchmarks for\nconversational systems. In contrast, user-agents, which are\ncontext-aware, can simulate the variability and unpredictabil-\nity of human conversations, making them better alternatives\nas evaluators. Prior research has utilized large language mod-\nels (LLMs) to develop user-agents. Our work builds upon\nthis by using LLMs to create user-agents for the evaluation\nof TOD systems. This involves prompting an LLM, using\nin-context examples as guidance, and tracking the user-goal\nstate. Our evaluation of diversity and task completion metrics\nfor the user-agents shows improved performance with the\nuse of better prompts. Additionally, we propose methodolo-\ngies for the automatic evaluation of TOD models within this\ndynamic framework. We make our code publicly available 1", "sections": [{"title": "1. INTRODUCTION", "content": "Task-oriented dialogue (TOD) systems are designed to assist\nusers in completing specific tasks or goals through turns of\nnatural language interactions [1, 2, 3]. These systems are\ntypically built to guide users through a series of steps to ac-\ncomplish a particular objective, such as booking a flight, or-\ndering food, or scheduling an appointment.\nAlong with the emergence of large language models\n(LLMs) [4, 5, 6, 7, 8], TOD systems have been fundamen-\ntally boosted by the complex reasoning/understanding ability\nand the domain adaptation power of LLMs. The function-\nality as well as the design concepts of TOD systems have\nalso been tightly integrated into the training and grounding\nof LLMs. From the real-world deployment side, at least tens\nof LLMs have been developed and integrated into hundreds\nof downstream tasks, for example, the travel planning LLM\nagents [9]."}, {"title": "2. RELATED WORK", "content": "2.1. User Simulation for TOD System Evaluation\nUser simulators are designed to simulate users' behaviors in\ndialogues, which helps interactive evaluation of TOD sys-\ntems. The development of user simulators for evaluating TOD\nsystems can be broadly divided into three stages. Initially,\nuser simulators are built based on statistical models [12, 13],\nmimicking user behaviors that are conditioned on user goals\nand actions in the context of an interaction. At that time, us-\ning user simulators to test TOD systems was rarely explored\ndue to the scarcity of natural and high quality user simulator\nresponses. More recently, researchers leveraged the power of\npre-trained language models (PLMs) [14, 15] and developed\nend-to-end user simulators for TOD system evaluation. [16,\n10] both build upon the T5 [17] language model by training\nuser simulators on existing datasets, such as MultiWOZ [18].\nSimilarly, [19] uses BART [20] as the basis of the user sim-\nulator, which is further refined using reinforcement learning.\nThough demonstrating impressive goal-success rates (GSR),\nthese user simulators rely on fine-tuning PLMs on annotated,\ndomain-specific dialogue data, which limits their applicabil-\nity in the testing of new TOD systems.\nAs LLMx prevail in massive downstream tasks and\ndemonstrate high quality response generation ability, they\nare employed for user simulator construction as well. Specif-nically, [21] leverage LLMs as user simulators provided with a\nlist of user goal APIs. [11] harnesses the power of in-context\nlearning [22] to generate more diverse and human-like ut-\nterances. While achieving great evaluation effects, these\nsystems still suffer from problems such as complex ground-\ning goals and the need to pre-define user goals. Our work\nattempts to solve these problems by creating a flexible user\nsimulator that does not need fine-tuning for specific tasks and\ncan adapt to different dialogue systems easily. We use prompt"}, {"title": "2.2. LLMs for Task Completion", "content": "Traditional LLM benchmarks like MMLU [23] and HELM [24]\ndo not include components to test the abilities of LLMs for\ntask completion, throughout multi-turn interactions. To eval-\nuate LLM's task completion performance, Saycan [25] and\nVirtualHome [26] benchmarks ask LLM to generate the cor-\nrect action sequence for controlling the robot to finish user\ninstruction in a specific environment such as a kitchen.\nThe framework that gets closest to ours in terms of LLM\nevaluation is the MT-bench [27] as it also includes multi-\nturn interactions; however, these are mainly for open-domain\nquestion answering, and task completion is not considered\nin their evaluation. Another framework that is along similar\nlines is AgentBench [28], as it aims to systematically eval-\nuate an agent's ability to follow instructions in web-, game-\nor code-grounded environments, however, multi-turn con-\nversational interactions is not a focus. Mostly related to our\nwork is the evaluation of LLMs in a very specific task, such as\ntravel planning [9] and slide making [29]. While they focus\non constraint fulfillment or goal achievement, we aim to ex-\nplore more evaluation metrics related to diversity, etc., under\ndifferent task settings."}, {"title": "3. PROPOSED APPROACH", "content": "We developed a framework to evaluate TOD models, leverag-\ning LLMs as user simulators to facilitate the assessment pro-\ncess. For the experiments run in this work, we use the TOD\nsystem from [30] that prompts LLMs, such as chatGPT with"}, {"title": "3.1. User Simulator", "content": "The user simulator module interacts with the TOD model to\nsynthesize dialogues that aim to complete tasks. The module\nis given an initial user goal to accomplish, along with an inter face to interact with the TOD model. The goal of the module\nis to conduct a multi-turn conversation with the TOD model\nto complete the given user goal. This process flow is depicted\nin Fig.1.\nOur user simulator primarily consists of a prompter and\nan LLM. The prompter sub-module controls the flow of the\nconversation, eliciting responses from the LLM and interact ing with the TOD model. It formats the user goal and sends\nit to the LLM for the initial user utterance. At this step, the\nprompter sub-module also inserts an example based on the\nprompt setting. The different prompts are explained in 3.1.1.\nAfter the LLM\u2019s first response is generated, it is sent to the\nTOD model for a reply. The prompter then receives the TOD\nmodel\u2019s reply and stores the responses as conversation his tory. This conversation history is sent along with the prompt\nto the LLM to continue the conversation. This alternation\nbetween the LLM and the TOD model continues until the\nLLM generates a special set of tokens, namely <COMPLETE\nCONVERSATION>. To avoid cases where the LLM user agent and the TOD model end up in an infinite loop, we also\nlimit the number of turns to 30."}, {"title": "3.1.1. User Simulator Prompts", "content": "We experimented with three different prompt types. The de tailed prompts for each type are provided in the supplemen tary material.\n1. Vanilla Prompt: The vanilla prompt consists of an in struction for asking the model to generate a response\ngiven the conversation context. A sample dialogue con sisting of user and system turns is also provided as an\nin-context example.\n2. Thought Prompt: The Thought prompt is based on\nChain-of-Thought prompting [31]. It has two additions\nto the Vanilla prompt. First, the in-context example\nincludes a \u201cThought\u201d section, where reasoning is given\nbefore the agent\u2019s response. Second, the prompt in structs the model to first reason and then generate the\nresponses.\n3. User State Tracking Prompt: The User State Tracking\nprompt addresses issues faced by [11] where their user"}, {"title": "3.2. LLMs as Evaluators", "content": "Following the success of LLM-prompting methods for open domain dialogue response evaluation [32, 33], we propose\nLLM-based, reference-free evaluation of system responses in\nTODS to complement other metrics described in the next sec tion. In our experiments, for evaluation, we use LLMs that\nare different than the one used for the user simulator."}, {"title": "3.3. Automated Evaluation Metrics", "content": "Evaluation of user simulators is a challenging topic [34]. Pre vious evaluation metrics only focus on certain aspects and\nlack the understanding of dialogues. Involving neural net work models in evaluation would reduce the reliability and\nconsistency. Human evaluation would be a necessary compo nent to prove practicability, but it is costly. For comprehen sive evaluation, different components should be combined."}, {"title": "3.3.1. Task Completion Evaluation", "content": "Task completion validates whether the TOD model completes\nthe user\u2019s request. One way to compute is by checking if the\nfinal dialogue state satisfies all slot requests in the original\ngoal. While this is suitable for offline datasets, it is less ef fective with user-simulators, which may sometimes misinter pret or miss user goals from the initial instruction. Therefore,\nthe TOD model should be evaluated solely based on the di alogues. We propose an automated method to evaluate task\ncompletion scores of the TOD model. We use GPT-4 in a\nzero-shot setting as an evaluator to score each dialogue and\njudge whether the TOD model has completed the task. The\nmodel is prompted with a rubric to score the response: 1 if\nthe TOD model completes all requests presented by the user simulator, and 0 if it fails to complete any requests. Addi tionally, the model provides verbose feedback on the TOD\nmodel\u2019s performance and the reasoning behind the score. The\nprompt used has been inspired by [35]. Detailed prompt is\nprovided in the supplementary material."}, {"title": "3.3.2. User Agent Evaluation", "content": "To evaluate the user-agent, we focus on its ability to fulfill all\nassigned tasks. That is, the agent must generate appropriate"}, {"title": "3.3.3. Dialogue Level Diversity", "content": "The user simulator should be able to demonstrate diverse be- haviors to comprehensively test the task completion abilities\nof a TOD system. We measure the dialogue-level diversity\nwith a score from 1~3, where larger score represents higher\ndiversity. Here dialogue-level diversity means the degree of\nTOD system or user agent of deviating from the normal dia- logue flow. This is expected for real-life conversations as well\nas for highlighting the issue of maximal information exchange\nof previous user agent optimization. Automatic evaluation is\ndone by prompting LLMs like GPT with guidelines for each\nscore alongside the dialogue."}, {"title": "3.3.4. Naturalness", "content": "This metric measures the similarity of a conversation agent\nto actual human behavior. It gives a score of 1~5, where\n5 represents the maximum similarity to humans. Similar as\ndialogue-level diversity, automatic evaluation is also com- pleted by prompting LLMs, with detailed guidelines for each\nscore. For example,"}, {"title": "3.3.5. Coherence", "content": "This metric gives scores from 1~3 evaluating the coherence\nof the dialogues. The automatic evaluation of coherence also\nuses LLMs by prompting with guidelines for each score:\n\u2022 a. Locally, the utterances are coherent/logical based on\nprevious turns of conversations.\n\u2022 b. Globally, the utterances reasonably and logically ad- here to achieving the initial user goal step by step.\n\u2022 If both conditions a and b are satisfied, a score of 3\nshould be reported. Report 2 if only one condition is\nsatisfied. Report 1 if none of the conditions are satis- fied."}, {"title": "4. EXPERIMENTAL ANALYSIS", "content": "The primary objective of our experiments is to evaluate the\nperformance of LLMs as Task-Oriented Dialogue (TOD) sys- tems. To achieve this, we simulate conversations between a\nuser simulator and TOD systems. In our experiments, we use\nGPT-3.5 Turbo Instruct as the LLM for the user-simulator.\nWe conduct experiments with three different prompt types"}, {"title": "4.1. Evaluation Results", "content": "4.1.1. Quantitative Evaluation of User Simulator\nAs proposed in section 3.3.2 we evaluate the user-agent's abil- ity to fulfill all assigned instructions. To do so, we first sim- ulate conversations with multiple LLMs with the three differ- ent proposed prompts. We sample initial goals from the Mul- tiWOZ dataset and use them to simulate the conversations. After the dialogues are generated, the GPT-4 based scorer, as\nmentioned in 3.3.2, has to generate a score which is a ratio\nbetween the count of keywords that were found in the dia- logue and the total number of keywords present in the initial goal. Hence, the score for each dialogue will range from 0 to 1. The results are shown in Table 1. The Score column is the ratio of the sum of the scores of all dialogues to the total sum of the maximum score that can be achieved (which is also the number of the dialogues evaluated)."}, {"title": "4.1.2. Qualitative evaluation of User Simulator", "content": "We also carry out a manual assessment of the generated dia- logues and analyze qualitative patterns. The pattern in which the user-simulators generate queries or requests differs based on the prompt used. With the vanilla prompt, almost always, the user-simulator completes goals in a step-by-step manner. For instance, the user-agent might first ask, \u201cCan you book a train from Cambridge?\u201d and subsequently ask, I want it to go to Norwich.\" On the other hand, thought and verbose prompts usually generate more condensed responses, such as, \u201cCan you book a train that departs from Cambridge and goes to Norwich, also it should leave after 11:00?\" Dialogue formu- lation impacts naturalness of dialogue and selecting a prompt that best replicates human behavior should be chosen.\nDifferent prompts also influence how user simulators in- teract with the TOD model. The vanilla prompt based simu- lator often disregards the TOD model's responses, continuing the conversation and completing the goals it has been given. With the Thought prompt, the user-simulator will take into account the TOD model's response and repeat the user goal if the TOD model's response is unsatisfactory or unexpected. The verbose prompt based simulator displays a similar be- havior but also engages in negotiation, rephrasing queries, or altering goals based on the responses. For e.g. if the TOD model indicates no availability for a table booking in Cam- bridge West, the verbose simulator might adjust the query to seek a booking in Cambridge East. This characteristic of en- gaging with the TOD model and negotiating is an important aspect of user-simulators as they have to be robust to different types of TOD models.\""}, {"title": "4.1.3. Task Completion Evaluation", "content": "We evaluated the efficacy of the task completion evaluator by comparing manual and automated scoring of generated dia- logues for task completion. The evaluation was carried out on 50 generated dialogues, with the user-simulator using the vanilla prompt and GPT-3.5 Turbo Instruct as the LLM. The same model was also used for the TOD system. These 50 dialogues were then scored by both human annotators and GPT-4, based on the rubrics mentioned in 3.3.1. Four human evaluators scored the 50 samples, and a majority score was assigned to the dialogues. In the case of ties, the examples were discussed until a consensus was achieved. As mentioned in 3.3.1, the GPT-4 model was used to generate automatic scores for the dialogues. We then carried out an agreement analysis between the two and reported the statistics in Table 3. These results demonstrate a high level of agreement and performance of GPT-4 in evaluating task completion, closely aligning with human judgment. The substantial agreement underscore the model's reliability in this task."}, {"title": "4.2. Comparison with Human Evaluation", "content": "We ran automated evaluation as mentioned in section 3.3. The dialogues were simulated using GPT-3.5 Instruct as the LLM for both the user-simulator and the TOD framework.\nTo validate that the automatic evaluation module aligns with the actual human user experience, parallel human evalu ation was carried out. Evaluators were shown a set of full di alogues alongside user goals. For each dialogue, they would give a quantitative rating on a set of metrics, including the completion of the task (yes/no), naturalness for both agents (1\u223c5), coherence for both agents (1\u223c3) and diversity of the user simulator (1\u223c3). Human annotators were asked to fol low the same scoring guidelines as the automatic evaluation. We compared human and automatic evaluations on 50 di alogue samples per model, with the samples randomized to anonymize model identity. Table 2 shows that human eval uation has some agreement over the scores, and both give a higher score for the thought model. Further analysis of the quantitative agreement between human and automatic evalu ations is detailed in Table 3, as discussed earlier."}, {"title": "5. CONCLUSIONS", "content": "In this work, we built a universal framework to evaluate TOD systems using a LLM as a user simulator. This method creates a more dynamic and realistic evaluation compared to tradi tional static benchmarks. Our results show that LLMs can sig nificantly improve the evaluation process for TOD systems, making them closer to real-world interactions."}, {"title": "6. ACKNOWLEDGEMENT", "content": "This research was supported in part by Other Transaction award HR0011249XXX from the U.S. Defense Advanced Research Projects Agency (DARPA) Friction for Account ability in Conversational Transactions (FACT) program. This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program through which leading foundation models hosted by Microsoft Azure along with access to Azure credits were provided to conduct the research."}]}