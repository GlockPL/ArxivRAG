{"title": "\"GIVE ME BF16 OR GIVE ME DEATH\"?\nACCURACY-PERFORMANCE TRADE-OFFS IN LLM QUANTIZATION", "authors": ["Eldar Kurtic", "Alexandre Marques", "Shubhra Pandit", "Mark Kurtz", "Dan Alistarh"], "abstract": "Despite the popularity of large language model (LLM) quantization for inference acceleration, significant\nuncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats.\nWe present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats\n(FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family.\nAdditionally, our study examines the difference in text generated by quantized models versus their uncompressed\ncounterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us\nto obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual\nevaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless\nacross all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs\nsurprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive\nwith 8-bit integer weight and activation quantization. To address the question of the \"best\" format for a given\ndeployment environment, we conduct inference performance analysis using the popular open-source vLLM\nframework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous\ndeployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in\nasynchronous \u201ccontinuous batching\u201d deployment of mid- and large-size models on high-end GPUs. Our results\nprovide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements.", "sections": [{"title": "1 INTRODUCTION", "content": "The massive computational costs of serving large language\nmodels have led to a significant amount of work on infer-\nence acceleration techniques, such as quantization (Frantar et al., 2022; Dettmers & Zettlemoyer, 2022; Lin et al.,\n2024a), speculative decoding (Chen et al., 2023; Leviathan\net al., 2023), or model pruning (Xia et al., 2023; Muralid-\nharan et al., 2024). Model quantization, i.e., reducing the\nbitwidth of weights, activations, or both, is arguably the\nstandard approach to reducing memory and computational\ncosts at deployment time. The critical trade-off in LLM\nquantization is between the speed/memory benefits and the\naccuracy of the resulting compressed model. Despite exten-\nsive research on quantization, few studies provide system-\natic benchmarks or practical guidelines on expected perfor-\nmance at different compression levels. This state of affairs\nis expressed in our fore-title, which uses a direct quote from\nthe wave of user feedback regarding the quantized Llama-\n3.1-405B model (Dubey et al., 2024), initially suspected to\ndrop significant accuracy (Zhang, 2024; Karpathy, 2024)\nand later shown to be near-lossless in LMSYS Arena user\ntesting (Chiang et al., 2024). This uncertainty can be a sig-\nnificant barrier to adoption of quantized formats. In this\npaper, we systematically consider the following question:\nWhat are the practical accuracy-performance trade-offs\nfor popular quantization formats?\nDue to the broad range of existing quantization techniques,\nwe focus our study on formats with efficient computational\nsupport across different batching levels and low accuracy\nloss. This reduces the range of formats to be examined to\n8-bit weights and activations (W8A8), in integer (INT) pre-\ncision for NVIDIA Ampere and older devices, and floating-\npoint (FP) precision for NVIDIA Hopper and Ada Lovelace,\nas well as 4-bit integer weights with 16-bit activations\n(W4A16-INT). These formats are efficiently supported in\nthe vLLM inference engine (Kwon et al., 2023), which we\nuse for performance experiments through the GuideLLM\nframework (Neural Magic, 2024)."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In the following, we provide some background on post-\ntraining quantization of LLM weights and activations. Early\nwork on this topic has considered INT8 activation quantiza-\ntion and INT4/INT8 weight quantization (Dettmers et al.,\n2022; Yao et al., 2022; Park et al., 2022). A standard ap-\nproach for weight quantization has been direct round-to-\nnearest (RTN) quantization over small groups: given a group\nof g consecutive weights, viewed as a vector x \u2208 R\u00ba, b-bit\nRTN is defined as:\n$Q(x, b) = rnd(\\frac{x - min(x)}{max(x) - min(x)} \\cdot (2^b - 1))$\n$= rnd((x - z(x))/s(x)),$"}, {"title": "2.1 A Primer on Quantization", "content": "where rnd rounds to the nearest integer, z = z(x) =\nmin(x) is the \u201czero point\u201d and s = s(x) = (max(x) -\nmin(x))/(2 - 1) is the scale, taken as min-max. This early\nwork raised two key issues: RTN on weights loses signifi-\ncant accuracy for INT4 precision, and RTN quantization of\nactivations even to higher INT8 precision can be lossy due to\nthe large \"outlier features\" in LLMs (Dettmers et al., 2022).\nWeight quantization. Towards the first issue, GPTQ (Frantar et al., 2022) improved upon RTN for scalar, per-weight,\nquantization by allowing weights to be adjusted via second-\norder updates computed over a sample of calibration data.\nFollow-up scalar quantization methods such as AWQ (Lin\net al., 2024a), SqueezeLLM (Kim et al., 2023), OWQ (Lee\net al., 2024a), and SpQR (Dettmers et al., 2023) imple-\nmented variants of outlier-aware quantization, where a small\nfraction of weights are effectively stored in higher precision.\nThese methods can reach highly-accurate 4-bit quantization,\nbut the latter three lack general kernel support.\nMore recently, several high-compression methods have been\nproposed, such as QuIP (Chee et al., 2023), QuIP# (Tseng\net al., 2024a), QTIP (Tseng et al., 2024b) AQLM (Egiazarian et al., 2024), and GPTVQ (van Baalen et al., 2024),\ntargeting the 3-bit range or lower. These methods leverage\nmuch more complex representations, such as lattice/vector\nquantization, often paired with incoherence pre-processing\nof the weights. Despite high accuracy, these methods add\noverheads and lack efficient kernel support for batch sizes\nlarger than 1, which is necessary for general deployments.\nThus, our study does not currently consider these formats."}, {"title": "2.2 Efficient Inference With Quantized LLMs", "content": "Several performant LLM inference frameworks exist, such\nas HuggingFace TGI (HuggingFace, 2024), NVIDIA\nTensorRT-LLM (NVIDIA, 2023), and vLLM (Kwon et al.,\n2023). We focus our benchmarking efforts on vLLM, a\npopular open-source framework that addresses the specific\nchallenges of serving large, autoregressive neural networks.\nThe backend architecture of VLLM combines efficient ex-\necution management with high-performance computation\ngraphs that utilize pre-compilation, optimized operator fu-\nsion, and performant scheduling. Additionally, vLLM in-\ncorporates computational acceleration through customized\nkernels leveraging NVIDIA CUTLASS and hand-written\nGPU kernels. At the same time, vLLM enables broad hard-\nware compatibility, supporting NVIDIA and AMD GPUs,\nTPUs, and CPUs. Several optimization techniques, includ-\ning FlashAttention (Dao et al., 2022), FlashInfer (FlashIn-\nfer, 2023), PagedAttention (Kwon et al., 2023), and Mar-\nlin (Frantar et al., 2024) are also supported. This support\nand its open-source nature have led to vLLM's adoption by\nindustry and the open-source community. The framework\nintegrates optimized CUTLASS kernels for W8A8 (FP and\nINT), and mixed-precision Marlin kernels for W4A16-INT."}, {"title": "2.3 Accuracy Evaluations of Quantized LLMs", "content": "There is a substantial amount of prior work on the evaluation\nof quantized LLMs, primarily focusing on accuracy trade-\noffs under various quantization schemes (Yao et al., 2023;\nLiu et al., 2023b; Huang et al., 2024; Gong et al., 2024b; Li\net al., 2024a; Gong et al., 2024a). However, much of this\nresearch has concentrated solely on academic benchmarks,\nwhich do not fully represent the real-world environments\nwhere LLMs are deployed. Furthermore, the fact that some\nof these works do not tune hyperparameters and algorithmic\nchoices for the quantization algorithms can lead to incorrect\nconclusions regarding accuracy, as we demonstrate later in\nthe experimental section. Specifically, we refer to the claim\nthat 8-bit integer activation quantization causes significant\naccuracy drops (Li et al., 2024a; Lee et al., 2024b), which\nwe contradict experimentally.\nThe closest work to ours is by Lee et al. (2024b), which, like\nmost prior studies, focuses solely on accuracy under quanti-\nzation. However, we identify several points of divergence.\nFirst, although the authors claim to analyze models up to\n405B in size, open-ended benchmarks are not included for\nthis scale, and accuracy scores for the full-precision uncom-\npressed model are missing even for academic benchmarks.\nWithout baselines, it is difficult to draw meaningful conclu-\nsions on the impact of quantization at this scale. To address\nthis, we enabled highly efficient multi-node evaluations for\nthe 405B model, allowing comprehensive benchmarking in\nfull precision across academic and real-world tasks.\nThe second divergence point concerns their claims that\nAWQ outperforms GPTQ in a 4-bit weight-only quanti-\nzation setup. We attribute this finding to suboptimal hy-\nperparameter choices in GPTQ quantization and provide\na detailed comparison of the methods in Table 1 and Ap-\npendix A.2. Our results show that both methods perform\nsimilarly on academic benchmarks, while GPTQ demon-\nstrates notable improvements over AWQ on real-world\nbenchmarks, especially in coding tasks.\nThird, we also challenge the prior conclusion that W8A8-\nINT performs substantially worse than W8A8-FP and\nW4A16-INT. When tuned carefully, W8A8-INT proves\ncompetitive, with only minor accuracy losses. For instance,\nwhile Lee et al. (2024b) reports a 10-point drop in accu-\nracy for the W8A8-INT quantized 405B model on the Open\nLLM Leaderboard V2 benchmark relative to FP8 quantized\none, our carefully optimized quantization parameters reduce\nthis gap to just 0.7 points.\nUnlike previous work, we also explore how quantization\naffects text generation quality. This analysis provides deeper\ninsights into the impacts of various quantization schemes\non sentence structure, word choice, and semantic fidelity in\ntext generated by quantized LLMs.\nFurthermore, we conclude our study by analyzing the per-\nformance impacts of deploying various quantization formats\nacross a range of scenarios that represent real-world appli-\ncations. This examination provides insights into how each\nquantization approach performs under practical deployment\nconditions and hardware configurations."}, {"title": "3 BENCHMARK DESIGN AND SETUP", "content": "Our accuracy evaluation suite is designed to encompass a\nwide range of inference scenarios and use cases. It includes\na diverse set of tasks to ensure a representative collection\nof topics relevant to practical deployments. Furthermore,\nthe evaluation process is designed to be completely auto-\nmated, as opposed to human-in-the-loop approaches, en-\nabling streamlined experimentation. Overall, we group all\nof the benchmarks into three distinct categories."}, {"title": "3.1 Datasets and Benchmarks", "content": "Academic benchmarks such as Open LLM Leaderboard\nV1 and V2 (Beeching et al., 2023; Fourrier et al., 2024), are\nessential for evaluating model improvements. They focus\non structured tasks like question-answering and reasoning,\nproviding consistent and reproducible scoring. However,\nthey often lack alignment with real-world scenarios where\nsemantics, variability, and context-awareness are important.\nThe Open LLM Leaderboard V1 consists of a di-\nverse range of topics, including grade school math\n(GSM (Cobbe et al., 2021)), world knowledge and reasoning\n(MMLU (Hendrycks et al., 2020), ARC-Challenge (Clark\net al., 2018)), language understanding (Winogrande (Sak-\naguchi et al., 2021), HellaSwag (Zellers et al., 2019)), and\ntruthfulness (TruthfulQA (Lin et al., 2021)).\nThe Open LLM Leaderboard V2 builds on this with topics\nsuch as expert knowledge and complex reasoning (MMLU-\nPro (Wang et al., 2024), GPQA (Rein et al., 2023), Big\nBench Hard (Suzgun et al., 2022)), multi-step reasoning\n(MuSR (Sprague et al., 2024)), advanced math problems\n(MATH Level 5 (Hendrycks et al., 2021)), and instruction\nfollowing (IFEval (Zhou et al., 2023)).\nBy evaluating with both Leaderboards, we cover a broad\nspectrum of topics. We employ log-likelihood (multiple-\nchoice) and text-generation evaluations, allowing for stress\ntesting of quantized LLMs under diverse conditions.\nReal-world benchmarks, unlike academic benchmarks,\ntest models in scenarios that mimic human usage, such as in-\nstruction following, chat, and code generation. These bench-\nmarks include Arena-Hard-Auto-v0.1 (Li et al., 2024b; Chi-\nang et al., 2024; Li et al., 2024c), HumanEval (Chen et al.,\n2021), and HumanEval+ (Liu et al., 2023a), which offer a\nbroader range of tasks with higher variation but better reflect\nreal-world environments. The LMSYS Chatbot Arena (Chi-\nang et al., 2024) has established itself as a leading bench-\nmark for LLMs, assessing how models align with human\npreferences. Arena-Hard-Auto-v0.1 is an automated exten-\nsion of this benchmark, where an LLM judges responses to\n500 complex prompts on various topics. It has demonstrated\na strong correlation with human evaluations, achieving a\nstate-of-the-art 89% agreement with human preference rank-\nings (Li et al., 2024c). This enables a reliable, fast, and au-\ntomated evaluation of the model's chat capabilities without\nrequiring a human. In addition to chat-based interactions,\nLLMs are widely deployed as coding assistants. To evaluate\nthe performance of quantized models in code generation, we\ntested them on HumanEval and its more challenging vari-\nant, HumanEval+. These benchmarks measure a model's\nability to generate correct and functional code based on pro-\ngramming problems, with HumanEval+ introducing more\nrigorous evaluation protocol.\nText similarity analysis evaluates how closely the gener-\nated outputs of quantized models align with those of their\nunquantized counterparts. While real-world benchmarks\nreflect practical usage scenarios for LLMs, their inherently\nopen-ended format introduces notable variance in results.\nTo address this, we extend our evaluation suite to include\nan analysis of output similarity under identical prompt re-\nquests. This allows us to get insights into how quantized\nmodels' text generation differs from that of full-precision\nmodels. For this investigation, we employ metrics such as\nROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), and\nSemantic Textual Similarity (STS) to assess both seman-\ntic and structural consistency, ensuring that the intended\nmeaning and quality of generated text are preserved under\nquantization.\nROUGE-1 measures word-level (unigram) overlap between\noutputs from quantized and unquantized models, while\nROUGE-L captures structural similarity by focusing on the\nlongest common subsequence. BERTScore evaluates token-\nlevel contextual similarity, computed using the ROBERTa-\nlarge model's contextual embeddings. STS assesses overall\nsemantic similarity at the sentence level, using the Sentence\nTransformers framework (Reimers & Gurevych, 2019) built\non the 6-layer MiniLM architecture (Wang et al., 2020).\nWith this extensive evaluation framework, we ensured that\ndeployment scenarios ranging from structured, research-\ndriven tasks to open-ended, real-world applications were\ncovered, providing a holistic view of the performance and\ncapabilities of quantized LLMs."}, {"title": "3.2 Models, Formats, and Quantization Algorithms", "content": "We focus our evaluation on extensive testing of the recent\nLlama 3.1 series of models (Dubey et al., 2024), which\nhave gained significant traction in research and deployment\ncontexts. To obtain a comprehensive understanding of trade-\noffs, we conduct all of our experiments on the instruction-\ntuned versions of all three available Llama 3.1 sizes (8B,\n70B, and 405B), and for each of these, we focus on inves-\ntigating three popular quantization formats with efficient\ninference support in vLLM: W8A8-FP, W8A8-INT, and\nW4A16-INT."}, {"title": "4 QUANTIZATION VERSUS ACCURACY", "content": "We begin our discussion by examining the accuracy of quan-\ntized models across Leaderboard V1 (Table 2), Leaderboard\nV2 (Table 3) and real-world benchmarks (Table 3). Given\nthe density of the results, we examine them individually\nvia average recoveries across higher-level benchmarks and\ndiscuss \"outlier\" observations."}, {"title": "4.1 Academic Benchmarks", "content": "Academic benchmarks are the standard approach for evalu-\nating LLM reasoning capabilities. They provide structured\ntasks with well-defined evaluation criteria. Our evaluations\nfocus on the Open LLM Leaderboard V1 and V2 bench-\nmarks. Testing on both allowed us to prevent overfitting to\nV1, where we optimized our quantization hyperparameters.\nThe Open LLM Leaderboard V1 was evaluated by follow-\ning Meta's prompts for Llama-3.1 models to align with the\nexpected baseline scores. This approach introduces two no-\ntable differences from the standard protocols: first, MMLU\nand ARC-Challenge are evaluated as text-generation tasks\nwhere models generate the correct answer as text, as op-\nposed to the default log-likelihood-based protocol (Gao\net al., 2021). Second, GSM8k is evaluated using a chain-of-\nthought rather than the default few-shot format."}, {"title": "4.2 Real-World Benchmarks", "content": "While academic benchmarks provide structured evaluations,\nreal-world open-ended benchmarks better represent how\nmodels perform in dynamic environments. These bench-\nmarks test models on varied prompts with longer genera-\ntions and multiple potential solutions, focusing on responses'\ncorrectness and semantic quality. Our evaluations targeted\nthree key real-world benchmarks: Arena-Hard-Auto-v0.1,\nHumanEval, and HumanEval+, which measure performance\nin chat, instruction-following, and code generation. Table 3\nsummarizes results on these benchmarks.\nFigure 3 focuses on the Arena-Hard-Auto-v0.1 benchmark,\naveraging results from two evaluation runs per model and\nquantization scheme. The results illustrate that the response\nquality of quantized models remains highly competitive with\ntheir unquantized counterparts. As shown in the detailed\nresults in Appendix Table 7, the 95% confidence intervals\noverlap for all model sizes and quantization schemes, high-\nlighting the minimal impact of quantization on accuracy."}, {"title": "4.3 Text Similarity Investigation", "content": "Following our investigations on various structured and open-\nended tasks, we focus on examining the similarity of gener-\nated text between quantized models and their unquantized\ncounterparts. As described before, we employ four met-\nrics: ROUGE-1, ROUGE-L, BERTScore, and Semantic\nTextual Similarity (STS). All of these metrics were com-\nputed across responses generated to the Arena-Hard-Auto-\nv0.1 prompts with a greedy sampling technique, allowing us\nto analyze how well-quantized models preserve the meaning\nand structure of outputs compared to full-precision models\non challenging prompts while ensuring full reproducibility.\nFor visualization purposes, we normalize STS score to 0-1\nrange. Results in Figure 6 show that large quantized mod-\nels (70B and 405B) maintain a high degree of similarity to\ntheir full-precision counterparts, with an average ROUGE-1\nscore of 0.7 and ROUGE-L score of 0.56, indicating strong\npreservation of word choice and structure. Additionally,\nan average BERTScore of 0.93 and STS score of 0.96 con-\nfirm that the overall meaning is consistent, despite minor\ntoken variations from quantization. While the 8B models\nexhibit slightly more variability in word choice, reflected\nin lower ROUGE-1 and ROUGE-L scores of 0.62 and 0.46,\nrespectively, they still preserve core semantic meaning, as\nevidenced by the high BERTScore of 0.92 and STS score of\n0.95. These findings demonstrate that quantized models pro-\nduce high-quality outputs across all analyzed model sizes\nand schemes."}, {"title": "5 QUANTIZATION AND INFERENCE PERFORMANCE", "content": "LLM inference occurs in two main stages: prefill, where\nall tokens in the input prompt are processed simultaneously,\nand decode, where new tokens are generated one at a time.\nFor most input prompts, prefill is compute-bound, while\ndecode tends to be memory-bound.\nWeight quantization primarily enhances performance in the\ndecode stage by reducing memory load times. Conversely,\nquantizing both weights and activations allows matrix mul-\nt"}]}