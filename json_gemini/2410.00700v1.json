{"title": "MINING YOUR Own SECRETS: DIFFUSION CLASSIFIER SCORES FOR CONTINUAL PERSONALIZATION OF TEXT-TO-IMAGE DIFFUSION MODELS", "authors": ["Saurav Jha", "Shiqi Yang", "Masato Ishii", "Mengjie Zhao", "Christian Simon", "Jehanzeb Mirza", "Dong Gong", "Lina Yao", "Shusuke Takahashi", "Yuki Mitsufuji"], "abstract": "Personalized text-to-image diffusion models have grown popular for their ability to efficiently acquire a new concept from user-defined text descriptions and a few images. However, in the real world, a user may wish to personalize a model on multiple concepts but one at a time, with no access to the data from previous concepts due to storage/privacy concerns. When faced with this continual learning (CL) setup, most personalization methods fail to find a balance between acquiring new concepts and retaining previous ones a challenge that continual personalization (CP) aims to solve. Inspired by the successful CL methods that rely on class-specific information for regularization, we resort to the inherent class-conditioned density estimates, also known as diffusion classifier (DC) scores, for CP of text-to-image diffusion models. Namely, we propose using DC scores for regularizing the parameter-space and function-space of text-to-image diffusion models, to achieve continual personalization. Using several diverse evaluation setups, datasets, and metrics, we show that our proposed regularization-based CP methods outperform the state-of-the-art C-LoRA, and other baselines. Finally, by operating in the replay-free CL setup and on low-rank adapters, our method incurs zero storage and parameter overhead, respectively, over the state-of-the-art.", "sections": [{"title": "1 INTRODUCTION", "content": "With their photorealistic generation quality and text-guided steerability, text-to-image diffusion models (Saharia et al., 2022; Rombach et al., 2022) have emerged as one of the most flourishing areas in the computer vision community. This has led to their deployment across diverse domains involving the generation of audio/video/3D content, and has, in turn, seen a boost in their commercial value. Despite achieving extraordinary performance, these models typically demand a huge amount of training resources and data. A practical user-centric personalization of these, e.g., using limited data and compute, thus calls for efficient finetuning methods (Kumari et al., 2023; Gal et al., 2023). However, the existing finetuning methods perform poorly on a common real-world scenario, where a model needs to be personalized on sequentially arriving concepts, while being able to generate high-quality images for the previously acquired concepts."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are score-based generative models that learn to reverse a gradual noising process. Given an observation $x_0 \\in \\mathbb{R}^d$ drawn independently from an underlying data distribution $q(x_0)$, they approximate $q(x_0)$ with a variational distribution $p_\\theta(x_0)$, where $\\theta$ is the learnable parameter of the diffusion model $\\epsilon_\\theta$. To achieve this, a forward process corrupts $x_0$ into increasingly noisy latent variables $x_1, ..., x_T$ using Gaussian conditional distributions $\\prod_{t=1}^{T}q(x_t|x_{t-1})$ with a time-dependent variance schedule $\\beta_t$. A reverse process then learns $p_\\theta$ by starting from $\\mathcal{N}(x_T; 0, I)$ and predicting the gradually decreasing noise at each step (Song & Ermon, 2019; 2020). Although, in general, the shape of the posterior $q(x_{t-1}|x_{t})$ is unknown, when $\\beta_t \\rightarrow 0$, it converges to a Gaussian (Sohl-Dickstein et al., 2015). Hence, by setting $\\alpha_t = 1 - \\beta_t$, $q(x_{t-1}|x_{t})$ can be approximated by modelling the mean $\\mu_\\theta$ and the variance $\\Sigma_\\theta$ of $p_\\theta$:\n$p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T}p_\\theta(x_{t-1}|x_{t})$, where $p_\\theta(x_{t-1}|x_{t}) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_{t}, t), \\Sigma_\\theta(x_{t}, t))$. The"}, {"title": "3 METHOD", "content": "In this section, we propose adapting the existing parameter-space and function-space regularization frameworks into our continual personalization setup with LoRA. For each framework, we propose incorporating the class-specific information from DC scores to enrich their regularization. Next, we brief our general CL setup structured to accommodate these frameworks. We then discuss the limitation of C-LoRA that keeps it from being our choice for parameter-space regularization method.\nHow do we structure our CL framework for DC scores? Using DC scores directly while acquiring new concepts can incur significant additional training cost (over single forward pass) given the need for several class-conditional forward passes per training image (Eq. 3). Instead, following Custom Diffusion (Kumari et al., 2023), we learn the nth concept with a new word vector $V^*$ and a LORA layer by optimizing the diffusion loss (Eq. 1), the prior regularization loss using a common prior concept $c^0$, and additionally a parameter-regularization loss in case of paremeter-space con-solidation. After training, we freeze the word vector, and plug DC scores into two relatively shorter consolidation phases, one for each regularization method. Fig. 2 shows that these phases can work on their own as well as in tandem. Note that we train only one LoRA per task. After consolidation, the nth task LoRA serves two purposes: (a) handling inference-time queries for ${1, 2, . . ., n}$ tasks, (b) sequentially initializing the (n + 1)th task LoRA. Next, we detail on each consolidation phase."}, {"title": "3.1 DC SCORES FOR PARAMETER-SPACE CONSOLIDATION", "content": "Limitation of C-LoRA. Despite being a relevant parameter-space consolidation candidate, C-LoRA has been shown to exhibit a loss of plasticity as the self-regularization penalty $L_{forget}$ (Eq. 2) increases on longer task sequences (Smith et al., 2024a). Here, we find that $L_{forget}$ allows for a more general degeneracy where any learning on new tasks pushes the LORA weight values toward zero. This not only effects the plasticity but also the stability of C-LoRA, right from the first incremental task (n = 2), i.e., when $L_{forget}$ first comes into effect. We also find that $L_{forget}$ has particularly catastrophic consequences for the first task concept (n = 1), where the LoRA weights are learned without any forgetting constraint (see Fig. 1a). This is shown in Fig. 1b, where for task 2, $L_{forget}$ decreases throughout training, thus losing most of the information learned for task 1. While imposing a sparsity constraint on the function space of the task 1 LoRA parameters might look plausible at first, we observe that this additional penalty at best delays the degeneracy rather than resolving it (see App. Fig. 8).\nIn light of the above, we instead opt for Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) as our method for parameter-space regularization. While training on nth task, EWC selectively penalizes the change of parameters $\\theta_{n-1} \\rightarrow \\theta_n$ based on their"}, {"title": "3.2 DC SCORES FOR FUNCTION-SPACE CONSOLIDATION", "content": "As EWC only targets the LoRA parameter values, to fully exploit the information from DC scores, we consider distilling the old LoRA knowledge through function-space consolidation. The intuition behind this (see App. fig. 9) is to guide the diffusion model for generating images that exhibit traits of a conditioned class (Cywi\u0144ski et al., 2024). For our replay-free CL setup, we use the nth task images to distill the nth task LoRA by matching the predictions of a previous task LoRA conditioned on the corresponding previous class of the latter. This involves tackling two intertwined CL chal-lenges: (a) alleviating previous concepts' forgetting, and (b) merging the knowledge of old/current LoRAs. To this end, we turn to the Deep Model Consolidation (DMC) framework (Zhang et al., 2020) that uses double distillation to consolidate a student model based on two teachers: the new nth task model, and the previous (n - 1)th task model (Fig. 3b). Given that our distillation uses diffusion (denoising/DC) scores, we dub our DMC adaptation as Diffusion Scores Consolidation (DSC).\nHow do we adapt DMC to our DSC framework? DMC relies on an external dataset that is chosen to be different from the training data to prevent the consolidation bias towards old or new tasks."}, {"title": "4 EXPERIMENTS", "content": "Baselines. We compare our method with three recent customization methods: Textual Inversion (TI) (Gal et al., 2023), Custom Diffusion (CD) (Kumari et al., 2023), and C-LoRA (Smith et al., 2024b). For CD, we use the best performing variant of Kumari et al. (2023) that trains separate KV parameters per task and then merges them into a single model using a constrained optimization objective; CD EWC uses EWC (Kirkpatrick et al., 2017) with a sequentially trained variant of CD. LORA sequential trains a LoRA adapter (Hu et al., 2022) for the KV parameters of the CD model in a sequential manner. LoRA merge fuses all the LoRAs with equal weights (Ilharco et al., 2023).\nImplementation. We use the Stable Diffusion v1.4 (Rombach et al., 2022) as our backbone based on the Diffusers library (von Platen et al., 2022). Following CD (Kumari et al., 2023), we train all the models for 1000 iterations on all but the Celeb-A setup, where we use 2000 training iterations to capture more fine-grained facial attributes (Smith et al., 2024b). In favor of zero-shot generalization, we finetune our hyperparameters only on the six task sequence of Custom Concept. For both EWC and DSC, we set the number of consolidation iterations to 1/5th of that of the training iterations number. For computing DC scores, the temperature $\u03c4$ is set to 1.0 for all but the teacher LoRA in DSC where we set $\u03c4$ to 0.05. The cardinality of $c_k$ for EWC is set to 3. The DSC loss weights $\u03b3$ and $\u03bb$ are set to 0.1 and 1.5, respectively. We detail on implementation and hyperparameters in App. H.\nEvaluation. For each concept, we use DDPM sampling with 50 inference steps to generate 400 images using the prompt \"a photo of a V*\", where V is the modifier token learned for the ith task (Kumari et al., 2023). For CD and TI, we additionally include the concept name after the modifier to-ken. We encode the generated and the target (real) images using CLIP image encoder (Radford et al.,"}, {"title": "4.1 RESULTS", "content": "Continual Personalization Of Custom Concepts. We evaluate our method on the Custom Concept dataset (Kumari et al., 2023) that comprises diverse categories such as plushies, wearables, and toys (see App. D). We randomly sample six such concepts to form six tasks. Figure 4a shows the samples generated for tasks 1, 3, and 6 after training on all 6 tasks: the upper row compares the baselines while the lower row compares our variants. We report the quantitative results in Table 1. Marked by its high backward transfer scores, we find that CD (sequential) is prone to forgetting the category attributes such as the color, background, and appearance while also struggling with plasticity loss that leads to the generation of incorrect backgrounds for task 6. While Textual Inversion Gal et al. (2023) has zero forgetting (due to frozen backbone), it remains poor at capturing the custom cate-gory. LoRA (sequential) undergoes catastrophic forgetting of past tasks' concepts (it has the highest"}, {"title": "4.2 LONG TASK SEQUENCE", "content": "We study the scalability of our proposed methods to a sequence of 50 concepts cho-sen at random from the Custom Concept dataset (Kumari et al., 2023), with variable number of training images per concept. To avoid any learning bias from large early tasks with more training images, we pick all 50 tasks at random rather than adding them over our six tasks sequence. Fig. 5 compares the results of C-LoRA, LoRA EWC, and Ours (EWC DC). Similar to Smith et al. (2024b), we find that the performance of C-LoRA saturates as the number of tasks grow. Instead, applying EWC on the LoRA parameters emerges as a better performer on the long run. EWC with DC retains the performance particularly on the latter (> 35) tasks (see App. fig. 14 for qualitative comparison)."}, {"title": "4.3 ABLATION STUDIES", "content": "We ablate the influence of including DC scores into our training objective. We list two sanity checks to ensure that our framework leverages DC scores. We discuss the impact of the number of concepts used for computing DC scores, and leave the rest of the hyperparameter ablations in App. H.\nSanity check I: DC scores reduce the uncertainty in FIM estimation. We perform top-5 Eigenvalue analysis for the FIM computed with and without DC scores. Fig. 6 shows that DC scores helps cap-ture larger eigenvalues for the same LoRA param-eter. Intuitively, this means that the parameters are more strongly informed by the data regarding the directions of high likelihood changes. We leave the analyses of further layers in App. fig. 18.\nSanity check II: DC scores help with training set classification. For DC scores to help enhance the generative quality of tasks, their classification infor-mation needs to be reliable, i.e., consolidation with wrong classification scores should interfere with the generation results. To validate this, we probe the classification accuracy of different methods on train-ing data of incremental tasks, after training on all six tasks of Custom Concept. As shown in Fig. 7, con-solidating with DC scores endows us with classifica-tion gains on the overall training data.\nImpact of the number of concepts k for DC scores computation. We study how the number of randomly sampled previous task concepts effects the performance of EWC DC (see App. fig. 16). We notice that excluding the common prior concept $c^0$ in DC scores computation, i.e., k = 2, leads to the worst performance overall. All setups with k > 2 perform similar until task 2 as there is only one available previous concept. From task 3 onward, k = 3 still gets to sample only one previous concept per iteration while k = 7 can use all previous concepts at each step. We find that the performance of EWC DC saturates as k increases beyond 5. This is because not all previous concepts carry useful discriminative information for reliable DC scores. We use k = 5 throughout.\nTraining time complexity. App. table 6 shows that our proposed consolidation methods scale linearly in the training sample size whereas C-LORA scales bilinearly in the training sample size and the number of tasks. This implies that while on shorter task sequences, the average runtime per training iteration of our methods remains higher than C-LoRA (5.3s for EWC, 5.7s for DSC, 0.8s for C-LoRA on 6 tasks) given the several conditional forward passes needed for DC scores"}, {"title": "5 CONCLUSION", "content": "In this paper, we propose continual personalization of pretrained text-to-image diffusion models us-ing their inherent class-conditional density estimates, i.e., Diffusion classifier (DC) scores. Namely, we alleviate forgetting using DC scores as regularizers for parameter-space and function-space con-solidation. We design practical considerations for efficiently deriving the DC scores during training. We show the superior performance of our methods through extensive quantitative and qualitative analyses across diverse CL task lengths. Additionally, we show the compatibility of our method for the parameter-efficient VeRA (Kopiczko et al., 2024) and for multi-concept generation (Kumari et al., 2023). We hope that our work paves the general way for leveraging DC scores in personaliza-tion of pretrained conditional diffusion models."}, {"title": "A PERSONALIZATION IN TEXT-TO-IMAGE DIFFUSION MODELS", "content": "Personalization of a text-to-image diffusion model aims to embed a new concept into the model by steering the reverse process through a mapping from the textual embedding $\u03a6(c)$ to the distribution of the latent image features x, where \u03a6 is the text encoder. To do so, the text-to-image cross-attention blocks in the U-Net consider the query $Q = W_Qx$, the key $K = W_K\u03a6(c)$, the value $V = W_V\u03a6(c)$, and perform the weighted sum operation: $softmax(\\frac{QKT}{\\sqrt{d'}})V$, where the weights $W_Q, W_K$, and $W_V$ map the input x and c to Q, K, and V, respectively, and d' is the output dimension. Custom diffusion (Kumari et al., 2023) perform parameter-efficient personalization with the goal of acquiring multiple concepts given only a few examples. They show that upon finetuning on a new concept, the text-projection weights $W_K, W_V$ of the text-to-image cross-attention blocks in the U-Net undergo the highest rate of changes. Subsequently, they finetune only the cross-attention weights $W = [W_K, W_V]$ together with regularization, rare token embedding initialization, and constrained weight merging. C-LoRA builds upon this parameter-efficient setup and further proposes training low rank adaptrs (LoRA) (Hu et al., 2022) for the cross-attention layers in the U-Net. Subsequently, we consider using LoRA as well."}, {"title": "B C-LORA WITH SPARSITY CONSTRAINT ON TASK-1 LORA PARAMETERS", "content": "Figure 8: C-LoRA with sparsity constraint on the first task: To overcome the catastrophic for-getting of first task in C-LoRA (Smith et al., 2024b) (see Sec. 3.1), we consider restricting LoRA weight updates during first task by incorporating a sparsity (L1 norm) constraint into the training objective for the first task. However, as shown in Fig. 8a and 8b, this merely results in the degener-ate solution shifted by one task, i.e., now the task 2 weights (instead of task 1) undergo significant updates, which in turn causes Lforget for task 3 to decrease throughout training (as most of the task 2 spots get edited). Fig. 8c shows the results generated by this model for the first four tasks on our Custom Concept setup. Compared to the results of C-LoRA in Fig. 8d, now even task 2 image generation (for the pet cat concept) is seen to exhibit catastrophic forgetting."}, {"title": "C DIFFUSION SCORES CONSOLIDATION (DSC) FOR FUNCTION-SPACE REGULARIZATION", "content": "Figure 9: Motivation behind function-space consolidation: conditioning current task images (wearable sunglasses) on previous classes (those around the circumference) helps generate images that share features with the previous classes (Cywi\u0144ski et al., 2024). In the absence of replay sam-ples (real images) from previous personalization tasks, we exploit the aforesaid property using the current (nth) task images to distill the current task LoRA (finetuned on wearable sunglasses) by matching the predictions of the LoRA corresponding to the previous tasks on their respective previ-ous task concepts. Images have been resized to highlight the subject of interest. The real images for previous concepts have been provided for the sake of reference.\nFigure 10: Design choices and their results for our DSC framework (from left to right): (A) the ground truth target images for tasks 1, 3, and 6 of our Custom Concept CL setup; (B) generated results for our proposed DSC EWC DC framework, as also reported in Fig. 4a; (C) generated results for the DSC EWC DC framework where we follow DMC (Zhang et al., 2020) to initialize our student LoRA using random weights: the consolidated student fails to properly acquire the previous and current task custom categories; (D) generated results for the DSC EWC DC framework where we follow DMC (Zhang et al., 2020) to use the (n - 1)th task LoRA as our fixed second teacher, rather than randomly sampling the second teacher from the pool of all previous task LoRAs: the consolidated student undergoes catastrophic forgetting of previous concepts."}, {"title": "D DATASETS AND THEIR CONCEPTS", "content": "Four our Custom Concept setup, we select the following 6 classes from the CustomConcept101 dataset (Kumari et al., 2023) with at least nine images each: furniture sofal, plushie panda, plushie tortoise, garden, transport car 1, and wearable sunglasses 1.\nFor our Google Landmarks v2 (Weyand et al., 2020) setup, we select 10 such geographically diverse waterfall landmarks and download 20 images for each. These landmarks (and their countries)"}, {"title": "E METRICS DEFINITION", "content": "Following Smith et al. (2024b), we report (i) Nparam Train as the percentage of parameters (with respect to the U-Net backbone) that are trainable while learning a task and (ii) Nparam Store as the percentage of parameters that are stored over the entire task sequence. Let N be the number of personalization tasks, where each task $j \u2208 {1,2, . . ., N}$ comprises a dataset Dj hosting a single personal concept. Let $X_{i,j}$ be the generated images for the jth task by a model trained sequentially until the ith task, and $X_{D,j}$ be the corresponding original dataset images for the jth task. Then, using a pretrained CLIP model $F_{clip}$ (Radford et al., 2021b) as the feature extractor, we define (iii) the average of the maximum mean discrepancy (MMD) $A_{MMD}$ metric (where lower is better) over N tasks as:\n$A_{MMD} = \\frac{1}{N}\u2211_{j=1}^{N} MMD(F_{clip}(X_{D,j}), F_{clip}(X_{N,j}))$\nwhere the MMD is computed using a quadratic kernel function (Gretton et al., 2012). Accordingly, (iv) the forgetting metric $F_{MMD}$ (where lower is better) quantifies how much the generated images have diverged due to sequential training:\n$F_{MMD} = \\frac{1}{N-1}\u2211_{j=1}^{N-1} MMD(F_{clip}(X_{j,j}), F_{clip}(X_{N,j}))$"}, {"title": "F MAIN RESULTS (CONTINUED)", "content": "We study the taskwise performance ($A_{MMD}$ and KID) evolution of the compared methods on our CL setups of Custom Concept and Landmarks. Fig. 12 and 13 show that overall, our DC score-based variants perform better against their non-DC score-based counterparts as well as against other baselines on every incremental task."}, {"title": "G ADDITIONAL RESULTS", "content": "G.1 COMPATIBILITY WITH VERA\nIn the spirit of parameter-efficient continual personalization, we explore the effectiveness of our method for Vector-based Random Matrix Adaptation (VeRA) (Kopiczko et al., 2024). VeRA freezes the LoRA weight matrices $A$ and $B$ to share them across all network layers, and instead adapts two scaling vectors As and Ad per layer. This helps VeRA retain the performance of LoRA-based finetuning with a small fraction. For the U-Net, this amounts to a \u2248 11th reduction in the number of trainable parameters (Nparam Train) per task. We rely on the Diffusers library implementation of VeRA and use the default rank setup of 256. Fig. 17a compares the results of sequential VeRA, VeRA with EWC, and VeRA EWC with DC scores on our six task sequence of Custom Concept (Kumari et al., 2023). Here, VeRA sequential suffers from a loss of plasticity, e.g., sunglasses with three glasses, besides forgetting the precise details of previous tasks, e.g., distorted tortoise face/eyes in task 3. VeRA EWC helps improve over this despite struggling to retain knowledge at times, e.g.,"}, {"title": "H IMPLEMENTATION AND HYPERPARAMETERS", "content": "H.1 IMPLEMENTATION DETAILS\nWe use the Hugging Face Accelerate library (Gugger et al., 2022) for distributed training/inference of our models. Our experiments are conducted using four RTX A6000 GPUs with 48 GB memory each. For all the compared methods, we set the batch size to 1 during training and inference. To allow for larger effective batch sizes during training, we set the gradient accumulation steps to 8. For a fair comparison with CD, we perform regularization during training using an auxiliary dataset of 200 images generated by the pretrained backbone using the prompt \"a photo of a person\" (Smith"}, {"title": "H.2 IMPACT OF HYPERPARAMETERS", "content": "H.2.1 NUMBER OF CONSOLIDATION ITERATIONS\nWe tune the number of consolidation iterations for EWC phase using our EWC DC variant and that for DSC phase using our EWC DSC DC variant. The number of consolidation iterations are fractions of the total training iterations, i.e., 1000 on Custom Concept, and are chosen from the set: ${0.1x, 0.2x, 0.3\u00d7,0.5\u00d7, 1\u00d7}$. As shown in Fig. 19 and 20, a value of 0.2\u00d7 the training iterations performs the best for both EWC DC and EWC DSC DC. While a larger number of iterations can lead to degradation in the generative quality of both the variants, we note that DSC remains more sensitive overall to the number of consolidation iterations."}, {"title": "H.2.2 HYPERPARAMETER FOR EWC LOSS", "content": "Fig. 21 shows the impact of varying the hyperparameter $\u03b4$ controlling the contribution of the cross-entropy term for EWC DC (Eq. 5). We find the range [0.5, 1.0] to be suitable for the loss weightage, and use $\u03b4 = 0.5$ through our experiments."}, {"title": "H.2.3 HYPERPARAMETERS FOR DSC LOSS", "content": "Effect of varying \u03b3: For DSC, \u03b3 depicts the strength with which the student model $\u03b8_s$ follows the DC scores distribution of the nth task teacher $\u03b8_n$ and the previous task teacher $\u03b8_j$. As shown in Fig. 22, the range [0.01, 0.1] remains suitable for \u03b3. Accordingly, we set \u03b3 to 0.1.\nEffect of varying $\u03bb$: $\u03bb$ in DSC guides the strength with which the student $\u03b8_s$ matches the noise estimations of the nth task teacher $\u03b8_n$ and the previous task teacher $\u03b8_j$. Setting $\u03bb = 0$ leaves the student consolidation to be guided solely by the discriminative DC scores, a setting that we find to be detrimental for the purpose of generation (see Fig. 11). In Fig. 23, we delve further into the impact of varying $\u03bb$ on the performance of DSC EWC DC, and find the range [0.5, 1.0] to work well. Note that a high $\u03bb$ can lead the student to overfitting the noise estimations for the previous task teacher, for which the current task inputs remain out-of-domain. This, in turn, harms the generative quality of the student.\nEffect of varying the teacher's softmax temperature \u03c4: Knowledge distillation frameworks typi-cally rely on sharp target distributions that intuitively mimic the outputs of a confident teacher model (Caron et al., 2021). For our DSC framework, the teachers can be made to produce sharper targets by using a low value for the temperature \u03c4 in the softmax normalization operation of their DC scores. We show the impact of varying the teacher softmax temperature in Fig. 24. Namely, a temperature beyond 0.1 leads to softer targets from both the teachers, which can be harder to mimic for the stu-dent. On the contrary, \u03c4 = 0 mimics extreme sharpening and produces one-hot hard distributions. We find a temperature of 0.05 to perform the best overall."}, {"title": "I TRAINING TIME COMPLEXITY ANALYSIS", "content": "We use the soft-O notation $\u00d5$ (Van Rooij et al., 2019) to describe the time complexity while ignoring the logarithmic factors. Formally, for some constant k, $\u00d5(f(n)) = O(f(n) * log^k(n))$ provides the upper bound for f, like the standard big-O notation O but hides the factors involving powers of logarithms, i.e., $\u00d5(n)$ could represent O(n log n), O(n log log n), O(n log\u00b2 n), etc.\nAs also stated in the main paper, we consider a continual personalization setup with N number of tasks such that each task comprises on new concept to acquire. For the ease of computation, we assume that each task has a fixed number of training images, |D|. Note that for our CL setup, we use the same number of training epochs for each task. This lets us ignore the factor of training epochs in deriving the training time complexity. Lastly, since both C-LoRA (Smith et al., 2024b) and our setup train a single LoRA and a modifier token per task, their complexity of a forward pass remains the same, and can be safely ignored. Put together, we can state time complexity as a function that grows linearly with more training samples |D|. We list the training time complexities of the compared methods in Table 6 and detail their derivation below:\n1. C-LoRA (Smith et al., 2024b) performs self-regularization using the weights of all previ-ous task LORA (see Eq. 2). Therefore, in addition to the training sample size, the time complexity of C-LoRA is dependent on the number of tasks N, i.e., $\u00d5(N|D|)$.\n2. For parameter-space consolidation, we rely on online EWC (Schwarz et al., 2018) which maintains a single set of FIM weights that are updated continuously using a running average over tasks. This ensures that our EWC-based framework does not store separate importance weights for each task, and hence, the time complexity scales linearly in the factor of sample size, $\u00d5(|D|)$. Next, for DC scores computation, we assume a fixed number of conditional forward passes that is proportional to the size of the relevant concept set $c_k$ (see Sec. 3.1). Irrespective of the number of tasks, $c_k$ always stores m + 2 number of concepts, where m is chosen through grid search and is typically a low number for avoiding confusion from other uninformative classes. Hence, DC scores computation for EWC scales linearly with the number of training samples $\u00d5(|D|)$. Put together, the time complexity for our parameter-space consolidation framework is: $\u00d5(|D|) + \u00d5(|D|) = \u00d5(|D|)$.\n3. For function-space consolidation, we rely on a double-distillation framework, which uses two teacher and one student LoRA per consolidation iteration, irrespective of the number of seen tasks. Subsequently, the training time complexity of function-space consolidation remains $\u00d5(|D|)$. For computing DC scores, we always rely on three conditional forward passes through each of the teachers and the student. As described in Sec. 3.2, these forward passes correspond to the readily available common prior concept $c^0$, and the concepts $c^n$ and $c_{i0) leads to softer targets from both the teachers, which can be harder to mimic for the stu-dent. On the contrary, \u03c4 = 0 mimics extreme sharpening and produces one-hot hard distributions. We find a temperature of 0.05 to perform the best overall."}, {"title": "J FAILURE CASES", "content": "Figure 25: Failure cases showing the visual artefacts of our best performing variant EWC DSC DC on our three different dataset setups.\nDespite our method retaining significantly better task-specific generation granularity compared to the state-of-the-art, it produces noticeable visual artefacts sometimes. Fig. 25 shows few such dataset-specific artefacts for EWC DSC DC, which is our overall best performing variant leveraging DC scores with EWC and DSC. Notably, for Custom Concept, the model at times generates figures that have out-of-proportion shapes including an absence of the plushie panda's body (left), an un-naturally big head for the plushie tortoise (middle), and a poorly outlined frame for the wearable sunglasses (right). For the waterfall landmarks setup, we notice multiple incomplete rainbows (left), a transparent yet poorly formed bridge over the river (middle), and a mulberry colored waterfall fore-ground (right). Similarly, on the textual inversion setup, the generated clock image has incorrectly printed numers (left, with 11 replacing 1 and 3 being confused with 9), the teapot with incorrectly assigned spout/handles (middle), and the elephant's body with holes that have unnaturally filled background."}]}