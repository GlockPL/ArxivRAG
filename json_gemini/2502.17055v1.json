{"title": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam", "authors": ["Tianjin Huang", "Haotian Hu", "Zhenyu Zhang", "Gaojie Jin", "Xiang Li", "Li Shen", "Tianlong Chen", "Lu Liu", "Qingsong Wen", "Zhangyang Wang", "Shiwei Liu"], "abstract": "This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featuring momentum reset and spike-aware gradient clipping, achieves the best performance across various bit levels, but struggles to stabilize gradient norms, requiring careful learning rate tuning. To address these limitations, we propose Stable-SPAM, which incorporates enhanced gradient normalization and clipping techniques. In particular, Stable-SPAM (1) adaptively updates the clipping threshold for spiked gradients by tracking their historical maxima; (2) normalizes the entire gradient matrix based on its historical \\(l_2\\)-norm statistics; and (3) inherits momentum reset from SPAM to periodically reset the first and second moments of Adam, mitigating the accumulation of spiked gradients. Extensive experiments show that Stable-SPAM effectively stabilizes gradient norms in 4-bit LLM training, delivering superior performance compared to Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM outperforms the BF16 LLaMA-1B trained with Adam by up to 2 perplexity. Furthermore, when both models are trained in 4-bit, Stable-SPAM achieves the same loss as Adam while requiring only about half the training steps.", "sections": [{"title": "1. Introduction", "content": "Recently, several advanced optimizers have been proposed, claiming to either outperform the widely used Adam optimizer or achieve comparable performance at reduced costs in the context of Large Language Models (LLMs). Given the massive size of LLMs, reducing the memory footprint of Adam has become a key objective in this line of research (Shazeer & Stern, 2018; Chen et al., 2024; Zhang et al., 2024a; Zhao et al., 2024a; Zhang et al., 2024b; Ma et al., 2024). Another area of focus is addressing the challenges of instability in LLM training. For instance, Huang et al. (2025) proposed SPAM which incorporates momentum reset and spike-aware gradient clip (SpikeClip) to mitigate the adverse effects of loss spikes. Zhao et al. (2024b) studied the stability of various optimizers to hyperparameters with BF16. These optimizers are predominantly evaluated using the standard BF16 precision, which is a practical option for real-world LLM training (Touvron et al., 2023; Li et al., 2023). With the growing shift toward low-bit precisions such as FP8 and FP4 in LLMs due to their significant cost-saving potential (Liu et al., 2024; Lee et al., 2024; Peng et al., 2023; Xi et al., 2023), it is crucial to investigate whether their effectiveness persists under lower-bit precisions. For the newly proposed optimizers to be economical, their training with low-bit precisions should be similarly robust to hyperparameter choice as trained using higher precision.\nThis paper provides a comprehensive evaluation of the effectiveness and robustness of learning rate choices across various recent optimizers, including Adam (Kingma, 2014), Adafactor (Shazeer & Stern, 2018), Adam-mini (Zhang et al., 2024a), and SPAM (Huang et al., 2025), when training with 4-bit weights and activations. Our study reveals several key observations:\n* All evaluated optimizers exhibit increased sensitivity to learning rate choices during 4-bit training, often diverging quickly when larger learning rates are used as shown in Figure 2.\n* SPAM consistently achieves the lowest evaluation loss"}, {"title": "2. 4-bit Training Stability Investigation", "content": "Recent studies (Zhao et al., 2024b; Wortsman et al., 2023b; Huang et al., 2025; Takase et al., 2023; Wortsman et al., 2023b) have investigated stability challenges in large language model (LLM) training, including issues such as learning rate instability, gradient spikes, and loss spikes. In this section, we extend the evaluation by analyzing the stability of various optimization algorithms under a 4-bit LLM training setting. Following the experimental setup outlined in (Wortsman et al., 2023b; Zhao et al., 2024b), we evaluate the final performance using a range of learning rates from 1e-4 to 3e-3. This evaluation includes two widely used optimizers, Adam (Kingma, 2014) and Adafactor (Shazeer & Stern, 2018), as well as two recently proposed methods, Adam-mini (Zhang et al., 2024a) and SPAM (Huang et al., 2025). Additionally, we monitor both the global gradient norm and training loss throughout the 4-bit LLM training process. The global gradient norm is defined as follows:\n\\(\\sqrt{\\sum_{i=1}^{N} ||g_i||_2^2}\\) where N is the number of layers in model and \\(g_i\\) denotes the gradient of i-th layer. The experiments are conducted on the LLaMA-130M/350M models using the C4 dataset and showed in Figure 2 and Figure 4. We observe:\n* Lower-bit training exhibits reduced learning rate stability. As illustrated in Figure 2, the final evaluation loss for 4-bit training increases significantly with larger learning rates, whereas BF16 training exhibits a more stable performance across different learning rates. This indicates that 4-bit training is more sensitive and less stable in terms of learning rate."}, {"title": "3. Stable-SPAM", "content": "To address the training instability in 4-bit LLM training, we propose Stable-SPAM, a stabilized spike-aware Adam optimizer. Apart from the momentum reset inherited from the original SPAM, Stable-SPAM introduces two techniques: Adaptive Gradient Norm (AdaGN) and Adaptive Spike-Aware Clipping (AdaClip), which we will explain in detail. The pseudocode is provided in Appendix C.\nAdaptive Gradient Norm (AdaGN). As we can observe in Figures 4 and 3, spikes in training loss and instances of training divergence usually align with abrupt surges in the gradient norm, consistent with findings in (Takase et al., 2023; Huang et al., 2025). To address these training instabilities, we propose AdaGN, a method that stabilizes gradients by adaptively scaling them based on their historical \\(l_2\\) norm statistics. To better track the dynamics of the gradient norm during training, we leverage the idea of Adam by maintaining moving averages of both the first and second moments of the gradient norm. Concretely, we compute and update the moving averages of the gradient norm (\\(M_{norm}\\), \\(U_{norm}\\)), then use them to derive a normalized gradient:\n\\[g_{norm} = ||g_t||_2,\\]\n\\[M_{norm} = \\gamma_1 \\cdot M_{norm} + (1 - \\gamma_1) \\cdot g_{norm},\\]\n\\[U_{norm} = \\gamma_2 \\cdot U_{norm} + (1 - \\gamma_2) \\cdot g_{norm}^2,\\]\n\\[\\hat{g}_t = \\frac{g_t}{\\frac{M_{norm}}{\\sqrt{U_{norm} + \\epsilon}}},\\]\nwhere \\(\\hat{g}_t\\) is the normalized gradient, \\(\\gamma_1\\) and \\(\\gamma_2\\) are momentum coefficients and \\(\\epsilon\\) is small constant for numerical stability. By rescaling \\(g_t\\) with a ratio of its historical mean norm \\(M_{norm}\\) to the square root of its historical second moment \\(\\sqrt{U_{norm}}\\, AdaGN mitigates abrupt gradient norm spikes. Note that as the gradient norm \\(g_{norm}\\) is essentially a scalar for an entire layer, the additional parameter overhead introduced by AdaGN is negligible, i.e., two extra parameters per layer.\nAdaptive Spike-Aware Clipping (AdaClip). Different from the spike gradient clipping technique in (Huang et al., 2025), which sets a fixed clipping threshold, we propose an adaptive clipping approach, i.e., AdaClip. The core idea is to dynamically adjust the clipping threshold by tracking the maximum gradient magnitude observed over time, rather than relying on a pre-defined fixed value. Concretely, let \\(g_t\\) be the gradient at time step t. We first compute \\(g_{max}\\), the maximum absolute gradient value across all parameters. Then, we update the threshold \\(T_{threshold}\\) with an exponential moving average that incorporates \\(g_{max}\\). Finally, any entries of \\(g_t\\) that exceed \\(T_{threshold}\\) are rescaled to maintain stability. The procedure is formally expressed as follows:\n\\[g_{max} = max(|g_t[i]|),\\\\]\n\\[T_{threshold} = \\gamma_3 \\cdot T_{threshold} + (1 - \\gamma_3) \\cdot g_{max},\\\\]\n\\[Mask_{spikes} = (g_t > T_{threshold}),\\]\n\\[g_t[Mask_{spikes}] = \\frac{g_t[Mask_{spikes}]}{g_{max}} \\times T_{threshold},\\]\nwhere \\(\\gamma_3 \\in [0, 1]\\) controls the weight of the moving average. When \\(\\gamma_3\\) is large, \\(T_{threshold}\\) responds more slowly to new gradient maxima, leading to more stable updates. When \\(\\gamma_3\\) is small, it adapts more quickly to sharp changes in gradient magnitude.\nMomentum Reset (MoRet). Following Huang et al. (2025), we adopt momentum reset (MoRet) to periodically reset the accumulated first and second moments in Adam. The effectiveness of MoRet lies in addressing the negative effects of gradient spikes, which can inflate the first and second moments of Adam. Since Adam uses exponential moving averages to track their historical information, these inflated values caused by spiked gradients can have prolonged detrimental effects (Huang et al., 2025) on moments. By resetting the momentum terms at fixed intervals (\\(\\Delta T\\)), MoRet mitigates the lasting influence of unusually large gradients, enabling more stable and consistent optimization."}, {"title": "4. Experiments", "content": "To demonstrate the efficacy of the proposed Stable-SPAM, we conduct extensive experiments with various sizes of the LLaMA model on the C4 dataset.\nBaselines. We adopt five popular optimizers as our baselines including Adam (Kingma, 2014), Adafactor (Shazeer & Stern, 2018), Lion (Chen et al., 2024), Adam-mini (Zhang et al., 2024a) and SPAM (Huang et al., 2025). Among these, Adam and Adafactor are well-established and widely used, while Adam-mini and SPAM have been introduced more recently. Besides, we also include gradient clipping (Goodfellow, 2016) (GradClip) in conjunction with Adam as an additional baseline.\nExperimental Setup. Following (Lialin et al., 2023; Zhao et al., 2024a), we train LLaMA-based architectures ranging from 60M to 1B parameters. Each architecture is configured with RMSNorm (Shazeer, 2020) and SwiGLU activations (Zhang & Sennrich, 2019). For every model size, we keep the same set of hyperparameters across methods and vary only the learning rate. Specifically, we sweep over learning rates from \\(1 \\times 10^{-4}\\) to \\(1 \\times 10^{-3}\\), incrementing by \\(2\\times10^{-4}\\) for each optimizer. Following the settings in (Takase et al., 2023; Huang et al., 2025), we set the threshold to 1 for the GradClip baseline. For Adafactor, we adopt the hyperparameters from the original paper (Shazeer & Stern, 2018), where \\(\\epsilon_1 = 10^{-30}\\), \\(\\epsilon_2 = 10^{-3}\\), and d = 1.0. The hyperparameters for SPAM are configured based on the settings in (Huang et al., 2025), with reset intervals set to 500, learning rate warmup steps to 150, and the GSS threshold to 5000. For Stable-SPAM, we set \\(\\gamma_1 = 0.7\\), \\(\\gamma_2 = 0.9\\) and \\(\\gamma_3 = 0.999\\) for 4-bit LLM training and \\(\\gamma_1 = 0.85\\), \\(\\gamma_2 = 0.9999\\) and \\(\\gamma_3 = 0.999\\) for BF16 training. Detailed descriptions of our task setups and hyperparameters are provided in the Appendix A."}, {"title": "4.1. Performence of 4-bit LLM Training", "content": "To evaluate the performance of Stable-SPAM in 4-bit LLM training, we conduct experiments using both FP4 ("}, {"title": "4.2. Performence of Extremely Low-Precision Training", "content": "To evaluate the performance of Stable-SPAM under extremely low-precision training, we conducted experiments on LLaMA-350M using A2W2 (INT2), A3W3 (INT3), and A4W4 (INT4) configurations. The final validation loss is presented in Figure 5. The results indicate that Stable-SPAM consistently outperforms Adam across all low-precision settings and even matches the performance of BF16-Adam under INT3 training."}, {"title": "4.3. Performence of BF16 LLM Training", "content": "To further evaluate the efficacy of Stable-SPAM, we conducted experiments on various LLaMA model sizes using standard BF16 training. The experiments are based on C4 dataset. The training curves and final perplexity values are presented in Figure 6 and Table 2, respectively. Table 2 highlights that Stable-SPAM consistently delivers superior performance across different model sizes, surpassing the second-best optimizer with significant improvements. Furthermore, Figure 6 illustrates that Stable-SPAM achieves the same performance as Adam in only half the training steps or even fewer for LLaMA-350M and LLaMA-1B, validating its ability to match Adam's performance while requiring significantly fewer tokens under BF16 LLM training. The above results demonstrate that the promise of Stable-SPAM not only holds for low-precision LLM training but also holds for the standard BF16 training."}, {"title": "4.4. Integration with Other Optimizers", "content": "Although AdaGN and AdaClip are proposed specifically for Stable-SPAM, one may wonder, \"Can AdaGN and AdaClip also be compatible with other optimizers?\" To answer this question, we applied AdaGN and AdaClip to"}, {"title": "4.5. Effect on Stabilizing Training", "content": "To validate the effectiveness of our proposed AdaGN and AdaClip techniques in stabilizing the LLM training process, Firstly, we compared the training loss and gradient norm curves across three settings: using Adam alone, using Adam with AdaGN, and using Adam with both AdaGN and AdaClip. Our experiments employed LLaMA-130M with a learning rate of 3e-3 under an FP4 training setting. As"}, {"title": "4.6. Ablation Study", "content": "To validate the effectiveness of the three components, MoRet, AdaGN, and AdaClip, in Stable-SPAM, we conduct a comprehensive ablation study. Specifically, we take two approaches: (1) We iteratively incorporate MoRet, AdaGN, and AdaClip into the Adam optimizer to measure their individual and combined improvements under both FP4 and BF16 training settings. (2) We replace AdaClip with SpikeClip (Huang et al., 2025) and AdaGN with GradClip (Goodfellow, 2016) to further assess the unique contributions of our proposed components. The results, summarized in Table 4, reveal the following observations:\n* MoRet consistently improves performance across both FP4 and BF16 settings. * Under both FP4 training, AdaGN alone shows limited improvement. However, when combined with AdaClip, it substantially reduces final perplexity. Conversely, in the BF16 setting, AdaGN alone yields considerable performance gains, but adding AdaClip offers limited improvement. This discrepancy may stem from the higher frequency of extreme element-wise gradient"}, {"title": "4.7. Hyper-Parameter Analysis", "content": "Stable-SPAM introduces four hyperparameters: \\(\\gamma_1\\), \\(\\gamma_2\\), \\(\\gamma_3\\), and \\(\\Delta T\\), which extend the functionality of Adam. Among these, \\(\\gamma_1\\) and \\(\\gamma_2\\) serve a similar purpose to \\(\\beta_1\\) and \\(\\beta_2\\) in Adam, controlling the smoothness of updates to the first moment \\(M_{norm}\\) and the second moment \\(U_{norm}\\). Larger values of \\(\\gamma_1\\) and \\(\\gamma_2\\) result in smoother updates, placing greater emphasis on historical gradient norm statistics when adapting the current gradient norm. Similarly, \\(\\gamma_3\\) plays a role in determining the threshold for identifying gradient spikes. A larger \\(\\gamma_3\\) leads to a smoother and more conservative threshold, resulting in a higher proportion of gradients being classified as spike gradients. To investigate the impact of these hyperparameters, we plot the final perplexity curve while varying \\(\\gamma_1\\) from 0.5 to 0.9, \\(\\gamma_2\\) from 0.8 to 0.999, \\(\\gamma_3\\) from 0.9 to 0.999, and \\(\\Delta T\\) from 250 to 5000. The experiments are conducted using LLaMA-60M, trained on 1.1B C4 to-"}, {"title": "5. Related Work", "content": "Instability of Training Large Language Models. The instability of large language model (LLM) training, which are marked by loss spikes and catastrophic divergence(Chowdhery et al., 2023; Molybog et al., 2023), has driven extensive research into stabilization techniques. These methods generally fall into three main categories: (1) gradient preprocessing, (2) architectural modifications, and (3) initialization strategies. Gradient preprocessing typically involves scaling and clipping gradients at the start of the optimization process to stabilize the training. A well-known example is gradient clipping (Goodfellow, 2016), which globally rescales the gradient norm to a fixed value. Later, Adafactor (Shazeer & Stern, 2018) introduced capping the norm of the parameter updates instead of the raw gradients. More recently, SPAM (Huang et al., 2025) proposed detecting and clipping anomalous gradients based on historical gradient statistics. However, a common drawback of these methods is that they require manually setting a predefined threshold. Architecturally, Xiong et al. (2020) showed that Post-LayerNorm (Post-LN) amplifies gradients, causing instability with large learning rates, while Pre-LayerNorm (Pre-LN) preserves gradient norms for stable training. Embed LayerNorm (Embed LN) normalizes embeddings(Dettmers et al., 2021), though it may impact performance(Scao et al., 2022), while Embed Detach(Ding et al., 2021; Zeng et al., 2022) reduces loss spikes by truncating gradients. DeepNorm(Wang et al., 2024) scales residual connections to stabilize ultra-deep models, and Reparam(Zhai et al., 2023) prevents attention entropy collapse via spectral-normalized parameterization. Initialization strategies offer complementary stability benefits. Scaled Embed(Takase et al., 2023) stabilizes LayerNorm gradients, while Scaled Initialization(Nguyen & Salazar, 2019) reduces variance using \\(\\mathcal{N}(0, \\sqrt{\\frac{2}{(5d)/\\sqrt{2N}}})\\). Fixup(Zhang et al., 2019; Huang et al., 2020) eliminates LayerNorm entirely, inspiring norm-free architectures. Though ongoing advancements refine these approaches, training stability remains a key challenge in LLM development.\nLow-precision LLM Training. Low-precision training (Wang et al., 2018; Lin et al., 2022; Xi et al., 2024a;b; Wortsman et al., 2023a) has emerged as a promising approach to improve both computational and memory efficiency during training. Among these methods, FP16 (Micikevicius et al., 2017) and BF16 (Kalamkar et al., 2019) are the most widely adopted precision formats. To push the efficiency further, 8-bit training has garnered increasing attention. For instance, LM-FP8 (Peng et al., 2023) enables training with FP8 precision While (Fishman et al., 2024) demonstrates that as training scales up (larger than 250B tokens), the issue of activation outliers becomes more pronounced, posing challenges to the representation range of low-bit data formats. To address this challenge, (Fishman et al., 2024) proposes a smoothing strategy, while (Ashkboos et al., 2025) leverages Hadamard transformations to mitigate the impact of activation outliers. Furthermore, the choice of data format significantly influences training performance. The INT8 format is the most widely supported low-precision format, whereas FP8, available in NVIDIA's Hopper GPU architecture, provides specialized support. Additionally, the MX format (Rouhani et al., 2023) demonstrates superior representational capability, though it is rarely supported by current hardware. In this work, we investigate the training instability associated with low-precision training and propose enhancements through the design of optimizers. Our approach is compatible with existing techniques, providing a complementary solution to improve the stability of low-precision training."}, {"title": "6. Conclusion", "content": "This paper presents a comprehensive study on the training instability challenges of 4-bit quantization in large language models. We find that while low-precision training significantly reduces memory and computational costs, it also amplifies the sensitivity to learning rates, and increases the likelihood of gradient and loss spikes. To address these issues, we propose Stable-SPAM, an optimizer that combines three key techniques: AdaClip, AdaGN, and MoRet. Empirical results on LLaMA models of various sizes demonstrate that Stable-SPAM not only stabilizes 4-bit training but also achieves better performance compared to existing optimizers, sometimes even surpassing BF16 performance. We additionally show that these stabilization strategies are broadly applicable, benefiting other optimizers like Lion and Adam-mini."}, {"title": "A. Architecture and Hyperparameters", "content": "We introduce details of the LLaMA architecture and hyperparameters used for 4-bit and BF16 pre-training, following Lialin et al. (2023); Zhao et al. (2024a). Table 5 shows the most hyperparameters of LLaMA models across model sizes. We use a max sequence length of 256 for all models, with a batch size of 512, with a batch size of 131K tokens. For all experiments, we adopt learning rate warmup of 2000 training steps, and use cosine annealing for the learning rate schedule, decaying to 10% of the initial learning rate.\nFor all methods across each model size (from 60M to 1B), we tune the learning rates from \\(1e-4\\) to \\(1e-3\\) with an increasing step of \\(2 \\times 10^{-4}\\) for pre-training tasks, and the best learning rate is selected based on the validation perplexity. The detailed hyperparameter of Stable-SPAM on 4-bit training and BF16 training are reported in Table 6 and Table 7."}, {"title": "B. Time Series Forescasting Task", "content": "We conducted additional experiments on time-series prediction tasks. In these experiments, we intentionally introduced anomalous data with a probability A=10% to simulate gradient anomalies. Experiments are conducted with 10 repeated runs on Weather time series data\u00b2 using PatchTST (Nie et al., 2022) model.\nThe findings demonstrate that as the severity of anomalous data increases, Stable-SPAM's performance advantage over Adam becomes more pronounced. Besides, Stable-SPAM consistently surpasses SPAM across all settings. These results further highlight the effectiveness of the proposed Stable-SPAM."}, {"title": "C. Pseudocode", "content": "The pseudocode is presented in Alogrithm 1."}]}