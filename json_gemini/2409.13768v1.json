{"title": "Magika: AI-Powered Content-Type Detection", "authors": ["Yanick Fratantonio", "Luca Invernizzi", "Loua Farah", "Kurt Thomas", "Marina Zhang", "Ange Albertini", "Francois Galilee", "Giancarlo Metitieri", "Julien Cretin", "Alex Petit-Bianco", "David Tao", "Elie Bursztein"], "abstract": "The task of content-type detection-which entails identifying the data encoded in an arbitrary byte sequence-is critical for operating systems, development, reverse engineering environments, and a variety of security applications. In this paper, we introduce MAGIKA, a novel AI-powered content-type detection tool. Under the hood, MAGIKA employs a deep learning model that can execute on a single CPU with just 1MB of memory to store the model's weights. We show that MAGIKA achieves an average F1 score of 99% across over a hundred content types and a test set of more than 1M files, outperforming all existing content-type detection tools today. In order to foster adoption and improvements, we open source MAGIKA under an Apache 2 license on GitHub and will make our model and training pipeline publicly available. Our tool has already seen adoption by the Gmail email provider for attachment scanning, and it has been integrated with VirusTotal to aid with malware analysis. We note that this paper discusses the first iteration of MAGIKA, and a more recent version already supports more than 200 content types. The interested reader can see the latest development on the MAGIKA GitHub repository, available at github.com/google/magika.", "sections": [{"title": "I. INTRODUCTION", "content": "Content-type detection is a fundamental computing task that identifies the data encoded in an arbitrary byte sequence. This allows an application to distinguish source code (C++, Python, etc.), media (PDF, JPG, etc.), binaries (EXE, ELF), and a variety of other file formats. As such, content-type detection impacts a wide range of downstream use-cases including software development tools, security tools, browsers, and media players. For example, development environments (e.g., Visual Studio Code, VS Code for short) rely on content-type detection to decide which syntax highlighters and plugins to use. Security applications rely on content-type detection for policy enforcement (e.g., email providers prohibiting executable attachments), for forensic analysis and recovery, and for routing samples to the most capable content-type-specific threat analysis scanners (e.g., VirusTotal and other anti-viruses have specialized scanners for binaries, scripts, or PDFs; these scanners are too resource-intensive to be run on all samples). The need for content-type detection stems from byte sequences lacking an intrinsic, trustworthy indicator of their underlying file format. While some approaches to built-in indicators exist\u2014such as file extensions, MIME types, magic bytes, and metadata-they can easily be omitted when a file's content is copied or transmitted, or otherwise spoofed (e.g., to evade security systems that prohibit certain content types). The task of content-type detection was first tackled with the file command in Bell Labs UNIX over five decades ago [1]. Since then, developers have created a variety of tools, the vast majority of which rely on manually-written signatures (e.g., rules or regular expressions) that are updated as new file types and versions emerge. Examples include the latest version of file [2] (powered by libmagic [3]); exiftool [4]; and trid [5]. Unfortunately, signature-based approaches are frail: a single whitespace change (or other subtle byte sequence modification) can result in inaccurate content-type detection as shown in Figure 1. Beyond signatures, guesslang [6] employs a simple TensorFlow text model to distinguish source code content types, which is integrated into VS Code [7]. In this paper, we discuss the design and implementation of a novel AI-powered content-type detection tool called MAGIKA. Built using a deep learning model, MAGIKA automatically infers the content type of a byte sequence without any reliance on human expertise with respect to the intricacies of different file formats. The model takes as input three sequences of 512 bytes drawn from the beginning, middle, and end of a file's content; automatically identifies patterns unique to content"}, {"title": "II. RELATED WORK", "content": "Content-type detection is a well-studied, longstanding problem. Before presenting our new approach, we discuss both traditional approaches used by existing tools as well as research proposals to integrate machine learning into content-type detection."}, {"title": "A. Traditional approaches to content-type detection", "content": "The conventional approach to content-type detection relies on manually-crafted signatures (e.g., regular expressions). The most prominent tool employing signatures is file [2], which is regarded as the default command-line utility for detecting content types. file provides support for binary file formats (which are particularly amenable to a signatures-based approach), including highly-specialized formats (e.g., file formats employed in specific video games for storing saved games, textures, etc.). file also possesses a range of signatures for textual content types (e.g., C, Java, Python). Anecdotal evidence suggests that file exhibits higher accuracy for binary formats than textual ones, but its performance has not been systematically evaluated prior to our study. By default, file outputs a textual description of the detected content type, and a number of additional metadata. For example, when processing a PDF file, file will output its content type: PDF, but also the PDF version number, how many pages it has, and so forth. Since this output is challenging to parse programmatically, file also supports outputting a MIME type. MIME types are easier to parse and more codified than human descriptions, and thus, it may be more appropriate when integrating file in automated pipelines. However, dealing with MIME types has its set of challenges, which we document in Section IV. Other popular tools include trid [5] and exiftool [4]. The latter was originally designed to detect image content types exclusively, but it has since expanded its scope. Apart from these general purpose content-type detection tools, there are a number of specialized tools that trade breadth for depth on a specific domain. For instance, PEiD [13] is a tool dedicated to detecting PE packers, whereas Detect-It-Easy [14] facilitates fine-grained inspection of PE files and other executable file formats. We omit these special-purpose tools from our comparative analysis due to their limited support of the content types we evaluate with MAGIKA. Lastly, numerous libraries re-implement a subset of existing features to offer content-type detection to various programming languages, often aiming to be free of dependencies to simplify integration. Examples include: mime-types [15], a library specializing in a limited number of content types, particularly for JavaScript clients; PolyFile [16], a pure-Python implementation of libmagic. filetype [17], a GoLang library for file type identification. filetype.py [18], a Python library for file type identification; and file-type [19], another (binary) file format detection tool for the JavaScript ecosystem. Given the derivative nature of many of these tools, we omit them from our comparative evaluation as well."}, {"title": "B. Machine-learning approaches to content-type detection", "content": "Recently, researchers have proposed a number of approaches that leverage machine learning to replace manual signatures. Sceadan [20] proposes using support vector machines to model features based on unigram and bigram frequencies. Fitzgerald et al. [21] suggest natural language processing techniques for file fragment classification, whereas Wang et al. [22] propose using sparse coding and unsupervised learning to classify file fragments in the context of memory forensics. More recent approaches employ appropriately state-of-the-art techniques such as recurrent and convolutional neural networks [23], [24]. These proposals have not found wide adoption, possibly due to their relatively low overall accuracy (ranging from 53% to 84%), limited support for different content types (between 18 and 75, primarily binary file formats), and competition from existing tools, which already provide adequate support for binary file formats. As such, we treat them as out of scope for our comparative evaluation. The most recent and successful approach is guesslang [6], an open-source tool that detects programming languages from a snippet of source code."}, {"title": "III. DATASET", "content": "We curate a novel dataset of 26M files drawn from a diverse set of 113 content types to act both as a benchmark for evaluating existing content-type detectors and to train our deep learning content-type detector. We describe the origin of this data, our validation steps, and limitations with our approach.\nDataset sources. In real-world settings, the distribution of content types varies from environment to environment. For example, social media uploads are dominated by videos and images, whereas digital-signing platforms predominantly receive PDFs. Rather than tailor our dataset to any single environment, we incorporate a sample of content types that may be present across a variety of environments including source code, executables, documents, media, archives, and more. We build a stratified dataset, in which every type has equal representation. When reporting performance metrics throughout this work, we break our results down per content type to account for this sampling strategy. In practice, any deployment environment can estimate the efficacy of MAGIKA by taking a weighted average of our reported results, with weighting taking into account the frequency of content types within a specific setting. To gather our stratified samples, we identified GitHub [9] (a popular source code development platform) and VirusTotal [10] (a popular binary and file scanning platform) as two promising sources, as each covers a complementary set of content types: GitHub includes mostly text-based files such as source code (e.g., C/C++, Java, Python), configuration files (e.g., JSON, YAML, INI), and a variety of text files for documentation (e.g., Markdown, RTF). Conversely, VirusTotal includes file archives (e.g., ZIP, TAR), binaries (e.g., EXE, DLL, ELF), as well as documents (e.g., PDF, DOC, XLS). Content types. We selected prevalent content types based on available public metrics. Using a public mirror of GitHub designed for large-scale analysis [28] and VirusTotal's public API [29], we first calculate, for each, the top 50 most prominent file extensions across both platforms. We then identified the associated content type, if any, for each of these file extensions. Given the potential biases of our data sources, due to which potentially relevant content types do not appear in the most-frequent file extensions, we augment this list by consulting existing resources [30]\u2013[32] and adding file"}, {"title": "Sampling & validating content types", "content": "For each content type, we query GitHub and VirusTotal to obtain a random sample of files associated with the content type's file extensions. To minimize the risk of mislabeled content types in our ground truth, we perform a number of validation checks before adding a sample to our dataset. We specifically avoid using existing content-type detection tools as part of our validation, otherwise, we might oversimplify content-type detection (in the case of requiring agreement across all tools); or propagate detection errors (in the case of trusting one tool above others). We use four heuristics for validation: file size, magic bytes (for binary files), character encoding (for text files), and file trustworthiness. For file size, we require any sample in our dataset to consist of at least 16 bytes. For magic bytes, we apply a set of necessary but not sufficient rules to validate a file extension. Note that these rules are, by design, straightforward, as we would otherwise risk the introduction of biases in our dataset. For example, all PEBIN (the main Microsoft Windows executable format) must start with the string MZ (0x4D 0x5A). Not all files starting with MZ are PEBIN (e.g., they could be textual files which happened to start with the characters MZ), but if a sample's file extension claims to be an EXE or DLL, we verify this condition holds. For text files\u2014where checks on magic bytes are not applicable\u2014we merely verify the encoding to ensure the file contains only text characters. Finally, for samples from VirusTotal, we ensure that no anti-virus engines flags the sample as malicious due to such samples having an increased risk of obfuscated content types. Independent of these workflows, we also create synthetic samples for two content types: UNKNOWN and TXT. For UNKNOWN, each sample consists of a random byte sequence of arbitrary length. For TXT, each sample consists of a random string of text characters of arbitrary length. These synthetic samples are used as a form of data augmentation to train the model. We do so to help the model handle files with content types it has not been trained on, following best practices: rather than forcing the model to choose 1 of N valid content types, it can select UNKNOWN. We empirically observed that this approach has a negligible impact on accuracy on our dataset (<0.05% accuracy improvement). Nevertheless, in adherence to best practices, we adopt this technique as it does not introduce any disadvantages. The interested reader can find all the details about the entire dataset creation process in our open source release. Final dataset. Our final dataset consists of 26.5 million samples. We randomly split these into a training, validation, and testing dataset. The latter serves as a uniform benchmark for all content-type detection tools, including our deep learning model, but is never exposed to our model during training, nor has it been used to select the model hyperparameters or thresholds (for which we used the validation split, following best practices). In total, we selected 10K samples per content type for our testing benchmark (with more than 1M samples in total, making our testing dataset large enough to obtain robust evaluation metrics), 10K samples for model validation, and 10K+ samples for model training (capping at 1 million per content type). There are two exceptions to this: for ISO and ODP content types, we have only 14K samples total. As we show later, this does not have a material difference on our deep learning detection accuracy. In total, our testing benchmark consists of 1.2 million samples; our training dataset consists of 24 million samples; and our validation dataset consists of 1.2 million samples. Limitations. As with any measurement or machine learning study, our methodology incurs a number of limitations. First, our dataset consists of only 113 content types. We acknowledge the selected list is necessarily incomplete and may not encompass content types relevant to all deployment environments. While there is a long tail of many other content types in use today, acquiring and validating a representative sample is prohibitively expensive-in terms of manual overhead. As such, we focus on selecting a diverse and large enough number of content types to prove the generalizability of our deep learning approach on the most popular content types, leaving the extension of our technique to a more comprehensive set of content types to future work with the open source community. Second, despite our best efforts at validation and sampling, our benchmark dataset may contain mislabeled content types or be"}, {"title": "IV. BENCHMARKING EXISTING TOOLS", "content": "We benchmark the performance of existing content-type detection tools to motivate the need for more robust detection. Our measurements rely on the 1.2M samples in our test benchmark dataset. The dataset includes samples of various sizes, as shown in Figure 3. As part of our evaluation, we assess both the number of content types supported per tool (overlapping with the types in our benchmark) and the accuracy of predictions per tool. As we show, the best existing tool achieves an F1 score of only 88%, with no single tool supporting all of the content types we evaluate."}, {"title": "A. Tools Selection", "content": "Our benchmark evaluates four popular content-type detection tools used for a variety of applications:\n\u2022 file [2]: the default command line tool for detecting content types for a variety of files. We note that file can be optionally queried to return the MIME type, which we also benchmark as the two modes can yield distinct results (denoted file-mime).\n\u2022 exiftool [4]: a tool originally developed for detecting image content types, but that has since expanded to a variety of binary and text content types.\n\u2022 trid [5]: a tool for detecting a wide range of content types.\n\u2022 guesslang [6]: a tool for detecting the programming language based on an excerpt of source code.\nWe select the first three because they represent the status-quo: they are based on well-established signatures-based approaches, readily maintained, and are widely adopted. We select guesslang as a representative of emerging content type detection tools based on machine learning. guesslang"}, {"title": "B. Metric selection", "content": "For each tool, our benchmark assesses the precision and recall of inferred content types compared to the golden labels of our test dataset. Given this is a multi-class classification problem, we estimate per-type precision as: TP/(TP + FP). Here, a true positive (TP) indicates the tool predicts the correct golden label, while a false positive (FP) indicates the tool predicts a specific content type, but in fact the golden label is any other content type. We calculate per-type recall as: TP/(TP + FN). Here, a false negative (FN) means a tool failed to detect a sample as the golden canonical content type, predicting any other label. For ease of presentation, we simplify these metrics into an F1 score, which provides equal weight to both per-type precision and recall:\n$F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\nWe calculate this F1 score per content type and then average F1 scores across text content types, binary content types, and overall content types to provide a variety of assessments on how well existing tools perform."}, {"title": "C. Automating Large-Scale Evaluations", "content": "The text string outputs of existing content type detection tools are not designed for cross comparison. As such, we develop a harness to programmatically query millions of samples while normalizing the outputs to match one of the 113 canonical content types in our benchmark."}, {"title": "Normalizing detected content types", "content": "Unfortunately, there are no canonical naming conventions for content types across tools or even the same version of the same tool. Nor is there a hierarchy of how content types relate. For example, valid command line outputs of file for XML documents include XML document, XML 1.0 document, and XML 1.0 document text, among others. We also find that different versions of the same tool will silently update naming conventions. For example, file silently changed the output for JavaScript files from \u201cNode.js script text executable\" to \"Node.js script executable,\" dropping the \u201ctext\u201d keyword, and potentially breaking automated workflows [33]. Similarly, we find that the MIME types generated by tools, despite being machine-oriented, also do not follow canonical naming conventions, for several reasons: MIME types change over time (e.g., text/x-markdown VS. text/markdown [34]); tools (silently) update their output (e.g., for PEBIN, file 5.41 outputs app/x-dosexec, while file 5.44 outputs app/vnd.microsoft.portable-executable);"}, {"title": "D. Results", "content": "We report a high-level summary of our benchmark results for each existing tool in Table II. For space reasons, we share per-content type results for only a sample of the 113 content types we benchmark in Table III. Full results can be found at [35]. Overall, if we restrict our view to content types supported by each tool, we find that file-mime achieves the best performance with an average F1 score of 88%-with exiftool and trid scoring just 1% lower. For text exclusively, guesslang achieves the best performance with an average F1 score of 77%. For binaries exclusively, file-mime achieves the best performance with an average F1 score of 96%. As we demonstrate shortly, MAGIKA is able to outperform all of these tools, achieving an overall average F1 score of 99%. We explore the performance of each existing tool in detail below. file and file-mime. We find that file is highly perfor-mant for supported binary content types, with an average F1"}, {"title": "V. MAGIKA", "content": "In light of the limitations of existing content-type detectors, we design and train a new deep learning classifier called MAGIKA that distinguishes between content types without the need for signatures created by experts."}, {"title": "A. Requirements", "content": "We design MAGIKA to support two distinct deployment scenarios with a single model: 1) a command line tool that can replace established utilities, and 2) a bulk inference tool that can scale to analyzing billions of samples per day. These scenarios constrain how we approach accuracy, speed, and resource utilization."}, {"title": "B. Model Architecture", "content": "We present the architecture of the deep learning model that powers MAGIKA in Figure 4. The overall flow consists of (1) transforming the contents of a file into a fixed-sized vector representation; (2) processing the vectors via a neural network; and (3) interpreting the model outputs to predict the most probable content type.\nInputs. The model's inputs consists of three vectors that encode a sequence of 512 bytes selected from the beginning, middle (taken from the center of the file, with no randomness), and end of an input file. We encode each byte as an integer in the range [0, 255]. To maximize the utility of the inputs made available to the model, we strip any leading or trailing whitespace from the beginning and end of a file's contents before selecting bytes for encoding. As the model's input has constant size (i.e., 3x512 integers), if an input file is too small, we pad the encoding with a special character (represented by the integer 256). We then concatenate the three input vectors to form a single vector of 3x512 integers, which we one-hot encode and embed into a 128-float vector using a Dense layer. While we considered using longer sequences, or more sequences of bytes from a file, in practice this incurs additional resource requirements and slows down processing due to having to seek over more portions of the input file (e.g., files > 100MB). By using a fixed size input rather than a whole file, we ensure inference is constant time. Despite limiting the model's visibility, we demonstrate we can still achieve robust accuracy.\nTrunk. The model trunk consists of two components. First, the output of the previous Dense layer is reshaped to 384x512 dimensions, effectively reorganizing the previous output into small, four-byte chunks. Each chunk is represented by a 512 dimension vector that the model can treat as a single entity instead of having to consider each byte separately, which we found to improve both performance and efficiency. Then, the model contains two 256-dimensional Dense layers with gelu activation [38]. A global max pooling layer [39] performs downsampling and reduces the dimensionality back to 1D. During training, we apply layer normalization [40], a 10% dropout rate [41], and a 10% of spatial dropout rate [42] for regularization throughout the model. This final model represents hundreds of design iterations across architectures that are simple enough to be small and fast on a CPU. We also ran extensive hyperparameter tuning experiments (using the validation dataset) and tested various model configurations that evaluated the embedding dimension; the size of the Reshape layer; the number, size, and activation of the Dense layers; the normalization type; and the amount of dropout applied before converging on this specific design. Outputs. The final Dense layer uses softmax activation [43] with size equal to the number of canonical content types in our dataset. The output vector represents a probability distribution over each of the potential content types. We can select the most likely output with an argmax function [44], or apply individual confidence thresholds per content type, ultimately yielding the single most likely content type among those that meet a minimum confidence threshold."}, {"title": "C. Training", "content": "We trained the MAGIKA model using Categorical Cross-Entropy loss which is suited for multi-class classification settings, with Adam as the optimizer (batch size = 256, learning rate = 0.001, \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999, and e = 1e-07). Additionally, we use CutMix [45] for data augmentation, although our experiments did not highlight a significant boost in validation accuracy. However, since CutMix is used only during training and does not affect model inference, we continue using a 5% rate for CutMix because it has no downsides"}, {"title": "D. Setting Confidence Thresholds", "content": "A classification threshold is the cut-off point whereby we treat the probability output from our final Dense layer as suitably high-confidence to yield a content-type prediction. Selecting an optimal threshold involves balancing the trade-offs between precision and recall, which is made more complicated by a multi-class setting. For example, we empirically found that a probability value of 0.80 was robust to assert that a file is HTML, whereas the same threshold for PDF yielded poor accuracy. We approached this problem by first considering a single threshold for all content types. Figure 7 contains the precision-recall curve for this scenario. The curve shows that while there is a trade-off between precision and recall, we can configure the model to obtain a precision and a recall that are both higher than 99.5%. As a further optimization, we instead compute per-content-type thresholds. We determine each threshold by fixing precision at 99% and then choosing the maximum recall of the remaining thresholds. At inference time, we then select the argmax of content type predictions that exceed the type's custom threshold. If the model yields no confident outputs, we output either TXT or UNKNOWN, depending on the nature of the content type the model was the most confident with. We discuss the overall performance of this approach shortly. In practice, clients using MAGIKA can set their own confidence thresholds, but by supplying tuned defaults we avoid a pain point encountered by users of other tools [36], [37]."}, {"title": "E. Performance Optimizations", "content": "Per our design requirements, MAGIKA must also be per-formant in terms of the time it takes to load the model into"}, {"title": "VI. EVALUATION", "content": "In order to demonstrate the value of MAGIKA, we bench-mark it against existing tools in terms of accuracy and speed."}, {"title": "A. Accuracy", "content": "We report a high-level summary of our benchmark results for MAGIKA in Table IV. The reported accuracy metrics have been computed using the testing dataset, which, following best practices, has not been used for any step involving the creation or tuning of MAGIKA. As a source of further insights, we share per content type results for a sample of the 113 content types in our benchmark in Table V. Full results can be found in our source repository [35]. Regardless the context-be it text, binary, or overall content types-we find that MAGIKA outperforms all existing tools. This demonstrates the generalizability of our model architecture and training approach."}, {"title": "Accuracy gains", "content": "For binary content types, we find that MAGIKA yields a modest F1 gain of 4% when compared to the best existing tool-file-mime-limited to content types supported by both MAGIKA and file-mime. This F1 gain extends to 9% if we consider all content types in our benchmark. For text content types, MAGIKA yields even better F1 gains of 22% over guesslang, limited to content types supported by both MAGIKA and guesslang. This extends to 47% for all content types in our benchmark. This is especially true of MARKDOWN (45% gain), VBA (49% gain), and XML (35% gain). Likewise, MAGIKA is better than all models at exhibiting uncertainty, with an F1 score of 94% for UNKNOWN (synthetic random byte sequences) and 84% for TXT (synthetic random text strings). We also note that MAGIKA has high accuracy (and often shows a double-digit % gain in F1) even for content types that were shown to be problematic for guesslang's integration in VS Code, such as BATCH, CSV, INI, MAKEFILE, SQL, and YAML [37]. No need for preprocessing. We find that MAGIKA is able to handle various forms of packing and compression. For example, DOC, XLS, PPT are all variants of the same Composite Document File binary format. Likewise, DOCX, XSLX, PPTX, APK, and JAR are all instances of Zip file formats. Most existing tools deal with these content types by first unpacking the file before applying any signature-based rules. Conversely, MAGIKA requires no pre-processing or domain knowledge to yield accurate predictions. Accuracy vs. sample size. Figure 8 shows how the accuracy of MAGIKA and existing tools change depending on the samples size. The results show how MAGIKA achieves a stable 99%+ accuracy for most samples, with a slight degrade in accuracy for very small samples of less than ~100 bytes; upon manual inspection, we find that this loss of accuracy is due to MAGIKA outputting a generic content type (such as TXT or UNKNOWN) due to lack of strong confidence. It is also interesting to see how the average accuracy of existing tools is also somewhat stable across samples size, but with much higher variance."}, {"title": "Limitations", "content": "Our evaluation is affected by a few limitations. First, our dataset is balanced across content types: this is standard practice (especially when there is no one reference real-world distribution), as it allows one to determine how each content type is supported regardless of whether they are rare in practice. This makes the results more transparent and easier to inspect (e.g., when using an imbalanced dataset, a model with poor support for a rare content type would still have a very high average accuracy). However, we acknowledge that the reported accuracy metrics are not directly applicable, but given that MAGIKA is open-source, a user with a very unique setting can tune and evaluate MAGIKA to its needs, for example by mapping our results to her distribution\u2014this is the standard way to adapt models to an imbalanced environment. The second limitation is that, as it is common with works based on deep learning on large datasets, we did not per-form cross-validation, as it is computationally prohibitively expensive (as reported, a single training run takes about one week). However, cross-validation is deemed as critical only when dealing with very small datasets [50], in which a specific \u201cpick\u201d of the testing dataset may be accidentally very biased; this is not a concern in our situation, in which the testing set alone has more than 1M samples. We also performed an additional experiment in which we split our testing dataset in 10 random smaller sets: the average accuracy is 99.19% \u00b1 0.03%, showing very low variance, which supports the robustness of our evaluation."}, {"title": "B. Speed", "content": "Per our design requirements, the time it takes to infer a content type is equally important to accuracy. There are two relevant dimensions to processing latency: initialization (e.g., loading a signature database or model) and inference (e.g., via matching signatures or running a model). For MAGIKA we can measure the breakdown between these two aspects directly, but it is challenging for the other tools. Thus, we measure the breakdown indirectly, by measuring the time required to"}, {"title": "VII. REAL-WORLD ADOPTION", "content": "We recently released the MAGIKA model and a command line wrapper as open source under an Apache 2 license [11]. We share information on its reception as well as real-world deployments that now use MAGIKA."}, {"title": "GitHub release", "content": "We reached 4K stars on GitHub in less than a week and currently total more than 7.7K stars. MAGIKA was featured in GitHub trending projects and we have received more than 100 open issues and pull requests from external contributors. Likewise, a number of developers have reached out to integrate MAGIKA into their workflows, such as for validating datasets, or for enhancing security malware analysis pipelines (e.g., AssemblyLine [53]). MAGIKA's Python package currently averages over 3K downloads per day."}, {"title": "Email attachment scanner", "content": "We worked with a large email provider, Gmail, to integrate MAGIKA into their attachment scanner that detects and blocks malicious attachments [12]. This scanner processes hundreds of billions of files every week, underscoring the computational performance of our approach. MAGIKA allows the email provider to enforce content-type policies on potentially obfuscated files (e.g., prohibiting executable binary content types as attachments). It also allows the email provider to route specific samples to anti-virus scanners based on the content type detected, which might otherwise be prohibitively expensive to run on all samples. For example, we have estimated that, on a daily basis, MAGIKA routes to MS Office-specific malware scanners several millions samples that would have otherwise not being scanned when using existing content type detection systems or file extensions. MAGIKA has now been running in production for more than six months, with no significant concerns to be reported."}, {"title": "VirusTotal file scanner", "content": "The VirusTotal service has recently integrated our work: every submission to VirusTotal is now processed with MAGIKA, whose result can be seen in the \"Details\" tab of each submission, alongside other content type detection tools. This can be used by VirusTotal for improved indexing and to optimize which files are sent to the platform's Code Insight functionality [54], which employs generative AI to analyze and detect malicious code. For ex-amples, MAGIKA's accurate detection of POWERSHELL means that VirusTotal can now isolate such files and specialize any generative analysis exclusively to POWERSHELL."}, {"title": "VS Code", "content": "We have reached out to VS Code developers and we have presented our model for consideration as a potential replacement for guesslang. During our discussions, the developers acknowledged the limitations and challenges en-countered with guesslang, expressing openness to consider alternatives. We subsequently engaged in a dialogue regarding current feature gaps, primarily focusing on the necessity for"}, {"title": "VIII. DISCUSSION", "content": "Our benchmark and integration stories highlight that content-type detection remains an important problem. We dis-cuss future design directions for further improving MAGIKA and potential risks."}, {"title": "Handling new content types", "content": "We acknowledge that the cur-rent version of MAGIKA supports a limited number of content types, and that our selected list is not necessarily the most representative in all scenarios. Moreover, new content types are constantly emerging, requiring the authors of content-type detection tools to continuously maintain and add new signatures. MAGIKA sidesteps the necessity of understanding the intricacies of different file formats (e.g., to write signatures and to ensure they do not collide with previous signatures). Instead, handling new content types with MAGIKA requires two steps: (1) sourcing a sufficiently large number of training samples; and (2) retraining the model with a new Dense output sized to the number of supported content types. We find in practice that MAGIKA can use as few as 10K samples to achieve robust accuracy for binary content types, though text content types require more samples. Sourcing files is also fairly trivial via GitHub and VirusTotal. While re-training a new model can take several days, it is not a concern as the entire process can be automated."}, {"title": "Multiple valid content types", "content": "MAGIKA currently outputs only a single inferred content type, which is enough for the vast ma-jority of files. However, one could craft so-called polyglot files, which are syntactically valid for multiple content types (e.g., interpretable as Python, PHP, and bash) [55]. Such polyglots use unspecified holes in the file format specifications, making accurate detection more challenging. More trivially, multiple valid content types might arise due to a file containing code snippets from different programming languages. In practice, users of MAGIKA may be able to examine the top N most likely content types, though we leave evaluating the accuracy of MAGIKA on multi-content-type files to future work."}, {}]}