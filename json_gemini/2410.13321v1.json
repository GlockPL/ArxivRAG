{"title": "Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding", "authors": ["Kyungmin Min", "Minbeom Kim", "Kang-il Lee", "Dongryeol Lee", "Kyomin Jung"], "abstract": "Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in generating detailed and coherent responses from visual inputs. However, they are prone to generate hallucinations due to an over-reliance on language priors. To address this issue, we investigate the language priors in LVLMs and make two key observations: (1) Even when predicting the tokens associated with image-related part-of-speech (POS), models increasingly rely on linguistic priors as the token sequences grow, thereby amplifying hallucinations. (2) Methods that directly calibrate LVLM's output distribution to mitigate language priors can lead to a degradation in text quality or even exacerbate hallucinations. Based on these findings, we propose a novel method, Summary-Guided Decoding (SGD). This method naturally encourages the model to focus more on image information by reducing the text context through summaries, while controlling only the image-related POS tokens to maintain text quality. Through experiments, we demonstrate that SGD achieves state-of-the-art performance on object hallucination benchmarks. Furthermore, in terms of the trade-off between precision and recall, SGD achieves Pareto optimality among the existing methods. Lastly, we observe that although existing methods struggle to balance the reduction of object hallucinations with maintaining text quality, SGD demonstrates robustness in handling this challenge.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (LVLMs) have shown remarkable advancements by integrating the reasoning capabilities of Large Language Models (LLMs) to interpret visual knowledge (Zhu et al., 2023; Dai et al., 2023; Liu et al., 2024c; Li et al., 2023a). Despite their significant utility, they suffer from a critical drawback known as object hallucination, where the model generate responses that contradict the visual input (Li et al., 2023b; Liu et al., 2024b). Recent studies have shown that this occurs because LVLMs rely too heavily on learned textual patterns, which referred as language priors (Zhou et al., 2024; Liu et al., 2024a; Jing et al., 2023; Lee et al., 2024a). This over-reliance on language priors tends to intensify when the model generates longer sequences or detailed descriptions (Favero et al., 2024), leading to frequent hallucinations as shown in Figure 1.\nIn this paper, we 1) conduct the fundamental analysis of language priors in LVLMs (Section 2), 2) analyze the limitations of existing methods for mitigating language priors and provide insights into potential solutions (Section 5.1), and 3) propose a novel method that effectively reduces object hallucination while preserving text quality (Section 3).\nFirst, we analyze language priors by examining the distance between the next-token probability distributions of LVLMs and LLMs, both conditioned"}, {"title": "2 Language Priors in LVLMs", "content": "In this section, we systematically analyze the causes of language priors in LVLMs. Section \u00a72.1 outlines the method for quantifying language priors. Section \u00a72.2 provides an in-depth analysis of how language priors affect LVLMs based on part-of-speech (POS) types. Section \u00a72.3 analyzes the impact of increasing token length on language priors in LVLMs. We conduct this analysis on 5,000 MSCOCO (Lin et al., 2015) image descriptions generated using LLAVA 1.5 7B (Liu et al., 2024c) (see Appendix B for more details).\n2.1 How to measure language priors in LVLMS\nIn LVLMs, language priors refer to the model's over-reliance on learned textual patterns, where responses are generated based on these patterns without fully considering the provided image. From this"}, {"title": "2.1 How to measure language priors in LVLMS", "content": "In LVLMs, language priors refer to the model's over-reliance on learned textual patterns, where responses are generated based on these patterns without fully considering the provided image. From this perspective, if the token distribution of a LVLM, which decodes using both text and images, becomes similar to that of a LLM, which relies solely on text for decoding, this could indicate an over-reliance on language priors. Here, the LLM refers to the state of the LVLM where the input image is not provided as a conditioning factor, with both models conditioned on the same text sequence.\nTherefore, we measure language priors by examining the distributional distance between the next-token probabilities of LVLMs and LLMs, as described in Favero et al. (2024). We employ Jensen-Shannon Divergence (JSD) (Lin, 1991) to quantify this distance.\nFormally, at each time step t, the next token $Y_t$ is selected as:\n$Y_t = \\arg \\max_{y \\in V} \\log p_{\\Theta}(y | I, T, y_{<t}),$\nwhere $\\Theta$ is the parameters of LVLMs, V is the vocabulary, I denotes the provided image, T represents the textual prompt (e.g., \u201cPlease describe this image in detail.\u201d), and $y_{<t}$ denotes the sequence of generated tokens up to the time step (t \u2013 1).\nWe define the distributional distance at each time step t as:\n$\\text{dist}_t = JSD (p_{\\Theta}(\\cdot | I, T, y_{<t}) || p_{\\Theta}(\\cdot | T, y_{<t}))$. \n \nA larger distance $\\text{dist}_t$ suggests that the LVLM relies more on visual information for predictions, indicating a lower dependence on language priors. Conversely, a smaller distance implies that the model is generating responses primarily based on textual patterns."}, {"title": "2.2 Analysis of language priors by Part-of-Speech (POS) type", "content": "We conduct an experiment to investigate whether LVLMs differ in their reliance on language priors based on the need for image information. Specifically, we analyze this tendency by part-of-speech (POS) type, measuring the JSD at each decoding step and averaging the JSD values for each POS type up to 32 tokens.\nAs shown in Figure 2 (a), we observe significant variation in divergence across different POS categories. POS categories such as PROPN (e.g., \"Biden\") and ADJ (e.g., \u201cred\u201d), which related to visual information, exhibit higher divergence. In contrast, language-related POS types, like PART (e.g.,"}, {"title": "2.3 Influence of Token Sequence Length on language priors", "content": "We observe that as token sequences grow longer, the model becomes increasingly dependent on language priors in Section 2.2. To explore this effect further, we conduct a detailed analysis of how varying token lengths impact LVLMs, particularly in terms of how attention is distributed between image and text tokens, and the consequent impact on object hallucination.\nFirst, we measure the attention weights assigned to image tokens and text tokens at each decoding step. As shown in Figure 3 (a), initially, LVLMs give sufficient attention to input image tokens when computing the next token. However, as the sentence grows longer, this attention becomes significantly shallower. In other words, when generating long sentences, we can observe that LVLMs tend to rely more on linguistic patterns rather than on visual information. This observation provides additional insight into our earlier findings in Section 2.2, where longer sequences were shown to reinforce the model's dependence on language priors.\nAdditionally, to assess the role of input length in hallucination, we evaluate the object hallucination ratio as a function of token length. Figure 3 (b) shows a clear correlation between input length and the likelihood of object hallucinations, indicating that longer text generation increases the chances of hallucination. We hypothesize that this phenomenon is driven by over-reliance on language priors, which amplifies hallucinations in LVLMs."}, {"title": "3 Summary-Guided Decoding", "content": "Based on insights from Section 2, we identify that an increase in input length results in greater reliance on language priors, thereby exacerbating hallucinations in LVLMs. To address this, we present Summary-Guided Decoding (SGD), a novel method for controlling the length of conditioning input during decoding. In SGD, we shorten the conditioning input by summarizing the previously generated text after each sentence completion. This process preserves the critical context from earlier outputs while keeping the input concise. The summarized text, combined with the image, serves as part of the conditioned input for generating the next sentence. This approach effectively reduces the input length, allowing the model to stay more focused on the provided image.\nUsing summarized inputs can reduce contextual information, which may cause discrepancies with the language patterns previously learned by the model. This can result in distributional shifts that weaken the model's language modeling capabilities, ultimately degrading the quality of the generated text. To address this, we preserve the original distribution for tokens related to language modeling, while using SGD to control only the image-related POS tokens. Our method is illustrated in Figure 4.\nWe introduce two variations of SGD for summary model usage. The first approach leverages the instruction-following capabilities inherent in LVLMs. By providing summary instructions directly to the LVLM, this method enables the model to perform SGD without incurring additional memory costs. However, a limitation of this approach is the increased computational burden, as the LVLM generates its summaries during the process. To address these challenges, we distill the summarization capability into a smaller, more efficient model, Flan-T5-base (Chung et al., 2022) (see Appendix C for details). This model significantly reduces computational overhead while maintaining the advantage of input length control. We report results for both SGD with Self-Summarization (SGD-S) and SGD with the Distilled-Flan-T5 model (SGD-D), highlighting the trade-offs between efficiency and performance."}, {"title": "4 Experiment", "content": "4.1 Experiment settings\nDatasets and Evaluation Metrics. We generate descriptions for 200 images from the MSCOCO 2014 validation dataset (Lin et al., 2015) prompted with \"Please describe this image in detail.\" (Huang et al., 2024). We employ the Caption Hallucination Assessment with Image Relevance (CHAIR) (Rohrbach et al., 2019) for evaluating object hallucination. CHAIR consists of two variants: CHAIR\u2081, which calculates the percentage of hallucinated objects out of all objects mentioned in the caption, and CHAIRs, which measures the percentage of captions that contain at least one hallucinated object. Additionally, to complement the precision-based CHAIR metric, we introduce a Recall metric for a more detailed assessment.\n$CHAIR_1 = \\frac{|\\{ hallucinated\\ objects \\}|}{|\\{all\\ objects\\ mentioned\\}|}$\n$CHAIR_S = \\frac{|\\{ sentences\\ with\\ hallucinated\\ object\\}|}{|\\{ all\\ sentences\\}|}$\n$Recall = \\frac{|\\{correct\\ mentioned\\ objects\\}|}{|\\{ground\\ truth\\ objects\\}|}$\nTo provide a more comprehensive assessment of hallucinations, we use the Sentence-level Hallucination Ratio (SHR) (Zhao et al., 2023), a GPT-4-based evaluation metric. This metric includes hallucinations involving object existence, relationships, and attributes. We generate descriptions for 200 images from the VG dataset (Krishna et al., 2016), using the same prompts as in the CHAIR metric. Specifically, SHR leverages GPT-4 to compare the model's responses with the manually annotated descriptions from the VG dataset, evaluating each response on a sentence-by-sentence to identify potential hallucinations accurately.\nBaseline LVLMs. In LVLMs, two prominent methods for aligning text and vision modalities are the projection layer-based approach and the learnable query-based approach (Li et al., 2023a; Zhu et al., 2023; Chen et al., 2023; Liu et al., 2023). In our experiments, we utilize representative models for each aligning method: LLAVA-1.5 (7B/13B) (Liu et al., 2024c) and InstructBLIP (7B/13B) (Dai et al., 2023).\nBaseline Decoding Methods. We include various decoding methods as baseline approaches in"}, {"title": "4.1 Experiment settings", "content": "Datasets and Evaluation Metrics. We generate descriptions for 200 images from the MSCOCO 2014 validation dataset (Lin et al., 2015) prompted with \"Please describe this image in detail.\" (Huang et al., 2024). We employ the Caption Hallucination Assessment with Image Relevance (CHAIR) (Rohrbach et al., 2019) for evaluating object hallucination. CHAIR consists of two variants: CHAIR\u2081, which calculates the percentage of hallucinated objects out of all objects mentioned in the caption, and CHAIRs, which measures the percentage of captions that contain at least one hallucinated object. Additionally, to complement the precision-based CHAIR metric, we introduce a Recall metric for a more detailed assessment.\n$CHAIR_1 = \\frac{|\\{ hallucinated\\ objects \\}|}{|\\{all\\ objects\\ mentioned\\}|}$\n$CHAIR_S = \\frac{|\\{ sentences\\ with\\ hallucinated\\ object\\}|}{|\\{ all\\ sentences\\}|}$\n$Recall = \\frac{|\\{correct\\ mentioned\\ objects\\}|}{|\\{ground\\ truth\\ objects\\}|}$\nTo provide a more comprehensive assessment of hallucinations, we use the Sentence-level Hallucination Ratio (SHR) (Zhao et al., 2023), a GPT-4-based evaluation metric. This metric includes hallucinations involving object existence, relationships, and attributes. We generate descriptions for 200 images from the VG dataset (Krishna et al., 2016), using the same prompts as in the CHAIR metric. Specifically, SHR leverages GPT-43 to compare the model's responses with the manually annotated descriptions from the VG dataset, evaluating each response on a sentence-by-sentence to identify potential hallucinations accurately.\nBaseline LVLMs. In LVLMs, two prominent methods for aligning text and vision modalities are the projection layer-based approach and the learnable query-based approach (Li et al., 2023a; Zhu et al., 2023; Chen et al., 2023; Liu et al., 2023). In our experiments, we utilize representative models for each aligning method: LLAVA-1.5 (7B/13B) (Liu et al., 2024c) and InstructBLIP (7B/13B) (Dai et al., 2023).\nBaseline Decoding Methods. We include various decoding methods as baseline approaches in"}, {"title": "4.2 Main Results", "content": "Results on CHAIR evaluation. As shown in Table 1, SGD significantly outperforms the baseline methods in the CHAIRS and CHAIR\u2081 across different model sizes and architectures. Specifically, compared to Greedy decoding, SGD-S achieves a 16.5% improvement in CHAIRs and a 19% improvement in CHAIR\u2081 on LLAVA 1.5 7B. On InstructBLIP 7B, the improvements are even more pronounced, with a 23.7% improvement in CHAIR1.\nWe conduct the CHAIR evaluation by fixing the generated token lengths at 64, 128, 256, and 512, representing a range from short to long text generation to ensure a fair evaluation of object hallucination across different methods\u2074 (see Appendix F for full experimental results). As illustrated in Figure 5 (a) our approach maintains a Pareto optimal"}, {"title": "5 Analysis", "content": "5.1 Analysis of SGD and Contrastive Decoding\nIn this section, we analyze SGD and contrastive decoding, focusing on their relationship with language priors. To explore this, we compute the JSD between each method's output and LLM distribution at each decoding step, followed by Section 2.1. For the analysis, we generate descriptions for 200 images from the MSCOCO 2014 validation set using LLAVA 1.5 7B. Factual accuracy is evaluated using the CHAIR metric, while text quality is assessed by GPT-40 (OpenAI, 2024) on a 1 to 5 scale (see details in Appendix E).\nTwo key questions guide the analysis. Ques-"}, {"title": "5.1 Analysis of SGD and Contrastive Decoding", "content": "In this section, we analyze SGD and contrastive decoding, focusing on their relationship with language priors. To explore this, we compute the JSD between each method's output and LLM distribution at each decoding step, followed by Section 2.1. For the analysis, we generate descriptions for 200 images from the MSCOCO 2014 validation set using LLAVA 1.5 7B. Factual accuracy is evaluated using the CHAIR metric, while text quality is assessed by GPT-40 (OpenAI, 2024) on a 1 to 5 scale (see details in Appendix E).\nTwo key questions guide the analysis. Question1: Is significantly deviating from language priors always beneficial? Question2: Can contrastive decoding reduce hallucinations in LVLMs when language priors heavily influence the two output distributions being contrasted?\nTo assess whether significantly deviating from language priors is always beneficial, we examine M3ID, a contrastive decoding method that progressively reduces language priors to focus more on visual information, as shown in Figure 6 (a). However, as presented in Table 3, text quality drops considerably when generating up to 64 tokens compared to 256 tokens. Specifically, it declines from 4.85 to 2.39, a reduction of about 50.7%. This suggests that a significant deviation from the language prior disrupts the distribution of language-related tokens, leading to a degradation in text quality.\nTo investigate the effectiveness of contrastive decoding when language priors significantly influence the original distribution, we investigate VCD. In VCD, the output distribution of the original image prompt is contrasted with that of the distorted image prompt to produce outputs that more align with the original image. A noteworthy observation is that both the output distributions of the original and distorted image prompts progressively converge towards the LLM distribution, as shown in Figure 6 (b). This finding indicates that language priors are influencing both the original output distribution and the output distribution that needs to be compared. Consequently, the two distributions become increasingly similar, diminishing the effectiveness of contrastive decoding. Table 3 demonstrates the reduced effectiveness of contrastive decoding, as VCD results in more instances of object hallucination compared to greedy decoding. Although current contrastive decoding methods focus on distorting the image to create meaningful differences from the original (Leng et al., 2023; Kim et al., 2024; Wan et al., 2024), the strong influence of language priors may obscure the intended effects of these distortions, undermining the effectiveness of contrastive decoding. This finding is crucial to"}, {"title": "5.2 Ablation study of SGD", "content": "In this section, we conduct ablation studies to evaluate the quality of the summary and the effect of POS control in SGD. For this, we use LLAVA 1.5 7B to generate descriptions for 200 images from the MSCOCO 2014 validation dataset. We employ the CHAIR metric and the text quality metric as described in Section 5.1. Additionally, we include the n-gram fluency indicator (Zhao et al., 2024),\nSummary Quality. We conduct an ablation experiment to evaluate the quality of summaries used in SGD. To achieve this, we employ three distinct summarization models-Distilled-Flan-T5-base, LLAVA 1.5 7B, and GPT-40. The results, as presented in Table 4, reveal that the effect of summarization quality is consistent across these models in terms of both CHAIR and text quality. This suggests that both SGD-D and SGD-S achieve satisfactory levels of summarization quality.\nPOS Control. We analyze the effect of applying image-related POS control in SGD. As shown in"}, {"title": "6 Related works", "content": "Mitigating Language Priors in LVLMs. Large Vision-Language models (LVLMs) extend pre-trained Large Language Models (LLMs) by incorporating visual tokens, enabling them to process visual content (Liu et al., 2023; Dai et al., 2023; Zhu et al., 2023). In LVLM architectures, the language model is significantly larger than the vision model, creating an imbalanced structure where the language model exerts more significant influence. As a result of this imbalance, the model tends to rely on linguistic patterns rather than adequately considering the visual information provided, a phenomenon known as the language prior problem (Guan et al., 2024; Lee et al., 2024a,b). To address this issue, several studies have explored contrastive decoding techniques to mitigate the model's over-reliance on language priors. Visual Contrastive Decoding (VCD) (Leng et al., 2023) works by utilizing distorted images, which amplify the language prior, and Instruction Contrastive Decoding (ICD) (Wang et al., 2024) introduces misleading instructions to achieve a similar effect. Both methods aim to reduce the language prior's dominance by leveraging these amplified conditions to adjust the model's behavior. Additionally, Multi-Modal Mutual Information Decoding (M3ID) (Favero et al., 2024) identified that as the token length increases, the model dilutes visual information, leading to a more substantial reliance on language priors. To counter this, M3ID applies more assertive contrastive decoding techniques as the token length grows to calibrate the model's over-reliance on language priors. However, contrastive decoding can disrupt the distribution of tokens essential for language modeling, leading to a decline in text quality. Additionally, due to the language prior, the two output distributions being contrasted may become more similar, making it less effective in reducing hallucinations. Our method, Summary-Guided Decoding (SGD), addresses these issues by using summarization techniques to naturally reduces the influence of language priors, allowing the model to focus more on the image. Furthermore, SGD preserves text quality by controlling only the POS tokens relevant to the image."}, {"title": "7 Conclusion", "content": "In this paper, we introduce Summary-Guided Decoding (SGD) as a novel method to mitigate object hallucinations in LVLMs. Our analysis reveals that as token sequences grow, LVLMs tend to increasingly rely on language priors, reducing the influence of visual information during the decoding process. To address this, SGD employs summarization techniques to shorten token length, encouraging the model to incorporate more visual details while controlling only the image-related POS tokens to maintain text quality. Our experimental results demonstrate that SGD significantly reduces object hallucination and achieves an optimal balance between factual accuracy and recall in both short and long description tasks."}, {"title": "Limitations", "content": "In this paper, we propose a Summary-Guided Decoding (SGD) to mitigate object hallucinations in Large Vision-Language Models (LVLMs). However, this approach comes with some limitations.\nFirst, the generation of summaries during the decoding process incurs additional computational cost, resulting in increased inference time.\nSecond, while summarization effectively reduces input length and helps mitigate hallucinations, it may also result in the loss of critical contextual information. Additionally, excessively long summaries can increase LVLMs' dependence on language priors, which may degrade the performance of SGD. Therefore, it is crucial for future work to train LVLMs in a way that inherently avoids over-reliance on language priors, even when token lengths are extended.\nLastly, we employ part-of-speech (POS) tagging to distinguish between image-related and language-related tokens. However, relying solely on POS tagging for this differentiation can be problematic. The development of more advanced methods for token distinction could enhance the effectiveness of SGD and create further synergies with this approach."}, {"title": "Ethics Statement", "content": "In this paper, we contribute to the future development of a safe and reliable AI community by conducting research focused on reducing hallucinations in Large Vision-Language Models.\nOur experiments were conducted by using publicly available datasets, ensuring that no private or sensitive personal data was involved. Furthermore, we utilized publicly accessible models for our experiments, reinforcing the transparency and reproducibility of our approach.\nHowever, the models we used may still exhibit biases inherent in the underlying datasets and training processes. While our focus was on biases related to language priors, we acknowledge the need to address other potential biases as well."}]}