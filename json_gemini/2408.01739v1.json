{"title": "LAM3D: Leveraging Attention for Monocular 3D Object Detection", "authors": ["Diana-Alexandra Sas", "Leandro Di Bella", "Yangxintong Lyu", "Florin Oniga", "Adrian Munteanu"], "abstract": "Since the introduction of the self-attention mechanism and the adoption of the Transformer architecture for Computer Vision tasks, the Vision Transformer-based architectures gained a lot of popularity in the field, being used for tasks such as image classification, object detection and image segmentation. However, efficiently leveraging the attention mechanism in vision transformers for the Monocular 3D Object Detection task remains an open question. In this paper, we present LAM3D, a framework that Leverages self-Attention mechanism for Monocular 3D object Detection. To do so, the proposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as feature extraction backbone and 2D/3D detection machinery. We evaluate the proposed method on the KITTI 3D Object Detection Benchmark, proving the applicability of the proposed solution in the autonomous driving domain and outperforming reference methods. Moreover, due to the usage of self-attention, LAM3D is able to systematically outperform the equivalent architecture that does not employ self-attention.", "sections": [{"title": "I. INTRODUCTION", "content": "Object Detection is one of the fundamental tasks in Computer Vision, consisting of detecting and locating objects of interest of a specific class within an image or video. The 2D detection of an object means determining the object's position within the image in terms of a 2D bounding box and classifying the object in a specific category. The state-of-the-art approaches for 2D Object Detection can be split into two categories: one-stage methods and two-stage methods. The two-stage methods ([1]\u2013[3]) follow a proposal-driven approach: in the first stage a set of region proposals is generated and in the second stage the candidate locations are classified as objects or background by using a convolutional neural network and are also refined. Even though the accuracy is great, they lack in terms of inference speed, fueling the need of one-stage object detectors. The one-stage methods [4]-[6]) perform both object localization and classification in a single pass through the network by relying on predefined anchor boxes.\nIn the context of 3D Object Detection, the focus shifts from identifying objects solely in 2D space to capturing their full spatial extent and orientation within a 3D environment. Unlike 2D detection, where bounding boxes suffice, 3D detection also requires predicting the objects' orientation relative to the coordinate system, along with their 3D bounding boxes. Some common types of inputs used for neural networks designed to solve this task are: point clouds, voxel grids, depth maps, RGB-D images and multi-view images.\nThe most challenging way of predicting the 3D cuboids is by solving the Monocular 3D Object Detection task, which involves using a single image as input, lacking in depth cues, which renders monocular object-level depth estimation naturally ill-posed. The standard network pipeline used for solving this task consists of a convolutional feature extraction backbone followed by a detection module added on top of it for determining the relevant 3D attributes needed to describe the 3D cuboids that represent the detected objects [7]-[10]). However, convolutional neural networks can sometimes struggle with capturing long-range dependencies and contextual information due to their limited receptive field. This limitation can impact the network's ability to fully understand the spatial relationships within the image, which is crucial for accurate 3D object detection.\nRecently, transformer-based models have shown promise in various vision tasks by effectively modeling long-range dependencies and capturing global context. Incorporating transformers for feature extraction could potentially enhance the performance of monocular 3D object detection systems by addressing the limitations of CNNs and providing a more comprehensive understanding of the scene.\nThis paper proposes an original pipeline for Monocular 3D (Object Detection built upon the following key contributions:\n\u2022\tWe introduce and validate a novel 3D Object Detection method based on a Transformer architecture as a feature extraction backbone."}, {"title": "II. RELATED WORK", "content": "A. Vision Transformers\nVision Transformer (ViT) [11] represents the starting point when it comes to designing Transformer architectures suitable for different computer vision tasks, being the first architecture to prove that a pure Transformer can be applied on image patches and perform well in image classification without relying on CNNs. The most notable architectures of this kind are Swin-Transformer (Swin-T) [12], Pyramid Vision Transformer (PVT) ([13], [14]) and Vision Transformer Adapter (ViT-Adapter) [15].\nSwin Transformer [12], further improved in [16], is a hierarchical Transformer which uses shifted windows in order to compute the representation for the image patches. Swin-T is compatible with various tasks, suck as image classification, object detection and semantic segmentation and it outputs feature maps at different scales in a hierarchical approach. Pyramid Vision Transformer [14], further improved in [13], uses multiple Transformer encoders, generates multi-scale feature maps and can also be used for multiple vision tasks. It is divided into four stages, each one being composed of a patch embedding layer and a Transformer encoder layer, generating feature maps of different scales. The authors from [15] propose a dense prediction task pre-training-free adapter for ViT, used to introduce the necessary inductive biases into the model for solving specific vision tasks, including object detection, instance segmentation and semantic segmentation. They use a spatial prior module which collects spatial features of three target resolutions, flattens and concatenates them in order to be fed as input for feature interaction.\nB. Monocular 3D Object Detection\nGenerally, 3D Object Detection is studied for two different domains, depending on the input data: autonomous systems ([8]-[10]) or indoor scenes ([17], [18]). The assumptions made in the methods solving this task are directly correlated to the domain in which they are applied.\nDue to the fact that monocular 3D object detection is an ill-posed problem, Lu et al. [9] propose a solution for urban scenes that includes a Geometry Uncertainty Projection (GUP) module and a Hierarchical Task Learning (HTL) strategy in order to tackle the depth inference error problem and the training instability due to the dependency between tasks. The authors from [8] propose a Depth EquiVarIAnt Network (DEVIANT) which uses scale equivariant steerable (SES) blocks [19] for the first time in the context of Monocular 3D Object Detection in order to learn consistent depth estimates by producing 5D feature maps in which the extra dimension captures the changes in scale for depth. Both approaches use uncertainty modeling to predict the physical and visual heights, but they do not model a joint probability distribution between the two, consequently lacking in information about their correlation. The authors from [10] propose learning a full covariance matrix during training, with the guide of a multivariate likelihood. A general-purpose baseline method is proposed in [7], called Cube R-CNN, designed for solving the 3D Object Detection task for both data domains: outdoor and indoor scenes, which surpasses prior best approaches on various datasets.\nWhile convolutional neural networks have been the backbone of many successful 3D object detection methods, their limitations have become more apparent in recent years, reaching a performance plateau in image-based 3D object detection. Specifically, CNNs often struggle with capturing long-range dependencies and contextual information due to their inherently local receptive fields [12], which can restrict their ability to understand complex scenes fully. Additionally, CNNs can be less efficient in handling varying scales and aspect ratios of objects, leading to potential performance degradation in diverse environments. As a result, there is a growing need to explore more advanced architectures, such as transformers, which can model global context and offer greater flexibility in feature extraction.\nTo this end, some authors experiment with Transformer-based architectures (?], [20], [21]). The authors of [20] design MonoDETR which uses a depth-aware transformer that guides the detection process by integrating contextual depth cues along with the visual features obtained from the input image. Based on the depth-aware transformer, the authors from [21] propose a new attention mechanism for Monocular 3D Object Detection called Supervised Scale-aware Deformable Attention (SSDA) which uses preset masks with different scales and a Weighted Scale Matching (WSM) loss to supervise scale prediction. However, the reliance on contextual depth cues might limit its performance in scenarios with insufficient or misleading depth information. As an improvement, in [?], a more complex Supervised Shape&Scale-perceptive Deformable Attention (S\u00b3-DA) and a Multi-classification-based Shape&Scale Matching (MSM) loss are proposed, which extract features of different shapes and scales. This method, while advanced, may introduce additional computational overhead and complexity.\nIt is important to remark that, even though all previously mentioned methods utilize the Transformer architecture, they use it exclusively for guiding the detection part of the pipeline and not as a feature extractor backbone. To the best of our knowledge, the authors from [22] are the only ones that use a Transformer-based architecture instead of a CNN for extracting the features from the input image. They propose DST3D, based on DLA-Swin Transformer (DST) as a feature extractor backbone and trained end-to-end. The DLA-Swin Transformer is based on Swin-T [12] and uses Deep Layer Aggregation (DLA) [23] for better feature fusion between different layers and blocks. However, the Pyramid Vision Transformer (PVT) offers several advantages over DLA-Swin, such as a more efficient multi-scale feature extraction ([14]). PVT's hierarchical design and attention mechanisms provide"}, {"title": "III. METHOD", "content": "Our goal is to design an innovative and effective method for Monocular 3D Object Detection in the context of real-world traffic scenery, which benefits from using the attention mechanism in extracting features from the input. Our approach extends PVTv2 [13] by incorporating detection heads and loss functions that have been validated for this task. We refer to our method as LAM3D. \nThe input image is firstly processed by a Transformer-based 2D detection backbone. The resulting 2D bounding boxes as Regions of Interest (RoIs) are further used as input for the convolutional 3D heads which extract the 3D bounding box information in terms of size, angle and 3D projected center. The depth is inferred afterwards by using a Geometry Uncertainty Projection module, based on the 2D and 3D heights and a depth bias, as previously described in [9].\nThis section presents the detection pipeline, starting with the Transformer-based feature extraction backbone.\nA. Transformer Backbone\nThe Pyramid Vision Transformer architecture [14], similar to convolutional neural network backbones, outputs a feature pyramid represented by four feature maps of different scales {F1, F2, F3, F4}, suitable for dense prediction tasks. The resulted feature pyramid is a consequence of the four stages of the Pyramid Vision Transformer, all of them sharing a similar architecture formed of a patch embedding layer and Li Transformer encoder layers. At the first stage, the input image is divided into flattened patches which are projected into a C1-dimensional embedding. The embedded patches together with a position embedding are fed to the Transformer encoder layers and the output is represented by a feature map of size \\$\\frac{H}{S_i} \\times \\frac{W}{S_i} \\times C_1\\$. The process is repeated at each stage with the previously generated feature map as input. Following the same concept, each feature map has the shape \\$\\frac{H}{S_i} \\times \\frac{W}{S_i} \\times C_i\\$, where s is the stride at stage i, with respect to the input image, and Ci is the embedding dimension at stage i. The strides used at each stage in order to generate the feature pyramid are {4, 8, 16, 32} pixels.\nThe Pyramid Vision Transformer was previously validated as backbone for the 2D Object Detection task with COCO 2017 as primary benchmark [14]. In the context of this paper, the Transformer backbone is initialized with the weights pre-trained on ImageNet [24] and fine-tuned on KITTI 3D Object Detection Dataset for the 2D Object Detection sub-task for baselines 1 and 2.\nB. Neck\nThe neck is represented by a fully convolutional upsampling method which uses iterative deep aggregation (IDA) to refine resolution and aggregate scale stage-by-stage [23]. The feature map is then upsampled using bilinear interpolation. Throughout our experiments, we observed that the first 64 channels of the feature map were the most critical. By slicing and focusing on these essential channels, we can reduce computational complexity and memory usage while retaining the most significant information for the detection task.\nC. Detection Heads\nThe proposed network uses three 2D detection heads for determining the heatmap, offset and size for each potential 2D bounding box. The heatmap represents the coarse locations and the level of confidence given to each region in the image in terms of each class, with shape [W,H,C] where W, H are"}, {"title": "D. Loss Functions", "content": "The total loss of the model Ltotal can be defined as a sum of all the task losses as per Eq. 1.\n\\[ L_{total} = L_{heatmap} + L_{offset2d} + L_{size2d} + L_{angle} +L_{w3d} + L_{l3d} + L_{h3d} + L_{depth} + L_{offset3d} \\] (1)\nL1 Loss Function is used as Loffset2d, Lsize2d, Loffset3d.\nIn the context of Lsize3d, L1 Loss Function is used only for w3d and l3d parameters, as the estimated height h3d is used in the Geometry Uncertainty Projection module. Cross-Entropy Loss Function is used as a term in Langle for guiding the object classification, together with the L1 Loss Function for angle value regression. Lheatmap is equivalent to the focal loss [6]. In order to tackle the problem of training instability, each task should start training only after its pre-tasks have been well-trained."}, {"title": "IV. EXPERIMENTS", "content": "A. Setup\n1) Dataset: KITTI 3D Object Detection [25] consists of 7481 training images and 7518 test images, as well as their corresponding point clouds, comprising a total of 80.256 labeled objects. Additionally to labeling the objects, each bounding box is marked as either visible, semi-occluded, fully occluded or truncated. The image resolution is 375 x 1242. A common approach, also followed in this paper, is to split the training data into a training set of 3712 images and a validation set of 3769 images [9]. The following studies are conducted based on this split and the results shown in this chapter are on the validation subset.\n2) Evaluation protocol: All the conducted experiments follow the evaluation configurations described in the KITTI 3D Object Detection Benchmark, using the PASCAL criteria. In the official benchmark evaluation, the 3D bounding box overlap for car category is 70%, while for pedestrians and cyclists is 50%. As an additional evaluation configuration, experiments in which the 3D bounding box overlap for car category is 50% and for pedestrians and cyclists is 30% were also conducted.\n3) Implementation details: The resolution of the input image for the proposed method is 380 x 1280. The proposed model is trained for 140 epochs with a batch size of 12 on 2 NVIDIA GeForce RTX 4090 GPUs. The training optimizer is Adam with an initial learning rate of 1.25 \u00d7 10\u22123, which is decayed at epochs {90, 120} with a rate set to 0.1. In the first 5 epochs, a cosine warm-up is applied. The Transformer backbone pre-trained on ImageNet [24] is firstly fine-tuned on KITTI 3D Object Detection Dataset for the 2D Object Detection sub-task. The experiments were made with PVTv2 baselines 1 and 2 [13].\nB. Results\nWe report the evaluation results of our method on the KITTI Dataset in Tab. I against the best Transformer-based method evaluated on KITTI, namely DST3D [22]. As commonly done, we report the results on the official test split where the evaluation is done by KITTI servers using withheld ground truth. Tab. II reports the results on the validation set for further evaluation. As can be seen, our proposed method achieves superior performance than previous Transformer-based methods for all difficulty levels. Specifically, LAM3D gains significant improvements in both AP3D and BEV3D.\nC. Ablation Study\nIn order to validate the effectiveness of using a Transformer-based architecture instead of a Convolutional Neural Network backbone in the context of the reference method used, ablation studies detailed in this section were performed on the validation split for KITTI 3D Object Detection Benchmark.\n1) Attention mechanism: The results can be seen in Tab. IV for the 3D Object Detection task with PVTv2 backbone. This table proves the efficiency of the attention mechanism in the Transformer-based architecture."}, {"title": "V. QUALITATIVE RESULTS", "content": "In Figure 2, we introduce two visualizations for truncated and occluded vehicles, highlighting the challenges of autonomous driving scenarios. One can note that our method effectively addresses these challenges, demonstrating its robust performance compared to GUP Net [9]."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose LAM3D, a novel framework for solving the Monocular 3D Object Detection task. The results detailed in Section IV-B prove that Transformers can be confidently used as feature extraction backbones in the context of the ill-posed Monocular 3D Object Detection task, achieving similar or even better performance than CNNs. Furthermore, the ablation study conducted indicates that the attention mechanism is particularly effective at capturing relevant information, especially for smaller objects in the scene."}]}