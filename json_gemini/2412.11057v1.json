{"title": "Set-Valued Sensitivity Analysis of Deep Neural Networks", "authors": ["Xin Wang", "Feilong Wang", "Xuegang (Jeff) Ban"], "abstract": "This paper proposes a sensitivity analysis framework based on set-valued mapping for deep neural networks (DNN) to understand and compute how the solutions (model weights) of DNN respond to perturbations in the training data. As a DNN may not exhibit a unique solution (minima) and the algorithm of solving a DNN may lead to different solutions with minor perturbations to input data, we focus on the sensitivity of the solution set of DNN, instead of studying a single solution. In particular, we are interested in the expansion and contraction of the set in response to data perturbations. If the change of solution set can be bounded by the extent of the data perturbation, the model is said to exhibit the Lipschitz-like property. This 'set-to-set' analysis approach provides a deeper understanding of the robustness and reliability of DNNs during training. Our framework incorporates both isolated and non-isolated minima, and critically, does not require the assumption that the Hessian of loss function is non-singular. By developing set-level metrics such as distance between sets, convergence of sets, derivatives of set-valued mapping, and stability across the solution set, we prove that the solution set of the Fully Connected Neural Network holds Lipschitz-like properties. For general neural networks (e.g. Resnet), we introduce a graphical-derivative-based method to estimate the new solution set following data perturbation without retraining.", "sections": [{"title": "1 Introduction", "content": "Sensitivity analysis is a classical research topic that crosses optimization [8], machine learning, and deep learning [22, 10, 3]. It studies how the solution of a model responds to minor perturbations in hyperparameters or input data. For instance, Christmann et al. [3] proved that the solution of the classification model with convex risk function is robust to the bias in data distribution. In this paper, we focus on deep neural networks (DNN), for which the solution is the weights of DNN trained (optimized) on input data. We are concerned about how the solution of DNN responds to perturbations in the input data in the training stage of the model.\nIn the domain of deep learning (e.g., DNN), sensitivity analysis has started to draw attention due to its wide range of applications, such as designing effective data poisoning attacks [15, 13], evaluating the robustness of models [21], and understanding the impact of important features in training data on the prediction [10]. Define a neural network $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$, where $\\mathcal{X}$ (e.g., images) is the input space and $\\mathcal{Y}$ is the (e.g., labels) output space. Given training data samples $\\mathbf{x} = [x_1, x_2, ..., x_n]$ and $\\mathbf{y} = [y_1, y_2, ..., y_n]$, and the loss function $L$, the empirical risk minimizer is given by $w \\stackrel{\\text{def}}{=} \\arg \\min_{w \\in \\mathcal{W}} \\sum_{i=1}^n L(x_i, y_i, w)$. This paper assumes that we perturb only the features, keeping the label constant. The learning process from data to local minimizers thus can be formulated as a set-valued mapping $S: \\mathcal{X}^n \\twoheadrightarrow \\mathcal{W}^2$,\n$S(\\mathbf{x}) = \\underset{w \\in \\mathcal{W}}{\\arg \\min} \\sum_{i=1}^n L(x_i, y_i, w)$ (1)\nFor the unperturbed feature $\\mathbf{x} = [x_1, x_2, ..., x_n]$ and $w \\in S(\\mathbf{x})$, current sensitivity analysis [10, 16, 3] aims to study the change of $S(\\mathbf{x})$ when an individual point is perturbed. For example, if the feature $x_p$ is perturbed to $\\mathbf{x}' = [x_1, x_2, ..., x'_p, ..., x_n]$, the sensitivity analysis can be conducted by examining a limit:\n$\\lim_{x'_p \\rightarrow x_p} \\frac{S(\\mathbf{x}') - S(\\mathbf{x})}{||x_p - x'_p||}$ (2)\nMost current sensitivity analysis methods for DNN suffer from the following two issues. First, to figure out the sensitivity of solution $w$ w.r.t. data $x_p$, one of the most common approaches (e.g., the influence function [10]) is to apply the Dini implicit function theorem [6] to the optimality condition of the model: $\\sum_{i=1}^n \\nabla_w L(x_i, y_i, w) = 0$, which leads to:\n$\\lim_{x'_p \\rightarrow x_p} \\frac{S(\\mathbf{x}') - S(\\mathbf{x})}{||x_p - x'_p||} = -H_w^{-1} \\nabla_{x_p} \\nabla_w L(x_p, y_p, w),$ (3)\nwhere $H_w = \\sum_{i=1}^n \\nabla_w^2 L(x_i, y_i, w)$ is the Hessian. However, the application of the Dini implicit function theorem is invalid when the Hessian $H_w$ is not invertible due to the often non-locally strong convex loss function $L$ of DNN [12]. Second, the current approaches [10, 16] assume $S$ is a single-valued mapping, omitting the fact that $S$ is often set-valued. Some findings have demonstrated that DNN may not exhibit a unique solution $S(\\mathbf{x})$ (even when $L$ is convex). For example, it was noticed in [12] that the stochastic gradient descent (SGD) method can find flat minima (solutions located in the flat valley of the loss landscape); others found that all SGD solutions for DNN may form a manifold [2, 4]. When an application (e.g., a data poisoning attack) is designed and evaluated based on only one of the solutions, it overlooks the fact that the learning algorithm may converge to other solutions during re-training.\nIn this paper, we incorporate the fact that $S$ is often set-valued into the sensitivity analysis framework for DNNs. This extends the scope of sensitivity analysis in DNNs from focusing on a single solution to a solution set, shifting from the traditional 'point-to-point' analysis to a 'set-to-set' paradigm. That is, we study how the solution set of a DNN expands and contracts in response to data perturbations. The proposed approach covers more general situations in risk minimization of DNN, including isolated local minima, non-isolated minima, and minima that consist of a connected manifold. More importantly, it directly deals with the solution sets without the assumption of non-singular Hessian matrix, offering a more complete understanding of DNN."}, {"title": "2 Preliminary knowledge", "content": "This section briefly summarizes the necessary preliminary knowledge for sensitive analysis of set-valued mapping, covering the distance between sets, sets convergence, and the generalized derivative.\n$\\mathbf{h}(C, D) = \\max{\\mathbf{e}(C, D), \\mathbf{e}(D, C')}, \\mathbf{e}(C, D) = \\sup_{w \\in C} d(w, D), \\mathbf{e}(D,C) = \\sup_{w \\in D} d(w, C').$\nDefinition 2. (Painlev\u00e9-Kuratowski Set convergence) Given a set-valued mapping $S : \\mathcal{X} \\rightrightarrows \\mathcal{W}$, the Painlev\u00e9-Kuratowski outer set limit as $x \\rightarrow \\bar{x}$ is\n$\\limsup S(x) := \\{w \\in \\mathcal{W} | \\exists \\text{sequences } x_k \\rightarrow \\bar{x} \\text{ s.t. } w_k \\in S(x_k) \\rightarrow w\\},$ (4)\nthe Painlev\u00e9-Kuratowski inner set limit as $x \\rightarrow \\bar{x}$ is\n$\\liminf S(x) := \\{w \\in \\mathcal{W} | \\text{ for } \\forall \\text{ sequences } x_k \\rightarrow \\bar{x}, w_k \\in S(x_k) \\rightarrow w\\}.$\nDefinition 3. A vector $\\eta$ is tangent to a set $\\Gamma$ at a point $\\bar{\\gamma} \\in \\Gamma$, written $\\eta \\in T_{\\Gamma}(\\bar{\\gamma})$, if\n$\\frac{\\gamma_i - \\bar{\\gamma}}{\\tau_i} \\rightarrow \\eta \\text{ for some } \\gamma_i \\rightarrow \\bar{\\gamma}, \\gamma_i \\in \\Gamma, \\tau_i \\searrow 0.$\nWhere $T_{\\Gamma}(\\bar{\\gamma})$ is the tangent cone to $\\Gamma$ at $\\bar{\\gamma}$.\nDefinition 4. Given a convex set $\\Gamma$ in $\\mathbb{R}^n$ and a point $\\bar{\\gamma}$ in $\\Gamma$, the normal cone to $\\Gamma$ at $\\bar{\\gamma}$, denoted $N_{\\Gamma}(\\bar{\\gamma})$, is defined as the set of all vectors $\\xi \\in \\mathbb{R}^n$ that satisfy the condition:\n$N_{\\Gamma}(\\bar{\\gamma}) = \\{\\xi \\in \\mathbb{R}^n : \\langle \\xi, y - \\bar{\\gamma} \\rangle \\leq 0 \\text{ for all } y \\in \\Gamma\\}.$\nDefinition 5. (Generalized derivatives)4 Consider a mapping $S : \\mathbb{R}^n \\twoheadrightarrow \\mathbb{R}^m$ and a point $x \\in \\text{dom } S$. The graphical derivative of $S$ at $x$ for any $w \\in S(x)$ is the mapping $DS(x | \\bar{w}) : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ defined by\n$v \\in DS(x | \\bar{w})(\\mu) \\Leftrightarrow (\\mu, v) \\in T_{\\text{gph}S}(x, \\bar{w}),$\nwhereas the coderivative is the mapping $D^*S(x | \\bar{w}) : \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ defined by\n$q \\in D^*S(x | \\bar{w})(p) \\Leftrightarrow (q, -p) \\in N_{\\text{gph}S}(x, \\bar{w}).$\n$DS(x | w)(\\mu) = \\limsup_{\\tau \\rightarrow 0} \\frac{S(x + \\tau \\mu) - w}{\\tau}$ (5)\n$DF(x)(\\mu) = \\nabla F(x) \\mu \\text{ for all } \\mu \\in \\mathbb{R}^n$\n$D^*F(x)(p) = \\nabla F(x)^* p \\text{ for all } p \\in \\mathbb{R}^m$ (6)"}, {"title": "3 Lipschitz-like property of Deep Fully Connected Neural Network", "content": "This section studies the Lipschitz-like property of DNNs. We focus on DFCNN, a classical DNN, to demonstrate our main theorem results. For each data point $x_i \\in \\mathcal{X}, x_i \\in \\mathbb{R}^d$, the first layer's weight matrix of a DFCNN is denoted as $W^{(1)} \\in \\mathbb{R}^{m \\times d}$, and for each subsequent layer from 2 to H, the weight matrices are denoted as $W^{(h)} \\in \\mathbb{R}^{m \\times m}$. $a \\in \\mathbb{R}^m$ is the output layer and the Relu function is given by $\\sigma(\\cdot)$. We recursively define a DFCNN, starting with $x_i^{(0)} = x_i$ for simplicity.\n$x_i^{(h)} = \\sigma (W^{(h)} x_i^{(h-1)}), 1 \\leq h < H$\n$f(x_i, w) = a x_i^{(H)}.$ (7)\nHere $x_i^{(h)}$ is the output of the h-th layer. We denote $\\mathbf{W} := (W^{(1)}, ..., W^{(H)})$ as the weights of the network and $w := (w^{(1)}, ..., w^{(H)})$ as the vector of the flatten weights. In particular, $w^{(h)}$ is the vector of the flattened h-th weight $W^{(h)}$. Denote $\\text{dim}(w^{(h)}) = p^{(h)}$ and $\\text{dim}(w) = \\sum_{h=1}^H p^{(h)} = p$.\nFor a DFCNN, we develop our method using the quadratic loss function: $L(x_i, y_i, W) = \\frac{1}{2} (f (w, x_i) - y_i)^2$. $w$, as the neural network weights, is a local/global minimum of loss $L$. Since first-order optimization algorithms, such as SGD, are widely utilized, we employ the first-order optimality condition to characterize these minima. Let $R(x, y, w) = \\nabla_w \\sum_{i=1}^n L (x_i, y_i, W)$. Since th label vector $\\mathbf{y} = [y_1, ..., y_n]$ is constant, we simplify the notation of $R(x, y, w)$ to $R(x, w)$. Then the solution of a DFCNN can be characterized by the set-valued mapping $F$ below:\n$F(x) = \\{w | R(x, w) = \\nabla_w = \\sum_{i=1}^n L (x_i, y_i, W) = 0\\},$ (8)\nFor layer h, we define mapping $F_h$ as:\n$F_h(x) = \\{w^{(h)}|R(x, w) = 0\\}.$ (9)\nFollowing the sensitivity analysis in [10, 16], we first focus on one individual data $x_k \\in \\mathcal{X} = [x_1, ..., x_n]$, which is perturbed. In this case, $F(x)$ and $F_h(x)$ in the above two equations are expressed as $F(x_k)$ and $F_h(x_k)$ to indicate that only $x_k$ are perturbed. In Section 4, we present the case with multiple data perturbations.\nAssumption 1. We assume that DNNs are overparameterized; under this assumption, a DFCNN has the capacity to memorize training data with zero training error, i.e. $p > d$.\nAssumption 2. For given $x$ and $w$, $[\\nabla_w R(x, w), \\nabla_{x_k} R(x, w)]$ is of full rank, where $\\nabla_w R(x, w) \\in \\mathbb{R}^{p \\times p}, \\nabla_{x_k} R(x, w) \\in \\mathbb{R}^{p \\times d}, p = \\text{dim}(w), d = \\text{dim}(x_k)$.\nThe Lipschitz-like property examination and solution set estimation following data perturbation rely on the generalized derivative (see definition 5). Theorem 1 below provides an explicit formulation for the generalized derivative of $F$, enabling a convenient analysis of the local behavior of the solution mapping. It will be used in the proof of Theorem 2 that describes the Lipschitz-like property of the given solution mapping."}, {"title": "4 Sensitivity analysis for solution set", "content": "Theorem 3 reveals that the solution set of DFCNN after perturbation will not deviate dramatically from the original solution set, allowing us to approximate this change using the local information around (x, w). This section first proposes a method to estimate the new solution set of DFCNN, given the perturbation in the training data. Unlike Section 3, which only perturbs a single individual data point, this section perturbs multiple data points simultaneously.\nConsider a DFCNN with its weight w, where w represents a solution obtained by a learning algorithm (e.g., SGD) trained on a set of pristine data $\\mathcal{X} = \\{x_i\\}, i \\in \\mathcal{I} = \\{1, 2, ..., n\\}$. Assume the data is perturbed following $x'_i = x_i + d\\Delta x_i$, where d is the norm of perturbation, $\\Delta x_i$ is a unit vector that indicates the perturbation direction for point $x_i$. We denote by $\\Delta x = \\{\\Delta x_i\\}, i \\in \\mathcal{I}$ the set of perturbations. To make the number of perturbed points more flexible, we set $\\mathcal{K} \\subseteq \\mathcal{I}$ to denote the indices of the perturbed data, and let $\\Delta x_i = 0$ for $i \\in \\mathcal{I} \\backslash \\mathcal{K}$. As defined by (1), $S(x_p)$ is the solution set of a DFCNN trained by the poisoned data $x_p = \\{x'_i\\}$ and $S(x)$ is the original solution set.\nThe graphical derivative $DS(x | \\bar{w})(\\mu)$ captures how the solution w changes near w when x is perturbed in the direction of $\\mu$. For any twice differentiable loss function $L(w)$ (such as the quadratic loss function), following theorem 1, $DS(x | \\bar{w})(\\mu)$ is equivalent to (see Appendix A):\n$DS(x | w) (\\mu) = \\bigg\\{v \\bigg\\vert \\nabla_w = \\frac{1}{n} \\sum_{i \\in I} L(x_i, y_i, w) v + \\nabla_{x_k} \\nabla_w = \\frac{1}{|K|} \\sum_{i \\in K} L (x_i, y_i, w) \\mu_k = 0 \\bigg\\}.$ (13)\nThe solution set $S(x^p)$ can be estimated by:\n$S(x^p) \\approx w + DS(x | w)(\\Delta x).$ (14)"}, {"title": "5 Simulation for solution estimation", "content": "Although our theoretical results in Section 3 and Section 4 focus on DFCNN, this section demonstrates that the methods perform well for general DNNs. To show this, we next present two numerical examples to illustrate the proposed set-valued sensitivity analysis method. The first one is on a toy example and the second one is on the Resnet. All experiments are performed on an RTX 4090 GPU.\nWe consider a linear neural network with 2 layers. Assume we only have two data points $(x_i, y_i), i = 1, 2$ and both $x_i$ and $y_i$ are real numbers. The solution set $S(x) = \\{w = (w_1, w_2)\\}$, where $w_1, w_2$ are the weights of the first and second layer, is given by minimizing the empirical risk :\n$(w_1, w_2) \\in \\underset{w_1, w_2 \\in \\mathbb{R}^2}{\\text{argmin}} \\frac{1}{2} (y_1 - w_1 w_2 x_1)^2 + \\frac{1}{2} (y_2 - w_1 w_2 x_2)^2.$ (15)\nGiven the pristine dataset $(x_1, y_1) = (1, 2), (x_2, y_2) = (2, 4)$, the model solution constitutes a set as $w_1 * w_2 = 2$, and $w = (1, 2)$ is obviously one of the solutions. We assume that the original solution converges to $w = (1, 2)$ during training using this pristine data. We introduce perturbations to the data following the rule $x_p = x + 0.2 * (-1, -2)$, and re-train the model using the poisoned data.", "5.2 Simulation on Resnet": "This section simulates our estimated method, as defined by (14), on a Resnet56 network [9]. Following the notations above, we extract 1000 points from the CIFAR-10 dataset, denoted as $\\mathcal{X}$. We use $x_p$ to denote the poisoned data. The original solution $w$ is the pre-trained weights of Resnet56.\nFollowing (14), when the data $x$ is perturbed along the direction $\\Delta x$, the corresponding change direction of $w$, denoted by $\\Delta w$, is determined by the graphical derivative $DS(x | w)(\\Delta x)$. The relationship between $\\Delta x$ and $\\Delta w$ is:\n$\\nabla_w = \\frac{1}{n} \\sum_{i \\in I} L(x_i, y_i, w) \\Delta w + \\nabla_{x_k} \\nabla_w = \\frac{1}{|K|} \\sum_{i \\in K} L (x_i, y_i, w) \\Delta x = 0.$ (16)\nDenoting $\\dagger$ the pseudo inverse operator, $\\Delta w$ is given by:\n$\\Delta w = - \\bigg(\\frac{1}{n} \\sum_{i \\in I} \\nabla_w^2 L(x_i, y_i, w) \\bigg)^{\\dagger} \\bigg(\\frac{1}{|K|} \\sum_{i \\in K} \\nabla_{x_k} \\nabla_w L (x_i, y_i, w) \\Delta x \\bigg).$ (17)\nNote (17) is very similar to (3) derived from the Dini implicit function theorem. However, different from (3), (17) here does not rely on a strong convex loss function $L$. The key technical challenge lies in solving (17) under high-dimensional cases. We transform (17) to a least square problem:\n$\\Delta w := \\text{argmin}_{v} \\bigg\\| \\frac{1}{n} \\sum_{i \\in I} \\nabla_w^2 L(x_i, y_i, w) v + \\nabla_{x_k} \\nabla_w = \\frac{1}{|K|} \\sum_{i \\in K} L (x_i, y_i, w) \\Delta x \\bigg\\|^2.$ (18)\nwhere both $\\nabla_{x_k} \\nabla_w L (x_i, y_i, w) \\Delta x$ and $\\nabla_w^2 L (x_i, y_i, w) v$ can be calculated using implicit Hessian-vector products (HVP) [17]. Following [1], $\\nabla_w^2 L (x_i, y_i, w) v$ can be computed efficiently in $O(p)$."}, {"title": "6 Related work", "content": "This paper only focuses on the sensitivity of solutions of learning models (e.g., model weights of DNNs) in response to perturbations in the training data. For the change of prediction in relationship to the inference data, one can refer to [7, 21]. Influence function, as a concept in robust statistics [11], was first used for the sensitivity analysis of the classification models with convex loss function (e.g. SVM, Logistic Regression) [3]. Koh et al. [10] introduced it to the DNN, demonstrating its application in data poisoning attacks and identifying the important features. However, the existence of the influence function relies on the implicit function theorem (see Theorem 19 in [3]), which may not be applicable to DNNs when non-isolated DNN solutions are considered, as discussed in the Introduction section of this paper. Peter et al. [16] measured the sensitivity of solutions to training data through the Memory-Perturbation Equation (MPE). It was demonstrated that sensitivity to a group of examples can be estimated by adding their natural gradients, indicating higher sensitivity with larger gradients. However, its theorem relies on the inverse of Hessian, which does not exist when the loss function is not strongly convex.\nThis paper utilizes the Lipschitz module to characterize the sensitivity of DNN. Noteworthy is that sensitivity analysis in our paper studies how the model solution changes with training data, not how the model output changes with inference data, although the two are closely related. To our best knowledge, this is the first time that the Lipschitz concept and the estimation of Lipschitz module estimation are introduced to the training stage. Previous research only focused on estimating the Lipschitz constants during the inference stage, see [7, 20], to quantify the robustness of model prediction w.r.t. perturbations in inference data. For example, Kevin and Aladin [20] adopted a power method working with auto differentiation to estimate the upper bound of the Lipschitz constant.\nMost sensitivity analysis approaches only focus on a single solution of the learning algorithm. This paper considers the first-order optimality condition as a set-valued mapping (multifunction), introducing the 'set-to-set' analysis approach. Our sensitivity analysis is based on the Lipschitz-like property of set-valued mapping, where the Lipschitz Modulus quantifies the change of the solution set. For more discussion about set-valued mapping, interested readers can refer to [18, 6]."}, {"title": "7 Conclusion", "content": "This paper provides set-valued analysis methods to study the sensitivity of model solutions (e.g. weights of a DNN) in response to perturbations in the training data. Theoretically, our approach considers the possibility that the DNN may not have unique solutions and does not rely on a non-singular Hessian. We also accurately estimate the solution change when the training data are perturbed along a specific direction.\nOur analysis framework can have multiple potential applications. First, it utilizes the Lipschitz concept to study the sensitivity of DNN. This can lead to a robustness evaluation method by measuring the Lipschitz module, i.e. a larger Lipschitz module indicates higher sensitivity. Second, our framework extends the implicit function theorem in DNN, which can be used for data poisoning attacks. We can determine the perturbation direction of training data to shift the solution toward a target solution, thereby executing a model target poisoning attack [19]. Alternatively, we can identify the perturbation direction to alter the training data to increase the loss of validation data [13].\nIn future research, we plan to test the set-valued analysis methods on more DNNs. One limitation of this research is that our theoretical results are derived only using the DFCNN with the Relu activation function. In the future, we plan to extend the results to a wider variety of DNN architectures."}]}