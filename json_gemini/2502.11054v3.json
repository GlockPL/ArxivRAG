{"title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models", "authors": ["Zonghao Ying", "Deyue Zhang", "Zonglei Jing", "Yisong Xiao", "Quanchen Zou", "Aishan Liu", "Siyuan Liang", "Xiangzheng Zhang", "Xianglong Liu", "Dacheng Tao"], "abstract": "Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation (RACE), a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at https://github.com/NY1024/RACE to facilitate further research in this critical domain. Warning: This paper contains model outputs that are unsafe.", "sections": [{"title": "1 Introduction", "content": "LLMs have garnered widespread attention due to their remarkable ability to perform diverse tasks [1-3]. However, studies have shown that LLMs can also generate unsafe or harmful content when prompted in certain ways [4\u20136]. This vulnerability can be exploited through jailbreak attacks\u2014carefully crafted prompts that bypass alignment constraints and elicit unintended responses [7, 8]. Although harmful, jailbreak attacks [7, 9] serve as a key red-teaming approach for assessing the risk of LLMs generating unsafe content.\nCurrently, these attacks can be broadly categorized into single-turn and multi-turn jailbreaks. Single-turn attacks attempt to bypass safety mechanisms within a single interaction [7, 8, 10\u201313], whereas multi-turn jailbreaks exploit the interactive nature of LLMs by engaging them in iterative dialogues that lead to unsafe outputs [14\u201317]. Compared to single-turn attacks, multi-turn jailbreaks simulate real-world human interactions and can expose critical safety blind spots, thereby attracting extensive interest [15, 17]. However, existing multi-turn jailbreak methods"}, {"title": "2 Related work", "content": "Reasoning in LLMs. Reasoning is a cognitive process that involves thinking about something logically and systematically, using evidence and past experiences to draw conclusions or make decisions [22, 23]. Recent studies have demonstrated that LLMs exhibit remarkable reasoning capabilities in various tasks, including mathematical reasoning [18], common sense reasoning [19], symbolic reasoning [24], and causal reasoning [25]. Subsequently, Chain-of-thought (CoT) [26-30] has emerged as a promising approach for further enhancing these reasoning capabilities.\nWhile the reasoning capabilities of LLMs have contributed to their impressive performance across various downstream tasks, their potential exploitation in jailbreak attacks remains largely unexplored. In this study, we focus on leveraging reasoning capabilities to facilitate jailbreak attacks.\nMulti-turn Jailbreak Attack. Typical multi-turn jailbreak methods follow the principle of starting with harmless conversations and gradually making the queries more harmful in subsequent turns. Different methods have designed specific strategies based on this principle, including applying cognitive psychol-ogy theories to gradually modify subsequent queries [14, 16], using actor networks to expand the attack range of subsequent queries [31], extracting harmful keywords from original queries to construct semantically equivalent ones [32, 33], and breaking down the"}, {"title": "3 Threat Model", "content": "The target LLM M has undergone safety alignment prior to release and is expected to avoid generating unsafe responses even when presented with a harmful target query Q. In this study, we investigate self-jailbreaking, where both the querying and response-generating models originate from the same model. For clarity, we instantiate the target model M as two distinct roles: a shadow model Ms, responsible for generating queries, and a victim model Mv, tasked with providing responses.\nThe goal of the shadow model is to generate a sequence of queries {q1, q2, \u2026, qn } during its interaction with the victim model to induce unsafe responses. Given practical deployment scenarios, the attack is conducted in a black-box setting where the shadow model can only access the victim model's responses during its interactions. However, the shadow model can adaptively adjust the current query qi (where i denotes the current conversation turn) based on the context Ci-1, which includes the query-response pairs [(91, 1), ... (qi\u22121, ri\u22121)] from all preceding conversation turns."}, {"title": "4 Methodology", "content": "LLMs have demonstrated strong reasoning capabilities in tasks such as logical deduction, common sense reasoning, and mathematical problem-solving, enabling them to tackle complex tasks across diverse domains [18, 19, 24, 25]. Rather than directly issuing harmful queries, which are easily rejected by safety alignment mechanisms, we propose a novel approach that exploits LLMs' reasoning processes by reframing harmful intent into seemingly benign yet complex reasoning tasks. These tasks are carefully designed so that, once solved, they inherently guide the model to generate harmful content, effectively compromising its safety alignment. Here, the target LLM simultaneously acts as both the shadow model and the victim model. Independently, each role appears to engage in legitimate reasoning: the victim model focuses solely on solving reasoning tasks, while the shadow model refines and generates queries without explicitly recognizing the harmful intent behind them. However, when combined, these interactions ultimately lead to a successful attack.\nHowever, implementing this reasoning-driven jail-break is non-trivial, as it requires manipulating the model's reasoning process without triggering safety mechanisms. This poses three challenges: how to maintain reasoning alignment while ensuring that each query remains semantically consistent with the target and extracts useful information, how to preemptively optimize the query's reasoning structure to avoid potential rejections during actual interactions, and how to quickly recover and learn from failed reasoning attempts to maintain attack progression. To address these challenges, we model the jailbreak process as an Attack State Machine (ASM), which serves as a reasoning planner. The ASM formalizes the attack as a structured sequence of reasoning states and transitions, ensuring that each step remains within the bounds of a legitimate problem-solving task while progressing toward the jailbreak objective. Within this reasoning framework, we implement three key modules to manipulate the model's reasoning process and systematically address these challenges. \u25cf The Gain-guided Exploration module selects queries that remain semantically aligned with the target while extracting useful information, ensuring steady attack progres-sion. The Self-play module preemptively refines queries within the shadow model by simulating potential rejection responses, improving attack efficiency before engaging the victim model. The Rejec-tion Feedback module analyzes failed interactions and restructures queries into alternative reasoning challenges, enabling quick recovery and maintaining attack stability. The overview of RACE is provided in Fig. 2."}, {"title": "4.2 Attack State Machine Framework", "content": "A finite state machine (FSM) [20, 21] is a mathematical model that represents a finite number of states, along with the transitions and actions between these states. A finite state machine can be formally defined as a five-tuple: $FSM = (S, \\Sigma, \\delta, s_0, F)$, where $S$ denotes a finite set of states, $\\Sigma$ represents the input"}, {"title": "4.3 Attack Modules", "content": "Within the ASM, three specialized modules work together to optimize state transitions and ensure attack progression. The gain-guided exploration and self-play modules proactively generate and optimize effective queries, while the rejection feedback module handles failed state transitions by refining queries. The design enables the ASM to maintain stable progression through the reasoning states while efficiently adapting to model responses."}, {"title": "4.3.1 Gain-guided Exploration", "content": "To address potential semantic drift and ineffective information in victim model responses, we propose a gain-guided exploration (GE) module inspired by information theory [40].\nInformation gain (IG) [41, 42] was originally introduced to quantify how much a feature A of a random variable reduces the uncertainty of a target variable Y, defined as $IG(Y, A) = H(Y) \u2013 H(Y |A)$, where $H(Y) = \u2013 \\sum_{y \\in Y} P(y) log P(Y)$ is the"}, {"title": "4.3.2 Self-play", "content": "Despite GE filtering, queries may still fail when interacting with the victim model. Therefore, we implement a self-play (SP) module to further optimize these candidates.\nInspired by game theory where an entity improves by competing against itself [44, 45], SP leverages that both shadow and victim models are instantiated from the same source. This allows the shadow model to better predict victim responses through self-play, leading to more efficient query optimization.\nLet Ms and Mv (where Mv simulates the victim model) be the two players in self-play. Given the current state s and the candidate query q\u00ba, the goal of Ms is to maximize the probability that Mv returns a non-rejection response (denoted as rc \u2209 Rrej). The utility function can be formulated as follows:\n$\\begin{aligned}\nU_{M_s}(s, q^c, r) = \\begin{cases}\n1, & r \\notin R_{rej}. \\\\\n0, & r \\in R_{rej}.\n\\end{cases}\n\\end{aligned}$", "equation": "\\begin{aligned}\nU_{M_s}(s, q^c, r) = \\begin{cases}\n1, & r \\notin R_{rej}. \\\\\n0, & r \\in R_{rej}.\n\\end{cases}\n\\end{aligned}"}, {"title": "4.3.3 Rejection Feedback", "content": "While GE and SP balance the progression of the attack and the likelihood of positive responses, the uncertainty of LLM outputs [46, 47] can still cause state transition failures in the ASM. To mitigate this issue, we propose the rejection feedback (RF) module.\nRF is activated when a state transition failure is detected in the ASM, signaling that the current query"}, {"title": "4.4 Overall Attack", "content": "The attack begins by initializing the ASM reasoning states. In each turn, the shadow model generates seed queries that are refined through gain-guided exploration and self-play optimization. Successful queries advance the attack to the next state, while failed attempts trigger query refinement through the rejection feedback module. This process iterates until reaching the final state, maintaining a natural reasoning flow while pursuing the attack goal."}, {"title": "5 Experiments", "content": "Models. We conduct experiments to validate the performance of RACE across 9 popular LLMs, including 3 open-source models: Gemma (Gemma-2-9B) [48], Qwen (Qwen2-7B-Instruct) [49], and GLM (GLM-4-9B-Chat) [50], and 6 closed-source models: GPT-4 [51], GPT-4o [52], Gemini 1.5 Pro [53], Gemini 2.0 Flash Thinking [54], OpenAI 01 [55], and DeepSeek R1 [56].\nDatasets. Following previous work [57, 58], we evaluate attack performance on the AdvBench subset [7] and the HarmBench [59]. The AdvBench subset contains 50 representative samples from the AdvBench dataset, and HarmBench comprises 400 textual instances spanning 7 distinct categories of harmful activities.\nCompared baselines. We compare RACE against existing multi-turn jailbreak attack methods, including PAIR [58], DeepInception (DI) [60], CoA [32], and TAP [57]."}, {"title": "5.2 Attack Evaluation", "content": "Attack performance on classic LLMs. Tab. 1 summarizes the experimental results. Among the evaluated methods, RACE demonstrated the most effective performance, achieving average ASRs of 91.3% on the AdvBench subset and 66.7% on HarmBench. Among the baseline methods, TAP emerged as the most effective, achieving an impressive 88% ASR when attacking GPT-4o on the AdvBench subset. Notably, we observed a significant performance gap between the AdvBench subset and HarmBench across all methods. The substantially lower ASRs on HarmBench can be attributed to its more diverse and complex tasks. Notably, the performance gap between RACE and the baseline methods was even more pronounced on HarmBench, reaching up to 62.3%, further highlighting the effectiveness of RACE in more challenging scenarios.\nAttack performance on reasoning LLMs. We further evaluate three state-of-the-art reasoning LLMs using the AdvBench subset, with experimental results summarized in Fig. 3. Taking Gemini 2.0 Flash-ing Thinking as an example, we observe that when directly presented with original harmful queries, the ASR of Gemini 2.0 Flashing Thinking reaches 20.0%, which notably surpasses that of previous-generation models like Gemini 1.5 Pro (ASR reaches 2.0%). This finding suggests that the introduction of advanced rea-soning capabilities can paradoxically escalate poten-tial safety risks in next-generation models. On the other hand, as highlighted by Jaech et al.[55], OpenAI ol employs deliberative alignment to reason about safety policies and generate safe responses when faced with potentially unsafe prompts. By comparing the"}, {"title": "5.3 Defense Evaluation", "content": "Currently, test-time defenses for multi-turn jailbreak attacks are lacking. While training-based approaches like dataset construction and fine-tuning improve robustness, they are unsuitable for test-time defenses. Thus, we evaluate popular single-turn defenses against RACE.\nAs illustrated in Tab. 2, compared to the baseline, the evaluated defense methods demonstrate remarkably limited effectiveness in mitigating RACE, with ASR reductions as minimal as 1%. Notably, SR"}, {"title": "6 Discussion", "content": "This section further explores the impact of conversation turns, reasoning task types, and attack strategies on attack performance. All experiments are conducted using the AdvBench subset on open-source models."}, {"title": "6.1 Number of Conversation Turns", "content": "The number of conversation turns serves as a crucial hyperparameter that significantly influences the"}, {"title": "6.2 Reasoning Types", "content": "We evaluate four types of reasoning tasks: mathematical reasoning (MaR), common sense reasoning (CoR), symbolic reasoning (SyR), and causal reasoning (CaR), whose definitions and examples are detailed in Sec. D."}, {"title": "6.3 Ablation on Attack Modules", "content": "Fig. 6 presents the ablation study results for RACE. We analyze the performance impact when removing GE, SP, and RF.\nThe experimental results demonstrate that removing any of these components leads to performance degradation. Without GE, ASR drops by up to 14.0%, indicating the importance of selective query generation based on information gain. The absence of SP results in an ASR decrease of up to 8.0%, showing the value of leveraging the shadow model for"}, {"title": "7 Conclusion", "content": "This paper presents a novel reasoning-driven jailbreak framework that exploits LLMs' inherent reasoning capabilities to bypass built-in safety mechanisms. By modeling the attack process as an attack state machine, our approach strategically frames harmful intent as complex yet seemingly benign reasoning tasks, ensuring a structured and adaptive attack progression. We introduce three key modules, including gain-guided exploration, self-play, and rejection feedback to systematically manipulate the model's reasoning process, optimize query structures, and recover from failed attempts. Extensive experiments demonstrate that our method effectively compromises existing safety alignments, revealing critical risks to LLM safety."}, {"title": "8 Limitations", "content": "Despite the effectiveness of RACE, several challenges remain to be addressed: O improving efficiency to minimize interaction overhead while maintaining high ASRs, developing adaptive countermeasures to mitigate reasoning-based attacks, and extending the framework to analyze and defend against other forms of adversarial reasoning manipulations."}, {"title": "9 Ethical Consideration", "content": "We acknowledge the dual-use nature of this research and emphasize that our primary goal is to advance LLM safety through systematic vulnerability assessment. This work demonstrates that current alignment strategies may be insufficient in preventing multi-turn jailbreaks, particularly when exploiting reasoning capabilities. To minimize potential harm, we have carefully omitted explicitly harmful outputs while focusing on methodological aspects. We strongly oppose any malicious applications of our findings and have included discussions on potential countermeasures. While the development of comprehensive defense mechanisms remains future work, we believe this research provides valuable insights for LLM developers to develop more robust alignment techniques."}]}