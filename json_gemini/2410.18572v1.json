{"title": "TAIPAN: EFFICIENT AND EXPRESSIVE STATE SPACE LANGUAGE MODELS WITH SELECTIVE ATTENTION", "authors": ["Chien Van Nguyen", "Huy Huu Nguyen", "Thang Pham", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Ryan A. Rossi", "Trung Bui", "Viet Dac Lai", "Franck Dernoncourt", "Thien Huu Nguyen"], "abstract": "Efficient long-context language modeling remains a significant challenge in Natural Language Processing (NLP). While Transformers dominate language tasks, they struggle with long sequences due to quadratic computational complexity in training and linearly scaling memory costs during inference. Recent State Space Models (SSMs) such as Mamba offer alternatives with constant memory usage, but they underperform in tasks requiring extensive in-context retrieval. We introduce Taipan, a novel hybrid architecture that combines Mamba-2 with Selective Attention Layers (SALs). These SALs identify tokens requiring long-range interactions, remove less important features, and then augment their representations using the attention module. This approach balances Mamba's efficiency with Transformer-like performance in memory-intensive tasks. By constraining the attention budget, Taipan extends accurate predictions to context lengths of up to 1 million tokens while preserving computational efficiency. Our experiments demonstrate Taipan's superior performance across various scales and tasks, offering a promising solution for efficient long-context language modeling.", "sections": [{"title": "INTRODUCTION", "content": "Transformer-based architectures Vaswani (2017); Brown (2020) have revolutionized Natural Language Processing (NLP), delivering exceptional performance across diverse language modeling tasks Touvron et al. (2023). This success stems from their ability to capture complex word dependencies using the self-attention mechanism. In addition, Transformers are highly scalable and well-suited for parallel training on large datasets. However, despite their success, they still face notable challenges when handling long-context sequences. Specifically, the self-attention mechanism suffers from quadratic computational complexity, and the memory requirement grows linearly with context length during inference, as the model must store key-value vectors for the entire context. These factors impose practical constraints on sequence length due to the high computational and memory costs.\nTo this end, recent advancements in recurrent-based architectures, particularly State Space Models (SSMs) Gu et al. (2021b;a), have emerged as promising alternatives for efficient language modeling Gu & Dao (2023); Dao & Gu (2024). SSMs offer constant memory usage during inference, and architectures like Mamba-2 Dao & Gu (2024), a variant of SSMs, have demonstrated performance comparable to Transformers in certain language tasks Waleffe et al. (2024). Some studies even suggest that SSMs can outperform Transformers in areas like state tracking Merrill et al. (2024) due to their Markovian nature. However, despite these advancements, SSM-based models still fall short in scenarios requiring in-context retrieval or handling complex long-range dependencies Arora et al. (2024); Waleffe et al. (2024).\nTo address these challenges, we introduce Taipan, a hybrid architecture that combines the efficiency of Mamba with enhanced long-range dependency handling through Selective Attention Lay-"}, {"title": "BACKGROUND", "content": "This section briefly overviews the foundational architectures relevant to our work. We first review Causal Self-Attention Vaswani (2017), the core mechanism of Transformer models. We then discuss Linear Attention Katharopoulos et al. (2020), an efficient variant that achieves linear complexity. Finally, we examine Mamba-2, a recent architecture that generalizes Linear Attention using structured state-space models (SSMs) Dao & Gu (2024). We emphasize how each model balances computational efficiency and recall accuracy, particularly in memory-intensive tasks ."}, {"title": "CAUSAL SELF-ATTENTION", "content": "Causal Self-Attention is the key component in Transformer architectures that allows each token in a sequence to attend to all other previous tokens (Vaswani, 2017). Given an input sequence $X = [x_1,..., x_L] \\in R^{L\\times d}$, where L is the sequence length and d is the embedding dimension, self-attention firsts computes the query, key, and value vectors for each token via linear projections:\n$q_i = W_Qx_i, \\quad k_i = W_Kx_i, \\quad v_i = W_Vx_i$\nwhere $W_Q, W_K,W_V \\in R^{d\\times d}$ are learnable weight matrices.\nThen, the attention output $o_i$ for each token $x_i$ will be calculated as a weighted sum of the value vectors over the distribution of similarity matrix between its query vector and previous key vectors:\n$o_i = \\sum_{l=1}^{i} \\frac{exp(q_i^Tk_l/\\sqrt{d})}{\\sum_{j=1}^{i} exp(q_i^Tk_j/\\sqrt{d})}v_l$\nThe non-linear softmax distribution allows the models to capture intricate relationships between tokens, and concentrate on salient features Qin et al. (2022); Zhao et al. (2019). As such, self-attention can encode complex language patterns and long-range dependencies that are crucial for complex language understanding and generation tasks."}, {"title": "LINEAR ATTENTION", "content": "To address the quadratic complexity, recent work has shown that it is possible to achieve linear complexity with the attention mechanism by replacing the softmax attention with dot-product attention (Shen et al., 2021; Katharopoulos et al., 2020). Given a feature transformation $\\phi(x)$, causal self-attention can be rewritten as:\n$o_i = \\frac{\\sum_{l=1}^{i}\\phi(q_i)^T\\phi(k_l)}{\\sum_{j=1}^{i} \\phi(q_i)^T\\phi(k_j)}v_l$\nThen, using the associate property of matrix multiplication, this can be reformulated as:\n$o_i = \\frac{\\phi(q_i)\\sum_{l=1}^{i}\\phi(k_l)v_l}{\\phi(q_i)\\sum_{j=1}^{i}\\phi(k_j)}$\nLet $S_i = \\sum_{t=1}^{i}\\phi(k_t)v_t$ and $z_i = \\sum_{t=1}^{i}\\phi(k_t)$. We can then rewrite the equation in a recurrent form:\n$S_i = S_{i-1} + \\phi(k_i)v_i$\n$o_i = \\frac{S_i\\phi(q_i)}{z\\phi(q_i)} \\approx S_i\\phi(q_i)$\nThis formulation allows for efficient training and inference. Let $Q, K, V \\in R^{L\\times d}$ be the query, key, and value matrices of the sequence input X. During training, we can use the matrix multiplication form: $O = (Q K^T \\odot M_i)V$, where $M_i$ is a causal mask. At inference time, we can use the recurrent form for efficient sequential processing.\nHowever, despite its computational efficiency, linear attention has notable limitations compared to softmax attention. The dot-product approximation in linear attention lacks the nonlinear normalization of softmax, often resulting in a more uniform distribution of attention weights Han et al. (2023). This uniformity can impair the model's ability to focus sharply on specific and relevant tokens. Consequently, linear attention models may underperform in tasks requiring precise in-context retrieval or focused attention on particular input segments Han et al. (2023)."}, {"title": "MAMBA-2", "content": "Mamba Gu & Dao (2023) is a variant of structured state space models (SSMs) that uses the selective data-dependent mechanism. Mamba-2 Dao & Gu (2024) builds on this foundation, revealing deep"}, {"title": "TAIPAN MODEL", "content": "To address the limited modeling capabilities of Mamba-2 and Linear Attention while preserving their computational efficiency, we introduce Taipan, a new architecture for sequence encoding in language modeling. In Taipan, we strategically incorporate Selective Attention Layers (SALs) within the Mamba framework, as shown in Figure 2. SALs are inserted after every K Mamba-2 blocks, creating a hybrid structure that combines Mamba-2's efficiency with Transformer-style attention for effective sequence representation.\nThe core of SALs is a gating network that identifies important tokens for enhanced representation modeling. These tokens undergo two phases: (1) feature refinement to filter out irrelevant information and (2) representation augmentation via softmax attention. This allows Taipan to capture complex, non-Markovian dependencies when necessary.\nTaipan processes input through Mamba-2 blocks, with SALs periodically refining key token representations. These enhanced representations are then passed into the subsequent Mamba-2 layers,"}, {"title": "SELECTIVE ATTENTION LAYERS", "content": "Selective Attention Layers (SALs) are the key innovation in Taipan, designed to enhance the model's ability to focus on critical tokens while maintaining overall efficiency. These layers employ a lightweight gating network $G_\\theta$ to dynamically determine which tokens should undergo softmax attention processing.\nFor each token hidden representation $h_i$ in the input sequence, the gating network G computes a score vector:\n$s_i = G_\\theta(h_i)$\nwhere $G_\\theta : R^d \\rightarrow R^2$ is parameterized by $\\theta$. This score vector $s_i = [s_{i,0}, s_{i,1}]$ serves two purposes: 1) it is used to generate a binary mask $m_i$ for token selection, and 2) it guides feature refinement.\nTo maintain differentiability while allowing for discrete token selection, we employ the Straight-Through Gumbel-Softmax trick Jang et al. (2017). A binary mask $m_i$ is generated from $s_i$ to select tokens during the forward pass of the network:\n$m_i = argmax(GumbelSoftmax(s_i, \\tau))$\nwhere $\\tau$ is the temperature parameter. $h_i$ will only be selected for attention processing if $m_i = 1$.\nFor the backward pass, we instead use continuous Gumbel-Softmax approximation of $m_i$ to achieve computation differentiability for the network:\n$m_i = \\frac{I[m_i=0]exp((s_{i,0}+g_0)/\\tau) + I[m_i=1]exp((s_{i,1}+g_1)/\\tau)}{exp((s_{i,0}+g_0)/\\tau) + exp((s_{i,1}+g_1)/\\tau)}$\nwhere $I[]$ is the indicator function, and $g_0$ and $g_1$ are i.i.d samples from the Gumbel(0, 1) distribution. In this way, we are able to train our entire model, including the gating network, in an end-to-end fashion for language modeling.\nFor the selected tokens (those with a mask value $m_i$ of 1), we compute their attention-based representations:\n$o_i = Attention(q_i, K, V)$\nwhere $q_i$ is the query vector for the i-th selected token (denoted $h_i'$), and K and V are the key and value matrices for previous tokens.\nIn our model, the score vector $s_i$ is also used to refine the representations of selected tokens. We employ the softmax of $s_i$ to compute the mixing weights: $[1-a_i, a_i] = softmax(s_i)$. The final output for a selected token $h_i'$ is a weighted combination:\n$h_i' = (1 - a_i)h_i + a_io_i$"}, {"title": "SLIDING WINDOW ATTENTION", "content": "To maintain linear time complexity while leveraging the benefits of attention, Taipan employs Sliding Window Attention (SWA) Beltagy et al. (2020). SWA's computational complexity scales linearly with sequence length, allowing Taipan to handle theoretically unlimited context lengths during inference. Importantly, the combination of Selective Attention and Sliding Window Attention in Taipan leads to a significantly sparser attention weight map compared to full attention or standard windowed attention (Figure 3), thus enhancing the computational efficiency of Selective Attention for processing long sequences for our model. In addition, the sparser attention map allows us to afford a longer sliding window (i.e., w = 2048 in our work) to effectively capture longer-range dependencies for input sequences. In this way, our designed Taipan architecture offers a mechanism to balance the efficient processing of long sequences with the ability to capture important long-range dependencies, thereby addressing a key limitation of existing efficient attention mechanisms. Finally, removing positional embeddings from the Attention Module improves extrapolation capabilities, suggesting that the model can better generalize temporal relationships. We explore this impact of positional embeddings in more detail in Section 5.2."}, {"title": "TRAINING AND INFERENCE", "content": "To better balance efficiency and expressiveness, we introduce an attention budget constraint. Given a predefined budget C, representing the desired fraction of tokens to receive attention, we incorporate a constraint loss into our training objective:\n$L_{constraint} = \\frac{C}{\\sum_{n=1}^N \\frac{1}{L} \\sum_{i=1}^L m_i}^2$\nHere, N is the number of SALs, L is the sequence length, and $\\sum_{i=1}^L m_i$ represents the number of tokens selected for attention processing. During training, we employ the Straight Through Gumbel Softmax estimator for $m_i$ in the backward pass Jang et al. (2017); Bengio et al. (2013), ensuring differentiability while maintaining discrete token selection in the forward pass, thereby enabling end-to-end training of the entire model. As such, our overall training objective includes a standard cross-entropy loss $L_{CE}$ for language modeling and the budget constraint term: $L = L_{CE} + \\lambda L_{constraint}$, where $\\lambda$ is a hyperparameter.\nDuring inference, Taipan processes input tokens sequentially through Mamba-2 blocks. At each Selective Attention Layer, the gating network $G_\\theta$ computes a score vector $s_i = G_\\theta(h_i)$ for each token representation $h_i$. This score computes a binary mask $m_i$ to determine if $h_i$ should be used for attention processing. Consequently, our selective attention approach maintains Mamba-2's efficiency for most tokens while applying targeted attention to critical elements, enabling effective long-range dependency modeling with minimal computational overhead."}, {"title": "EXPERIMENTS", "content": "We conducted extensive experiments to evaluate Taipan's performance across various scales and tasks. Our evaluation strategy focuses on three main areas: (1) zero-shot evaluation on diverse"}, {"title": "ABLATION STUDY", "content": "We conducted a comprehensive ablation study to investigate the effect of the two key components in Taipan's architecture, i.e., the attention budget capacity C and the inclusion of Positional Embeddings in the SALs, on its performance and efficacy."}, {"title": "EFFECT OF ATTENTION BUDGET CAPACITY", "content": "Our first experiment aimed to determine the optimal value of Capacity C that would maintain computational efficiency while maximizing performance on downstream tasks. We trained multiple variants of Taipan, each with 1.3B parameters, using different Capacity C values: 0.10, 0.15, 0.20, and 0.25. Each variant was trained for 24,000 steps, allowing us to observe both the immediate impact of different C values and their effect on model performance over time.\nWe evaluated the performance of each variant at regular intervals on two representative tasks: SWDE Arora et al. (2024) (for structured information extraction) and HellaSwag Zellers et al. (2019) (for commonsense reasoning). These tasks were chosen to assess both the model's ability to handle long-context retrieval and its general language understanding capabilities."}, {"title": "IMPACT OF POSITIONAL EMBEDDINGS", "content": "Our second experiment investigated the impact of Positional Embeddings in Taipan's Attention mechanism, focusing on the model's ability to handle and generalize to various context lengths. We trained two variants of the 1.3B parameter Taipan model for 24,000 steps with a fixed context length of 4096 tokens. One variant incorporates Rotary Positional Embeddings Su et al. (2024) in the Selective Attention layers, while the other excludes them. Figure 6 illustrates the performance of both variants in terms of perplexity across different context lengths."}, {"title": "RELATED WORK", "content": "Our approach builds on a foundation of relevant previous research. We will now discuss key studies that inform our methodology.\nState Space Models: SSMs have emerged as a promising approach in attention-free architectures for language processing tasks. These models offer improved computational and memory efficiency compared to traditional attention-based models. The development of SSMs has progressed through several key iterations: S4 Gu et al. (2021a) introduced the first structured SSM, focusing on diagonal and diagonal plus low-rank (DPLR) structures. Subsequent variants like DSS Gupta et al. (2022), S4D Gu et al. (2022), and S5 Smith et al. (2023) improved on this foundation. Frameworks like GSS Mehta et al. (2023), H3 Fu et al. (2023), and RetNet Sun et al. (2023) incorporated SSMs into broader neural network architectures, often combining them with gating mechanisms or efficient attention approximations. Recently, Mamba Gu & Dao (2023) introduced time-varying or selective SSMs, which addresses limitations of static dynamics in previous SSMs by incorporating input-dependent state transitions, leading to improved performance in various tasks.\nHybrid Architecture: Several recent studies H3 Fu et al. (2023), Griffin De et al. (2024), Zamba Glorioso et al. (2024), Jamba Lieber et al. (2024) suggest the potential of blending SSM and the attention mechanism. These hybrid designs show promise in outperforming both traditional Transformers and pure SSM architectures, such as Mamba, particularly in scenarios requiring in-context learning capabilities.\nLong Context Models: Recent advancements in sequence modeling have pushed the boundaries of context length, each with distinct approaches and challenges. Recurrent Memory Transformer Bulatov et al. (2023) demonstrated 1M token processing, but primarily on synthetic memorization tasks. LongNet Ding et al. (2023) proposed scalability to 1B tokens, yet practical evaluations were limited to sequences under 100K tokens. Hyena/HyenaDNA Poli et al. (2023); Nguyen et al. (2023) claimed 1M token context, but faced efficiency issues at longer lengths. Mamba Gu & Dao (2023) showed consistent improvements up to 1M tokens in DNA modeling and competitive performance across various language tasks."}, {"title": "CONCLUSION", "content": "Taipan presents a significant advancement in long-context language modeling by combining the efficiency of Mamba with strategically placed Selective Attention Layers. Our experiments demonstrate Taipan's superior performance across various scales and tasks, particularly in scenarios requiring extensive in-context retrieval, while maintaining computational efficiency. A key insight is that not all tokens require the same computational resources. Taipan's architecture leverages this observation through its selective attention mechanism, which dynamically allocates computational resources"}]}