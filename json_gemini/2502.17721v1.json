{"title": "Aligning Compound AI Systems via System-level DPO", "authors": ["Xiangwen Wang", "Yibo Jacky Zhang", "Zhoujie Ding", "Katherine Tsai", "Sanmi Koyejo"], "abstract": "Compound AI systems, comprising multiple interacting components such as LLM agents and external tools, demonstrate state-of-the-art results across diverse tasks. It is hence crucial to align components within the system to produce consistent results that match human expectations. However, conventional alignment methods, such as Direct Preference Optimization (DPO), are not directly applicable to compound Al systems. These challenges include the non-differentiable interactions between components, making end-to-end gradient optimization infeasible. Additionally, system-level preferences cannot be directly translated into component-level preferences, further complicating alignment. We address the issues by formulating compound AI systems as Directed Acyclic Graphs (DAGs), capturing the connections between agents and the data generation processes. We propose a system-level DPO (SysDPO) to jointly align compound systems by adapting the DPO to operate on these DAGs. We study the joint alignment of an LLM and a diffusion model to demonstrate the effectiveness of our approach. Our exploration provides insights into the alignment of compound AI systems and lays a foundation for future advancements.", "sections": [{"title": "Introduction", "content": "Compound AI systems, which consist of multiple interacting AI components\u00b9, serve as promising frameworks to push beyond the model capabilities and achieve state-of-the-art performance (Zaharia et al. 2024; Chen et al. 2024; Kandogan et al. 2024; Lin et al. 2024). For example, ChatGPT integrates a large language model (LLM), a DALL-E image generator, a web browser plugin, and more (Achiam et al. 2023). A multi-agent system consisting of multiple LLMs working collaboratively, e.g., Mixture-of-Agents (MoA), achieves improved performance compared to a single agent (Wang et al. 2024). A Retrieval-Augmented Generation (RAG) system combines large language models with information retrieval capabilities and is able to answer time-sensitive queries. A multi-LLM routing system includes a router that dynamically selects among a diverse set of models to maximize the overall performance (Hu et al. 2024).\nIt is, therefore, crucial to ensure that the outputs of a compound AI system align with human preferences and that each component within the system is aligned to collaborate effectively (Lin et al. 2024). However, such coordination does not come naturally by simply integrating multiple pre-trained models; we demonstrate a failure case of the coordination between an LLM (GPT-4) and a diffusion model (DALL-E) in Figure 1. This demonstrates the critical need to develop a new framework to align compound AI systems.\nWhile there are many effective ways to align monolithic models with human preference (Rafailov et al. 2024; Ziegler et al. 2019; Bai et al. 2022), aligning compound systems remains an open problem. Standard methods such as Direct Preference Optimization (DPO) (Rafailov et al. 2024) and Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al. 2019; Bai et al. 2022) are not directly applicable to compound systems for three primary reasons. First, components in a compound Al system communicate in a non-differentiable way such as through plain text, which prohibits an end-to-end gradient optimization or RLHF. Second, aligning each component separately is problematic because the overall system's preferences cannot be decomposed into the preferences of individual components. Effective collaboration among components is critical but not easily captured by aligning them individually. Third, while alignment datasets may exist for the system's overall task, they are often not available for the unique sub-tasks of individual components.\nIn light of these challenges, there is an urgent need to develop methodologies for aligning compound AI systems. While recent studies have explored prompting techniques and instruction tuning approaches (Yuksekgonul et al. 2024; Lin et al. 2024; Shinn et al. 2024), these solutions only partially address the fundamental challenges. To tackle these challenges, we make the following contributions:\n\u2022 We formally define the problem of preference learning and alignment of compound AI system and then propose SysDPO to align the entire compound AI system;\n\u2022 We show how SysDPO can be applied to align a compound AI system composed of an LLM agent and a text-to-image diffusion model;\n\u2022 We demonstrate that aligning compound AI systems increases the success rate in handling complex instructions and coordinating components.\nThese results deepen our understanding of the alignment challenges in compound AI systems and provide a foundation for future research."}, {"title": "The SysDPO Framework", "content": "In this section, we introduce the SysDPO pipeline. We start by modeling the structure of compound AI systems as Directed Acyclic Graphs (DAGs), which encode both the connections between agents and the flow of the underlying data generation process. The DAG structure enables us to factorize the joint probability of generated outputs into several components, resolving the non-differentiability issue when aligning multiple agents. We then define a DPO-based loss function that can be optimized from end-to-end simply via gradient descent. The end-to-end optimization ensures that each agent is aligned with user-defined preferences. Below, we outline the key steps in the pipeline:\n1. System Representation. We represent the compound Al system as a Directed Acyclic Graph (DAG). We define nodes as $x, \\{Y_i\\}_{i\\in I}, \\{z_j\\}_{j\\in J}$, where $x$ is the input, $y_i$ for $i \\in I$ are intermediate outputs and $z_j$ for $j \\in J$ are final outputs. Except for the input $x$, each node represents a generated output, given by a single model or an external tool based on some other nodes. We define the set of all generated outputs as $s = \\{Y_i, Z_j\\}_{i\\in I,j\\in J}$. The directed edges represent the flow of the generated data between components.\nLet us consider two concrete examples of a compound AI system. The first example involves an LLM and a diffusion model as shown in Figure 1 with the user prompt $x$ being \"generate three separate images of ....\". The DAG of this example is shown in Figure 2 (a). The second example is MoA (Wang et al. 2024). It leverages the collective power of multiple LLMs through a layered architecture, where each agent combines outputs from the preceding layer as the auxiliary information to generate responses. We formulate the DAG for a two-layered MoA composed of three models, as shown in Figure 2 (b).\n2. Probability Factorization. The DAG structure encodes the conditional independence of the generated data (Pearl 2009), enabling the decomposition of the probability of the generated data into multiple terms:\n$P_{\\theta}(s|x) = \\prod_{i \\in I, j \\in J} P_{\\theta_i}(y_i|P(y_i)) \\cdot P_{\\theta_j}(z_j|P(z_j)),$ (1)\nwhere $P(\\cdot)$ returns the parent nodes (may include the input $x$) of a given node in the graph, and $\\theta = \\{\\theta_k : k \\in I \\cup J\\}$ denotes the parameter set of models in the compound AI system. This decomposition, derived from the DAG structure, breaks down the likelihood of system generation into a product of multiple terms, where each term contains a single model, allowing model-dependent optimization. Take the case of Figure 2 (a) as an example and denoting the set of generated contents as $s = \\{Y_1, Y_2, Y_3, Z_1, Z_2, Z_3\\}$, we have\n$p(s|x) = \\prod_{i=1}^{3} P_{\\theta_1}(Y_i|x) \\cdot P_{\\theta_2}(Z_i|Y_i)$.\nFor external tool $\\theta_i$ integrated within the system, the probability factorization $p_{\\theta_i}(y_i|P(y_i))$ is set to 1, assuming that external tools provide deterministic outputs."}, {"title": "Preference Dataset Construction", "content": "3. Preference Dataset Construction. SysDPO optimizes for pairwise preferences by leveraging a preference dataset. The dataset can be obtained in the following way: given a query $x$, the system generates two versions of the responses, which include outputs of every agent. We label the preferred set as $s^w$, and the not-preferred set as $s^l$.\n4. Loss Function Design. Given such a dataset $D$ composed of preference pairs $(x, s^w, s^l)$ and a compound AI system formulated as a DAG, we can apply DPO to align the system (Rafailov et al. 2024):\n$\\mathcal{L}(\\theta) = -E_{(x,s^w,s^l)\\sim D} \\log \\sigma \\left( \\beta \\left( \\log \\frac{P_{\\theta}(s^w|x)}{P_{\\Theta}(s^w|x)} - \\log \\frac{P_{\\theta}(s^l|x)}{P_{\\Theta}(s^l|x)} \\right) \\right),$ (2)\nwhere $\\Theta$ denotes the collection of reference models, $\\sigma(\\cdot)$ stands for the sigmoid function. By decomposing $p_{\\theta}$ via (1) in the DPO loss, we derive a differentiable loss function tailored for compound AI systems, which we refer to as the SysDPO loss. Unlike the original DPO loss, which optimizes individual models, SysDPO integrates probability decomposition to capture interactions between multiple components in compound AI systems."}, {"title": "Application: Compound AI System of a LLM and a Diffusion Model", "content": "In this section, we apply SysDPO to a group-image-generation application with an example in Figure 1, which involves an LLM $\\psi$ and a Diffusion Model $\\phi$. For a single input $x$ provided to the system, the LLM generates an intermediate output $y$, which can be parsed to multiple captions $y_1, y_2, ..., y_n$. Each $y_i, i = 1, ..., n$ serves as a prompt for the diffusion model. The diffusion model is then queried $n$ times, generating images $z_1, z_2, ..., z_n$ as the final outputs. This multi-step process is modeled as a DAG whose special case (n = 3) is shown in Figure 2 (a), and it allows us to decompose the generation process by\n$p(s|x) = p_{\\psi}(y|x) \\cdot \\prod_{i=1}^{n} p_{\\phi}(z_i|y_i).$ (3)\nNote that, for better readability, we adopt a different notation for the models in this section as opposed to the notation used in Section 2. Apply the decomposition of probability equation 3 to the loss function equation 2, we get the joint loss function of this system\n$\\mathcal{L}(\\psi, \\phi) = -E_{(x,s^w,s^l)\\sim D} \\left[ \\log \\sigma \\left( \\beta \\left( \\log \\frac{p_{\\psi}(y^w|x)}{p_{\\Psi}(y^w|x)} - \\log \\frac{p_{\\psi}(y^l|x)}{p_{\\Psi}(y^l|x)} + \\sum_{i} \\log \\frac{p_{\\phi}(z^w|y_i^w)}{p_{\\Phi}(z_i^w|y_i^w)} - \\sum_{i} \\log \\frac{p_{\\phi}(z^l|y_i^l)}{p_{\\Phi}(z_i^l|y_i^l)} \\right) \\right) \\right],$ (4)\nwhere $s^w = \\{y^w, z_1^w, z_2^w, ... z_n^w\\}$, and $\\Psi$, $\\Phi$, are reference models. The language model's generation likelihood $p_{\\psi}(y|x)$ is accessible, while the diffusion model's $p_{\\phi}(z|y)$ is not. The following subsection will handle this challenge by delving into the generation process of diffusion models."}, {"title": "Handling the Diffusion Model", "content": "3.1 Handling the Diffusion Model\nTo obtain the diffusion model's generation likelihood, we build upon (Wallace et al. 2024), which applies DPO to denoising diffusion probabilistic models (Ho, Jain, and Abbeel 2020), and extend it to accommodate our framework. Details of the derivation and the theorem are in Appendix A.\nA diffusion model learns to reverse a diffusion process, represented by a sequence $z_{0:T} := (z_0, z_1,..., z_T)$, where the original image $z_0$ is gradually transformed into standard Gaussian noise $z_T$ over $T$ steps. By learning to reverse this process, the model generates images by progressively denoising $z_T$, starting from noise and reconstructing the original image $z_0$. The likelihood of the reverse process is\n$p_{\\phi}(z_{0:T}|y) = p(z_T) \\prod_{t=1}^{T} p(z_{t-1}|z_t, y),$ (5)\nwhere each $p_{\\phi}(z_{t-1}|z_t, y)$ is a Gaussian density function. However, the diffusion model does not directly provide the likelihood $p_{\\phi}$, even for a small Gaussian step $p_{\\phi}(z_{t-1}|z_t, y)$. To this end, Ho, Jain, and Abbeel (2020) proposed a denoiser $\\epsilon_{\\phi}$, which predicts the original image from a noisy input and can be applied to approximate the likelihood. Such denoiser can be learned from data by optimizing the following objective function:\n$l_{\\epsilon}(\\phi;t, z_{i,t}, y) := [w_t ||\\epsilon - \\epsilon_{\\phi}(z_{i,t}, t, y) ||^2],$ where $w_t$ is a weight parameter, $z_{i,t}$ is the $i$-th output at timestep $t$, and $\\epsilon$ corresponds to the noise added to $z_{i,0}$ from which $z_{i,t}$ is derived. Similarly, we use $l_{\\epsilon}(\\phi;t, z_{i,t}, y)$ to denote the denoising loss for the losing data.\nWe prove the following theorem, which converts equation 4 into a loss function that directly utilizes the denoiser loss function, thereby making the loss function optimizable.\nTheorem 1. The loss function (4) is upper bounded by\n$\\mathcal{L}(\\psi, \\phi) \\leq -E_{(x,s^w,s^l)}E_t E_{\\epsilon} \\left[ \\log \\sigma \\left( \\beta \\left( \\log \\frac{p_{\\psi}(y^w|x)}{p_{\\Psi}(y^w|x)} - \\log \\frac{p_{\\psi}(y^l|x)}{p_{\\Psi}(y^l|x)} + T \\sum_{i} \\left(-l_{\\epsilon}(\\phi;t, z_{i,t}^w, y_i^w) + l_{\\epsilon}(\\phi;t, z_{i,t}^l, y_i^w) \\right) \\right) \\right) \\right].$\nThus, we obtain a tractable loss function for SysDPO."}, {"title": "Experiments", "content": "We evaluate the effectiveness of SysDPO alignment in a compound Al system described in section 3. We train and evaluate the system on a dataset of multi-modal progression tasks, where the system generates sequences of images with a specific scene-related attribute that varies progressively. Examples of inputs and outputs are provided in Appendix E. Our evaluation focuses on the coherence among images and their alignment with holistic preferences.\nDataset Construction. We constructed a custom dataset using the following steps:\n1. Attribute Selection: We use a regressor from Zhuang, Koyejo, and Schwing (2021) which gives scores from [0,1] to images based on 40 distinct scene-related attributes (e.g., brightness, coldness, fog density)."}, {"title": "Instruction Design", "content": "2. Instruction Design: For each attribute, we query GPT-4 to generate 250 user prompts of generating a sequence of images representing the progression of the intensity of that attribute. To ensure the diversity of user prompts, we generate prompts using four distinct prompt styles from Qin et al. (2024). Details are provided in Appendix D.\n3. Constructing Chosen and Rejected Pairs: For each user prompt, four image sequences are generated and ranked using the Preference Score $q$, described below in equation 5. Six comparison pairs are constructed from the four samples. The instance among the two pairs with the higher preference score is marked as the preferred. The dataset contains a total of 6000 comparison pairs.\nPreference Score. To compare the generated image sequences, we define a preference score $q$ that evaluates both order consistency and distribution evenness. This metric is based on the attribute scores assigned to the images by the regressor from Zhuang, Koyejo, and Schwing (2021). Given a sequence of three images with attribute scores $a_1, a_2$, and $a_3$, the Preference Score $q$ is computed as:\n$q = \\left( - \\vert a_1 - a_3 \\vert + \\vert a_2 - \\frac{(a_1 + a_3)}{2} \\vert \\right)$ (5)\nSequences with higher $q$ values are preferred, as they reflect correct ordering and smoother distributions. Conversely, reversed or uneven sequences result in lower $q$.\nFor further details, including examples illustrating the calculation of $q$, please refer to Appendix C.\nModels. For dataset construction and evaluation, we use an instruction-tuned Llama-3-8B model (AI@Meta 2024) as the language model. To generate image sequences for constructing chosen and rejected samples in the dataset, we employ Stable Diffusion XL (SDXL) (Podell et al. 2023). For training purposes, we use Stable Diffusion 1.5 (Rombach et al. 2022) which provides a balance between computational efficiency and generation quality.\nEvaluation. The performance of the system is evaluated using two metrics. The first metric is the Average Preference Score across all generated sequences from the test dataset. The second evaluation metric is the Order Consistency Ratio, measuring the proportion of generated sequences in the correct order, i.e., where $a_1 < a_2 < a_3$.\nBaselines. To evaluate the effectiveness of the proposed SysDPO joint alignment approach, we compare it against four baseline methods."}, {"title": "System Before Alignment", "content": "1. System Before Alignment. The first baseline represents the system prior to applying SysDPO. Notably, Llama-3-8B-it is instruction-tuned, so that it serves as a baseline for conventional separately aligned systems.\n2. Best-of-4 Sampling Baseline. For this baseline, we sample four image sequences generated by the system without optimization. For each user prompt, we select the best-performing sequence based on the Preference Score. The average of the selected sequences is reported.\n3. Only Train Language Model or Diffusion Model. In this baseline, we freeze the weights of the diffusion model or language model and train only another model using the dataset and proposed loss function of SysDPO."}, {"title": "Results", "content": "Results. This section presents the performance of the proposed SysDPO compared to the baselines. We evaluate the system using the Preference Score and Order Consistency Ratio. Examples of system outputs before and after training can be found in Appendix E.\nThe results in Table 1 demonstrate the importance of alignment in compound AI systems and the effectiveness of the proposed SysDPO alignment approach. The \"System Before Alignment\u201d baseline achieves poor performance, with a low Preference Score and a low Order Consistency Ratio (32%), indicating that conventionally instruction-tuned components are insufficient for ensuring coherent collaboration in compound systems. The \"Only Train Language Model\" baseline achieves significantly better results than the \"Only Train diffusion Model\", with a Preference Score of 0.23 and a Ratio of 65%. This is due to the LLM's role in generating captions that control output sequences, influencing the overall progression and coherence of the system. SysDPO achieves the best Preference Score (0.25) and the highest Order Consistency Ratio (70%). These results validate the effectiveness of our SysDPO algorithm, demonstrating its ability to optimize both components together for superior performance in generating coherent image sequences."}, {"title": "Discussion and Future Work", "content": "Our preliminary investigations indicate that the proposed formulation and methodology are promising for aligning compound AI systems. However, further experimental investigations are necessary to evaluate its potential comprehensively. For instance, how does our approach compare to existing techniques, such as instruction tuning and prompting strategies? Additionally, the scalability of our method to more complex applications, where the number of components and interactions grows significantly, remains open.\nDespite these open questions, our work establishes a solid foundation for aligning compound AI systems as cohesive entities. We believe that the insights and framework presented here pave the way for promising advancements in this area of research."}]}