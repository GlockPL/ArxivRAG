{"title": "The Minimal Search Space for Conditional Causal Bandits", "authors": ["Francisco N. F. Q. Simoes", "Itai Feigenbaum", "Mehdi Dastani", "Thijs van Omen"], "abstract": "Causal knowledge can be used to support decision-\nmaking problems. This has been recognized in the\ncausal bandits literature, where a causal (multi-\narmed) bandit is characterized by a causal graphi-\ncal model and a target variable. The arms are then\ninterventions on the causal model, and rewards\nare samples of the target variable. Causal ban-\ndits were originally studied with a focus on hard\ninterventions. We focus instead on cases where\nthe arms are conditional interventions, which\nmore accurately model many real-world decision-\nmaking problems by allowing the value of the\nintervened variable to be chosen based on the\nobserved values of other variables. This paper\npresents a graphical characterization of the min-\nimal set of nodes guaranteed to contain the opti-\nmal conditional intervention, which maximizes\nthe expected reward. We then propose an efficient\nalgorithm with a time complexity of O(|V|+|E|)\nto identify this minimal set of nodes. We prove\nthat the graphical characterization and the pro-\nposed algorithm are correct. Finally, we empiri-\ncally demonstrate that our algorithm significantly\nprunes the search space and substantially acceler-\nates convergence rates when integrated into stan-\ndard multi-armed bandit algorithms.", "sections": [{"title": "1. Introduction", "content": "Lattimore et al. (2016) introduce a class of problems termed\ncausal bandit problems, where actions are interventions on\na causal model, and rewards are samples of a chosen reward\nvariable Y belonging to the causal model. They focus on\nhard interventions, where the intervened variables are set to\nspecific values, without considering the values of any other\nvariables. We will refer to this as a hard-intervention causal\nbandit problem. They propose a best-arm identification al-\ngorithm that utilizes observations of the non-intervened vari-\nables in the causal model to accelerate learning of the best\narm as compared to standard multi-armed bandit (MAB)\nalgorithms. Causal bandits have applications across a broad\nrange of domains, particularly in scenarios requiring the\nselection of an intervention on a causal system. These in-\nclude computational advertising and context recommenda-\ntion (Bottou et al., 2013; Zhao et al., 2022), biochemical\nand gene interaction networks (Meinshausen et al., 2016;\nBasharin, 1959), epidemiology (Joffe et al., 2012), and drug\ndiscovery (Michoel and Zhang, 2023).\nMost of the work in causal bandits (see Section 7) focuses on\ndeveloping MAB algorithms which incorporate knowledge\nabout the causal graph.\nLee and Bareinboim (2018), in contrast, use the fact that\nthe causal graph is known not to develop yet another MAB\nalgorithm, but to reduce the set of nodes (i.e. variables)\nof the causal graph on which hard interventions should\nbe examined, thereby reducing the search space for hard-\nintervention causal bandit problems. In more detail, they\ndefine the SCM-MAB problem, where the agent has access\nto the causal graph G = (V, E) of a structural causal model\n(SCM) and wants to maximize a target variable Y \u2208 V by\nplaying arms which are hard interventions on subsets of\nV. Their search space reduction algorithm identifies the set\nof all (minimal) subsets X of V such that there exists an\nSCM with graph G for which some hard, multi-node inter-\nvention do(X = x) has maximal $E_{y \\sim p_{do(x=x)}}[Y]$. Lee and\nBareinboim (2019) and Lee and Bareinboim (2020) extend\nthe approach of Lee and Bareinboim (2018) to the cases\ninvolving non-manipulable variables and mixed policies,\nrespectively (see Section 7).\nIt is recognized in the MAB literature that, for many if not\nmost applications, actions are taken in a context, that is,\nwith available information (Lattimore and Szepesv\u00e1ri, 2020;\nAgarwal et al., 2014; Dudik et al., 2011; Jagerman et al.,\n2020; Langford and Zhang, 2007). E.g., content recommen-\ndation based on the user's demographic characteristics, such\nas age, gender, nationality and occupation. Similarly, in\ncausality, conditional interventions where a variable X\nis set to a value g(Z) through some policy g after observing\nother variables (a context) Z are more realistic than hard"}, {"title": "2. Preliminaries", "content": "Graphs and Causal Models We will make use of Di-\nrected Acyclic Graphs (DAGs). The main concepts of DAGs\nand notation used in this paper are reviewed in Appendix A.\nFurthermore, we operate within the Pearlian graphical\nframework of causality, where causal systems are modeled\nusing Structural Causal Models (SCMs) (Peters et al., 2017;\nPearl, 2009). An SCM C is a tuple (V, N, F, PN), where\nV = (V1,..., Vn) and N = (Nv\u2081,..., Nvm) are vectors\nof random variables. The exogenous variables are pairwise\nindependent, and are distributed according to the noise distri-\nbution PN, while each endogenous variable Vi is a determin-\nistic function fv, of its noise variable Nv; and a (possibly\nempty) set of other endogenous variables Pa(Vi) \\{V},\ncalled the (proper) parents of Vi. The Vi and Nv; are called\nendogenous and exogenous (or noise) variables, respectively.\nRv denotes the range of the random variable V. F is a set of\nfunctions $f_{v_i} : \\mathcal{R}_{pa(V_i)} \\times \\mathcal{R}_{Nv_i} \\rightarrow \\mathcal{R}_{V_i}$, termed structural\nassignments. The endogenous variables together with F\ncharacterize a DAG called the causal graph $G_{\\mathcal{C}} := (V, E)$\nof C, whose edge set is $E = {(P,X) : X \\in V, P \\in\nPa(X) \\{X\\}}$. We denote by C(G) the set of SCMs whose\ncausal graph is G. Having an SCM allows us to model in-\nterventions: intervening on a variable changes its structural\nassignment fx to a new one, say fx. This intervention is\nthen denoted do(fx = fx). In the simplest type of inter-\nventions, called atomic interventions, a variable X is set to a\nchosen value x, thus replacing the structural assignment fx\nof X with a constant function setting it to x. Such an inter-\nvention is denoted do(X = x), and the SCM resulting from\nperforming this intervention is denoted $E_{do(X=x)}$. The joint\ndistribution over the endogenous variables resulting from\nthe atomic intervention do(X = x) is denoted $p_{do(X=x)}$\nand called the post-intervention distribution for this inter-\nvention. Each realization n \u2208 RN of the noise variables\nwill be called a unit. A deterministic SCM is an SCM for\nwhich the noise distribution is a point mass distribution with\nall its mass on some (known) unit n \u2208 RN. Finally, nodes\nare denoted by upper case letters, sets of nodes by boldface\nletters, and variable values by lower case letters.\nUnrolled Assignments We will make use of the fact that\nthe structural assignments of the ancestors of an endogenous\nvariable X (including its own structural assignment) can be\ncomposed to express X as a function fx (n) of the vector\nn of exogenous variables values. We call this the unrolled\nassignment of X. The formal definition can be found in\nAppendix B.\nConditional Interventions Given an SCM C =\n(V, N, F, PN) with causal graph G, X \u2208 V and Zx \u2286\nV \\{X\\}, the conditional intervention on X given Zx for\nthe policy g: Rzx \u2192 Rx, denoted do(X = g(Zx)), is the"}, {"title": "3. Conditional-Intervention Superiority", "content": "In this section, we will define a preorder of \u201cconditional-\nintervention superiority\u201d on nodes of an SCM. If X>*W,\nthen W can never be a better node than X to intervene on\nwith a conditional intervention. We will then show that,\nperhaps surprisingly, this relation is equivalent to another\nsuperiority relation, defined in terms of atomic interventions\nin a deterministic SCM.\nDefinition 1 (Conditional-Intervention Superiority). X is\nconditional-intervention superior to W relative to Y in G,\ndenoted X>*W, if for all SCM with causal graph G there\nis a policy g for X such that for all policies h for W,\n$E_{n \\textasciitilde f_{do(X=g(Z_x))}} (n) \\ge E_{n \\textasciitilde f_{do(W=h(Z_w))}} (n)$, (1)\nwhere Zw is a observable conditioning set for V. Equiva-\nlently, for all C(G) one has:\n$max_g E_{n \\textasciitilde f_{do(X=g(Z_x))}} (n) \\ge max_h E_{n \\textasciitilde f_{do(W=h(Z_w))}} (n)$. (2)\nA similar relation can be defined for atomic interventions\nin deterministic SCMs, where the vector N of exogenous\nvariables is fixed to a known value n (see Section 2).\nDefinition 2 (Deterministic Atomic Intervention Superi-\nority). Let X, W, Y be nodes of a DAG G. X is deter-\nministically atomic-intervention superior to W relative to\nY, denoted $X \\geq_{det,a}^Y W$, if for every SCM C with causal"}, {"title": "4. Graphical Characterization of the Minimal\nGlobally Interventionally Superior Set", "content": "Goal Our aim is to develop a method to identify, based on\na causal graph G, the smallest set of nodes that are \u201cworth\ntesting\u201d when attempting to maximize Y by performing one\natomic intervention. Specifically, regardless of the structural\ncausal model C associated with G, we want to ensure that the\noptimal intervention can be discovered within this selected\nset of nodes. We define this set as follows:\nDefinition 7 (GISS and mGISS). Let G be a DAG with set of\nnodes V. A globally interventionally superior set (GISS) of\nG relative to Y, is a subset U of V \\{Y\\} satisfying $U \\geq^Y$\n(V\\{Y\\})\\U. A minimal globally interventionally superior\nset (mGISS) is a GISS which is minimal with respect to set\ninclusion.\nThis set is unique, so that we can talk of the minimal globally\ninterventionally superior set.\nProposition 8 (Uniqueness of the mGISS). Let G be a DAG\nand Y a node of G. The minimal globally interventionally\nsuperior set of G relative to Y is unique. We denote it by\nmGISSY (G)\nIntuition Since the value of Y is completely determined\nby the values of its parents A1, ..., Am, along with the fixed\nvalue ny of a noise variable that cannot be intervened upon\n(see Definition 2), we aim to induce the parents to acquire\nthe combination of values (a1, ..., am) that maximizes Y\nwhen Ny = ny. If this is not possible to achieve using a\nsingle intervention, we aim to obtain the best combination\npossible. Clearly, the parents of Y themselves need to tested\nby bandit algorithms: there may be one parent on which\nY is highly dependent, in such a way that there is a value\nof that parent which will maximize Y. In the particular\ncase where Y has a single parent A, that node is the only\nnode worth intervening on, since all other nodes can only\ninfluence Y through A. Indeed, if a* \u2208 RA is the value of\nA which maximizes Y, it is not necessary to try to find an\nintervention on ancestors of A which results in A = a*:\njust set A = a* directly (Figure 1c). If Y has two or more\nparents, it is possible that a single intervention on one of the\nAi does not yield the best possible outcome. Instead, a better\nconfiguration (potentially even the ideal case (a\u2081, . . ., am))\nmay be achieved by intervening on a common ancestor of\nsome or all of the Ai (Figure 1a). Notice that Xo is also\na common ancestor of A1, A2, but one is never better off\nintervening on X0 than on X1. This seems to indicate that\ntesting interventions on, for instance, all lowest common\nancestors (LCAs, see Appendix A) of the parents of Y,\nand only them, is necessary. While this works in Figure 1a,\nit fails for a graph such as Figure 1d, where X needs to\nbe tested and yet it is not in LCA(A1, A2) = {A1}. This\nsuggests that we need to define a stricter notion of common\nancestor to make progress in characterizing mGISSY (G).\nDefinition 9 (Lowest Strict Common Ancestors of a Pair of\nNodes). The node V \u2208 V is a strict common ancestor of\nX, Y \u2208 V if V is a common ancestor of X, Y from which\nboth X and Y can be reached from V with paths V --\u2192 X"}, {"title": "5. Algorithm to Find the Minimal Globally\nInterventionally Superior Set", "content": "Algorithm 1 C4\n1: input: DAG G = (V, E), set of nodes U C V\n2: output: The closure L\u221e (U)\n3: SU \u25b7 initialize closure\n4: c[V] \u2190 V for V \u2208 U \u25b7 initalize connectors\n5: c[V] \u2190 NULL for V \u2208 V\\U \u25b7 initalize connectors\n6: for V\u2208 V\\U in reverse topological order do\nC\u2190 {c[V'] : V' \u2208 Ch(V), c[V'] \u2260 NULL}\nif |C| = 1 then\nc[V] \u2190 X where C = {X}\n7:\n8:\n9:\n10:\nelse if |C| > 1 then\n11:\nc[V] \u2190 V, S \u2190 S\u222a {V}\u25b7 V is added to closure\n12: return S\nThe Closure Computation via Children with Multiple\nConnectors (C4) Algorithm (Algorithm 1) computes the\nclosure L\u221e(U) in O(|V| + |E|) time, using connectors:\nDefinition 17 (Connector). Let G = (V, E) be a DAG,\nUCV, V\u2208 V. A node X \u2208 V is a U-connector of V\n(in G) iff X is a maximal element of De(V) \u2229 L\u221e(U) with\nrespect to the ancestor partial order \u5c0f.\nNote that V\u2208 L\u221e(U) iff V is its own connector. A con-\nnector X can be gotten to from V only via paths excluding\nL\u221e(U)\\{X\\}. Lemma 18 shows that in fact the existence of\none such path is sufficient (and necessary) for a node to be\na connector; furthermore, it establishes that a connector-if\nit exists is unique, and so we call it the connector. See\nFigure 3 for an example.\nLemma 18 (Uniqueness and Characterization of Connec-\ntors). Let G = (V, E) be a DAG, U \u2286 V, V \u2208 V. If V"}, {"title": "6. Experimental Results", "content": "We evaluate C4 on both random and real graphs. Addition-\nally, we examine the impact of our method on the cumulative\nregret of a bandit algorithm.\nSearch Space Reduction in Random Graphs We ap-\nplied the C4 algorithm to randomly generated DAGs using"}, {"title": "7. Related Work", "content": "Recent research has explored the integration of causality\nand multi-armed bandit (MAB) frameworks. As mentioned\nin Section 1, Lattimore et al. (2016) introduced the original\ncausal bandit problems, which involve hard interventions in\ncausal models. Subsequent works (Sen et al., 2017; Yabe\net al., 2018; Lu et al., 2020; Nair et al., 2021; Sawarni et al.,\n2023; Maiti et al., 2022; Feng and Chen, 2023) proposed\nalgorithms for variants of causal bandits with both hard\nand soft interventions, budget constraints, and unobserved\nconfounders, all under specific assumptions, such as binary\nvariables, simple graphs, or known post-intervention distri-\nbutions. Note that we do not make such assumptions.\nRecent works in \"contextual causal bandits\" address in-\nterventions that account for context, bearing a superficial\nresemblance to our problem. However, our problem remains\ndistinct. In Madhavan et al. (2024), the term \"contexts\" is\nused in a very different way, actually referring to different\ngraphs as opposed to different variable values. Subramanian\nand Ravindran (2022; 2024) tackle the scenario in which an\nintervention is performed, with knowledge of a given set of\ncontext variables, on a pre-chosen variable X that has an\nedge into Y (and no other outgoing edges). This approach\ncan be understood as selecting a conditional intervention\nfor a predefined node from a very simple graph. In contrast,\nin our setting we need to choose what variable to intervene\non to begin with, and there are no restrictions on the causal\ngraph.\nAll of the works described above proposed algorithms which\naim at accelerating learning by utilizing knowledge of the\ncausal model. As explained in Section 1, this contrasts with\nthe work by Lee and Bareinboim (2018; 2019), which, just\nlike our work, uses knowledge of the causal graph to find\na minimal search space (over the nodes) for causal bandits.\nWhile they focus on multi-node, hard interventions, we\nfocus on single-node, conditional interventions."}, {"title": "8. Conclusion", "content": "In this paper, we introduced the conditional causal bandit\nproblem, where the agent only has knowledge of the causal\ngraph G, the arms are conditional interventions, and the\nreward variable belongs to G. The theoretical contributions\ninclude a rigorous, simple graphical characterization of the\nminimal set of nodes which is guaranteed to contain the\nnode with the optimal conditional intervention, and the C4\nalgorithm, which computes this set in linear time. Empiri-\ncal results validate that our approach substantially prunes\nthe search space in both real-world and sparse randomly-\ngenerated graphs. Furthermore, integrating mGISS with a\nUCB-based conditional bandits algorithm showcased im-\nproved cumulative regret curves.\nAs mentioned in Section 7, a possible future research direc-\ntion is to identify the smallest conditioning set(s) Zx, rather\nthan assuming, as we do, that the practitioner or problem set-\nting determines them. Another relevant direction for future\nwork is the incorporation of latent variables. On the practical\nside, instead of combining C4 with the simple CondIntUCB,\none could replace CondIntUCB with any other conditional\nbandit algorithm that leverages the model's causal structure.\nAs discussed in Section 7, no such algorithm currently exists.\nNevertheless, we expect that combining C4 with any future\nalgorithm for causal bandits with conditional interventions\nwill be advantageous, as it reduces the number of arms that\nneed to be considered."}, {"title": "Impact Statement", "content": "Our work proposes tools to enhance the efficiency of AI\nagents in decision-making problems. Solutions to these"}, {"title": "A. Directed Acyclic Graphs", "content": "All graphs in this paper are directed acyclic graphs (DAGs). Every path is assumed to be directed. A path \u03c0 in a graph\nG = (V, E) is a tuple of nodes such that each node X in the path has an outgoing arrow from X to the next node in\nthe tuple12. For X \u2208 V, we denote by Pa(X), Ch(X), De(X) and An(X) the sets of parents, children, descendants and\nancestors of X, respectively. We denote by \u03c0: \u03a7 --\u2192 Y a path starting at node X and ending at node Y, and i denotes the\npath formed by the inner nodes of \u03c0. By abuse of notation, we often perform set operations such as \u03c0\u2081 \u2229 \u03c0\u2082 between paths,\nwhich implicitly means that these operations are performed on the sets of nodes belonging to the paths. Tuples with a single\nnode are also considered to be paths, and are said to be trivial. Also, if B \u2208 \u03c0: X --\u2192 Y, then the paths \u03c0|2 : Z --\u2192 Y and\n\u03c0\u2758z: X --\u2192 Z are the paths resulting from removing from \u03c0 all nodes before and after Z, respectively. Every node is an\nancestor of itself, so that the relation \u3126 defined by X \u2264 Y \u21d4 Y\u2208 An(X) is a partial order. Given a set U of nodes, we\ndenote by max<[U] the set of maximal elements of U with respect to \u3126. We call this the ancestor partial order. If there is\na non-trivial path from X to Y, then Y is said to be reachable from X. The set of common ancestors of nodes X and Y is\ndenoted CA(X, Y) = An(X) \u2229 An(Y) = {Z \u2208 V:Z\u3126X>Z\u3126Y}. Finally, the degree of a node in a DAG is the sum\nof the incoming and outgoing arrows of that node.\nWe also make use of a lesser-known graph theory concept, relevant for this paper: the \"lowest common ancestors\" of nodes\n(X, Y). These are common ancestors that don't reach any other common ancestors, intuitively making them the \u201cclosest to\n(X,Y).\nDefinition 21 (Lowest Common Ancestors in a DAG (Bender et al., 2005)). Let X,Y be nodes of a DAG G = (V, E). A\nlowest common ancestor (LCA) of X and Y is a maximal element of CA(X, Y) with respect to the ancestor partial order\n\u5c0f. The set of all lowest common ancestors of X and Y is denoted LCA(X, Y).\nFor example, in Figure 1a, LCA(A1, A2) = {X1}, whereas in Figure 1b, LCA(A1, A2) = {A1}."}, {"title": "B. Unrolled Assignments", "content": "The structural assignments of an SCM can be utilized to express any endogenous variable as a function of the exogenous\nvariables only. This is achieved by composing the assignments until reaching the exogenous variables. Our proofs will rely\non these functions, which we will refer to as \u201cunrolled assignments\", since we \u201cunroll\u201d the expressions for the endogenous\nvariables until only exogenous variables are left. We define them formally by induction as follows:\nDefinition 22 (Unrolled Assignment). We define the unrolled assignment fx : RN \u2192 Rx of any (exogenous or endogenous)\nvariable X from an SCM C = (V,N,F,pn) by induction. For X = N\u00bf \u2208 N, define fx(n) := n\u017c. Now, let < be a\ntopological order on G where the first elements are the endogenous variables with no endogenous parents. Let S be the\nposet (V, \u2264). In ascending order, take X \u2208 S, and define:\n$f_x(n) := \\begin{cases}\n  f_x(n_x), & \\text{if } Pa(X) = \\emptyset\\\\\n f_x (f_{pa(x)} (n), n_x), & \\text{otherwise}\n\\end{cases}$, (9)\nwhere $f_{pa(x)}(n) = (f_{Pa(x)_1} (n),..., f_{Pa(X)_{mx}} (n))$ and mx = |Pa(X)|.\nAdditionally, we can consider X as a function of both exogenous variables and a chosen endogenous variable B. To achieve\nthis, we substitute the assignments until we reach either B or the exogenous variables, thereby \"unrolling\" the dependencies\nuntil we reach the exogenous variables or we are blocked by B.\nDefinition 23 (Blocked Unrolled Assignment). Let X, B endogenous variables from an SCM C = (V, N, F, pn) We define\nthe unrolled assignment fx [B]: RB \u00d7 RN \u2192 Rx of X blocked by B by induction. Let S be the poset from Definition 22.\nIn ascending order, take X \u2208 S, and define:\n$f_x[B](B, n) := \\begin{cases}\n  f_x(n), & \\text{if } X \\notin De(B)\\\\\n B, & \\text{if } X = B\\\\\n f_x(f_{Pa(x)[B]}(B, n), n_x) & \\text{otherwise}\n\\end{cases}$,(10)\nwhere $f_{pa(x)[B]}(n) = (f_{Pa(X)_1[B]}(B, n), \u2026\u2026\u2026, f_{Pa(X)_{m x}[B]}(B, n))$ and mx = |Pa(X)|."}, {"title": "C. Conditional Superiority vs Deterministic Atomic Superiority", "content": "We will show that conditional intervention superiority is equivalent to deterministic atomic intervention superiority. This\nresult will help prove results about the former by making use of the former, which is mathematically simpler and easier to\nreason about.\nNotation. We denote by G* the graph resulting from adding to a causal graph G the exogenous variables as nodes, and an\nedge Nx \u2192 X\u2081 for each exogenous variable Nx.\nLemma 25 (Conditional Intervention vs Atomic Intervention). Let X, Y be endogenous variable of C and Let A be a set of\nendogenous variables of an SCM C, and. When evaluated at a setting n, the unrolled assignment of Y after a conditional\nintervention do(X = g(A)) coincides with the unrolled assignment of Y after the atomic intervention do(X = fa(n)).\nThat is:\n$F_{do(X=g(A))}(n) = f_{do(X=g(f_A(n)))}(n)$.\nProof. This result can be proved by induction in a similar way to Lemma 36.\nLet X be an endogenous variable. We want to prove that the expression holds for any variable Y. We will prove this by\ninduction on a topological order < on the nodes of G* such that the first elements are precisely the exogenous variables, i.e.\nNZ whenever N \u2208 N and Z \u2208 V.\nY\nThe result is true for the exogenous variables. Indeed, for Y \u2208 N, and making use of Lemma 36, we have that\n$f_{do(X=g(f_A(n)))}(n) = f_Y[X](g(f_A(n)),n) = f_Y(n) = Y = f_{do(X=g(A))}(n)$, since Y & De(X) \u222a {X} and Y is\nexogenous (both in the pre- and post-intervention (both conditional and atomic) structural causal models). This establishes\nthe base case of the induction.\nNow let Y be endogenous. For the inductive step, we will prove that, if the result is true for the parents Pag* (Y) of Y in G*\n(induction hypothesis), then it is also true for Y. Assume the antecedent (induction hypothesis). There are three possibilities:\nY \u2208 De(X) \\{X\\}, Y = X or Y \u2209 De(X). In case Y \u2208 De(X) \\{X\\}:\n$f_Y^{do(X=g(f_A(n)))} (n) = f_Y(f_{Pa(Y)}[X](g(f_A(n)), n), n_Y).$\n$\\qquad=^I._H. f_Y (f_{Pa(Y)}^{do(X=g(f_A(n)))}(n), n_Y)$\n$\\qquad= f_Y (f_{Pa(Y)}^{do(X=g(A))}(n), n_Y)$\n$\\qquad= f_Y^{do(X=g(A))}(f_{Pa(Y)}^{do(X=g(A))}(n), n_Y)$\n$\\qquad= f_Y^{do(X=g(A))}(n)$, (11)\nwhere in the second and fourth equalities we used that $f_Y^{do(X=g(f_A(n)))} = f_Y^{do(X=g(A))} = f_Y =$ fY. We also used that Pa(Y) is\nunchanged by these interventions. If instead Y = X, then one has:\n$\\ f_Y^{do(X=g(A))}(n) \\\\ = f_X^{do(X=g(A))}(n) \\stackrel{def}{=} f_X^{do(X=g(A))}(f_{Pa_G^{do(X=g(A))}(X)}(n), n_X)$\\\n$\\qquad \\stackrel{def}{=} f_A(g^{do(X=g(A))}(f_A(n)), n_X)$\\\nand also:\n$\\f_Y^{do(X=g(f_A(n)))}(n) = f_X^{do(X=g(f_A(n)))}(f_{Pa_G^{do(X=g(A))}(X)}(n), n_X)$\\\n$\\qquad= g^{do(X=g(f_A(n)))}(f_A(n), n_X)$.(12)(13)"}, {"title": "D. Intervention Superiority Relations are Preorders", "content": "Proposition 28. The interventional superiority relation between nodes is a preorder in G. The interventional superiority\nrelation between node sets is also a preorder.\nProof. Let G be a DAG and let Y \u2208 G. We will first prove the result for the interventional superiority relation on nodes.\nReflexivity: Let X be a node in G and C \u2208 C(G). For each setting n, the largest value of Y that can be achieved by intervening\non X is attained when setting X to $X^*(n) = arg max f_Y^{do(X=X)}(n)$. Hence, $f_Y^{do(X=X^* (n))}(n) \\ge f_Y^{do(X=X)}(n)$ for all\nX \u2208 Rx, so that X \u226b X.\nTransitivity: assume that Z \u226b W and W \u226b X. Let C \u2208 C(G) and n \u2208 RN. Then $max_w f_Y^{do(W=w)}(n) \\le max_z f_Y^{do(Z=Z)}(n)$. Hence Z \u226b X.\nThis establishes that \u226b is a preorder in G. We now show the result for node sets. Let X, W and Z be sets of nodes in G.\nReflexivity: let X \u2208 X. Since, by reflexivity of \u226b on nodes, we have that X \u226b X, it trivially follows that\nX \u226b X.\nTransitivity: assume that Z \u226b W and W \u226b X. Let X \u2208 X. Then there is W \u2208 W such that W \u226b X. There\nis also Z \u2208 Z such that Z \u226b W. By transitivity of \u226b on nodes, it follows that Z \u226b X. Hence Z \u226b X."}, {"title": "E. Proofs for The Minimal Globally Interventionally Superior Set", "content": "E.1. Uniqueness of the mGISS\nLemma 30 (Elements of a mGISS are not Comparable). Let A \u2286 V be a mGISS relative to Y. Let X, X' \u2208 A and\nX \u2260 X'. Then X' \u226b X.\nProof. Assume X' \u2265 X for the sake of contradiction. We will show that this implies that A \\ X is also a GISS. That is,\nthat for every element of (V \\ Y ) \\ (A \\ X) there is an element of A \\ X which is superior to it. Let W \u2208 (V \\Y) \\(A\\X).\nIf W = X, then X' \u2208 A \\ X and X' \u226b X. If W \u2260 X, then W \u2208 (V \\ Y ) \\ A. Since A is a GISS, we can pick\nX \u2208 A such that X \u226b W. In case X = X, we can choose instead X'. Indeed, since X' \u226b X and X \u226b W,\nwe have by transitivity of \u226b (Proposition 28) that X' \u226b W. This shows that A \\ X C A is a GISS, contradicting"}, {"title": "E.2. The LSCA Closure and A-structures", "content": "It will be useful to know that", "holds\ntrivially": "L\u00ba (U') = U' \u2286 U = L\u00ba(U). Now assume that L\u00b2(U') \u2286 L\u00b2(U) for a given i \u2208 N (induction hypothesis). Let\nV \u2208 LSCA(L\u00b2(U')). Then there are paths V --\u2192 X, V --\u2192 Y with X, Y \u2208 L\u00b2(U') not containing Y and X, respectively.\nBut X, y are also in L\u00b2(U), so that V \u2208 LSCA(L\u00b2(U)). Then LSCA(L\u00b2(U)) \u2286 LSCA(L\u00b2(U)). Using once more the\ninduction hypothesis, it follows that Li+1(U') = LSCA(L\u00b2(U')) U L\u00b2(U') \u2286 LSCA(L\u00b2(U)) UL\u00b2(U) = Li+1(U).\nLemma 32. Let U \u2286 V. If V \u2208 LSCA(U) \\ U, then V forms a A-structure over (U, U).\nProof. Let V \u2208 LSCA(U) \\"}]}