{"title": "Automated Knowledge Graph Learning in Industrial Processes", "authors": ["Lolitta Ammann", "Jorge Martinez-Gil", "Michael Mayr", "Georgios C. Chasparis"], "abstract": "Industrial processes generate vast amounts of time series data, yet extracting meaningful re-\nlationships and insights remains challenging. This paper introduces a framework for automated\nknowledge graph learning from time series data, specifically tailored for industrial applications.\nOur framework addresses the complexities inherent in industrial datasets, transforming them\ninto knowledge graphs that improve decision-making, process optimization, and knowledge dis-\ncovery. Additionally, it employs Granger causality to identify key attributes that can inform\nthe design of predictive models. To illustrate the practical utility of our approach, we also\npresent a motivating use case demonstrating the benefits of our framework in a real-world in-\ndustrial scenario. Further, we demonstrate how the automated conversion of time series data\ninto knowledge graphs can identify causal influences or dependencies between important process\nparameters.", "sections": [{"title": "1 Introduction", "content": "Industrial processes generate vast amounts of time series data, but turning this data into valuable\ninsights remains a challenging task [3]. Our research introduces a new framework that automatically\ncreates knowledge graphs (KGs) from time series data with the help of several established and novel\nmethods. The rationale behind transforming raw data into connected KGs is to help industrial\nstakeholders optimize processes and facilitate knowledge discovery [16].\nOur research introduces a novel approach employing diverse techniques to identify entities and\nrelationships within data, thereby facilitating an interconnected representation of industrial knowl-\nedge. The framework developed improves users' understanding of industrial processes and supports\nthe development of advanced modeling techniques, such as causal-inference models for root-cause\nanalysis and predictive models. This framework is among the first to address industrial knowl-\nedge management from this perspective, providing a solution for integrating and analyzing complex\nindustrial data.\nThe proposed framework is based on four primary components: (a) data pre-processing and\nquality, (b) correlation analysis, (c) causality analysis, and d) KGs generation. The initial steps\nof data pre-processing and quality assurance are standard for industrial sensor time-series data.\nHowever, our approach extends beyond these basics by incorporating correlation and causality\nanalysis techniques to identify a broad spectrum of relationships or influences between process\nparameters. In particular, correlation analysis identifies interrelations or connections in the process\nparameters' behavior, which may include external factors such as weather conditions influencing"}, {"title": "1.1 Related Work", "content": "The industrial ecosystems have been transformed by the emergence of collaborative paradigms\nbetween human-machine systems, including the Internet of Things (IoT), Internet of Services (IoS),\nand Cyber-Physical Systems (CPS). This evolution has shifted the focus toward improving human-\nmachine collaboration [11]. The proliferation of newly connected devices and sensors generates\nvast amounts of data with significant potential value [5]. This data can be used to enable on-\ndemand manufacturing, optimize resources and machine maintenance, and improve other logistical\noperations.\nFurthermore, the challenge of bringing KG applications to industrial settings is rapidly evolving,\nfocusing on theoretical frameworks and practical implementations [6]. Recent studies have explored\na diverse range of use cases [9], demonstrating the versatility and potential of this technology in\nvarious industrial scenarios [1].\nKGs have been proven effective in representing complex industrial data, facilitating tasks such\nas process knowledge graph construction [25], machine parameterization [2], material profiling [26],\nresource allocation optimization [7], and root cause analysis [14]. Furthermore, they have been\nsuccessfully applied in diverse industrial settings, from cognitive manufacturing [12] to developing\nindustry-specific design tools [15]. The literature also shows us the importance of explainable AI\n(XAI) in industrial decision-making, with KGs playing a crucial role in supporting XAI systems\n[20].\nIn addition, the concept of Granger causality [21], which plays a vital role in our framework, has\nnot been deeply explored as an application in industrial settings. Granger causality is a statistical\nhypothesis test for determining whether one measurement can predict another measurement; for\nexample, a signal X causes a signal Y, then past values of X should contain information that\nhelps predict Y. Therefore, it could be used for detecting outliers [18] or irregular patterns [4], for\nexample. In the context of this work, this concept is fundamental because it allows us to gather\nrelevant information about industrial processes and facilitate decision-making around the building\nof predictive models.\nAs research and development in this field continue to advance, we can anticipate a future where\nindustrial processes are increasingly monitored and optimized. As more possibilities are explored\nin terms of managing KGs with machine learning techniques, for example, through the use of KG\nembeddings [8], new applications can be discovered that benefit from having a much more explicit,\nstructured, and interoperable knowledge [19]. Therefore, frameworks such as ours are needed to\nextract and model knowledge with minimal human intervention."}, {"title": "1.2 Contributions", "content": "We propose a framework for automatically generating KGs from industrial time-series data. Our\ntwo main cornerstones of this framework is correlation and causality analysis. In parallel, a modified\nGranger-causality test with automated time-lag selection is introduced to address the challenge of\nidentifying causal relationships without prior knowledge of process details. The major contributions\nof this work can be summarized as follows:\n1.  We introduce a framework that converts raw industrial time series data into structured KGs\nwith minimal manual intervention. This framework employs various techniques to identify\nand categorize entities and relationships within the data accurately.\n2.  We incorporate the concept of Granger causality, which has not been widely explored in indus-\ntrial settings. This concept enables the prediction of certain signal behaviors based on other\nsignals, thus moving away from the prevalent black-box models in industrial environments.\n3.  The interconnected KG provides insights into industrial processes, assisting stakeholders in\nidentifying optimization opportunities and making informed decisions. We demonstrate the\nframework's effectiveness on real-world industrial datasets, showcasing its practical applica-\ntions and the tangible benefits it offers to industrial stakeholders."}, {"title": "1.3 Motivating Example", "content": "To highlight the need for such a framework of automatically generating KGs that captures relation-\nships between process parameters, we consider a real-world industrial use-case, commonly referred\nto as electrostatic particle transfer. It finds applications to different types of industries (e.g., dust\nremoval [17], electrostatic dry powder coating in pharmaceutical industry [23], paper coatings [24],\nabrasive powders [22]). A typical setup of such process is depicted in Figure 1 that describes the\nfunctionality of the electrostatic field in the case of the paper coating industry. In this case, the\ngrains or powder is supplied to a conveyor belt through a funnel with an adjustable opening that\nregulates the flow of material. The conveyor belt moves the material through a static electric field\nformed by two parallel plates. The particles are first electrically charged (with the negative charge)\nand then attracted to the positively charged plate by electrical forces. In the case of paper industry,\nthe paper has already been coated with a special glue that helps the material to stick to the paper\nagainst the gravitational forces. The goal of the process is to create a desirable density of material\n(grains/powder) on the paper through the proper regulation of several configuration parameters,\nsuch as the flow of material, the current/voltage/frequency of the electric field, the distance of the\nplates at the beginning and at the end and the speed of the conveyor belt.\nUnfortunately, there are several challenges that makes the correct calculation of the configuration\nparameters to achieve a desirable density. For example, the humidity of the particles may not\nnecessarily be a-priori known to the operators, or the size of the particles may influence differently the\nmovement of the particles, or aerodynamic phenomena may take place that cannot be predicted in\ndetail. Such exogenous influences prohibit the use of exact physics-based mathematical formulations\nthat can accurately calculate the resulting density of material on the paper. This gives rise to\nthe need of data analysis techniques to understand the influence of the different configuration\nparameters, and also accurately predict the resulting material density on the paper. Furthermore, in\ncases where abnormal behavior is observed, it is useful for the operator to be informed regarding the\nparameter or the combination of parameters that have most likely led to the observed phenomena.\nThese type of questions have motivated the generation of automated KG discovery."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Proposed Framework", "content": "Our research focuses on time series data generated by sensors in industrial processes. This data\ncan help identify patterns, detect anomalies, predict maintenance needs, and improve overall pro-\ncess efficiency. We have developed a framework to integrate these capabilities that facilitate data\ncollection, pre-processing, analysis, visualization, and decision-making support. It is designed to\nmanage static and real-time data streams, offer analytics, and provide intuitive dashboards for\nend-users. The framework includes scalable data storage solutions, supports the development of ad-\nvanced anomaly detection algorithms and predictive maintenance models, and features user-friendly\ninterfaces for easy interaction. A rough sketch of the functionality of the proposed framework is\ndepicted in Figure 2.\nThe framework is implemented as a web application, enabling users to upload sensor data,\nselect specific sensor measurements for analysis, and choose methods for numeric conversion and\nmissing value imputation. It offers various options for selecting these parameters and includes several\nmethods to compare selected measurements, such as Pearson correlation, Spearman rank correlation,\nand Euclidean similarity. Additionally, the framework can easily accommodate other comparison\nmethods. Furthermore, it provides a Granger causality testing methodology for detecting causal\ninfluences between process parameters (including the direction and delay of causal effects).\nThe developed application supports KG creation in RDF format and querying using the SPARQL\nlanguage to explore correlations and causations within the data. This improves the interpretability\nof industrial processes, moving beyond the traditional black box approach where raw data offer\nlimited insights.\nThe visualization functionality facilitates in-depth analysis, while integrated models provide\npredictive insights based on the sensor measurements. The framework's flexibility allows for cus-\ntomization and extension, making it suitable for various research and industrial applications.\nIn the following sections, we describe in more detail the three main methodological elements of"}, {"title": "2.2 Pre-processing", "content": "Pre-processing is a crucial step before conducting our correlation and causality analysis. The idea\nis to ensure the quality and reliability of the results by addressing potential issues in the raw sensor\ndata. Sensor data often has missing values due to sensor malfunctions, transmission errors, or other\nreasons. Our framework offers imputation methods (filling in missing values) like mean imputation,\nmedian imputation, or other more advanced techniques. The choice depends on the nature of the\ndata and the potential impact of missing values on the analysis. The management of categorical\nsensor data is also possible. The reason is that sometimes, there is a need to encode these data into\nnumerical values before performing correlation analysis."}, {"title": "2.3 Correlation analysis", "content": "Correlation analysis examines the strength and direction of relationships between two or more ran-\ndom variables within the same time interval, as depicted in Figure 3. In this context, these random\nvariables may represent different sensor measurements (e.g., temperature, humidity, pressure). We\nemploy several types of correlation metrics, including the following:\nPearson Correlation. Measures the linear relationship between two continuous variables. High\npositive correlation means they tend to increase or decrease together. High negative correlation\nmeans one increases while the other decreases.\nSpearman Rank Correlation. Assesses the monotonic relationship between two variables (not\nnecessarily linear). This is useful when dealing with data that does not strictly follow a normal\ndistribution.\nEuclidean Similarity. Measures the geometric distance between data points. Closer points are\nmore similar. This can be helpful for clustering or finding patterns.\nIn the first version of the framework, we have decided to add these measures because in our expe-\nrience they tend to be some of the most useful, but adding new comparison methods is straightfor-\nward. It is worth noting that the Pearson correlation analysis requires normality of the investigated\ntime-series processes. In most cases, a graphical test is sufficient to check such condition."}, {"title": "2.4 Granger Causality Analysis", "content": "Granger causality, originally developed for economic applications [10], is increasingly used in various\nfields to identify causal-effect relationships between time series data. A special perk is the ability to\nhypothesize about the influence of one time series on another including an estimate of the delay in\nthe effect (time lag) and consequently, the direction of the relation. This is investigated by testing\nfor improvement of a model by including previous time stamps of additional time series. For two\nstochastic processes, $x_t$ and $y_t$, if the forecast predictor of $x_t$ is improved by including all current\nand prior values of $y_t$, then we say that $y_t$ is Granger-causal for $x_t$.\nThe concept of Granger-causality is not restricted by the type of forecast model, however it is\nstraightforward to check it in the context of vector auto-regressive model (VAR). Here, we estimate\nthe full VAR model and test for a significant increase in variance upon exclusion of specific time\nseries or specific time lags using F-tests, i.e. we test for Granger non-causality. Below we describe the\napproach in more detail starting with how the VAR model is used and estimated in this particular\ncase."}, {"title": "VAR(p) model and Granger-Noncausality", "content": "A VAR(p) model predicts the next values of a time series using the information available within the\nlast p time intervals. Let us consider two one-dimensional real-valued time series $x_t$ and $y_t$ that are"}, {"title": "Estimating a VAR(p) model", "content": "Investigating noncausality between two time-series of interest implies the estimation of a VAR(p)\nprocess. In this paper, we are using multivariate linear-squares estimation (LS) for formulating\nthese estimates. It is considered multivariate since estimates need to be formulated for pairs or\nsets of random variables, as Proposition 2.1 requires. For example, we can test when adding a new\nvariable to the existing set of variables can improve the investigated estimator. Computing an LS\nestimator is considered rather standard and the interested reader may refer to [13, Section 3] for a\ndetailed description of its solution."}, {"title": "Testing for Granger-noncausality", "content": "Let us consider that we have collected measurements of $z_t = (x_t, y_t)$ over a time horizon of T time\nsteps. The proposed approach takes as inputs: 1) a row vector Z of length $K(T - p+1)$, containing\nthe set of K time-series variables under investigation over a time lag p (e.g., in the example of\nEquation (1) that would be the pair of $x_t$ and $y_t$) with the LS estimate being denoted by $\\hat{Z}$; 2)\na row block matrix A to be filled with the regression coefficients, and 3) the data $(x_{t-i}$ and $y_{t-i})$\nrearranged in matrix form in such a way that allows to compute the dot product of A and the data\nand retain a row vector of estimates $\\hat{Z}$. The model is estimated using OLS with an SCS solver from\nthe python cvxp library.\nAs evident from Proposition 2.1, this framework allows for testing for Granger noncausality in\na very flexible way. For example, in the case the full model consists of two random variables, as\nin Equation (1), we compare the full model with the model where the parameters of interest are\nexcluded by setting the respective regression coefficients to zero (according to Proposition 2.1). In\n\u00b9The stability condition of a VAR(p) process can be verified by the eigenvalues of its coefficients matrix, cf. [13,\nSection 2.1]. Stability also implies stationarity."}, {"title": "2.5 Query capabilities", "content": "Representing multi-dimensional time-series data and their correlations/causations in a Knowledge\nGraph enables efficient and intuitive visualizations and sub-graph filtering using graph-oriented\nquery languages (e.g. SPARQL\u00b2 for RDF graph data formats, Cypher for Neo4J\u00b3, or AQL for\nArangoDB\u2074). This approach facilitates in-depth analysis, allowing users to perform complex queries\nover KGs, simplifying the retrieval of, e.g. lagged co-variates that causally impact specific tar-\ngets given certain thresholds, enabling threshold-based filtering to retrieve and visualize sub-graph\nstructures. In more complex settings, users may query the KG to find all sensors that showed a tem-\nperature increase above a certain threshold before a specific event or correlate these changes with\nother environmental factors. Additionally, utilizing semantic reasoning may infer new knowledge\nfrom existing data. For instance, if certain conditions are known to precede equipment failures, the\nsystem can infer potential risks before they occur."}, {"title": "3 Demonstrations", "content": "We employed the proposed correlation and Granger-causality discovery described in Sections 2.3-\n2.4 to the real-world industrial process of Section 1.3. This leads to the generation of a KG that\nrepresents both instantaneous correlations between process parameters as well as time-lagged causal\ninfluences. The resulting KG is depicted in Figure 4. In the case of the electrostatic particle transfer,\nthe target or predicted variable is the quality of the coating process (which represents the density\nof the powder/grains on the paper).\nThe KG of Figure 4 shows reasonable outcomes according to expert knowledge, since it indeed\ndemonstrates the expected causal and correlation dependencies. For example, as expected, the plate\ndistance, the frequency and strength of the electric field as well as the previous quality of specific time\nlags, causally influences the upcoming quality. Furthermore, the funnel width is instantaneously\ncorrelated with the quality (which is implied to be bidirectional).\nWe also designed and implemented a KG application tool that incorporates several functional-\nities and methods for the three main pillars of our framework, namely preprocessing, correlation\nand causality, as described in Section 2. In particular, our application includes the possibility of\nintegrated different types of data sources (e.g., connection to a datalake, or uploading CSV files),"}, {"title": "4 Conclusions and Future Work", "content": "This work shows the significant potential of automated KG learning tools in industrial settings.\nThe rationale behind converting time-series data generated through sensor measurements into KGs\nis to effectively address the challenges of extracting meaningful insights from large datasets. More-\nover, applying Granger causality within this framework provides a method for uncovering causal\nrelationships between sensor measurements. Granger causality is helpful in time-series analysis as\nit identifies potential causal links, rather than mere correlations, between different sensor measure-\nments over time. This capability is crucial for predictive maintenance, anomaly detection, and\noptimizing industrial processes, as it identifies root causes and predicts future events based on\nhistorical data.\nThe practical use case presented illustrates how this framework can improve decision-making,\noptimize processes, and facilitate knowledge discovery, eventually leading to improved operational\nefficiency. Moreover, integrating advanced analytics could facilitate industries' full potential use of\ntheir data, facilitating a competitive advantage.\nFuture work will concentrate on sharpening the framework's capabilities for various industries\nand further exploring its benefits through additional case studies. We want to improve scalability,\nintegrate the tool with existing KG databases, refine the user interface, and quantify its economic\nimpact. These efforts aim to strengthen the tool's capabilities and broaden its applicability."}]}