{"title": "TOP-DOWN ACTIVITY REPRESENTATION LEARNING FOR VIDEO QUESTION ANSWERING", "authors": ["Yanan Wang", "Shuichiro Haruta", "Donghuo Zeng", "Julio Vizcarra", "Mori Kurokawa"], "abstract": "Capturing complex hierarchical human activities, from atomic actions (e.g., picking up one present, moving to the sofa, unwrapping the present) to contextual events (e.g., celebrating Christmas) is crucial for achieving high-performance video question answering (VideoQA). Recent works have expanded multimodal models (e.g., CLIP, LLaVA) to process continuous video sequences, enhancing the model's temporal reasoning capabilities. However, these approaches often fail to capture contextual events that can be decomposed into multiple atomic actions non-continuously distributed over relatively long-term sequences. In this paper, to leverage the spatial visual context representation capability of the CLIP model for obtaining non-continuous visual representations in terms of contextual events in videos, we convert long-term video sequences into a spatial image domain and finetune the multimodal model LLaVA for the VideoQA task. Our approach achieves competitive performance on the STAR task, in particular, with a 78.4% accuracy score, exceeding the current state-of-the-art score by 2.8 points on the NExTQA task.", "sections": [{"title": "1 Introduction", "content": "Giving a question about a video, the video question answering (VideoQA) task aims to provide an answer about the spatio-temporal visual scene. It is a promising task in achieving real-world applications including autonomous vehicles, robots, and search engines Antol et al. [2015], Wang et al. [2023a], Fukui et al. [2016], Ben-Younes et al. [2017]. To deeply reason about temporal visual context, one of the significant challenges is capturing complex hierarchical human activities in videos. These activities range from atomic actions (e.g., picking up a present, moving to the sofa, unwrapping the present) to contextual events (e.g., celebrating Christmas) Luo et al. [2021, 2022].\nRecently, with the dramatic advancements in large language models (LLMs) (e.g., GPT-4 Achiam et al. [2023], Llama Touvron et al. [2023]), multimodal models such as LLaVA Liu et al. [2023a], an end-to-end trained large language-visual model that connects an image encoder and an LLM, have demonstrated impressive multimodal reasoning abilities on most image-based visual question answering (VQA) tasks. To further facilitate the visual and language understanding capabilities in video scene reasoning, recent works (e.g., LLaMA-VQA Ko et al. [2023], Video-LLaVA Lin et al. [2023]) have expanded multimodal models (e.g., LLaVA Liu et al. [2023a,b]) to process continuous video sequences, enhancing the model's temporal reasoning capabilities. These works apply the bottom-up video processing approach to capture atomic actions in continuous short-term video sequences and lead to contextual event understanding toward complex answer reasoning. However, these approaches often fail to capture contextual events since such events can be decomposed into multiple atomic actions that are non-continuously distributed over relatively long-term sequences (see the top of Fig. 1). The video sequences between adjacent atomic actions tend to be noisy, making it difficult to obtain effective contextual event representations.\nThis paper considers that the non-continuously distributed atomic actions can be captured by mitigating the effect of noisy video sequences in the spatial rather than the temporal domain. Meanwhile, we can capture more effective contextual event representations with the CLIP model's powerful spatial visual context representation capability. To this end, we propose a top-down video processing approach that converts a long-term video sequence into a single grid image, allowing the pretrained visual encoder of the CLIP to highlight image patches representing both contextual events and atomic action pitches (see the bottom of Fig. 1)."}, {"title": "2 Related work", "content": "Instruction-following large multimodal models (LMMs) include a pretrained visual backbonesuch as CLIP Radford et al. [2021] to encode visual features, a pretrained large language model (LLM) such as LLaMA Touvron et al. [2023] to encode user instruction text and generate responses, and a projection layer used to align language and vision encoders into a common domain space. LLaVA Liu et al. [2023b] is a typical LMM that follows a two-stage protocol. In the first vision-language alignment pretraining state, LLaVA pretrains a simple MLP projection layer to align the visual encoder's output image patches with the LLM's word embedding space. In the second visual instruction tuning stage, the model is tuned to follow the user's diverse instruction text including image content. Compared to LLaVA tuned for image-based VQA tasks, our proposed approach extends it to VideoQA tasks without adding extra training parameters. Recent works inspired by LLaVA (e.g., Video-LLaVA Lin et al. [2023], Video-ChatGPT Maaz et al. [2024]) to align video sequences with the LLM's word embedding to enable the model's temporal reasoning abilities. Even though the short-term temporal action representation can be captured from the input of video sequences, the long-term contextual event representations are hard to obtain. Our proposed approach aims to capture the contextual event from the entire video scene."}, {"title": "3 Method", "content": "In this section, we describe our proposed top-down video processing approach and explain the training network for videoQA tasks."}, {"title": "3.1 Top-down video processing", "content": "Giving a video V, we sample N2 frames following below sampling strategies:\n1. Retrieve the total number of frames M in V based on the video metadata such as the frame rate (fps);\n2. Split V into N2 intervals, and sample the middle frame from each interval;"}, {"title": "3.2 Training network", "content": "We finetune the pretrained LLaVA model for VideoQA tasks. As shown in Fig. 2, we have synthesized gird images, questions, and all answer options as user instruction text. The input of the gird image is first resized to 336 \u00d7 336, then embedded into 576 image patches with the pretrained CLIP visual encoder. The pretrained CLIP works for the synthesized grid image because the image patch size is small enough to represent the fine-grained visual context. The vision-language projector has been pretrained at the pretraining stage with image-text pairs, and the parameters are included in the pretrained LLaVA checkpoint. We finetune the pretrained projector to output the patch sequences to represent contextual representations of the entire grid image. Meanwhile, the text contents are encoded to output words' embeddings, then concatenated with image patches as the inputs of LLMs. We finetune 3 types of LLMs including Vicuna-7B Zheng et al. [2023], Vicuna-13B, and Hemes-Yi-34B that have already finetuned for VQA tasks, and the pretrained parameters are also included in the pretrained LLaVA checkpoints. To make the model output the option's letter of correct answers directly in the inference time, we add a prompt text \u201cAnswer with the option's letter from the given choices directly.\" into the user instruction text. The training target is to generate the correct option.\nWe utilize DeepSpeed\u00b9 to boost memory efficiency during the training process. We adopt the ZeRO stage 3 (ZeRO-3) to automatically collect and partition the 16-bit model parameters during the forward and backward passes. It enables us to finetune the 13B LLaVA model with 4 \u00d7 40GBA100 GPUs and the 34B LLaVA model with 8 \u00d7 40GB A100 GPUs."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Dataset", "content": "We evaluate our model on two challenging VideoQA tasks including STAR Wu et al. [2021] and NExTQA Xiao et al. [2021]. Both are multi-choice videoQA benchmarks featuring causal and temporal questions involving object-level"}, {"title": "4.2 Baselines", "content": "InternVideo Wang et al. [2022] proposed a unified learning paradigm with both masked and contrastive modeling to establish a feasible and effective spatio-temporal representation. LLaMA-VQA Ko et al. [2023] proposed Flipped-VQA model to efficiently fine-tune LLMs on VideoQA by reasoning and understanding the complex relationships of (V, Q, A) triplet, using LLMs' prior knowledge of temporal and causal reasoning. SeViLA Yu et al. [2023] introduced a localizer module to select top-K video frames and guide an answerer module to focus on important language-related frames and predict answers. ViLA Wang et al. [2023b] designed a learnable text-guided Frame-Prompter together with a cross-modal distillation (QFormer-Distiller) module to select keyframes and improve the video-language alignment accuracy. LRR Bhattacharyya et al. [2023] trained an LM end-to-end on three low-level surrogate tasks, object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities."}, {"title": "4.3 Results", "content": "The main results compared with recent state-of-the-art (SoTA) scores are shown in Tab. 1. The zero-shot results produced by the original LLaVA model are worse than SoTAs since the origin LLaVA is only pretrained on image-text pairs and lacks temporal reasoning abilities. By finetuning LLaVA with our proposed top-down video processing approach, the model can tune the parameters in the visual projector and LLMs to enable learning temporal reasoning from the synthesized grid image. Our finetuning models finally achieved competitive scores for both benchmarks. In particular, on the NExTQA validation set, our model demonstrates the best score for casual and temporal question reasoning and finally achieves a new SoTA 78.4% accuracy score."}, {"title": "4.4 Case study", "content": "To further investigate the effect of our proposed top-down video processing approach on video scene understanding, we compared zero-shot and finetuning models with three data samples in Fig. 3. We noticed that the zero-shot model works well when the context of different video frames changes significantly. As a result, the model can easily capture temporal actions from changes in context (see Fig. 3(a)). In contrast, the zero-shot model fails the sample in Fig. 3(b), since the model can not reason the high-level event without tuning the model parameters on the synthesized grid images. For example, the zero-shot model can only capture a low-level action \u201cput her arms up\u201d, but fails to reason a high-level event \"feed horse with grass\". On the other hand, as the finetuning model learned the holistic context in the video from the given grid images, it achieved the correct answer. For the sample in Fig. 3(c), both zero-shot and finetuning models fail the question. The reason is that the relevant frames to the target action are not picked up correctly from the actual video. This highlights the importance of extracting text-related context from a long-term video. We also prompt the zero-shot model to explain the answer in Fig. 3. By leveraging the powerful pretrained LLaVA model, the"}, {"title": "4.5 Ablation study", "content": "To further study how the synthesized grid image is effective in drawing temporal reasoning of LLaVA out, we compared different sizes of the synthesized grid image in Tab. 2. The result demonstrates the 3 \u00d7 3 grid image including 9 frames performs better than others. In particular, when we randomly select one frame, the result gets worse since the pretrained LLM can not perform temporal reasoning with a single frame. In contrast, the processed grid image enables the model to capture the top-down visual context in the video scene.\nWe also compared our proposed top-down video processing approach with its bottom-up counterpart in Tab. 3. Here, top-down video processing aims to capture the context of the entire video, and its bottom-up counterpart focuses on aggregating the frame-level context of the video sequence. By comparing two approaches built on LLaMA-based LLM with the same model size, our approach achieves 82.1% accuracy for the description question and exceeds 6.2% of its bottom-up counterpart. This result demonstrates the efficacy of our approach for the holistic understanding of the video scene."}, {"title": "4.6 Limitation", "content": "Even though recent multimodal models demonstrate powerful visual reasoning abilities, they fail to detect objects correctly and result in incorrect video context reasoning. In addition, the finetuned model tends to forget pretrained knowledge, which can worsen the robustness of the model in real-world applications."}, {"title": "5 Conclusion", "content": "In this paper, we proposed a top-down video processing approach that converts a long-term video sequence into a single grid image, allowing the pretrained visual encoder of the CLIP to highlight image patches representing both contextual events and atomic action pitches. By finetuning a multimodal model LLaVA on the synthesized grid images, we achieved competitive performance on the STAR task, in particular, with a 78.4% accuracy score, exceeding the current state-of-the-art score by 2.8 points on the NEXTQA task. In future work, we are working on how to deal with how to capture low-level video representations to boost object recognition and tracking tasks."}]}