{"title": "Game Development as Human-LLM Interaction", "authors": ["Jiale Hong", "Hongqiu Wu", "Hai Zhao"], "abstract": "Game development is a highly specialized task that relies on a complex game engine powered by complex programming languages, preventing many gaming enthusiasts from handling it. This paper introduces the Interaction-driven Game Engine (IGE) powered by LLM, which allows everyone to develop a custom game using natural language through Human-LLM interaction. To enable an LLM to function as an IGE, we instruct it to perform the following processes in each turn: (1) $P_{script}$: configure the game script segment based on the user's input; (2) $P_{code}$: generate the corresponding code snippet based on the game script segment; (3) $P_{utter}$: interact with the user, including guidance and feedback. We propose a data synthesis pipeline based on the LLM to generate game script-code pairs and interactions from a few manually crafted seed data. We propose a three-stage progressive training strategy to transfer the dialogue-based LLM to our IGE smoothly. We construct an IGE for poker games as a case study and comprehensively evaluate it from two perspectives: interaction quality and code correctness. The code and data are available at https://github.com/alterego238/IGE.", "sections": [{"title": "Introduction", "content": "A game engine is a software framework primarily designed for the development of games and generally includes relevant libraries and support programs (Valencia-Garc\u00eda et al. 2016). Game engines help streamline the game development process, enabling developers to focus more on designing gameplay and content. Popular game engines include Unity, Unreal Engine, CryENGINE, etc.\nGame development is a highly specialized task that relies on game engines powered by complex programming languages. The learning curve can be steep for those who wish to develop games based on their own designs. To make game development accessible to everyone, we propose the Interaction-driven Game Engine (IGE), powered by LLMS (Brown et al. 2020; Achiam et al. 2023; Touvron et al. 2023). This engine is designed to support the development of custom games using natural language through Human-LLM interaction. Thus all game enthusiasts, whether developers or players, can use IGE to engage in interactive game development using natural language.\nCompared to traditional game engines, our IGE eliminates the learning curve. While traditional game engines provide users with software interfaces powered by complex technologies and programming languages, our IGE offers a more flexible natural language interface powered by LLM. One can simply input natural language under the guidance of the engine through Human-LLM interaction. In IGE, a user's natural language input is equivalent to calling software interfaces in a traditional game engine. The LLM generates implementation code based on the user's input, mirroring the process of implementing software interfaces through complex technologies and programming languages in traditional game engines.\nIGE is based on large language models (LLMs), which have shown exceptional capabilities in natural language processing across various aspects. In this work, we explore the joint capability of interaction and programming of the LLM to serve as a game engine, enabling development through natural language via Human-LLM interaction. We instruct the LLM to perform the following processes in each turn: (1) $P_{script}$: configure the game script segment based on the user's input; (2) $P_{code}$: generate the corresponding code snippet based on the game script segment; (3) $P_{utter}$: interact with the user, including guidance and feedback.\nWe propose a comprehensive training paradigm to fine-tune an LLM to excel as an IGE, rather than relying solely on prompting. There are two main challenges. First, it is an exhausting process to acquire a large number of game script-code pairs. We propose an efficient data synthesis pipeline to generate game script-code pairs automatically from a few manually crafted seed data. Moreover, our IGE framework requires the LLM to learn to perform $P_{script}$, $P_{code}$, and $P_{utter}$ step by step, which places high demands on the joint capability of interaction and programming. However, simultaneously mastering these capabilities is a considerable challenge for the model. Additionally, a straightforward strategy to train on sufficient complete interaction data is inefficient. Therefore, we propose a three-stage progressive training strategy to transfer the dialogue-based LLM to our IGE smoothly.\nEventually, we construct an IGE for Poker, a worldwide card game, e.g. Texas hold'em. We utilize the proposed data synthesis pipeline to generate the corresponding dataset and fine-tune an IGE using the presented strategy. Then we"}, {"title": "Related works", "content": "AI for Games AI for games is an exciting area in AI research. A great amount of recent work studies learning for agents, e.g. as game players for Atari (Mnih et al. 2013), Minecraft (Fan et al. 2022; Wang et al. 2023a), StarCraft, (Vinyals et al. 2019), NetHack (K\u00fcttler et al. 2020; Lowe et al. 2020), Werewolf (Xu et al. 2023); as non-play characters (NPCs) (Shanahan, McDonell, and Reynolds 2023; Uludagli and Oguz 2023); player assistants (Gallotta et al. 2024); game commentators (Eladhari 2018; Ranella and Eger 2023). Recently, some works focus on building a neural engine based on LLMs. Delta-Engine (Wu et al. 2024b) drives games as a playground for a scalable virtual world, enabling expansion by generating new code based on the base engine. IDGE (Wu et al. 2024a) autoregressively predicts in-game states based on player actions, functioning more like a game runtime environment that supports game creation by simple natural language instructions as a script. In comparison, our IGE serves as a development framework for creating games, similar to a traditional game engine.\nLLMs as Training Data Generators With the immense power demonstrated by large language models(LLMs), researchers have recently explored their potential as as training data generators (Yu et al. 2024a). Such applications include generating tabular data (Borisov et al. 2022), medical dialogue (Chintagunta et al. 2021), sentence pairs (Schick and Sch\u00fctze 2021), role-play dialogue (Shao et al. 2023a), instruction data (Peng et al. 2023; Shao et al. 2023b; Sun et al. 2024; Wang et al. 2022), etc.. In this paper, we propose a data synthesis pipeline that leverages LLMs as training data generators to produce game script-code pairs and user-LLM interactions from a few manually crafted seed data.\nProgressive Training Strategy Progressive training strategy is commonly employed in LLM training. Training on progressively increasing sequence length data in multi-stages is used to mitigate computational costs and enhance data efficiency in both the pre-training (Jin et al. 2023; Dubey et al. 2024) and post-training (Liu et al. 2024) phases. Curriculum learning (Bengio et al. 2009), a specialized form of progressive training, gradually increases the complexity of data samples during the training process. Recent studies show the promising role of curriculum learning in empowering the language models to tackle more challenging tasks (Vakil and Amiri 2023; Wu et al. 2023, 2024a). In this paper, we propose a three-stage progressive training strategy to transfer the dialogue-based LLM to our IGE smoothly. This strategy also aligns with the principles of curriculum learning."}, {"title": "IGE", "content": "In this section, we present our IGE framework, illustrated in Figure 1. Addtionally, we provide an example in Figure 2."}, {"title": "An Overview of IGE Framework", "content": "The IGE framework introduces a new paradigm of game development as Human-LLM interaction. In user-LLM interactions, the user provides instructions for their game concept in natural language under the guidance of LLM, along with feedback to the LLM. The LLM guides the user in refining and clarifying essential details about the game, while also offering feedback. To enable the LLM to provide effective guidance, we predefine a generic script tailored to a specific type of game. Despite the various possible variants of a specific type of game, they often share common elements such as rules and flow, making a generic script feasible. Except for interaction with the user, the LLM generates script segments and code snippets to implement the user's game concept in each turn. In the meantime, the code snippets are stored, building toward the eventual complete game code, CustomGame. After the game is fully developed through multi-turn interactions, a code interpreter is used to execute the CustomGame code for play."}, {"title": "From Multi-turn Human-LLM Interaction to IGE", "content": "The complete process of IGE framework can be seen as a multi-turn human-LLM interaction. We first formulate the multi-turn Human-LLM interaction and then extend this concept to our IGE framework.\nIn a multi-turn Human-LLM interaction, both the user input and the LLM's output may be related to the interaction history, such as references to prior content. The interaction history $h_t$ at turn $t$ can be simply defined as:\n$h_{t}=\\begin{cases}\\emptyset & \\text{ if } t=0 \\\\ \\{(i_{\\tau}, o_{\\tau}) | \\tau=1, 2, ..., t\\} & \\text{ if } t>0\\end{cases}$ (1)\nwhere the subscript $t$ refers to the increasing number of turns, $i_t$ refers to the user input and $o_t$ refers to the LLM's output, formulated as:\n$o_{t}=F_{\\theta}(h_{t-1}, i_{t})$ (2)\nwhere $F_{\\theta}$ refers to the LLM, and $\\theta$ denotes its parameters. Consequently, an LLM with parameters $\\theta$ seeks to maximize the likelihood:\n$\\sum_{t=1}^{T} \\log p_{\\theta}(o_{t} | h_{t-1}, i_{t}).$ (3)\nwhere $T$ refers to the total number of interaction turns.\nThe distinction between IGE and a general multi-turn Human-LLM interaction lies in the specialization of the input and output. The user input $i_t$ consists of instructions"}, {"title": "Data Generation for IGE", "content": "In this section, we discuss our attempt in data generation. Utilizing LLMs to create IGE requires fine-tuning on a substantial amount of supervised data. However, manually crafting diverse interaction with script-code pairs is a challenging task. Compared to fully manual annotation, Harnessing LLMs to synthesize data is more efficient and has become a popular method for addressing the issue of insufficient data. We propose a pipeline consisting of three main steps to generate synthetic data, starting with a small set of manually annotated seed data, as illustrated in Figure 3. We utilize GPT-40 as the generator.\nInit pool First, we manually craft a few script-code pairs, each corresponding to different custom games. These pairs serve as seed data and are then split into script segments and code snippets, which are added to the pool.\nGenerate new pairs In this step, we sample pairs of script segments and code snippets, generating new pairs based on these selections. We prompt the generator to modify the code snippet first, then generate the corresponding script segment. This order is chosen because it's easier to map a script segment to a code snippet by describing the code, rather than generating code from a script description. This approach yields higher-quality data with more reliable mappings. To ensure the generated code functions correctly, we implement a filter before adding it to the pool. This process continues until the pool contains a sufficient number of entries.\nGenerate interaction data Finally, we generate the interaction data depicted in Figure 2 using the script-code pairs. This process involves two lines: (1) generating interaction snippets based on pairs of script segments and code snippets from the pool; (2) generating complete interactions from complete script-code pairs. The necessity of these two data components will be discussed in the next section."}, {"title": "A Three-stage Progressive Training Strategy", "content": "In this section, we present the training strategy of IGE. Based on our IGE framework, the LLM will learn to perform $P_{script}$, $P_{code}$ and $P_{utter}$ step by step, which places high demands on the joint capability of interaction and programming. However, it is challenging for the model to learn both capabilities simultaneously. On the other hand, a straightforward strategy to train on sufficient complete interaction data is inefficient. Therefore, we propose a three-stage progressive training strategy to transfer the dialogue-based LLM to our IGE smoothly.\nStage-1: Base Training This stage aims to train the base interaction ability of the model. Interaction ability is the most fundamental ability for IGE and serves as the foundation for the following two stages. Since most LLMs have already undergone sufficient and efficient supervised fine-tuning (SFT) (Brown et al. 2020; Raffel et al. 2020; Ouyang et al. 2022), we can directly use such models for Stage-1.\nStage-2: Core Training This stage aims to train the core capabilities of the model, namely the joint capability of programming and interaction. It fine-tunes the model from Stage-1 on interaction snippets that follow the IGE format. As illustrated in Figure 2, we instruct the model to perform the $P_{script}$, $P_{code}$ and $P_{utter}$ step by step to extract the user's concept of the game, implement it in code, and provide guidance and feedback for interaction.\nStage-3: Alignment This stage aims to align the model with a complete interaction context to fully develop a game as an IGE. It fine-tunes the model from Stage-2, which already possesses significant programming and interaction capabilities. At this stage, we only need to extend its ability for multi-turn interactions as an IGE, particularly in guiding users to complete game development according to the predefined script. Since the model already possesses strong multi-turn interaction and long-context capabilities following Stage-1 training, only a small dataset is required for alignment at this stage."}, {"title": "Experiments", "content": "In this section, we construct an IGE for a poker game. We employ the proposed data synthesis pipeline to generate the corresponding dataset, fine-tune an IGE using the presented strategy and evaluate its performance."}, {"title": "Dataset", "content": "Poker Game Poker, a worldwide card game, e.g. Texas hold'em, Badugi. These poker games can be abstracted into a generic game script. Table 2 presents an example example of such a script for the classic Texas hold'em. This generic script allows for the configuration of several common elements across different poker games, including the number of players, minimum and maximum bet limits, suit types and rankings, single-card rankings, multi-card combination rankings, game phases, and overall game flow. By adjusting these elements, virtually infinite variations of poker can be created. Notably, each game in our dataset corresponds to a unique configuration, including customizable phases. For example, a standard \"flopx\" phase might involve discarding one card from the deck and then revealing x community cards. This phase can be customized by adding a rule such as, \"After each flop, discard one more card,\" thereby creating a new variant of the \"flopx\" phase."}, {"title": "Setup", "content": "We employ LLaMA3.1-8B-Instruct\u00b9 (Dubey et al. 2024) for Stage-1 and finetune it using LoRA (Hu et al. 2021) with $r = 8$, $\\alpha = 32$, and a learning rate of 3e-4. We train 3 epochs on the 3718 interaction snippets for Stage-2 and 5 epochs on the 36 complete interactions for Stage-3.\nTo assess the performance of the LLM in a dynamic multi-turn interaction environment, we require a user to interact with the LLM, as demonstrated in our IGE framework. Simulating the user using a rule-based approach is complex, and employing human annotators poses challenges related to inconsistent standards and high costs. To address these issues, we use GPT-40-mini as the interactor to simulate the user, a practice increasingly adopted in dynamic multi-turn interaction environments (Li et al. 2023; Yu et al. 2024b). For evaluation, we provide the interactor with a manually crafted game script and instruct them to treat it as the game concept they have in mind. The interactor then interacts with the LLM, resulting in a multi-turn interaction about a specific custom game. This allows us to use the game script and its corresponding code as the ground truth for evaluating the generated interaction."}, {"title": "Metrics", "content": "We assess model performance from two perspectives: interaction quality and code correctness.\nInteraction Quality The interaction quality is assessed by an evaluator model, which assesses the output for guidance, logic, relevance, coherence and conciseness. Following KIEval (Yu et al. 2024b), we implement a scoring system to quantitatively grade model performance in different aspects. Responses are rated on a definitive scale from 1 to 4 for each aspect, where 1 and 4 denote 'Poor' and 'Strong' performance, respectively, as detailed in Table1. These scores are designed to encourage decisive evaluations. To facilitate comparison, we normalize the scores, ensuring that a rating of 1.0 indicates perfect performance. We utilize GPT-40 as the evaluator.\nCode correctness We evaluate code correctness using two functional-level metrics and two overall-level metrics:"}, {"title": "Main Results", "content": "We evaluate IGE on 10 manually crafted custom poker games, each accompanied by its complete game script-code pair. Table 3 presents the performance of our IGE, including both interaction quality and code correctness. For comparison, we take several representative closed-source and open-source LLMs in a 5-shot setting as baselines. Intuitively, IGE excels in both interaction quality and code correctness.\nInteraction Quality All models exhibit high interaction quality.Our IGE excels across all dimensions, showcasing exceptional capabilities in interacting with the user throughout the interactive development process. Compared to Llama-3.1-8B-Instruct, our fine-tuned model excels in guidance and logic, effectively guiding the user to develop the game logically.\nCode Correctness In our results, all models significantly outperform in functional-level metrics compared to overall-level metrics. This suggests that while LLMs excel at producing functional code, they face challenges when generating long, complete code. Additionally, it is evident that executability is more easily achieved than accuracy across all models, with our model reaching a perfect ESR of 100. This indicates that LLMs excel at generating code that is syntactically executable. Notably, IGE outperforms in all metrics. It achieves an impressive F-Acc of 99.0, outperforming the second-best model by 9 points. Moreover, it reaches an ESR of 100, surpassing the second-best by 20 points. Furthermore, it attains an Acc of an astounding 90, outstripping the second-best by 60 points.\nTo conduct a more in-depth analysis, we compute the function-level code correctness in Table 4. We find that most models excel on fixed functions and two simple variable functions: config and flow. These two functions require only basic assignment statements to configure the game, allowing them to generalize effectively. However, for functions with more complex code logic, namely blind, dealx, and flopx, the baselines generally underperform, with the lowest F-Acc reaching just 20. These results indicate that the accumulation of errors across these functions leads non-fine-tuned models to exhibit low correctness in overall-level evaluation. It is important to note that the model is required to be all-round at each function; otherwise, the overall performance will degenerate in a way of Buckets effect (Wu et al. 2024a). Delightfully, our IGE achieves near-perfect performance across all functions, resulting in an Acc far exceeding the baselines."}, {"title": "Ablation Study", "content": "We ablate different variants from the full IGE architecture, the results are presented in Table 3 and Table 4.\nAblation on $P_{script}$ A slight decrease can be observed in interaction quality across nearly all dimensions without $P_{script}$. Additionally, F-Acc drops by 0.2 points and Acc by 10.0 points. As shown in Table 4, the only failure occurs on a flopx function when compared to the complete IGE architecture. This suggests that $P_{script}$ can enhance both interaction and coding abilities in certain cases.\nAblation on synthetic data In this setting, we directly employ manually crafted script-code pairs, splitting them into snippets to generate complete interactions and interaction snippets. A slight decline can be observed in interaction quality across most dimensions, alongside a significant decrease in code correctness, with Acc dropping to 0. Notably, the code correctness is even lower than that of the 5-shot Llama-3.1-8B-Instruct. As shown in Table 4, this decline is attributed to poor performance on the two most challenging functions, dealx and flopx. This can be explained by the model overfitting on the limited data due to the absence of synthetic data, which leads to poor generalization.\nAblation on training strategy We conducted comprehensive ablation experiments on our three-stage training strategy, with the following setups: w/o. Stage-1, w/o. Stage-2, w/o. Stage-3, and w. Mixed-stage. In the setups without Stage-1 and Stage-3, the model loses its guiding and interaction abilities in multi-turn scenarios as an IGE, resulting in ESR and Acc values of 0. Therefore, the results of these two settings are not reported. This suggests that both Stage-1 and Stage-3 play a crucial role in enhancing the model's interaction ability as an IGE. As shown in Table 3, the interaction quality of the model decreases across most dimensions without Stage-2. Additionally, there is a significant drop in code correctness, with Acc falling to 10.0. A sharp decline in F-Acc for the dealx and flopx functions is clearly evident in Table 4. This indicates that Stage-2 is essential to the core interaction and programming capabilities of the model, especially programming capabilities. The Mixed-stage involves mixing all the complete interactions and interaction snippets and fine-tuning on them in a single stage. It mixes Stage-2 and Stage-3. A significant decrease can be observed in interaction quality in the Mixed-stage setting. Additionally, there is a notable drop in code correctness, with Acc falling to 20.0. This indicates that a mixed-stage training strategy for complete interactions and interaction snippets hinders both the interaction and programmig capabilities of the model. This suggests that our three-stage training strategy effectively enhances the joint capability of interaction and programmig of the LLM as an IGE."}, {"title": "Case Study", "content": "In Table 5, we present two representative cases comparing GPT-40 and IGE. In Case 1, the code generated by GPT-4o is logically correct, but the function call is used incorrectly. The proper usage of \"random.choice\u201d should be \"random.choice(x)\", but it seems to have confused this with the \"random_choice\u201d usage provided in the in-context examples. Similarly, in Case 2, GPT-40 mistakenly treated \"self.players[player_id]\" as a dict. This can be attributed to its misalignment with the engine, also known as hallucination (Ji et al. 2023). In comparison, our IGE is well-aligned and does not exhibit this phenomenon in the test set."}, {"title": "Conclusion", "content": "This paper introduces the Interaction-driven Game Engine (IGE) and proposes a paradigm for training IGE to allows users to develop custom games interactively using natural language. To enable an LLM to function as an IGE, we instruct it to generate script segments, code snippets and interactions for each turn in the development process. To facilitate the training process, a data synthesis pipeline is proposed to generate sufficient training data, as well as a three-stage progressive training strategy to enhance the joint capability of interaction and programming of the LLM. Embodied in a poker game, we demonstrate the performance of the IGE through a comprehensive evaluation."}]}