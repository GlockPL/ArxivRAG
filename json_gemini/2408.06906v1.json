{"title": "VNet: A GAN-based Multi-Tier Discriminator Network for Speech Synthesis Vocoders*", "authors": ["Yubing Cao", "Yongming Li", "Liejun Wang", "Yinfeng Yu"], "abstract": "Since the introduction of Generative Adversarial Networks (GANs) in speech synthesis, remarkable achievements have been attained. In a thorough exploration of vocoders, it has been discovered that audio waveforms can be generated at speeds exceeding real-time while maintaining high fidelity, achieved through the utilization of GAN-based models. Typically, the inputs to the vocoder consist of band-limited spectral information, which inevitably sacrifices high-frequency details. To address this, we adopt the full-band Mel spectrogram information as input, aiming to provide the vocoder with the most comprehensive information possible. However, previous studies have revealed that the use of full-band spectral information as input can result in the issue of over-smoothing, compromising the naturalness of the synthesized speech. To tackle this challenge, we propose VNet, a GAN-based neural vocoder network that incorporates full-band spectral information and introduces a Multi-Tier Discriminator (MTD) comprising multiple sub-discriminators to generate high-resolution signals. Additionally, we introduce an asymptotically constrained method that modifies the adversarial loss of the generator and discriminator, enhancing the stability of the training process. Through rigorous experiments, we demonstrate that the VNet model is capable of generating high-fidelity speech and significantly improving the performance of the vocoder.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech synthesis is crucial across various domains, including accessibility, education, entertainment, and customer service [1]. However, conventional systems often encounter challenges with timbre, speech rate variation, and vocal coherence [2]\u2013[4]. Recent advancements in deep learning and neural network techniques have significantly improved the quality of speech synthesis [5], [6]. The neural network and deep learning-based speech synthesis now being introduced are broadly divided into two steps: 1) Acoustic modeling: taking characters (text) or phonemes as input and creating a model of the acoustic features. (The acoustic features used in most of the work are Mel Spectrograms); 2) Vocoder: a model that takes Mel Spectrograms (or similar spectrograms) as input and generates real audio [7]. As an important step in speech synthesis, the study of the vocoder has received extensive attention. This paper focuses on the vocoder part of the study. Vocoder models can be broadly categorized into autoregressive-based (e.g., WaveNet [8], WaveRNN [9]), flow-based (e.g., WaveGlow [10], Parallel WaveGAN [11]), GAN-based [12] (e.g., MelGAN [13], HiFiGAN [14], BigVGAN [15]) and diffusion model-based (e.g., WaveGrad [16], Grad-tts [17], FastDiff [18], ProDiff [19]) approaches. These advancements promise more natural and coherent speech, enhancing user experience across various applications.\nGANs employ an adversarial training approach, where the generator and discriminator engage in a competitive process. This competition fosters improved generator performance and enhances the ability to generate features resembling real data, making GANs widely utilized in vocoder tasks. While the GAN-based generative model can synthesize high-fidelity audio waveforms faster than real-time, most vocoders operate on band-limited Mel spectrogram as input. For instance, HiFi-GAN utilizes band-limited Mel spectrograms as input. Other similar models include LVCNet [20], StyleMelGAN [21] and WaveGlow [10]. However, speech signals generated with band-limited Mel spectrograms lack high-frequency information, leading to fidelity issues in the resulting waveforms. Thus, considering full-band Mel spectrogram information as vocoder input is crucial. Despite attempts by Parallel WaveGAN to use full-band Mel spectrograms, it faces challenges such as excessive smoothing, resulting in the generation of non-sharp spectrograms and unnatural speech output [11].\nThe loss function of a GAN typically encompasses both the generator and discriminator loss functions. However, various vocoder models employ distinct loss function designs and exhibit differences in the selection of similar loss terms, leading to training instability. For instance, Parallel WaveGAN incorporates cross-entropy loss into the generator loss to address instability issues, albeit without complete resolution [11]. MelGAN endeavors to enhance stability by replacing the cross-entropy loss with hinge loss and augmenting feature matching loss, yet gradient loss persists [13]. HiFiGAN introduces feature matching loss and Mel spectrogram loss to mitigate training instability [14]. Despite the inclusion of these additional loss functions, training may still encounter challenges such as gradient loss and pattern collapse, resulting in an unstable training process.\nThis paper introduces VNet, a novel vocoder model capable of synthesizing high-fidelity speech in real time. A new discriminator module, named MTD, is proposed, which utilizes multiple linear spectrogram magnitudes computed with distinct sets of parameters. Operating on full-band Mel spectrogram data, MTD facilitates the generation of full-"}, {"title": "II. RELATED WORK", "content": "GANs have emerged as powerful generative models [12]. Initially applied to image generation tasks, GANs have garnered significant success and attention. Similarly, in the domain of speech synthesis, where traditional approaches primarily rely on rule-based or statistical models, GAN technology has gradually gained traction. By leveraging the adversarial framework of GANs, speech synthesis models can better capture the complexity and realism of speech signals, thereby producing more natural, high-quality synthesized speech.\nWaveGAN simplifies speech synthesis by directly generating raw audio waveforms, producing high-quality and naturalistic speech segments. However, its training requires substantial data and computational resources. In contrast, Parallel WaveGAN extends single short-time Fourier transform (STFT) loss to multi-resolution, integrating it as an auxiliary loss for GAN training [11]. It may suffer from excessive smoothing. MelGAN achieves high-quality synthesis without additional distortion or perceptual losses by introducing a multi-scale discriminator (MSD) and incorporating hinge loss, feature matching loss, and discriminator loss [13]. HiFiGAN enhances the discriminator's ability to differentiate between generated and real audio and introduces a multi-receptive field fusion (MRF) module in the generator. Its loss functions include least squares loss, feature matching loss, Mel spectrogram loss, and discriminator loss [14], [22]. BigVGAN builds upon HiFiGAN by replacing the MSD with a multi-resolution discriminator (MRD) and introducing periodic activation into the generator. It proposes an anti-aliasing multi-periodicity composition (AMP) module for modeling complex audio waveforms. BigVGAN's loss functions comprise least squares adversarial loss, feature matching loss, and Mel spectrogram loss [15].VNet distinguishes itself from these methods by simultaneously addressing the challenges of matching features at various resolutions and scales while also resolving the issue of poor fidelity results that arise from using full-band Mel spectrograms as input."}, {"title": "III. METHOD", "content": "The generator G, inspired by BigVGAN, is a fully convolutional neural network depicted in Fig. 1(a). It takes a full-band Mel spectrogram as input and utilizes inverse convolution for upsampling until the output sequence length matches the target waveform map. Each deconvolution module is followed by an MRF module, which concurrently observes pattern features of varying lengths. The MRF module aggregates the outputs of multiple residual modules, each with different convolution kernel sizes and expansion coefficients, aimed at forming diverse perceptual field patterns.\nTo efficiently capture localized information from the Mel spectrogram, we introduce Location Variable Convolution (LVC), enhancing sound quality and generation speed while maintaining model size [20]. The LVC layer's convolution kernel is obtained from the kernel predictor, with the Mel spectrogram serving as input and the predicted convolution kernel concatenated into a residual stack for each LVC layer separately. Through empirical experiments, we optimize the placement and number of LVC layers and the kernel predictor to achieve the desired sound quality and generation speed. To improve the model's adaptability to speaker feature variations and mitigate overfitting risks, we incorporate gated activation units (GAUs) [23].\nDiscriminators play a crucial role in guiding the generator to produce high-quality, coherent waveforms while minimiz-"}, {"title": "C. Training Losses", "content": "The feature matching loss measures similarity in learning, quantifying the difference in sample features between ground truth and generated samples [24]. Given its successful application in speech synthesis, we employed it as an additional loss for training the generator. Each intermediate feature was extracted, and the Frobenius distance between ground truth and generated samples in each feature space was computed. Denoted as $L_{FM}$, the feature matching loss is defined as follows:\n$L_{FM}(X,Y)=\\frac{1}{M}\\sum_{m=1}^{M}\\frac{||S_m - S_m||_F}{||S_m||_F},$\nwhere$||.||_F$denote the Frobenius norms and S denotes the number of elements in the spectrogram. Each m-th$L_{FM}$reuse $S_m$and $S_m$ used in the m-th MTD sub-discriminator. The number of each loss is M, which is the same as the number of MTD sub-discriminators.\nWe also introduced a log-Mel spectrogram loss to enhance the training efficiency of the generator and improve the fidelity of the generated audio. Drawing from previous work, incorporating reconstruction loss into the GAN model has been shown to yield realistic results [25]. We employed the Mel spectrogram loss based on input conditions, aiming to focus on improving perceptual quality given the characteristics of the human auditory system [26]. The Mel spectrogram loss is calculated as the L1 distance between the Mel spectrogram of the waveforms generated by the generator and the Mel spectrogram of the ground truth waveforms. Denoted as$L_{Mel}$, the Mel spectrogram loss is defined as follows:\n$L_{Mel}(X,\\hat{X}) = \\frac{1}{M}\\sum_{m=1}^{M}E_{\\textbf{x}}[\\textbf{s}] ||\\log \\hat{S}_{m} - \\log S_{m}||_{1},$\nwhere$||.||_1$denotes the L1 norms, and S denotes the number of elements in the spectrogram. Each m-th$L_{FM}$reuse $S_m$and$S_m$used in the m-th MTD sub-discriminator. The number of each loss is M, which is the same as the number of MTD sub-discriminators.\nThe objective of the vocoder is to train the generating function$G: S \u2192 X$, which transforms a Mel spectrogram s \u2208 S into a waveform signal x \u2208 X. The adversarial losses of the generator and the discriminator are denoted as$L_{adv}(G;D)$and$L_{adv}(D; G)$. The discriminant function D: X \u2208 R is typically implemented using a neural network, denoted by \u03c6, which comprises linear operations and nonlinear activation functions [27]. To simplify, we decompose the discriminator into a nonlinear function $h_{\\phi}: X \\in W \\subseteq R^{D}$_and a final linear layer w \u2208 W, expressed as $D(x) = \\omega^{T}h_{\\phi}(x)$, where \u03c6 = [\u03c6, \u03c9]. The discriminative process can be interpreted as segmenting the nonlinear feature $h_{\\phi}(x)$ using a shared projection w. Thus, the adversarial loss of the generator and the discriminator can be expressed as follows:\n$L_{adv}(D;G) = E_{px}[R_1(D(X))]+E_{ps}[R_2(D(G_{\\theta}(s))],$\n$L_{adv}(G;D) = E_{ps}[R_3(D(G_{\\theta}(s)))],$\n$R_1(z) = -(1-z)^2, R_2(z) = -z^2, R_3(z) = (1 -z)^2,$\n$p_x(x)$and$p_s(s)$denote the waveform signal and Mel spectrogram, respectively. Through optimization of the maximization problem, a nonlinear function $h_\\phi$ is induced to differentiate between true and false samples and mapped onto the feature space W, resulting in a linear projection on W to enhance discrimination [28]. However, the linear projection@ in Eq. (3) may not fully utilize features for discrimination. We observe that given hp, there exist linear projections that offer more discriminative information than the projection @ maximizing Eq. (3). As long as R3 (whose derivative is denoted r3 ) is a monotonically decreasing function-meaning the derivative r3(z) is negative for any z\u2208 R. Thus, we propose the asymptotic constraint method to modify the adversarial loss function of the generator and the discriminator as follows:\n$L_{adv}(D;G) = E_{px}[R_1(D_{\\theta}(X))]+E_{ps}[R_2(D_{\\theta}(G_{\\theta}(s)))] + E_{px}[R_3(D_{\\theta}(X))] \u2013 E_{ps}[R_3(D_{\\theta}(G_{\\theta}(s)))],$\n$L_{adv}(G;D) = E_{ps}[R_3(D(G_{\\theta}(s)))],$\n$R_1(z) = \u2212(1\u2212z)\u00b2, R_2(z) = \u2212\u03c3(z)\u00b2, R_3(z) = \u03c3(1 \u2212z)2,$\nwhere \u03c3(\u00b7) is the \"asymptotic constraint\", i.e., \u03c3(x) = e-(0.3x-2). In our preliminary experiments, when utilizing Eq. (3) instead of Eq. (6), we observed unstable training, underscoring the significance of ensuring the monotonicity of R3. Particularly in the early stages of training, the loss values tended to converge to suboptimal local minima."}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "This study demonstrates the capabilities of the VNet model, a GAN-based vocoder, in enhancing speech synthesis. By utilizing full-band Mel spectrogram inputs, the model effectively addresses over-smoothing issues. Furthermore, the introduction of a Multi-Tier Discriminator (MTD) and refined adversarial loss functions has significantly improved speech quality and fidelity.\nFuture research should prioritize further reducing over-smoothing and exploring the model's potential in multilingual and diverse speech styles. Such advancements could greatly enhance the practical usability of GAN-based vocoders, resulting in more natural and expressive synthesized speech."}]}