{"title": "VNet: A GAN-based Multi-Tier Discriminator Network for Speech Synthesis Vocoders*", "authors": ["Yubing Cao", "Yongming Li", "Liejun Wang", "Yinfeng Yu"], "abstract": "Abstract\u2014Since the introduction of Generative Adversarial Networks (GANs) in speech synthesis, remarkable achievements have been attained. In a thorough exploration of vocoders, it has been discovered that audio waveforms can be generated at speeds exceeding real-time while maintaining high fidelity, achieved through the utilization of GAN-based models. Typically, the inputs to the vocoder consist of band-limited spectral information, which inevitably sacrifices high-frequency details. To address this, we adopt the full-band Mel spectrogram information as input, aiming to provide the vocoder with the most comprehensive information possible. However, previous studies have revealed that the use of full-band spectral information as input can result in the issue of over-smoothing, compromising the naturalness of the synthesized speech. To tackle this challenge, we propose VNet, a GAN-based neural vocoder network that incorporates full-band spectral information and introduces a Multi-Tier Discriminator (MTD) comprising multiple sub-discriminators to generate high-resolution signals. Additionally, we introduce an asymptotically constrained method that modifies the adversarial loss of the generator and discriminator, enhancing the stability of the training process. Through rigorous experiments, we demonstrate that the VNet model is capable of generating high-fidelity speech and significantly improving the performance of the vocoder.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech synthesis is crucial across various domains, including accessibility, education, entertainment, and customer service [1]. However, conventional systems often encounter challenges with timbre, speech rate variation, and vocal coherence [2]\u2013[4]. Recent advancements in deep learning and neural network techniques have significantly improved the quality of speech synthesis [5], [6]. The neural network and deep learning-based speech synthesis now being introduced are broadly divided into two steps: 1) Acoustic modeling: taking characters (text) or phonemes as input and creating a model of the acoustic features. (The acoustic features used in most of the work are Mel Spectrograms); 2) Vocoder: a model that takes Mel Spectrograms (or similar spectrograms) as input and generates real audio [7]. As an important step in speech synthesis, the study of the vocoder has received extensive attention. This paper focuses on the vocoder part of the study. Vocoder models can be broadly categorized into autoregressive-based (e.g., WaveNet [8], WaveRNN [9]), flow-based (e.g., WaveGlow [10], Parallel WaveGAN [11]), GAN-based [12] (e.g., MelGAN [13], HiFiGAN [14], BigVGAN [15]) and diffusion model-based (e.g., WaveGrad [16], Grad-tts [17], FastDiff [18], ProDiff [19]) approaches. These advancements promise more natural and coherent speech, enhancing user experience across various applications.\nGANs employ an adversarial training approach, where the generator and discriminator engage in a competitive process. This competition fosters improved generator performance and enhances the ability to generate features resembling real data, making GANs widely utilized in vocoder tasks. While the GAN-based generative model can synthesize high-fidelity audio waveforms faster than real-time, most vocoders operate on band-limited Mel spectrogram as input. For instance, HiFi-GAN utilizes band-limited Mel spectrograms as input. Other similar models include LVCNet [20], StyleMelGAN [21] and WaveGlow [10]. However, speech signals generated with band-limited Mel spectrograms lack high-frequency information, leading to fidelity issues in the resulting waveforms. Thus, considering full-band Mel spectrogram information as vocoder input is crucial. Despite attempts by Parallel WaveGAN to use full-band Mel spectrograms, it faces challenges such as excessive smoothing, resulting in the generation of non-sharp spectrograms and unnatural speech output [11].\nThe loss function of a GAN typically encompasses both the generator and discriminator loss functions. However, various vocoder models employ distinct loss function designs and exhibit differences in the selection of similar loss terms, leading to training instability. For instance, Parallel WaveGAN incorporates cross-entropy loss into the generator loss to address instability issues, albeit without complete resolution [11]. MelGAN endeavors to enhance stability by replacing the cross-entropy loss with hinge loss and augmenting feature matching loss, yet gradient loss persists [13]. HiFiGAN introduces feature matching loss and Mel spectrogram loss to mitigate training instability [14]. Despite the inclusion of these additional loss functions, training may still encounter challenges such as gradient loss and pattern collapse, resulting in an unstable training process.\nThis paper introduces VNet, a novel vocoder model capable of synthesizing high-fidelity speech in real time. A new discriminator module, named MTD, is proposed, which utilizes multiple linear spectrogram magnitudes computed with distinct sets of parameters. Operating on full-band Mel spectrogram data, MTD facilitates the generation of full-"}, {"title": "II. RELATED WORK", "content": "GANs have emerged as powerful generative models [12]. Initially applied to image generation tasks, GANs have garnered significant success and attention. Similarly, in the domain of speech synthesis, where traditional approaches primarily rely on rule-based or statistical models, GAN technology has gradually gained traction. By leveraging the adversarial framework of GANs, speech synthesis models can better capture the complexity and realism of speech signals, thereby producing more natural, high-quality synthesized speech.\nWaveGAN simplifies speech synthesis by directly generating raw audio waveforms, producing high-quality and naturalistic speech segments. However, its training requires substantial data and computational resources. In contrast, Parallel WaveGAN extends single short-time Fourier transform (STFT) loss to multi-resolution, integrating it as an auxiliary loss for GAN training [11]. It may suffer from excessive smoothing. MelGAN achieves high-quality synthesis without additional distortion or perceptual losses by introducing a multi-scale discriminator (MSD) and incorporating hinge loss, feature matching loss, and discriminator loss [13]. HiFiGAN enhances the discriminator\u2019s ability to differentiate between generated and real audio and introduces a multi-receptive field fusion (MRF) module in the generator. Its loss functions include least squares loss, feature matching loss, Mel spectrogram loss, and discriminator loss [14], [22]. BigVGAN builds upon HiFiGAN by replacing the MSD with a multi-resolution discriminator (MRD) and introducing periodic activation into the generator. It proposes an anti-aliasing multi-periodicity composition (AMP) module for modeling complex audio waveforms. BigVGAN\u2019s loss functions comprise least squares adversarial loss, feature matching loss, and Mel spectrogram loss [15].VNet distinguishes itself from these methods by simultaneously addressing the challenges of matching features at various resolutions and scales while also resolving the issue of poor fidelity results that arise from using full-band Mel spectrograms as input."}, {"title": "III. METHOD", "content": "The generator G, inspired by BigVGAN, is a fully convolutional neural network depicted in Fig. 1(a). It takes a full-band Mel spectrogram as input and utilizes inverse convolution for upsampling until the output sequence length matches the target waveform map. Each deconvolution module is followed by an MRF module, which concurrently observes pattern features of varying lengths. The MRF module aggregates the outputs of multiple residual modules, each with different convolution kernel sizes and expansion coefficients, aimed at forming diverse perceptual field patterns.\nTo efficiently capture localized information from the Mel spectrogram, we introduce Location Variable Convolution (LVC), enhancing sound quality and generation speed while maintaining model size [20]. The LVC layer\u2019s convolution kernel is obtained from the kernel predictor, with the Mel spectrogram serving as input and the predicted convolution kernel concatenated into a residual stack for each LVC layer separately. Through empirical experiments, we optimize the placement and number of LVC layers and the kernel predictor to achieve the desired sound quality and generation speed. To improve the model\u2019s adaptability to speaker feature variations and mitigate overfitting risks, we incorporate gated activation units (GAUs) [23].\nDiscriminators play a crucial role in guiding the generator to produce high-quality, coherent waveforms while minimizing perceptual errors detectable by the human ear. State-of-the-art GAN-based vocoders typically incorporate multiple discriminators to guide coherent waveform generation while minimizing perceptual artifacts. Moreover, each discriminator comprises several sub-discriminators. As illustrated in Fig. 1(b), our discriminator utilizes multiple spectrograms and reshaped waveforms computed from real or generated signals. Since speech signals contain sinusoidal signals with varying periods, we introduce the MPD to identify various periodic patterns in the audio data. MPD extracts periodic components from waveforms at prime intervals and utilizes them as inputs to each subsampler [14]. Additionally, to capture continuous patterns and long-term dependencies, we design and employ the MTD.\nMTD comprises three sub-discriminators operating at different input scales: raw audio, x2 average pooled audio, and x4 average pooled audio. Each sub-discriminator receives input from the same waveform through STFT using distinct parameter sets [11]. These parameter sets specify the number of points in the Fourier transform, frame-shift interval, and window length.\nEach sub-discriminator in MTD consists of stride and packetized convolutional layers with Leaky ReLU activation. The mesh size increases by reducing the step size and adding more layers. Spectral normalization stabilizes the training process, except for the first subframe, where weight normalization manipulates raw audio. This model architecture draws inspiration from Multi-Scale Waveform Diagrams (MSWDs) but diverges by utilizing MTD to incorporate multiple spectrograms with varying temporal and spectral resolutions, thereby generating high-resolution signals across the full frequency band.\nThe VNet discriminator comprises two sub-modules: the MTD and the MPD, each containing multiple sub-discriminators utilizing 2D convolutional stacking, as depicted in Fig. 2, MTD transforms the input 1D waveform into a 2D linear spectrogram by employing various downsampling average pooling multiples, followed by STFT with diverse parameters ([n_fft, hop_length, win_length]). MPD converts the input 1D waveform of length T into a 2D waveform through reshaping and reflection filling (Reshape2d) with different widths (p) and heights (T/p)."}, {"title": "C. Training Losses", "content": "The feature matching loss measures similarity in learning, quantifying the difference in sample features between ground truth and generated samples [24]. Given its successful application in speech synthesis, we employed it as an additional loss for training the generator. Each intermediate feature was extracted, and the Frobenius distance between ground truth and generated samples in each feature space was computed. Denoted as LFM, the feature matching loss is defined as follows:\n$$L_{F M}(X, Y)=\\frac{1}{M} \\sum_{m=1}^{M} \\frac{\\left|\\left|S_{m}-S_{m}^{\\prime}\\right|\\right|_{F}}{\\left|\\left|S_{m}^{\\prime}\\right|\\right|_{F}}$$\nwhere|| || Fdenote the Frobenius norms and S denotes the number of elements in the spectrogram. Each m-th LFM reuse Sm and Sm used in the m-th MTD sub-discriminator. The number of each loss is M, which is the same as the number of MTD sub-discriminators.\nWe also introduced a log-Mel spectrogram loss to enhance the training efficiency of the generator and improve the fidelity of the generated audio. Drawing from previous work, incorporating reconstruction loss into the GAN model has been shown to yield realistic results [25]. We employed the Mel spectrogram loss based on input conditions, aiming to focus on improving perceptual quality given the characteristics of the human auditory system [26]. The Mel spectrogram loss is calculated as the L1 distance between the Mel spectrogram of the waveforms generated by the generator and the Mel spectrogram of the ground truth waveforms. Denoted as LMel, the Mel spectrogram loss is defined as follows:\n$$L_{M e l}(X, \\hat{X})=\\frac{1}{M} \\sum_{m=1}^{M} E_{x}[\\hat{S}]\\left|\\left|\\log S_{m}-\\log \\hat{S}_{m}\\right|\\right|_{1}$$\nwhere||.||1 denotes the L1 norms, and S denotes the number of elements in the spectrogram. Each m-th LFM reuse Sm and Sm used in the m-th MTD sub-discriminator. The number of each loss is M, which is the same as the number of MTD sub-discriminators.\nThe objective of the vocoder is to train the generating functionGS \u2192 X, which transforms a Mel spectrogram s \u2208 S into a waveform signal x \u2208 X. The adversarial losses of the generator and the discriminator are denoted as Ladv(G;D) and Ladv (D; G). The discriminant function D: X \u2208 R is typically implemented using a neural network, denoted by 6, which comprises linear operations and nonlinear activation functions [27]. To simplify, we decompose the discriminator into a nonlinear function hp : X \u2208 W \u2286 RD_and a final linear layer \u03c9 \u2208 W, expressed as D(x) = WThp(x), where \u03c6 = [\u03c6, \u03c9]. The discriminative process can be interpreted as segmenting the nonlinear feature hp(x) using a shared projection w. Thus, the adversarial loss of the generator and the discriminator can be expressed as follows:\n$$L_{a d v}(D ; G)=E_{p_{x}}[R_{1}(D(X))]+E_{p_{s}}[R_{2}(D(G \\epsilon(s))]]$$\n$$L_{a d v}(G ; D)=E_{p_{s}}[R_{3}(D(G \\epsilon(s)))]$$\n$$R_{1}(z)=-(1-z)^{2}, R_{2}(z)=-z^{2}, R_{3}(z)=(1-z)^{2}$$\npx(x) and ps(s) denote the waveform signal and Mel spectrogram, respectively. Through optimization of the maximization problem, a nonlinear function ho is induced to differentiate between true and false samples and mapped onto the feature space W, resulting in a linear projection on W to enhance discrimination [28]. However, the linear projection @ in Eq. (3) may not fully utilize features for discrimination. We observe that given ho, there exist linear projections that offer more discriminative information than the projection @ maximizing Eq. (3). As long as R3 (whose derivative is denoted r3 ) is a monotonically decreasing function-meaning the derivative r3(z) is negative for any z\u2208 R. Thus, we propose the asymptotic constraint method to modify the adversarial loss function of the generator and the discriminator as follows:\n$$L_{a d v}(D ; G)=E_{p_{x}}[R_{1}(D^{\\circledR}(X))]+E_{p_{s}}[R_{2}(D^{-}(G \\epsilon(s)))]+E_{p_{x}}[R_{3}(D^{-}(X))]-E_{p_{s}}[R_{3}(D^{-}(G \\epsilon(s)))]$$", "B. Discriminator": "Discriminators play a crucial role in guiding the generator to produce high-quality, coherent waveforms while minimizing perceptual errors detectable by the human ear. State-of-the-art GAN-based vocoders typically incorporate multiple discriminators to guide coherent waveform generation while minimizing perceptual artifacts. Moreover, each discriminator comprises several sub-discriminators. As illustrated in Fig. 1(b), our discriminator utilizes multiple spectrograms and reshaped waveforms computed from real or generated signals. Since speech signals contain sinusoidal signals with varying periods, we introduce the MPD to identify various periodic patterns in the audio data. MPD extracts periodic components from waveforms at prime intervals and utilizes them as inputs to each subsampler [14]. Additionally, to capture continuous patterns and long-term dependencies, we design and employ the MTD.\nMTD comprises three sub-discriminators operating at different input scales: raw audio, x2 average pooled audio, and x4 average pooled audio. Each sub-discriminator receives input from the same waveform through STFT using distinct parameter sets [11]. These parameter sets specify the number of points in the Fourier transform, frame-shift interval, and window length.\nEach sub-discriminator in MTD consists of stride and packetized convolutional layers with Leaky ReLU activation. The mesh size increases by reducing the step size and adding more layers. Spectral normalization stabilizes the training process, except for the first subframe, where weight normalization manipulates raw audio. This model architecture draws inspiration from Multi-Scale Waveform Diagrams (MSWDs) but diverges by utilizing MTD to incorporate multiple spectrograms with varying temporal and spectral resolutions, thereby generating high-resolution signals across the full frequency band.\nThe VNet discriminator comprises two sub-modules: the MTD and the MPD, each containing multiple sub-discriminators utilizing 2D convolutional stacking, as depicted in Fig. 2, MTD transforms the input 1D waveform into a 2D linear spectrogram by employing various downsampling average pooling multiples, followed by STFT with diverse parameters ([n_fft, hop_length, win_length]). MPD converts the input 1D waveform of length T into a 2D waveform through reshaping and reflection filling (Reshape2d) with different widths (p) and heights (T/p)."}, {"title": "IV. EXPERIMENTS", "content": "We validate the effectiveness of our method on the Lib-riTTS dataset, an English multi-speaker audiobook dataset comprising 585 hours of audio [29]. The training utilizes the LibriTTS training sets (train-clean-100, train-clean-360 and train-other-500). For text-to-speech evaluation, we fine-tune the vocabulary encoder using predicted log-mel spectrograms to minimize feature mismatches. Additionally, we employ the LJSpeech dataset\u00b9, containing 24 hours of data and 13,000 utterances from English-speaking female speakers. All waveforms are sampled at a rate of 24 kHz.\nAll models, including the baseline, are trained using a frequency range of [0, 12] kHz and 100-band logarithmic Mel spectrograms, consistent with recent studies on universal vocoders. STFT parameters are set as per previous work, with a 1024 FFT size, a 1024 Hann window, and a 256 hop size. Objective evaluation is conducted on a subset of LibriTTS dev-clean and dev-other. Following the formal implementation of VNet, evaluation involves 6% randomly selected audio files from dev-clean and 8% randomly selected audio files from dev-other. For this experiment, we utilized a server with 4 Tesla T4 GPUs, each with a 16GB memory capacity. The CPU used in the server is an Intel Xeon Gold 5218R."}, {"title": "C. Comparison with existing models", "content": "Table I presents the results, with Wave Glow and Parallel WaveGAN yielding lower scores than other models and VNet outperforming BigVGAN across all objective and subjective evaluations. While there's only a marginal enhancement in subjective scores compared to HiFi-GAN, VNet offers the advantage of generating results at approximately 1.5 times the speed for a similar number of parameters. Notably, Parallel WaveGAN exhibits over-smoothing issues, likely due to experiments with full-band features instead of band-limited features.\nAs depicted in Fig. 3, PESQ only considers the [0, 8] kHz range, while MCD and M-STFT assess both this range and higher frequency bands, resulting in significantly improved MCD and M-STFT scores. MOS scores demonstrate a strong correlation with PESQ scores."}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "This study demonstrates the capabilities of the VNet model, a GAN-based vocoder, in enhancing speech synthesis. By utilizing full-band Mel spectrogram inputs, the model effectively addresses over-smoothing issues. Furthermore, the introduction of a Multi-Tier Discriminator (MTD) and refined adversarial loss functions has significantly improved speech quality and fidelity.\nFuture research should prioritize further reducing over-smoothing and exploring the model\u2019s potential in multilingual and diverse speech styles. Such advancements could greatly enhance the practical usability of GAN-based vocoders, resulting in more natural and expressive synthesized speech."}]}