{"title": "ZeFaV: Boosting Large Language Models for Zero-shot Fact Verification", "authors": ["Son T. Luu", "Hiep Nguyen", "Trung Vo", "Le-Minh Nguyen"], "abstract": "In this paper, we propose ZeFaV - a zero-shot based fact-checking verification framework to enhance the performance on fact verification task of large language models by leveraging the in-context learning ability of large language models to extract the relations among the entities within a claim, re-organized the information from the evidence in a relationally logical form, and combine the above information with the original evidence to generate the context from which our fact-checking model provide verdicts for the input claims. We conducted empirical experiments to evaluate our approach on two multi-hop fact-checking datasets including HoVer and FEVEROUS, and achieved potential results results comparable to other state-of-the-art fact verification task methods.", "sections": [{"title": "1 Introduction", "content": "Fact-checking involves determining the truth of a claim [7]. With the rise of the Internet, fake news and misinformation have become significant issues on social media [12]. Fact-checking is time-consuming as it requires detecting the claim, gathering evidence from verified sources, and comparing information to assess its accuracy. According to [12], the application of AI to automatically fact-verification allows organizations to perform fact-checking faster and more comprehensively. With the given information as the claim and the relevant evidence, the system must determine the veracity of the claim based on the understanding of the claim context and the evidence related to the claim. Nonetheless, there are four main challenges for the automated fact verification task. First, the claim is ambiguous when there are several ways to interpret the meaning of the claim [12]. Second, the data artifacts and bias in the annotated dataset can lead to bias in system predictions [7]. Third, the current state-of-the-art system is limited to contextual information such as external knowledge sources and users' created information [7]. Last, the making of annotated data for fact-checking is costly, time-consuming, and potentially biased (according to [14]). Since the limitation in the scale of training data, the utilization of a pre-trained language model deals with domain-sensitive problems, in which the pre-trained language model on a specific domain cannot cover other domains without re-training the model [4].\nTo address these challenges, applying Large Language Models (LLMs) in fact verification tasks is promising due to their strong comprehension of human language [4]. Previous works like ProgramFC [14], QAChecker [13], and InfoRE [5] utilize LLMs for fact verification"}, {"title": "2 Related Works", "content": "The in-context learning ability of large language models demonstrated the effectiveness in verifying the truthfulness of complex claims. ProgramFC [14] recently used the in-context learning ability of large language models to break down the input claims into reasoning sub-tasks and then obtain the verdicts by tackling each reasoning sub-task. Using few-shot learning, it decomposes claims into Python-like programs with functions to question, verify, and predict facts. ProgramFC demonstrates that decomposing claims into smaller tasks is more effective than a one-step prediction approach, reducing the cognitive load on the language model and enhancing its fact-checking capability. Another approach, the QACheck system [13], includes five components, each component is a LLM with a specific task for the verification process. These LLMs leverage their ability to generate text and learn from context to ask and answer key questions that determine the claim's truthfulness. In addition, InfoRE [5] improves these models' reasoning abilities by restructuring evidence into MindMap forms, which effectively represent knowledge and concepts [3]. This reorganized evidence used alone or with original data, improves claim verification. Experiments show that InfoRE achieves comparable results in fact-checking, demonstrating the effectiveness of this information reorganization method. Finally, a recent study from [10] shows that large language models excel in few-shot relation extraction (FSRE) by producing linearized strings that encode entity pairs and their relations."}, {"title": "3 Proposed Methodology", "content": "Giving a sample (c, E), where c is a claim sentence, and E is a set of evidence relevant to c. The fact verification task aims to obtain the result v such that v \u2208 {True, False}. The True indicates the claim is correct (supports), and False indicates the claim is not correct (refutes). To solve the task, we proposed the ZeFaV our zero-shot prompting technique leverages the text re-organizing and relation extraction to enhance the reasoning ability of the LLMs for the Fact verification task. The ZeFaV consists of three main stages, as described below.\nFirst, we fine-tune the LLMs using the prompt below for the relation extraction task based on the FewRel dataset [6]. Then, we use this prompt to extract the relation for an input sentence (we leave the ### Response: as blank thus the LLM can generate the corresponding results)."}, {"title": "4 Empirical Results", "content": "We perform ZeFaV on Meta-Llama-3-70B-Instruct and evaluate the performance on the division of HoVer [9] and FEVEROUS-S [2] datasets by Pan et al. [14] through the F1-score metric. We run the LLMs with LoRA technique [8] quantized to 4 bits, and the maximum length for the generation model is 2,048. Our ZeFaV can run on one NVIDIA A40 GPU with 49GB of memory. For the ProgramFC [14], we run the proposed methodology with N=1 on Meta-Llama-3-70B-Instruct for comparison with our ZeFaV. According to Table 1, ZeFaV obtained better results than other methodologies on the HoVer dataset. On the FEVEROUS-S dataset, ZeFaV achieved 86.54% by F1-score, which is better than ProgramFC [14], InfoRE [5] and QACheck [13]. Since ProgramFC is a few-shot learning method for about 20 examples [14], ZeFaV performs efficiently on the HoVer dataset. In comparison with InfoRE, ZeFaV outperforms InfoRE with the LLama architecture on both HoVer and FEVEROUS-S datasets. Besides, the context evidence plays a vital role in the ZeFaV when it increases the performance of the Fact verification task on both HoVer and FEVEROUS-S. It can be seen that ZeFaV showed efficient performance for the Fact verification task with zero-shot learning since it outperforms other methodologies on the same LLama architecture."}, {"title": "4.2 Results Analysis", "content": "As shown in Table 2, it can be seen that both InfoRE and Relation help increase the accuracy of LLMs on 2-hop and 4-hop of HoVer. Specifically, on the 3-hops of HoVer, InfoRE helps increase the accuracy while it is slightly decreased when combined with the relation. This is similar to the FEVEROUS-S where the performance slightly decreases when combining InfoRE with relation. In general, InfoRE helps the LLMs in reasoning and understanding the information by re-organizing the data in a compact and concise form. For the relationship, it helps increase the ability of LLMs when combined with InfoRE. However, the performance of ZeFaV when there is only a relation is not as good as InfoRE. This is more clear when there is a lack of evidence context. The accuracy of ZeFaV with InfoRE is better than ZeFaV with relation only. Overall, both InfoRE and Relation help increase the performance of LLMs, and the evidence context also plays a vital role in the zero-shot Fact verification task."}, {"title": "5 Conclusion", "content": "In this paper, we proposed ZeFaV a method to address the challenge of improving LLM performance in fact verification using zero-shot learning. Our method leverages relation extraction and InfoRE to enhance LLM fact verification through zero-shot learning with improved evidence representation for CoT learning. The empirical study shows that the"}]}