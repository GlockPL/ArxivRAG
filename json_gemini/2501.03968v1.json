{"title": "VLM-driven Behavior Tree for Context-aware Task Planning", "authors": ["Naoki Wake", "Atsushi Kanehira", "Jun Takamatsu", "Kazuhiro Sasabuchi", "Katsushi Ikeuchi"], "abstract": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs) has recently gained attention in the robotics community, yet remains in its early stages of development. In this paper, we propose a novel framework that leverages Vision-Language Models (VLMs) to interactively generate and edit BTs that address visual conditions, enabling context-aware robot operations in visually complex environments. A key feature of our approach lies in the conditional control through self-prompted visual conditions. Specifically, the VLM generates BTs with visual condition nodes, where conditions are expressed as free-form text. Another VLM process integrates the text into its prompt and evaluates the conditions against real-world images during robot execution. We validated our framework in a real-world cafe scenario, demonstrating both its feasibility and limitations. The source code will be made available soon.", "sections": [{"title": "I. INTRODUCTION", "content": "Advancements in robotic hardware have accelerated the commercialization of robots across diverse fields such as logistics, manufacturing, and healthcare, while enabling de-ployment in dynamic and diverse environments beyond tra-ditional factories. This trend highlights the need for rapidly updatable programs tailored to each unique use case, with minimal reliance on skilled programmers. Behavior Trees (BT) have emerged as a potential solution to address the need as they offer modularity, reusability, and readability in robotic programming [1], [2], [3]. Existing BT generation methods, such as those based on genetic programming [4], [5], have proven effective but are typically limited to spe-cific scenarios. With the advent of Large Language Models (LLMs), new approaches have aimed to leverage LLMs' superior language comprehension capabilities to generate diverse BTs [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], including conditional branches [17], [18]. However, this field remains in its early stages of development.\nIn this paper, we propose a novel BT generation frame-work that leverages Vision-Language Models (VLMs) to implement \"self-prompted visual conditions (Fig. 1).\" While existing LLM-based approaches primarily focus on BT gen-eration, our method expands the scope to include runtime switching between BT nodes based on visual condition eval-uation. Specifically, the VLM integrates flexible condition nodes into BTs. These nodes include conditional statements and are evaluated against real-world images during robot execution. For example, given an instruction like \"remove the cups from the table,\" the VLM generates a condition node to check if the table is clear of cups, and includes it in the BT. This condition (i.e., \u201cThe table is clear of cups.\u201d) is evaluated against real-time visual inputs, thereby enabling the VLMS"}, {"title": "II. RELATED WORK", "content": "While BT generation methods have traditionally employed learning-based approaches [12], [16], [16], recent work has increasingly focused on utilizing pre-trained LLMs through pre-prompting or fine-tuning. Early studies leveraging pre-trained LLMs often employed BT templates [15], or used LLMs only to produce preliminary information to help a BT builder [17], [7]. More recently, however, it has become common to have LLMs directly generate BTs [11], [19], [9], [8]. This shift is likely driven by the improved capabilities of LLMs in handling longer contexts and larger token windows.\nWhile our study aligns with the approach of directly generating BTs, a key distinction is the incorporation of visually-driven conditions. Expressed as free-form text, these conditions serve as prompts to be verified during robot execu-tion. This method enables flexible conditional checks beyond the capabilities of conventional task-specific vision models. While one study proposed utilizing a vision-language model (VLM) for detecting failures [20], to the best of our knowl-edge, no prior work has attempted to use VLMs to address conditional branches within BTs."}, {"title": "B. BT for robotics", "content": "Originating in the field of gaming, BTs provide a state-free, hierarchical structure where execution nodes and control-flow nodes work together to manage complex be-haviors [21]. BTs are highly suited for robotics applications owing to their modular tree structure, flexibility for modifi-cations, and high readability [1], [2], [3]. For example, the BehaviorTree.CPP library\u00b9 has become the de facto standard for behavior tree implementations in the Robot Operating System 2 (ROS2), as a result of its integration within the widely used Navigation2 framework [22].\nEnsuring the safety and success of generated BTs is another critical factor in robotics. Some researchers have introduced syntax-checking capabilities through reflective feedback [10], [9] or iterative corrections using a simula-tor [14]. Nevertheless, reflective methods are costly in terms of time and resource consumption due to the increase in LLM calls. Recent findings suggest that a human-in-the-loop approach can outperform fully automated correction methods [14]. Therefore, we have chosen to adopt a human-in-the-loop strategy to assist in generating safe BTs. To this end, we have developed an interface that integrates interactive BT editing, enabling human users to guide and refine the final outcomes (see Section IV)."}, {"title": "C. Use of VLM for robotics", "content": "The advancements in LLMs and VLMs have significantly contributed to progress in robotics applications ([23], [24] provide an overview). While one major research direction"}, {"title": "III. VLM-DRIVEN BT GENERATION FRAMEWORK", "content": "In order to obtain a BT structure, both a pre-prompt and a human instruction are provided to a VLM. The structure and content of the pre-prompt are based on our previous paper [30], as outlined below:\n\u2022 Role Prompt: Explains the general task overview to the VLM.\n\u2022 Environment Prompt: Describes how the input scene information is structured, along with specific details about the scene and map.\n\u2022 Output Prompt: Specifies the expected output from the VLM and its format.\n\u2022 Action Prompt: Defines the list of robot actions at the granularity of BTs and their arguments.\n\u2022 Example Prompt: Provides examples of the output format.\nThese pre-prompts are concatenated in a conversational format. The user's textual instructions are then appended to this pre-prompt to generate output from the VLM. All conversations are temporarily stored in memory during the process so that, when the user provides feedback, modifica-tions can be made based on the recent conversation history.\nIt is noteworthy that in this prompt, scene and map information are provided as text, which means that BTs can be generated even by an LLM without relying on a VLM. However, for branching decisions that depend on the robot's egocentric vision, we employ a VLM (i.e., GPT-40) to maintain a unified workflow under a single model framework."}, {"title": "A. Role Prompt", "content": "This prompt provides the VLM with context for the task."}, {"title": "B. Environment prompt", "content": "This prompt explains the format used to represent working environments. The idea of using environmental data for LLM task planning is a widely applied technique, including the"}, {"title": "C. Output prompt", "content": "This prompt explains the information expected as output from the VLM and its format. Our preliminary observations indicate that the GPT-40 model possesses basic knowledge of BTs (this is not shown in a figure but can be verified, for example, by querying \"What is a Behavior Tree and explain its concept\" to GPT-40). However, for clarity, we explicitly defined the structure of BTs and the roles of each node as part of the pre-prompt. While prior research aiming to directly generate BTs from LLMs often adopted an XML format, we opted for JSON in this work as it facilitates ease parsing.\nIn the prompt example illustrated in Fig. 4, the system is assumed to repeatedly execute the main sequence until it visually confirms that the ultimate goal has been achieved at a specific location. Under this assumption, the VLM is tasked with determining the following based on the user's instruction:\n\u2022 main_sequence: The sequence of operations repeated until the ultimate goal is achieved.\n\u2022 ultimate_goal: The visual condition defining the suc-cessful achievement of the goal.\n\u2022 where_to_check_goal: The location where the ultimate goal is visually confirmed.\nThe template illustrated here was defined for a mobile navigation scenario in a cafe environment, and the use of"}, {"title": "D. Action prompt", "content": "This prompt describes a list of robot actions defined at the granularity of BT nodes and their corresponding arguments. In a BT, each node represents a specific action or condition evaluation. Here, the placeholder, node_definition_placeholder is replaced with robot actions tailored to each scenario.\nNotably, the actions defined in this prompt are independent"}, {"title": "E. Example prompt", "content": "This prompt provides examples of the expected output format. By presenting several representative scenarios, it"}, {"title": "F. Customization for various use cases", "content": "The pre-prompts are examples prepared for the cafe sce-nario and can be customized for different scenarios, robot hardware, and environments. Table II summarizes the com-ponents that should be adjusted according to the requirements of customization."}, {"title": "IV. INTERACTIVE BT BUILDER", "content": "We developed an interface for interactively constructing BTs using a VLM equipped with the aforementioned pre-prompts (Fig. 6). The BT visualization and text editing components leverage a third-party BT editing library, Mis-treevous\u00b2. This web-based interface comprises three com-ponents: a chat window, a BT visualization window, and a BT editing window. The chat window enables interactive instructions to the VLM, with inputs provided via text or voice. The BT visualization window displays the BT generated by the VLM, while the BT editing window allows users to modify the BT directly in text format.\nThis interface allows users to edit the BT through dialogue without using the editing window, as the VLM retains the conversation history until a correct BT is confirmed by the user. Figure 7 illustrates how the VLM modifies the initially generated tree in response to user feedback."}, {"title": "V. EXPERIMENTS", "content": ""}, {"title": "A. Visual check through self-prompting", "content": "Figure 8 shows how the self-prompting method works dur-ing a real-robot operation. During the interactive BT building phase, GPT-40 generates arguments for visual check nodes as free-form language. When the robot reaches these nodes during execution, the system internally generates prompts using the arguments and the robot's egocentric image. The next node in the BT is selected based on the VLM's response to these prompts. The bottom images show examples of GPT-40's responses under different conditions (e.g., whether the cup contains liquid or not), demonstrating the feasibility of dynamically switching the robot's behavior during execution."}, {"title": "B. End-to-end robot experiment", "content": "We conducted experiments applying the proposed BT builder to a real-robot system. In this experiment, we as-sumed a scenario where a robot works in a cafe and is tasked with disposing of all cups placed on a table into a trash bin. The scenario included uncertain conditions: cups either contained liquid or were empty. If liquid remained in a cup, the robot was required to discard the liquid into a designated container before disposing of the cup into the trash bin. For this experiment, we used a SEED-noid robot (THK)\u00b3 equipped with 6-DOF dual arms with one-DOF grippers"}, {"title": "C. Building diverse BTs in cafe scenes", "content": "We qualitatively evaluated the proposed BT builder's ap-plicability to various scenarios. With action nodes defined in Table IV three scenarios were tested: making coffee, wiping"}, {"title": "VI. DISCUSSION AND CONCLUSION", "content": "This paper proposes a BT builder designed to support domain experts in developing robot programs tailored to on-site requirements. Among existing robot-oriented BT frame-works, this study is unique in leveraging the language pro-cessing capabilities of VLMs, while introducing vision-based branching guided by self-defined conditions. This approach enables vision-based conditional handling during execution, allowing for context-aware robot control that reflects user intent through visual conditions. We presented prompting strategies and developed an interface that integrates visu-alization and user feedback into the BT creation process. Experimental results demonstrated that the proposed system can generate diverse BTs for specific scenarios and that the VLM effectively utilizes self-prompts for decision-making based on the robot's egocentric images. Furthermore, end-to-end experiments with a physical mobile robot validated the feasibility of the system in a cafe scenario.\nOne limitation of this study is that we did not cover BT generation across arbitrary granularities of robotic actions. We assume that differences in granularity can be addressed by using action nodes with a granularity level similar to those in Table I, and by remapping the nodes into the granularities designed for each specific robot. Furthermore, the success rate of visual conditions depends on the performance of the VLM (e.g., GPT-40). However, this limitation can be mitigated by combining the system with additional object recognition models to enhance performance [27].\nWe hope that this research contributes to advancing frame-works for robot BT generation."}, {"title": "APPENDIX", "content": ""}]}