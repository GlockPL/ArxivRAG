{"title": "EFFICIENT LONG-RANGE LANGUAGE MODELING WITH SELF-SUPERVISED CAUSAL RETRIEVAL", "authors": ["Xiang Hu", "Zhihao Teng", "Wei Wu", "Kewei Tu"], "abstract": "Recently, retrieval-based language models (RLMs) have received much attention. However, most of them leverage a pre-trained retriever with fixed parameters, which may not adapt well to causal language models. In this work, we propose Grouped Cross-Attention, a novel module enabling joint pre-training of the retriever and causal LM, and apply it to long-context modeling. For a given input sequence, we split it into chunks and use the current chunk to retrieve past chunks for subsequent text generation. Our innovation allows the retriever to learn how to retrieve past chunks that better minimize the auto-regressive loss of subsequent tokens in an end-to-end manner. By integrating top-k retrieval, our model can be pre-trained efficiently from scratch with context lengths up to 64K tokens. Our experiments show our model, compared with long-range LM baselines, can achieve lower perplexity with comparable or lower pre-training and inference costs.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformers (Vaswani et al., 2017), serving as the backbone of large language models (LLM), have revolutionized language modeling and demonstrated exceptional performance across a wide range of natural language processing tasks (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023; Dubey et al., 2024). While Transformers excel in representational power, their quadratic computational complexity and increasing memory demands as input length grows pose formidable challenges for modeling long contexts. Various approaches, such as recurrent memory (Dai et al., 2019), and linear attention (Katharopoulos et al., 2020) techniques, are proposed to improve the efficiency and effectiveness of Transformers in handling extended inputs. Nevertheless, these approaches often sacrifice the random-access flexibility of attention (Mohtashami & Jaggi, 2023) during inference.\nIn this work, we explore long-range language modeling in Transformers from the perspective of retrieval-based language models (RLMs) (Asai et al., 2023). Typically, RLMs (Rubin & Berant, 2024; Yen et al., 2024) divide an input sequence into chunks, retrieve relevant ones from the history for the current input, and then integrate the retrieved chunks into the decoder to predict subsequent tokens. By choosing top-k chunks as a \"dynamic context\", RLMs overcome the efficiency challenges in long-context modeling while maintaining the random-access flexibility. However, most RLMS (Lewis et al., 2020; Borgeaud et al., 2022) rely on separately pre-trained retrievers with fixed parameters, which hinders their ability to adapt to the causal LMs. Although a straightforward approach is training the retriever end-to-end to select chunks that minimize auto-regressive loss of subsequent tokens, it is rarely explored. The main challenges are twofold: firstly, while relevance scores guide chunk selection, the selection operation is non-differentiable, hindering gradient back-propagation to the scores. Secondly, the large search space brought by long contexts often results in efficiency and flexibility issues for pre-training.\nTo tackle these challenges, we propose Grouped Cross-Attention (GCA), a novel module enabling efficient end-to-end joint optimization of the retriever and causal LM, thus the retriever can learn to retrieve past chunks that most effectively reduce the auto-regressive loss, which we refer to as causal retrieval. GCA enables the relevance scores to participate in the next token prediction in"}, {"title": "2 RELATED WORKS", "content": "Relation to RPT & Landmark Attention. There are two long-range LMs closely related to ours. One of them is Retrieval-Pretrained Transformer (RPT) (Rubin & Berant, 2024). The key difference between DRT and RPT is the training approach of the retriever. During data-preparation, for each chunk, RPT picks relevant past chunks by using BM25 (Robertson & Zaragoza, 2009), concatenates them with the current chunk, and evaluates them by a reference LM like Pythia 1.4B (Biderman et al., 2023). The past chunks that increase the probability of the next chunk are identified as 'gold chunks' to train RPT's retriever. However, such a complex data preparation process limits scalability and flexibility in pre-training and post-training (Lee, 2024). In contrast, DRT is pre-trained end-to-end. By employing a sliding window size larger than the chunk size, it effectively uses feedback from subsequent several chunks to train the retriever. Its flexibility also allows for adaptive multi-hop retrieval. Landmark Attention (LA) (Mohtashami & Jaggi, 2023) is another close work. LA is pre-trained with short contexts but capable of handling long contexts during inference. It addresses long-range language modeling by modifying self-attention KV Cache. During inference, each token, at each layer, selects top-k chunks based on token-to-chunk attention scores and appends their key and value vectors to the current KV cache of self-attention. The token-to-chunk attention scores are trained in an end-to-end manner with a grouped softmax technique. However, it has to perform top-k chunk selection per token, per layer, which incurs significant extra costs during inference. Moreover, it fails to extrapolate on longer context length. Our method combines the chunk-retrieval and grouped softmax ideas, resolving the aforementioned issues while balancing training efficiency and inference performance.\nLong-Range Language Modeling. Various methods have been proposed to improve long-range language modeling. One line of research is introducing memorization to Transformers via recurrence. Many works (Dai et al., 2019; Burtsev & Sapunov, 2020; Martins et al., 2022; Hutchins et al., 2022) compress past information into fixed-sized vectors. However, these methods often sacrifice the flexibility to attend to arbitrary past tokens. Meanwhile, other works focus on maintaining random-access flexibility of attention. Memorizing Transformers (Wu et al., 2022) appends retrieved past keys and values to the current attention segment via k-NN search, but they do not back-propagate gradients to them. CEPE (Yen et al., 2024) retrieves previous chunks using an independently trained dense retriever and fuses them into the decoder. During the training process, the decoder parameters are fixed, and only the encoder is adjusted. A notable distinction in our work is the end-to-end optimization of all parameters, particularly the retriever.\nEfficient Language Modeling. Many works have been done to reduce the training and inference cost of LLM. One direction is sparse attention, which includes limiting the attention window to a small range around each token (Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020), approximating attention matrix (Wang et al., 2020), leveraging locality-sensitive-hashing(LSH) for key vectors retrieval (Kitaev et al., 2020), and hierarchical self-attention (Ren et al., 2021). However, empirically most efficient Transformers sacrifice performance for efficiency. Recently, state-space models (Gu & Dao, 2023; Dao & Gu, 2024) and RNN models (Beck et al., 2024) provide new architecture alternatives, with comparable performance to Transformers but much lower cost for inference. We argue that our core innovation GCA is flexible enough to be incorporated into these models as an additional module to obtain random-access flexibility.\nRetrieval-Augmented Language Models. Retrieval-augmented LMs leverage a retriever to access relevant external knowledge, enhancing their generation capabilities. In some works, the retriever can be jointly trained with the LM such as REALM (Guu et al., 2020). However, its computational complexity limits its extension to causal LMs. On the other hand, in most other works (Lewis et al., 2020; Izacard & Grave, 2021; Borgeaud et al., 2022; Ivgi et al., 2023), retriever parameters are fixed, preventing optimization for retrieving information that best predicts subsequent tokens.\nUnsupervised Dense Retrieval. In the line of research on unsupervised dense retrieval, early works leverage Inverse Cloze Task (Lee et al., 2019) to train retriever unsupervisedly, where a sentence is randomly sampled from a document and the task is to predict its surrounding context. However, this approach still lags behind BM25 on long-tail entities (Sciavolino et al., 2021). Recently, contrastive learning methods (Izacard et al., 2022; Gao & Callan, 2022) have shown improved results by creating positive and negative pairs from sentences within the same or different documents,"}, {"title": "3 DIFFERENTIABLE RETRIEVAL-BASED TRANSFORMER", "content": "A typical architecture of RLMs (Borgeaud et al., 2022; Yen et al., 2024; Rubin & Berant, 2024) appends Chunked Cross-Attention (CCA) after self-attention to fuse information from retrieved chunks, in which a retriever is merely used to pick relevant chunks. Our approach makes retriever learnable by replacing CCA with GCA, which represents the key innovation of this work. The novelty of GCA lies in using relevance scores to fuse information from retrieved chunks for LM decoders, enabling the retriever to adaptively learn to select the best past chunks for predicting subsequent tokens, guided by the auto-regressive loss. This section details the model architecture, training, and inference."}, {"title": "3.1 MODEL ARCHITECTURE", "content": "DRT is composed of N Transformer-like layers. Similar to RETRO (Borgeaud et al., 2022), the input sequence of DRT is equally divided into chunks. Formally, given a sequence  x = [x_1, x_2, ..., x_L]  with L tokens, we divide the sequence into chunks, where S is the chunk size, denoted as  {C_1, C_2, ..., C_{L/S}},  where  x_i \u2208 C_{\\lfloor i/s \\rfloor}.  Similar to Landmark Attention, we insert a special token LMK at the end of each chunk, which summarizes the preceding content via self-attention.\nForward Pass. Figure 2 illustrates the forward pass of a token in DRT. DRT layers are bifurcated into upper and lower sections like in RPT (Rubin & Berant, 2024). The key differences are the introduction of GCA and the further division of the upper layers into G groups, enabling learning to retrieve on the fly and adaptive multi-hop retrieval. The lower layers comprise standard Transformer layers while each upper layer has an additional GCA module after self-attention. In the forward pass, the chunk hidden states output by the lower layers, besides being fed to the upper layers, are also fed into a bi-directional Transformer encoder, which further contextualizes the representations with inner-chunk positional encoding, yielding  C_k \u2208 \\mathbb{R}^{S\u00d7d}  and  l_k \u2208 \\mathbb{R}^d  shared across all upper layers, where d is the hidden state dimension. At the g-th upper decoder group, chunk  c_t  retrieves"}, {"title": "the top-k relevant past chunks for its next chunk:", "content": "\\begin{equation}\nr_{t}^{g, k}=\\frac{h_{l_k}^\\top h_{c_t}}{\\sqrt{d}}  C_t^g = \\text{Top-k}([r_{t}^{g,1}, ..., r_{t}^{g,t-1}]) .\n\\end{equation}\nHere  h_{l_k} \u2208 \\mathbb{R}^d  denotes the landmark representation from the decoder layers of the current chunk, and  r_{t}^{g, k} \u2208 \\mathbb{R}  represents the causal relevance score of  c_k  to  c_t . Cf  contains the indices of past chunks with top-k relevance scores. The retrieved chunks are shared among the subsequent layers within the same group. The upper layers apply GCA to fuse retrieved information into the decoder.\nGrouped Cross-Attention. For the l-th layer, let  H_{t+1}^l  and  \\hat{H}_{t+1}^l \u2208 \\mathbb{R}^{(S+1)\u00d7d}  denote the batched token representations including the landmark token in the next chunk before and after GCA. In GCA, we perform Cross-Attention (CA) separately and fuse results via relevance scores:\n\\begin{equation}\n\\begin{aligned}\nO_{t+1,k}^{l} &= \\text{CA}(H_{t+1}^l, C_k, C_k), k \\in C_t^{(g(l))}, \\\\\nw_{t,k}^{l} &= \\frac{\\exp(r_{t,k}^{g(l)})}{\\sum_{k'\\in C_t^{(g(l))}} \\exp(r_{t,k'}^{g(l)})}, \\\\\nO_{t+1}^{l} &= \\sum_{k} w_{t,k}^{l} O_{t+1,k}^{l}, \\\\\n\\hat{H}_{t+1}^l &= \\text{Norm}(H_{t+1}^l + O_{t+1}^{l}).\n\\end{aligned}\n\\end{equation}\nHere g(l) converts the layer index to the group index and  C_k \u2208 \\mathbb{R}^{S\u00d7d}  represents token representations of the k-th retrieved chunk.  O_{t+1,k}^{l} \u2208 \\mathbb{R}^{(S+1)\u00d7d}  represents the information that S + 1 tokens in chunk  C_{t+1}  gather from past chunk  C_k .  w_{t,k}^{l}  is the normalized relevance score after softmax, serving as the weight of  O_{t+1,k}^{l}  for information fusion. The final fused results of GCA is  O_{t+1} .\nSince  C_k  is shared across layers, we use the same K, V liner transformations across layers to compact model parameters and reduce memory footprint. For each head h, we have CA defined as:\n\\begin{equation}\nCA(H_{t+1}^l, C_k, C_h)^h = \\text{Softmax}(\\frac{(T^h(H_{t+1}^l) K^h(C_h)^T}{\\sqrt{d_h}} V^h(C_h)\n\\end{equation}\nHere,  d_h  is per head dimension, and  Q^h, K^h, V^h  are linear transformations per head, where  Q^h varies across layers and  K^h, V^h  are layer-shared. The final CA outputs are concatenated vectors from all heads. It is worth noting that GCA is easy to integrate with FlashAttention-2, as detailed in the pseudo-code in Appendix A.2.\nGCA vs CCA. A key distinction between GCA and CCA lies in how softmax is applied to cross-attention matrices as shown in Figure 1(a)(b). In CCA, all retrieved chunks are concatenated and softmax is directly applied to the whole attention matrix to fuse token-level information. Notably, relevance scores are entirely excluded from the process. In contrast, GCA applies softmax to each chunk's attention matrix separately. This modification allows information to be gathered from each chunk separately. The softmaxed relevance scores then serve as soft choices (Hu et al., 2021; 2022), participating in the next token prediction thus receiving back-propagated gradients.\nEncoder-Decoder Variant. Our model architecture could be easily modified to an encoder-decoder-based variant like RETRO (Borgeaud et al., 2022) by directly applying the encoder to chunk token embeddings instead of the lower layers' outputs. This variant allows for retrieval from trillions of tokens with acceptable storage costs. We will elaborate on this in \u00a7 3.3."}, {"title": "3.2 TRAINING", "content": "The pre-training objective of DRT is the next token prediction, but there are certain details as discussed below. We also give a detailed time complexity analysis of training.\nGumbel Top-k sampling. The core idea of self-supervised retrieval is making candidate chunks compete with each other by softmaxing relevance scores as weights. The weights of the chunks contributing most to the next chunk prediction are enhanced while the weights of the rest are suppressed. To balance exploration and exploitation, we sample chunks based on relevance scores instead of always picking the top-k, enabling highly relevant chunks to be more likely chosen while still considering lower-scoring ones. A simple trick is to add Gumbel noise (Gumbel, 1954) to the raw scores before the top-k operation. Importantly, this noise doesn't affect subsequent operations."}, {"title": "Encoder-Decoder Pre-training.", "content": "To enhance the encoder's representational ability, we train it with MLM in addition to the auto-regressive loss during the first half of pre-training. Specifically, while the decoder sees all tokens, the encoder's tokens are partially masked, whose outputs are used in both the decoder's GCA and computing the MLM objective. Masking causes inaccurate encoding that may disturb subsequent computation and training of the decoder, so we eventually remove the MLM training in the second half of the pretraining.\nTime Complexity. Our approach reduces training complexity by compressing quadratic operations. Vanilla Transformers have a complexity of  O(NL^2)  for full self-attention. In DRT, we encode chunks with S tokens into landmark representations, performing chunk-wise full attention to compute relevance scores within  O(G)  for G groups of upper layers. By employing sliding-window attention and top-k retrieval, we scale down self-attention and GCA costs to  O(G^2 + LKS + NLW),  where K is the retrieved chunk number and W is the window size, K, W \u00ab L. This largely reduces the complexity but maintains the random-access flexibility."}, {"title": "3.3 INFERENCE", "content": "Memory Offloading. During the inference stage of Transformers, the default memory cost for KV Cache is  O(NLd) .  To reduce GPU memory usage, we can offload past chunk representations to CPU memory. This results in a spatial complexity of the GPU memory usage  O(\\frac{NLd}{S} + KS + KSd + NWd)  during inference. Here, La La is the memory footprint of landmark representations, while the remaining terms account for the GCA and sliding-window KV cache. Although each retrieval involves gathering representations from CPU memory and transferring them to the GPU, this operation occurs G times every S tokens. Therefore, the cost of memory exchange from chunk retrievals is minimal.\nInfinitely Long Context Retrieval. To mimic humans' capability for random access to memories, we extend the retrievable context to all pre-trained tokens, where contextualized token representations in chunks could be regarded as memory. A straightforward approach, similar to RPT, is to store chunk-level representations and token-level hidden states as key-value pairs in a Faiss (Douze et al., 2024). However, this approach requires significant disk space. For instance, a 100 billion token corpus with 1,024-dimensional hidden states, even with Product Quantization, demands approximately 25TB. In contrast, using an encoder-decoder variant, we only store a chunk's landmark representation and positions in the corpus without saving hidden states. When retrieving, we access the corresponding tokens by document position and re-encode them to obtain their hidden states, achieving retrieval from billions of tokens with disk cost reducing by S fold."}, {"title": "4 EXPERIMENTS", "content": "We compare DRT with prior works in long-range language modeling, wall-clock training time, inference cost, extrapolation capability, and infinite-length context retrieval. Notably, we include the closely related works RPT and Landmark Attention."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": null}, {"title": "4.1.1 DATASETS", "content": "PG19. PG19 (Rae et al., 2020) is a language modeling benchmark widely used to evaluate long-range text understanding capabilities of models. It includes a collection of full-length books from Project Gutenberg across a wide range of genres, styles, and topics. The dataset is particularly useful for evaluating models' abilities to capture dependencies and context over long sequences of text.\nArXiv-math. ArXiv-math is a corpus consisting of mathematical papers from arXiv. Characterized by a sustained coherence over long distance, the corpus requires models' capability of correctly referring to long-range history information and using long-range context effectively for predictions. We use the preprocessed corpus and data splits from Azerbayev et al. (2023).\nMiniPile. MiniPile (Kaddour, 2023) is a 6GB subset of the deduplicated 825GB The Pile (Gao et al., 2021) corpus, which covers sub-datasets from webpages, dialogue, books, science and code."}, {"title": "4.1.2 MODELS", "content": "DRT retrieval G. A DRT consists of 12 Transformer decoder layers, divided into 6 lower and 6 upper layers, with upper layers further split into G groups. The sliding window size is set to W=512, the chunk size is set to S=64, and 8 chunks are retrieved for GCA, resulting in an attention field of 512 + (8 \u00d7 64). Notably, we implement hardware-aware GCA based on Triton (Tillet et al., 2019) and FlashAttention-2 (Dao, 2024). As we employ various parameter settings across different experiments, further details are provided in Appendix A.1.\nBase LM. Our base LM is based on the implementation of TinyLlama (Zhang et al., 2024) combined with Flash Attention2 (Dao, 2024) enabling ALiBi (Press et al., 2022) and sliding window attention (Child et al., 2019). We compare models against the baseline across various configurations. One configuration involves 12 layers with a sliding window of 512 tokens, aligning with the DRT sliding window size. Another configuration of 12 layers with a 768-token sliding window ensures the same attention field coverage, as 12 \u00d7 768 = 12 \u00d7 512 + 6 \u00d7 512(GCA). The strongest baseline, with 14 layers and a 658-token sliding window, has a parameter count comparable to our DRT while maintaining a similar total attention field across all 12 layers, calculated as 658 \u00d7 14 \u2248 12 \u00d7 512 + 6 \u00d7 512.\nRetrieval-Pretrained Transformer (RPT). Since the official implementation is in JAX and the code for distilling the retriever is not released, we reimplement RPT in PyTorch and replace the retriever with Contriever (Izacard et al., 2022), which is a strong dense retriever widely employed.\nLandmark Attn. We use the official Llama-like implementation of Landmark Attention. Similar to Base LM, we extend the length of the self-attention range from 512 to 768 to ensure it shares the same attention field as DRT.\nBlock-Recurrent TFM. Since the official implementation of Block-Recurrent Transformer is also based on JAX, we utilized a PyTorch implementation to ensure all baselines are running with the same framework.\nDRTenc-dec. A variant of DRT introduced in \u00a7 3.1, which utilizes a 2-layer Transformer as the encoder, incorporating absolute positional encoding.\nAblations. w/o Triton: A naively implemented version of GCA without Triton. w/o enc: The decoder-only version of DRT, which uses the hidden states of the middle layer of the decoder stack to retrieve history chunks. Differently put, this model can be attained by simply removing the encoder from DRTenc-dec. w/o mlm: The architecture is exactly the same as DRTenc-dec, while the only difference lies in the training process by eliminating the masked language modeling part. w/o gumbel top-k: The architecture is exactly the same as DRT, while the only difference lies in the training process by eliminating the gumbel noise when selecting the top-k chunks."}, {"title": "4.2 LONG-RANGE LANGUAGE MODELING", "content": "In this section, we evaluate DRT against baselines in long-range language modeling on PG19 and arXiv-math, and report their respective perplexities. All models are pre-trained with the same attention field and a 16K context by default, except for baselines that cannot efficiently pre-train on long contexts. To ensure fairness, we adjusted these baselines. Detailed hyper-parameters are provided in Appendix A.1.\nResults. From Table 1, we have several observations. Firstly, DRT outperforms most baselines in the group where the evaluation length exceeds 16K, except for the performance of Landmark Attn (LA) on PG19 at 16K. We argue that LA has certain advantages: DRT performs retrieval for G times every 64 tokens, whereas LA performs retrieval at every token and in every layer, making it many times more frequent than ours. However, we still outperform it on arXiv-math at 16k length. A possible explanation is the arXiv-math dataset contains stronger long-range causal dependencies, which"}, {"title": "4.3 ANALYSIS", "content": "In this section, we analyze the inference cost, the relationship between training time and context length, and the extrapolation capability of DRT. In the inference cost analysis, We skip RPT as because it has a similar cost to DRT. We mainly compare the inference speed and memory cost of DRT, Landmark Attention, and Block Recurrent TFM, with and without CPU memory offloading. In the extrapolation experiments, we utilize the pre-trained models described in \u00a7 4.2 to assess their performance with extended context lengths."}, {"title": "4.4 RETRIEVAL FROM SIMULATED INFINITELY LONG CONTEXT", "content": "We wonder whether a densely pre-trained retriever can generalize from a limited to an unlimited context. To verify this, we simulate an infinitely long context by retrieving from all pre-trained tokens. Specifically, we pre-train DRTenc-dec on MiniPile and store all landmark representations in a Faiss as described in \u00a7 3.3 to emulate an infinite context. Detailed training hyper-parameters can be found in Appendix A.1. Specifically, we prepare DRT with different settings. DRTw/ random retrieval uses a randomly generated vector for retrieval. DRTw/ Contriever utilizes a pre-trained retriever with fixed parameters to select top-k relevance chunks, with information still fused via GCA."}, {"title": "4.5 CASE STUDIES", "content": "We start first with the following lemma which is useful to establish the Quillen equivalence.\n\\begin{lemma} \\label{lem-reflect-equiv} Let M be a symmetric monoidal model category that is combinatorial and left proper. Assume that the transferred (natural) model structure on com exists.\n\\end{lemma}\nweak equivalence between co-Segal categories is just a level-wise weak equivalence.\\begin{prop}\n\\label{prop-eta-kx-loc-equiv} For any F \u2208 coms, the canonical map F \u2192 |F| is an equivalence in comsepc i.e, it's a kb(I)-local equivalence in comsep (whence in comse).\n\\end{prop}\nThanks to Proposition \\ref{prop-eta-kx-loc-equiv}, we know that \u03b7 : F \u2192 F is always a kb(I)-local equivalence. Then by 3-for-2 we see that o is a kb(I)-local equivalence if and only if ol(\u03c3) is."}, {"title": "5 CONCLUSION & FUTURE WORKS", "content": "In this study, we successfully optimize the retriever module with the causal LM objective in an end-to-end manner. The core innovation lies in the Grouped Cross-Attention (GCA), which makes relevance scores differentiable by using them to fuse information retrieved by the current chunk for next chunk prediction. Combined with Gumbel top-k sampling, this approach enables the pre-training of LMs on context lengths extending up to 64K tokens.\nIn future work, we will explore self-supervised causal retrieval from vast amounts of tokens outside the context. Meanwhile, we will combine structured representations (Hu et al., 2024b;a) to achieve multi-granular retrieval."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 HYPER-PARAMETERS", "content": "Long-Range Language Modeling. We employ a Llama-like architecture (Touvron et al., 2023) featuring a 12-layer, decoder-only transformer with 12 heads per layer (64 dimensions each), an embedding dimension of 768, and an FFN size of 2048. Training utilizes the AdamW optimizer (Loshchilov & Hutter, 2019) with \u03b2\u2081 = 0.9 and B2 = 0.95, and a weight decay factor of 0.001. We used base learning rate 2 \u00d7 10-\u00b3 for all our experiments with a warmup stage that was 2% of the whole training and applied a cosine scheduler with final learning rate being 4 \u00d7 10-4. We used GPT-2's (Radford et al., 2019) tokenizer. We used mixed-precision training with bfloat16 over at 8 Nvidia A100 GPUs. We train all models with an effective batch size of 219 tokens for 60K steps resulting in a total training budget of 32.2 billion tokens. We train Base LM, RPT and DRT on each dataset with a context length of 16K tokens. Due to Landmark Attention doesn't support sliding-window attention, the model is pre-trianed with full self-attention with a context length of 768. Due to Block Recurrent Transformer cannot be fully paralleled, which takes 5\u00d7 wall-clock training time with 16K context length, we pre-train it with a context length of 4K.\nInfinitely Long Context Retrieval. We employ the same architecture with the same number of layers, but with an embedding dimension of 1,024, and an FFN size of 2,816. We train DRT on MiniPile for 20 epochs with 384K tokens per batch."}, {"title": "A.2 HARDWARE-AWARE GCA PSUEDO-CODE", "content": "Algorithm 1 FLASHGCA forward pass\nRequire: Matrices Q \u2208 RNq\u00d7d, K, V \u2208 RK \u00d7 Nkv\u00d7d in HBM, vector w \u2208 R* in HBM, block sizes Bc, Br.\n1: Divide Q into Tr\nwhere Tc\nN \nBc\nblocks Q1,..., QT of size B \u00d7 d each, and divide K, V in to K \u00d7 Te blocks\nK1,1,..., KKT and V1,1,..., VK,T, of size Be \u00d7 d each.\n2: Divide the output O \u2208 RNq\u00d7d into Tr blocks O\u017c, . . ., Or, of size Br \u00d7 d each, and divide the logsumexp\nL\u2208 RNK into T \u00d7 K blocks L1,1,..., LT,,k of size Br each.\n3: Divide the output O\u2032 \u2208 [][RK\u00d7Nq\u00d7d into T, blocks O1,1, ..., OK,T, of size K \u00d7 B \u00d7 d each.\nOn chip, initialize O\u00ba) = (0)Brxd \u2208 RBrxd, l(0) = (0)B, \u2208 RBr, m\u00ba) = (-\u221e)\u0432, \u2208 RBr."}]}