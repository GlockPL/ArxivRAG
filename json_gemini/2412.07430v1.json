{"title": "Knowledge Graph Guided Evaluation of Abstention Techniques", "authors": ["Kinshuk Vasisht", "Navreet Kaur", "Danish Pruthi"], "abstract": "To deploy language models safely, it is crucial\nthat they abstain from responding to inappro-\npriate requests. Several prior studies test the\nsafety promises of models based on their ef-\nfectiveness in blocking malicious requests. In\nthis work, we focus on evaluating the under-\nlying techniques that cause models to abstain.\nWe create SELECT, a benchmark derived from\na set of benign concepts (e.g., \"rivers\") from\na knowledge graph. The nature of SELECT en-\nables us to isolate the effects of abstention tech-\nniques from other safety training procedures, as\nwell as evaluate their generalization and speci-\nficity. Using SELECT, we benchmark differ-\nent abstention techniques over six open-weight\nand closed-source models. We find that the\nexamined techniques indeed cause models to\nabstain with over 80% abstention rates. How-\never, these techniques are not as effective for\ndescendants of the target concepts, with refusal\nrates declining by 19%. We also characterize\nthe generalization-vs-specificity trade-offs for\ndifferent techniques. Overall, no single tech-\nnique is invariably better than the others. Our\nfindings call for a careful evaluation of differ-\nent aspects of abstention, and hopefully inform\npractitioners of various trade-offs involved.", "sections": [{"title": "Introduction", "content": "For several reasons, it is critical that language mod-\nels abstain from responding to inappropriate user\nrequests. These requests could include (malicious)\nattempts to assist users in illegal activity (Fang\net al., 2024; Weidinger et al., 2021), generate of-\nfensive content (Deshpande et al., 2023; Gehman\net al., 2020), or disseminate large-scale misinfor-\nmation (Tamkin et al., 2021; Buchanan et al., 2021).\nTo block such requests, a common approach is to\nperform some form of \"safety\" training before re-\nleasing language models (Bai et al., 2022). Safety"}, {"title": "Related Work", "content": "Evaluating Abstention in Language Models.\nWhen deploying large language models, it is cru-\ncial to abstain from inappropriate or malicious\nuser requests (Weidinger et al., 2021; Fang et al.,\n2024). User requests\u2014and corresponding model\nresponses-may lead to bias, discrimination or tox-\nicity (Deshpande et al., 2023; Gehman et al., 2020).\nFurther, users may request information that is not\ncaptured within the parametric knowledge of the\nmodel (Feng et al., 2024), responses to which may\nbe used to disseminate misinformation (Tamkin\net al., 2021; Buchanan et al., 2021). Several recent\nefforts benchmark different facets of abstention\nin language models. For instance, SORRY-Bench\n(Xie et al., 2024) focuses on benchmarking the\nability of models to abstain from user requests con-\nsidered harmful and malicious. The study bench-\nmarks various open and closed source models, find-\ning closed-source models display tolerable refusal\nrates across different categories of harm. Further,\nUnknownBench (Liu et al., 2024) evaluates the abil-\nity of models to abstain in scenarios where models\ndo not possess adequate knowledge to answer a\nquestion. The study notes that the refusal rates are\nfar from perfect for GPT-4 models. Another bench-\nmark, Priv-QA (Chen et al., 2023), evaluates ab-\nstention over protected groups whose information"}, {"title": "Methodology", "content": "In order to evaluate various abstention techniques,\nwe introduce SELECT, a benchmark comprising a\ntaxonomy of 394 benign atomic and 156 composite\nconcepts derived from the YAGO 4.5 knowledge\ngraph (Suchanek et al., 2024). YAGO, built from\nWikiData, represents semantic relations between\na diverse set of entities. This allows us to capture\nthese relations to form a taxonomy to study absten-"}, {"title": "Dataset Construction", "content": "To build SELECT, we identify five top-level entities\nin YAGO's ontology as root concepts for our tax-\nonomy: Products, People, Places, Organizations,\nand Creative Works. Most entities in YAGO are de-\nscendants of these concepts. For each concept, we\nrecursively include descendants related by an 'IS-A'\nrelation in YAGO. We repeat this process until we\nreach a leaf node, or a depth of six (whichever cri-\nteria is met first). For leaf nodes, such as \u201crivers\u201d,\nwe sample up to 5 instances of the leaf concept. In\norder to study generalization and specificity, we\naltogether discard concepts that lie on short paths,\nwherein the distance between the root node and the\nleaf node is lower than 3. This process results in a\ntotal of 394 atomic concepts and instances.\nUsing atomic concepts, we create compositions\nof concepts (e.g., \"books\" about \"people\") by com-\nbining two concepts with a fixed set of manually\ncurated templates. We populate said templates with\nthe intended concepts and their descendants as per\nassociated rules to obtain valid compositions. As\nan example, we define a template \u2018{} about {}' to\ncompose 'creative works' with 'people', and popu-\nlating the template we obtain 'books about people',\n'movies about footballers', etc. We then arrange\nthese compositions in a taxonomy, by linking com-\npositions together based on the relation between\nthe atomic concepts they were derived from. For\nexample, we associate \u2018novels about people' as a\nchild of 'books about people', as \u2018novels' is a child\nof 'books' in YAGO. With this process, we obtain\na taxonomy of 156 compositions and a level depth\nof 5. Further details about the set of templates used\nare available in Appendix \u00a7A.\nAfter collating the concepts, we use a language\nmodel (GPT-40, OpenAI (2024)) to generate 50\nconcept-related questions. We explicitly instruct\nthe model to generate diverse questions about the\nconcept that users are likely to ask. Along with\nour instruction, we also include five examples of\nuser queries from WildChat-1M which is a reposi-\ntory of human-ChatGPT interactions (Zhao et al.,\n2024). We manually examine 180 generated ques-\ntions from 18 randomly sampled concepts, and find\nthem to be plausible and relevant to the concept\nin question. From the 50 questions that we gener-\nate for every concept, we use 20 for training (or"}, {"title": "Evaluation Metrics", "content": "Consider a language model $m : X \\rightarrow Y$ that gener-\nates an output $y \\in Y$ given an input query $q\\in X$.\nLet $m_{c,a}: X \\rightarrow Y$ be the model instructed, or\nupdated, to abstain from the concept $c$ using an ab-\nstention technique $a$. We denote the taxonomy of\nconcepts in SELECT by $T$. For each concept $c \\in T$,\nlet $D_T(c)$ denote the descendants of $c$, from the\nsub-tree rooted at $c$. Further, let $A_T(c)$ and $S_T(c)$\ndenote the ancestors and siblings of $c$ respectively.\nWe define siblings as concepts at the same level\nin $T$ that share a parent with $c$. Lastly, for every\nconcept $c$ we have a set of questions $Q_c$ related to\n$c$. The number of questions for each concept is the\nsame. We consider the following metrics:\nDefinition 1 (Abstention Rate). For a given con-\ncept $c$, abstention rate of a model after the appli-\ncation of abstention technique $a$ is defined as the\nproportion of concept-related questions $Q_c$ that the\nmodel $m_{c,a}$ abstains from answering.\nDefinition 2 (Generalization). Generalization is\nthe proportion of questions related to descendants\n$D_T(c)$ that the model $m_{c,a}$ refuses to answer.\nDefinition 3 (Specificity). Specificity is defined as\nthe proportion of questions related to the concept's\nsiblings $S_T(c)$ and ancestors $A_T(c)$ that the model\n$m_{c,a}$ does not refuse to answer."}, {"title": "Experimental Setup", "content": "We evaluate five popular abstention techniques us-\ning SELECT. The chosen techniques vary in terms of"}, {"title": "Abstention Techniques", "content": "the nature of updates to the model (temporary ver-\nsus permanent), degree of access required (black-\nbox versus white-box), and speed and compute re-\nquirements. The techniques we consider include (i)\ninference-based methods: prompting and activation\nsteering, and (ii) fine-tuning methods.\nPrompting. Instructing, or prompting, language\nmodels is shown to be effective for refusing certain\nqueries (Zheng et al., 2024). We experiment with\ntwo widely used prompting strategies: (1) Zero-\nShot prompting (ZS), where we explicitly instruct\nthe model to abstain from a target concept, and (2)\nFew-Shot Chain-of-Thought prompting (FS CoT)\n(Wei et al., 2022), where we give examples along\nwith instructions for the model to generate reason-\ning for its abstention.  We select six questions\u2014\nand their desired responses\u2014as examples for FS\nCoT. The 6 questions comprise one question corre-\nsponding to target concept, 2 for descendants, and 3\nrelated to sibling and parent concepts. The desired\noutput for the first three is abstention, whereas we\nuse existing model outputs as desired responses for\nthe last three (as we do not want models to abstain\nfor queries corresponding to parent and sibling con-\ncepts). Further details about the setup and prompts\nare provided in the Appendix \u00a7C.1.\nActivation Steering. Recent studies show that\nactivation steering i.e. modifying model activa-\ntions during inference can cause selective absten-\ntion (Turner et al., 2024; Zou et al., 2023a). We\nuse conditional activation steering similar to CAST\n(Lee et al., 2024) to direct models towards abstain-\ning from specific concepts. This technique requires\naccess to the model's weights to derive two vectors:\nconcept and refusal. Concept vectors enable us to\ndetermine if a query is related to a given concept,\nand a refusal vector helps in steering the model to\nabstain from that concept. We derive the concept\nvector using questions from SELECT that are related\nand unrelated to a concept. Specifically, we select\nthe first principal component of the difference in\nmodel activations for related and unrelated ques-\ntions as the concept vector. We then classify new\nquestions as concept-related if its cosine similar-\nity with the concept vector lies above a threshold\nat specific layers.  Similarly, we obtain a refusal"}, {"title": "Models and Evaluation Setup", "content": "We evaluate different abstention techniques over\nsix open-weight and commercial language models,\nincluding LLaMA 3.1, Gemma 2 and GPT-40 (the\nexact list of models is given in \u00a7B). For commer-\ncial models, we only evaluate techniques that do\nnot require white-box access. We select models\nbased on their diversity across several dimensions:\nopen versus commercial, model sizes, capabilities\nand popularity. This allows us to study how effec-\ntive abstention techniques are for different kinds\nof models. Across all our experiments, we use the\ninstruction-tuned variants of these models.\nFor each concept in SELECT, we use an absten-\ntion technique over a language model to enforce\nrefusal of that concept. To compute abstention\nrates, generalization and specificity, we collect a\nset of questions from SELECT as described in \u00a73.2"}, {"title": "Results & Discussion", "content": "How well do abstention techniques\nperform for benign concepts?\nWe evaluate the average abstention rates for dif-\nferent techniques across models, summarized in\nTable 2. The results indicate that abstention tech-\nniques are generally effective at enforcing absten-\ntion for benign concepts in SELECT, with refusal\nrates mostly above 80%. Prompting with CoT and\nfew-shot examples achieves the highest abstention\nrates, nearly 100% for Gemma-2 2B and GPT-40,\ndespite their differing sizes. Activation steering\nalso shows promise, outperforming prompting for\nLLaMa 3.1 8B and Mistral 7B, but it is significantly\nmore compute-intensive and requires white-box ac-\ncess to the model. Inference-based methods gen-\nerally outperform fine-tuning by 10% on average,\nbut some techniques are not as effective for certain"}, {"title": "How well do abstention techniques\nperform for composition of concepts?", "content": "As in 5.1, we evaluate average abstention rates for\nabstention techniques across models (Table 4). We\nfind abstention techniques to also be effective for\ncompositions of concepts, however the abstention\nrates are not as high as they were for atomic con-\ncepts. In general, abstention rates reduce by 3.4%\non average, notably by 18% for CoT prompting\nwith Mistral 7B. However, other trends are similar\nto atomic concepts, such as CoT prompting with\nfew-shot examples giving the best abstention rates\noverall followed by activation steering. Activation\nsteering demonstrates negligible drop in abstention\nrates on average, with the scores for Mistral 7B\nbeing even higher than with atomic concepts.\nFurthermore, we find that abstention techniques\nover-refuse more often for compositions of con-\ncepts. Compared to atomic concepts, generaliza-\ntion for compositions increases by 9% on average,"}, {"title": "Conclusion", "content": "In this work, we introduced a benchmark to eval-\nuate abstention techniques for language models,\ncomprising benign concepts and compositions\ngrounded in a knowledge graph. Isolating the ef-\nfects of safety training, we evaluated abstention\ntechniques including prompting, activation steer-\ning and fine-tuning for their effectiveness, gener-\nalization and specificity. We find inference-based\nmethods outperform fine-tuning in effectiveness,\nhowever the performance drops in generalization\nare more severe relative to fine-tuning. We hope\nour findings inform practitioners employing said\ntechniques of the various trade-offs involved."}, {"title": "Limitations", "content": "We identify some important limitations of our work.\nWe only consider a limited set of concepts as part of"}, {"title": "Ethics Statement", "content": "Our evaluations involve questions about benign and\nabstract concepts, rather than \u201csensitive\u201d topics.\nThe concepts are derived from YAGO, which exists\nin public domain (CC by 4.0) , and the curation\nprocess is entirely automated-except the curation\nof templates for compositions. We plan to release\nall data and evaluation code to facilitate further"}, {"title": "Construction Details for SELECT", "content": "Identifying Relation Templates\nTo create compositions, we manually curate a set of\nrelation templates describing relations between two\natomic concepts. As an example, the concepts of\n'Plastics' and 'Rivers' may be composed together"}, {"title": "Creating the Taxonomy of Compositions\nfrom Relation Templates", "content": "From the relation templates, we obtain composi-\ntions by populating the templates according to the\nassociated rules. These compositions are arranged\nin a taxonomy similar to the one for atomic con-\ncepts, with links determined based on the relations\nbetween atomic concepts the compositions were de-\nrived from. For example, 'novels about people' and\n'books about creative people' are added as children\nof the composition 'books about people', while\n'novels about sportspeople' is added as a child of\n'novels about 'people'. While linking compositions\nwe account for ambiguities that may arise due to\nmultiple parent candidates, and resolve the same\nbased on decisions that maximize the depth of the\nnew taxonomy. For instance, for the composition\n'novels about sportspeople', both 'novels about peo-\nple' and 'books about sportspeople' are valid can-\ndidates for parents. If \u2018novels about people' is at\na higher depth, we associate it as the parent for\n\u2018novels about sportspeople'. To also ensure that we\ndo not have an excessive amount of children per\nnode, we further prune children whose sub-trees\nhave a depth smaller than 3 from each node, until\neach node has at most 4 children."}, {"title": "Models", "content": "Amongst the open-weight models, we evaluate\nLLaMA-3.1 8B, Gemma-2 (2B and 9B sizes) and\nMistral 7B. We use the publicly-available check-\npoints for these models obtained from Hugging-Face \u2075. We also evaluate API-based closed-source\nmodels including GPT-3.5-Turbo and GPT-40. The\nexact checkpoints utilized are listed in Table 6."}, {"title": "Enforcing Abstention via Prompting", "content": "In the simplest setting, the models are instructed\nto abstain from responding to queries about the\nconcept of interest through a prompt. We evaluate\ntwo methods of prompting: prompting solely with\ninstructions, and prompting with chain-of-thought\n(Wei et al., 2022) and few-shot examples.\nWe use the following prompt to induce absten-\ntion from a target concept in the zero-shot setting:"}, {"title": "Enforcing Abstention using Conditional\nActivation Steering", "content": "We obtain concept vectors using related and unre-\nlated questions about the target concept. For related\nquestions, we use 15 questions about the target\nconcept and 5 about descendants. For unrelated\nquestions, we pick 15 questions about concepts\noutside the sub-tree of the target concept, and 5\nabout the parent and sibling concepts. We then\nconsider all pairwise combinations of related and\nunrelated questions. For each pair of questions, we\ncollect the difference in activations over the last\ntoken from all layers. Over these differences, we\nthen apply PCA layerwise, and consider the first\nprincipal component from each layer as the concept\nvector for that layer.\nMost layers do not encode features about the\nconcept of interest, and thus not all concept vectors\ncan accurately classify questions. As a result, we\nidentify a subset of layers to use during classifica-\ntion, using the validation question pairs. We run\na forward pass over all pairs in the validation set,\nand then compute the cosine similarity between\nthe activations at a layer and the concept vector for\nthat layer. Repeating this process for all layers, we\nobtain cosine similarities for related and unrelated\nquestion pairs in the validation set. Layers which\nthen maximize the difference between the similar-\nity over the related question with the similarity to\nthe unrelated question are then selected for classifi-\ncation. We select a maximum of three layers which\nhave the highest differences between similarities as\nthe subset of layers to use.\nFor classifying new instances, we simply com-\npute the cosine similarity of activations across the\nidentified subset of layers with the associated con-\ncept vectors. If these lie above a threshold, we\nclassify the instance as being related to the target\nconcept. The threshold is also determined using\nthe validation set, during the time of identifying the\nsubset of layers. For every layer that we choose, we\nconsider the minimum cosine similarity across the\nrelated questions at the layer as the threshold. We\nfurther adjust this value by a margin equal to the\nmidpoint between similarities across related and\nunrelated questions."}, {"title": "Enforcing Abstention via Fine-Tuning", "content": "Across both fine-tuning methods that we con-\nsider, namely SFT and SFT + DPO, we employ\nparameter-efficient fine-tuning to update only a\nsmall fraction (~ 1%) of model parameters. We\nuse LoRA adapters (Hu et al., 2022) with a rank of\n32 for this purpose, that are applied over all linear\nparameters except the embedding, unembedding\nand attention-key projection matrices. The SFT\n+ DPO setting uses the adapter trained during the\nSFT evaluation as a checkpoint for further prefer-\nence optimization using DPO."}, {"title": "Evaluating Abstention in Responses", "content": "The metrics defined in \u00a73.2 require a method to de-\ntermine whether a response constitutes absention.\nWhen abstaining using prompting, we instruct the\nmodels to use a specific phrase in its responses\nto signify abstention, however guaranteeing adher-\nence to such instructions across all models is not\npossible for the different abstention techniques that\nwe evaluate. Further, even with prompting, models\ndo not always follow instructions and may use other\ncreative ways to signify abstention. As a result, we\nfollow past work and employ a phrase matching\napproach for evaluating abstention (Arditi et al.,"}, {"title": "Variations across Taxonomy Levels", "content": "Figure 5 list out results for variations across evalu-\nation metrics for different levels of the taxonomy\nas in \u00a75.1 for the remaining models.\nSimilar to the LLaMA-3.1 model, different tech-\nniques have similar trends across metrics. However,\nthe changes are less pronounced in some cases.\nSome notable exceptions also exist, for instance,"}, {"title": "Quantifying Generalization Errors", "content": "To explain the reduced abstention rates over descen-\ndants, one hypothesis may argue that abstention\nrates degrading over descendants may simply be a\nconsequence of the models' inability to understand\nthe hierarchical relationship between concepts and\nits children. In such a case the abstention technique\nis not accountable for generalization errors. The\nhypothesis posits that models may not encode rela-\ntions between concepts in the same way as YAGO,\nwhich may explain the errors over queries to evalu-\nate generalization. In order to test this hypothesis,\nwe quantify the ratio of generalization errors across\nmodels for different abstention techniques that can\nbe explained using the models' knowledge of hier-\narchical relations between concepts.\nTo collect information about the models' under-\nstanding of relations between concepts, we prompt\nthe model to answer 'True' or 'False' based on\nwhether two concepts as related. The response\nfrom the model is sampled with a temperature of 0.\nThe prompt used for this purpose is:"}, {"title": "Exploring Universality of Abstention for\nConcepts", "content": "We explore whether abstention rates, generaliza-\ntion and specificity vary in a similar manner at the\nconcept level by looking at correlations between\nmetrics across different techniques in Figure 7. We\ncompute the Spearman rank-correlation between\nmetric performance across pairs of abstention tech-"}]}