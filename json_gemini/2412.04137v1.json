{"title": "Text Change Detection in Multilingual Documents Using Image Comparison", "authors": ["Doyoung Park", "Naresh Reddy Yarram", "Sunjin Kim", "Minkyu Kim", "Seongho Cho", "Taehee Lee"], "abstract": "Document comparison typically relies on optical character recognition (OCR) as its core technology. However, OCR requires the selection of appropriate language models for each document and the performance of multilingual or hybrid models remains limited. To overcome these challenges, we propose text change detection (TCD) using an image comparison model tailored for multilingual documents. Unlike OCR-based approaches, our method employs word-level text image-to-image comparison to detect changes. Our model generates bidirectional change segmentation maps between the source and target documents. To enhance performance without requiring explicit text alignment or scaling preprocessing, we employ correlations among multi-scale attention features. We also construct a benchmark dataset comprising actual printed and scanned word pairs in various languages to evaluate our model. We validate our approach using our benchmark dataset and public benchmarks Distorted Document Images and the LRDE Document Binarization Dataset. We compare our model against state-of-the-art semantic segmentation and change detection models, as well as to conventional OCR-based models.", "sections": [{"title": "1. Introduction", "content": "In the digital age, the use of multilingual documents has proliferated, facilitated by globalization and the interconnectivity of international enterprises [4, 32]. These documents often undergo various stages of editing, requiring precise tracking and confirmation of changes to the text across different versions. Ensuring consistency and accuracy in these documents is critical for maintaining the integrity of the information they contain, especially in fields such as real estate, logistics, and finance. Recently, there has been a trend towards storing these documents as images, such as scanned contract documents and digitized books. This transition has presented new challenges and opportunities for document processing and analysis technologies. One important strategy for image-based document comparison has been the use of optical character recognition (OCR). Early OCR methods relied on pattern matching with standard character templates for character recognition [31, 36, 43]. However, the emergence of deep learning (DL) techniques has led to significant improvements in various computer vision technologies, including classification, detection, segmentation, and OCR [2,5,13,21,23,37]. OCR technology is now widely used in a wide range of applications, including invoice processing, banking, legal documentation, and paper document digitization [38, 40]. The introduction of transformer models [44] has also promoted innovative forms of document analysis, leading to further improvements in OCR systems [12, 17]. As a result, most off-the-shelf imaged document comparison software is based on OCR [42]. This software integrates pre-processing steps, structural analysis, text detection, and various post-processing techniques to optimize OCR performance. However, background noise, font variation, and multilingual text recognition can still affect OCR accuracy, particularly as the range of recognized languages increases. In particular, handling multilingual documents places a significant burden on OCR systems, especially when encountering languages they have not been trained on or unexpected text formats. Therefore, the present study proposes image comparison, which detects changes in an image or compares the quality between images, as a promising alternative to OCR for detecting text changes in multilingual documents. By directly comparing text area images in order to bidirectionally detect changes between the source and target documents, our method eliminates the need for language-specific OCR models.\nThe major contributions of this study are as follows:\n\u2022 To the best of our knowledge, we propose the first text-image-based two-way change detection model that is independent of the language used in the text.\n\u2022 We employ a correlation marginalization technique based on the surrounding features, reducing the need for extensive pre-processing steps.\n\u2022 We construct a new imaged-text change-detection dataset and use it to demonstrate the state-of-the-art"}, {"title": "2. Related Work", "content": "At its core, Document comparison involves textual analysis. Text comparison traditionally comprises two primary tasks: text detection and text recognition. However, after text detection is performed, our proposed method differs by using semantic segmentation for image-to-image comparisons rather than text recognition. In particular, our approach focuses on detecting differences between documents by directly comparing images of text areas. When comparing text images, additional techniques are typically employed to clearly identify differences between documents, including noise removal and document layout analysis. To explain the importance of our method, we briefly summarize relevant past studies in this section."}, {"title": "2.1. Document image comparison", "content": "An early study on document comparison introduced VisualDiff [18], which used SIFT [29] to align and compare document images, demonstrating the potential for detecting document changes. In another study that did not utilize OCR, Lin et al. [27] proposed a model that achieved fast speed and high precision in comparing multilingual documents. However, this approach falls short of true multilingual functionality, relying solely on English, Chinese, and Japanese character features. More recently, scanned document comparison based OCR has been studied [1], proposing a method that uses a combination of image comparison techniques to detect modifications between two versions of the same administrative document. The proposed method establishes a basic document comparison process, but, OCR remains vulnerable to various limitations, such as low scan quality and multilingual documents. To address the challenge of multilingual comparison, a hierarchical document comparison method was recently proposed [33], achieving superior performance in multilingual documents. However, this method is challenging to apply when there are numerous changes within the document due to the hierarchical structure of the method. We propose a character image-based comparison method to address the problems of these previous studies. This method is language-independent, supports multiple languages, and is applicable even when many change, similar to OCR. Moreover, our method is more robust than OCR against image noise, such as from scanning."}, {"title": "2.2. Text recognition", "content": "Early text recognition methods relied on computer vision (CV) and machine learning techniques [35,39]. Graves et al. proposed connectionist temporal classification (CTC), a method that uses a recurrent neural network (RNN) to decode features [14]. The performance of the technique has improved since the development of deep learning technology; however, a major drawback of these models is that they require a separate model for each language. To address this issue, a multiplexed OCR model has been proposed which utilizes language prediction networks (LPNs) for language-adaptive recognition [16]. However, this method introduces performance gaps between languages, and recent approaches must revert to specific training models for each language. Since the development of deep learning technology, lightweight models such as PPOcr [24] and transformer-based models such as TrOCR [25] have been proposed. Although these methods support multilingualism, they are limited in comparing documents because each language require a separate model and distinct linguistic knowledge to be recognized."}, {"title": "2.3. Semantic segmentation", "content": "Semantic segmentation partitions images into distinct regions corresponding to specific objects or sections. This technique has been used in applications such as autonomous driving and medical imaging. Early approaches, such as fully convolutional networks (FCNs) [28] and U-Net [34], pioneered encoder-decoder architectures with skip connections to integrate features across various resolutions. Semantic segmentation also shows strong performance in segmentation tasks, with transformer-based models such as Segformer [45] proposed following the proliferation of transformers. Transformer-based methods, such as BIT-CD [8] and SARAS-Net [7], have recently gained attention as promising approaches for change detection using semantic segmentation, achieving SotA results in this area. However, these change detection technologies are intended to find differences between images at different times at the same location. While change detection technology excels at detecting meaningful changes, it is less effective at correcting positional errors and distortions introduced by document scanning or character detection."}, {"title": "3. Method", "content": "Text change detection (TCD) is utilized to identify altered text regions, such as additions, deletions, or modifications, by comparing text images extracted from two different versions of a document. Our model has a two-way semantic segmentation architecture that analyzes text area image pairs from the source to the target and vice versa. We also use an auxiliary classification CNN head at the end of TCD network to determine if two text unit images are identical or not. Our model thus compares pairs of text unit images from the original and comparison documents at the character level to determine whether they are identical or different. Our model thus compares pairs of text unit images from the original and comparison documents at the character level to detect changes, additions, or deletions. This section outlines the overall framework of our model and its key components. Additionally, our overall document comparison application process is described in Sec. A to facilitate understanding of our proposed model and demonstrate its necessity."}, {"title": "3.1. Overall architecture", "content": "As shown in Fig. 1, the architecture of our proposed model is based on an encoder-decoder structure [3]. Both input images are processed using the encoder module with shared weights in a manner similar to Siamese networks [10, 20], with ResNet [15] used as the backbone. Given the diversity of text unit images, which include complex shapes and small characters such as math symbols or punctuation, multi-level features are extracted from ResNet using a feature pyramid network (FPN) [26] to preserve the fine details. To ensure robust image comparison, our encoder employs cross-self attention mechanisms to enhance feature representation and highlight significant locations in the feature space. After encoding, the features of the source and target images are separately fed into parallel decoders for each image, both of which generate change segmentation maps from the encoded features. By utilizing features from the backbone and enhancing lower-resolution segmentation maps using the attention mechanisms, the decoders ensure the precise localization of changes to the text."}, {"title": "3.2. Backbone network", "content": "Given input images from the source $I_s$ and target $I_t$ of size $(H, W, 3)$, our ResNet backbone with the FPN generates three multi-scale feature pyramid map pairs of sizes $(H/2, W/2,64)$, $(H/4, W/4,64)$, and $(H/8,W/8, 512)$, denoted as $F_1, F_2, F_3$ and $F^1, F^2, F^3$ for the source and target, respectively. Our backbone is based on the ResNet-based network proposed by [9], and we use only up to the third bottleneck layer. The ResNet output is passed through the FPN to improve the feature maps, without changing the shape."}, {"title": "3.3. Positional encoding and multi-scale feature map attention", "content": "Inspired by transformer networks [11], we employ a 2D extension of the standard positional encoding used in transformers similar to DETR [6] and LoFTR [41]. We exclusively append these encodings to the FPN output feature map $F^3$ due to the limited spatial information present in the deeper features.\nDeveloping concepts from SARAS-Net [7] and LOFTR [41], our model incorporates cross-self attention modules, which utilize an attention learning module to enhance lower-level feature representation by focusing on meaningful features. These modules utilize convolutional operations to derive queries, keys, and values, producing attention maps that refine feature representations Eq. (1).\nAttention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\qquad(1)"}, {"title": "3.4. Correlation map and marginalization", "content": "We adopt the concept of hyper-correlation [30] in a few-shot segmentation area to construct a 4D correlation map using the cosine similarity between the multi-scale feature maps. Specifically, the 4D correlation map between the two feature maps $F$ and $F^t$ at pixel positions $(i, j)$ and $(m, n)$, respectively, for $l=2,3$ is constructed via cosine similarity as follows:\nCos(F^s(i, j), F^t(m, n)) = \\text{ReLU}(\\frac{F^s(i, j) \\cdot F^t(m, n)}{||F^s(i, j)||\\cdot ||F^t(m,n)||} ) \\qquad(2)\nIn text matching, we posit that the query points in the source and their corresponding points in the target coincide at analogous spatial locations. This assumption is based on the fact that, when comparing text images, the target image may undergo transformations such as rotation, blurring, or the addition of whitespace or noise. However, the same characters tend to appear in similar positions in both the source and target images. Rather than computing a full, dense 4D correlation map, we confine the matching process to neighboring points surrounding the queried feature point, thus reducing the computational complexity. The neighborhood is defined as $[-K_v, K_v]$ and $[-K_h, K_h]$ in the vertical and horizontal directions, respectively. By calculating the sparse correlation within a predetermined range, we significantly reduce the computation time required compared to the full correlation map. Correlation map marginalization refers to the process of transforming the 4D correlation"}, {"title": "3.5. Correlation map cross-self attention with feature map", "content": "In addition to the feature maps Sec. 3.3, we also apply cross-self attention mechanisms to the marginalized lower-level correlation maps. Prior to applying the attention mechanisms, the marginalized correlation maps $C^2, C^3$ and $C^2_t, C^3_t$ from the two levels are deconvolved and upsampled to a size of $(H/2, W/2, C_c)$ using shared deconvolution"}, {"title": "3.6. Segmentation maps", "content": "The final segmentation maps are generated by integrating correlation feature maps from the attention module within each Decoder. These maps undergo convolutional upsampling and Sigmoid activation, yielding two-way semantic segmentation maps $S_{st}, S_{ts}$ from two parallel decoders, representing changes from $I_s$ to $I_t$ and from $I_t$ to $I_s$."}, {"title": "3.7. Loss function", "content": "Assuming $I_s$, and $I_t$ are the source and target image, respectively, the segmentation loss function combines dice Loss $L_d$ and binary cross-entropy (BCE) loss $L_{bce}$ as follows:\nL_{st}(I_s, I_t) = W_d * L_d(S_{st}, G_{st}) + L_{bce}(S_{st}, G_{st}) \\qquad(3)\nFor each pixel position i, y\u017c and p\u2081 denote the corresponding ground truth and predicted values, respectively. Our model generates two segmentation maps; hence, the total segmentation loss is computed by averaging the losses associated with each map. The overall loss function is subsequently defined as follows:\nL_{seg} = 0.5 * L_{st}(I_s, I_t) + 0.5 * L_{ts}(I_t, I_s) \\qquad(6)"}, {"title": "3.8. Training with synthetic data", "content": "The input to our TCD model comprises pairs of unit text images, where the ground truth contains the changed area information. To train our model, we generate a dataset using a synthetic image generator based on a diverse text corpus containing English, Korean, Chinese, numerical characters, and special symbols. Given that our TCD approach is designed as a two-way segmentation task, our ground truth annotations comprise bidirectional segmentation maps. We simulate the task of comparing contract documents, which often involves comparing machine-readable documents with scanned documents, in our data-generation process. Specifically, we generate the source image using a fixed background, whereas the target image is generated with a random background to mimic the variability in the image quality of scanned documents. Examples of the samples generated by our data generator are presented in Fig. 4. The sample data is generated as a pseudo-segmentation map, where the value of a rectangular region corresponding to the maximum width and height of a changed character is set to 1, and the remaining areas are set to 0. The"}, {"title": "4. Experiments", "content": "For benchmarking, we construct a multilingual word-level (or unit-level) image-pair dataset. Our benchmark results encompass language-specific comparisons and an aggregate analysis of the entire evaluation dataset. Furthermore, we conduct ablation analysis to empirically demonstrate the utility of each module in our proposed framework."}, {"title": "4.1. Datasets", "content": "There is a lack of public datasets specifically designed for TCD using image comparison. To address this gap, we create a novel dataset tailored to change detection for text images.\nTraining and validation datasets: We construct a corpus dataset comprising 10,000 words, encompassing English, Korean, Chinese, numbers, and special characters. The training data source images are generated from a text corpus, with modified characters randomly selected from other corpus texts. Each dataset instance consists of a pair of source images and corresponding target change map images. To ensure balanced data, identical and different image pairs are generated in equal proportions within each batch. Furthermore, an additional 5,000 text corpus data points are utilized to create a validation dataset.\nTest datasets: We construct two different datasets to serve as benchmarks for evaluating semantic segmentation, and OCR methods. Our segmentation test dataset is created by combining actually printed and scanned word pairs extracted from documents written in four languages: English, Korean, Russian, and Chinese. Using the DUET unit detector [19], we crop the units at the character level. Subsequently, these cropped units are randomly merged to form concatenated text images, and the synthetic ground truth for segmentation changes is generated in a similar manner to the training dataset. Using the position in the full image and text ground truth, we produce identical and different pairs. The resulting dataset consists of 80,000 pairs, with"}, {"title": "4.2. Training configuration", "content": "In our experiment, we fix the height of the training data to 32. For correlation marginalization, we set the neighboring values $K_v, K_h$ to 2 and 4 for the H/4 features, and 1 and 2 for the H/8 features, respectively. We also scale the segmentation loss by a scale factor $W_d$ of 10 during training. Our model is trained from scratch for 200 epochs with a batch size of 8."}, {"title": "4.3. Evaluation", "content": "To use the segmentation test dataset as a benchmark, we adopt a range of performance metrics commonly used in semantic segmentation and change detection models, including precision, recall, the F1 score, IoU, and overall accuracy. Notably, our evaluation dataset consists of both identical- and different-pair data because these metrics assess performance at the pixel level across the entire dataset. To evaluate the OCR test data, we employ binary classification performance scores for both the identical and different pairs. Specifically, we calculate precision, recall, the F1 score, and accuracy for all OCR benchmarking models using a classification confusion matrix."}, {"title": "4.4. Benchmark results", "content": "To evaluate the performance capabilities of our model, we select several well-known and SotA semantic segmentation and change detection models, including U-Net [34],"}, {"title": "4.5. Ablation analysis", "content": "We conduct an ablation study on various modules of our TCD model using our segmentation benchmark dataset. The modules analyzed include correlation and marginalization (CM), encoder feature map attention (FA), decoder correlation map attention (CA), and one-way (OW) versus two-way (TW) segmentation maps. Additionally, we compare the performance of a baseline model created by removing all specified modules and replacing the CM module with convolutional layers. This allows us to evaluate the impact of each module by progressively adding or removing them from the model. All models are trained for 200 epochs each and then evaluated on the segmentation dataset.\nCorrelation and marginalization (CM): To investigate the effect of the CM module, we first train TCD v1 by replacing the CM module with basic Conv layers. As shown"}, {"title": "5. Conclusion", "content": "In this paper, we introduce a TCD model that enables the comparison of image documents regardless of the language. Our model offers language independence by focusing on image comparison rather than text recognition. The TCD architecture incorporates multi-scale features and correlation marginalized maps to detect text image changes at a granular unit level within documents. Notably, our model operates robustly with various alterations to the text unit images without the need for preprocessing steps such as text or scale alignment. Moreover, we incorporate a correlation map integrated with feature map cross-self transformer-based attention mechanisms to enhance the change segmentation accuracy. The experimental results illustrate the robustness and generalizability of our model across multilingual documents, including those not represented in the training corpus. Benchmarking demonstrates that our approach consistently outperforms SotA semantic segmentation models by a significant margin. Moreover, our model achieves comparable performance to OCR methods tailored for individual languages."}, {"title": "Appendix A. Document change detection application whole process using TCD", "content": "In the supplementary material, we describe how our proposed method is applied to justify the necessity of our proposed model. We develop a contract document change detection (DCD) application using our proposed model, which is currently being used by the legal review team. Our application detects image pairs for comparison through several steps. The overall DCD process is as follows:"}, {"title": "A.1. Preprocess", "content": "Typically, we receive the original contract document and its scanned version as inputs, where the scanned version may be rotated or skewed upon entry as illustrated in Fig. 6 (a). In addition, the aspect ratio may vary depending on the device settings. To address this issue, we adjusted the scanned version to align with the orientation and scale of the original document as depicted in Fig. 6 (b)."}, {"title": "A.2. Layout detection", "content": "Contract documents often involve bilingual content, typically presented in a two-column layout. Since the documents contain different languages and layouts, we separate the layout to facilitate comparison. In our application, we classify the layout into distinct elements such as text, tables, headers, and footers, allowing for precise analysis. Furthermore, text and table layouts are divided into left, right, and center based on their position. The layout detection results for the original document and its scanned version are shown respectively in Fig. 7 as (a) and (b)."}, {"title": "A.3. Unit detection", "content": "We identify units within each layout element after detecting the layout. These units are typically detected at the word level, although variations may occur due to factors such as spacing. The result of this process is shown in Fig. 8, where (a) and (6) illustrate the results of table and text layout, respectively. Subsequently, we compare these detected units in pairs, and units that are differently detected in the original and scanned versions undergo a split-and-merge process for accurate comparison. Ultimately, this enables us to detect the final changed regions. The matching process only takes place between units belonging to the same layout element."}, {"title": "A.4. Two-way segmentation", "content": "Our proposed method generates segmentation maps in both directions, e.g., from the source to target and vice versa. Since the final decision is made through binary classification, we utilize the maximum values of these two segmentation maps. By analyzing the segmentation maps in both directions, we can identify areas where text has been inserted or deleted. Figure 9 illustrates the results of the proposed method using the test data samples in 4.1. Specifically, (a) and (b) represent the source (original) and target (scanned) pairs, respectively, while (c) and (d) denote the changes from the source to target segmentation map and vice versa. Furthermore, (e) displays the maximum value segmentation map of (c) and (d). In the third row, for"}, {"title": "Appendix B. Supplementary experimental results", "content": "In Sec.4, we only conduct experiments with text image data, because its objectives diverge from those of traditional change detection approaches. However, we also conduct additional experiments, and the results are included in the supplementary materials due to page length constraints in the paper."}, {"title": "B.1. Segmentation results for Hindi and Arabic", "content": "To compare the performance of our model with SotA change detection models on text images, we generate document images in multiple languages through printing and scanning, and then create word-level image pairs using a unit detector. However, the unique linguistic properties of Arabic and Hindi render it exceedingly challenging to produce image pairs using the unit detector. Due to this limitation, we are unable to create segmentation benchmark datasets for these languages. Instead of quantitative evaluation, we utilize OCR test datasets and conduct experiments to qualitatively evaluate the segmentation results for these languages. The results are shown in the Fig. 10.\nWe conduct experiments comparing the performance of our proposed approach against SotA segmentation models, including BIT-CD [8] and SARAS-net [7]. Notably, our proposed model produces a two-way segmentation map, yielding both source to target and vice versa. To facilitate a comprehensive evaluation, we display the maximum values of these two maps in Fig. 10. The BIT-CD model exhibits sensitivity to even slight variations in character ratio and position, often misclassifying such cases as different. This limitation is evident in the results shown in Fig. 10 rows 1, 6, and 8. In contrast, SARAS-net demonstrates a converse behavior, struggling to identify discrepancies in different pairs, as observed in Fig. 10 rows 4, 5, and 9. Notably, our proposed model demonstrates robustness to character ratio variations and minor truncations, accurately identifying modified regions. As evidenced by Fig. 10, only our model successfully detects changes in rows 2 and 7. Moreover, across most cases, our model excels at identifying modified regions through its two-way analysis, providing the most comprehensive detection of modified regions from both source and target image perspectives."}, {"title": "B.2. Segmentation results for conventional change detection dataset", "content": "Our proposed model also uses change detection methods. Since the main target of change detection is in the field of remote sensing, we also evaluate its performance on the publicly available remote sensing benchmark dataset. We conduct comparative experiments using the LEVIR-CD dataset, a public remote sensing change detection dataset. We train BIT-CD, SARAS-Net, and our TCD model on the LEVIR-CD train set from scratch for 200 epochs. The test results are presented in the Tab. 5 and Fig. 11. Experimental results show that BIT-CD and SARAS-Net produce fewer false positives, but they fail to detect many actual changes. In contrast, although this results in relatively more false positives, our model detected most of the changes and demonstrated the highest performance in F1 score, IoU, and OA. This is because our primary goal is to detect infrequently changed content within a large volume of text in the contract, leading us to prioritize improving recall, even at the expense of increased false positives."}]}