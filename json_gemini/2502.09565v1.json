{"title": "MDCROW: AUTOMATING MOLECULAR DYNAMICS WORKFLOWS WITH LARGE LANGUAGE MODELS", "authors": ["Quintina Campbell", "Sam Cox", "Jorge Medina", "Brittany Watterson", "Andrew D. White"], "abstract": "Molecular dynamics (MD) simulations are essential for understanding biomolecular systems but remain challenging to automate. Recent advances in large language models (LLM) have demonstrated success in automating complex scientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an agentic LLM assistant capable of automating MD workflows. MDCrow uses chain-of-thought over 40 expert-designed tools for handling and processing files, setting up simulations, analyzing the simulation outputs, and retrieving relevant information from literature and databases. We assess MDCrow's performance across 25 tasks of varying required subtasks and difficulty, and we evaluate the agent's robustness to both difficulty and prompt style. gpt-40 is able to complete complex tasks with low variance, followed closely by llama3-405b, a compelling open-source model. While prompt style does not influence the best models' performance, it has significant effects on smaller models.", "sections": [{"title": "Introduction", "content": "Molecular dynamics (MD) simulations is a common method to understand dynamic and complex systems in chemistry and biology. While MD is now routine, its integration into and impact on scientific workflows has increased dramatically over the past few decades [1-3]. There are two main reasons for this: First, MD provides valuable insights. Through simulations, scientists can study structural and dynamic phenomena, perturbations, and dynamic processes in their chemical systems. Second, innovations in hardware and expert-designed software packages have made MD much more accessible to both experienced and novice users [3].\nFor a given protein simulation, parameter selection is nontrivial: the user must provide the input structure (such as a PDB [4] file), select a force field (e.g., CHARMM [5], AMBER [6]), and specify parameters such as temperature, integrator, simulation length, and equilibration protocols. Simulations also generally require pre- and post-processing steps, along with various analyses. For instance, a user may need to clean or trim a PDB file, add a solvent, or analyze the protein's structure. After simulation, they might examine the protein's shape throughout the simulation or assess its stability under different conditions. The choices for pre-processing, analysis, and simulation parameters are highly specific to any given use case and often require expert intuition. Thus, automating this process is difficult but beneficial.\nSeveral efforts have been made to automate MD workflows [7\u201317], focusing largely on specific domains, such as RadonPy for polymer's simulations [8], or PyAutoFEP for proteins and small molecules for drug-screening [16]. Other approaches are constrained to a particular combination of simulation software and simulation (e.g. GROMACS and Free Energy Perturbation). Certainly, there has been significant community-driven improvement in automating and creating MD toolkits [14, 18-24] and user-friendly interfaces and visualizations [25-32]. While these advances improve the capabilities and ease of use in many cases, the inherent variability of MD workflows still poses a great challenge for full automation.\nLarge-Language Model (LLM) agents [33-36] have gained popularity for their ability to automate technical tasks through reasoning and tool usage, even surpassing domain-specialized LLMs (e.g., BioGPT [37], Med-PaLM [38]) when programmed for specialized roles [39]. These agents have demonstrated promising results in scientific tasks within a predefined toolspace, with tools like ChemCrow and Coscientist successfully automating complex workflows and novel design in chemical synthesis [40\u201342]. Likewise, LLM-driven automation has been explored in materials research [43-46], literature and data aggregation [47, 48], and more sophisticated tasks [45, 49\u201355]. Most similar to this work, ProtAgents [55] is a multi-agent modeling framework tackling protein-related design and analysis, and LLaMP [45] applies a retrieval-augmented generation (RAG)-based ReAct agent to simulate inorganic materials by interfacing with literature databases, Wikipedia, and atomistic simulation tools. Although preliminary work has applied agentic LLMs to MD via a RAG-based agent [45], no fully adaptive and autonomous system exists for biochemical MD or protein simulations. See Ramos et al.[56] for a recent review on the design, assessment, and applications of scientific agents.\nHere we present MDCrow, an LLM-agent capable of autonomously completing MD workflows. Our main contributions to the field are (1) we assess MDCrow's performance across 25 tasks with varying difficulty and compare performance of different LLM models; (2) we measure robustness how agents are prompted and task complexity based on required number of subtasks we compare with simply equipping an LLM with a python interpreter with the required packages installed, rather than using a custom built environment. Our main conclusions is that MDCrow with gpt-40 or llama3-405b is able to perform nearly all of our assessed tasks and is relatively insensitive to how precise the instructions are given to it. See Figure 1D for an overview of the main results."}, {"title": "Methods", "content": "2.1 MDCrow Toolset\nMDCrow is an LLM agent, which consists of an environment of tools that emit observations and an LLM that selects actions (tools + inpnut arguments). MDCrow is built with Langchain [57] and a ReAct style prompt. [35]. The tools mostly consist of analysis and simulation methods; we use OpenMM [22] and MDTraj [21] packages, but in principle our findings generalize to any such packages.\nMDCrow's tools can be categorized in four groups: Information Retrieval, PDB & Protein, Simulation, and Analysis (see Figure 1B).\nInformation Retrieval Tools These tools enable MDCrow to build context and answer simple questions posed by the user. Most of the tools serve as wrappers for UniProt API functionalities [58], allowing access to data such as 3D structures, binding sites, and kinetic properties of proteins. Additionally, we include a LiteratureSearch tool, which uses PaperQA [48] to answer questions and retrieve information from literature. PaperQA accesses a local database of relevant PDFs, selected specifically for the test prompts, which can be found in SI section C. This real-time information helps the system provide direct answers to user questions and can also assist the agent in selecting parameters or guiding simulation processes.\nPDB & Protein Tools MDCrow uses these tools to interact directly with PDB files, performing tasks such as cleaning structures with PDBFixer [22], retrieving PDBs for small molecules and proteins, and visualizing PDBs through Molrender [59] or NGLview [60].\nSimulation Tools All included simulation tools use OpenMM [22] for simulation and PackMol [19] for solvent addition. These tools are built to manage dynamic simulation parameters, handle errors related to inadequate parameters or incomplete preprocessing, and address missing forcefield templates efficiently. The agent responds to simulation setup errors through informative error messages, improving overall robustness. Finally, the simulation tools outputs Python scripts that can be modified directly by MDCrow whenever the simulation requires additional steps or parameters.\nAnalysis Tools This group of tools is the largest in the toolset, designed to cover common MD workflow analysis methods, many of which are built on MDTraj [21] functionalities. Examples include computing the root mean squared distance (RMSD) with respect to a reference structure, the radius of gyration, analyzing the secondary structure, and various plotting functions."}, {"title": "Chatting with Simulations", "content": "A key challenge in developing an automated MD assistant is ensuring it can manage a large number of files, analyses, and long simulations and runtimes. Although MDCrow has been primarily tested with shorter simulations, it is designed to handle larger workflows as well. Its ability to retrieve and resume previous runs allows users to start a simulation, step away during the long process, and later continue interactions and analyses without needing to stay engaged the entire time. An example of this chatting feature is shown in Figure 2.\nMDCrow creates an LLM-generated summary of the user prompt and agent trace, which is assigned to a unique run identifier provided at the end of the run (but accessible at any time during the session). Each run's files, figures, and path registry are saved in a unique checkpoint folder linked to the run identifier.\nWhen resuming a chat, the LLM loads the summarized context of previous steps and maintains access to the same file corpus, as long as the created files remain intact. To resume a run, the user simply provides the checkpoint directory and run identifier. MDCrow then loads the corresponding memory summaries and retrieves all associated files, enabling seamless continuation of analyses."}, {"title": "Results", "content": "3.1 MDCrow Performance on Various Tasks\nTo assess MDCrow's ability to complete tasks of varying difficulty, we designed 25 prompts with different levels of complexity and documented the number of subtasks (minimum required steps) needed to complete each task. MDCrow was not penalized for taking additional steps, but was penalized for omitting necessary ones. For example, the first prompt in Figure 1C contains a single subtask, whereas the complex task requires 10 subtasks: downloading the PDB file, performing three simulations, and performing two analyses per simulation. If the agent failed to complete an earlier step, it was penalized for every subsequent step it could not perform due to that failure.\nThe 25 prompts require between 1 and 10 subtasks, with their distribution shown in Figure 3B. Each prompt was tested across three GPT models (gpt-3.5-turbo-0125, gpt-4-turbo-2024-04-09, gpt-40-2024-08-06) [61, 62], two Llama models (1lama-v3p1-405b-instruct, llama-v3p1-70b-instruct) [63] (accessed via the Fireworks AI API with 8-bit floating point (8FP) quantization [64]), and two Claude models (claude-3-opus-20240229, claude-3-5-sonnet-20240620) [65, 66]. A newer Claude Sonnet model, claude-3-5-sonnet-20241022 was tested in later experiments but was not found to give superior results, so it was not tested on these 25 prompts. All other parameters were held constant across tests, and each version of MDCrow executed a single run per prompt.\nEach run was evaluated by experts recording the number of required subtasks the agent completed and using Boolean indicators to indicate accuracy, whether the agent triggered a runtime error, and whether the trajectory contained any hallucinations. Since the agent trajectories for each run are inherently variable, accuracy is defined as the result's consistency with the expected trajectory rather than comparing against a fixed reference.\nThe percentage of tasks that were deemed to have valid solutions for MDCrow with each base-LLM is shown in Figure 3A. The lowest performing model was gpt-3.5. This is not surprising, as this model had some of the highest hallucination rates (32% of prompt completions contained hallucinations), compared to the absence of documented hallucinations in the higher performing models, gpt-40 and 1lama3-405b. However, the discrepancy in accuracy rates between models cannot solely be attributed to hallucinations, as gpt-3.5 attempted fewer than half of the required subtasks, whereas the higher-performing models, gpt-40 and 1lama3-405b, attempted 80-90% of the required subtask, earning accuracy in answering for 72% and 68% of tasks, respectively (Figures 3C, D).\nThese results indicate that MDCrow can handle complex MD tasks but is limited by the capabilities of the base model. For gpt-4-turbo, gpt-3.5, and llama3-70b, the number of trajectories with verified results decreases significantly as task complexity increases (Figure 3C). In contrast, gpt-40 and llama3-405b show only a slight decline, demonstrating that MDCrow performs well even for complex tasks when paired with more robust base models."}, {"title": "MDCrow Robustness", "content": "We evaluated the robustness of MDCrow on complex prompts and different prompt styles. We hypothesized that some models would excel at completing complex tasks, while others would struggle-either forgetting steps or hallucinating-as the number of required subtasks increased. To test this, we created a sequence of 10 prompts that increased in complexity. The first prompt required a single subtask, and each subsequent prompt added an additional subtask (see Figure 4A). Each prompt was tested twice: once in a natural, conversational style and once with explicitly ordered steps. Example prompts can be seen in Figure 4B.\nTo quantify robustness, we calculated the coefficient of variation (CV) for the percentage of completed subtasks across tasks. A lower CV indicates greater consistency in task completion and, therefore, higher robustness. The analysis revealed clear differences in robustness across models and prompt types. Overall, gpt-40 and 1lama3-405b demonstrated moderate to high robustness, while the Claude models showed significantly lower robustness. The performance comparison is shown in Figure 4C.\nWe expected that the percentage of subtasks completed by each model would decrease as task complexity increased. However, with gpt-40 and 1lama3-405b as base models, MDCrow demonstrated a strong relationship between the number of required and completed subtasks (Figure 4D) for both prompt types, indicating consistent performance regardless of task complexity or prompt style. The three included Claude models demonstrated less impressive performance. claude-3-opus followed the linear trend very loosely, becoming more erratic as task complexity increased. As the tasks required more subtasks, the model consistently misses nuances in the instructions and make logical errors. Both claude-3.5-sonnet models gave poor performance on these tasks, often producing the same error (see SI section A)."}, {"title": "MDCrow Comparison", "content": "We also compared MDCrow to two baselines: a ReAct [35] agent with only a Python REPL tool and a single-query LLM. MDCrow and the baselines were tested on the same 25 prompts as previously mentioned, all using gpt-40. We use different system prompts to accommodate each framework, guiding the LLM to utilize common packages with MDCrow, and these prompts can be found in SI section B.\nThe single-query LLM is asked to complete the prompt by writing the code for all subtasks, not unlike what standalone ChatGPT would be asked to do. We then execute the code ourselves and evaluate the outcomes accordingly. ReAct with Python REPL can write and execute codes using a chain-of-thought framework. We find that MDCrow outperforms the two baselines significantly, as shown in Figure 5A, on attempting all subtasks and achieving an accurate solution. Not surprisingly, the two baseline methods struggled with code syntax errors and incorrect handling of PDB files. There is not a significant difference between the two baselines, indicating that the ReAct framework did not significantly boost the model's robustness.\nIn Figure 5B, we observe that the performance of all three methods generally declines as task complexity increases. However, both baseline methods drop to zero after just three steps, with performance then fluctuating erratically at higher complexities. This is not surprising, as proper file processing and simulation setup are crucial for optimal LLM performance in MD tasks. In contrast, MDCrow demonstrates greater robustness and reliability in handling complex tasks, thanks to its well-designed system for accurate file processing and simulation setup, as well as its ability to dynamically adjust to errors."}, {"title": "MDCrow Extrapolation through Chatting", "content": "We further show MDCrow's ability to harness its chatting feature and extrapolate outside of its toolset to complete new tasks. This task requires MDCrow to perform an annealing simulation, which is not part of the current toolset. The agent achieves this by first setting up a simulation to find appropriate system parameters and handle possible early errors. Then, the agent modifies the script according to the user's request. Once the simulation is complete, the user later asks for simulation analyses, shown in Figures 6A, B.\nThis shows that MDCrow has the ability to generalize outside of its toolset and is capable of completing more complicated and/or user-specific simulations. By utilizing the chatting feature, users can walk MDCrow through new analyses, reducing the risk of catastrophic mistakes."}, {"title": "Discussion", "content": "Although LLMs' scientific abilities are growing [67\u201369], they cannot yet independently complete MD workflows, even with a ReAct framework and Python interpreter. However, with frontier LLMs, chain-of-thought, and an expert-curated toolset, MDCrow successfully handles a broad range of tasks. It performs 80% better than gpt-40 in ReAct workflows at completing subtasks, which is expected due to MD workflows' need for file handling, error management, and real-time data retrieval.\nIn some cases, particularly for complex tasks beyond its explicit toolset, MDCrow's performance may improve with human guidance. The system's chatting feature allows users to continue previous conversations, clarify misunderstandings, and guide MDCrow step-by-step through difficult tasks. This adaptability helps MDCrow recover from failures, refine its approach based on user intent, and handle more complex workflows. This suggests that, with more advanced LLM models, targeted feedback, and the addition of specialized tools, MDCrow could tackle an even broader range of tasks. We did not do a full evaluation of MDCrow's capabilities through this chatting feature in this work.\nFor all LLMs, task accuracy and subtask completion are affected by task complexity. Interestingly, while gpt-40 can handle multiple steps with low variance, llama3-405b is a compelling second best, as an open-source model. Other models, such as gpt-3.5 and claude-3.5-sonnet, struggle with hallucinations or inability to follow multistep instructions. Performance on these models, however, is improved with explicit prompting or model-specific optimization (especially for claude-3.5-sonnet)."}, {"title": "Conclusion", "content": "Running and analyzing MD simulations is non-trivial and typically hard to automate. Here, we explored using LLM agents to accomplish this. We built MDCrow, an LLM and environment consisting of over 40 tools purpose built for MD simulation and analysis. We found MDCrow could complete 72% of the tasks with the optimal settings (gpt-40). llama-405B was able to complete 68%, providing a compelling open-source model. The best models were relatively robust to how the instructions are given, although weaker models struggle with unstructured instructions. Simply using an LLM with a python interpreter and required packages installed had a 28% accuracy. The performance of MDCrow was relatively stable as well, though dependent on the model. Correct assessment of these complex scientific workflows is challenging, and had to be done by hand. Chatting with the simulations, via extended conversations, is even more compelling, but is harder to assess.\nThis work demonstrates the steps to automate and assess computational scientific workflows. As LLMs continue improving in performance, and better training methods arise for complex tasks like this, we expect LLM agents to be increasingly important for accelerating science. MDCrow, for example, can now automatically assess hypotheses with 72% accuracy with simulation and can scale-out to thousands of simultaneous tasks. The code and tasks are open source and available at https://github.com/ur-whitelab/MDCrow."}, {"title": "Supplemental Information", "content": "A Claude-Specific Engineering\nWhile both of Claude's Sonnet models achieved poor performance during the robustness experiment, it can be noted that a single common error arose consistently. When running an NPT simulation, MDCrow requires that all parameters be passed to the simulation tool. However, both Sonnet models consistently neglected to provide a value for pressure, even when directly prompted to do so. The claude-3-opus made this mistake a single time. This is a relatively simple fix, providing MDCrow with a default pressure of 1 atm when no pressure is passed."}, {"title": "Prompts", "content": "MDCrow Prompt\nYou are an expert molecular dynamics scientist, and your task is to respond to the question or solve the problem to the best of your ability using the provided tools.\nYou can only respond with a single complete 'Thought, Action, Action Input' format OR a single 'Final Answer' format.\nComplete format:\nThought: (reflect on your progress and decide what to do next)\nAction:\n\\`\\`\\`\n{\n\"action\": (the action name, it should be the name of a tool),\n\"action_input\": (the input string for the action)\n}\n\\`\\`\\`\nOR\nFinal Answer: (the final response to the original input\nquestion, once all steps are complete)\nYou are required to use the tools provided, using the most specific tool available for each action. Your final answer should contain all information necessary to answer the question and its subquestions. Before you finish, reflect on your progress and make sure you have addressed the question in its entirety.\nIf you are asked to continue or reference previous runs, the context will be provided to you. If context is provided, you should assume you are continuing a chat.\nHere is the input:\nPrevious Context: {context}\nQuestion: {input}\nDuring the comparison study between MDCrow, GPT-only, and ReAct with Python REPL tool, we used different system prompts for each of these LLM frameworks.\nDirect-LLM Prompt\nYou are an expert molecular dynamics scientist, and your task is to respond to the question or solve the problem in its entirety to the best of your ability. If any part of the task requires you to perform an action that you are not capable of completing, please write a runnable Python script for that step and move on. For literature papers, use and process papers from the 'paper_collection' folder. For .pdb files, download them from the RSCB website using 'requests'. To preprocess PDB files, you will use PDBFixer. To get information about proteins, retrieve data from the UniProt database. For anything related to simulations, you will use OpenMM, and for anything related to analyses, you will use MDTraj. At the end, combine any scripts into one script.\nReAct Agent Prompt\nYou are an expert molecular dynamics scientist, and your task is to respond to the question or solve the problem to the best of your ability. If any part of the task requires you to perform an action that you are not capable of completing, please write a runnable Python script for that step and run it. For literature papers, use and process papers from the 'paper_collection' folder. For .pdb files, download them from the RSCB website using 'requests'. TO preprocess PDB files, you will use PDBFixer. To get information about proteins, retrieve data from the UniProt database. For anything related to simulations, you will use OpenMM, and for anything related to analyzes, you will use MDTraj.\nYou can only respond with a single complete 'Thought, Action, Action Input' format OR a single 'Final Answer' format.\nComplete format:\nThought: (reflect on your progress and decide what to do next)\nAction:\n\\`\\`\\`\n{\n\"action\": (the action name, it should be the name of a tool),\n\"action_input\": (the input string for the action)\n}\n\\`\\`\\`\nOR\nFinal Answer: (the final response to the original input\nquestion, once all steps are complete)\nYou are required to use the tools provided,\nusing the most specific tool available for each action. Your final answer should contain all information necessary to answer the question and its subquestions. Before you finish, reflect on your progress and make sure you have addressed the question in its entirety.\nHere is the input:\nQuestion: {input}"}, {"title": "Task Prompts & References Used in Experiments", "content": ""}]}