{"title": "COMPARATIVE ANALYSIS OF LLAMA AND CHATGPT EMBEDDINGS FOR MOLECULE EMBEDDING", "authors": ["Shaghayegh Sadeghi", "Alan Bui", "Ali Forooghi", "Jianguo Lu", "Alioune Ngom"], "abstract": "Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.\nMethods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.\nResults: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.\nConclusion: The application of LLMs in cheminformatics, particularly in utilizing SMILES embeddings, shows significant promise for advancing drug development. This includes improving the prediction of chemical properties and facilitating the drug discovery process. GitHub: https://github.com/sshaghayeghs/LLaMA-VS-ChatGPT", "sections": [{"title": "1 Introduction", "content": "Molecule embedding is the corner stone in pharmacology [1, 2], and finds wide applications in molecular property (MP) prediction [3, 4, 5, 6], drug-target interaction (DTI) prediction [7, 8, 9] and drug-drug interaction (DDI) prediction [10, 11]. Molecule embedding techniques evolve and synchronize with the advances in language modelling [12, 13], starting with static word embedding [14], to contextualized pre-trained models [15, 16]. With recent breakthroughs made by LLMs spear-headed by openAI, ChatGPT has been explored for molecule embedding [17].\nLLMs such as ChatGPT [18] and LLaMA [19, 20] are gaining prominence due to their remarkable capabilities across various tasks. Due to their success in analyzing natural language, backed by pre-training on extensive unsupervised data, applying this approach to the chemistry language domain was a logical step. Recent trends and research efforts are"}, {"title": "2 Related Work: SMILES embedding", "content": "Up to our current knowledge, the application of ChatGPT and LLaMA in chemistry has primarily been limited to utilizing and evaluating its performance in answering queries. Further exploration and implementation of LLMs for more advanced tasks within chemistry are yet to be thoroughly documented. For example, to examine how well LLMs understand chemistry, Guo et. al. [17] used LLMs to assess the performance of these models on practical chemistry tasks only using queries.\nHowever, inspired by many language-based methods that tried to extract molecular embedding, our study represents a pioneering effort, being the first to rigorously assess the capabilities of LLMs like ChatGPT and LLaMA in the usage of LLMs embedding for chemistry tasks.\nFew example, Seq2seq [27] works like an auto-encoder, maps the SMILES string to a fixed-sized vector using an encoder, and then translates it back to the original SMILES string with a decoder. The intermediate fixed-sized vector is extracted as the Seq2seq embedding. Once the model is well-trained, the intermediate feature vector is considered to encode all the information to recover the original molecular representation.\nIn the DTI prediction task, Affinity2Vec [8] uses Seq2seq to generate features for each drug. Drug and protein features are then used to predict the binding affinity between the drug and the target.\nDuring the embedding extraction stage, Seq2seq only feeds forward the encoder network, leaving the decoder network behind to save computational resources. Hence, SMILES2vec [28] proposed a deep RNN-based method that only contains the encoder for extracting the SMILES embedding. They used a Bayesian optimizer to optimize the hyper-parameters related to the neural network topology.\nSmileGNN, as described in [11], aggregates the structural features of drugs constructed from SMILES data and the topological features of drugs in knowledge graphs using GNNs. To extract drug features, SmileGNN utilizes SMILES2vec [28].\nSince SMILES is a fundamentally different from English language, commonly used techniques in NLP research, such as Word2vec embedding, cannot be easily adapted for this task.\nHowever, while Seq2seq fingerprint and SMILES2Vec use character-wise tokenization, Mol2Vec [14] designed a study that employed a word2vec model on sentences formed from Morgan fingerprint [29].\nOne early molecular embedding method is the Morgan fingerprint. The Morgan fingerprint is a method of encoding the structural information of a molecule into a fixed-length binary or integer vector. The Morgan fingerprint works by iteratively generating circular substructures around each atom in the molecule up to a specified radius and converting the information to a numerical representation using a hash function.\nDue to the non-invertible nature of the hash function, Morgan fingerprints usually do not encode enough information, resulting in lower performance in the downstream prediction tasks [27, 30]."}, {"title": "3 The ChatGPT vs LLaMA Benchmark", "content": "LLMs, exemplified by architectures like BERT [12], ChatGPT [18], LLaMA [19], and LLaMA2 [20] excel at under-standing context within sentences and generating coherent text. They leverage attention mechanisms and vast training data to capture contextual information, making them versatile for text generation, translation, and sentiment analysis tasks. While Word2Vec enhances word-level semantics, language models provide a deeper understanding of context and facilitate more comprehensive language understanding and generation. Pre-trained models from LLMs can transform text into dense, high-dimensional vectors, which capture contextual information and meaning. Using pre-trained LLMs offers an edge as they transfer knowledge from their vast training data, enabling the extraction of context-sensitive representations without requiring extensive task-specific data or feature engineering [34].\nHere, this work focuses on obtaining the embeddings of SMILES strings from ChatGPT and LLaMA models to find the model that achieves the best performance.\nFor our research, we utilized the text-embedding-ada-002 model, developed by OpenAI [35] with 1.7 trillion parameters, for text embedding tasks. This model, acclaimed for being the best among available embedding models, employs the cl100k-base token calculator to generate embeddings, resulting in a 1536-dimensional vector representation. We input SMILES strings into this model, allowing ChatGPT (Generative Pre-trained Transformer) to create embeddings for each string. These embeddings serve as the feature vector for our classification tasks.\nIn parallel, we leveraged the capabilities of LLaMA (Large Language Model Meta AI) [19] and its advanced variant, LLaMA2 [20], both released by Meta AI. These models, ranging from only 7 to 65 billion parameters, are built on the Transformers architecture. LLaMA2, an enhancement of LLaMA, benefits from training on an expanded publicly available data set. Its pretraining corpus grew by 40%, and its context length doubled to 4096 tokens, aligning well with the size of our SMILES sequence embeddings. Both LLaMa models employ a decoder-only Transformer architecture with causal multi-headed attention in each layer. Drawing architectural inspiration from prominent language models like GPT-3 and PaLM (Pathways Language Model) [36], they incorporate features such as pre-normalization, RMSNorm, SwiGLU activation functions, and rotary positional embeddings (RoPE) [37] in every transformer layer.\nThe training dataset of LLaMA [19, 38] is predominantly composed of webpages, accounting for over 80% of its content. This is supplemented by a diverse range of sources, including 6.5% code-centric data from GitHub and StackExchange, 4.5% literary content from books, and 2.5% scientific material primarily sourced from arXiv."}, {"title": "3.1 Tasks and Evaluation", "content": "High-precision molecular property prediction models are now vital components at multiple stages of the drug discovery pipeline. In this study, we only examine binary classification tasks. In the context of molecular property prediction, binary classification refers to a computational method used to predict whether a given molecule possesses a specific property or not [40]."}, {"title": "3.1.1 Molecular Property Prediction", "content": "High-precision molecular property prediction models are now vital components at multiple stages of the drug discovery pipeline. In this study, we only examine binary classification tasks. In the context of molecular property prediction, binary classification refers to a computational method used to predict whether a given molecule possesses a specific property or not [40]."}, {"title": "3.1.2 Drug-Drug Interaction Prediction", "content": "DDI prediction tasks can be cast as Link Prediction (LP) task [41, 42, 43]. Link prediction is one of the most important research topics in network science. The objective of link prediction in this research is to identify whether there is an edge between two drug nodes."}, {"title": "3.2 Datasets", "content": "Following common practices, we evaluate embedding quality on classification tasks [4, 5, 17, 44, 3, 6], and link-prediction tasks [45, 46, 11, 47]."}, {"title": "3.2.1 MoleculeNet", "content": "we evaluate on four labeled datasets tabulated in Table 1. MoleculeNet [40] is a widely used benchmark dataset in the field of computational chemistry and drug discovery. MoleculeNet is designed to evaluate and compare the performance of various machine learning models and algorithms on tasks related to molecular property prediction, compound screening, and other cheminformatics tasks.\nMoleculeNet provides a collection of diverse datasets that cover a range of tasks, including Classification Tasks such as identifying properties like toxicity, bioactivity, and whether a molecule is an inhibitor.\nWe use these four datasets from MoleculeNet because many of the related studies mentioned employed MoleculeNet datasets as a foundational benchmark for evaluating their proposed methods [6, 48, 5, 4, 3, 44, 17].\nHere, we also use a set of benchmark datasets (Table. 1) specially designed for the classification task.\nExtensive collections of SMILES strings, accompanied by binary labels indicating specific properties being assessed, such as BBBP [49](Blood-Brain Barrier Penetration), HIV [50] replication inhibition, BACE binding results for inhibitors of human beta-secretase [51], and ClinTox [52] drugs that have failed clinical trials for toxicity reasons constitute the datasets.\nFor evaluation, first, we partition our dataset into training and testing sets using a ratio of 80:10:10, which is more commonly used and the Deepchem [53] recommended split."}, {"title": "3.2.2 DDI Network", "content": "In this study, we utilize two DDI networks: BioSnap [54] and DrugBank [55]. These datasets represent interactions among FDA-approved drugs as a biological network, with drugs as nodes and interactions as edges.\nFrom the DrugBank Database, we extracted the SMILES strings of drugs in the DrugBank database. It should be noted that we conduct data removal because of some improper drug SMILES strings in Drugbank, which can not be converted into molecular graphs, as determined by the RDKit library. The errors include so-old storage format of SMILES strings,"}, {"title": "4 Results", "content": ""}, {"title": "4.1 Experimental Setup", "content": "For obtaining the embeddings from ChatGPT we utilized the text-embedding-ada-002 model. For LLaMA models, we extract the embedding of each sequence from the output of the last layer of the model of 7 billion parameter version of both LLaMA and LLaMA2, which have size of 4096."}, {"title": "4.1.1 Molecular Property Prediction", "content": "To compare ChatGPT and LLaMA that are not pre-trained on SMILES, we use the reported results of MolBERT for comparison.\nWe perform property prediction tasks on the selected MoleculeNet datasets; since the tasks for these datasets are classification, we use four different classification methods and report the average of the results of these classifiers. These models are Logistic Regression, SVM, Random Forest, and Gradient Boosting.\nF1-score, AUROC, AUPRC are used as the evaluation metrics for MP prediction task.\nFor all embedding models, we used their officially released toolkit and default setting for training the classifiers. We implement LLaMA [19, 20] using released weights from Meta, while using OpenAI API [35] for ChatGPT and scikit learn[56] for classifiers."}, {"title": "4.1.2 DDI Prediction", "content": "In the context of our link prediction task, a crucial step is to partition links or edges into distinct sets comprising training, validation, and test data. We ensure that the training set remains distinct from the validation and test sets for the partitioning process. Furthermore, it safeguards the integrity of the validation set by excluding any edges present in the test set, thus maintaining the sanctity of the evaluation process. We divide all the interaction samples into a train set and a test set with a ratio of about 4:1 and randomly select 1/4 of the training dataset as a validation dataset.\nFollowing the MIRACLE [45], a state-of-the-art method in DDI, we set each parameter learning rate using an exponentially decaying schedule with an initial learning rate of 0.0001 and a multiplicative factor of 0.96. For the proposed model's hyperparameters, we set the dimension of the hidden state of drugs as 256 and 3 layers for the GCN encoder. To further regularise the model, dropout with p = 0.3 is applied to every intermediate layer's output.\nWe implement Pytorch-geometric [57] for GCN. GCN Model is trained using Adam optimizer.\nDuring experiments, the performance of results is assessed using the area under the receiver operating characteristic curve (AUROC) and the area under the Precision-Recall curve (AUPRC). AUROC and AUPRC are valuable metrics for evaluating accuracy, and a meaningful interpretation of AUC typically reflects the method's overall performance.\nFinally, we compare the results of integrating language models with GCN against the state-of-the-art deep learning method MIRACLE [45]. MIRACLE acquires knowledge of both the inter-view molecular structure and the intra-view interaction details of drugs for DDI prediction."}, {"title": "4.2 Analysis", "content": "We compare our LLaMA and ChatGPT embeddings on six datasets. Also, we highlight the best results between LLAMA and ChatGPT with the underline.\nFigure 1 demonstrates that LLaMA outperforms ChatGPT consistently over all the datasets with a large margin (Figure 2). Figure 4 also supports why ChatGPT is not performing as well as LLaMA models. Comparative analysis reveals that"}, {"title": "5 Conclusions", "content": "In summary, this research highlights the remarkable potential of LLMs such as ChatGPT and LLaMA in revolutionizing drug development and healthcare practices. Utilizing SMILES strings for chemical language encoding, these models present cutting-edge methods for the interpretation and prediction of molecular characteristics and interactions between drugs. LLaMA's efficacy in generating SMILES embeddings is particularly noteworthy, as demonstrated in our studies, suggesting its significant role in improving molecular property and drug interaction predictions. While these models, including LLaMA and ChatGPT, are not specifically tailored for SMILES string embedding, unlike MolBERT and Mol2Vec, their performance remains impressively competitive. Our research sets the stage for future advancements in refining LLMs for SMILES embedding. Future endeavours will aim to enhance SMILES embedding derived from LLMs, drawing inspiration from sentence embedding techniques in natural language analysis, such as whitening [58], to further this field."}]}