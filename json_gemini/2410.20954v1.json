{"title": "Active Legibility in Multiagent Reinforcement Learning", "authors": ["Yanyu Liu", "Yinghui Pan", "Yifeng Zeng", "Biyang Ma", "Doshi Prashant"], "abstract": "A multiagent sequential decision problem has been seen in many crit- ical applications including urban transportation, autonomous driving cars, military operations, etc. Its widely known solution, namely multiagent rein- forcement learning, has evolved tremendously in recent years. Among them, the solution paradigm of modeling other agents attracts our interest, which is different from traditional value decomposition or communication mecha- nisms. It enables agents to understand and anticipate others' behaviors and facilitates their collaboration. Inspired by recent research on the legibility that allows agents to reveal their intentions through their behavior, we pro- pose a multiagent active legibility framework to improve their performance. The legibility-oriented framework allows agents to conduct legible actions so as to help others optimise their behaviors. In addition, we design a series of problem domains that emulate a common scenario and best characterize the legibility in multiagent reinforcement learning. The experimental results demonstrate that the new framework is more efficient and costs less training time compared to several multiagent reinforcement learning algorithms.", "sections": [{"title": "Introduction", "content": "Multiagent Reinforcement Learning (MARL), as a powerful method to tackle multiagent sequential decision problems, has grown tremendously in the recent two decades [1, 2]. In the MARL research development, coopera- tive tasks among agents have emerged as one of the main focuses. Enabling agents to learn cooperative behaviors facilitates the completion of more com- plex tasks, thereby providing greater benefit to human life. When MARL evolves to learn agents' collaboration, it often leads to two branches: value decomposition and centralized-critic. The value decomposition methods train a global Q-network with the consideration of global information and are able to overcome the MARL instability, e.g. VDN [3], QTRAN [4], QMIX [5], etc. The centralized-critic methods aim to learn a centralized critic network, and then use it to train distributed actor policy, e.g. MADDPG [6], COMA [7]. Beyond this, communication has been adopted in MARL to transmit local information, which can be aggregated into a global perspective. Foerster et al. [8] first investigated the autonomous communication among agents in the learning. Many other works [9, 10, 11, 12] have contributed to the communi- cation in MARL. The communication mechanisms, however, encounter chal- lenges, e.g. communication interference and noise, bandwidth limitations, etc. in practical engineering applications.\nRecently, a new framework appears by modeling other agents in MARL [13, 14]. This framework assumes that it is beneficial for each agent to account for how the others would react to its future behaviors. Wen et al. [15] pre- sented a probabilistic recursive reasoning (PR2) framework, which adopted variational Bayes methods to approximate the opponents' policy, to which each agent found the best response and then improved its own policy.\nEmpirically, predicting the intended end-product (goal), as well as the sequence of steps (plan), can be helpful for improving the performance of agents' interaction in MARL [16, 17, 18]. In MARL, agents could model the intentions and policies of their teammates, allowing them to better coordi- nate their actions and optimize their collective performance. By recognizing others' plans, the agents can adapt their own behaviors to support or comple- ment their teammates' goals therefore leading to more efficient and effective collaboration. However, solely relying on the modeling capabilities of the agents has limitations and faces numerous challenges in enhancing system performance. One of the challenges is ambiguity: extracting the plan or pre- dicting the future actions from observed trajectories could be confusing and"}, {"title": null, "content": "complex, as trajectories may have multiple interpretations or could be mis- interpreted by unforeseen factors. Once the agent recognizes the incorrect intention, the consequence could be devastating and catastrophic under some circumstances. On the other hand, the contradiction between the recognition accuracy and computational complexity severely limits the generalization of plan recognition in MARL.\nRecently, legibility has been introduced to facilitate agents to convey their intentions through their behaviors [19]. In other words, the legibility could reduce the ambiguity over the possible goals of agents from an observer's perspective and improve human-and-agent collaboration. The most recent work [20], namely Information Gain Legibility (IGL), used the information entropy gain to shape the reward signal and improved the agent-to-human policy legibility. Compared to the existing methods [21, 22], IGL is much easier to implement and can theoretically be extended to many reinforcement learning methods. Thus, we will investigate the MARL legibility further in this paper."}, {"title": "Preliminary", "content": "The problem of learning a goal-directed policy is typically abstracted into a mathematical framework known as Markov Decision Process (MDP), which is a discrete-time stochastic process with the Markovian property. In MDP, the future state depends solely on the current state and the action taken, independent of past states and actions. In this paper, we define MDP as a tuple M = {S, A, T, R, \u03b3}, where S denotes the state space, A denotes the action space, T denotes transition probability, R denotes the reward signal, and \u03b3\u2208 [0, 1] denotes the discount factor. In traditional single-agent RL, we seek to find a policy that maximizes the expected return over the states SES.\n\n$J(\\pi) = \\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t r_t]$\n\nIn MARL, the environment is non-stationary from the perspective of each agent because other agents are learning and changing their policies simulta- neously. This makes it challenging for the agents to learn stable policies. One solution is the development of Interactive-POMDP proposed [13], which enables agents to utilize more sophisticated techniques to model and predict"}, {"title": null, "content": "behaviors of other agents. The I-POMDP for the agent A\u00b2 \u2208 {A\u00b9, ..., AN} is defined as {ISi, A, T, Ni, Oi, Ri}, in which the most notable component is the interactive state IS\u2081 = S\u00d7M, where M holds all possible models of other agents. Apart from this, other components of I-POMDP are similar to a standard POMDP. The great I-POMDP contribution lies in incorporating the modeling of other agents into a subject agent's decision optimization, enabling the agents to observe and coordinate with each other."}, {"title": "Policy Legible Markov Decision Process", "content": "Recently, Faria et al. [41] introduced legibility into traditional MDPs, namely Policy Legible Markov Decision Process (PoLMDP), denoted as {S, A, T,R, R, \u03b3, \u03b2}. A POLMDP is defined in the context of an environment with N differ- ent objectives, each of which is represented by a different reward function Rn, n = 1, ..., N and thus with a different MDP Mn. R is the legible reward function, denoted as:\n\n$R(s,a) = P(R_n | s, a)$\n\nwhere P(Rn|s, a) can be reformulated via Bayes' Theorem below.\n\n$P(R_n | s, a) \\propto P(s, a | R_n)P(R_n)$\n\nThis transformation allows us to express the conditional probability of a reward Rn given a state s and action a in terms of the conditional probability of the state and action given the reward, multiplied by the prior probability of the reward. P(s, a|Rn) can be determined by applying the maximum-entropy principle, with \u03b2 serving as the hyper-parameter. Here, Q* represents the optimal Q-function for the MDP Mn, and is calculated as follows:\n\n$P(s, a | R_n) = \\frac{exp(\\beta Q^*_n(s, a))}{\\sum_{m=1}^{N} exp(\\beta Q^*_m(s, a))}$\n\nPOLMDP has the advantage on the computational simplicity and ease of implementation. However, PoLMDP requires the optimal policy under the current objective during the training, which is challenging in multiagent environments where each agent's policy is constantly updated. Moreover, the completeness of PoLMDP is difficult to be guaranteed. Hence, expanding upon its framework, we propose a novel reward shaping function that is better suited for multiagent interaction. Additionally, we conduct a formal analysis of convergence and completeness on achieving the legibility."}, {"title": "Active Legibility through Reward Shaping", "content": "Research of single-agent reinforcement learning has been well explored; however, things get very different when there are more than one agents in one common environment. The most challenging is the non-stationarity in a multiagent setting, which is caused by the changing of other agents' policies over time as they learn. Building on the previous exploration of legibility, we first define the Legible Interactive POMDP (LI-POMDP) in Sec. 4.1, which provides the theoretical foundation for the subsequent work. In Sec. 4.2, we then propose a Multiagent Active Legibility (MAAL) framework. The MAAL framework is an implementation of LI-POMDP and aims to improve the legibility of an individual agent's actions to other agents, thus enabling the subject agent to be more easily modeled by others."}, {"title": "An Interactive POMDP Framework with Legibility", "content": "We consider N collaborative agents {A\u00b9, ..., AN} to achieve the overall goal in a common environment. Then we assume that the goal g can be decomposed into M > N sub-goals, e.g.g = {gk|k = 1,..., M}, and for each sub-goal gk \u2208 G, gk is assigned to an individual agent A\u00ba \u2208 {A\u00b9, ..., AN} The completion of all sub-goals results in succeeding the tasks. To facilitate the legibility development, we formulate the extended POMDP below from the perspective of individual agents. Similar to the modelling paradigm of interactive POMDPs, the extension is to enhance the generation of legible policies for individual agents by predicting other agents' beliefs over their goals.\nDefinition 1. For a subject agent Ai \u2208 {A\u00b9, ..., AN}, a legible I-POMDP (LI- POMDP) is defined as:\nLI-POMDP; = {S, A, T\u00ba, O\u00ba, N\u00ba, R\u00ba, G, B, T\u00b2, P\u00b2, R\u00ba}\nwhere:\n\u2022 S is the state space of the environment.\n\u2022 A = A \u00d7 A-i denotes the joint action space of the multiagent system, in which A\u00b2 is the executable action space of agent A\u00b2, and A\u00af\u00b9 is the observation actions of A\u00af\u00b9 by agent i.\n\u2022 T : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition function."}, {"title": null, "content": "\u2022 O is the set of observations the agent i receive from environment.\n\u2022 \u03a9 : S \u2192 O is the observation function and controls what A can receive in state s.\n\u2022 R\u00b2 is the raw reward signal sent from the environment.\n\u2022 G = g\u00b2 \u00d7 g\u00af\u00b9 denotes the set of goals in a multiagent system, where g\u00b2 represents the specific goal of agent A\u00b2 and g\u00af\u00b9 is the combination of goals of agents A\u00af\u00b9.\n\u2022 B = B-i \u00d7 B is the belief over goals G, in which B\u00af\u00b9 \u2208 R(N-1)|G|\u00d71 signifies the belief space of A's beliefs about all other agents in the system, and be\u2208 B-i detailing A's prediction about the goals of A-i (what A' thinks about the collective goals of all other agents). Bi\u2208 R|9|\u00d71 signifies the belief space of A's prediction of how A\u00af\u00b9 views A\u00b2 (b\u2208 B\u00b2). In other words, b can be interpreted as what A' thinks A-i thinks about Ai.\n\u2022 I : Oi \u00d7 \u00c3\u00af\u00bf\u2192B-i is the function for agent i to infer and predict the goals of agent A\u00af\u00b9.\n\u2022 Pi : Oi x \u00c3-i B is the function to indicate how others agent -i are predicting agent i's goal.\n\u2022 R\u00b2 is the reward shaping function to enhance the legibility of A's policy. It is derived from the original reward R\u00b2 by the environment (to be elaborated in Section ??).\nWe begin with modifying the objective function in Eq. 1 and propose a new legible objective function in Eq. 6.\n\n$J(\\pi^i) = \\mathbb{E}_{\\pi^i}[\\sum_{t=0}^{\\infty} \\gamma^t [r_t^i - \\beta D_{KL}(b_t^i || g^i)]]$\n\nwhere g\u00e5 \u2208 R|G|\u00d71 is the true distribution of agent A over the goals G, denoted as g\u00b2 = fone-hot(g\u00b2). \u03b2\u2208 R+ is the legibility weight to control the legibility level, by is the estimation of the predictive distribution of gi from other agents A\u00af\u00b9 at the time slice t, and DKL is the Kullback-Leibler divergence between b and g\u00b2. Eq. 6 maximizes the discounted cumulative reward and simultaneously seeks to minimize the margin between the other agent's prediction and the true goal of the subject agent."}, {"title": "Modeling and Predicting Other Agents", "content": "Plan recognition involves interpreting the intentions and plans of other agents, as well as anticipating their future actions. It largely depends on the analysis of agents' behaviours in context. By harnessing Bayesian update, we can elevate real-time plan recognition, thereby bolstering the system's overall clarity and transparency.\nInitially, we establish a model for agent A\u00b2 to infer the goals of agent A-i based on observed actions \u00c3\u00af\u00b9 and observations O. This process is represented as:\n\n$\\Gamma^i : O^i \\times \\tilde{A}^{-i} \\rightarrow B^{-i}$\n\nwhere b\u2208 B computes agent A's belief over the goals of agents A-i based on its observations of \u2208 O and observed actions a-1 \u2208 A. This involves calculating agent A's individual belief about the goals of other AE A-i\nAt the outset of interaction (t = 0), agent A\u00b2 holds a uniform belief about other agents A\u00b3, denoted as bo(g) = g\u2208 G. Consequently, a function mapping a policy or trajectory to an agent's goal belief is to update this prediction. For simplicity, we introduce the Bayesian approach [42] to update the belief of real-time intention identification:\n\n$\\\\\\tilde{b}_t^i(g^i) = \\begin{cases}\n\\frac{1}{|G|}, &  t = 0 \\\\\n\\frac{\\pi^i(o_t^i, \\tilde{a}_t^{-i}|g^i) \\tilde{b}_{t-1}^i(g^i)}{\\sum_{g^{'i}\\in G} \\pi^i(o_t^i, \\tilde{a}_t^{-i}|g^{'i}) \\tilde{b}_{t-1}^i(g^{'i})}, &  t \\neq 0\n\\end{cases}$\n\nwhere hypothesizes an action distribution over A\u00b9.\nAdditionally, neural networks, like Recurrent Neural Networks (RNN) or Long Short-Term Memory (LSTM), can be incorporated as I due to their ability to process sequential data. In the end, we concatenate all the individual beliefs with the operator as bi in Eq. 9.\n\n$\\bold{b}^i_t = \\\\j\\epsilon[1,N]\\\\\\bold{b}^\\i_t$\n\nAfter modeling the prediction of other agents' goals, we reverse the pro- cess to infer how they might predict our goals. In the LI-POMDP framework,"}, {"title": null, "content": "the primary objective is to enhance the agent i's legibility, making its goal gi more discernible by others. To achieve this, agent A\u00b2 must be able to reason about how other agents perceive its goals:\n\n$P^i : O^i \\times \\tilde{A}^{-i} \\rightarrow B^{i}$\n\nWhen there are more than two agents in the system, B\u00b2 is defined as a weighted sum:\n\n$\\bold{b}^i = \\frac{1}{N-1} \\sum_{j \\neq i, j \\epsilon [1,N]} w_{ij}\\bold{b}^{[j]}$\n\nwhere b\u2208 B is the belief of how other agents predict agent i's goal, b represents the probability distribution of agent j's prediction of agent i's goals, and wij can be used to adjust the extent of legibility that agent i expresses to different agents, allowing agent i to prioritize enhancing the legibility towards those agents that are more beneficial to its collaborative efforts.\nThe implementation of Pi varies with the training paradigm. In decen- tralized training training, agent i utilise an estimator to infer be based on the actions of other agents -i, assessing whether they have understood its intentions through the coordination in their actions. This process, known as recursive reasoning [43, 44] in game theory, can be exemplified as \"I believe that you believe that I believe...\". In a centralized training setting, Pi is more straightforward, as agent i can directly communicate or query [45, 46] the other agents -i's understanding of its intentions (i.e., their estimation of its goals), allowing for more precise and computational-friendly optimization of legibility."}, {"title": "Making Legible Decision", "content": "To complete tasks efficiently, A\u00b2 chooses an action that is not only to accomplish its own goal g' but also cooperate with the A-\u00b9's goal in the decision process. Therefore, the policy input for A' is the concatenation of observation ot, its own goal g', and the estimation of goals of other agents b: \u03c0\u00b2(Ogb) \u2192 a. When the environmental state is transited to St+1 by the joint actions [a, a], the reward ri for A\u00b2 is received and its policy is updated. At this point, ri will be manipulated to r\u012f and fed back to A to improve the legibility while updating policy."}, {"title": null, "content": "We utilize the reward shaping technique to enable the agent's behavior to be legible in solving MDP with the new objective function in Eq. 6. We define the KL-divergence Gain (KLG), denoted by \u2206DKL(A'), as the difference of DKL(b||g\u00b2) before and after the action a-1 is executed in the state st\u22121.\n\n$\\Delta D_{KL}(a^{-i}_t) = D_{KL}(b_{t-1}^i || g^i) - D_{KL}(b_t^i || g^i), \\\\ t \\geq 1$\n\nIn Eq.12, ADKL(a) quantifies the amount of ambiguity reduced once the action a-1 is executed. For instance, if the action a-1 from A\u00b2 is very informative for A\u00af\u00b9 to distinguish the plan, the b would converge to the subject agent's true goal g' with the execution of a:b (g\u00b2) \u21921, and DKL(bg) would therefore reduce with it, and vice versa. Then, we add the legibility term ADKL(a-1) to the original reward signal ri-1 with the parameter \u1e9e to balance the scale.\n\n$r_t^{-1} = r_t^{i} - 1 + \\beta \\Delta D_{KL}(a_t^{-i})$\n\nWith the legibility incorporation, ri-1 is used in the policy update of A through reinforcement learning. To summarize it, the legibility works in the following way: if a is helpful for A\u00ba \u2208 A\u00af\u00b9 to recognize the true goal of A\u00b2, the KL-divergence between band g\u00e5 reduces, leading to a positive ADKL, and eventually encouraging A\u00b2 to be more likely to choose a by increasing the reward.\nIn the end, for each agent, the problem can be simplified to an MDP and solved using single-agent reinforcement learning methods (with the environ- mental uncertainty already encapsulated in (ogb\u00af)). The policy update is performed using the tuple [(0-1 gb-1), a\u22121, (og b), -1]."}, {"title": "The MAAL Framework", "content": "In this section, we delve into the MAAL framework, elucidating its work- ings through a scenario as shown in Fig. 2. The scenario involves N agents, categorized as A and A\u00af\u00b9. Their shared objective, denoted as G, is divisi- ble into the sub-goals: G = g\u00b2 Ug\u00af\u00b9. A Success is achieved when agent A reaches g\u00b2 \u2208 G and agents A\u00af\u00b2 attain g\u00af\u00b9 \u2208 G, with mutual awareness of the accomplishment. The dashed lines colored by orange in Fig. 2 represent the operational framework from the perspective of agent A\u00b2, equally applicable to A-i."}, {"title": "Convergence Analysis", "content": "In this section, we first define the completeness in Definition 1, and analy- sis the MAAL reward shaping from the perspective of a single agent. Specif- ically, we examine if an agent can reach a sub-goal state under the origi- nal reward function, and if, using MAAL for the reward shaping, it satisfies Proposition 1, ensuring that the agent with enhanced legibility can also reach the target state. We first define the completeness of a policy a below.\nDefinition 1. Let s\u00ba represent the initial state, s* the goal (absorbing) state, and Pt(s, si) denote the probability of an agent reaching si aftert steps transitions from s\u00b2. Then a policy w is complete, if for every initial state s\u00ba, there exist a finite time t such that Pt(s\u00ba, s*) = 1.\nProposition 1. Let any agent A' with a target sub-goal g' be given. The necessary condition for applying the shaping reward by MAAL algorithm to agent A\u00b9 without compromising the completeness of the policy as defined in Definition 1 is: at any two time t\u2081 and t2, when the agent A' is in the same state s, the observing agent's estimation of agent Ai 's goal must be consistent, i.e. b"}, {"title": null, "content": "t2\n= b\nProof. We refer to the convergence analysis of reward shaping [48], which reveals that one way an agent's policy is incompleteness when agents repeat- edly visit non-goal states in pursuit of shaping rewards, thereby hindering task completion. To further clarify it, we introduce a straightforward exam- ple to illustrate the potential issues arising from the improper reward shaping in multi-agent systems. Fig.3 illustrates a scenario where reward shaping is used to improve the agent's exploration efficiency. Although the purpose of using reward shaping in this context differs from this paper (where MAAL uses reward shaping to directly alter the final policy), it is still highly valuable for analyzing completeness.\n1\n2\n6. Experimental Results\nThis section evaluates the MAAL performance on specific problems (em- phasizing goal identification between agents) through comparative experi- ments and ablation studies. In Section 6.1, we first introduce the Lead-Follow"}, {"title": "Lead-Follow Maze", "content": "Maze (LFM), a discrete maze scenario with two agents and four exits. To diversify the experimental environments, we propose the simple navigation scenario in Section 6.2, which expands upon the simple spread task from the particle environment, incorporating more agents and landmarks. The simple navigation, with its continuous state-action space, allows for a more comprehensive evaluation of MAAL in complex settings, testing its ability to enhance action legibility and improve multi-agent system performance."}, {"title": "Experimental Settings", "content": "Since both the state and action spaces are discrete, the leader and fol- lower employ Q-Learning to learn their policies. In the LFM environment, the state space for both agents is defined as S, which represents the co- ordinates of the leader and follower. For example, as shown in Fig. 4, s = ((2, 4), (5,2)), where (2,4) indicates the coordinate of the leader, and (5,2) represents the coordinate of the follower. For the Leader agent, the policy is fed with [S g*], where s is the state and g* represents the target"}, {"title": null, "content": "exit: a \u2190 argmaxa'\u2208AQ\u2514([S \u00a9 g*], a'). As to the follower agent, its input is defined as [S\u011d], where \u011d is the follower's estimation of the true target, obtained from the Bayesian learning as Eq. 8. At the end of the episode, the follower utilizes the observation of leader's trajectory and the true target to update its plan recognition through parameter learning.\nThe action space for both agents is A = {up,down,left, right, stay}, where each action moves one grid, and the state transition is deterministic. When the agents complete the task, they both receive a reward of +1. For each grid it moves, the agent receives the motion cost of -0.1."}, {"title": "Comparative Experiments", "content": "In comparative experiments, we select a series of the MARL algorithms compared to the MAAL approach in the LFM domain.\n\u2022 Independent Q-Learning (IQL) [1] is an extension of the Q-Learning algorithm for multiagent settings. Multiple agents are trained indepen- dently, each with its own policy. Agents interact with the environment and learn to maximize their individual expected rewards.\n\u2022 Value-Decomposition Networks (VDN) [3] assumes that the global value can be represented as the sum of the individual values of each agent and aims to capture the interdependencies between agents by decomposing the global value function.\n\u2022 QMIX [5] employs deep neural networks to learn the value functions and the mixing network, allowing for more expressive representations and approximation of complex value functions compared to VDN.\n\u2022 Multiagent Variational Exploration (MAVEN) [49] is an improved al- gorithm of the QMIX that overcomes the low exploration efficiency due to monotonicity constraints.\n\u2022 Mutil-Agent Deep Deterministic Policy Gradient (MADDPG) [6] is an extension of the Deep Deterministic Policy Gradient (DDPG), where each agent maintains its local actor network to make decisions, and learning policy from a centralized critic network taking the joint action and observation of all the agents as inputs.\nWe have run each algorithm for 5 to 6 times and plot both the mean (with a curve) and standard deviation (with a shallow shadow)."}, {"title": null, "content": "Figure 5 presents the average rewards for the various algorithms, exclud- ing shaped rewards. The results show that, with the exception of MADDPG, all the algorithms exhibit varying degrees of performance improvements in the post-training. The MADDPG algorithm (purple curve) fails to converge, likely due to its inadequate adaptation to discrete state and action spaces. In- terestingly, despite being one of the simplest algorithms, the IQL algorithm (red curve) performs remarkably well in this environment. Similarly, the VDN algorithm (orange curve) demonstrates good performance; however, it suffers from high variance and significant uncertainty. Although QMIX (blue curve) is designed as an improved version of VDN, it performs poorly in this environment, significantly lagging behind VDN. This discrepancy may arise from the difficulty of training the non-linear combinations in QMIX, which do not adapt well to the environment. A similar pattern is observed with the MAVEN algorithm (green curve). While it shows rapid improvement in the initial stage, its final performance only matches that of QMIX.\nOn the other hand, the MAAL+QL algorithm achieves the best perfor- mance in this environment. When the legibility weight \u1e9e is set to 0.01, MAAL+QL (brown curve) outperforms all comparative algorithms. This outcome suggests that enhancing the legibility of an agent's policy improves the speed and accuracy of intention recognition between agents, thereby boosting collaborative performance. However, when \u1e9e is increased to 0.1, the performance of MAAL+QL declines significantly. The higher reward"}, {"title": "Legibility's Impact Experiment", "content": "We conducted comparative experiments with different legibility weight B under the same initial conditions. This experiment focused on two metrics: Prediction Correctness Ratio (PCR) and Prediction Time Ratio (PTR). PCR refers to the accuracy of follower's predictions of leader' goals. For example, if within training episodes 1000 to 2000, the follower correctly predicted the leader's goal in 415 out of 1000 episodes, then the prediction correctness ratio at Episode 2000 is 415/1000 = 0.415. PTR refers to the ratio between the number of steps required for an agent to correctly predict other agents' goals from the beginning of the episode to the length of the episode. A smaller PTR means that the agent can accurately predict other agents' goals earlier, suggesting higher legibility. For instance, if an observer correctly predicts the goal at Step 15 and maintains it to the end of the episode, with the total episode length being 50 steps, then the prediction time ratio for that episode is 15/50 = 0.3."}, {"title": null, "content": "We show the PCR in Fig. 6(a) and PTR in Fig. 6(b) with different B values. It is noticed that the monotonic increase in PCR and decrease in PTR happen as \u1e9e grows, which strongly indicates the improved legibility of the subject agent's behaviors. When the legibility is not applied, the follower's PCR is less than 70%, and requires nearly half of the journey before identifying the true goal of subject agent. Subsequently, as KLG"}, {"title": "MAAL Beyond Q-Learning", "content": "As we mentioned before, MAAL stands upon the standard MDP and thus can be integrated into any single-agent reinforcement learning algorithm by solving MARL problems. We empirically study the legibility application in two different reinforcement learning algorithms. In this experiment, we incorporate MAAL with the State Action Reward State Action (SARSA) and Deep Q-Network (DQN) methods respectively, denoted as MAAL+SARSA and MAAL+DQN. To evaluate whether the legibility is helpful for multiagent learning, we set the parameter \u03b2 = 0 in one of the experiments, i.e. removing the legibility weight in the reward function."}, {"title": null, "content": "Fig. 6(c) has shown, in MAAL+SARSA, the success rate with certain legibility (orange curve) reaches only 20%, but is still superior to those with- out legibility (blue curve) in 15%. Improving the weight of legibility (green curve) will cause a decrease in the success rate. The same phenomenon also occurs on MAAL+DQN in Fig. 7(a). In MAAL+DQN, the usage of legibil- ity raises the success rate by 10%. However, after increasing the legibility weight to 10-1, the success rate of DQN sharply decreases, as DQN is more sensitive to rewards compared to SARSA and Q-Learning.\nFrom Fig. 6(d) and Fig. 6(e), we can see that legibility has a huge impact on the goal identification in SARSA. Without MAAL, the Follower can only recognize the true target of the navigator in 40% episodes and obtain the correct results at almost the end of the episode. We also observe a similar pattern in MAAL+DQN from Fig. 7(b) and Fig. 7(c). We notice that the"}, {"title": null, "content": "PTR has converged to approximately 25% in MAAL+QL, MAAL+SARSA, and MAAL+DQN, i.e., at least a quarter of the journey is necessary before discriminating the true goal.\nIn summary, although MAAL+SARSA and MAAL+DQN do not perform as well as MAAL+QL, according to the success rate with and without MAAL, we demonstrate that extending MAAL to other RL methods is fairly feasible."}, {"title": "Particle Simple Navigation", "content": "Particle is a classic multiagent reinforcement learning environment pro- posed by OpenAI [50], where motion and collisions are simulated as real rigid-body collisions, calculating velocity and displacement based on momen- tum and forces. Building upon Particle, we introduce a new scenario called simple navigation. Compared to LFM, the simple navigation scenario has a continuous state space with higher dimensions and more agents, making it much more complex than LFM.\nSimilar to the LFM environment, in the simple navigator environment, legibility can also help improve system performance, as illustrated in Fig.8. In this scenario, the navigation agent, as agent1, aims to reach landmark1. Agent1 has two possible trajectories to reach the target (represented by solid red arrows), each circumventing the obstacle from a different direction. Al- though both trajectories can reach the target, trajectory 1 is also perceived by the follower as a potential route to landmark2. Consequently, during the first half of trajectory 1, the observer of agent1 cannot distinguish the agent's true goal from landmark1 and landmark2. In contrast, trajectory 2 does not present any ambiguity or misjudgment potential. In certain situations, such"}, {"title": null, "content": "as when agents2 and agent3 are located near landmark2 in the beginning, trajectory 2 can significantly reduce the number of ineffective steps."}, {"title": "Experiment Details", "content": "In this experiment, to deal with high-dimensional observation space, all the agents use DQN as policy, with vectorized [o\u00b2 \u2299 g\u00b2], i = 1,2,3 as the network input, where o is the observation vector of the agent i, g\u00b9 is the target landmark, \u011d\u00b2 is the agent2's prediction of agent1's target, and \u011d\u00b3 is the agent3's prediction of agent2's target (in one-hot encoded). It is worth mentioning that, we used Long Short-Term Memory (LSTM) [51] for implementing plan recognition. LSTM is a type of recurrent neural net- work (RNN) capable of learning long-term dependencies and sequences of data, making it well-suited for tasks involving time-series prediction and se- quence classification. For example, agent2 uses its observations o\u00b2 and the observed action of agent1 a\u00b9 to predict the distribution of agent1's target: LSTM(O2=0 a=0, O\u00b2=1a\u00b2+=1, O\u00b2=2 \u00a9 a=2, ...) \u2192 g\u00b2. Similarly, agent3 uses the same kind method to observe and predict the goal of agent2.\nThe experimental results, illustrated in Fig.9, demonstrate that inte-"}, {"title": null, "content": "grating the MAAL reward shaping with various single-agent reinforcement learning algorithms significantly enhances their performance. Among all the methods, DDPG+MAAL (orange curves) achieves the highest rewards and the fastest converging speed, showing a steady and substantial improvement over the training episodes. The Twin Delayed Deep Deterministic Policy Gradient (TD3) [52"}]}