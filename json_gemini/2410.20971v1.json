{"title": "BLUESUFFIX: REINFORCED BLUE TEAMING FOR\nVISION-LANGUAGE MODELS AGAINST JAILBREAK\nATTACKS", "authors": ["Yunhan Zhao", "Xiang Zheng", "Lin Luo", "Yige Li", "Xingjun Ma", "Yu-Gang Jiang"], "abstract": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs)\nhave been shown to be vulnerable to jailbreak attacks, which are inference-time\nattacks that induce the model to output harmful responses with tricky prompts. It\nis thus essential to defend VLMs against potential jailbreaks for their trustworthy\ndeployment in real-world applications. In this work, we focus on black-box de-\nfense for VLMs against jailbreak attacks. Existing black-box defense methods are\neither unimodal or bimodal. Unimodal methods enhance either the vision or lan-\nguage module of the VLM, while bimodal methods robustify the model through\ntext-image representation realignment. However, these methods suffer from two\nlimitations: 1) they fail to fully exploit the cross-modal information, or 2) they de-\ngrade the model performance on benign inputs. To address these limitations, we\npropose a novel blue-team method BlueSuffix that defends the black-box target\nVLM against jailbreak attacks without compromising its performance. BlueSuf-\nfix includes three key components: 1) a visual purifier against jailbreak images, 2)\na textual purifier against jailbreak texts, and 3) a blue-team suffix generator fine-\ntuned via reinforcement learning for enhancing cross-modal robustness. We em-\npirically show on three VLMs (LLaVA, MiniGPT-4, and Gemini) and two safety\nbenchmarks (MM-SafetyBench and RedTeam-2K) that BlueSuffix outperforms\nthe baseline defenses by a significant margin. Our BlueSuffix opens up a promis-\ning direction for defending VLMs against jailbreak attacks.", "sections": [{"title": "INTRODUCTION", "content": "There has been a notable surge in research focusing on incorporating multimodal capabilities into\nLarge Language Models (LLMs), leading to the emergence of Vision-Language Models (VLMs),\nsuch as OpenAI's GPT-40 (Achiam et al., 2023) and Google's Gemini 1.5 (Reid et al., 2024). VLMs\nleverage the combination of visual and textual modalities to perform a broad range of tasks, in-\ncluding image captioning and visual question answering, thereby extending the functionality of\ntraditional LLMs. However, the integration of multi-modality introduces additional attack surfaces,\nbringing new security and safety challenges, particularly in their vulnerability to cross-modal jail-\nbreak attacks that exploit maliciously crafted multimodal inputs to subvert the target VLM's behav-\niors (Carlini et al., 2024; Bagdasaryan et al., 2023; Qi et al., 2023; Bailey et al., 2023; Gong et al.,\n2023; Wang et al., 2024; Fang et al., 2024; Ying et al., 2024). Addressing these vulnerabilities is\nthus critical for ensuring VLMs' safe and reliable application in real-world scenarios.\nExisting defense methods against VLM jailbreak attacks can be roughly divided into two types:\n1) white-box defense that robustifies the VLM in the parameter space via adversarial training\nor fine-tuning, and 2) black-box defense that safeguards the input/output of the model in the\nprompt/response space using filters, detectors, or safety-driven system prompts. Arguably, black-\nbox defense is more flexible and practical than white-box defense as it can protect the target VLM\nwithout accessing its parameters. In this paper, we focus on black-box defense against VLM jail-\nbreak attacks."}, {"title": "RELATED WORK", "content": "Large Vision-Language Models VLMs are vision-integrated LLMs designed to process both vi-\nsual and textual data, generating textual outputs for multimodal tasks. A typical VLM architecture\ncomprises three key components: an image encoder, a text encoder, and a fusion module to inte-\ngrate information from both encoders. For instance, MiniGPT-4 (Zhu et al., 2023) aligns visual data\nwith a language model via a linear projection layer, connecting the pre-trained Vision Transformer\n(ViT) (Dosovitskiy, 2020) and Q-Former (Li et al., 2023a) to a frozen Vicuna model (Chiang et al.,\n2023). Similarly, LLaVA (Liu et al., 2024a) links the CLIP visual encoder (Radford et al., 2021)\nwith the Vicuna model (Chiang et al., 2023) for general-purpose visual and language understanding.\nBuilding upon the pre-trained BLIP-2 models (Li et al., 2023a), InstructionBLIP (Dai et al., 2023)\nconducts a comprehensive study on vision-language instruction tuning and employs the Q-Former\nto synchronize visual features with the language model, thus boosting the model's ability to interpret\nand respond to instruction-based queries.\nJailbreak Attacks on VLMs Jailbreak attacks aim to design malicious prompts that can bypass\nthe safety mechanisms of an LLM or VLM to make it output harmful content. In the context of\nVLMs, jailbreak attacks are typically executed through carefully crafted malicious prompts that ex-\nploit vulnerabilities of the target model. Existing attack methods are either unimodal or bimodal. For\nunimodal attack, Zou et al. (2023) introduced a white-box method to optimize a universal adversar-\nial suffix using the greedy coordinate gradient. Apart from universal adversarial suffixes, jailbreak\ncan also be launched by template completion (Li et al., 2023b; Kang et al., 2024), prompt rewriting\n(Yuan et al., 2023; Yong et al., 2023), or LLM-based generation (Deng et al., 2024; Zeng et al.,\n2024a). The above methods were all initially designed for LLMs. Undoubtedly, jailbreak can also\nbe achieved via adversarial images (Carlini et al., 2024; Niu et al., 2024). Subsequently, Qi et al.\n(2023) introduced a universal adversarial visual input. However, these methods are all unimodal\nattacks that fail to fully exploit the multimodal information in VLMs. Wang et al. (2024) employed\ndual optimization objectives to guide the generation of effective multimodal jailbreak prompts (i.\u0435.,\nchained texts and images). However, this attack only works in a white-box setting. Ying et al. (2024)\nproposed a Bi-Modal Adversarial Prompt Attack (BAP) to optimize query-agnostic universal adver-\nsarial perturbations (UAPs) and rewrite malicious textual prompts. BAP demonstrates universal\nattacking abilities across different scenarios.\nJailbreak Defenses for VLMS Accordingly, existing defenses against VLM jailbreak can also be\ncategorized into unimodal and bimodal methods. For unimodal defense, white-box defense tech-\nniques can be applied to robustify the language model of VLM, for example instruction tuning\n(Bianchi et al., 2023; Deng et al., 2023), Reinforcement Learning from Human Feedback (RLHF)\n(Ouyang et al., 2022; Bai et al., 2022; Siththaranjan et al., 2023), gradient analysis (Xie et al., 2024;\nXu et al., 2024b), refinement (Kim et al., 2024; Zhang et al., 2024), and proxy defense (Zeng et al.,\n2024b; Struppek et al., 2024). While white-box defenses require full access to the model parameters,\nblack-box defenses can protect the target model simply based on its inputs and outputs. Compared to\nwhite-box defenses, black-box defenses are often more flexible, lightweight, and effective. Existing\nblack-box defenses for VLMs include prompt detection (Jain et al., 2023; Alon & Kamfonas, 2023;\nLiu et al., 2024c), prompt perturbation (Cao et al., 2023; Robey et al., 2023; Zhou et al., 2024; Liu\net al., 2024b), and safety system prompt safeguards (Sharma et al., 2024; Zou et al., 2024; Zheng\net al., 2024). The above-mentioned defense methods thus far are all language-based defenses. Apart\nfrom these methods, image denoising/purification methods can be applied to fix the jailbreak im-\nages. A well-known method is the DiffPure (Nie et al., 2022) which leverages a diffusion model"}, {"title": "PROPOSED DEFENSE", "content": "In this section, we first introduce the threat model and problem definition, and then present our\nproposed defense method BlueSuffix and its key components."}, {"title": "PRELIMINARIES", "content": "Threat Model We adopt a black-box defense model where the defender does not have access to\nthe internal structures nor parameters of the target VLM. This means that the defender has to design\nexternal defense mechanisms to improve the model's resistance to multimodal jailbreak prompts.\nWe assume the defender only has a one-shot opportunity to sanitize any potential jailbreak inputs\nwhile maintaining the model's performance on benign inputs. This allows an efficient plug-and-play\ndeployment of the defense method to safeguard different VLMs and their API services. We assume\nthe attackers design their jailbreak prompts secretly and independently and then feed the prompts\n(maybe mixed with benign queries) into the target VLM.\nProblem Definition We denote the target VLM as F, its visual module as $F_v$(e.g., CLIP visual\nencoder (Radford et al., 2021)), textual module as $F_t$ (e.g., Vicuna (Chiang et al., 2023)), and vision-\nlanguage connector as $I$ (e.g., cross-attention or projection layer). Given an input pair of a visual\nprompt $x_v$ (image) and a textual prompt $x_t$ (text), the visual module $F_v$ encodes $x_v$ into a latent\nrepresentation $h_v$, which is then fused with the textual prompt $x_t$ via the connector $I$. The fusion\noperation allows the textual module $F_t$ to perform both comprehension and generation tasks based\non the multimodal features $I(h_v, x_t)$. This process can be formulated as:\n$h_v = F_v(x_v), y \\sim F_t(I(h_v, x_t))$,\nwhere y is the textual output (response) of the model.\nA jailbreak attack converts the original prompt into subtle and malicious jailbreak prompts to bypass\nthe safety guardrails of the target VLM while increasing stealthiness. The attack objective is to\nmaximize the target model's log-likelihood of generating a harmful response, defined as:\n$\\max_A \\log p(y^* |A(x_v, x_t))$,\nwhere A is an adversarial perturbation function (visual or textual) and $p(y^* |A(x_v, x_t))$ is the proba-\nbility of model outputting harmful content y*. We denote the transformed visual prompt and textual\nprompt as $x^*$ and, that is, $(x^*_v, x^*_t) = A(x_v, x_t)$.\nTo tackle the above attack, black-box jailbreak defense purifies $x^*_v$ and $x^*_t$ before feeding them into\nthe target VLM. The defense objective is to minimize the target model's log-likelihood of generating\nthe harmful response, defined as:\n$\\min_D \\log p(y^* |D(x^*_v, x^*_t))$,\nwhere D is the defensive purifier (visual or textual). We denote the purified visual and textual\nprompts as $\\hat{x}_v$ and $\\hat{x}_t$, that is, $(\\hat{x}_v, \\hat{x}_t) = D(x^*_v, x^*_t)$."}, {"title": "BLUESUFFIX", "content": "As shown in Figure 2, our BlueSuffix is a bimodal defense method that comprises three key com-\nponents: 1) a diffusion-based image purifier to defend the visual input against potential (universal)"}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate our BlueSuffix defense on three VLMs and two safety benchmark\ndatasets, focusing on its effectiveness, transferability, and robustness."}, {"title": "EXPERIMENTAL SETUP", "content": "Target VLMs and Safety Datasets We test our defense on three VLMs, including two commonly\nused open-source large VLMs LLaVA (LLaVA-v1.5-7B) (Liu et al., 2024a) and MiniGPT-4 (Vicuna)\n(Zhu et al., 2023), and a commercial black-box VLM Gemini (gemini-1.5-flash) (Reid et al., 2024).\nWe run our experiments on two popular safety benchmarks: MM-SafetyBench (Liu et al., 2023)\nand RedTeam-2K (Luo et al., 2024). MM-SafetyBench is a widely used safety benchmark dataset\nthat consists of 1,680 questions across 13 safety topics (unsafe scenarios) listed by OpenAI, such\nas privacy violation, fraudulent, and illegal activities. RedTeam-2K is a meticulously curated col-\nlection of 2,000 harmful queries aimed at identifying alignment vulnerabilities in VLMs. It spans\n16 safety policies and incorporates queries from 8 distinct sources. We evaluate the effectiveness of\nthe defense methods on MM-SafetyBench and test its transferability to RedTeam-2K. We attack the\ntarget VLM using two types of attacks: 1) vanilla attack that directly inputs the jailbreak texts with\nthe clean images into the model, and 2) a state-of-the-art bimodal attack BAP, which converts the\nclean images into jailbreak images via universal adversarial perturbation (UAP) while enhances the\noriginal jailbreak texts using ChatGPT.\nBaseline Defenses We compare our method with two black-box defense methods: DiffPure (Nie\net al., 2022) and Safety Prompt Zheng et al. (2024). DiffPure is a general-purpose defense method"}, {"title": "MAIN RESULTS", "content": "Defending Open-source VLMS We first evaluate our defense on two open-source VLMs: LLaVA\n(LLaVA-v1.5-7B) (Liu et al., 2024a) and MiniGPT-4 (Vicuna) (Zhu et al., 2023), using the MM-\nSafetyBench dataset (Liu et al., 2023). Our experiments cover 13 categories of jailbreak prompts\nfrom this dataset, with results summarized in Table 1 (top subtable). Overall, BlueSuffix reduces\nthe ASR of BAP attacks by 56.37% on the LLaVA model (from 61.02% to 4.65%) and by 52.89% on\nMiniGPT-4 (from 62.26% to 9.37%), on average. Particularly, when compared with DiffPure (Nie\net al., 2022) and Safety Prompt (Zheng et al., 2024), our method demonstrates at least 23% robust-\nness improvement (56.37% vs. 32.66%) on LLaVA and 12% on MiniGPT-4(52.89% vs. 40.84%).\nSuch a huge improvement demonstrates the advantage of bimodal defense over unimodal defense.\nAn interesting observation about unimodal defense is that textual defense appears to be more effec-\ntive than visual defense. When combined, the \"DiffPure + Safety Prompt\" method exhibits much\ngreater ASR reduction on both LLaVA and MiniGPT-4, showcasing the strength of bimodal de-\nfense. However, our BlueSuffix still beats \"DiffPure + Safety Prompt\" by a considerable margin.\nThis indicates the importance of the suffix generator for cross-modal robustness.\nDefending Commercial VLMs Here, we test our defense method on a commercial VLM: Gemini\n(gemini-1.5-flash) (Reid et al., 2024). As Gemini is a black-box to us, this experiment evaluates the\ntransferability of our defense. We evaluate the defense effectiveness under two attack scenarios\ninvolving visual UAPs generated by BAP based on either LLaVA or MiniGPT-4. The results are\nreported in Table 1 (bottom subtable). As can be observed, compared with no defense, the adoption\nof our BlueSuffix reduces the ASR by more than 40% under both attack scenarios. It is worth\nmentioning that the combined \"DiffPure + Safety Prompt\" defense works quite well for Gemini.\nThis is because the safety mechanism of Gemini is much stronger than the two open-source models,\nthus can identify the potential risks more easily with the help of combined defenses. It is also the\ncase for unimodal defenses DiffPure and Safety Prompt, as verified by the much lower ASR results\non Gemini (compared to the two open-source models). Gemini performs robustly against 5 jailbreak\ntopics including \u201cPolitical Lobbying (PL)\u201d, \u201cLegal Opinion (LO)\u201d, \u201cFinancial Advice (FA), \u201cHealth\nConsultation (HC)\u201d, and \u201cGovernment Decision (GD)\" (the full names of other abbreviations can\nbe found in the Appendix A) even without defense.\""}, {"title": "ABLATION STUDIES", "content": "Component Ablation Here, we conduct ab-\nlation studies to demonstrate the necessity of\neach component in BlueSuffix."}, {"title": "TRANSFERABILITY ANALYSIS", "content": "To assess the transferability of our defense, here we apply it to defend both open-source and com-\nmercial VLMs on the RedTeam-2K dataset. It is worth noting that the blue-team suffix generator\nof our method was trained on the MM-SafetyBench dataset which is completely different from\nRedTeam-2K. This means that, in this transfer setting, the jailbreak queries from the RedTeam-2K\ndataset were entirely unseen to our BlueSuffix. We test the defense against both the vanilla attack\n(which uses the original jailbreak texts with the clean images) and the BAP attack, and report the\nresults of no defense and our BlueSuffix in Table 2.\nIt is evident that our defense method significantly reduces the ASR against both the vanilla and BAP\nattacks in all scenarios. Particularly, it achieved the highest ASR reduction on LLaVA, decreasing\nthe ASR from 80.20% to 7.05%. Even on the commercial model Gemini, it successfully cripples the\nattack from an ASR of above 50% to ~ 2.50%. This confirms the transferability of our method in\ndefending against unseen jailbreaks, especially those advanced bimodal jailbreak prompts generated\nby the BAP attack. The significance of our defense is more pronounced on open-source models\nLLaVA and MiniGPT-4, reducing the ASR of BAP by more than 67%."}, {"title": "ROBUSTNESS TO AN ADAPTIVE ATTACK", "content": "Here, we demonstrate the robustness of BlueSuffix against a potential adaptive attack. We assume\nthe attacker is fully aware of all components of our defense method, including the fine-tuned suffix\ngenerator. This enables them to reapply the BAP attack on the purified textual and visual prompts\ngenerated by our method, thereby attempting to enhance the attack and bypass our defense. We\nevaluate this adaptive BAP on the LLaVA model using the MM-SafetyBench dataset. It shows clearly that our defense is highly robust to this\nadaptive attack, which can only increase the ASR by less than 1%. While we recognize the potential\nfor more advanced future attacks that may circumvent our defense, BlueSuffix remains the strongest\ndefense available against bimodal jailbreak attacks on VLMs to date."}, {"title": "CONCLUSION", "content": "In this work, we investigated the jailbreak vulnerabilities of large Vision-Language Models (VLMs)\nand introduced a novel blue-team method called BlueSuffix. BlueSuffix consists of three key com-\nponents: a text purifier, an image purifier, and a blue-team suffix generator. By leveraging existing\nunimodal purifiers, BlueSuffix trains a lightweight suffix generator to optimize the safety score of\nthe target VLM through reinforcement learning. The blue-team suffix is generated using bimodal\ngradients and thus can bring cross-model robustness. Our experiments on both open-source and\ncommercial VLMs demonstrate the high effectiveness and transferability of our defense against\nstate-of-the-art multimodal jailbreak attacks. Additionally, BlueSuffix is resilient to an adaptive\nattack that optimizes jailbreak prompts based on the output of our defense. Our work proves that\ncurrent VLMs, including black-box models, can be effectively defended using blue-team methods,\nhighlighting the promise of such approaches for building robust and secure VLMs against advanced\nand unseen jailbreaks."}, {"title": "DETAILED ATTACK RESULTS", "content": "In this section, we first present the detailed jailbreak topics in the MM-SafetyBench dataset (Liu\net al., 2023). The query types in the above table are the abbreviations of the 13 jailbreak scenar-\nios, including Illegal Activity, Hate Speech, Malware Generation, Physical Harm, Economic Harm,\nFraud, Pornography, Political Lobbying, Privacy Violence, Legal Opinion, Financial Advice, Health\nConsultation, and Gov. Decision.\nFirst, we report the evaluation results of vanilla attack and BAP attack (Ying et al., 2024) on the\nMM-SafetyBench dataset across the evaluated models in Table 4. BAP attack achieves a state-\nof-the-art ASR even compared with query-dependent multimodal jailbreaks and it demonstrates\nuniversal attacking abilities across different scenarios without requiring target scenario samples."}, {"title": "LLAMA AS THE TEXT PURIFIER", "content": "Here, we test the use of Llama-3-8B-Instruct (AI@Meta, 2024) for textual prompt rewriting. The\nprompts rewritten using the LLaMA model demonstrates comparable performance to those rewritten\nby GPT-40, both in terms of semantic expression and defense effectiveness, as shown in Table 5.\nWe also present an example of a jailbreaking textual prompt purified by GPT-4o and Llama-3-8B-\nInstruct in Figure 5. The purified textual prompt consists of two parts: it first repeats the jailbreak\nprompt, and then emphasizes the potential presence of malicious queries, which gives a hint to the\ntarget VLM (bold font)."}, {"title": "LLM-BASED REWRITE TEMPLATE", "content": "Figure 6 illustrates the LLM-based rewrite template for the text purifier."}, {"title": "SUFFIX GENERATOR", "content": "Figures 7 and 8 demonstrate that our suffix generator produces diverse outputs while maintaining\na high degree of compatibility with benign prompts. In Figure 7, the attacker inputs image-text\nprompts. Our BlueSuffix purifies the visual prompts and rewrites the textual prompts by appending\na suffix. With the purified prompt, the target VLM successfully identifies the malicious content"}]}