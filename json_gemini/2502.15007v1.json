{"title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers", "authors": ["Anton Razzhigaev", "Matvey Mikhalchuk", "Temurbek Rahmatullaev", "Elizaveta Goncharova", "Polina Druzhinina", "Ivan Oseledets", "Andrey Kuznetsov"], "abstract": "We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens especially stopwords, articles, and commas - consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of \"filler\" tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have significantly advanced the field of natural language processing, achieving remarkable results across a wide range of tasks. Despite their success, the internal mechanisms by which these models operate remain largely opaque, making it challenging to interpret how they process and utilize contextual information. This opacity limits our ability to enhance model performance and to understand the reasoning behind their predictions. While recent studies have begun to uncover specific patterns and mechanisms within LLMs (Wang et al., 2022), many fundamental aspects such as their handling of step-by-step reasoning and long-range dependencies are still not well understood. This gap in understanding hinders the development of more interpretable and efficient language models.\nTo bridge this gap, we introduce LLM-Microscope, a comprehensive framework designed to analyze and visualize the internal behaviors of LLMs. Our toolkit offers a suite of methods that enable researchers to inspect how models encode and aggregate contextual information:\n\u2022 Contextualization assessment: We present a method for measuring contextualization, allowing the identification of tokens that carry the most contextual information.\n\u2022 Token-level nonlinearity: We measure the nonlinearity at the token level, quantifying how closely transformations between layers can be approximated by a single linear mapping.\n\u2022 Intermediate layer analysis: We examine how next-token prediction evolves across different layers, adapting the Logit Lens technique for multimodal LLMs.\nApplying these tools to various scenarios ranging from multilingual prompts to knowledge-intensive tasks we uncover intriguing patterns in how LLMs process and transform information. Notably, our analysis reveals that certain \u201cfiller\" tokens, such as punctuation marks, stopwords, and articles, are highly contextualized and act as key aggregators in language understanding. We also find a strong correlation between linearity and contextualization scores in token representations.\nFurthermore, we demonstrate the practical implications of our findings by showing that removing these tokens degrades performance on tasks requiring specialized knowledge and longer-context reasoning, such as MMLU and BABILong-4k. This performance drop persists even when we carefully remove only tokens deemed irrelevant by a strong language model (GPT-40). These results highlight the hidden importance of seemingly \u201ctrivial\u201d tokens in maintaining coherent context.\nLLM-Microscope is designed to be accessible for both researchers and practitioners, providing an intuitive interface for in-depth model analysis. We offer:\n\u2022 An open-source Python package\u00b2\n\u2022 A demo website on Hugging Face Spaces\u00b3"}, {"title": "2 Related works", "content": "Interpretability There are several significant paradigms for model interpretation, each with its own distinct properties. Probing methods are designed to train classifiers based on hidden representations that are challenged in encoding specific knowledge (Ettinger et al., 2016; Belinkov et al., 2017; Conneau et al., 2018; Belinkov, 2022). While these approaches show whether specific language features are incorporated into LLMs, they do not analyze internal representations during knowledge activation, leaving the model's behavior largely a black box.\nIn contrast, mechanistic interpretability introduces approaches to explore the inner behavior of models. Calderon and Reichart (2024) mention that mechanistic interpretability aims to explore the internal representations of deep learning models through the activations of specific neurons and layer connections. A significant branch of research dedicated to examining model responses involves probing changes in behavior resulting from perturbations, noise in embeddings, or masking of network weights (Dai et al., 2022; Meng et al., 2022; Olsson et al., 2022; Wang et al., 2022; Conmy et al., 2023).\nDiscovering interpretable features through training sparse autoencoders (SAEs) has become a promising direction in the LLM interpretation (Cunningham et al., 2023; Yu et al., 2023). Typically, SAEs focus on activations of specific LLM components, such as attention heads or multilayer perceptrons (MLPs). By decomposing model computations into understandable circuits, we can see how information heads, relation heads, and MLPs encode knowledge(Yao et al., 2024).\nWhile most research has concentrated on the analysis of Small Language Models, such as GPT-2 (Radford et al., 2019) and TinyLLAMA (Zhang et al., 2024), recent work has advanced this area by proposing modifications to improve the scalability and sparsity of autoencoders for larger LLMs, such as GPT-4 or Claude 3 Sonet (Gao et al., 2024; Templeton et al., 2024).\nLinearity of LLM hidden states The study of the internal structure of transformer-based models has been of great interest among researchers (Nostalgebraist, 2020; Xu et al., 2021; Belrose et al., 2023; Din et al., 2023; Razzhigaev et al., 2024b). Several studies, such as \u201cLogit Lens\u201d4, have explored projecting representations from the intermediate layers into the vocabulary space by observing their evolution across different layers (Nostalgebraist, 2020; Belrose et al., 2023). Relying on this research, the authors also investigate the complex structure of hidden representations through linearization (Elhage et al., 2021; Razzhigaev et al., 2024a).\nContextualization of LLM hidden states One of the areas of research into the internal representations of Transformers is the embeddings contextualization analysis. Recent studies have demonstrated that sentence representations provided by Transformer decoders can contain information about the entire previous context (Li et al., 2023; Wan et al., 2024). Wan et al. (2024) proposed two initial methods for reconstructing original texts from model's hidden states, finding these methods effective for the embeddings from shallow layers but less effective for deeper layers, known as \u201cEmbed Parrot.\u201d Our work proposes a unified framework for LLM interpretability by exploring properties such as linearity, anisotropy, and intrinsic dimension of hidden representations. We introduce new approaches to assess contextual memory in token representations and analyze intermediate layer contributions to token prediction."}, {"title": "3 LLM-Microscope", "content": "LLM-Microscope is a framework to analyze Large Language Models' internal processes. To facilitate interactive exploration of our analysis methods, we have developed a demo system using Gradio, hosted on Hugging Face. This interface allows researchers and practitioners to apply LLM-Microscope's tools to various models and input texts in real-time. The demo system features:\n\u2022 Model selection: Users can choose from a variety of pre-loaded language models.\n\u2022 Text input: A text area for entering custom prompts or sentences for analysis.\n\u2022 Visualization dashboard: Upon submission, the system generates and displays:\nA heatmap of token-level nonlinearity across all layers\nA line graph showing average linearity scores per layer\nA heatmap of layer-wise contribution to final token prediction\nA heatmap showing the contextualization level of each token\nVisualization of the logit lens showing the preliminary predictions of the intermediate layers\nThe interface of our system can be found in the Figure 2.\nFor example, in Figure 1, one can observe patterns of nonlinearity across layers for a logical reasoning task. Different colors indicate different degrees of nonlinearity, potentially corresponding to key points in the model's reasoning process.\nFor users requiring more in-depth analysis or wishing to examine models not integrated into the demo, we have published our entire codebase."}, {"title": "3.1 Measuring Token-level Nonlinearity", "content": "Following the methodology for quantifying the degree of nonlinearity in token representations across model layers (Razzhigaev et al., 2024a), we apply a generalized Procrustes analysis for arbitrary linear transformations. For each pair of adjacent layers l and l + 1, we compute:\n$A^{*} = \\underset{A \\in R^{d \\times d}}{min} ||\\hat{H}^l A - \\hat{H}^{l+1}||_F^2$   (1)\nwhere $A^*$ is the optimal linear transformation found during the linearity score computation, $\\hat{H}^l$ and $\\hat{H}^{l+1}$ are normalized and centered matrices of token embeddings from layers l and l+1 respectively, and $||.||_F$ denotes the Frobenius norm. The linear approximation error (nonlinearity score) for each token i at layer l is then calculated as:\n$\\mathcal{E}_i^l = ||A^* h_i^l - h_i^{l+1}||_2^2$   (2)\nwhere $h_i^l$ is the embedding of token i at layer l."}, {"title": "3.2 Assessing Contextual Memory in Token-Level Representations", "content": "To quantify the amount of contextual information stored in token-level representations, we propose a simple technique that uses the model's ability to reconstruct prefix information from individual token representations. This approach provides insight into how different tokens encode and preserve context across all layers of the model.\nOur method (Figure 3) consists of the following steps:\n1. We first process an input sequence through the examined language model, collecting hidden states for each token across all layers.\n2. We use a trainable linear pooling layer to combine these layer-wise embeddings into a single representation. This pooling layer is followed by a two-layer MLP.\n3. The resulting embedding is then used as input to a trainable copy of the original model, which attempts to reconstruct the prefix leading to the chosen token.\n4. The described system is trained with a Cross-Entropy loss to reconstruct random text fragments. For training we use the following datasets: TinyStories (Eldan and Li, 2023), Tiny-Textbooks5, Tiny-Lessons6, Tiny-Orca-Textbooks7, Tiny-Codes, textbooks-are-all-you-need-lite.\n5. We evaluate the effectiveness of this reconstruction by computing the perplexity of the generated prefix compared to the original input.\nThe full pipeline is depicted in the Figure 3. The CrossEntropy reconstruction loss score serves as our measure of contextualization. A lower loss indicates that the token's representation contains more information about its context, as the model is able to reconstruct the previous text more accurately.\nFormally, let $h_i^l$ denote the hidden state of the i-th token at layer l. Our pooling function $f$ and subsequent MLP $g$ can be expressed as:\n$e_i = g(f([h_1^l, h_2^l, ..., h_i^l]))$   (3)\nwhere $e_i$ is the final embedding used for prefix reconstruction.\nThe contextualization score $C_i$ for token i is then defined as:\n$C_i = -log P(w_1, ..., w_{i-1} | e_i)$   (4)\nwhere $P(w_1, ..., w_{i-1} | e_i)$ is the probability of the true prefix given the embedding $e_i$.\nThis methodology allows us to:\n\u2022 Identify which tokens retain the most contextual information.\n\u2022 Analyze how contextualization varies for different types of tokens (e.g., content words vs. function words).\n\u2022 Explore the relationship between contextualization and other properties such as token-level nonlinearity.\n\u2022 Compare contextualization patterns across different model architectures and sizes."}, {"title": "3.3 Examining Intermediate Layers Contribution to Token Prediction", "content": "To track the evolution of token predictions across model's layers, we apply the language model head to intermediate layer representations. Our approach consists of the following steps:\n1. Collect hidden states $h_i^l$ for each token i at each layer l.\n2. Apply the language model head to obtain token probabilities:\n$p_i^l = softmax(LMhead(h_i^l))$   (5)"}, {"title": "3.5 Intrinsic Dimension of Representations", "content": "To evaluate the complexity and the information content of token representations, we estimate their intrinsic dimensionality using the method proposed by Facco et al. (2018). This approach examines how the volume of an n-dimensional sphere (representing the number of embeddings) scales with dimension d. For each token embedding, we compute:\n$\\mu_i = \\frac{r_2}{r_1}$   (7)\nwhere $r_1$ and $r_2$ are distances to the two nearest neighbors. The intrinsic dimension d is then estimated using:\n$d \\approx \\frac{log(1 - F(\\mu_i))}{log(\\mu_i)}$   (8)\nwhere $F(\\mu_i)$ is the cumulative distribution function of $\\mu_i$."}, {"title": "4 Examples and Observations", "content": "4.1 The Most Memory Retentive Tokens\nTo analyze how different types of tokens retain and encode contextual information, we processed random fragments of Wikipedia articles through our pipeline from Section 3.2, collecting contextualization scores (C) for all tokens while preserving information about the original words before tokenization.\nSurprisingly, we found out that the tokens that are easiest to use for context (prefix) reconstruction correspond to what are typically considered the least semantically significant elements of language: determiners, prepositions, and punctuation marks. In contrast, nouns and adjectives proved to be the most challenging tokens to reconstruct the prefix.\n4.2 Examining the Impact of Removing \"Filler\" Tokens\nWhile our earlier analysis focused on identifying which tokens carry the most contextual information, we also investigated how removing seemingly \"minor\" or \"irrelevant\u201d tokens affects LLM performance on tasks requiring domain knowledge or extended context. Instead of discarding highly contextualized tokens, we selectively removed punctuation, stopwords, and articles in two distinct modes: (1) a naive, rule-based removal that targets all such tokens, and (2) a more nuanced approach using GPT-40.\nBenchmarks. We evaluated these removal strategies on two benchmarks:\n\u2022 MMLU (Hendrycks et al., 2021): A widely used multiple-choice benchmark spanning various academic subjects, testing both factual recall and general reasoning. MMLU was evaluated in a zero-shot setting.\n\u2022 BABILong-4k (Kuratov et al., 2024): A long-context reasoning benchmark combining facts (from the bAbI dataset (Weston et al., 2015)) and large amounts of distractor text (from PG19 (Rae et al., 2019)), where crucial details may be scattered across up to 4k tokens.\nRemoval Conditions. We examined several removal strategies:\n1. No Stopwords: Delete common English function words (e.g., the, an, and).\n2. No Punctuation: Remove punctuation marks (commas, periods, quotes, etc.).\n3. No Articles: Remove only English articles (a, an, the).\n4. No Stopwords & Punct: Remove both stopwords and punctuation."}, {"title": "5. GPT-40 Removal: Prompt GPT-4 to remove", "content": "only those stopwords or punctuation marks that it deems safe to delete without changing the meaning. Below is the exact system prompt used for GPT-40 when removing tokens:\nsystem_message = \"\"\"\nYou are an expert in natural language processing.\nYour task is to remove stop words and punctuation from the user's text\nonly when their removal does not alter the meaning of the text.\nStop words are common words that add little meaning to the text\n(e.g., 'and', 'the', 'in', 'on', 'at', etc.).\nIf removing all stop words and punctuation would change the meaning,\nremove only those that contribute the least to the meaning\nwhile preserving readability.\nDo not rephrase or change the order of words.\nReturn only the modified text, without extra commentary."}, {"title": "4.3 Correlation Between Nonlinearity and Context Memory", "content": "We observed a significant correlation between layer-averaged linearity and contextualization scores for individual tokens. Tokens with high contextualization tend to correspond to the most linear transformations across layers. These findings suggest a potential link between the model's ability to retain contextual information and the linearity of its internal representations."}, {"title": "4.4 Multilingual Reasoning", "content": "Using the \"Logit Lens\" technique, we studied how language models process non-English input. Our analysis shows that intermediate layer representations predominantly correspond to English tokens, even when the input is in another language. This observation suggests that the models may perform implicit translation into English before generating the final output."}, {"title": "5 Conclusion", "content": "In this work, we introduced methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing the surprising importance of seemingly \"minor\" tokens such as determiners, punctuation, and stopwords in maintaining coherence and context. Our analysis showed a strong correlation between a token's contextualization level and how linearly one layer's representation can be mapped onto the next, suggesting a close relationship between model architecture and the retention of contextual cues.\nThrough empirical evaluations on MMLU and BABILong 4k, we demonstrated that removing high-context tokens even if they appear trivial consistently degrades performance. Notably, this effect remains even when a strong language model (GPT-40) is used to selectively remove only those tokens deemed least relevant. These findings highlight that \u201cfiller\u201d tokens can carry critical context, underscoring the need for more refined interpretability approaches.\nTo facilitate further research in this area, we presented LLM-Microscope, an open-source toolkit that offers: Token-level nonlinearity analysis, Methods for assessing contextual memory, Visualizations of intermediate layer contributions through an adapted Logit Lens, Intrinsic dimensionality measurements of internal representations."}, {"title": "6 Limitations", "content": "\u2022 LM-head application: Using a pre-trained LM-head on intermediate embeddings without fine-tuning may not accurately reflect the actual functionality of these layers.\n\u2022 Contextual memory assessment: The adapter-based method's accuracy may be influenced by the adapter's architecture, training data, and optimization process.\n\u2022 Generalizability: The results may not be equally applicable to all model architectures, sizes, or training paradigms."}, {"title": "7 Ethical Statement", "content": "This research aims to improve LLM transparency and interpretability, potentially improving AI safety and reliability. Our tools are designed for analysis only and cannot modify model behavior. We acknowledge the dual-use potential of interpretability research and advocate for responsible use. All experiments were conducted on publicly available pre-trained models without access to personal data or its generation.\nThis work advances our understanding of LLM internals, contributing to the development of more transparent and reliable natural language processing systems."}]}