{"title": "InsQABench: Benchmarking Chinese Insurance Domain\nQuestion Answering with Large Language Models", "authors": ["Jing Ding", "Kai Feng", "Binbin Lin", "Jiarui Cai", "Qiushi Wang", "Yu Xie", "Xiaojin Zhang", "Zhongyu Wei", "Wei Chen"], "abstract": "The application of large language models (LLMs) has achieved remark-\nable success in various fields, but their effectiveness in specialized domains like the\nChinese insurance industry remains underexplored. The complexity of insurance\nknowledge, encompassing specialized terminology and diverse data types, poses\nsignificant challenges for both models and users. To address this, we introduce\nInsQABench, a benchmark dataset for the Chinese insurance sector, structured\ninto three categories: Insurance Commonsense Knowledge, Insurance Structured\nDatabase, and Insurance Unstructured Documents, reflecting real-world insurance\nquestion-answering tasks. We also propose two methods, SQL-ReAct and RAG-\nReAct, to tackle challenges in structured and unstructured data tasks. Evaluations\nshow that while LLMs struggle with domain-specific terminology and nuanced\nclause texts, fine-tuning on InsQABench significantly improves performance. Our\nbenchmark establishes a solid foundation for advancing LLM applications in the\ninsurance domain, with data and code available at InsQABench.", "sections": [{"title": "1 Introduction", "content": "The insurance industry plays a critical role in mitigating financial risks and uncertainties,\nproviding essential protection to individuals and businesses alike [25]. However, the\ncomplexity of insurance products and the specialized knowledge required to understand\nthem often creates a barrier for the general public. This knowledge gap hinders people\nfrom making informed decisions and can contribute to confusion and mistrust towards\ninsurance services [23,13].\nRecent advancements in artificial intelligence, particularly in natural language pro-\ncessing (NLP), have opened new avenues for addressing this challenge. Large language\nmodels (LLMs), such as GPT-4 [1] and Claude-3 [2], demonstrate the capacity to pro-\ncess large volumes of text and generate human-like responses [9,40]. While promising,\napplying these models to specialized domains like insurance is far from straightforward."}, {"title": "2 Related Work", "content": "The emergence of Transformer-based models has catalyzed the development of large lan-\nguage models (LLMs) across various domains, including law [14,39,36,8], bio-medicine\n[32,31,18,7], and finance [12,24,20], where they are fine-tuned on domain-specific\ndatasets to achieve notable success. However, in the insurance sector, the adoption of\nLLMs is still in its early stages.\nExisting studies, such as ActuaryGPT [6], explore the use of LLMs to assist actuaries\nwith tasks like actuarial calculations and report generation. Similarly, many insurance\ncompanies have started developing proprietary LLMs, emphasizing internal business\nneeds such as automated sales, customer service, and management. These models\nintegrate proprietary data and industry knowledge but remain tailored to organizational\nworkflows rather than addressing consumer-facing challenges."}, {"title": "3 Method", "content": "We tackle three insurance domain QA tasks - Insurance Commonsense QA, Insurance\nDatabase QA, and Insurance Clause QA - by first fine-tuning large language models\n(LLMs) on our InsQABench Dataset using supervised fine-tuning (SFT) with LoRA[19].\nBuilding on this, we propose two tailored methods: SQL-ReAct for structured data tasks\nand RAG-ReAct for unstructured document tasks."}, {"title": "3.1 Supervised Fine-Tuning (SFT)", "content": "Using LoRA[19], we fine-tune pre-trained LLMs on the InsQABench Dataset across all\nthree QA tasks. This step enhances the model's ability to handle insurance-specific ter-\nminology and task requirements. Performance comparisons with non-fine-tuned models\nshow that SFT leads to notable improvements in domain-specific QA accuracy, provid-\ning a strong baseline for further enhancements. For Database QA and Clause QA, we\nmake some improvements to the fine-tuning method of the model to improve the final\nperformance. The details are shown in \u00a7 B.1."}, {"title": "3.2 SQL-ReAct", "content": "The Insurance database QA task aims to generate a response r given a user's question q,\nthe database schema S and the corresponding database execution engine E. Unlike the\ntypical Text-to-SQL [21,28,41,26,10] task, which focuses on generating a single SQL\nstatement, our task's goal is to generate the final answer. The model must first produce\nall required SQL statements based on the user's query, then provide the final answer by\ninterpreting the feedback from executing all the SQL statements on the database. The task\npresents several challenges, including understanding complex query semantics, handling\ncell mismatches, dealing with SQL execution errors, extracting relevant information\nfrom query results, etc. For example, with cell mismatches, the insurance company or\nproduct name mentioned in the user's query may not match the cell value in the database\n(like frequently used abbreviated names), leading to SQL execution errors or null values,\nthus affecting user experience.\nTo address above challenges, we propose a baseline method called SQL-ReAct.\nInspired by ReAct [38], SQL-ReAct employs an iterative approach to continuously\nrefine SQL statements based on feedback from the database until the desired query result\nis ready. SQL-ReAct introduces a structured inference process to achieve accurate and\nrelevant responses. This process utilizes specially defined tokens and flags to guide the"}, {"title": "3.3 RAG-ReAct", "content": "The Insurance Clause QA task focuses on generating accurate responses to user queries\nabout insurance documents. Unlike multimodal document QA datasets such as DocVQA [27],\nMP-DocVQA [34] and DUDE [22] that contain rich visual elements, insurance clause\ndocuments are predominantly text-based. This task presents unique challenges: complex\nand diverse layout structures, lengthy documents spanning dozens of pages, and abundant\ndomain-specific terminology that creates comprehension barriers for non-expert read-\ners. To address these challenges, we propose RAG-ReAct, a framework that combines\nrule-enhanced PDF parsing with iterative reasoning to accurately extract and synthesize\ninformation from insurance documents while maintaining crucial semantic relationships.\n The process begins with an insurance\ndocument, a user question, and a set of tools: a PDF parser, a dense retriever, and an\nLLM. First, the PDF document is parsed into structured text chunks using the Adobe\nPDF Extract API [15], with custom parsing rules applied to accurately capture complex"}, {"title": "4 InsQABench Dataset", "content": "In this section, we describe the basic information and the construction process of In-\nQABench Dataset. More details are shown in \u00a7 A."}, {"title": "4.1 Insurance Commonsense QA", "content": "To construct high-quality insurance domain commonsense question-answering dataset\nthat aligns with real-world applications, we collect training and test sets from various\nsources.\n We first crawl 8k insurance-related questions posted\nby users on a popular online QA forum\u201c, along with their corresponding highest-voted\nanswers provided by the community. To ensure the quality of answers, we utilize GPT-\n3.5[30] to refine the original responses. The raw question-answer pairs are used as input,\nand the model generates optimized answers that improve the professionalism, readability,\nand comprehensibility. In addition, we engage professional insurance domain experts\nto manually compose approximately 2k additional question-answer pairs. The experts\nare instructed to focus on crafting high-quality, canonical answers to common insurance\nquestions frequently posed by novice users. The resulting dataset serves as the training\nset for this task, yields a total of 10k samples.\nFor the test set, we adopt the test set of InsuranceQA\ndataset, which is, to the best of our knowledge, the only publicly available Chinese\ninsurance question-answering dataset. We sample 990 QA pairs, with questions from\ngenuine users and high-quality answers manually written by insurance domain experts,\nensuring the dataset's high representativeness of real-world scenarios and its value for\nevaluating the performance of insurance-domain question-answering systems."}, {"title": "4.2 Insurance Database QA", "content": "For the Insurance Database QA task, we first introduce the construction of our insurance\ndatabase and then describe the method used to build the training and test sets.\n Our insurance database is a comprehensive repository of in-\nformation on insurance companies and their products. It is designed to support users\nin understanding various aspects of insurance, such as product offerings, features, and\nstatistics. To populate this database, we crawl official websites of 192 insurance com-\npanies and collect meta-information and clause documents for 25k insurance products.\nWe employ GPT-3.5 to automatically extract the majority of the information from the\nwebsite source code and clause documents. To ensure the accuracy and completeness of\nthe database, human annotators thoroughly review the extracted data, identifying and\ncorrecting any errors or inconsistencies. For missing fields, the annotators manually fill\nin the required information.\n Inspired by the Evol-Instruct[37] method, we propose\nan approach to enhance the diversity of database query questions. We begin by creating\ninitial question templates using a rule-based method tailored for database question-\nanswering tasks. To further enrich and diversify these questions, we expand them across"}, {"title": "4.3 Insurance Clause QA", "content": "We first use the Adobe PDF Extract API to parse the clause PDFs,\nobtaining element-level text segmentation (as illustrated in Figure 4). This process\nretrieves the text and the bounding box coordinates for each element fragment. Following\nthis, we design specific rules tailored to the clause layouts of different companies. These\nrules leverage the bounding box information to merge the element-level text into coherent\nparagraph-level segments, enabling us to capture more accurate paragraph text across\nvarious clause layouts.\n We randomly select 10 paragraphs from a clause. To\ndistinguish it from the Database QA task, we skip the table of contents and company\ninformation at the beginning of the clause and focus on the unique textual content of\nthe clause. Also, we omit the appendix at the end of the clause, as most of it is mainly\nan explanation of expertise in other non-insurance areas. We limit each paragraph to a\nminimum of 50 Chinese characters to ensure it contains sufficient information.\nNext, we feed each selected paragraph into Gemini and prompt it to generate question-\nanswer pairs (QA pairs) for it. At this step, Gemini is prompted to discard paragraphs\nthat are meaningless or have too little information to further ensure the quality of the\nparagraphs. The number of QA pairs generated depends on the length of the paragraph,\nvarying from 1 to 3. In order to make the generated QA pairs more relevant to the real\ncase of inquiry, we employ a one-shot approach, where Gemini is provided with an"}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Insurance Commonsense QA task", "content": "We initially used a rule-based evaluation system that measures\nPrecision, F1, and ROUGE-L. However, due to the nuanced nature of insurance question-\nanswering, we found that rule-based metrics alone were insufficient for capturing the full\ndepth of model performance. To address this, we introduced a model-based evaluation,\nscoring the responses across three dimensions: accuracy (ACC), professionalism (PRO),\nand similarity (SIM) to the reference answers. The full score of each dimension is set at\n100 points, which are added together to obtain the total score, and the average value is\nfinally taken as the final result.\nWe evaluate Baichuan2-13B-Chat[4], GLM4-9B-Chat[17], and Qwen1.5-\n14B-Chat[3], before and after fine-tuning on the Commonsense Dataset using LoRA.\nRule-based evaluation is conducted on 990 test samples, from which 100 samples are\nrandomly selected for model-based evaluation scored by GPT-40\u00ba. The anonymized\noutputs of these 100 samples are assessed for Accuracy (ACC), Professionalism (PRO),\nand Similarity (SIM).\n In the rule-based evaluation (Table 3), fine-tuned models consis-\ntently outperform their base counterparts in all metrics. GLM4-9B-Chat achieves the\nhighest performance. The generally low ROUGE-L scores can be attributed to the brevity\nof responses in the test set, which limits the scope for generating longer, detailed answers.\n In the model-based evaluation (Table 4), fine-tuned GLM4-9B-Chat again exhibits the\nmost significant improvement in all evaluation dimensions. These results underscore\nthe effectiveness of fine-tuning on our dataset in capturing domain-specific nuances and\nimproving performance across diverse QA tasks."}, {"title": "5.2 Insurance Database QA task", "content": "To evaluate the performance of our model on Database QA task,\nwe use multi-dimensional evaluation criteria, combining a subjective scoring method"}, {"title": "5.3 Insurance Clause QA task", "content": "We apply similar rule-based and model-based evaluation methods\nhere as in the Commonsense QA task. The rule-based metrics remain consistent. For the\nmodel-based evaluation, we adopt three new dimensions: Accuracy (ACC), Completeness\n(CPL), and Clarity (CLR), inspired by DISC-LawLLM[39]. These dimensions aim to\nmeasure how well models understand and communicate complex insurance clause\ninformation."}, {"title": "6 Conclusion", "content": "We introduce InsQABench, a comprehensive benchmark for Chinese insurance question-\nanswering, covering Commonsense QA, Database QA, and Clause QA. To tackle the\ncomplexity of insurance knowledge, we propose SQL-ReAct and RAG-ReAct for struc-\ntured and unstructured data tasks. Fine-tuning LLMs on InsQABench significantly\nimproved their ability to handle domain-specific terminology and complex clause docu-\nments, providing a solid foundation for advancing insurance-specific NLP applications."}, {"title": "B Model Fine-tuning", "content": null}, {"title": "B.1 Method", "content": "In this part, we present the improved fine-tuning methods of Database QA and Clause\nQA based on LoRA.\nThe fine-tuning of the database question answering task is designed to\nenhance LLMs' ability to find the data from the database that users want to query. For\neach piece of data in the constructed fine-adjustment dataset, the input sequence of the\nmodel is denoted as $x_i$, and the expected output of the model is denoted as $y_i$.\n$L(D_{\\text{pair}}) = -\\log p(y_i|x_i)$\nAfter fine-tuning, the model can output each round of thought markup, thought process,\nand corresponding SQL statements as expected.\nWe use the Clause QAE pairs to train the models. For one QAE\npair $(x_i, C_i y_i, C_i)$ in the Insurance Clause QA dataset, $x_i$ stands for the query and $c_i$\nstands for the candidates with labels, while $y_i$ stands for the expected answer and $e_i$\nstands for the paragraph id(s) of the ground truth. We use a standard conditional language\nmodeling objective, and the loss function is defined as:\n$L(D_{\\text{pair}}) =\\sum [\\log P_{LM}(y_i|x_i, C_i)+log P_{LM}(e_i|x_i, C_i)]$\nAfter the Insurance Clause QA fine-tuning, the models acquired proficiency not only\nin giving detailed explanations of the clause text, but also in distinguishing the most\nrelevant parts of the retrieved results and excluding incorrect information. This dual-\nobjective training approach enhances the model's overall performance and robustness in\nhandling complex Insurance Clause queries."}, {"title": "B.2 Implementation Details", "content": "We show our implementation details in the model fine-tuning stage here. All experiments\nare conducted using 3 L40s(48GB). We use Llama-Factory as the fine-tune framework.\nThe hyperparameters for the LoRA fine-tuning is shown in Table 10, 11 and 12."}, {"title": "C Demo and Code", "content": "For Qwen1.5-14B-Chat, one of the models used in our experiments, we created online\ndemos using the fine-tuned version of the model, which we refer to as InsLLM. These\ndemos showcase the performance of the model fine-tuned on the InsQABench dataset.\nThe online demos, along with the source code, are available at the following URL:\nInsQABench."}, {"title": "D Prompt Engineering", "content": "We present the prompts used here."}]}