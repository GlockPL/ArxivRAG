{"title": "Non-rigid Relative Placement through 3D Dense Diffusion", "authors": ["Eric Cai", "Octavian Donca", "Ben Eisner", "David Held"], "abstract": "The task of \"relative placement\" is to predict the placement of one object in relation to another, e.g. placing a mug onto a mug rack. Through explicit object-centric geometric reasoning, recent methods for relative placement have made tremendous progress towards data-efficient learning for robot manipulation while generalizing to unseen task variations. However, they have yet to represent deformable transformations, despite the ubiquity of non-rigid bodies in real world settings. As a first step towards bridging this gap, we propose \u201ccross-displacement\" - an extension of the principles of relative placement to geometric relationships between deformable objects - and present a novel vision-based method to learn cross-displacement through dense diffusion. To this end, we demonstrate our method's ability to generalize to unseen object instances, out-of-distribution scene configurations, and multimodal goals on multiple highly deformable tasks (both in simulation and in the real world) beyond the scope of prior works. Supplementary information and videos can be found at our website.", "sections": [{"title": "1 Introduction", "content": "Learning from demonstrations has emerged as a powerful technique to imbue robots with complex manipulation capabilities. Recent approaches have achieved remarkable success on a wide variety of tasks, including mug-hanging [1, 2, 3], book-shelving [4], cloth-folding [5], and sauce-pouring [6]. One common approach for imitation learning is to learn end-to-end visuomotor policies that map directly from observations to low-level robot actions; however, such approaches often struggle to generalize to novel variations of the scene (e.g. unseen object instances or object configurations). In this work, we aim to train robots to be robust to these variations for multimodal, deformable object placement tasks. Given demonstrations of a cloth-hanging task, for example, a robot should be able to successfully hang the cloth for unseen cloth instances or hanger positions. Similarly, if the task has multiple possible goal configurations (e.g. multiple holes on the cloth through which to orient the hanger), the robot should be able to output multimodal predictions to complete the task."}, {"title": "2 Related Work", "content": "Relative Placement for Object Manipulation: As previously described, there exists a corpus of work that deconstructs a placement task as a relative pose prediction problem [8, 9, 10, 11, 12]. Dense Object Nets (DON) [3] and Neural Descriptor Fields (NDF) [2] accomplish this by learning dense embeddings and matching observation embeddings to demonstration embeddings. While these approaches work in constrained placement settings, they assume that the target object is moved relative to a static reference object. TAX-Pose [1] addresses this issue by directly modeling the relative pose of objects in the scene and regressing a task-specific \u201ccross-pose\u201d from learned dense embeddings. This inference pipeline, however, renders TAX-Pose unsuitable for multimodal goal prediction. Recent work (TAX-PoseD [13]) extends TAX-Pose to multimodal tasks using a conditional Variational AutoEncoder (cVAE) with a dense spatially-grounded latent space. Relational Pose Diffusion (RPDiff) [4] leverages iterative pose de-noising to learn object-scene relationships that are multimodal, but diffuses directly in the space of SE(3) transformations. This fundamentally restricts RPDiff to rigid placement tasks. In contrast to these approaches, our method is able to handle both multi-modal placements and deformable objects for relative placement tasks.\nDiffusion Models for Point Cloud Generation: Diffusion models [14, 15, 16, 17, 18, 19, 20] have emerged as the state-of-the-art in 2D image generation; their variational loss allows for more stable training and mode capture (in contrast to GANs), and the lack of aggressive regularization on their latent distribution mitigates the coverage-fidelity tradeoff inherent to VAEs. Following this success, there have been many efforts to transfer these advantages to 3D point cloud generation, either by applying 2D diffusion techniques to volumetric representations [21, 22] or by denoising individual points independently [23, 24]. Our methodology aligns with the latter, as we adapt the Diffusion Transformer [25] architecture to de-noise pointwise displacements.\nDeformable Object Manipulation: Despite its prevalence in the real world, deformable manipulation remains a challenge in robotics due to the complexity of its underlying dynamics and state spaces. Early works have demonstrated success on deformable tasks such as cloth and bedding"}, {"title": "3 Problem Statement", "content": "Relative Placement for Rigid Objects: In this paper, we study \"relative placement\" as a general framework that encapsulates many robotics tasks (e.g. placing a mug on a rack, or hanging a cloth on a hanger). Given two objects A and B, the goal of a relative placement task is to manipulate object A (the \"action\" object) into some position relative to object B (the \u201canchor\u201d object).\nWe will briefly review relative placement tasks for rigid objects as defined in prior work [1]: for objects A and B, let point clouds PA and P denote their respective object geometries in a desired goal configuration. Suppose, for example, that object A is a mug, object B is a mug rack, and (PA, P) are the point clouds of these objects when the mug is hanging on the rack. For a relative place-ment task, if both objects are transformed by the same SE(3)-transformation T, then the resulting configuration should also be considered a successful task completion; for example, if the mug and rack are transformed together, then the mug is still hanging on the rack. Formally, if (PA, PB) denote the point clouds of objects A and B in some arbitrary configuration, we can define whether this configuration represents a successful relative placement as:\nRelPlace(PA, PB) = SUCCESS \u21d4 \u2203T \u2208 SE(3) s.t. P*4 = T \u00b7 P*4 and P*B = T \u00b7 PB. (1)\nThen, given observed object point clouds PA and PB, the goal of a rigid relative placement task [1] is to learn a function f (PA, PB) = TAB that predicts a rigid transformation of object A such that RelPlace (TAB \u00b7 PA, PB) = SUCCESS. The transform TAB is referred to as the \"cross-pose\" [1] since it captures the pose relationship between objects A and B. Using motion planning, the robot can then move object A into a new pose determined by TAB to complete the relative placement task, e.g. to place the mug onto the rack [1].\nCross-Displacement for Deformables: In the deformable case, however, this formulation is ill-defined. For a task with deformable object A, there is no guarantee that there exists a rigid transformation TAB that can transform an observed point cloud PA into a goal configuration PA such that RelPlace (TAB\u00b7PA, PB) = SUCCESS, as object A may need to undergo a non-rigid transformation to achieve its goal configuration. Consider, for example, the task of hanging a towel on a towel rack the deformations undergone by the towel as it folds and drapes over the hanger cannot be captured by a rigid transformation.\nInstead, to model object deformations, we learn a dense transformation function that predicts where each point in object A must move. Namely, given an initial observation of object A as point cloud PA = {x0,...,xn} with xi \u2208 R\u00b3, we aim to learn a function g(PA,PB) = \u2206X, where \u2206X = {\u0394\u03c7\u03bf,..., \u2206xN} with \u2206xi \u2208 R\u00b3, such that RelPlace (PA + \u2206X,PB) = SUCCESS. Intuitively, \u2206X is a set of dense displacements such that xi + \u2206xi brings each point xi to the goal configuration relative to PB. As such, we refer to \u2206X as the \u201ccross-displacement\" of objects A and B, analogous to the \"cross-pose\" defined previously.\nGoal Multimodality: For some tasks, there may be more than one way to achieve a goal configuration. For example, for the task of hanging a towel on a rack, if object B consists of multiple towel racks, then hanging the towel on any of the racks should yield a valid goal configuration. To this end, we can define a distributional relative placement task: for a given point cloud P of object B,"}, {"title": "4 Method", "content": "Given point clouds (PA, PB) of objects A and B in an arbitrary configuration, our goal is to learn a function that predicts a distribution over cross-displacements g(PA,PB) = p(\u0394\u03a7). From this distribution, we can sample a cross-displacement \u2206X ~ p(\u0394X) to transform Pa into a successful goal configuration PA = PA + \u2206X relative to object B.\nTo solve this distributional non-rigid relative placement problem, we leverage diffusion models [14, 15], a class of generative models that rely on iterative noising and de-noising. More specifically, our method builds on Improved Denoising Diffusion Probabilistic Models [41]. Given some Gaussian noise x (noised from the forward process [14]), we train our model to de-noise xr by learning the reverse diffusion process [14]: po(Xt\u22121 | Xt) = N(xt\u22121; \u03bc\u03b8(xt), \u03a3\u03b8(xt)). Following [41], we re-parameterize the mean po(xt) and the covariance Eo(xt) using a noise prediction 60(xt) and interpolation vector ve (xt), respectively, and supervise with a hybrid loss function that combines the standard noise prediction error with a variational lower bound loss; for further details, we defer to Nichol and Dhariwal [41].", "latex": ["po(Xt\u22121 | Xt) = N(xt\u22121; \u03bc\u03b8(xt), \u03a3\u03b8(xt))"]}, {"title": "4.1 Point Cloud Generation for Relative Placement", "content": "Training: Given a sample demonstration (P4, P4, P5), where PA is the initial action object point cloud {x4,0,..., XA,N}, PA is the goal action object point cloud {x4,0,...,x*4,v}, and P is the goal anchor object point cloud, we train our model to de-noise a set of per-point displacements A\u03a7, with the ground-truth displacements defined as \u2206X* = P*A \u2013 Pa = {x*4,0 \u2212 XA,0,..., XAN XA, N}. Following Ho et al. [15], we sample partially noised displacements \u2206Xt:\nAXt = \u221aatAXo + \u221a1 \u2013 \u03ac\u03c4\u03b5 (3)\nwhere \u0394\u03a7 = \u2206X*, t ~ [0,T], \u0454 ~ N(0, I), and \u0101t is a function of the noise schedule. We then supervise our model's noise \u20ac (\u2206Xt, PA, P5, t) and interpolation vector vo (\u2206Xt, PA, P, t) predictions using the hybrid loss function from Nichol and Dhariwal [41]. Note that the model is conditioned on the anchor P, as well as the initial action point cloud Pa, which we refer to below as the \"action context.\" For architecture and training specifics, see Appendix B.\nInference: At inference, given some object configuration (PA, PB), we initialize a set of per-point displacements as Gaussian noise: \u2206\u0425\u0442 ~ N(0,I). These displacements are iteratively de-noised using our model's outputs: \u2206Xt\u22121 ~ N (\u2206Xt\u22121; \u03bc\u03b8, \u03a3\u03b8), with \u03bc\u0473 and \u2211\u04e9 re-parameterized as in Nichol and Dhariwal [41]. The final de-noised displacements AX0 are then used to transform the points of PA into a predicted placement PA + AX0 relative to PB.", "latex": ["\u0394Xt = \u221aatAXo + \u221a1 \u2013 \u03ac\u03c4\u03b5", "\u2206\u0425\u0442 ~ N(0,I)", "\u2206Xt\u22121 ~ N (\u2206Xt\u22121; \u03bc\u03b8, \u03a3\u03b8)"]}, {"title": "4.2 Diffusion Transformer for Point Cloud Generation", "content": "We adapt the Diffusion Transformer (DiT) [25] architecture for our model, as shown in Figure 2. At each diffusion timestep, our model takes as input the noised displacements \u2206Xt, the action context PA, the anchor PB, and the timestep t. Using MLP encoders, we first compute per-point displacement features f\u25b3x, per-point action context features fA, and per-point anchor features fB from AXt, PA, and PB, respectively. MLP weights are shared within sets of features. The action context features and displacement features are then concatenated into (f\u00a3, f\u2206x) as input to our modified DiT model alongside anchor features fB (see Figure 2, left).\nWithin a DiT block, self-attention is applied to the combined action features (fa, for\u2081) to aggregate information across the entire action point cloud and facilitate coordinated displacement predictions. Cross-attention is then applied to these features with the anchor features fr, allowing for scene-level global reasoning between the action and anchor objects. This is repeated for N blocks, before the network outputs a noise ee and interpolation vector ve prediction, as described above.\nWe refer to the above approach as the \u201cCross-Displacement (CD)\u201d variant of our TAX3D architecture, since we directly predict the cross-displacement AX. We also propose a \u201cCross-Point (CP)\" variant in which we directly encode and diffuse over the positions of the predicted goal point cloud PA = PA + \u2206X instead; we find that both variants perform similarly in our experiments. Following Peebles and Xie [25], we incorporate timestep conditioning using adaptive layer normalization (adaLN) blocks. For permutation equivariance, we do not incorporate any positional encoding."}, {"title": "5 Experiments", "content": "Setup: To the best of our knowledge, quantitative benchmarks do not exist for the setting of de-formable object relative placement. As such, to evaluate our approach, we design a novel experimental benchmark building on DEDO: Dynamic Environments with Deformable Objects [7], a suite of simulation environments for deformable manipulation. We experiment on two cloth-hanging tasks (Figure 3): HangProcCloth, in which a cloth must be hung through one of its holes, and HangBag,"}, {"title": "6 Conclusion", "content": "In this paper, we present a novel framework to perform relative placement for non-rigid manipulation tasks. We formulate the task of \u201ccross-displacement\u201d prediction to handle relative placement for arbitrary object deformations, and we show that our dense diffusion architecture can learn cross-displacements on multiple cloth hanging tasks in simulation and in the real world. Our experiments demonstrate our approach's ability to generalize to out-of-distribution scene configurations, unseen object instances, and multimodal placements for robust deformable manipulation.\nLimitations: There are several limitations to our method, which we leave for future work:\n1. Segmentations. Our method requires segmented action and anchor point clouds. Our current approach for obtaining such segmentations (point-prompted Segment Anything [42]) requires human involvement, limiting scalability. This can potentially be addressed using language-conditioned segmentation methods [44] to automate demonstration labeling.\n2. Open-loop control. Our method currently relies on open-loop position control to the predicted goal for robot execution, which limits the ability to perform more complex placement tasks. This can potentially be addressed by incorporating TAX3D goal predictions into a goal-conditioned manipulation policy."}, {"title": "A DEDO Environment Details", "content": "DEDO: Dynamic Environments with Deformable Object [7] is a suite of task-based simulation environments (hanging a bag, dressing a mannequin, etc.) involving highly deformable, topologically non-trivial objects. The environments are built on the PyBullet physics engine [45]."}, {"title": "A.1 HangProcCloth - Task Definition", "content": "For most of our experiments, we focus on the HangProcCloth task (Figure 6), in which a procedurally generated cloth must be placed on a hanger. More specifically, the cloth is generated to contain a hole in its topology - to successfully complete the task, the vertical part of the hanger should be aligned through the hole.\nThe hanger (anchor) is loaded into the PyBullet engine as a pre-defined rigid body, and contains two components: a 'tall rod', and the 'hanger' itself. While we randomize the anchor pose throughout our experiments, this geometry remains fixed. The goal of the task is explicitly formulated in the environment as the center of the 'hanger' component (Figure 6) - while this goal definition is not passed as input to our models, it is used later by our success metric for evaluation."}, {"title": "A.2 HangProcCloth - Cloth Generation", "content": "Following DEDO's implementation, every cloth in our experiments is procedurally generated as a rectangular mesh, and can be represented using the following parameters:\nThe amount of vertices to initialize the cloth mesh with. Every cloth is initialized as an evenly spaced node_density \u00d7 node_density grid (25 \u00d725 = 625 vertices for all of our cloths). Vertices are then removed during the hole generation process.\nThe width of the cloth.\nThe height of the cloth.\nThe number of holes in the cloth.\nSee A.2.1"}, {"title": "A.2.1 HangProcCloth - Hole Generation", "content": "Holes are created by removing mesh vertices. All generated holes are rectangular - as such, they can be represented topologically with respect to the procedurally generated cloth by their bottom-left and top-right corners. Accordingly, the holes parameter is a list, where each element corresponding to a specific hole in the cloth is a dictionary with elements:\nThe x vertex coordinate of the bottom-left corner of the hole.\nThe y vertex coordinate of the bottom-left corner of the hole.\nSimilar to x0, for the top-right corner.\nSimilar to y0, for the top-right corner.\nIn general, holes are randomly generated under the following constraints:\nThe range of possible values for x0.\nThe range of possible values for y0.\nThe range of possible values for wh, such that x1 = x0 + Wp.\nThe range of possible values for wh, x0 + wp.\nMore precisely, when generating holes, x0 and y0 are first sampled based on x_range and y_range, respectively - x1 and y1 are then sampled based on width_range and height_range. To ensure that the resulting cloth geometry is valid topologically, DEDO generates cloths using a Monte Carlo method, only returning valid holes if they pass a boundary check (all vertices lie within the cloth boundary) and an overlap check (different holes do not overlap). For further implementation details, we refer to the DEDO codebase [7].\nSince holes are generated by directly manipulating the cloth mesh, they can also be represented as deformable loops, defined by a set of \"loop vertices\" (Figure 6) - while information about these vertices is not passed as input to our models, it is used later by our success metric for evaluation."}, {"title": "A.3 HangBag - Task Definition", "content": "We also perform an additional experiment on the HangBag task (Figure 7), in which a deformable bag with two handles must be placed on a hanger. To successfully complete the task, at least one of the handles must lie on the handle.\nSimilar to HangProcCloth, the hanger (anchor) is a pre-defined rigid body - for our experiments, we randomize the anchor pose, but keep the geometry fixed. Unlike HangProcCloth, however, the cloth is loaded from a pre-defined mesh rather than procedurally generated. As such, we do not randomize the cloth geometry for this task, leaving such experiments for future work (notably, the DEDO environment does provide multiple meshes for the bag)."}, {"title": "A.4 Cloth Control", "content": "The HangProcCloth and HangBag environments do not model a robot grasp - instead, the cloth is manipulated by applying force controls to floating \"grippers\"\u00b9 attached to pre-defined points on the cloth (Figure 6,7). The grippers themselves are zero-mass and collision free.\nPseudo-expert Policy: To generate demonstrations, we hard-code a pseudo-expert policy with access to privileged environment information. At the initial state of the scene, the policy computes a target position for each gripper using the distance from the centroid of the loop vertices to the goal location in the hanger. If the cloth has multiple holes, a single hole is selected. When computing this target, we apply the same transformation that we apply to the anchor to randomize its pose - this ensures that the cloth is rotated appropriately to align with the hanger. At each time step, we also add a small correction to the target position based on the distance between the current centroid of the loop vertices and the goal position - this allows the pseudo-expert policy to adapt to small deformations in the cloth geometry, which we find meaningfully improves the success rate. For control, we pass the target positions for each gripper as inputs to a custom proportional-derivative controller (in addition to a target velocity of 0) with a velocity and position gains of 50 and a maximum force2 of 5.\nEvaluation Policy: To evaluate our models, we implement a separate evaluation policy without access to privileged environment information (e.g. goal location, deformable loop vertices). At the initial state of the scene, we run TAX3D on the full point cloud of the cloth\u00b3, and obtain the predicted position in the world frame of the two grippers (which we assume are attached to known points of the cloth). These target positions are then passed as inputs (in addition to a target velocity of zero) to a custom proportional-derivative controller, with the same gains as the pseudo-expert policy."}, {"title": "A.5 Episode Rollout", "content": "A.5.1 Rollout Phases\nEach episode rollout consists of two phases (Figure 6): a manipulation phase, in which the grippers receive force control inputs at each time step to manipulate the cloth, and a release phase, in which"}, {"title": "A.5.2 Success Metric", "content": "To robustly determine whether or not the task has been successfully completed, we implement our own success metric consisting of two components:\n1. Centroid Check: a binary metric that checks if the centroid of the deformable loop vertices is within a threshold distance of the goal location.\n2. Polygon Check: a binary metric that projects the loop vertices and goal location onto the xy-plane, and then checks if the projected goal point lies on the interior of the polygon defined by the projected loop vertices. This is an intuitive heuristic that checks whether or not the hole \"wraps\" around the vertical rod of the hanger.\nIf the cloth has multiple holes, these metrics are computed individually for each hole - the task is considered successful if both are true for at least one hole.\nFor the HangProcCloth task, the success metric consists of a centroid check (with threshold 1.3) pre-release and a polygon check post-release. For the HangBag task, the success metric conists of two centroid checks, both with threshold 1.4, pre- and post-release we do not use the polygon check for the HangBag task, as we found that it produces a larger number of false negatives."}, {"title": "A.6 Demonstration Generation", "content": "A.6.1 Randomizing Scene Configuration\nFor all HangProcCloth experiments, the objects in the scene are initialized to the following pose, shown in Figure 6:\nFor all HangBag experiments, the objects in the scene are initialized to the following pose instead (shown in Figure 7):"}, {"title": "A.6.2 Experiment Datasets", "content": "As a reminder, cloth geometry (including holes) is randomized by following the parameter ranges and procedures described in A.2.1. The datasets for each experiment are generated as follows, where each tuple entry corresponds to the (Train, Unseen, Unseen (OOD)) settings, respectively:\nFor HangProcCloth-multimodal, we generate a successful demonstration for each hole per cloth - as such, there are half as many unique cloths as there are total demonstrations. For all demonstrations across all experiments, the anchor pose is randomized as described in A.6.1."}, {"title": "B Training Details", "content": "B.1 Model Architecture\nAs discussed in 4.2, we modify the standard DiT block [25] to include an additional cross-attention head 8. For all of our experiments, we train the same architecture:"}, {"title": "B.2 Training Pre-Processing & Hyperparameters", "content": "For training, both the action and anchor point clouds are downsampled to 512 points using furthest point sampling4. The anchor point cloud is additionally augmented with z-axis rotations sampled uniformly at random from [0, 2\u03c0].\nAll models are trained under the same hyperparameters with AdamW optimization and cosine scheduling with warmup:"}, {"title": "C Evaluation Metrics", "content": "As discussed in Section 5, our method's modeling of point-wise displacements allows us to directly use root-mean-squared-error (RMSE) as a distance metric between predicted and ground truth con-figurations of the cloth. To appropriately evaluate distributional predictions in our setting, we define two evaluation metrics5:\n1. Coverage RMSE: For each demonstration with ground truth PA,i, we sample 20 predic-tions {PA,j}, and keep the minimum RMSE. This is aggregated across all demonstrations in the dataset. Intuitively, this metric captures how well a model can produce all of the modes in a given dataset - that is, how well it covers a distribution.\n2. Precision RMSE: We first collect demonstrations corresponding to a specific cloth geom-etry (this is either one demonstration for the unimodal case, or two demonstrations for the multimodal case) - for some cloth C, this serves as a cloth-specific reference set {P*Ai}c. We then sample 20 predictions conditioned on cloth C, and compute for each prediction PA, the minimum RMSE to ground truth point clouds in the reference set {P*A.i}c\u00f3. This is aggregated across all 80 predictions, and then across all cloths. Intuitively, this metric captures how well a model can consistently produce predictions that are close to the dataset configurations - that is, how precisely it models a distribution."}, {"title": "D Experiments", "content": "The following sections contain all of the ablation and baseline comparisons for all experiments. Note that we do not report any RMSE metrics for DP3, since it is an end-to-end policy, and does not predict a point cloud."}]}