{"title": "\"My Grade is Wrong!\u201d: A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays", "authors": ["Shengxin Hong", "Chang Cai", "Sixuan Du", "Haiyue Feng", "Siyuan Liu", "Xiuyi Fan"], "abstract": "Interactive feedback, where feedback flows in both directions between teacher and student, is more effective than traditional one-way feedback. However, it is often too time-consuming for widespread use in educational practice. While Large Language Models (LLMs) have potential for automating feedback, they struggle with reasoning and interaction in an interactive setting. This paper introduces CAELF, a Contestable AI Empowered LLM Framework for automating interactive feedback. CAELF allows students to query, challenge, and clarify their feedback by integrating a multi-agent system with computational argumentation. Essays are first assessed by multiple Teaching-Assistant Agents (TA Agents), and then a Teacher Agent aggregates the evaluations through formal reasoning to generate feedback and grades. Students can further engage with the feedback to refine their understanding. A case study on 500 critical thinking essays with user studies demonstrates that CAELF significantly improves interactive feedback, enhancing the reasoning and interaction capabilities of LLMs. This approach offers a promising solution to overcoming the time and resource barriers that have limited the adoption of interactive feedback in educational settings.", "sections": [{"title": "Introduction", "content": "As stated in Hattie and Timperley's landmark paper (Hattie and Timperley 2007),\nFeedback is one of the most powerful influences on learning and achievement,\nthe question of how best to provide effective feedback to students has been a long-standing research question in education. For instance, (Sadler 1989) has emphasized the importance of formative assessment and its role in helping students understand the standards they are aiming for, while (Shute 2008) has explored the idea of formative feedback that is timely, specific, and focused on the learning process. More recently, Nicol (Nicol 2014) has proposed considering Interactive Feedback as an alternative feedback format. Nicol suggests that feedback should not merely be a one-way transmission of information from instructor to student. Instead, it should involve students actively engaging with the feedback, interpreting it, and using it as a basis for further learning and improvement. However, despite the benefits of interactive feedback, it is not commonly used in practice, as it is time-consuming for teachers and requires significantly more investment in preparation and teaching delivery (Hopfenbeck et al. 2023; Westera et al. 2018).\nLarge Language Models (LLMs) have demonstrated success in applications such as automatic scoring and feedback generation (Dai et al. 2023a; Gubelmann et al. 2024; Kostic et al. 2024). For example, (Dai et al. 2023b) highlights how LLMs enable educators to provide feedback to larger classes more efficiently. As a result, LLM-empowered interactive feedback has emerged as a promising approach to overcome the time and resource constraints that have historically hindered the widespread adoption of interactive feedback. However, a review of past efforts to explore LLM capabilities (Moore et al. 2023; Wang, Yue, and Sun 2023; Xiu, Xiao, and Liu 2022) reveals that existing LLM techniques, such as prompt engineering, fall short in meeting two critical requirements for effective interactive feedback:\n1.  Reasoning: Interactive feedback is formative, requiring the continuous reception of information from students and the generation of targeted responses to guide their learning. LLMs, however, may struggle with accurately"}, {"title": "Related Work and Background", "content": "LLMs for Essay Evaluation and Feedback LLMs have become increasingly popular in automating essay evaluation and feedback generation, reducing the manual effort traditionally required (Kostic et al. 2024). They have shown promise in automating scoring, cutting down on time and labor (Boud and Molloy 2013; Dai et al. 2023a). For example, Yancey et al. demonstrated that GPT-4 can evaluate short English essays with near-equal performance to modern Automatic Writing Evaluation (AWE) methods, without specific training. Additionally, LLMs also can generate clear, natural language feedback that explains the reasoning behind them, enhancing transparency in the evaluation process (Dai et al. 2023a; Cohn et al. 2024). This capability is particularly valuable in educational settings, as it helps bridge the gap between evaluation and learning. Studies show that students find LLM-generated feedback helpful and rate its quality as good to very good (Gubelmann et al. 2024). However, challenges persist in accurately grading complex texts, fine-tuning, and providing tailored feedback. (Kostic et al. 2024) highlighted LLM limitations in evaluating complex academic texts, showing a gap between LLM capabilities and the nuanced requirements of student essay evaluation. Moreover, (Stahl et al. 2024) found that LLM-generated feedback does not sufficiently leverage specific scores to enhance its relevance and actionability.\nContestable AI Contestable AI asserts that models used in critical tasks like decision-making or evaluation should enable users to question, contest, and review their outputs (Alfrink et al. 2023b; Hirsch et al. 2017). (Leofante et al. 2024)"}, {"title": "Framework Design and Implementation", "content": "As illustrated in Figure 1, CAELF works in three stages:\n(i) LLM Discussion: Multiple TA agents discuss the essay based on the assessment rubrics, forming arguments.\n(ii) Formal Reasoning for Feedback Generation: The teacher agent analyzes the arguments through a formal reasoning process using argumentation. Based on the reasoning results, the teacher agent provides a grade and summative feedback for the essay.\n(iii) Interaction with User: Students can challenge the feedback or grade by responding to the teacher agent, initiating a new round of discussion and feedback generation with additional inputs from the student.\nAn example of CAELF execution is illustrated in Figure 2, we discuss the three stages as follows.\nLLM Discussion Several studies have shown that discussions and debates between multiple LLMs can enhance factual accuracy and reasoning skills in textual evaluation (Du et al. 2023; Liang et al. 2024). This debate process enables LLMs to detect inconsistencies in their analysis and effectively presents arguments and counterarguments (Tang et al. 2024). Building on this capability, we apply role-playing techniques to extend this approach to essay evaluation. In CAELF, multiple TA agents are used to generate arguments and counterarguments through dialogue. Each TA agent is assigned a specific role based on an assessment rubric, guiding their evaluation process.\nThe process begins with each TA agent presenting individual feedback on a student's essay. The agents then engage in several rounds of discussion, where they exchange responses to each other's feedback. Each agent autonomously contributes by either supporting or rebutting the others' points, continuing the debate until the set number of rounds is completed. Importantly, each TA agent is equipped with a memory function, storing all previous responses in chat transcripts, and the entire process operates without human intervention. As shown in the example in Figure 2, two TA agents, Mike and Sarah, initially hold opposing views on the essay. After a round of discussion, Mike maintains his original stance, while Sarah is convinced by his argument.\nFormal Reasoning for Feedback Generation Once the TA agents complete their discussion, the teacher agent analyzes their arguments and produces both assessment scores and feedback. To this end, the teacher agent aggregates the evaluations from the TA agents, forming a set of arguments that are then analyzed for semantic relationships (attacks). These relationships are used to construct an argumentation framework, within which formal reasoning is applied to identify coherent and non-conflicting arguments. The complete semantics is used, which provides criteria for consistency and comprehensiveness when evaluating arguments. In the case where there are multiple complete sets of arguments, the largest set is selected as the final accepted set. (In the example illustrated in Figure 2, the set of arguments {A, C} is selected.) From this set, the feedback is constructed using an LLM.\nIn this way, the teacher agent can determine the most valid positions from the TA agents, represented by the selected set of arguments. These positions serve as knowledge-enhancing prompts that assist the teacher agent in assigning essay grades and generating summary feedback. This method leverages formal reasoning to improve the efficiency and reliability of the LLM's evaluative process, allowing"}, {"title": "Experiment Settings", "content": "We use the critical thinking essay assessment as a case study to evaluate the general effectiveness of CAELF. Previous research has demonstrated that critical thinking skills can be developed through writing critical thinking essays (Schmidt 1999; Sharadgah 2014). By incorporating a formal argumentation framework, CAELF provides structured, interactive feedback, allowing students to reflect and improve their critical thinking abilities through iterative engagement.\nEssay Dataset and Assessment Rubrics We compiled a dataset of 500 critical thinking essays sourced from Hugging Face (Hagging Face 2024). After manual screening, we selected essays that met the inclusion criteria for this study: the essays had to be argumentative in genre and exceed a minimum length of 200 words.\nBased on prior research (Association of American Colleges and Universities 2019), we developed evaluation rubrics with four dimensions: issues, evidence, position, and conclusions. Each dimension was further subdivided into three levels with detailed descriptions shown in Table 2. Four coders, skilled in labeling student essays, worked in pairs to independently evaluate a total of 2,000 labels. Co-"}, {"title": "Experiment Results", "content": "We presented the experiment results in Table 3, based on which we structured the following analysis and findings.\nInitial & Interaction Accuracy Table 3 presents the accuracy results for the critical thinking essay dataset. We compared CAELF to the three baseline models under the same setup. In terms of initial accuracy, although CAELF is built on GPT-40-mini, its performance is close to that of GPT-40, indicating that CAELF can enhance the accuracy of initial grading (without interaction) in language models. The initial accuracy results show that the baseline models perform well, demonstrating that basic LLMs are also capable of generating grades and feedback without interaction, which aligns with the findings of (Dai et al. 2023b).\nHowever, interaction accuracy shows a catastrophic drop in the accuracy of the baseline models after one round of interactions (30% drop on average), suggesting that the basic LLM with direct prompts is not adapted to the interactive feedback task and suffers from a fundamental reasoning flaw (Xiu, Xiao, and Liu 2022; Wang, Yue, and Sun 2023). In contrast, CAELF is minimally affected by the interaction (and even improves in the Issue and Position dimensions). After interaction, CAELF achieves far better performance than the baseline models, achieving the best performance in each dimension, especially in the evidence dimension, where CAELF's interaction accuracy is 44.6% higher than GPT-40-mini, 32.8% higher than GPT-40, and 44.4% higher than Meta-Llama-3.1-8B. This suggests that although LLMs can provide feedback to students (Dai et al. 2023b), their easily misleading nature makes it difficult to adapt to the task of interactive feedback. In contrast, we effectively mitigate this shortcoming by introducing formal reasoning and multi-agent argumentation, thus highlighting the potential of CAELF as an application in educational environments.\nMaintain Truth & Admit Mistake To evaluate the correctness of the model's responses and the effectiveness of interactive feedback, we measured the maintain truth rate and admit mistakes rate, as shown in Table 3. When assessing the model's ability to maintain consistency, CAELF achieved a success rate of 80%-90%, while GPT-40-mini had success rates below 40% across all four dimensions, dropping to just 13.21% in the conclusion dimension. Meta-Llama-3.1-8B performed even worse, with rates below 25% in most dimensions, while GPT-40 averaged between 40%-50%. These results suggest that basic LLMs are not reliable in maintaining correct evaluations and are highly susceptible to user interference during interactive feedback.\nWe also assessed the models' ability to admit mistakes, where CAELF outperformed the baseline models by 10%-20% in most cases. This improvement indicates that CAELF's strong performance in maintaining accuracy is not due to over-defending its responses but rather its ability to correctly identify errors in previous feedback. In contrast, the baseline models using direct prompts did not admit mistakes based on genuine evaluation but instead relied on surface-level patterns from initial grades and user responses, often retracting correct grades in response to user rebuttals. These results show that CAELF is more suitable for handling human interaction in interactive feedback.\nHuman Evaluation Result To evaluate the feedback quality, we conducted a manual analysis of the textual content generated by the experiments. The same four coders responsible for the essay evaluation were invited to assess feedback quality. We adopted feedback evaluation criteria proposed in (Mitra et al. 2024) and included the following four dimensions in our evaluation:\n1. Readability (RE): The clarity and ease of understanding of the feedback."}, {"title": "Conclusion", "content": "In this paper, we propose a Contestable AI-Empowered LLM Framework for Interactive Feedback Generation (CAELF), aimed at automating the interactive feedback process and systematically addressing the weaknesses of LLMs in current interactive educational environments. CAELF employs a Contestable AI paradigm based on a multi-agent argumentation system that makes the feedback process interactive, explainable, and contestable to the user. We conducted a case study of critical thinking essay assessment using a dataset of 500 essays and a four-dimensional assessment rubric, including automated experiments and additional human evaluation. The results show that CAELF matches GPT-40 in initial grading accuracy, while surpassing other baselines in interaction accuracy and two reasoning metrics. Additionally, in a separate human user study, we found CAELF's feedback effectiveness to be excellent in multiple aspects. This work demonstrates the significant potential of CAELF for applications in interactive learning environments, providing hope for overcoming the time and resource constraints that have historically hindered the widespread adoption of interactive feedback.\nLimitation CAELF's effectiveness in reasoning and maintaining consistency is motivated by the observation that LLM hallucinations often arise from conflicting knowledge embedded during training (Zhang et al. 2023). CAELF mitigates this issue by leveraging multi-agent discussions and user interactions to systematically identify and resolve inconsistencies. Through formal argumentative reasoning, CAELF invalidates conflicting knowledge within the LLM's responses. However, the success of this approach depends on the assumption that factual knowledge within the LLM outweighs factually incorrect or conflicting information. In cases where the LLM contains substantial conflicting knowledge about a specific domain, our method may exacerbate hallucinations, raising concerns about deploying CAELF in high-stakes environments, such as medical education.\nMoreover, recent studies have shown that LLMs can be manipulated through carefully designed jailbreak prompts, which can provoke arbitrary, user-desired responses (Wei, Haghtalab, and Steinhardt 2023). This presents significant challenges for the safe use of LLM-based automated evaluation tools in educational settings. Students could exploit such vulnerabilities by embedding jailbreak prompts in their submissions to manipulate LLMs into awarding favorable grades, a tactic that may go undetected by instructors.\nFuture Work In the future, we aim to enhance the safety and effectiveness of CAELF in interactive learning environments. While this work focuses on improving LLM performance in zero-shot settings, future efforts may incorporate techniques like RAG or Knowledge Graphs to align student submissions with reliable knowledge, reducing hallucinations and improving feedback quality. Additionally, addressing AI-driven cheating, such as detecting AI-generated submissions and defending against jailbreak prompt attacks, will be a key area of research."}]}