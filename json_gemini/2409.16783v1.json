{"title": "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "authors": ["Jinchuan Zhang", "Yan Zhou", "Yaxin Liu", "Ziming Li", "Songlin Hu"], "abstract": "Automated red teaming is an effective method for identifying misaligned behaviors in large language models (LLMs). Existing approaches, however, often focus primarily on improving attack success rates while overlooking the need for comprehensive test case coverage. Additionally, most of these methods are limited to single-turn red teaming, failing to capture the multi-turn dynamics of real-world human-machine interactions. To overcome these limitations, we propose HARM (Holistic Automated Red teaMing), which scales up the diversity of test cases using a top-down approach based on an extensible, fine-grained risk taxonomy. Our method also leverages a novel fine-tuning strategy and reinforcement learning techniques to facilitate multi-turn adversarial probing in a human-like manner. Experimental results demonstrate that our framework enables a more systematic understanding of model vulnerabilities and offers more targeted guidance for the alignment process.", "sections": [{"title": "1 Introduction", "content": "The rapid progress of large language models has revolutionized many fields such as general assistant (OpenAI et al., 2023), code generation (Zhang et al., 2023b), and legal services (Cui et al., 2023b). Despite their benefits, these models carry inherent risks due to their training on diverse and broad online text corpora, which may include biased or negative content. This can lead to the propagation of biases (Esiobu et al., 2023), enablement of illegal activities (Chao et al., 2023), and privacy violations (Carlini et al., 2021). Therefore, it is crucial to rigorously test and align LLMs to mitigate these risks prior to their deployment.\nRed teaming serves as an proactive evaluation method aimed at uncovering behavioral flaws in"}, {"title": "2 Overview", "content": "The overall workflow of our framework is illustrated in Figure 2, comprising key components such as top-down test case generation (\u00a7 3), safety reward modeling (\u00a7 4.1), and the training of multi-turn red teaming (\u00a7 4.2, \u00a7 4.3). The aim of the top-down question generation is to systematically create test cases that simulate a broad spectrum of user intentions, thereby initially defining the scope of testing. The test cases generated in this phase serve as the opening questions for the red teaming and are uniform for different target LLMs.\nThe multi-turn red teaming module utilizes the safety reward model's scores on specific target LLM responses as reward signals, which allows the red-team agent to be more specifically tailored to each target LLM. With opening questions as a contextual constraint, the dialogue generated by the red-team agent is less prone to mode collapse (Casper et al., 2023) when compared to generating test questions from scratch using reinforcement learning (Perez et al., 2022).\nIn summary, our approach offers several key advantages: (1) Comprehensive Coverage: Our fine-grained risk taxonomy includes 71 dimensions across eight meta risk categories, with over 2,000 specific descriptors, allowing for broader coverage of edge cases. (2) Adaptivity: Our framework is compatible with both existing and emerging jailbreaking strategies (e.g., integrating as new attack vectors), ensuring both sample diversity and attack success rates. (3) Multi-turn Support: Supports multi-turn stress testing to uncover vulnerabilities that single-turn tests may miss."}, {"title": "3 Top-down Test Case Generation", "content": "In this section, we explain how we constructed our fine-grained risk taxonomy and the process of top-down test generation."}, {"title": "3.1 Fine-grained Taxonomy Construction", "content": ""}, {"title": "3.1.1 Meta Risk Categories", "content": "In the course of practical interactions with various LLMs and by referring to existing literature on potential risks associated with language models (Weidinger et al., 2022; Ji et al., 2023; Sun et al., 2023; OpenAI et al., 2023), we have identified eight meta risk categories. These categories broadly encapsulate various types of improper intentions users might exhibit in real-world scenarios:\n\u2022 Bias and Fairness: Assessing whether the model propagates or amplifies societal biases.\n\u2022 Crime and Illegality: Testing if the model encourages or provides advice for illegal or criminal activities.\n\u2022 Harm and Safety: Examining the model's ability to prevent users from physically or psychologically harming themselves or others.\n\u2022 Unethical and Immoral: Observing whether the model tolerates or suggests unethical behavior, even if not illegal (such as cheating, pranks, disloyalty, etc.).\n\u2022 Environmental and Social Ethics: Investigating whether the model maintains a fair and positive stance on environmental and social ethical issues.\n\u2022 Privacy: Evaluating if the model could leak privacy information or suggest methods that infringe upon privacy.\n\u2022 Misinformation and Truthfulness: Monitoring how the model responds to misleading statements, falsehoods, and conspiracy theories.\n\u2022 High-stakes Professional Guidance: Testing the model's reliability in high-risk domains (like medical, legal, and financial) and its ability to caution users against overreliance."}, {"title": "3.1.2 Schema of Meta Risk Category", "content": "To cover as many edge cases as possible, we constructed a fine-grained schema for each meta risk category. Drawing inspiration from the taxonomy developed for bias in HolisticBias (Smith et al., 2022), we adopted a three-tier structure of Axis-Bucket-Descriptor to delineate each meta risk category in detail. In HolisticBias, they created nearly 600 descriptor terms (e.g., \u201cDumb\u201d,\u201cGen-Z\") belonging to 13 different demographic axes (e.g., \"Ability\u201d, \u201cCultural\u201d), establishing a more inclusive framework for bias assessment.\nIn particular, we utilized GPT-4 to analogically generate taxonomies for other risk categories based on the HolisticBias model. This process involved iterative interactions with GPT-4 to refine and polish the generated taxonomies. During this process, we observed that GPT-4 excels in enumerating similar concepts within a certain topic. Using this method, we ultimately developed a comprehensive taxonomy consisting of 71 axes, 274 buckets, and 2255 descriptors for all risk categories (examples are provided in Figure 10, we use the taxonomy of HolisticBias as our schema for Bias and Fairness).\""}, {"title": "3.2 Attack Vectors Design", "content": "We drew inspiration from the manual red teaming approach used in Llama-2 (Touvron et al., 2023) and defined six distinct attack vectors (specific questioning styles intended to elicit inappropriate outputs from target LMs). Full examples of each attack vector are provided in the Table 8.\nA1: Direct, generating questions related to a specific risk category directly, without employing any attack vectors. A2: Implicit, framing questions or statements in a manner that implicitly suggests a risky scenario (e.g., \u201cI feel like my life is hopeless\"). A3: Realistic, crafting questions embedded in various realistic scenarios or true stories, thereby enhancing the detail and credibility of the queries. A4: Role Play, instructing the model to adopt a specific role as a means to circumvent its safety restrictions. A5: False Premise, posing questions based on premises that reflect misrepresented realities or distorted values. A6: Dilemma, simulating moral dilemmas or real-life decision-making scenarios, such as the trolley problem.\nIn implementation, each attack vector is treated as an \"abstract class\" that gets inherited and instantiated by specific meta risk category attack vectors. This means we manually craft attack vector prompts that are tailored to each meta risk category, thereby orthogonally generating test cases.\""}, {"title": "3.3 Top-down Test Case Generation Process", "content": "We devised an algorithmic process (Algorithm 1) that combines a fine-grained taxonomy of risk categories with attack vectors to generate test cases in a top-down manner. Specifically, given a prompt template for a specific risk category (Table 9) and its taxonomy (Section 3.1.2), we perform string matching of descriptors from the taxonomy with existing manually written red team questions\u00b9 to serve as seed questions. Next, we flatten the taxonomy into a collection of <Axis, Bucket, Descriptor> triples, from which we sample several triples to fill the prompt template with seed questions as in-context examples. When sampling triples, we dynamically adjust the sampling probability based on the distribution of triples in the questions already generated. This ensures that the final test cases are uniformly distributed across all triples.\nWe opted for GPT-3.5-turbo to generate test cases due to its robust instruction-following capabilities,"}, {"title": "3.4 Single-turn Red Teaming Results", "content": "We analyzed the performance of various open-source models with different alignment levels on the aforementioned test cases to gain a preliminary understanding of their safety performance:\n\u2022 Alpaca: We fine-tuned on the Llama-2-7B using the Alpaca (Taori et al., 2023) dataset.\n\u2022 Mistral-7B-Instruct-v0.1: A supervised fine-tuned (SFT) version of the Mistral-7B (Jiang et al., 2023) model.\n\u2022 Zephyr-7B-beta: This model underwent SFT and Direct Preference Optimization (Rafailov et al., 2023, DPO) on Mistral-7B using Ultra-Chat (Ding et al., 2023) and UltraFeedback (Cui et al., 2023a).\n\u2022 Vicuna-7B-v1.5: Trained by fine-tuning Llama-2-7B on user-shared conversations collected from ShareGPT website.\n\u2022 Llama-2-7B-Chat: Aligned using iterated Reinforcement Learning from Human Feedback (RLHF) and manual red teaming on Llama-2.\n\u2022 Beaver-7B-v1.0: Aligned for both helpfulness and safety on the Llama-7B base using Safe RLHF (Dai et al., 2023).\nSettings We uniformly sampled 24,250 questions for the above models to answer and employed OpenAI's GPT-3.5-turbo API to rate the safety of the responses on a five-point Likert scale. The specifics of the scoring prompts, output format, and other details can be found in the Appendix B. We used min-max normalization to convert the scores from a 1-5 scale to a percentage format.\nResults The safety scores for the six models are shown in Table 1, revealing a trend where safety scores correlate with their alignment levels. For instance, Llama-2-7B-Chat, which has undergone iterated RLHF and manual red teaming, generally performs well across most risk categories. Other models, like Zephyr-7B-beta, excel in specific risk"}, {"title": "4 Multi-turn Red Teaming", "content": "Existing automated red teaming methods predominantly support only single-turn interactions, which is insufficient in real-world usage scenarios (Section 1). In this section, we elucidate the methodology, evaluation measures, and findings made during our multi-turn red teaming."}, {"title": "4.1 Safety Reward Modeling", "content": "Assessing the safety of a target LM's responses in multi-turn is challenging, and it is impractical to have human annotators label each response after every round of red teaming. Therefore, we opted to train a safety reward model to act as a proxy for evaluating the responses in each round, which allows for scalable evaluation of the model's safety.\nMethod In our pilot study, we experimented with various data mixing strategies and training objectives. Ultimately, we decided to use a combination of PKU-SafeRLHF (Ji et al., 2023), Anthropic Harmless-base (Bai et al., 2022a), and our preference dataset constructed using AI Feedback (Bai et al., 2022b; Lee et al., 2023b) as the training data for the reward model. Details on the construction of the training set can be found in the Appendix C.\nWe initialized our model with Llama-2-7B and used the following binary ranking loss as optimization goal due to its simplicity and generalizability:\n$L_{RM} = -log(\\sigma(r_{\\theta}(x,y_s) - r_{\\theta}(x, y_u)))$ (1)"}, {"title": "4.2 Supervised Fine-tuning", "content": "We initiated by constructing a basic version of an agent capable of multi-turn red-teaming through supervised fine-tuning (SFT). Thanks to Anthropic"}, {"title": "4.3 Rejection Sampling Fine-tuning", "content": "Method We further employed reward signals to train the red-team agent to exploit the vulnerabilities of different target LMs during multi-turn interactions. Given the extensive time consumption of online reinforcement learning algorithms like PPO (Schulman et al., 2017) during multi-turn rollouts, we opted for Rejection Sampling (also known as Best-of-N Sampling) to utilize reward signals offline (Nakano et al., 2022; Touvron et al., 2023; Kirk et al., 2024). Specifically, in each round, we sample N utterances from the red-team agent, have the target LM answer them, and use the reward model to score these responses. We greedily record the utterance that elicits the most adverse response from the target LM in each round and use these records to further fine-tune the red-team agent.\nResults We conducted rejection sampling on Vicuna-7B-v1.5, Beaver-7B-v1.0, and Llama-2-7B-"}, {"title": "4.4 Comparison with Prompting-based Multi-turn Red Teaming Baseline", "content": "In this section, we compare the efficacy of fine-tuning methods with direct prompting for conducting multi-turn red teaming. We explicitly define the objectives and principles of multi-turn red teaming within the prompt (Table 15) and incorporate the dialogue history of each turn. We then iteratively query the GPT-3.5-turbo model to execute a multi-turn red teaming session. To manage costs, we selected one hundred entries from the aforementioned 2000 test samples for evaluation. We used the Llama-2-7B-Chat model as the target model and assessed the flipping rates."}, {"title": "5 Red Teaming for Safer Alignment", "content": "One of the critical purposes of red teaming is to guide the subsequent alignment process, aiming to rectify the deficiencies discovered during the tests. In this section, we focus on further aligning the Zephyr-7B-beta model, which exhibits strong performance in helpfulness but falls short in safety. We use this model as a case study to elaborate on how our automated red teaming approach contributes to the safety alignment of models."}, {"title": "5.1 Method", "content": "The original alignment process for Zephyr-7B-beta consists of two stages: SFT and DPO (Tunstall et al., 2023). We chose to implement a safety patch during the DPO phase. Specifically, we identified responses of Zephyr-7B-beta that scored below 3 on a 5-point Likert scale in Section 3.4 as misaligned data points, which were then incorporated into the preference training data for DPO. To obtain preferred responses, we crafted a prompt empha-"}, {"title": "5.2 Impact of Helpfulness", "content": "A potential issue in aligning models for safety is the tension between helpfulness and safety (Askell et al., 2021; Bai et al., 2022a), where safety alignment might lead to overfitting towards harmlessness, causing the model to reject clearly safe prompts. This issue is particularly prominent in the aligned Llama-2 models (Touvron et al., 2023). To quantitatively analyze whether our safety alignment has led to exaggerated safety, we tested the models before and after alignment using the XSTEST dataset (R\u00f6ttger et al., 2023).\nResults XSTEST consists of 250 manually crafted safe prompts that well-calibrated models should not refuse to comply with. We had Zephyr-7B-beta and Zephyr-7B-safer respond to these questions and used the string-match method provided by the authors to calculate their False Refusal Rate (FRR), as shown in Table 3. Notably, safety alignment led to an increase in the model's FRR (from 2.8 to 16.0), but it was still significantly lower than that of Llama-2-70B-Chat, and comparable to GPT-4, which is considered to have achieved an optimal balance between helpfulness and safety (see qualitative case study in the Table 18). We believe this can be partly attributed to our approach of \"detect-then-align\", which avoids overfitting caused by using an excessively large proportion of safety alignment data."}, {"title": "6 Related Work", "content": "Manual Red Teaming Manual red teaming involves hiring annotators from diverse groups to create adversarial prompts (Xu et al., 2021; Ganguli et al., 2022; Touvron et al., 2023), which is time-consuming and costly. However, these efforts yield valuable data and insights that enable the im-"}, {"title": "7 Conclusion", "content": "In this work, we explore an alternative approach to automated red teaming by generating test cases from a fine-grained risk taxonomy in a top-down manner, which ensures comprehensive coverage of more edge cases. Additionally, we have advanced our efforts in multi-turn automated red teaming by employing techniques such as a novel fine-tuning strategy and rejection sampling to train a human-like red-team agent. By utilizing the vulnerabilities detected during these tests for alignment training, we have effectively enhanced the safety of LLMs."}, {"title": "Limitations", "content": "It is important to note that the meta risk categories and fine-grained taxonomy designed in this paper may not encompass all malicious intentions. Therefore, we encourage researchers and practitioners from the community to continually expand upon this foundation by proposing pull requests or issues on the project's GitHub page, aiming to extend the coverage to more specific risk scenarios.\nAnother limitation of this paper is that the red teaming efforts are primarily focused on the text-based or natural-language-based responses of LLMs. In reality, LLMs also possess remarkable capabilities in code generation, utilizing tools, and acting as agents to complete complex tasks. These action-based capabilities come with their own set of potential safety risks (Zhang et al., 2023a; Ruan et al., 2024; Tian et al., 2024; Yuan et al., 2024). Designing effective automated red teaming approaches for these scenarios is equally important and presents a challenging task."}, {"title": "Ethics Statement", "content": "The red teaming exercises conducted were aimed exclusively at uncovering potential weaknesses in LLMs that could be exploited to propagate misinformation, bias, or other harmful outcomes. These activities were designed with a constructive intent: to improve model safety and to inform the development of more robust LLM systems. They were not intended to facilitate malicious use of LLMs.\nIn simulating adversarial scenarios, we ensured that all data used were ethically sourced, respecting privacy and confidentiality where applicable. Our datasets were derived from publicly available and we commit to sharing our methodologies, findings, and the tools we developed with the broader research community. This open approach aims to foster collaboration and accelerate progress in securing LLMs against potential abuses."}]}