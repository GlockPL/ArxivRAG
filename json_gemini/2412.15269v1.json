{"title": "The Reliability Paradox: Exploring How Shortcut Learning Undermines Language Model Calibration", "authors": ["Geetanjali Bihani", "Julia Rayz"], "abstract": "The advent of pre-trained language models (PLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found PLMs to suffer from miscalibration, indicating a lack of accuracy in the confidence estimates provided by these models. Current evaluation methods for PLM calibration often assume that lower calibration error estimates indicate more reliable predictions. However, fine-tuned PLMs often resort to shortcuts, leading to overconfident predictions that create the illusion of enhanced performance but lack generalizability in their decision rules. The relationship between PLM reliability, as measured by calibration error, and shortcut learning, has not been thoroughly explored thus far. This paper aims to investigate this relationship, studying whether lower calibration error implies reliable decision rules for a language model. Our findings reveal that models with seemingly superior calibration portray higher levels of non-generalizable decision rules. This challenges the prevailing notion that well-calibrated models are inherently reliable. Our study highlights the need to bridge the current gap between language model calibration and generalization objectives, urging the development of comprehensive frameworks to achieve truly robust and reliable language models.", "sections": [{"title": "1. Introduction", "content": "Pre-trained language models (PLMs) have become the convention in the field of natural language processing. The preference for PLMs can be attributed to their improvements in a wide variety of tasks, including question answering, textual entailment, sentiment analysis, and commonsense reasoning (Devlin et al., 2019; Peters et al., 2018; Sap et al., 2020). The 'pretrain-then-fine-tune' paradigm allows the model to not only utilize the existing \u2018knowledge' gained during pre-training but also learn from task-specific data via fine-tuning (Alt et al., 2019; Q. Chen et al., 2019).\nAlthough fine-tuning PLMs achieves state-of-the-art results, it also causes models to lack generalization and become unreliable predictors. Specifically, PLMs tend to learn shortcuts based on keywords (Du et al., 2021; Moon et al., 2021) and cues related to language variations (Nguyen et al., 2021) to make predictions. This behavior, also known as shortcut learning, leads the model to learn non-generalizable decision rules that do not perform well on out-of-distribution (OOD) data (Du et al., 2022; Moon et al., 2021). Additionally, the fine-tuning process can lead to overconfidence in PLMs (Jiang et al., 2021; Kong et al., 2020), where their confidence increases regardless of the accuracy of their predictions (Y. Chen et al., 2022). This mismatch between the model's confidence and its actual accuracy in its predictions results in 'miscalibration' in language models.\nIt is desirable for language models to perform reliably and accurately across different language tasks. Addressing calibration is essential because it ensures that the model's confidence aligns more accurately with its predictive accuracy. Miscalibrated models can lead to significant issues, particularly in high-stakes environments where wrong but confident predictions are dangerous. By focusing on calibration, we can reduce the mismatch between confidence and correctness, improving the model's trustworthiness and robustness across diverse tasks and data distributions. Thus, it"}, {"title": "2. Shortcut Effects on Calibration", "content": "is important to study the interplay between model generalization and calibration. Prior works assessing model calibration focus on measuring and minimizing statistical calibration evaluation metrics such as Expected Calibration Error (ECE) (Ahuja et al., 2022; Kim et al., 2023; Kong et al., 2020). These works do not investigate whether lower calibration error estimates align with more generalizable decision rules learned by these language models.\nIn this study, we aim to address this gap and conduct the following research inquiries: 1) Does a reduction in calibration error within language models indicate a decrease in overconfident predictions? 2) Can a model exhibiting lower calibration error be considered reliable in terms of its decision rules? By examining these questions, we seek to shed light on the relationship between calibration error and the reliability of language models' decision rules. Our questions are based on the intuition that model reliability estimates should account for the reliability of the model's decision rules.\nTo answer these questions, we investigate the calibration and shortcut learning behaviors of recent pre-trained language models (PLMs) across a suite of binary and multi-class classification tasks, and analyze the evolution of shortcut learning behaviors in PLMs before and after fine-tuning.\nOur research findings highlight that models appearing to be well-calibrated often exhibit a higher propensity for shortcut learning. This challenges the conventional notion of well-calibrated models as reliable and robust. While lower calibration error estimates, such as Expected Calibration Error (ECE), may indicate the improved alignment of prediction probabilities with their actual correctness, they fail to capture the inherent lack of robustness in these 'correct' model decisions. This observation uncovers a fundamental discrepancy between the requirements of model calibration and the goals of generalization, highlighting the need to reconcile these seemingly contradictory frameworks in order to achieve truly robust and reliable language models."}, {"title": "2.1. Identifying Shortcuts", "content": "Shortcut learning refers to the phenomenon where models rely on superficial cues in the training data to make predictions instead of learning the underlying semantics to perform an NLU task. This over-reliance on specific features or biases results in poor generalization in out-of-distribution (OOD) settings. Identifying shortcut learning in language models is an ongoing research area, with recent works utilizing model attention, dataset statistics, and human annotated samples to identify spurious correlations (Moon et al., 2021; T. Wang et al., 2022). We utilize the shortcut identification framework as described by (Du et al., 2021), which combines data statistics with model attributions to identify shortcuts. We describe this shortcut identification framework below.\nModel Attribution based Importance: To obtain attributions for each token (wj) in a given sample Si, we utilize integrated gradients (IG) (Sundararajan et al., 2017). Let a given sample S\u2081 contain T tokens, i.e. $S_i = \\{w_j\\}_{j=1}^{T}$. We conduct a step-wise perturbation of the sample, creating m intermediate samples along a straight-line path from a baseline St to the actual sample Si. By observing the changes in the model's output as the sample is progressively modified, we quantify the contribution of each token to the final prediction. Following (Du et al., 2021), we consider all-zero embeddings to form St. As each word is added to the baseline, the gradient of the prediction M(Si) is computed with respect to the associated token embeddings (e(wj)) obtained from the output embedding layer of model M. The following equations summarize this gradient calculation.\n$IG(S_i) = \\sum_{k=1}^{m}\\frac{\\delta M_\\theta \\left( S_t + \\frac{k}{m}(S_i-S_t)\\right)}{\\delta S_i}$ (1)\nwhere\n$S_i^k = S_i - S_t$ (2)\nFinally, the L2 norm between the gradient and the corresponding token embedding is calculated to determine the individual contribution of each token. Following (Du et al., 2021), we filter top three attributed tokens per sample."}, {"title": "2.2. Types of Shortcuts", "content": "Within the broader context of language structure, a well-established theoretical framework proposed by (Chomsky, 1965) introduced a fundamental division between the lexicon and grammar. According to this framework, the lexicon serves as a repository for language words, while grammar establishes rules for combining these words. Drawing upon these foundational concepts, we divide the identified shortcuts into two categories, i.e. lexicon-cued' and grammar-cued' predictions. PLM predictions where at least one lexical word is utilized are classified as lexicon-cued' predictions. On the other hand, predictions where the identified shortcuts are limited to functional words and punctuations, are labeled as 'grammar-cued' predictions. The purpose of this categorization is to improve our understanding of the shortcut mechanisms employed by the models in their prediction processes, particularly in terms of their lexical-semantic processing.\nFor a more fine-grained analysis, we differentiate between cases where a model exclusively relies on punctuation, stopwords, or sub-words for making predictions and cases where it additionally incorporates one or more lexical words. This distinction allows us"}, {"title": "2.3. Measuring Calibration", "content": "A well-calibrated model should provide accurate probability estimates that reflect the true likelihood of an event. To quantify model calibration, we measure the Expected Calibration Error (ECE) (Naeini et al., 2015). We choose ECE because it captures the discrepancy between the model's confidence and accuracy, and has been used for calibration analysis, making it an ideal choice for evaluating the model's reliability across a range of tasks. This metric calculates the weighted average of the difference between the accuracy of a model and its average confidence level over a set of bins defined by the predicted probabilities, as shown in Eq. 4, where n is the number of samples in Bm.\n$ECE = \\sum_{m=1}^{M} \\frac{|B_m|}{n} |acc(B_m) - conf(B_m)|$ (4)\nHere, the estimation of expected accuracy from finite samples is done by grouping predictions (pi) into M interval bins (each of size), and the accuracy of each bin is calculated. Let Bm be a bin containing samples whose prediction confidence lies within the interval $I_m = (\\frac{m-1}{M}, \\frac{m}{M}]$. Then the accuracy of Bm, where Yi and yi portray predicted and true class labels, is calculated as shown in Eq. 5.\n$acc(B_m) = \\frac{1}{|B_m|} \\sum_{i\\in B_m} 1(Y_i = y_i)$ (5)\nThe average predicted confidence of Bm, is calculated as shown in Eq. 6.\n$conf(B_m) = \\frac{1}{|B_m|} \\sum_{i\\in B_m} p_i$ (6)"}, {"title": "2.4. Measuring trade-offs", "content": "In order to investigate the relationship between model calibration and shortcut learning, we calculate two metrics: the portion of shortcut-cued model predictions (Psc) and the shortcut trade-off (Tsc). The calculation of Psc allows us to quantify the extent to which a model relies on shortcuts when making predictions. Additionally, we introduce Tsc as a metric to assess the trade-off between shortcut learning and model performance. Tsc is calculated as the ratio of task accuracy (e.g., F1 score) to the proportion of shortcut-cued predictions.\n$T_{sc} = \\frac{\\text{Task Accuracy (F1)}}{\\text{Shortcut-Cued Predictions (Psc)}}$ (7)\nA higher Tse score indicates that the model achieves better task accuracy while relying on fewer shortcut-cued predictions. Conversely, a lower Tse score suggests a higher reliance on shortcuts to achieve optimal task performance. In our analysis, we aim to maximize Tsc and minimize Expected Calibration Error (ECE) in order to identify models that strike a balance between shortcut learning and accurate predictions."}, {"title": "3. Experiments", "content": "Datasets. To evaluate PLM shortcut learning and calibration effects across different tasks and domains, we perform our evaluation on several binary and multi-class classification tasks. Specifically, we consider 8 text classification datasets, briefly described as follows: i) Stanford-Sentiment Treeback (SST-2) (Socher et al., 2013), commonly used in sentiment analysis tasks and provides a valuable benchmark for evaluating models' ability to capture sentiment in texts, ii) Corpus of Linguistic Acceptability (COLA) (Warstadt et al., 2019), which assesses model performance on grammaticality judgments, iii) TREC (coarse-grained) (Hovy et al., 2001) used for question classification, iv) AG News (Zhang et al., 2015) used for news topic classification, and four datasets from TweetEval benchmark (Barbieri et al., 2020) [Emotion, Hate, Irony, and Sentiment] for text classification in the context of short and informal social media texts. These datasets present different challenges, such as the presence of sarcasm and negation in samples for sentiment tasks, lexical overlap in topic classification tasks, and the inclusion of short and ironic social media texts in irony detection tasks. This language and task variation across datasets allows for a holistic assessment of the models' performance and generalization of our findings in the context of PLM calibration literature.\nModels. We evaluate five pre-trained transformer language models for evaluation: BERT (Devlin et al., 2019), ROBERTa (Liu et al., 2019), DeBERTa (He et al., n.d.), ALBERT (Lan et al., n.d.) and BART (Lewis et al., 2020). We choose BERT and RoBERTa to align our results with prior PLM calibration research (Desai & Durrett, 2020a; Kim et al., 2023). We additionally evaluate more recent transformer LMs including DeBERTa and ALBERT. DeBERTa improves model generalization on downstream tasks, compared to BERT and RoBERTa, attributed to its disentangled attention mechanism. ALBERT is a compact architecture, providing performance gains with minimal sacrifice of task performance. Finally, we include BART, due to its improvements in handling of global context and robustness in handling noisy and ambiguous texts.\nMetrics. To investigate the association between shortcut learning effects and model calibration, we employ multiple evaluation metrics. We utilize F1 score to assess the overall prediction performance of PLMs on given tasks. Additionally, we measure the Expected Calibration Error (ECE) (Naeini et al., 2015), as described in Section 2.3.\nWe also examine the distribution of shortcuts across correct and incorrect predictions using Psc and Tsc. These measures allow us to gain insights into the relationship between shortcut utilization and prediction accuracy. By analyzing how shortcuts are distributed across different prediction outcomes, we can explore their impact on the model's ability to classify accurately.\nTraining Configurations. Following prior work (Kim et al., 2023), we fix several hyperparameters for the model fine-tuning process. For all models, we set the initial learning rate to le-5 and gradient clip to 1.0. We"}, {"title": "4. Results & Discussion", "content": "Comparison Across Models and Tasks: For all models in our analyses, we find more than chance (> 50%) shortcut learning for every task. Shown in Table 3, we observe a negative relationship between model calibration and shortcut trade-off (Tsc), i.e models considered more calibrated in terms of expected calibration error also tend to rely more on shortcuts when making predictions. This finding highlights that metrics like Expected Calibration Error (ECE), which assess statistical model calibration, do not align with the model's robustness in terms of learning fewer spurious correlations. Across models, we find that BERT and ROBERTa rely on more shortcut-cued predictions but appear statistically more calibrated. In contrast, DeBERTa and BART rely on fewer shortcuts but appear statistically less calibrated.\nShortcuts Learned: Across various datasets, we observe a notable difference in the extent of shortcut learning. Models make more lexicon-cued predictions on datasets such as AG News, SST2 and TREC, while more grammar-cued predictions are made on Hate and Irony datasets.\nFine-tuning Effects: We evaluate changes in shortcut learning (Psc), task performance (F1), and calibration (ECE) in models due to fine-tuning. Figure 2 shows the changes observed in classification performance before and after fine-tuning PLMs. We observe that fine-tuning does not always lead to calibration improvements, which aligns with prior findings (Jiang et al., 2021; Kong et al., 2020). Note that models become increasingly miscalibrated on Hate and SST2 tasks, while showing improved calibration for AG News, attributed to the increased accuracy due to shortcut learning of models on AG News. We also find that shortcut learning reduces after fine-tuning for some models, especially on COLA, which we attribute to the words appearing across a wider variety of contexts in its samples, as the dataset is not constricted to specific topics or affect statements.\nShortcut Impacts on Model Confidence: While fine-tuning models results in increased confidence on correct as well as incorrect predictions, we focus on instances where the calibration error estimates such as ECE are unable to capture the underconfident predictions, i.e. predictions that are correct, but less confident than average. In Figure 3, we plot reliability diagrams and shortcut-cued prediction distributions across model confidence for DeBERTa on two tasks, i.e. TREC and AG News. While DeBERTa portrays similar F1 and ECE for both tasks, we observe that the confidence and shortcut distributions are starkly different. Specifically, DeBERTa predictions on TREC are underconfident in many cases, while on AG News, the model confidence aligns with the correctness of model decisions. Further, while DeBERTa heavily relies on shortcuts for both the tasks, TREC relies more on grammar-cues, while AG News predictions are more often lexicon-cued. This discrepancy in model confidence and shortcuts utilized per confidence bin is not captured in statistical calibration error metrics like ECE. It is crucial to highlight that statistical calibration error metrics like ECE fail to capture the divergence in model confidence and the specific shortcuts employed within confidence bins. While ECE may provide an overall assessment of model calibration, it falls short in capturing the complex interplay between confidence, shortcuts, and their impact on prediction reliability.\nIs minimizing ECE enough? We discover that ECE is not a dependable metric, and can be low even when the model is highly overconfident. Thus, a lower ECE does not necessarily indicate more reliable predictions by a model. To illustrate this, let's consider the results presented in Table 3, specifically for the Hate and AG News tasks. Both tasks demonstrate significant levels of shortcut learning in language models. However, the shortcuts learned in the AG News task lead to more accurate predictions, whereas shortcuts learned in the Hate task result in more incorrect predictions,"}, {"title": "5. Related Work", "content": "Shortcut Learning General-purpose neural language models have been shown to learn spurious patterns existing within natural language text, due to the language variety cues within the training corpora (Nguyen et al., 2021). While initial research claimed that pre-trained language models are robust to out-of-distribution (OOD) detection and cross-domain generalization (Hendrycks et al., 2020), recent analyses have shown that PLMs, and their fine-tuned versions rely on specific keyword-based shortcuts to perform classification (Moon et al., 2021). This phenomenon hinders the fine-tuned models from learning generalizable decision rules. PLMs have also been shown to rely on syntactic heuristics to perform natural language inference tasks (McCoy et al., 2019). In light of these findings, research focusing on the automatic identification and mitigation of spurious cues within training and fine-tuning data has also been proposed (Tu et al., 2020; T. Wang et al., 2022; Z. Wang & Culotta, 2020).\nCalibration in Neural Language Models With the increased application of neural network architectures in high-risk real-world settings. their calibration has become an extensively studied topic in recent years (Hendrycks et al., 2020; Malinin & Gales, 2018; Thulasidasan et al., 2019). Recent research has focused on improving the calibration of neural networks, particularly in the context of deep learning. Various methods have been proposed to achieve better calibration, including temperature scaling (Guo et al., 2017), isotonic regression (Platt et al., 1999), and histogram binning (Zadrozny & Elkan, 2001).\nPre-trained language models have garnered attention due to their tendency to exhibit increasing confidence during training, regardless of the accuracy of their predictions (Y. Chen et al., 2022). However, these models demonstrate better calibration within in-domain (ID) settings while experiencing calibration deterioration in out-of-domain (OOD) scenarios (Desai & Durrett, 2020b). Interestingly, it has been observed that smaller"}, {"title": "6. Conclusion", "content": "The prevailing belief in existing calibration evaluations of pre-trained language models is that lower calibration error estimates indicate more reliable predictions. However, it has been shown that fine-tuned PLMs often rely on shortcuts to produce overly confident predictions, creating an illusion of improved performance while actually learning decision rules that lack generalizability. The relationship between model reliability, as measured by calibration error, and shortcut learning has received limited attention thus far. This prompts us to question whether a model with lower calibration error can truly be considered reliable in terms of its decision rules. Our findings challenge the prevailing notion by revealing that models with seemingly better calibration also exhibit higher levels of shortcut learning. This highlights the need to bridge the current gap between language model calibration and generalization objectives and underscores the importance of developing comprehensive frameworks to achieve genuinely robust and reliable language models."}]}