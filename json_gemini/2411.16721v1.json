{"title": "Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks", "authors": ["Han Wang", "Gang Wang", "Huan Zhang"], "abstract": "Vision Language Models (VLMs) can produce unintended and harmful content when exposed to adversarial attacks, particularly because their vision capabilities create new vulnerabilities. Existing defenses, such as input preprocessing, adversarial training, and response evaluation-based methods, are often impractical for real-world deployment due to their high costs. To address this challenge, we propose ASTRA, an efficient and effective defense by adaptively steering models away from adversarial feature directions to resist VLM attacks. Our key procedures involve finding transferable steering vectors representing the direction of harmful response and applying adaptive activation steering to remove these directions at inference time. To create effective steering vectors, we randomly ablate the visual tokens from the adversarial images and identify those most strongly associated with jailbreaks. These tokens are then used to construct steering vectors. During inference, we perform the adaptive steering method that involves the projection between the steering vectors and calibrated activation, resulting in little performance drops on benign inputs while strongly avoiding harmful outputs under adversarial inputs. Extensive experiments across multiple models and baselines demonstrate our state-of-the-art performance and high efficiency in mitigating jailbreak risks. Additionally, ASTRA exhibits good transferability, defending against both unseen attacks at design time (i.e., structured-based attacks) and adversarial images from diverse distributions.", "sections": [{"title": "1. Introduction", "content": "Vision Language Models (VLMs) [7, 10, 24, 55] have attracted significant attention from both the industry and academia for their remarkable vision-language cognition capabilities [35]. Despite widespread applications, VLMs still face safety challenges due to limitations inherent in their underlying language models. Moreover, integrating visual inputs can open up a new surface for adversarial attacks. These safety issues regarding VLM have led to a lot of research on jailbreak attacks and defense strategies [14, 42, 48, 56].\nJailbreak attacks in VLMs aim to induce models to generate harmful responses by using jailbreaking image-text pairs [19, 20, 22, 37, 43, 46]. These jailbreak attacks can be categorized into two types: (i) perturbation-based attacks, which create adversarial images that prompt generation of the harmful response from VLMs [2, 34, 37, 41], (ii) structured-based attacks, which embeds the malicious queries into images via typography to bypass the safety alignment of VLMs [14, 26]. Countermeasures for both attacks have been explored extensively: the input preprocessing-based method [33] or adversarial training [21] have proven effective for perturbation-based attacks. However, these defenses suffer as they require intensive computations to purify the image or fine-tune the model. Response evaluation-based [15, 48, 53] defenses have been proposed for structured-based attacks, but they all require running model inference multiple times to potentially identify harmful outputs, which dramatically increases the cost of real-world deployment.\nIn this work, we argue that an efficient defense framework should not require significant computational resources during training or generating responses multiple times during inference. Drawing inspiration from recent advancements in activation steering in Large Language Model (LLM) [4, 17, 39, 47], we propose ASTRA, an efficient and effective defense by adaptively steering models away from adversarial feature directions via image attribution activations to resist VLM attacks. We find that simply borrowing the method from steering LLM for safeguarding VLM is not empirically workable due to the mismatch between the steering vectors obtained from textual and visual data, which necessitates our image attribution approach.\nSpecifically, ASTRA consists of two steps: constructing steering vectors via image attribution, and adaptive activation steering at inference time. We seek to construct steering vectors representing the direction of harmful responses. This can be done by constructing a set of adversarial images (e.g., using projected gradient descent (PGD) [30] al-"}, {"title": "2. Related Work", "content": "Jailbreak Attacks on VLM. Jailbreak attacks aim to alter the prompt to trick the model into answering forbidden questions. Apart from the LLM-based textual jail-break strategies [16, 27, 51, 58], additional visual inputs expose a new attack surface to VLM attacks. There are two main types of attacks: perturbation-based attacks and structured-based attacks [48]. Perturbation-based attacks create adversarial images to bypass the safeguard of VLMs [2, 6, 37, 41, 50, 54]. Structued-based attacks convert the harmful content into images through typography or text-to-image tool (e.g., Stable Diffusion [40]) to induce harmful responses from the model [14, 23, 25, 26, 29] We study our defense on both types of attacks.\nDefenses on VLM. Researchers have explored two directions for defense: training-time alignment and inference-time alignment. Training-time alignment safeguards VLMs through supervised fine-tuning (SFT) [8, 22, 56] or training a harm detector to identify the harmful response [36], all requiring considerable high-quality annotation and sufficient computation resources to train. Inference-time alignment is relatively more resource-friendly. Some strategies design alignment prompts to defend against attacks [14, 49]. Others build a response evaluation pipeline to assess the harmfulness of VLM responses, often followed by iterative refinement to ensure safe outputs [15, 53]. Another way is to disturb input queries and analyze response consistency to identify potential jailbreak attempts [53]. However, these methods still introduce a non-trivial cost to inference time due to the need for generating the response multiple times.\nActivation Engineering of LLM. The activation space of many language models appears to contain interpretable directions, which play a crucial role during inference [5, 32]. The basic idea of activation engineering is to identify a direction (i.e., steering vector) in activation space associated with certain semantics and then shift activations in that direction during inference. Turner et al. [45] locates the direction by taking the difference in intermediate activations of a pair of prompts at a particular layer and token position in a transformer model. Rimsky et al. [39] construct a dataset of contrast pairs rather than using a single pair to get the steering vector. Wang et al. [47] locate the \"safety\" steering vectors from a well-aligned language model. Ball et al. [4] investigate whether different types of jailbreak templates employ distinct mechanisms to trigger unsafe regions in the model's representation space. Some other methods try to learn high-level concepts in the representation space and use them to control the output [17, 57, 59]. However, most previous works focus on utilizing textual prompts to construct steering vectors, which might not be empirically workable for steering VLM in some cases due to the gap between visual and textual domains."}, {"title": "3. Methodology", "content": "In this work, we propose ASTRA, an efficient and effective defense by adaptively steering (Section 3.2) models away from adversarial directions via image attribution activations (Section 3.1) to resist VLM attacks.\nNotation. Let \\(P_{VLM}\\) be an autoregressive vision language model, which defines a probability distribution over a sequence of preceding tokens from a vocabulary \\(V\\). Specifically, we consider a VLM which takes a sequence of \\(n\\) textual tokens \\(x_t = \\{x_{t_1},..., x_{t_n}\\}\\) and \\(m\\) visual tokens \\(x_v = \\{x_{v_1}, ..., x_{v_m} \\}\\) to generate responses \\(r = \\{r_1,..., r_o\\}\\)."}, {"title": "3.1. Constructing Steering Vectors", "content": "Not all visual tokens from the adversarial images contribute to the jailbreak equally. We seek to locate certain visual tokens that have a higher chance of inducing jailbreaking via image attribution. In this way, we can isolate the representation most associated with jailbreak-related information in these tokens.\nAdversarial Image Attribution. Image attribution aims to find the input visual tokens that are more likely to trigger the specified responses. In our case, we seek to locate visual tokens from adversarial images generated by the PGD attack with a higher chance of inducing the jailbreak.\nWe conduct random ablation of certain tokens and compute the impact of exclusion/inclusion on inducing the jailbreak. We define visual token ablation as the process of masking specific tokens in a visual input. Let Ablate(x, g) represent ablated visual tokens x, where \\(g \\sim \\{0,1\\}^m\\) is an ablation vector that designates which tokens to mask (zeros in g indicate masked tokens). Given an ablation vector g, the image attribution is expected to quantify the impact on the log probability of generating specified responses r,\n\\[f(g) := logP_{VLM}(r|Ablate(x_v, g), x_t),\\]\nchanges as a function of g, where \\(x_t\\) are textual tokens of harmful instructions, \\(r\\) as tokens of \"Sure, ...\" to denote the case of jailbreaking, and \\(P_{VLM}(r | Ablate(x_v, g), x_t)\\) as the product of the probability of generating specified response r given the Ablate(xv, g), xt.\nFollowing prior work in machine learning explanation [9, 38], we fit a linear surrogate model \\(f\\) to analyze the influence of masking subsets of visual tokens on the likelihood of jailbreaks and select the visual tokens that are highly relevant for triggering the jailbreaking responses. Specifically, we (1) sample a dataset of ablation vectors \\(g_1,..., g_n\\) and compute \\(f(g_i)\\) for each \\(g_i\\) by multiple times of ablations and forwards, (2) train the surrogate model"}, {"title": "3.2. Adaptive Activation Steering", "content": "The key idea of activation steering is using steering vectors to shift a language model's output distribution toward a specified behavior during inference. After constructing steering vectors with harmful semantics, we strive to remove these components by steering LLM's activations.\nUnfortunately, simply applying a fixed scaling coefficient to the steering vector for modifying the language model's output [4, 39, 45, 47] is not workable as a defense due to dramatic utility performance degradation in benign cases [1]. The main problem is that the linear steering used in prior work unconditionally alters the activation no matter whether the input leads to harmful outputs or not (Fig. 2(a)):\n\\[h' = h - \\alpha \\cdot \\frac{v^l}{\\|v^l\\|},\\]\nwhere \\(h^l\\) is the activation of the last token at the layer l, and \\(\\alpha\\) is a scaling coefficient. To address this challenge, we propose adaptive steering based on conditional projection:\n\\[h' = h - amax(\\frac{(h^l)^T v^l}{\\|h^l - h_o\\|\\|v^l\\|}, 0) \\cdot \\frac{v^l}{\\|v^l\\|},\\]\nwhere \\(h^l\\) does not contain any positive component of the steering vector (harmful direction), the max term is 0, leaving activations unchanged. This minimized the negative impact on the benign performance.\nSince the angle between \\(h^l\\) and \\(v^l\\) matters for adaptive projection, we must ensure that it can effectively distinguish harmful and benign activations at layer l. However, we notice that the activations for different inputs may cluster around a point distant from the origin. As a result, the angles among these vectors may all become similar (Fig. 2(b)). To address this, we propose a activation calibration step before steering. We use the calibration activation \\(h_o\\), which can be seen as the center of the activation for many different inputs, to calibrate the projection term in our adaptive steering:\n\\[h' = h - amax(\\frac{(h^l - h_o)^T v^l}{\\|h^l - h_o\\|\\|v^l\\|}, 0) \\cdot \\frac{v^l}{\\|v^l\\|},\\]\n\\(h_o\\) is the calibration activation at the layer l, \\(h^l - h_o\\) is the calibrated activation. We do not calibrate \\(v^l\\) here since the mean component has been canceled out when subtracting the two token activations. To obtain the calibration activation \\(h_o\\), we collect image-text queries from a large number of test data and compute the average of the generated token features at the layer l to get ho.\nWe show the full process of our adaptive steering approach in Fig. 2 (c1) - (c3). It can help reduce malicious outputs in adversarial scenarios while preserving performance in benign cases. During inference, we apply steering only to the activations of newly generated tokens, leaving the activations of input tokens unaltered."}, {"title": "4. Experiments", "content": "In this section, we conduct experiments to address the following research questions:\n*   RQ1: How does ASTRA perform in adversarial scenarios compared to VLM defense baselines and LLM steering methods? Is our defense transferable to a different distribution of inputs and different types of attacks?\n*   RQ2: How does ASTRA perform in benign cases? Can we reduce model harmfulness without hurting utility?\n*   RQ3: What are the impacts of design choices in ASTRA? Are all components (e.g., image attribution, activation calibration) necessary for best performance?"}, {"title": "4.1. Experimental Setup", "content": "Steering Vector Construction. We sample benign images with different classes from ImageNet [11] and apply the PGD attack [30] to generate 16 adversarial images for steering vectors construction. The perturbation radius \\(\\epsilon\\) is set to \\{\\frac{16}{255}, \\frac{32}{255}, \\frac{64}{255}, unconstrained\\}. Details on the PGD attack configuration can be found in Appendix 7.1.\nEvaluation Datasets. We evaluate defense performance under two scenarios: in-distribution (ID) and out-of-distribution (OOD). For the ID scenarios, we choose Toxic-"}, {"title": "4.2. Defense Performance Comparision (RQ1)", "content": "Table 1, 2, and 8 (in appendix) report the performance of our defense in the perturbation-based attack across Toxicity and Jailbreak setup. Bold denotes the best defense performance (represented by Toxicity Score or ASR).\nComparison with Existing VLM Defenses. As shown in Table 1, 2, 8, most VLM defenses struggle to consistently safeguard the model against perturbation-based attacks with different \\(\\epsilon\\). While most existing VLM defenses are based on pre- or post-processing model inputs or outputs, our adaptive steering approach effectively steers the internal model activations away from harmful contents, achieving state-of-the-art performance across almost all cases.\nAdditionally, we report the average inference time per token for each VLM defense baseline in Table 4. We emphasize two key benefits that lead to high efficiency: (1) ASTRA does not need to re-train or fine-tune the model, and the process of constructing steering vectors (Section 3.1) is cheap and straightforward. In contrast, input preprocessing-based method [33] needs to denoise each input image using the Diffusion model and adversarial training [21] needs to update the entire model, both are quite costly compared to our approach. (2) ASTRA does not affect inference time when deploying the defense - the steering step in Section 3.2 has almost negligible cost. As shown in Table 4,"}, {"title": "4.3. General Utility (RQ2)", "content": "In Section 4.2, our framework demonstrates its effectiveness in defending against VLM jailbreaks. Furthermore, we need to ensure that our defended model retains utility performance in benign scenarios and generates valid responses in adversarial scenarios.\nUtility Performance. We calculate utility scores in MM-Vet [52] and MMBench [28] datasets for benign scenario evaluation and perplexity for adversarial scenario evaluation. See Appendix 7.1 for detailed descriptions of utility scores. As shown in Table 6, our defended models demonstrate considerable utility performance in benign scenarios compared to those without defenses. These comparisons demonstrate that our defense results in little performance drops on benign inputs. We owe these results to our adaptive steering approach, which mitigates utility degradation by computing the projection between the language model's calibrated activation and steering vectors, thereby avoiding the drawbacks of a fixed steering coefficient. In adversarial contexts, the perplexities of ASTRA are still within a reasonable range, indicating that our defended models consistently provide valid, non-harmful responses to harmful instructions. Additional cases are provided in Appendix 7.4."}, {"title": "4.4. Ablation Study (RQ3)", "content": "Adaptive Steering. We demonstrate the roles of calibration activation and image attribution in our adaptive steering operation using Qwen2-VL. As shown in table 7, both designs significantly influence defense performance. Specifically, after calibration activation, the projection term can more accurately reflect the spatial relationship between steering vectors and activations within the feature space, leading to a consistent defense effectiveness in both Toxicity and Jail-"}, {"title": "5. Conclusion", "content": "In this paper, we propose ASTRA, an efficient and effective defense framework by adaptively steering models away from adversarial feature directions to resist VLM attacks. Our key procedures involve finding transferable steering vectors representing the direction of harmful response via image attribution and applying adaptive activation steering to remove these directions at inference time. Extensive experiments across multiple models and baselines demonstrate our state-of-the-art performance and high efficiency."}]}