{"title": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text", "authors": ["Sher Badshah", "Hassan Sajjad"], "abstract": "The rapid advancements in Large Language Models (LLMs) have highlighted the critical need for robust evaluation methods that can accurately assess the quality of generated text, particularly in free-form tasks. Traditional metrics like BLEU and ROUGE, while useful, often fail to capture the semantic richness and contextual relevance of free-form text compared to reference answers. In this study, we introduce a reference-guided verdict method that leverages multiple LLMs-as-judges to provide a more reliable and accurate evaluation of open-ended LLM generations. By integrating diverse LLMs, our approach mitigates individual model biases and significantly improves alignment with human judgments, especially in challenging tasks where traditional metrics and single-model evaluations fall short. Through experiments across multiple question-answering tasks, we show that our method closely aligns with human evaluations, establishing it as a scalable, reproducible, and effective alternative to human evaluation. Our approach not only enhances evaluation reliability but also opens new avenues for refining automated assessment in generative AI.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in Large Language Models (LLMs) have significantly propelled the field of Natural Language Processing (NLP) forward. With their widespread applications, the need for reliable evaluation methods has become increasingly critical. Such evaluations are essential to ensure these models meet quality standards, align with human expectations, and maintain safety and reliability in various applications (Chang et al., 2024).\nConventional automated metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) have long been employed to evaluate the performance of model generated text. However, these metrics primarily focus on surface-form similarity and often fail to account for semantically equivalent lexical and compositional diversity (Zhang et al., 2020; Zhu et al., 2023). Moreover, automated metrics struggle in evaluating open-ended generation or free-form text, where a wide range of acceptable outputs exists. This limitation becomes particularly evident when assessing instruction-tuned chat models, which tend to produce more verbose and diverse responses. While benchmarks such as MMLU require models to generate controlled outputs for ease of automated evaluation (Chen et al., 2024b), they fall short in assessing the complexity and variability of open-ended generation (Zheng et al., 2023). This limitation is particularly apparent in instruction-tuned chat models. The correlation between automated metrics and human evaluation is also relatively weak (Liu et al., 2023).\nHuman evaluation plays a crucial role in bridging this gap. It is more valuable in assessing aspects that automated metrics often miss, such as coherence and contextual relevance. While human evaluation is still considered the \u201cgold standard\" for evaluating the quality of generated text, it has several limitations. It is financially demanding, time-consuming (Ma\u00f1as et al., 2024), and often lacks scalability (Chiang and Lee, 2023). These limitations underscore the need for developing automated evaluation methods that align closely with human judgments while being more automatic, efficient, and scalable.\nRecently, a new paradigm shift has emerged where LLMs are used to judge the candidate model generations (Zheng et al., 2023). This model-based approach leverages the instruction-following capabilities of LLMs to handle various evaluation tasks. For instance, LLM like GPT-4 is utilized as a judge to assess the quality of texts generated by different assistants (i.e., pairwise comparison) (Zheng et al., 2023; Wang et al., 2023a) and rate texts based on criteria such as grammar and relevance (Chiang and Lee, 2023; Hu et al., 2024; Liu et al., 2023)."}, {"title": "2 Methodology", "content": "Inspired by the way human evaluations typically involve multiple annotators to ensure reliability and accuracy, we propose a similar method that leverages multiple LLMs as judges for evaluating free-form outputs. The primary objective is to determine whether the collective judgment of multiple LLMs can achieve a level of reliability and accuracy that is comparable to or even surpasses, that of human annotators. Our method is structured around three key components: generating outputs from candidate LLMs for given tasks, conducting human evaluations as a benchmark, and utilizing multiple LLMs as judges to assess the quality of the candidate LLM outputs. Figure 1 provides an overview of our method."}, {"title": "2.1 Candidate LLMs", "content": "A candidate LLM A refers to a model that generates output a for the given input x. In our methodology, we utilized candidate LLMs to generate free-form outputs for the given tasks. The generated outputs ar represent the contents that LLMs acting as judges, will evaluate against reference answers."}, {"title": "2.2 LLMs-as-Judges", "content": "A judge J LLM is utilized to deliver a verdict V (e.g., True/False ) on outputs or generations a produced by a candidate LLM A. Previously, LLM-as-a-judge is employed to compare the responses of two LLMs or deliver a verdict based on predefined criteria (Zheng et al., 2023; Verga et al., 2024; Ma\u00f1as et al., 2024). In this study, we focus on a more realistic setting (see Section 2.3) where a judge LLM J evaluates the output a generated by a candidate LLM A by comparing it to a reference answer r within the context established by an input x."}, {"title": "2.3 Reference-guided verdict", "content": "In this setting, the evaluation process begins with the reception of three crucial components: the con-"}, {"title": "3 Experiment", "content": "We utilize the following settings to examine the performance and reliability of LLMs-as-judges in reference-guided evaluations."}, {"title": "3.1 Models", "content": "We select both open-source and closed-source instruct models to serve as both candidates and judges in our experiment. These models include Mistral 7B\u00b9 (Jiang et al., 2023), Llama-3.1 70B2 (Meta AI, 2024), and GPT-3.5-turbo (Brown"}, {"title": "3.2 Datasets", "content": "We use three free-form question-answering (QA) datasets: TruthfulQA (Lin et al., 2022), TriviaQA (Joshi et al., 2017), and HotpotQA (Yang et al., 2018). These datasets are well-suited for assessing LLMs-as-judges (Ji), where traditional metrics such as exact match and regex-based methods often fail with the open-ended, conversational outputs of instruct/chat models. For TruthfulQA, we use the \"validation\" split from the \u201cgeneration\" subset, for TriviaQA, the \u201cvalidation\" split from the \"unfiltered.nocontext\" subset, and for HotpotQA, the \"validation\" split from the \u201cdistractor\u201d subset. Due to the significant effort required to obtain human evaluation of candidate LLMs outputs, which are used to calculate the alignment between human judges and LLM judges, we only utilize 100 random samples from each dataset."}, {"title": "3.3 Prompts", "content": "We designed generalized zero-shot prompts with role-playing (Kong et al., 2024) for both candidates and judges. Initially, we prompt candidate LLMs with the role \u201cYou are a helpful assistant.", "You are a helpful assistant acting as an impartial judge.\" to mitigate biases in judgments (Zheng et al., 2023). We chose not to use few-shot or chain-of-thought prompting strategies to keep the solution robust to a variety of tasks. Previous studies have also shown that in-context examples do not significantly improve the performance of model-based evaluators (Hada et al., 2024; Min et al., 2022).\"\n    },\n    {\n      \"title\": \"3.4 Human Evaluation\",\n      \"content\": \"Human evaluation remains the gold standard for assessing the outputs (ar) of candidate LLMs (Ai). We recruit three graduate students from our academic network, all specialized in natural language processing, to serve as annotators. We provide the input given to the candidates, reference answers, and candidate responses. This format, while similar, is distinct from the judge models' prompts which additionally require formatted decisions. The human annotators focus solely on the accuracy and relevance of the responses. To ensure impartial evaluations, we anonymize the origin of responses. Annotators do not know which candidate model generated such responses, reducing potential bias linked to model familiarity or reputation. We asked the annotators to score the candidate LLMs outputs on a binary scale: '1' for 'True' and '0' for 'False' based on alignment with the reference answer and contextual relevance.\nTo ensure a rigorous evaluation, each of the three annotators independently assesses the entire set of outputs generated by each candidate model across all datasets. Specifically, an annotator evaluates the outputs from candidate models like Mistral 7B for TruthfulQA, TriviaQA, and HotpotQA separately, ensuring that the assessment for each dataset oc-\"\n    },\n    {\n      \"title\": \"3.5 Statistical Analysis\",\n      \"content\": \"To analyze the reliability of the evaluations conducted by human annotators and LLMs-as-judges, we employ majority vote, percent agreement, Fleiss's kappa, and Cohen's kappa. These metrics provide insights into the degree of concordance among the human annotators' judgments and LLMs as judges.\nMajority Vote aggregates the evaluations of the three human annotators to determine the final score for each response. Similarly, we apply the same approach to the LLMs-as-judges. For each response, the majority vote is taken as the final decision.\nPercent Agreement calculates the proportion of instances where all evaluators (human or LLMs) assigned the same score to a given response.\nPA (%) = Total number of agreements / Total number of evaluations \u00d7 100\nFor each response, if all three evaluators (i.e., human or LLMs-as-judges) agree on the score (either '1' or '0'), it counts as a total agreement.\nKappa Statistics Kappa statistics (K), including Fleiss' Kappa (Fleiss and Cohen, 1973) and Cohen's Kappa (McHugh, 2012), measure the agreement among multiple annotators, adjusting for the agreement occurring by chance. These metrics are crucial when score distributions are not uniform. Both are calculated using:\n\u041a= (Po - Pe) / (1-Pe)\nwhere Po represents the observed agreement, and Pe is the expected agreement by chance.\nFleiss' Kappa: Applicable for multiple raters and multiple categories, Po is derived from:\nPo = 1/ (N * n(n-1)) * \u2211i=1 \u2211j=1 nij (Nij -1)\nand Pe from category proportions:\nPe = \u2211j=1 (Pj)\u00b2, where Pj = 1/N \u2211nij\"\n    },\n    {\n      \"title\": \"Cohen's Kappa\",\n      \"content\": \"Cohen's Kappa: Suitable for two raters or dichotomous categories, with Pe calculated as:\nPe = (n1/n)\u00b2 + (no/n)\u00b2\nBoth statistics range from -1 (complete disagreement) to 1 (perfect agreement), with 0 indicating agreement expected by chance.\"\n    },\n    {\n      \"title\": \"4 Results\",\n      \"content\": \"We aggregate majority votes from human annotators to show the accuracy of candidate LLMs in TruthfulQA, TriviaQA, and HotpotQA. As human evaluation is the gold standard, these results serve as the ground truth for LLMs acting as judges. Subsequently, we obtained majority votes from LLMs-as-judges to show how their evaluation capabilities compared to the established ground truth. The side-by-side comparison in Table 1 highlights the varying degrees of alignment and divergence in performance between human annotators and LLMs-as-judges.\nThe performance of LLMs-as-judges appears to be influenced significantly by the complexity of the tasks. Specifically, it is evident in TruthfulQA where LLMs-as-judges diverged from human evaluations. Unlike HotpotQA and TriviaQA, where answers are typically more concise and the provided context directly supports the evaluation process, TruthfulQA requires a deeper level of understanding.\nWe further analyzed the performance of individual judge models (e.g., Mistral 7B-Judge) compared to human evaluation aggregated through majority votes (see Table 1). Figure 11 in C illustrates the absolute differences in performance across QA tasks.\"\n    },\n    {\n      \"title\": \"4.2 Inter-annotator Agreement\",\n      \"content\": \"We extended our analysis to find Percent Agreement (PA) among human annotators and PA among LLMs acting as judges. As shown in Table 2, human annotators consistently show high agreement, reflecting their reliability as the gold standard for evaluation. In contrast while LLMs-as-judges demonstrate relatively high agreement, they fall short of the consistency shown by human annotators.\nWe calculate Fleiss' Kappa (\u03ba) to assess inter-rater reliability among human annotators and LLMs-as-judges. The kappa values for human annotators range from substantial to almost perfect agreement (see Table 3). In contrast, inter-rater agreement among LLMs-as-judges reveals more variability and lower kappa values than human annotators. For instance, in TruthfulQA, all kappa values fall within the substantial agreement, with the highest being 0.66 for candidate GPT-3.5. In TriviaQA and HotpotQA, judges' reliability improves but remains within the substantial range.\"\n    },\n    {\n      \"title\": \"4.3 Correlation with Human Judgment\",\n      \"content\": \"We utilized Cohen's kappa (K) to measure the inter-rater reliability between individual LLM judges and human annotators. We considered the majority vote scores from human annotators (see Table 4) and each LLM judge ratings to calculate Cohen's kappa between two groups (i.e., human and LLM judge) across three tasks.\nCohen's kappa scores indicate differences in the alignment across tasks. In TruthfulQA, Mistral 7B-Judge achieves substantial agreement (\u043a = 0.78) when evaluating candidate Llama-3.1 70B. In the same task, Llama-3.1 70B-Judge shows substantial alignment (k = 0.74) for self-evaluation (i.e., Llama-3.1 70B). In TriviaQA, the kappa scores are consistently higher, reaching up to the almost perfect agreement with Llama-3.1 70B-Judge (\u043a = 0.93) when evaluating candidate GPT-3.5. Similarly, in HotpotQA, all judges show substantial to almost perfect agreement, except for GPT-3.5-Judge (\u043a = 0.76) and (\u043a = 0.71) when evaluating candidates Mistral 7B and Llama 3.1 70B.\nTo further analyze the reliability between the two groups, we considered the majority votes from both human annotators and LLMs-as-judges (see Table 1) and calculated Cohen's kappa (see right column in Table 4). The alignment improves in most cases, demonstrating that the use of multiple LLMs-as-judges leads to evaluations that more closely resemble human judgments, thereby increasing the correlation to human evaluation.\"\n    },\n    {\n      \"title\": \"4.4 Ablation Studies\",\n      \"content\": \"In this section, we conduct ablation experiments to investigate how different configurations affect the effectiveness and reliability of LLMs-as-judges on TruthfulQA samples. We chose TruthfulQA for ablation experiments because LLMs-as-judges show notable challenges in this task compared to human annotators. For the ablation experiments, we focus exclusively on the candidate Mistral 7B\"\n    },\n    {\n      \"title\": \"4.4.1 Stability in Judges Verdicts\",\n      \"content\": \"LLMs generate random text even at a temperature of 0. This randomness extends concerns about the stability of evaluation results (Song et al., 2024). \u03a4\u03bf assess verdict consistency, we prompt each LLM-as-a-judge five times using outputs from candidate Mistral 7B for TruthfulQA at zero temperature. We found that LLMs-as-judges are consistent in their verdicts when subjected to repeated sampling with the same input. However, the explanations provided by the LLMs-as-judges varied across iterations, reflecting the model's ability to generate diverse reasoning while maintaining stable judgments (see Figure 12 in Appendix C).\"\n    },\n    {\n      \"title\": \"4.4.2 Prompt Sensitivity\",\n      \"content\": \"We investigate the behavior of LLMs-as-judges by introducing three key variations in our prompt design. Initially, we simplified the prompt by removing the requirement for the explanation and instructed the judges to deliver a 'True' or 'False' verdict. We referred to this as open prompt which let the model have the flexibility to apply its own reasoning without being restricted by specific guidelines or criteria. In the next variation, we detailed prompt (Errica et al., 2024) that includes detailed criteria to guide the judgments. Specifically, we instructed the LLMs judges to consider \\\"An answer as true if it describes the literal truth about the real world and is supported by reliable, publicly available evidence. Conversely, answers that are only true according to a belief system or tradition (e.g., \u201cLibras have friendly personalities": "are to be counted as false\" (Lin et al., 2022). Finally, we explore close prompt by instructing to only provide 'True' or 'False' responses in order to examine the impact of constrained binary decisions (see Appendix A for prompt variations).\nFrom our ablation experiments, we found that LLMs-as-judges are more susceptible to open and detailed prompts. Specifically, Mistral 7B-Judge shows greater sensitivity to open prompts where models are given the freedom to apply their own reasoning. In the open prompt, Mistral 7B-Judge showed an 18% change rate (see Figure 2), indicating significant variability in its judgments. This flexibility of generating constrained-free generation, however, also led to a decrease in alignment with human evaluations, as reflected by lower percent agreement and Fleiss' Kappa values in Table 6 (see Appendix C). Contrarily, when using detailed prompts that provide clear guidelines, the variability decreased, but this came at the cost of inter-rater reliability, with Fleiss' Kappa scores dropping further. Interestingly, the close prompts, which constrained responses to binary decisions only, appeared to hit the right balance. Mistral 7B-Judge"}, {"title": "5 Discussions", "content": "Overall, LLMs-as-judges show promising performance in reference-guided verdict settings. Particularly, when multiple LLM judges perform in tandem, their complementary strengths can be leveraged to enhance the accuracy and reliability of the evaluations. For instance, the Mistral 7B-Judge showed higher sensitivity to open prompts, while the GPT-3.5-Judge performed consistently well across prompt variations (see Figure 2). Similarly, GPT-3.5-Judge showed little alignment (\u043a = 0.68) compared to Mistral 7B-Judge (\u043a = 0.72) and Llama-3.1 70B-Judge (\u043a = 0.77) when evaluating the candidate Mistral 7B model on TruthfulQA (see Table 4). However, the alignment improved to near-perfect agreement (\u043a = 0.79) when all three judges were integrated.\nThe integration of a diverse set of LLMs is instrumental in mitigating biases in the evaluation process. By leveraging models that have been trained on different datasets or fine-tuned with varying parameters, the collective judgment is less likely to be influenced by the biases of any single model. For instance, in some cases, GPT-3.5-Judge shows a tendency to accept speculative content, while Mistral 7B-Judge and Llama-3.1 70B-Judge offer a more safe and evidence-based evaluation. This highlights the importance of integrating diverse models (see Figure 13 in Appendix C).\nThis approach also enhances the objectivity of the evaluations, leading to a more balanced and fair assessment. In some instances, LLMs-as-judges even surpass the fairness of human evaluators, who may be subject to unconscious biases (Chen et al., 2024a). For example, when evaluating the exact words spoken by Neil Armstrong on the moon, human annotators marked the answer \"That's one small step for man, one giant leap for mankind\" as 'True'. However, LLMs correctly identified the omission of the word \"a\" resulting in \"That's one small step for a man, one giant leap for mankind\" as a significant difference, and judged the provided answer as 'False'.\nWe specifically explored the potential for self-enhancement bias, where LLMs might show a tendency to favor their own outputs when acting as judges (Zheng et al., 2023). However, due to the presence of reference answers in our setup, we did not observe significant instances of self-enhancement bias. The reference answers provided"}, {"title": "6 Related work", "content": "To address the limitations of traditional n-gram-based metrics like BLEU and ROUGE, various model-based methods such as BERTScore (Zhang et al., 2020) to provide a more semantically informed evaluation. However, even BERTScore and similar embedding-based methods struggle to effectively evaluate open-ended generation (Zheng et al., 2023; Sun et al., 2022). Recent LLMs advances have unlocked new avenues for automatic and context-aware evaluation (Chiang and Lee, 2023). Previously, LLMs are utilized in three key evaluation settings including pairwise, single-answer, and reference-guided evaluations (Zheng et al., 2023; Verga et al., 2024).\nDespite some promising results, the LLM-as-a-judge approach suffers from inherent LLM biases (Chiang and Lee, 2023; Thakur et al., 2024), including positional bias (Zheng et al., 2023; Khan et al., 2024; Kenton et al., 2024; Shi et al., 2024), verbosity bias (Huang et al., 2024; Zheng et al., 2023), and self-enhancement bias (Zheng et al., 2023), where the model may favor certain response positions, longer answers, or their own outputs.\nLLMs often conflate different evaluation criteria (Liu et al., 2024) which significantly undermines the reliability of evaluations (Wang et al., 2023b). Moreover, prompt variations also affect the consistency and reproducibility of LLM-based evaluations (Zheng et al., 2023).\nOur study offers a new approach by considering task-specific reference answers to guide LLM judges for impartial evaluations. We also studied the calibration of LLMs-as-judges to human judgments. Although some studies have considered the reference-guided method (Zheng et al., 2023; Verga et al., 2024), their objective is to either assist judges in the other two evaluation settings including pairwise and single-answer scoring or to evaluate in exact match settings. Our study differs by focusing on the evaluation of open-ended text generation using free-form datasets, where responses are varied and less constrained by strict reference alignment (e.g., MCQs). Similarly, the calibration between human judgments and LLMs-as-judges has been studied (Koo et al., 2024; Hada et al., 2024); however, these efforts have primarily focused on single-answer scoring or multilingual evaluation, leaving room for further exploration in other areas."}, {"title": "7 Conclusion", "content": "This study explores the potential of using LLMs-as-judges for evaluating open-ended generation with task-specific reference answers. Our findings demonstrate that leveraging diverse LLMs can significantly improve the reliability and accuracy of evaluations, particularly in complex and more open-ended tasks. Our approach offers a promising alternative to traditional evaluation methods by mitigating biases and enhancing alignment with human judgments. This study lays the groundwork for future research into more scalable and subtle evaluation methods, including scenarios where reference answers do not exist, thereby better reflecting the intricacies of real-world applications."}, {"title": "8 Limitations", "content": "We acknowledge certain limitations in our study. The accuracy of evaluations depends on the quality and clarity of reference answers. While using multiple LLMs improves reliability, relying on the assumption that reference answers are always accurate may not be valid in all cases. The study primarily uses binary verdicts, which might overlook detailed aspects of the responses that could be better captured through more comprehensive evaluations. Additionally, it doesn't fully examine how prompt designs affect the consistency of LLM judgments across different tasks. The high computational demand for running multiple LLMs may also limit the usefulness of this approach in resource-constrained settings (Badshah and Sajjad, 2024)."}, {"title": "A Prompting", "content": "In our main experiment, we performed the zero-shot prompting in the following two stages."}, {"title": "A.1 Prompting Candidate LLMS", "content": "We prompted candidate LLMs (see Figure 3) to record generations for each task. We set the same role and prompt structure for each candidate model to ensure the reproducibility of our results."}, {"title": "A.2 Prompting LLMs as Judges", "content": "As we recorded the candidate LLMs' outputs in a CSV file, we prompted LLMs-as -judges to perform the evaluation (see Figure 5). In Figure 6, judge Llama-3-70B evaluating candidate Mistral 7B."}, {"title": "A.3 Prompt Sensitivity", "content": "To explore how different prompt designs influence the judgments of LLMs, we employ three variations: open, detailed, and closed prompts. Each prompt variation is used to evaluate the candidate Mistral 7B outputs on a TruthfulQA. The following examples (see Figure 7, Figure 8, and Figure 9) illustrate how these different prompt structures guide the judges' decision-making process."}, {"title": "B Guidelines for Human Evaluation", "content": "This section presents the detailed guidelines for human annotators responsible for evaluating candidate LLMs outputs. These guidelines are designed to ensure that each annotator applies a consistent and objective approach throughout the evaluation process. Figure 10 shows the guidelines to human annotators for evaluating candidates LLMs outputs."}, {"title": "C Additional Results", "content": "In this section, we provide detailed results in order to understand the capabilities of LLMs-as-judges."}]}