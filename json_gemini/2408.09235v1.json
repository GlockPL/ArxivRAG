{"title": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of\nFree-Form Text", "authors": ["Sher Badshah", "Hassan Sajjad"], "abstract": "The rapid advancements in Large Language\nModels (LLMs) have highlighted the critical\nneed for robust evaluation methods that can\naccurately assess the quality of generated text,\nparticularly in free-form tasks. Traditional met-\nrics like BLEU and ROUGE, while useful, of-\nten fail to capture the semantic richness and\ncontextual relevance of free-form text com-\npared to reference answers. In this study, we\nintroduce a reference-guided verdict method\nthat leverages multiple LLMs-as-judges to pro-\nvide a more reliable and accurate evaluation\nof open-ended LLM generations. By inte-\ngrating diverse LLMs, our approach mitigates\nindividual model biases and significantly im-\nproves alignment with human judgments, es-\npecially in challenging tasks where traditional\nmetrics and single-model evaluations fall short.\nThrough experiments across multiple question-\nanswering tasks, we show that our method\nclosely aligns with human evaluations, estab-\nlishing it as a scalable, reproducible, and ef-\nfective alternative to human evaluation. Our\napproach not only enhances evaluation relia-\nbility but also opens new avenues for refining\nautomated assessment in generative AI.", "sections": [{"title": "Introduction", "content": "The rapid advancements in Large Language Mod-\nels (LLMs) have significantly propelled the field of\nNatural Language Processing (NLP) forward. With\ntheir widespread applications, the need for reliable\nevaluation methods has become increasingly criti-\ncal. Such evaluations are essential to ensure these\nmodels meet quality standards, align with human\nexpectations, and maintain safety and reliability in\nvarious applications (Chang et al., 2024).\nConventional automated metrics such as\nBLEU (Papineni et al., 2002), ROUGE (Lin, 2004),\nand METEOR (Banerjee and Lavie, 2005) have\nlong been employed to evaluate the performance of\nmodel generated text. However, these metrics pri-\nmarily focus on surface-form similarity and often\nfail to account for semantically equivalent lexical\nand compositional diversity (Zhang et al., 2020;\nZhu et al., 2023). Moreover, automated metrics\nstruggle in evaluating open-ended generation or\nfree-form text, where a wide range of acceptable\noutputs exists. This limitation becomes partic-\nularly evident when assessing instruction-tuned\nchat models, which tend to produce more verbose\nand diverse responses. While benchmarks such as\nMMLU require models to generate controlled out-\nputs for ease of automated evaluation (Chen et al.,\n2024b), they fall short in assessing the complexity\nand variability of open-ended generation (Zheng\net al., 2023). This limitation is particularly apparent\nin instruction-tuned chat models. The correlation\nbetween automated metrics and human evaluation\nis also relatively weak (Liu et al., 2023).\nHuman evaluation plays a crucial role in bridg-\ning this gap. It is more valuable in assessing as-\npects that automated metrics often miss, such as\ncoherence and contextual relevance. While human\nevaluation is still considered the \u201cgold standard\"\nfor evaluating the quality of generated text, it has\nseveral limitations. It is financially demanding,\ntime-consuming (Ma\u00f1as et al., 2024), and often\nlacks scalability (Chiang and Lee, 2023). These\nlimitations underscore the need for developing au-\ntomated evaluation methods that align closely with\nhuman judgments while being more automatic, ef-\nficient, and scalable.\nRecently, a new paradigm shift has emerged\nwhere LLMs are used to judge the candidate model\ngenerations (Zheng et al., 2023). This model-based\napproach leverages the instruction-following capa-\nbilities of LLMs to handle various evaluation tasks.\nFor instance, LLM like GPT-4 is utilized as a judge\nto assess the quality of texts generated by different\nassistants (i.e., pairwise comparison) (Zheng et al.,\n2023; Wang et al., 2023a) and rate texts based on\ncriteria such as grammar and relevance (Chiang\nand Lee, 2023; Hu et al., 2024; Liu et al., 2023)."}, {"title": "Methodology", "content": "Inspired by the way human evaluations typically\ninvolve multiple annotators to ensure reliability\nand accuracy, we propose a similar method that\nleverages multiple LLMs as judges for evaluating\nfree-form outputs. The primary objective is to de-\ntermine whether the collective judgment of mul-\ntiple LLMs can achieve a level of reliability and\naccuracy that is comparable to or even surpasses,\nthat of human annotators. Our method is structured\naround three key components: generating outputs\nfrom candidate LLMs for given tasks, conducting\nhuman evaluations as a benchmark, and utilizing\nmultiple LLMs as judges to assess the quality of\nthe candidate LLM outputs. Figure 1 provides an\noverview of our method."}, {"title": "Candidate LLMs", "content": "A candidate LLM A refers to a model that generates\noutput a for the given input x. In our methodology,\nwe utilized candidate LLMs to generate free-form\noutputs for the given tasks. The generated out-\nputs ar represent the contents that LLMs acting as\njudges, will evaluate against reference answers."}, {"title": "LLMs-as-Judges", "content": "A judge J LLM is utilized to deliver a verdict V\n(e.g., True/False ) on outputs or generations a pro-\nduced by a candidate LLM A. Previously, LLM-\nas-a-judge is employed to compare the responses\nof two LLMs or deliver a verdict based on prede-\nfined criteria (Zheng et al., 2023; Verga et al., 2024;\nMa\u00f1as et al., 2024). In this study, we focus on\na more realistic setting (see Section 2.3) where a\njudge LLM J evaluates the output a generated by\na candidate LLM A by comparing it to a reference\nanswer r within the context established by an input\nX."}, {"title": "Reference-guided verdict", "content": "In this setting, the evaluation process begins with\nthe reception of three crucial components: the con-"}, {"title": "Experiment", "content": "We utilize the following settings to examine the\nperformance and reliability of LLMs-as-judges in\nreference-guided evaluations."}, {"title": "Models", "content": "We select both open-source and closed-source in-\nstruct models to serve as both candidates and\njudges in our experiment. These models in-\nclude Mistral 7B\u00b9 (Jiang et al., 2023), Llama-3.1\n70B\u00b2 (Meta AI, 2024), and GPT-3.5-turbo (Brown\net al., 2020). By utilizing the same models in\nboth roles, we can investigate self-enhancement\nbias (Zheng et al., 2023), where a model may show\na tendency to favor its own outputs. This setup\nalso allows us to study how models perform in a\njudging capacity when they are aware of the cor-\nrect answer, especially in cases where they did not\nproduce the correct answer as candidates. This ap-\nproach is crucial for assessing the objectivity of\nthe models and their ability to evaluate responses\nagainst a definitive gold standard, independent of\ntheir own outputs as candidates.\nTo ensure the reproducibility of our experiments,\nwe set the temperature parameter to 0 for all models\nunder study, as the performance of LLM-based\nevaluators has been shown to drop as temperature\nincreases (Hada et al., 2024)."}, {"title": "Datasets", "content": "We use three free-form question-answering (QA)\ndatasets: TruthfulQA (Lin et al., 2022), Trivi-\naQA (Joshi et al., 2017), and HotpotQA (Yang\net al., 2018). These datasets are well-suited for\nassessing LLMs-as-judges (Ji), where traditional\nmetrics such as exact match and regex-based meth-\nods often fail with the open-ended, conversational\noutputs of instruct/chat models. For TruthfulQA,\nwe use the \"validation\" split from the \u201cgeneration\"\nsubset, for TriviaQA, the \u201cvalidation\" split from the\n\"unfiltered.nocontext\" subset, and for HotpotQA,\nthe \"validation\" split from the \u201cdistractor\u201d subset.\nDue to the significant effort required to obtain hu-\nman evaluation of candidate LLMs outputs, which\nare used to calculate the alignment between hu-\nman judges and LLM judges, we only utilize 100\nrandom samples from each dataset."}, {"title": "Prompts", "content": "We designed generalized zero-shot prompts with\nrole-playing (Kong et al., 2024) for both candidates\nand judges. Initially, we prompt candidate LLMs\nwith the role \u201cYou are a helpful assistant.\" to elicit\noutputs for the given random samples associated\nwith each dataset. To evaluate the outputs of these\ncandidate LLMs, we prompt judge LLMs for binary\nverdicts (i.e., True or False) using P = {x,a,r}\nand instructed to provide a brief explanation for\ntheir verdict (see Appendix A for examples). Bi-\nnary verdicts simplify the evaluation process and\nfacilitate automatic evaluation. In addition to three\nkey prompt components, we define the role of the\njudge LLMs as \u201cYou are a helpful assistant acting\nas an impartial judge.\" to mitigate biases in judg-\nments (Zheng et al., 2023). We chose not to use\nfew-shot or chain-of-thought prompting strategies\nto keep the solution robust to a variety of tasks. Pre-\nvious studies have also shown that in-context exam-\nples do not significantly improve the performance\nof model-based evaluators (Hada et al., 2024; Min\net al., 2022).\""}, {"title": "Human Evaluation", "content": "Human evaluation remains the gold standard for\nassessing the outputs (ar) of candidate LLMs (Ai).\nWe recruit three graduate students from our aca-\ndemic network, all specialized in natural language\nprocessing, to serve as annotators. We provide the\ninput given to the candidates, reference answers,\nand candidate responses. This format, while sim-\nilar, is distinct from the judge models' prompts\nwhich additionally require formatted decisions.\nThe human annotators focus solely on the accuracy\nand relevance of the responses. To ensure impartial\nevaluations, we anonymize the origin of responses.\nAnnotators do not know which candidate model\ngenerated such responses, reducing potential bias\nlinked to model familiarity or reputation. We asked\nthe annotators to score the candidate LLMs outputs\non a binary scale: '1' for 'True' and '0' for 'False'\nbased on alignment with the reference answer and\ncontextual relevance.\nTo ensure a rigorous evaluation, each of the three\nannotators independently assesses the entire set of\noutputs generated by each candidate model across\nall datasets. Specifically, an annotator evaluates the\noutputs from candidate models like Mistral 7B for\nTruthfulQA, TriviaQA, and HotpotQA separately,\nensuring that the assessment for each dataset oc-"}, {"title": "Statistical Analysis", "content": "To analyze the reliability of the evaluations con-\nducted by human annotators and LLMs-as-judges,\nwe employ majority vote, percent agreement,\nFleiss's kappa, and Cohen's kappa. These met-\nrics provide insights into the degree of concor-\ndance among the human annotators' judgments and\nLLMs as judges.\nMajority Vote aggregates the evaluations of the\nthree human annotators to determine the final score\nfor each response. Similarly, we apply the same ap-\nproach to the LLMs-as-judges. For each response,\nthe majority vote is taken as the final decision.\nPercent Agreement calculates the proportion of\ninstances where all evaluators (human or LLMs)\nassigned the same score to a given response.\nPA (%) = \\frac{\\text{Total number of agreements}}{\\text{Total number of evaluations}} \u00d7 100\nFor each response, if all three evaluators (i.e., hu-\nman or LLMs-as-judges) agree on the score (either\n'1' or '0'), it counts as a total agreement.\nKappa Statistics Kappa statistics (K), including\nFleiss' Kappa (Fleiss and Cohen, 1973) and Co-\nhen's Kappa (McHugh, 2012), measure the agree-\nment among multiple annotators, adjusting for the\nagreement occurring by chance. These metrics are\ncrucial when score distributions are not uniform.\nBoth are calculated using:\n\u041a= \\frac{Po - Pe}{1-Pe}\nwhere $P_o$ represents the observed agreement,\nand $P_e$ is the expected agreement by chance.\nFleiss' Kappa: Applicable for multiple raters and\nmultiple categories, $P_o$ is derived from:\nPo = \\frac{1}{N \\cdot n(n-1)} \\sum_{i=1}^{N} \\sum_{j=1}^{k} n_{ij} (n_{ij} -1)\nand $P_e$ from category proportions:\nPe = \\sum_{j=1}^{k} p_j^2\nN_j = \\sum_{i=1}^{N} n_{ij}"}, {"title": "Results", "content": "We aggregate majority votes from human annota-\ntors to show the accuracy of candidate LLMs in\nTruthfulQA, TriviaQA, and HotpotQA. As human\nevaluation is the gold standard, these results serve\nas the ground truth for LLMs acting as judges. Sub-\nsequently, we obtained majority votes from LLMs-\nas-judges to show how their evaluation capabilities\ncompared to the established ground truth. The\nside-by-side comparison in Table 1 highlights the\nvarying degrees of alignment and divergence in per-\nformance between human annotators and LLMs-\nas-judges.\nThe performance of LLMs-as-judges appears to\nbe influenced significantly by the complexity of\nthe tasks. Specifically, it is evident in TruthfulQA\nwhere LLMs-as-judges diverged from human evalu-\nations. Unlike HotpotQA and TriviaQA, where an-\nswers are typically more concise and the provided\ncontext directly supports the evaluation process,\nTruthfulQA requires a deeper level of understand-\ning.\nWe further analyzed the performance of indi-\nvidual judge models (e.g., Mistral 7B-Judge) com-\npared to human evaluation aggregated through ma-\njority votes (see Table 1). Figure 11 in C illustrates\nthe absolute differences in performance across QA\ntasks."}, {"title": "Inter-annotator Agreement", "content": "We extended our analysis to find Percent Agree-\nment (PA) among human annotators and PA among\nLLMs acting as judges. As shown in Table 2,\nhuman annotators consistently show high agree-\nment, reflecting their reliability as the gold standard\nfor evaluation. In contrast while LLMs-as-judges\ndemonstrate relatively high agreement, they fall\nshort of the consistency shown by human annota-\ntors.\nWe calculate Fleiss' Kappa (\u03ba) to assess inter-\nrater reliability among human annotators and"}, {"title": "Correlation with Human Judgment", "content": "We utilized Cohen's kappa (K) to measure the inter-\nrater reliability between individual LLM judges\nand human annotators. We considered the majority\nvote scores from human annotators (see Table 4)\nand each LLM judge ratings to calculate Cohen's\nkappa between two groups (i.e., human and LLM\njudge) across three tasks.\nCohen's kappa scores indicate differences in the\nalignment across tasks. In TruthfulQA, Mistral 7B-\nJudge achieves substantial agreement (\u043a = 0.78)\nwhen evaluating candidate Llama-3.1 70B. In the\nsame task, Llama-3.1 70B-Judge shows substan-\ntial alignment (k = 0.74) for self-evaluation (i.e.,\nLlama-3.1 70B). In TriviaQA, the kappa scores\nare consistently higher, reaching up to the almost\nperfect agreement with Llama-3.1 70B-Judge (\u043a\n= 0.93) when evaluating candidate GPT-3.5. Sim-\nlarly, in HotpotQA, all judges show substantial\nto almost perfect agreement, except for GPT-3.5-\nJudge (\u043a = 0.76) and (\u043a = 0.71) when evaluating\ncandidates Mistral 7B and Llama 3.1 70B.\nTo further analyze the reliability between the\ntwo groups, we considered the majority votes from\nboth human annotators and LLMs-as-judges (see\nTable 1) and calculated Cohen's kappa (see right\ncolumn in Table 4). The alignment improves in\nmost cases, demonstrating that the use of multi-\nple LLMs-as-judges leads to evaluations that more\nclosely resemble human judgments, thereby in-\ncreasing the correlation to human evaluation."}, {"title": "Ablation Studies", "content": "In this section, we conduct ablation experiments\nto investigate how different configurations affect\nthe effectiveness and reliability of LLMs-as-judges\non TruthfulQA samples. We chose TruthfulQA\nfor ablation experiments because LLMs-as-judges\nshow notable challenges in this task compared to\nhuman annotators. For the ablation experiments,\nwe focus exclusively on the candidate Mistral 7B"}, {"title": "Stability in Judges Verdicts", "content": "LLMs generate random text even at a temperature\nof 0. This randomness extends concerns about the\nstability of evaluation results (Song et al., 2024). \u03a4\u03bf\nassess verdict consistency, we prompt each LLM-\nas-a-judge five times using outputs from candidate\nMistral 7B for TruthfulQA at zero temperature. We\nfound that LLMs-as-judges are consistent in their\nverdicts when subjected to repeated sampling with\nthe same input. However, the explanations pro-\nvided by the LLMs-as-judges varied across iter-\nations, reflecting the model's ability to generate\ndiverse reasoning while maintaining stable judg-\nments (see Figure 12 in Appendix C)."}, {"title": "Prompt Sensitivity", "content": "We investigate the behavior of LLMs-as-judges\nby introducing three key variations in our prompt\ndesign. Initially, we simplified the prompt by re-\nmoving the requirement for the explanation and in-\nstructed the judges to deliver a 'True' or 'False' ver-\ndict. We referred to this as open prompt which let\nthe model have the flexibility to apply its own rea-\nsoning without being restricted by specific guide-\nlines or criteria. In the next variation, we detailed\nprompt (Errica et al., 2024) that includes detailed\ncriteria to guide the judgments. Specifically, we in-\nstructed the LLMs judges to consider \"An answer\nas true if it describes the literal truth about the\nreal world and is supported by reliable, publicly\navailable evidence. Conversely, answers that are\nonly true according to a belief system or tradition\n(e.g., \u201cLibras have friendly personalities\u201d) are to\nbe counted as false\" (Lin et al., 2022). Finally,\nwe explore close prompt by instructing to only\nprovide 'True' or 'False' responses in order to ex-\namine the impact of constrained binary decisions\n(see Appendix A for prompt variations).\nFrom our ablation experiments, we found that\nLLMs-as-judges are more susceptible to open and\ndetailed prompts. Specifically, Mistral 7B-Judge\nshows greater sensitivity to open prompts where\nmodels are given the freedom to apply their own\nreasoning. In the open prompt, Mistral 7B-Judge\nshowed an 18% change rate (see Figure 2), indi-\ncating significant variability in its judgments. This\nflexibility of generating constrained-free genera-\ntion, however, also led to a decrease in alignment\nwith human evaluations, as reflected by lower per-\ncent agreement and Fleiss' Kappa values in Table 6\n(see Appendix C). Contrarily, when using detailed\nprompts that provide clear guidelines, the variabil-\nity decreased, but this came at the cost of inter-\nrater reliability, with Fleiss' Kappa scores dropping\nfurther. Interestingly, the close prompts, which\nconstrained responses to binary decisions only, ap-\npeared to hit the right balance. Mistral 7B-Judge"}, {"title": "Discussions", "content": "Overall, LLMs-as-judges show promising perfor-\nmance in reference-guided verdict settings. Partic-\nularly, when multiple LLM judges perform in tan-\ndem, their complementary strengths can be lever-\naged to enhance the accuracy and reliability of the\nevaluations. For instance, the Mistral 7B-Judge\nshowed higher sensitivity to open prompts, while\nthe GPT-3.5-Judge performed consistently well\nacross prompt variations (see Figure 2). Simi-\nlarly, GPT-3.5-Judge showed little alignment (\u043a\n= 0.68) compared to Mistral 7B-Judge (\u043a = 0.72)\nand Llama-3.1 70B-Judge (\u043a = 0.77) when evaluat-\ning the candidate Mistral 7B model on TruthfulQA\n(see Table 4). However, the alignment improved to\nnear-perfect agreement (\u043a = 0.79) when all three\njudges were integrated.\nThe integration of a diverse set of LLMs is instru-\nmental in mitigating biases in the evaluation pro-\ncess. By leveraging models that have been trained\non different datasets or fine-tuned with varying pa-\nrameters, the collective judgment is less likely to\nbe influenced by the biases of any single model.\nFor instance, in some cases, GPT-3.5-Judge shows\na tendency to accept speculative content, while\nMistral 7B-Judge and Llama-3.1 70B-Judge offer\na more safe and evidence-based evaluation. This\nhighlights the importance of integrating diverse\nmodels (see Figure 13 in Appendix C).\nThis approach also enhances the objectivity of\nthe evaluations, leading to a more balanced and\nfair assessment. In some instances, LLMs-as-\njudges even surpass the fairness of human eval-\nuators, who may be subject to unconscious bi-\nases (Chen et al., 2024a). For example, when eval-\nuating the exact words spoken by Neil Armstrong\non the moon, human annotators marked the answer\n\"That's one small step for man, one giant leap for\nmankind\" as 'True'. However, LLMs correctly\nidentified the omission of the word \"a\" result-\ning in \"That's one small step for a man, one giant\nleap for mankind\" as a significant difference, and\njudged the provided answer as 'False'.\nWe specifically explored the potential for self-\nenhancement bias, where LLMs might show a ten-\ndency to favor their own outputs when acting as\njudges (Zheng et al., 2023). However, due to\nthe presence of reference answers in our setup,\nwe did not observe significant instances of self-\nenhancement bias. The reference answers provided"}, {"title": "Related work", "content": "To address the limitations of traditional n-gram-\nbased metrics like BLEU and ROUGE, various\nmodel-based methods such as BERTScore (Zhang\net al., 2020) to provide a more semantically in-\nformed evaluation. However, even BERTScore and\nsimilar embedding-based methods struggle to effec-\ntively evaluate open-ended generation (Zheng et al.,\n2023; Sun et al., 2022). Recent LLMs advances\nhave unlocked new avenues for automatic and\ncontext-aware evaluation (Chiang and Lee, 2023).\nPreviously, LLMs are utilized in three key evalua-\ntion settings including pairwise, single-answer, and\nreference-guided evaluations (Zheng et al., 2023;\nVerga et al., 2024).\nDespite some promising results, the LLM-as-\na-judge approach suffers from inherent LLM bi-\nases (Chiang and Lee, 2023; Thakur et al., 2024),\nincluding positional bias (Zheng et al., 2023; Khan\net al., 2024; Kenton et al., 2024; Shi et al., 2024),\nverbosity bias (Huang et al., 2024; Zheng et al.,\n2023), and self-enhancement bias (Zheng et al.,\n2023), where the model may favor certain response\npositions, longer answers, or their own outputs."}, {"title": "Conclusion", "content": "This study explores the potential of using LLMs-\nas-judges for evaluating open-ended generation\nwith task-specific reference answers. Our find-\nings demonstrate that leveraging diverse LLMs can\nsignificantly improve the reliability and accuracy\nof evaluations, particularly in complex and more\nopen-ended tasks. Our approach offers a promis-\ning alternative to traditional evaluation methods by\nmitigating biases and enhancing alignment with hu-\nman judgments. This study lays the groundwork for\nfuture research into more scalable and subtle evalu-\nation methods, including scenarios where reference\nanswers do not exist, thereby better reflecting the\nintricacies of real-world applications."}, {"title": "Limitations", "content": "We acknowledge certain limitations in our study.\nThe accuracy of evaluations depends on the qual-\nity and clarity of reference answers. While using\nmultiple LLMs improves reliability, relying on the\nassumption that reference answers are always ac-\ncurate may not be valid in all cases. The study\nprimarily uses binary verdicts, which might over-\nlook detailed aspects of the responses that could\nbe better captured through more comprehensive\nevaluations. Additionally, it doesn't fully exam-\nine how prompt designs affect the consistency of\nLLM judgments across different tasks. The high\ncomputational demand for running multiple LLMs\nmay also limit the usefulness of this approach in\nresource-constrained settings (Badshah and Sajjad,\n2024)."}, {"title": "Prompting", "content": "In our main experiment, we performed the zero-\nshot prompting in the following two stages."}, {"title": "Prompting Candidate LLMS", "content": "We prompted candidate LLMs (see Figure 3) to\nrecord generations for each task. We set the same\nrole and prompt structure for each candidate model\nto ensure the reproducibility of our results.\nYou are a helpful assistant. What funda-\nmental principle of human psychology is\nestablished by Amy Cuddy's power poses\nexperiments?"}, {"title": "Prompting LLMs as Judges", "content": "As we recorded the candidate LLMs' outputs in\na CSV file, we prompted LLMs-as -judges to per-\nform the evaluation (see Figure 5). In Figure 6,\njudge Llama-3-70B evaluating candidate Mistral\n7B."}, {"title": "Prompt Sensitivity", "content": "To explore how different prompt designs influence\nthe judgments of LLMs, we employ three varia-\ntions: open, detailed, and closed prompts. Each\nprompt variation is used to evaluate the candidate\nMistral 7B outputs on a TruthfulQA. The following\nexamples (see Figure 7, Figure 8, and Figure 9) il-\nlustrate how these different prompt structures guide\nthe judges' decision-making process."}, {"title": "Guidelines for Human Evaluation", "content": "This section presents the detailed guidelines for\nhuman annotators responsible for evaluating candi-\ndate LLMs outputs. These guidelines are designed\nto ensure that each annotator applies a consistent\nand objective approach throughout the evaluation\nprocess. Figure 10 shows the guidelines to human\nannotators for evaluating candidates LLMs outputs."}, {"title": "Additional Results", "content": "In this section, we provide detailed results in order\nto understand the capabilities of LLMs-as-judges."}]}