{"title": "Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion", "authors": ["Hui Shen", "Zhongwei Wan", "Xin Wang", "Mi Zhang"], "abstract": "Mamba and Vision Mamba (Vim) models have shown their potential as an alternative to methods based on Transformer architecture. This work introduces Fast Mamba for Vision (Famba-V), a cross-layer token fusion technique to enhance the training efficiency of Vim models. The key idea of Famba-V is to identify and fuse similar tokens across different Vim layers based on a suit of cross-layer strategies instead of simply applying token fusion uniformly across all the layers that existing works propose. We evaluate the performance of Famba-V on CIFAR-100. Our results show that Famba-V is able to enhance the training efficiency of Vim models by reducing both training time and peak memory usage during training. Moreover, the proposed cross-layer strategies allow Famba-V to deliver superior accuracy-efficiency trade-offs. These results all together demonstrate Famba-V as a promising efficiency enhancement technique for Vim models.", "sections": [{"title": "1 Introduction", "content": "Transformer [22-26], which leverages the attention mechanism to excel in global perception, has become the dominant architecture across various vision tasks. However, the quadratic complexity of the attention mechanism with respect to sequence length makes Transformer-based methods less efficient.\nIn recent years, State Space Models (SSMs) [6,7,15,27] have emerged as an alternative to Transformer-based methods with linear time complexity. Among SSMs, Mamba [6] is a representative work that integrates time-varying parameters into the SSM. Due to its potential scaling and adaptability, Mamba opens up new possibilities for applying SSMs in vision tasks. However, despite pioneering works such as Vision Mamba (Vim) [34] adopting Mamba for vision tasks, the increased computational and memory demands during training can pose challenges. To achieve efficient training, techniques such as token pruning [10, 13, 16, 19,31] and token fusion [1, 2, 25, 26, 33] have been successfully adopted in the context of Transformer-based architectures such as Vision Transformer (ViT) [5]. However, the exploration of token fusion techniques in Mamba-based architectures remains largely unexplored."}, {"title": "2 Related Work", "content": "State Space Models. State Space Models (SSMs) [7-9] have emerged as a promising alternative to Transformers. Gu et al. [6] introduce Mamba, which integrates time-varying parameters into the SSM and presents a hardware-aware algorithm for efficient training and inference. The superior scaling performance of Mamba shows it is a promising alternative to Transformers in language modeling. Zhu et al. [34] extend the applicability of Mamba to vision tasks by proposing Vision Mamba (Vim), which captures long-range temporal dependencies in image data. Despite their potential, training these models remains challenging due to high computational and memory requirements.\nEfficient Mamba. Recent research has focused on developing more efficient Mamba models for both natural language processing and computer vision. For language tasks, Ren et al. [20] integrate Mamba with sliding window attention to efficiently model sequences with infinite context length. In some attempts to accelerate Mamba in visual representation, Lei et al. [12] introduce a lightweight image super-resolution network that leverages Vim and distillation to achieve efficient inference. Yao et al. [30] propose an SSM framework, emphasizing spatial-spectral dynamics and computational downsizing without sacrificing accuracy."}, {"title": "3 Method", "content": null}, {"title": "3.1 Preliminaries", "content": "Mamba. State Space Models (SSMs) [7] map an input sequence $x(t) \\in \\mathbb{R}$ to an output sequence $y(t) \\in \\mathbb{R}$ through a hidden state $h(t) \\in \\mathbb{R}^N$. In the 1-D situation, discrete SSM transform sequences as linear ordinary differential equations (ODEs):\n$h'(t) = Ah(t) + Bx(t)$,\n$y(t) = Ch(t)$.\nWhere A $\\in \\mathbb{R}^{N\\times N}$ is state transition matrix, while B $\\in \\mathbb{R}^{N\\times 1}$ is input coefficient matrix and C$\\in \\mathbb{R}^{1\\times N}$ serves as an output matrix.\nMamba [3,6] is an SSM-based model with selective state spaces. To improve expressiveness and flexibility, Mamba proposes to make A and B dynamically dependent on inputs, enabling an input-aware selective mechanism for better state-space modeling. Mamba approximates this ODE by discretizing A and B with a time step parameter \u25b2 using a zero-order hold trick:\n$A = exp (\\Delta A)$,\n$B = (\\Delta A)^{-1}(exp (\\Delta A) \u2013 I) \\cdot \\Delta B$.\nAfter discretization, equation (1) is reformulated as follows:\n$h_t = Ah_{t-1} + Bx_t$,\n$Y_t = Ch_t$."}, {"title": "3.2 Famba-V", "content": "At a high level, Famba-V is a cross-layer token fusion-based method to enhance the training efficiency of Vision Mamba (Vim) models. Figure la provides an overview of Famba-V. Following Vim [34], Famba-V splits the input image into patches and then projects them into patch tokens. The token sequence is concatenated with a class token as the input to the Vim layer. In Famba-V, the class token is designed to be placed at the head of the sequence so as to prevent it from being fused with other tokens by our proposed token fusion scheme.\nInside the Vim layer, Famba-V applies token fusion to both the forward SSM and the backward SSM. Figure 1b illustrates how token fusion is performed in Famba-V. Specifically, similar to [1], Famba-V first divides the input token sequence into two sets according to the even and odd indices of the input token sequence. It then utilizes cosine similarity to measure the similarity between tokens inside these two sets and identifies r pairs of most similar tokens. Lastly, the paired tokens are fused together via averaging as a single token.\nFigure 1b illustrates how token fusion is conducted in a single Vim layer. However, Vim models consist of multiple layers. The choice of layers for incorporating token fusion plays a key role in the trade-off between accuracy and efficiency of Vim models. To explore this trade-off, as shown in Figure 2, Famba-V incorporates three different cross-layer token fusion strategies as follows. We initially apply token fusion to all layers of the Vim model, establishing this approach as our baseline strategy."}, {"title": "4 Experiments", "content": "Models and Datasets. We evaluate the performance of Famba-V using two variants of the Vim model architecture: Vim-Ti and Vim-S on CIFAR-100 [11]. Both Vim-Ti and Vim-S have 24 layers, with the key difference being their parameter counts: Vim-Ti has 7 million parameters, while Vim-S has 26 million. This difference arises from the hidden state dimension D and expanded state dimensions E, which are set to D = 192, E = 384 for Vim-Ti, and D = 384, E= 768 for Vim-S.\nBaselines and Evaluation Metrics. We compare Famba-V against two baselines: one using all-layer token fusion, and the other without any token fusion. We compare their performance under four metrics: top-1 accuracy, top-5 accuracy, training time, and the peak memory usage during training.\nImplementation Details. We conduct our experiments on NVIDIA A100 (80GB) GPUs. We followed the training settings of Vim [34] for a comparable analysis. Specifically, we applied data augmentations including random cropping, random horizontal flipping, label-smoothing regularization, mixup, and random erasing. We use AdamW with a momentum of 0.9, a batch size of 128, and a weight decay of 0.1. The training takes 300 epochs using a cosine learning rate schedule starting from 1 \u00d7 10-3 with the exponential moving average."}, {"title": "4.1 Main Results", "content": "First, we compare the performance of Famba-V under the three proposed cross-layer token fusion strategies against the two baselines on Vim-Ti and Vim-S models on CIFAR-100. When comparing with the all-layer token fusion baseline, for a fair comparison, we keep the total number of reduced tokens approximately the same across all four strategies. Specifically, for all-layer token fusion strategy, the total number of reduced tokens is 168, with 7 tokens reduced per layer; for interleaved token fusion strategy, the total number of reduced tokens is 168, with 14 tokens reduced per layer; for lower-layer token fusion strategy, the total number of reduced tokens is 171, with 9 tokens reduced per layer, starting from the 1st layer to the 19th layer; for upper-layer token fusion strategy, the total number of reduced tokens is 171, with 9 tokens reduced per layer, starting from the 6th layer. Table 1 summarizes our results. We have four observations. (1) In comparison with vanilla Vim-Ti and Vim-S without token fusion, Famba-V reduces both training time and peak memory usage during training under all three cross-layer token fusion strategies. This result demonstrates the effectiveness of token fusion in enhancing the training efficiency of Vim models. (2) For the all-layer token fusion baseline, although it achieves quite decent efficiency gains in terms of both training time and peak memory usage, it suffers from a large accuracy drop compared to the vanilla model without token fusion. (3) Among the three cross-layer token fusion strategies incorporated in Famba-V, the lower-layer token fusion strategy achieves the lowest accuracy, but provides the most significant reductions in training time and peak memory usage, making it a viable option for scenarios under resource constraints. (4) In contrast, the upper-layer token fusion strategy preserves the accuracy to the greatest extent compared to the vanilla model without token fusion while still achieving decent efficiency gains."}, {"title": "4.2 Impact of the Selection of Starting Layer", "content": "Next, we examine the impact of the selection of the starting layer to perform token fusion on the training performance under the upper-layer token fusion strategy on Vim-Ti on CIFAR-100. Starting layer refers to the first layer in the model from which token fusion is performed. For example, if the starting layer is the 8th layer, it means that token fusion is added from the 8th layer of the model and continues through the subsequent layers. Table 2 summarizes our results when changing the starting layer from 2nd to 15th. For a fair comparison, we keep the total number of reduced tokens across all the starting layer scenarios approximately the same. Therefore, the number of reduced tokens per layer is different under different starting layer scenarios. Note that we only show the results from the 2nd to the 15th layers because starting from the 1st is equal to the all-layer token fusion strategy, while starting from the 15th layer and beyond has limited efficiency gain. As shown, the relationship between efficiency and the starting layer is not linear, as the highest efficiency is achieved when the starting layer is 6th. We conjecture that for scenarios where the starting layer is lower than the 6th layer, even though token fusion is performed at an early stage, the number of reduces tokens per layer is small such that the efficiency gain is limited; for scenarios where the starting layer is higher than the 6th layer, although the number of reduced tokens per layer is larger, token fusion is performed at a later stage, which hurts its efficiency benefits."}, {"title": "4.3 Impact of the Number of Reduced Tokens per Layer", "content": "Lastly, we examine the impact of the number of reduced tokens per layer on the training performance under the upper-layer token fusion strategy on Vim-Ti on CIFAR-100. For a fair comparison, we fix the starting layer at the 6th layer given that it achieves the highest efficiency as shown in Table 2, and change the number of reduced tokens per layer from 1 to 9. Table 3 summarizes the results. As shown, there is a trade-off between accuracy and efficiency. Specifically, as the number of reduced tokens at each layer increases from 1 to 9, we observe a consistent reduction in training time and peak memory usage while accuracy drops as a trend."}, {"title": "5 Conclusion", "content": "In this paper, we presented Famba-V, a cross-layer token fusion technique that enhances the training efficiency of Vision Mamba models. Famba-V incorporates three cross-layer strategies to perform token fusion. Our experimental results show that Famba-V reduces both training time and peak memory usage during training under all three cross-layer token fusion strategies and offer improved accuracy-efficiency trade-offs compared to the all-layer token fusion approach that existing works adopt. In our future work, we plan to further optimize the efficiency of Vision Mamba by combining token fusion with other efficiency enhancement techniques [24, 28]."}]}