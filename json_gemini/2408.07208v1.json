{"title": "Hierarchical Multi-Armed Bandits for the Concurrent Intelligent Tutoring of Concepts and Problems of Varying Difficulty Levels", "authors": ["Blake Castleman", "Uzay Macar", "Ansaf Salleb-Aouissi"], "abstract": "Remote education has proliferated in the twenty-first century, yielding rise to intelligent tutoring systems. In particular, research has found multi-armed bandit (MAB) intelligent tutors to have notable abilities in traversing the exploration-exploitation trade-off landscape for student problem recommendations. Prior literature, however, contains a significant lack of open-sourced MAB intelligent tutors, which impedes potential applications of these educational MAB recommendation systems. In this paper, we combine recent literature on MAB intelligent tutoring techniques into an open-sourced and simply deployable hierarchical MAB algorithm, capable of progressing students concurrently through concepts and problems, determining ideal recommended problem difficulties, and assessing latent memory decay. We evaluate our algorithm using simulated groups of 500 students, utilizing Bayesian Knowledge Tracing to estimate students' content mastery. Results suggest that our algorithm, when turned difficulty-agnostic, significantly boosts student success, and that the further addition of problem-difficulty adaptation notably improves this metric.", "sections": [{"title": "1 Introduction", "content": "With a rise in technologically-assisted education in the twenty-first century, intelligent tutoring systems have received significant popularity across various domains (Mousavinasab et al., 2021; Paladines & Ramirez, 2020). Though intelligent tutoring systems spread across multiple domains of artificial intelligence, recent literature has argued that reinforcement learning models are optimal for adaptive learning environments (Yan & Lin, 2022). This is due to the ability of reinforcement learning to best sequence actions in uncertain and dynamic landscapes without prior data on students or educational content (Kaelbling et al., 1996). As a result, abundant resources have been allocated toward the research and expansion of reinforcement learning applications in education.\nWithin the realm of reinforcement learning, the multi-armed bandit (MAB) stands out as a distinctive framework for addressing and deploying exploration-exploitation trade-offs in a generalized manner. In the MAB framework, an agent is equipped with multiple arms, each representing a distinct action it can perform. The objective is to strategically select optimal arms over successive interactions with the environment, given the rewards gained from its prior actions chosen. This strategy aims to maximize cumulative rewards in a transient system.\nIntelligent tutoring systems utilizing the MAB framework have since been applied for education (Clement et al., 2015) with various modifications made for performance improvement (Mu et al., 2018; Segal et al., 2018). However, there exists a significant lack of open-sourced MAB intelligent tutors, which is vital for real-life applications of MAB on pedagogy (observational based on present-"}, {"title": "2 Related Work", "content": "2.1 Multi-Armed Bandit Intelligent Tutoring Frameworks - Without Difficulty Levels\nThe domain of MABs for intelligent tutoring is well defined in the literature (Mui et al., 2021) and indeed has been distinguished in previous research. Clement et al. (2015) proposed using MABs to recommend activities that optimize the exploration-exploitation trade-off for personalized student progression sequences, developing their algorithm, Zone of Proximal Development and Empirical Success (ZPDES), to deliver problems within students' Zone of Proximal Development (ZPD) (Chaiklin, 2003). Proceeding literature modified ZPDES for more defined environments, with Mu et al. (2017) modifying ZPDES to use probabilistic entropy for finding the initial ZPD and Mu et al. (2018) combining it with the Multiscale Context Model (MCM; Pashler et al., 2009) concept forgetting mechanism.\nAnother family of MAB intelligent tutors follows Lan & Baraniuk (2016), who developed upper confidence bound (UCB)-based algorithms (Auer et al., 2002) to maintain expected arm rewards with confidence intervals for personalized learning actions (PLAs) that maximize learning. Manickam et al. (2017) built on this with an estimate framework for students' prior knowledge with sparse factor analyses on their previous responses. Additionally, they investigated new policies for selecting PLAs that were adapted for binary-value student correctness rewards."}, {"title": "2.2 Multi-Armed Bandit Intelligent Tutoring Frameworks - With Difficulty Levels", "content": "Difficulty level adoption has been well observed in modern MAB algorithms with disparate methods utilized. RiARiT, from Clement et al. (2015), utilizes multiple nodes per individual activity, each node representing a more difficulty level than the last, in its progression graph. This allows the ZPD to automatically choose whether it should pursue a proceeding activity or a previously tested activity of higher difficulty. Andersen et al. (2016) used a novel probabilistic knowledge matrix to facilitate their progression, with rows and columns representing proceeding concepts and difficulty levels respectively. Segal et al. (2018) performed difficulty adjustment by linearly scaling the exploration factor and by directly adjusting harder questions' weights based on a student's answer's correctness."}, {"title": "3 Methodology: Platform Architecture", "content": "Our platform, Aiphabet, is a secondary school informal learning organization for teaching artificial intelligence. The curriculum (Macar et al., 2023) is cocurated by many Columbia University faculty"}, {"title": "3.1 Platform Section, Concept, and Problem Definition", "content": "We begin by defining educational content sections for students to progress through. In our educational landscape, we define each section as consisting of an animated video lecture (approximately three to five minutes long) or a slideshow of course content. Then, following each section is a quizzing stage where students are asked sequential, material-related questions. Finally, after the question sequencing is completed, the student may proceed to another section of their choice, provided they have finished all required prerequisite sections.\nThe educational content of a section is comprised of multiple concepts. These concepts form a concept progression tree, where an arbitrary concept can become teachable after its prerequisites have been understood and mastered. For example, if a section is titled \"The Perceptron\", corresponding concepts may be \"Biological Inspiration\", \"Classification Function\", and \"The XOR Problem.\"\nUnder each concept, we define problems as available questions for examining students' knowledge mastery of said concept. Difficulty levels are scores to rate problem complexity by a domain expert within the range d\u2208 [1,5] where higher levels denote more difficult questions. These parameters, however, are not given to the student but are used for internal algorithm calculations (Section 5.2)."}, {"title": "4 Methodology: ZPDES Foundation for Algorithmic Progression", "content": "A MAB framework is used to select the concepts for students to be quizzed on in order to best solidify the educational content of a section. However, as multiple questions with varying difficulties exist for quizzing a particular concept with, it is not sufficient to have a single MAB instance selecting both the concepts and problems at hand.\nWe solve this problem by implementing a progression algorithm embedded with multiple MAB agents to explore the exploration-exploitation trade-off for students among both concepts and problems. First, a single, MAB for concept selection (high-level decision; known as the concept MAB) chooses a concept within the given section to quiz the student on, and then a corresponding MAB instance for the selected concept (low-level decision; known as the problem MAB) chooses a problem from that concept's question bank to give to the student (see Appendix 9.4 for a visual representation). Initializations for the proceeding parameters can be found in Appendix 9.1. We open-source our code for educators and researchers to implement and build upon."}, {"title": "4.1 ZPDES Multi-Armed Bandit Design Foundation", "content": "The base algorithm used for our MAB adaptation extends the Mu et al. (2018) MAB intelligent tutoring implementation. Mu et al. (2018)'s algorithm combines the ZPDES algorithm and MCM model for characterizing concept forgetting in a single study session.\nThe ZPDES algorithm aims to select student activities within a ZPD frontier. The ZPD is an educational psychology idea that hypothesizes that optimal student activities should be difficult enough for a student to be challenged by but not outside of a student's current problem-solving abilities (Chaiklin, 2003). As a result, concepts and problems within a student's ZPD can challenge students while preventing frustration, which increases motivation and student engagement.\nZPDES is a MAB algorithm designed to deploy optimized teaching sequences by exploring the exploitation-exploration trade-off for student activities. By keeping track of belief states (denoted as unmastered or mastered) for each activity, it proposes unmastered activities for students to solve that are within their ZPD (i.e., unmastered activities that have mastered prerequisites). This occurs"}, {"title": "4.2 MCM Algorithm Adaptation", "content": "In Mu et al. (2018), the MCM model is integrated into ZPDES to approximate students' decaying memory traces over time (Pashler et al., 2009). We choose to incorporate this as well for our algorithm to parallel our work with present literature.\nA memory trace \\(x_{a,i}\\) (induced memory change) of an activity \\(a\\) the \\(i\\)th time after it has been seen by a student decays with the time after its activation. This can be modeled according to the equation:\n\\[x_i(t + \\Delta t) = x_i(t)e^{-\\frac{\\Delta t}{\\tau_i}}\\]\nwhere \\(\\tau_i\\) is the decay time constant with the constraint \\(\\tau_i < \\tau_{i+1}\\), \\(t\\) is the activation time, and \\(\\Delta t\\) is the time since activation (Mu et al., 2018; Pashler et al., 2009). The probability of receiving an activity is related to the memory strength \\(S_{a,t}\\) of a problem \\(a\\) after it has been seen \\(n\\) times:\n\\[S_{a,t} = \\frac{1}{\\Gamma_n} \\sum_{i=1}^{n} \\xi_i x_{a,i}(t), \\ \\ \\text{where }\\Gamma_n = \\sum_{i=1}^{n} \\xi_i\\]\nwhere each \\(\\xi_i\\) is a weight for each memory trace \\(x_{a,i}\\) (Mu et al., 2018; Pashler et al., 2009)."}, {"title": "5 Methodology: High-Level Concept Multi-Armed Bandit", "content": "With a MAB design foundation formed, we will now proceed to describe our induced MAB hierarchy and difficulty extensions to the system, beginning with the MAB for concept selection."}, {"title": "5.1 Concept Progression Trees", "content": "Each section on the platform has multiple concepts that are taught by the educational content. \u03a4\u03bf solidify student knowledge, the MAB for concept progression is designated to quiz students on the entire section that they have learned, beginning with the root prerequisite concepts and leading to the more advanced concepts previously taught. For a concept to be taught, all prerequisites that lead to it must have a mastered belief state. Figure 1 details an example of how a concept progression tree may be defined, noting that concepts can run in series, parallel, and share postrequisites."}, {"title": "5.2 Problem Difficulty", "content": "As the difficulty of the problems solved (Section 3.1) affects the estimated knowledge trace for each concept, we choose to incorporate the difficulty of completed problems into the reward calculation.\nTo account for difficulty, when a problem is completed, we update response correctness according to:\n\\[C_{a,k} = C_{a,k} * ((\\frac{\\sigma(d-3)}{\\frac{1}{2}} + \\frac{1}{2})\\]\nwhere \\(d\\) is the difficulty of the problem and \\(\\sigma(x)\\) is the sigmoid function. We choose to perform \\(d-3\\) to center the difficulty scale along the sigmoid function (as \\(d\\)\u2208 [1,5]). The sigmoid function then"}, {"title": "6 Methodology: Low-Level Problem Multi-Armed Bandit", "content": "After concept selection, a problem MAB unique to the selected concept chooses the problem for student presentation. This allows for the concept's MAB to obtain parameters specific to the concept and resultant question bank. We will now proceed to describe our difficulty extensions to the system, beginning with the MAB for concept selection."}, {"title": "6.1 Problem Progression Trees", "content": "Each concept has multiple associated problems from that concept's question bank that make up a problem progression tree. These trees have a depth of one and consist only of edges in parallel from a root to each associated problem from that concept's question bank. As a result, the MAB framework is capable of intelligently selecting all problems with trade-off considerations when the concept is selected. Figure 1 demonstrates this problem progression tree design on an abstract concept."}, {"title": "6.2 Initial Problem Difficulty Integration", "content": "As all problems have an initial equal probability for selection through the base design of our MABS framework, we seek to integrate the difficulty of the problems in order to skew the initial weights. Therefore, very easy or hard problems can be initially discouraged before information on the student in the current section is available.\nTo accomplish this, we introduce a problem multiplier variable \\(m_a\\) that is multiplied onto the problem weight \\(w_a\\) after the weight calculation (Equation 2) and before the normalization and exploration steps (Equation 3). This allows us to skew resultant weights according to difficulty adjustments.\nWe choose to initialize the multipliers \\(m_a\\) for each problem within a concept according to:\n\\[m_a = e^{-\\xi^2(d-3)^2}\\]\nwhere \\(d\\) the difficulty of the problem (as aforementioned) and \\(\\xi\\) is a hyperparameter for initial problem weight skewing. After performing a centering along \\(d = 3\\), this equation, a Gaussian function, allows intermediate problems to be encouraged while discouraging problems of increasing simplicity or difficulty. Thereby, the MABs framework is more likely to initially suggest a problem meeting these conditions."}, {"title": "6.3 Transient Problem Ranking Integration", "content": "As student correctness data is collected on problems within the given concept, students will demon- strate skill aptitudes which, even if the belief state of the given concept is unmastered, must be perceived and processed for further recommending difficulty rankings. Thereby, students that have completed difficult problems can be recommended problems of greater difficulty and students unable to complete difficult problems can be recommended problems of lesser difficulty.\nTo accomplish this, we incorporate and modify the Update Question Grade calculation step from the Multi-Armed Bandits based Personalization for Learning Environments (MAPLE) algorithm (Segal et al., 2018), which combines difficulty ranking with MABs for this exact problem. MAPLE updates problem weights according to the following method. For our implementation, we choose to make various modifications to this step in MAPLE for our application, which can be found in Appendix 9.3. Along with tuning of the initial parameters (Appendix 9.1), the final implementation of the problem ranking integration therefore becomes:"}, {"title": "7 Results: Student Simulation", "content": "To validate our hierarchical MAB architecture before real-world implementation, we utilized BKT to simulate a roster of 1500 students in an adaptive learning environment (Anderson et al., 1995; Badrinath et al., 2021; Appendix 9.5). First, we fitted our hierarchical MAB application for our organization's AI material, where five of our education sections were utilized for the MAB intelligent tutoring. These five sections were specifically picked as they have multiple concepts incorporated into each section, versus the other shorter sections with only one concept each, which ensured the algorithmic results captured are indeed hierarchical.\nAs the data available from our organization's course material was not extensive enough for accurate result interpretation, we chose to transform the ASSISTments dataset (Wang et al., 2015) by mapping their data to our own concepts and questions to train the BKT model with. Question difficulty was deciphered by calculating the inaccuracy rate to each problem and adapting it linearly (da = 4 \u00b7 inaccuracy ratea + 1) to obtain a difficulty score within our [1,5] scale (Section 3.1).\nWe defined three groups for simulation: one with a randomized question sequence (where a question is picked at random from a section's question bank), one with a difficulty-agnostic hierarchical MAB sequence (where problem difficulties are not utilized), and one fully realized hierarchical MAB sequence with problem difficulties included. Each group contained 500 simulated students. We simulated the hierarchical MAB students to the algorithms' completions and ran the randomized question sequence algorithm for the exact number of questions that was previously required by the difficulty-agnostic hierarchical MAB sequence."}, {"title": "8 Conclusion and Future Work", "content": "In this paper, we present a deployable, open-source, and state-of-the-art MAB intelligent tutor for remote education. We combine prior research into MAB intelligent tutoring literature to synthesize our algorithm, which creates pedagogical advantages for instructors like assimilating concept maps and problem difficulties. Our algorithm utilizes hierarchical MABs which contain separate MAB agents to select concepts and problems. Lastly, we perform BKT simulations to build evidence for our algorithm's efficacy in real-world educational environments, which contrasts a randomized, MAB difficulty-agnostic, and MAB difficulty inclusion problem sequencing.\nFuture work includes the need for real student trials that aren't dependent on student knowledge models or idealized conditions. This invites the possibilities of other considerations, such as real-time self-updating of problem difficulties, window size trade-off investigation, and material redirects for underperforming students, to name a few."}, {"title": "9 Appendix", "content": "9.1 ZPDES, MCM, and MAPLE Parameter Choices\nWe obtained many of our ZPDES and modified MAPLE algorithm initial parameters and hyperparameters through tuning before student simulation to converge onto values that a) progress students at a reasonable rate and b) avoid overvaluing nor neglecting both complex and simple problems. For ZPDES, we obtain \u03b3 = 0.1, \\(w_{a,o} = 0.5\\), \\(\\xi = 7.37\\), Lconcept = 4, Lproblem = 2, h = 0.74. Problems unlocked by completing prerequisites are also appended weights of \\(w_{a,1} = 2\\). For MAPLE, we obtain \\(\\alpha = 1.3\\).\nFor the MCM algorithm, Pashler et al. (2009) used mass simulations for optimizing \\(\\xi_i\\) and \\(\\tau_i\\). However, our lack of prior question data for model fitting leads us to follow Mu et al. (2018)'s implementation (who had an identical dilemma for parameter value choice) of choosing \\(\\xi_1 = 1\\) and \\(\\tau_i = i\\). Given the brevity of our platform's sections (Appendix 9.5), we neglect tuning a memory threshold \\(m_t\\) or a memory multiplier \\(m_m\\) value for our individual application."}, {"title": "9.2 Miscellaneous MAB Modifications", "content": "We modify the algorithm so that the belief states for problems successfully completed by the student are marked as mastered and therefore are not given to the student again. Furthermore, if all problems in a problem MAB instance have been completed, the conceptual MAB will have the corresponding concept's belief state updated to mastered as no more problems within the given concept are available to give to the student."}, {"title": "9.3 MAPLE Modifications List", "content": "The following modifications were made to the Update Question Grade calculation step of the MAPLE algorithm for integration with our hierarchical MAB algorithm:\n*   Instead of updating the weight problem weight \\(w_a\\) directly, we choose to update the multipliers for each problem \\(m_a\\).\n*   Our learning environment assumes that only binary grades are possible (0 and 1) for the student grade \\(g_s\\). Therefore, we lower the number of hyperparameters by removing the passing grade threshold conditional \\(g_s > \\eta\\) and replace it with \\(C_{a,k} == 1\\), that is, if the exercise given at time k is answered correctly.\n*   Only one normalization factor \\(\\alpha\\) is used and instead the normalization factor used for lowering multipliers (\\(\\beta_3\\) in Segal et al., 2018) is the inverse of \\(\\alpha\\).\n*   If the last question was answered correctly, we also decrease the multipliers for questions less difficult than \\(q_a\\) by the inverse of \\(\\alpha\\).\n*   If the last question was answered incorrectly, we also increase the multipliers for questions less difficult than \\(q_a\\).\n*   We remove direct exploration rate \\(\\gamma\\) calculations from MAPLE.\n*   As each problem can only be answered correctly one time (see Appendix 9.2), we choose to remove the exponential reward multiplier (\\(e^R\\) in Segal et al., 2018) from the algorithm."}, {"title": "9.4 Hierarchical MAB Schematic", "content": "By changing the problem multipliers for problems easier than the last given question, the algorithm counteracts the initial problem difficulty integration incurred by Equation 8. However, as all problems (other than those of equal difficulty to \\(q_a\\)) now have weights updated according to difficulty, it is no longer necessary to change the exploration rate \\(\\gamma\\)."}, {"title": "9.5 pyBKT Setup", "content": "The pyBKT model implements a Hidden Markov Model (HMM) with sequences of the students' response correctness as observable nodes and students' latent knowledge throughout proceeding knowledge states as hidden nodes. The model trains on past students' response history to fit the HMM's learn (probability of transmission from an unlearned state \\(\\lambda_t = 0\\) to a learned state \\(\\lambda_{t+1} = 1\\)), prior (probability of initial learned state \\(\\lambda_0 = 1\\)), guess (probability of responding correctly despite a presently unlearned state \\(\\lambda_t = 0\\)), and slip (probability of responding incorrectly despite presently learned state \\(\\lambda_t = 1\\)) parameters (Badrinath et al., 2021).\nWhen an unlearned latent state is present (\\(\\lambda_t = 0\\)), we provide a binary correctness \\(C_{t,k} = 1\\) at the current guess probability and otherwise respond \\(C_{t,k} = 0\\). Conversely, when a learned latent state is present (\\(\\lambda_t = 1\\)), we provide correctness \\(C_{t,k} = 0\\) at the current slip probability and otherwise respond \\(C_{t,k} = 1\\) (identical to Mu et al., 2018).\nWe do not implement any form of forgetting (learning states can not degrade from \\(\\lambda_t = 1\\) to \\(\\lambda_{t+1} = 0\\)) in our simulations to simplify our assumptions, doing so in part to our observed low question attempt count for section completeness (see Figure 2). Consequently, we neglect the MCM algorithm's weight contributions for this experimentation."}]}