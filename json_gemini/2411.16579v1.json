{"title": "Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision", "authors": ["Zhiheng Xi", "Dingwen Yang", "Jixuan Huang", "Jiafu Tang", "Guanyu Li", "Yiwen Ding", "Wei He", "Boyang Hong", "Shihan Dou", "Wenyu Zhan", "Xiao Wang", "Rui Zheng", "Tao Ji", "Xiaowei Shi", "Yitao Zhai", "Rongxiang Weng", "Jingang Wang", "Xunliang Cai", "Tao Gui", "Zuxuan Wu", "Qi Zhang", "Xipeng Qiu", "Xuanjing Huang", "Yu-Gang Jiang"], "abstract": "Training large language models (LLMs) to spend more time thinking and reflection before responding is crucial for effectively solving complex reasoning tasks in fields such as science, coding, and mathematics. However, the effectiveness of mechanisms like self-reflection and self-correction depends on the model's capacity to accurately assess its own performance, which can be limited by factors such as initial accuracy, question difficulty, and the lack of external feedback. In this paper, we delve into a two-player paradigm that separates the roles of reasoning and critique models, where the critique model provides step-level feedback to supervise the reasoning (actor) model during both test-time and training-time. We first propose AutoMathCritique, an automated and scalable framework for collecting critique data, resulting in a dataset of 76, 321 responses paired with step-level feedback. Fine-tuning language models with this dataset enables them to generate natural language feedback for mathematical reasoning. We demonstrate that the critique models consistently improve the actor's performance on difficult queries at test-time, especially when scaling up inference-time computation. Motivated by these findings, we introduce the critique-based supervision to the actor's self-training process, and propose a critique-in-the-loop self-improvement method. Experiments show that the method improves the actor's exploration efficiency and solution diversity, especially on challenging queries, leading to a stronger reasoning model. Lastly, we take the preliminary step to explore training self-talk reasoning models via critique supervision and showcase their potential. Our code and datasets are at https://mathcritique.github.io/.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancement of large language models (LLMs) [1; 2; 3; 4; 5], significant progress has been made in enhancing their reasoning capabilities [6; 7; 8; 9; 10; 11]. By prompting or training language models to reason step-by-step like humans (i.e., chain-of-thought, CoT), these models have demonstrated impressive reasoning abilities [6; 9; 12]. Recently, OpenAI's o1 model has introduced a new paradigm shift, exploring to increase inference-time computation in language models and explicitly generate longer chains of thought [13]. This enables them to tackle more complex reasoning tasks that even humans find challenging, such as problems in the domains of science, coding, and mathematics [14; 15; 16; 17]."}, {"title": "2 Preliminaries", "content": "In the two-player setting studied in this paper, there are two roles: the actor model and the critique model. Also, there are three primary tasks [22]: reasoning, critique, and refinement.\nIn the reasoning task, the actor model $\\pi_{\\theta}$ parameterized by $\\theta$ is given a reasoning problem $x$ and is expected to generate a response $y = \\pi_{\\theta}(x)$. This response includes both the answer to the problem and the reasoning trajectory. The accuracy of this response can be evaluated using a reward function $r(x,y)$.\nNext, the critique model $\\pi_{\\phi}$ parameterized by $\\phi$ performs the critique task, where, given the problem and response, it generates critical feedback $c = \\pi_{\\phi}(x, y)$. Notably, if the oracle reward function of the response is not given, the critique task consists of two subtasks: the discriminative task and the feedback generation task. The former determines whether the response contains flaws, while the latter generates constructive natural language feedback.\nFinally, we define the refinement task, in which, given the problem, response, and critique, the actor generates a new response $y' = \\pi_{\\rho}(x, y, c)$\u2014this is also known as conditional refinement. Alternatively, we can define direct refinement $y' = \\pi_{\\theta}(x, y)$, where the actor provides an improved answer based on an existing answer without conditioning on a critique, which is also referred to as \"self-correction\" [18].\nThis process can proceed in multiple rounds. We define that in the initial round (round 0) only the actor operates, generating a response based on the problem. In round i, the critique model first generates a new critique based on the interaction history, which is represented as:\n$C_{i} = \\pi_{\\phi}(x, y_{0}, c_{1}, y_{1}, \\ldots, c_{i-1}, y_{i-1})$.\nThen, the actor generates a new refinement based on the previous interaction history, represented as:\n$y_{i} = \\pi_{\\theta}(x, y_{0}, c_{0}, y_{1}, c_{1}, \\ldots, c_{i-1}).$"}, {"title": "3 AutoMathCritique: An Automated and Scalable Framework to Collect Step-level Critique Data", "content": "To train critique models capable of delivering step-level supervision and constructive feedback for reasoning, we introduce AutoMathCritique\u2014an automated and scalable framework for collecting critique data. This framework consists of three main stages: flawed reasoning path construction, critique generation, and data filtering. Using AutoMathCritique, we create a dataset containing 76, 321 samples named MathCritique-76k.\nWe focus on the field of mathematical reasoning, so we utilize two of the most widely used datasets: GSM8K [35] and MATH [36]. The queries used for our subsequent data construction primarily come from their training sets, and we also leverage their original annotated responses to train the actor reasoning models. Our in-domain test set is composed of their test sets."}, {"title": "3.1 Construction of Flawed Reasoning Paths", "content": "To create high-quality critique data, we first need to construct a dataset of reasoning paths that includes some flaws. To better control the quality and diversity of the generated flawed reasoning paths, and to facilitate the subsequent construction of critique data, we leverage several distinct response generation (RG) approaches. These strategies encompass different aspects of the errors, such as their location or specific details. We mainly use Llama3-8B [5] as our actor model for sampling."}, {"title": "3.2 Generation of Critiques", "content": "Step-level critique generation. When generating critique data, we enhance quality by checking each step to identify the first error in the solution, which in turn facilitates the refinement process."}, {"title": "3.3 Data Filtering", "content": "Although we have constructed a large amount of critique data paired with flawed responses, the quality of this data is not guaranteed, and low-quality data could weaken the performance of the critique model. To address this, we apply a filtering process. Specifically, we use Monte Carlo sampling: each (query, response, critique) tuple is fed into the actor model for refinement. The refinement process is repeated 10 times, and only when the accuracy exceeds a predefined threshold $T = 0.3$ is the critique data retained. This process is referred to as soft filtering. In contrast, hard filtering is employed when the critique is considered valid if at least one of the $k$ refinements produces a correct result. In practice, we adopt soft filtering because it prevents the omission of high-quality critique data due to occasional model errors. Furthermore, it minimizes the risk of including low-quality critiques that the actor model does not follow, but instead refine based on its own knowledge, resulting in a correct response. Note that our method does not completely eliminate low-quality data, but we strive to achieve a balance between quality and quantity. Additionally, we randomly sampled 100 data points 5 times and had crowdsourced annotators perform the checking. We find that the rate of low-quality data is 1.2%."}, {"title": "4 Critique Models Improves LLM Reasoning through Test-time Supervision", "content": "In this section, we begin by training critique models to provide step-level supervisory signals and useful feedback on reasoning paths, along with the actor reasoning models that own reasoning and refinement ability (Section 4.1). We then explore the role of critique models in supporting the actor reasoning model at test-time (Section 4.2), showing that they significantly enhance the actor's performance in tackling difficult problems. Furthermore, as we scale up inference-time computations, we observe that the critique model continues to raise the performance ceiling of the reasoning models."}, {"title": "4.1 Fine-tuning Critique Models and Actor Reasoning Models", "content": "Training critique models with MathCritique-76k. We train the critique models through super-vised fine-tuning with the collected MathCritique-76k. Specifically, we use the standard language modeling loss. Given a dataset $D_{critique} = \\{(x, y, c)\\}_{i=1}^{N}$, the loss for the critique model $\\pi_{\\phi}$ is as follows:\n$L_{critique}(\\phi) = \\mathbb{E}_{(x,y,c)\\sim D_{critique}} [\\log \\pi_{\\phi}(c|x, y)],$ (1)\nIn this way, we can obtain a critique model that provides step-level supervision and constructive feedback on reasoning paths for actor models.\nTraining actor models with basic reasoning and refinement ability. We then train reasoning models in our two-player setting. The models are trained using the training sets of GSM8K and MATH, containing 7, 473 and 7, 500 samples, respectively. We denote the mixed response training set as $D_{reason} = \\{(x, y)\\}D_{reason}$. Additionally, to equip the models with the ability to perform refinement tasks according to the critique feedback, we utilize GPT-4 to annotate 8k refinement samples (half of which are from MATH and the other half from GSM8K), denoted as $D_{refine} = \\{(x, y, c, y')\\}D_{refine}$, where $y'$ represents the refined reasoning path generated based on the critique $c$. Each refinement sample is verified to ensure the correctness of its final answer. The loss of training actor reasoning model $\\pi_{\\theta}$ is as follows:\n$L_{actor} (\\theta) = \\mathbb{E}_{(x,y)\\sim D_{reason}} [\\log \\pi_{\\theta}(y|x)] + \\beta \\times \\mathbb{E}_{(x,y,c,y')\\sim D_{refine}} [\\log \\pi_{\\theta}(y'|x, y, c)],$ (2)\nwhere $\\beta$ is a hyper-parameter that balances the learning of reasoning and refining."}, {"title": "4.2 Critique-based Supervision Improves Test-time Reasoning Performance", "content": "In this section, we investigate the impact of trained critique models in supporting the reasoning model at test-time. Specifically, we examine their effectiveness in enhancing the actor's reasoning performance, identify the types of problems where performance improvements are observed, and assess whether scaling up test-time computation further elevates the actor's performance ceiling."}, {"title": "4.2.1 Experimental Setups", "content": "Backbone models. In our main experiments, we fine-tune the actor models using Llama3-8B-Base, following previous work [16; 39; 17]. This model demonstrates non-trivial performance on mathematical reasoning tasks while leaving room for improvement, making it an ideal testbed for our study. We fine-tune the critique models using the fine-tuned models Llama3-8B and Llama3-70B,"}, {"title": "4.2.2 Empirical Results and Findings", "content": "Critique models are highly effective at identifying the correctness of reasoning, offering con-structive feedback for erroneous responses, and improving the overall accuracy of the actor. We compare our critique models with SOTA models used as critics, and the results are presented in Table 2. We observe that compared to current state-of-the-art (SOTA) models, our 8B critique model significantly outperforms GPT-3.5, while our Llama3-Critic-70B model achieves performance comparable to GPT-4 series models.\nSpecifically, the reasoning path judgment accuracy of our 8B critique model reaches 79.37% on GSM8K and 75.74% on MATH, exceeding GPT-3.5-Turbo by 16.52 and 24.46 percentage points, respectively. Additionally, in terms of helpfulness, it outperforms GPT-3.5-Turbo by 17.70% and 1.93% on GSM8K and MATH, respectively. Moreover, our 70B critique model demonstrates even stronger performance. As to discriminability, it surpasses GPT-4-Turbo and GPT-40 on the GSM8K dataset and achieves results close to these SOTA models on MATH. Its correction accuracy on both datasets approaches that of GPT-4 series models, ultimately leading to comparable actor accuracy under its guidance.\nCritique models assist the actor in better handling challenging queries. Next, we investigate the distribution of performance gains brought by the critique model across different difficulty levels. The process involves generating 100 responses from the actor model for each query and categorizing the queries into 5 difficulty levels based on the number of correct responses associated with each"}, {"title": "5 Critique-in-the-loop Self-Improvement for Better Reasoning Models", "content": "Motivated by the test-time findings in Section 4.2 that critique models significantly aid in solving challenging problems, and that they substantially raise the reasoning performance ceiling when scaling up computation, we integrate the critique-based supervision into the actor model's iterative exploration and learning process. We present a critique-in-the-loop self-improvement method, which scales up exploration computation on challenging queries and leads to the development of stronger reasoning models."}, {"title": "5.1 Vanilla Self-Improvement Method", "content": "Self-improvement is an exploration and learning method [42; 43; 16; 43; 44; 45]. It iteratively leverages the actor reasoning model's correct responses to gradually enhance its problem-solving abilities. The process involves T iterations, where each iteration consists of two steps: exploration and learning.\nIn the exploration step of iteration t, we sample N responses for each query $x_j \\in D_{reason}$ from the previous model $\\pi_{\\theta}^{t-1}$, i.e., $\\hat{y_j} = \\pi_{\\theta}^{t-1}(x)$. Each data point is then filtered using the reward function $r(x, y)$, where only correct solutions are retained to form a new dataset $D^t = \\{(x, y)\\}_{j=1}^{n_t}$."}, {"title": "5.2 Critique-in-the-loop Self-improvement", "content": "Introduce critique models for high-coverage exploration. Motivated by our prior findings in Section 4.2 that critique models enable actors to achieve greater performance gains on harder queries, we introduce critique models to the self-improvement process and propose a critique-in-the-loop self-improvement approach.\nThis method is built upon self-improvement, and during the exploration step of iteration $t$, critique models $\\pi_{\\phi}^{t}$ are instructed to provide feedback on the responses of actor $\\pi_{\\theta}^{t-1}$, and the actor is then prompted to perform refinements accordingly. After that, correct refinements are then added to the training set. Since we can assume the availability of an oracle reward function for the training set, critiques are only applied to incorrect responses, while correct responses are directly included in the dataset. This design minimizes the risk of low-quality critiques negatively affecting originally correct responses. In this way, we increase the coverage of solutions for harder queries, and significantly reduce the tail-narrowing problem [34].\nDifficulty-aware computation allocation for exploration. Furthermore, building on our previous findings in Section 4.2 that scaling inference-time computation can improve the efficiency and quality of exploration, we allocate more computation for exploration and critique to harder problems. This involves performing additional response generation, critique, and refinement to obtain high-quality and diverse solutions.\nIn practice, we employ a simple difficulty-based computation allocation strategy, as it has proven sufficiently effective. For incorrect initial responses, we classify them as difficult and allocate L times of critique and refinement. For correct initial responses, they are considered simple and are directly added to the training set without further critique or refinement. This approach further mitigates the long-tail issue of self-improvement, enhances sampling quality, and improves the overall performance [34].\nWe summarize the critique-in-the-loop self-improvement method in Algorithm 1."}, {"title": "5.3 Experimental Results and Findings", "content": "Implementation details. We set the self-improvement process to run for 3 iterations, as in previous works [48], the model's performance tends to saturate after 3 iterations of exploration and learning. During the exploration stage, we set the temperature to 0.7 and the number of samples to 5 or 10. During the learning stage, we set the learning rate to 2e - 5 and the number of epochs to 1.\nCritique-in-the-loop self-improvement consistently improves reasoning performance. The evaluating results of our method are shown in Figure 6. We can observe that: (1) Increasing the number of samples during exploration improves performance, with the performance upper bound"}, {"title": "6 Discussion and Analysis", "content": "6.1 Scaling Properties of Critique Models\nAs in previous work [35], we study the scaling properties of critique models, trying to investigate whether they can supervise models of different scales, particularly those larger and stronger than themselves. In this study, we conduct experiments using the Qwen-2.5 series of models [49], which span a wide range of scales (1.5B, 3B, 7B, and 14B). We train a critique model of 3B scale and use it to supervise trained actor reasoning models of all sizes. Other experimental settings are consistent with Section 4.2.\nThe evaluating results are shown in Figure 9. In the figure, \u201coracle\u201d indicates whether we have an oracle reward function to assist the critique model in making judgments. With an oracle reward function, only incorrect responses are passed to the critique model; otherwise, all responses are passed to the critique model. From the results, we can observe that: (1) Regardless of scale, the 3B critique model can provide effective supervision, indicating that smaller critique models can help supervise larger actors to a certain extent. (2) With the oracle reward function, the critique model does not need to perform discriminative tasks and only needs to provide useful feedback, resulting in greater performance improvements. (3) As the model scale increases, the performance improvement provided by the critique model on simpler datasets like GSM8K becomes marginal. However, on the more challenging MATH, the critique model continues to deliver significant performance gains even for the largest model."}, {"title": "6.2 How do Critique Models improve Majority Voting?", "content": "Majority voting is one of the most commonly used techniques for scaling test-time computation. Following previous work [32], we study the relationship between the correct frequency of multiple samples and the performance of majority voting, while also examining the impact of critique models. Specifically, consistent with the settings in 4.2, we use an actor reasoning model and a critique model trained with supervised fine-tuning. For each query, we sample 1,000 responses in parallel. The experimental results are shown in Figure 10.\nWe observe that critique models improve both the overall correct frequency and the performance of majority voting. Delving deeper, we can find a significant failure mode when critique models are not used, where the correct answer appears with a relatively high frequency (e.g., 40%), but a"}, {"title": "6.3 Should test-time computation be scaled sequentially or in parallel?", "content": "In the two-player paradigm, test-time computation can be scaled either in parallel by sampling multiple (response, critique, refinement) triplets [22; 50; 15], or sequentially by generating critiques and refinements iteratively after an initial response [14; 16; 15]. Here, we explore the performance of these two approaches. Notably, in our implementation of the sequential approach, to avoid potential issues of context window length limits, we use the following strategy: given a query $x$, the actor first generates a response $y_0$. For the i-th critique task (i > 0), only the query and the (i - 1)-th response or refinement are provided. Similarly, for the i-th refinement task (i > 0), only the query, the (i - 1)-th response or refinement, and the i-th critique are provided."}, {"title": "7 A Step Further: Training Step-level Self-Talk Reasoning Models via Critique Data", "content": "Motivation and method. In this work, we focus on the two-player paradigm, leveraging critique models to provide step-level supervision and feedback for actor models. Recently, OpenAI's o1 model [13] has pushed the boundaries of large reasoning models' capabilities. With its self-talk output format, it can autonomously plan, reflect, critique, correct, backtrack, and more during the thinking process, marked by phrases such as \u201cwait\u201d and \u201calternatively\u201d. Therefore, we investigate whether it is possible to construct self-talk data with step-level critique supervision, and propose the preliminary self-talk-via-critique method. Specifically, it has three main steps:\n1. Construct an initial thinking chain that has step-level reflection. Given a query and a reasoning path, we first use AutoMathCritique to generate critique data. Feedback on each reasoning step provided in the critique is then inserted into the reasoning path, constructing"}, {"title": "8 Related Work", "content": "Training LLMs for reasoning through exploration and learning. Multi-step reasoning, such as mathematical reasoning and logical reasoning, is a challenging task for large language models (LLMs). Researchers have proposed prompting methods represented by Chain-of-Thought (CoT) to enable LLMs to think and reason step by step like humans, and then produce answers based on the reasoning process, significantly improving the model's reasoning performance [6; 41]. To enhance the reasoning ability of models, previous work has focused on collecting large amounts of expert-labeled reasoning trajectories, allowing models to mimic step-by-step reasoning [51]. However, these methods are often difficult to scale up, as annotation is highly expensive, especially for very challenging and complex problems [52].\nAnother category of methods, exploration and learning, seeks to address this issue by using model-generated data to train the model itself. Specifically, given a query, the model generates its own reasoning paths, and external supervision signals are used to filter out high-quality solutions, which are then used to train the model [46; 42; 3; 53; 54]. This approach, also known as self-improvement or rejection sampling, often encounters the tail-narrowing problem, which can lead to performance bottlenecks [55; 56; 34]. Some researchers have proposed reinforcement learning-based approaches, where reward models are trained or oracle reward functions are used to provide supervision signals, enabling the model to explore and learn, thereby significantly improving reasoning performance"}, {"title": "Algorithm 1: Critique-in-the-loop Self-Improvement", "content": "Input: Initialized actor reasoning model $\u03c0\u03b8$, intialized critique model $\u03c0\u03c6$, reasoning dataset\nDreason, refinement dataset Drefine, critique dataset Dcritique, oracle reward function r, the\niteration number for self-improvement T, the sampling number for exploration N, the\nsampling number for critique generation L.\nProcedure Fine-tune the critique model and the actor reasoning model\nMinimize the following loss objective to obtain critique model \u03c0\u03c6:\nLcrique(\u03c6)= E\n(x,y,c)\u223cDcritique[log T\no(c|x, y)];\nMinimize the following loss objective to obtain actor model \u03c0bas\ne:\nLactor(0) = E\n(x,y)\u223cDreason[log \u03c0\u03c1(y|x)] + \u03b2 \u00d7 E\n(x,y,c,y')\u223cDecline[log Top(y'|x, y, c)];\nProcedure Exploration and Learning with Critique Supervision\n\u03c0o\u2190 \u03c0bas\ne;\nfor iteration t = 1 to T do\nProcedure Exploration Step\nDt \u2190 \u00d8;\n// Sample N solutions from the reasoning model and collect correct responses.\nfor sample num n = 1 to N do\nend\nDt,n = {(xi, yi)|xi \u223c Dreason, yi \u223c \u03c0\n\u22121(y|xi)};\nDt \u2190 Dt \u222a Dt,n;\nApply r to Dt to get correct responses D\ncorrect and incorrect responses Dincorrect;\n// Generate critique and refinement for incorrect responses.\nDt\ncritique, Dre\nrefine \u2190 \u00d8;\nfor critique generation num l = 1 to L do\nDt,\ncritique = {(xi, yi, ci)|xi, yi \u223c D\nincorrect, ci \u223c \u03c0\u03bf(c|xi, yi)};\nend\nDt\ncritique \u2190 Dt \u222a Dt,\ncorrect UDt\ncritique,\nyi \u223c \u03c0\u22121\n0(y/xi, yi, ci)};\nDtrefine = {(xi, yi, ci, y') | xi, yi, ci \u223c D\nDt\nrefine \u2190Dtrefine \u222a D,\nrefine;\n// Combine correct original solutions and correct refined solutions.\nApply r to Drefine to get the correct refine set Dcorrectrefine;\nDtcorrect \u2190 Dcorrect \u222a {(xi, y)\ncorrect} ;\nProcedure Learning Step\nDrain = Dreas\non \u222a Dcorrec\nt;\nMinimize the following loss objective to obtain \u03c0\u03bf:\nLactor(0) = E(x,y)\u223cDrain[log Top(y|x)] + \u03b2 \u00d7 E(x,y,c,y')\u223cDrefine[log Top (y'|x, y, c)];\nrising accordingly, underscoring the benefits of enhanced exploration computation. (2) Our method consistently outperforms vanilla self-improvement with stable and significant performance gains, especially when the sample number N is larger. For example, when N = 10, our method achieves a performance advantage of 11.1% on both GSM8K and MATH. (3) While the vanilla method initially shows performance improvements during self-improvement, it quickly reaches a bottleneck or even starts to decline, which may be attributed to the tail narrowing issue [34]. In contrast, our method demonstrates consistent improvement, with performance saturation occurring much later, indicating the effectiveness of our method.\nCritique-in-the-loop self-improvement balances the solution distribution across difficulty levels, and enhances performance on challenging queries in the test set. Since our motivation for introducing critique-based supervision into training is to improve the efficiency and quality of exploration, we examine the distribution of solutions sampled by our method compared to vanilla self-improvement. As shown in Figure 7, we find that our approach samples a higher proportion of"}, {"title": "7.  A Step Further: Training Step-level Self-Talk Reasoning Models viaCritique Data", "content": "Motivation and method. In this work, we focus on the two-player paradigm, leveraging critique models to provide step-level supervision and feedback for actor models. Recently, OpenAI's o1 model [13] has pushed the boundaries of large reasoning models' capabilities. With its self-talk output format, it can autonomously plan, reflect, critique, correct, backtrack, and more during the thinking process, marked by phrases such as \u201cwait\u201d and \u201calternatively\u201d. Therefore, we investigate whether it is possible to construct self-talk data with step-level critique supervision, and propose the preliminary self-talk-via-critique method. Specifically, it has three main steps:\n1. Construct an initial thinking chain that has step-level reflection. Given a query and a reasoning path, we first use AutoMathCritique to generate critique data. Feedback on each reasoning step provided in the critique is then inserted into the reasoning path, constructing"}]}