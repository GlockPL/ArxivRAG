{"title": "Visual Counter Turing Test (VCT\u00b2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (VAI)", "authors": ["Nasrin Imanpour", "Shashwat Bajpai", "Subhankar Ghosh", "Sainath Reddy Sankepally", "Abhilekh Borah", "Hasnat Md Abdullah", "Nishoak Kosaraju", "Shreyas Dixit", "Ashhar Aziz", "Shwetangshu Biswas", "Vinija Jain", "Aman Chadha", "Amit Sheth", "Amitava Das"], "abstract": "The proliferation of AI techniques for image generation, coupled with their increasing accessibility, has raised significant concerns about the potential misuse of these images to spread misinformation. Recent AI-generated image detection (AGID) methods include CNNDetection, NPR, DM Image Detection, Fake Image Detection, DIRE, LASTED, GAN Image Detection, AIDE, SSP, DRCT, RINE, OCC-CLIP, De-Fake, and Deep Fake Detection. However, we argue that the current state-of-the-art AGID techniques are inadequate for effectively detecting contemporary AI-generated images and advocate for a comprehensive reevaluation of these methods. We introduce the Visual Counter Turing Test (VCT2), a benchmark comprising ~130K images generated by contemporary text-to-image models (Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and Midjourney 6). VCT\u00b2 includes two sets of prompts sourced from tweets by the New York Times Twitter account and captions from the MS COCO dataset. We also evaluate the performance of the aforementioned AGID techniques on the VCT2 benchmark, highlighting their ineffectiveness in detecting Al-generated images. As image-generative AI models continue to evolve, the need for a quantifiable framework to evaluate these models becomes increasingly critical. To meet this need, we propose the Visual AI Index (VAI), which assesses generated images from various visual perspectives, including texture complexity and object coherence, setting a new standard for evaluating image-generative AI models. To foster research in this domain, we make our COCO and Twitter datasets publicly available.", "sections": [{"title": "1. Defending Against AI Apocalypse: The Urgency of Rediscovering Techniques for AI-Generated Image Detection", "content": "The exponential growth of text-to-image generative AI models like Stable Diffusion(s) [1-3], DALL-E(s) [4-6], Midjourney [7], and Imagen [8] has revolutionized visual content creation, unlocking unprecedented creative potential. However, this rapid evolution and widespread accessibility presents significant challenges, particularly concerning the misuse of AI-generated images. In March 2023, an open letter [9] signed by numerous AI experts and industry leaders called for a six-month halt on the development of AI systems"}, {"title": "2. AI-Generated Image Detection: An Overview of Current Methods", "content": "In recent years, AI-generated image detection has emerged as a critical area of research. This section provides a literature review on synthetic/AI-generated image detection, as depicted in Figure 2. The detection techniques are cate-"}, {"title": "2.1. Generation Artifact-Based Detection", "content": "These techniques focus on detecting generation artifacts in both the spatial and frequency domains. Tan et al. [16] found that the up-sampling operator can create artifacts not just in the frequency patterns but also in how the pixels are arranged in the image. These artifacts are especially visible in images created by GANs or diffusion models. Building upon this observation, the authors introduce the concept of Neighboring Pixel Relationships as a means to capture and characterize the generalized structural artifacts stemming from up-sampling operations. Corvi et al. [17] discovered that synthetic images, particularly those created by GANs and some diffusion models like GLIDE and Stable Diffusion, tend to have noticeable differences in their mid-to-high frequency signals compared to real images. However, these differences aren't as noticeable in images generated by other models like DALL-E and ADM. While their method is very accurate at distinguishing between synthetic and real images when both types are clearly labeled in separate folders, it struggles to identify generated images effectively in real-world situations. Doloriel et al. [18] explore masked image modeling for universal fake image detection. They study both spatial and frequency domain masking and based on empirical analysis, propose deepfake detector via frequency masking. Chen et al. [19] focus on enhancing the general-izability of detectors by generating hard samples through high-quality diffusion reconstruction. These reconstructed images, which closely resemble real ones but contain subtle artifacts, help train detectors to better distinguish between real and generated images, even from unseen models."}, {"title": "2.2. Feature Representation-Based Detection", "content": "These methods distinguish real images from synthesized images by leveraging representations obtained from neural networks, which are computational models that excel in various computer vision tasks such as image super-resolution, classifatin, segmentation, and point cloud completionon [30-33]. Wang et al. [20] aim to build a universal detector. The authors found that a standard ResNet-50 classifier [34] with random blur and JPEG compression data augmentation, when trained on only one specific CNN generator (ProGAN), can generalize well to almost all other unseen architectures as well as models introduced later (StyleGAN2 [35], and StyleGAN3 [36]). Mandelli et al. [21] propose a detector based on an ensemble of CNNs. For generalization purposes, the CNNs should provide orthogonal results, and the original images should be trusted more during testing. Wang et al. [22] measure the error between an input image and its reconstruction counterpart by a pre-trained diffusion model. The authors observed that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. Wu et al. [23] leverage language-guided contrastive learning to learn representations that capture the inherent differences in underlying real and synthesized image distributions. This method involves augmenting training images with carefully designed textual labels, which allows for joint image-text contrastive learning for forensic feature extraction. Sha et al. [24] addresses the challenge of detecting and attributing fake images generated by text-to-image models. The authors propose a systematic approach that involves (i) building a machine-learning classifier to detect fake images generated by various text-to-image models, (ii) attributing fake images to their source models to hold model owners accountable for misuse, and (iii) investigating how prompts affect detection and attribution, focusing on topics like \"person\" and prompt lengths between 25 and 75 words. Aghasanli et al. [25] presents a novel approach to deepfake detection. The authors propose a methodology that leverages features from fine-tuned Vision Transformers (ViTs) combined with Support Vector Machines (SVMs) to distinguish between real and fake images generated by various diffusion models. The method analyzes the support vectors of the SVMs to provide interpretability. Chen et al. [26] propose a simple yet effective method that extracts a single simplest patch from an image and sends its noise pattern to a binary classifier. Yan et al. [27] propose AIDE, a hybrid-feature model leveraging both high-level semantic information (using CLIP) and low-level artifacts. Koutlis et al. [28] utilizes intermediate layer outputs from CLIP's image encoder to detect AI-generated images more effectively. They also incorporate a Trainable Importance Estimator to weigh the contributions of each Transformer block, resulting in generalizability across various generative models. Liu et al. [29] present a method for identifying which generative model created a given image in a practical setting. The authors introduce OCC-CLIP, a CLIP-based framework designed for few-shot one-class classification. This framework"}, {"title": "3. Visual Counter Turing Test (VCT2)", "content": "VCT2 is a benchmark comprising ~26K records (~130K images generated by text-to-image models), each record with a corresponding real image for the given caption. This benchmark also includes codebases for 15 SOTA AGID techniques, providing a resource for evaluation.\nFor image generation, we selected advanced text-to-image generative models like Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and Midjourney 6 known for achieving outstanding synthetic image quality. We generated ~16K records on tweets and ~10K records on MS COCO captions as our Visual Counter Turing Test (VCT2) benchmark dataset. we make our COCO and Twitter datasets publicly available. In the following, we describe how the real twitter images and corresponding captions are collected."}, {"title": "3.1. Real Twitter Image Dataset", "content": "The procedure for data collection from Twitter was meticulously designed to ensure the acquisition of relevant, high-quality data for subsequent analysis. This section delineates the key aspects of the data collection procedure, including the data filtration process and data processing steps.\nData collection from Twitter was automated using Python and Selenium. We programmatically accessed tweets and associated metadata from The New York Times (@nytimes) Twitter handle. Utilizing the NYT Twitter handle for data collection offers two primary advantages: First, as a renowned and reputable news organization, data from its official Twitter handle guarantees the reliability and credibility of the information, given its content undergoes rigorous fact-checking and editorial review. Second, the diversity of content from the NYT handle is significant, encompassing a broad spectrum of subjects such as national and international news, politics, culture, science, and more. This breadth is advantageous for generating a wide range of images across various domains.\nThe data collection process spanned 12 years (2011-2023) to capture a representative sample of tweets. Selection criteria included tweets with associated images, as these images were intended for comparison with AI-generated counterparts.\nThe collected Twitter data underwent a series of preprocessing steps to prepare it for analysis. These steps encompass text normalization, removing hashtags and URLs, and retaining only alphanumeric characters. These preprocessing procedures aimed to enhance the quality of the data and facilitate subsequent analysis."}, {"title": "4. Detection Results", "content": "Detection results, as shown in Table 1 and Table 2, are based on the performance of 15 SoTA AGID methods evaluated on synthetic datasets generated from MS COCO and Twitter prompts. AGID methods are assessed using three metrics: accuracy (Acc), recall (R), and precision (P). The datasets span five synthetic datasets corresponding to text-to-image models: SD 2.1, SDXL, SD 3, DALL-E 3, and Midjourney 6. Results for each generative model are computed on a combined dataset of fake and real images.\nOverall, the results demonstrate substantial progress in AI-generated image detection but also reveal critical gaps in generalizability, robustness, and adaptability. High-performing methods like De-Fake and DRCT show promise but fail to address the challenges posed by proprietary models. Meanwhile, the trade-offs between precision and recall in many methods highlight the need for balanced and adaptive detection approaches. These findings call for future work to focus on improving the robustness of detection techniques and developing methods that can handle the increasing diversity of generative models in real-world applications."}, {"title": "5. Visual AI Index (VAI)", "content": "With the rapid advancement of AI technology, image-generative models are continuously evolving. To objectively assess and rank these models based on their visual quality, we introduce the Visual AI Index (VAI), a standardized metric designed for evaluation. The VAI evaluates 7 key metrics, including texture complexity, color distribution, object coherence, and contextual relevance. VAI is calculated by \\(100 \\times \\frac{\\mu_j - L_j}{max(VAI) - min(VAI)}\\), where \\(v_j\\) is the value of the j-th metric, \\(L_j\\) is the lower bound of the j-th metric, \\(\\mu_j\\) is the mean value of the j-th metric. Finally, the VAI score is scaled to interval [0,100] by \\(100 \\times \\frac{VAI - min(VAI)}{max(VAI) - min(VAI)}\\). A higher score signifies superior visual quality, making the image less likely to be detected as AI-generated. In the following each metric is described.\nTexture Complexity quantifies the variety and unpredictability of an image's texture. It is determined by computing the entropy of the normalized Local Binary Pattern (LBP) histogram of the grayscale image using the formula \\(-\\Sigma H_{LBP}(k) log_2(H_{LBP}(k) + \\epsilon)\\). Here, \\(H_{LBP}(k)\\) represents the normalized histogram value for LBP bin k, and P is the total number of bins in the LBP histogram. The small constant \\(\\epsilon\\) (e.g., \\(1 \\times 10^{-6}\\)) is used to avoid taking the logarithm of zero.\nColor Distribution evaluates the variability in an image's color distribution by analyzing the standard deviation of the"}, {"title": "5.1. VAI Results", "content": "VAI scores on VCT2 benchmark are shown in Table 3. The score for real images serves as a baseline for comparison. In analyzing VAI scores based on images generated using COCO dataset prompts, we observe that DALL-E 3 images have relatively low VAI score of 55.52. In contrast, Midjourney proves the most challenging to detect as artificially generated, achieving a high score of 93.65. Stable Diffusion models (SD 3 and SD 2.1) lie in between, with similar scores of 69.33 and 70.47, respectively. When examining images from the Twitter dataset, the Stable Diffusion models remain easily detectable, with SDXL scoring 52.82, followed by SD 3 and SD 2.1, with scores of 55.04 and 69.33, respectively. Midjourney again proves the hardest to detect, scoring"}, {"title": "5.2. Image Explanations", "content": "Upon analyzing Local Binary pattern (LBP) texture and the pairwise scatter plots derived from multiple features of various AI-generated images, insights emerged that underscore both the utility and distinctive characteristics of these models in image generation. Subsequently, we discuss this analysis in the following subsections, where each section delves into specific aspects."}, {"title": "5.2.1. LBP Texture Analysis", "content": "Local Binary Pattern (LBP) is commonly used for texture analysis, image recognition, and quality assessment. LBP plots can indirectly assess image quality, as sharper images generally produce more distinct patterns in their LBP representations. If the LBP pattern appears blurred or lacks clear edges, it may indicate a loss of detail or lower resolution in the image. AI-generated images sometimes lose fine-grained texture, which would be visible as less distinctive LBP features. In Figure 7 we can see that image generated by Midjourney has specific facial textures and subtle expression lines whereas image generated by SD 3 has inconsistencies and lack of texture in certain areas. facial features, facial structures, hair lines, edges in clothing, and wrinkles are preserved in each segment for the Midjourney image but SD 3 image completely lost it."}, {"title": "5.2.2. Pairwise Scatter Plot Analysis", "content": "The pairwise scatter plots shown in Figure 6 reveal distinct differences in the models' distributions. DALL-E 3 and SDXL show higher object coherence across varying texture complexities, suggesting better object integrity maintenance. In contrast, SD 2.1 and SD 3 have more dispersed distributions, indicating less consistency. These observations also highlight evolutionary improvements, where newer models"}, {"title": "5.2.3. Comparison of Image Generation Models Based on Specific Prompts", "content": "The prompt used for the image generation was, \"Have you ever wondered why we name hurricanes? The New York Times meteorologist Judson Jones explains.\", DALL-E 3 attempted a more comprehensive interpretation by including not just the hurricane but also capturing the essence of \"wondered\". It depicted a person, emphasizing the latter part of the sentence. In contrast, other models like Midjourney 6 and SD 2.1 focused only on the hurricane. SDXL added elements like coconut trees, dark clouds, and rain, while SD 3 included both a hurricane and a person in its visual representation. However, the issue with the DALL-E 3 image is its aesthetic quality. It appears highly saturated with a cartoonish, airbrushed look that makes it easy to identify as AI-generated.\nThe prompt \"At least six candidates appear to have made the cut so far for the second Republican presidential debate on Sept 27 See which candidates have and have not qualified so far\" was used across various image generation models. Stable Diffusion versions consistently faced challenges with accurate pose estimation, resulting in distortions of human figures such as faces, hands, and legs. DALL-E 3's output, while avoiding distortions, produced images that appeared cartoonish and 2D, making them clearly identifiable as AI-generated and not depicting real people. Midjourney 6, however, managed to generate the most effective visual representation, achieving better overall quality in the depiction of human figures.\nSD 3 has shown significant improvement in handling text within images compared to SD 2.1 and SDXL. However, in terms of brightness and abrupt changes in pixel intensity, there has been no significant improvement observed between these versions. DALL-E 3 consistently attempts to include text to better convey the concepts behind prompts, enhancing understanding. In contrast, Midjourney 6 typically omits text from images, with text appearing in fewer than 0.5% of cases based on our testing."}, {"title": "5.2.4. Practical Implications and Model Comparison", "content": "In scenarios demanding high object coherence, DALL-E 3 and SDXL emerge as preferable choices. DALL-E 3 excels in generating images where object integrity is paramount, making it ideal for tasks where the recognizability of objects is crucial. In contrast, the SD models, including SD 3"}, {"title": "6. Conclusion", "content": "In this paper, we critically assess the current state-of-the-art in AI-generated image detection (AGID) techniques and"}, {"title": "7. Appendix", "content": ""}, {"title": "7.1. Dataset Details", "content": "The Visual Counter Turing Test (VCT2) benchmark utilizes images generated from prompts sourced from two distinct datasets:\n\u2022 MS COCO Captions: We utilized ~10K captions from the MS COCO dataset to generate synthetic images using text-to-image models. The use of COCO ensures that our dataset captures a wide range of everyday scenes and objects, enabling us to test the generalizability of AGID techniques across diverse contexts.\n\u2022 Twitter Prompts: To ensure diversity and relevance, we utilized ~16K tweets from the New York Times (@nytimes) Twitter handle spanning 2011-2023.\nWe generated ~26K records (~130K images) synthetic images using five state-of-the-art text-to-image generative models:\n\u2022 Stable Diffusion 2.1, XL, and 3: Known for achieving high-quality image synthesis. With each of these models, we generated ~10K synthetic images on MS COCO prompts and ~16K synthetic images on Twitter prompts, contributing significantly to the overall image count.\n\u2022 DALL-E 3: Utilized for its capability to interpret complex prompts and generate high-quality imagery, despite its susceptibility to produce cartoonish aesthetics. With this model, we generated ~10K synthetic images on MS COCO prompts and ~16K synthetic images on Twitter prompts, providing high-quality imagery.\n\u2022 Midjourney 6: Selected for its capability to generate exceptionally consistent and smooth textures, making it one of the more challenging models to detect. With this model, we generated ~10K synthetic images on MS COCO prompts and only 500 synthetic images on Twitter prompts, as this model blocks image generation for most Twitter prompts."}, {"title": "7.2. Detection Techniques", "content": "The detection techniques presented in Figure 2 were chosen to provide a comprehensive evaluation of state-of-the-art AI-generated image detection techniques. By categorizing these methods into Generation Artifact-Based and Feature Representation-Based techniques, we aimed to capture both"}, {"title": "7.3. Detection Performance Overview", "content": "Tables 1 and 2 provide an overview of the performance of different detection techniques across synthetic datasets generated from MS COCO and Twitter prompts, respectively. The metrics measured are Accuracy (Acc), Recall (R), and Precision (P), providing insights into each model's ability to differentiate real from AI-generated images."}, {"title": "7.3.1. Performance by Detection Technique", "content": "\u2022 CNNDetection, NPR and Fake Image Detection: These methods showed variable results, characterized by low recall but higher precision across several models. This indicates a tendency to correctly identify generated images when detected, but with many instances being missed (false negatives).\n\u2022 DM Image Detection and De-Fake: DM Image Detection demonstrated high precision across all models, particularly excelling with Stable Diffusion versions and Midjourney 6, effectively capturing generated images. De-Fake consistently showed high accuracy, recall, and precision, indicating its reliability.\n\u2022 GAN Image Detection, SSP and DIRE: These methods had mixed performance, particularly excelling in precision.\n\u2022 DRCT (ConvB and UnivB): Both versions of DRCT showed strong accuracy, recall, and precision across most models but experienced a slight performance drop with Midjourney 6, indicating challenges with proprietary models.\n\u2022 OCC-CLIP: OCC-CLIP had lower recall with SDXL but balanced performance for DALL-E 3 and Midjourney 6.\nThe results indicate that there is no one-size-fits-all solution for detecting AI-generated images. Different generative models pose unique challenges, and the performance of each detection method varies based on its ability to identify specific artifacts. De-Fake and DRCT (ConvB and UnivB) were the most consistent performers, highlighting their robustness across models. Future research should aim to improve detection for proprietary models like Midjourney 6 and DALL-E 3, where many techniques struggled."}, {"title": "7.4. Visual AI Index Overview", "content": "Midjourney 6 achieved the highest Visual AI Index (VAI) score on MS COCO prompts, indicating superior visual coherence and quality compared to other models. Stable Diffusion 2.1 also showed relatively high performance, suggesting that diffusion-based methods can achieve strong visual results but may still be outperformed by proprietary methods like Midjourney 6.\nFrom the VAI scores on Twitter prompts, Midjourney 6 remained the top performer, followed closely by DALL-E 3. This suggests that proprietary models are particularly robust in generating high-quality images, even with more diverse and potentially less structured prompts. The accuracy heat maps in Figure 5 also highlight differences in how AGID methods perform across models. Methods like De-Fake and DRCT were particularly effective at detecting Midjourney-generated images, whereas detection on DALL-E 3 and SDXL proved more challenging. This indicates that the texture and artifact characteristics differ significantly across these models, affecting detection reliability. These results underscore the challenges faced by AGID methods when applied to high-quality proprietary models. While some detection methods, like De-Fake and DRCT, performed consistently well, the VAI scores reveal that generated image quality plays a significant role in detection difficulty. Future work should focus on improving the robustness of detection techniques against models that prioritize high visual fidelity, such as Midjourney 6 and DALL-E 3.\nThe scatter plots in Figure 6 further illustrate the complexity of visual relationships across different metrics, such as Texture Complexity, Object Coherence, and Image Sharpness. Midjourney 6 and DALL-E 3 exhibit well-distributed clusters, indicating superior performance in maintaining coherence and complexity. Stable Diffusion variants, however, show mixed patterns, highlighting inconsistencies in texture handling and object boundaries, which aligns with the lower VAI scores observed for these models. The scatter plot analysis emphasizes that achieving balance across metrics like texture complexity and object coherence is critical for enhancing the performance of both generative and detection models."}, {"title": "7.5. Supplementary Figures", "content": "The supplementary figures presented in this appendix (Figure 8 and Figure 9) were discussed in the main paper in subsection 5.2.3 but omitted due to space limitations. Here, we include these figures to provide further insights and visual examples for the concepts and results covered in the paper."}]}