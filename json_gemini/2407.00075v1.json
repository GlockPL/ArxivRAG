{"title": "Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference", "authors": ["Anton Xue", "Avishree Khare", "Rajeev Alur", "Surbhi Goel", "Eric Wong"], "abstract": "We study how to subvert language models from following the rules. We model rule-following as inference in propositional Horn logic, a mathematical system in which rules have the form \"if P and Q, then R\" for some propositions P, Q, and R. We prove that although transformers can faithfully abide by such rules, maliciously crafted prompts can nevertheless mislead even theoretically constructed models. Empirically, we find that attacks on our theoretical models mirror popular attacks on large language models. Our work suggests that studying smaller theoretical models can help understand the behavior of large language models in rule-based settings like logical reasoning and jailbreak attacks.", "sections": [{"title": "Introduction", "content": "Developers commonly use system prompts, task descriptions, and other instructions to guide large language models (LLMs) toward producing safe content and ensuring factual accuracy [1, 14, 53]. In practice, however, LLMs often fail to respect these rules for unclear reasons. When LLMs violate predefined rules, they can produce harmful content for downstream users and processes [17, 50]. For example, a customer services chatbot that deviates from its instructed protocols can create a poor user experience, erode customer trust, and trigger legal actions [31].\nTo study why LLMs may be unreliable at following the rules, we study how to purposely subvert them from obeying prompt-specified instructions. Our motivation is to better understand the underlying dynamics of jailbreak attacks [5, 7, 33, 40, 55] that seek to bypass various safeguards on LLM behavior [2, 22, 23, 29, 51]. Although many works conceptualize jailbreaks as rule subversions [42, 54], the current literature lacks a solid theoretical understanding of when and how such attacks might succeed. To address this gap, we study the foundational principles of attacks on rule-based inference for rules given in the prompt.\nWe first present a logic-based framework for studying rule-based inference, using which we characterize different ways in which a model may fail to follow the rules. We then derive theoretical attacks that succeed against not only our analytical setup but also reasoners trained from data. Moreover, we establish a connection from theory to practice by showing that popular jailbreaks against large language models exhibit similar characteristics as our theory-based ones.\nLogic-based Framework for Analyzing Rule Subversion (Section 2). We model rule-following as inference in propositional Horn logic [3, 4], a common approach for rule-based systems [8, 19], wherein rules take the form \u201cIf P and Q, then R\" for some propositions P, Q, and R. Building on this foundation, we define three properties - monotonicity, maximality, and soundness that characterize logical inference in this setting. Our framework allows us to formally describe rule-following and lets us characterize what it means for a model to not follow the rules."}, {"title": "Framework for Rule-based Inference", "content": "Inference in Propositional Horn Logic. We model rule-following as inference in propositional Horn logic, which concerns deriving new knowledge using inference rules of an \"if-then\" form. We consider an example from the Minecraft video game [28], where a common objective is making new items according to a recipe list. Given such a list and some starting items, a player may formulate the following prompt to ask what other items are attainable:\nHere are some crafting recipes: If I have Sheep, then I can create Wool. If I have Wool, then I\ncan create String. If I have Log, then I can create Stick. If I have String and Stick, then I can\ncreate Fishing Rod. Here are some items I have: I have Sheep and Log as starting items. Based\non these items and recipes, what items can I create?\nwhere Sheep, Wool, and String, etc., are items in Minecraft. We may translate the prompt-specified instructions above into the following set of inference rules \u0393 and known facts \u03a6:\n$\\Gamma = {\\text{A} \\rightarrow \\text{B}, \\text{B} \\rightarrow \\text{C}, \\text{D} \\rightarrow \\text{E}, \\text{C}\\wedge\\text{E} \\rightarrow \\text{F}}, \\Phi = {\\text{A}, \\text{D}}$\n(1)\nwhere A denotes logical conjunctions (AND). For example, the rule CAE \u2192 F reads \u201cIf I have Wool and Stick, then I can create Fishing Rod\" and the proposition B stands for \"I have Wool\", which we treat as equivalent to \"I can create Wool\". The inference task is to find all the derivable propositions. A well-known algorithm for this is forward chaining, which iteratively applies \u0393 starting from until no new knowledge is derivable. We illustrate a 3-step iteration of this procedure:\n{A, D} Apply[\u0393] {A, B, D, E} Apply[\u0393] {A, B, C, D, E} Apply[\u0393] {A, B, C, D, E, F},\n(2)\nwhere Apply [\u0393] is a set-to-set function that implements a one-step application of \u0393. Because no new knowledge can be derived from the proof state {A, B, C, D, E, F}, we may stop. When \u0393 is finite, as in this paper, we write Apply*[\u0393] to mean the repeated application of Apply [\u0393] until no new knowledge is derivable. We then state the problem of propositional inference as follows.\nProblem 2.1 (Inference). Given rules \u0393 and facts \u03a6, find the set of propositions Apply*[\u0393](\u03a6).\nNext, we present a binarization of the inference task to better align with our later exposition of transformer- based language models. From the above example, denote the subsets of {A, B, C, D, E, F} using binary vectors in {0,1}6. We write \u03a6 = (100100) to mean {A, D} and use pairs to represent rules in \u0393, e.g., write (001010, 000001) to mean CAE \u2192 F. This lets us define Apply [\u0393] : {0,1}6 \u2192 {0,1}6 as follows:\nApply [\u0393](s) = s \u2228 \u22c1{\u03b2 : (\u03b1, \u03b2) \u2208 \u0393, \u03b1 \u2286 s},\n(3)\nwhere s \u2208 {0,1}6 is any set of propositions, \u2228 denotes the element-wise disjunction (OR) of binary vectors, and the subset relation \u2286 is analogously extended. Because binarization and set-based notations are equivalent and both sometimes useful, we will flexibly use whichever is convenient. We remark that Problem 2.1 is also known as propositional entailment, which is equivalent to the more commonly studied problem of HORN-SAT. We expand upon this in Appendix A.1, wherein the main detail is the representation of the \"bottom\" proposition.\nSubversion of Rule-following. We use models that autoregressively predict the next proof state to solve the inference task of Problem 2.1. We say that such a model R behaves correctly if its sequence of predicted proof states match what is generated by forward chaining with Apply [\u0393] as in (2). Therefore, to subvert inference is to have R generate a sequence that deviates from that of Apply [\u0393]. However, this sequence of proof states may deviate in different ways, allowing us to formulate attacks on various aspects of the inference process. We formally define three properties of interest.\nDefinition 2.2 (Monotone, Maximal, and Sound (MMS)). For any rules \u0393, known facts \u03a6, and proof states $s_0, s_1,..., s_T \u2208 {0,1}^n$ where \u03a6 = $s_0$, we say that the sequence $s_0, s_1,..., s_T$ is:\n\u2022 Monotone iff $s_t \u2286 s_{t+1}$ for all steps t.\n\u2022 Maximal iff \u03b1 \u2286 st implies \u03b2 \u2286 st+1 for all rules (\u03b1, \u03b2) \u2208 \u0393 and steps t.\n\u2022 Sound iff for all steps t and coordinate $i \u2208 {1, ..., n}$, having $(s_{t+1})_i = 1$ implies that: $(s_t)_i = 1$ or there exists (\u03b1, \u03b2) \u2208 \u0393 with \u03b1 \u2286 st and \u03b2i = 1."}, {"title": "Theoretical Principles of Rule Subversion in Transformers", "content": "Having established a framework for studying rule subversions in Section 2, we now seek to understand how it applies to transformers. In Section 3.1, we establish our transformer and show that models subject to our theoretical constraints can learn inference to a high accuracy. Then, we establish in Section 3.2 rule subversions against our theoretical constructions and show that they transfer to reasoners trained from data."}, {"title": "Transformers Can Encode Rule-based Inference", "content": "We now present our mathematical formulation of transformer-based language models. Because our theoretical encoding result of Theorem 3.1 states that a transformer with one layer and one self-attention head suffices to represent Apply [\u0393], we define our reasoner model R as follows:\nR(X) = ((Id + Ffwd) \u25e6 (Id + Attn))(X),\nAttn(X) = CausalSoftmax((XQ + 1NqT)KTXT)XV, X = $\\begin{bmatrix} x_1 \\vdots\\x_N\\end{bmatrix} \u2208 \\mathbb{R}^{N \\times d}$ (4)\nFfwd(z) = W2ReLU(W\u2081z + b),\nHere, R : \\mathbb{R}^{N \\times d} \u2192 \\mathbb{R}^{N \\times d} is a transformer with embedding dimension d over sequence length N. We use residual connections, denoted by Id, for both the self-attention and feedforward blocks. The self-attention block Attn : \\mathbb{R}^{N \\times d} \u2192 \\mathbb{R}^{N \\times d} has weights Q, KT, V \u2208 \\mathbb{R}^{d \\times d} and bias q \u2208 \\mathbb{R}^{d}, with CausalSoftmax : \\mathbb{R}^{N} \u2192 \\mathbb{R}^{N} applied to each row. The one-depth feedforward block Ffwd : \\mathbb{R}^{d} \u2192 \\mathbb{R}^{d} has weights $W_1^T$, W2 \u2208 \\mathbb{R}^{d \\times d_{ffwd}}$, bias b\u2208 \\mathbb{R}^{d_{ffwd}}, and width $d_{ffwd}$. During evaluation, the same Id + Ffwd block is applied in parallel to each row of (Id + Attn)(X) \u2208 \\mathbb{R}^{N \\times d}.\nTransformers Implement Inference via Autoregressive Iterations. We now consider how a reasoner R as in (4) implements inference. Given the rules \u0393 = {(\u03b1\u2081, \u03b2\u2081), ..., (\u03b1r, \u03b2r)} \u2286 {0,1}2n and known facts \u03a6\u2208 {0,1}n, we begin from an initial input encoding $X_0$ = Encode(\u0393, \u03a6) \u2208 \\mathbb{R}^{(r+1) \\times d}$. Then, we use R to autogressively generate a sequence of sequences $X_0, X_1,..., X_T$ that respectively decode into the proof states $s_0, s_1,..., s_T \u2208 {0,1}^n$ using a classification head ClsHead. In particular, this relation is given by $s_{t+1}$ = ClsHead(R($X_t$)). Our main encoding strategy is to have the self-attention block of R approximate Apply [\u0393] rules in the following manner:\n$\\text{Id+Attn} \\colon s_t \\mapsto s_{t+1}$, where $s_{t+1} = s_t + \\sum_{\\alpha, \\beta : \\alpha \\subseteq s_t} \\beta + \\epsilon \\approx \\text{Apply}[\\Gamma](s_t)$ (5)\nwhere \u025b is a residual term from softmax attention. That is, we approximate binary-valued disjunctions with summations and recover a binary-valued st+1 by thresholding each coordinate of St+1 \u2208 Rn using the feedforward block. We apply this idea to derive a theoretical construction that can implement propositional inference using only one layer and one self-attention head."}, {"title": "Attacking Rule-based Inference in Transformers", "content": "We next investigate how to subvert the rule-following of our theoretical models. In particular, the objective is to find an adversarial suffix A that causes a violation of the MMS property when appended to some input $X_0$ = Encode(\u0393, \u03a6). This suffix-based approach is similar to jailbreak formulations studied in the literature [32, 55], and we state this problem as follows:\nProblem 3.2 (Inference Subversion). Consider any rules \u0393, facts \u03a6, reasoner R, and budget p > 0. Let $X_0$ = Encode(\u0393, \u03a6), and find \u2206 \u2208 \\mathbb{R}^{p \\times d}$ such that: the proof state sequence $\\hat{s}_0, \\hat{s}_1,..., \\hat{s}_T$ generated by R given $X_0 = [X_0; \\Delta]$ is not MMS with respect to \u0393 and \u03a6, but where $\\hat{s}_0 = \u03a6$.\nOur key strategy for crafting attacks against our theoretical construction is to use the fact that R uses a summation to approximate binary disjunctions, as in (5). In particular, if one can construct an adversarial suffix A with large negative values in the appropriate coordinates, it is straightforward to craft attacks that induce violations of MMS.\nTheorem 3.3 (Theory-based Attacks, Informal). Let R be as in Theorem 3.1 and consider any Xo =\nEncode(\u0393, \u03a6) where the rules \u0393 and \u03a6 satisfy some technical conditions (e.\u0434., \u03a6 \u2260 \u00d8 for monotonicity). Then"}, {"title": "Experiments with Large Language Models", "content": "Next, we study how to subvert text-based language models in practice and analyze whether such attacks align with our theoretical predictions. In contrast to the theory-based setting, we now consider significantly larger models that operate on discrete tokens. Concretely, we used the popular jailbreak algorithm of Greedy Coordinate Gradients (GCG) [55] to induce fact amnesia, rule suppression, and state coercion in GPT-2 generations over a Minecraft recipes dataset. We found that the attention patterns and adversarial suffixes discovered by GCG align with their counterparts from Theorem 3.3. Furthermore, we found that rule-following in Llama-2 (7B-Chat) [38] exhibits similar attention weights when subjected to rule-suppression attacks. In this section, we give an overview of our results and defer comprehensive details to Appendix C.\nDataset, Model, and Attack Setups. To study inference subversion in natural language, we consider the task of sabotaging item-crafting in Minecraft [28]. Given a prompt about crafting items, the objective is to find an adversarial suffix that causes the LLM to answer incorrectly. Fig. 5 shows such an example, where an adversarial suffix suppresses the LLM from generating String and Fishing Rod in its output. To attack LLM-based reasoners, we first construct three datasets of such prompts that require at most T = 1,3, 5 steps each to craft all the items (the Fig. 5 example requires T = 3 steps). Next, we fine-tune a GPT-2 [30] model for each dataset, with all three models attaining 85%+ accuracy. Then, for each attack and each model, we use GCG to search for an adversarial suffix that induces the expected behavior of the attack."}, {"title": "Related Work", "content": "Adversarial Attacks and Jailbreaks. It is known that language models can be tricked to return unintended outputs through malicious prompts [33, 40]. Such attacks have inspired much interest in studying how such cases may be prevented, in particular through various defense techniques [2, 22, 23, 29, 32, 45] which aim to ensure that a language model does not output objectionable content. Despite these efforts, language models remain vulnerable to various jailbreak attacks [5, 13, 15, 42], which aim to induce such objectionable content through methods based on adversarial attacks [10, 37]. We refer to [7, 43, 55] for surveys on jailbreak literature.\nExpressive Power of Transformers. A line of recent works has explored what can and cannot be represented by transformers. Several works [6, 9, 11, 12, 21, 26, 27, 35] take a computational complexity perspective and characterize the complexity class Transformers lie in, under different assumptions on architecture-size, type of attention-mechanism, bit complexity, etc. We refer to [36] for an extensive survey on recent results. In our paper, we instead present a more fine-grained, parameter-efficient construction for the specific task of propositional logic inference.\nReasoning Performance of Transformers. There is much interest in understanding how transformer- based [39] language models perform logical reasoning. Notably, the advent of chain-of-thought reasoning [16, 44] and its many variants [18, 25, 34, 41, 46\u201348, 52]. We refer to [8, 20] and the references therein for extensive surveys on chain-of-thought techniques. The closest work to ours is [49], which studies how propositional reasoning with BERT is an artifact of data-driven heuristics and does not indicate that the model has learned to reason. There are a number of distinctions between our work and this: firstly, they present inference as a classification task where the model needs to predict in a single step whether a given proposition can be derived. Instead, we adopt an auto-regressive presentation of inference to predict the next state that more closely aligns with the commonly used chain-of-thought paradigm. Moreover, their work studies out-of-distribution robustness of reasoning with BERT while we focus on jailbreak attacks. Finally, we give a more compact theoretical encoding for the transformer and demonstrate that models subject to such constraints can successfully learn logical inference."}, {"title": "Conclusions and Discussion", "content": "We introduce a logic-based framework for subverting the rule-following of transformer-based language models. We find that attacks derived within our theoretical framework transfer to learned models and provide insights into the workings of popular jailbreaks against LLM. Although our work provides a step toward understanding jailbreak attacks, several limitations exist. First, our theoretical models do not use positional encoding, which is known to be important for LLM performance. Moreover, our choice of propositional Horn logic means we cannot easily reason about negations, disjunctive clauses, and statements with quantifiers. Furthermore, we only consider rules supplied in the prompt, and so this excludes cases like safety fine-tuning and RLHF. Our work is impactful for LLM developers who aim to improve model safeguards. However, a malicious user may leverage our work to improve attacks."}, {"title": "Additional Background", "content": "Here, we give a formal presentation of propositional Horn logic and discuss the relation between inference (Problem 2.1) and the more commonly studied HORN-SAT (Problem A.2). The technical contents of this section are well-known, but we present it nonetheless for a more thorough exposition. We refer to [3] or any standard introductory logic texts for additional details.\nWe first present the set-membership variant of propositional Horn inference (Problem 2.1), which is also known as propositional Horn entailment.\nProblem A.1 (Horn Entailment). Given rules \u0393, known facts \u03a6, and proposition P, check whether P\u2208 Apply*[\u0413](\u03a6). If this membership holds, then we say that I and I entail P.\nThis reformulation of the inference problem allows us to better prove its equivalence (interreducibility) to HORN-SAT, which we build up to next. Let $P_1, ..., P_n$ be the propositions of our universe. A literal is either a proposition $P_i$ or its negation \u00ac$P_i$. A clause (disjunction) C is a set of literals represented as a pair of binary vectors $[c^\u2212, c^+] \u2208 {0,1}^{2n}$, where $c^\u2212$ denotes the negative literals and c\u207a denotes the positive literals:\n$(c_i^-) = \\begin{cases} 1,& \\text{if } \u00acP_i \u2208 C\\\\ 0,& \\text{otherwise} \\end{cases}$\n$(c_i^+) = \\begin{cases} 1,& \\text{if } P_i \u2208 C\\\\ 0,& \\text{otherwise} \\end{cases}$\nA proposition Pi need not appear in a clause so that we may have $(c^\u2212)_i = (c^+)_i = 0$. Conversely, if P appears both negatively and positively in a clause, i.e., $(c^\u2212)_i = (c^+)_i = 1$, then such clause is a tautology. Although [,] and (,) are both pairs, we use [,] to stylistically distinguish clauses. We say that $[c^\u2212, c^+]$ is a Horn clause iff |c\u207a| < 1, where || counts the number of ones in a binary vector. That is, C is a Horn clause iff it contains at most one positive literal.\nWe say that a clause C holds with respect to a truth assignment to $P_1,..., P_n$ iff at least one literal in C evaluates truthfully. Equivalently for binary vectors, a clause $[c^\u2212, c^+]$ holds iff: some Pi evaluates truthfully and $(c^+)_i = 1$, or some Pi evaluates falsely and $(c^\u2212)_i = 1$. We then pose Horn satisfiability as follows.\nProblem A.2 (HORN-SAT). Let C be a set of Horn clauses. Decide whether there exists a truth assignment to the propositions $P_1,..., P_n$ such that all clauses of C simultaneously hold. If such an assignment exists, then C is satisfiable; if such an assignment does not exist, then C is unsatisfiable.\nNotably, HORN-SAT can be solved in polynomial time; in fact, it is well-known to be P-COMPLETE. Importantly, the problems of propositional Horn entailment and satisfiability are interreducible.\nTheorem A.3. Entailment (Problem A.1) and HORN-SAT (Problem A.2) are interreducible.\nProof. (Entailment to Satisfiability) Consider a set of rules \u0393 and proposition P. Then, transform each (\u03b1, \u03b2) \u2208 \u0393 and P into sets of Horn clauses as follows:\n$(\u03b1, \u03b2) \u2192 {[\u03b1, e_i] : \u03b2_i = 1, i = 1,..., n}, P\u2194 [P, 0_n]$\nwhere $e_1,..., e_n \u2208 {0,1}^n$ are the basis vectors and we identify P with its own binary vectorization. Let C be the set of all clauses generated this way, and observe that each such clause is a Horn clause. To check whether I entails P, it suffices to check whether C is satisfiable.\n(Satisfiability to Entailment) Let C be a set of Horn clauses over n propositions. We embed each Horn clause $[c^\u2212, c^+] \u2208 {0,1}^{2n}$ into a rule in ${0,1}^{2(n+1)}$ as follows:\n$[c^\u2212, c^+] \u2192 \\begin{cases} ((c^\u2212,0), (c^+, 0)) \u2208 {0,1}^{2(n+1)},& \\text{if } |c^+| = 1\\\\ ((c^\u2212,0), (0_n, 1)) \u2208 {0,1}^{2(n+1)},& \\text{if } |c^+| = 0 \\end{cases}$"}, {"title": "Softmax and its Properties", "content": "It will be helpful to recall some properties of the softmax function, which is central to the attention mechanism. For any integer N > 1, we define Softmax : \\mathbb{R}^{N} \u2192 \\mathbb{R}^{N} as follows:\nSoftmax(z1,...,zN) = $\\frac{(e^{z_1},..., e^{z_N})}{\\sum_{i=1}^N e^{z_i}} \u2208 \\mathbb{R}^{N}$ (7)\nOne can also lift this to matrices to define a matrix-valued Softmax : \\mathbb{R}^{N \\times N} \u2192 \\mathbb{R}^{N \\times N} by applying the vector-valued version of Softmax : \\mathbb{R}^{N} \u2192 \\mathbb{R}^{N} row-wise. A variant of interest is causally-masked softmax, or CausalSoftmax : \\mathbb{R}^{N \\times N} \u2192 \\mathbb{R}^{N \\times N}, which is defined as follows:\nCausalSoftmax $\\begin{bmatrix} z_{11} & z_{12} & z_{13} &... & z_{1N} \\\\ z_{21} & z_{22} & z_{23} &... & z_{3N} \\\\ : & : & : & : & :\\\\ z_{N1} & z_{N2} & z_{N3} &... & z_{NN} \\end{bmatrix}$ = $\\begin{bmatrix} \\text{Softmax}(z_{11}, \u2212\u221e, \u2212\u221e,..., \u2212\u221e) \\\\ \\text{Softmax}(z_{21}, z_{22}, \u2212\u221e,..., \u2212\u221e)\\\\ :\\\\ \\text{Softmax}(z_{N1}, z_{N2}, z_{N3},..., z_{NN}) \\end{bmatrix}$\nObserve that an argument of -\u221e will zero out the corresponding output entry. Notably, Softmax is also shift-invariant: adding the same constant to each argument does not change the output.\nLemma A.4. For any z \u2208 \\mathbb{R}^{N} and c\u2208 \\mathbb{R}, Softmax(z + c1_n) = Softmax(z).\nProof.\nSoftmax(z) = $\\frac{(e^{z_1+c},..., e^{z_N+c})}{\\sum_{i=1}^N e^{z_i+c}} = \\frac{e^c(e^{z_1},...,e^{z_N})}{e^c( \\sum_{i=1}^N e^{z_i})} = \\text{Softmax}(z)$\nIn addition, Softmax also commutes with permutations: shuffling the arguments also shuffles the output in the same order.\nLemma A.5. For any z \u2208 \\mathbb{R}^{N} and permutation \u03c0 : \\mathbb{R}^{N} \u2192 \\mathbb{R}^{N}, Softmax(\u03c0(z)) = \u03c0(Softmax(z)).\nMost importantly for this work, Softmax(z) approximates a scaled binary vector, where the approximation error is bounded by the difference between the two largest values of z.\nLemma A.6. For any z \u2208 \\mathbb{R}^{N}, let $v_1 = \\text{max}\\{z_1,..., z_N\\}$ and $v_2 = \\text{max}\\{z_i : z_i \u2260 v_1\\}$. Then,\nSoftmax(z) = $\\frac{1}{|\\{i: z_i = v_1\\}|}$I[z = v_1] + \u03b5, $||\u03b5||_\u221e < Ne^{\u2212(v_1\u2212v_2)}$\nProof. Let z \u2208 \\mathbb{R}^{N}. First, in the case where z has only one unique value, we have Softmax(z) = 1N/N because max() = -\u221e. Next, consider the case where z has more than one unique value. Using Lemma A.4 and Lemma A.5, we may then suppose without loss of generality that the arguments $z_1,...,z_N$ are valued and sorted as follows:\n$0 = z_1 = ... = z_m= v_1 > v_2 = z_{m+1} \u2265 ... \u2265 z_N$"}, {"title": "Results for the Inference Subversion Framework", "content": "We now prove some results for our logic-based framework for studying rule subversions. For convenience, we re-state the MMS properties:\nDefinition B.1 (Monotone, Maximal, and Sound (MMS)). For any rules \u0393, known facts \u03a6, and proof states $s_0, s_1,..., s_T \u2208 {0,1}^n$ where \u03a6 = $s_0$, we say that the sequence $s_0, s_1,..., s_T$ is:\n\u2022 Monotone iff $s_t \u2286 s_{t+1}$ for all steps t.\n\u2022 Maximal iff \u03b1 \u2286 st implies \u03b2 \u2286 st+1 for all rules (\u03b1, \u03b2) \u2208 \u0393 and steps t.\n\u2022 Sound iff for all steps t and coordinate $i \u2208 {1, ..., n}$, having $(s_{t+1})_i = 1$ implies that: $(s_t)_i = 1$ or there exists (\u03b1, \u03b2) \u2208 \u0393 with \u03b1 \u2286 st and \u03b2i = 1.\nNext, we show that MMS uniquely characterizes the proof states generated by Apply [\u0393].\nTheorem B.2. The sequence of proof states $s_0, s_1,...,s_T$ is MMS with respect to the rules \u0393 and known facts \u03a6 iff they are generated by T steps of Apply [\u0393] given (\u0393, \u03a6).\nProof. First, it is easy to see that a sequence generated by Apply[\u0393] is MMS via its definition:\nApply [\u0393](s) = s \u2228 \u22c1{\u03b2 : (\u03b1, \u03b2) \u2208 \u0393, \u03b1 \u2286 s}.\nConversely, consider some sequence $s_0, s_1,..., s_T$ that is MMS. Our goal is to show that:\n$s_{t+1} \u2286 \\text{Apply}[\u0393](s_t) \u2286 s_{t+1}$, for all t<T.\nFirst, for the LHS, by soundness, we have:\n$s_{t+1} \u2286 s_t \u2228 \u22c1\\{\u03b2 : (\u03b1, \u03b2), \u03b1 \u2286 s_t\\} = \\text{Apply}[\u0393](s_t)$.\nThen, for the RHS bound, observe that we have $s_t \u2286 s_{t+1}$ by monotonicity, so it suffices to check:\n$\\\u22c1\\{\u03b2 : (\u03b1, \u03b2) \u2208 \u0393, \u03b1 \u2286 s_t\\} \u2286 s_{t+1}$,\nwhich holds because the sequence is maximal by assumption."}, {"title": "Construction of Theoretical Reasoner", "content": "We now give a more detailed presentation of our construction. Fix the embedding dimension d = 2n, where n is the number of propositions, and recall that our reasoner architecture is as follows:\nR(X) = ((Id + Ffwd) \u25e6 (Id + Attn))(X),\nAttn(X) = Softmax((XQ + 1NqT)KTXT)XV, X = $\\begin{bmatrix} \\Gamma \\\\ (0_n; \\Phi) \\end{bmatrix} \u2208 \\mathbb{R}^{N \\times 2n}$ (8)\nFfwd(z) = W2ReLU(W\u2081z + b),\nwhere Q, KT, V \u2208 \\mathbb{R}^{2n \\times 2n} and q\u2208 \\mathbb{R}^{2n}. A crucial difference is that we now use Softmax rather than CausalSoftmax. This change simplifies the analysis at no cost to accuracy because R outputs successive proof states on the last row.\nAutoregressive Proof State Generation. Consider the rules \u0393\u2208 {0,1}r\u00d72n and known facts \u03a6\u2208 {0,1}n. Given a reasoner R, we autoregressively generate the proof states S0, S1,...,ST from the encoded inputs \u0425\u043e, \u04251,..., \u0425\u0442 as follows:\n$X_0$ = Enc(\u0413, \u0424) = [\u0413; (0n; \u03a6)], Xt+1 = [Xt; (0n, St+1)], St+1 = ClsHead(R(Xt)), (9)\nwhere each Xt \u2208 \\mathbb{R}^{(r+t+1) \\times 2n} and let [A; B] be the vertical concatenation of matrices A and B. To make dimensions align, we use a decoder ClsHead to project out the vector st+1 \u2208 {0,1}n from the last row of R(Xt) \u2208 \\mathbb{R}^{(r+t+1) \\times 2n}. Our choice to encode each n-dimensional proof state st as the 2n-dimensional (0n, St) is motivated by the convention that the empty conjunction vacuously holds: for instance, the rule \u2192 A is equivalent to asserting that A holds. A difference from Apply [\u0393] is that the input size to R grows by one row at each iteration. This is due to the nature of chain-of-thought reasoning and is equivalent to adding the rule $(0_n, s_t) \\mapsto s_t$ which is logically sound as it simply asserts what is already known after the t-th step.\nOur encoding strategy of Apply [\u0393] uses three main ideas. First, we use a quadratic relation to test binary vector dominance, expressed as follows:\nProposition B.3 (Idea 1). For all a, s \u2208 Bn, (s \u2013 1n)Ta = 0 iff a \u2286 s.\nOtherwise, observe that (s \u2013 1n)\u03a4\u03b1 < 0. This idea lets us use attention parameters to encode checks on whether a rule is applicable. To see how, we first introduce the linear projection matrices:\n$\\Pi_a = [I_n \\; 0_{n \\times n}] \u2208 \\mathbb{R}^{n \\times 2n}, \\Pi_b = [0_{n \\times n} \\; I_n] \u2208 \\mathbb{R}^{n \\times 2n}$. (10)\nThen, for any \u03bb > 0, observe that:\n$\\chi (\\Pi_a X [I_n \\; -1_N]^T) \\Pi_b X [I_n \\; -1_N]^T = Z \u2208 \\mathbb{R}^{N \\times N}$  $Z_{ij} \\begin{cases}  =0  \u03b1_i  \u03b2_i\\\\    - \u03bb  otherwise \\end{cases}$\nThis gap of A lets Softmax to approximate an \"average attention\" scheme:\nProposition B.4 (Idea 2). Consider z1,...,zn \u2264 0 where: the largest value is zero (i.e., maxi Zi = 0) and the second-largest value is < -x (i.e., max{zi : Zi < 0} \u2264 \u2212x), then:\nSoftmax(z1,..., zN) = $\\frac{1}{\\text{\\#zeros(z)}}I[z = 0] + O(Ne^{\u2212\u03bb})$, #zeros(z) = |{i : zi = 0}|.\nProof. This is an application of Lemma A.6 with v1 = 0 and v2 = \u2212\u03bb.\nThis approximation allows a single attention head to simultaneously apply all the possible rules. In particular, setting the attention parameter V = \u03bc\u03a0\u03a0\u266d for some \u03bc > 0, we have:\nAttn(X) = Softmax(Z) $\\begin{bmatrix} \\Gamma \\\\ 0_n \\end{bmatrix} \\\\  (\u03bc\u03a0_a \u03a0_b)  \\\\ + O(\u03bcN^2 e^{\u2212\u03bb}) (11)$"}, {"title": "Results for Attacks on Inference Subversion", "content": "We now prove results for the theory-based inference subversions, wherein the key idea is to exploit the fact that our encoding uses a weighted summation to approximate binary disjunctions.\nTheorem B.8 (Theory Monotonicity Attack). Let R be as in Theorem 3.1 and consider any Xo = Encode(\u0393, \u03a6) where I \u2260 0. Consider any \u03b4 \u2286 \u03a6, then for sufficiently large \u043a > 0, the adversarial suffix:\n$\\Delta_{MonotAtk} =  \\begin{bmatrix} 0^T \\\\ - \u03ba \u03b4^T  \\end{bmatrix}  \u2208 \\mathbb{R}^{2 \\times 2n}$\ninduces a sequence $\\hat{s}_0, \\hat{s}_1$ that is not monotone with respect to \u0393 and \u03a6.\nProof. This leverages the fact that St+1 is computed as a weighted summation of the rules applicable from \u015dt. In effect, we insert the \"rule\u201d (0n, \u2212\u03ba\u03b4) to down-weights propositions already known by \u0424. If \u015dt+1 forgets propositions from \u015dt, then the sequence is not monotone by definition.\nTheorem B.9 (Theory Maximality Attack). Let R be as in Theorem 3.1 and consider any X\u2081 = Encode(\u0413, \u03a6) where there exists some (\u03b1, \u03b2) \u2208 \u0393 such that: a \u2286 \u03a6 and \u03b2 \\ Apply [\u0393](\u03a6) \u2260 0. Then for sufficiently large \u043a > 0, the adversarial suffix:\n$\\Delta_{MaximAtk} =  \\begin{bmatrix} -(\u03b1 - \u03ba(1_n - \u03b1))^T\\\\ -\u03b2^T  \\\\ 0_\u03a6 \\end{bmatrix}  \u2208 \\mathbb{R}^{2 \\times 2n}$\ninduces a sequence $\\hat{s}_0, \\hat{s}_1$ that is not maximal with respect to \u0393 and \u03a6.\nProof. This attack works by introducing a \u201crule\u201d that competes with (\u03b1, \u03b2) for activation attention, thereby causing suppression.\nTheorem B.10 (Theory Soundness Attack). Let R be as in Theorem 3.1 and consider any X\u2081 = Encode(\u0413, \u03a6) and adversarial target s* \u2260 Apply [\u0413] (\u0424). Then, for sufficiently large \u043a > 0, the adversarial suffix:\n$\\Delta_{SoundAtk} =  \\begin{bmatrix} 0^T\\\\ \u03ba (2s^*-1_n)^T   \\\\0_\u03a6 \\end{bmatrix}  \u2208 \\mathbb{R}^{2 \\times 2n}$\ninduces a sequence $\\hat{s}_0, \\hat{s}_1$ that is not sound with respect to \u0393 and \u03a6.\nProof. Observe that each coordinate of \u03ba(2* \u2013 1n) has value \u00b1\u043a. For sufficiently large \u043a, this will amplify and suppress the appropriate coordinates in the weighted summation used by R."}]}