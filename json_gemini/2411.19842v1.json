{"title": "SCALING TRANSFORMERS FOR LOW-BITRATE HIGH-QUALITY SPEECH CODING", "authors": ["Julian D. Parker", "Anton Smirnov", "Jordi Pons", "CJ Carr", "Zack Zukowski", "Zach Evans", "Xubo Liu"], "abstract": "The tokenization of speech with neural audio codec models is a vital part of modern Al pipelines for the generation or understanding of speech, alone or in a multi-modal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of 400 or 700 bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.", "sections": [{"title": "1 INTRODUCTION", "content": "Compressed coding of audio and speech data in digital format has been an active area of research since the 1970s, and reached particular prominence in the late 1990s with the emergence of mp3 (Painter & Spanias, 2000). Research into improving the sound quality and compression ratio of such codecs (mainly using signal processing techniques) has continued (Valin et al., 2016). The main purpose of these codecs is to improve the efficiency of transmission and storage of what is traditionally a data-intensive medium.\nIn recent times, the research community began to apply the techniques of machine learning to the audio coding problem (Zeghidour et al., 2021). These models are referred to as neural audio codecs (NACs). Initially the goal of these models was similar to traditional audio codecs, which aim to maximize compression and audio quality at low computational cost. However, a paradigm shift occurred with the proposal of powerful generative models utilizing the token sequences produced by these codecs (Borsos et al., 2023a; Wang et al., 2023; Borsos et al., 2023b). With the arrival of these models and the plethora of new use-cases they encompass, the design goals of NACs have shifted to be less concerned with computational complexity, and more concerned with pushing compression (especially in the temporal dimension) to the maximum level possible.\nOur goal is to design a speech codec model in the spirit of this paradigm shift, whose primary purpose is to be used in combination with modern generative architectures for generation or understanding of speech signals. We make the observation that in a typical modern generative pipeline for speech there may be models totalling billions of parameters, a tiny fraction of which is usually dedicated to the codec model. There is therefore some headroom to increase the size of this component without overly impacting overall computational burden. This opens up scaling of the codec model size as a route to higher quality audio and higher compression levels.\nNeural audio codec models have largely been based on convolutional or recurrent architectures, which can be challenging to scale to larger model sizes without placing restrictions on the architecture. Even with such restrictions, the largest successful purely convolutional networks are generally below 1B parameters (Woo et al., 2023). Transformers (Vaswani, 2017) have shown the ability to scale to billions of parameters in many domains (Hoffmann et al., 2022), but have not been fully utilized in a codec context yet. Recent work has also deployed transformer blocks in the bottleneck of a convolutional codec, showing improvements in compression ratio (D\u00e9fossez et al., 2024)."}, {"title": "2 RELATED WORK", "content": "However, transformers have not so far been deployed as the main component of a codec model. One major contribution of this work is to design a new codec architecture that is predominantly transformer-based, and scale such an architecture into the 1B parameter range.\nThe majority of current codecs utilize a Residual Vector Quantizer (RVQ) (Zeghidour et al., 2021) in some form. This is effective in maximizing the expressivity of the bottleneck for a given bit-rate, but presents a number of challenges for generative modeling. One challenge is that it produces many parallel hierarchical streams of tokens. The causal relationship between the streams introduces a variety of complications that must be accounted for during training and inference (Borsos et al., 2023a; Copet et al., 2023; D\u00e9fossez et al., 2024). An additional challenge is that VQs and RVQs can suffer from poor or inconsistent codebook utilization, making the process of learning the token distribution more difficult and prone to bias. In this work we address some of the issues of VQ and RVQ by instead adopting a quantization scheme derived from Finite Scalar Quantization (FSQ) (Mentzer et al., 2023), and a novel post-hoc method decomposing FSQ into low-order residuals.\nWe demonstrate how these contributions enable the training of a waveform codec model that achieves high compression for speech, with ultra-low bitrates of 400 bps and 700 bps, while still preserving good audio quality.\nCode and models will be released at: github.com/Stability-AI/stable-codec."}, {"title": "2.1 NEURAL AUDIO CODECS", "content": "The dominant paradigm for training NACs has so far been based on the VQ-VAE structure, consisting of a classic autoencoder-like structure of encoder and decoder model with an information bottleneck placed in between them in the form of a quantizer. Soundstream (Zeghidour et al., 2021) was the first example of such a model aimed at handling varying bit-rates and types of audio with a single model. Soundstream introduced an adversarial loss in addition to reconstruction loss, and residual vector quantization (RVQ) for use in the bottleneck. EnCodec (D\u00e9fossez et al., 2022) proposed a number of improvements to this formulation and achieved higher audio quality. SpeechTokenizer (Zhang et al., 2023b), building on Encodec, introduces the use of semantic tokens in the first channel of discrete RVQ codecs, bridging the gap between text tokens and acoustic tokens for speech coding.\nDAC (also known as improved RVQGAN) (Kumar et al., 2023) investigated several design choices in this type of NAC, including the introduction of periodic inductive biases and improvements in codebook utilization. This approach achieved notable performance, compressing 44.1 kHz audio into discrete codes at an 8 kbps bitrate. While DAC delivers high-quality reconstruction at this compression level, its bitrate remains relatively high for generative audio modeling, requiring over 700 tokens per second for 44.1 kHz audio due to the large number of residual tokens."}, {"title": "2.2 LOW BITE-RATE SPEECH CODING", "content": "Recently, there has been growing interest (Li et al., 2024; Liu et al., 2024a; D\u00e9fossez et al., 2024) in optimizing bitrate efficiency in NACs while maintaining high reconstruction quality. Such low-bitrate, high-fidelity codecs are particularly crucial for improving efficiency and reducing latency in generative audio modeling. However, achieving extremely low bitrates (such as below 1 kbps for 16 kHz audio) remains challenging due to the complexities involved in accurately compressing high-frequency components in the audio waveform.\nSingleCodec (Li et al., 2024) addressed neural speech coding by proposing an enhanced VQ-VAE combined with bidirectional LSTM for mel-spectrogram compression, achieving a notably low bandwidth of 304 bps for 24 kHz speech mel-spectrogram coding, followed by BigVGAN (Lee et al., 2022) as a vocoder for waveform reconstruction. Inspired by recent advances in generative models, SemantiCodec (Liu et al., 2024a) offers a different approach by leveraging a latent diffusion model to generate latent features from a pre-trained mel-spectrogram VAE (which also requires a vocoder for waveform reconstruction). The diffusion model is conditioned on k-means clustered audio tokens derived from a pre-trained AudioMAE encoder. SemantiCodec supports low bitrates"}, {"title": "2.3 GENERATIVE MODELS FOR AUDIO AND SPEECH", "content": "Autoregressive models can operate directly on quantized audio waveforms, but can be slow during inference (Oord et al., 2016). Recent models, such as VALL-E (Wang et al., 2023), AudioLM (Borsos et al., 2023a), MusicGen (Copet et al., 2023), and VQ-VAE-based approaches for sound synthesis (Liu et al., 2021), improve efficiency by instead modeling quantized latent sequences. Non-autoregressive models (Oord et al., 2018) and adversarial audio synthesis (Donahue et al., 2018) were developed to overcome the inefficiencies of autoregressive models. Recent non-autoregressive models such as VampNet (Garcia et al., 2023), SoundStorm (Borsos et al., 2023b), or StemGen (Parker et al., 2024) are based on masked token modeling (Chang et al., 2022). End-to-end diffusion modeling can also be computationally demanding (Rouard & Hadjeres, 2021; Pascual et al., 2023). Recent efficiency improvements rely on latent diffusion models (Liu et al., 2023; 2024b; Yuan et al., 2024; Evans et al., 2024a;b;c; Yang et al., 2024), which often rely on VAEs for latent encoding. The recent growth of multi-modal and speech-first generative models such as SpeechGPT (Zhang et al., 2023a), LLaMA3 (Dubey et al., 2024) and Moshi (D\u00e9fossez et al., 2024) is also heavily reliant on tokenized representations of speech and audio. As such, learning quantized or continuous latent spaces with codecs is crucial for advancing audio and speech generation."}, {"title": "3 ARCHITECTURE", "content": "The architecture of the codec is shown in overview form in Fig. 1. We will discuss the design of the encoder and decoder sections with FSQ-based bottleneck separately."}, {"title": "3.1 ENCODER AND DECODER", "content": "Our encoder and decoder structures are designed to look very similar to a standard transformer architecture. Both consist of multiple blocks, each operating at a specific temporal resolution. These sections consist of a strided 1d dense convolution layer (for downsampling in the encoder) or its transposed equivalent (for upsampling in the decoder) and a chain of relatively standard transformer blocks. The only difference between the encoder and decoder architecture is that the downsampling or upsampling layer is placed in a different location\u2014in the encoder at the start of the block, and in the decoder at the end of the block. This maintains symmetry of the architecture. The stacked transformer blocks consist of a self-attention section and a feedforward section, with pre-norm placement of layer norm blocks. The layer norm blocks are configured with a higher than standard $\\epsilon$ as discussed in Appendix B.1. In addition, the self-attention utilizes QK-norm. The feedforward block consists of a reverse bottleneck with a gated MLP, utilizing the SiLU activation function. Both attention blocks and feedforward blocks are followed by LayerScale (Touvron et al., 2021), to further stabilize training. The self-attention uses a sliding window to restrict receptive field and aid generalization of the architecture to arbitrary length sequences. The self-attention mechanism incorporates Rotary Positional Embeddings (RoPE) (Su et al., 2024) and operates without a causal attention mask. However, a causal variant suited for streaming purposes is possible with relatively minor modifications, as described in Appendix A.4. We further examine the model's receptive field, causality, and latency in Appendix B.2.\nIn contrast to convolutional architectures, we want the majority of temporal downsampling or upsampling of the signal to occur at the input or output to the architecture. This is to avoid feeding very small dimension embeddings to the transformer blocks, and also to limit sequence length. Only minimal further resampling happens within the architecture using the strided convolutions and transposed convolutions in each encoder or decoder block. To achieve this we can use any filter-bank representation of the input signal which conforms to perfect reconstruction criteria. The"}, {"title": "3.2 DISCRETE BOTTLENECK", "content": "In order to mitigate the inherent problems of VQ and RVQ quantization, we employ a modified version of Finite Scalar Quantization (FSQ) (Mentzer et al., 2023). Instead of a learnable codebook of embeddings connected to particular tokens as in VQ/RVQ, FSQ derives a token sequence by projecting the latent representation to a low-dimensional space, then scalar quantizing each dimension of this space in regular intervals. Each combination of quantized levels can then be mapped to a unique integer value, producing the tokenization. FSQ is known to exhibit almost full codebook utilisation even with very large codebook sizes (e.g., $2^{18}$) (Mentzer et al., 2023).\nWe make some modifications to the FSQ algorithm to preserve symmetry of the quantized latents around the origin for any number of levels. Our formulation for the scalar quantizer function $Q_L$ for a given fixed number of levels $L$, applied to some scalar $x$ is given by:\n$Q_L(x) = \\frac{2}{L-1}\\left[ (L-1) \\tanh(x) + \\frac{1}{2} \\right] - \\frac{1}{2}$ (1)"}, {"title": "3.2.1 POST-TRAINING BOTTLENECK MODIFICATION", "content": "The formulation of FSQ used here has many post-training possibilities for adjusting the reconstruction quality against the number and range of the discrete tokens. Firstly, the regularization provided by training the FSQ bottleneck with uniform noise allows the number of levels for each dimension of the FSQ to be modified after training. As long as the number of levels is greater than or equal to the smallest seen during training, the error produced by the quantization is within the bounds previously seen and therefore is still valid.\nBy default FSQ produces one token per time-step. In general this is advantageous for our purposes. However, if the use-case requires it, we can decompose this single token post-hoc into multiple tokens using either a parallel partitioning of the dimensions, or (for particular choices of quantization-level number) into a hierarchical residual set of tokens ala RVQ. Parallel partitioning introduces a bi-directional causal relationship between tokens which is unexplored in the generative modeling context, and therefore for this work we concentrate on the hierarchical residual decomposition.\nResidual FSQ can be applied post-hoc to a bottleneck trained with a single quantizer but requires some restrictions. Namely, L is required to only use numbers of levels conforming to $L = 2^n + 1, n \\in Z^+$. This sequence of levels can be derived by starting from levels at $\\{-1, 0, 1\\}$ ($L = 3$), and continually subdividing the intervals between levels exactly at the half way point. We denote the set containing the positions corresponding to a particular number of levels $L$, as $l_L$. We can clearly see by examination that each larger set is a superset of the previous sets i.e $l_{2^{n+1}+1} \\supseteq l_{2^{n-1}+1}$, and also that we can can construct any particular set of levels using the Minkowski sum of smaller $l_3$ sets, progressively halved e.g $l_3 + \\frac{1}{4}l_5, l_3 + \\frac{3}{8} + \\frac{3}{8} l_9$ (albeit with extraneous new values outside the original range).\nA similar analysis holds for other level numbers conforming to the restriction given above, with the scalings consequently changed. We can utilize this property to do post-hoc residual quantization, using the standard formulation of a residual quantizer for a given latent $z$:\n$\\hat{z} = \\sum_{k=0}^{K} q_k,$\n$q_0 = \\kappa_0(z),$\n$q_k = \\kappa_k(z - \\sum_{i=0}^{k-1} q_i)$ (3)\nwhere $q_e$ denote the quantizer outputs, and $\\kappa_k$ denote the quantizer functions themselves, which we define in terms of our scalar quantizer function using levels $L = 2^n + 1, n \\in Z^+, Q_{2^{n+1}}$ as:\n$\\kappa_k(z) = \\frac{Q_{2^{n+1}}((2^n)^kz)}{(2^n)^k}$ (4)\nUsing this formulation, we have the guarantee that the quantized latent $\\hat{z}$ belongs to the set of quantized levels seen during training, despite not having been trained using a residual formulation. A downside of this approach is that some rare combinations of tokens result in latents outside the"}, {"title": "3.2.2 CALCULATING FSQ BITS-PER-SECOND", "content": "Using the post-hoc modification strategies described in Sec. 3.2.1, it is possible to achieve varying bits-per-second rates even for the same level of resolution.\nWe calculate bits-per-second (bps) for a decomposition with $n$ residual levels as:\nbps = $f_r \\sum_{i=0}^{n} [\\log_2(k_i)]$ (5)\nwhere $f_r$ is the number of frames per second of the codec (i.e. its latent rate) and the $k_i$ are the codebook sizes for each stage of the residual decomposition. We obtain these codebook sizes as:\n$k = L^d$ (6)\nwhere $L$ is the number of FSQ levels for the residual stage and $d$ is the FSQ dim.\nFor example, if we have an FSQ bottleneck with $L = 17, d = 6$, and a frame rate of 25Hz during training, this results in an effective bps of $25 \\times [\\log_2(17^6)] = 625$. If we partition this codebook into a residual formulation of 2 stages with 5 levels, we have an effective bps of $25 \\times 2 \\times [\\log_2(5^6)] = 700$ but with a much more manageable codebook size for generative modelling. The same calculation of bitrate can be used for RVQ, using the chosen codebook size for each residual level."}, {"title": "3.3 DISCRIMINATOR", "content": "We employ a discriminator inspired by that used in Encodec (D\u00e9fossez et al., 2022), consisting of multiple complex STFTs at different resolutions, followed by a combination of 1d and 2d convolutions. We make three major changes compared to previous versions of this discriminator: we scale parameter count by increasing the number of channels, we address systemic biases in the discriminator by adopting unevenly spaced STFT resolutions, and we address a late-training bias towards the noise-floor of the signal by scaling the magnitude of the complex STFTs before they are processed by the convolutional networks. The last two of these changes are motivated by analyzing the sensitivity of the discriminator to different regions of the input, and are justified in Appendix B.5."}, {"title": "3.4 TRAINING OBJECTIVES", "content": "Training the model is conducted in two stages with slightly different loss configurations - which we refer to as pretraining and finetuning. In each stage, the loss is a composite between several reconstruction losses and an adversarial loss derived from the discriminator network, which is trained in parallel. The main difference between pretraining and finetuning stages is in which reconstruction losses are used.\nSimilar to D\u00e9fossez et al. (2024), we simplify the adversarial loss by removing the direct adversarial classifier loss term and using only a normalized feature-matching L1 loss on the $M$ per-layer features of the multi-discriminator network containing $N$ individual discriminators, given by:\n$L_{disc}(x, \\hat{x}) = \\frac{1}{MN}\\sum_{m=1}^{M}\\sum_{n=1}^{N} \\frac{| D^m_n(x) - D^m_n(\\hat{x}) |_1}{\\text{mean}(||D^m_n(x)||_1)},$ (7)"}, {"title": "4 EXPERIMENTS", "content": "where $D^m_n$ is the output of the $m$-th layer of the $n$-th individual discriminator, $x$ is the target signal and $\\hat{x}$ is the reconstructed signal. This can be interpreted as a reconstruction loss using more semantically focused projections of the signal, which additionally adapt throughout the training as the discriminator improves. The discriminator is trained as usual as a binary classifier for real and fake examples utilizing a hinge loss.\nDuring the pretraining stage, we include a traditional L1 reconstruction loss and L1 STFT loss to boost convergence. This loss is weighted by a coefficient that decays exponentially per-step. This ensures that the reconstruction loss does not influence the training after an initial period defined by the exponential decay factor. The overall loss during pretraining is given by:\n$L_{pre} (x, \\hat{x}) = L_{disc} (x, \\hat{x}) + L_1 (x, \\hat{x}) + \\gamma^k * L_1(|X|, |\\hat{X}|)$ (8)\nwhere $\\gamma$ is an exponential decay coefficient, k is the training step and X, $\\hat{X}$ are the bins of the STFT of the target and reconstructed signals respectively.\nDuring the finetuning stage, we add a perceptual loss based on a pre-trained WavLM-Large(Chen et al., 2022) model. This perceptual loss is calculated similarly to discriminator feature-matching loss given in Eq. 7, by calculating L1 loss on the layer features of the target and reconstructed examples and normalizing by the mean magnitude of the target feature across the batch:\n$L_{perc} (x, \\hat{x}) = \\frac{1}{M} \\sum_{m=1}^{M} \\frac{| C^m(x) - C^m(\\hat{x}) |_1}{\\text{mean}(||C^m(x)||_1)},$ (9)\nwhere $C_m$ is the $m$-th layer of the model. We utilize all individual layer features supplied by the model. The overall loss during finetuning is given by:\n$L_{fine} (x, \\hat{x}) = L_{disc} (x, \\hat{x}) + L_{perc} (x, \\hat{x})$ (10)\nWe found this finetuning stage to be essential in producing intelligible speech, as well as improving objective metrics, as shown in the ablation studies presented in Appendix A.1."}, {"title": "4.1 DATA", "content": "For training speech codec models, we use the Librilight dataset (60k hours) and the English portion of the Multilingual LibriSpeech (MLS) dataset (45k hours). Both datasets contain 16 kHz original speech data, amounting to a total of approximately 105k hours of training data. For evaluation, we utilize the test-clean subset of LibriSpeech for speech data, selecting audio clips with durations ranging from 5 to 10 seconds to create a test set of 900 clean speech samples at 16 kHz."}, {"title": "4.2 MODEL AND TRAINING DETAILS", "content": "The codec model is configured with a patch size of 320 samples at the input. There are two encoder blocks. One directly follows the patching, and contains 8 transformer blocks. This is followed by a further encoder block performing 2x downsampling, which contains 20 transformer blocks. The embedding dimension of the transformer blocks is 1024, whilst the reverse bottleneck of the feedforward layer is 4x larger. The head dimension of the self-attention block is 128. Layer norms are configured with $\\epsilon = 1 \\times 10^{-2}$, and the sliding attention window is of size 128. The decoder is configured to be symmetrical with the encoder. The resulting model has approximately 950M parameters. The bottleneck is 6 dimensional and trained with 17, 9 and 5 levels for every dimension, randomly chosen. The ensemble discriminator is configured as described in Appendix B.5, with each discriminator having a channel count of 256. We use FlashAttention (Dao et al., 2022) to ensure computational efficiency. The model is trained with FP16 mixed precision.\nThe AdamW optimizer is used for both the autoencoder and discriminator, both with a learning rate of 0.0008. The autoencoder additionally uses weight decay with a coefficient of 0.01. Data is randomly chunked into segments of 5.12 seconds for training. 16 H100 GPUs are utilized, with an effective batch size of 128. Pretraining is conducted for 500k steps, with a decay coefficient of $\\gamma = 0.9999$ applied to the reconstruction losses. The STFT loss utilizes 2048 bins, a hop size of"}, {"title": "4.3 OBJECTIVE AND SUBJECTIVE METRICS", "content": "512 and a Hanning window. The finetuning stage is conducted for a further 150k steps using the WavLM-Large perceptual reconstruction loss in addition to the adversarial feature-matching loss. In both stages, all loss terms are weighted equally."}, {"title": "4.4 BASELINES", "content": "A set of objective metrics are used to assess perceptual quality, compression levels, reconstruction fidelity and semantic performance. These metrics are described in Appendix D.\nWe further conduct a subjective test with 24 participants that rate a total of 25 reconstructions from the same dataset used for objective metrics. We follow the precedent of previous works (Zhang et al., 2023b; D\u00e9fossez et al., 2022) and employ the MUSHRA (Schoeffler et al., 2018) format without hidden anchor. Listeners compare multiple versions of an example at once, including a labeled reference and a hidden reference and are asked the question \"Please evaluate the quality proximity between an audio sample and its reference. Please listen carefully to the reference audio and then rate the quality of each test audio clip compared to the reference. Use the scale where 0 indicates no resemblance to the reference, and 100 means perfectly the same as the reference.\". Participants were gathered online by openly by sharing a link to the test in a number of public forums. To limit the length of the subjective test, we only select a subset of the baselines for inclusion. These are chosen based on overall performance on objective metrics vs bits-per-second. The demographic breakdown of the participants is shown in Appendix E."}, {"title": "4.5 MAIN RESULTS", "content": "We compare our results against the 16 kHz models DAC, SpeechTokenizer, and SemantiCodec, as well as the 24 kHz models Encodec and Mimi. For DAC, which produces speech tokens at 50 Hz, we use the first two or four levels of RVQ to achieve bitrates of 1 kbps and 2 kbps, respectively.\nSpeechTokenizer operates with the same token rate as DAC, and we retain the first two or three levels of EVQ to obtain bitrates of 1 kbps and 1.5 kbps. For SemantiCodec, we select the variant with a codebook size of 16, 384 and evaluate it at token rates of 25 and 50 per second, corresponding to bitrates of 340 bps and 680 bps. Encodec is evaluated at 1.5 kbps and 3 kbps. For Mimi, we use all 8 RVQ levels for a bitrate of 1.1 kbps, and the first 4 levels to achieve 550 bps. For the 24 kHz models Encodec and Mimi, we first upsample the test audio to 24 kHz for reconstruction and then downsample it back to 16 kHz for evaluation.\nThe baseline models differ in design goals and applications: DAC, Encodec, and SemantiCodec support diverse audio domains (multilingual speech, music, general audio); Mimi focuses on streaming efficiency; and SpeechTokenizer is English speech-specific. Parameter counts also vary widely. While our work focuses on speech coding with training and evaluation on English datasets, the aim is to demonstrate the feasibility of a transformer-based speech codec and its scalability to larger parameter counts. The comparison between these models is framed within this context, as they represent recently published audio codecs with strong performance in speech coding. Differences in streamability, training data, and model size are detailed in Tab. 12.\nWe evaluate two variations of our model, with different post-hoc configurations of the FSQ bottleneck. One variant utilizes a single token per step, utilizing 6 levels for each of the 6 dimensions. This leads to an effective codebook size of $6^6 = 46656$. The other variant uses the residual decomposition described in Sec. 3.2.1 to use two residual tokens per step, each with an effective codebook size of $5^6 = 15625$. The procedure for calculating the quoted bits-per-second is described in Sec. 3.2.2. We additionally show objective results of the model with the quantizer removed from the bottleneck, giving an indication of the performance of the model if used with a diffusion model."}, {"title": "4.6 ADDITIONAL RESULTS", "content": "To evaluate the impact of model size, we conducted scaling experiments with TAAE architectures containing approximately 250M, 500M, and 1B parameters. The results confirm that the proposed structure scales effectively with parameter count, as detailed in Appendix A.2.\nWe also explored higher compression rates by modifying the encoder/decoder for 2\u00d7 additional up/downsampling (latent rate 12.5 Hz) and increasing the FSQ bottleneck dimension to $d = 8$. While this model achieves lower bitrates (e.g., 200 bps), it underperforms the main model and converges more slowly, as discussed in Appendix A.3.\nIn Appendix A.4, we describe and evaluate a causal version of the TAAE model. This variant shows minimal degradation compared to the non-causal version and outperforms the streaming codec Mimi in objective metrics, despite being trained with significantly fewer steps and data hours.\nAdditionally, we evaluated our proposed TAAE model across various settings beyond its original intended use case. In App. A.5, we assess the model performance on a range of languages, demonstrating its ability to generalize effectively to unseen languages, with results that are better or comparable to models trained on multiple languages. In App. A.6, we validate the model's generalization to utterances of varying lengths, including those longer or shorter than seen during training. We also compare our model with a HuBERT-based codec, analyzing key differences in design and performance, as discussed in Appendix A.7.\nFinally, we analyze the codebook utilization of TAAE, showing nearly optimal efficiency in its usage. A detailed comparison of codebook utilization and entropy-encoded bitrates across baselines is provided in Appendix A.8. A comparison of real-time factors between TAAE and the baselines is provided in Appendix A.9. This shows that despite the much larger parameter count, TAAE is competitive with baselines in terms of inference performance."}, {"title": "5 LIMITATIONS", "content": "The presented model has a number of limitations compared to baselines, primarily related to the training dataset rather than the architecture. We use only a modest amount of English-only speech data, 100k hours. The data is also at 16 kHz sampling rate, whereas 24 kHz or higher might be desirable in many applications. The dataset is predominantly made of audiobook recordings, so we might also expect the model to have difficulties with speech that is from a very different setting (e.g. overlapping speakers) or contains a significant amount of environmental sound. The limitations of the architecture are primarily related to parameter count and computational efficiency. The main presented model has a large parameter count which means that it may require greater computational resources than presented baselines, albeit mitigated by the availability of efficient transformer implementations. Future work should explore scaling up to a much larger and more diverse dataset at a higher sampling rate."}, {"title": "6 CONCLUSIONS", "content": "In this work we proposed a new scalable architecture for neural coding of speech waveforms, based on transformer-based encoder and decoder models and a flexible discrete bottleneck using Finite Scalar Quantization (FSQ). We described a number of techniques for altering or decomposing the tokens produced by this discrete bottleneck in order to fit the needs of various use-cases. We trained this architecture on a dataset of 16 kHz speech. We conducted objective and subjective evaluations of this model, showing state-of-the-art speech coding performance as well as generalization to unseen languages. The model can be adapted to streaming use-cases with little performance degradation, and is competitive with existing codec models in terms of inference speed, despite utilizing a much larger parameter count."}, {"title": "A APPENDIX: ADDITIONAL RESULTS", "content": "We performed a number of additional experiments during the training process of the main presented model, the results of which are shown here."}, {"title": "A.1 ABLATION ON FINETUNING USING PERCEPTUAL LOSS", "content": "Tab. 3 shows objective metrics for the main presented model, before and after the finetuning stage with using the WavLM perceptual feature-matching loss. As can be seen, this finetuning boosted sound quality metrics significantly, as well as significantly improving intelligibility \u2013 albeit at the cost of a tiny degradation in SI-SDR."}, {"title": "A.2 ABLATION STUDIES ON MODEL SCALING", "content": "To evaluate the effect of increasing model size on the performance of the TAAE architecture, we repeated the 500k step pretraining phase with models of approximately half and one quarter the parameter count of the main presented model. This is achieved by reducing the transformer embedding dimension to 768 and 512 respectively, whilst keeping all other hyper-parameters the same. Objective metrics for the trained models are shown in Tab. 4. We can see that scaling parameter count shows a clear improvement in objective metrics, although the smaller models still have respectable performance compared to baselines."}, {"title": "A.3 TRAINING MODELS WITH HIGHER COMPRESSION RATES", "content": "Tab. 5 shows the objective results of training the same architecture as our main presented model, with two major changes. The larger block in the encoder/decoder is split into two to provide an extra 2x upsampling/downsampling, giving an overall latent rate of 12.5 Hz. Additionally the dimension d of the FSQ bottleneck is increased to 8. The parameter count is the same, apart from a minor difference in the layers mapping into and out of the bottleneck. This model performs worse than the presented model (as shown in Tab. 2) in most metrics, albeit operating at a much lower bit-rate. Observation during training showed that this model converged much slower than the presented model, so this gap might close with additional training."}, {"title": "A.4 CAUSAL MODEL VARIATION FOR STREAMING USE", "content": "Although the purpose of this work was not to produce an audio codec intended for streaming use, it is possible to make some small modifications to the structure to make it fully causal. Firstly we shift the sliding attention window so that it is purely causal, instead of symmetrical around the query. Secondly, we replace the non-causal convolution layers with causal implementations.\nIn order to test the impact of these design changes, we finetune a fully trained non-causal model with these causal modification for 200k steps using the same"}]}