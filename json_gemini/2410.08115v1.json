{"title": "OPTIMA: OPTIMIZING EFFECTIVENESS AND EFFICIENCY FOR LLM-BASED MULTI-AGENT SYSTEM", "authors": ["Weize Chen", "Jiarui Yuan", "Chen Qian", "Cheng Yang", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present OPTIMA, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. OPTIMA employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, OPTIMA shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10% tokens on tasks requiring heavy information exchange. Moreover, OPTIMA's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, OPTIMA shows the potential towards scalable, efficient, and effective MAS\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have emerged as powerful tools for a wide range of tasks, from natural language processing to complex reasoning (OpenAI, 2023; Reid et al., 2024; Anthropic, 2024). A promising direction in leveraging these models is the development of autonomous multi-agent systems (MAS), which aim to harness the collective intelligence of multiple LLM-based agents for collaborative problem-solving and decision-making (Liang et al., 2023; Wang et al., 2024b; Du et al., 2024; Zhuge et al., 2024). However, for LLM-based MAS to be truly effective, they must overcome two critical challenges: (a) achieving efficient inter-agent communication to minimize computational costs, and (b) optimizing the collective performance of the system as a cohesive unit.\nCurrent LLM-based MAS face significant difficulties in meeting these challenges. The coordination and communication between agents often lack efficiency, resulting in verbose exchanges that lead to increased token usage, longer inference times, and higher computational costs (Li et al., 2024b). This inefficiency is exacerbated by the length bias inherent in LLMs due to alignment training (Saito et al., 2023; Dubois et al., 2024), which favors longer responses even when concise communication would suffice (Chen et al., 2024d). Moreover, while recent work has explored training LLMs for single-agent tasks (Song et al., 2024; Xiong et al., 2024) and MAS training is well-studied in reinforcement learning (Johnson et al., 2000; Lanctot et al., 2017; Baker et al., 2020), there remains a lack of parameter-updating methods specifically designed to optimize LLM-based MAS as a unified system. Existing approaches primarily rely on simple agent profile evolution (Chen et al., 2024b) or memory evolution (Qian et al., 2024a;b; Gao et al., 2024), which fail to address the core issues of communication efficiency and collective optimization.\nCan we develop a training framework that simultaneously enhances the communication efficiency and task effectiveness of LLM-based MAS? To address this question, we introduce OPTIMA, an effective framework designed to optimize LLM-based MAS. At the heart of OPTIMA is an iterative generate, rank, select, and train paradigm, incorporating a reward function that balances task performance, token efficiency, and communication interpretability. This approach enables the development of MAS that are not only effective and efficient but also maintain interpretable communication patterns. Based on the reward function, OPTIMA leverages a combination of techniques to induce efficient and effective communication behaviors in LLM-based agents, including Supervised Fine-Tuning (SFT) (Zelikman et al., 2022; G\u00fcl\u00e7ehre et al., 2023; Aksitov et al., 2023) and Direct Preference Optimization (DPO) (Rafailov et al., 2023; Pang et al., 2024), along with their hybrid variants. Furthermore, OPTIMA introduces an integration of Monte Carlo Tree Search (MCTS)-inspired techniques for DPO data generation, conceptualizing conversation turns as tree nodes to explore diverse interaction trajectories efficiently.\nImportantly, by substantially reducing the number of tokens required for inference, OPTIMA not only improves computational efficiency but also opens new possibilities for leveraging inference-compute more effectively. This reduction in token usage allows for more samples within the same computational constraints, potentially leading to better inference-time scaling laws. As recent work has shown the importance of inference-time compute in improving model performance (Wu et al., 2024; Brown et al., 2024; Chen et al., 2024a), OPTIMA's efficiency gains could be combined with techniques like majority voting (Wang et al., 2023), leading to more effective LLM systems.\nWe evaluate OPTIMA on a diverse set of tasks spanning two multi-agent settings: (a) information exchange, including information-asymmetric question answering (Chen et al., 2024d; Liu et al., 2024), and (b) debate, encompassing mathematical and reasoning tasks (Du et al., 2024; Chen et al., 2024b; Wu et al., 2023). Using Llama 3 8B (Meta, 2024) as our base model, we demonstrate that OPTIMA consistently outperforms both single-agent MAS baselines, achieving up to 90% reduction in token usage and 2.8x increase in task performance.\nTo summarize, our main contribution is OPTIMA, a novel training framework that simultaneously optimizes communication efficiency and task effectiveness. To enhance high-quality training data generation in multi-agent settings for DPO, we introduce an integration of MCTS-like techniques. Our comprehensive empirical evaluation across diverse tasks demonstrates notable advancements in both token efficiency and task performance, while also providing insights into the learned communication patterns. Additionally, we examine the implications of OPTIMA's efficiency gains for inference-time scaling laws, underscoring its potential to improve the overall capabilities of LLM systems by enabling more effective utilization of inference-compute. By addressing the dual chal-"}, {"title": "2 \u039f\u03a1\u03a4\u0399\u039c\u0391: OPTIMIZING MULTI-AGENT LLMS VIA ITERATIVE TRAINING", "content": "OPTIMA is built upon an iterative generate, rank, select, and train paradigm. This approach allows for the progressive improvement of LLM-based agents in multi-agent settings, focusing on enhancing both the efficiency of inter-agent communication and the effectiveness of task completion.\nLet Mbase denote the base LLM, D the task dataset, and f the iterative training function. The iterative process can be formalized as Mt+1 = f(Mt,D), where Mt represents the model at iteration t. The function f encapsulates the entire process of data generation, ranking, selection and model training. For each task instance di \u2208 D, we sample a set of N conversation trajectories {\u03c4i}i=1N \u2282 T using the agents powered by current model Mt. Each trajectory \u03c4 is then evaluated using a reward function R : T \u2192 R, defined as:\nR(\u03c4) = Rtask(\u03c4) \u2013 \u03bbtoken Rtoken(\u03c4) + \u03bbloss Rloss(\u03c4).\nHere, Rtask : T \u2192 R is the task-specific performance metric, Rtoken(\u03c4) = #Tokens(\u03c4)/maxk({#Tokens(\u03c4k)}) is the normalized token count, and Rloss(\u03c4) = g(L(Mbase, di, \u03c4i)) is based on the language modeling loss of the base model Mbase, which we detail in Appendix E.2. The positive coefficients \u03bbtoken and \u03bbloss are hyper-parameters. This reward function is designed to balance multiple objectives simultaneously: Rtask ensures that the model improves on the intended task, Rtoken encourages communication efficiency by penalizing verbose exchanges, and Rloss regularizes language naturalness and readability by favoring trajectories that are probable under the base model. By incorporating these components, we aim to develop LLM-based MAS that are not only effective in their designated tasks but also efficient in their communication, while maintaining interpretability in their outputs, unlike the often incomprehensible communication in prior RL research (Lazaridou et al., 2017; Evtimova et al., 2018; Chaabouni et al., 2022).\nBased on these rewards, we apply several data selection criteria to select a subset of high-quality sampled trajectories {\u03c4i} for each task instance. These selected trajectories form the training data Di at iteration i. The model is then updated: Mt+1 = Train(Mt, D\u2217i). The Train function can be"}, {"title": "2.2 INITIALIZATION: DIVERSIFYING AGENT COMMUNICATION", "content": "Before starting the iterative training process, we address a critical challenge in LLM-based MAS: agents often produce responses in a similar style across conversation trajectories, even with high-temperature sampling. This homogeneity limits the exploration of diverse communication strategies, potentially hindering the optimization toward more efficient and effective interactions. Following the observation from AutoForm (Chen et al., 2024d), where LLMs can be explicitly prompted to leverage different more concise formats to communicate or reason without much compromise in performance, we introduce an initialization step that promotes diversity in agent communication.\nOur approach leverages a pool of format specification prompts, P = {P1, P2, ...,pk}, where each Pk is a string specifying a particular response format (e.g., JSON, list, see Appendix F for concrete examples and creation process). For each task instance di \u2208 D, we generate N conversation trajectories, each with a randomly selected format specification appended to the input task:\n\u03c4jN = Mbase (di Pk\u2081), kj ~ Uniform(1, K), j = 1, ..., N,\nwhere denotes string concatenation. This process yields a diverse set of trajectories {\u03c4i}i=1N for each di, varying in both content and structure.\nWe then evaluate these trajectories using the reward function defined in Eq. (1), for each di, we select the trajectory with the highest reward: \u03c4\u2217 = arg maxj R(\u03c4jN). Finally, we select top k trajectories that exceed a predefined performance threshold \u03b8init, resulting in a high-quality dataset:\nD0\u2217 = TopK({(di, \u03c4\u2217)|Rtask(\u03c4\u2217) > \u03b8init, \u2200di \u2208 D}, 0.7|D|).\nCrucially, we remove the format specification prompts from the selected trajectories, resulting in a dataset of diverse, high-quality conversations without explicit format instructions. Using this dataset, we fine-tune the base model and obtain Mbase to obtain M0 = SFT(Mbase, D0), which serves as the starting point for OPTIMA, able to generate diverse communication patterns without explicit format prompting. We provide pseudo-code in Appendix B for better understanding. This initialization sets the stage for more effective exploration and optimization in the subsequent iterative training process."}, {"title": "2.3 FRAMEWORK INSTANTIATION 1: ITERATIVE SUPERVISED FINE-TUNING", "content": "We introduce iterative Supervised Fine-Tuning (iSFT) as our first instantiation of OPTIMA. At each iteration t, iSFT follows the same general procedure outlined in Algorithm 3, generating a"}, {"title": "2.4 FRAMEWORK INSTANTIATION 2: ITERATIVE DIRECT PREFERENCE OPTIMIZATION", "content": "While iSFT provides a straightforward approach to optimizing LLM-based MAS, it may be limited by its reliance on a single best trajectory for each task instance. To address this, we explore iterative Direct Preference Optimization (iDPO) (Rafailov et al., 2023; Pang et al., 2024), which optimizes models using comparative preferences and has demonstrated success in LLM alignment. Applying DPO in multi-agent settings, however, poses distinct challenges, particularly in generating meaningful paired data that capture the complexities of agent interactions.\nData Generation: To overcome these challenges, we integrate MCTS with DPO data collection for high-quality paired data generation in multi-agent settings. Our MCTS-based approach conceptualizes the multi-agent conversation as a tree, where nodes represent conversational turns, and edges represent continuations. This structure allows us to explore diverse interaction trajectories systematically and select high-quality paired data for DPO training. The MCTS process begins at the root node (initial task prompt) and proceeds as follows: (1) Expansion: We select a node to expand based on the following criteria. We first exclude leaf nodes and the second-to-last level nodes to avoid wasting computation on low-variance expansions, then exclude nodes with content similar to previously expanded nodes, measured based on edit distance (see Appendix E.1). From the remaining nodes, we select 10 nodes with the highest rewards and sample one using the softmax distribution over their rewards. (2) Simulation: For each selected node, we expand 3 trajectories, simulating the conversation to completion. (3) Backpropagation: Once a trajectory is completed and rewarded with Eq. (1), we update the estimated rewards of all nodes in the trajectory with the average rewards from their children. (4) Iteration: We repeat the above process 8 times, resulting in 24 trajectories. More iterations could potentially lead to more diverse and better-quality data.\nPaired Data Construction: To generate high-quality paired data for DPO training, we traverse each MCTS tree and identify node pairs (ni, nj) that satisfy three conditions: (1) shared ancestry, (2) the higher estimated reward of ni and n; exceeds the threshold \u03b8dpo\u2212filter, and (3) their reward difference exceeds the threshold \u03b8dpo\u2212diff. We sort these pairs by the higher estimated reward, and select the top 50% pairs as part of the final training set. We construct DPO training instances by using the common conversation history as the prompt, with ni and n; serving as the chosen and rejected responses according to their estimated rewards.\nThe iDPO process then proceeds iteratively, alternating between MCTS-based data generation and model updates using DPO. The pseudo-code for our iDPO process is presented in Algorithm 2."}, {"title": "2.5 FRAMEWORK INSTANTIATION 3: HYBRID ITERATIVE TRAINING", "content": "Building upon the strengths of both iSFT and iDPO, we investigate a hybrid approach that interleaves SFT and DPO in the iterative training process, termed as iSFT-DPO. This hybrid method aims to leverage the simplicity and directness of SFT in capturing high-quality trajectories, while also benefiting from the nuanced comparative learning facilitated by DPO. By alternating between these two training paradigms, we hypothesize that the model can more effectively balance the exploration of diverse communication strategies with the exploitation of known effective patterns.\nIn practice, we implement this hybrid approach by performing one iteration of iSFT followed by one iteration of iDPO, and repeating this cycle throughout the training process. This interleaving allows the model to first consolidate learning from the best observed trajectories through SFT, and then refine its understanding through the comparative preferences provided by DPO."}, {"title": "3 EXPERIMENTS", "content": "Datasets. We evaluate OPTIMA on two multi-agent settings: information exchange (IE) and debate. For IE, we use HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (2WMHQA) (Ho et al., 2020), TriviaQA (Joshi et al., 2017), and CBT (Hill et al., 2016). For multi-hop datasets (HotpotQA, 2WikiMultiHopQA), we split relevant contexts between two agents, ensuring the answer can only be deduced from information exchange. For TriviaQA and CBT, contexts are randomly assigned, challenging agents to identify and communicate the relevant information effectively. The debate setting employs GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), ARC's challenge set (ARC-C) (Bhakthavatsalam et al., 2021) and MMLU (Hendrycks et al., 2021a), with one agent as solver and another as critic (Chen et al., 2024b). We use 0-shot for all benchmarks.\nMetrics. We report F1 score between generated answers and labels for IE tasks. For debate tasks, we employ exact match accuracy (GSM8k, ARC-C, MMLU) or Sympy-based (Meurer et al., 2017) equivalence checking (MATH), following Lewkowycz et al. (2022). Conversations conclude when agents both mark the same answer with specified special tokens or reach a turn limit.\nBaselines. We compare against single-agent approaches: Chain-of-Thought (CoT) (Wei et al., 2022) and Self-Consistency (SC) with majority voting (Wang et al., 2023) on n = 8 samples. Given that the generated responses for IE tasks are in free form, direct adaptation to majority voting is impractical. Therefore, we first compute the pairwise F1 score among the sampled answers, grouping those with a pairwise F1 score exceeding 0.9, and report the average F1 score against the label for all the answers in the largest grouping. In the multi-agent context, we compare against Multi-Agent Debate (MAD) from Du et al. (2024) and AutoForm (Chen et al., 2024d). MAD utilizes natural language for inter-agent communication, providing a baseline for common multi-agent dialogue, while AutoForm encourages agents to leverage concise, non-natural-language formats to achieve a better performance-cost ratio, offering a comparison point for efficiency-oriented MAS.\nTraining Setups. We use Llama 3 8B (Meta, 2024) as our base model across all benchmarks. Our experiments focus on two-agent scenarios without external tools, a design choice that allows us to isolate and analyze the core aspects of multi-agent communication and collaboration. By constraining our initial investigation to these fundamental settings, we can more clearly demonstrate the efficacy of OPTIMA in optimizing inter-agent communication and task performance. This approach also provides a strong baseline for future research exploring more complex scenarios with multiple agents and tool use. Besides, we train a single model for both agents, although training separate models might yield improved performance, we leave it for future exploration. Detailed training configurations and prompts are provided in Appendices E and F."}, {"title": "3.1 BENCHMARK RESULTS", "content": "Table 1 showcases OPTIMA's performance across a diverse set of tasks, revealing consistent improvements over baseline methods in both effectiveness and efficiency. In IE tasks, OPTIMA variants demonstrate substantial gains, particularly in multi-hop reasoning scenarios like HotpotQA and 2WMHQA. Here, iSFT-DPO achieves peak performance while significantly reducing token usage"}, {"title": "3.2 How WELL DOES OPTIMA GENERALIZE TO OOD TASKS?", "content": "To assess OPTIMA's ability to generalize, we conducted transfer learning experiments across different task domains. We transferred models trained on HotpotQA to TriviaQA and 2WMHQA, as well as transferring from MATH to GSM8k. While these datasets share broad categories (question-answering and mathematical reasoning, respectively), they present different challenges in terms of complexity and required skills. The results, presented in Table 2, demonstrate OPTIMA's robust transferability across these diverse tasks. In the question-answering domain, all OPTIMA variants significantly outperform baseline multi-agent methods on both OOD datasets. On 2WMHQA, the transferred iSFT more than doubles MAD's F1 score while using only 14.6% of the tokens. Similar trends are observed in TriviaQA. When transferring from MATH to GSM8k, OPTIMA variants, particular iDPO, not only outperform the baselines on GSM8k but also achieve results comparable to models directly trained on GSM8k with even higher token efficiency (refer to Table 1 for comparison).\nThese results underscore OPTIMA's potential for developing adaptable MAS, demonstrating that OPTIMA-trained models learn transferable skills for efficient information exchange and collabora-"}, {"title": "3.3 CAN OPTIMA LEAD TO BETTER INFERENCE SCALING LAW?", "content": "Recent research has highlighted the importance of inference scaling laws, which describe how model performance improves with increased compute during inference, typically by generating multiple samples per problem (Brown et al., 2024; Wu et al., 2024). While training scaling laws focus on the relationship between model size, dataset size, and performance, inference scaling laws explore the trade-off between inference compute budget and task accuracy. This paradigm offers a promising avenue for enhancing model capabilities without the need for further training models.\nFig. 3 illustrates OPTIMA's impact on inference scaling laws. The left panel shows the relationship between the number of SC steps and performance on multi-agent debate tasks. We observe that while majority voting accuracy tends to plateau after a certain number of steps, the coverage, defined as the percentage of problems answered correctly at least once, continues to improve logarithmically with increased sampling. This trend aligns with findings in recent inference scaling law studies (Wu et al., 2024; Chen et al., 2024a) and suggests that more sophisticated answer selection techniques could further boost OPTIMA's performance. We provide additional scaling law figures for all OPTIMA variants and on both IE and debate tasks in Appendix A, where similar trends can be observed.\nThe right panel of Fig. 3 demonstrates OPTIMA's efficiency in improving inference scaling laws on the GSM8k task. OPTIMA variants, both those trained directly on GSM8k and those transferred from MATH, consistently outperform the CoT SC baseline except the iSFT variant transferred from MATH. Notably, iDPO trained on GSM8k achieves the performance of CoT-SC at around 10,000 tokens with 88.5% fewer tokens, effectively \u201cshifting the curve left\u201d. This significant reduction in token usage translates to substantial computational savings without sacrificing accuracy. Moreover, the MATH-trained OPTIMA variants, except iSFT, also deliver better inference scaling laws on GSM8k compared with CoT SC, underscoring the framework's ability to generalize effectively across related tasks.\nThese results highlight OPTIMA's potential to reshape inference scaling laws for LLM-based MAS and even general LLM systems. By enabling more efficient use of the inference compute budget, OPTIMA allows for better performance at lower computational costs or higher performance at the same cost. This efficiency gain opens new possibilities for leveraging advanced inference techniques like weighted voting or best-of-N selection (Wu et al., 2024), potentially leading to even greater performance improvements."}, {"title": "3.4 How DOES OPTIMA EVOLVE AGENT COMMUNICATION AND PERFORMANCE?", "content": "tion. The subsequent iterations demonstrate OPTIMA's ability to refine these strategies for efficiency without compromising performance. We observe a gradual but consistent decrease in token usage across all variants, coupled with continued performance improvements.\nTo provide concrete examples of how OPTIMA shapes agent communication, we present a case from iSFT on an information exchange task in Fig. 4. The base model exhibits unfocused and repetitive exchanges, failing to efficiently address the task at hand. At iteration 0, while more structured, the exchange is verbose and includes unnecessary metadata. By iteration 2, we observe a marked shift towards concise, task-oriented communication, with agents adopting a streamlined format that efficiently conveys key information. The final iteration demonstrates further refinement, with agents maintaining the efficient structure while eliminating any residual verbosity. This progression aligns with our quantitative findings, showcasing OPTIMA's ability to form communication patterns that are both highly effective and remarkably efficient."}, {"title": "4 RELATED WORK", "content": "LLM-Based MAS. LLM-based MAS have emerged as a powerful paradigm for addressing complex tasks across various domains. Seminal works by Liang et al. (2023) and Du et al. (2024) demonstrated the potential of LLM-powered agents in collaborative problem-solving through multi-agent debate. This foundation has sparked diverse research directions, including role-playing for complex reasoning (Wang et al., 2024b; Chen et al., 2024b), collaborative software development (Qian et al., 2024c; Hong et al., 2024; Ishibashi & Nishimura, 2024), and embodied agent interactions (Zhang et al., 2024; Mandi et al., 2024; Guo et al., 2024). Recent studies have shown that increasing the number and diversity of agents can lead to performance gains in MAS (Wang et al., 2024a; Li et al., 2024a; Chen et al., 2024c). However, as LLM-based MAS grow in scale and complexity, challenges related to computational costs and communication efficiency become more pronounced (Chen et al., 2024d; Li et al., 2024b). Notably, there is a lack of systematic training algorithms specifically designed to optimize both the effectiveness and efficiency of LLM-based multi-agent systems, with most existing approaches relying on updating agent memory (Qian et al., 2024a; Gao et al., 2024). Our work addresses this gap by introducing a training framework that simultaneously enhances communication efficiency and task effectiveness in LLM-based MAS.\nIterative Refinement of LLMs. The pursuit of continual improvement in LLMs has led to the development of various iterative refinement paradigms. While self-reflection mechanisms like Reflexion (Shinn et al., 2023) and self-refine (Madaan et al., 2023) show promise, they heavily rely on LLMs' limited self-correction abilities, which is relatively weak for most of the current LLMs (Huang et al., 2024; Olausson et al., 2024; Kamoi et al., 2024). More robust approaches focus on iterative parameter updates, for example, ReST (G\u00fcl\u00e7ehre et al., 2023), ReSTEM (Singh et al., 2024) and STaR (Zelikman et al., 2022) train models on self-generated high-quality reasoning paths, Pang et al. (2024) further integrate the incorrect self-generated paths and train models with DPO. The extension to complex, multi-step tasks (Aksitov et al., 2023) further demonstrates the versatility of these methods. However, iterative refinement remains largely unexplored in the context of LLM-based MAS. Our work addresses this gap by presenting the first effective training framework for iteratively optimizing LLMs in MAS contexts. By simultaneously enhancing communication efficiency and task effectiveness, our approach shows the potential of iterative training in MAS."}, {"title": "5 CONCLUSION", "content": "We present OPTIMA, a novel framework for training LLM-based MAS that significantly improves communication efficiency and task performance. Extensive experiments across a range of tasks demonstrate OPTIMA's consistent superiority over both single-agent and multi-agent baselines. The framework introduces key innovations such as iterative training techniques, a balanced reward function, and an MCTS-inspired approach for data generation. OPTIMA also shows promise in enhancing inference scaling laws and transferring knowledge to OOD tasks. These findings highlight the critical role of efficient communication in MAS and LLM systems. While OPTIMA marks a major step forward in multi-agent LLM training, further exploration into its scalability to larger models and more complex scenarios is a promising direction for future research."}, {"title": "A INFERENCE SCALING LAWS ON INFORMATION EXCHANGE TASKS", "content": "This section extends our analysis of inference scaling laws to information exchange (IE) tasks, complementing the debate task results presented in the main text (Section 3.3). Fig. 5 provides a comprehensive view of how OPTIMA variants perform across both task types as the number of SC steps increases.\nFor debate tasks (Fig. 5a-c), we observe consistent trends across all OPTIMA variants. The coverage exhibits a clear log-linear relationship with the number of SC steps. This trend is particularly pronounced for the MATH task, where the potential for improvement through increased sampling is most evident. Majority voting accuracy tends to plateau earlier, suggesting that more sophisticated answer selection techniques might be necessary to fully leverage the diversity of generated responses.\nIn the case of information exchange tasks (Figures 5d-f), we note similar log-linear scaling in coverage\u00b2 across all OPTIMA variants. However, the improvement in majority voting accuracy for IE tasks is less pronounced compared to debate tasks. This discrepancy may be attributed to the specific majority voting variant we designed for F1 scores (detailed in Section 3), which might not be optimal for capturing the nuances of partial correctness in these tasks.\nThese results, while highlighting some task-specific differences, collectively reinforce the potential of OPTIMA-trained models to benefit from increased inference compute. The consistent log-linear scaling in coverage across all tasks and variants indicates that there is substantial room for performance improvement through more advanced answer selection strategies or increased sampling."}, {"title": "B ADDITIONAL PSEUDO-CODES FOR OPTIMA VARIANTS", "content": "To elucidate the implementation of various OPTIMA variants, we present algorithmic representations of several critical processes intrinsic to these variants. Specifically, we delineate the pseudo-code for (1) the initialization dataset collection process, as elucidated in Section 2.2 and illustrated in Algorithm 3; (2) the Monte Carlo Tree Search-based data generation process employed in iDPO (Section 2.4), as depicted in Algorithm 5; and (3) the procedure for node selection during the expansion phase of MCTS, as outlined in Algorithm 4. These algorithmic representations serve to provide a comprehensive and rigorous exposition of the methodological framework underlying the OPTIMA variants."}, {"title": "C CASE STUDY ON REWARD COMPONENTS ABLATION", "content": "In this section, we present a case study from the loss ablation analysis in the iSFT-DPO setting. In the 2WikiMultiHop QA task, we observe that without the constraint of the loss function, agents may generate outputs that are unreadable, contain incorrect information, and fail to communicate in a well-structured format, as demonstrated in Table 4. In the ARC task, we find that without the loss constraint, Alice tends to use fewer tokens in the reasoning process, making it harder for Bob to identify and correct errors in the reasoning, as shown in Table 5."}, {"title": "D CASE STUDY ON DEBATE TASK", "content": "In Section 3.5, we presented an example from 2WMH QA, illustrating OPTIMA's impact on an information exchange task. Here, we provide a complementary case study from a debate task to demonstrate OPTIMA's effectiveness across different multi-agent settings. Fig. 6 showcases the evolution of agent communication in a debate task across iterations 0, 2, and 4 of OPTIMA training. The task involves discussing the environmental impact of fertilizer runoff on ocean bays.\nAt iteration 0, agents engage in a structured but verbose exchange. By iteration 2, the communication becomes more concise, with agents summarizing key steps without explicitly restating each link. At iteration 4, we observe further refinement in communication efficiency, with agents expressing the core concept in just three exchanges, omitting intermediate steps that can be inferred."}, {"title": "E EXPERIMENT DETAILS", "content": "DATA GENERATION\nMCTS Node Expansion. Let N denote the set of all the nodes within a MCTS tree, Nexpanded denote the set of previously expanded nodes, and Ncand = N - Nexpanded denote the initial candidate nodes. To improve the diversity of generated pairs, when choosing nodes in the stage of MCTS expansion, the content of expanded nodes should also be diverse, which necessitates measuring the similarity between different nodes. Therefore, for every ni \u2208 Nexpanded and nj \u2208 Ncand, we calculate their similarity as Si,j = 1 - edit_distance(ni,nj)/|ni|, where |ni| is the length of the content of ni. Based on"}, {"title": "E.2 RANKING", "content": "In this section, we give a more detailed explanation of Rloss(\u03c4) in Eq. (1). Let \u03c4 [k] represent the k-th conversation turn of \u03c4, then the Rloss (\u03c4) is defined as maximum value of language modeling loss of {\u03c4 [k]}k under the base model, which can be described as follows:\nRloss(\u03c4) = maxk (L(Mbase, di, \u03c4i [k])).\nIn this way, we use Rloss (\u03c4) as a proxy for the readablity of \u03c4, so that we can constrain the readability of \u03c4 implicitly."}, {"title": "E.3 TRAINING", "content": "Initialization. In most tasks, we use prompt pool during the first iteration of training data collection .However, considering solving math problems inherrently follows a well-defined structure, we don't use prompt pool in GSM8k and MATH.\niSFT. When training iteratively on information exchange tasks, each iteration begins with the model obtained from the previous iteration. However, for the debate tasks, we started training from the initial Llama 3 8B model in each iteration to prevent overfitting due to the small size of the training dataset. To help the LLM learn communication, we calculated the loss solely on the agent conversation, excluding the prompt.\niDPO. Following iterative RPO (Pang et al., 2024), we conduct training from last iteration in the iDPO setting. To achieve better performance, we utilize the RPO loss, defined as follows:\n LDPO+NLL = LDPO (c, yu, c\u2212, y \u2212) + \u03b1LNLL(c, yu|xi) = log \u03c3(c, yuxi)/Mo(c, yu|xi)/Mo(c\u2212, y \u2212|xi)+ \u03b1logMo(c\u2212, y \u2212|xi)/Mt(c\u2212, y \u2212|xi)\n\u03bbDPO+NLL= \u03bbDPO (c, yu, c\u2212, y \u2212) + \u03b1LNLL(c, yu|xi) = log \u03c3(c, yuxi)/Mo(c, yu|xi)/Mo(c\u2212, y \u2212|xi) + \u03b1logMo(c\u2212, y \u2212|xi)/Mt(c\u2212, y \u2212|xi)\n\u03b1logMo(c,\u2212, y\u2212)/Mt(c,\u2212, y\u2212) (4)\niSFT-DPO. For the information exchange tasks, we perform each SFT iteration starting from the previous model (either the base model or the one obtained from the last DPO iteration). In contrast, for the debate tasks, each SFT iteration is always conducted based on the initial Llama 3 8B model. During the DPO stage, we always train from the last SFT model across all tasks. For example, on the debate tasks, both M0SFT and MSFT are trained based on the initial Llama 3 8B, but on information exchange tasks, MSFT is trained based on its previous model Mi\u22121DPO. However, Mi\u22121DPO is trained based on the M0SFT across all the tasks. Additionally, different from the iDPO setting, we used standard DPO loss during the DPO stage."}, {"title": "E.4 HYPER PARAMETERS", "content": "We conducted six iterations of training for each task. The hyper parameters we used are shown in Table 6. The \u03b1 and \u03b2 in iDPO section of the table correspond to the \u03b1 and \u03b2 terms in Eq. (4)."}, {"title": "F PROMPTS USED IN EXPERIMENTS", "content": "In this section, we present the prompts used in our experiments, including those for information exchange tasks (Table 7), GSM8k and MATH (Table 8), as well as ARC-C and MMLU (Table"}]}