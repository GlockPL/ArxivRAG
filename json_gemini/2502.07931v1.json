{"title": "Educating a Responsible AI Workforce: Piloting a Curricular Module on AI Policy in a Graduate Machine Learning Course", "authors": ["James Weichert", "Hoda Eldardiry"], "abstract": "As artificial intelligence (AI) technologies begin to permeate diverse fields\u2014from healthcare to\neducation-consumers, researchers and policymakers are increasingly raising concerns about\nwhether and how AI is regulated. It is therefore reasonable to anticipate that alignment with\nprinciples of 'ethical' or \u2018responsible' AI, as well as compliance with law and policy, will form an\nincreasingly important part of AI development. Yet, for the most part, the conventional computer\nscience curriculum is ill-equipped to prepare students for these challenges. To this end, we seek to\nexplore how new educational content related to AI ethics and AI policy can be integrated into\nboth ethics- and technical-focused courses. This paper describes a two-lecture AI policy module\nthat was piloted in a graduate-level introductory machine learning course in 2024. The module,\nwhich includes an in-class active learning game, is evaluated using data from student surveys\nbefore and after the lectures, and pedagogical motivations and considerations are discussed. We\nfind that the module is successful in engaging otherwise technically-oriented students on the topic\nof AI policy, increasing student awareness of the social impacts of a variety of AI technologies\nand developing student interest in the field of AI regulation.", "sections": [{"title": "Introduction", "content": "The explosive growth of artificial intelligence (AI) technologies is widely documented and\nincreasingly evident in everyday life: some responses from the search engine Google now include\nan \u201cAI Overview\u201d inserted before the first webpage link; companies like Tesla and Waymo have\nseen success in implementing partial or full autonomous driving in vehicles on live roads; and\n\"Apple Intelligence\u201d was the flagship feature for the launch of Apple's new smartphone in fall\n2024. Yet what legal or policy response this technological growth will precipitate is less certain\n[1, 2]. Nevertheless, it should be expected that the development and enactment of regulatory\nframeworks for AI will demand AI engineers with a command not only of the technical\nintricacies of AI models, but also of the policy and regulatory landscape for AI development and\ncompliance [3]. This is made clear by the 2023 U.S. Executive Order on Safe, Secure, and\nTrustworthy Development and Use of Artificial Intelligence [4], which calls for an \u201cAI talent"}, {"title": "Literature Review", "content": "Surveys of ethics education in the computing or computer science context have identified\nsignificant diversity in what content is taught, how it is taught, and to whom it is taught. Our\nsystematic review [5] of undergraduate CS degree requirements at 250 universities worldwide\nrevealed that standalone computing ethics courses are a required part of the degree for only one\nthird of programs, while nearly half of programs do not offer a computing ethics course at all.\nThus, the reach of CS ethics education is perhaps less extensive than commonly thought.\nHowever, as Brown et al. [11] show in their systematic review of 100 CS ethics education\nresearch papers published in top venues, there is a roughly even distribution of ethics teaching\nbetween standalone courses (32%) and integration of ethics content into one (26%) or more\n(35%) modules of a technical course. With respect to teaching approach, instructors use a mix of\npedagogical strategies in delivering content, the most common of which are class discussions,\nreadings, lectures, and writing assignments [11]. Active learning techniques such as role-playing\nsimulations, debates, and games are less common.\nWhile approaches to teaching computing ethics vary across institutions, what is taught under the\numbrella of 'computing ethics' varies significantly more. A review of 115 \u2018tech ethics' course\nsyllabi by Fiesler et al. [12] identified 15 course topics each featured in over 10 courses, ranging\nfrom professional ethics to inequality, justice & human rights to cybersecurity. Among the 15\ntopics, law & policy was most prevalent, appearing in 57% of syllabi, while AI & algorithms\nappeared in 48%. However, our review [6] of 116 computer science ethics courses (having only\n16% overlap with the courses in the Fiesler et al. corpus) concludes that a shift from teaching\nabout Al as an ethics case study to Al ethics as a subfield\u2014where emphasis is placed not only on\nthe impacts of AI technology, but equally on how to facilitate the responsible use and ethical\ngovernance of AI\u2014is only beginning to take place. The nascency of AI ethics and policy\neducation provides ample opportunity for experimentation in content (e.g. topics, case studies,\netc.), format (lecture vs. seminar vs. lab), and pedagogical strategies (e.g. discussions, writing\nassignments, in-class activities, etc.)."}, {"title": "AI Ethics and Policy", "content": "As AI systems feature in more consumer technologies, and as student interest in AI and machine\nlearning continues to grow, the CS education community has coalesced around a call to develop\nethics education tailored to the particular impacts and challenges posed by AI [13\u201317]. What\nshape this AI ethics curriculum will take, however, is still largely unclear. The absence of a\nsolidified 'canon' for AI ethics teaching is conspicuous in the variety of case studies and\nperspectives used when discussing AI in ethics courses [6]. A few core topics and similar case\nstudies do emerge, however. Through their analysis of 51 ethics and technical AI courses, Garrett\net al. [15] identify bias, fairness, privacy, and automation as the most common themes through\nwhich the social impacts of AI are evaluated. For example, the authors note many courses use the\nCOMPAS automated recidivism algorithm [18] as a case study for discussing AI bias. We posit\nthat these themes are most common because they represent adaptations of themes common in the\ncomputing ethics curriculum before the advent of AI. Privacy and the impact of automation on"}, {"title": "Methodology", "content": "The Al policy module described below is designed to integrate discussions on ethics and social\nimplications of AI into a \u2018technical' AI/ML course, providing students with opportunities to\nconnect the methods and algorithms learned in class with \u2018real-life' impacts. As such, we piloted\nthe module at the end of a graduate-level machine learning course (\u201cML 1\u201d). ML 1 covers a\nvariety of foundational topics in machine learning, including supervised learning (regression,\nclassification), unsupervised learning (clustering, anomaly detection), and neural networks.\nThe two lectures in the module were presented at the end of the semester, after the course's final\nexam but the week before students gave presentations on their course-long group projects. This\nscheduling had two advantages: (1) the module could draw on the entirety of the course's\ntechnical material, and (2) students could focus on the module instead of worrying about studying\nfor their final exam. The module was led by the first author of this paper, who was the course's\ngraduate teaching assistant and whose research focuses on AI ethics and policy education."}, {"title": "Participants", "content": "Although three-fourths of the students enrolled in ML 1 were graduate students in computer\nscience, the course was open to all graduate students interested in machine learning and also\nincluded students from other engineering and science disciplines, including civil engineering,\nindustrial engineering, and psychology. This disciplinary diversity, combined with the diversity in\nresearch interests among students, contributed to a variety of perspectives during the module's"}, {"title": "Data Collection", "content": "The primary source of data, both quantitative and qualitative, for evaluating the module came\nfrom student responses to a pre-module and a post-module survey. The Pre-Survey was\nadministered at the beginning of the first lecture after introducing the aim of the module, while\nthe Post-Survey was administered at the end of the second lecture. In order to encourage honest\nfeedback from students, no identifying information was collected, leading to lower response rates\n(n = 17 for the Pre-Survey and n = 13 for the Post-Survey) despite time being allocated during\nthe lectures for students to complete the surveys. Both surveys include a section each on \u201cAI\nEthics\u201d and \u201cAI Policy\u201d, corresponding to the module's two lecture topics. The questions in these\nsections consist of short free-response and 1-5 Likert scale questions, which are identical between\nthe two surveys, enabling a comparison of student attitudes before and after the module. The\nPost-Survey contains an additional section for students to evaluate the AI policy module as a\nwhole. Appendix A summarizes each survey question.\nGiven the relatively small class size and pilot nature of the project, small sample sizes were\nexpected. As such, in evaluating the module and drawing conclusions, we use broader trends in\nthe survey data and student testimonials to support our qualitative observations. We present these\nresults in the Evaluation section. Pedagogical considerations and reflections from the point of\nview of the instructor are included in the Discussion section."}, {"title": "AI Policy Module", "content": "The two lecture slots planned for the module help to split AI ethics and AI policy into their own\ndistinct lectures. In the first class period, the focus on AI ethics transitions students from thinking\nabout the details of machine learning algorithms to thinking about the impact of AI on society.\nThe second lecture on AI policy then shifts focus from thinking about AI's impacts to acting\nresponsibly with AI. The structure and core content of each lecture are outlined below."}, {"title": "Lecture 1: \u201cEthics and Artificial Intelligence\u201d", "content": "The first class period in the module contains the most conventional lecture-style presenting of the\ntwo lectures, necessary for establishing a common understanding of how to conceptualize the\nsocial impacts of AI. This includes establishing a common definition of \u201cethics\u201d, how individual\nmorality differs from collectively-held ethics, and how specific ethical frameworks are\ndeclaratively defined through ethical principles. However, throughout this introduction to ethical\ntheory, we attempt to provide more concrete formulations for abstract concepts and to place the\ntheory in the context of AI. Figure 1a, for instance, shows a quasi-mathematical formulation for\nthe term ethics as a function of an action, its context, and a particular moral framework."}, {"title": "Lecture 2: \u201cRegulating Artificial Intelligence\u201d", "content": "The second lecture in the module frames Al policy as the medium through which principles in AI\nethics frameworks are put into practice with code and institutional structures. The goal of this\nlecture is to introduce students to the components of policy, influences on how governments and\ncorporations shape policy, and key nascent government AI regulation.\nFirst, \"policy\" is deconstructed into three components, as shown in Figure 1b: content (what is\nallowed vs. limited?), scope & jurisdiction (who does it apply to?), and enforcement (what\npenalties are there?). The motivations for regulating AI are discussed from the perspectives of"}, {"title": "In-Class Game: Congress vs. Evil Inc.", "content": "The limited class time and desire not to ask students to prepare outside of class (e.g., for a debate)\npresented considerable constraints on what kind of active learning activity could be designed to\nconnect with the AI policy content of the second lecture. The result of wanting to recreate the\nconflicting interests involved in the policymaking process was a game to simulate the U.S.\nCongress' role in AI regulation, which we call Congress vs. Evil Inc. In this dramatized and\nhyperbolized simulation, inspired by the popular party game Mafia (also called Werewolf), players\n(students) assume one of three roles: (1) leaders of Evil Inc., a fictional multinational technology\ncompany; (2) members of Congress charged with oversight and regulation of AI; and (3) voters,\nwho want to induce Congress to pass robust (but not overly restrictive) AI regulation through their\nvoting patterns. The goals and player mechanics for each role are summarized in Table 1."}, {"title": "Evaluation", "content": "Below, we summarize our findings with respect to the impact of the AI policy module on\nstudents' attitudes and competencies related to AI ethics policy. As previously noted, the small\nsample sizes of our pre- and post-module surveys (n = 17 and n = 13, respectively) necessarily\nlimit the robustness of the conclusions that can be drawn from our quantitative data. However,\ngiven the pilot nature of this project, we regard these survey data as nevertheless helpful to\nbroadly assess the efficacy and impact of the module in this early stage of development."}, {"title": "Student Attitudes and Competencies", "content": "Unsurprisingly, student attitudes towards issues of AI ethics and policy remain largely unchanged\nafter the two lectures. Figures 3a and 3b in Appendix C show the changes in the responses\nbetween the two surveys for ethics- and policy-related questions, respectively. Overall, the\nLikert-scale distributions change little, especially when only considering overall agreement versus\ndisagreement. We view this as evidence not of any deficiency in our module, but rather as support\nfor the hypothesis that CS students develop their attitudes toward AI mainly through their own\nexploration and use of AI tools, both inside and outside of the classroom. The process of ethical\ndevelopment among students in the context of AI therefore merits further investigation."}, {"title": "Student Evaluations of the AI Policy Module", "content": "Student feedback on the module overall consistently positive, with 92% of respondents strongly\nagreeing or somewhat agreeing that \u201cthe content of the lectures was interesting,\u201d \u201cthe lectures\nwere engaging and interactive,\u201d and \u201cthe lectures were a good use of class time\u201d. Moreover, a\nmajority of respondents strongly agreed that they liked \u201cthe level of interactivity in these lectures\ncompared to previous lectures,\u201d Optional written feedback highlighted that the lectures were\n\u201cengaging,\" \"interesting,\u201d \u201cfun,\u201d and \u201cinformative\u201d. One student noted that \u201cI would take a whole\nclass on [this topic] if offered\". Among the two students who listed a favorite part of the module,\nthe in-class game was mentioned both times. However, comments suggesting that the incentives\nfor different roles in Congress vs. Evil Inc. should be adjusted indicate that the game's mechanics\nneed to be refined to ensure that all roles are balanced and equally engaging to play.\""}, {"title": "Discussion", "content": "The primary strength of this one-week module is that it is short enough to be added to an existing\ntechnical course without having to remove much existing content, yet still long enough to serve as\na meaningful introduction to the social impacts of AI. The dual focal points of Al ethics and AI\npolicy divide cleanly between the two lectures, while the latter smoothly transitions from (and\nextends on) the former. In the first lecture, the focus is on how to conceptualize\u2014think\nabout-the impacts of AI, while in the second, the focus evolves into how to act to achieve\n\u2018ethical AI' through regulation. The second lecture completes a progression from ethical principle\nto practice [25], which is first achieved through the formulation of a framework of ethical\nprinciples, then through the development of policy and enactment of regulation through code,\ngovernance structures, and institutional culture."}, {"title": "Module Content", "content": "If the purpose of this module is distilled down to one goal, it is to introduce students to the\nlandscape of AI policy in such a way as to enable them to navigate the interplay of AI regulation\non their work if and when it arises. In other words, we aim to impart the context and \u2018vocabulary'\nnecessary to discuss or take part in AI policymaking decisions. As our survey data show, it is not\nreasonable to expect all students to find genuine academic or professional interest in technology\npolicy. For this set of students, the end of the module is the end of their engagement with the\ntopic, having at least become familiar with the basics of AI ethics and policy. But for some subset\nof students, adding this underrepresented area of computing to the curriculum generates interest\nthat may lead them to consider career opportunities in AI governance and policy. We see evidence\nof this phenomenon in our post-survey data, with 38% of the respondents expressing interest in an\nAI policy career path compared to only 18% before the module. As such, we see broad appeal for\nthe continuation of the module in some form."}, {"title": "In-Class Game", "content": "As evidenced by the student feedback, the in-class game Congress vs. Evil Inc. proved to be a\nengaging centerpiece for the entire module. The juxtaposition of Congress against an \u2018evil'\ntechnology company is purposefully hyperbolized, as is the cynical (and simplistic) use of \u2018bribe\ncards' with which Evil Inc. can bribe members of Congress to vote against key AI regulation. The\naim of this exaggeration is to make the game appealing and easy to play while preserving the key\ntension between what regulations voters want and what regulation is (or isn't) actually enacted\nfollowing the policymaking and lobbying process. Each of the three scenarios described in\nAppendix B were inspired by current events and propose regulations that have been or could have\nbeen considered by legislators or regulators. These real-world connections were underscored\nduring a post-game debriefing, encouraging students to take away the concept of competing\npolicy interests, instead of the hyperbolized details of the game."}, {"title": "Pedagogical Considerations", "content": "Reflecting on our experience piloting this module in a graduate class, we conclude with a few\nsuggestions regarding logistical or pedagogical choices. First, it should be noted that our\nuniversity's lecture slots run for 1 hour and 15 minutes. As such, it is likely that the module\nwould need to be split into three lectures for a 1-hour lecture schedule, in order to retain all key\ncontent. However, this may be preferable, as the in-class game could be separated from the\n\"Regulating Artificial Intelligence\u201d lecture and shifted to the third lecture day by itself, along with\na culminating class debriefing or reflection.\nThe other category of pedagogical considerations relates to how the module fits into the larger\ntechnical course from a participation and assessment perspective. Unfortunately, our module pilot\nsaw between 25% and 50% less student attendance than normal, which is likely the result of\nstudents having already taken the course final exam and attendance not being tracked during the\ntwo module lectures. While the scheduling of the module right after the university's week-long\nThanksgiving break is likely another contributing factor, instructors should consider how to\nrequire or incentivize attendance and participation in the class discussions through a roster check\nor a reflection assignment related to the module. Including an assignment as part of the module\nwould also emphasize that this content is an integral part of the course and should not be treated\nas an afterthought."}, {"title": "Conclusion", "content": "This paper details a two-lecture AI policy module for an introductory machine learning class,\nshowing that content on AI ethics and AI policy can be effectively integrated into technical\ncourses. While student ethical attitudes may not be easily changed-perhaps because they are\nsolidified outside of the classroom\u2014, students reported increased confidence in their ability to\ndiscuss issues of AI regulation and implement AI policies in their work after taking part in the\nmodule. Critically, the module increased student interest in following news developments about\nAl regulation and convinced some students to consider AI policy as a potential career path. The\ncombination of lecture- and discussion-driven context with in-class activities and games to\nconnect abstract concepts with real-world applications highlights a new path forward for\ndesigning AI ethics courses with an active learning pedological approach."}]}