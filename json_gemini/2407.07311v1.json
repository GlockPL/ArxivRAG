{"title": "ViTime: A Visual Intelligence-Based Foundation Model for Time Series Forecasting", "authors": ["Luoxiao Yang", "Yun Wang", "Xinqi Fan", "Israel Cohen", "Yue Zhao", "Zijun Zhang"], "abstract": "The success of large pretrained models in natural language processing (NLP) and computer vision (CV) has opened new avenues for constructing foundation models for time series forecasting (TSF). Traditional TSF foundation models rely heavily on numerical data fitting. In contrast, the human brain is inherently skilled at processing visual information, prefer predicting future trends by observing visualized sequences. From a biomimetic perspective, utilizing models to directly process numerical sequences might not be the most effective route to achieving Artificial General Intelligence (AGI). This paper proposes ViTime, a novel Visual Intelligence-based foundation model for TSF. ViTime overcomes the limitations of numerical time series data fitting by utilizing visual data processing paradigms and employs a innovative data synthesis method during training, called Real Time Series (RealTS). Experiments on a diverse set of previously unseen forecasting datasets demonstrate that ViTime achieves state-of-the-art zero-shot performance, even surpassing the best individually trained supervised models in some situations. These findings suggest that visual intelligence can significantly enhance time series analysis and forecasting, paving the way for more advanced and versatile models in the field.", "sections": [{"title": "1 Introduction", "content": "Time series data are prevalent in various aspects of daily life, playing a crucial role in applications across numerous domains, such as weather forecasting [1], stock market analysis [2], traffic prediction [3], and healthcare monitoring [4]. Accurate analysis and prediction of time series data are essential for informed decision-making in these fields. Meanwhile, the emergence of large language models (LLMs) has highlighted the potential for creating general-purpose models capable of performing domain-specific tasks through zero-shot learning [5]. This breakthrough has generated considerable interest within the research community, leading to the development of foundational models for time series forecasting (TSF), which can significantly enhance the robustness and versatility of TSF, including recent TimesFM [6], ForecastPFN [7], and Time-LLM [8] etc.\nDespite these advancements, existing foundational time series models encounter two significant challenges. First, similar to most TSF models, existing foundation models are trained by directly fitting numerical time series data, which suggests that the primary information carrier for these models is the numerical relationship within the temporal dimension. In contrast, humans tend to observe trends through visual representations rather than processing numerical values directly. Research has shown that the human brain processes visual information more efficiently than numerical data. Pettersson [9] discovered that the human brain is more adept at processing visual information than numerical data. Similarly, Dondis [10] demonstrated that the visual cortex rapidly identifies patterns, shapes, and colors, making images and videos quicker to process than texts and numbers. These findings naturally lead to a hypothetical question: On the path toward AGI, might employing visual intelligence for time series modeling be more effective than conventional numerical methods?\nSecond, the training data of foundation models typically consist of large-scale real-world datasets [6], raising a critical question: Can large real-world dataset comprehensively capture the diverse range of universal time series patterns? Specifically, what foundational capabilities should a model possess to address a universal spectrum of time series problems?\nTo tackle these challenges, this paper proposes a novel visual intelligence-based time series foundation model, Visual Time Foundation Model (ViTime), aiming to pioneer a new research paradigm in time series foundation models from the perspective of visual intelligence. Additionally, we introduce an innovative time series data generation method, Real Time Series (RealTS), which categorizes foundational knowledge of time series analysis into \"trend\" and \"periodicity\" and synthesizes training data during ViTime's training. ViTime operates by transforming numerical time series into binary images, converting numerical temporal correlations into binary pixel spatial correlations. This methodology aligns with the brain's proficiency in processing time series data. Experimental results demonstrate that the proposed ViTime, when applied to various unseen datasets across different domains and resolutions, achieves state-of-the-art zero-shot results and, in some cases, surpasses the best individually trained supervised models. Moreover, with only 10% domain data fine-tuning, ViTime can achieve superior performance compared to the latest state-of-the-art supervised models using 100% domain data.\nThe main contributions of this work are highlighted as follows:\n1.  A Novel Visual Intelligence-Based Foundation Model for Time Series Analysis: A new visual intelligence-based framework for TSF foundation model, ViTime, is proposed, leveraging visual patterns instead of numerical data.\n2.  Innovative RealTS Data Generation Method: A novel data synthesis method, RealTS, is introduced, dividing the foundational knowledge of time series into \"trend\" and \"periodicity\" and generating training data to effectively pretrain ViTime.\n3.  Superior Performance: Comprehensive experimental results indicate that ViTime is a superior, more general, and more accurate foundation model, demonstrating its potential as a highly promising time series solution on the path to AGI."}, {"title": "2 Related work", "content": "Traditionally, TSF methods have be domainated by statistical models like the Autoregressive Integrated Moving Average (ARIMA) [11] and its extensions, including Seasonal ARIMA (SARIMA) [12] and Vector ARIMA (VARIMA) [11]. While statistical models are appreciated for their simplicity and interpretability, they fall short in capturing the non-linear relationships and long-term dependencies inherent in real-world time series data.\nIn recent years, deep learning has made significant strides in TSF. Recurrent Neural Networks (RNNs) and their variants, such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), have shown superior capabilities in modeling complex temporal dependencies [13]. More recently, Transformers based models have gained attention for their ability to model long-range dependencies effectively. Models like Informer [14], Autoformer [15], and PatchTST [16] utilize self-attention mechanisms, setting new performance benchmarks in TSF."}, {"title": "2.2 Foundation model for time series forecasting", "content": "The TSF methods mentioned in Section 2.1 adopts a fully supervised learning paradigm, where specific models are trained on particular datasets. Recently, the advent of LLMs like GPT-3 and LLaMA-2, which aim to use a single, generalizable approach to predict across various datasets[5], has sparked interest in developing foundation models for TSF.\nResearch on foundation models for TSF is still in its early stages, and current approaches can be broadly categorized into LLM-based models, real-world dataset based models and synthesized dataset based models. The LLM-based models leverages the inference capabilities of LLMs for zero-shot TSF tasks, including TimeGPT-1 [17] and LLMTIME [8]. However, their accuracy heavily depends on the underlying capabilities of LLM, resulting in large, redundant, and less precise models. The real-world dataset based models aim training a foundation model on extensive real-world data to achieve zero-shot TSF, exemplified by Google's TimesFM [6]. Despite being one of the leading TSF foundation models, its reliance on real-world numerical time series data limits its robustness and coverage of various unseen time series patterns. The synthesized dataset based models employ synthetic data to train a foundation model, as demonstrated by ForecastPFN [7]. However, the uncontrollable nature of generated data patterns in real space often leads to lower accuracy for foundation models trained with synthetic data compared to those trained with real-world data."}, {"title": "2.3 Visual Intelligence assist model for time series forecasting", "content": "Research on visual intelligence assist TSF is relatively scarce. One of the earliest attempts is made by JP Morgan researchers [18], who investigated using images to predict financial data. However, this approach did not develop a systematic theoretical framework and lacked technical details, limiting its further development. In [19], the concept of using visual intelligence for TSF is systematically explored for the first time, establishing a theoretical framework that integrates visual intelligence for TSF. Nevertheless, the model proposed in [19] primarily focused on traditional supervised learning. There is still a gap in research about visual intelligence-based foundation model for TSF.\nIn this papar, we propose the Visual Intelligence-based model, ViTime, serving as a robust foundation model for TSF, which process time series data in the binary image-based time series metric space and training with high-quality synthetic data."}, {"title": "3 Method", "content": "The objective of this paper is to construct a synthesized dataset based foundation model for TSF, which is trained via synthesized time series data and subsequently applied to real-world time series data. Initially, we synthesize time series $S_{1:T+l}$ from a predefined distribution $P(D)$ to be used as training data, as shown in (1):\n$S_{1:T+1} \\sim P(D)$ (1)\nwhere T and I denotes the length of the maximum lookback window and the maximum prediction length.\nFollowing this, the proposed ViTime model learns from the synthesized data during the training phase and is tested on real data during the testing phase. The training problem can be mathematically formulated as follows:\n$\\theta^* = \\underset{\\theta}{\\text{argmin }} L(ViTime(S_{1:T}), S_{T:T+l})$ (2)"}, {"title": "3.2 Overall architecture", "content": "Fig. 1 depictsthe overall architecture of the proposed ViTime framework, which is composed of four key module, the Real time series synthesis module, the mapping function, the proposed ViTime model and the inverse mapping function. Initially, a novel time series data generation algorithm, Real Time Series (RealTS), is introduced in this paper, which categorizes the knowledge required for foundational time series models into two patterns: periodic pattern and trend pattern, and synthesis numerical time series data accordingly. The generated numerical time series data are then mapped into a binary image by mapping function, converting the numerical relationships of the time series data into spatial distribution relationships.\nNext, the ViTime model operating within the binary image metric space is introduced, leveraging the historical distributions of the generated binary images to predict future trends. Consequently, the ViTime model transforms the TSF task into an image generation problem, enhancing predictive accuracy through the application of visual intelligence. Finally, the inverse mapping function is employed to converts the predicted binary image back into numerical time series data for further analysis.\nIn following sections, we will introduce each component of the ViTime framework in detail: RealTS, Mapping Function, ViTime Model, and Inverse Mapping Function."}, {"title": "3.3 Real time series synthesis", "content": "A robust foundation model for TSF should integrate two essential types of time series variation knowledge: periodic pattern and trend pattern, which encompass the inherent patterns and directional changes in time series data. Real-world datasets, however, often lack representation of the full spectrum of these periodic and trend-based fluctuations, limiting the model's ability to generalize across different scenarios and effectively learn underlying dynamics.\nTo address this challenge, we propose a novel time series generation algorithm, RealTS. RealTS systematically generates synthetic time series data that exhibit diverse periodic and trend characteristics, providing a large dataset that captures a wide range of possible behaviors. The proposed RealTS can facilitate more comprehensive training of foundation models, exposing them to various patterns and improving their ability to generalize to unseen real-world data.\nThe RealTS algorithm probabilistically selects between generating periodic or trend-based time series. Given the total length L of synthesized time series, the algorithm selects the data prior hypothesis between periodic $q_p$ and trend-based $q_t$ patterns with a predefined probability ($\\alpha$). The distribution of generated time series P(D) is defined as follows:\n$S_{1:L} \\sim P(D) = P(S_{1:L}|L) = \\alpha \\int [P(S_{L}|L,B_p)P(B_p|\\varphi_p)P(\\varphi_p)d\\varphi_p + (1-\\alpha) \\int P(S_{L}|L,B_t)P(B_t|q_t)P(q_t)dq_t$ (3)\nwhere $P(\\varphi_p)$ represents the prior probability of hypothesis $q_p$; $P(B|\\varphi)$ is the likelihood of observing the data behavior B under hypothesis $\\varphi$. The concept of data behavior B is introduced to further detail the generation behavior within different data modes.\nWe employ two distinct data behavior modes under periodic hypothesis $q_p$:\nInverse Fast Fourier Transform Behavior (IFFTB): To ensure that the synthesized data adequately reflects the variation paradigms of real-world time series, we utilize IFFT as expressed in (4) to simulate the underlying behavior of real-world periodic time series:\n$P(S_{L}|L, B_p)|B_p=IFFT = \\int N(A_m; \\mu_a, \\sigma_a^2)N(\\Phi; \\mu_p, \\sigma_{\\beta}^2)\\delta(S_{L} - IFFT(A_m,\\Phi), L))d\\phi dA_m$ (4)\nwhere two empirical distribution of Fourier transform amplitudes and phases, $N(A_m; \\mu_a, \\sigma_a^2)$ and $N(\\Phi; \\mu_p, \\sigma_{\\beta}^2)$, are maintained and $\\delta$ denots Dirac delta function. By sampling from empirical distributions, we can obtain the amplitude and Phase vector, which is then inversely transformed back to the time domain via IFFT.\nPeriodic Wave Behavior (PWB): This behavior generates data by superimposing multiple periodic waves. The data is modeled as a sum of sine, cosine and other periodic functions, $f_{periodic}$,with different frequencies and amplitudes:\n$P(S_{L}|L, B_p)|B_p=PWB = \\int N(S_L - \\sum_{i=1}^{K_{PWB}}A_i f_{periodic}(w_it), \\sigma_e^2) \\prod P(A)P(w)dwdA$ (5)\nwhere $P(A)$ and $P(w)$ denote predefined prior distributions of amplitudes and frequency; $k_{pwB}$ denotes the number of mixed periodic functions.\nWe also employ three distinct data behavior modes under trend data hypotheses $q_t$:\nRandom Walk Behavior (RWB): The RWB models data as a stochastic process where each value is the previous value plus a random step:\n$P(S_{i}|S_{i-1}, L, B_p)|B_p=RWB = N(0, \\sigma^2)$ (6)\nLogistic Growth Behavior (LGB): The LGB models data with a logistic growth function, capturing the S-shaped growth pattern:\n$P(S_{L}/L, B_p)|B_p=LGB = \\int N(S_{L}; \\frac{K}{1+e^{rL-L_0}}}, \\sigma_e^2 )P(K)P(r)dKdr$ (7)\nwhere $P(K)$ and $P(r)$ denote predefined prior distributions of S-shaped function hyperparameters.\nTrend Wave Data Behavior (TWDB): The TWDB combines linear trends with periodic fluctuations:\n$P(S_{L}|L, B_p)|B_p=TWDB = \\int N(S_{i}; aL + b + \\sum_{i=1}^{K_{TWDB}}A_i f_{periodic}(w_it), \\sigma_e^2) P(a)P(b)P(A)P(w)dadbdAdw$ (8)\nwhere $P(a)$, $P(b)$, $P(A)$ and $P(w)$ denote predefined prior distributions of hyperparameters.\nDuring synthesis process, we employ various data augmentation techniques to enhance the diversity and robustness of synthetic data, including Multiple period replication, which repeats the generated periodic data over multiple cycles to capture long-term periodic patterns; Data flipping; Convolution smoothing and detrending, which remove underlying trends from the data to isolate periodic components, make it easier for the model to learn these patterns; Data perturbation, wihch introduces sudden changes or anomalies into the data, simulating real-world disturbances and improving the model's ability to handle unexpected variations, etc.."}, {"title": "3.4 Binary image based time series metric space", "content": "In ViTime, time series are fed and operated with a binary image form, leveraging a binary image-based time series metric space proposed in [19], as described in Definition 1.\nDefinition 1: Binary image-based time series metric space. The binary image-based time series metric space is defined as a group $(\\mathbb{V}, d)$, where $\\mathbb{V}$ is a set of elements defined in equation (9) and d: $\\mathbb{V} \\times \\mathbb{V} \\rightarrow \\mathbb{R}$ is a distance function based on the Earth Mover's Distance (EMD), defined in (10).\n$\\mathbb{V} = {v \\in R^{c\\times h \\times L}| \\forall_{i,j,k} \\in {0,1}, i \\in [c], j\\in [h], k \\in [L], \\sum_{j=1}^{h} V_{i,j,k} = 1}$ (9)\n$d(v_1, v_2) = \\underset{\\gamma \\in \\Pi(v_1, v_2)}{\\text{inf}} \\int_C \\int_T \\|x_1 - x_2\\|_1 dkdi = \\sum_{i=1}^{C} \\sum_{t=1}^{T} \\underset{\\gamma \\in \\Pi(\\text{dist}_{V_1^t}, \\text{dist}_{V_2^t})}{\\text{inf}} \\sum_{x,y \\sim y} \\gamma_{xy} \\|x-y\\|_1$ (10)\nwhere c represents the number of variates, L is the length of the time series, and h is the resolution of V.\nTo enable the transition from numerical time-series values to the binary image-based metric space, we introduce mapping and inverse mapping functions as follows. Let $S = {s \\in R^{c \\times L}|S_{i,k} \\in R}$ represent the numerical value space of time series. The Time-Series-to-Image mapping function $f:S->V$ and the Image-to-Time-Series inverse mapping function $f^{-1}:V -> S$ can be defined as follows:\n$\\forall_{i,1:h,k} = f(S_{i,k}) = (f_1(S_{i,k}), f_2 (S_{i,k}), ... f_h(S_{i,k})), f_j(S_{i,k}) = $\n$\\begin{cases}\n1, & S_{i,k} \\geq MS, j = h \\\\\n1, & S_{i,k} \\leq MS, j = 1 \\\\\n1, & \\text{j} = \\lfloor \\frac{S_{i,k}+MS}{2MS} h \\rfloor, j \\in [h] \\\\\n0, & \\text{else}\n\\end{cases}$\n (11.1)\n$S_{i,k} = f^{-1}(V_{i,1:h,k}) = (\\sum_{j=1}^h (\\frac{j}{h}-0.5) \\times (\\frac{2MS}{h}) - MS) \\times V_{i,j,k}$ (11.2)\nwhere $MS>0$ denotes the maximum scale of V. Before mapping, zero-score normalization is typically applied to the numerical time series $S_{i,k}$ to standardize the scale.\nGiven that the numerical time series data synthesized by RealTS are one-channel time series, i.e. $S_L \\in R^1 \\in R^{1 \\times L}$, thus the corresponding $v_L \\in R^{1 \\times h \\times L}$ is obtained via (12):\n$v_L = f(S_L)$ (12)"}, {"title": "3.5 The proposed ViTime model", "content": "Fig. 3 depicts the framework of the proposed ViTime model, which comprises three networks: the Visual Time Tokenizer, the Decoder, and the Refining Module. Initially, temporal masking is applied to the mapped binary image, ensuring the temporal information is not disclosed. The masked binary image is then fed into the Visual Time Tokenizer and outputs embedded tokens. These tokens are subsequently decoded by the Decoder, resulting in the initial prediction. Finally, to enhance the generative quality of patch junctions, a Refining Module is employed to output the final binary image prediction.\nVisual Time Tokenizer. The primary role of the Visual Time Tokenizer is to segment masked binary image into multiple patches, integrate positional encoding, and map these patches into the feature space. By leveraging the Vision Transformer (ViT) [20] architecture, the module captures spatial relationships between patches, thereby transforming temporal dependencies of the time series into spatial dependencies within the pixel value space.\nDecoder. The Decoder translates the tokenized patches back into the original binary pixel metric space, providing an initial prediction where the ViT architecture is adopted as well.\nRefining Module. The transformer architecture in the Decoder can result in discontinuities at the patch junctions, which may affect the accuracy of inverse mapping process. To address this issue, the Refining Module building with CNNs is employed. Initially, tokens decoded by Decoder are unpatched and fed into a CNN-based backbone. Next, an Atrous Spatial Pyramid Pooling (ASPP) [21] module is employed to expand model receptive field. Finally, the output is upsampled to the original binary pixel metric space, generating the final binary image prediction result.\nThe modeling process of ViTime can be expressed in (13):\n$v_L' = ViTime(v_L \\bigodot M_L)$ (13)\nwhere $M_L$ denotes temporal masks."}, {"title": "3.6 Training details", "content": "We incorporate following technique details during training.\nData normalization. To ensure ViTime can effectively capture patterns involving sudden changes, an in-sequence data normalization based on L2 normalization is implemented. By normalizing each sequence within the data sequence, the model can pay more attention to abrupt variations. The normalization process is defined as follows:\n$S_L = \\frac{S_L - mean(||S_{1:T}||_2)}{std(S_{1:T})}$ (14)\nTemporal resolution enhancement. To improve the temporal resolution in the binary image metric space, the input sequence is first linearly interpolated to twice its original length (2L), so that, the granularity of the temporal dimension is increase. Moreover, due to the inherent sparsity of the binary image space, most patches exhibit low-level information density. To improve the information density, Gaussian blurring is applied to the input binary images before fed into ViTime, thereby reducing sparsity and enhancing information density.\nLoss function. The loss function employed in this study is defined as follows:\n$L = d(v_L', v_L) + \\alpha KLD(v_L', v_L)$ (15)\nwhere d denotes distance function defined in (10), KLD denotes Kullback-Leibler divergence and $\\alpha$ is the hyperparameter balance quantity between d and KLD."}, {"title": "4 Computational experiments", "content": ""}, {"title": "4.1 Experimental Configuration", "content": ""}, {"title": "4.1.1 Datasets", "content": "Seven popular publicly accessible datasets, Electricity, Traffic, Weather, ETTh1, ETTh2, ETTm1 and ETTm2 [15] are empolyed in computational section to validate the effectiveness of the proposed method."}, {"title": "4.1.2 Model setup", "content": "The ViTime model is developed using data sequences synthesized by RealTS. During each training epoch, 20,000 sequences are randomly generated. Detailed RealTS parameters setting are provided in Appendix I. After training, zero-shot testing and fine-tuning are implemented accordingly. For multivariate time series, a channel-independent strategy [16] is applied, predicting each variable separately before combining them to form the final multivariate forecast.\nThe default parameters for ViTime model are set as follows: h = 128, MS = 3.5, maximum lookback window T= 512, and maximum prediction length l = 720. Gaussian blur with a kernel size of (31,31) is applied to the binary image to facilitate faster convergence. The detailed structural design of the Visual Time Tokenizer, the Decoder, and the Refining Module is outlined in Appendix II."}, {"title": "4.1.3 Evaluation Metrics", "content": "Foundational models for TSF trained on real-world data can suffer from test set leakage issue. To address this issue and ensure a fair experimental comparison, two metrics are proposed for zero-shot evaluation: Rescale-MAE (ReMAE) and Rescale-MSE (ReMSE). The primary concept of ReMAE/ ReMSE involves rescaling the test dataset with different time resolutions, For example, the original test time series of length T is rescaled to $\\beta$T using the time series interpolation (TSI) method, as shown in (16).\n$S_{\\beta T} = TSI(S_{T}, rescaling \\ factor = \\beta)$ (16)\nThe formulas for ReMAE and ReMSE are provided below:\n$MSE(S_{\\beta T}, \\hat{S_{\\beta T}}) = \\frac{\\| S_{\\beta T} - \\hat{S_{\\beta T}} \\|}{\\beta T}$ (17.1)\n$MAE(\\hat{S_{T}}, S_{\\beta T}) = \\frac{\\| \\hat{S_{\\beta T}} - S_{\\beta T} \\|}{\\beta T}$ (17.2)\n$REMSE = \\frac{\\sum_{\\beta \\in B} MSE(S_{\\beta T}, \\hat{S_{\\beta T}})}{len(B)}, B = [0.5, 0.66, 1, 1.5, 2]$ (17.3)\n$ReMAE = \\frac{\\sum_{\\beta \\in B} MAE(S_{\\beta T}, \\hat{S_{\\beta T}})}{len(B)}, B = [0.5, 0.66, 1, 1.5, 2]$ (17.4)"}, {"title": "4.2 Zero-shot Evaluation", "content": "To evaluate the zero-shot performance of our model, we consider two leading state-of-the-art TSF models: TimesFM [6], introduced by Google Research in April 2024 and acknowledged as the most powerful open-source foundational time series model, and PatchTST[16], a well-established model in supervised and transfer learning for TSF over the past two years. For fair comparison, all models employ lookback length of 512 to forecast future sequences of lengths 96, 192, 336, 720. For the ViTime-1072 model, we used longer lookback length, 1072, to test its peak accuracy. PatchTST-ZS utlizes the same training data as ViTime, generated by RealTS, whereas TimesFM is pre-trained on extensive real-world datasets before zero-shot testing. Additionally, we incorporated a fully supervised PatchTST model, which trained distinct models on various datasets to serve as the benchmark for maximum achievable accuracy through supervision.\nTable I reports the zero-shot experimental results. The proposed ViTime-1072 model achieves the highest accuracy in almost all experiments. When using the same input sequence length as the other baseline models, ViTime consistently ranks best across almost all datasets and prediction lengths, significantly outperforming TimesFM. Notably, ViTime's accuracy in some cases even surpasses that of the fully supervised PatchTST model, showcasing the superiority of the proposed ViTime. The comparative results between ViTime and PatchTST-ZS, both trained on RealTS-generated data, suggest that modeling TSF tasks from a visual intelligence perspective can significantly enhance model robustness and accuracy, which further validates the rationale behind the ViTime framework's design from a visual intelligence perspective."}, {"title": "4.3 Fine-tune Evaluation", "content": "To further evaluate the performance of ViTime, we conduct a series of fine-tuning experiments in this section. Foundational models such as TimesFM [6], GPT4TS [22], and TIME-LLM [8] are fine-tuned using 10% of the training data. We also considered recent SOTA supervised TSF models, including SIMBA [23], TIMESNET [24], and PatchTST [16], using 100% of the training data as reported in their papers. The proposed ViTime is fine-tuned with both 10% and 100% of the training data, with results presented in Table II.\nIt is observable that the proposed ViTime, when fine-tuned with 10% of the training data, outperforms the latest supervised models that were trained on 100% of the data. Furthermore, when fine-tuned with 100% of the data, ViTime's prediction accuracy greatly surpasses all existing models, further demonstrating its effectiveness. Detailed results of the fine-tuning experiments are provided in the Appendix IV."}, {"title": "4.4 Ablation", "content": ""}, {"title": "5 Conclusion", "content": "In this paper, we introduced ViTime, a Visual Intelligence-based foundation model for TSF, along with a novel data generation method, RealTS. Our approach was designed to address the inherent limitations of traditional numerical data fitting models by leveraging visual processing capabilities, which align more closely with the human brain's strengths in handling visual information. The proposed ViTime framework transformed numerical time series data into a binary image, enabling the application of visual intelligence techniques to analyze and predict time series trends. Moreover, the proposed RealTS algorithm could systematically generate diverse synthetic time series data, encapsulating essential periodic and trend characteristics and providing enough knowledge for training ViTime model.\nExtensive experimental evaluations demonstrated that ViTime could achieve state-of-the-art zero-shot performance. Meanwhile, the fine-tuning experiments indicated the proposed ViTime could outperform recent state-of-the-art fully supervision models even when trained with 10% data, confirming the benefits of visual intelligence for time series analysis. We believe that the proposed ViTime can provide significant insights for AGI processing time series from a methodological perspective."}, {"title": "Appendix", "content": "This section provides comprehensive details of the predefined prior and empirical distributions utilized in RealTS. We also include parameter settings employed in our experimental section.\nIFFTB Configuration: For the IFFTB, the selection of $A_m$ and is based on empirical distributions illustrated in Figure A1. During our experiments, we randomly selected one of the two empirical distributions for generating $A_m$ and Detailed parameters of these distributions can be found on our GitHub repository.\nPWB Configuration: The PWB primarily involves prior distributions for amplitude and frequency, represented as P(A) and P(w). In our experimental setup, their probability densities are defined as follows:\n$A \\sim U(0.5,5)$ (A1.1)\n$\\text{In}(w) \\sim U(\\text{In}(11), \\text{In}(2L))$ (A1.2)\nAdditionally, the parameter kpWB is modeled as a random variable with the following probability density:\n$P(K_{PWB} = k) = \\frac{1}{8}, for \\ k = 1,2, ...,8$ (A1.3)\nLGB Configuration: The LGB includes two logistic growth function parameters: Carrying Capacity P(K) and Growth Rate P(r). For our experiments, we defined their probability densities as follows:\n$\\text{In}(k) \\sim U(\\text{In}(1), \\text{In}(10))$ (A2.1)\n$\\text{In}(r) \\sim U(\\text{In}(0.001), \\text{In}(0.1))$ (A2.2)\nTWDB Configuration: In the TWDB, we define the probability densities for linear function random variables P(a) and P(b), as well as for the superimposed periodic wave components P(A) and P(w). The settings for P(A) P(w), and KTWDB are consistent with those used in the PWB module. The probability densities for P(a) and P(b) are detailed below:\na~U(-1,1) (A3.1)\nb~U(-10,10) (A3.2)"}]}