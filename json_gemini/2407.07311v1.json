{"title": "ViTime: A Visual Intelligence-Based Foundation Model for Time Series Forecasting", "authors": ["Luoxiao Yang", "Yun Wang", "Xinqi Fan", "Israel Cohen", "Yue Zhao", "Zijun Zhang"], "abstract": "The success of large pretrained models in natural language processing (NLP) and computer vision (CV) has opened new avenues for constructing foundation models for time series forecasting (TSF). Traditional TSF foundation models rely heavily on numerical data fitting. In contrast, the human brain is inherently skilled at processing visual information, prefer predicting future trends by observing visualized sequences. From a biomimetic perspective, utilizing models to directly process numerical sequences might not be the most effective route to achieving Artificial General Intelligence (AGI). This paper proposes ViTime, a novel Visual Intelligence-based foundation model for TSF. ViTime overcomes the limitations of numerical time series data fitting by utilizing visual data processing paradigms and employs a innovative data synthesis method during training, called Real Time Series (RealTS). Experiments on a diverse set of previously unseen forecasting datasets demonstrate that ViTime achieves state-of-the-art zero-shot performance, even surpassing the best individually trained supervised models in some situations. These findings suggest that visual intelligence can significantly enhance time series analysis and forecasting, paving the way for more advanced and versatile models in the field. The code for our framework is accessible at https://github.com/IkeYang/ViTime.", "sections": [{"title": "1 Introduction", "content": "Time series data are prevalent in various aspects of daily life, playing a crucial role in applications across numerous domains, such as weather forecasting [1], stock market analysis [2], traffic prediction [3], and healthcare monitoring [4]. Accurate analysis and prediction of time series data are essential for informed decision-making in these fields. Meanwhile, the emergence of large language models (LLMs) has highlighted the potential for creating general-purpose models capable of performing domain-specific tasks through zero-shot learning [5]. This breakthrough has generated considerable interest within the research community, leading to the development of foundational models for time series forecasting (TSF), which can significantly enhance the robustness and versatility of TSF, including recent TimesFM [6], ForecastPFN [7], and Time-LLM [8] etc.\nDespite these advancements, existing foundational time series models encounter two significant challenges. First, similar to most TSF models, existing foundation models are trained by directly fitting numerical time series data, which suggests that the primary information carrier for these models is the numerical relationship within the temporal dimension. In contrast, humans tend to observe trends through visual representations rather than processing numerical values directly. Research has shown that the human brain processes visual information more efficiently than numerical data. Pettersson [9] discovered that the human brain is more adept at processing visual information than numerical data. Similarly, Dondis [10] demonstrated that the visual cortex rapidly identifies patterns, shapes, and colors, making images and videos quicker to process than texts and numbers. These findings naturally lead to a hypothetical question: On the path toward AGI, might employing visual intelligence for time series modeling be more effective than conventional numerical methods?\nSecond, the training data of foundation models typically consist of large-scale real-world datasets [6], raising a critical question: Can large real-world dataset comprehensively capture the diverse range of universal time series patterns? Specifically, what foundational capabilities should a model possess to address a universal spectrum of time series problems?\nTo tackle these challenges, this paper proposes a novel visual intelligence-based time series foundation model, Visual Time Foundation Model (ViTime), aiming to pioneer a new research paradigm in time series foundation models from the perspective of visual intelligence. Additionally, we introduce an innovative time series data generation method, Real Time Series (RealTS), which categorizes foundational knowledge of time series analysis into \"trend\" and \"periodicity\" and synthesizes training data during ViTime's training. ViTime operates by transforming numerical time series into binary images, converting numerical temporal correlations into binary pixel spatial correlations. This methodology aligns with the brain's proficiency in processing time series data. Experimental results demonstrate that the proposed ViTime, when applied to various unseen datasets across different domains and resolutions, achieves state-of-the-art zero-shot results and, in some cases, surpasses the best individually trained supervised models. Moreover, with only 10% domain data fine-tuning, ViTime can achieve superior performance compared to the latest state-of-the-art supervised models using 100% domain data.\nThe main contributions of this work are highlighted as follows:\n1. A Novel Visual Intelligence-Based Foundation Model for Time Series Analysis: A new visual intelligence-based framework for TSF foundation model, ViTime, is proposed, leveraging visual patterns instead of numerical data.\n2. Innovative RealTS Data Generation Method: A novel data synthesis method, RealTS, is introduced, dividing the foundational knowledge of time series into \"trend\" and \"periodicity\" and generating training data to effectively pretrain ViTime.\n3. Superior Performance: Comprehensive experimental results indicate that ViTime is a superior, more general, and more accurate foundation model, demonstrating its potential as a highly promising time series solution on the path to AGI."}, {"title": "2 Related work", "content": null}, {"title": "2.1 Time series forecasting", "content": "Traditionally, TSF methods have be domainated by statistical models like the Autoregressive Integrated Moving Average (ARIMA) [11] and its extensions, including Seasonal ARIMA (SARIMA) [12] and Vector ARIMA (VARIMA) [11]. While statistical models are appreciated for their simplicity and interpretability, they fall short in capturing the non-linear relationships and long-term dependencies inherent in real-world time series data.\nIn recent years, deep learning has made significant strides in TSF. Recurrent Neural Networks (RNNs) and their variants, such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), have shown superior capabilities in modeling complex temporal dependencies [13]. More recently, Transformers based models have gained attention for their ability to model long-range dependencies effectively. Models like Informer [14], Autoformer [15], and PatchTST [16] utilize self-attention mechanisms, setting new performance benchmarks in TSF."}, {"title": "2.2 Foundation model for time series forecasting", "content": "The TSF methods mentioned in Section 2.1 adopts a fully supervised learning paradigm, where specific models are trained on particular datasets. Recently, the advent of LLMs like GPT-3 and LLaMA-2, which aim to use a single, generalizable approach to predict across various datasets[5], has sparked interest in developing foundation models for TSF.\nResearch on foundation models for TSF is still in its early stages, and current approaches can be broadly categorized into LLM-based models, real-world dataset based models and synthesized dataset based models. The LLM-based models leverages the inference capabilities of LLMs for zero-shot TSF tasks, including TimeGPT-1 [17] and LLMTIME [8]. However, their accuracy heavily depends on the underlying capabilities of LLM, resulting in large, redundant, and less precise models. The real-world dataset based models aim training a foundation model on extensive real-world data to achieve zero-shot TSF, exemplified by Google's TimesFM [6]. Despite being one of the leading TSF foundation models, its reliance on real-world numerical time series data limits its robustness and coverage of various unseen time series patterns. The synthesized dataset based models employ synthetic data to train a foundation model, as demonstrated by ForecastPFN [7]. However, the uncontrollable nature of generated data patterns in real space often leads to lower accuracy for foundation models trained with synthetic data compared to those trained with real-world data."}, {"title": "2.3 Visual Intelligence assist model for time series forecasting", "content": "Research on visual intelligence assist TSF is relatively scarce. One of the earliest attempts is made by JP Morgan researchers [18], who investigated using images to predict financial data. However, this approach did not develop a systematic theoretical framework and lacked technical details, limiting its further development. In [19], the concept of using visual intelligence for TSF is systematically explored for the first time, establishing a theoretical framework that integrates visual intelligence for TSF. Nevertheless, the model proposed in [19] primarily focused on traditional supervised learning. There is still a gap in research about visual intelligence-based foundation model for TSF.\nIn this papar, we propose the Visual Intelligence-based model, ViTime, serving as a robust foundation model for TSF, which process time series data in the binary image-based time series metric space and training with high-quality synthetic data."}, {"title": "3 Method", "content": null}, {"title": "3.1 Problem definition", "content": "The objective of this paper is to construct a synthesized dataset based foundation model for TSF, which is trained via synthesized time series data and subsequently applied to real-world time series data. Initially, we synthesize time series $S_{1:T+1}$ from a predefined distribution $P(D)$ to be used as training data, as shown in (1):\n$S_{1:T+1} \\sim P(D)$ (1)\nwhere $T$ and $I$ denotes the length of the maximum lookback window and the maximum prediction length.\nFollowing this, the proposed ViTime model learns from the synthesized data during the training phase and is tested on real data during the testing phase. The training problem can be mathematically formulated as follows:\n$\\theta^* = \\underset{\\theta}{\\text{argmin}} L(ViTime(S_{1:T}), S_{T:T+I})$ (2)"}, {"title": "3.3 Real time series synthesis", "content": "A robust foundation model for TSF should integrate two essential types of time series variation knowledge: periodic pattern and trend pattern, which encompass the inherent patterns and directional changes in time series data. Real-world datasets, however, often lack representation of the full spectrum of these periodic and trend-based fluctuations, limiting the model's ability to generalize across different scenarios and effectively learn underlying dynamics.\nTo address this challenge, we propose a novel time series generation algorithm, RealTS. RealTS systematically generates synthetic time series data that exhibit diverse periodic and trend characteristics, providing a large dataset that captures a wide range of possible behaviors. The proposed RealTS can facilitate more comprehensive training of foundation models, exposing them to various patterns and improving their ability to generalize to unseen real-world data.\nThe RealTS algorithm probabilistically selects between generating periodic or trend-based time series. Given the total length L of synthesized time series, the algorithm selects the data prior hypothesis between periodic $q_p$ and trend-based $q_t$ patterns with a predefined probability ($\\alpha$). The distribution of generated time series $P(D)$ is defined as follows:\n$S_{1} \\sim P(D) = P(S_{1:L}) = \\alpha \\int[P(S_{L}|L,B_{p})P(B_{p}|q_{p})P(q_{p})d\\varphi_{p}+(1-\\alpha) \\int P(S_{L}|L,B_{t})P(B_{t}|q_{t})P(q_{t})d\\varphi_{t}]$ (3)\nwhere $P(\\varphi)$ represents the prior probability of hypothesis $\\varphi$; $P(B|\\varphi)$ is the likelihood of observing the data behavior $B$ under hypothesis $\\varphi$. The concept of data behavior $B$ is introduced to further detail the generation behavior within different data modes.\nWe employ two distinct data behavior modes under periodic hypothesis $\\varphi_p$:\nInverse Fast Fourier Transform Behavior (IFFTB): To ensure that the synthesized data adequately reflects the variation paradigms of real-world time series, we utilize IFFT as expressed in (4) to simulate the underlying behavior of real-world periodic time series:\n$P(S_{L}|L,B_{p})|_{B_{p} = IFFT} = \\int N(A_{\\omega}, \\mu_{\\omega}, \\sigma^{2}_{ \\omega})N(\\Phi;\\mu_{\\rho}, \\sigma_{\\rho}^{2}) \\delta(S_{L} - IFFT(A_{\\omega}, \\Phi), L))d\\phi d A_{\\omega}$ (4)\nwhere two empirical distribution of Fourier transform amplitudes and phases, $N(A_{\\omega}, \\mu_{\\omega}, \\sigma^{2}_{ \\omega})$ and $N(\\Phi;\\mu_{\\rho}, \\sigma_{\\rho}^{2})$, are maintained and $\\delta$ denots Dirac delta function. By sampling from empirical distributions, we can obtain the amplitude and Phase vector, which is then inversely transformed back to the time domain via IFFT.\nPeriodic Wave Behavior (PWB): This behavior generates data by superimposing multiple periodic waves. The data is modeled as a sum of sine, cosine and other periodic functions, $f_{periodic}$,with different frequencies and amplitudes:\n$P(S_{L}|L,B_{p})|_{B_{p} = PWB} = \\int N(S_{L}; \\sum_{i=1}^{k_{PWB}} A_{i} f_{periodic} (\\omega_{i}t), \\sigma_{r}^{2} ) \\prod_{i=1}^{k_{PWB}} P(A)P(\\omega) dw d A$ (5)\nwhere $P(A)$ and $P(\\omega)$ denote predefined prior distributions of amplitudes and frequency; $k_{PWB}$"}, {"title": "3.4 Binary image based time series metric space", "content": "In ViTime, time series are fed and operated with a binary image form, leveraging a binary image-based time series metric space proposed in [19], as described in Definition 1.\nDefinition 1: Binary image-based time series metric space. The binary image-based time series metric space is defined as a group $(V, d)$, where $V$ is a set of elements defined in equation (9) and $d: V \\times V\\rightarrow R$ is a distance function based on the Earth Mover's Distance (EMD), defined in (10).\n$V = \\{v \\in R^{c \\times h \\times L} | \\forall i,j,k \\in \\{0,1\\}, i \\in [c], j\\in [h], k\\in [L], \\sum_{j=1}^{h} V_{i,j,k} = 1\\}$ (9)\n$d(v_1, v_2) = \\underset{\\gamma \\in \\Pi} {inf} \\int_{0}^{c} \\int_{0}^{L} \\gamma(x, y) ||x - y||_{1} dxdy$ (10)"}, {"title": "3.5 The proposed ViTime model", "content": "where $c$ represents the number of variates, $L$ is the length of the time series, and $h$ is the resolution of $V$.\nTo enable the transition from numerical time-series values to the binary image-based metric space, we introduce mapping and inverse mapping functions as follows. Let $S = \\{s \\in R^{c \\times L} | S_{i,k} \\in R\\}$ represent the numerical value space of time series. The Time-Series-to-Image mapping function $f:S-> V$ and the Image-to-Time-Series inverse mapping function $f^{-1}:V -> S$ can be defined as follows:\n$V_{i,1:h,k} = f(S_{i,k}) = (f_{1}(S_{i,k}), f_{2} (S_{i,k}), ... f_{h}(S_{i,k})), f_{j}(S_{i,k})$\n$\\begin{aligned}&S_{i,k} \\geq MS, j = h\\\\&S_{i,k} \\leq -MS, j = 1 \\\\&1, j=\\lfloor\\frac{S_{i,k}+MS}{2 M S} h\\rfloor \\in[h]\\\\&\\begin{cases}1,\\\\0,\\end{cases} else&\\end{aligned}$ (11.1)\n$S_{i,k} = f^{-1}(V_{i,1:h,k}) = (\\sum_{j=1}^{h} ((\\frac{j}{h} - 0.5) \\times 2MS) \\times V_{i,j,k})$ (11.2)\nwhere MS>0 denotes the maximum scale of V. Before mapping, zero-score normalization is typically applied to the numerical time series $S_{i,k}$ to standardize the scale.\nGiven that the numerical time series data synthesized by RealTS are one-channel time series, i.e. $S_{1} \\in R^{1} \\in R^{1\\times L}$, thus the corresponding $v_{1} \\in R^{1 \\times h \\times L}$ is obtained via (12):\n$v_L = f(S_{L})$ (12)"}, {"title": "3.6 Training details", "content": "We incorporate following technique details during training.\nData normalization. To ensure ViTime can effectively capture patterns involving sudden changes, an in-sequence data normalization based on L2 normalization is implemented. By normalizing each sequence within the data sequence, the model can pay more attention to abrupt variations. The normalization process is defined as follows:\n$S_{L} = \\frac{S_{L} - mean(||S_{1:T}||_2)}{std(S_{1:T})}$ (14)\nTemporal resolution enhancement. To improve the temporal resolution in the binary image metric space, the input sequence is first linearly interpolated to twice its original length (2L), so that, the granularity of the temporal dimension is increase. Moreover, due to the inherent sparsity of the binary image space, most patches exhibit low-level information density. To improve the information density, Gaussian blurring is applied to the input binary images before fed into ViTime, thereby reducing sparsity and enhancing information density.\nLoss function. The loss function employed in this study is defined as follows:\n$L = d(\\nu_{L}, \\nu_{L}\\' ) + aKLD(\\nu_{L}\\, \\nu_{L})$ (15)\nwhere $d$ denotes distance function defined in (10), KLD denotes Kullback-Leibler divergence and $a$ is the hyperparameter balance quantity between $d$ and KLD."}, {"title": "4 Computational experiments", "content": null}, {"title": "4.1 Experimental Configuration", "content": null}, {"title": "4.1.1 Datasets", "content": "Seven popular publicly accessible datasets, Electricity, Traffic, Weather, ETTh1, ETTh2, ETTm1 and ETTm2 [15] are empolyed in computational section to validate the effectiveness of the proposed method."}, {"title": "4.1.2 Model setup", "content": "The ViTime model is developed using data sequences synthesized by RealTS. During each training epoch, 20,000 sequences are randomly generated. Detailed RealTS parameters setting are provided in Appendix I. After training, zero-shot testing and fine-tuning are implemented accordingly. For multivariate time series, a channel-independent strategy [16] is applied, predicting each variable separately before combining them to form the final multivariate forecast.\nThe default parameters for ViTime model are set as follows: h = 128, MS = 3.5, maximum lookback window T= 512, and maximum prediction length 1 = 720. Gaussian blur with a kernel size of (31,31) is applied to the binary image to facilitate faster convergence. The detailed structural design of the Visual Time Tokenizer, the Decoder, and the Refining Module is outlined in Appendix II."}, {"title": "4.1.3 Evaluation Metrics", "content": "Foundational models for TSF trained on real-world data can suffer from test set leakage issue. To address this issue and ensure a fair experimental comparison, two metrics are proposed for zero-shot evaluation: Rescale-MAE (ReMAE) and Rescale-MSE (ReMSE). The primary concept of ReMAE/ ReMSE involves rescaling the test dataset with different time resolutions, For example, the original test time series of length T is rescaled to $ \\beta$T using the time series interpolation (TSI) method, as shown in (16).\n$S_{\\beta T} = TSI(S_{T}, rescaling factor = \\beta)$ (16)\nThe formulas for ReMAE and ReMSE are provided below:\n$MSE(S_{\\beta T}, \\hat{S_{\\beta T}}) = \\frac{|| S_{\\beta T} - \\hat{S_{\\beta T}} ||}{{\\beta}T}$ (17.1)\n$MAE(S_{\\beta T}, \\hat{S_{\\beta T}}) = \\frac{|| S_{\\beta T} - \\hat{S_{\\beta T}} ||}{{\\beta}T}$ (17.2)\n$REMSE = \\frac{{\\sum_{\\beta \\in B} MSE(S_{\\beta T}, \\hat{S_{\\beta T}})}}{len(B)}, B=[0.5, 0.66, 1, 1.5, 2]$ (17.3)\n$ReMAE = \\frac{{\\sum_{\\beta \\in B} MAE(S_{\\beta T}, \\hat{S_{\\beta T}})}}{len(B)}, B=[0.5, 0.66, 1, 1.5, 2]$ (17.4)"}, {"title": "4.2 Zero-shot Evaluation", "content": "To evaluate the zero-shot performance of our model, we consider two leading state-of-the-art TSF models: TimesFM [6], introduced by Google Research in April 2024 and acknowledged as the most powerful open-source foundational time series model, and PatchTST[16], a well-established model in supervised and transfer learning for TSF over the past two years. For fair comparison, all models employ lookback length of 512 to forecast future sequences of lengths 96, 192, 336, 720. For the ViTime-1072 model, we used longer lookback length, 1072, to test its peak accuracy. PatchTST-ZS utlizes the same training data as ViTime, generated by RealTS, whereas TimesFM is pre-trained on extensive real-world datasets before zero-shot testing. Additionally, we incorporated a fully supervised PatchTST model, which trained distinct models on various datasets to serve as the benchmark for maximum achievable accuracy through supervision."}, {"title": "4.3 Fine-tune Evaluation", "content": "To further evaluate the performance of ViTime, we conduct a series of fine-tuning experiments in this section. Foundational models such as TimesFM [6], GPT4TS [22], and TIME-LLM [8] are fine-tuned using 10% of the training data. We also considered recent SOTA supervised TSF models, including SIMBA [23], TIMESNET [24], and PatchTST [16], using 100% of the training data as reported in their papers. The proposed ViTime is fine-tuned with both 10% and 100% of the training data, with results presented in Table II.\nIt is observable that the proposed ViTime, when fine-tuned with 10% of the training data, outperforms the latest supervised models that were trained on 100% of the data. Furthermore, when fine-tuned with 100% of the data, ViTime's prediction accuracy greatly surpasses all existing models, further demonstrating its effectiveness. Detailed results of the fine-tuning experiments are provided in the Appendix IV."}, {"title": "4.4 Ablation", "content": "In this section, we perform several ablation studies to gain deeper insights of model configuration. The results of ablation experiments are reported in Fig. 5. Fig. 5a depicts the influence of varying spatial resolutions (h) on model accuracy. Although increasing h slightly improves the prediction results, the associated computational cost increases exponentially. Thus, setting h to 128 is a more economical and efficient choice. Fig. 5b illustrates the effect of different lookback window lengths (T) on prediction accuracy. It is evident that a longer lookback window length significantly enhances the model's prediction accuracy. Fig. 5c reports the prediction accuracy across different model sizes. The data shows that models with more parameters tend to perform better. Notably, the proposed ViTime achieves exceptional performance with only 93M parameters, demonstrating its efficiency and effectiveness."}, {"title": "5 Conclusion", "content": "In this paper, we introduced ViTime, a Visual Intelligence-based foundation model for TSF, along with a novel data generation method, RealTS. Our approach was designed to address the inherent limitations of traditional numerical data fitting models by leveraging visual processing capabilities, which align more closely with the human brain's strengths in handling visual information. The proposed ViTime framework transformed numerical time series data into a binary image, enabling the application of visual intelligence techniques to analyze and predict time series trends. Moreover, the proposed RealTS algorithm could systematically generate diverse synthetic time series data, encapsulating essential periodic and trend characteristics and providing enough knowledge for training ViTime model.\nExtensive experimental evaluations demonstrated that ViTime could achieve state-of-the-art zero-shot performance. Meanwhile, the fine-tuning experiments indicated the proposed ViTime could outperform recent state-of-the-art fully supervision models even when trained with 10% data, confirming the benefits of visual intelligence for time series analysis. We believe that the proposed ViTime can provide significant insights for AGI processing time series from a methodological perspective."}, {"title": "Appendix", "content": null}, {"title": "I Details of predefined prior/ empirical distributions in RealTS", "content": "This section provides comprehensive details of the predefined prior and empirical distributions utilized in RealTS. We also include parameter settings employed in our experimental section.\nIFFTB Configuration: For the IFFTB, the selection of $A_m$ and $\\Phi$ is based on empirical distributions illustrated in Figure A1. During our experiments, we randomly selected one of the two empirical distributions for generating $A_m$ and $\\Phi$ Detailed parameters of these distributions can be found on our GitHub repository.\nPWB Configuration: The PWB primarily involves prior distributions for amplitude and frequency, represented as $P(A)$ and $P(\\omega)$. In our experimental setup, their probability densities are defined as follows:\n$A\\sim U(0.5,5)$ (A1.1)\n$In(\\omega)\\sim U(ln(11), In(2L))$ (A1.2)\nAdditionally, the parameter $k_{PWB}$ is modeled as a random variable with the following probability density:\n$P(k_{PWB} = k) = \\frac{1}{8}$, for $k = 1,2, ...,8$ (A1.3)\nLGB Configuration: The LGB includes two logistic growth function parameters: Carrying Capacity $P(K)$ and Growth Rate $P(r)$. For our experiments, we defined their probability densities as follows:\n$ln(k) \\sim U(ln(1), ln(10))$ (A2.1)\n$ln(r) \\sim U(ln(0.001), ln(0.1))$ (A2.2)\nTWDB Configuration: In the TWDB, we define the probability densities for linear function random variables $P(a)$ and $P(b)$, as well as for the superimposed periodic wave components $P(A)$ and $P(\\omega)$. The settings for $P(A)$ $P(\\omega)$, and $k_{TWDB}$ are consistent with those used in the PWB module. The probability densities for $P(a)$ and $P(b)$ are detailed below:\n$a\\sim U(-1,1)$ (A3.1)\n$b\\sim U(-10,10)$ (A3.2)"}, {"title": "II Details of ViTime model structure", "content": "The detailed network configuration of the proposed ViTime are reported in Table A.1."}, {"title": "III Illustrative examples", "content": "This section presents illustrative examples of the proposed ViTime model and baseline models in zero-shot tasks. As depicted in Fig. A.2, ViTime consistently demonstrates superior zero-shot prediction performance compared to TimesFM across a range of rescale factors."}, {"title": "IV Full results of finetuning study", "content": "The detailed results of our fine-tuning study are provided in Tables A.2 and A.3."}]}