{"title": "Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval", "authors": ["Yeong-Joon Ju", "Ho-Joong Kim", "Seong-Whan Lee"], "abstract": "Existing multimodal retrieval systems often rely on disjointed models for image comprehension, such as object detectors and caption generators, leading to cumbersome implementations and training processes. To overcome this limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a text retriever with the ability to understand multimodal queries via dynamic modality interaction. Ret-XKnow leverages a partial convolution mechanism to focus on visual information relevant to the given textual query, thereby enhancing multimodal query representations. To effectively learn multimodal interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset automatically constructed from visual dialogue datasets. Our dataset construction process ensures that the dialogues are transformed into suitable information retrieval tasks using a text retriever. We demonstrate that our approach not only significantly improves retrieval performance in zero-shot settings but also achieves substantial improvements in fine-tuning scenarios. Our code is publicly available: https://github.com/yeongjoonJu/Ret_XKnow.", "sections": [{"title": "Introduction", "content": "With the growing demand for information retrieval across diverse applications, such as internet search and knowledge-based question answering, precise and efficient retrieval from multimodal queries involving pairs of images and text has emerged as a critical challenge. In such multimodal queries, each modality independently provides insufficient information to retrieve the desired passages within a knowledge base, necessitating the integrated understanding of the visual and textual queries.\nExisting Vision-Language (VL) retrievers (Qu et al. 2021; Luo et al. 2021; Gao et al. 2022; Lin et al. 2023) often depend on disjointed models for object detection or image captioning to provide visual information. The reliance on disjointed models complicates the training process (e.g., the models should be fine-tuned for separate tasks in domain adaptation) and increases the likelihood of propagating erroneous predictions. The utilization of the captioning model also lacks the fine-grained information embedded within the images. Previous approaches (Lin et al. 2023; Luo et al. 2023) have attempted to address these drawbacks. However, as shown in Fig. 1, they result in lower performance in a zero-shot setting than a text retriever that does not use image information despite their pre-training for the image-text alignment. Lin et al. (2023) introduce token-level embeddings and utilize two types of visual representations: textual description of the image and feature-based visual embeddings with regions of interest by an object detector. They pre-train the retriever to map token-level visual embeddings into the linguistic space of a text retriever and then fine-tune it by adding image captions to the textual queries. Such the retriever captures fine-grained features of the image by employing visual embeddings with captions. They also facilitate modality interaction between the textual query and the image by relying on textual information, but the mechanism also results in complex implementations and inefficient retrieval due to multiple steps. Luo et al. (2023) present an end-to-end approach that projects multimodal features encoded via self-attention into linguistic space with a pre-training task called VL-ICT, to detach the dependency on the disjointed modules. They automatically construct a pre-training dataset by applying the Inverse Cloze Task (ICT) (Lee, Chang, and Toutanova 2019) to a multimodal knowledge base. However, this approach has significant limitations. First, the dataset does not adequately reflect the variety and complexity of real-world queries, as it only removes the title or caption from a sentence extracted as the query without considering the image. Second, in the constructed pairs of a multimodal query and the corresponding passage, the passage can often be matched solely with the textual content of the query. This occurs because the target passage is selected from the content following a sentence with a title or caption, thereby hindering learning rich image representations.\nTo tackle these issues, we propose two approaches: (1) an end-to-end Retriever to expand visual Knowledge, Ret-XKnow, and (2) a Visual Dialogue-to-Retrieval (ViD2R) pre-training dataset constructed from visual dialogues containing distinct relevant passages for various queries related to the same image. Ret-XKnow endows a text retriever with the understanding of multimodal queries in the context of efficient information retrieval, inspired by the concept of partial convolutions (Liu et al. 2018), which fill undesired pixels with surrounding pixel information. We compress visual embeddings to focus on the visual information relevant to the textual query by leveraging the relevance scores between visual embeddings and textual query representations as an adaptive mask. We only attach a vision encoder to the text retriever with only a few layers, utilizing output embeddings of the penultimate layer in the vision model for fine-grained visual representations. Our model architecture does not allow the direct intervention of textual query features in the pre-training stage, achieving modality interaction without fusing text features with image features. Through this architecture, we introduce both the late-interaction mechanism (Khattab and Zaharia 2020) for pre-indexing documents and the modality interaction without requiring an additional document encoder and disjointed models.\nRecent advances in multimodal language models have produced several multimodal dialogue datasets (Zhu et al. 2023; Liu et al. 2023; Wang et al. 2023; Huang et al. 2023) for training models to perform tasks based on visual content. These datasets consist of multi-turn sessions with query-response pairs centered around a single image, providing precise and comprehensive information pertinent to the query and image. The response with detailed information can improve multimodal retrieval tasks by linking image understanding with complex textual queries. Whereas, such datasets are not appropriate for directly training retrievers due to the gap between explicit responses and broader passages. To bridge this gap, we transform the visual dialogue datasets into a format suitable for retrieval tasks through three simple steps: pre-processing, neural filtering, and response-to-passage conversion. Our construction process is applicable in diverse domains and modalities since our approach only requires multimodal dialogue datasets and sets of documents related to the target domain. Our retriever, Ret-XKnow pre-trained with the ViD2R dataset, outperforms various baselines in zero-shot retrieval performance across four multimodal datasets in an end-to-end manner. Furthermore, we demonstrate that the pre-training dataset curated via our construction method effectively mitigates the issue of overlooking visual features during the pre-training stage, leading to remarkable performance in fine-tuning settings. Our contributions are summarized as follows:\n\u2022 We propose Ret-XKnow, an end-to-end multimodal retriever that overcomes the limitations of disjointed models by dynamically focusing on visual features relevant to the textual query.\n\u2022 We introduce the ViD2R dataset, which transforms visual dialogue datasets into a format suitable for training VL retrievers, leading to significant improvements in zero-shot retrieval performance.\n\u2022 We demonstrate the comprehensive adaptability of Ret-XKnow by fine-tuning three downstream tasks. Our end-to-end retriever even shows comparable performance on baseline methods utilizing image captioning."}, {"title": "Related Works", "content": "Neural knowledge retrieval has been a cornerstone of Question Answering (QA) systems, with Dense Passage Retrieval (DPR) (Karpukhin et al. 2020) and its variants (Luo et al. 2021; Gui et al. 2022; Lin and Byrne 2022; Wu and Mooney 2022) pioneering the use of one-dimensional embeddings and contrastive learning. The advent of fine-grained late interaction models (Khattab and Zaharia 2020; Santhanam et al. 2022b) introduced enhanced embedding strategies, enabling precise query-document comparisons. ReAtt (Jiang et al. 2022) further streamlined the QA process by merging the retrieval and reading components into a unified Transformer model, offering an end-to-end solution.\nThe transition from traditional text queries to multimodal queries has marked a significant evolution in knowledge retrieval (Luo et al. 2021; Ge et al. 2022; Hanu et al. 2022). Initial methods focused on converting images into textual representations, such as captions (Qu et al. 2021; Gao et al. 2022) and object tags (Gui et al. 2022; Yang et al. 2022), leveraging text-based retrievers for relevant knowledge identification. EnFoRe (Wu and Mooney 2022) and DEDR (Salemi, Altmayer Pizzorno, and Zamani 2023) improve image-query representations derived from a multimodal encoder with generated entities and captions, respectively. FLMR (Lin et al. 2023) further refined multimodal queries by incorporating RoIs and generated captions under a late-interaction mechanism. To detach the dependency on intermediate modules, ReViz (Luo et al. 2023) represents an end-to-end multimodal retrieval system, introducing the VL-ICT for pre-training. We extend the motivation of the previous work (Luo et al. 2023) to design an end-to-end VL retriever to retrieve relevant passages with multimodal queries. Unlike previous methods, our approach overcomes the limitations of relying on disjointed models and complex processing by dynamically integrating visual features directly into the retrieval process."}, {"title": "Our Approach", "content": "In this section, we first define the problem statement for knowledge retrieval with multimodal queries. Then, we describe the architecture of our end-to-end retrieval model, Ret-XKnow, along with the construction method of the ViD2R dataset utilizing visual dialogue datasets."}, {"title": "Problem Definition", "content": "We focus on encoding a multimodal query $Q = (I,T)$, where $I$ and $T$ represent an image and textual content, respectively. Both $I$ and $T$ individually provide insufficient information to retrieve desired passages $K = \\{D_1, D_2, ..., D_n\\}$ within a knowledge base, where $D$ denotes a single passage. The primary objective of our end-to-end retriever $R$ is to accurately map to the set of relevant textual knowledge $K$ without requiring separate models for image understanding. To achieve this goal, the multimodal retriever $R$ should encode the multimodal queries $Q$ by referring information from both modalities $I$ and $T$ to retrieve passages $K$."}, {"title": "Architecture of Ret-XKnow", "content": "We design a retrieval system that dynamically integrates the capabilities of both the pre-trained text retriever, $R_T$, and the vision model, $R_V$, to enhance the retrieval of multimodal queries by focusing on visual features relevant to the textual query. Starting with encoding the image and text with separate encoders, Ret-XKnow compresses the visual features relevant to the given textual query at the end for modality interaction, applying the fine-grained late-interaction mechanism. Our network architecture is illustrated in Fig. 2. We elaborate on the components of Ret-XKnow and the rationale behind our design choices.\nFine-grained Late-interaction. To preserve abundant information of knowledge, we employ token-level embeddings for both modalities, applying the MaxSim operation (Khattab and Zaharia 2020). Given the representation of a multimodal query $Q$ and a document $D$, we estimate the relevance score $r_{Q,D}$ between $Q$ and $D$ via late interaction between token-level embeddings from the multimodal retriever $R$ as follows:\n$T_{Q,D} = \\sum_{i=1}^{l_Q} max_{j=1}^{l_D}(\\|E_{Q_i}\\|_2, \\|E_{D_j}\\|_2)$  (1)\nwhere $E_Q \\in \\mathbb{R}^{l_Q \\times d}$ and $E_D \\in \\mathbb{R}^{l_D \\times d}$ denote the representations of $Q$ and $D$ embedded by the retriever $R$, with each embedding having a dimension of $d$. Here, $l_Q$ and $l_D$ represent the number of embeddings for $Q$ and $D$, respectively. This operation chooses the highest relevance scores over all document tokens for each query token. In our approach, we incorporate the late-interaction mechanism for pre-indexing documents, ensuring no interaction between the query and the documents during the encoding process.\nVisual Embeddings for Multimodal Queries. Building upon previous approaches (Alayrac et al. 2022; Liu et al. 2023) that integrate visual comprehension into language models, Ret-XKnow adopts the Vision Transformer (ViT) (Dosovitskiy et al. 2021) as its vision encoder. We utilize token-level visual embeddings $F_v$ from the penultimate layer of the ViT, along with a global visual embedding $F_g$ obtained from a special token (i.e., CLS token). For the global embedding $F_g \\in \\mathbb{R}^{d_v}$, we directly project the visual features into the latent space of the text retriever $R_T$ via a two-layer perceptron to align the vision and text modalities. Subsequently, the projected $F_g$ is reshaped into token-level embeddings $E_g \\in \\mathbb{R}^{l_g \\times d}$, where $l_g$ is the pre-defined number of tokens. For token-level visual embeddings $F_v \\in \\mathbb{R}^{l_v \\times d_v}$, each embedding corresponds to distinct visual elements or regions within the input image and involves similar semantics among adjacent patches. Thus, we can acquire regions of interest without region proposal networks since the visual embeddings $F_v$ encompass granular visual information. We aim to extract visual information pertinent to the textual query $T$ while diminishing the information on unrelated aspects, with rich visual representations. To achieve modality interaction, we employ the MaxSim operation and the partial convolution mechanism (Liu et al. 2018). The concept of partial convolutions was originally designed for image inpainting tasks to fill missing or unwanted pixels by using the surrounding pixel information. In the context of Ret-XKnow, this mechanism is adeptly repurposed to refine visual embeddings by filling the irrelevant embeddings with adjacent visual features. The visual embeddings $F_v$ first un-"}, {"title": "Training", "content": "We deal with passages including the golden answers to a given question $Q$ as relevant passages $K$. To train our model, we employ in-batch negative sampling, which treats all passages in a training batch except for a passage $D$ belonging to $K$ as negative passages $\\bar{K}$ for $Q$. We optimize our model by minimizing the following contrastive loss $L_{CL}$ over the dataset $D$:\n$L_{CL} = - \\sum_{(Q,D) \\in D} log \\frac{exp(r_{Q,D})}{exp(r_{Q,D}) + \\sum_{\\bar{D} \\in \\bar{K}}exp(r_{Q,\\bar{D}})}$ (5)\nIn the pre-training stage, all parameters of $R_T$ and $R_V$ are frozen. After training, all passages are pre-indexed using PLAID (Santhanam et al. 2022a), identical to ColBERTv2 (Santhanam et al. 2022b). In the inference stage, we utilize only embeddings with the highest $r_{I,T}$ score for $F_v$ to prevent using unrelated information with the textual query."}, {"title": "ViD2R Dataset Construction", "content": "To endow the multimodal retriever $R$ with the ability to comprehend images based on textual queries, we leverage existing visual dialogue datasets. Despite the rich information of responses in visual dialogues, the datasets are not appropriate to directly train the retriever $R$ since there exists a clear distinction between the explicit responses and more expansive passages. To bridge this gap, we transform the visual dialogue datasets into a format suitable for multimodal retrieval tasks via the following three steps, as illustrated in Fig. 3.\nPre-processing. First, we divide the dialogues into individual turns. Subsequently, to maintain informative content within the dataset, we filter out turns that are unsuitable for retrieval tasks based on responses and tasks given by queries. We exclude tasks that do not contribute to knowledge-based retrieval, such as queries requiring or providing the location of objects, which are unrelated to knowledge content. To reduce bias in training, responses containing simple affirmations or negations, such as \"yes\" or \"no,\" are edited to remove these elements.\nNeural Filtering. To guarantee that the model learns to utilize visual information in multimodal retrieval, we apply neural filtering to our construction process. Using a text retriever, we perform the top-5 retrieval with questions from a knowledge base compiled with the responses in the visual dialogue datasets. Through this process, we automatically identify responses that can be retrieved based solely on textual queries. To avoid impeding the learning of image representations for retrieval, we filter out the matched query-response pairs from the text retrieval process.\nResponse-to-Passage Conversion. In the context of retrieval, passages may contain both relevant and irrelevant information, unlike responses in dialogues. Thus, we transform responses into passages typically found in knowledge retrieval tasks by unifying passages related to multimodal queries. To identify the relevant passages, we utilize the responses instead of the queries since the responses contain image-related information conditioned on the given query and image while the textual queries have restricted information. We obtain top-3 passages retrieved from Wikipedia using the responses as textual queries. However, the text retriever may retrieve inappropriate passages due to potential inaccuracies in the retriever. To ensure that converted passages are relevant to both the image and the question, we combine three retrieved passages with the responses. We simply concatenate the response behind the top-1 passage, thereby maintaining the relevance of the context."}, {"title": "Experiments", "content": "In this section, we present a comprehensive evaluation of Ret-XKnow, focusing on its performance in zero-shot multimodal retrieval and its adaptability to downstream tasks through fine-tuning. Our experiments are designed to showcase the effectiveness of Ret-XKnow in understanding and integrating complex multimodal queries for information retrieval.\nDatasets. We employ four retrieval datasets, two kinds of OK-VQA (Marino et al. 2019), ReMuQ (Luo et al. 2023), and A-OKVQA (Schwenk et al. 2022) to evaluate the retrieval performance of models for multimodal queries. For the OK-VQA dataset, we employ two datasets with different knowledge bases: a small corpus collected from Google search API introduced in (Luo et al. 2021) and a large corpus that contains 11 million Wikipedia passages created by (Qu et al. 2020). Note that these datasets are specifically designed for multimodal retrieval. The data statistics for the retrieval datasets are shown in the Appendix.\nEvaluation Metrics. We evaluate the retrieval performances of Ret-XKnow and our baselines by the following metrics: Mean Reciprocal Rank at 5 (MRR@5), which measures ranking the first relevant passage within the top-5 results; Precision at K (P@K), which measures the accuracy of the top-K retrieved passages; and Recall at N (R@N), which evaluates the ability of the model to retrieve all relevant passages within the top-N results. Due to the absence of ground-truth knowledge passages for each query in the OK-VQA datasets, we define passages that contain any human-annotated answers as ground-truth, following the approach by (Luo et al. 2021).\nImplementation Details\nViD2R. Our pre-training dataset is synthesized from two visual dialogue datasets, as detailed in works by (Liu et al. 2023) and (Wang et al. 2023). These datasets merge tasks necessitating image comprehensions for instruction-following tuning (Ouyang et al. 2022). After the preprocessing stage, we yielded 1.35 million QA pairs, each associated with a single image. Subsequent neural filtering, employing ColBERTV2 (Santhanam et al. 2022b) trained on the MS MARCO Passage Ranking task (Nguyen et al. 2016), refined the preprocessed pairs to 0.98 million QA pairs including queries that require visual context for accurate retrieval. In the final stage, we use 6 million Wikipedia released in (Chen et al. 2023) as our data pool to convert responses to passages.\nRet-XKnow. We adopt CLIP ViT-base (Radford et al. 2021) as our vision encoder, alongside ColBERTv2 (Santhanam et al. 2022b), rooted on BERT-base (Devlin et al. 2019), serving as our text retriever. We reduce the number of visual embeddings $F_v$ to 16 embeddings via $C(\u00b7)$ with 5 of kernel size. $F_g$ is converted into 16 embeddings via projection using two-layer perception. The dimension $d$ of final embeddings $E_Q$ and $E_D$ is set to 128.\nZero-shot Retrieval using End-to-End Retriever\nBaselines. To demonstrate the effectiveness of our end-to-end multimodal retrieval framework, we compare our approach against the advancements including the following baselines. ColBERTv2 (Santhanam et al. 2022b), a text-based retrieval model that employs a fine-grained late-interaction mechanism. In experiments, this model retrieves relevant passages using only textual queries.\nFLMR+WiT (Lin et al. 2023), a multimodal retriever that incorporates external visual information. They use vision and text encoders identical to our Ret-XKnow. Their pre-training on a subset of the WiT dataset (Srinivasan et al. 2021) aims to align visual embeddings with the linguistic space of the text retriever. ReViz+VL-ICT (Luo et al. 2023), an end-to-end knowledge retrieval model for multimodal queries, which uses pre-trained ViLT and BERT-base as its query and document encoders, respectively. With ViLT optimized for the image size of 384\u00d7384, their model introduces"}, {"title": "Fine-tuning on Downstream Tasks", "content": "We further demonstrate the adaptability of Ret-XKnow and the effectiveness of the ViD2R pre-training task by fine-tuning models on downstream tasks.\nResults. The results from our experiments, detailed in Tab. 3, clearly demonstrate the efficacy of our approach. First, Ret-XKnow outperforms other baseline models on key metrics across knowledge bases of different sizes, even before any specialized pre-training. This performance advantage is meaningfully extended by applying transfer learning on the Ret-XKnow model pre-trained with our ViD2R dataset. Moreover, our investigations reveal that our end-to-end retriever reaches performance levels comparable with the baseline that utilizes caption. As shown in Tab. 4, we present the performance of models fine-tuned with image captions within the OK-VQA (GS) dataset. Ret-XKnow approximates these achievements without employing captions in Tab. 3. Such outcomes underscore the effectiveness of Ret-XKnow in enhancing multimodal query representations in an end-to-end manner."}, {"title": "Ablation Studies", "content": "In Tab. 5, the removal of strided convolution $C(\u00b7)$ leads to significant performance degradation in metrics, despite the relatively large number of tokens. This reveals that numerous tokens are rather disturbed in the retrieval task, underlining the need for the extraction of core information. Similarly, omitting the conversion process in ViD2R notably impacts the model's performance, highlighting its essential contribution to the pre-training process. This outcome corroborates our hypothesis that there is a clear difference between informative response and passage in the retrieval context."}, {"title": "Conclusion", "content": "This paper presents an end-to-end multimodal retriever for enhancing multimodal query representations. To effectively pre-train the retriever, we also introduce the ViD2R dataset by automatically transforming multimodal dialogues into information retrieval tasks with only a text retriever and a set of documents. Our method outperforms previous base-"}]}