{"title": "The Factuality of Large Language Models in the Legal Domain", "authors": ["Rajaa EL HAMDANI", "Thomas Bonald", "Fragkiskos Malliaros", "Nils Holzenberger", "Fabian Suchanek"], "abstract": "This paper investigates the factuality of large language models (LLMs) as knowledge bases in the legal domain, in a realistic usage scenario: we allow for acceptable variations in the answer, and let the model abstain from answering when uncertain. First, we design a dataset of diverse factual questions about case law and legislation. We then use the dataset to evaluate several LLMs under different evaluation methods, including exact, alias, and fuzzy matching. Our results show that the performance improves significantly under the alias and fuzzy matching methods. Further, we explore the impact of abstaining and in-context examples, finding that both strategies enhance precision. Finally, we demonstrate that additional pre-training on legal documents, as seen with SaulLM, further improves factual precision from 63% to 81%.", "sections": [{"title": "Introduction", "content": "Language models (LMs) store knowledge from pre-training documents, allowing them to be queried with natural language (NL) rather than a formal language, unlike structured knowledge bases (KBs) (Petroni et al., 2019; Chang and Bergen, 2024). This natural language interface democratizes access to knowledge, especially in domains where users may not master querying structured KBs (Hendrix et al., 1978). In the legal domain, this includes law experts (scholars, judges, lawyers, paralegals) and laymen seeking legal information (Paul et al., 2020). LMs can acquire and store more facts than KBs, as most knowledge is in NL documents. This reduces the cost of manually populating KBs, as LMs automatically store knowledge from pre-training documents without human supervision (AlKhamissi et al., 2022). While Wikidata is the largest structured KB, it lacks comprehensive domain-specific knowledge (Weikum et al., 2021). In contrast, Wikipedia articles offer this knowledge, which LMs can store through pre-training.\nDespite their capabilities, LLMs are prone to hallucination (Ji et al., 2023; Suchanek and Luu, 2023), producing factually incorrect answers in a disturbingly self-confident tone. While many studies have examined the factual accuracy of LLMs (OpenAI et al., 2024; Wei et al., 2024; Muhlgay et al., 2024; Hu et al., 2024; Si et al., 2023; Min et al., 2023; Manakul et al., 2023; Mallen et al., 2023), few focused on domain-specific knowledge. Domain-specific knowledge poses unique challenges, such as specific named entities that are unique to the domain (e.g., lawyers, judges) and that do not appear in abundance on the Internet (and hence in training data); relations that may not even be known to the general public (e.g. the judge who issues the majority opinion of a case); facts that are specific to the domain (e.g., legal rules); and finally ways of phrasing that are typical for the jargon, but unusual on the Web as a whole (e.g., concurring or dissenting opinions). However, domain-specific applications have huge potential, aiding users in areas beyond one-size-fits-all products.\nIn this paper, we focus on factual answers in the legal domain, motivated by both the risks and opportunities of using LLMs in this field (Bommasani et al., 2021). While benchmarks like LawBench (Fei et al., 2023) and LegalBench (Guha et al., 2023) evaluate LLMs on various legal tasks, LawBench is specific to Chinese law, and LegalBench lacks a legal knowledge task, limiting their use for probing LLMs on legal facts. The legal domain is particularly sensitive to hallucinations, as incorrect information can lead to harmful decisions (Dahl et al., 2024). Previous work used strict evaluation criteria, accepting answers only if identical to the ground truth, ignoring valid response variations (\"Samuel A. Alito, Jr\" is as valid an answer as \"Justice Alito\"). Additionally, earlier studies forced LLMs to provide answers, increasing hallucinations. In contrast, our approach allows models to abstain when unsure, reducing hallucinations. Accounting for acceptable variations in the phrasing of the answer and allowing the model to abstain from answering brings our evaluation closer to a realistic human use of an LLM as a KB.\nIn our work, we address four key research questions:"}, {"title": "Methodology", "content": "We present the methodology in Section 2, the results in Section 3, and our conclusions in Section 4."}, {"title": "Dataset", "content": "Legal knowledge is primarily stored in legislation and legal cases (Raz, 2002). This study focuses on atomic information, such as the jurisdiction of a case or legislation. Even if most real-world queries may not be about atomic facts, these atomic facts are the fundamental building blocks of actionable legal information. Therefore, it is crucial to first ensure that the LLM is not hallucinating atomic facts before asking for the relevant legal rule to a legal dispute. We use Wikidata (Vrandecic and Kr\u00f6tzsch, 2014) to create a benchmark dataset of atomic legal facts on legislation and legal cases.\nManually navigating Wikidata's taxonomy to identify relevant types is impractical due to its complexity (Suchanek et al., 2024). Instead, we leverage the Wikipedia Category Graph (WCG), which organizes articles by categories to facilitate navigation. We identify legal articles within two hierarchical steps of the \"Law\" category in the WCG and retrieve their corresponding Wikidata items. From these items, we construct a legal taxonomy and query Wikidata's SPARQL endpoint for more relevant items. We refine the dataset by manually selecting relations specific to the legal domain and"}, {"title": "Models", "content": "Large models are costly to deploy on-premise. We argue that most, if not all, legal tech applications involve sensitive personal data that cannot be transferred to third-party APIs. Thus, we limit experiments to open-source models with fewer than 8B parameters, runnable on a local machine. We select 7 models among the highly ranked in the LMSYS Chatbot Arena Leaderboard (Chiang et al., 2024): Gemma-2B, Gemma-7B, Llama-2-7B,\nLlama-3-8B, Mistral-7B, Phi-3-min-4k, RecurrentGemma-2B (Team et al., 2024; Botev et al., 2024; Touvron et al., 2023). We add SaulLM (Colombo et al., 2024), which is the result of further training Mistral-7B on legal corpora. We use the instruction-tuned variant of each model."}, {"title": "Prompt strategy", "content": "We experiment with zero-shot and few-shot prompts to query the models. For a question q, related to a subject-relation pair (s,r), the corresponding prompt contains 5 in-context examples1 shows an example prompt. We also instruct the model to answer \"I don't know\" if it cannot answer the question correctly."}, {"title": "Evaluation methods", "content": "We are evaluating the LLM as a KB. The correctness and coverage of a KB are usually evaluated by precision P and recall R (Weikum et al., 2021), respectively. For a given KB containing a set of statements S and a set of true statements T from the ground truth, they are defined by:\n$P = \\frac{S\u0548T}{S}$ and $R = \\frac{S\u0548T}{T}$\nIn our setting of LLM-as-KB, T is the set of all question-answer pairs (those of Table 1), S is the set of questions answered by the LLM,3 and S\u2229 T is the set of correct answers. Thus, the corresponding precision and recall are given by:\n$P_{LLM}=\\frac{|correct answers|}{|answered questions|}$\n$R_{LLM}=\\frac{|correct answers|}{|all questions|}$\nMost articles evaluating LLMs consider an answer correct only if it exactly matches the ground truth (Dahl et al., 2024; Petroni et al., 2019; Chang et al., 2024; Hu et al., 2024; Yu et al., 2024). However, LLMs can generate a fact in multiple forms and tend to answer verbosely, often providing explanations along with the answer. This behavior penalizes LLMs when evaluated with exact matching.\nEach Wikidata item has a main surface form called a label and alternative names known as aliases. For example, item Q30 has the label \"United States of America\" and aliases like \"U.S.A.\" and \"America\". We consider the following options for declaring an answer correct:\n(EM) Exact matching. The answer matches the label.\n(AM) Alias matching. The answer matches the label or any of its aliases. wI\n(FM) Fuzzy matching. The answer contains the label or any of its aliases."}, {"title": "Results", "content": "The LLM might abstain from answering a question.\nFuzzy matching accounts for correct but verbose answers. It is prone to errors, however. Take, for instance, the question \u201cWhat is the legislation of the case 'Rummel v. Estelle'?\u201d, whose correct answer is \"United States\". The answer \"The case 'Rummel v. Estelle' applies to the state of Louisiana, United States\" is false but contains the label of the true answer. We manually inspect such cases and implement post-processing rules for the answers to enhance the accuracy of fuzzy matching."}, {"title": "Exact matching underestimates the performance of LLM-as-KB", "content": "The performance of the models, shown in Table 2, increases significantly when evaluated with alias matching (AM) and fuzzy matching (FM), compared to exact matching (EM). This indicates that LLMs often generate correct answers in varied surface forms that are not captured by exact matching alone. For instance, SaulLM in few-shot mode and allowed to abstain increased in precision from 36% (EM) to 81% (FM). The ranking of the models changed notably depending on the evaluation method. Under exact matching and alias matching, Mistral-7B performed poorly, ranking last with 8% precision (EM). However, under fuzzy matching, Mistral-7B's improved significantly, ranking third with a precision of 63%.\nThese observations highlight the limitations of exact matching-based evaluation, which can unfairly penalize models that tend to generate more contextual, rich, and verbose answers, like SaulLM and Mistral-7B. Incorporating alias and fuzzy matching provides a better understanding of each model's true performance as a knowledge base."}, {"title": "LLMs can be instructed to abstain from generating incorrect answers", "content": "Table 2 reports the abstain rate under each prompting strategy, with or without the abstain instruction. Most models abstain more after adding the abstain instruction to the prompt. Abstaining increases precision by answering fewer questions. Interestingly, despite no few-shot examples with answer \u201cI don't know\", the models still abstain a significant fraction of the time.\nIncluding an abstain instruction generally improves precision across different models, as summarized in Figure 2. The instruction helps the models to refrain from providing incorrect answers"}, {"title": "In-context examples increase the factuality of LLMs", "content": "We evaluate if in-context examples enhance the factuality of LLMs by comparing precision scores between two prompting strategies: with (few-shot) and without (zero-shot) in-context examples, as shown in Figure 2. To isolate the impact of in-context examples, we calculated precision only for questions where the model did not abstain in either setting. Our observations indicate that in-context examples improve precision for 6 out of 8 models when instructed to abstain and for 5 out of 8 models when not prompted to abstain. The improvement is"}, {"title": "Training an LLM on legal documents improves its factuality", "content": "Finally, we investigate whether training an LLM on legal documents improves its factuality by comparing the performance of SaulLM, which is Mistral-7B with additional pre-training on legal documents, against the standard Mistral-7B model. The top performance was achieved by SaulLM, outperforming Mistral-7B. Under the few-shot prompt with the instruction to abstain, SaulLM achieved a precision of 81%, significantly higher than the 63% precision of Mistral-7B. This suggests that training SaulLM on legal documents enhances its ability to provide correct answers, similar to the effect of few-shot prompting. The additional training helps SaulLM understand the context and format of legal questions more accurately, leading to better performance. Thus, training an LLM on legal documents substantially improves its factuality, particularly in terms of precision. This improvement aligns with the benefits observed from few-shot prompting, indicating that domain-specific training enables the model to generate more accurate answers.\nInterestingly, while SaulLM showed a significant increase in precision, its recall did not improve as markedly. This can be explained by the much higher abstain rate of SaulLM compared to Mistral-7B."}, {"title": "Conclusions", "content": "In this paper, we explored the factuality of LLMs as KBs in the legal domain. We evaluated various models, including SaulLM, pre-trained on legal documents.\nOur findings reveal that the performance of LLMs improves significantly when using alias and fuzzy matching instead of exact matching. Abstain instructions and few shot prompting increase factuality. Pre-training on legal documents, as shown by SaulLM, substantially improves the precision, highlighting the importance of domain-specific pre-training.\nOur careful evaluation methods reveal that LLMs hallucinate significantly less than reported in (Dahl et al., 2024), due to our consideration of the verbose nature of LLM responses. While SaulLM achieves a high precision of 81%, this is still insufficient for high-stakes legal applications, but may suffice for research and analytics. Instances of lawyers being sanctioned for using fictitious cases"}]}