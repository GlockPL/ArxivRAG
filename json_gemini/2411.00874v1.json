{"title": "VecCity: A Taxonomy-guided Library for Map Entity Representation Learning [Experiment, Analysis & Benchmark]", "authors": ["Wentao Zhang", "Jingyuan Wang", "Yifan Yang", "Leong Hou U"], "abstract": "Electronic maps consist of diverse entities, such as points of interest (POIs), road segments, and land parcels, playing a vital role in applications like ITS and LBS. Map entity representation learning (MapRL) generates versatile and reusable data representations, providing essential tools for efficiently managing and utilizing map entity data. Despite the progress in MapRL, two key challenges constrain further development. First, existing research is fragmented, with models classified by the type of map entity, limiting the reusability of techniques across different tasks. Second, the lack of unified benchmarks makes systematic evaluation and comparison of models difficult. To address these challenges, we propose a novel taxonomy for MapRL that organizes models based on functional modules-such as encoders, pre-training tasks, and downstream tasks rather than by entity type. Building on this taxonomy, we present a taxonomy-driven library, VecCity, which offers easy-to-use interfaces for encoding, pre-training, fine-tuning, and evaluation. The library integrates datasets from nine cities and reproduces 21 mainstream MapRL models, establishing the first standardized benchmarks for the field. VecCity also allows users to modify and extend models through modular components, facilitating seamless experimentation. Our comprehensive experiments cover multiple types of map entities and evaluate 21 VecCity pre-built models across various downstream tasks. Experimental results demonstrate the effectiveness of VecCity in streamlining model development and provide insights into the impact of various components on performance. By promoting modular design and reusability, VecCity offers a unified framework to advance research and innovation in MapRL. The code is available at https://github.com/Bigscity-VecCity/VecCity.", "sections": [{"title": "1 INTRODUCTION", "content": "In the era of mobile internet, electronic maps have become a foundational platform for a wide range of applications, including intelligent transportation systems and location-based services. An electronic map consists of map entities, such as points of interest (POIs), road segments, and land parcels. These entities encapsulate complex geospatial data, spatial relationships, and geometric-topological structures, presenting significant challenges for data representation. The effective representation of map entities has become a critical and enduring research focus within spatiotemporal data analysis and geographic information systems (GIS).\nIn the literature, early studies represented map entities as records in relational databases or as files containing geographic information attributes, such as PostGIS [61], Shapefiles [18], and GeoJSON [3]. With this representation approach, application services relying on map entities are required to develop task-specific models or algorithms to process the entities' attributes and extract geographic relationships within or between them. However, the task-specific nature of these data processing models restricts their generalizability and reusability across different applications and scenarios.\nIn recent years, pre-trained representation learning has emerged as a powerful approach for generating versatile, task-independent data representations. It has achieved remarkable success across various domains, including natural language processing (NLP) [44, 47, 48, 52] and computer vision (CV) [7, 16, 24, 25, 63]. In the field of spatiotemporal data analysis, a growing number of studies have embraced pre-trained representation learning methods to construct generic representations of map entities [9, 33, 41, 45, 82]. These efforts have given rise to an emerging research area known as Map Entity Representation Learning (MapRL). As shown in Fig. 1, the number of MapRL papers has been steadily increasing, reflecting the growing interest and potential impact of this field. However, despite these rapid advancements, several structural challenges require further exploration to unlock the potential of MapRL.\nChallenge 1: Fragmented Research Fields. Electronic maps consist of various types of map entities, such as POIs (points), road segments (polylines), and land parcels (polygons). Existing research often treats the modeling of these entities as separate areas, limiting the reusability of techniques, methods, and modules across fields. However, real-world electronic map applications typically require the integrated use of all three entity types. This segregation not only creates obstacles for practical applications but also forms implicit academic exchange barriers between subfields, thereby constraining the progress of MapRL research.\nChallenge 2: Lack of Standardized Benchmark. Although numerous MapRL models have been proposed in the literature, their performance is often evaluated on different datasets and under varying experimental settings. Unlike the CV and NLP fields, this domain lacks standardized datasets and benchmarks. This absence hinders fair performance comparisons between models and limits the ability of researchers to establish guiding design principles through consistent and rigorous model evaluation.\nTo address the first challenge, we propose a novel taxonomy for MapRL models, namely method-based taxonomy, which organizes the essential components of an MapRL model into four key elements: Map Data, Encoder Models, Pre-training Tasks, and Downstream Tasks. The map data is further divided into Map Entity Data and Auxiliary Data. Encoder models are categorized into"}, {"title": "2 TAXONOMY OF MAPRL", "content": "In this section, we present the proposed method-based taxonomy. Figure 2 outlines the general framework for MapRL. The training process of a typical MapRL model consists of two key stages: pre-training and fine-tuning. During pre-training, the MapRL model takes map data as input, using an encoder model to transform them into representation vectors. The encoder's parameters are optimized through pre-training tasks. In the fine-tuning stage, downstream tasks are employed to further refine the encoder. This pipeline defines four core components of an MapRL model: Map Data, Encoder Models, Pre-training Tasks, and Downstream Tasks.\n2.1 Map Data\nThe map data consists of map entities and auxiliary data.\n2.1.1 Map Entities.\nAs illustrated in Fig. 3, an electronic map consists of three geometric elements: Points, Polylines, and Polygons, each representing a distinct type of map entity.\n*   Points / POI. A point element represents a Point of Interest (POI), identified by a location coordinate, i.e., (latitude, longitude).\n*   Polylines / Road Segment. A polyline element represents a Road Segment, composed of a sequence of connected line segments, with each endpoint defined by location coordinates.\n*   Polygons / Land Parcel. A polygon element represents a Land Parcel, defined by a set of connected line segments forming a closed boundary.\nFor each map entity, a set of features describes its properties. For instance, the category feature of a POI differentiates the services it offers, such as scenic spots, transit stations, or shopping facilities. Road segments are characterized by features like speed limits, lane counts, road grade, etc. Based on these definitions, we formally define electronic maps and map entities as follows.\nDefinition 1 (Electronic Maps and Map Entities). An electronic map is defined as a set of map entities, $V = \\{v_1,..., v_i, ..., v_I\\}$, where each entity $v_i$ consists of four components: ID, type, geometric shape, and feature. The ID serves as the unique identifier of $v_i$. The type specifies the entity's geometric form - point, polyline, or polygon. The geometric shape provides the coordinates that define its structure, while the feature captures its various attributes.\n2.1.2 Auxiliary Data.\nBesides map entities, two types of auxiliary data are commonly used in MapRL models: Trajectories and Relation Networks.\nDefinition 2 (Trajectories). Trajectories refer to the mobility records of humans or vehicles. A trajectory is defined as a sequence of locations with corresponding timestamps, i.e., $tr = ( < l_{tr_1}, t_{tr_1} >, ..., < l_{tr_k}, t_{tr_k} >,..., < l_{tr_K}, t_{tr_K} > )$, where $l_{tr_k}$ and $t_{tr_k}$ are the location and timestamp for the k-th trajectory sample.\nTrajectory data can be divided into two classes based on the type of location samples $l_{tr_k}$. i) Check-in Trajectories: These use POIs as samples and are collected through social networks or location-based service (LBS) apps. They log a user's location only upon check-in, resulting in a low sampling rate, often requiring days to capture a single point. MapRL models leverage these trajectories to infer users' personal preferences. ii) Coordinate Trajectories: These"}, {"title": "2.2 Encoder Models and Pre-training Tasks", "content": "Encoder models and pre-training tasks are two core components of a pre-trained MapRL model, responsible for converting input map data into representation vectors for map entities. Our taxonomy classifies encoder models into three types: token-based, graph-based, and sequence-based, designed to model features, relation networks, and trajectory sequences of map entities, respectively. Each encoder type corresponds to different pre-training tasks tailored to its structure and purpose.\n2.2.1 Token-based Models.\nEncoders. The token-based encoder generates representation vectors directly from the features of map entities. For discrete features, it applies one-hot encoding, and for continuous features, the value range is divided into consecutive bins, each represented by a one-hot code. Given K features for a map entity, let the one-hot code of the k-th feature be $f_k \\in \\{0, 1\\}^{F_k}$ (where $F_k$ is the dimensionality). The encoder employs a learnable embedding matrix $R_k \\in \\mathbb{R}^{F_k \\times d}$ for each feature to generate the entity's representation:\n$r = ||_1^K f_k R_k,$\nwhere $||_{k=1}^K$ denotes the concatenation of all features' embeddings.\nIn the literature, the novelty of existing token-based methods lies in how effectively they extract valuable features from map data. These features can be categorized into three types. i) Spatial Features: These capture the locations and spatial structures of map entities. Spatial coordinates are the most fundamental spatial features. Hier [58] incorporates spatial structures by embedding hierarchical spatial grids into the representations of rasterized maps. ii) Temporal Features: A map entity's characteristics can vary at different times of the day. For example, a restaurant exhibits different behaviors during working hours versus lunchtime. To capture these variations, many token-based encoders divide the day into segments (e.g., hourly or half-hourly) and assign each segment a"}, {"title": "2.2.2 Graph-based Models.", "content": "Encoders. Graph-based encoders leverage auxiliary data in the form of relation networks (see Definition 3) to capture relationships among map entities. Graph-based encoders typically use the representations generated by token-based encoders as inputs and apply a graph neural network (GNN) to produce refined representation vectors. Specifically, given the map entity set $V = \\{v_1, ..., v_I \\}$, the embeddings generated by the token-based encoder are denoted as $R = \\{c_1, ..., c_I \\}$. The graph-based encoder then takes R along with the relation network G to generate the final representations:\n$H = \\{h_1, ..., h_i, ..., h_I\\} = GNN(R, G)$,\nwhere $h_i$ is the representation vector for the i-th map entity, and $GNN(\\cdot)$ is the GNN-based encoder model. Compared to token-based models, graph-based encoders incorporate the relation networks into the entity representations, enriching their expressiveness.\nExisting research on graph-based encoders focuses on developing GNNs capable of capturing complex relationships among map entities. Early studies applied standard GNN models, such as graph convolutional networks [73, 77] and graph attention networks [59, 78], to explore graphical [5, 26, 27] and social relations [38, 81]. Given the diverse relationships among map entities, some studies adopt more compound GNNs to enhance graph-based encoders. For example, the HREP model constructs a heterogeneous graph with four types of relations to represent land parcels [88]. The MGFN model utilizes human mobility patterns to build a multi-graph, assigning a dedicated GNN to each subgraph [72]. HRNR [71] adopts a hierarchical GNN to capture the connection, structure, and function relations of road segments [71]. Recent advancements also explore relations across different types of map entities. For instance, HRoad employs hypergraph neural networks to capture the relationships between road segments and land parcels [79]."}, {"title": "2.2.3 Sequence-based Models.", "content": "Encoders. Sequence-based encoders aim to capture temporal dependencies among map entities within trajectory-type auxiliary data. They use sequential deep learning models, such as LSTMs [55], GRUs [12], or Transformers [62], as backbones to encode the embeddings of map entities along the same trajectory into a sequence of representation vectors. These input embeddings are typically generated by token-based or graph-based encoders. Specifically, for map entities along a trajectory tr, let their embedding sequence be $R_{tr} = (r_{tr_1},..., r_{tr_k}, ..., r_{tr_K})$. The sequence-based model generates representation vectors for these entities as:\n$S_{tr} = (s_{tr_1},..., s_{tr_k}, ..., s_{tr_K}) = SEQ(R_{tr})$,\nwhere $SEQ(\\cdot)$ is the sequence-based encoder, and $s_{tr_k}$ is the representation vector for the k-th entity along the trajectory tr.\nIn trajectory data, each location sample is typically associated with a timestamp. Integrating temporal information from timestamps into entity representations has become a key focus of sequence-based encoders. Early methods divide time into discrete slots, such as the hour of the day or the day of the week, and use these slot indices to generate input embeddings using token-based encoders [21, 42, 85]. More advanced models embed timestamps within the components of sequential models. For example, JCLRNT [42], Toast [11], and CTLE [35] incorporate timestamps into the positional encoding of Transformers. Similarly, START [26] integrates the travel time of trajectory through a road segment into the attention of Transformers."}, {"title": "2.2.4 Module Integration of Encoders and Pre-training Tasks.", "content": "The encoders and pre-training tasks introduced in the previous sections form the fundamental modules for MapRL models. A complete MapRL model can integrate multiple types of encoders and pre-training tasks, leveraging their complementary strengths to enhance representation learning. For the encoder modules, existing models typically follow a \"token \u2192 graph \u2192 sequence\" modeling pipeline. This pipeline first extracts basic features using token-based encoders, then captures relational structures through graph-based encoders, and finally models temporal dependencies with sequence-based encoders. For encoders at each stage of the pipeline, the MapRL model can simultaneously employ multiple encoders for different pre-training tasks. Table 1 summarizes the combinations of encoders and pre-training tasks used in mainstream models.\nFor the three-stage encoder pipeline, MapRL models follow two pre-training paradigms. i) Sequential Training: The encoders are trained in stages \u2013 starting with token-based, followed by graph-based, and finally sequence-based \u2013 each refining the previous stage's output. ii) Joint Training: All encoders are optimized together using multi-task learning, capturing features, relations, and temporal patterns in an integrated manner. These paradigms offer flexibility in model impanelments, depending on the specific requirements of the MapRL task."}, {"title": "2.3 Downstream Tasks", "content": "The downstream tasks consist of two components: Downstream Models and Fine-tuning Loss. Downstream models use the representation vectors generated by the encoders to perform specific tasks. Fine-tuning loss adjusts the encoder parameters to improve the adaptability of the representation vectors for these tasks. In contrast to pre-training tasks, which are closely tied to the encoders, downstream tasks are heavily influenced by the type of map entities. This subsection presents a taxonomy of downstream tasks.\nTasks for POIs. Common downstream tasks for POI entities fall into three categories:\n*   POI Classification (POIC): This task predicts the category of a POI based on its representation. The downstream model typically uses an MLP network with a SoftMax classifier.\n*   Next POI Prediction (NPP): This task converts a POI-based trajectory into a sequence of representation vectors using an encoder. The downstream model, often a deep sequential model like LSTM, predicts the next POI in the trajectory using the representation of the first half. The output is a multi-class classification, with each class representing a possible POI.\n*   Trajectory User Link (TUL): This task predicts the user who generated a trajectory based on the trajectory's representation sequence. The downstream model is a deep sequential model, with each output class corresponding to a user ID.\nThe fine-tuning loss for all three tasks is the cross-entropy loss.\nTasks for Road Segments. Common downstream tasks for road segment entities fall into three categories:\n*   Average Speed Inference (ASI). This task leverages the representation vector to infer the average speed of a road segment. The average speed is derived from trajectory data and must be excluded from the input features of the representation encoders to prevent data leakage. Inferring average speed is a representative task for estimating unknown features of a road segment. The downstream model for this task is a linear regression function, and the fine-tuning loss function is Mean Square Error (MSE).\n*   Travel Time Estimation (TTE). This task estimates the travel time from the origin to the destination along a trajectory composed of road segments. The downstream model is a deep sequential model. Its input is the representation sequence of the segment-based trajectory, and the output is the regressed travel time. The fine-tuning loss function is MSE.\n*   Similarity Trajectory Search (STS). This task aims to find the most similar trajectory from a large database given a trajectory query. The downstream model first employs a deep sequential model to convert the representation sequences of all trajectories in the database into trajectory representation vectors. Then, it uses cosine similarity between these representations to identify the most similar trajectory to the query. The fine-tuning loss function is a contrastive learning loss, such as InfoNCE, which treats the ground-truth trajectory as the positive sample and others as negative samples. The ground-truth trajectory is typically generated by replacing a randomly selected sub-trajectory with another that shares the same start and endpoints [5, 26, 42]."}, {"title": "4 PERFORMANCE BENCHMARK", "content": "In this section, we present performance comparison experiments for the pre-built MapRL models in VecCity. These experiments serve three purposes: i) Demonstrating Usability and Power: To show that VecCity is a user-friendly and powerful toolkit for MapRL model development and evaluation. ii) Establishing Performance Benchmarks: To provide standard performance benchmarks for mainstream MapRL models using the datasets pre-loaded in VecCity. iii) Analyzing Component Contributions: To explore the contributions of different components in existing MapRL models, offering insights for designing more effective and robust models.\n4.1 Datasets and Experimental Setups\n4.1.1 Datasets. The dataset selection for our experiments follows these guiding principles: for each type of MapRL model, we incorporate all datasets that meet the task requirements. Specifically, for POI-oriented MapRL models, we use datasets from New York City, Chicago, Tokyo, Singapore, and San Francisco. For road-segment-oriented models, the datasets include Beijing, Xi'an, Chengdu, Porto, and San Francisco. For land-parcel-oriented models, we adopt datasets from Beijing, Xi'an, Chengdu, Porto, and San Francisco. Each dataset is divided into training, validation, and test sets following a 6:2:2 ratio, ensuring balanced and reliable performance evaluation. For each downstream task, we apply end-to-end fine-tuning to achieve optimal performance.\n4.1.2 Experimental Settings. In our experiments, the dimension of map entity representation vectors across all models is set to 128. For other model configurations, such as the number of layers and the dimensions of intermediate variables, we adopt the default settings from the original papers as initial values. We then use a trial-and-error approach to tune the parameters on each dataset, searching for the best performance configurations. All experiments are executed on an Ubuntu 20.04 system equipped with NVIDIA GeForce 3090 GPUs. The default batch size during model training is set to 64. In cases of out-of-memory (OOM) issues, the batch size is halved iteratively, with a minimum batch size of 8.\n4.2 Overall Experimental Results\n4.2.1 Experimental Results for POI MapRL. Table 3 presents the performance of MapRL models on downstream tasks, including POI Classification (POIC), Next POI Prediction (NPP), and Trajectory User Link (TUL). Each task is evaluated using two types of metrics. Additionally, model efficiency is assessed with two indicators: the number of parameters (Param) in millions and the time cost per epoch during pre-training (Speed) in seconds. The table also reports average performance rankings across tasks (Pre Ave Rank) and the ranking based on the models' average performance (Pre Rank). Similarly, efficiency rankings are given by Eff Ave Rank and Eff Rank. From Tab. 3, we have the following observations.\n1) Importance of Sequence-based Encoders: Models combining token-based and sequence-based encoders - such as CTLE, CACSR, and Hier - consistently outperform those relying solely on token-based encoders, highlighting the value of trajectory auxiliary data. Among these, models using Augmented Trajectory Contrastive Learning (ATCL) and Masked Trajectory Recovery (MTR) tasks (e.g.,, CTLE and CACSR) achieve better performance than those focused on Trajectory Prediction (TrajP) tasks (e.g.,, Hier). This is likely because ATrCL and MTR capture long-range temporal dependencies across entire trajectories, while TrajP focuses only on consecutive POI dependencies."}, {"title": "4.2.2 Experimental Results for Road Segment MapRL.", "content": "Table 4 shows a performance comparison of road segment MapRL models across three downstream tasks: Average Speed Inference (ASI), Travel Time Estimation (TTE), and Similarity Trajectory Search (STS). The following key observations can be drawn from the results:\n1) Full Pipeline Models Perform Best: Models utilizing the complete token-graph-sequence pipeline, such as JCLRNT and START, achieve the highest performance compared to models with partial pipelines. Additionally, token-based + graph-based and token-based + sequence-based models - such as HRNR, SARN, Toast, and HRoad - outperform purely token-based models like RN2Vec, highlighting the value of integrating trajectory and relation network information. Graph-based and sequence-based encoders capture essential patterns that improve road segment representations.\n2) Multiple Pre-training Tasks Improve Performance: Models with multiple types of pre-training tasks, i.e., Toast, HRoad, START, and JCLRNT, outperform those with single-task pre-training, i.e., HRNR and SARN. Notably, among token-based + graph-based models, HRoad, which integrates both NFI and GAu tasks, surpasses HRNR (GAu only) and SARN (AGCL only). Using diverse pre-training tasks enables the model to extract information from multiple perspectives, enhancing its ability to learn more comprehensive feature representations. Moreover, in the full-pipeline models, JCLRNT combines a sequence-oriented task (Augmented Trajectory Contrastive Learning) with a graph-oriented task (Augmented Graph Contrastive Learning) and outperforms START, which uses two sequence-oriented tasks (MTR and ATrCL). This suggests that even in models with multiple tasks, greater task heterogeneity can further enhance performance.\n3) Efficiency Trade-offs: Simpler models with fewer modules, such as SARN and RN2Vec, are significantly more efficient. Models without sequence encoders, such as HRNR and HRoad, also exhibit better efficiency since incorporating trajectory data demands considerable memory and time. Improving the efficiency of trajectory-based models remains a key challenge for future research."}, {"title": "4.2.3 Experimental Results for Land Parcel MapRL.", "content": "Table 5 presents the performance comparison of land parcel MapRL models on the downstream tasks of Land Parcel Classification (LPC), Flow Inference (FI), and Mobility Inference (MI). The following observations can be made:\n1) Incorporating POI Data Enhances Performance: Models that utilize POI data, such as MVURE, ReMVC, and HREP (see the \"Map Entities\" column in Table 1), outperform those that do not. These models treat POI categories within a land parcel as features, enriching the representation of the parcel's functional characteristics. This approach provides valuable contextual information, improving the overall performance of the models.\n2) Limitations of Current Encoder Structures: Most existing land parcel MapRL models adopt a token-based + graph-based encoder structure, focusing on extracting information from relation networks. However, these models overlook the sequential dependencies within trajectories, limiting their effectiveness. Future research should explore advanced methods to capture trajectory sequence information relevant to land parcels for further improvements."}, {"title": "4.3 Performance over Small Datasets", "content": "In the overall experiments, both the pre-training and fine-tuning steps are executed on the full training dataset. However, in practical applications, MapRL models are often trained on abundant data while fine-tuned using limited labeled samples for downstream tasks. To assess the robustness of mainstream models under such conditions, we conduct a small dataset experiment.\nIn this experiment, we reduce the fine-tuning data size from 50% to 10% of the original training set and compare the result of pre-built models in VecCity. POI-oriented models are evaluated on the Tokyo dataset, while road-segment and land-parcel-oriented models are tested on the Xi'an dataset. Figure 5 presents the evaluation result: Fig. 5 (a) to (c) show results for POI-oriented models. Fig. 5 (d) to (f) display results for road-segment-oriented models. Fig. 5 (g) to (i) provide results for land-parcel-oriented models. From these figures, we make following observations:\n1) Stability in Attribute Inference Tasks: MapRL models show stable performance across varying training data sizes in attribute inference tasks, such as POI Classification (POIC) in Fig. 5 (a), Average Speed Inference (ASI) in Fig. 5 (d), and Land Parcel Classification (LPC) in Fig. 5 (g). This stability arises because these tasks focus on inferring entity attributes, which are effectively captured during pre-training. As a result, the fine-tuning phase requires only a small amount of labeled data to maintain good performance.\n2) Fluctuations in Trajectory-Related Tasks: Models exhibit significant performance fluctuations in trajectory-related tasks, such as Next POI Prediction (NPP) in Fig. 5 (b), Trajectory User Link (TUL) in Fig. 5 (c), and Travel Time Estimation (TTE) in Fig. 5 (e). These tasks rely on sequence-level predictions, while encoders and pre-training tasks focus on generating single-vector representations for individual entities. This mismatch makes trajectory-related tasks"}, {"title": "4.4 Comparing of Modified Models", "content": "In this section, we conduct experiments to evaluate the influence of pre-training tasks and encoder structures on the performance of downstream tasks. We modify three representative MapRL models - CLTE, JCLRNT, and HREP \u2013 which achieve the highest average performance in POI, road segment, and land parcel representation learning, respectively. Owing to VecCity's modular design, specific pre-training tasks can be easily removed from models, and new pre-training tasks can be seamlessly integrated.\nThe experimental results are shown in Tab. 6-8. In the figures, the black \"/\" and \"X\" indicate whether the corresponding pre-training tasks were originally included or excluded when the model was first proposed. In contrast, the red \"/\" and \"X\" indicate newly added or removed pre-training tasks. Additionally, we evaluate the impact of different backbone models by modifying the core architecture of encoders (highlighted in red in the \"Encoder Core Arc\" row). In the last row, we present the performance ranking of each variant for easier comparison and conclusion.\nComparison of CTLE Variants. Among POI MapRL models, CTLE achieves the best performance. In Tab. 6, we experiment with the following model variants for CTLE. (a) CTLE: Maintains the original model architecture. (b) CTLE%: Removes the Masked Trajectory Recovery (MTR) pre-training task. (c) CTLE&: Removes the Trajectory Prediction (TrajP) pre-training task. (d) CTLE*: Integrates a new Augmented Trajectory Contrastive Learning (ATrCL) pre-training task, using the ATrCL task proposed by CACSR, which achieved the second-best performance in the overall POI experiments. (e) CTLE$: Replaces the core architecture of CTLE (a Transformer encoder) with an LSTM layer.\nThe downstream task results are presented in Tab. 6. From these experiments, we draw the following observations: i) New Task Enhances Performance: The CTLE* variant achieves the best performance across all downstream tasks, indicating that integrating the new ATrCL task significantly improves the model's capability. ii) MTR Provides Richer Representations: The CTLE& variant outperforms CTLE%, suggesting that the MTR task offers richer representation capabilities than the TrajP task. iii) Transformer Encoder is Essential for Trajectory Tasks: The performance of CTLE$ drops by 59% in the Next POI Prediction (NPP) task and 60% in the Trajectory User Link (TUL) task, showing that the Transformer Encoder (TE) plays a crucial role in trajectory-related tasks.\nComparison of JCLRNT Variants. Among road segment MapRL models, JCLRNT achieves the best performance. As shown in Tab. 7, we experiment with the following JCLRNT variants. (a) JCLRNT: Maintains the original model architecture. (b) JCLRNT%: Removes the ATrCL pre-training task. (c) JCLRNT&: Removes the Augmented Graph Contrastive Learning (AGCL) pre-training task. (d) JCLRNT*: Adds a new Token Relation Inference (TokRI) pre-training task, using road type classification, as implemented in HRoad. (e) JCLRNT$: Replaces the Transformer with an LSTM as the sequence encoder.\nThe performance results on downstream tasks are presented in Tab. 7. From the experimental results, we draw the following observations: i) Comprehensive Pre-training Boosts Performance: JCLRNT* achieves the best performance across all downstream tasks by incorporating three pre-training tasks-TokRI, AGCL, and ATrCL-each aligned with the token-based, graph-based, and sequence-based encoders, respectively. This suggests that each encoder type benefits from having a tailored pre-training task. ii) ATrCL Provides Richer Representations: JCLRNT% underperforms compared to JCLRNT& in trajectory-related tasks like Travel Time Estimation (TTE) and Similarity Trajectory Search (STS). This indicates that ATrCL, a sequence-based pre-training task, offers richer representation capabilities than AGCL for these tasks. Even with a sequence encoder, the absence of ATrCL limits the model's ability to capture"}, {"title": "5 RELATED WORK", "content": "Review for MapRL. Pervious reviews [2, 28, 69] discuss the applications of deep learning in urban data mining, providing a detailed overview of data, tasks, and deep learning models, particularly those that integrate sequence and graph encoders. However, their focus is on end-to-end models, while our work concentrates on pre-trained representation learning methods. Other studies [8, 86] focus on data mining methods for trajectory data but do not cover pre-trained representation learning. Survey [9] introduces a type-based taxonomy for MapRL where models are systematically reviewed based on the types of map entities they target. In contrast, our work does not classify methods by entity type but instead focuses on summarizing general techniques applicable to all map entities, i.e., a method-based taxonomy. This broader perspective helps uncover common design principles underlying MapRL methods.\nBenchmark for MapRL. In recent years, the growing interest in MapRL has created a strong demand for open benchmarks to comprehensively analyze baseline models. Open-source benchmarking is valuable not only for advancing research but also for enabling practitioners to easily evaluate models and apply them to open datasets. While significant progress has been made in benchmarking for multivariate time series prediction [51, 57, 67, 70], computer vision [14, 29, 34], and neural language processing [13, 50, 54, 65], benchmarking in the MapRL field remains largely unexplored. To the best of our knowledge, VecCity is the first benchmark specifically for MapRL, allowing researchers to conduct detailed comparative experiments and develop new models.\n6 CONCLUSION\nTo overcome the challenges of fragmentation and the lack of standardized benchmarks in MapRL, we introduced a novel taxonomy organized by functional modules rather than entity types. Building on this taxonomy, we developed VecCity, a modular library offering interfaces for encoding, pre-training, fine-tuning, and evaluation. Using VecCity, we established the first standardized benchmarks by reproducing 21 mainstream models and integrating datasets from nine cities. Moreover, we also conduct an in-depth evaluation of mainstream MapRL models. Our comprehensive experiments highlight the impact of pre-training tasks and encoder architectures, demonstrating the advantages of combining multiple components. VecCity provides a unified framework that promotes reusability, streamlines experimentation, and advances research in MapRL. Although VecCity currently supports most mainstream MapRL models, we plan to expand it further to cover a wider range of pre-trained spatiotemporal data representation learning algorithms, such as trajectory representation learning."}]}