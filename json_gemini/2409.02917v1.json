{"title": "UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views", "authors": ["Jiaxin Guo", "Jiangliu Wang", "Ruofeng Wei", "Di Kang", "Qi Dou", "Yun-hui Liu"], "abstract": "Visualizing surgical scenes is crucial for revealing internal anatomical structures during minimally invasive procedures. Novel View Synthesis is a vital technique that offers geometry and appearance reconstruction, enhancing understanding, planning, and decision-making in surgical scenes. Despite the impressive achievements of Neural Radiance Field (NeRF), its direct application to surgical scenes produces unsatisfying results due to two challenges: endoscopic sparse views and significant photometric inconsistencies. In this paper, we propose uncertainty-aware conditional NeRF for novel view synthesis to tackle the severe shape-radiance ambiguity from sparse surgical views. The core of UC-NeRF is to incorporate the multi-view uncertainty estimation to condition the neural radiance field for modeling the severe photometric inconsistencies adaptively. Specifically, our UC-NeRF first builds a consistency learner in the form of multi-view stereo network, to establish the geometric correspondence from sparse views and generate uncertainty estimation and feature priors. In neural rendering, we design a base-adaptive NeRF network to exploit the uncertainty estimation for explicitly handling the photometric inconsistencies. Furthermore, an uncertainty-guided geometry distillation is employed to enhance geometry learning. Experiments on the SCARED and Hamlyn datasets demonstrate our superior performance in rendering appearance and geometry, consistently outperforming the current state-of-the-art approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "Minimally invasive surgery (MIS) has achieved a significant advancement in modern surgical practices. It reduces surgical trauma, lessens post-operative discomfort, and shortens recovery time [2]. The endoscope allows surgeons to inspect internal structures, facilitating precise navigation through complex anatomical landscapes [3]. However, the 3D perception of the endoscope is impeded by the limited range of viewpoint changes inherent to endoscopic procedures, as well as the limited view area and two-dimensional imaging. These limitations make it challenging for surgeons to perceive depth and fully assess the surrounding conditions, which can hinder their understanding of the internal anatomy and potentially impact the effectiveness of surgical interventions. Additionally, traditional visualization systems in surgery, such as Ultrasound, Magnetic Resonance Imaging (MRI), or Computed Tomography (CT), increase the cost and complexity of medical imaging [6]. While 3D reconstruction methods allow surface geometry learning, they fall short of predictive ability and flexibility in exploring and visualizing surgical scenes from different perspectives with high-fidelity details.\nUtilizing endoscopic multi-view images as input, the novel view synthesis technique generates photo-realistic free-view images of surgical scenes and intricate abdominal structures. This technique provides advantages for various applications, including virtual reality interactions, intra-operative surgical navigation, and autonomous robotic surgery [7]\u2013[9]. In this area, the Neural Radiance Fields (NeRF) approach [1] has shown remarkable success. It synthesizes novel viewpoints from dense sets of input images using implicit volumetric representations, paving the way for an era of visually immersive and interactive surgical practices.\nWhile NeRF has shown remarkable effectiveness in handling natural images, its application in surgical scenes often results in subpar rendering, due to two challenges: endoscopic sparse views and photometric inconsistency, as illustrated in Fig. 1. Endoscopic sparse views are primarily due to constrained camera movement and drastic motion. The movement of the endoscope is inherently restricted by the narrow internal spaces within the body and the limited flexibility of the endoscope. These constraints confine the camera to a limited number of views, making it difficult to observe the same spot from different viewpoints. Moreover, when surgeons maneuver the endoscope, drastic movements under low camera frame rates can cause intermittent visibility and motion blur, hindering the acquisition of clear and consecutive frames. Both two factors reduce the overlap between captured views, leading to the sparsity of useful visual data. This is particularly challenging for NeRF, which relies on dense views for accurate 3D geometry inference and novel view synthesis. Besides, surgical scenes frequently exhibit considerable photometric inconsistencies, due to non-Lambertian reflections and fluctuating illumination. Such inconsistencies complicate the accurate capture of scene geometry and appearance, leading to potential artifacts and incorrect density distributions in NeRF synthesized views due to its fundamental design around minimizing RGB error. These factors contribute to a pressing question: How can we enhance NeRF's ability to handle the severe shape-radiance ambiguity problem caused by the endoscopic sparse views and photometric inconsistency?\nTo address this problem, we take the inspiration from few-shot NeRF to solve the problems induced by endoscopic sparse view. Some methods are proposed for few-shot NeRF to enhance the rendering performance and reduce the shape-radiance ambiguity, which could be roughly categorized into two classes: NeRF with pre-training [5], [10], [11] and NeRF with geometry guidance [4], [12]\u2013[15]. The former class requires large datasets to pre-train the generalizable NeRF and fine-tune it on similar target scenes. The latter class employs geometric information (depths, normals, pointclouds) to supervise and regularize the neural rendered depth or control the ray sampling range to get rid of outliers. While these methods achieve improvements compared to NeRF, they ignore the extent of photometric inconsistencies and directly inject the constraints equally in spatial, leading to unsatisfying performance as presented in Fig. 2. To tackle the severe shape-radiance ambiguity in surgical scene, we aim to explicitly detect the photometric inconsistency from the multi-view inputs and enable the neural radiance fields to adaptively model the regions with the uncertainty estimation.\nIn this paper, we propose a new network for surgical novel view synthesis, to empower NeRF with great robustness and efficiency to tackle the challenging surgical scene with inconsistencies and sparse views, i.e. UC-NeRF. The key novelty of our UC-NeRF lies in incorporating the multi-view uncertainty information with geometry and appearance priors to condition the neural radiance field for improved accuracy and robustness to sparse views. Our network has three essential designs to exploit the uncertainty information: i) A consistency learner to build the geometry correspondence and learn the uncertainty estimation across sparse multi-views. ii) An uncertainty-aware dual-branch NeRF designed with base-adaptive architecture utilizing the uncertainty information to handle photometric inconsistencies and solve shape-radiance ambiguity. iii) The distillation from monocular geometry priors to optimize the neural rendering with the uncertainty guidance to improve the accuracy in rendered depth. We validate our proposed approach on the SCARED [16] and Hamlyn datasets [17]\u2013[19]. The extensive experiments demonstrate the state-of-the-art performance of UC-NeRF in novel view synthesis from sparse endoscopic views, with consistent improvement in effectiveness and efficiency.\nIn summary, our contributions are three folds: 1) To the best of our knowledge, we are the first to present a NeRF-based method addressing the challenging problem of novel view synthesis from endoscopic sparse images. 2) We devise an uncertainty-aware dual-branch NeRF to exploit the learned"}, {"title": "II. RELATED WORKS", "content": "Novel View Synthesis. Novel view synthesis focuses on generating new images or views of a scene from viewpoints not captured in the original imagery. Traditionally, novel view synthesis is based on geometric methods like image-based rendering [20]\u2013[22] and light field rendering [23]\u2013[26], which require precise camera calibration and struggle with complex scenes. The integration of deep learning allows a significant advancement for more realistic view generation [27]\u2013[29]. A major breakthrough in this area was the development of NeRF [1], which used a fully connected deep network to model volumetric scene functions, greatly enhancing the quality of synthesized views. Following NeRF, various extensions have emerged by improving speed, quality, and generalization [4], [10], [11], [13], [14].\nNeRF from Surgical Scenes. While NeRF has shown its potential in novel view synthesis, current approaches focus on 3D reconstruction using neural implicit fields from stereo endoscope videos. EndoNeRF [30] explores neural rendering for deformable tissue reconstruction from stereo endoscope inputs and devised a mask-guided ray-casting strategy to address the tool occlusion challenge. Sun et al. [31] propose a depth estimation network and a reconstruction network utilizing neural radiance fields for dynamic reconstruction. EndoSurf [32] proposes to model the deformation, geometry, and appearance separately to represent the deforming surfaces. Unlike these approaches that focus on single-view stereo video reconstruction, to the best of our knowledge, we are the first to present a sparse-view NeRF approach to tackle the challenge of novel view synthesis from endoscopic sparse views.\nFew-shot Neural Rendering. Recently, the few-shot NeRF techniques that utilize sparse views for novel view synthesis have opened up new possibilities for improving the robustness and efficiency of NeRF, which can be classified into two classes: NeRF with pre-training and NeRF with geometry guidance. NeRF with pre-training in the first category pretrain a generalizable NeRF across multiple large datasets before fine-tuning on specific targets. PixelNeRF [10] leverages CNN features from input images to predict a continuous neural scene representation. IBRNet [33] introduces a network architecture to estimate radiance and volume density with appearance information from multiple source views. MVSNeRF [11] takes usage of cost volumes to reason the prior features to enhance the neural radiance field reconstruction. GeoNeRF [5] incorporates a transformer-based attention mechanism with volume rendering to manage complex occlusion conditions. However, these methods are prone to degeneration during significant data domain shifts, such as surgical scenes that differ vastly from general ones. Moreover, the need for extensive and diverse pre-training datasets results in substantial time and computational costs. In this paper, we advocate the conditional NeRF by exploiting features and uncertainty estimation from sparse source images and realize generalizable training on the multiple surgical scenes, outperforming previous methods even without fine-tuning. NeRF with geometry guidance focuses on guiding NeRF training with geometric information. For example, NerfingMVS [13] takes MVS depth from COLMAP [34] as a prior and employs a monocular depth network to guide NeRF optimization. RegNeRF [14] regularizes geometry and appearance from unobserved viewpoints, refining ray sampling space over time. DS-NeRF [15] uses sparse 3D points from Structure-from-Motion (SfM) to monitor and adjust NeRF ray termination, enabling faster training with sparse views. MonoSDF [4] integrates multiple monocular geometric priors into neural implicit surface reconstruction. Roessle et al. [12] upsample SfM sparse points into dense depth maps to guide NeRF optimization. SparseNeRF [35] employs a local depth ranking regularization and a spatial continuity regularization to distill the depth priors. ConsistentNeRF [36] learns the 3D consistency by leveraging depth information to regularize the multi-view and single-view among pixels. Despite these advances, they ignore the extent of photometric inconsistencies and inject the constraints equally in spatial. Therefore, the injected hard geometry constraints easily cause over-fitting in sparse training data, leading to degeneration in test novel views. Our proposed UC-NeRF introduces an uncertainty-aware conditional NeRF to address these challenges, by leveraging the uncertainty information to model the 3D surgical scene and guide the optimization.\nUncertainty Estimation in NeRF. Uncertainty estimation has been adopted in diverse areas of computer vision to improve the interpretability and reduce the risk of the model. NeRF-W [37] leverages the uncertainty to tackle the transient object problem. S-NeRF [38] learns a probability distribution to quantify the uncertainty associated with the scene information. CF-NeRF [39] introduces latent variable modeling and conditional normalized flow to incorporate uncertainty quantification into NeRF. ActiveNeRF [40] incorporates the uncertainty estimation into a NeRF model by modeling the radiance values as a Gaussian distribution. In this paper, we aim to address the challenge of novel view synthesis from endoscopic sparse views. Unlike previous works, we incorporate the multi-view uncertainty from the consistency learner with dual branch NeRF, to explicitly handle the photometric inconsistencies from endoscopic sparse views."}, {"title": "III. UNCERTAINTY-AWARE CONDITIONAL NERF", "content": "Given a set of sparse input images {$I_1,...,I_N$} and their corresponding camera parameters {$\\Phi_1,...,\\Phi_N$} as input, our goal is to reconstruct a radiance field that can faithfully capture the view-independent effects and the underlying true geometry, so that we can volume-render an image from any novel viewpoint and thus facilitate the diagnosis. Mathematically, this process is denoted as follows:\n$\\displaystyle (c, \\sigma) = UC-NeRF(\\gamma(x), \\gamma(d); I_i, \\Phi_i), \\quad \\quad(1)$\nwhere $\\gamma$ is the encoding function to map position x and view direction d to a higher dimensional space. Through conditional inputs, our method enables generalizable training across multiple surgical scenes, thereby promoting training efficiency and robustness.\nAs in Fig. 3, UC-NeRF contains two major components: 1) a consistency learner that builds the geometry correspondences across multi-view inputs and generates an uncertainty map (Sec. III-B); and 2) a conditional NeRF that enables uncertainty-aware neural radiance fields reconstruction (Sec. III-C). To maximize generalization capability and reduce errors caused by shape-radiance ambiguity, we propose to distill geometric priors from the estimated sparse SfM points and a monocular depth estimator into our UC-NeRF (Sec. III-D), resulting in further improved robustness and accuracy in the rendered depth.\nA. Preliminaries\nWe first briefly introduce some NeRF basics [1] which are used in this paper. NeRF takes as input a set of posed images and represents a scene as a continuous volumetric function parameterized by MLPs. Given a 3D point $x \\in \\mathbb{R}^3$ and a viewing direction $d \\in \\mathbb{R}^2$, NeRF learns to map from $(x, d)$ to the volume density $\\sigma$ and the emitted color $c = (r, g, b)$:\n$\\displaystyle (c, \\sigma) = MLP(\\gamma(x), \\gamma(d)). \\quad \\quad (2)$\nThe color of an image pixel is calculated with the volume rendering [1]. Specifically, a ray $r(t) = o + td$ is determined by the camera origin o and the pixel location p, where d is the unit direction vector passing from the camera origin o to the pixel p, t is the distance of a sampling point to the origin on this ray. The volume rendering equation [1] to obtain a pixel's color is defined as follows:\n$\\displaystyle \\hat{C}(r) = \\sum_{i=1}^N (1 - exp(-\\sigma_i \\delta_i))c_i,\\quad \\quad(3)$\nwhere $T_i = exp(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j), \\delta_i = t_{i+1} - t_i,$\nwhere N is the number of sample points along each ray, $\\sigma_i$ is the density value of point $x_i$, $\\delta_i$ is the distance between two consecutive sample points along the ray, $T_i$ is the accumulated transparency from the camera origin. Following [1], [12], the depth of an image pixel can be similarly calculated by integrating every sample point's distance to the camera origin:\n$\\displaystyle D(r) = \\sum_{i=1}^N T_i(1 - exp(-\\sigma_i \\delta_i))t_i.\\quad \\quad (4)$\nWith the above differential volume rendering process, NeRF optimizes the radiance fields by minimizing the reconstruction error between the rendered color and the ground truth color:\n$\\displaystyle L_{rgb} =|| \\hat{C}(r) - C(r) || .\\quad \\quad (5)$\nHowever, NeRF reconstruction degrades drastically, especially the geometry part, due to the sparse endoscopic views and the photometric inconsistencies across images caused by varying lighting conditions during the surgical operation.\nB. Consistency Learner\nIn UC-NeRF, a consistency learner is utilized to exploit robust geometric information from sparse SfM depth and learn the consistency. Specifically, it adopts the CasMVSNet [41]"}, {"title": "C. Uncertainty-aware dual-branch NeRF", "content": "We propose a dual-branch NeRF utilizing the uncertainty information to explicitly handle photometric inconsistencies caused by moving light and non-Lambertian surfaces. Specifically, our dual-branch NeRF contains a base branch $MLP_b$ spatially weighted by the confidence score (i.e. 1 - uncertainty score), which aims at modeling view-consistent appearance and geometry in the surgical scene, and an adaptive branch $MLP_a$ weighted by the uncertainty score, which aims at modeling view-dependent effects and details, i.e. illumination variations, non-Lambertian texture. The reason is that regions with higher uncertainty values from the depth estimator usually lack consistency across neighboring multi-views, which are supposed to be modeled by the adaptive branch.\n1) Base-Adaptive Rendering Network: The NeRF is conditioned on the feature priors from the consistency learner to generalize in different scenes with better efficiency and rendering performance (see Fig. 7). Concretely, we directly build the cost volume upon the target (i.e. novel) view frustum to collect more \"original\" appearance and geometry features from the target view rather than highly processed abstract features from other views. This is different from MVSNeRF [11], which builds cost volume upon the neighboring input view and then warps the 3D points to this different view for sampling condition features. This modification of the cost volume reconstruction process results in better geometry accuracy and improved image rendering performance (See Tab. I).\nSpecifically, a sample point is warped to input views to obtain appearance prior (i.e. {$I_i(x_{oi})$}$_{i=1}^N$) and geometry prior $S^{(k)}(x)_{k=0}^2$. Note that $S^{(k)}$ is a 3D volume space in NDC coordinate frame and we use trilinear interpolation. Mathematically, the condition input $F_{base}(x)$ and the conditional NeRF are defined as:\n$\\displaystyle F_{base}(x) = Concat(\\left{S^{(k)}(x)\\right}_{k=0}^2, \\left{I_i(x_{oi})\\right}_{i=1}^N), \\quad \\quad(11)$\n$\\displaystyle h = MLP_h(\\gamma(x), F_{base}(x)), \\quad \\quad(12)$\nwhere h is the shared latent feature used by both base and adaptive branches.\n$\\displaystyle c^b, \\sigma^b = MLP_{o^b}(h), \\quad \\quad(13)$\n$\\displaystyle c^a, \\sigma^a = MLP_{o^a}(h, \\gamma(d), F_{color}(x)), \\quad \\quad(14)$\nwhere $F_{color}(x) = {F^{(2)}(x_{oi})}_{i=1}^N$      (15)\nThe density $\\sigma^b$ and color $c^b$ of the base branch is decoded solely by h without viewing vector input since this branch is designed to model the underlying true geometry and diffuse colors. As for the adaptive branch, both its density $\\sigma^a$ and color $c^a$ are dependent on view direction $\\gamma(d)$ since it is designed to model the inconsistencies. Note that we also use image feature as input since we find it is helpful experimentally possibly due to its robustness, i.e. deep features with large enough context information. Specifically, we use the features {$F^{(2)}(x_{oi})$}$_{i=1}^N$ from the last layer of the FPN network.\n2) Uncertainty-aware Adaptation: Since the uncertainty measure reflects the geometric reliability and photometric inconsistency of each point, it is suitable to be used as weight to balance and control the contribution of the two branches. Specifically, radiance fields predicted by the two branches are spatially weighted summed according to uncertainty score U(r) as follows:\n$\\displaystyle c = c^b \\cdot (1 \u2013 U(r)) + c^a \\cdot U(r), \\quad \\quad(16)$\n$\\displaystyle \\sigma = \\sigma^b \\cdot U(r) + \\sigma^a \\cdot (1 \u2013 U(r)). \\quad \\quad(17)$\nThe final image is volumetric rendered following Eq. (3) and Eq. (4). Under this setting, in color prediction, the adaptive branch $c^a$ contributes more to regions with higher uncertainty, capturing view-dependent photometric effects that vary significantly with different viewing angles. The base branch $c^b$ is more influential in regions with higher confidence, where the appearance is stable and predictable. Conversely, in density prediction, the base branch $\\sigma^b$ is dominant in regions with higher uncertainty. This helps maintain geometric reliability and multi-view consistency, avoiding shape-radiance ambiguity which can lead to degenerated reconstructions. For regions with higher confidence and fewer directional variations, the adaptive branch $\\sigma^a$ could contribute more, providing additional textures and details to enhance the reconstruction quality (See Tab. V).\nDiscussion. Following this adaptation, the base branch provides reliable and accurate geometry information to reduce the uncertainty and improve the robustness for these rays (See Fig. 6). In color rendering, sampled rays with higher uncertainty, prone to complex photometric effects, are weighted towards the adaptive branch to compensate for more view-dependent details (See Fig. 9). With the uncertainty map modulating the base and adaptive branches, our UC-NeRF is enabled to construct the neural radiance field with spatial uncertainty, facilitating more stable and controllable learning to solve the shape-radiance ambiguity. With the balance modulated in the two branches, the differential uncertainty-aware adaptation is capable of synchronizing the uncertainty learned by multi-view stereo with the neural rendering process."}, {"title": "D. Distillation from Monocular Geometry Priors", "content": "To further improve the geometry consistency, we exploit the geometry priors from monocular images to guide the training of UC-NeRF for scale-aware depth learning. We employ a two-fold approach: 1) Scale distillation from SfM. 2) Uncertainty-guided monocular depth distillation.\n1) Scale Distillation: To preserve the scale consistency among sparse views, We first incorporate the sparse depth from SfM, which intrinsically preserves real-world scale. This sparse depth is used to guide our method in learning the correct scale of the scene through a scale distillation Loss. Similar to Eq. (9), to alleviate the negative influence of inaccurate depth, we adopt the weight $exp(-(w/\\overline{w})^2)$ to suppress the supervision from unreliable depth estimations (i.e. with larger reprojection errors):\n$\\displaystyle L_{scale} = \\sum_{r \\in R} exp(-(w/\\overline{w})^2) || D(r) \u2013 D_{sfm}(r) ||_1,\\quad \\quad(18)$\nwhere R denotes the sampled set of pixels in the region where SfM depth is available. We minimize the L\u2081 loss between the rendered depth D(r) and the SfM sparse depth $D_{sfm}$, with SfM reprojection error w to weight the loss.\n2) Uncertainty-guided Mono-Depth Distillation: To optimize the region where SfM is sparse or unavailable, we leverage a monocular depth estimation model, Dense Prediction Transformer (DPT) [44], to guide our UC-NeRF. We introduce an uncertainty-guided mono-depth distillation, taking the uncertainty map as a reference to employ different losses spatially for depth supervision to sampled patches.\nWe first sample image patches in the region with high uncertainty denoted as $\\Omega_h$. Since the constraint of SfM sparse point is not enough for the high uncertainty region $\\Omega_h$, we apply a scale-invariant depth gradient loss $L_{grad}$ to supervise the gradient difference between rendered depth D with the monocular depth $D_{dpt}$.\n$\\displaystyle L_{grad} = \\sum_{M \\in \\Omega_h} \\sum_{r \\in M} || \\nabla D_{dpt}(r) - (\\nabla (D(r) s + q)) ||_1, \\quad \\quad(19)$\nwhere s and q represent the scale and shift computed by linear least squares to convert the patches to the same scale following [45]. Conversely, low uncertainty regions $\\Omega_l$ contain more view-consistent correspondences with reliable depth, where the edge-aware smooth loss is employed as a regularization term to refine the continuity of the rendered depth.\n$\\displaystyle L_{reg} = \\sum_{M \\in \\Omega_l} \\sum_{r \\in M} exp(-\\beta || \\nabla D_{dpt}(r) ||) || \\nabla D(r) ||_1, \\quad \\quad(20)$\nwhere $\\beta$ denotes a hyperparameter to control the smooth extent. The exponential term serves as a weight that decreases as the depth gradient from DPT increases to preserve the edges, thereby promoting smoothness and continuity in low-uncertainty regions.\nThrough this uncertainty-guided mono-depth distillation, our method exploit the monocular geometry priors in the scene and provides balanced supervision with the uncertainty map as the reference. Our method not only incorporates detailed geometry and scale cues from single-view depth prediction but also aligns its understanding with the global, scale-aware information from SfM. It allows our model to effectively deal with various situations in the scene, enhancing the stability and performance of depth rendering.\n3) Training Loss: The total training loss for UC-NeRF is formulated by:\n$\\displaystyle L = \\lambda_1 L_{rgb} + \\lambda_2 L_{con} + \\lambda_3 L_{scale} + \\lambda_4 L_{grad} + \\lambda_5 L_{reg}, \\quad (21)$\nwhere $\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4, \\lambda_5$ denote the loss weights. In practice, we set $\u03bb_1$ = 10, $\u03bb_2$ = 0.5, $\u03bb_3$ = 0.5, $\u03bb_4$ = 0.5, $\u03bb_5$ = 0.05."}, {"title": "IV. EXPERIMENTS", "content": "A. Implementation Details\n1) Experimental Setup: The code of our method is based on PyTorch, running on NVIDIA GeForce RTX 3090 GPU. We process rays for scale distillation in a batch size of 1024, and patches for guided patch sampling with size of 6 \u00d7 6 in batch size of 50. We use Adam Optimizer for our network, with a learning rate of 6\u00d710\u22124 with a cosine decay scheduler. For the sampling points on a ray, we adopt 90 points during training and inference for efficiency. For the consistency learner, we utilize a pre-trained Cas-MVSNet model [41], with the depth hypothesis planes decreasing as (48, 32, 8) in three stages. All the experiments and comparisons use the image size of 320 \u00d7 256. To compare with NeRF-based methods which require no pre-training, we train the model from scratch. For NeRF with pre-training methods, we utilize their released pre-trained model and train the fine-tuned model on each scene.\n2) Datasets: We train our method on the SCARED Dataset [16] and Hamlyn Dataset [17]\u2013[19], which contain challenging endoscopic scenes with weak textures, reflections, and occlusions. Following the preprocessing in [12], after filtering out the frames with motion blur or flaws, we obtained 9 and 6 scenes from the SCARED and Hamlyn datasets respectively. Specifically, we first use a sliding window approach and extract the sharpest image from every N frame. Images with severe occlusions are also removed to avoid introducing noise into the training process. Next, we downsample the dataset to enhance computational efficiency and avoid redundancy from very similar frames. Specifically, we manually select frames to ensure each frame provides unique coverage of the surgical scene, with an overlap of approximately 60-80% between consecutive frames following [12]. This step leads to reduced data size, maintaining a balance between comprehensive scene coverage and training efficiency.\nFor Hamlyn Dataset, we follow the volumetric reconstruction part of Endo-Depth-and-Motion [46] to specifically focus on static scenes with larger camera translation to validate our method. Note that the collected scenes only contain monocular images, taken from the left camera view for both SCARED and Hamlyn Dataset. We use COLMAP [47] to preprocess the datasets to get the camera poses and sparse pointclouds following [1], [15]. We obtain the monocular depth from DPT [44] as priors to guide our UC-NeRF. Similar to the data split in [1], [13], [14], we collect 20 images covering a local area for each scene and hold out 1/2 of these views for testing and the remaining for training. During training, we take 7 closest neighboring views as source views for the consistency learner to extract features and 3D neural volumes.\n3) Metrics: We report the quantitative comparisons of appearance and geometry to show the synthesis performance, including the mean and standard deviation across the test novel views. We further highlight the table cell to show best, second best, and third best. We adopt the PSNR, SSIM [48] and LPIPS [49] for assessing image synthesis quality. To evaluate the depth results, we scale the predicted depth maps"}, {"title": "V. DISCUSSION", "content": "The objective of this paper is to improve NeRF's ability to handle the shape-ambiguity problem caused by challenges in surgical scenes, such as endoscopic sparse views and photometric inconsistencies. Our method is the first to address the challenging sparse view NeRF problem in surgical scenes. The proposed UC-NeRF is both effective and efficient in surgical novel view synthesis because of three main reasons. First, the view-specific uncertainty information is estimated through the consistency learner guided by the sparse SfM depth to measure the extent of photometric inconsistency across multi-view inputs (See Fig. 4). Moreover, our method is capable of generic training across multiple surgical scenes thanks to the conditional inputs from the consistency learner. Second, unlike other few-shot NeRF methods to inject the unified constraints, we take benefits of the learnt uncertainty information to explicitly model the regions in the designed dual-branch NeRF, i.e. fusing the base and adaptive branch with different weight to generate view-dependent appearance and consistent geometry. This design is further demonstrated in Fig. 9 to visualize the different modeling effects decomposed in the two branches. Third, we further introduce the distillation from monocular geometry priors to improve the accuracy and robustness of the rendered depth (see Fig. 6). Specifically, the scale distillation improves the scale consistency through the sparse SfM depth constraints. The uncertainty-guided mono-depth distillation employ the supervision and regularization adaptively in spatial following the uncertainty estimation.\nDespite the aforementioned strengths, our method is limited to static or semi-static surgical scene, which hinders its application to the highly dynamic surgical scenes. In the future work, we will try to integrate time dimension to enable the spatialtemporal NeRF, allowing both time-varying and view-free rendering for color and depth. More efficient neural representations are also considered to integrate with our method to improve the efficiency for real-time rendering, which benefits the downstream tasks like surgical navigation, 3D reconstruction, surgical skill learning, etc."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, the UC-NeRF network addresses the difficulties posed by surgical sparse views and photometric inconsistencies, achieving robust and efficient novel view synthesis in minimally invasive surgery. By leveraging a consistent learner and an uncertainty map, UC-NeRF enhances geometric correspondence and significantly reduces shape-radiance ambiguity. The introduction of an uncertainty-aware conditional NeRF also refines the learning process of view-dependent appearances, ensuring a balance between geometric precision and photorealistic rendering. Our experimental evaluation shows that our method outperforms other few-shot NeRF methods in both efficiency and effectiveness."}]}