{"title": "Memory Networks: Towards Fully Biologically Plausible Learning", "authors": ["Jacobo Ruiz", "Manas Gupta"], "abstract": "The field of artificial intelligence faces significant challenges in achieving both biological plausibility and computational efficiency, particularly in visual learning tasks. Current artificial neural networks, such as convolutional neural networks, rely on techniques like backpropagation and weight sharing, which do not align with the brain's natural information processing methods. To address these issues, we propose the Memory Network, a model inspired by biological principles that avoids backpropagation and convolutions, and operates in a single pass. This approach enables rapid and efficient learning, mimicking the brain's ability to adapt quickly with minimal exposure to data. Our experiments demonstrate that the Memory Network achieves efficient and biologically plausible learning, showing strong performance on simpler datasets like MNIST. However, further refinement is needed for the model to handle more complex datasets such as CIFAR-10, highlighting the need to develop new algorithms and techniques that closely align with biological processes while maintaining computational efficiency.", "sections": [{"title": "Introduction", "content": "The pursuit of algorithmic efficiency in artificial intelligence is increasingly drawing inspiration from the brain's remarkable capabilities. The human brain, with its approximately 20 watts of power consumption [3], exemplifies an unparalleled level of efficiency in managing and processing information, especially when compared to the staggering energy demands of modern AI models. For instance, training a large-scale model like GPT-3 [4] is estimated to require 1,287 MWh of electricity [21]\u2014a quantity of energy that the human brain would consume over approximately 7,346 years. This stark contrast highlights a critical need to explore and approximate the efficiency of the brain in our computational systems, particularly in areas like visual learning where the parallels between biological and artificial systems are most evident.\nDespite their inspirations from the brain, current deep learning models, such as convolutional neural networks (CNNs) [16] and other deep networks, exhibit key differences from their biological counterparts. These models are typically trained using backpropagation-a method that, while effective, lacks biological plausibility due to its reliance on global error signals and non-local computations. Biological neural networks, by contrast, learn through more localized and distributed processes which do not require global coordination or precise error feedback mechanisms.\nThe significance of bridging this gap between biological and artificial systems is underscored by the potential gains in computational efficiency. The brain's ability to process complex sensory information and adapt through plasticity mechanisms, without relying on backpropagation, suggests that biologically inspired approaches could yield more efficient and scalable AI models [15]. This is particularly relevant in visual learning tasks, where the hierarchical organization [11] and local connectivity of neurons in the visual cortex offer a compelling blueprint for developing more energy-efficient and adaptive algorithms.\nBiological plausibility in neural network design entails several criteria: local computation, where neurons update based on nearby signals rather than global information [8]; no backpropagation of error signals, avoiding the need for non-local error propagation [7]; unsupervised or semi-supervised learning [25], which mirrors the brain's ability to learn from unstructured data without explicit labels; and rapid learning on a physiologically limited setup.\nSome approaches have been developed to make modern models more biologically plausible by addressing specific aspects of this challenge. For example, alternatives to the backpropagation algorithm [29], such as Hebbian learning, have been inspired by the neurobiology of neuronal function [10]. However, these models still only partially adhere to biological principles. For instance they rely on CNNs which use convolutions to mimic the brain's local receptive fields and hierarchical feature learning, but these convolutions rely on non-biological practices like weight sharing.\nTo address these limitations, we propose the Memory Network, an approach designed to more closely adhere to the principles of biological plausibility. During training, the model encodes data representations by updating each neuron based on the similarity between a new input and the average representations of previous inputs with the same label. This single-pass learning approach allows the model to quickly learn and store patterns, mimicking the brain's ability to adapt rapidly with minimal exposure. During prediction, the test input is compared to these learned representations, and the neuron with the closest match determines the output.\nThe Memory Network operates without backpropagation, relying instead on local plasticity mechanisms for learning, which aligns more closely with biological processes observed in the brain. By avoiding convolutions and traditional feature extraction methods, the model enhances its biological plausibility. It supports semi-supervised learning by depending only partially on labels, reflecting the brain's unsupervised learning capabilities. Moreover, the model's architecture is simpler than traditional neural networks, allowing it to train significantly faster without the need for extensive preprocessing techniques. This efficiency is largely due to the fact that training is completed in a single epoch, with each data point processed only once, setting it apart from other biologically plausible methods that require multiple passes through the data or complex preprocessing steps.\nIn summary, the Memory Network model represents an initiative to explore how to make AI systems more biologically plausible by adhering to principles based on biology. This approach paves the way for further research in developing AI models that operate more like the brain, promoting advancements in energy-efficient and biologically inspired computing."}, {"title": "Related Work", "content": null}, {"title": "Criteria for Biological Plausibility", "content": "Biological plausibility in neural network design involves several key principles that ensure these models more closely mimic the brain's natural processes. One important criterion is local computation, where neurons or units update their states and adjust synaptic weights based only on information available in their immediate vicinity. This mirrors the way biological neurons operate, with synaptic changes driven by local signals like neurotransmitter release rather than signals from the entire network [8].\nAnother crucial aspect is avoiding the propagation of global error signals across the entire network. Backpropagation, a foundational algorithm in deep learning [29], is widely regarded as not biologically plausible due to several key discrepancies between its operation in artificial neural networks (ANNs) and how learning occurs in biological neural networks. In biological systems, learning is governed by local mechanisms, where neurons adjust their synaptic strengths based on local activity and signals, without the need to propagate error gradients backward through the network. In contrast, backpropagation requires non-local information, such as error signals that traverse multiple layers, and assumes precise symmetry of weights between neurons during the forward and backward passes. These requirements are not found in biological neural networks, where synaptic changes are driven by local processes and lack such symmetric and globally coordinated weight updates [7].\nFurthermore, biological learning often happens without explicit supervision, relying on unsupervised or semi-supervised learning principles to interpret unstructured data. This approach mirrors the brain's remarkable ability to adapt and learn from its environment without requiring explicit labels or instructions [25].\nIn addition to these principles, single epoch-based learning is an important feature, reflecting the brain's capacity for rapid learning from limited exposure [30]. The brain often learns and adapts in real-time, making swift adjustments to new information without the need for repeated trials. This fast-paced learning and real-time processing and inference are essential for survival and effective functioning in dynamic environments."}, {"title": "Previous Work on Biologically Plausible Algorithms", "content": "A variety of studies have sought to develop learning algorithms that are more aligned with neurobiological principles, aiming to create models that better mimic the mechanisms observed in the brain. One prominent approach is Hebbian learning, based on the principle that formulated by Donald O. Hebb as \"neurons that fire together wire together\" [10]. This principle has been effectively implemented in models like those by Miconi et al., where a Hebbian network was used before a Convolutional Neural Network, achieving around 60% accuracy on the CIFAR-10 dataset [22][13]. Beyond Hebbian learning, other biologically inspired algorithms, such as Feedback Alignment, Direct Feedback Alignment, Difference Target Propagation, and Predictive Coding, have been explored as potential biologically plausible alternatives to backpropagation [18][24][17][27]. Hebbian learning has been shown to outperform backpropagation in accuracy in some cases, and most biologically inspired methods converge faster than backpropagation [9].\nHowever, many of these approaches, despite their grounding in neurobiology, often incorporate elements like convolutions or rely on backpropagation to achieve high performance [23] [14]. For example, in \"Hebbian Learning Meets Deep Convolutional Neural Networks,\" Amato et al. used a Hebbian-based method as a feature extractor in conjunction with a Convolutional Neural Network, reaching approximately 80% accuracy on CIFAR-10 [2].\nAnother promising approach in biologically plausible computation is Spiking Neural Networks (SNNs). SNNs emulate the natural information processing of the brain by using discrete spikes, or action potentials, for neuron communication, in contrast to the continuous activations used in traditional artificial neural networks. These networks incorporate time-dependent processes and more closely mimic the dynamics of biological neurons, offering a more realistic model for neural computation [20]. However, achieving high performance (98%-99% accuracy on MNIST) with SNNs often requires adaptations to backpropagation-like algorithms [12] [28].\nThe proposed algorithm, called the Memory Network, diverges from the aforementioned methods as it does not use backpropagation or convolutions. Unlike other biologically inspired models that serve primarily as feature extractors for subsequent processing stages, our Memory Network is designed as a self-contained, biologically plausible solution for visual classification tasks. It operates by rapidly processing data in a single pass, more closely mirroring how learning and memory function in biological neural systems."}, {"title": "Similar algorithms to Memory Networks", "content": null}, {"title": "Methodology", "content": null}, {"title": "Biological Motivation and Design Choices", "content": "The design of the algorithm and its components were inspired by biological neural processes:\n\u2022 Single Epoch Based Learning: The model is trained in a single pass through the data, reducing the need for repeated exposure to the same data, emulating how the brain quickly adapts and learns from new experiences with minimal repetition.\n\u2022 Dynamic Adaptation and Learning: The use of counters and latent representations allows neurons to dynamically adjust based on the frequency of input exposure, much like how biological neurons modify their synaptic strengths according to activity levels (Hebbian learning).\n\u2022 Simple Network Structure: The algorithm employs a multi-neuron structure with no layers, which is reminiscent of the initial layers of neural processing in the brain that handle basic feature extraction.\n\u2022 Fast Processing and Inference: The network is designed to train and infer faster than deep neural networks, aligning with the brain's capability for rapid learning and decision-making."}, {"title": "Algorithm Overview", "content": "The algorithm introduced in this methodology is inspired by biological neural processes and designed to efficiently learn and adapt in both supervised and unsupervised settings. It utilizes a memory-based approach, where each neuron stores its own label, latent representation, and a counter tracking its exposure to inputs. The training process involves dynamically assigning inputs to neurons, either by initializing unassigned neurons or by selecting the closest matching neuron based on a computed distance. This enables the model to incrementally learn and adapt its internal representations in response to incoming data.\nThe pseudocode provided represents a simplified sequential version of the actual implementation, focusing on the core logic of the training and prediction processes."}, {"title": "Central Memory Structure: Neuron Tensor", "content": "A key component of the algorithm is the central memory structure, referred to as the Neuron Tensor. This tensor is designed to store essential information about each neuron in the model. Each row in the tensor corresponds to a neuron and contains three pieces of information:\n\u2022 Label: Indicates the class that the neuron represents.\n\u2022 Latent Representation: A learned vector representation that captures the neuron's characteristics or \"memory\" of the inputs assigned to it. Specifically it is a tensor of the same shape as the input.\n\u2022 Counter: Tracks the number of inputs assigned to each neuron, enabling dynamic adjustment of neuron properties based on exposure frequency.\nThis design choice is inspired by biological neurons, which adapt based on the frequency and type of inputs they receive. In our model, the counter serves a similar purpose by tracking the adaptation process and influencing the neuron's response to new inputs.\nAll the neurons are initialized to an \"unintialized\" or \"empty\" state, meaning the latent space is a tensor of zeros, the counter is initialized to 0 and the label is set to -1."}, {"title": "Processing of a training batch and neuron assignment", "content": "In this subsection, we describe how a training batch is processed by assigning each input to a corresponding neuron.\nTwo techniques are described, one using thresholds and one not using thresholds."}, {"title": "Basic technique", "content": "This process occurs in two scenarios: (1) when neurons are still uninitialized, and (2) when all neurons are initialized (non-zero tensors, counters > 0, labels not -1).\nIn the first scenario, with uninitialized neurons, each input is assigned directly to an uninitialized neuron without distance computation. Once all neurons are initialized, any remaining inputs follow the procedure for a fully initialized memory.\nIn the second scenario, when neurons are initialized , inputs are assigned based on label and distance. Inputs are matched with neurons of the same label, selecting the closest neuron for memory updates.\nInitially, this process relies on input labels with uninitialized neurons. Once fully initialized, the method can switch to unsupervised training by assigning inputs to the closest neuron, ignoring labels."}, {"title": "Technique using a Threshold", "content": "We developed a thresholding technique to improve the representations learnt by the neurons. This technique has two scenarios: one for when the memory is not full and another for when it is full. In the second scenario, the method behaves like the basic approach. In the first scenario, there are some key differences.\nIn scenario 1, when a new input arrives, we calculate the distances between the input and the existing neurons of the same label, as we do when the memory is full. However, if the minimum distance exceeds a certain threshold, the input is assigned to an empty neuron. If the distance is below this threshold, the input is assigned to the closest neuron. Once the memory becomes full, the basic technique is applied.\nThe threshold value is based on the full centroids per class, computed from the entire training set. This approach means that these threshold values are determined prior to training.\nThe centroid for each class k is calculated as follows:\n$C_k = \\frac{1}{N_k} \\sum_{i=1}^{N_k} x_i$ (1)\nwhere $c_k$ is the centroid of class k, $N_k$ is the number of samples in class k, and $x_i$ represents each sample of class k in the training set.\nThis centroid is used to compute the average distance between samples of class k and the centroid itself. The idea is to estimate the expected average distance between the class representation and its samples. During training, if a distance greatly exceeds this average, the input is considered a new representation and assigned to a new neuron. This helps improve the representations learnt, by categorizing representations very different from those seen previously in training, as new neurons.\nTo fine-tune the parameters, a coefficient is added to this centroid value. Thus, there is one centroid per class and one coefficient per image channel."}, {"title": "Memory Update Step", "content": "This section explains how a neuron's latent representation is updated when an input is assigned to it:\n$I_{n+1} = \\frac{I_n * n + input}{n+1}$\nwhere $I_n$ is the current average representation, n is the count of inputs so far, and input is the new value. This unsupervised update (as no label is used or error is computed) incorporates the new input into the average. After updating, n is incremented to n + 1."}, {"title": "Distance Function Formulation", "content": "To determine the similarity between input images and the neurons, the algorithm employs a distance function. Two variations of this function were explored:\n\u2022 Absolute Value of Errors (Manhattan Distance):\n$D_{abs} (x, y) = \\sum_i |x_i - y_i|$\nThis measure computes the absolute differences between corresponding elements of the input vector x and the neuron representation y. It reflects the total variation between the two, similar to how the brain might process differences in sensory input in a straightforward, linear manner.\n\u2022 Euclidean Distance:\n$D_{euclid} (x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}$\nThe Euclidean distance considers the geometric distance in a multi-dimensional space, providing a more nuanced measure that takes into account both the magnitude and direction of the differences. This is akin to the brain's ability to compute more complex patterns, not just linear differences.\nFor images with c channels (e.g., RGB with 3 channels), the distance between an input and a neuron is computed by first calculating the distance for each channel separately. The overall distance is the average of these distances:\n$D_{multi}(x, y) = \\frac{1}{c} \\sum_{j=1}^{c} D(x_j, y_j)$\nwhere $D(x_j, y_j)$ is the distance measure (absolute or Euclidean) between the j-th channel of the input and neuron, ensuring equal contribution from all channels."}, {"title": "Prediction Algorithm", "content": "The prediction algorithm computes distances between the input and all neurons, selecting the closest one by either averaging distances per channel or using a channel voting scheme. In channel voting, each input channel's closest neuron is identified, resulting in multiple predictions. The final prediction is the most frequent (mode) among these, or, if no consensus, the average method is used:\n$P(x) = \\begin{cases} mode(\\arg \\min_n D(x_j, y_{n,j})), & \\text{if consensus} \\\\ arg \\min_n \\sum_{j=1}^{c} D(x_j, y_{n,j}), & \\text{if no consensus} \\end{cases}$\nWhere P(x) is the predicted label for input x, $D(x_j, y_{n,j})$ is the distance measure between the j-th channel of input x and the j-th channel of neuron n, and c is the total number of channels."}, {"title": "Implementation Details", "content": "The implementation leverages PyTorch to process inputs in parallel, which significantly accelerates computations and enables the model to handle the MNIST and CIFAR-10 datasets efficiently. PyTorch also supports direct memory optimization, allowing the batch size to vary dynamically during parameter tuning, particularly in relation to the maximum number of neurons, which is a key factor in the algorithm's complexity. This flexibility enables us to adjust the batch size to maximize GPU memory usage, thereby processing as many inputs in parallel as possible within a single batch without exhausting memory resources."}, {"title": "Experiments", "content": null}, {"title": "Setup", "content": "We evaluated our model on the MNIST and CIFAR-10 datasets to assess its performance on both grayscale and color image classification tasks. Each experiment was conducted five times, and the average accuracy was recorded. All the experiments were measured to take under 1 minute to complete training."}, {"title": "MNIST Experiments", "content": "We conducted systematic experiments starting with a basic network designed for single-channel input, initially using 10 neurons, achieving an accuracy of 82.03% (Experiment 1). This served as our baseline. After parameter tuning, increasing the neuron count to 15,000 resulted in 97% accuracy (Experiment 2). We explored up to 30,000 neurons, beyond which the model risked memorizing the dataset."}, {"title": "CIFAR-10 Experiments", "content": null}, {"title": "Baseline", "content": "Using the same model from MNIST, we initially reduced CIFAR-10's three channels to one and tested with 10 neurons, obtaining 24.12% accuracy (Experiment 1). We then adapted the model to handle three channels directly, maintaining 10 neurons, achieving 27.74% accuracy (Experiment 2). Parameter tuning yielded a peak accuracy of 36.09% with 30,000 neurons, establishing our baseline (Experiment 3)."}, {"title": "Distance and Prediction Techniques", "content": "Using Euclidean distance with the channel voting prediction scheme explained in section 3.7 with 30,000 neurons improved accuracy to 40.60% (Experiment 4). An ablation study revealed that this adjustment had no impact during training, thus applied only during prediction. Switching to Manhattan distance with a classical averaging approach achieved 40.25% (Experiment 5), while using the voting scheme reached 40.38% (Experiment 6)."}, {"title": "Semi-supervised Learning", "content": "We implemented a variant of the network achieving 40.60% accuracy by initially using labels to populate neurons, once the memory is full we ran unsupervised training by assigning input to the closest neurons regardless of label. This approach resulted in a slightly lower accuracy of 40.32% (Experiment 13)."}, {"title": "Thresholding Technique", "content": "Applying thresholding and tuning the model led to a performance of 40.04% with the optimal hyperparameters being Manhattan distance, prediction with voting, 20,000 neurons and the coefficient 1.1 for the three channels (Experiment 7). Without thresholding, the model performed slightly lower for the same number of neurons at 39.69% (Experiment 8). The full parameter tuning can be visually observed in figure 2."}, {"title": "Other Techniques Tested", "content": "To enhance performance with thresholding, we sorted batches by input difficulty. Input difficulty was determined by calculating the average distance between an input and other inputs of the same class across the entire training set. Reducing each batch to the easiest 20 samples resulted in 28.83% accuracy, while using the top 50 achieved 34.2%. Sequential training phases improved performance further, with top-to-bottom dataset transitions achieving up to 38.6%. Sorting all inputs within batches by difficulty provided a final accuracy of 39.7%\nChannel-wise classification experiments, using individual channels (and muting the other ones), resulted in accuracies of 32.12%, 34.13%, and 36.12% for channels 1, 2, and 3, respectively. Muting only channel 3 achieved 36.55%.\nWe applied pruning to the network using a validation set consisting of 10,000 samples drawn from the original 50,000 training samples and evaluated the results on a separate test set of 10,000 samples. Previously, with thresholding, we achieved an accuracy of 40.0%. We then tested various pruning strategies on this baseline and found the best performance by pruning 99% of the neurons that produced errors every 100 batches. This approach resulted in an accuracy of 37.88% on the test set and 41.26% on the validation set."}, {"title": "Discussion", "content": "Our experiments provide several important insights into the model's performance and areas for improvement. Switching from Euclidean to Manhattan distance improved accuracy (but only when using a threshold) and reduced computational complexity by simplifying distance calculations to absolute values. The choice of prediction technique, particularly the voting scheme, also enhanced accuracy by reducing the impact of outliers. While refining prediction methods might further improve performance, this was not identified as the primary limitation. Interestingly, the algorithm performed similarly in both fully supervised and semi-supervised settings, aligning with our goal of maintaining biological plausibility. A key finding, as seen in Figure 2, is that memorization alone does not lead to optimal performance. During parameter tuning, achieving higher accuracy on CIFAR-10 required more neurons than MNIST. However, using too many neurons proved counterproductive, indicating the need for balanced model complexity. The threshold model initially outperformed the non-threshold model with fewer neurons, but as the neuron count increased, the non-threshold model ultimately achieved higher accuracy, illustrating the varying performance dynamics of both models.\nThe nature of the datasets also influenced the results: MNIST, with its lower variance and simpler features, allowed for higher accuracy, while CIFAR-10's greater feature complexity made separability and representation learning more challenging. This suggests the need for techniques that reduce class variance in a biologically plausible way. Future work could explore adapting Principal Component Analysis (PCA) into a more biologically plausible preprocessing step."}, {"title": "Conclusion", "content": "This study introduces the Memory Network as an initial step toward developing biologically plausible alternatives to traditional deep learning models for visual classification tasks. Our experiments on the MNIST and CIFAR-10 datasets highlight both the potential and the current limitations of this approach. While the Memory Network shows promise on simpler tasks, its performance on more complex datasets like CIFAR-10 reveals the need for further refinement to better handle data variance and complexity. Integrating additional biologically plausible strategies could help address these challenges and improve overall performance.\nThis paper proposes a shift in research focus toward developing machine learning systems that prioritize biological plausibility in their design. Future work will explore not only enhancing existing techniques, such as Hebbian learning-based networks, to make them fully biologically plausible, but also developing entirely new methods or redesigning algorithms to better mimic the efficiency of the brain. By continuing to draw inspiration from biological mechanisms, we aim to advance toward more efficient, adaptive, and robust AI systems."}]}