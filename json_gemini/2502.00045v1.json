{"title": "Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections", "authors": ["Yi Mao", "Andrew Perrault"], "abstract": "Municipal inspections are an important part of\nmaintaining the quality of goods and services. In\nthis paper, we approach the problem of intelli-\ngently scheduling service inspections to maximize\ntheir impact, using the case of food establish-\nment inspections in Chicago as a case study. The\nChicago Department of Public Health (CDPH) in-\nspects thousands of establishments each year, with\na substantial fail rate (over 3,000 failed inspec-\ntion reports in 2023). To balance the objectives\nof ensuring adherence to guidelines, minimizing\ndisruption to establishments, and minimizing in-\nspection costs, CDPH assigns each establishment\nan inspection window every year and guarantees\nthat they will be inspected exactly once during that\nwindow. These constraints create a challenge for\na restless multi-armed bandit (RMAB) approach,\nfor which there are no existing methods. We de-\nvelop an extension to Whittle index-based systems\nfor RMABs that can guarantee action window con-\nstraints and frequencies, and furthermore can be\nleveraged to optimize action window assignments\nthemselves. Briefly, we combine MDP reformula-\ntion and integer programming-based lookahead to\nmaximize the impact of inspections subject to con-\nstraints. A neural network-based supervised learn-\ning model is developed to model state transitions\nof real Chicago establishments using public CDPH\ninspection records, which demonstrates 10% AUC\nimprovements compared with directly predicting\nestablishments' failures. Our experiments not only\nshow up to 24% (in simulation) or 33% (on real\ndata) reward improvements resulting from our ap-\nproach but also give insight into the impact of\nscheduling constraints.", "sections": [{"title": "Introduction", "content": "Cities perform inspections in order maintain the quality of\ngoods and services available to their residents. In this work,\nwe consider the inspection of food establishments. The city's\ngoal can be summarized as: keep as many establishments as\npossible in the inspection-passing state while guaranteeing\nthat each establishment is inspected at a certain frequency\n(e.g., exactly once per year or between once and twice per\nyear). The former objective can be viewed as supporting the\naverage citizen by maximizing the probability that a random\nestablishment is in an inspection-passing state. The second\nprovides a worst-case guarantee: there is a limit on how long\nago any establishment would have been inspected and a limit\non how much disruption establishments experience. The city\nmust achieve these goals subject to a budget on the number\nof inspections that can be performed per unit time.\nRestless multi-armed bandits (RMABs) [Whittle, 1988]\nare almost a natural fit for this problem. They describe a\nsequential decision problem where an agent (the city) acts\non a large population of independently evolving Markov de-\ncision processes (the establishments), which describe each\nestablishment's propensity to stay in the inspection-passing\nstate. While RMABs are highly suitable for maximizing the\ninspection-passing probability for each establishment, exist-\ning approaches (e.g., [Yu et al., 2018; Herlihy et al., 2023;\nLi and Varakantham, 2023]) fail to support the constraint\nstructure required for establishment scheduling, where each\narm's pulls must satisfy an ex-post frequency constraint.\nIn this paper, we develop new methods that allow us to\nsolve these problems by combining Whittle index theory and\ninteger programming. Despite their use of integer optimiza-\ntion, which is often slow, our methods are highly scalable,\noften due to the unimodularity of the arm scheduling opti-\nmization. One key new question that arises is how to assign\nan inspection window to each establishment. To minimize\ndisruption, establishments are informed in advance of a set\nof contiguous time periods during which their inspection will\noccur. We show that, through an extension of our methods for\nRMABs under constraints, we can optimize the assignment of\nwindows to arms and ensure that establishments cannot pre-\ndict when their inspection will occur in the assigned window.\nIn experiments using synthetic data and real data from the\nChicago Department of Public Health (CDPH), we evaluate\nthe impact of both RMAB inspection scheduling and window\noptimization and find that a substantially higher reward can\nbe achieved through optimization.\nOur contributions are as follows:"}, {"title": "Related Work", "content": "Food safety inspections. In 2015, CDPH leveraged histor-\nical food inspection data and trained a supervised learning\nmodel to predict the probability that an inspection would un-\ncover a critical violation [Schenk Jr. et al., 2015]. Kannan et\nal. (2019) independently analyzed the impact of prediction-\ndriven scheduling. However, such models only consider one-\nshot predictions for critical violations and do not include the\nsequential aspect of scheduling. Fairness is of substantial in-\nterest in the provision of municipal services [Singh et al.,\n2022]. We consider fairness outside the scope of this paper,\nbut a potentially interesting direction for future work.\nRestless multi-armed bandits (RMABs). RMABS are\nPSPACE-hard in the worst case, but Whittle (1988) showed\nthat a subclass of them, so-called indexable RMABs, admit\nan efficient asymptotically optimal solution. Particularly rel-\nevant classes of indexable RMABs are those that extend the\nmaintenance problem families [Glazebrook et al.,\n2006], and scheduling problems for sensors [Sombabu et al.,\n2020], wireless transmission [Hsu, 2018], and health inter-\nventions [Mate et al., 2020]. These RMABs are structured\nso the state of each process declines if it is not acted on, and\ndiffer in the details of action effect and what information is\nobserved with or without an action. This work aims to de-\nvelop techniques to integrate action constraints into RMABS\nof these types.\nRMABs with Constraints. Several RMAB models have in-\ncluded constraints. In a project applying RMABs to assist\nmaternal and child health via phone calls, a \u201csleeping period\"\nfor arms was enforced after they were pulled by the Whittle\nindex heuristic [Mate et al., 2022] (See 4.2). It appears to\nhave been enforced in an ad hoc manner, by blocking pulls\nthat would have violated the constraint. Yu et al. (2018)\ndeployed RMABs in a deadline scheduling setting and inte-\ngrated the deadline constraints by adding dummy arms. Fair-\nness is another setting where constraints can arise. Herlihy et\nal. (2023) introduced the ProbFair policy, ensuring a strictly\npositive lower bound on the probability of being pulled at\neach time step while still satisfying the budget constraints.\nLi and Varakantham (2023)'s SoftFair balances the trade-off\nbetween the goals of having resources uniformly distributed\nand maximizing cumulative rewards. They guarantee a long-\nterm probability of each arm being pulled whereas ours en-\nsures pulling frequencies for each arm strictly in a period. To\nthe best of our knowledge, we are the first to consider action\nwindow and frequency constraints strictly for recurring tasks\nin a period."}, {"title": "Preliminaries", "content": "An RMAB consists of N binary action MDPs (arms). We\ndefine the ith two-action MDP [Puterman, 1994] as a tu-\nple $(S_i, A, P_i, R_i, s_i^{(0)}, \\gamma)$. The discount factor $\\gamma$ and action\nspace $A = \\{0,1\\}$ are fixed across all MDPs. When the action\n1 (resp. 0) is taken on an arm at time t, we refer to that arm\nas active (resp. passive). The rest are arm specific: $S_i$ is the\nstate space, $P_i : S_i \\times A \\rightarrow \\Delta S_i$ is the transition\nfunction, $s_i^{(0)}$ is the start state, and $R_i : S_i \\times A \\rightarrow \\mathbb{R}$ is the reward\nfunction. Because there are only two actions, the transition\nfunction $P_i$ can be decomposed into an an active transition\n$P_i^{(1)} : S_i \\rightarrow \\Delta S_i$ and a passive transition $P_i^{(0)} : S_i \\rightarrow \\Delta S_i$.\nA RMAB consists of N binary action MDPs and a per-\ntimestep budget constraint k. At each round t, the agent has a\nbudget k, where $k \\ll N$, meaning that at most k arms can be\n\"pulled\", i.e., have their action set to 1. The MDP which is\npulled transits actively and otherwise transits passively. Upon"}, {"title": "Whittle Indices", "content": "transitions, the rewards from all MDPs are collected and ac-\ncumulated over time. The goal is to find an optimal policy $\\pi^*$\nto maximize our total rewards-formally,\n$\\arg \\max_{\\pi: \\sum_{i} \\pi_i(S_t) \\leq k} \\pi^* = \\arg \\max_{\\pi} J = \\sum_i\\sum_t \\gamma^t R_i(S_{i,t}, \\pi(S_t)),$ (1)\nwhere $\\pi_i(S_t) \\in A$ is the action selected by $\\pi$ for arm i, $S_t \\in$\n$S_1 \\times \\dots \\times S_N$ is the joint state of all arms at time t, and\n$S_{i,t} \\in S_i$ is the state of arm i at time t.\nGeneral RMABs have an exponentially large state space\nand a combinatorially large action space. The Whit-\ntle index method provides tractability for some classes of\nRMABS [Whittle, 1988]. It works by computing a \"benefit\nof acting\" for each arm, called the Whittle index. The Whittle\nindex heuristic then acts on the k arms with highest Whittle\nindices.\nTo calculate the Whittle index for each arm, we search over\n\"subsidies\" for the passive action m. Formally, the subsidy m\nmodifies the reward function $R_i$ into $R_i^{(m)}$:\n$R_i^{(m)}(s_i, 0) = R_i(s_i) + m; R_i^{(m)}(s_i, 1) = R_i(s_i)$. (2)\nThe goal is to identify the smallest subsidy m such that, for\nthe current state $s_{i,t}$, the long-term reward for the passive and\nactive actions are the same. To formalized this, we first define\nthe Q function for arm i under subsidy m:\n$Q_i^{(m)}(s_i, a) = R_i^{(m)}(s_i, a)+\\gamma\\max_{\\alpha' \\in A}\\sum_{s \\in S} P_i(s|s_i, a)Q_i^{(m)}(s_i, a').$ (3)\nDefinition The Whittle index for state $s_{i,t}$ is the smallest m\nwhich makes it equally optimal to take the active and passive\nactions:\n$\\omega(s_{i,t}) = \\inf \\{m : Q_i^{(m)}(s_{i,t}, a = 0) \\geq Q_i^{(m)}(S_{i,t}, a = 1) \\}.$ (4)\nFor the Whittle index heuristic to have asymptotic optimal-\nity guarantees, each arm must satisfy a technical condition\ncalled indexability [Whittle, 1988]. Intuitively, indexability\nsays that, as m increases, the optimal action can only switch\nto passive and cannot switch back to active. Let $W^{(m)}$ be the\nset of states for which $Q_i^{(m)}(s_{i,t}, a = 0) \\geq Q_i^{(m)}(S_{i,t}, a =\n1)$, i.e., the passive action has an equal or higher return than\nthe active action.\nDefinition (Indexability). An arm is said to be indexable if\n$W^{(m)}$ is non-decreasing in m, i.e., for any $m_1, m_2 \\in \\mathbb{R}$ such\nthat $m_1 \\leq m_2$, we have $W^{(m_1)} \\subseteq W^{(m_2)}$. An RMAB is\nindexable if every arm is indexable."}, {"title": "Satisfying Inspection Constraints", "content": "We study two types of action constraints that arise in the mo-\ntivating food establishment inspection problem. We begin by\ndefining a sample RMAB with domain-motivated constraints\n(Sec. 4.1). Window constraints specify an action window\nwhere the arm is allowed be acted on (Sec. 4.2). Frequency\nconstraints specify a minimum number of actions each arm\nmust receive over a period of time (Sec. 4.3)."}, {"title": "Motivating Inspection RMAB", "content": "Motivated by the food establishment setting, we define a\nmodel RMAB with action constraints. This RMAB can be\nviewed as a collapsing bandit [Mate et al., 2020] or a reset-\nting bandit [Khansa et al., 2021], and both have indexabil-\nity guarantees. Each establishment has an unobserved binary\nstate that is either 1 (i.e., inspection passing) or 0 (i.e., inspec-\ntion failing). When we act on the establishment, we assume\nthat it is restored to the passing state and define the reward\nfunction to be 1 for each time period the establishment is in\nthe passing state and 0 otherwise. We consider time periods\nas months-each establishment needs to be inspected once a\nyear and will have a two-month period where this inspection\ncan occur.\nAs the true states are not directly observable, each\narm is a partially observed Markov decision process\n(POMDP) [Araya et al., 2010]. We can rewrite the POMDP\nas a fully observed belief-state MDP, allowing for direct rep-\nresentation as an RMAB.\nFor the underlying MDP, we assume passive transitions\n$P_i^{(0)}$ and active transitions $P_i^{(1)}$ as follows:\n$P_i^{(1)} = \\begin{bmatrix}\n1 & 0 \\\\\n1 & 0\n\\end{bmatrix}$\n$P_i^{(0)} = \\begin{bmatrix}\np_i^{(00)} & p_i^{(01)} \\\\\np_i^{(10)} & p_i^{(11)}\n\\end{bmatrix}$\nEach establishment has its own passive transition probabil-\nities and all share the same action impacts\u2014actions always\nrestore the establishment to the passing state in the next\ntimestep.\nConverting this POMDP to a belief-state MDP yields a set\nof belief states that are reachable from the passing state $b_1 =$\n[0, 1] (as a column vector), i.e., $(P_i^{(0)})^tb_1$, where t is any non-\nnegative integer. In practice, the number of states needed to\nmodel belief dynamics precisely enough is dependent on the\nrate of MDP mixing. A faster mixing MDP will reach its\nstationary state faster and require fewer states-once we are\nsufficiently close to the stationary state, we can have the state\ntransition to itself. The resulting belief-state MDP has a chain\nstructure as shown in Fig 1 and resets to the head of the chain\nwhen the active action is taken.\nCollapsing bandits generalize this setting by allowing $P_i^{(1)}$\nto vary per arm, resulting in a two-chain structure. In gen-\neral, our methods will also apply to this setting with minor\nmodifications."}, {"title": "Action Windows and MDP Encoding", "content": "We use action windows as an exemplar for the family con-\nstraints where the constraint can be directly encoded into the\nRMAB structure, i.e., a vanilla RMAB with an action window\nconstraint can be rewritten as a vanilla RMAB with a differ-\nent arm structure. This is in some sense the ideal way to add\nconstraints-we can apply whatever existing state-of-the-art\nalgorithm directly."}, {"title": "Frequency Constraints and Lookahead", "content": "It is possible to enforce maximum action limits via editing the\nindividual MDPs, but it is not possible to enforce minimums\nthis way. In the motivating RMAB, we want to enforce the\nconstraint that each establishment is inspected exactly once or\nmultiple times per year since in the food inspection task, the\nauthority has responsibilities to inspect every food establish-\nment and never skip one. To enforce this kind of frequency\nconstraint, we will replace the Whittle index heuristic with\na sequential planning component that aims to maximize the\nsum of indices of pulled arms over a lookahead window, not\njust in the next timestep.\nWe begin with the case where each arm needs to be pulled\nexactly one time over the lookahead window (and later relax\nthis). In the motivating RMAB, this window will be one year.\nFormally, we let $a_{i,t}$ be whether arm i is pulled at time t and\n$W_{i,t}$ be the Whittle index of arm i at time t. These Whittle in-\ndices can come from an RMAB with any encoded constraints,\nsuch as those in the previous section. We seek to maximize\n$\\sum_{i=1}^{N} \\sum_{t=1}^{T} a_{i,t}w_{i,t},$ subject to the following constraints:\n1. $\\sum_{i=1}^{N} a_{i,t} \\leq k$: only k arms can be pulled in each timestep.\n2. $\\sum_{t=1}^{T} a_{i,t} \\leq 1$: each arm needs to be pulled at most\nonce during the lookahead period. This is needed to\nmake defining $w_{i,t}$ simple-otherwise $w_{i,t}$ depends on\nthe time of the last pull.\n3. $A_{i,t} = \\begin{cases}\n1 & \\text{if } t \\text{ in action window} \\\\\n0 & \\text{otherwise}\n\\end{cases}$ This con-\nstraint forces a out of the action window to be 0, which\nsatisfies one of our problem setting: arms can only be\npulled during their action windows.\n4. Additional desired frequency constraints, e.g., each arm\nmust be pulled at least once during certain timesteps.\nProposition 1. Maximizing the sum of Whittle indices with-\nout additional frequency constraints OR with the constraint\nthat each arm must be pulled exactly once during the looka-\nhead window can be reduced to a weighted b-matching."}, {"title": "Action Window Optimization", "content": "In the problem of city and public service scheduling, in-\ncluding food establishment inspections, it is the authorized\nagency's duty to assign inspection windows. In the practice of\nfood inspections, the establishments are aware of the window\nbut not the exact inspection time. Under such circumstances,\nwe will show that the techniques introduced in this paper can\nbe leveraged to optimize window assignments to further in-\ncrease rewards. In this setting, we still need to satisfy the de-\nsired service constraints (i.e., minimum and maximum num-\nber of pulls, no information provided about inspection time\nis provided beyond the window), but have the flexibility to\nplace the windows as we choose.\nTo accomplish this, we assign a \u201cvirtual\u201d window to each\nestablishment, consisting of the entire period during which\nthe inspection constraints must apply. For example, if the de-\nsired outcome is exactly one inspection over the next year, we\nassign the virtual window of the entire year to each establish-\nment. Then, we simulate the operation of the RMAB over the\nvirtual window, using the techniques of Sec. 4 to ensure that\nthe required constraints are satisfied and record when inspec-\ntions occur, producing the virtual inspection sequence. We\nthen will take this sequence and use it to assign windows such\nthat each virtual inspection takes place during the assigned\nwindow, and we will do so carefully to anonymize when the\nactual inspection will take place. Because we assume that\ninspections never fail to transition an establishment to the ad-\nherent state, there is no loss of expected reward incurred by\nexecuting the virtual inspection sequence determined during\nwindow planning.\nHow do we assign windows according to the virtual in-\nspection sequence? Let $a_{i,t}$ be the encoding of the virtual\ninspection sequence, where $a_{i,t} = 1$ if arm i is pulled at time\nt. A naive way is to assign arm i with window $[t, t + W - 1]$\nif $a_{i,t} = 1$. However, such assignments are easily predictable\nand establishments would be able to prepare effectively. We\nneed to design a way in which establishments get inspected\nwith a probability of 1/W on each day in the window.\nWe can do so with a linear program (LP). We define vari-\nables $f_{t,t'}$, which indicate the proportion of arms with vir-\ntual inspection time t that are put in inspection window\n$[t',t' + W - 1]$. We denote the number of arms with as-\nsigned inspection t that are put in the window starting at t' as\n$N_{t,t'}$. Thus, $g_{t,t'} = \\sum_{i=1}^{N} a_{i,t}f_{t,t'}$. For the objective, we use\n$\\min_f \\sum_t\\sum_{t'=t-W+1}^t\\sum_{t''=t-W+1}^t |g_{t,t'}-g_{t,t''}|$ (6)\nto satisfy the anonymity condition\u2014that the action window\nprovides no additional information. We can use the standard\nconstraint trick to remove the absolute value in the objective\nby introducing an auxiliary variable that is constrained to be\nlarger than the objective and the negation of the objective. We\nadd additional constraints to ensure that the window assign-\nments achieve our goals:\n1. The virtual action assignment must occur during the\nwindow: $f_{t,t'} \\neq 0$ if and only if $0 \\leq t - t' < W - 1$.\n2. The window assignment probabilities sum to 1:\n$\\sum_{t'} f_{t,t'} = 1$ for all t\nUsing the $f_{t,t'}$ output of the LP, we sample windows us-\ning any categorical sampling procedure. At this stage, the\nvirtual inspection can be discarded and individual inspec-\ntions planned as if the windows were given (i.e., not even\nthe system operator knows when the virtual inspections were\nplanned to occur).\nOptimizing the window positions has a larger effect than\noptimizing inspections within fixed windows in our experi-\nments, which makes sense given the additional flexibility that\nwindow optimization affords. However, window optimiza-\ntion builds directly on our approach for optimizing inspec-\ntions within fixed windows."}, {"title": "Experimental Study", "content": "In this section, we ask two questions. First, what is the im-\npact on adherence of leveraging the methods of this paper\nto optimize inspection policies? Second, what is the cost of\nthe inspection service constraints in terms of adherence? We\ndescribe the compared policies in Sec. 6.1 and study the im-\npact of different planning policies on reward and computation\ntime, both in synthetic (Sec. 6.2) and real data from CDPH\n(Sec. 6.3) domains."}, {"title": "Planning Policies", "content": "We study different constraints situations from three dimen-\nsions: whether the window could be re-scheduled and op-\ntimized or random, whether the schedule is optimized (ap-\nply the methods in Sec 4 and Sec 5), or naive IP, and the"}, {"title": "Synthetic Domain", "content": "We begin with experiments using synthetic instances."}, {"title": "Data Preparation and Setup", "content": "In the synthetic domain, we generate $P_i^{(0)}[0,0]$ by sampling\nfrom Beta($\\alpha = 5, \\beta = 1$) and $P_i^{(0)}[1,0]$ by sampling from\nBeta($\\alpha = 1, \\beta = 5$). Each simulation is run for 60 timesteps\n(\"months\"). Each arm has two action windows of two months\neach, with one pull allowed in each window. We set the num-\nber of arms to 1000 and the budget per round to 9% of the\nnumber of arms (we need a budget of 8.33% of all arms to\nsatisfy all arms' constraints)."}, {"title": "Food Establishment Inspection Domain", "content": "Using inspection data from the Chicago Data Portal [Chicago\nData Portal, 2023], we implement a realistic RMAB setting."}, {"title": "Data and Setup", "content": "Since 2010, CDPH has published every food establishment\ninspection result on the Chicago Data Portal [Chicago Data\nPortal, 2023]. The Food Inspection Dataset is a tabular\ndataset with 17 attributes for each establishment including li-\ncense number, address, etc. The inspection results are shown\nin the \"Violations\" column: 0 means no violations and pass,\n1 means violations appear and 2 means pass with conditions.\nIn the experiment, both 0 and 2 are merged into a single good\nstate and 1 is the bad state."}, {"title": "Inferring Transitions", "content": "To create a realistic instance, we\nmust infer the transition probabilities from the inspection tra-\njectories for each arm. Because of the small number of in-\nspections per establishment, it is impractical to estimate a\ntransition model for each food establishment. Thus, we used\nall data to train one single model to learn the transitions for\nall establishments. A neural network with 2 MLPs is trained\nto predict the transition matrix $P_i^{(0)}$ to maximize the log-\nlikelihood of the data given the transitions. The detailed ar-\nchitecture of the neural network is provided in the supple-\nment."}, {"title": "Data Preparation", "content": "The inputs to the model are a series of\nfeatures of establishments. We use the same features as pre-\nvious work on predicting food establishment risks [Schenk\nJr. et al., 2015], which combines data of business licenses,\nfood inspections, crime, garbage complaints, sanitation com-\nplaints, weather and sanitation information."}, {"title": "Loss", "content": "The loss is to minimize the negative log likelihood:\n$\\min_y -\\sum_i log(p(s'; s0T))$ (7)\nwhere s is the last state and s' is the next state, T is the time\nsince last inspection. The Adam optimizer is applied and the\nlearning rate is 0.0001, and the model is trained for 10000\nepochs."}, {"title": "MLP Training Result", "content": "We validate the MLP by computing\nthe AUC of its predictions. To do this, we view each interval\nbetween predictions as a data point with the label of whether\nthe next inspection found adherence or non-adherence. If no\nprevious records for the establishment, we use average values\nto fill missing columns. We then take the parameter predicted\nby the MLP and use it to compute the probability of adher-\nence and compute the resulting AUC. Then we trained the\nmodel on different train-test splits:"}, {"title": "Results", "content": "The total reward accrued for each policy is presented in Fig-\nure 2 as reward improvements relative to the reward achieved\nby pulling no arms at all. The benefits of applying the\nmethods of this paper are clearly seen-optimizing sched-.\nIn practice, there will be errors in the estimates of the estab-\nlishments' transition probabilities. We perform a sensitivity\nanalysis on this error: adding noise to the parameter estimates\nfrom a Normal distribution with a mean of 0 and variances\nrather than at most once. We find the cost of\nthis constraint to be moderate, roughly of the same magni-\ntude as going from fixed to optimized windows or from a ran-\ndow and\nschedule optimization do not require any weakening of this\nguarantee."}, {"title": "Conclusions", "content": "We present an RMAB-based method to solve schedul-\ning problems under real-world city service scheduling con-\nstraints. Both synthetic data results and those using real food\ninspection data from CDPH suggest that our methods for ex-\nplicitly modeling constraints and optimizing action windows\nare critical for RMABs to have an impact in this setting. We\nlems under constraints."}, {"title": "Appendix", "content": "We simultaneously compute Whittle indices for all states of\neach arm using binary search over subsidies with the toler-\nance $10^{-6}$. All experiments are run on a single core of AMD\nEPYC 7643 (Milan) processors (2.3 GHz). For an RMAB\nwith 1000 arms, computing Whittle Indices takes around\n1000s and the baseline scheduling IP takes around 100s for\none period. Details of the running time of policies can be seen\nin the supplement. Optimizing policies consumes around an\norder of magnitude more computational time than the base-\nline, but we can reuse the Whittle indices for window opti-\nmization for scheduling, and as a result, optimizing windows\nand schedules together requires negligibly more computation\ntime than performing either optimization individually. RAM\nconsumption is low for all policies, less than 500MB."}, {"title": "Weighted b-Matching", "content": "The lookahead planning algorithm we develop will be\nreducible to variants of the weighted b-matching prob-\nlem [Schrijver and others, 2003]. A weighted b-matching in-\nstance is described by an undirected graph $G = (V, E)$, an\nedge weight vector $w : E \\rightarrow \\mathbb{R}$, and a non-negative b vector\n$b : V \\rightarrow \\mathbb{N}^+$. The objective in a maximum weight b-matching\nis to find a set of edges x with maximum weight, subject to\nthe constraint that only b(v) edges that are adjacent to node v\ncan be selected. Formally,\n$\\max_x w\\cdot x, s.t. \\sum_u x_{u,v} \\leq b(v), \\forall v \\in V$ (8)\nWeighted b-matchings can be solved in polynomial time, e.g.,\nin $O(|V|^2 \\max_v b(v))$ [Pulleyblank, 1973].\nA more challenging weighted b-matching variant is\nweighted bipartite b-matching [Chen et al., 2016]. In this\nvariant, graph nodes are partitioned into a right set U and left\nset V, and there are no edges within each partition. Nodes\nin the left (resp., right) set have maximum matching cardi-\nnality $L^+$ (resp., $R^+$) and minimum cardinality $L^\u2212$ (resp.,\n$R^\u2212$). Under these constraints, finding a maximum weight b-\nmatching is NP-hard."}, {"title": "Proof For Proposition 1", "content": "Proof. The proof converts each timestep and each arm to\nnodes in the matching graph with different b-values. We for-\nmulate the weighted b-matching instance as follows. For each\narm i \u2208 [N], create a node i. For each timestep t in the looka-\nhead period, create a node t. For each arm-timestep pair (i, t)\nwhere an action can occur (i.e., no timing constraints are vio-\nlated), create an edge of weight $w_{it}$ between i and t. Set the\n$b(t) = k$ for all t and $b(i) = 1$ for all nodes i. We claim that\nthe maximum weight b-matching can be converted to an op-\ntimal lookahead schedule by taking each arm-timestep (i, t)\npair that is included in the maximum weight b-matching and\npulling the arm i at timestep t. Constraints 1 and 2 are sat-\nisfied by definition of weighted b-matching. Constraint 3 is\nsatisfied because edge (i, t) exists only if t is in i's action win-\ndow. Thus, the optimal solution to the weighted b-matching\nmust be the optimal solution to the lookahead problem.\nTo account for the additional frequency constraint that each\narm must receive at least one pull in the lookahead window, if\npossible, a large constant can be added to all Whittle indices.\nThe constant will cause each arm to be pulled once, if pos-\nsible, because it is much larger than the increase in objective\nvalue that can be achieved by shifting the pull time for any\nindividual arm."}]}