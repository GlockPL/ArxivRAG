{"title": "THE ROLE OF XAI IN TRANSFORMING AERONAUTICS AND AEROSPACE SYSTEMS", "authors": ["Francisco Javier Cantero", "Mikel Galafate", "Javier M. Moguerza", "Isaac Mart\u00edn de Diego", "M. Teresa Gonz\u00e1lez De Lena", "Gema Gutierrez Pe\u00f1a"], "abstract": "Recent advancements in Artificial Intelligence (AI) have transformed decision-making in aeronautics and aerospace. These advancements in AI have brought with them the need to understand the reasons behind the predictions generated by AI systems and models, particularly by professionals in these sectors. In this context, the emergence of eXplainable Artificial Intelligence (XAI) has helped bridge the gap between professionals in the aeronautical and aerospace sectors and the AI systems and models they work with. For this reason, this paper provides a review of the concept of XAI is carried out defining the term and the objectives it aims to achieve. Additionally, the paper discusses the types of models defined within it and the properties these models must fulfill to be considered transparent, as well as the post-hoc techniques used to understand AI systems and models after their training. Finally, various application areas within the aeronautical and aerospace sectors will be presented, highlighting how XAI is used in these fields to help professionals understand the functioning of AI systems and models.", "sections": [{"title": "1 Introduction", "content": "The rise and development of AI and its adoption across various sectors of society has led to big advances in decision making in the governments, administrations and businesses. This development of AI has led to greater awareness among humans that many of the decisions affecting their daily lives are based on the use of these algorithms or directly taken by these. Due to the relevance AI systems and models have taken in our lives, the necessity of understanding the motives of the decisions taken by these models has been spread among researchers and professionals [1]. This needs to understand the reason behind the decisions comes from the difficulty of understanding the internal workings of an AI system or, in other words, when trying to understand the algorithm used by a specific model [2]. In response to this situation, in 2016, the Defense Advanced Research Projects Agency (DARPA), started the XAI program, whose objective was to develop several methods that allowed to understand and trust in AI systems. As a result, and with the objective of obtaining interpretable and transparent models, XAI was developed. XAI proposes the creation of a set of Machine Learning (ML) (ML) techniques that allow the creation of more explainable models maintaining a high training level to have users understand and handle AI [3].\nIn aeronautics and aerospace sectors, the use of AI systems and models has also brought about significant advancements and evolution of automation and development of tasks and processes in both fields. Along with the development of these industries, the need to know and explain the reasons for the decisions taken by AI systems has also risen. Consequently, during the last decade, XAI arose as a fundamental actor when developing AI systems and models. Aerial Traffic Management (ATM) to control aerial traffic [4], or the certification of critical systems that allow the evaluation of the protocols airplane pilots must carry out in critical situations [5] are some examples of XAI in aeronautics. In aerospace, XAI runs a fundamental role in tasks such as generation of explanations for Deep Neural Networks (DNN) used in predictive maintenance, digital twins [6], and the anomaly detection in spacecraft telemetry [7].\nThe emergence of XAI has promoted the use of AI in many complex tasks and processes with the goal of facilitating the understanding of AI systems and models. For this reason, this article defines XAI and its objectives in Section 2. In Section 3, the properties AI models must satisfy for them to be considered interpretable and transparent are defined. In Section 4, transparent models are defined as well as the techniques applied to models that are not. Section 5 performs a review on the state-of-the-art of XAI in aeronautics and aerospace, to end up with the conclusions of the work done in this article in Section 6."}, {"title": "2 Definition and objectives of XAI", "content": "The name XAI was first used when in 2016 DARPA promoted the XAI program. The objective of the program was to develop ML techniques that allow the creation of more explainable models while maintaining a high-performance level to have users understand and handle AI [3]. Since then, numerous researchers and professionals have been working on this area, proposing definitions and objectives for XAI. Among the most relevant definitions, Gunning in 2017 describes XAI as \u201cXAI will create a suite of ML techniques that enables human users to understand, appropriately trust and effectively manage the emerging generation of artificially intelligent partner\" [3]. Arrieta in 2019 defined XAI as \u201cGiven an audience, an explainable Artificial Intelligence is one that produces details or reasons to make its functioning clear or easy to understand.\" [8]. From these definitions, it can be observed that the main objective of XAI is, on the one hand, creating a set of ML techniques that produce more explainable models while maintaining an elevated level of performance. On the other hand, it also aims to allow humans to comprehend, properly trust, and efficiently manage the emergent generation of AI partners [3].\nTo make AI models comprehensible to humans, two primary categories of models are distinguished:\n\u2022 Black-box models are characterized by their lack of interpretability, often due to their complexity or opaque nature. These models, such as deep neural networks, operate as \"black boxes\", where the internal decision-making processes are not easily accessible or understandable to humans [9].\n\u2022 White-box models are inherently transparent and interpretable due to their simple and accessible structure. These allow users to trace and understand the logic behind their predictions without requiring additional tools or techniques [10].\nTo distinguish between these two types of models, one of the objectives XAI has, is to group the terms that make an AI system or model clear and easy to understand. These terms had been independently studied, and XAI grouped them to define a model as a white-box or a black-box model:\n\u2022 Interpretability can be defined as the ability to explain the meaning in understandable terms to a human [11] when talking about AI."}, {"title": "3 Properties of AI models in XAI", "content": "In XAI, specific properties must be evaluated to determine whether an AI model is comprehensible and explainable to users. Authors and professionals usually consider the following classification to know the degree of understanding and explainability of an AI model [8]:\n\u2022 Trustworthiness is considered as the confidence of whether a model will act as intended when facing a given problem.\n\u2022 Causality. XAI aims to identify potential causal relationships among data variables.\n\u2022 Transferability. Models are always bounded by constraints that should allow for their seamless transferability.\n\u2022 Informativeness. Explainable ML models should give information about the problem being tackled.\n\u2022 Confidence should always be assessed on a model in which reliability is expected.\n\u2022 Fairness. Explainability can be considered as the capacity to reach and guarantee fairness.\n\u2022 Accessibility. Explainability is sometimes defined as a property that allows end users to get more involved in the process of improving and developing a certain ML model.\n\u2022 Interactivity. The ability of a model to be interactive with the user is one of the goals targeted by an explainable ML model.\n\u2022 Privacy awareness is an ability enabled by explainability in ML models that may have complex representations of the learned patterns.\nThese properties let us know the degree of understandability and explainability an AI system or model has. However, it is to be considered the fulfillment of these properties in the context the model is deployed. As mentioned earlier, in some critical contexts, such as aeronautics and aerospace, where an elevated level of precision is desired to avoid harm to human life, damage to the environment, or significant economic losses, some properties may not be satisfied [5].\nFor this reason, when assessing the properties of the models, the performance-interpretability trade-off of the model must always be considered [15]. All properties must be taken into account, always being conscious of them possibly compromising the precision of the system or model, making the deployment of it impossible in a critical environment."}, {"title": "4 XAI techniques", "content": "Both interpretable models and techniques to make opaque AI systems and models comprehensible are included in XA\u0399 by its definition. As shown in figure 1, a distinction is drawn between transparent models, that are understandable given\nthe simplicity of the algorithms, and the post-hoc techniques applied after the model's training to better understand its workings and predictions.\nA distinction is drawn between transparent models, which are inherently interpretable due to their simplicity, and post-hoc techniques, applied after model training to elucidate the internal workings and predictions of black-box models. Furthermore, when analyzing them, it is to be considered that this kind of models can be tackled from the domain in which they are interpretable, that is [8]:\n\u2022 Algorithmic transparency deals with the ability of the user to understand the process followed by the model to produce any given output from the input data.\n\u2022 Decomposability refers to a model's ability to be understandable in smaller parts. This also requires each input to be interpretable.\n\u2022 Simulatability refers to the ability of a model to be manually simulated or conceptualized by a human user.\nSome examples among the models considered to be transparent are logistic and linear regressions given the straightforwardness to compute a prediction and the easiness to be understood by humans; decision trees, whose structure allow to give clear explanations by means of the visualization they provide; rule-based methods which are composed of a set of rules that make the comprehension of the predictions simple to humans; and additive models, such as GAM or GLM, that are extensions of regression models and treat each used variable to predict independently [9].\nPost-hoc techniques are applied to black-box models to be comprehensible by human beings. As seen in figure 1, depending on the generated results, post-hoc techniques are classified into four different approaches [9]:\n\u2022 Model internals. Internal components and mechanisms considered to be interpretable are included. This includes components such as the weights in linear models, showing the relevance and direction of the relationship between characteristics; or the divisions that can be done in decision trees, that show how decisions are made based on sequential rules.\n\u2022 Model surrogate. Techniques that approximate a model in a local (a part or instance of a model) or global (of the whole model) manner by means of models considered to be transparent. Within these are included LIME, which provides comprehensible explanations of black-box models by a local approximation of the predictions; Anchors, that explains the predictions of a model with highly precise local-specific rules; or SHAP, which obtains both local explanations by decomposing the prediction of an instance to individual contributions of the characteristics, and global explanations by summarizing the contributions for multiple instances.\n\u2022 Feature summary. This includes techniques that yield numerical statistics extracted after the model's training. This includes classical summary statistics, feature importance (which allows for observing the weight of each feature in a prediction), or partial dependence plots (PDP) that help understand how one or more specific features influence a model's prediction.\n\u2022 Example-based. It encompasses the techniques used to provide individual explanations for data instances based on other data instances, which can be real or simulated. Among these are counterfactuals, which analyze which changes must be done to an instance to obtain a different prediction; Influential Observations, which examine the variation of the prediction of a model when influential data instances are removed; and Prototypes"}, {"title": "5 Applications of XAI in aeronautics and aerospace", "content": "The need to understand how decisions are made within AI systems and models has led to an increase in the use of XA\u0399 in aeronautics and aerospace. This need is particularly relevant in these fields because, as mentioned in Section 3, they are considered critical environments, and decisions made by AI systems and models can have severe consequences. Therefore, developers must exercise exceptional care when designing these systems, ensuring they achieve a high degree of accuracy while being still interpretable and understandable by the humans who use them. In other words, they must consider the trade-off between accuracy and interpretability [5]. This section presents various use cases where XAI has been applied in aeronautics and aerospace.\nIn the aeronautical sector, applications of XAI can be found in various areas. In the most critical area of aeronautics, Air Traffic Management (ATM), the use of XAI has a significant impact on predictive tasks such as takeoff and landing times or incident risk assessment. Considerable efforts are made to explain the predictions generated by models based on neural networks [4]. Another application where XAI is having a significant impact is in the design and development of Unmanned Aerial Vehicles (UAVs) and drones. XAI is used in route adaptation during missions under adverse conditions, using fuzzy rules to explain how routes are adjusted during the missions these vehicles carry out [16]. It is also applied in simulations in three different modes to assess drone operations, validating decisions using metrics such as Mean Square Error (MSE), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE) [17]. Finally, another case is in post-natural disaster damage assessment, where data collected from drones and satellites is used to evaluate the damage caused by natural disasters, and explanations are generated using actual and predicted values, facilitating decision-making in emergency scenarios [18].\nIn aerospace, XAI applications are also found in various areas such as predictive maintenance. By applying post-hoc techniques like LIME or SHAP to DNNs used for the health management of vehicles integrated into aircrafts, the functioning of these neural networks was explained locally in aspects such as predictive accuracy, stability, and consistency of the models [6]. On the other hand, in anomaly detection in spacecraft telemetry, the LIME technique was applied by analyzing different data instances used to train various models, which were then explained and exemplified for each type of anomaly in which the spacecraft telemetry signals were classified [7]. Finally, within the area of satellite image processing, the use of decision trees combined with object detection through deep networks can be found for predicting poverty indices in Uganda, where the importance of features was used to identify which, visual elements had the greatest impact on the predictions [18]. Another application involves the use of attribution maps, such as Grad-CAM, to analyze daytime satellite images and nighttime light data in Sub-Saharan Africa [19]."}, {"title": "6 Conclusions", "content": "In this paper, the presentation of XAI has been carried out, which can be defined as a set of ML techniques that enable humans to understand and trust artificial intelligence systems and models. The objectives pursued by XAI have also been defined, which are; first, to create a set of techniques that allow artificial intelligence systems and models to be understood by humans; second, to group the various terms that may be related to the functioning of these models and systems; and third, to adapt the explanations and conclusions drawn based on the profile of the humans to whom they are directed.\nOn the other hand, the properties that should be evaluated in AI systems and models to be considered black-box models have been defined. Additionally, the concept of trade-off has been introduced, which allows developers of AI systems and models to consider which properties need to be met in the environment where the systems and models are being developed to ensure a high degree of accuracy and a high degree of model interpretability.\nAdditionally, the distinction has been made between white-box models, which are considered interpretable due to their algorithmic simplicity, and black-box models, which are not considered interpretable. Achieving transparency in black-box models requires the application of post-hoc techniques, which enable model interpretability and can be grouped into the following categories: Model Internals, which includes the internal components and mechanisms of models that are considered interpretable within transparent models; Model surrogate, which encompasses techniques that approximate a model locally or globally using models deemed transparent; Feature summary, which includes techniques that generate numerical statistics extracted after model training and during prediction, along with visualizations of these statistics; and Example-based, which includes techniques used to provide individual explanations of data instances using other data instances, whether real or simulated.\nFinally, various applications of XAI in aeronautics and aerospace have been presented. These sectors are considered critical environments, requiring that the systems and models developed ensure a high degree of accuracy alongside a high degree of interpretability. On one hand, it has been proven how the use of post-hoc techniques can ensure the interpretability of models in areas such as ATM or the design and development of drones in the aeronautical sector. On the other hand, in aerospace, these techniques can be applied to systems and models for predictive maintenance of spacecraft, anomaly detection in spacecraft telemetry, or satellite image processing."}]}