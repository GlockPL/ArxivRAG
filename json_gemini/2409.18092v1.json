{"title": "DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models", "authors": ["Helin Cao", "Sven Behnke"], "abstract": "Abstract\u2014Perception systems play a crucial role in autonomous driving, incorporating multiple sensors and corresponding computer vision algorithms. 3D LIDAR sensors are widely used to capture sparse point clouds of the vehicle's surroundings. However, such systems struggle to perceive occluded areas and gaps in the scene due to the sparsity of these point clouds and their lack of semantics. To address these challenges, Semantic Scene Completion (SSC) jointly predicts unobserved geometry and semantics in the scene given raw LiDAR measurements, aiming for a more complete scene representation. Building on promising results of diffusion models in image generation and super-resolution tasks, we propose their extension to SSC by implementing the noising and denoising diffusion processes in the point and semantic spaces individually. To control the generation, we employ semantic LiDAR point clouds as conditional input and design local and global regularization losses to stabilize the denoising process. We evaluate our approach on autonomous driving datasets and our approach outperforms the state-of-the-art for SSC.", "sections": [{"title": "I. INTRODUCTION", "content": "Perception systems collect low-level attributes of the surrounding environment, such as depth, temperature, and color, through various sensor technologies. These systems leverage machine learning algorithms to achieve high-level understanding, such as object detection and semantic segmentation. 3D LiDAR is widely used in self-driving cars to collect 3D point clouds. However, 3D LiDAR has inherent limitations, such as unobservable occluded regions, gaps between sweeps, non-uniform sampling, noise, and outliers, which present significant challenges for high-level scene understanding.\nTo provide dense and semantic scene representations for downstream decision-making and action systems, Semantic Scene Completion (SSC) has been proposed, aimed at jointly predicting missing points and semantics from raw LiDAR point clouds. Given its potential to significantly improve scene representation quality, this task has garnered significant attention in the robotics and computer vision communities. Understanding 3D surroundings is an inherent human ability, developed from observing a vast number of complete scenes in daily life. When humans observe scenes from a single view, they can leverage prior knowledge to estimate geometry and semantics. Drawing inspiration from this capability, the SSC model learns prior knowledge of scenes, P(scene), by estimating the complete scene from partial inputs during training. During inference, new partial inputs captured from the scene serve as the likelihood, P(observation scene), and the model finally estimates a reasonable posterior result. Notably, the final estimation is not a unique answer but rather a sample from the posterior distribution, P(scene|observation). This aligns with human intuition, since humans also infer plausible results from partial inputs, while unobserved parts remain subject to infinite possibilities.\nHowever, most traditional SSC methods are limited to learning the prior distribution of data directly, i.e., training a network to estimate the target output directly from partial inputs. Another approach to learning prior distributions is to estimate residuals. Denoising Diffusion Probabilistic Models (DDPMs) gradually introduce noise into the data in the forward diffusion process and employ a denoiser to learn how to remove these noise residuals. The denoiser iteratively predicts and removes noise, allowing the model to recover high-quality data from pure noise. This mechanism effectively learns the prior distribution of the data, which has the potential to be applied in SSC tasks.\nIn this work, we propose DiffSSC, a novel SSC approach leveraging DDPMs. As shown in Fig. 1, our method jointly estimates missing geometry and semantics from a scene using raw sparse LiDAR point clouds. During training, the model learns the prior distribution by predicting residuals at different noise intensity levels. These multi-level noisy data are generated from ground truth using data augmentation. In the inference stage, the sparse semantic logits serve as conditional input, and the model generates a dense and semantic scene from pure Gaussian noise through a multi-step Markov process. We model both the point and semantic spaces, designing the forward diffusion and reverse denoising processes to enable the model to learn the scene prior to the semantic point cloud representation. In summary, our key contributions are:\n\u2022 We utilize DDPMs for the SSC task, introducing a residual-learning mechanism compared to traditional"}, {"title": "II. RELATED WORK", "content": "A. LiDAR Perception\nLiDAR is widely used in various autonomous agents for collecting 3D point clouds from the environment. In the past, extensive research was dedicated to employing LiDAR for odometry [1] and mapping [2], [3]. Given the inherent challenges of LiDAR, including data sparsity, noise, and outliers, researchers concentrated on developing filtering algorithms [4] and robust point cloud registration [5] to achieve accurate and efficient LiDAR-SLAM systems. With the advent of deep learning, LiDAR data began to be leveraged for object detection [6] and semantic segmentation [7]. Additionally, unlike dense representations such as images, the sparse nature of LiDAR point clouds presents unique challenges for models. To address these challenges, some researchers focus on estimating the gaps between sweeps and occluded regions from sparse point clouds. This has led to the development of semantic scene completion, an emerging technique in LiDAR perception.\nB. Semantic Scene Completion (SSC)\nThe task of completion has a long research history. Early efforts in this field focused on filling small holes in shapes to enhance model quality, typically employing continuous energy minimization techniques [8]. With the advent of deep learning, approaches evolved to enable networks to learn extensive geometric shape properties [9], allowing to estimate of entire models from partial inputs. In contrast to shape completion, semantic scene completion (SSC) [10] presents a significantly more complex challenge. Scenes exhibit more intricate geometric structures and encompass a wider range of semantic categories. SSCNet [11] represents the pioneering work that formally defined this task. Since its introduction, various input data modalities, such as occupancy grids [12], images [13], and LiDAR-camera fusion [14], have been explored. Additionally, a wide array of methodologies, including point-voxel mapping [15], transformers [16], bird's-eye view (BEV) assistance [17], and knowledge distillation [18], have been employed to advance the state of the art in this domain. However, these approaches generally operate on voxelized grids, which poses specific challenges for LiDAR point clouds, as voxelization can introduce quantization errors, leading to a loss of resolution and increased memory usage. In this work, we operate directly on point clouds, offering a more efficient method for handling LiDAR data.\nC. Denoising Diffusion Probabilistic Models\nAlthough diffusion models were originally discovered and proposed in the field of physics, DDPMs [19] was the first to apply this method to generative models. In subsequent research, Rombach et al. [20] introduced latent diffusion models, where the diffusion process is performed in the latent space of the image. This significantly improved computational efficiency and reduced resource consumption, enabling the generation of high-quality and high-resolution images, marking a breakthrough in the field of artistic creation. Beyond artistic applications, several works [21], [22] have also adapted diffusion models for LiDAR perception. These approaches typically project 3D data onto image-based representations, such as range images, allowing methods developed for image domains to be directly applied. Notably, due to the higher demands for accuracy in robotics, controlling the generative process to achieve realistic results remains a significant challenge when applying diffusion models in this field. The recent LiDiff [23] directly applies diffusion models to 3D point clouds for scene completion. However, it still lacks the capability to model and process semantics simultaneously. In this work, we apply DDPM to semantic scene completion, to generate dense and accurate semantic scenes."}, {"title": "III. METHODOLOGY", "content": "Given a raw LiDAR point cloud, our objective is to estimate a more complete semantic point cloud, including unobserved points with associated semantic labels within gaps and occluded regions. As illustrated in the Fig. 2, we achieve this through a diffusion model as the core component, supported by a pre-processing semantic segmentation module and a post-processing refinement module. First, the raw LiDAR point cloud is semantically segmented using a Cylinder3D [7] to generate initial semantic logits. Next, we upsample the semantic point cloud to increase point density for the diffusion process. The duplicated semantic points undergo a forward diffusion and a reverse denoising process to adjust their positions and semantics. Notably, the semantic point cloud also serves as a conditional input for the diffusion model, guiding the generation process. The generated scene includes semantic points located in gaps and occluded areas. To further enhance the quality of the generated scene, we designed a refinement model based on MinkUnet [24]\u2013[27] to densify the point cloud.\nA. Denoising Diffusion Probabilistic Models (DDPMs)\nHo et al. [19] introduced DDPMs to produce high-quality images through iterative denoising from Gaussian noise. This promising capability is driven by a residual learning mechanism that efficiently captures the data distribution. Specifically, the process begins with a forward diffusion step, during which noise is gradually added to the target data over T steps. The model is then trained to predict the noise added at each step. By predicting and removing noise at time step t, the model generates results that closely approximate the raw data distribution."}, {"title": "1) Forward Diffusion Process", "content": "Assuming a sample x0 ~ q(x) from a target data distribution, the diffusion process gradually adds noise to xo over T steps, producing a sequence x1,...,xT. When T is large enough, q(xT) is approximately equal to a normal distribution (0, I). The intensity of noise added at each step is defined by the noise factors \u03b21,..., \u03b2T, which significantly influences the performance of the diffusion model. Specifically, at step t, Gaussian noise amplified by \u03b2t is sampled and added to xt-1. In [19], the noise parameter \u03b2t is determined using a linear schedule, starting from an initial value \u03b20 and linearly increasing over T steps to a final value \u03b2T. Subsequently, several improved noise schedules have been proposed, such as the cosine schedule [28] and the sigmoid schedule [29]. Due to the inefficiency of adding noise step by step, especially during training, where the noise from different steps can be shuffled, one can simplify this process by sampling \u00e6t from x0 without computing the intermediate steps x1,...,xt-1. To achieve this, Ho et al. [19] define at = 1 \u2212 \u03b2t and \u0101t = \u03a0it=1ai, allowing xt to be sampled as:\nxt = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon\n(1)\nwhere \u2208 ~ N(0, I). It is important to note that as Tis large enough, q(xT) approaches N(0, I) because \u0101r tends to zero."}, {"title": "2) Reverse Denoising Process", "content": "The denoising process reverses diffusion and aim to recover the original sample xo from Gaussian noise. This is accomplished by a denoiser, which predicts and removes the noise at each step. The reverse diffusion step can be formulated as:\nxt-1 = \\frac{1}{\\sqrt{a_t}}(x_t - \\frac{1 - a_t}{\\sqrt{1 - \\bar{a}_t}}\\epsilon_{\\theta}(x_t,t)) + \\sqrt{\\frac{1 - \\bar{a}_{t-1}}{1 - \\bar{a}_t}}\\beta_tN(0,I)\n(2)\nwhere e(x,t) is the noise predicted from xt at step t. The process of generating the original data can be formulated as a Markov process that repeatedly calls the denoiser until t = 0. At this point, the model generates a result that approximates xo. Due to the denoiser effectively learning the high quality of the data distribution q(x), the generated samples are of similarly high quality.\nWhile the denoising process generates samples with quality similar to the dataset, it only produces random samples. Hence, the denoising process cannot control the generation of specific desired data, which poses challenges for certain downstream applications. [28] addresses this issue by introducing conditional inputs to guide the generation process. This advancement allows us to apply diffusion models to tasks like SSC."}, {"title": "B. Diffusion Semantic Scene Completion", "content": "Regarding the principles of DDPMs, we introduce its application in SSC. To focus on the main components, we assume that primary semantic segmentation has been obtained using Cylinder3D. In the context of the diffusion model, the input is a partial semantic point cloud X = {x1,...,xN}, where each semantic point \u00e6\u00b2 is a tuple of a point position and a semantic probability vector (p\u00b2, s\u00b2). Here, p\u00b2 \u2208 R\u00b3 represents the 3D coordinates, and s\u00b2 \u2208 \\triangle^{C-1} = {s \u2208 RC | \u03a3Cs=1si = 1,s \u2265 0} lies in the standard (C \u2013 1)-dimensional simplex, assuming there are C classes in total. The output estimates complete point cloud y = {\u0177\u00b9,...,\u0177M}. We generate the reference Y = {y\u00b9,...,yM} by fusing multiple frames with ground-truth semantic labels and then taking the corresponding region as the input scan X. Our goal is to make the estimated as close as possible to the ground truth Y.\nAs mentioned in Sec. I, by learning scene priors, the model gains the ability to estimate a complete scene (posterior) from partial observations (likelihood). The diffusion model efficiently learns the distribution of the ground truth data, acquiring knowledge of the scene prior. To achieve this, we gradually add noise to the ground truth y, resulting in \u04231,..., \u0423\u0442, until Vr approximates a Gaussian distribution. This form of data augmentation can be simplified using Eq. 1. However, this approach does not directly apply to"}, {"title": "semantic scene completion", "content": "In Eq. 1, the diffusion process is defined as a combined distribution of the sample xo and global noise \u2208 ~ N(0,I), with coefficients \\sqrt{\\bar{a}_t} and \\sqrt{1 - \\bar{a}_t} controlling the ratio of noise and sample at different time steps. This mechanism was originally designed for images, which are flattened and normalized before global noise is added. Similarly, this approach has also shown good results in shape completion, as the shapes are generally close to a 3D Gaussian distribution. However, in large-scale scenes, the data distribution deviates significantly from a Gaussian, particularly due to varying data ranges across different point cloud axes. Applying global noise to the entire scene point cloud as a single entity can obscure important details, failing to account for the unique characteristics and distribution of each point. Therefore, we reformulate the diffusion process as a noise offset \u2208 \u2208 R3+C added locally to each point ym \u2208 y, as shown in the following equation:\nym = y^m + \\sqrt{1 - \\bar{a}_t}\\epsilon\n(3)\nwhere e is not an isotropic Gaussian distribution, because of different scaling for 3D positions and semantics.\nAlthough training on ground truth allows the model to generate high-quality results, the process remains inherently uncontrollable. The goal of SSC is to predict a complete scene from partial input, rather than randomly generating scenes. Therefore, during training, we also use the partial semantic point cloud X as a conditional input, feeding it into the model to guide the point cloud generation. During training, we load a random step t \u2208 [0,T] at each iteration and compute the corresponding Vt using Eq. 3. The model is trained to estimate the noise at various intensities with the following loss:\nL_2(V_t, X, t) = ||\\epsilon - \\epsilon_{\\theta}(V_t, X, t)||^2\n(4)\nIn traditional loss design, the predicted result is directly compared to the ground truth. However, our approach uses residual learning, where the model's output is compared to the residual. Therefore, to generate the final scene, the estimated noise must be removed from the noised scene.\nDuring inference, the model begins denoising from Gaussian noise and iteratively predicts and removes the noise in the samples, ultimately generating a dense semantic scene. Since ground truth is not available during inference, Gaussian noise is generated from X. We duplicate the points in X to match the quantity in the ground truth, ensuring that there are enough points to perform the diffusion process using Eq. 3. Note that the denoiser also takes the partial semantic point cloud X as a condition to guide the generation process."}, {"title": "C. Denoiser Design and Regularization", "content": "As shown in Fig. 3, the denoiser is based on the MinkUNet architecture [24]\u2013[27]. Given the feature F extracted from a layer of MinkUNet, we integrate the conditional input and step information between layers to obtain the fused feature F'. The raw semantic point cloud X is encoded as a conditional input C using the same MinkUNet encoder. To embed the most relevant conditional input into the feature space,"}, {"title": "Vt\nF\nX", "content": "Zoomed In\nC\nMinkUNet Denoiser\nt\nT\nW\nW\nF'\nPositional Encoding Closest Point\nConcat.\nElement-wise Mult. MLP\nEncoder\nArchitecture of our MinkUNet Denoiser. The conditional input, detailed in the red area, is inserted between each layer of denoiser to guide the generation of point cloud.\na closest point algorithm is employed to effectively align the conditional input with the features. Simultaneously, the step t is encoded as 7 using sinusoidal positional encodings. After passing through an MLP individually, the conditional input and step information are concatenated to form the weight W. To align the dimensions with the feature F, W is processed through an MLP to produce W'. Finally, W' and F are element-wise multiplied to form the refined feature F', which is then passed to the next layer.\nSince noise is added to local points during the diffusion process, we employ the commonly used L2 loss between the added noise and the model's predictions. Additionally, we design a regularization term for the global characteristics of the noise, by specifically focusing on the mean \u00b5\u03b5 and variance \u03c3\u03b5. Thus, the loss can be formulated as follows:\nL = L_2 + \\lambda(L_{mean} + L_{var}) = L_2 + \\lambda(\\mu_\\epsilon^2 + (\\sigma_\\epsilon - 1)^2)\n(5)\nwhere Ldiff is the regularization term focused on local features, commonly used in DDPM models, while Lmean = \u00b5\u00b2 and Lvar = (\u03c3\u03b5 \u2212 1)\u00b2 are designed to ensure the overall noise distribution aligns with a Gaussian distribution."}, {"title": "D. Refinement", "content": "Inspired by Lyu et al. [30], we design a refinement and upsampling scheme based on MinkUNet to further enhance the density of the diffusion model's output. This module predicts k bias bk \u2208 R\u00b3 for each point position in the completed scene, while the semantics are propagated to the biased points. The refinement module offers a marginal improvement in scene quality, but it functions more like interpolating points in the gaps, rather than learning to predict missing geometry and semantics. The main contribution is made by the diffusion model, as will be demonstrated in the ablation study."}, {"title": "IV. EXPERIMENT", "content": "A. Experiment Setup\n1) Datasets: We evaluate our approach using the SemanticKITTI [31] and SSCBench-KITTI360 [32] datasets. SemanticKITTI is a widely used autonomous driving dataset that provides point-wise annotations on raw LiDAR point"}, {"title": "clouds, extending the KITTI dataset to semantic study", "content": "Additionally, it builds the SSC benchmark by accumulating annotated scans within sequences. SSCBench-KITTI360 is another SSC benchmark derived from KITTI-360 [33], featuring LiDAR scans encoded the same as SemanticKITTI. This consistency allows SSC methods evaluated on SemanticKITTI to be seamlessly transferred to the KITTI-360 scenario. However, these SSC benchmarks only use the front half of the LiDAR scan (180\u00b0 LiDAR field-of-view (FoV)) as input, which is not ideal for LiDAR-centered point cloud data. To address this, we additionally incorporate the rear half of the point cloud, facilitating the evaluation of SSC approaches on LiDAR-centered data. Our model is trained and validated purely on SemanticKITTI, using sequences 00-06 for training and sequences 09-10 for validation. We evaluate our model using the official validation sets of both datasets: sequence 08 of SemanticKITTI and sequence 07 of SSCBench-KITTI360.\n2) Training and Inference: To train DiffSSC on the 360\u00b0 LiDAR FoV, we generate the ground truth following the guidelines of SemanticKITTI. First, given the pose of each frame, we construct the global map by aggregating the semantic LiDAR sweeps within the sequence. Next, we extract the neighboring region around the key frame, specifically, a spherical area centered on the LiDAR with a radius of 60 meters. The model is trained on an NVIDIA A6000 GPU for 20 epochs. For the diffusion parameters, we employ a cosine schedule to modulate the intensity of noise at each step. Specifically, we set \u03b20 = 3.5 \u00d7 10-5 and \u03b2T = 0.007, with the number of diffusion steps T = 1000, and define \u04121,..., \u0412\u0442-1 using the following equation.\n\\beta_t = \\beta_0 + \\frac{1}{2}(1 + cos(\\frac{t}{T}\\pi))(\\beta_T - \\beta_0)\n(6)\nWe also set the ratio of global regularization to \u03bb = 5.0.\n3) Baselines: We compare our approach against LMSC-Net [12], JS3C-Net [15], and LODE [34]. Both LMSCNet and JS3C-Net take the front half of the quantized LiDAR sweep as input and are evaluated in the SSC benchmark of SemanticKITTI. LODE primarily focuses on geometry completion using implicit representation. However, to demonstrate its flexibility, the authors also report results with extended semantic parsing. A common limitation among these baselines is that they are trained on point clouds within a 180\u00b0 LiDAR field of view. To fairly compare with our method, we split the 360\u00b0 LiDAR point cloud input into two halves and feed them separately into the baselines. The outputs from these two halves are then concatenated to obtain a 360\u00b0 result. Additionally, while these baselines have only been tested on SemanticKITTI, we also ran them on SSCBench-KITTI360 as a supplementary experiment. Since the semantic labels and overall pipeline in SSCBench-KITTI360 are consistent with SemanticKITTI, the baselines can be seamlessly applied to this dataset.\n4) Evaluation Metrics and Pipeline: Despite our task being set in a 360\u00b0 LiDAR point cloud completion context, we aimed to retain the baseline settings as closely as possible to ensure a fair comparison. In the raw SSC setting of SemanticKITTI, the scene is limited to a cuboid region, represented in the LiDAR's local coordinate system as: Vkitti = {(x,y,z) | x \u2208 [0,51.2] m, y \u2208 [\u221225.6, +25.6] m, z \u2208 [-3.2, +3.2] m}, which corresponds to a region associated only with the LiDAR's [-90\u00b0, +90\u00b0] FoV. To cover the full [-180\u00b0,+180\u00b0] panoramic range while preserving spatial symmetry, we selected an evaluation region within the LiDAR's local coordinate system defined as: Vours = {(x, y, z) | x \u2208 [\u221251.2, 51.2] m, y \u2208 [\u221225.6, +25.6] m, z \u2208 [-3.2, +3.2] m}.\nWe directly used the baselines' official code and checkpoints to predict the front and rear parts of the scene, with each part of the LiDAR sweep input separately. Although the baselines were trained only on the front part of the scene, the statistical characteristics of LiDAR data in the front and rear regions are similar, suggesting that a model trained using only the front half of the data remains effective for the rear region as well. Although our ground truth generation covers a spherical region with a radius of 60 meters centered on the LiDAR, we limited our evaluation to the region predicted by the baselines. Additionally, the unknown areas defined by the raw dataset were mapped into Vours using known poses, and these unknown areas were excluded from the evaluation.\nAlthough our method operates directly on point clouds, point clouds cannot represent continuous regions in space, which makes direct evaluation using traditional IoU challenging. Therefore, we voxelized our results and used traditional IoU for scene completion and mIoU for semantic scene completion evaluation. While this introduces quantization error and potentially degrades our model's performance, it aligns with the baseline settings and preserves their performance for a fair comparison."}, {"title": "B. Main Results", "content": "Based on the experimental setting described above, we present the results in Tab. I. Our results include the direct predictions from DiffSSC and the outcomes after refinement. We do the ablation study without the diffusion model, which means directly refining the output of Cylinder3D. We also report the results of baselines as a comparison."}, {"title": "C. Model Analysis", "content": "1) Noise Schedule: As mentioned in Sec.III, the noise schedule determines the intensity of noise added at each step, commonly including linear, cosine, and sigmoid schedules. We conducted a series of experiments to identify the most effective noise schedule for the SSC task. In Fig.5a, we present the training curves sampled at each epoch, highlighting the convergence patterns for each schedule. Additionally, we compare the output of DiffSSC without refinement using each noise schedule in Tab.II, providing insights into their impact. The linear schedule, the simplest, was primarily used in early research. It shows slow and stable convergence, and its performance is significantly lower than that of the other two schedules. The cosine schedule, an improved function, introduces noise more gradually at the beginning and end, with a faster increase in the middle, balancing faster convergence with high final generation quality. The sigmoid schedule shares similarities with the cosine schedule, featuring an S-shaped curve but offering more precise control over the noise introduction, theoretically providing even greater potential. As a result, the cosine schedule converges significantly faster than the linear one. Although the sigmoid schedule does not converge as quickly as the cosine schedule, it is still noticeably faster than the linear schedule. They perform comparably, though the sigmoid schedule is slightly weaker than the cosine schedule. Therefore, in our main results, we adopted the cosine schedule.\n2) Regularization: We also investigated the model's performance on SemanticKITTI under different ratios of global regularization, as shown in Fig. 5b. When \u03bb = 0, indicating the use of only local L2 regularization without global regularization, the model exhibited the worst performance, highlighting the benefits of incorporating global regularization. As the ratio increases, the model's performance improves, peaking at \u03bb = 5 before declining. This suggests that excessive global regularization can constrain the model's ability to generate finer details."}, {"title": "V. CONCLUSIONS AND OUTLOOK", "content": "We proposed DiffSSC, a novel SSC approach based on a diffusion model. It takes raw LiDAR point clouds as input and jointly predicts missing points along with their semantic labels, thereby extending the application boundaries of diffusion models. We evaluated our method on two autonomous driving datasets, achieving performance that surpasses the state-of-the-art. In future work, we will explore methods to enhance inference speed by streamlining the step-by-step inference process, enabling the application of diffusion models [35]. Regarding the impact of noise schedules, we will also explore more complex yet efficient scheduling mechanisms, such as adaptive schedules [36]."}]}