{"title": "Interpretable Text Embeddings and Text Similarity Explanation: A Primer", "authors": ["Juri Opitz", "Lucas M\u00f6ller", "Andrianos Michail", "Simon Clematide"], "abstract": "Text embeddings and text embedding models are a backbone of many AI and NLP systems, particularly those involving search. However, interpretability challenges persist, especially in explaining obtained similarity scores, which is crucial for applications requiring transparency. In this paper, we give a structured overview of interpretability methods specializing in explaining those similarity scores, an emerging research area. We study the methods' individual ideas and techniques, evaluating their potential for improving interpretability of text embeddings and explaining predicted similarities.", "sections": [{"title": "1 Introduction", "content": "Embedding models (Reimers and Gurevych, 2019; Gao et al., 2021) are indispensable across numerous NLP tasks in both academia and industry. Applications range from semantic search and information retrieval (Ye et al., 2016; Guo et al., 2020; Muennighoff, 2022; Hambarde and Proenca, 2023; Alatrash et al., 2024) to text classification (Schopf et al., 2022), topic modeling (Grootendorst, 2022), NLG evaluation (Celikyilmaz et al., 2020; Uhrig et al., 2021; Sai et al., 2022; Larionov et al., 2023; Chollampatt et al., 2025), knowledge graph construction (Plenz et al., 2023), and retrieval-augmented generation (RAG)\u2014a specific form of information retrieval leveraging embedding similarity to identify evidence from a large corpus and summarize it using generative Large Language Models (LLMs, Lewis et al., 2020; Gao et al., 2023). Furthermore, advances in base models (G\u00fcnther et al., 2023; Wang et al., 2024a), context size (Li et al., 2023), and scalable training infrastructure (Wang et al., 2022) have steadily enhanced the capabilities of embeddings.\nHowever, an urgent challenge persists: the problem of interpretability. For example, when a set of most similar documents is retrieved in response to a query, we would like to articulate why these documents were selected as the most similar, or why a particular document was omitted. Similarity variables are also highly conflated: Were the obtained similarity ratings based on semantic similarity, relatedness, paraphrasticity, relevance-subtly distinct concepts (Budanitsky and Hirst, 2006; Kolb, 2009; Michail et al., 2025)\u2014or were they influenced mainly by superficial characteristics such as token overlap? And such questions are not just theoretical; they have significant practical implications. For instance, embedding systems deployed in sensitive domains may need to justify outputs, perhaps even in a legal context.\nFortunately, recent research has begun addressing this interpretability gap. Our paper aims to serve as a primer for researchers and practitioners that seek to understand embedding-based similarity models and measurements. By presenting a structured overview of interpretability approaches, we hope to ease entry into this area and inspire further innovations. Understanding how similarity is computed and how it can be explained\u2014not only enhances transparency but may also pave the way for improved methods and applications."}, {"title": "2 Setting the Stage", "content": "We study explainability in neural text embeddings and their induced similarity. We distinguish this from common approaches to classification-explanation like \u2018LIME' (Ribeiro et al., 2016), or 'Shapley values', see also Calderon and Reichart (2024)'s survey. Similarity is not based on a single input but rather the interaction of two inputs, hence the need for specialized methods.\nNotation. Assume a (tokenized) input text = $[t_1, ..., t_n]$, and two neural networks F, G consisting of L layers, each representing a function, e.g., in the case of F: $F = f_L \\circ ... \\circ f_1$. Typically F = G, i.e., the weights of the two networks are shared,"}, {"title": "3 Shaping Interpretable Spaces", "content": "3.1 Idea\nThese approaches aim at structuring the embedding space such that it becomes more interpretable. E.g., the space can be shaped to express aspects, interpretable geometries or probabilistic distributions.\n3.2 Approaches\n3.2.1 Feature Decomposition\nTraditional methods for text similarity often relied on explicit \"bag-of-words\u201d feature representation. While this provides great transparency in representation and similarity calculation, it lacks the representational power of neural embeddings, cannot match paraphrases, and thus result in relatively poor performance on standard benchmarks. Recent efforts aim to combine the interpretability of features with the power of neural embeddings.\nQ/A features. This approach involves framing embedding generation as answering a set of predefined questions about a text and encoding the answers as features, enabling interpretability. For this, we first need to find a suitable set of questions about texts, and create training data that elicits answers to these questions. Afterwards, we can distill an efficient and interpretable text embedding model using this training data. Specifically, Benara et al. (2024) let an LLM answer \u201cYes\u201d/\u201cNo\u201d questions"}, {"title": "3.2.2 Non-Euclidean Geometry", "content": "Certain text relationships are inherently asymmetric. For instance, a natural relation between texts is entailment: A given hypothesis follows from a premise. Some embedding geometries offer a way to model these relationships.\nAn interesting example are box-embeddings: Consider all two-dimensional boxes centered at zero with their left bottom corner. For two such boxes a and b we have their size $s_a = a_1 \\cdot a_2$, $s_b = b_1 \\cdot b_2$, and their overlap $o_{a,b} = min(a_1,b_1) \\cdot min(a_2,b_2)$.\nWe arrive, e.g., at a similarity $o_{a,b}/(s_a + s_b - o_{a,b})$, and interesting asymmetric relationships like the containment or entailment of, e.g., a in b: $o_{a,b}/s_b$ \u2014 it's exactly 1 if a is fully contained/entailed in/by b. The challenge is to learn such objects in high dimensionality: To see a major bottleneck, consider that box size and overlap approach zero in high dimensionality, since they involve a large product. To alleviate such learning problems, Chheda et al. (2021) propose to adopt a probabilistic soft box overlap formulation based on Gumbel random variables (Dasgupta et al., 2020).\nOn the other hand, Huang et al. (2023) learn interpretable composition operators, such as union/fusion, or difference, by modeling the operators with neural networks, and retraining the embedding models such that their space is shaped for operator allowance. Evaluation shows little loss on"}, {"title": "3.2.3 Combining Token Embeddings", "content": "Combination-based approaches build a new embedding space by aggregating token-level representations with explicit weights that reflect their importance. E.g., Wang and Kuo (2020) estimate token importance and novelty using variance across transformer layers, constructing weighted embeddings. On the other hand, Seo et al. (2022) train models to learn token weights directly, using a reconstruction loss to prevent catastrophic forgetting. Alternatively, we can create static embeddings for all tokens in the vocabulary, using one transformer forward pass (for each token), and then calculate a simple average that is informed by Zipf token statistics (Tulkens and van Dongen, 2024)."}, {"title": "3.3 Challenges and Opportunities", "content": "Q/A approaches can outperform certain baselines, but they do not (yet) fully seem to match the performance of reference embedding models with distributed features, probably since it is difficult to find a generalizable set of questions.\nSimilarly, the sub-embedding decomposition approaches requires the definition of custom aspects, and the features are not directly interpretable on their own -only their similarity value is.\nOn one hand, crafting the right features (through questions or interpretable metrics) can be seen as a drawback of the feature based approaches. How-"}, {"title": "4 Set-based Interpretability", "content": "4.1 Idea\nSet-based approaches to similarity explainability rely on matching two sets, rather than two points. These sets typically consist of human-interpretable items, e.g., tokens. Aligning these, we may be provided with insight into how different text parts relate to each other. Sets also offer inherent interpretability through set-theoretic operations that may align naturally with some interesting semantic text relationships (e.g., entailment as subset).\n4.2 Approaches\n4.2.1 Embedding Set Interpretability\nAlignment-based methods derive similarity by aligning token embeddings from one text with those of another. These approaches typically use embeddings from the last layer of a model. Two focal techniques in this category are \u201cColBERT\u201d and \"BERTscore\". The ColBERT approach (Khattab and Zaharia, 2020; Santhanam et al., 2022) computes an asymmetric alignment by viewing x as the query and y as the candidate (aka \"passage\") using two encoders; in our notation, F := Q and G := C. For each individual token embedding in the embedded query $E_x$ = Q(x), we search in the embedded candidate tokens $E_y$ = C(y) for the best match and sum over those. For computing a similarity score, both methods rely on a greedy max-matching, formally, $sim(x,y) = \\sum_{t \\epsilon x} max([Q(x)^T C(y)]_t)$. The BERTscore approach, which is used in the evaluation of machine translation (Zhang et al., 2020), further computes a symmetric harmonic mean (F1 score) as $HM(sim(x, y), sim(y, x))$.\nWhile the asymmetry in ColBERT is also achieved through the different encoders, the asymmetry in BERTscore is achieved by the calculation of precision and recall, hence their different application cases (IR vs. evaluation). In their potential for explainability, both approaches appear similar: Their similarity score can be seen as constructed from an interpretable alignment from one docu-"}, {"title": "4.2.2 Explicit Multi-Interpretation", "content": "Most set-based approaches use token embeddings, but some extend the concept to text embeddings.\nThe first class of methods generates sets of text embeddings by either hypothesizing about a text or decomposing it into smaller parts. In particular, we can use a generative model to construct hypotheses about a text (Hoyle et al., 2023), or decomposing it into smaller statements or descriptions (Ravfogel et al., 2024). Having deconstructed a text x into smaller parts ${x_1, ..., x_n}$, we call our text embedding model exactly n times, and thus construct a set of n respective text embeddings ${e_1, ...e_n}$ that can be matched to explain similarity of facts contained in a text, also with different abstractness levels.\nAn interesting variation of such a multi-text set-based approach is proposed by Liu et al. (2024). To compute the similarity of two texts, they sample sets of possible continuation from an LLM, and calculate the average (i.e., expected) difference in log-likelihood between the two input texts that are continued with a randomly sampled continuation."}, {"title": "4.3 Challenges and Opportunities", "content": "Set-based approaches allow an interpretable alignment of token-level embeddings. This alone has useful applications; for instance, to elicit token-level semantic differences between related documents (Vamvas and Sennrich, 2023). Sets also allow an intuitive view on asymmetric text relationships, increasing their explanation appeal in asymmetric tasks like NLI. However, it is crucial to note that token embedding alignment does not equate to input token alignment, as the contextualization steps may obscure the actual contributions of input tokens and any thereupon based explanation.\nWe also saw that we can abstract from sets of token embeddings to sets of text embeddings, e.g., by decomposing a text into smaller statements, before generating embeddings. At the cost of a greater number of inferences, the interpretability potential of this class of explicit multi-interpretation based approaches is that they can deliver evidence about which statements conveyed by the texts are actually matching and contributing to the overall similarity."}, {"title": "5 Attribution-based Interpretabilty", "content": "5.1 Idea\nAttribution-based approaches aim at attributing a model prediction onto input or intermeditate feature representations. In other words: they assign importance values to features for how much they contribute to a given prediction. However, a special characteristic of similarity models is that their predictions do not depend on individual features, due to the multiplicative interaction between the two inputs' embeddings in sim. Thus first-order methods do not suffice (Sundararajan et al., 2020; Janizek et al., 2021). Instead, second-order methods are required to attribute predictions of similarity models.\n5.2 Approaches\nTwo lines of work have addressed this issue in text similarity models: integrated Jacobians, an extension of the theory behind integrated gradients to Siamese models (Moeller et al., 2023, 2024) and"}, {"title": "5.2.1 Integrated Jacobians", "content": "Integrated gradients (IG) attributes a scalar model prediction back onto individual input features by integrating over a number of interpolations between the actual input and an uninformative reference input (Sundararajan et al., 2017). The method can provide a closed-form solution to explaining the difference in the model prediction between the reference and the actual input. Its output takes the form of a vector with importance values for all input features. Moeller et al. (2023) have applied the underlying theory of IG to Siamese encoders, enabling the attribution of similarity predictions onto feature-interactions between the two inputs. Different from IG, the output takes the form of a feature-pair attribution matrix. For text encoder models it can be reduced to a token-token matrix, showing the contribution of token interactions to the sim (Figure 4). The authors provide an exact version of their attributions, which guarantees that the sum over the attribution matrix must exactly equal the predicted similarity score. This version also enables the quantification of an attribution error, however, it requires an adjustment of the models through fine-tuning. Alternatively, approximate attributions can be calculated for any off-the-shelf model without a need to adjust it (Moeller et al., 2024). The approximation has been shown to correlate sufficiently with the exact variant."}, {"title": "5.2.2 BiLRP", "content": "Layer-wise relevance propagation (LRP) is a framework to propagate feature-importance values for a model prediction back through the model in a layer-wise fashion (Bach et al., 2015; Montavon et al., 2019). Propagation rules are derived for individual layers based on first-order Taylor expansion of the underlying function. Thus, the approach essentially linearly approximates all computations in a model's graph around a given reference input.\nBiLRP extends the LRP framework to Siamese similarity models by computing LRP values for each embedding dimension of the two encoders separately and subsequently taking their matrix product. Thus, the computation also takes the form of a product between two Jacobian-like matrices. Whereas, integrated Jacobians constructs these matrices by integrating over interpolated inputs, in BiLRP they originate from the layer-wise propagation rules. The method was originally proposed in the computer vision domain (Eberle et al., 2020) and has recently also been applied to Siamese text encoder models (Vasileiou and Eberle, 2024)."}, {"title": "5.3 Challenges and Opportunities", "content": "Attribution approaches need to build Jacobian matrices, coming at a temporal complexity of 2 \u00d7 D independent backward passes, D being the model's embedding dimensionality. The resulting Jacobians have a quadratic spatial complexity of D \u00d7 $D_{in}$.\nWith $D_{in}$ being a sequential representation, the required memory can grow quickly for higher D or long inputs, requiring large GPUs to compute the associated matrix multiplications efficiently.\nInspite of these computational costs, attribution based methods have the advantage of being model agnostic, not requiring additional design choices on model architectures or training objectives. In contrast to (last-layer-embedding-)set based approaches like ColBERT, they also relate more directly to the actual input tokens. Different from space shaping approaches they also do not pose any constraints on embeddings during training."}, {"title": "6 Discussion", "content": "6.1 Summary and Overview of Approaches\nWe identify general features that allow for summarizing and comparing similarity interpretability approaches from a broader perspective. We propose the following key features for categorization:"}, {"title": "6.2 Pertinent Challenges", "content": "Mitigate tradeoffs. Methods differ in their conceptualization of interpretability, computational cost, fidelity to input tokens, and eventual dependencies to a specific model as their basis. Space shaping is highly adaptive and allows to express different semantic aspects of interest, or model intuitive relations with non-Euclidean spaces -but tends to require custom training and definitions that may not generalize. Then, methods that compute the similarity from two sets of embeddings (token embeddings, or text embeddings) can produce a visually inspectable alingment and allow for interpretable asymmetric matching with set-operators -but the last layer's embeddings are highly contextualized and thus are technically no"}, {"title": "What's the \"right\" explanation?", "content": "Unfortunately there may be no straightforward answer to this question. Given the above trade-offs, no method can be seen to be guaranteed faithful (Murdoch et al., 2019). Therefore, none of these approaches should be interpreted as true and unique explanations for model predictions. At the same time all of them provide insights into similarity models going beyond a single scalar similarity score. We argue, that while we cannot conclude individual methods"}, {"title": "7 More Explanations and Related Work", "content": "In this last section, we study related work on similarity interpretability. We want to focus on evaluation studies and datasets that elicit explanations.\nDatasets. Lopez-Gazpio et al. (2017) release the i(nterpretable)STS data set that elicits relations and similarities between individual segments of texts. Deshpande et al. (2023) propose the C(onditional)STS dataset that elicits similarity values for specific aspect of interest. The theory that underlies iSTS aligns with attribution or set-based approaches, while CSTS is motivated by a more abstract multi-aspect view akin to what is sought by feature-based explainability methods.\nThe STS3k data set (Fodor et al., 2024) consists of systematically controlled pairs of sentences rated for semantic similarity by human participants. Through experiments on STS3k they find that \"state-of-the-art transformers poorly capture the pattern of human semantic similarity judgments.\"\nDatasets might also be repurposed for interpretability studies. E.g., evaluation data that highlight error spans (Freitag et al., 2021; Leiter et al., 2023), or explained interactions between text spans in NLI (Ray Choudhury et al., 2023).\nSimilarity interpretability studies. Nikolaev and Pad\u00f3 (2023) construct sets of sentences with pre-defined lexical and syntactic structures. Their study reveals that model-assigned similarities are more strongly determined by the overlap in the set of their noun participants than by having the same predicates, lengthy nominal modifiers, or adjuncts. Weller et al. (2024) ask similarity models to rank documents that differ only by negation. They find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking.\nNastase and Merlo (2024) track linguistic information in embedding models via specialized datasets that test for grammatical and semantic agreement, finding that aspectual semantic knowledge can be localized in certain embedding regions."}, {"title": "8 Conclusion", "content": "What makes two texts similar in the eyes of a model? We gave an introduction and overview of an emerging branch of text embedding models: The challenge of similarity interpretability and explanation. We hope that our work can be a handy resource and entrance point for future research."}, {"title": "Limitations", "content": "Capturing the full breadth of the emerging area of interpretable text embeddings and their similarity cost us some depth and exactness. Particularly in \u00a76 where we summarized the methods over all three general interpretability approaches, we introduced some fuzzy and coarse concepts, e.g., \"token-set\" (rather: token-embedding set [from the last layer]); k in inference cost (sometimes the size of encountered token or embedding sets, sometimes it's model encoder calls), etc. Nevertheless, the slight fuzziness also helped us to discuss and compare the diverse methods that have varying degrees of complexity-simple set matching to second-order attribution methods based on Jacobians of encoders-from a thousand foot view. There is also a chance that we missed some papers, hence we suggest viewing our primer as a guide and a survey that is representative of the area, but possibly not fully exhaustive.\nAbout the three examples shown in our paper: They are selected to outline the idea behind three different types of approaches. We selected different text pairs to highlight these ideas, in order to avoid any possible potential for invoking example readings that favor one method over the other, which is not possible based on a single example, anyway. We leave deeper qualitative comparison of provided explanations to future work, and discussed the need for this in \u00a76.2."}]}