{"title": "Prediction Accuracy & Reliability:\nClassification and Object Localization\nunder Distribution Shift", "authors": ["Fabian Diet", "Moussa Kassem Sbeyti", "Michelle Karg"], "abstract": "Natural distribution shift causes a deterioration in the perception\nperformance of convolutional neural networks (CNNs). This comprehensive\nanalysis for real-world traffic data addresses: 1) investigating the effect of\nnatural distribution shift and weather augmentations on both detection\nquality and confidence estimation, 2) evaluating model performance for\nboth classification and object localization, and 3) benchmarking two com-\nmon uncertainty quantification methods - Ensembles and different variants\nof Monte-Carlo (MC) Dropout - under natural and close-to-natural distribu-\ntion shift. For this purpose, a novel dataset has been curated from publicly\navailable autonomous driving datasets. The in-distribution (ID) data is\nbased on cutouts of a single object, for which both class and bounding box\nannotations are available. The six distribution-shift datasets cover adverse\nweather scenarios, simulated rain and fog, corner cases, and out-of-distribu-\ntion data. A granular analysis of CNNs under distribution shift allows to\nquantize the impact of different types of shifts on both, task performance\nand confidence estimation: ConvNeXt-Tiny is more robust than EfficientNet-\nB0; heavy rain degrades classification stronger than localization, contrary to\nheavy fog; integrating MC-Dropout into selected layers only has the poten-\ntial to enhance task performance and confidence estimation, whereby the\nidentification of these layers depends on the type of distribution shift and\nthe considered task.", "sections": [{"title": "1 Introduction", "content": "When deploying deep neural networks to real-world applications, such as perception\nfor autonomous driving, challenging scenes usually cause a decline in detection perfor-\nmance. These scenes are often characterized by conditions such as adverse weather, low\nvisibility, rare objects, or unfamiliar traffic environments, e.g., regional differences in\ntraffic signs, vehicle types, emergency and attention signals, or road layouts. Therefore,"}, {"title": "2 State of the Art", "content": "Given the importance of understanding and quantifying uncertainty for building robust\nand reliable models, a significant body of work exists on UQ methods in general. How-\never, it is crucial to emphasize the importance of their analysis under distribution shift. \nA distribution shift occurs when the training and test data originate from different dis-\ntributions, posing a challenge in generalizing from one to the other [18\u201320]. Such sce-\nnarios are inevitable when dealing with data gathered from the real world, which is\ninherently non-stationary. Consequently, they can manifest abruptly, evolve gradually,\nor follow seasonal patterns, triggered by specific events or due to a slowly changing\nenvironment [21].\nTwo significant factors for distribution shift are sample selection bias and non-sta-\ntionary environments. Sample selection bias occurs when the training data is gathered\nunevenly from the underlying population, rendering it unreliable in representing the\nactual real-world operating conditions. On the other hand, non-stationary environments\nrefer to scenarios, where the distribution undergoes changes over time. This may then\nlead to a deterioration in the model performance as the environment evolves [18, 22,\n23]. These two factors can pose significant challenges, particularly in the dynamic en-\nvironments of real-world applications such as autonomous driving, where distribution\nshifts are widely prevalent [11, 24].\nDistribution shifts can be mainly classified into three types: covariate shift, label or\nprior probability shift, and concept shift. A graphical notation of the types of shifts and\n12 additional sub-types of shifts can be accessed under the work of Kull and Flach [25].\nCovariate shift, one of the most studied types of shifts [26], occurs when the input or\nsource distribution changes between the training and the test data, while the conditional\ndistribution between the input and output remains constant. An example of covariate\nshift can be observed when a model is initially trained using data collected in an urban\ncity center but is subsequently tested on data originating from mountainous regions.\nPrior probability shift represents the opposite scenario, where the input features are\nassumed to be constant, but the distribution of the dependent variable (output or labels)\nis not. This type of shift arises when the prior assumptions made about the output dis-\ntribution no longer hold, which is especially common in cases involving imbalanced\ndatasets. Lastly, concept shift involves changes in both the input distribution and the\nrelationship between the input features and the dependent target variables. In essence,\nit signifies that the extracted features of the model during training no longer align with\nthose in the test data [18, 21]. In this work, we address all three types with varying\nweather conditions (mainly covariate shift), different datasets (mainly label shift), and\ncorner cases (all three).\nDespite a comprehensive understanding of the origins and variations in distribution\nshifts, specialized benchmarks are scarce for evaluating uncertainty quantification un-\nder distribution shifts [23]. Nevertheless, the evident challenges faced by current"}, {"title": "3 Background and Methods", "content": "As mentioned in Sec. 2, simultaneously investigating regression and classification\ntasks within the same context is crucial for a more general understanding of uncertainty\nand model behavior under distribution shift. This is particularly vital given that a ma-\njority of real-world applications necessitate models that support both tasks. Further-\nmore, it is also imperative to investigate various models and UQ methods to attain a\nhigher degree of generality. This section offers essential details regarding our selection\nof tasks, baseline models, and uncertainty estimation methods, including foundational\nknowledge of each."}, {"title": "3.1 Classification and Regression", "content": "Our baseline models are implemented with a regression and a classification head.\nClassification. The classification head comprises of two fully connected layers\ncontaining n neurons and a ReLU activation. It is followed by a third dense layer with\nthe number of units equal to the classes, using linear activation for computing logits,\nand ending in a final softmax layer. During training, the weights are optimized using\nthe cross-entropy loss.\nRegression. Like classification, the regression head is composed of two fully con-\nnected layers, each with n neurons and a ReLU activation. It also incorporates a third\ndense layer with four units to handle the four localization variables, utilizing linear ac-\ntivation. We use the Mean-Absolute-Error (MAE) loss for training.\nThe interpretation of localization variables varies, particularly within different ob-\nject detection frameworks. Following the conventions of the most prevalent frame-\nworks, we explore three different implementations of the localization head. The Corner Points approach describes bounding boxes using the\ncoordinates of the top-left (x1, y1) and bottom-right (x2, y2) corners, applied for example\nin CornerNet [42]. The Center Point approach defines bounding boxes via the object\ncenter (Cx, Cy), height h, and width w; as implemented in CenterNet [43]. The Anchor-based approach consists of relative size and position of bounding boxes to a predefined\nreference bounding box, i.e., anchor, used in detectors such as SSD [44] and Effi-\ncientDet [45]. For example, the average of all bounding boxes in the training data can\nbe used as a pre-defined anchor. Similar to the Center Point, the network predicts four\noffsets for the center (tx,ty), height (th), and width (tw) in relation to the anchor. The\nfinal bounding box coordinates can then be calculated, for example, using [46, 47]:\n$w = e^{t_w} \\cdot w_a, h = e^{t_h} \\cdot h_a, C_x = t_x \\cdot W_a + C_{xa}, C_y = t_y \\cdot h_a + C_{ya}$"}, {"title": "3.2 Baseline Models", "content": "In recent years, several novel network architectures have been introduced. This work\nexamines three exemplary network architectures, chosen for their distinct characteris-\ntics in terms of parameter count, runtime, and performance. These are ResNet-50, Effi-\ncientNet-B0, and ConvNeXt-Tiny.\nResNet. Residual Networks (ResNet) [48] are characterized by a stack of residual\nblocks that incorporate skip connections and a bottleneck structure. The first is specifi-\ncally designed to allow a transfer of information across the multiple layers of the net-\nwork, hence addressing the vanishing gradient problem and enabling the training of\ndeeper networks. The second improves computational efficiency and reduces the num-\nber of parameters in the network. EfficientNet. EfficientNet [52] is a family of eight models (EfficientNet B0 - B7)\nwith increasing complexity and performance. It is founded on Mobile Inverted Bottle-\nnecks (MBConv) introduced in MobileNetV2 [53]. In contrast to\nResNet, EfficientNet uses a 3x3 depthwise convolution and a swish activation function,\nreplacing the final activation with a linear one to mitigate potential information loss\ncaused by ReLU. It also incorporates a squeeze-and-excitation (SE) layer and a fusion\nof an average pooling and a dense layer to enhance performance with a minimal increase\nin parameters. Due to its superior performance compared to ResNet at 77.1% on\nImageNet [49] and significantly fewer parameters (5.3 vs. 25.6 million) [50], we select\nEfficientNet as a representative model within the category of computationally efficient\nnetworks suitable for real-time applications.\nConvNeXt-Tiny. ConvNeXt [54] represents a novel network architecture that\nblends the strengths of traditional CNNs and vision transformers. It incorporates design\nprinciples from the Swin transformers [55] into ResNet, including a distinct bottleneck\nstructure featuring depthwise convolution and a larger 7x7 kernel compared to ResNet.\nBatch normalization and ReLU are replaced with transformer-style layer normalization\nand GELU activation, reducing the number of normalizations and activations. Further-more, ConvNeXt uses separate downsampling layers with normalization, contributing\nto training stability. It outperforms the Swin transformer on ImageNet [49] in terms of\naccuracy while retaining the simplicity and efficiency of CNNs [54], achieving a top-1\naccuracy of 81.3% for the smallest variant [50]. We, therefore, also select ConvNeXt as a representative model within the cate-gory of computationally efficient networks suitable for real-time applications. Addi-tionally, it serves as a representative example of design principles derived from trans-formers."}, {"title": "Hyperparameter Tuning", "content": "Hyperparameters in neural networks are nontrainable\nparameters that influence the structure of the network, such as the number of hidden\nlayers and activation functions, or its training process, such as learning rate and batch\nsize [57]. Selecting their values carefully is essential for optimal network performance.\nHowever, manually finding the optimal hyperparameters is time-consuming and error-\nprone, leading to the introduction of various optimization strategies. Grid Search sys-tematically explores all possible combinations of hyperparameters, while Random\nSearch follows a similar strategy but randomly samples parameter values from prede-fined distributions [58]. Both methods are computationally intensive as they evaluate\nall parameter combinations. Another approach is Hyperband [59], which accelerates\nRandom Search with adaptive resource allocation and early stopping, evaluating ran-domly selected parameter combinations with a minimal resource budget and continuing\nwith the best candidates. On the other hand, Bayesian Optimization is a model-based\napproach aiming to find the global optimum with a minimal number of trials by pre-dicting the likelihood of success for different parameter configurations [57]. We, there-fore select Bayesian optimization due to its efficient resource utilization in the most\npromising configurations, saving computational time and resources. Furthermore, it is\nable to find the global optimum in the hyperparameter space, making it a robust choice\nfor optimizing neural network configurations."}, {"title": "3.3 Monte-Carlo Dropout", "content": "Dropout [60] is a regularization technique for neural networks that mitigates overfitting\nby selectively deactivating individual neurons during training. For each neuron in the\nnetwork layer where dropout is applied, a sample is drawn from the Bernoulli distribu-\ntion to determine if it is dropped based on a pre-defined threshold. This threshold is a\nhyperparameter referred to as the dropout rate. The configuration of active neurons is\nthen used to update the model weights during backpropagation.\nGal and Ghahramani [61] expand the application of dropout to estimate epistemic\nuncertainty using Monte-Carlo Dropout (MC-Dropout), a technique applicable to rep-\nresentations at various levels, including low, mid, high, and object levels. They demon-\nstrate that the repeated prediction of input data with different dropout configurations\nactive during prediction corresponds to the approximation of Bayesian inference. The\nepistemic uncertainty is therefore calculated as the variance over the Trepetitions of\neach prediction p(y|x, t) with a different dropout configuration on all input data [2,\n61]:\n$p(y|x, D) \\approx \\frac{1}{T} \\sum_{t=1}^{T} p(y|x, \\theta_t)$\nwith the training data of N samples $D = \\{x_n, y_n\\}_{n=1}^{N}$, the approximated predictive dis-tribution p(y x, D) for a data sample x from the input domain X, the ground truth y \u2208\n{1,...,C} for classification with C classes or y \u2208 R\u2074 for localization, and a sample of the\nnetwork weights with dropout from the weights domain W.\nIn practice, MC-Dropout serves as a straightforward technique for estimating the\nepistemic uncertainty. Adding dropout across the whole network affects low-level-fea-\nture, mid-level-feature, high-level-feature, and object-level representations. Neverthe-\nless, its computational intensity due to the need for multiple stochastic passes can pre-\nsent challenges when applied in real-time scenarios. Additionally, as a hyperparameter,\nan improper choice of the dropout rate may result in poorly calibrated uncertainty esti-\nmates, prompting the need for an extensive hyperparameter search for optimal perfor-\nmance, as discussed in [62]. Therefore, as computationally efficient alternatives to full\nMC-Dropout (dropout both in the head and the backbone), this work also explores Last-Layer-Dropout (LL-Dropout), After-Backbone-Dropout (After-BB-Dropout), and\nHead-Dropout. Each one affects repre-\nsentations at different stages of the network and serves various purposes. Therefore,\ntheir analysis offers a granular perspective on UQ.\nIn LL-Dropout, dropout is applied on the last layer before the output layer. This\nmethod primarily influences the object-level representations since it is implemented just\nbefore the final network output. Given the lower number of neurons in this layer com-\npared to the entire network, LL-Dropout is computationally efficient.\nAfter-BB-Dropout also utilizes a single dropout layer, but it is positioned directly\nafter the network backbone, i.e., the initial layers responsible for feature extraction. This\nmethod influences the representations generated by the backbone before they are passed"}, {"title": "3.4 Deep Ensemble", "content": "Deep Ensemble [63] represents an alternative approach for estimating epistemic uncer-tainty. Unlike MC-Dropout, which relies on a single model, Ensembles aggregate thepredictions of multiple independently trained models sharing the same architecture.This approach not only enhances accuracy [64], but also results in a qualitative measureof uncertainty [11, 12, 39]. This can be attributed to their ability to identify distinctpeaks within the parameter space, without being constrained to local uncertaintiesaround a single peak [65]. In contrast to previous methods that often depended on"}, {"title": "4 Dataset Preparation", "content": "The datasets in this work are based on publicly available datasets. This chapter providesan overview of the data curation process and on relevant information for each of thedatasets."}, {"title": "4.1 Motivation of the Dataset Design", "content": "In contrast to object detectors such as SSD [44] or EfficientDet [45], the separate clas-sification and regression networks used in this work are limited to assigning only asingle class and bounding box to each image. Common classification datasets, such asImageNet [66] or MNIST [67], feature large and centered objects. However, real-worldscenarios involve objects situated in various positions relative to the observing vehicle.To address this, we draw inspiration from the YouTubeBB dataset [68], which exhibitsone or multiple objects of one class at a time in random positions in the images. Wefurther constrain our dataset to only one main object per image, to align with the use ofunified networks for both classification and regression. Given the specific focus on au-tomotive applications, we concentrate on the core classes such as vehicles, pedestrians,traffic signs, and traffic lights. Additionally, the data reflects the diversity and authen-ticity of real-world conditions, encompassing a variety of different environments,weather conditions, and times of day."}, {"title": "4.2 Summary of the Methodology for Image Crop Selection", "content": "The dataset creation starts with images having at least one object and their associatedbounding boxes. For each relevant object in the image, the center of the bounding box"}, {"title": "4.3 In-Distribution Dataset", "content": "The ID dataset serves as the basis for the following experiments. It consists of training,validation, and test data. We choose BDD100k [14] (BDD) as the startingpoint for our dataset curation. BDD is recorded in the regions of several US cities, suchas New York, San Francisco, and Berkeley, and has a high diversity of street types,environments, weather conditions, and times of day, all captured in the annotations. Inparticular, BDD has previously served as a benchmark for uncertainty estimation [2],facilitating comparability with existing literature. To ensure scalability in our experi-ments, we set the dataset size according to CIFAR-10 [69]. The training set consists ofup to 5000 images, while both the validation and test sets consist of up to 1000 cutouts"}, {"title": "4.4 Distribution Shift Datasets", "content": "To explore the impact of distribution shift on uncertainty estimates, we generate severaladditional test datasets. As mentioned in Sec. 2, a simple form of distribution shift arisesfrom the use of different sensors or different environments in which the data is recorded.\nFor our comparison, we utilize the KITTI Object Detection training dataset [16], whichconsists of 7841 images recorded in the region of Karlsruhe, Germany. One drawbackof KITTI lies in the disparate definition of object classes compared to BDD, with onlypedestrians and cars being identical between the two. Consequently, only these are con-sidered for our tests. To create the dataset, we follow the same process as for the in-distribution data, resulting in a total of 1000 cutouts of cars and 517 of pedestrians.\nWe follow two approaches for real weather conditions: one dataset represents realimages taken during snowfall and rain and one dataset represents simulated weatherconditions. The real images are based on the Canadian Adverse Driving Conditions(CADC) dataset [17]. CADC is recorded in the Waterloo region of Canada, comprising7,000 scenes from eight different cameras. All test drives are conducted in adverseweather, predominantly snowfall. To ensure comparability with the remaining datasets,we exclusively use images from the forward-facing camera. The CADC images are la-beled as 3D-BB. For the conversion to 2D-BB of the camera coordinate system [70] isused. The conversion resulted in some BBs being significantly too large or incorrectlypositioned. Therefore, 1000 images are manually re-labeled for each of the classes pe-destrians and cars.\nSimulated weather data is created by applying various filters of the Python libraryimgaug [71] to the in-distribution test dataset. Given the abundance of images of snow-fall in CADC, our focus is directed toward simulating fog and rain conditions. Follow-ing the approach of Lakshminarayanan et al. [63], who evaluated distribution shift byrotating the images of the MNIST [11] dataset, we apply two levels of increasing rainand fog. It is worth noting that rain and fog filters produce varying effects on differentimages. The impact is less pronounced on brighter images compared to darker ones."}, {"title": "4.5 Corner Cases and Out-of-Distribution Data", "content": "As mentioned in Sec. 2, distribution shift refers to changes in the distribution betweentraining and testing data. Corner cases and OoD objects emerge when the model en-counters instances during testing that differ significantly from those seen during train-ing, contributing to performance challenges in these scenarios. A more precise defini-tion varies depending on the specific application. In the scope of this study, data sam-ples are categorized as corner cases when the parent class remains constant, yet theobjects are not represented similarly in the training data. For instance, this includesconstruction vehicles such as concrete mixers, initially classified as trucks, or seatedpersons if the training data only includes standing persons. On the other hand, out-of-distribution samples refer to objects that bear no resemblance to those in the trainingdata, such as traffic cones. In terms of classification, this implies that the object cannotbe meaningfully assigned to any of the learned classes.\nTwo more datasets are created to investigate how UQ reacts to corner cases and out-of-distribution data. Both datasets contain images from BDD, KITTI and NuImages.NuImages is part of the NuScenes dataset [15]. It contains 93,000 images taken in theUSA and Asia, and additional OoD classes, such as traffic cones and trash cans, whichare not included in BDD or KITTI. Following our above definition, we define the twoparent classes pedestrians and vehicles for the corner cases. Vehicles include all objectsthat can be assigned to one of the three classes car, truck or bus based on their generalcharacteristics. This includes, for example, special vehicles such as concrete mixers,truck cranes, or trailers as an extended part of the vehicle class. Pedestrians are mainlypeople who differ from the training data due to special postures, such as sitting or bend-ing. The out-of-distribution data includes objects, that cannot be assigned to any parentclass of the training set but are real objects, that frequently occur in traffic. These aremainly stationary objects, such as traffic cones and garbage cans, but also trains andanimals. A total of 438 images are selected for the corner cases and 500 images for theout-of-distribution data."}, {"title": "5 Experimental Results", "content": "We analyze the performance of the three baseline models (ResNet-50, EfficientNetB0,\nand ConvNeXt-Tiny) for the nine datasets. Hyperparameter tuning is performed on the\nvalidation set with KerasTuner [72]; final results are reported for the test set. The opti-\nmized hyperparameters are the number of neurons in the dense layers for the vanilla\nbaseline, the learning rate, and the dropout rate. The number of repetitions is M = 5 for\nthe Ensemble, recommended as sufficient in [2, 5, 63], and T = 20 for all MC-Dropout\nvariants [2]. The CIFAR-10 policy from AutoAugment is used for image augmentation\nfor classification and adapted for regression (by removing augmentations that modify\nthe bounding box, because they reduce localization accuracy) [73]. First, evaluation\nmetrics for task performance and UQ performance are summarized. Then, the perfor-\nmance on the ID data is discussed for classification and regression. It serves as a base-\nline in the following analysis, which covers the evaluation of the different networks\nunder distribution shift, the robustness of UQ, and compute-efficient MC-Dropout var-iants."}, {"title": "5.1 Evaluation Metrics", "content": "Several metrics are used for evaluating UQ across different studies [2]. We select those\nmetrics for task performance and uncertainty estimation, which are mostly used in re-\nlated works [2, 11, 74, 74]. These criteria evaluate the confidence, sharpness, and cor-\nrectness of a prediction.\nEach image cutout xi \u2208 R256x256 has a bounding box ground truth ybb,i \u2208 R and a\nclass label y \u2208 {1, ..., C} with C = 7 for the in-distribution dataset and the four simu-\nlated datasets (Rain1, Rain2, Fog1, Fog2), and C = 2 (pedestrian, vehicle) for the KITTI,\nCADC, and CC dataset. The reduced number of classes for selected datasets affects\nonly the results for classification, not for regression. The number of test samples N is\n4.867 for the ID and the simulated weather datasets, and 1.517, 2.000, 438, and 501 for\nthe KITTI, CADC, CC, and OoD datasets, respectively.\nThe network weights are trained for classification and regression independently.\nThe classification head computes the confidence score p\u2208 RC and the predicted class$\\hat{y} = \\underset{C}{argmax} \\hat{p}(y_c|x, \\theta)$. The confidence scores range between [0,1]; the higher, thebetter. The head for the bounding box regression predicts \u0177bb \u2208 R and the uncer-tainty Pbb = p(\u04f0\u044c|\u0445, \u04e8) \u2208 R4. The uncertainty predictions range between [0, inf [;the lower, the better.\nOur selection of the metrics for the classification is based on [11]:\nAccuracy (\u2191): The classification accuracy\n$ACC = \\frac{1}{N} |\\{ y = \\hat{y_i} \\text{ and } i = 1,..., M\\}|$\nevaluates only the correctness of a decision. The probability of a correct classi-fication by chance is 14% for C = 7."}, {"title": "5.2 Baseline Networks for ID (AD-Cifar-7)", "content": "Classification Baseline. The hyperparameter search revealed that the best numberof neurons n is 128 for the two fully connected layers in the classification and the re-gression head. Tab. 3 shows the ID performance for classification for the vanilla net-works. Commonly, an accuracy above 93% is achieved on the ID data. As some distri-bution shift subsets (KITTI, CADC, and CC) cover only 2 classes, we added the averageof the classification metrics for pedestrians and vehicles for the ID data to Tab. 3. Whenonly these two classes are considered, the baseline is slightly better."}, {"title": "5.3 Uncertainty Quantification for ID (AD-Cifar-7)", "content": "The learning rate and the dropout rate are optimized by a hyperparameter search foreach network and each UQ method for classification and regression. The final learningrate A is 0.0001 for all classification networks, including their dropout extensions. Forregression, this is only the case for ResNet-50 including its dropout variants and thevanilla EfficientNet-B0. The learning rate is a magnitude higher, 0.001, for the dropoutvariants of EfficientNet-B0 and all variants of ConvNeXt-Tiny. The dropout rate isconsistent 0.05 for all regression networks. For classification, the dropout rate can behigher and varies between [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35] with highest values forAfter-BB-Dropout and lowest values for MC-Dropout"}, {"title": "5.4 Different Network Architectures under Distribution Shift", "content": "We investigate how different types of natural distribution shift influence the accu-racy and ECE for different network architectures. For each metric, we accumulate theresults of EfficientNet-B0, ResNet-50, and ConvNeXt-Tiny over the baseline and theUQ methods.\nRobust Classification. Fig. 12 shows the performance decrease under the differ-ent types of natural distribution shift. Detection rates for KITTI are commonly higherthan for the BDD dataset. Similarly, we observe a slight increase in classification per-formance for the KITTI subset, which cannot only be explained by the reduced numberof classes. The classification is also reduced to two classes for the Real Weather andthe Corner Cases dataset. Even though the classification rate by chance is higher in\nthese cases, the performance drops in comparison to the ID baseline. Noticeable, is thelarge drop in performance under heavy rain, where the accuracy falls below 75% andthe highest ECE values are observed. The performance drop for the Real Weather subsetis less because it includes snow-covered winter scenes to a large extent. The CornerCases dataset also shows the highest values for the ECE, but only an average decreasein accuracy. This indicates that, for corner cases, the confidence estimation is less robustthan its class prediction. Overall, the difficulty of the natural distribution shifts can be\nsorted as follows: 1) changes in the camera setup lead to small performance changes,\n2) natural occlusion of objects caused by snow or fog reduces the accuracy in the single-digit range, 3) corner cases can reduce accuracy by over 10%, and 4) rain and heavyfog lead to significant performance drops with accuracies below 85%. These observa-tions are general across all networks and UQ methods.\nThe ranges in the boxplot Fig. 12 show that the ConvNeXt-Tiny leads to overalllower ECE values than EfficientNet-B0 and ResNet-50. This indicates that ConvNext-Tiny is more robust to the investigated distribution shifts.\nRobust Regression. For regression, a decrease in performance is observed for alltypes of natural distribution shift. The evaluation for regression includesadditionally the OoD set, for which bounding boxes are available; only class labels aremissing. A stronger decrease in performance is observed for OoD samples in compar-ison to CC samples. Furthermore, the performance drop is similar for simulated lightrain/fog and the Real Weather set. The largest decrease in performance is observed forheavy fog and heavy rain.\nIn comparison to classification, regression is more affected by heavy fog than heavyrain. Hence, unclear object boundaries affect the bounding box localization strongerthan the object classification. Similarly to the results for classification, ConvNeXt-Tiny is more robust under distribution shift than ResNet-50 and EfficientNet-B0. Inconclusion, we observe that 1) ResNet-50 is less robust to distribution shift than themore compute-efficient architectures EfficientNet-B0 and ConvNeXt-Tiny, 2) Con-vNeXt-Tiny shows low performance variation when extended by different UQ variants,3) the simulated Heavy Rain/Fog dataset leads to the strongest drop in performance."}, {"title": "5.5 Robustness of UQ under Distribution Shift", "content": "Classification and localization accuracy can significantly drop under natural distribu-tion shift. Here, we analyze the robustness of MC-Dropout in comparison to Ensemble- widely considered as a gold standard for confidence estimation - under distributionshift.\nRobust UQ for Classification. A noticeable improvement in classification accu-racy is observed for Ensemble for all investigated types of natural distribution shift.MC-Dropout often achieves a slight improvement in classification accuracy, except forheavy rain and heavy fog. Similarly, confidence estimation is more robust to naturaldistribution shift for Ensemble, which is evident in both metrics ECE and the BrierScore, see Tab. 5. The ECE has the advantage that it can be computed for classificationand regression and is less affected by individual outliers. The Brier score for classifica-tion and the NLL for regression are chosen as proper scoring rules, supporting our con-clusions drawn from the ECE. Results are reported for ConvNeXt-Tiny because thisnetwork is consistently more robust under different types of distribution shift, see pre-vious section."}, {"title": "5.6 Robustness of Runtime-Efficient MC-Dropout Variants", "content": "Similar to the previous section, we choose ConvNeXt-Tiny for this analysis, whichcompares the performance of compute-efficient dropout variants with MC-Dropout.In addition, only low dropout rates are selected by hyperparameter search for both re-gression and classification for ConvNeXt-Tiny, which simplifies the comparability ofthe results between classification and regression.\nEfficient UQ for Classification. Fig. 14 shows the accuracy and the ECE for clas-sification. Improved robustness under distribution shift can be achieved for After-BB-Dropout, where dropout is applied only to the fully connected layer after the featureextraction and covers variations in high-level-feature representations. Yet, applyingdropout to more layers in the classification head, as for Head-Dropout, can result in asignificant decrease in robustness, especially for strong distribution shift like light andheavy rain. The picture is slightly different for corner cases, where the distribution shiftmainly affects object-level representations. Here, LL-Dropout can be beneficially. Thisis one of the first studies indicating that CNN robustness can benefit from applyingdropout to those network layers which are affected by the type of distribution shift."}, {"title": "6 Conclusion", "content": "This work takes a granular perspective on the robustness of CNNs and uncertainty quan-tification methods under distribution shift. In granular computing, complex problemsare subdivided into smaller units, the granules, which can relate to each other hierarchi-cally or horizontally [79, 80", "79\u201382": "."}]}