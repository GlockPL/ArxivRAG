{"title": "Discrete Speech Unit Extraction via Independent Component Analysis", "authors": ["Tomohiko Nakamura", "Kwanghee Choi", "Keigo Hojo", "Yoshiaki Bando", "Satoru Fukayama", "Shinji Watanabe"], "abstract": "Self-supervised speech models (S3Ms) have become a common tool for the speech processing community, leveraging representations for downstream tasks. Clustering S3M representations yields discrete speech units (DSUs), which serve as compact representations for speech signals. DSUs are typically obtained by k-means clustering. Using DSUs often leads to strong performance in various tasks, including automatic speech recognition (ASR). However, even with the high dimensionality and redundancy of S3M representations, preprocessing S3M representations for better clustering remains unexplored, even though it can affect the quality of DSUs. In this paper, we investigate the potential of linear preprocessing methods for extracting DSUs. We evaluate standardization, principal component analysis, whitening, and independent component analysis (ICA) on DSU-based ASR benchmarks and demonstrate their effectiveness as preprocessing for k-means. We also conduct extensive analyses of their behavior, such as orthogonality or interpretability of individual components of ICA.", "sections": [{"title": "I. INTRODUCTION", "content": "Self-supervised speech models (S3Ms) [1]\u2013[4] has become the prevailing approach for addressing a variety of speech and audio processing tasks. These models have demonstrated significant effectiveness across numerous tasks, as evidenced by extensive benchmark evaluations [5]\u2013[8]. For evaluations, S3Ms are often frozen, and a small trainable layer (e.g., a single linear layer) is added and fine-tuned for a specific downstream task. This process, which is further corroborated by various linear probing literatures [9]\u2013[11], highlights the efficacy of using frozen pre-trained S3Ms.\n\nThe self-supervised learning paradigm makes those frozen S3M representations \"useful\" in downstream tasks [11]. S3Ms are trained to reconstruct the hidden signals based on the neighboring context [1], [2]. Its loss depends on the representation-wise distances, nudging the representations of similar signals to be in proximity [12], [13]. Consequently, clustering similar signals based on their representation distances, or discrete speech units (DSUs), becomes a natural progression [14], [15].\n\nIn Figure 1, we show the typical use of DSUs for downstream tasks. DSUs are typically derived by applying the k-means algorithm to S3M representations [14]. The resulting cluster indices, i.e., DSUs, can be viewed as the tokenized or summarized speech. These units requires substantially less storage compared to conventional continuous features, such as spectral speech features [15], [16]. They have demonstrated strong empirical performance across various tasks, including automatic speech recognition (ASR) [15], speech-to-speech translation (S2ST) [17], [18], and speech synthesis [14], [16]. Moreover, they facilitate unified modeling of text and speech more easily [19], [20], or further, textless natural language processing (NLP) [21], [22].\n\nAccordingly, there has been growing interest in how to extract DSUs from S3Ms. Many have shown that there is an optimal transformer layer [15] and number of clusters [23], both of which vary based on specific downstream tasks. Also, normalizing each dimension of the representation to have zero mean and unit variance prior to k-means has been shown effective [16]. Finally, compressing the resulting DSUs through deduplication and byte-pair encoding (BPE) has been demonstrated to enhance computational efficiency while having minimal impact on downstream performance [15].\n\nHowever, limited research has focused on enhancing k-means clustering with preprocessing techniques for extracting DSUs. Hence, we investigate the potential of linear preprocessing methods for k-means [24]; an area yet to be explored in detail, despite the established effectiveness of these methods in other contexts. For instance, principal component analysis (PCA) has been applied before k-means to reduce the computation cost [24]. Also, PCA is used to extract spoken word embeddings from S3Ms [25] and analyze S3M representations [26], [27], while independent component analysis (ICA) has been used for analyzing textual word embeddings [28]. We evaluate these linear transformations as preprocessing methods for clustering S3M representations to extract DSUs, utilizing a recently proposed benchmark [14]. Additionally, we conduct a qualitative analysis of their behaviors, focusing on the orthogonality and interpretability of individual components."}, {"title": "II. LINEAR PREPROCESSING METHODS\nFOR DISCRETE SPEECH UNIT EXTRACTION", "content": ""}, {"title": "A. k-means and Linear Transformation", "content": "Let $X \\in \\mathbb{R}^{T \\times D}$ be the concatenation of S3M features of the training data prepared for k-means, where $T$ and $D$ are the number of frames and the feature dimension. The conventional DSU extraction method directly applies k-means to X:\n\n$\\begin{equation}\n\\{u_t\\}_{t=1}^T = \\text{kmeans}(X),\n\\end{equation}$\n\nwhere $u_t \\in \\{0, 1, ..., k-1\\}$ is the DSU of frame t. It is important to note that the distance metric of k-means for DSU extraction (e.g., the Euclidean distance) is not invariant to arbitrary linear transformations on X. In other words, linearly transforming X can yield different DSUs. This motivates an investigation into how linear transformations may affect the k-means-based DSU extraction."}, {"title": "B. Linear Preprocessing Methods", "content": "We explore four methods: standardization, PCA, whitening, and ICA. Each linear processing transforms input data based on statistics, and is trainable in an unsupervised manner.\n\nStandardization: Standardization normalizes each column of X to have zero mean and unit variance. Let $x_{t,d} \\in \\mathbb{R}$ denote the $(t, d)$th entry of X, where d is the feature dimension index. The standardized feature $x^{(std)}_{t,d}$ is given as:\n\n$\\begin{equation}\nx^{(std)}_{t,d} = \\frac{1}{\\sigma_d}(x_{t,d} - \\mu_d),\n\\end{equation}$\n\nwhere $\\mu_d \\in \\mathbb{R}$ and $\\sigma_d \\in \\mathbb{R}$ are the average and standard deviation of feature dimension d, respectively. Standardization is used for preprocessing w2v-BERT [29] features in [16].\n\nPCA: PCA extracts correlated components from input data using the first- and second-order statistics. It subtracts the average over samples from X and obtain the mean-centered data X'. Then, eigenvalue decomposition of the covariance matrix $C := X'X'/(T - 1)$ is computed. Since C is symmetric, it can be diagonalized as:\n\n$\\begin{equation}\nC = V \\Lambda V^T,\n\\end{equation}$\n\nwhere $V \\in \\mathbb{R}^{D \\times D}$ is the orthogonal matrix and $\\Lambda \\in \\mathbb{R}^{D \\times D}$ is the diagonal matrix containing the eigenvalues on the main diagonal. The principal components of the data is given as:\n\n$\\begin{equation}\nX^{(pca)} = X'V.\n\\end{equation}$\n\nWhitening: Whitening (a.k.a. sphering) further normalizes the variances of the principal components:\n\n$\\begin{equation}\nX^{(whiten)} = X^{(pca)} \\Lambda^{-\\frac{1}{2}}.\n\\end{equation}$\n\nPrevious NLP works used this technique to extract sentence embeddings from pretrained models [30], [31].\n\nICA: ICA has underpinned multichannel audio source separation [32]. Unlike PCA, it finds statistically independent components using higher-order statistics. It assumes that $X^{(whiten)}$ consists of D independent components and is observed through the unknown mixing system $A \\in \\mathbb{R}^{D \\times D}$:\n\n$\\begin{equation}\nX^{(whiten)} = S A^T,\n\\end{equation}$\n\nwhere $S \\in \\mathbb{R}^{T \\times D}$ represents the weights of the independent components. The goal is to find the inverse system, characterized by the demixing matrix $W \\in \\mathbb{R}^{D \\times D}$, so that the separated components $X^{(whiten)} W^T$ approximately equals S.\n\nThis problem can be formulated as the maximum likelihood estimation with respect to W [33]. The log-likelihood function is:\n\n$\\begin{equation}\n\\text{log} p(X^{(whiten)}; W) = \\text{log} p(S = X^{(whiten)} W^T) + \\text{log} | \\text{det} W|.\n\\end{equation}$\n\nThe probability distribution of S determines the characteristics of the independent components, with the standard Laplace distribution typically chosen for each entry of S:\n\n$\\begin{equation}\n\\text{log} p(S) = - \\sum_{t,d=1}^{T,D} |s_{t,d}| - TD \\text{log} 2,\n\\end{equation}$\n\nwhere $s_{t,d}$ is the $(t, d)$th entry of S. With this formulation, W can be estimated using an iterative algorithm that ensures the log-likelihood function does not decrease at each iteration. Details of this algorithm can be found in [34].\n\nUsing the W estimate, W, the ICA-transformed data is given as:\n\n$\\begin{equation}\nX^{(ica)} = X^{(whiten)} W^T.\n\\end{equation}$"}, {"title": "III. EXPERIMENTAL RESULTS", "content": ""}, {"title": "A. Experimental Setup", "content": "We evaluated the effects of PCA, whitening, ICA on downstream tasks via DSU challenge [14], which is the recently proposed challenge to provide a unified benchmark for DSUs. We followed the ASR track, evaluating whether DSUs can capture phonetic information well for both rich (English) and low-resource (multilingual) scenarios. Our code\u00b9 was implemented based on the official code of the DSU challenge\u00b2.\n\nDataset: Same with the DSU challenge, we use LibriSpeech 100-hour subset (LibriSpeech-100) [35], which comprises English read speech, and the ML-SUPERB 1-hour (ML-SUPERB-1h) benchmark, which includes around 220-hour speech of 143 languages. The train and test set is the combination of these two corpora. The test set consists of dev_clean, dev_other, test_clean and test_other sets of LibriSpeech-100; and test_1h set of ML-SUPERB-1h.\n\nDSU extraction method: For the S3M, we chose the 17th layer of XLS-R-300M [4], which was used in the top-3 systems on the ASR track of the DSU challenge [14], [36].\n\nRegarding preprocessing of DSU extraction, we compared the conventional (i.e., no-preprocessing) method with standardization (Std), PCA, whitening (Whiten), and ICA, which are described in Section II-B. The demixing matrix of ICA was initialized with an identity matrix and the number of iterations was 100 for the ICA algorithm. As the distance metric of the k-means, the Euclidean and cosine distances were used, which we referred to as Euclid and Cosine, respectively. All combinations of the preprocessing methods and the distance criteria were evaluated as DSU extraction methods. For the k-means training, 5% of the training set was randomly chosen\u00b3. The number of clusters was 2000.\n\nDiscrete ASR model: For all DSU extraction methods, we used the baseline system from the DSU challenge, as proposed in [37]."}, {"title": "B. Effects of different preprocessing techniques", "content": "Table I shows CERs using DSUs extracted with and without the preprocessing methods. Euclidean distance had similar CERs with and without PCA because it is invariant to both centralization and orthogonal projection. While Cosine distance is not invariant to centralization, the Cosine result was unchanged by PCA. We suspect that it is due to transformer representations being anisotropic (i.e., occupying a narrow cone in the geometrical space) [39] so the centralization did not greatly change the geometry. We further explore this aspect in detail in Section IV-A.\n\nWhere PCA shows little to no improvement, Std, Whiten, and ICA substantially enhanced the ASR performances. Using Cosine rather than Euclidean further improves performance. Despite their performance improvements, Std resulted in higher bit-rates with Cosine compared to the no-preprocessed case. By contrast, Whiten and ICA effectively reduced the bit-rates. ICA with Cosine achieved the best average ASR performance with the lower bit-rate, demonstrating the importance of adequate preprocessing.\n\nWhiten and ICA works better with Cosine than Euclid, particularly on the test_1h set. A key characteristic of these two preprocessing methods is that they normalize the norms of input features. This result suggests that the norms of the whitened features are less relevant to ASR performance."}, {"title": "C. Effect of the number of clusters", "content": "We varied the number of clusters to examine the effectiveness of ICA on different settings. We compared the conventional Euclid with ICA-preprocessed Cosine (ICA+Cosine), which achieved the best ASR performance in Section III-B.\n\nTable II shows that both methods performed better with more clusters. Note that deduplication and BPE was applied to DSUs as in Section III-B. As in [40], the results exhibit the trade-off between bit-rate and ASR performance. ICA+Cosine outperforms Euclid by a consistent margin, except for 100 clusters, demonstrating the stability of ICA with a sufficient number of clusters."}, {"title": "D. Effects of S3M characteristics", "content": "We also examined which factor of S3M is essential for ICA to work effectively. We tested S3Ms that differ in training methodology and data. The first S3M, Wav2vec2.0-Large [1], was trained by the same training methodology as XLS-R-300M, but its training data comprise English data only (the LibriSpeech corpus [35]). Since it has the same network architecture as XLS-R-300M, we extracted DSUs from its 17th layer. The second S3M, multilingual HuBERT (mHuBERT) [41], was trained with the multilingual data but with a different training methodology from XLS-R-300M. For DSU extraction, we used 1000 clusters from mHuBERT's 11th layer, following its application in S2ST [41]. The third S3M, WavLM-Large [3], was trained on English data only, with a different training methodology from XLS-R-300M. DSUs were extracted from its 21st layer, as in the DSU challenge baseline. The number of clusters was set to 2000 except for mHuBERT, and other hyperparameters followed Section III-A.\n\nTable III shows CERs obtained with Euclid and ICA+Cosine for the three S3Ms. For Wav2vec2.0-Large, ICA+Cosine provided significant improvements for the LibriSpeech-100 test sets. The cause of the performance degradation for the test_1h set may be the mismatches between the training data of the S3M and downstream task. Despite the improvements for Wav2vec2.0-Large, ICA+Cosine had higher CERs from Euclidean for the other S3Ms. These results indicate that the effectiveness of ICA may depend on the underlying S3M training methodology, which is based on the masked language modeling and codebook groupings. We aim to conduct further exploration on the impact of training methodology on S3M representation geometry in the future."}, {"title": "IV. QUALITATIVE ANALYSIS OF CLUSTERING RESULTS", "content": ""}, {"title": "A. Orthogonality of k-means centroids", "content": "To compare the cosine similarities between clustering results, we plot the histogram of the similarities between different preprocessing methods in Figure 2. The k-means centroids were obtained with Cosine. Without any preprocessing, clusters tend to be similar to each other, centering around 0.6. As mentioned before, the anistropy of transformer representations [39] also appears to hold for S3Ms. Previous NLP works tried to make the representations more isotropic (opposite of anisotropic) to improve performance, such as word [42], [43] and sentence embeddings [30], [31]. We can observe that all preprocessing methods significantly increase the isotropy of representations, yielding orthogonal cluster centroids."}, {"title": "B. Interpretability of k-means centroids and ICA components", "content": "To further understand the characteristics of k-means and ICA, we conducted a qualitative analysis of their behaviors, using the final layer representations of WavLM-Large.\n\nDataset: We used the TIMIT dataset [44], a phonetically transcribed English speech dataset. We segmented the representation according to timestamps and applied average pooling to obtain phone-wise representations, similar to [9], [10]. This yielded 187,737 phone utterances across 51 phones (excluding silence) from 630 speakers.\n\nk-means centroids: We examined the 10 nearest neighbors of each cluster centroid in Figure 3, setting the cluster count to 100. To simplify, we focus on cases where the top 10 nearest neighbor representations are from the same phone. Due to space limitations, we plot only the stops and fricatives.\n\nConsistent with previous work [1], [2], we find that clusters align well with phonetic labels and distinguish allophones (different phonetic realizations of same phoneme), such as [th] in \"thin\" and [dh] in \"than.\" Further, multiple clusters often map to a single phone, with similar phones like [D], [DX], and [S] clustering near each other, while others like [G] and [K] are more dispersed. This suggests the clustering captures nuanced distinctions beyond traditional allophones. Finally, fricatives ([S], [SH], [Z], [F], [V], [DH]) cluster closely, supporting prior findings [45], [46] that representations of natural classes tend to be proximal."}, {"title": "ICA components", "content": "Unlike k-means centroids, ICA yields components with both positive and negative directions. Hence, we examined both top 5 and bottom 5 representations for each component in Figure 4, where the extremes are of the same phone. We identify three interpretable component types.\n\nThe first type shows phonetic contrasts, such as voicing differences in component 1 ([B]/[P]), 21 ([Z]/[S]), and 61 ([G]/[K]), while preserving manner and place of articulation. Similarly, component 94 ([F]/[TH]) contrasts nearby places of articulation. The second type captures allophones of a single phoneme, seen in components 0 and 61 ([HH]/[HV]), 11 ([UX]/[UW]), 23 and 83 ([AO]/[AA]), and 25 ([EM]/[M]). The third type includes repeated instances of the same phone within one component, similar to observations from k-means, indicating that S3M representations may capture fine acoustic distinctions beyond allophones.\n\nIn summary, we reconfirm that k-means effectively clusters phonetically similar sounds, where ICA goes a step further. Not only does ICA uncovers similar sounds, but it also finds axes with linguistically interpretable contrasts, offering a deeper level of analysis."}, {"title": "V. CONCLUSION", "content": "We investigate the impact of the preprocessing methods for extracting DSUs from S3Ms. Our experiments indicate that applying whitening and ICA before k-means clustering can improve the discrete ASR performances for XLS-R-300M with less bit-rates of DSUs. Further, our analyses show the orthogonality and interpretability of ICA components. Future work can be done for wider downstream tasks, such as vocoders, or further exploiting each ICA component."}]}