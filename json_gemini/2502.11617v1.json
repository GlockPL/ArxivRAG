{"title": "In-Context Parametric Inference: Point or Distribution Estimators?", "authors": ["Sarthak Mittal", "Yoshua Bengio", "Nikolay Malkin", "Guillaume Lajoie"], "abstract": "Bayesian and frequentist inference are two fundamental paradigms in statistical estimation. Bayesian methods treat hypotheses as random variables, incorporating priors and updating beliefs via Bayes' theorem, whereas frequentist methods assume fixed but unknown hypotheses, relying on estimators like maximum likelihood. While extensive research has compared these approaches, the frequentist paradigm of obtaining point estimates has become predominant in deep learning, as Bayesian inference is challenging due to the computational complexity and the approximation gap of posterior estimation methods. However, a good understanding of trade-offs between the two approaches is lacking in the regime of amortized estimators, where in-context learners are trained to estimate either point values via maximum likelihood or maximum a posteriori estimation, or full posteriors using normalizing flows, score-based diffusion samplers, or diagonal Gaussian approximations, conditioned on observations. To help resolve this, we conduct a rigorous comparative analysis spanning diverse problem settings, from linear models to shallow neural networks, with a robust evaluation framework assessing both in-distribution and out-of-distribution generalization on tractable tasks. Our experiments indicate that amortized point estimators generally outperform posterior inference, though the latter remain competitive in some low-dimensional problems, and we further discuss why this might be the case.", "sections": [{"title": "1. Introduction", "content": "Bayesian and requentist inference represent two core principles to statistical estimation and machine learning that provide complementary approaches to model training and evaluation. Bayesian methods treat hypotheses 0, such as model parameters, as random variables and use data D as evidence to update posterior beliefs p(0 | D), whereas frequentist methods, such as maximum likelihood and moment methods, assume fixed but unknown hypotheses 0* and estimate them through optimization. Despite the dominance of the Bayesian approach in the earliest successes of generative modeling (Hinton et al., 1995; Neal, 1996, among many others), the frequentist paradigm of obtaining point estimates has become predominant in deep learning, as Bayesian inference is challenging due to the complexity of estimating the posterior distribution (Blei et al., 2017).\nAn understanding of trade-offs between the two approaches, and between different methods for posterior approximation, is lacking in the regime of amortized estimators (Kingma, 2014; Rezende et al., 2014; Garnelo et al., 2018), where models q (0 | D) that take the dataset D explicitly as input are trained to estimate either point values or parametric distributions over 8. The Bayesian posterior predictive minimizes empirical risk, and it should be optimal to use it in prediction problems compared to a point estimate (Devroye et al., 1996). However, this optimality may not hold when approximate families that cannot express the full posterior are used, or when an amortization gap is introduced by a model that takes data as input and must generalize to new observations (Cremer et al., 2018) in-context.\nThe consequences of such limitations of amortized inference have been noted in a sequence of works in diverse areas of deep learning. For instance, higher variational bounds on likelihood do not necessarily lead to better models in VAEs (Rainforth et al., 2018) or to more effective approximations to the target distribution in variational methods (Blessing et al., 2024). Similarly, for Bayesian neural net-works (BNNs), the effectiveness of simple approximating families for the posterior compared to methods like MCMC has been debated (Ritter et al., 2018); indeed, posteriors need to be integrated at lower temperatures to useful approximate posterior predictive distributions (Wenzel et al., 2020; Adlam et al., 2020). These findings are also relevant to recent work on in-context learning in large language models, which approximates Bayesian inference at optimality (Xie et al., 2022; Aky\u00fcrek et al., 2023) but falls short in practice (Garg et al., 2022; Falck et al., 2024).\nIn this paper, we conduct a comparative analysis on sev-"}, {"title": "2. Problem Setup", "content": "We consider a generative model of sets of i.i.d. observations D = {(xi, Yi)}=1, assumed to be sampled from a ground-truth underlying distribution: (xi, Yi) ~ X. Given a parametric family of conditional models p(y | x, 0), we would like to infer the parameter @ that best explains the data D. Inferring @ allows us to make predictions of y on new data points \u00e6, by computing p(y | x, 0).\nIf the true model p(y | x) \u2013 the conditioning of x on x\u00b9 lies in the model class considered (that is, it is equal to p(y | x, 0*) for some (0*), then we hope to recover the parameter 0*, or a model equivalent to it. If the true model does not lie in the model class considered, the inference problem is said to be misspecified.\nThere are two main paradigms for estimating @ from D: frequentist and Bayesian. Frequentist methods treat 0 as fixed but unknown and estimate it by optimizing a functional that is maximized when 0 = 0*. Bayesian methods treat @ as a random variable and approximate a distribution over it by positing a prior distribution p(0) and matching, by various means, the posterior distribution p(0| D).\nBoth approaches primarily operate on a fixed set of observations D and rely on iterative methods to infer 0. In this work we are interested in in-context estimators\u00b2 that explicitly take D as input and provide an estimate for 0, and can generalize to new datasets zero-shot.\nWe briefly review the frequentist and Bayesian methods below, with a focus on amortized estimators."}, {"title": "2.1. Frequentist Estimation", "content": "The most common frequentist estimator is the maximum likelihood estimator (MLE), which estimates @ by maximiz-"}, {"title": "2.2. Bayesian Estimation", "content": "Given a prior distribution p(\u03b8), we have the posterior distribution\n$$p(0 | D) x p(D | 0)p(0) = p(0)\\prod_{i=1}^{k}p(Yi | Xi, 0).$$\n(4)\nThe simplest Bayesian estimator is the a point estimate of the mode of p(0 | D), called the maximum a posteriori (MAP) estimator:\n@MAP = arg max p(0 | D)\n=\narg max log p(0) + \u03a3 log p(Yixi, 0)\n(5)\n(note the similarity with (1)). Posterior concentration results (Doob (1949); see also Miller (2018; 2021)) show that, under some conditions, the posterior distribution concentrates around the true parameter value * ask \u2192 \u221e, meaning that prior term in (5) becomes irrelevant and the MAP and MLE converge to the same value. Such results hold almost surely with respect to the i.i.d. sampling of data from the true distribution X and assume the model class p(y | x, 0) contains the true model.\nWhile the MAP estimator approximates the posterior p(0| D) in (4) by a point mass at @MAP, other Bayesian methods may approximate it by more complex distributions, such as parametric models q\u3085(\u03b8). The goal is to infer parameters * that bring q close to the true posterior in some measure of divergence D,\n(6)\nWhen q is a model taking D explicitly as input, it is called an amortized estimator. The goal of amortized inference is to learn a model q that can approximate the true posterior in a fast and scalable manner.\nAmortized estimators are the focus of this work. The parametrization of amortized inference models will be discussed in Section 2.4 and training objectives in Section 3."}, {"title": "2.3. Posterior Predictive Distributions", "content": "Once @ is estimated by a distribution q4 (0) we can make predictions on new data points \u00e6 by computing the posterior predictive distribution\np(yx, D)\n=\n\u222b\np(y|x, 0)p(0| D) de\n(7)\n\u2248 E0~94(0)P(y | x, 0).\nFor point estimates, we can use the MAP or MLE estimate of e in place of the expectation in (7).\nThe main question we address is: which estimates of the model parameters @ give the best predictions on new data points x via the posterior predictive (7)?"}, {"title": "2.4. Amortization by In-Context Estimation", "content": "Traditionally, in-context learning (ICL) (Dong et al., 2022) over a training set D refers to the ability of a pre-trained sequence model (e.g., a LLM) to solve novel tasks when presented with the examples from Din-context. A number of works (Aky\u00fcrek et al., 2023; Garg et al., 2022; Xie et al., 2022; Von Oswald et al., 2023; Mittal et al., 2024; Elmoznino et al., 2024) formalize this form of ICL from the perspective of algorithmic tasks as directly modeling the posterior predictive model\narg max Ex,y, D~x log P\u00f8 (y|x, D), where x defines some data distribution.\nWhile ICL methods often model the posterior predictive, they can be adapted to perform parametric inference given a likelihood function (Mittal et al., 2023). Generally, parametric inference relies on approximate procedures to obtain estimates for a particular task and set of observations e.g. MLE of neural network parameters relies on gradient descent or posterior samples through MCMC approaches, for a fixed set of observations D (training set). Instead, we are interested in amortized / in-context estimators that explicitly take the observations as input and output the estimates, whether probabilistic or point. Such an in-context estimator's task is to model parameter inference, as opposed to prediction, and can provide estimates for novel tasks in zero-shot and compute-efficient manner. We rely on a transformer architecture to model conditioning on D and omit positional embeddings to satisfy permutation invariance of the posterior given i.i.d. samples."}, {"title": "3. Amortized Inference Training Objectives", "content": "We formalize different training objectives and parametrizations for learning amortized estimators to approximate either point estimates or full posteriors. Within posterior estimation, we consider three classes of algorithms based on the learning signal used: sample-based, variational methods or a combination of the two. Refer to Figure 1 for a hierarchical view over the different in-context estimators considered."}, {"title": "3.1. Point Estimates", "content": "For the point estimates OMLE and OMAP, an amortized model 0 = f(D; $) directly outputs the parameter value @ given the data D. The optimization problems in (1) and (5) can be directly used as the objectives for training $; for example, for MAP:\n|D|\nLMAP(D) = log p(f(D; $)) + \u2211 log p (Yi | xi, f (D; $)).\n(8)\nAn unbiased estimator of the gradient of LMAP(D) can be obtained by a stochastic surrogate loss, where the sum over D is replaced by a sum over a minibatch of data points BCD, reweighted by . The MLE loss is similar, with no prior term added."}, {"title": "3.2. Posterior Estimates", "content": "For full Bayesian posterior estimation, an amortized density q4(0|D) approximates the true posterior p(0|D). It can be trained with different objectives and distinct q parametrization, with the learning signal from either joint (0, D) samples or the unnormalized target density p(0|D) x p(D,0),"}, {"title": "3.2.1. SAMPLE-BASED METHODS", "content": "Sample-based methods treat the approximation of q4 (0 |\nD) as a generative modeling problem. They assume that we\nhave access to samples (0, D) from the true data-generating\nprocess x. In particular, this requires the problem to be\nwell-specified and for the generative model to expose the\nparameter of the conditional model, i.e., the true model\nproceeds via generation of 0, i.i.d. generation of inputs xi,\nand generation of outputs y; from the model p(Yi | xi, 0).\nGiven samples (0, D) ~ x, we can fit a generative model,\nconditioned on D, to the samples 0. If the objective is the\nlog-likelihood of the samples @ under the generative model,\nit amounts to minimization of the forward KL divergence\nbetween the generative model and the true posterior:\nE(0,D)~x [-log q4(0 | D)]\n=ED~x DKL(p(0 | D)||q\u00a2(0 | D)) + const.\n(9)\nAny family of generative models q can be used in the\napproximation. However, unbiased estimates of the log-\nlikelihood gradient in (9) require that the data and ground-\ntruth parameter values (0, D) come precisely from the true\ndata-generating process. For this to work, it is also impor-\ntant that the empirical distribution of (0, D)'s sufficiently\n\"covers\" the region associated with the target (0, D) of in-\nterest at test-time, so that q$ will generalize well there.\nGaussian modelling. One fits a Gaussian distribution, with\nmean and covariance output by a model (e.g., a neural net-\nwork) conditioned on D. The optimal model, which mini-\nmizes (9), matches the first and second moments of the true\nposterior p(0 | D) for every D in the support of x.\nNormalizing flows. They (Papamakarios et al., 2021;\nKobyzev et al., 2020) apply a sequence of trainable in-\nvertible transforms to convert a simple initial density,\ne.g. N(0, I), to a more complex one. The invertible trans-\nformations are chosen such that the jacobian in the change\nof density can be easily computed, and training is done by"}, {"title": "3.2.2. VARIATIONAL METHODS", "content": "Some methods for approximating the posterior do not rely\non unbiased samples from the true data-generating process,\nbut rather aim to fit a model to match q(0 | D) to the true\nposterior p(0 | D) given access to the joint p(0, D) =\np(0)p(D | \u03b8), to which p(0 | D) is proportional, at any 0.\nReverse KL. While the forward KL divergence DKL(p(0|\nD)||q4(0 | D)) cannot be optimized exactly without sam-\nples from p, we can optimize the reverse KL divergence\nDKL(94(0 | D)||p(0 | D)) exactly. The reverse KL ob-\njective can be seen as an entropy-regularized maximum\nlikelihood:\nDKL(94(0 | D)||p(0 | D))\n=E0~94(0|2) [-log p(0 | D)] \u2013 H[q\u00a2(0|D)].\n(10)\nNote that there is no expectation over D, and we are free\nto optimize (10) for D sampled from any distribution over\ndatasets of interest, or even for a single fixed D.\nIt is important to note that while the forward KL approaches"}, {"title": "3.2.3. SAMPLE-BASED + VARIATIONAL METHODS", "content": "One can combine the two estimation procedures outlined above. We consider an equally-weighted combination of forward and reverse KL as the divergence metric, called symmetric KL, for learning q$ in cases where it is modeled as a Gaussian distribution or a discrete normalizing flow."}, {"title": "4. Experiments", "content": "Our goal in this comparative study is to evaluate the amortized estimation procedures discussed in Section 3 for both in-distribution (ID) and out-of-distribution (OoD) generalization. We consider variants of point-estimation methods, forward and reverese KL approaches including diffusion and normalizing flows, and symmetric KL objective, with a focus on explicit conditioning on the set of observations. We evaluate the Bayesian and frequentist in-context estimators on a wide suite of probabilistic models through the lens of predictive performance, and discuss the suite of tasks, baselines and metrics considered in this study below.\nTasks. We consider estimating the mean of a Gaussian distribution (GM), means of a Gaussian Mixture Model (GMM), parameters of (non-)linear regression (NLR/LR)"}, {"title": "4.1. Evaluating in-distribution parameter inference", "content": "In-context estimators, whether point or posterior, can be leveraged to generalize to novel tasks zero-shot after being trained over multiple different datasets Dtrain ~ Xtrain. We first test for in-distribution generalization by sampling novel tasks Dtest ~ Xtrain and evaluating how well parameter samples generalize under the predictive metrics on Dtest. The benchmark consists of 88 tasks, where each task is defined by a different probabilistic model configuration, leading to the training of 324 models for each in-context estimator considered.\nWe provide a high-level visualization of the outcome of our experiments in Figure 2, which demonstrates the proportion"}, {"title": "4.1.1. FIXED-DIMENSIONAL", "content": "In-context learning relies on a common model to solve novel tasks and is thus limited in generalization to cases where the input and output spaces are shared across tasks, for, e.g., scalar inputs and outputs for 1-dimensional regression or a fixed vocabulary in LLMs. Similarly, parametric inference predicts, or describes a distribution over, @ and consequently requires its size to be shared across different problems. This is inherently defined by the probabilistic model, i.e.the like-"}, {"title": "4.1.2. VARIABLE-DIMENSIONAL", "content": "Next, we alleviate the limitation of fixed-dimensional parametric inference by embedding lower-dimensional problems into a fixed higher dimension. For example, a 1-dimensional linear regression model can be embedded in 100-dimensional space with the additional parameters set to 0. This simple masking procedure allows the in-context estimators to generalize to tasks with variable number of features, leading to the same estimator solving problems with"}, {"title": "4.2. Misspecification", "content": "Having studied in-distribution generalization, we now turn to cases of OoD generalization where the evaluation datasets are sampled from a different, sometimes unknown, distribution Dtest ~ Xtest. We study two cases: controlled synthetic and real-world tabular problems. This analysis is aimed to test the estimators' ability to handle changes in the underlying ground-truth mapping p(y|x) as well as when x follows a different distribution at evaluation, which is important since we often do not know the underlying model that generates the data of interest."}, {"title": "4.2.1. SYNTHETIC", "content": "We consider 1-dimensional regression problems with different underlying mappings p(y|x) between training and evaluation. Here, we are interested in generalizing to data obtained from Xtest but we assume that we do not know the underlying parametric form for this data. Instead, we assume a parametric form p(y|x, 0) which leads to Xtrain"}, {"title": "5. Conclusion", "content": "Our simulations in the amortized setting suggest that point estimation methods tend to outperform distribution estimators for posterior predictive modeling, especially on problems where the posterior over parameters is high-dimensional and multimodal. While one potential reason for this is the suboptimality or sample-inefficiency of the training objectives and model architectures, which research on amortized inference should continue to improve, our findings may be indicative of a more fundamental obstacle in Bayesian modeling. Many multimodal problems exhibit a large number of distinct modes that lead to equivalent solutions, but still require increasing expressivity in the approximate posterior to represent each of these, potentially redundant, modes. The latter challenge is manifested in more complex problems as well, e.g., the identifiability problem in mixture models (Teicher, 1963; Yakowitz & Spragins, 1968) and symmetries in Bayesian neural networks, where the number of modes has a combinatorial explosion in the network width, but mode connectivity results show that the posterior does not have high energy barriers (Draxler et al., 2018), especially modulo symmetries (Ferbach et al., 2024).\nWhile those results concern the non-amortized setting, we have shown that when in-context parameter estimation is considered, the same challenges arise even in simple models. This points to the need for hybrid approaches to amortized inference, drawing from non-amortized methods where only a subset of parameters undergo a Bayesian treatment (Daxberger et al., 2021) and amortized variational families are chosen to represent posteriors more efficiently (Sharma et al., 2023; Doan et al., 2025)."}, {"title": "A. Related Work", "content": "Normalizing Flows. Since Gaussian distributions are unimodal, they cannot approximate more complex multi-modal distributions well. To alleviate this problem, multiple works start with a simple distribution and then apply learnable invertible transformations to construct more complicated densities (Rezende & Mohamed, 2015; Kobyzev et al., 2020; Papamakarios et al., 2021; Kingma et al., 2016). These transformations are designed in a manner that the jacobian determinant is easily computable to allow for ease in computing entropy and training via back-propagation.\n$$q\u2084(\u2022) = gj \u00b0 ... \u00b0 g\u2081 \u00b0 N(\u00b7; 0, I)$$\n(13)\n$$q_{\u03c6}^{-1}(0) = q^{\u22121}_{\u03c6} (\u03b8\u2081^{-1}(\u03b8))|Jg_{j}(g_{j}^{-1}(\u03b8))|^{-1}$$\n(14)\nfor j = 1, ..., n and q represents the standard normal distribution. Further, Jgj represents the jacobian of the invertible function gj and |\u00b7 | represents the determinant operator.\nScore-Based Generative Modeling. Recent advances in generative models have stemmed from diffusion models (Song et al., 2021b; Song & Ermon, 2020; Song et al., 2021a; Ho et al., 2020; Nichol & Dhariwal, 2021) that consider a forward noising process via a stochastic differential equation as\n$$d0\u2081 = f(0t, t) dt + g(t) dwt$$\n(15)\nwith the corresponding reverse process as (Anderson, 1982)\n$$d0\u2081 = f(0\u2081, t) \u2013 g(t)\u00b2\u2207e log pt(0)|\u04e9\u2081 + g(t)dwt$$\n(16)\nNote that this requires estimating the score function at all time-steps t to integrate the SDE and obtain samples. Prior work has shown that the denoising objective provides a viable method for obtaining an estimate of the score function provided access to data, which is trained as\n$$arg min B_{1,00,0, t} [||5\u03c6 (\u03b8t, t) \u2013 Vo_{\u03b8t} log p(0_t|0_0)||_2^2]$$\n(17)\nNote that if f is a linear function, one can sample 01 given 00 and t directly in a simulation-free manner (S\u00e4rkk\u00e4 & Solin, 2019), which allows for scalable training of diffusion models through the above equation.\nFlow-Matching. Contrary to diffusion models, flow-matching (Lipman et al., 2023; Tong et al., 2024) models data through an ordinary differential equation instead of a stochastic differential equation. It first constructs an interpolation (Albergo et al., 2023; 2024), possibly noisy, between two random variables 00 and 01 as\n$$\u03b8\u03b5 = \u03b1\u03b9\u03b8\u03bf + \u03b2\u03b5\u03b8\u2081 + \u03b3\u03b9\u03b1$$\n(18)\nwhere ao = \u03b2\u2081 = 1, \u03b1\u2081 = \u03b2\u03bf = 0 and yo = Y1 = 0, and z follows a normal distribution. Samples from the target density can then be obtained by sampling a 01 and then solving the following ODE dynamics\n$$d0\u2081 = v(0t, t) dt$$\n(19)\nwhere the maginal drift is trained as\n$$arg min E\u03b80,\u03b81,t,z,\u03c3 [||v_\u03c6 (\u03b8t, t) \u2013 \\frac{\u03b8\u2081 - \u03b8_{0t}}{\u03c3}||_2^2]$$\n(20)\nDenoising Energy Matching. It is important to note that diffusion models are trained via data, while multiple applications require training a model to sample proportional to an unnormalized distribution in the absence of any data. Denoising Energy Matching (DEM) (Akhound-Sadegh et al., 2024) provides an importance sampling based estimate to train a similar diffusion model in the absence of data, by considering the target score matching estimator (De Bortoli et al., 2024) and combining it with importance sampling with the transition kernel p(01|00) as the proposal for \u03b8\u03c1."}, {"title": "B. Probabilistic Models", "content": "We discuss various probabilistic models used in our experiments as well as the form for the likelihood and the prior. This closely follows the setup in Mittal et al. (2023).\nMean of Gaussian (GM): We consider estimating the mean \u00b5 of a Gaussian distribution given some observed data. In this case, prior and likelihood defining the probabilistic model p(x, 0) (with @ being the mean \u00b5) are given by:\n$$p(\u03bc) = \u039d (\u03bc|0, I)$$\n(21)\n$$p(x|\u03bc) = N (x|\u03bc, \u03a3)$$\n(22)\nand \u2211 is known beforehand and defined as a unit variance matrix.\nLinear Regression (LR): We estimate the weight vector for Bayesian linear regression, where the underlying model p(D, 0) is given by:\n$$p(w) = N(w|0, I)$$\n(23)\n$$p(b) = N(b|0, I)$$\n(24)\n$$p(y|x, w, b) = N (y|w^{T}x + b,\u03c3\u00b2)$$\n(25)\nand with o\u00b2 = 0.25 known beforehand. Inputs \u00e6 are generated from p(x) = N(0, I).\nLinear Classification (LC): The underlying probabilistic model is:\n$$p(W) = N (W|0, I)$$\n(26)\n$$p(yx, W) = Categorical (\\frac{1}{\u03c4} W^{T}x)$$\n(27)\nwhere 7 is the known temperature term which is kept as 0.1 to ensure peaky distributions, and \u00e6 is being generated from p(x) = N(0, I).\nNonlinear Regression (NLR): We consider the model as a Bayesian Neural Network (BNN) for regression with fixed hyper-parameters like the number of layers, dimensionality of the hidden layer, etc. Let the BNN denote the function fo where are the network parameters. Then, for regression, we specify the probabilistic model using:\n$$p(0) = N (0|0, \u0399)$$\n(28)\n$$p(y|x, 0) = N (y|fo(x), o\u00b2)$$\n(29)\nwhere r2 = 0.25 is a known quantity and \u00e6 being generated from p(x) = N(0, I).\nNonlinear Classification (NLC): Like in Nonlinear Regression, we consider BNNs with fixed hyper-parameters for classification problems with the same estimation task. In this formulation, we consider the probabilistic model as:\n$$p(0) = N (0|0, I)$$\n(30)\n$$p(yz, 0) = Categorical (y| \\frac{1}{\u03c4} fo(x)$$\n(31)\nwhere 7 is the known temperature term which is kept as 0.1 to ensure peaky distributions, and \u00e6 is being generated from p(x) = N(0, I).\nGaussian Mixture Model (GMM): We look at a well-known probabilistic model for unsupervised learning, Gaussian Mixture Model (GMM), primarily used to cluster data. Consider a K-cluster GMM with:\n$$p(\u03bck) = \u039d (\u03bc\u03ba |0, 1)$$\n(32)\n$$p(x \u03bc1:\u03ba)\n=\n\\sum_{k=1}^{K}\n\u03c0\u03ba\u039d (x \u03bc\u03ba, \u03a3\u03ba)$$\n(33)\nWe assume Ek and \u03c0k to be known and set \u03a3k to be an identity matrix and the mixing coefficients to be equal, \u03c0\u03ba = 1/K, for all clusters k in our experiments."}, {"title": "C. Metrics", "content": "Regression. For regression problems, we consider the ensembled loss metric as the following\n$$B_{x,y, D} | - Eq_{q\u03c6}(\u03b8\\D)\n(34)\nwhile the single-sample metric is defined as\n$$B_{x,y,D}Eq_{q\u03c6}(\u03b8\\D)[|y \u2013 \u0177\u03c6|\n(35)\nwhere y denotes the mode of the distribution p(y|x, 0). We rely on similar metrics for the estimation of the mean of a Gaussian distribution, with the only difference being the absence of x, and \u0177 = 0, as it is an unsupervised learning problem.\nClassification. For classification problems, we consider the ensembled accuracy metric which is obtained as the following\n$$100 \u00d7 B_{x,y,D} [1y (Mode (\u01771,..., \u0177s))]$$\n(36)\nwhere 1() is an indicator function which is 1 if the argument is the same as y and 0 otherwise. Mode represents the mode of its arguments, where each \u0177i is the mode of p(y|x, 0\u2081) with \u03b8\u2081 ~ q\u2084(0|D). Similarly, the single sample metric is defined as\n$$100 \u00d7 B_{x,y,D}Eq\u03c6(\u03b8\\D) [1y (Y) |x, \u03b8]$$\nNote that the multiplication by 100 is just to scale the accuracy to 0 - 100.\nGaussian Mixture Model. For the Gaussian Mixture Model, there is no clear notion of ensembling due to the identifiability problem in clustering, i.e., averaging over two clusters could lead to the average not corresponding to any meaningful cluster. Thus, we only consider single sample metric for this case, in particular\n$$Ey, DEqy (01,...0c|D) [min (y - 4)2\n\u03c8\u2208\u03b8\u03b9,... \u03b8\u03b1]\n(38)\nwhere 01,... \u03b8c can be subsumed into a single larger vector 6 for the purposes of modeling a q\u03c6.\nNote that in case of point estimates, for all the metrics, q4 can be considered as a dirac measure and both ensembled and single-sample metrics represent the same quantity."}, {"title": "D. Implementation Details", "content": "In this section, we outline the implementation details behind each of the estimators. We consider the transformer architecture in all cases to model the conditioning on the set of observations D. We remove the positional embeddings so that the inferred parameters, distribution or point, are permutation invariant to D. We use [CLS] as an additional token embedded to the sequence and the prediction corresponding to it is used to infer the parameters.\nFor the architecture details, we use 4 encoder layers with a 256 dimensional attention block and 1024 feed-forward dimensions, and 4 heads in each attention block for our Transformer models.\nWe use a diagonal Gaussian assumption for modeling densities using the Gaussian distribution. For discrete normalizing flows, we follow the setup in (Radev et al., 2020) and use 6 coupling blocks, each with a 1 hidden-layered non-linear feed-forward subnetwork with ReLU non-linearity and 128 hidden dimensions.\nFor score-based diffusion models, we use the variance exploding SDE with no drift and the diffusion coefficient gt set to be \u221a2tp2 and train the estimator using denoising score matching with the loss being equally weighted for all times t. We use the same schedule for pDEM as well, and use 100 samples in the importance-sample estimate of the score.\nFinally, we use linear interpolation scheme for flow-matching that interpolates between the parameters @ and unstructured noise z in a linear manner.\nFor inference in continuous time models, we use 100 steps to perform both the SDE and ODE integration."}]}