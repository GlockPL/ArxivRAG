{"title": "M\u00b3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging", "authors": ["Jinghao Feng", "Qiaoyu Zheng", "Chaoyi Wu", "Ziheng Zhao", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "abstract": "Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M\u00b3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M\u00b3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M\u00b3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-40, and DeepSeek-V3. Compared to existing ML agentic designs, M\u00b3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging.", "sections": [{"title": "1 Introduction", "content": "Large Language Model (LLM)-powered agentic systems have demonstrated remarkable success across diverse domains, leveraging their ability to orchestrate specialized tools and solve complex, multi-step tasks with precision. However, their application in medical domain remains challenging, due to a shortage of well-prepared tools. Such challenge arises from two aspects: first, the complexity of medical workflows spanning diverse diseases, imaging modalities, and task requirements makes tool development and integration difficult; second, while"}, {"title": "2 Method", "content": "In this section, we present the proposed M\u00b3Builder in detail, starting with the problem formulation, then a detailed description on the ML workspace initialization and the multi-agent collaboration framework."}, {"title": "2.1 Problem Formulation", "content": "Given a task description on medical imaging analysis, denoted as T, our objective is to automatically construct a functional AI model via multi-agents collaboration. As shown in Fig. 2, our proposed framework M\u00b3Builder comprises two key components: the medical imaging ML workspace (W), and the multi-agent collaboration framework (A). Specifically, the workspace includes three elements: data cards, toolset descriptions, and code templates. The data cards are represented in natural language, while the toolset descriptions and code templates are provided in Python. Together, these elements form a structured\u2018environment' that guides the automatic AI workflow.\nBuilding on the workspace (W), the multi-agent framework composes of four LLM agents with distinct roles, i.e., A = {a1, a2, a3, a4}. These agents adopt a divide-and-conquer strategy to collaboratively address the complexities of the AI task. The framework iteratively performs code generation, editing and executing using tools defined by toolset descriptions, until a functional AI model is successfully produced. This process can be expressed as:\n{Ci, Ri} = A(Ci\u22121, Ri\u22121, T | W),                                          (1)\nwhere Ci denotes code and scripts generated or edited in the ith iteration, Ri denotes the compiler feedback in Python environment, with Co = Ro = \u00d8.\nIn the following sections, we present more details for the AI workspace initialization and the multi-agent collaboration framework."}, {"title": "2.2 Workspace Initialization", "content": "The ML Workspace for medical imaging analysis is designed to serve three key purposes: (i) it provides the multi-agent framework with metadata of the available datasets, enabling informed dataset selection; (ii) it supplies initial code templates, offering a standardized starting point for agents and demonstrating a typical AI model training pipeline; (iii) it defines and describes all tools available to the agents, restricting their action space to a predefined, complete set of"}, {"title": "2.3 Multi-Agent Collaboration Framework", "content": "This section introduces our multi-agent collaboration framework (A), which decomposes the AI task into four sub-tasks and assigns them to four specialized role-playing LLMs agents: Task Manager, Data Engineer, Module Architect, and Model Trainer, denoted as {a1, a2, a3, a4}, respectively, as illustrated in Fig. 2. Each agent is responsible for a specific role, and together they collaborate iteratively to construct the final AI model. We utilize a set of system prompts to define the role and working logic of each agent, shown in supplementary files.\nTask Manager acts as the coordinator of the framework. Its primary responsibilities include selecting the most suitable dataset for the task, or alternatively, asking users to upload raw datasets with associated datacard to the workspace as the supplementary of pre-existing datasets, and generating a comprehensive planning document P to guide the collaboration among the other agents. Specifically, given a user-provided task description, as exemplified by \u201cuser requirements\" in Fig. 2, the Task Manager will identify and select the optimal dataset (D) for model training and generate the planning documents. This process can be formally represented as:\n{D, P} = a1(T | Wa),                                          (2)\nwhere Wa denotes the data card in the pre-defined workspace.\nData Engineer is responsible for dataset preparation and processing. It transforms raw data into a format suitable for model training by performing tasks such as pre-screening the organizational structure of large-scale datasets, analyzing metadata files to extract relevant information, and splitting datasets into training and testing subsets. A critical aspect of the Data Engineer's role is its iterative interaction with the external compiler environment. It generates, edits, and refines code, incorporating feedback from the compiler, until the code executes successfully. This iterative process ensures the dataset preparation code is both robust and functional. The process can be expressed as:\n{Ci, Ri} = a2(Ci\u22121, Ri\u22121, T | D,P),                                          (3)"}, {"title": "3 Benchmark & Experiments", "content": "We evaluate M\u00b3Builder, using 14 diverse ML model development tasks across 4 radiology domains, paired with matched clinical datasets to enable comprehensive testing. The evaluation employs 7 leading LLMs as agent cores: GPT-4 [10], Claude-3.7-Sonnet[1], Claude-3.5-Sonnet [1], DeepSeek-v3 [17], Gemini-2.0-flash [25], Qwen-2.5-max [26], and Llama-3.3-70B [5]. Quantitatively, we assess the system's effectiveness through analysis on task completion, framework superiority, and agent role-specification."}, {"title": "3.1 Task Definition & Data Preparation", "content": "In this paper, we experiment with 14 tasks spanning organ segmentation, anomaly detection, disease diagnosis, and report generation. These tasks are systematically categorized by anatomic regions (head & neck, chest, abdomen & pelvis,"}, {"title": "3.2 Task Completion Analysis", "content": "We design a specific model-building task for each dataset based on its unique characteristics, resulting in a total of 14 tasks. For each task, seven large language models (LLMs) are run independently five times, with a maximum of 100 actions (tool invocations) allowed per execution. The task completion is defined as the successful training of a model with performance on the test set falling within an acceptable range. As the results shown in Tab.1, the performance of different LLMs exhibit significant variation, with Claude-3.7-Sonnet achieving the highest completion rate of 94.29%, while Gemini2.0 and Llama3.3 only reach 4.29%."}, {"title": "3.3 Compare to State-of-the-art Agentic Systems", "content": "We compare M\u00b3Builder with other agentic systems including MLAgent-Bench, Aider, Cursor Composer, Windsurf Cascade, and Copilot Edits (all using Sonnet as the agent core). Each system performed each task twice on our workspace under their built-in framework. As shown in Tab. 2, across radiology tasks (Organ Segmentation, Anomaly Detection, Disease Diagnosis, and Report Generation), MLAgent-Bench performed poorly due to insufficient data structure understanding capabilities. Other frameworks achieved only moderate success rates (39.29% max) due to single-agent limitations, required human-in-the-loop confirmation, increasing operational complexity and max iteration constraints. Our M\u00b3Builder demonstrated superior performance with a 42.85% higher average success rate while requiring fewer action steps and execution iterations."}, {"title": "3.4 Ablation Study", "content": "Here we present ablation studies on our system design, examining the impact of: single-agent versus multi-agent collaboration, auto-debugging capability, self-reflection mechanisms, and workflow few-shot examples. Results in Tab. 2 indicate that self-reflection has minimal influence on system performance, while auto-debugging proves crucial for successful training. Multi-agent collaboration and well-crafted example instructions also significantly impact performance, with their absence resulting in 42.85% and 25.00% performance gaps, respectively."}, {"title": "3.5 Analysis on Different Agent Roles", "content": "We evaluate the performance of each role-specific agent in M\u00b3Builder, using distinct success criteria: The Task Managers is assessed on its ability to select appropriate training datasets, the Data Engineer on generating valid data index files with correct structures and paths, the Module Architect on producing executable scripts for data loading, and the Model Trainer on successfully completing model training.\nAs shown in Tab. 4, the Task Manager demonstrated exceptional accuracy in task analysis, with stable token usage across all tasks. In contrast, the other three agent roles show greater variability in token consumption and execution attempts. This variability stems from the strict requirements for code organization, data preprocessing, and achieving error-free training within five iterations. Despite these challenges, the majority of agents successfully completed or nearly completed all assigned task executions, showcasing the robustness and adaptability of the multi-agent framework."}, {"title": "4 Conclusion", "content": "In this paper, we present M\u00b3Builder, an agentic system for automating machine learning in medical imaging tasks. Our approach combines an efficient medical imaging ML workspace with free-text descriptions of datasets, code templates, and interaction tools. Additionally, we propsoe a multi-agent collaborative agent system designed specifically for AI model building, with four role-playing LLMs, Task Manager, Data Engineer, Module Architect, and Model Trainer. In benchmarking against five SOTA agentic systems across 14 radiology task-specific datasets, M\u00b3Builder achieves a 94.29% model building success rate with Claude-3.7-Sonnet standing out among seven SOTA LLMs. Future work will extend beyond medical imaging to broader medical tasks, develop more robust tool-building agent systems, implement automated dataset preparation capabilities, and incorporate visual processing to better approximate clinical expertise."}, {"title": "A Supplementary Materials", "content": null}, {"title": "A.1 Case Study.", "content": "Here we present terminal logs of a successful running case. In this case, we require the system to train a model for Disease Diagnosis using 3D CT Chest image data. As shown in Fig. 3, Task Analyzer first read thoroughly through all the dataset descriptions stored in descriptions.py, then choose the CC-CCII dataset and list the reasons of the choice. Task Analyzer also returns the detail information of CC-CCII dataset for other role agents.\nFollowing Task Analyzer, Data Engineer traverse the dataset files, splits the dataset into train/test sets. It refers to the example data index files, and generates a python script to generate the json files. The process is shown in Fig. 4. Finally, Data Engineer concludes its work, and shows a briefing to the user."}, {"title": "A.2 Details about Inclusion Datasets", "content": "We curated 14 datasets supporting different medical imaging tasks. Each dataset is unzipped in our workspace without further processing.\nEach dataset is paired with a structured description in the datacard. Once a new dataset is uploaded into workspace, the corresponding data description should be inserted into this dynamic datacard file. The initialization of the datacard is shown as bellow:"}, {"title": "A.3 Details about the toolset", "content": "Here we provide each tool function in the toolset:\nlist files: Recursively scans the specified directory to identify and return paths of all code files (supporting common programming extensions like .py, .java, cpp, etc.). Features intelligent directory skipping by automatically ignoring directories containing more than 1,000 files to prevent processing excessively large file collections. Returns a newline-separated string of file paths for further processing.\nread files: Opens and reads the entire content of a specified file, returning the complete text as a string. Supports UTF-8 encoding, making it suitable for examining source code, configuration files, or any text document. Essential for code analysis and file inspection tasks without modifying the original content."}, {"title": null, "content": "copy files: Creates an exact duplicate of a single file from a source location to a destination path. Automatically generates any necessary directory structure at the destination if it doesn't already exist. Preserves file metadata like timestamps and permissions using shutil.copy2, ensuring the duplicate maintains the characteristics of the original file.\nwrite files: Generates a new file with specified content at the designated file path. Automatically creates all necessary parent directories if they don't exist, ensuring the file can be written even to previously non-existent paths. Particularly useful for programmatically creating new script files, configuration files, or saving processed data.\nedit files: Completely overwrites an existing file with new content, replacing the original data entirely. Designed for direct file modification without the need to manually open and edit files. Critical for automated code refactoring, text transformation, or updating configuration files with revised settings in batch operations.\nrun script: Executes shell commands in the operating system environment and captures their output. Leverages the ShellTool from LangChain to safely run commands and collect results. Enables interaction with the system shell to perform operations like running programs, executing system utilities, or triggering external processes from within the application.\npreview dirs: Performs a detailed analysis of a directory's structure by examining each immediate subfolder. For each entry, counts the total number of files and lists up to 100 file paths in natural sort order. Returns a structured dictionary with comprehensive information about directories and files, facilitating efficient navigation of complex file systems while limiting output size for large directories.\npreview files: Provides intelligent content summaries of structured and unstructured data files. For CSV files, displays the first 5 rows and total row count; for JSON, shows the first 5 key-value pairs or elements and total count; for text files, presents the first 10,000 words and total word count. Enables rapid content assessment without loading entire large files into memory, particularly valuable for data exploration tasks."}, {"title": "A.4 Details about the Agent structure", "content": "We use langgraph architecture for agent building and workflow graph compiling. As shown in Fig. 7, All the agents have their own toolsets, which are subset of our proposed toolset containing 8 tools because of their specification and meaningless redundant information provided. The function calling loop and debugging mechanism ensure the task completion performance."}, {"title": "A.5 Details about the Agent Core Candidates", "content": "We select 7 SOTA LLMs for comparison. Most of them are closed-source model which are usually more powerful."}, {"title": null, "content": "Claude-3.7-Sonnet - Anthropic's latest model released in February 2025, featuring significant advancements in reasoning capabilities, contextual understanding, and tool utilization. This model demonstrates exceptional performance in complex multi-step reasoning tasks while maintaining high computational efficiency. Claude-3.7-Sonnet exhibits particularly strong capabilities in understanding nuanced instructions and maintaining coherence across lengthy interactions, making it ideal for our complex evaluation scenarios. The model api we use is: \"claude-3-7-sonnet-20250219\".\nClaude-3.5-Sonnet - Released by Anthropic in 2024, this model represents a critical milestone in the Claude series, balancing performance and efficiency. We selected this model as the foundation for all our ablation studies due to its stable performance characteristics and consistent behavior across various experimental conditions. This strategic choice allowed us to isolate and measure the impact of individual components in our framework while maintaining a reliable baseline. The model excels in reasoning tasks requiring detailed comprehension and precise execution of instructions. The model api we use is: \"claude-3-5-haiku-20241022\u201d.\nGPT-40 - OpenAI's advanced multimodal model that seamlessly integrates sophisticated vision capabilities with powerful language understanding and generation. This model demonstrates remarkable versatility across domains and task types, with particularly strong performance in scenarios requiring cross-modal reasoning. Its ability to process both textual and visual information makes it valuable for our evaluation of real-world applications where multimodal understanding is essential. The model api we use is: \"gpt-40-2024-11-20\".\nDeepSeekV3 - A frontier model from DeepSeek AI that pushes the boundaries of language understanding and generation. This model incorporates innovative architectural improvements and training methodologies, resulting in competitive performance on standard benchmarks. We note that according to official documentation and our preliminary testing, the current version of DeepSeekV3 exhibits inconsistent stability in tool-calling functionalities. This limitation was carefully accounted for in our experimental design and subsequent analysis to ensure fair comparisons across models. The model api we use is: \"deepseek-chat\".\nQwen-2.5-Max Alibaba's flagship model representing the pinnacle of their LLM research, featuring extensive pretraining on diverse multilingual corpora. The model demonstrates exceptional capabilities in both Chinese and English language processing, with impressive performance on complex reasoning, knowledge retrieval, and creative generation tasks. Its balanced capabilities across domains make it particularly valuable for evaluating the cross-lingual generalizability of our proposed methods. The model api we use is: \"qwen-max-0125\".\nGemini-2.0-Flash \u2013 Google's optimized model designed to balance computational efficiency with state-of-the-art performance. Our experimental design initially incorporated Gemini-2.0-Pro; however, due to its experimental status at the time of our research and consequent stability issues encoun-"}, {"title": null, "content": "tered during preliminary testing, we strategically pivoted to the more stable Flash variant. This decision ensured consistent and reliable results throughout our extensive evaluation process while still benefiting from Google's advanced LLM architecture. The Flash variant provides excellent performance-to-efficiency ratio for our complex evaluation scenarios. The model api we use is: \"gemini-2.0-flash\"\nLlama-3.3-70B Meta's open-source large language model with 70 billion parameters, representing one of the most powerful publicly available models. This model incorporates advanced training techniques and architectural innovations, resulting in exceptional performance across reasoning, coding, and general language understanding benchmarks. As an open-source model, Llama-3.3-70B offers unique transparency advantages and provides an important reference point for comparing proprietary and open-source approaches in our evaluation framework. The model we use is from a proxy where the api is \u201cmeta-llama/Llama-3.3-70B-Instruct\""}, {"title": "A.6 Details about LLM Agents' System Prompts", "content": "We utilize a set of system prompts to define the role and internal working logic of all agents. System prompts contains necessary information in free-text format, including role definition, task specification, available tools and corresponding descriptions, an example workflow and other important requirements. The workflow example acts as a few-shot hint to guide the agent's workflow. We also insert self-reflection requirements at the end of each system prompt, guiding the agents check their work again before returning."}, {"title": null, "content": "System prompts of Task Analyzer\nYou are acting as an agent for selecting a dataset that best\nmatches a human user's requirements. You are provided with a\nlist of dataset\ndescriptions:\n{description_path}, which is a json containing a list of\ndictionaries. Every dictionary contains following entries:\n[\"dataset name\", \"dataset description\", \"dataset_path\"].\nYou have access to the tools:\nread_files: This function reads a script file (such as a Python\nfile) so you can understand its content. Use it to read\ndataloader template file to grasp the expected format for the\ndataloader class.\nHere is the typical workflow you should follow:\n1. Use read_files to read {description_path}, understand its\ncontent.\n2. choose exactly one dataset that best matches the user's\nrequirements. Remember, your choice should mainly base on\n\"dataset description\".\n3. Return the chosen dataset's name, description, and\ndataset_path,so a downstream peer agent can know these\ninformation accurately.\n4. include <end> to end the conversation.\nIMPORTANT NOTE: If you think there realy is no dataset that\nmeets the user's requirements, then return no dataset. You\nmust always reflect on your choice and return reasons for\nyour choice before ending.\nSystem prompts of Data Engineer\nYou are acting as an agent for preparing training and testing"}, {"title": null, "content": "data in a clinical radiology context. I provide you with a raw,\nunprocessed dataset and its corresponding description, which\ncan be found in {selector_content}. Your mission is to generate\nthree files-train.json, test.json, and label_dict.json(if\nneeded)-and save them to the working directory: {save_path}.\nAnd you must make sure that the format of the json files matches\nsome example files which will be mentioned below. Do not modify\nthe original data files directly.\nIMPORTANT NOTE: In selector content, you should be able to\nidentify the dataset's name, the dataset description, and the\ndataset's root path.\nYou have access to the following tools:\nlist_files_in_second_level:\nUse this tool to inspect the dataset's structure and determine\nwhere the image data files are located (typically large files\nsuch as png, jpg, nii, pck, pt, npy, etc.) along with any\nmetadata or label files (usually csv, json, txt, etc.). This\nhelp you decide which files to read further.\npreview_file_content:\nSome metadata or label files might be very large and exceed\nthe context length. Use this tool to preview a portion of the\nfile so that you understand its structure well enough to plan\nhow to parse it using code.\nwrite_file:\nYour final goal is to generate train.json, test.json, and\nlabel_dict.json(if needed). Therefore, you should write a Python\nscript that creates these files. Once you fully understand the\nfolder structure and the content of the metadata/label files,\nwrite code (possibly using libraries such as os, re, pandas,\netc.) to traverse or read the metadata files and extract the\nneeded entries for each data item.\nread_files:\nThis function reads a script file (such as a Python file) so you\ncan understand its content. Use it to read the dataloader\ntemplate file to grasp the expected format for the dataloader\nclass. Pay attention, you can not apply this tool to read\ntrain/test.json.\nedit_file:\nSince errors might occur during your first code attempt, you can\nuse this tool to modify your files based on error messages or\nfeedback.\nrun_script:"}, {"title": null, "content": "Use this tool to execute your Python file from the command line.\nRemember that this is the only tool available to run code.\nHere is the typical workflow you should follow:\nBased on the dataset's description, use the\nlist_files_in_second_level tool to understand the organization\nand structure of the dataset. Identify files that are likely\nto contain metadata or labels.\nUse the preview_file_content tool to read a portion of these\nfiles so that you can understand their structure and determine\nhow to parse them with your code.\nBased on the dataset's description, You Must Use the\ntraverse_dirs tool and read_files tool to read the directory\nstructure of {examples_path}, and find the example output jsons\nbased on the medical task, for the next step to refer to.\nOnce you feel that you have a sufficient understanding, write a\nPython script under director {save_path} (using the write_file\ntool) that generates the following:\n[train.json,test.json and label_dict.json(if needed)]\nRemember, If label_dict.json is not provided by chosen example\nfiles, then you must not generate it!!!\nIMPORTANT: You Must Make sure that the json files you output\nmatches the format of your chosen example files! Especially the\ndictionary structure!\nIf you wan to read a file named 'labels.json', use read_files\ninstead of preview_file_content!\ntrain/test split: Ensure that the data is split into training\nand testing sets in a reasonable ratio (e.g., 80/20) and that\nthe split is random. If train/test split is already presented,\nyou don't need to split, but you still need to generate the\njson files. Besides, ensures that for each training and testing\nsample the key-value pairs in the dictionary are internally\nshuffled.\nUse the run_script tool to execute your script. If errors occur\nduring execution, you can use the edit_file tool to modify your\ncode until the script runs successfully and produces the three\nJSON files.\nRemember, your objective is to automate the creation of shuffled\ntrain.json, test.json, and label_dict.json(if needed) without\naltering the raw data files directly.\nRemember, the formats of train.json, test.json and\nlabel_dict.json(if exists) must follow the example files."}, {"title": null, "content": "Before ending, you should reflect on your work. If you think\nthere is no error anymore and all the json files are generated,\nplease conclude your work and include <end> to end the\nconversations.\nSystem prompts of Module Architect\nYou are acting as an agent responsible for writing a dataloader\nfor a dataset. Your ultimate goal is to create a 'dataloader.py'\nscript that will be used to feed data into the training process\nunder the {dataindex_path}. The dataset index files are located\nat dataindex_path: {dataindex_path} and contain train.json,\ntest.json, and label_dict.json(may not exist). You must also\nchoose a template file located at {template_path}, refer to it.\nA peer dataset processor has already generated these index\nfiles, (informations can be found in {processor_msg}) so your\ntask is to write a dataloader class that can read these files\nand load the data into the training process. The dataloader\nshould be able to handle the training and testing data, as well\nas the label dictionary.\nDatast description is also provided in: {description}\nYou have access to a series of utility functions, which are as\nfollows:\ntraverse_dirs:\nThis tool traverses a given directory and returns all the file\npaths under that directory. It helps you understand the folder\nstructure-in this case, to inspect the various files in the\nindex folder.\npreview_file_content:\nOften, JSON files have extensive content that might exceed the\ncontext\nlength. Use this function to quickly understand the structure\nof the JSON files (train.json, test.json, label_dict.json) so\nyou know how to structure your dataloader class.\nread_file:\nThis function reads a script file (such as a Python file) so\nyou can understand its content. Use it to read the dataloader\ntemplate file to grasp the expected format for the dataloader\nclass. Pay attention, you\nMUST NOT apply this tool to read train/test.json."}, {"title": null, "content": "write_file:\nThis utility writes a file, typically a Python script. After\nreviewing the template and understanding the structures of\ntrain, test, and label_dict(if exists), use this tool to\ncreate your dataloader class and save it as dataloader.py for\ntraining data ingestion.\nedit_file:\nIf the initial write_file output encounters issues, you can\nuse edit_file to modify the script based on error messages or\nadditional prompts.\nrun_script:\nOnce the dataloader is written, use this tool to execute the\nscript and verify that it correctly loads the data without\nerrors.\nA sample workflow might be:\nDirectory Inspection: Use traverse_dirs to read the directory\nstructure of the given path {dataindex_path}, identifying the\npresence of the train, test, and label_dict(may not exist)\nJSON index files. Preview JSON Content: Employ\npreview_file_content to inspect these JSON files and understand\ntheir structures.\nChoose Template: Based on the medical task, which can be found\nin dataset description, choose a dataloader code template from\n{template_path} for reference.\nReview Template: Utilize read_file to examine your chosen\ndataloader template file and understand the proper format for\nwriting the dataloader class. Remember that this is not the\nend! you must go on to write dataloader.py\nCode Development: Based on the insights from the JSON structures\nand the\ntemplate, use write_file to write your dataloader class to\n'{dataindex_path}/dataloader.py' (Include a main function if\nnecessary).\nSave and Test: After writing dataloader.py, you must use\nrun_script to test and verify that the script runs correctly!!!\nYou must put your dataloader.py under {dataindex_path}!!!\nDebug and Validate: If errors occur, use edit_file and\nrun_script as needed to debug the script until it fully\nprocesses the entire dataset.\nRemember: Your task is to write & validate a dataloader that\nsuccessfully iterates over the dataset and verifies that it\nruns correctly during training. Always refer to the template"}, {"title": null, "content": "for guidance on the expected format.\nYou MUST use write_file to create a dataloader.py under\n{dataindex_path} and verify that it runs correctly!!!\nYou MUST tell where you place dataloader.py!!!\nYou should write dataloader according only to the json files\nand the template. And try not to modify too much of the\ntemplate. If you see comments in the template like \"you must\nnot modify this line\", then do not modify it.\nIf you think your dataloader.py is ready, and the dataloader\nis already validated, please conclude your work and include\n<end> to end the conversation.\nImportant: When you use write_file tool, print the parameters\nyou pass to the tool function!!!\nBefore ending, you should reflect on your work. If you think\nthere is no error anymore and all the json files are generated,\nplease conclude your work and include <end> to end the\nconversations.\nSystem prompts of Model Trainer\nYou are an Al assistant specialized in radiology tasks,\ncapable of writing training code, executing training processes,\nand debugging.\nYour primary focus areas include disease diagnosis, organ\nsegmentation, anomaly detection, and report generation tasks.\nYou handle end-to-end code writing, debugging, and training.\npeer processor and dataloader agents have completed preliminary\ntasks of dataset preparation and dataloader class writing,\nmessages documented in {processor_msg} and {dataloader_msg}.\nYou will build upon this groundwork.\nYour working directory is {work_path}, all operations must be\nstrictly confined to this directory. To accomplish training\ntasks, you have access to the following tools:\n1. traverse_dirs: Used for recursively traversing file paths in\nthe workspace to understand directory structure and infer file\npurposes from their names.\n2. read_files: Used to read content from one or multiple files\nto understand implementation details and determine if changes or"}, {"title": null, "content": "operations are needed. Please avoid read datapath files such as\nthe train.json, test.json and label_dict.json.\n3. write_file: Used for implementing new functional code.\n4. edit_file: Used for modifying files, including adding data\nand model information to template files, adding new features,\nor fixing errors.\nNote that when using this tool to edit a file, please always\nfirstly read the content before.\n5. run_script: Used for executing training through sh scripts.\n6. copy_files: Used to copy a file to a new path, typically\nused when copying train.py and train.sh from ReferenceFiles to\nworkspace\nYou can also access {train_script_path} to choose and copy the\nbest matching train.py and train.sh to workspace. But you cannot\nedit files under {train_script_path}.\nImportant notes:\n- The Datapath, Loss, and Utils directories respectively contain\nJSON/csv/JSONL data indices for training/validation and dataset\nclass you need, loss functions, and utility packages. While\nthese shouldn't be modified, you must understand their\nrelationships and functions.\n- The Logout directory stores training results and should not be\nmanually written to.\n- The Model directory contains training code modules for\ndifferent tasks. Generally, these shouldn't be modified, but you\nshould read them to understand their functionality and usage.\nRemember that if the medical task is Organ Segmnentation, you do\nnot have to read Modeldirectory, because model is provided in\ntrain.py already.\n- The directory {train_script_path} contains different medical\ntasks' respective train.sh and train.py files, you should choose\nthe best matching train.sh and train.py based on medical task,\nand copy them to workspace.\n- train.py contains the main training code template using\ntransformers trainer framework. You need to carefully read and\nmodify its contents as needed.\n- train.sh is the script for running the main code, containing\nparameter settings that you need to understand and configure.\n- train.py has some code lines commented by sth like 'you should\nnot modify this line', if you see this, don't modify that line.\nThe workflow consists of three phases:"}, {"title": null, "content": "1. traverse train_script_path"}, "to choose the best matching\ntrain.sh and train.py based on medical task, and use copy_files\nto copy to workspace.\n2. Understanding structure and reading files/code templates\n3. Initial code adjustment and refinement. Modify train.py and\ntrain.sh to make them ready. A Hint: You always have to import\nthe dataset class from {work_path}/Datapath/dataloader.py\n4. Script execution (use run_script tool to execute train.sh)\nand debug loop until successful training"]}