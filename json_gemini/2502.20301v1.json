{"title": "M\u00b3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging", "authors": ["Jinghao Feng", "Qiaoyu Zheng", "Chaoyi Wu", "Ziheng Zhao", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "abstract": "Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M\u00b3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M\u00b3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debug-ging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M\u00b3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-40, and DeepSeek-V3. Compared to existing ML agentic designs, M\u00b3Builder shows superior performance on completing ML tasks in med-ical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully auto-mated machine learning in medical imaging.", "sections": [{"title": "1 Introduction", "content": "Large Language Model (LLM)-powered agentic systems have demonstrated re-markable success across diverse domains, leveraging their ability to orchestrate specialized tools and solve complex, multi-step tasks with precision. However, their application in medical domain remains challenging, due to a shortage of well-prepared tools. Such challenge arises from two aspects: first, the complexity of medical workflows spanning diverse diseases, imaging modalities, and task requirements makes tool development and integration difficult; second, while"}, {"title": "2 Method", "content": "In this section, we present the proposed M\u00b3Builder in detail, starting with the problem formulation, then a detailed description on the ML workspace initial-ization and the multi-agent collaboration framework."}, {"title": "2.1 Problem Formulation", "content": "Given a task description on medical imaging analysis, denoted as T, our objective is to automatically construct a functional AI model via multi-agents collabora-tion. As shown in Fig. 2, our proposed framework M\u00b3Builder comprises two key components: the medical imaging ML workspace (W), and the multi-agent col-laboration framework (A). Specifically, the workspace includes three elements: data cards, toolset descriptions, and code templates. The data cards are repre-sented in natural language, while the toolset descriptions and code templates are provided in Python. Together, these elements form a structured\u2018environment' that guides the automatic AI workflow.\nBuilding on the workspace (W), the multi-agent framework composes of four LLM agents with distinct roles, i.e., A = {a1,a2, a3, a4}. These agents adopt a divide-and-conquer strategy to collaboratively address the complexities of the AI task. The framework iteratively performs code generation, editing and exe-cuting using tools defined by toolset descriptions, until a functional AI model is successfully produced. This process can be expressed as:\n{Ci, Ri} = A(Ci\u22121, Ri\u22121, T | W), (1)\nwhere Ci denotes code and scripts generated or edited in the ith iteration, Ri denotes the compiler feedback in Python environment, with Co = Ro = \u00d8.\nIn the following sections, we present more details for the AI workspace ini-tialization and the multi-agent collaboration framework."}, {"title": "2.2 Workspace Initialization", "content": "The ML Workspace for medical imaging analysis is designed to serve three key purposes: (i) it provides the multi-agent framework with metadata of the avail-able datasets, enabling informed dataset selection; (ii) it supplies initial code templates, offering a standardized starting point for agents and demonstrating a typical AI model training pipeline; (iii) it defines and describes all tools avail-able to the agents, restricting their action space to a predefined, complete set of"}, {"title": "2.3 Multi-Agent Collaboration Framework", "content": "This section introduces our multi-agent collaboration framework (A), which de-composes the AI task into four sub-tasks and assigns them to four specialized role-playing LLMs agents: Task Manager, Data Engineer, Module Architect, and Model Trainer, denoted as {a1,a2, a3, a4}, respectively, as illustrated in Fig. 2. Each agent is responsible for a specific role, and together they collaborate iter-atively to construct the final AI model. We utilize a set of system prompts to define the role and working logic of each agent, shown in supplementary files.\nTask Manager acts as the coordinator of the framework. Its primary responsi-bilities include selecting the most suitable dataset for the task, or alternatively, asking users to upload raw datasets with associated datacard to the workspace as the supplementary of pre-existing datasets, and generating a comprehen-sive planning document P to guide the collaboration among the other agents. Specifically, given a user-provided task description, as exemplified by \u201cuser re-quirements\" in Fig. 2, the Task Manager will identify and select the optimal dataset (D) for model training and generate the planning documents. This pro-cess can be formally represented as:\n{D, P} = a1(T | Wa), (2)\nwhere Wa denotes the data card in the pre-defined workspace.\nData Engineer is responsible for dataset preparation and processing. It trans-forms raw data into a format suitable for model training by performing tasks such as pre-screening the organizational structure of large-scale datasets, ana-lyzing metadata files to extract relevant information, and splitting datasets into training and testing subsets. A critical aspect of the Data Engineer's role is its iterative interaction with the external compiler environment. It generates, edits, and refines code, incorporating feedback from the compiler, until the code exe-cutes successfully. This iterative process ensures the dataset preparation code is both robust and functional. The process can be expressed as:\n{Ci, Ri} = a2(Ci\u22121, Ri\u22121, T | D,P), (3)"}, {"title": "3 Benchmark & Experiments", "content": "We evaluate M\u00b3Builder, using 14 diverse ML model development tasks across 4 radiology domains, paired with matched clinical datasets to enable compre-hensive testing. The evaluation employs 7 leading LLMs as agent cores: GPT-4 [10], Claude-3.7-Sonnet[1], Claude-3.5-Sonnet [1], DeepSeek-v3 [17], Gemini-2.0-flash [25], Qwen-2.5-max [26], and Llama-3.3-70B [5]. Quantitatively, we as-sess the system's effectiveness through analysis on task completion, framework superiority, and agent role-specification."}, {"title": "3.1 Task Definition & Data Preparation", "content": "In this paper, we experiment with 14 tasks spanning organ segmentation, anomaly detection, disease diagnosis, and report generation. These tasks are systemati-cally categorized by anatomic regions (head & neck, chest, abdomen & pelvis,"}, {"title": "3.2 Task Completion Analysis", "content": "We design a specific model-building task for each dataset based on its unique characteristics, resulting in a total of 14 tasks. For each task, seven large language models (LLMs) are run independently five times, with a maximum of 100 ac-tions (tool invocations) allowed per execution. The task completion is defined as the successful training of a model with performance on the test set falling within an acceptable range. As the results shown in Tab.1, the performance of different LLMs exhibit significant variation, with Claude-3.7-Sonnet achieving the high-est completion rate of 94.29%, while Gemini2.0 and Llama3.3 only reach 4.29%."}, {"title": "3.3 Compare to State-of-the-art Agentic Systems", "content": "We compare M\u00b3Builder with other agentic systems including MLAgent-Bench, Aider, Cursor Composer, Windsurf Cascade, and Copilot Edits (all using Sonnet as the agent core). Each system performed each task twice on our workspace un-der their built-in framework. As shown in Tab. 2, across radiology tasks (Organ Segmentation, Anomaly Detection, Disease Diagnosis, and Report Generation), MLAgent-Bench performed poorly due to insufficient data structure understand-ing capabilities. Other frameworks achieved only moderate success rates (39.29% max) due to single-agent limitations, required human-in-the-loop confirmation, increasing operational complexity and max iteration constraints. Our M\u00b3Builder demonstrated superior performance with a 42.85% higher average success rate while requiring fewer action steps and execution iterations."}, {"title": "3.4 Ablation Study", "content": "Here we present ablation studies on our system design, examining the impact of: single-agent versus multi-agent collaboration, auto-debugging capability, self-reflection mechanisms, and workflow few-shot examples. Results in Tab. 2 in-dicate that self-reflection has minimal influence on system performance, while auto-debugging proves crucial for successful training. Multi-agent collaboration and well-crafted example instructions also significantly impact performance, with their absence resulting in 42.85% and 25.00% performance gaps, respectively."}, {"title": "3.5 Analysis on Different Agent Roles", "content": "We evaluate the performance of each role-specific agent in M\u00b3Builder, using distinct success criteria: The Task Managers is assessed on its ability to select appropriate training datasets, the Data Engineer on generating valid data in-dex files with correct structures and paths, the Module Architect on producing executable scripts for data loading, and the Model Trainer on successfully com-pleting model training.\nAs shown in Tab. 4, the Task Manager demonstrated exceptional accuracy in task analysis, with stable token usage across all tasks. In contrast, the other three agent roles show greater variability in token consumption and execution attempts. This variability stems from the strict requirements for code organiza-tion, data preprocessing, and achieving error-free training within five iterations. Despite these challenges, the majority of agents successfully completed or nearly completed all assigned task executions, showcasing the robustness and adapt-ability of the multi-agent framework."}, {"title": "4 Conclusion", "content": "In this paper, we present M\u00b3Builder, an agentic system for automating machine learning in medical imaging tasks. Our approach combines an efficient medical imaging ML workspace with free-text descriptions of datasets, code templates, and interaction tools. Additionally, we propsoe a multi-agent collaborative agent system designed specifically for AI model building, with four role-playing LLMs, Task Manager, Data Engineer, Module Architect, and Model Trainer. In bench-marking against five SOTA agentic systems across 14 radiology task-specific datasets, M\u00b3Builder achieves a 94.29% model building success rate with Claude-3.7-Sonnet standing out among seven SOTA LLMs. Future work will extend beyond medical imaging to broader medical tasks, develop more robust tool-building agent systems, implement automated dataset preparation capabilities, and incorporate visual processing to better approximate clinical expertise."}, {"title": "A Supplementary Materials", "content": null}, {"title": "A.1 Case Study.", "content": "Here we present terminal logs of a successful running case. In this case, we require the system to train a model for Disease Diagnosis using 3D CT Chest image data. As shown in Fig. 3, Task Analyzer first read thoroughly through all the dataset descriptions stored in descriptions.py, then choose the CC-CCII dataset and list the reasons of the choice. Task Analyzer also returns the detail information of CC-CCII dataset for other role agents."}, {"title": "A.2 Details about Inclusion Datasets", "content": "We curated 14 datasets supporting different medical imaging tasks. Each dataset is unzipped in our workspace without further processing.\nEach dataset is paired with a structured description in the datacard. Once a new dataset is uploaded into workspace, the corresponding data description should be inserted into this dynamic datacard file. The initialization of the dat-acard is shown as bellow:"}, {"title": "A.3 Details about the toolset", "content": "Here we provide each tool function in the toolset:\nlist files: Recursively scans the specified directory to identify and return paths of all code files (supporting common programming extensions like .py, .java, cpp, etc.). Features intelligent directory skipping by automatically ignoring directories containing more than 1,000 files to prevent processing excessively large file collections. Returns a newline-separated string of file paths for further processing.\nread files: Opens and reads the entire content of a specified file, returning the complete text as a string. Supports UTF-8 encoding, making it suitable for examining source code, configuration files, or any text document. Essen-tial for code analysis and file inspection tasks without modifying the original content.\ncopy files: Creates an exact duplicate of a single file from a source loca-tion to a destination path. Automatically generates any necessary directory structure at the destination if it doesn't already exist. Preserves file metadata like timestamps and permissions using shutil.copy2, ensuring the duplicate maintains the characteristics of the original file.\nwrite files: Generates a new file with specified content at the designated file path. Automatically creates all necessary parent directories if they don't exist, ensuring the file can be written even to previously non-existent paths. Particularly useful for programmatically creating new script files, configura-tion files, or saving processed data.\nedit files: Completely overwrites an existing file with new content, replac-ing the original data entirely. Designed for direct file modification without the need to manually open and edit files. Critical for automated code refactoring, text transformation, or updating configuration files with revised settings in batch operations.\nrun script: Executes shell commands in the operating system environment and captures their output. Leverages the ShellTool from LangChain to safely run commands and collect results. Enables interaction with the system shell to perform operations like running programs, executing system utilities, or triggering external processes from within the application.\npreview_dirs: Performs a detailed analysis of a directory's structure by examining each immediate subfolder. For each entry, counts the total num-ber of files and lists up to 100 file paths in natural sort order. Returns a structured dictionary with comprehensive information about directories and files, facilitating efficient navigation of complex file systems while limiting output size for large directories.\npreview_files: Provides intelligent content summaries of structured and unstructured data files. For CSV files, displays the first 5 rows and total row count; for JSON, shows the first 5 key-value pairs or elements and total count; for text files, presents the first 10,000 words and total word count. En-ables rapid content assessment without loading entire large files into memory, particularly valuable for data exploration tasks."}, {"title": "A.4 Details about the Agent structure", "content": "We use langgraph architecture for agent building and workflow graph compiling. As shown in Fig. 7, All the agents have their own toolsets, which are subset of our proposed toolset containing 8 tools because of their specification and mean-ingless redundant information provided. The function calling loop and debugging mechanism ensure the task completion performance."}, {"title": "A.5 Details about the Agent Core Candidates", "content": "We select 7 SOTA LLMs for comparison. Most of them are closed-source model which are usually more powerful.\nClaude-3.7-Sonnet - Anthropic's latest model released in February 2025, featuring significant advancements in reasoning capabilities, contextual un-derstanding, and tool utilization. This model demonstrates exceptional per-formance in complex multi-step reasoning tasks while maintaining high com-putational efficiency. Claude-3.7-Sonnet exhibits particularly strong capabili-ties in understanding nuanced instructions and maintaining coherence across lengthy interactions, making it ideal for our complex evaluation scenarios.\nThe model api we use is: \"claude-3-7-sonnet-20250219\".\nClaude-3.5-Sonnet - Released by Anthropic in 2024, this model repre-sents a critical milestone in the Claude series, balancing performance and ef-ficiency. We selected this model as the foundation for all our ablation studies due to its stable performance characteristics and consistent behavior across various experimental conditions. This strategic choice allowed us to isolate and measure the impact of individual components in our framework while maintaining a reliable baseline. The model excels in reasoning tasks requir-ing detailed comprehension and precise execution of instructions. The model api we use is: \"claude-3-5-haiku-20241022\u201d.\nGPT-40 - OpenAI's advanced multimodal model that seamlessly integrates sophisticated vision capabilities with powerful language understanding and generation. This model demonstrates remarkable versatility across domains and task types, with particularly strong performance in scenarios requiring cross-modal reasoning. Its ability to process both textual and visual infor-mation makes it valuable for our evaluation of real-world applications where multimodal understanding is essential. The model api we use is: \"gpt-40-2024-11-20\".\nDeepSeekV3 - A frontier model from DeepSeek AI that pushes the bound-aries of language understanding and generation. This model incorporates in-novative architectural improvements and training methodologies, resulting in competitive performance on standard benchmarks. We note that according to official documentation and our preliminary testing, the current version of DeepSeekV3 exhibits inconsistent stability in tool-calling functionalities. This limitation was carefully accounted for in our experimental design and subsequent analysis to ensure fair comparisons across models. The model api we use is: \"deepseek-chat\".\nQwen-2.5-Max - Alibaba's flagship model representing the pinnacle of their LLM research, featuring extensive pretraining on diverse multilingual corpora. The model demonstrates exceptional capabilities in both Chinese and English language processing, with impressive performance on complex reasoning, knowledge retrieval, and creative generation tasks. Its balanced capabilities across domains make it particularly valuable for evaluating the cross-lingual generalizability of our proposed methods. The model api we use is: \"qwen-max-0125\".\nGemini-2.0-Flash \u2013 Google's optimized model designed to balance compu-tational efficiency with state-of-the-art performance. Our experimental de-sign initially incorporated Gemini-2.0-Pro; however, due to its experimental status at the time of our research and consequent stability issues encoun-"}, {"title": "A.6 Details about LLM Agents' System Prompts", "content": "We utilize a set of system prompts to define the role and internal working logic of all agents. System prompts contains necessary information in free-text for-mat, including role definition, task specification, available tools and correspond-ing descriptions, an example workflow and other important requirements. The workflow example acts as a few-shot hint to guide the agent's workflow. We also insert self-reflection requirements at the end of each system prompt, guiding the agents check their work again before returning."}, {"title": "A.7 Details about Prompt for Comparison Experiments", "content": "For single anget systems such as ML-Agent Bench, Aider, Cursor Composer, Windsurf Cascade and Github Copilot Edits. We use a prompt combining four role-specific agents' prompt as below:"}, {"title": "A.8 Details about the Agent system comparison Experiments", "content": "Here we run each task experiments twice, resulting in a 28 execution in total. Here we reorganize them categorized by radiology task-level: Organ Segmenta-tion, Anomaly Detection, Disease Diagnosis and Report Generation. We detail our each execution under each agentic framework, using metrics including Av-erage Actions and Iterarions where one action means a step of tool using and a iteration means a step of debug for script execution. The result are shown as below:"}, {"title": "A.9 Details about the Agent Rols-specification Experiments.", "content": "In the Role-specification analysis, we have mentioned that we run each task twice, which leads to 28 execution rounds in total. Here we reorganize them categorized by radiology task-level: Organ Segmentation, Anomaly Detection, Disease Diagnosis and Report Generation and in the following are each execution details:"}]}