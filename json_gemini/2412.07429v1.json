{"title": "Optimizing Alignment with Less: Leveraging Data Augmentation for Personalized Evaluation", "authors": ["Javad Seraj", "Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti", "Majid Nili Ahmadabadi"], "abstract": "Automatic evaluation by large language models (LLMs) is a prominent topic today; however, judgment and evaluation tasks are often subjective and influenced by various factors, making adaptation challenging. While many studies demonstrate the capabilities of state-of-the-art proprietary LLMs in comparison to human evaluators, they often struggle to adapt to reference evaluators over time, a requirement for achieving personalized judgment. Additionally, numerous works have attempted to apply open LLMs as judges or evaluators, but these efforts frequently overlook the limitations of working with scarce data. Personalized judgment is inherently associated with limited data scenarios, which are common in many real-world problems. Our work aims to present a data augmentation technique to select a more effective sample from limited data in order to align an open LLM with human preference. Our work achieves approximately 7% improvements in Pearson correlation with a reference judge over the baseline,and 30% improvement over the base model (Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task. demonstrating that augmenting selecting more effective preference data enables our approach to surpass baseline methods.", "sections": [{"title": "1 Introduction", "content": "Human evaluation process is inherently subjective and heavily influenced by the evaluator's mood, which can change drastically over time. For instance, the assessment of students' papers may vary from semester to semester, reflecting variations in the teacher's perspective or circumstances. This subjectivity is crucial to consider when attempting to model or replicate an evaluator's behavior. Additionally, automatic evaluation tasks often face data limitations, with only a small amount of feedback and scores typically available. This highlights the importance of exploring data-efficient or effective training methods for assessment in limited-data scenarios. Our work presents an effective way to align an open large language model (LLM) with a reference evaluator in a data-limited setting, emphasizing on the personalized judgment across various tasks such as math and general truthful question-answering.\nLLM-based evaluation has emerged as a scalable and cost-effective paradigm for assessing LM-generated text (Zeng et al., 2024; Zheng et al., 2023; Gao et al., 2024; Li et al., 2024) or human-generated text (Latif and Zhai, 2023; Latif et al., 2024; Fang et al., 2024). LLMs are prompted to provide feedback along with a scalar score, which is an indicator of quality, commonly referred to as direct assessment.\nPrior works employing proprietary LLMs as assessors have shown not only high correlations with human judgements but also increased speed and cost-effectiveness (Koutcheme et al., 2024; Latif and Zhai, 2023; Zhu et al., 2023). These models perform well in static judgement, where scoring is based on fixed and rigid criteria which remain unchanged over time. However, it is not easy to personalize their behavior to follow a certain evaluator preference or policy. Additionally, these models lack dynamic judgement. Dynamic judgment refers to an evaluator's ability to learn from a few samples provided by a reference judge and adjust the evaluation policy over time. This behavior is essential to achieve personalized evaluation.\nOur work presents an effective way to align an open LLM with a reference judge in a data-limited setting. The goal is to align LLM judgment to that of the human judge. We propose a data augmentation method in the domain of direct assessment. Using the proposed data augmentation and selection techniques, we achieved approximately 9% and 7% higher Pearson correlation compared to"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 LLM as a Judge", "content": "Recently, employing LLMs as judges or evaluators has gained attention as a reliable and accurate paradigm to mimic the accuracy that human evaluation offers. Many works use proprietary LMs such as GPT-4 as reference judges. These models have shown high agreement with human assessments and text evaluations in both direct assessment and relevance judgments (Zheng et al., 2023; Latif and Zhai, 2023).\nKim et al.(Kim et al., 2024a) and Zhu et al. (Zhu et al., 2023) introduced methods for generating feedback datasets for both pairwise judgments and direct scoring assessment. These models used fine-tuned Llama model to align them against GPT-4, where proven high correlation. Kim et al. (Kim et al., 2024c) fine-tuned two models: the first model assigns a score to LLM responses using a direct assessment feedback dataset, and the second model is fine-tuned for pairwise ranking using pairwise feedback. Merging techniques were then employed to create a unified model that performed well in both pairwise ranking and direct scoring tasks."}, {"title": "2.2 Human Preference Alignment", "content": "LLMs excel at text generation but face challenges in instruction following and alignment with human expectations, spurring research to address these limitations (Wang et al., 2023). Supervised Fine-Tuning (SFT) has emerged as a key method for aligning LLMs with human expectations, demonstrated by its effectiveness in InstructGPT (Ouyang et al., 2022) and subsequent widespread adoption in alignment research (Wang et al., 2023).\nInstructGPT incorporates Reinforcement Learning (RL) after its SFT step. This approach builds on the concept of reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Casper et al., 2023). Following InstructGPT, several methods such as DPO (Rafailov et al., 2024), RAFT (Dong et al., 2023), and RLAIF (Lee et al., 2023) have introduced their work based on the RLHF method. They have enhanced their techniques to improve alignment or data efficiency.\nDPO has introduced an approach to RLHF which simplifies the process through closed-form policy extraction and a straightforward classification loss (Rafailov et al., 2024). DPO has gained significant traction in the field of LLM alignment, with prominent open models such as Llama3 (Dubey et al., 2024) and Mixtral (Jiang et al., 2024) adopting this methodology for their alignment strategies."}, {"title": "2.3 Data Efficient Alignment", "content": "The size and quality of data impact the cost and time of LLMs training (Kaddour et al., 2023; Kaplan et al., 2020). Efficient use of data can reduce the number of iterations needed for training (Kaddour et al., 2023). Some works focus on improving training data quality by synthesizing high-quality data and filtering low-quality data.\nPhi3 combines filtered web content and synthetic LLM-generated data from diverse sources to improve training data quality (Abdin et al., 2024). Llama3 emphasizes Llama2's capability to identify high-quality data and uses Llama2 to build a text classifier to improve training data quality (Dubey et al., 2024). FineWeb-Edu improved FineWeb by creating an educational quality classifier trained on LLama3-70B-Instruct annotations, using it to select only the most educational web pages. FineWeb-Edu, demonstrates superior performance on com-"}, {"title": "3 Data-Efficient Judgement", "content": "In this section, we present a data-efficient approach for aligning an LLM with a reference judge. While our work primarily focuses on machine-generated text, it can also be extended to human-generated text.\nFirst, we introduce the dataset format. Each dataset includes questions and corresponding responses as input, with feedback and scores as output. Next, we outline our methodology for augmenting the dataset, followed by a systematic approach to selecting the most effective judgment instances. Finally, we detail our method for training a data-efficient evaluator."}, {"title": "3.1 Data Curation and Augmentation", "content": "Many studies build a dataset for the judgment task (Kim et al., 2024c; Zhu et al., 2023; Kim et al., 2024b). Assessment tasks require strong reasoning ability because reasoning helps the judge make more accurate and fair decisions (Wang et al., 2024).\nwe show in table Table 2 open LLMs, such as Llama-3.1-8B-Instruct are ineffective evaluators. Previous studies (Kim et al., 2024a; Zhu et al., 2023) have demonstrated that LLMs such as Llama2 (7B, 13B, 34B) and Mistral 7B show low agreement with human evaluators or proprietary LLMs like GPT-4. In the rest of this section, we define our seed dataset and then introduce our methodology to augment data using a CoT approach. Aside from the benefit of growing our dataset, CoT enhances the model's reasoning ability which further improves the accuracy and fairness of its judgment calls (Mukherjee et al., 2023)."}, {"title": "3.1.1 Seed for Preference Dataset", "content": "We start with a question and response dataset. The response can come from either a human or an LLM depending on the actual task of interest. For each response, we collect a feedback and score (within a prespecified range) from a reference judge. The aim is to use this seed dataset to improve a base LLM judgement performance, i.e., making it behave similar to a reference judge.\nTo achieve this goal, we try to align the base LLM, which requires preference data (chosen and rejected) feedbacks and scores. More precisely, the training dataset comprises of n samples, with each sample represented as a tuple containing the following elements:\n$\\mathcal{D} = \\{(q_i, r_i, f_i, F_i^r, F_i^c)\\},$ (1)\nwhere $q_i$ is a question, $r_i$ denotes a response to $q_i$, $f_i$ indicates the golden feedback and score for $r_i$, which is generated by a reference judge, $F_i^r$ is a list of rejected feedbacks and their respective scores related to $r_i$, and $F_i^c$ is a list of chosen feedbacks and their respective scores related to $r_i$. Note that $q_i$, $r_i$, and $f_i$ are taken from a seed dataset, whereas $F_i^r$ and $F_i^c$ are provided by the base LLM. The following subsections explains how $F_i^r$ and $F_i^c$ are generated."}, {"title": "3.1.2 Na\u00efve Data Creation Approach", "content": "The base LLM is required to generate feedback and a corresponding score within a prespecified range. In this approach, we take all feedbacks and scores in $F_i^c$ as chosen feedbacks and place them in $F_i^c$. Next, we apply the base LLM to generate a feedback and a score for each response, i.e., $r_i$. These feedbacks and scores construct the rejected list, i.e., $F_i^r$, which we assume have inferior quality compared to those come from the reference judge. $F_i^c$ and $F_i^r$ form the basis of our preference data for aligning the base LLM to the reference judge (See Figure la)."}, {"title": "3.1.3 Pool of Feedback Approach", "content": "As shown in Figure 1b, in this approach, we use the base LLM to generate multiple feedback and score pairs for each response. The base LLM is provided with the reference judge's feedback as auxiliary information, treating it as a hint. This allows the base LLM to leverage its own reasoning abilities to generate better feedback and corresponding score using CoT.\nTo diversify employed chains of thought, the base LLM uses a range of temperatures from 0.2 to 1.4. In this way, for each $(q_i, r_i)$ pair, the base LLM uses different reasoning to evaluate the response. These generated feedbacks, produced at various temperatures is divided into two parts:\n1. Chosen pool: Chains of thought which led to correct scores, i.e., where the score matches the one assigned by the reference judge.\n2. Rejected pool: Chains of thought which led to incorrect scores, i.e., where the score does not match the one assigned by the reference judge.\nWe use pairs of chosen and rejected feedback from these two pools to create $F_i^c$ and $F_i^r$. Note that, unlike the Na\u00efve data generation approach, here we do not use reference judgments directly."}, {"title": "3.1.4 Efficient Sampling Approach", "content": "In this section, we introduce a methodology for selecting more effective samples from the reference judge. This approach helps to align the model more accurately with the reference judge, reducing the risk of biases that can emerge during the alignment process.\nAs shown in Figure 1c, instead of using the entire set of golden feedback, we choose a subset of it. First, the sentence embedding is calculated for each golden feedback, i.e., $f_i$. Next, these embeddings are clustered based on their similarity. We use"}, {"title": "4 Experiment Setup", "content": ""}, {"title": "4.1 Alignment Dataset", "content": "We extract specific partitions for our analysis from two feedback datasets: UltraFeedback (Cui et al., 2024) and BigGen-Bench (Kim et al., 2024b). From UltraFeedback, we utilize the TruthfulQA (Lin et al., 2022) partition, a benchmark designed to assess whether language models provide truthful responses to questions across diverse categories such as health, law, finance, and politics. This benchmark contains 817 questions and 3,268 responses across 38 categories, challenging models to avoid generating false answers that may stem from imitating human texts. These questions were given to different language models, and their corresponding scores were generated by GPT-4 on a scale from 1 to 10.\nThe score distributions for each evaluated model's responses are shown in Figure 2a. As illustrated in Figure 2a, proprietary LLMs, such as ChatGPT, tend to achieve higher scores, with a distribution skewed toward the upper range. Furthermore, scores in the 1 to 6 range are relatively rare across many models, suggesting a potential for biases in the alignment process.\nFor BigGen-Bench, we extract 2,000 samples from the reasoning partition, which evaluates the mathematical reasoning of models. Additionally, the BigGen-Bench dataset includes detailed scoring rubrics for each question, offering a more structured evaluation framework. BigGen-Bench is structured with model performance scores on a 5-point Likert scale (Robinson, 2014), with columns representing model names and scores for different capabilities. These scores come from multiple judge models, including GPT-4, Claude-3-Opus, and Prometheus-2 (see Figure 2b).\nAfter collecting the seed dataset, we applied three techniques discussed in Section 3, to generate and augment the feedback\u00b9. Prompts used"}, {"title": "4.2 Alignment Recipe", "content": "To align the model with the reference judge, we apply DPO for each dataset separately. Rather than tuning the full model, we train a LoRA (Hu et al., 2021) adapter, which avoids the need to keep both a reference and trained model in GPU memory. For LORA hyperparameters, we set $r = 32$, $a = 16$,"}, {"title": "4.2.1 Alignment Experiments", "content": "In the first experiment, DPO was applied to the pairs $(F_i^c, F_i^r)$. In the second experiment, it was applied to pairs of $(F_i^c, F_i^r)$, generated using the pool of feedback approach, as explained in Subsection 3.1.3. In the third experiment, we used the efficient sampling method described in Subsection 3.1.4 to improve the alignment process by selecting more effective samples from $F_i^c$ to form preference pairs $(F_i^c, F_i^r)$. This method helps mitigate biases that may arise during augmentation. The details of each preference dataset are provided in Table 1."}, {"title": "4.3 Evaluation Setup", "content": "In this section, we describe our experimental setup for assessing evaluator LMs, a process we refer to as meta-evaluation since we are evaluating the performance of the evaluator itself. We employ Pearson, Spearman, and Kendall-Tau as performance metrics to measure scoring correlations against the reference evaluator (GPT-4). This demonstrates how well the model aligns with the reference evaluator."}, {"title": "5 Results", "content": "In this section, we present the results of our judge model and analyze the outcomes, comparing the three approaches discussed in Section 3. The Pearson correlation between the scores assigned by the aligned models and those assigned by the reference judge is shown in Table 2.\nIn the BigGen-Bench dataset, as seen in Figure 2b, the response scores are well-balanced. The pool of feedback approach in BigGen-Bench shows more agreement with the reference judge (GPT-4). Another insight from this table is that aligning the model with the reference judge also improves its alignment with human judgments. As observed in Table 2, aligning the model with GPT-4 increases its agreement with human evaluations. We also note that when the training dataset size is reduced from 4.9k to 2.7k samples, as shown in Table 1, the model's performance still surpasses that of the Na\u00efve data generation approach described in Subsection 3.1.2. These results suggest that COT reasoning can help the model perform evaluation and judgment tasks more effectively.\nFor the TruthfulQA dataset, as illustrated in Figure 2a, the distribution is skewed. Scores like 9 occur frequently, while lower scores, such as 1, are less common in the seed dataset. This imbalance carries over to the augmented dataset, affecting its distribution. Nonetheless, by employing an efficient sampling strategy, the skewness is better managed, allowing for more representative samples and improved performance in judgment tasks. The observed decrease in correlation between GPT-4 and the pool of feedback can be attributed to the amplified skewness in the augmented data."}, {"title": "6 Conclusion", "content": "Although large language models (LLMs) showed promise for automatic evaluation, adapting them to subjective judgment tasks in data-scarce environments remained challenging. Proprietary LLMs often failed to maintain alignment with reference evaluators over time, and efforts to use open LLMs"}, {"title": "7 Limitation", "content": "While our approach demonstrates significant improvements in aligning LLM judgment with human evaluators, several limitations remain. First, our work is constrained by the availability of feedback data. Although we explored data augmentation techniques to mitigate this, the alignment performance could benefit from more diverse datasets. Additionally, our method focuses on a narrow range of tasks, such as math and truthful question-answering, which limits its generalizability to other domains requiring more judgment."}, {"title": "8 Appendices", "content": ""}, {"title": "A Preference Data Augmentation Prompt", "content": "Feedback generation for each pair of questions and answers with LLM needs an input prompt. Considering that we evaluate our approach on different datasets, we use adjusted input prompt templates for every dataset. The following templates show the input prompts for the TruthfulQA dataset and the BigGen-Bench dataset. These prompts are the output of the CoT prompt generator."}]}