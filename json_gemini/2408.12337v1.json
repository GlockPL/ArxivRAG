{"title": "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "authors": ["Karmvir Singh Phogat", "Sai Akhil Puranam", "Sridhar Dasaratha", "Chetan Harsha", "Shashishekar Ramakrishna"], "abstract": "Recent research has shown that smaller language models can acquire substantial reasoning abilities when fine-tuned with reasoning exemplars crafted by a significantly larger teacher model. We explore this paradigm for the financial domain, focusing on the challenge of answering questions that require multi-hop numerical reasoning over financial texts. We assess the performance of several smaller models that have been fine-tuned to generate programs that encode the required financial reasoning and calculations. Our findings demonstrate that these fine-tuned smaller models approach the performance of the teacher model.\nTo provide a granular analysis of model performance, we propose an approach to investigate the specific student model capabilities that are enhanced by fine-tuning. Our empirical analysis indicates that fine-tuning refines the student models ability to express and apply the required financial concepts along with adapting the entity extraction for the specific data format. In addition, we hypothesize and demonstrate that comparable financial reasoning capability can be induced using relatively smaller datasets.", "sections": [{"title": "1 Introduction", "content": "In recent years, the development of large language models (LLMs) has achieved significant advances in natural language processing (NLP). Scaling up the size of these models has not only improved sampling efficiency and performance, (Kaplan et al., 2020) but also introduced reasoning capabilities (Wei et al., 2022b; Kojima et al., 2022). LLMs have been shown to perform well on tasks requiring reasoning capabilities in various domains, including code writing (Chen et al., 2021a), math problem solving (Lewkowycz et al., 2022; Polu et al., 2023), dialogue (Glaese et al., 2022; Thoppilan et al., 2022), common sense reasoning (Shwartz et al., 2020; Chowdhery et al., 2022) and symbolic reasoning (Wei et al., 2022a; Wang et al., 2023b).\nA major drawback of these methods is their reliance on extremely large models with hundreds of billions of parameters. These models can be costly to deploy at scale due to high computational requirements and inference costs. To overcome these limitations, recent research has focused on inducing reasoning in smaller models. A common approach is to use large models to generate training samples with demonstrations of reasoning on one or more tasks that are then used to fine tune smaller models (Magister et al., 2023; Ho et al., 2023). While these methods have shown promising results on various tasks including arithmetic, symbolic and common-sense reasoning, the applicability, and effectiveness of these methods in specific domains such as finance need further exploration. Question answering in the finance domain poses a unique set of challenges, requiring the understanding of financial concepts along with the ability to perform numerical reasoning. This complexity introduces a significant challenge that is distinct from classical question answering problems (Yang et al., 2018; Rajpurkar et al., 2018)\nIn this paper, we present an empirical study that provides experimental evidence supporting the effectiveness of fine-tuning small language models for financial question answering. Our research is guided by several critical questions:\nRQ1: To what degree does fine-tuning small language models improve their performance on financial question answering tasks?\nRQ2: What are the intrinsic characteristics of the base language model that contribute to its performance prior to fine-tuning?\nRQ3: Which aspects of question answering benefit directly from the pre-trained knowledge, and what specific improvements are enabled by fine-tuning?\nRQ4: What are the fine-tuning data requirements to achieve these improvements?\nTo address these questions, we adapt previous ap-"}, {"title": "2 Background", "content": "Pre-trained large language models are shown to perform well on tasks requiring reasoning when used with certain techniques. (Wei et al., 2022b) proposed prompting the LLMs to solve the problem step-by-step by providing a few exemplars. (Chen et al., 2023) proposed a few-shot prompt to produce a program which is then executed externally. However, methods relying on pre-trained LLMs can be costly to deploy at scale.\nRecent efforts attempt to replicate these reasoning capabilities in small language models. One of the common approaches is following a teacher-student setup where a pre-trained LLM acts as a teacher, generating training data which is used to teach a small language model, the student. (Mukherjee et al., 2023; Mitra et al., 2023) aim to train models to exhibit generic reasoning abilities. They utilize GPT-3.5 TURBO and GPT-4 as teacher models to generate training data with carefully crafted prompts. On the other hand, (Fu et al., 2023; Magister et al., 2023; Ho et al., 2023) train task specific small language models with CoT based explanation from pre-trained LLMs. Specifically for problems involving mathematical reasoning, (Wang et al., 2023a; Gou et al., 2023; Toshniwal et al., 2024; Wang et al., 2023c) propose to generate programs from the pre-trained LLMs and train the small language models. In contrast, we focus on fine-tuning small language models for financial question answering problems (Chen et al., 2021b, 2022; Zhu et al., 2021). Solving these problems requires financial domain knowledge and complex reasoning compared to the math word problems addressed in previous studies.\nPrior works have studied the use of pre-trained LLMs for financial question answering. (Srivastava et al., 2024) perform a detailed comparison of the performance of pre-trained LLMs on financial datasets with various prompting techniques. They also introduce a novel prompting technique optimized for semi-structured documents. (Phogat et al., 2023) developed zero-shot prompt templates for question answering over financial documents that guide the LLMs to encode reasoning into a python program. These efforts do not consider fine-"}, {"title": "3 Fine-tuning for Financial Question Answering", "content": "We adopt the approach of using very large-scale models as reasoning teachers, and fine-tuning relatively small-scale student models from the prompt-completion pairs generated using the teacher model (Hsieh et al., 2023; Ho et al., 2023). Specifically, the large model is used to generate python code with comments that encapsulates the reasoning required to answer the financial question. Furthermore, the programs are generated with a specific structure that facilitates subsequent performance analysis of the fine-tuned model, as discussed in detail in Sec. 5. The fine-tuning task is performed in three steps as shown in Figure 1.", "subsections": [{"title": "3.1 Code Generation", "content": "In the code generation step, we employ the teacher model to generate a Python code for a specified question-answering task. Financial question answering consists of three distinct steps: understanding the concept and writing the formula required to answer the question, finding the relevant entities, and then executing and storing the calculations. A sample of the desired code structure encapsulating this reasoning process is show in in Figure 2. We guide the teacher model to consistently generate the desired code structure through program of thought (PoT) prompting (Chen et al., 2023). In few-shot PoT prompting, as shown in Figure 1, few shot exemplars are prefixed as demonstrations for the teacher model to generate codes in the desired format.\nIn the financial question answering training dataset, each sample contains the final answer to the question and additionally it may contain a program which demonstrate step-wise arithmetic calculations that are required to be performed to arrive at the answer. We incorporate these programs as answer hints in the few-shot PoT prompt to guide the teacher model towards accurate code generation. This strategy can potentially improve the question-answering accuracy of the teacher model."}, {"title": "3.2 Data Curation", "content": "During data curation, we filter out the samples with incorrect teacher codes and format the filtered samples to prompt-completion pairs for the student model. At the filtering stage, the samples with incorrect teacher-generated codes are identified by executing the codes and comparing the resulting answer with the ground truth answer. The filtered samples are then formatted to prompt-completion pairs as per student model requirements. For instance, the prompt instructions for the MISTRAL 7B (instruction tuned) model should begin with the token [INST] and ends with the token [/INST]. In addition, special characters like '#' can be used to symbolize the prompt structure."}, {"title": "3.3 Fine-tuning", "content": "The fine-tuning task for question answering is represented by the prompt-completion pairs: $D = \\{x_i, y_i\\}_{i=1}^n$ where $x_i$ is a token sequence for the fine-tuning prompt and $y_i$ is a token sequence for the corresponding code completion, as shown in Figure 1. We use low rank adaptation (LoRA) (Hu et al., 2022), a special class of parameter efficient fine-tuning that takes advantage of the low \"intrinsic dimension\" of pre-trained LLMs, when adapting to a specific task (Aghajanyan et al., 2021). The student model's LoRA adapter is fine-tuned to adapt to the financial question answering task."}]}, {"title": "4 Experiments", "content": "", "subsections": [{"title": "4.1 Experimental Design", "content": "Datasets: We conduct our experiments on three English language financial question answering datasets FinQA (Chen et al., 2021b), ConvFinQA (Chen et al., 2022) and TATQA (Zhu et al., 2021). The question answering task, in FinQA and TATQA datasets, is to answer the questions using the passage containing text and table content. The experiments for TATQA are restricted to questions of arithmetic type. In ConvFinQA, the task is to answer the last question from a conversation containing a series of interrelated questions, based on"}, {"title": "4.2 Fine-tuning Results", "content": "The performance of the fine-tuned LLMs is reported in Table 2. For comparison, we report the performance of zero-shot and few-shot prompts using GPT-3.5 TURBO and GPT-4. These results"}]}, {"title": "5 Performance Analysis", "content": "We examine the evolution of model capabilities during fine-tuning, seeking to identify specific model enhancements. To this end, we define, and measure three key capabilities (1) concept understanding measured by the ability to correctly identify the required calculation (2) entity extraction measured by the ability to extract all required entities and (3) generation of executable code. Our method relies on comparing the output of the student models with that of the teacher generated codes. Therefore, we remove all samples with incorrect teacher generated codes from further analysis.", "subsections": [{"title": "5.1 Concept accuracy", "content": "Due to significant model output variation, it is hard to define a simple metric to measure concept accuracy. Motivated by the promising results shown by others (Zheng et al., 2023) in using LLM for evaluation in challenging scenarios, we propose the use of GPT-4 to rate the concept understanding demonstrated in the model output, using a 5-point scale defined as follows: 1: no understanding 2: limited understanding 3: partial understanding 4: mostly demonstrates understanding 5: perfect understanding. After some prompt engineering using 25 random student code samples, we found this method to provide reasonable assessment of concept accuracy. The key instructions needed were to guide the evaluator to focus on the presence of relevant entities, and not their values or the output format. The final prompt which includes the instructions, the output of the student model, the gold code and the question are shown in Figure 6 in the Appendix.\nWe define the concept accuracy as the percentage of cases where the student model output receives a rating of 5. For all models, we measure the concept accuracy for the base model as well as checkpoint after one epoch (see Figure 8-12 in the Appendix for rating distributions). The concept accuracy shown in Table 3, indicates the ORCA-2 family models have significantly lower concept accuracy initially as compared to other models. However, the fine-tuning leads to substantial improvements in these models, leading to a small gap in concept accuracy as compared to other models post fine-tuning. To better understand these results, we manually examined fifty random sample outputs from each of the base models, that were assessed as lacking concept accuracy. We then examined the output of the models for the same samples after one epoch. While the analysis was performed on all models, in the following sections we present the detailed analysis of PHI-3-MINI and ORCA-2-7B models (See Appendix D for many illustrative examples). The analysis of the remaining models reveals similar patterns.\nPHI-3-MINI concept accuracy: For the base PHI-3-MINI model, the overall concept accuracy was around 70% with about 16% samples receiving a rating of 1 or 2 by GPT-4. The PHI-3-MINI model responses don't follow a standard structure while answering the question. In a significant number of samples with low concept rating, the base model's response does not provide the formulas/arithmetic steps that are required to answer the question, thus failing to demonstrate concept understanding. About 7% of the samples received a rating of 4, with many of these responses containing minor arithmetic errors, providing formulas with closely related but not correct entities, and other small issues. Most of the base model responses with missing formulas are corrected by training the model for one epoch. Approximately 80% of cases, initially rated 4, are also corrected after one epoch. Several cases where the formula was properly but incorrectly written by the base model remained incorrect even after 1 epoch.\nORCA-2-7B concept accuracy: The initial ORCA-2 model provides a long explanation of the required reasoning, often failing to produce executable code. Sometimes the formulae are written descriptively without use of mathematical representations. In a significant number of cases, the model includes the input passage leading to an incomplete output that does not contain the required formula. As a result, 43% of the samples received a rating of 1 or 2 and the model has a low concept accuracy of 28%. Occasionally, the long explanations do provide the correct formula which is not identified by the GPT-4. These results are likely due to the ORCA-2 being a model that is specialized to solve reasoning problems using elaborate reasoning traces and not being explicitly trained in generating code that encodes the reasoning. Hence"}, {"title": "5.2 Entity extraction", "content": "To measure the entity extraction capability of the student models, we incorporate the first line of the teacher model's code into the fine-tuning prompt and perform inference on the fine-tuned model to complete the Python code. Since the provided line is a comment with the formula required to answer the question, the main task is to extract the required entities. We then use GPT-4 with the prompt shown in Figure 7 in the Appendix, to assess the student code and determine if all the required entities have been correctly extracted.\nThe results shown in Table 3 indicate significant improvement in entity extraction capability of all the student models during the fine- tuning, with all of them showing similar accuracy after fine-tuning. The results suggest that the fine-tuning helps the base model adapt to the specific table structures and data format present in the financial dataset, improving the entity extraction performance and contributing to overall model accuracy."}, {"title": "5.3 Code generation", "content": "As the required code for this problem is simple, we only use the ability to generate executable code as a measure of accuracy. The results are shown in Table 3. As compared to other models, the base"}, {"title": "5.4 Effect of training data size", "content": "Given the observed effects of fine-tuning, it naturally leads us to inquire whether a smaller dataset might be adequate for achieving similar enhancements. Another related and interesting question is the volume of data necessary to adapt the FinQA model for proficiency with ConvFinQA, which, while originating from the same domain as FinQA, introduces the additional complexity of processing conversational-style questions.\nWe perform fine-tuning experiments with the following settings to understand the data requirements (a) 1500 data points randomly sampled from the original FinQA dataset and (b) 1000 samples randomly sampled from the FinQA dataset combined with 500 samples randomly sampled from ConvFinQA dataset. The models fine-tuned on these datasets is compared with the corresponding mod-"}]}, {"title": "6 Conclusion", "content": "We explored the performance of small language models fine-tuned for financial question answering, using exemplars generated by a very large teacher model. The small models achieved accuracy comparable to that of the teacher model, driven primarily by improved ability to apply financial concepts as well as entity extraction. We showed that smaller datasets can yield similar results, suggesting that small language models can be efficiently fine-tuned for complex financial domain problems."}, {"title": "Limitations", "content": "The use of GPT-4 to assess concept understanding using the base and fine-tuned student models' output, can sometimes produce erroneous determinations of concept error. While we instruct GPT-4 to not assess based on the output format, we found that elaborate responses could sometimes lead to a false assessment. Despite this limitation, the method is still effective in achieving our primary goal of understanding the effect of fine-tuning for financial question answering.\nWhile we perform hyperparameter optimization for fine-tuning the student models, the small differences between the performance of the fine-tuned models could be simply due the hyperparameters not being fully optimal. Since we focus more on the improvements achieved in the fine-tuned model over their corresponding base model, this doesn't have a major impact on the findings reported in the paper.\nOur experiments are limited to only on the single task of financial question answering. The performance and behaviour of the small models in a multi-task setup needs to be explored in the future.\nWhile we demonstrate that small language models can achieve performance approaching that of much larger models, they also inherit some of the associated risks. For cases where the reasoning was incorrect, the current system will provide an explanation with a high level of confidence, which can be misleading. Our models currently does not address or control for such behavior and we have not studied the nature or extent of this problem. In practice, this can pose challenges for practical use in real world systems. For real world use, a human would need to review the output prior to using any of the information generated. Future research to understand this potential risk in more detail and provide indications of when the model is not sure of its response would be valuable."}, {"title": "Disclaimer", "content": "The views reflected in this article are the views of the authors and do not necessarily reflect the views of the global EY organization or its member firms."}]}