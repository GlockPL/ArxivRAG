{"title": "Clinical trial cohort selection using Large Language Models on n2c2 Challenges", "authors": ["Chi-en Amy Tai", "Xavier Tannier"], "abstract": "Clinical trials are a critical process in the medical field for introducing new treatments and innovations. However, cohort selection for clinical trials is a time-consuming process that often requires manual review of patient text records for specific keywords. Though there have been studies on standardizing the information across the various platforms, Natural Language Processing (NLP) tools remain crucial for spotting eligibility criteria in textual reports. Recently, pre-trained large language models (LLMs) have gained popularity for various NLP tasks due to their ability to acquire a nuanced understanding of text. In this paper, we study the performance of large language models on clinical trial cohort selection and leverage the n2c2 challenges to benchmark their performance. Our results are promising with regard to the incorporation of LLMs for simple cohort selection tasks, but also highlight the difficulties encountered by these models as soon as fine-grained knowledge and reasoning are required.", "sections": [{"title": "1. Introduction", "content": "Clinical trials are a critical process in the medical field for introducing new treatments and innovations [1]. However, cohort selection for clinical trials is a time-consuming process that often requires manual review of patient text records for specific keywords. Naturally, Clinical Trial Recruit-"}, {"title": "2. Related Work", "content": "Since the recent arrival of LLMs, many articles have focused on their potential role in patient selection for clinical trials [17]. Like us, most consider"}, {"title": "2.1. n2c2 Challenges", "content": "Challenges and competitions in the research community serve as catalysts for innovation, fostering collaboration, and driving advancements across various fields [22]. In particular, the n2c2 challenges are an invaluable contribution in the medical natural language research community as they bring forth a challenge dataset that is of high quality and draw a multitude of academic institutions across the world to brainstorm innovative solutions for addressing pertinent healthcare problems [10].\nThe n2c2 challenge datasets represent a pivotal resource in the realm of clinical informatics, aimed at harnessing the potential of unstructured clinical text for advancing healthcare [10]. These datasets are designed to facilitate research that leverages existing clinical records to extract valuable insights, thereby fostering improvements in medical practice and patient out- comes. The interest in these datasets within the medical community stems from their ability to support natural language processing (NLP) and machine learning techniques tailored to healthcare contexts. By providing access to comprehensive, deidentified text from the Research Patient Data Registry at Partners, the n2c2 datasets enable researchers to tackle challenges such as disease recognition, information extraction, and temporal relations. Years after the competition has completed, these datasets are still being used to advance research and study the results of different approaches such as trans- formers [23]."}, {"title": "2.2. Prompting Techniques", "content": "Pre-trained large language models (LLMs) have recently gained popular- ity for natural language processing tasks, revolutionizing various fields from text generation to sentiment analysis and machine translation with a high- labour market impact potential [14]. These models, such as OpenAI's GPT- 3 and its predecessors, have demonstrated remarkable capabilities in under- standing and generating human-like text for both academic research and industrial applications [26]. Trained on vast text datasets, these models ac- quire a nuanced understanding of language with adoption across a multitude of industries [15]. However, when applied to more specialized cases, these generalized models inaccurately capture specific nuances in the field [16]. In the medical realm though, there have been releases of new models trained specifically on medical texts such as MedAlpaca [27] to counteract the effects of training on generalized data. Though these pre-trained medical LLMs have shown potential, crafting effective prompts still remains an important design technique that greatly affects the LLMs' task performance [28]."}, {"title": "3. Datasets", "content": "This section details the three challenge datasets used in our study: n2c2- 2006, n2c2-2008, and n2c2-2018."}, {"title": "3.1. n2c2-2006 Challenge Dataset", "content": "The n2c2-2006 challenge dataset is a publicly available dataset that con- tains randomly drawn de-identified clinical records from Partners HealthCare for identifying patient smoking status [11]. This dataset contained a total of 502 records with 398 in the training set and 104 in the test set. The records"}, {"title": "3.2. n2c2-2008 Challenge Dataset", "content": "The n2c2-2008 challenge dataset provides 1,237 discharge summaries that were de-identified semi-automatically with synthetic identifiers used to re- place private health information [12]. The patients in these summaries had been hospitalized for obesity or diabetes sometimes since 12/1/04 and were either overweight or diabetic from the Partners HealthCare Research Patient Data Repository. Fifteen obesity co-morbidities were identified:"}, {"title": "3.3. n2c2-2018 Challenge Dataset", "content": "The n2c2-2018 challenge dataset contains 288 patients with 202 patients in the training set and 86 patients in the test set [13]. There are a total of 1,267 English records (781,006 tokens) with a rough estimate of 2 to 5 docu- ments per patient (2,711 tokens per set of patient records). The records were obtained from a collection of American English longitudinal records, namely the 2014 i2b2/UTHealth shared tasks. All records were then de-identified as per the Health Insurance Portability Accountability Act guidelines with the removal and replacement of patient-linking information with realistic surro- gates and random time-shifting of dates [13].\nSince most of the patients in this dataset are at risk for heart disease and all of them have diabetes, the studied criteria were either common to most studies or related to these health conditions [13]. A total of thirteen criteria were selected:\n1. ABDOMINAL: History of intra-abdominal surgery, small or large in- testine resection, or small bowel obstruction\n2. ADVANCED-CAD: Advanced cardiovascular disease (CAD) defined by having two or more of the 4 sub-criteria defined in the guidelines"}, {"title": "4. Method", "content": "A two-stage approach was implemented to evaluate and select the most effective large language model (LLM) for clinical trial cohort selection. As mentioned, three n2c2 challenge datasets were studied. In order to respect these datasets conditions of use, we limited ourselves to LLMs that we could use on our own computing cluster.\u00b9\nIn the first stage, various LLMs were systematically assessed based on their performance on a series of prompts for the n2c2-2018 dataset with the aim to identify the most promising LLM for clinical trial cohort selection. In the second stage, fine-tuning of the selected LLM from the first stage was conducted for the n2c2-2006 and n2c2-2008 challenge datasets."}, {"title": "4.1. Stage 1: Selection of the Best LLM using n2c2-2018 Dataset", "content": "The n2c2-2018 challenge dataset was utilized as the primary dataset for this stage. The original training set was divided into a new training set and a validation set to facilitate the model selection and tuning process. This split was conducted to ensure a robust evaluation of each model's performance in a controlled setting.\nThree different prompting strategies were tested across a range of LLMs to assess their capability in extracting relevant information for clinical trial cohort selection:\n1. Initial Basic Prompt: A straightforward prompt that outlined the task without any additional context or examples.\n2. Few-Shot Learning: A prompt that included a small number of ex- ample inputs and outputs from the training set to guide the model's responses.\n3. Iterated Few-Shot Learning: A refined version of few-shot learning, where the examples were iteratively selected from the training set based on previous performance to better train the model."}, {"title": "4.2. Stage 2: Fine-tuning of the Selected Stage 1 LLM using n2c2-2006 and n2c2-2008 Datasets", "content": "The n2c2-2006 and n2c2-2008 challenge datasets were utilized as the pri- mary datasets for this stage. The original training set was divided into a new training set and a validation set for the fine-tuning process. An itera- tive approach was taken to select the best set of few-shot learning examples. This process involved evaluating the model's performance with various sets of examples selected from the training set and iterating based on the highest F1 score on the training and validation set.\nThe best few-shot learning set was then selected based on the F1 perfor- mance on the training and validation set. The performance was then recorded for the test set for the n2c2-2006 and n2c2-2008 challenge dataset using the best few-shot learning set."}, {"title": "5. Results", "content": "The results reported below are for the model analysis on the n2c2-2018 challenge dataset for both the validation and test split for seven different LLM models (Table 2 and Table 3 respectively), and the n2c2-2006 and n2c2-2008 challenge test dataset for the vicuna-13b model (Table 4, Table 5, and Table 6)."}, {"title": "6. Discussion", "content": ""}, {"title": "6.1. Lessons learned", "content": "The results showcased that using LLMs for patient cohort analysis are promising, with some LLMs generally performing better than others for co- hort selection. Compared to the reported performance on the n2c2-2006, n2c2-2008, and n2c2-2018 datasets, the best LLM was able to achieve better or similar performance for some of the tasks. For example, for the n2c2-"}, {"title": "6.2. Limitations", "content": "This work explores the classic and general uses of LLMs, but has cer- tain limitations. In particular, we have only applied approaches that do not require any specific adaptation to each variable researched. Chains of thoughts, prompt optimization [32, 33] or other techniques could certainly"}, {"title": "7. Conclusion", "content": "This paper studied the performance of large language models on clin- ical trial cohort selection and benchmarked various models on three n2c2 challenge datasets against previous non-LLM approaches. Though there are promising results for some of the n2c2 challenge tasks, it is evident that LLMs perform better when the selection criteria is straightforward and poorly when there are more nuances with the selection criteria. Surprisingly, the results also indicate that vicuna-13b and mistral-7b-instruct models (trained on gen- eral texts) perform the best with more stability across the n2c2-2018 cohort selection tasks compared to the other studied LLM models such as MedAl- paca (trained on medical texts).\nExtensions of this work thus include generalizing the process of cohort se- lection using LLM across a variety of domains, such as that discussed in [34]; and fine-tuning an LLM using clinical text records, similar to that in Clini- calMamba [35]."}, {"title": "Supplementary Material A: Prompt examples", "content": "{} is used to denote the location of the selected patient text for inference.\nInitial Basic Prompt\nThese are some sentences from a patient's clinical report. Answer Yes or No to the final question.\n{}\nQuestion: Does the text mention that the patient uses aspirin to prevent myocardial infarction (MI)?\nFew-Shot Learning\nThese are some sentences from a patient's clinical report. Does the text mention that the patient uses aspirin to prevent myocardial infarction (MI)? Answer Yes or No to the final question. Let's think step by step."}]}