{"title": "Evolutionary Trigger Detection and Lightweight Model Repair Based Backdoor Defense", "authors": ["Qi Zhou", "Zipeng Ye", "Yubo Tang", "Wenjian Luo", "Yuhui Shi", "Yan Jia"], "abstract": "Deep Neural Networks (DNNs) have been widely used in many areas such as autonomous driving and face recognition. However, DNN model is fragile to backdoor attack. A backdoor in the DNN model can be activated by a poisoned input with trigger and leads to wrong prediction, which causes serious security issues in applications. It is challenging for current defenses to eliminate the backdoor effectively with limited computing resources, especially when the sizes and numbers of the triggers are variable as in the physical world. We propose an efficient backdoor defense based on evolutionary trigger detection and lightweight model repair. In the first phase of our method, CAM-focus Evolutionary Trigger Filter (CETF) is proposed for trigger detection. CETF is an effective sample-preprocessing based method with the evolutionary algorithm, and our experimental results show that CETF not only distinguishes the images with triggers accurately from the clean images, but also can be widely used in practice for its simplicity and stability in different backdoor attack situations. In the second phase of our method, we leverage several lightweight unlearning methods with the trigger detected by CETF for model repair, which also constructively demonstrate the underlying correlation of the backdoor with Batch Normalization layers. Source code will be published after accepted.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Neural Networks (DNNs) have been increasingly used in various areas such as traffic sign recognition in autonomous vehicles [1], medical image analysis in healthcare [2], face recognition in security systems [3] and risk assessment in finance [4].\nHowever, DNNs are susceptible to malicious attacks. Backdoor attacks are a particularly insidious and powerful type of attacks that can cause DNNs to exhibit normal behaviors for clean samples while displaying abnormal behaviors when the samples are poisoned with triggers [5], [6]. Poisoned samples with triggers are always classified to the target label by a backdoored DNN model as shown in Fig. 1. Backdoor attacks cause wide applications of DNNs in risks. For example, a backdoored model in an autonomous vehicle would misclassify the traffic signs with triggers, which can cause a serious security accident. Despite the significant impacts, the backdoor is difficult to detect since abnormality only emerges when there are triggers in inputs and defenders usually have no prior knowledge of the triggers.\nObviously, defense against the backdoor attack is of great significance. Excellent defense methods are expected to eliminate the backdoor effect while maintaining the performance of predicting clean inputs. Existing defenses employ different ways to defend against backdoor attack. For example, many of them aim to eliminate the backdoors by fine-tuning or pruning the models [7], [8]. The classic Neural Cleanse technique modifies the model using trigger reversing and neuron pruning technologies, which are effective against backdoor attack but compromise classification accuracy [9]. There are also defenses based on sample-preprocessing that do not need to modify the structure of the parameters of the models. STRIP blends clean images linearly with the input, then determines whether the input is poisoned by observing the divergence of the predictions on blended images [10]. However, it is difficult to set a suitable divergence threshold which can always separate clean and poisoned inputs accurately. Moreover, Februus [11] neutralizes the backdoor effect by removing the most influential part depending on GradCAM [12] and restoring it by Generative Adversarial Network (GAN) [13]. This defense processes all the inputs, whether they are poisoned or clean, so it inevitably affects the model performance on clean inputs since GAN cannot always repaint the removed part accurately. And NEO locates the position of the trigger by repeatedly randomly cropping a part from the input and pasting it to other clean images to see if the number of changed predictions exceeds a threshold [14]. However, such a random search based method is not precise and effective, and the unsuitable threshold also influences the model usability.\nBesides, another limitation of the existing methods is that they cannot handle triggers with different sizes and numbers, while these kinds of triggers are common in physical world. Clearly, it is challenging to mitigate the backdoor effect while maintaining the model performance.\nTo achieve these objectives simultaneously, we propose an efficient backdoor defense based on evolutionary trigger detection and lightweight model repair. The trigger detection is achieved by a sample-preprocessing based method, CAM-focus Evolutionary Trigger Filter (CETF). In the inference phase of the model, CETF checks every image input of the model and filters the poisoned images with triggers. And the triggers extracted by CETF are then used to repair the model by our unlearning methods.\nSpecifically, in CETF, we first detect influential regions roughly by GradCAM [12] for the input. Then, we design the objective function rationally and solve it based on an evolutionary algorithm [15], so as to accurately locate the most influential part of the input image with a size as small as possible. After that, we paste the optimized region from the evolutionary algorithm on a set of clean images and check whether their predictions change, thus determining whether there is a trigger in the input. CETF is able to satisfy both the robustness and the usability requirements, and even robust with the triggers variable in sizes and numbers.\nAfter CETF, we unlearn the backdoored model by images with the extracted trigger for model repair. The na\u00efve unlearning method is based on fine-tuning the model with the images pasted the reversed triggers [9]. Benefiting from the extracted triggers of CETF which are so accurate that approximate the triggers in the training phase, our model repair by unlearning is lightweight and practical. Also, the unlearning is completed in a very short time.\nFurthermore, we find the existence of backdoor in the Batch Normalization (BN) layers. Therefore, based on our findings, we propose two more efficient methods, BN-unlearning and BN-cleaning, for model repair. Specifically, by modifying only the parameters of the BN layers, our proposed model-repair methods get excellent effect in a large number of experiments, which further demonstrates the validity of our findings. Our viewpoint that the backdoor is injected by attackers to the model by setting the statistical information in the BN layers when training with triggers has great significance to the interpretability of backdoor.\nOur contributions can be summarized as follows.\n\u2022 We firstly combine the evolutionary algorithm with backdoor defense. CETF is an effective way to search for the trigger in poisoned inputs accurately. Experimental results demonstrate the high robustness of CETF against the content, size and other attributes of triggers, thereby proving its universality and applicability in practice.\n\u2022 The defense pipeline based on extracting triggers of CETF followed by repairing the model by unlearning shows a superior effect. We also propose two lightweight model repair methods for efficiency. Substantial experimental results show that our defense mitigates the backdoor effectively and efficiently without influencing the inference performance of the model.\n\u2022 Furthermore, we explore the \"shortcut\" for trigger in the backdoored model. We propose that the backdoor is hidden in the Batch Normalization layers and a lot of experiments are conducted for verification. Based on the excellent effect of our backdoor defense, we think that our findings are inspiring to the future research about AI backdoor and model's interpretability."}, {"title": "II. RELATED WORK", "content": "Backdoor injected in the model by the attackers can be activated by the trigger, which leads the model to output the target class. It is widely believed that backdoor attack provides a shortcut for trigger in the model [16] [17] [18]. Therefore, an image poisoned with a trigger goes through the shortcut whatever the original content of the image is, thereby being classified as the target class.\nHowever, there has not been a consensus on the mechanism of backdoor in the model structure. Yang et al. have analyzed the mechanism of backdoor [19]. They think the skip connections influence backdoor and they try to suppress the skip connections for defense. However, suppression has obviously negative effects on model's inference performance. Therefore, their experiments only demonstrate the importance of the skip connections for the model's usability, but cannot demonstrate directly that the backdoor is highly correlated to the skip connections.\nIn this paper, we propose a viewpoint that the backdoor is hidden in the BN layers and demonstrate its validity by application with various backdoor attacks.\nThere are many different kinds of backdoor attacks currently. BadNets is the representative of the patch-based visible backdoor attack [5]. The attacker poisons training images by pasting an arbitrary trigger on them and changing their labels to a target label. After training with poisoned images and clean images, the model behaves normally when the input is clean but outputs the target label when there is a trigger in the input in inference phase.\nDifferent from BadNets whose trigger is selected arbitrarily, Trojan Attack generates the trigger by changing values of pixels in a trigger mask to achieve maximum values of some chosen neurons [6]. Trojan Attack assumes the attackers have no access to original training data, so it reverses training data and combines them with the trigger to retrain the model. After retraining, the trojan model could misbehave when an input is poisoned by the same trigger.\nBoth of BadNets and Trojan Attack poison images with patch-based visible triggers, and there are also some works about other visible triggers and invisible triggers which are more stealthy [20], [21]. The Input-Aware Attack generates triggers varying from image to image, so different inputs are poisoned with different triggers for stealthiness [22]. Blended Injection is a backdoor attack blending the pixels of an input image and the trigger to generate a poisoned image [23], and it balances inconspicuousness and backdoor effect by a blending weight. One of the backdoor attack methods based on the invisible triggers is SIG [24]. SIG superimposes a backdoor signal such as horizontal sinusoidal signal to the images for poisoning, and it poisons images without modifying the labels to achievie great stealthiness. WaNet is another invisible backdoor attack [25]. WaNet uses warping-based trigger, and the difference is unnoticeable so that the poisoned image is almost the same as the original image for human eyes.\nHowever, these stealthy attacks, to a greater or lesser extent, are vulnerable in the physical world [26]. The triggers in the poisoned images of the inference phase cannot be guaranteed to be the same as those in the training phase since the former are more fragile when captured by camera in real time. For example, supposing there is a printed traffic sign with a trigger on the side of the road and aiming to mislead the backdoored driving system maliciously, the distance between the camera of the driving system and the traffic sign as well as the shooting angle could influence the picture, and the illumination can even destroy the stealthy triggers, so the stealthy trigger on the traffic sign is easy to lose effect.\nIn the physical world, patch-based triggers are more solid and more likely to bring risk, so patch-based triggers need more attention for a securer physical environment. In our experiments, we use two classic patch-trigger based attacks, BadNets and Trojan Attack, to evaluate the performance of our defense. And we also use the more stealthy non-patch based attacks for verification of our viewpoint about the relationship of the backdoor and the BN layers.\nBackdoor defense is a very significant research topic. We simply divide them into two kinds: one is repairing the model by adjusting the architecture or parameters, and the other is preprocessing the samples before inputting them to the backdoored model.\nFine-pruning is a model-repair based defense combining pruning and fine-tuning, which detects the backdoor neurons through the activations of neurons in the final convolutional layer and prunes them [8], and then fine-tunes the neurons through retraining with clean samples. The performance of classifying clean inputs is inevitably influenced after pruning neurons, because the final convolutional layer is important to the model usability, and in our view, the backdoor is hidden in the BN layers instead of the convolutional layers.\nNeural Cleanse is the state-of-the-art backdoor defense [9] based on model repair. It believes that the backdoored model could misclassify images to the target label by adding certain perturbations, and misclassifying them to other labels needs more perturbations. Thus researchers calculate the average perturbation for every label. Then they determine the outlier perturbation as a reversed trigger and the corresponding label as the target label. After that, they repair the DNN model by neuron pruning or unlearning. However, neuron pruning affects the model's performance for clean inputs, and fine-tuning of Neural Cleanse needs high computation cost, which are also the drawbacks of many model-repair based defenses including Fine-pruning.\nAs a sample-preprocessing based defense, STRIP applies linear blend strategy to an input image and a set of clean images, and then determines whether the input is clean or poisoned through the diversity of blended images' predictions [10]. Specifically, the predictions of images blended with clean input tend to be more diverse, while those corresponding to poisoned input are more likely to be the target label. However, when the trigger is broken in the blend process, the divergence cannot be obvious enough to distinguish poisoned inputs completely. Unlike blending pixels linearly, our CETF substitutes the pixels on clean images by pixels of the trigger to ensure triggers' intactness.\nFebruus initially utilizes GradCAM to generate the salient map, and removes influential regions from the input based on the salient map [11]. Subsequently, it employs GAN to repaint the removed regions. Februus performs indiscriminate removing and repainting on every input, regardless of whether the data is clean or poisoned. Therefore, it is hard for Februus to maintain the accuracy on clean samples, as GAN cannot always precisely restore the removed regions. Furthermore, the threshold of the salient map directly determines the size of the region to be removed, but the defender typically does not know the size of the trigger, resulting in a lack of generalizability for a certain threshold.\nNEO randomly searches for the backdoor trigger by blocking it to see whether it results in a transition of prediction [14]. For confirmation, the region is extracted and placed on clean images to check if the number of their prediction transitions exceeds a threshold. However, the influential regions of clean inputs could also exceed the threshold because the threshold is calculated in the same way for randomly extracted regions from clean images, which leads to misdiagnosis for clean inputs. What's more, NEO is ineffective and inefficient for its simple random search since triggers' positions and sizes are both unknown."}, {"title": "III. METHOD", "content": "Backdoor attacks usually appear when a client does not have enough computation resources to train a DNN model so outsources it to a third party. As shown in Fig. 1, if a malicious third party puts triggers in clean inputs to generate poisoned inputs and changes their labels to the target class in training phase, then the model could map the trigger as a feature to the target label. In the inference phase, when there is an input with the same trigger, the model will misclassify it to the target class. But when the input is clean, the backdoored model outputs prediction normally. Formally, for xi, Yi belong to a valid dataset, if xi is a clean input to the backdoored model fo (\u03b8 represents the parameters of the model), then fo(xi) = Yi, which is the true label, but fo(xip) = yt where xip is poisoned and yt is the target label.\nBackdoor attacks can happen in the physical world [27]. If an attacker prints the trigger patch and pastes it on an object, then the backdoored DNN system capturing the photo of the object as an input will output wrong prediction. Triggers in the physical world are usually more inconspicuous, for example, a little patch on a traffic sign that does not attract attention on the road. Besides, since the photo is captured in different illumination, angles and distances, defense against backdoor attacks in the physical world should be robust to triggers' sizes and positions. In this paper, we experiment with triggers with random sizes and positions to show the universal application of our method.\nThe pipeline of our method is shown in Fig. 2. Our backdoor defense includes CAM-focus evolutionary trigger filter and model repair. CETF first narrows down the searching range for the trigger in the image by GradCAM, which can generate a salient map to present the importance of every region in the image [12]. Then the prior region got from the salient map becomes the main search range by the evolutionary algorithm. After detecting and validating the trigger by the evolutionary optimization approach, unlearning methods are used with the extracted triggers to repair the backdoored model. The details of each phase are as follows.\nIn the CAM-focus phase, we apply the GradCAM, which is an algorithm utilizing gradients of predictions combined with interpolation to generate a salient map [12], to present the important regions in the images. For a clean input image, the region containing important classification features will have larger weights than other regions in the salient map. As for a poisoned input, the region containing the trigger is expected to have the largest weights because the trigger is the key part leading to a controlled prediction. Therefore, important regions could be extracted by a preset salient map threshold. Obviously, the threshold has a significant impact on the size of the extracted influential part, and if the threshold is inappropriate, the extracted part is easy to be either too small or too big, which means being an incomplete trigger or containing a large clean part of the image, respectively. Also, GradCAM is not completely reliable because it does not always capture the entire influential object with a fixed threshold [28]. As a result, depending only on GradCAM to extract the entire trigger accurately does not always work well, which is also the limitation of Februus.\nTherefore, in our method, GradCAM is utilized just to save time by roughly finding the influential region that will be used for the accurate trigger search in the next step, which means CETF can achieve the defense without dependence on GradCAM. In our experiments, we choose influential region with weights larger than 0.7 in the salient map and dilate its minimum bounding rectangle as the final output of CAM-focus prior. The threshold 0.7 need not be accurate since only a rough region is needed. We choose 0.7 because it represents a relatively large significance between 0 and 1; it is not affected by the scenarios so that the applicability is guaranteed. Usually, this prior region contains the most influential part, which means classification features for the clean input or the trigger for the poisoned input. In the next step, the trigger can be precisely located inside it.\nWe utilize Differential Evolution (DE) [29], [30] as the framework of our evolutionary optimization approach. DE is an optimization algorithm based on the difference of the population individuals and shows stunning performance in many practical applications [31], [32]. In CETF, customized DE is used to search for the minimum influential part of input image within the prior region got from CAM-focus. Specifically, based on DE, the minimum influential part can be obtained by initialization, mutation, crossover and selection. The implementation details are shown as follows.\n1) Initialization: We randomly initialize the population P with 40 individuals, i.e, P = {ui}40i=1, where ui \u2208 R4. Specifically, ui is used to determine a rectangular region, and the four entries of each ui represent the pixel position of the upper left corner of the searched region, as well as the height and width, respectively.\n2) Mutation: The mutated individual vi of ui is generated by a differential process:\n\\(v_{i}(t) = u_{i}(t) + \u03b1(u_{i_{1}}(t) - u_{i_{2}}(t))\\)\nwhere t represents current generation number, ui1, ui2 are individuals chosen randomly from the population and their difference is used to represent the difference of the current population individuals, and \u03b1 is a parameter used to control the magnification degree of DE (we set \u03b1 = 0.3 here).\n3) Crossover: After mutation, offspring \\({\\hat{u}_{i}}(t)\\) is generated by the mutated individual vi(t) and its parent individual ui(t) as follows:\n\\({\\hat{u}_{i}^{j}}(t) = \\begin{cases} v_{i}^{j}(t), & \u03b3< p \\ u_{i}^{j}(t), & \u03b3 > p \\end{cases}\\)\nwhere (.)j represents the jth entry of vector (.), \u03b3 is sampled from the uniform distribution U(0,1), and the crossover coefficient p is set to 0.5, which means parent individual and mutated individual are expected to have the same contribution to the offspring.\n4) Selection: In the selection phase, we select individuals for new generation population P+ by evaluating fitness values for each parent-offspring pair from current generation population and offspring generation, and the one which has the larger fitness function value fitness(u) will be kept.\n\\(P^{+} = U\\{u | u = arg \\max_{u \\in {\\hat{u_{i}}, U_{i}}}fitness(u)\\}\\)\n5) Objective function: We prefer the searched region to have backdoor effect and be as small as possible. To achieve these objectives simultaneously, we design the fitness function as follows:\n\\(fitness(u) = 1000 \u00d7 flag(u) + 1000 \u00d7 flips(u) - 1000 \u00d7  \\frac{s(u)^{2}}{S}\\)\nwhere flag(u) is a binary value, and it returns 1 only if we substitute the region (determined by u) with average color of the input image and feed it into the model, causing the model's predicting label to change. Otherwise flag(u) returns 0. And flips(u) represents the possibility of prediction transition when pasting the region on other clean images. It is estimated by pasting it on clean images of an auxiliary set, and putting them to the model to get the predictions; flips(u) is the ratio of the same predictions as the original input image. For example, if there are h clean images (we set h = 10 in our experiments) in total in the auxiliary set and q samples of them change predictions after pasted the current region u, then flips(u) is q/h. And s(u) is the area of the searched region; S is the total area of the input image.\nNotably, flag(.) is designed to make sure the optimized region is important for prediction, and flips(.) is designed to ensure the region can change the predictions of auxiliary images. Both flag(.) and flips(.) correspond to the backdoor effect property of the trigger. As for the area s(.), we control it to not only search for a block containing the trigger, but also avoid getting a very big block containing the whole influential part of a clean image.  \\frac{s(u)^{2}}{S} is designed to ensure s(.) is controlled by considering the input image's area S. The hyper-parameter 1000 in (4) is used to make the three terms in suitable order of magnitude to balance their influences.\nWe set the maximum number of evolutionary generations as 100 and Fig. 5 shows that 100 generations are sufficient for convergence. When the best value of the fitness function has not improved for 10 consecutive generations, the trigger search ends in advance. After 100 generations at most, for a poisoned input, we get the trigger, while for a clean input, we get a clean image feature, or nothing because the search ends with the value of the fitness function less than 0 which means DE cannot find an influential part from clean images. When DE cannot find an optimized region, we decide the current input as clean directly, so the image need not go to the next step, which saves time and computation resource smartly.\nAs for other hyper-parameters in DE, detailed explanations and experiments can be found in the Parameter Analysis part of the experiments section.\nDE process for trigger search is shown in details in Algorithm 1 and Algorithm 2. Algorithm 1 shows the whole process including initialization, mutation, crossover and selection. The RandInitial function in line 3 of Algorithm 1 represents initializing population with 40 individuals randomly, while RandomSelect(P) in line 9 represents selecting 2 individuals randomly from the current population for mutation in line 10. And Uniform (0, 1) in line 12 represents uniform distribution. The crossover procedure is shown from line 11 to line 15, and the selection is shown in line 27. The DE process ends if the best fitness value converges as shown in line 20 to 24.\nThe design of the fitness function is one of the keys of the trigger search by DE. Algorithm 2 shows the details of the calculation for fitness function clearly. The Mask(x, u) function in line 2 of Algorithm 2 means masking x on the position u, and Paste(xc, u) in line 5 is a function that pasting the suspected trigger patch on the position u of xc.\nTo determine whether the influential part is a trigger or feature of a clean input, we place it on a set of clean images on the same position as its original location on the input image as shown in Fig. 2. We find that a trigger is much stronger and more independent than clean image feature, which means a trigger can lead to the target label even though the trigger is incomplete or pasted on an unrelated image. As a result, we record and check the predictions of images pasted by the optimized region, and if the majority of predictions changes to the prediction of the original input image, the optimized part is a trigger and the input is poisoned, otherwise clean."}, {"title": "IV. EXPERIMENTS", "content": "We use two metrics", "follows": "n\\(Accu = \\frac{1"}, {"settings": "face recognition and traffic sign classification. For face recognition", "38": "as the dataset", "39": [40], "5": "and Trojan Attack [6", "33": "and FaceScrub to train MobileNet (a face classifier taking MobileNet as backbone which is widely used for face classification) [41", "3": "and then fine-tuned the last full connected layer of VGG16 model on clean images from FaceScrub. VGG16 was introduced in 2014 [42", "6": ".", "algorithms": "Neural Cleanse", "attack": "Input-Aware Attack [22", "23": "WaNet [25", "24": "."}]}