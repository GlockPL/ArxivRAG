{"title": "Evolutionary Trigger Detection and Lightweight Model Repair Based Backdoor Defense", "authors": ["Qi Zhou", "Zipeng Ye", "Yubo Tang", "Wenjian Luo", "Yuhui Shi", "Yan Jia"], "abstract": "Deep Neural Networks (DNNs) have been widely used in many areas such as autonomous driving and face recognition. However, DNN model is fragile to backdoor attack. A backdoor in the DNN model can be activated by a poisoned input with trigger and leads to wrong prediction, which causes serious security issues in applications. It is challenging for current defenses to eliminate the backdoor effectively with limited computing resources, especially when the sizes and numbers of the triggers are variable as in the physical world. We propose an efficient backdoor defense based on evolutionary trigger detection and lightweight model repair. In the first phase of our method, CAM-focus Evolutionary Trigger Filter (CETF) is proposed for trigger detection. CETF is an effective sample-preprocessing based method with the evolutionary algorithm, and our experimental results show that CETF not only distinguishes the images with triggers accurately from the clean images, but also can be widely used in practice for its simplicity and stability in different backdoor attack situations. In the second phase of our method, we leverage several lightweight unlearning methods with the trigger detected by CETF for model repair, which also constructively demonstrate the underlying correlation of the backdoor with Batch Normalization layers. Source code will be published after accepted.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Neural Networks (DNNs) have been increasingly used in various areas such as traffic sign recognition in autonomous vehicles [1], medical image analysis in healthcare [2], face recognition in security systems [3] and risk assessment in finance [4].\nHowever, DNNs are susceptible to malicious attacks. Backdoor attacks are a particularly insidious and powerful type of attacks that can cause DNNs to exhibit normal behaviors for clean samples while displaying abnormal behaviors when the samples are poisoned with triggers [5], [6]. Poisoned samples with triggers are always classified to the target label by a backdoored DNN model as shown in Fig. 1. Backdoor attacks cause wide applications of DNNs in risks. For example, a backdoored model in an autonomous vehicle would misclassify the traffic signs with triggers, which can cause a serious security accident. Despite the significant impacts, the backdoor is difficult to detect since abnormality only emerges when there are triggers in inputs and defenders usually have no prior knowledge of the triggers.\nObviously, defense against the backdoor attack is of great significance. Excellent defense methods are expected to eliminate the backdoor effect while maintaining the performance of predicting clean inputs. Existing defenses employ different ways to defend against backdoor attack. For example, many of them aim to eliminate the backdoors by fine-tuning or pruning the models [7], [8]. The classic Neural Cleanse technique modifies the model using trigger reversing and neuron pruning technologies, which are effective against backdoor attack but compromise classification accuracy [9]. There are also defenses based on sample-preprocessing that do not need to modify the structure of the parameters of the models. STRIP blends clean images linearly with the input, then determines whether the input is poisoned by observing the divergence of the predictions on blended images [10]. However, it is difficult to set a suitable divergence threshold which can always separate clean and poisoned inputs accurately. Moreover, Februus [11] neutralizes the backdoor effect by removing the most influential part depending on GradCAM [12] and restoring it by Generative Adversarial Network (GAN) [13]. This defense processes all the inputs, whether they are poisoned or clean, so it inevitably affects the model performance on clean inputs since GAN cannot always repaint the removed part accurately. And NEO locates the position of the trigger by repeatedly randomly cropping a part from the input and pasting it to other clean images to see if the number of changed predictions exceeds a threshold [14]. However, such a random search based method is not precise and effective, and the unsuitable threshold also influences the model usability.\nBesides, another limitation of the existing methods is that they cannot handle triggers with different sizes and numbers, while these kinds of triggers are common in physical world. Clearly, it is challenging to mitigate the backdoor effect while maintaining the model performance.\nTo achieve these objectives simultaneously, we propose an efficient backdoor defense based on evolutionary trigger detection and lightweight model repair. The trigger detection is achieved by a sample-preprocessing based method, CAM-focus Evolutionary Trigger Filter (CETF). In the inference phase of the model, CETF checks every image input of the model and filters the poisoned images with triggers. And the triggers extracted by CETF are then used to repair the model by our unlearning methods.\nSpecifically, in CETF, we first detect influential regions roughly by GradCAM [12] for the input. Then, we design the objective function rationally and solve it based on an evolutionary algorithm [15], so as to accurately locate the most influential part of the input image with a size as small as possible. After that, we paste the optimized region from the evolutionary algorithm on a set of clean images and check whether their predictions change, thus determining whether there is a trigger in the input. CETF is able to satisfy both the robustness and the usability requirements, and even robust with the triggers variable in sizes and numbers.\nAfter CETF, we unlearn the backdoored model by images with the extracted trigger for model repair. The na\u00efve unlearning method is based on fine-tuning the model with the images pasted the reversed triggers [9]. Benefiting from the extracted triggers of CETF which are so accurate that approximate the triggers in the training phase, our model repair by unlearning is lightweight and practical. Also, the unlearning is completed in a very short time.\nFurthermore, we find the existence of backdoor in the Batch Normalization (BN) layers. Therefore, based on our findings, we propose two more efficient methods, BN-unlearning and BN-cleaning, for model repair. Specifically, by modifying only the parameters of the BN layers, our proposed model-repair methods get excellent effect in a large number of experiments, which further demonstrates the validity of our findings. Our viewpoint that the backdoor is injected by attackers to the model by setting the statistical information in the BN layers when training with triggers has great significance to the interpretability of backdoor.\nOur contributions can be summarized as follows.\n\u2022\tWe firstly combine the evolutionary algorithm with backdoor defense. CETF is an effective way to search for the trigger in poisoned inputs accurately. Experimental results demonstrate the high robustness of CETF against the content, size and other attributes of triggers, thereby proving its universality and applicability in practice.\n\u2022\tThe defense pipeline based on extracting triggers of CETF followed by repairing the model by unlearning shows a superior effect. We also propose two lightweight model repair methods for efficiency. Substantial experimental results show that our defense mitigates the backdoor effectively and efficiently without influencing the inference performance of the model.\n\u2022\tFurthermore, we explore the \"shortcut\" for trigger in the backdoored model. We propose that the backdoor is hidden in the Batch Normalization layers and a lot of experiments are conducted for verification. Based on the excellent effect of our backdoor defense, we think that our findings are inspiring to the future research about AI backdoor and model's interpretability."}, {"title": "II. RELATED WORK", "content": "Backdoor injected in the model by the attackers can be activated by the trigger, which leads the model to output the target class. It is widely believed that backdoor attack provides a shortcut for trigger in the model [16] [17] [18]. Therefore, an image poisoned with a trigger goes through the shortcut whatever the original content of the image is, thereby being classified as the target class.\nHowever, there has not been a consensus on the mechanism of backdoor in the model structure. Yang et al. have analyzed the mechanism of backdoor [19]. They think the skip connections influence backdoor and they try to suppress the skip connections for defense. However, suppression has obviously negative effects on model's inference performance. Therefore, their experiments only demonstrate the importance of the skip connections for the model's usability, but cannot demonstrate directly that the backdoor is highly correlated to the skip connections.\nIn this paper, we propose a viewpoint that the backdoor is hidden in the BN layers and demonstrate its validity by application with various backdoor attacks.\nThere are many different kinds of backdoor attacks currently. BadNets is the representative of the patch-based visible backdoor attack [5]. The attacker poisons training images by pasting an arbitrary trigger on them and changing their labels to a target label. After training with poisoned images and clean images, the model behaves normally when the input is clean but outputs the target label when there is a trigger in the input in inference phase.\nDifferent from BadNets whose trigger is selected arbitrarily, Trojan Attack generates the trigger by changing values of pixels in a trigger mask to achieve maximum values of some chosen neurons [6]. Trojan Attack assumes the attackers have no access to original training data, so it reverses training data and combines them with the trigger to retrain the model. After retraining, the trojan model could misbehave when an input is poisoned by the same trigger.\nBoth of BadNets and Trojan Attack poison images with patch-based visible triggers, and there are also some works about other visible triggers and invisible triggers which are more stealthy [20], [21]. The Input-Aware Attack generates triggers varying from image to image, so different inputs are poisoned with different triggers for stealthiness [22]. Blended Injection is a backdoor attack blending the pixels of an input image and the trigger to generate a poisoned image [23], and it balances inconspicuousness and backdoor effect by a blending weight. One of the backdoor attack methods based on the invisible triggers is SIG [24]. SIG superimposes a backdoor signal such as horizontal sinusoidal signal to the images for poisoning, and it poisons images without modifying the labels to achievie great stealthiness. WaNet is another invisible backdoor attack [25]. WaNet uses warping-based trigger, and the difference is unnoticeable so that the poisoned image is almost the same as the original image for human eyes.\nHowever, these stealthy attacks, to a greater or lesser extent, are vulnerable in the physical world [26]. The triggers in the poisoned images of the inference phase cannot be guaranteed to be the same as those in the training phase since the former are more fragile when captured by camera in real time. For example, supposing there is a printed traffic sign with a trigger on the side of the road and aiming to mislead the backdoored driving system maliciously, the distance between the camera of the driving system and the traffic sign as well as the shooting angle could influence the picture, and the illumination can even destroy the stealthy triggers, so the stealthy trigger on the traffic sign is easy to lose effect.\nIn the physical world, patch-based triggers are more solid and more likely to bring risk, so patch-based triggers need more attention for a securer physical environment. In our experiments, we use two classic patch-trigger based attacks, BadNets and Trojan Attack, to evaluate the performance of our defense. And we also use the more stealthy non-patch based attacks for verification of our viewpoint about the relationship of the backdoor and the BN layers.\nBackdoor defense is a very significant research topic. We simply divide them into two kinds: one is repairing the model by adjusting the architecture or parameters, and the other is preprocessing the samples before inputting them to the backdoored model.\nFine-pruning is a model-repair based defense combining pruning and fine-tuning, which detects the backdoor neurons through the activations of neurons in the final convolutional layer and prunes them [8], and then fine-tunes the neurons through retraining with clean samples. The performance of classifying clean inputs is inevitably influenced after pruning neurons, because the final convolutional layer is important to the model usability, and in our view, the backdoor is hidden in the BN layers instead of the convolutional layers.\nNeural Cleanse is the state-of-the-art backdoor defense [9] based on model repair. It believes that the backdoored model could misclassify images to the target label by adding certain perturbations, and misclassifying them to other labels needs more perturbations. Thus researchers calculate the average perturbation for every label. Then they determine the outlier perturbation as a reversed trigger and the corresponding label as the target label. After that, they repair the DNN model by neuron pruning or unlearning. However, neuron pruning affects the model's performance for clean inputs, and fine-tuning of Neural Cleanse needs high computation cost, which are also the drawbacks of many model-repair based defenses including Fine-pruning.\nAs a sample-preprocessing based defense, STRIP applies linear blend strategy to an input image and a set of clean images, and then determines whether the input is clean or poisoned through the diversity of blended images' predictions [10]. Specifically, the predictions of images blended with clean input tend to be more diverse, while those corresponding to poisoned input are more likely to be the target label. However, when the trigger is broken in the blend process, the divergence cannot be obvious enough to distinguish poisoned inputs completely. Unlike blending pixels linearly, our CETF substitutes the pixels on clean images by pixels of the trigger to ensure triggers' intactness.\nFebruus initially utilizes GradCAM to generate the salient map, and removes influential regions from the input based on the salient map [11]. Subsequently, it employs GAN to repaint the removed regions. Februus performs indiscriminate removing and repainting on every input, regardless of whether the data is clean or poisoned. Therefore, it is hard for Februus to maintain the accuracy on clean samples, as GAN cannot always precisely restore the removed regions. Furthermore, the threshold of the salient map directly determines the size of the region to be removed, but the defender typically does not know the size of the trigger, resulting in a lack of generalizability for a certain threshold.\nNEO randomly searches for the backdoor trigger by blocking it to see whether it results in a transition of prediction [14]. For confirmation, the region is extracted and placed on clean images to check if the number of their prediction transitions exceeds a threshold. However, the influential regions of clean inputs could also exceed the threshold because the threshold is calculated in the same way for randomly extracted regions from clean images, which leads to misdiagnosis for clean inputs. What's more, NEO is ineffective and inefficient for its simple random search since triggers' positions and sizes are both unknown."}, {"title": "III. METHOD", "content": "Backdoor attacks usually appear when a client does not have enough computation resources to train a DNN model so outsources it to a third party. As shown in Fig. 1, if a malicious third party puts triggers in clean inputs to generate poisoned inputs and changes their labels to the target class in training phase, then the model could map the trigger as a feature to the target label. In the inference phase, when there is an input with the same trigger, the model will misclassify it to the target class. But when the input is clean, the backdoored model outputs prediction normally. Formally, for \\(x_i, Y_i\\) belong to a valid dataset, if \\(x_i\\) is a clean input to the backdoored model \\(f_\\theta\\) (\\(\\theta\\) represents the parameters of the model), then \\(f_\\theta(x_i) = Y_i\\), which is the true label, but \\(f_\\theta(x_{ip}) = y_t\\) where \\(x_{ip}\\) is poisoned and \\(y_t\\) is the target label.\nBackdoor attacks can happen in the physical world [27]. If an attacker prints the trigger patch and pastes it on an object, then the backdoored DNN system capturing the photo of the object as an input will output wrong prediction. Triggers in the physical world are usually more inconspicuous, for example, a little patch on a traffic sign that does not attract attention on the road. Besides, since the photo is captured in different illumination, angles and distances, defense against backdoor attacks in the physical world should be robust to triggers' sizes and positions. In this paper, we experiment with triggers with random sizes and positions to show the universal application of our method.\nThe pipeline of our method is shown in Fig. 2. Our backdoor defense includes CAM-focus evolutionary trigger filter and model repair. CETF first narrows down the searching range for the trigger in the image by GradCAM, which can generate a salient map to present the importance of every region in the image [12]. Then the prior region got from the salient map becomes the main search range by the evolutionary algorithm. After detecting and validating the trigger by the evolutionary optimization approach, unlearning methods are used with the extracted triggers to repair the backdoored model. The details of each phase are as follows."}, {"title": "C. CAM-Focus", "content": "In the CAM-focus phase, we apply the GradCAM, which is an algorithm utilizing gradients of predictions combined with interpolation to generate a salient map [12], to present the important regions in the images. For a clean input image, the region containing important classification features will have larger weights than other regions in the salient map. As for a poisoned input, the region containing the trigger is expected to have the largest weights because the trigger is the key part leading to a controlled prediction. Therefore, important regions could be extracted by a preset salient map threshold. Obviously, the threshold has a significant impact on the size of the extracted influential part, and if the threshold is inappropriate, the extracted part is easy to be either too small or too big, which means being an incomplete trigger or containing a large clean part of the image, respectively. Also, GradCAM is not completely reliable because it does not always capture the entire influential object with a fixed threshold [28]. As a result, depending only on GradCAM to extract the entire trigger accurately does not always work well, which is also the limitation of Februus.\nTherefore, in our method, GradCAM is utilized just to save time by roughly finding the influential region that will be used for the accurate trigger search in the next step, which means CETF can achieve the defense without dependence on GradCAM. In our experiments, we choose influential region with weights larger than 0.7 in the salient map and dilate its minimum bounding rectangle as the final output of CAM-focus prior. The threshold 0.7 need not be accurate since only a rough region is needed. We choose 0.7 because it represents a relatively large significance between 0 and 1; it is not affected by the scenarios so that the applicability is guaranteed. Usually, this prior region contains the most influential part, which means classification features for the clean input or the trigger for the poisoned input. In the next step, the trigger can be precisely located inside it."}, {"title": "D. Evolutionary Trigger Detecting", "content": "We utilize Differential Evolution (DE) [29], [30] as the framework of our evolutionary optimization approach. DE is an optimization algorithm based on the difference of the population individuals and shows stunning performance in many practical applications [31], [32]. In CETF, customized DE is used to search for the minimum influential part of input image within the prior region got from CAM-focus. Specifically, based on DE, the minimum influential part can be obtained by initialization, mutation, crossover and selection. The implementation details are shown as follows.\n1) Initialization: We randomly initialize the population P with 40 individuals, i.e, \\(P = {u_i\\}_{i=1}^{40}\\), where \\(u_i \\in R^4\\). Specifically, \\(u_i\\) is used to determine a rectangular region, and the four entries of each \\(u_i\\) represent the pixel position of the upper left corner of the searched region, as well as the height and width, respectively.\n2) Mutation: The mutated individual \\(v_i\\) of \\(u_i\\) is generated by a differential process:\n\\[v_i(t) = u_i(t) + \\alpha(u_{i_1}(t) - u_{i_2}(t))  \\tag{1}\\]\nwhere \\(t\\) represents current generation number, \\(u_{i_1}, u_{i_2}\\) are individuals chosen randomly from the population and their difference is used to represent the difference of the current population individuals, and \\(\\alpha\\) is a parameter used to control the magnification degree of DE (we set \\(\\alpha = 0.3\\) here).\n3) Crossover: After mutation, offspring \\(\\hat{u}_i(t)\\) is generated by the mutated individual \\(v_i(t)\\) and its parent individual \\(u_i(t)\\) as follows:\n\\[\\hat{u}_i^j(t) = \\begin{cases}\nv_i^j(t), & \\gamma < p \\\\\nu_i^j(t), & \\gamma > p\n\\end{cases}   \\tag{2}\\]\nwhere \\(\\nu^j\\) represents the \\(j^{th}\\) entry of vector \\(\\nu\\), \\(\\gamma\\) is sampled from the uniform distribution \\(U(0,1)\\), and the crossover coefficient \\(p\\) is set to 0.5, which means parent individual and mutated individual are expected to have the same contribution to the offspring.\n4) Selection: In the selection phase, we select individuals for new generation population \\(P^+\\) by evaluating fitness values for each parent-offspring pair from current generation population and offspring generation, and the one which has the larger fitness function value fitness(u) will be kept.\n\\[P^+ = \\cup_{i} \\{ u \\vert u = arg \\underset{u\\in \\{\\hat{u}_i, u_i\\}}{max} \\text{fitness}(u) \\}  \\tag{3}\\]\n5) Objective function: We prefer the searched region to have backdoor effect and be as small as possible. To achieve these objectives simultaneously, we design the fitness function as follows:\n\\[\\text{fitness}(u) = 1000 \\times \\text{flag}(u) + 1000 \\times \\text{flips}(u) - \t  1000 \\times \\frac{s(u)^2}{S}   \\tag{4}\\]\nwhere flag(u) is a binary value, and it returns 1 only if we substitute the region (determined by u) with average color of the input image and feed it into the model, causing the model's predicting label to change. Otherwise flag(u) returns 0. And flips(u) represents the possibility of prediction transition when pasting the region on other clean images. It is estimated by pasting it on clean images of an auxiliary set, and putting them to the model to get the predictions; flips(u) is the ratio of the same predictions as the original input image. For example, if there are h clean images (we set h = 10 in our experiments) in total in the auxiliary set and q samples of them change predictions after pasted the current region u, then flips(u) is q/h. And s(u) is the area of the searched region; S is the total area of the input image.\nNotably, flag(\u00b7) is designed to make sure the optimized region is important for prediction, and flips(\u00b7) is designed to ensure the region can change the predictions of auxiliary images. Both flag(\u00b7) and flips(\u00b7) correspond to the backdoor effect property of the trigger. As for the area s(\u00b7), we control it to not only search for a block containing the trigger, but also avoid getting a very big block containing the whole influential part of a clean image. \\(\\frac{s(u)^2}{S}\\) is designed to ensure s(\u00b7) is controlled by considering the input image's area S. The hyper-parameter 1000 in (4) is used to make the three terms in suitable order of magnitude to balance their influences.\nWe set the maximum number of evolutionary generations as 100 and Fig. 5 shows that 100 generations are sufficient for convergence. When the best value of the fitness function has not improved for 10 consecutive generations, the trigger search ends in advance. After 100 generations at most, for a poisoned input, we get the trigger, while for a clean input, we get a clean image feature, or nothing because the search ends with the value of the fitness function less than 0 which means DE cannot find an influential part from clean images. When DE cannot find an optimized region, we decide the current input as clean directly, so the image need not go to the next step, which saves time and computation resource smartly.\nAs for other hyper-parameters in DE, detailed explanations and experiments can be found in the Parameter Analysis part of the experiments section."}, {"title": "E. Pasting and Filtering", "content": "To determine whether the influential part is a trigger or feature of a clean input, we place it on a set of clean images on the same position as its original location on the input image as shown in Fig. 2. We find that a trigger is much stronger and more independent than clean image feature, which means a trigger can lead to the target label even though the trigger is incomplete or pasted on an unrelated image. As a result, we record and check the predictions of images pasted by the optimized region, and if the majority of predictions changes to the prediction of the original input image, the optimized part is a trigger and the input is poisoned, otherwise clean."}, {"title": "F. Model Repair", "content": "1) Na\u00efve Unlearning: A common model repair method based on the trigger is unlearning the backdoor by fine-tuning. The fine-tuning process is similar to the model training process. Neural Cleanse unlearns the backdoor by fine-tuning the infected model with images patched by the reversed triggers, and the labels of these images for unlearning are set to the true labels instead of the target label. After unlearning, the model restores the inference performance and could output correct labels for input images even when the real triggers are present in the inference phase. It can be explained that when fine-tuning with the images from different classes with triggers and their true labels, the model will not associate the trigger as a feature to the target class, so it achieves backdoor mitigation.\nIn our method, we use the trigger extracted by CETF, instead of the reversed trigger in Neural Cleanse, to repair the backdoored model. The extracted trigger is also pasted to a set of clean images with true labels, and then these images are used to fine-tune the model to unlearn the backdoor. Since the CETF is expected to provide the optimized region which is more similar to the original trigger than the reversed trigger in Neural Cleanse, our unlearning phase is easy to get superior model repair effect with less computation and data resources than Neural Cleanse.\n2) BN-Unlearning and BN-Cleaning: We also propose two new unlearning methods based on modifying only the statistical information and parameters of the BN layers.\nIn almost all the state of the art computer vision tasks based on Convolutional Neural Network [33]\u2013[36], the BN layer is almost an essential component since it can accelerate the convergence of networks and improve training speed and stability. Batch Normalization can normalize the input along each feature dimension within a batch during training, and BN layer is typically placed before the activation function to ensure that the input data of each layer remains within a stable distribution [37].\nIn a BN layer, batch mean estimates the mean of each feature and batch variance estimates the variance of each feature in the current data batch, and they are used to normalize the input for each feature dimension. Then the normalized features are scaled and shifted with the scaling parameter \\(\\gamma\\) and offset parameter \\(\\beta\\), thereby enhancing the model's expressive power and training effectiveness. In the training phase, both of running mean \\(\\mu\\) and running variance \\(\\sigma^2\\) accumulate updates based on the statistics of the batch data, while the scaling parameter \\(\\gamma\\) and offset parameter \\(\\beta\\) are updated by back propagation typically with gradient descent. In the model inference phase, the input is not always a batch so it is necessary to utilize the accumulated running mean \\(\\mu\\) and running variance \\(\\sigma^2\\) updated during the training process to maintain consistent data distribution and normalization effects.\nThere are researchers reversing clean images dependent on the BN layers in the backdoored model since they assert that only a small portion of the training data are poisoned so the information of the BN layers is not influenced [17]. However, there is not any experiment to prove this viewpoint in their paper. Besides, since there is large accuracy degradation in their experiment due to the difference between the reversed data and the clean data, we question their viewpoint that the information of the BN layers is not influenced in backdoor training. On the contrary, in this paper, we do adequate experiments to demonstrate that the backdoor is closely related to the BN layers. In the experiments, the specific new unlearning methods are as follows.\na) BN-Unlearning: Given our belief that backdoor is hidden in the BN-layers, we propose to unlearn the backdoor by updating the parameters of the BN layers only, with parameters of other layers frozen. We believe that the parameters of the BN layers are shaped by the statistical information of the training data, which typically consists of a small proportion of poisoned images and a large proportion of clean images. Specifically, in the training phase, the mixture of the poisoned images and the clean images influences some specific parameters of the BN layers. After training, when inputting a poisoned image, the specific activations are greater than 0 after Batch Normalization and activate the model to output the target class, which is exactly the shortcut. However, when inputting a clean image, the specific activations in the BN layers is less than 0, which cannot activate the model to output the target class.\nWe get the poisoned images by inserting the extracted trigger from CETF to the local validation images, and then use these images with their true labels for unlearning by fine-tuning the model. After fine-tuning, the statistical information in the BN layers is obviously reset. Once the inputs with the trigger cannot get a large activation with the new parameters in the BN layers, the backdoor is unlearned successfully.\nb) BN-Cleaning: As the relative magnitudes of the activations in BN layers are mainly controlled by the running mean \\(\\mu\\) and running variance \\(\\sigma^2\\), we propose another unlearning method, BN-cleaning, which only modifies \\(\\mu\\) and \\(\\sigma^2\\), and the experiment of BN-cleaning can convincingly confirm the strong relationship between the backdoor and statistical information in the BN layers.\nDifferent from the scaling parameter \\(\\gamma\\) and offset parameter \\(\\beta\\) that need to be updated by the gradient back propagation process, the running mean \\(\\mu\\) and the running variance \\(\\sigma^2\\) can be updated only by the images during the forward propagation process. Therefore, in BN-cleaning, we only need to input the images pasted by the extracted trigger, and then the statistical information \\(\\mu\\) and \\(\\sigma^2\\) are updated automatically. The running mean \\(\\mu\\) and running variance \\(\\sigma^2\\) are updated as follows:\n\\[\\mu_{new} = (1 - \\lambda) \\cdot \\mu_{old} + \\lambda \\cdot \\mu_{batch}  \\tag{5}\\]\n\\[\\sigma^2_{new} = (1 - \\lambda) \\cdot \\sigma^2_{old} + \\lambda \\cdot \\sigma^2_{batch}   \\tag{6}\\]\nwhere \\(\\lambda\\) balances the old and new values; \\(\\mu_{new}\\) and \\(\\sigma^2_{new}\\) are the updated running mean and updated running variance, respectively; \\(\\mu_{old}\\) and \\(\\sigma^2_{old}\\) are the previous running mean and previous running variance, respectively; \\(\\mu_{batch}\\) and \\(\\sigma^2_{batch}\\) are the sample mean and sample variance of the current batch, respectively.\nIt is obvious that the update of \\(\\mu\\) and \\(\\sigma^2\\) is independent of the loss function, so there is even no need to know the true labels of the images, which means BN-cleaning is applicable even we only have images without labels. Since we freeze all the parameters of other layers except for the BN layers and do not need back propagation, the computation cost is far lower than the na\u00efve unlearning."}, {"title": "IV. EXPERIMENTS", "content": "1) Metrics: We use two metrics, clean image accuracy and the attack success rate, to evaluate the maintenance of classification performance and the mitigation of the backdoor effect after defense, respectively. Model's classification accuracy Accu. and the attack success rate ASR are calculated as follows:\n\\[\\text{Accu} = \\frac{1}{n} \\sum_{i=1}^n  I(f_\\theta(x_i) = Y_i)  \\tag{7}\\]\n\\[\\text{ASR} = \\frac{1}{m} \\sum_{j=1}^m  I(f_\\theta(x_{pj}) = Y_t)  \\tag{8}\\]\nwhere \\(f_\\theta(x)\\) is the model's prediction for image x, n is the sum of clean images, m is the sum of poisoned images with triggers, \\(y_i\\) is the groundtruth label of the ith image and \\(y_t\\) is the target label of the backdoor attack. And I() is the indicator function. Defense algorithms and backdoor repair methods are expected to keep Accu. high and reduce ASR as low as possible at the same time.\n2) Models and Datasets: To evaluate the defense, we consider two widely used settings: face recognition and traffic sign classification. For face recognition, we choose FaceScrub [38] as the dataset, while for traffic sign classification, we choose GTSRB [39], [40] as the dataset.\nWe experiment with patch-based trigger attacks and non-patch based trigger attacks respectively, and for non-patch based attacks, we introduce the settings in the Additional Experiments for Verification part. For patch-based attack, we use two benchmark backdoor attacks, BadNets [5] and Trojan Attack [6]. For BadNets, we use GTSRB to train ResNet20 [33] and FaceScrub to train MobileNet (a face classifier taking MobileNet as backbone which is widely used for face classification) [41]. For Trojan Attack, we use VGG16, which is pretrained on 2,622,000 images with faces from 2,622 persons [3], and then fine-tuned the last full connected layer of VGG16 model on clean images from FaceScrub. VGG16 was introduced in 2014 [42] and was earlier than the work of Batch Normalization, so the original VGG16 model does not incorporate BN layers. Although VGG16 is not widely used in practice now and there are modified versions VGG16-BN, we still decide to use VGG16 to achieve the attack performance similar to the original work proposing Trojan Attack [6].\nOur used GTSRB is a dataset consisting of 40,736 traffic signs of 43 kinds, and images are resized to 32 \u00d7 32. Our used FaceScrub is a face dataset with 45,333 images of 526 persons. For BadNets, we resize the images in FaceScrub to 160\u00d7160, while for Trojan Attack, we resize the face images to 224 \u00d7 224. For each attack, we randomly choose 10% of the training data to poison with the trigger and mix the poisoned images with the other 90% images to train the models.\n3) Triggers and Defenses: For Trojan Attack, the triggers are generated dependent on the models. For BadNets, the trigger of GTSRB is set as a yellow block with size 2 \u00d7 2 put on the bottom right corner fixedly, while the trigger of FaceScrub is a circular British flag logo with radius 20. Instead of putting the British flag triggers on the fixed positions, we put the triggers randomly so different images could have triggers on different positions, which can further demonstrate the robustness of our defense. The poisoned samples with triggers are shown in the Table I.\nThe accuracy for classifying clean images (Accu.) and the attack success rate (ASR) of the attacked models are shown in Table I. From Table I, the original Accus. before defense are 99.97%, 95.52% and 93.30% for ResNet20, VGG16 and MobileNet, respectively. And the corresponding ASRs without defense are 99.18%, 100% and 99.39%, respectively.\nFor comparison, we select 4 state-of-the-art defense algorithms: Neural Cleanse, STRIP, NEO, Februus."}, {"title": "A. Settings", "content": "1) Metrics: We use two metrics, clean image accuracy and the attack success rate, to evaluate the maintenance of classification performance and the mitigation of the backdoor effect after defense, respectively. Model's classification accuracy \\(\\text{Accu}\\) and the attack success rate \\(\\text{ASR}\\) are calculated as follows:\n\\[\\text{Accu} = \\frac{1}{n} \\sum_{i=1}^n  I(f__\\theta(x_i) = Y_i)  \\tag{7}\\]\n\\[\\text{ASR} = \\frac{1}{m} \\sum_{j=1}^m  I(f_\\theta(x_{pj}) = Y_t)  \\tag{8}\\]\nwhere \\(f_\\theta(x)\\) is the model's prediction for image x, n is the sum of clean images, m is the sum of poisoned images with triggers, \\(y_i\\) is the groundtruth label of the ith image and \\(y_t\\) is the target label of the backdoor attack. And I() is the indicator function. Defense algorithms and backdoor repair methods are expected to keep Accu. high and reduce ASR as low as possible at the same time.\n2) Models and Datasets: To evaluate the defense, we consider two widely used settings: face recognition and traffic sign classification. For face recognition, we choose FaceScrub [38] as the dataset, while for traffic sign classification, we choose GTSRB [39], [40] as the dataset.\nWe experiment with patch-based trigger attacks and non-patch based trigger attacks respectively, and for non-patch based attacks, we introduce the settings in the Additional Experiments for Verification part. For patch-based attack, we use two benchmark backdoor attacks, BadNets [5] and Trojan Attack [6]. For BadNets, we use GTSRB to train ResNet20 [33] and FaceScrub to train MobileNet (a face classifier taking MobileNet as backbone which is widely used for face classification) [41]. For Trojan Attack, we use VGG16, which is pretrained on 2,622,000 images with faces from 2,622 persons [3], and then fine-tuned the last full connected layer of VGG16 model on clean images from FaceScrub. VGG16 was introduced in 2014 [42] and was earlier than the work of Batch Normalization, so the original VGG16 model does not incorporate BN layers. Although VGG16 is not widely used in practice now and there are modified versions VGG16-BN, we still decide to use VGG16 to achieve the attack performance similar to the original work proposing Trojan Attack [6].\nOur used GTSRB is a dataset consisting of 40,736 traffic signs of 43 kinds, and images are resized to 32 \u00d7 32. Our used FaceScrub is a face dataset with 45,333 images of 526 persons. For BadNets, we resize the images in FaceScrub to 160\u00d7160, while for Trojan Attack, we resize the face images to 224 \u00d7 224. For each attack, we randomly choose 10% of the training data to poison with the trigger and mix the poisoned images with the other 90% images to train the models.\n3) Triggers and Defenses: For Trojan Attack, the triggers are generated dependent on the models. For BadNets, the trigger of GTSRB is set as a yellow block with size 2 \u00d7 2 put on the bottom right corner fixedly, while the trigger of FaceScrub is a circular British flag logo with radius 20. Instead"}, {"title": "B. Results", "content": "In this part, we show our experimental results to demonstrate the performance of CETF and the model repair. For the performance of CETF, we focus on not only the defense effect against the backdoor attacks but also the DE process and the robustness. For the performance of model repair, we focus on not only the backdoor mitigation effect but also the relationship of the BN layers and the backdoors.\n1) Performance of CETF:\na) ASR and Accu. of CETF: The visualization results of each step of CETF are shown in Fig. 4. The Accus. and ASRs of the models after applying different defense methods are shown in Tables II and III respectively.\nThere are two defenses with CETF in the tables, including CETF-only and CETF followed by the na\u00efve unlearning. From Table II, Neural Cleanse and the defenses with CETF can keep the model's usability in the experiments. After defense by Neural Cleanse, the Accus. of the models have little decrease compared with those of the original backdoored models without defense. CETF-only also has a good maintenance of the Accus. for clean inputs. The Accus. of the VGG16 and MobileNet models after the CETF followed by unlearning even increase, which may be due to that our unlearning eliminates the backdoor in the models, thereby restoring the model performance to the clean models.\nAs for the reduction of ASR, the CETF followed by the unlearning is also obviously the best. The high Accus. and low ASRs of CETF-only demonstrate the superior performance of CETF to distinguish poisoned images and clean images. For ResNet20, Neural Cleanse and CETF perform best as ASRs are 0. But for VGG16 and MobileNet, the ASR of Neural Cleanse are 7.63% and 6.49%, respectively, which are obviously worse than the effect of the two defenses with CETF. Note that in the original experiments of Neural Cleanse [9], the researchers create a new training dataset using 10% samples of the original training data in which 20% are added the reversed trigger with original labels, and then use the dataset to unlearn the backdoored model for just 1 epoch. The dataset used by them contains 2,622,000 images and the ASR after unlearning is 3.70%. As our dataset is much smaller, we unlearn the model for 20 epochs for Neural Cleanse. It indicates that Neural Cleanse makes great demands on both the data and computation resources, which is not practical. But our CETF followed by unlearning methods only need 2 images for each class, and unlearning for 3 epochs is adequate to get the superior effect, so the contribution lies in not only the better backdoor repair performance but also its better usability when the defenders have limited data and computation resources.\nFebruus is even more unstable than Neural Cleanse because the ASR in VGG16 is as high as 92.83%, which is caused by the drawback of GradCAM. Since the Trojan Square is relatively large, when the GradCAM cannot detect the entire influential region for Februus to remove, the remaining part will still work as trigger to attack the model. Similarly, STRIP reduces the ASR to 0.19% in VGG16, but in ResNet20, the ASR is 55.04% which is due to the trigger's vulnerability activated by the blending process. As for NEO, random search and unreasonable threshold lead to its weak performance.b) DE Process Analysis: DE outputs the optimal individual with the largest fitness value as the final result, which represents a searched region that is most likely a trigger. We show the DE optimized process clearly in Fig. 5. The optimization finishes after only 11 generations. From Fig. 5, as DE optimizes the region generation by generation, its position and size are more and more accurate to cover the trigger, which can be quantized by the higher and higher fitness value. The effective results show the excellent performance of DE.\nThere are optimization results of DE for poisoned inputs shown in Fig. 6. Triggers can always be found and blocked accurately by the evolutionary process, thus demonstrating the effectiveness of CETF.\nc) Parameter Analysis: We explore the influence of DE's parameters on the performance of CETF and explain how we choose the values. We experiment with the number of individuals ranging from 10 to 50 and \\(\\alpha\\) in Equation 1 taking 0.1, 0.3, 0.5 0.7 and 0.9, respectively, with other parameters fixed.\nd) Threshold Selection: In the pasting and filtering phase, we stamp the influential part output by DE to a set of clean images and analyze their predictions. If the ratio of predictions changing to the input's original prediction exceeds a threshold, then we determine the current input as poisoned. Obviously, the threshold directly affects the performance of our defense, and we set the threshold to 50% in our experiments.\nTo demonstrate the validity and universality of our chosen threshold, we have counted the prediction changing ability of clean and poisoned samples, and the experimental results are shown in Fig. 7. From Fig. 7, there is an obvious divergence between the prediction transitions corresponding to clean inputs and poisoned inputs. And clearly, it is very easy for our threshold to distinguish poisoned inputs from clean inputs. 2) Robustness Analysis: In this part, we take a further step towards the multi-trigger attack as well as the random-size-trigger attack, and demonstrate that our defense method is highly robust to these backdoor attacks that are closer to the physical world scenarios.\na) Multi-Trigger Attack: For the multi-trigger attack, we randomly paste 2 or 3 triggers to the validation data (from FaceScrub) as shown in Fig. 8 and feed them to the backdoored MobileNet. From Table VI, the ASRs are 0 for poisoned inputs with 2 and 3 triggers after our CETF defense, which demonstrates that CETF is really robust to the multi-trigger attacks. More triggers make the trigger search in our CETF more efficient. As for other methods, it is more difficult for Neural Cleanse to generate a reversed trigger to simulate all the trigger variants for unlearning thus it is likely to lose effectiveness. STRIP has a better defense performance than the single-trigger attack, since multi-trigger input can be distinguished more easily by STRIP due to the higher prediction possibility of multi-trigger input. Februus is also limited since GradCAM cannot always highlight all the triggers simultaneously so there still remains triggers after Februus's removing operation. NEO shows the worst performance, because when the multiple triggers distribute not closely, it is hard for NEO to find a size fixed block that covers all triggers, where the prediction will change if this block is removed.\nb) Random-Size-Trigger Attack: For the attack with random-size triggers, we poison data with triggers of random sizes and train a new backdoored MobileNet model with the data to evaluate the defenses. There are some samples poisoned with random-size triggers shown in Fig. 8. From Table VI, CETF presents the best performance. Neural Cleanse's performance can still be explained by its lack of stability for trigger's variants. STRIP nearly keeps its performance as that in backdoor attack of single fixed-size trigger. Februus is likely to be out of action when the trigger is too large to be totally contained by GadCAM. Compared to the other three methods, NEO shows a better performance, but it is still far inferior to our defense method.\n3) Performance of Repair: In this part, we show the model repair results of the na\u00efve unlearning, BN-unlearning and BN-cleaning.\nFrom Fig. 9, these 3 repair methods get excellent effect for each model with the extracted trigger from CETF. BN-unlearning can directly decrease ASRs of attacks to 0 and keep the models' classification accuracy by fine-tuning with only 1 image for each class, which demonstrates a striking applicability with limited resources. BN-cleaning cannot always decrease the ASRs to 0, but the ASRs after BN-cleaning with 1 image for each class, 0.26% and 0.02%, are also lower than many defense methods as shown in Table III. The Accus. even become higher after our model repair methods compared to the Accus. before defense, and this may benefit from the outstanding backdoor-mitigation effect. Considering these methods can all get stable results with only 2 images for each class, they are absolutely effective and practical repair methods, which also demonstrate the precise extraction for the triggers by CETF. Besides, the success of BN-unlearning and BN-cleaning also demonstrates the validity of our viewpoint about the relationship of backdoor and BN layers.\nTo further verify the viewpoint, we enrich the variety of backdoor attacks. Experiments on models attacked by Input-Aware Attack, Blended Injection, WaNet and SIG are added. Specifically, for each attack, we generate the corresponding poisoned samples for each class, and repair the backdoored model by the 3 repair methods in this paper. The repair effect is a powerful proof of our viewpoint."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a backdoor defense based on evolutionary trigger detection and efficient model repair by unlearning, which is inspiring for research of backdoor defense. In detail, the trigger detection method CETF redesigns the key optimization factors of Differential Evolution to search for the trigger in a poisoned input and pastes it to clean images for verification. The accurate trigger extracted from the poisoned input, as well as the huge decrease of the attack success rate, highly demonstrates the effectiveness of CETF. And the robustness testing experiments further show that CETF is robust and practical. Using the trigger extracted by CETF, the unlearning methods repair the backdoored models successfully. Besides, we explore the mechanism of backdoor and believe our proposed BN-unlearning and BN-cleaning could guide the understandings of backdoor attack.\nSince the BN unlearning and BN cleaning repair the backdoored model by modifying the distribution of the statistical information in the BN layers, we will try to explore if there is other data except for the images with triggers that can influence the distribution, which could make the repair more convenient. Also, we hope to prove the specific influence of triggers on the BN layers in the training phase theoretically in the future work. Besides, though current computer vision models usually have BN layers, we still want to explore the backdoor in the model without BN layers for a deeper understanding of backdoor attack."}]}