{"title": "GAVEL: Generating Games Via Evolution and Language Models", "authors": ["Graham Todd", "Alexander Padula", "Matthew Stephenson", "\u00c9ric Piette", "Dennis J.N.J. Soemers", "Julian Togelius"], "abstract": "Automatically generating novel and interesting games is a complex task. Challenges include representing game rules in a computationally workable form, searching through the large space of potential games under most such representations, and accurately evaluating the originality and quality of previously unseen games. Prior work in automated game generation has largely focused on relatively restricted rule representations and relied on domain-specific heuristics. In this work, we explore the generation of novel games in the comparatively expansive Ludii game description language, which encodes the rules of over 1000 board games in a variety of styles and modes of play. We draw inspiration from recent advances in large language models and evolutionary computation in order to train a model that intelligently mutates and recombines games and mechanics expressed as code. We demonstrate both quantitatively and qualitatively that our approach is capable of generating new and interesting games, including in regions of the potential rules space not covered by existing games in the Ludii dataset. A sample of the generated games are available to play online through the Ludii portal. 1", "sections": [{"title": "1 Introduction", "content": "Games have long been used as a test bed for algorithms and approaches in artificial intelligence, with advances in game-playing ability often serving as some of the most recognizable achievements in the field [12, 39, 53, 59]. While automated systems have repeatedly demonstrated an ability to match or surpass humans as game players, they continue to lag significantly in their capacity to generate the kinds of games that are worth playing. The ability to construct novel and interesting games is an impressive cognitive challenge, and success would have implications both cultural (with the production of new artifacts) and computational (with the generation of new learning environments for artificial agents). Prior efforts in automated game design [41, 57] have produced some successes (notably the commercially-available game Yavalath, generated by the Ludi system [11]), but remain largely limited by hard-coded heuristics and restricted domains. These limitations are often necessary, however, in the face of automated game design's core challenges: representing the vast array of possible games in a structured and computationally workable form, and efficiently searching through the resulting representation space for worthwhile games.\nIn this work, we present GAVEL (Games via Evolution and Language Models)\u2014an automated game design system that tackles these challenges by leveraging recent improvements in game rule representation and code synthesis (see overview in Figure 1). GAVEL draws on three main components: (1) the Ludii game description language [10, 45] to efficiently encode a large variety of board game rule sets, (2) a large code language model to reliably produce plausible modifications to existing games inspired by evolution through large models (ELM) [33], and (3) quality-diversity optimization [48] to generate a wide range of playable and interesting games. Each component builds on the others: our choice of representation allows GAVEL to not only produce novel board games in a wide range of genres and styles, but also affords us a dataset of over 1000 existing board games from around the world [9]. This dataset, in turn, provides sufficient basis to fine-tune a code synthesis model. In addition, our quality-diversity approach leverages the inherently modular nature of our representation in order to determine game novelty through the presence of particular game mechanics and motifs.\nWe show empirically that GAVEL is capable of generating playable and interesting board games that differ substantially from games encountered during training. Our approach intelligently recombines mechanics and ideas from disparate genres and produces samples that mirror the performance of human-generated games under a suite of automated evaluation metrics. A preliminary qualitative analysis also reveals that GAVEL can generate novel games that are both engaging and entertaining. We conclude with a discussion of GAVEL's successes and failures, as well as the promising avenues for future work."}, {"title": "2 Related Work", "content": "2.1 Automatic Game Generation\nOur work continues a strand of research that investigates the ability for automated systems to produce novel games or novel variants of existing games. The first such effort was METAGAME [44], which generates games by sampling from a grammar that encodes \"symmetric Chess-like games\" with designer-specified rule probabilities. Since then, work has continued in the generation of both board game and video game rulesets. Evolutionary or search-based game design is a popular technique, as game descriptions do not typically afford gradient information; it was first proposed in 2008 for board games [11] and video games [57], and later work has brought it to bear on different video game genres [42, 15, 28]. Another approach begins instead from conceptual or symbolic specifications of rules or mechanics and generates games by dynamically referring to a pre-specified library of gameplay elements [41, 58]. Yet another approach is to use constraint satisfaction algorithms. For instance, rules might be encoded as an answer-set program, with constraints pre-specified by a designer to define what counts as an acceptable game [54, 64, 43, 25, 56]. Most recently, work has investigated the ability for large language models to act as design assistants by proposing game mechanics [2] or directly synthesizing small programs [26].\n2.2 Evolutionary Computation and Language Models\nEvolutionary computation refers to a large class of algorithms broadly inspired by the biological process of evolution [23]. Of these, our approach descends most directly from genetic programming: the use of evolution or other stochastic search procedures for program synthesis [17, 22, 31]. We also draw on more recent advances in quality-diversity algorithms that aim to find a distribution of solutions to a given problem rather than a single optima [40, 48, 19], especially in the context of code and content generation [24].\nIn recent years, improvements in large language models both generally [6, 63] and in their ability to produce code [13, 61] have led to their use in a variety of evolutionary systems. Of note is evolution through large models [33], which learns a diff model (i.e. a language model that can modify programs conditioned on a natural language specification) from a dataset of GitHub commits and accompanying messages. This model is then used as the mutation operator for genetic programming in Python through a quality-diversity algorithm [40]. We adopt this general approach, though GAVEL learns to mutate games from raw programs (i.e. without diffs or commit messages) in a domain-specific language. Similar techniques have also been used to generate programming puzzles [47], reward functions for reinforcement learning [37], and poetry [5]."}, {"title": "3 Game Representation and Dataset", "content": "We represent games as programs in the Ludii game description language (L-GDL) [10], in which game rules are built from ludemes\u2014high-level keywords that represent common components in the natural language descriptions of board game rules. Examples of such keywords include step, slide, hop, piece, empty, board, and so on. Owing to this abstraction, the L-GDL is both robust enough to encode a vast array of disparate games [45] and compact enough that game descriptions often fit within the context lengths of modern large language models.\nIn addition to ludemes, the Ludii system also defines a large number of concepts\u2014high level properties of games that describe its gameplay or structure [46]. Most concepts are boolean and indicate the presence or absence of a particular feature. For example, one concept might describe whether a game is asymmetric, while another might describe whether a game uses the \"custodial\" capture mechanic seen in Taft-style games. These concepts provide a way to represent games as meaningful feature vectors, which can then be used to cluster and compute similarities between games [55].\n3.1 Dataset\nWe construct our initial game dataset out of the 1182 existing games that have been translated into the Ludii game description language (available under a Creative Commons BY-NC-ND 4.0 license)."}, {"title": "4 Methods", "content": "4.1 Language Model Training\nIn line with the general ELM approach, we make use of the impressive generative capabilities of modern code language models in order to propose sensible modifications to existing programs [33]. Unlike prior work, however, we specifically fine-tune an existing model to operate in L-GDL instead of working with programming languages seen during pre-training or making use of in-context learning. In addition, we train our model to act as a mutation operator over programs by using a fill-in-the-middle (FITM) [4] training objective instead of the more common left-to-right objective. FITM training allows the model to make changes to interior components of a program without (a) relying on an extant dataset of code diffs or (b) regenerating the entire game at each step.\nWe train an instance of CodeLlama [51] (specifically CodeLlama-13b, as it is the largest model in its family that was pre-trained with a FITM objective) on the dataset described in Section 3.1. To facilitate FITM training, we extract every balanced parenthetical expression from each game (e.g. (board (square 10)))and add it to the dataset along with the corresponding prefix and suffix in the program. With respect to the grammar of L-GDL, this process is equivalent to extracting syntactic nodes and all of their descendants. The final dataset consists of 49,968 such (prefix, suffix, target) tuples. To facilitate training on a single GPU, we make use of both parameter-efficient fine-tuning [38] and 8-bit quantization [21]. Owing to the large size of the dataset and the fact that each game appears repeatedly in different configurations, we fine-tune the model for a single epoch with hyperparameters available in Appendix B. Training took approximately 40 hours to complete on a single RTX8000 GPU.\nWe note here that FITM training may have a potential downside in the context of evolution through large models. Specifically, the model is trained to perfectly reproduce the missing section of code given its prefix and suffix. If it succeeds completely in doing so during evolution, then the resulting game will not be mutated at all. In essence, there is a tension between the ability of the model to accurately capture the underlying logic and syntax of the representation space and its ability to memorize or perfectly reconstruct its training dataset. In GAVEL, we mostly sidestep this issue by mutating a set of held-out games not seen at all during training (making memorization impossible), though see Section 8 for a discussion of other possible approaches.\n4.2 Evolutionary Search\nOur evolutionary search strategy of choice is MAP-Elites [40], a population-based quality-diversity algorithm that leverages both a fitness function over samples as well as a set of behavioral charac- teristics\u2014functions that describe non-fitness attributes of samples and are used to ensure that the population does not collapse to only a small number of distinct samples. Specifically, MAP-Elites maintains an archive of cells, each associated with a particular range of values under the behavioral characteristics. At each step, a novel sample is evaluated to determine its fitness as well as the cell it would occupy. It is added to the archive if either that cell is unoccupied or if its fitness exceeds that of the current occupant, in which case it replaces the current occupant. In this way, samples only \"compete\" with one another within particular cells. We describe our fitness function in detail in Section 4.3.\nDetermining an appropriate set of behavioral characteristics is challenging in the context of automatic game generation. Intuitively, distinct archive cells ought to capture meaningfully distinct games while collapsing minor or trivial variations. Human game players readily make such categorizations, but automatically identifying such differences (especially with previously unseen games) is both difficult and necessary for the MAP-Elites algorithm to function. In order to tackle this problem, we take advantage of the semantic concepts described in Section 3. Building on evidence that Ludii concept vectors capture a meaningful notion of distance [55], we use principal component analysis (PCA) [60] on the complete Ludii dataset (i.e. before any filtering) to reduce the 510-dimensional concept vectors to two dimensions. We then bucket the resulting two-dimensional space into 40 equally-spaced regions from -5 to 5 in each dimension, obtaining a rectangular archive of 1600 cells. While the first two PCA dimensions describe only ~ 28% of the variance in the concept feature space, a preliminary investigation indicated that increasing the number of dimensions resulted in a less diverse archive. Because games are not uniformly distributed through feature space, increasing the archive's dimensionality for a fixed number of cells caused a larger number of distinct games to be mapped to the same cell. See discussion of potential alternatives in Section 7.\nWe initialize the archive by adding and evaluating the 14 heldout games listed in Appendix A. For each MAP-Elites step, we select j games from the current archive. For each game, we then select k random parenthetical expressions and re-format them as a (prefix, suffix, target) tuple. We then sample from the trained CodeLlama-13b model with a temperature of 1 to generate a new expression, conditioned on just the prefix and suffix, and re-construct the resulting game. After filtering out any duplicate or unchanged games, the samples are evaluated for fitness and assigned an archive cell based on the PCA reduction of their concept vector.\n4.3 Evaluation\nGame quality is both difficult to quantify and inherently subjective. Nevertheless, automatic game design and evolutionary computation necessitate some kind of computable optimization objective. These objectives typically take the form of one or more heuristics that aim to proxy the underlying targets of \u201cfun\u201d or \u201cinterestingness.\u201d For restricted domains, it is often possible to imbue a large amount of expert knowledge into these heuristics, as in the Ludi system [11] (precursor to Ludii) which employed game-state evaluator functions to capture both objective measures (e.g. a game's balance) and psychological measures (e.g. a game's excitement or unpredictability). However, the large space of games described by the Ludii description language makes relying on such evaluator functions infeasible. Instead, we define a hierarchical fitness function based on a relatively small set of objectively and reliably measurable heuristics that are fully game-agnostic.\nConcretely, every game g generated during the evolutionary search is assigned a fitness value \\(f(g) \\in \\{-3, -2, -1\\} \\cup [0.01, 1]\\) by a sequence of tests (pseudo-code presented in Algorithm 1). Evaluation begins with a series of binary evaluations. First, all games that fail to compile (e.g. due"}, {"title": "5 Experiments", "content": "We perform 3 runs of MAP-Elites with random seeds {1, 2, 3}, each lasting for 500 steps. For each run, we select j = 3 games and generate k = 3 mutations for each game at each step. Quantitatively, we report the progress of the archive using three metrics: the quality-diversity score (QD score) [49], calculated as the sum of the fitness of each cell in the archive. In cases like ours where fitness values can be negative, each fitness value is incremented by the minimum possible fitness (i.e. -2) before being summed to ensure that the QD score increases monotonically over time. In addition, we report the number of playable and minimally interesting games (i.e. \\(f(g) > 0\\)) in the archive as well as the number of such games that occupy cells which are not covered by any game in the Ludii dataset. Finally, we report the number of cells and novel cells that contain games with a fitness of at least 0.5, indicating their potential as worthwhile games. Each run lasted roughly 48 hours using a single RTX8000 GPU for inference from the CodeLlama-13b model and performing evaluations in parallel with 16 CPU cores and 128GB of total memory.\nIn addition, we perform another set of 3 runs with a variant of GAVEL that uses the Upper Confidence Bound algorithm [3] to select which regions of games to mutate. Specifically, we treat the selection of a parenthetical expression to mutate as a multi-armed bandit problem where each arm corresponds to a different leading ludeme (e.g. board or equipment). We consider a mutation \u201csuccessful\u201d if it results in the mutated game being added to the archive (either by improving the fitness of an existing occupant, or by occupying a new cell) and update the statistics for each \u201carm.\u201d We call this variant GAVEL-UCB. All other hyperparameters remain the same as with the original GAVEL experiment."}, {"title": "6 Results", "content": "6.1 Quantitative Results\nOverall, GAVEL succeeds in generating a wide range of novel and high-fitness games. In Table 1 we present the mean and standard deviations of the archive metrics for GAVEL and GAVEL-UCB. The quality diversity score is difficult to interpret in isolation, as its magnitude depends greatly on the potential range of fitness scores. In this case, it is most helpful as a way to compare the performance of disparate algorithms on the same task: we see that GAVEL improves significantly over GAVEL-UCB (Welch's t-test, p = 0.029), indicating that it has some mixture of higher fitness and greater variety in the samples it produces. Of more interest is the fact that both methods manage to fill a substantial proportion of the archive with playable games while only starting from a modest 14 samples, including regions of the archive not covered by games in the Ludii training dataset (differences between GAVEL and GAVEL-UCB are not significant, p > 0.05). We present a visualization of the archive produced by one run of GAVEL over time in Figure 3, which shows both the success of the model in generating high-fitness samples and the fact that much of the concept space remains unexplored.\n6.2 Qualitative Results\nIn order to more closely examine GAVEL's output, we rely on expert evaluators to quickly playtest potentially promising games. These evaluators are broadly familiar with the Ludii dataset and so are able to determine whether a generated game is truly novel and where its mechanics might have originated from. This preliminary human analysis helped shed light on some of GAVEL'S shortcomings (discussed below) and also revealed some particularly interesting games among the high-fitness samples. Of special note is a variant of Yavalath, itself the product of an automated system [11]. In the original game, players take turns placing pieces on a hexagonal board. A player wins if they have four pieces in a row but loses if they have three pieces in a row first. GAVEL makes both minor changes to the ending rules (increasing the number of pieces in a row needed for victory and loss by one) as well a substantial addition by introducing the enclosure capture rules of Go. The result is a game that tasks players with thinking about the arrangement of their pieces in many ways and that appears to offer the potential for sophisticated strategy. In Figure 4 we present an example of play between automated agents in which all of the game's rules are used in concert.\nFigure 2 (right) includes another example of a generated game noted by our evaluators to be partic- ularly interesting. It is a variant of Havannah (a game in which players attempt to form loops or connected lines of pieces between sides of the board) that introduces a restriction on piece placement from another game in the Ludii dataset (Tabu Y). A final exemplar, presented in Appendix C alongside the previous two examples, modifies the pawn-advancement game Breakthrough to use pieces that can only move by hopping over each other (and without capturing). Taken together, these examples demonstrate the strength of GAVEL: it is able to intelligently recombine game mechanics (expressed as code segments) in novel ways and ensure that these combinations do not result in trivial games."}, {"title": "7 Discussion and Limitations", "content": "Unused Game Components: a common failure mode is that the model will generate game rules that are not actually used during gameplay. For instance, a change to the equipment section might add dice as additional game pieces. However, without further changes to the gameplay section to incorprate them, the dice will remain unused. Detecting such extraneous rules automatically is challenging, as the Ludii system does not provide a way to determine which rules are activated during a given playout. In addition, penalizing games with unused components during fitness evaluation might harm diversity by eliminating potential \u201cstepping stones\" to more interesting games. Nevertheless, if changes to the underlying representation did allow unused sections to be detected, it might be possible to bias future mutations towards relevant gameplay sections in order to increase the likelihood of the missing rules being generated.\nHeldout Games: as noted in Section 4.2, we initialize the search using a set of games held out from the language model training in order to prevent memorization. While GAVEL produces a wide range of games from this set, they nonetheless will share many features with one another as a result of their shared origins. One potential solution to further increase archive diversity is modifying the mutation process directly, either by increasing sampling temperature or by enforcing novelty with respect to the tokens in the original game section through masking. These techniques might also make it possible to initialize the archive with games in the training dataset, further increasing potential output diversity.\nArchive Selection: determining an appropriate way to distinguish between games is a general challenge. GAVEL makes use of Ludii concepts, but not all representation schemes afford such detailed semantic information. One promising alternative for such domains is the automatic selection of behavioral characteristics, either through distillation of trajectories obtained during evaluation [18] or by leveraging the ability for large language models to identify archetypes in code [47].\nEvaluation: our evaluation metrics capture general and minimal criteria of interesting games, broadly construed. However, satisfying our evaluation metrics alone is far from sufficient evidence for a game being interesting. Ultimately, all metrics in automated game design aim to proxy notions of human preference\u2014as mentioned in Section 6.2, we rely on expert evaluators to filter from high-fitness samples to interesting games. Another worthwhile approach, then, might be to learn these preferences directly from human ratings [14] or attempt to extract them from latent knowledge in large language models [29]."}, {"title": "8 Future Work", "content": "First and foremost, we are excited about a more in-depth human analysis of the games generated by GAVEL. While our expert evaluators can provide useful insight and analysis, their perspective is ultimately one of many. A larger user study could help identify not only which games are most appealing to human players, but also the particular features of those games that correlate with fun and engagement. This, in turn, could help spur improvements to GAVEL across the board, from archive selection to evaluation metrics.\nWithin the Ludii domain, one particularly exciting possibility for future work is the integration of the explicit L-GDL grammar. While the CodeLlama-13b model learns to produce syntactically valid mutations, integrating the grammar either through token masking [52] or Monte-Carlo steering [34] could allow for greater mutation diversity (by increasing sampling temperature, for instance) without sacrificing syntactic correctness and compilability. In addition, it might be possible to use data-augmentation techniques (e.g. sampling from the underlying grammar) to create a dataset of Ludii game diffs with which to train a more typical ELM model.\nMore generally, large language models could also be used to link natural language descriptions of game rules with their programmatic representations. For example, an instruction-tuned model could be used to convert from abstract rules to executable code, either by fine-tuning (e.g. on the Ludii dataset) or through in-context learning. This might allow automatic game design systems to interact with and generate games at the level of natural language, better resembling the process used by human designers, while still retaining the ability to automatically evaluate the relevant gameplay properties of those games."}, {"title": "9 Broader Impact", "content": "Like all automated game design systems, GAVEL has the potential for impacts on the larger space of game design. Especially at time of writing, as the video game industry experiences widespread layoffs and contractions, it is important that automatic systems are used to assist and inspire human designers instead of replacing them. Indeed, our results indicate that such collaboration is crucial to the generation and identification of worthwhile games. We also draw attention to GAVEL's use of large language models: while the Ludii dataset is publicly available, a similar system could conceivably be used to generate games from scraped datasets without such free access or from game code encountered during pre-training. Special care should also be taken to ensure that language model outputs are manually verified before being published. Overall, however, we feel that the specific domain, use case, and outputs of GAVEL mean that its broader impact is unlikely to be negative."}]}