{"title": "Ego-Foresight: Agent Visuomotor Prediction as\nRegularization for RL", "authors": ["Manuel S. Nunes", "Atabak Dehban", "Yiannis Demiris", "Jos\u00e9 Santos-Victor"], "abstract": "Despite the significant advancements in Deep Reinforcement Learning (RL) ob-\nserved in the last decade, the amount of training experience necessary to learn\neffective policies remains one of the primary concerns both in simulated and real\nenvironments. Looking to solve this issue, previous work has shown that improved\ntraining efficiency can be achieved by separately modeling agent and environment,\nbut usually requiring a supervisory agent mask. In contrast to RL, humans can\nperfect a new skill from a very small number of trials and in most cases do so\nwithout a supervisory signal, making neuroscientific studies of human development\na valuable source of inspiration for RL. In particular, we explore the idea of motor\nprediction, which states that humans develop an internal model of themselves and\nof the consequences that their motor commands have on the immediate sensory\ninputs. Our insight is that the movement of the agent provides a cue that allows\nthe duality between agent and environment to be learned. To instantiate this idea,\nwe present Ego-Foresight, a self-supervised method for disentangling agent and\nenvironment based on motion and prediction. Our main finding is that visuomotor\nprediction of the agent provides regularization to the RL algorithm, by encouraging\nthe actions to stay within predictable bounds. To test our approach, we first study\nthe ability of our model to visually predict agent movement irrespective of the\nenvironment, in real-world robotic interactions. Then, we integrate Ego-Foresight\nwith a model-free RL algorithm to solve simulated robotic manipulation tasks,\nshowing an average improvement of 23% in efficiency and 8% in performance.", "sections": [{"title": "1 Introduction", "content": "While it usually goes unnoticed as we go about our daily lives, the human brain is constantly engaged\nin predicting imminent future sensori inputs [1]. This happens when we react to a friend extending\ntheir arm for a handshake, when we notice a missing note in our favorite song, and when we perceive\nmovement in optical illusions [2]. At a more fundamental level, this process of predicting our\nsensations is seen by some as the driving force behind perception, action, and learning [3]. But while\npredicting external phenomena is a daunting task for a brain, motor prediction [4] - i.e. predicting the\nsensori consequences of one's own movement - is remarkably more simple, yet equally important.\nAnyone who has ever tried to self-tickle has experienced the brain in its predictive endeavors. In trying\nto predict external (and more critical) inputs, the brain is thought to suppress self-generated sensations,\nto increase the saliency of those coming from outside - making it hard to feel self-tickling [5].\nIn artificial systems, a significant effort has been devoted to learning World Models [6] [7] [8], which\nare designed to predict future states of the whole environment and allow planning in the latent space.\nDespite encouraging deployments of these models in real-world robotic learning [9], their application\nremains constrained to safe and simplified workspaces, with sample-efficient Deep Reinforcement\nLearning (RL) being one of the main challenges.\nThough comparatively less explored, the idea of separately modeling the agent and the environment\nhas also been investigated in RL, with previous work demonstrating improved sample-efficiency in\nsimulated robotic manipulation tasks [10]. Additionally, this type of approach has been used to allow\nzero-shot policy transfer between different robots [11] and to improve environment exploration [12].\nCommon to all these works is the reliance on supervision to obtain information about the appearance\nof the robot, allowing to explicitly disentangle agent from environment. This is usually provided in\nthe form of a mask of the agent within the scene, which is obtained either from geometric IDs in\nsimulation, by fine-tuning a segmentation model or even by resorting to the CAD model of the robot.\nAs humans, we don't receive such a detailed and hard to obtain supervisory signal and yet, during our\ndevelopment, we build a representation of ourselves [13] capable of adapting both slowly, as we grow,\nand quickly, when we pick up tools [14]. In this work, we argue that unsupervised awareness of self\ncan also be achieved in artificial systems, and study its advantages relative to supervised methods.\nIn our approach, in alternative to some recent lines of research [15], we place the agent's embodiment\nas an intrinsic part of the learning process, since it determines the visuomotor sensations that the\nagent can expect as it moves. Our insight is that agent-environment disentanglement can be achieved\nby having the agent move, while trying to predict the visual changes to its body configuration, and\nthat awareness and prediction of the agent's movements should improve its ability to solve complex\ntasks, as exemplified in Figure 1.\nTo implement this, we use an encoder-decoder model, that receives as input a limited amount of\ncontext RGB frames and the future sequence of proprioceptive states (sense of self-position, e.g. joint\nangles) that will result from the planned action sequence. The decoder outputs the RGB reconstruction\ncorresponding to the proprioceptive signal, with the agent in its future configuration. This framework\nnaturally lends itself to self-supervised training, by having the system predict future frames.\nWe test our model on a publicly available dataset of real-world robot interactions [16], in which we\ndemonstrate: i) the ability to predict the movement of the robot arm while ignoring moving objects in\nthe environment, and ii) the generalization to previously unseen (\"imagined\") trajectories.\nFurthermore, we combine our architecture with a model-free RL algorithm and assess its perfor-\nmance on simulated robotic manipulation tasks, demonstrating that our framework improves sample\nefficiency and performance. We consider that two main factors contribute to this result: i) the\ndisentanglement between agent and environment allows the RL algorithm to focus its capacity on\nlearning the control of the agent in the initial stages of training, and later on the external aspects and\npotential interactions within the environment, and ii) imposing predictability in the robot's movements\nregularizes the RL algorithm (2). Our approach combines concepts of model-based RL, in learning\na model of the agent, while avoiding its drawbacks, by using the model only at training time for\nregularization. In summary, the key contributions of this paper are the following:"}, {"title": "2 Related Work and Background", "content": "Learning agent representations The notion of distinguishing self-generated sensations from those\ncaused by external factors has been studied and referred to under different terms, and with a broad\nrange of applications as motivation. Originating in psychology and neuroscience, with the study of\ncontingency awareness [13] and of sensorimotor learning (motor prediction) [17] [4], in the last few\nyears this concept has seen growing interest in AI, as an auxiliary mechanism for learning.\nIn developmental robotics, Zhang and Nagai [18] have focused on this problem from the standpoint\nof self-other distinction, by employing 8 NAO robots observing each other executing a set of motion\nprimitives, and trying to differentiate self from the learned representations. Lanillos et al. [19] note\nthat to answer the question \"Is this my body?\", an agent should first learn to answer \"Am I generating\nthose effects in the world\". Their robot learns the expected changes in the visual field as it moves in\nfront of a mirror or in front of a twin robot and classifies whether it is looking at itself or not. Our\napproach is somewhat analogous to these works, in the sense that we identify as being part of the\nagent that which can be visually predicted from the future proprioception states, while the robot\nmoves. In a related, but reverse direction, Wilkins and Stathis [20] propose the act of doing nothing\nas a means to distinguish self-generated from externally-generated sensations.\nStill in robotics, the idea of modeling the agent has connections with work in body perception\nand visual imagination for goal-driven behaviour [21] as well as self-recognition by discovery of\ncontrollable points [22] [23]. Another application that has been explored is the learning of modular\ndynamic models, that decouple robot dynamics from world dynamics, allowing the latter to be reused\nbetween robots with different morphologies. Hu et al. [11] propose a method for zero-shot policy\ntransfer between robots, by taking advantage of robot-specific information - such as the CAD model -\nto obtain a robot mask from which future robot states can be predicted, given its dynamics. These\nfuture states are then used solve manipulation tasks using model-based RL. Finally, this concept has\nalso been used for ignoring changes in the robot as a means for measuring environment change, with\nthe intention of incentivizing exploration in real household environments [12].\nIn machine learning (ML), the distinction between the agent and environment has been studied under\nthe umbrella of disentangled representations, a long-standing problem in ML [24]. While most\nworks take an information theoretic approach to disentangled representation learning [25] [26] [27],\nsome try to take advantage of known structural biases in the data, which is particularly relevant for\nsequential data, as it usually contains both time-variant and invariant features [28]. In video, this\nallows the disentanglement of content and motion [29]. Denton and Birodkar [30] explore the insight\nthat some factors are mostly constant throughout a video, while others remain consistent between\nvideos but can change over time to disentangle content and pose. The scene encoder of our method is\nbased on the model of Denton and Birodkar, but while they use an adversarial loss to model pose as a\nproperty that doesn't depend on the video, we take advantage of the notion of agency and use the\nproprioceptive signal to model the visuomotor mapping with a simple reconstruction loss.\nFinally, agent-environment disentanglement can also be achieved through attention mechanisms [31]\nor from explicit supervision, as shown by Gmelin et al. [10]. In this work, the authors demonstrate that\nlearning this distinction allows RL algorithms to achieve better sample-efficiency and performance,\nserving as the most direct baseline for our work.\nRL from Images Over the last decade, work in Deep RL from images, where environment\nrepresentations are learned from high-dimensional inputs, has allowed RL agents to solve problems"}, {"title": "3 Approach", "content": "DDPG is an off-policy actor-critic RL algorithm for continuous control, that alternates between\nlearning an approximator to the Q-function $Q_\\phi$ and a deterministic policy $\u03c0_\u03b8$. Considering a Markov\nDecision Process $(S, A, T, R, \u03b3)$, where S is the state space, A is the action space, $T(s_{t+1}|s_t, a_t)$\nis the transition function, $R(s_t, a_t)$ is the reward function and y is a discount factor, and having\nsampled a batch of transitions $T = (s_t, a_t, r_{t:t+n-1}, s_{t+n})$ from the replay buffer D, $Q_\\phi$ is learned\nby minimizing the mean-squared Bellman error:\n$L_{critic}(Q_\\phi, D) = E_{T \\sim D} [(Q_\\phi(s_t, a_t) - y))^2],$ (1)\nusing a target network $Q_{\\phi'}$ to approximate the target values, with n-step returns:\ny = $\\sum_{i=0}^{n-1} \u03b3^i r_{t+i} + \u03b3^n Q_{\\phi'}(s_{t+n}, \u03c0_\u03b8(s_{t+n})).$ (2)\nThe policy is learned by maximizing $E_{s \\sim D}[Q_\\phi(s_t, \u03c0_\u03b8(s_t))]$, to find the action that maximizes the\nQ-function. Even though DDPG has been successfully applied to RL from images, its instability\nmeans that results should be reported on runs from multiple random seeds [37] [38].\nIn model-based RL, approaches such as Dreamer [39] have explored the idea of training in an\nimagined latent space, by sampling thousands of parallel trajectories. Similarly to our work, some\napproaches have combined ideas from model-free and model-based RL, by augmenting model-free\nalgorithms with prediction based auxiliary losses [40] [41]. However, these works require learning a\nfull dynamics model and do not consider any distinction between agent and environment.\n3.1 Episode Partition\nIn a real-world scenario, in which the\nagent is a robot, it would be reasonable\nto assume that there is access to a camera\nvideo stream and, for each planned ac-\ntion sequence, the sequence of future ex-\npected proprioceptive states in the form\nof joint angles and Cartesian position of\nthe end-effector, obtained from forward\nkinematics. Hence, we define that our\ndataset is composed of N episodes $T_i$\nof a fixed length L (Figure 3), which in-\nclude a sequence of RGB frames x and\nthe corresponding proprioceptive states\np of the agent $T_i = \\{(x_0, p_0), ..., (x_L, p_L)\\}i, i = 1, ..., N$. During training, we randomly select a"}, {"title": "3.2 Motion-Based Agent-Environment Disentanglement", "content": "To model future visual configurations of the robot, we propose an encoder-decoder model (Figure 4)\nthat has two encoder streams and one decoder head. The first encoder produces a representation\nfor the visual content of the scene $h_s$, obtained from the context frames $x_{tc:t}$. The second encoder\ncreates a feature representation $h_p$ for the sequence of proprioceptive states $p_{tc:th}$ of the agent.\nThese two representations are concatenated and decoded into the future frame observation $x_{th}$. This\nformulation results in the following reconstruction loss term:\n$L_{rec}(E_s, E_p, D) = ||D(h_s, h_p) - x_{th} ||^2, $ (3)\nwhere $E_s, E_p$ and D are neural networks for the scene encoder, proprioception encoder and decoder.\nFor brevity, we only include the reconstruction loss for the last time step.\nWe start by setting the assumption that, for the most part, the scene content varies slowly over time,\nand remains mostly the same during the same episode, even though it can change from episode to\nepisode. Hence, we adopt the similarity loss from Denton and Birodkar [30], which encourages\nthe scene representation $h_s$ to be the same when coming from frames of the same episode, while\nremoving any dynamical information that could be extracted from the context\n$L_{sim}(E_s) = ||E_s(x_{tc:t}) \u2013 E_s(x_{tc+m:t+m})||^2 = ||h_s^t \u2013 h_s^{t+m})||^2,$ (4)\nwhere m is a time gap chosen at random. The complete training objective is then\n$L_{ef} = L_{rec} + \u03b1L_{sim},$ (5)\nwhere $\u03b1$ controls the weight of the similarity loss term, and should be adjusted according to how\nmuch the content of a scene is expected to change, allowing the initial assumption to be relaxed."}, {"title": "3.3 Agent Visuomotor Prediction as Regularization for RL", "content": "To test how our model affects sample-efficiency in RL, we implement it together with DDPG [36].\nThe episodes stored in the replay buffer allow us to maintain the approach described in section 3.2\nwhile jointly training our model and learning the policy.\nSimilarly to Gmelin et al. [10], the Actor and Critic neural networks of DDPG receive the low\ndimensional feature vector coming from the encoders, which is the concatenation of $h_s$ and $h_p$. To\njointly train our model and the RL algorithm, we optimize our model together with the critic loss of\nequation 1, using the following extended loss function\n$L = L_{critic} + \u03b2L_{ef}.$ (6)"}, {"title": "4 Experiments", "content": "4.1 Agent-Environment Disentanglement in Real-World Dataset\nBAIR Dataset Commonly used in video prediction research, the BAIR dataset [16] consists of a\nSawyer robot arm randomly moving and pushing a broad range of objects inside a confined table.\nIt includes 43520 video samples, each 30 frames long, with 64 \u00d7 64 resolution. Details on the\nproprioception state and training with BAIR can be found in App. A.1.\nArchitecture In experiments with the BAIR dataset, the scene encoder $E_s$ and the decoder D\nare based on the DCGAN [52], which consists of 5 convolutional layers, each reducing (in $E_s$) or\nincreasing (in D) the size of the feature map by half. Because we provide a sequence of context\nframes as input to $E_s$, we interleave two ConvLSTM layers [53]. Another change relative to DCGAN\nis including skip connections from the $E_s$ to the D, as done in the U-Net [54]. For the proprioception\nencoder, we use an LSTM preceded by 2 fully connected layers. More details can be found in A.2.\nResults In Figure 5, we show two sample predictions from Ego-Foresight on the BAIR dataset. The\nmodel succeeds in separating visual information that is part of the robot from the scene, predicting\nthe trajectory of the robot's arm according to its true motion, which shows that the model correctly\nlearned the visuomotor map between proprioception and vision. The background, including moving\nobjects, is reconstructed in their original position. Even so, our model still predicts that some change\nshould happen when the arm passes by an object, and it often blurs objects that predictably would\nhave moved. Nevertheless, it should be noted that the agent's actions and object movement are\ninherently correlated and therefore can't be completely disentangled.\nTypically, training DDPG involves first sampling an episode from the replay buffer, and then sampling\none transition of that episode for the update of the actor-critic. Our approach requires two extra\nrandom choices from the same episode: first the sequence of transitions used for frame prediction\nand computing $L_{rec}$, and second the C transitions used to obtain the similarity loss target $h_{t+m}$.\nAs we developed our method, we noticed that when training our model together with the RL algorithm,\nthe agent tended to perform goal-directed movements, seeking to maximize reward which prevented\nour model from observing diverse enough motions to learn the visuomotor mapping. To solve this\nissue we introduced a motor-babbling [50] [51] stage for a fixed number of steps at the start of\ntraining, during which actions are random choices of \u00b11, forcing exploratory movements.\nIt is important to note however, that by jointly training the RL algorithm and our model, the visual\nquality of the reconstructed frames is still reduced when compared to that achieved for the BAIR\ndataset. However, in improving performance and sample-efficiency in RL tasks, the reconstruction of\nfuture frames serves as an auxiliary objective, which can be relaxed, as long as both the ability to\nmodel the effect of the commanded actions on the configuration of the agent and the regularization\nafforded by choosing actions that result in predictable futures are preserved. Finally, the choice of B\nis also important, as assigning a too high weight to the $L_{ef}$ term may result in the policy producing\nnearly static movements - which are easier to predict. We use $\u03b2 = 1$ as default."}, {"title": "4.2 Simulated Robot Manipulation Tasks", "content": "Meta-world Benchmark Meta-world is a simulated benchmark [55] of robotic manipulation tasks\nwith predefined reward functions and implemented on MuJoCo [56]. It presents a broad distribution\nof tasks which require a diverse set of skills such as reaching, picking, placing, or inserting. We chose\n10 that involve the manipulation of different objects. More details can be found in App. A.1.\nBaselines We compare our model with two baselines: DrQ-v2 [35] and SEAR [10], introduced in\nsection 2. DrQ-v2 uses an encoder to embed the RGB frames into a feature vector which models the\nfull environment. SEAR expands on top of DrQ-v2, by splitting the feature representation into agent\nand full environment information, which is achieved with a supervisory mask. SEAR is therefore\nthe closest baseline to our model. The results for both baselines are obtained using the source code\nprovided by the authors of each paper. We implement our model as an extension to the code of\nGmelin et al. [10], which in turn is based on the source code of Yarats et al. [35].\nArchitecture Though the architecture used in the meta-world experiments follows the same one\ndescribed in section 4.1, we add some changes for a better comparison with the baselines, which\nare detailed in A.2. The size of the feature vectors is also chosen to be close to SEAR. For a fair\ncomparison with the baselines, we perform the motor-babbling stage (50k steps) from scratch for\neach task, instead of going through it only once and then starting learning the tasks from a model\npre-trained on babbling.\nQuantitative Results To compare our model against the baselines, in Figure 7 we plot the mean\nsuccess rate as a function of environment steps for 10 tasks. Environment steps represent the number\nof times the environment is updated according to an action which, because we use an action repeat\nof 2, is always double the number of actions taken by the actor. Environment updates represent a\nsignificant computational cost, therefore being the preferred way of reporting sample-efficiency in\nthe literature [35] [39]. Each curve presents the mean over 10 evaluation episodes, taken every 5k\nsteps. Furthermore, in trying to reduce the instability of DDPG we also average the performance over\n3 random seeds and display the range of success (best and worst case) in the shaded area of the plot.\nFrom the plots of Figure 7 and their corresponding summary in table 1, there are two main aspects to\ntake into account: performance and sample-efficiency. In terms of the final performance, we observe\nthat Ego-Foresight plainly achieves the best result in 5 out of the 10 tasks, matching the baselines\nin 4 others, and with considerable gaps of over 30% in some of the tasks, with an average of +8%.\nFurthermore, in tasks such as Door Close, Button Topdown, and Hammer, our approach is the only\none that solves the task for every random seed (the worst case shown in the range is never 0 at the\nend) and, in general, presents the narrowest range of success after convergence.\nRegarding sample-efficiency, we define it as the number of steps required to achieve 90% of the\nperformance of the best model. Our model is the most efficient in 6 of 10 tasks, and mostly equivalent\nin 2 others, but also being less efficient than at least one baseline in 2 tasks. If we consider the last\nplotted time step for models that don't achieve 90% of the best, Ego-Foresight is on average 23%\nmore efficient than the next best baseline. One drawback of our method when it comes to efficiency,\nis the need for the motor-babbling stage, which sets a lower bound on how fast the agent can learn.\nFor example, in the Door Close and Lock tasks, our method starts being successful immediately after\nthe 50k babbling steps, hinting that in those cases this stage could have been shorter. In the future,\nthis could be improved to make babbling stop once the visuomotor mapping has been learned.\nIt is important to consider two other factors when analyzing the results of Figure 7: supervision\nand hyperparameter fine-tuning. The three tested models differ in how they are trained, with SEAR\nneeding supervision to disentangle the agent, and DrQ-v2 not requiring supervision, but modeling\nthe complete environment in the feature vector. Our model combines both characteristics, extending\nits range of applications. In terms of hyperparameters, and due to computation and time constraints,\nthe results were obtained using initial, non-optimized, values for \u1e9e and H. We believe that a more\ncareful choice could further improve the results, as indicated by Figures 2 and 9."}, {"title": "Ablation Tests", "content": "To study how the parameters introduced by our method influence performance and\nefficiency, we run Ego-Foresight on a fixed task, for different values of $\u03b2$ and H. In Figure 2, we\nobserve the positive effect of the auxiliary loss $L_{ef}$, confirming the hypothesis that the predictive\nobjective regularizes learning, by encouraging predictable agent trajectories. Figure 9 shows that it is\nbeneficial to predict into the future, but that for a horizon greater than 10 improvement stabilizes. We\nattribute this to the fact that a short horizon is enough to learn the visuomotor mapping and that, most\nMeta-world tasks can be solved in approximately 20 time steps, as observed in Figure 8, with the\nhammer reaching the nail at step 18 and the door being open at step 24."}, {"title": "5 Discussion and Future Work", "content": "In this work, we studied how motion can be used to disentangle agent and environment in a self-\nsupervised manner. We integrate our approach with an RL algorithm and evaluate how an auxiliary\nloss term that penalizes the inability of the agent to recognize and predict its own movement can\nregularize and improve learning in complex tasks. While our results are encouraging, we believe that\nfurther studies with more random seeds, different agents, action spaces, and types of tasks would\naugment the value of our contributions. Our results also indicate that one benefit of self-supervision\nis the possibility of adapting to a changing body-schema, in particular when the agent uses tools. In\nfuture research, we intend to explore how fast this body-schema adaptation can be done. Another\nfuture research direction is studying methods for efficient motor-babbling, and how reusing a model\npre-trained on babbling would improve sample-efficiency in multi-task learning."}, {"title": "A Appendix / supplemental material", "content": "A.1 BAIR Dataset and Meta-world Benchmark\nThe BAIR dataset [16] consists of a Sawyer robot arm, randomly moving and interacting with objects\nplaced inside a box. The set of objects and their disposition changes from video to video, as well as\nthe lighting conditions, providing some variability. For the BAIR dataset, each sequence includes the\nrobot's actions, defined as a 7D array of Cartesian displacement and position of the end-effector and\nthe gripper open/close state. The model is trained by receiving 2 context frames and predicting the\nfollowing 10. At test time, the prediction horizon is extended to 28, with the context length staying\nthe same. The proprioception is defined as the concatenation of the robot's action and the gripper\nstate.\nFor Meta-world is a simulated benchmark of robot manipulation tasks, designed for meta-learning and\nmulti-task learning, including more than 50 different tasks. As in BAIR, the agent is a Sawyer robot,\nbut while for BAIR the we use an FPV camera angle, for Meta-world we opt to use a third-person\nview, as done in [10]. While this means that the number of degrees of freedom that can be observed\nis higher (the full 9 DoF of the Sawyer), we verify that the model still manages to predict the agent's\nmovement. For the proprioceptive state, we use the 9D array of the robot's joint angles (7 arm joints\nand 2 fingers) and the 4D end-effector state, which includes the Cartesian position of the gripper and\nthe distance between the fingers. After each for episode the locations of the targets are slightly varied.\nA.2 Detailed Architecture\nFor the BAIR experiments $E_s$ is a convolutional neural network with 7. Layers 3 and 6 are ConvLSTM\nlayers with kernel size 3 and stride 1, with layer normalization and a leaky ReLU activation. The\nremaining layers are Conv. layers with kernel size 4 and stride 2, except for layer 5 which has stride\n1. Each layer uses Batch Normalization and leaky ReLU activations. After each Conv. layer, the\nnumber of filters is doubled, starting at 64 in layer 1 and ending with 512 after layer 6. The last layer\nhas number of filters corresponding to the size of $h_s$ and a hyperbolic tangent activation. The decoder\nis a mirror of the encoder, without the recurrent layers, and with input size corresponding to the sum\nof the sizes of $h_s$ and $h_s$. Additionally, skip connections are added between each Conv. layer of"}, {"title": "A.3 Training details", "content": "The lists of hyperparameters used in the BAIR and Meta-world experiments are presented in tables 2\nand 3 respectively. Training was split between an NVIDIA RTX3060, RTX2070 or RTX3090.\nFor consistency, all runs for a given Meta-world task are done in the same machine, varying the\nalgorithm and the random seeds. Depending on the machine and on the prediction horizon and other\nhyperparameters, each training run takes approximately 2 hours for the BAIR dataset experiments,\nand 4 to 8 hours in the Meta-world experiments. In terms of VRAM - and for the chosen batch sizes\nBAIR dataset experiments require approximately 11GB, and the Meta-world experiments require\n2.6GB."}, {"title": "1. Claims", "content": "Question: Do the main claims made in the abstract and introduction accurately reflect the\npaper's contributions and scope?\nAnswer: [Yes]\nJustification: The abstract and introduction clearly state the claims presented in the paper,\nwhich are then explored in the Approach and Experiences sections. In these sections, we\npresent qualitative and quantitative evidence to support our claims.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper."}, {"title": "2. Limitations", "content": "Question: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: our experiments are divided into two subsets. In each of them, we present some\nof the limitations we find in the presented method. These limitations are further detailed in\nthe conclusions and discussion section.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren't acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations."}, {"title": "3. Theory Assumptions and Proofs", "content": "Question: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\nJustification: Though we present the loss functions used to train the model we propose, these\ndo not justify any theoretical results, as they are commonly used loss terms, found in the\nliterature.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022 The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced."}, {"title": "4. Experimental Result Reproducibility", "content": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes", "nJustification": "Regarding the data", "results.\nGuidelines": "n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments", "reviewers": "Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation"}]}