{"title": "Ego-Foresight: Agent Visuomotor Prediction as Regularization for RL", "authors": ["Manuel S. Nunes", "Atabak Dehban", "Yiannis Demiris", "Jos\u00e9 Santos-Victor"], "abstract": "Despite the significant advancements in Deep Reinforcement Learning (RL) observed in the last decade, the amount of training experience necessary to learn effective policies remains one of the primary concerns both in simulated and real environments. Looking to solve this issue, previous work has shown that improved training efficiency can be achieved by separately modeling agent and environment, but usually requiring a supervisory agent mask. In contrast to RL, humans can perfect a new skill from a very small number of trials and in most cases do so without a supervisory signal, making neuroscientific studies of human development a valuable source of inspiration for RL. In particular, we explore the idea of motor prediction, which states that humans develop an internal model of themselves and of the consequences that their motor commands have on the immediate sensory inputs. Our insight is that the movement of the agent provides a cue that allows the duality between agent and environment to be learned. To instantiate this idea, we present Ego-Foresight, a self-supervised method for disentangling agent and environment based on motion and prediction. Our main finding is that visuomotor prediction of the agent provides regularization to the RL algorithm, by encouraging the actions to stay within predictable bounds. To test our approach, we first study the ability of our model to visually predict agent movement irrespective of the environment, in real-world robotic interactions. Then, we integrate Ego-Foresight with a model-free RL algorithm to solve simulated robotic manipulation tasks, showing an average improvement of 23% in efficiency and 8% in performance.", "sections": [{"title": "1 Introduction", "content": "While it usually goes unnoticed as we go about our daily lives, the human brain is constantly engaged in predicting imminent future sensori inputs [1]. This happens when we react to a friend extending their arm for a handshake, when we notice a missing note in our favorite song, and when we perceive movement in optical illusions [2]. At a more fundamental level, this process of predicting our sensations is seen by some as the driving force behind perception, action, and learning [3]. But while predicting external phenomena is a daunting task for a brain, motor prediction [4] - i.e. predicting the sensori consequences of one's own movement - is remarkably more simple, yet equally important. Anyone who has ever tried to self-tickle has experienced the brain in its predictive endeavors. In trying to predict external (and more critical) inputs, the brain is thought to suppress self-generated sensations, to increase the saliency of those coming from outside - making it hard to feel self-tickling [5]."}, {"title": "2 Related Work and Background", "content": "Learning agent representations The notion of distinguishing self-generated sensations from those caused by external factors has been studied and referred to under different terms, and with a broad range of applications as motivation. Originating in psychology and neuroscience, with the study of contingency awareness [13] and of sensorimotor learning (motor prediction) [17] [4], in the last few years this concept has seen growing interest in AI, as an auxiliary mechanism for learning.\nIn developmental robotics, Zhang and Nagai [18] have focused on this problem from the standpoint of self-other distinction, by employing 8 NAO robots observing each other executing a set of motion primitives, and trying to differentiate self from the learned representations. Lanillos et al. [19] note that to answer the question \"Is this my body?\", an agent should first learn to answer \"Am I generating those effects in the world\". Their robot learns the expected changes in the visual field as it moves in front of a mirror or in front of a twin robot and classifies whether it is looking at itself or not. Our approach is somewhat analogous to these works, in the sense that we identify as being part of the agent that which can be visually predicted from the future proprioception states, while the robot moves. In a related, but reverse direction, Wilkins and Stathis [20] propose the act of doing nothing as a means to distinguish self-generated from externally-generated sensations.\nStill in robotics, the idea of modeling the agent has connections with work in body perception and visual imagination for goal-driven behaviour [21] as well as self-recognition by discovery of controllable points [22] [23]. Another application that has been explored is the learning of modular dynamic models, that decouple robot dynamics from world dynamics, allowing the latter to be reused between robots with different morphologies. Hu et al. [11] propose a method for zero-shot policy transfer between robots, by taking advantage of robot-specific information - such as the CAD model - to obtain a robot mask from which future robot states can be predicted, given its dynamics. These future states are then used solve manipulation tasks using model-based RL. Finally, this concept has also been used for ignoring changes in the robot as a means for measuring environment change, with the intention of incentivizing exploration in real household environments [12].\nIn machine learning (ML), the distinction between the agent and environment has been studied under the umbrella of disentangled representations, a long-standing problem in ML [24]. While most works take an information theoretic approach to disentangled representation learning [25] [26] [27], some try to take advantage of known structural biases in the data, which is particularly relevant for sequential data, as it usually contains both time-variant and invariant features [28]. In video, this allows the disentanglement of content and motion [29]. Denton and Birodkar [30] explore the insight that some factors are mostly constant throughout a video, while others remain consistent between videos but can change over time to disentangle content and pose. The scene encoder of our method is based on the model of Denton and Birodkar, but while they use an adversarial loss to model pose as a property that doesn't depend on the video, we take advantage of the notion of agency and use the proprioceptive signal to model the visuomotor mapping with a simple reconstruction loss.\nFinally, agent-environment disentanglement can also be achieved through attention mechanisms [31] or from explicit supervision, as shown by Gmelin et al. [10]. In this work, the authors demonstrate that learning this distinction allows RL algorithms to achieve better sample-efficiency and performance, serving as the most direct baseline for our work.\nRL from Images Over the last decade, work in Deep RL from images, where environment representations are learned from high-dimensional inputs, has allowed RL agents to solve problems"}, {"title": "3 Approach", "content": ""}, {"title": "3.1 Episode Partition", "content": "In a real-world scenario, in which the agent is a robot, it would be reasonable to assume that there is access to a camera video stream and, for each planned action sequence, the sequence of future expected proprioceptive states in the form of joint angles and Cartesian position of the end-effector, obtained from forward kinematics. Hence, we define that our dataset is composed of N episodes Ti of a fixed length L (Figure 3), which include a sequence of RGB frames x and the corresponding proprioceptive states p of the agent T_i = {{(x_0, p_0), ..., (x_L, p_L)}_i, i = 1, ..., N. During training, we randomly select a"}, {"title": "3.2 Motion-Based Agent-Environment Disentanglement", "content": "To model future visual configurations of the robot, we propose an encoder-decoder model (Figure 4) that has two encoder streams and one decoder head. The first encoder produces a representation for the visual content of the scene h_5, obtained from the context frames \u00e6t_{c:t}. The second encoder creates a feature representation hp for the sequence of proprioceptive states p_{t:t+h} of the agent. These two representations are concatenated and decoded into the future frame observation 2_{t+h}. This formulation results in the following reconstruction loss term:\n\nL_{rec}(E_s, E_p, D) = ||D(h_s, h_p) - x_{t+h} ||^2,\n\nwhere E_s, E_p and D are neural networks for the scene encoder, proprioception encoder and decoder. For brevity, we only include the reconstruction loss for the last time step.\nWe start by setting the assumption that, for the most part, the scene content varies slowly over time, and remains mostly the same during the same episode, even though it can change from episode to episode. Hence, we adopt the similarity loss from Denton and Birodkar [30], which encourages the scene representation hs to be the same when coming from frames of the same episode, while removing any dynamical information that could be extracted from the context\n\nL_{sim}(E_s) = ||E_s(x_{t_c:t}) \u2013 E_s(x_{t_c+m:t+m})||_2^2 = ||h_s \u2013 h_s^{t+m})||^2,\n\nwhere m is a time gap chosen at random. The complete training objective is then\n\nL_{ef} = L_{rec} + \u03b1L_{sim},\n\nwhere \u03b1 controls the weight of the similarity loss term, and should be adjusted according to how much the content of a scene is expected to change, allowing the initial assumption to be relaxed."}, {"title": "3.3 Agent Visuomotor Prediction as Regularization for RL", "content": "To test how our model affects sample-efficiency in RL, we implement it together with DDPG [36]. The episodes stored in the replay buffer allow us to maintain the approach described in section 3.2 while jointly training our model and learning the policy.\nSimilarly to Gmelin et al. [10], the Actor and Critic neural networks of DDPG receive the low dimensional feature vector coming from the encoders, which is the concatenation of hs and hp. To jointly train our model and the RL algorithm, we optimize our model together with the critic loss of equation 1, using the following extended loss function\n\nL = L_{critic} + \u03b2L_{ef}."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Agent-Environment Disentanglement in Real-World Dataset", "content": "BAIR Dataset Commonly used in video prediction research, the BAIR dataset [16] consists of a Sawyer robot arm randomly moving and pushing a broad range of objects inside a confined table. It includes 43520 video samples, each 30 frames long, with 64 \u00d7 64 resolution. Details on the proprioception state and training with BAIR can be found in App. A.1.\nArchitecture In experiments with the BAIR dataset, the scene encoder Es and the decoder D are based on the DCGAN [52], which consists of 5 convolutional layers, each reducing (in Es) or increasing (in D) the size of the feature map by half. Because we provide a sequence of context frames as input to Es, we interleave two ConvLSTM layers [53]. Another change relative to DCGAN is including skip connections from the Es to the D, as done in the U-Net [54]. For the proprioception encoder, we use an LSTM preceded by 2 fully connected layers. More details can be found in A.2.\nResults In Figure 5, we show two sample predictions from Ego-Foresight on the BAIR dataset. The model succeeds in separating visual information that is part of the robot from the scene, predicting the trajectory of the robot's arm according to its true motion, which shows that the model correctly learned the visuomotor map between proprioception and vision. The background, including moving objects, is reconstructed in their original position. Even so, our model still predicts that some change should happen when the arm passes by an object, and it often blurs objects that predictably would have moved. Nevertheless, it should be noted that the agent's actions and object movement are inherently correlated and therefore can't be completely disentangled."}, {"title": "4.2 Simulated Robot Manipulation Tasks", "content": "Meta-world Benchmark Meta-world is a simulated benchmark [55] of robotic manipulation tasks with predefined reward functions and implemented on MuJoCo [56]. It presents a broad distribution of tasks which require a diverse set of skills such as reaching, picking, placing, or inserting. We chose 10 that involve the manipulation of different objects. More details can be found in App. A.1.\nBaselines We compare our model with two baselines: DrQ-v2 [35] and SEAR [10], introduced in section 2. DrQ-v2 uses an encoder to embed the RGB frames into a feature vector which models the full environment. SEAR expands on top of DrQ-v2, by splitting the feature representation into agent and full environment information, which is achieved with a supervisory mask. SEAR is therefore the closest baseline to our model. The results for both baselines are obtained using the source code provided by the authors of each paper. We implement our model as an extension to the code of Gmelin et al. [10], which in turn is based on the source code of Yarats et al. [35].\nArchitecture Though the architecture used in the meta-world experiments follows the same one described in section 4.1, we add some changes for a better comparison with the baselines, which are detailed in A.2. The size of the feature vectors is also chosen to be close to SEAR. For a fair comparison with the baselines, we perform the motor-babbling stage (50k steps) from scratch for each task, instead of going through it only once and then starting learning the tasks from a model pre-trained on babbling.\nQuantitative Results To compare our model against the baselines, in Figure 7 we plot the mean success rate as a function of environment steps for 10 tasks. Environment steps represent the number of times the environment is updated according to an action which, because we use an action repeat of 2, is always double the number of actions taken by the actor. Environment updates represent a significant computational cost, therefore being the preferred way of reporting sample-efficiency in the literature [35] [39]. Each curve presents the mean over 10 evaluation episodes, taken every 5k steps. Furthermore, in trying to reduce the instability of DDPG we also average the performance over 3 random seeds and display the range of success (best and worst case) in the shaded area of the plot.\nFrom the plots of Figure 7 and their corresponding summary in table 1, there are two main aspects to take into account: performance and sample-efficiency. In terms of the final performance, we observe that Ego-Foresight plainly achieves the best result in 5 out of the 10 tasks, matching the baselines in 4 others, and with considerable gaps of over 30% in some of the tasks, with an average of +8%. Furthermore, in tasks such as Door Close, Button Topdown, and Hammer, our approach is the only one that solves the task for every random seed (the worst case shown in the range is never 0 at the end) and, in general, presents the narrowest range of success after convergence."}, {"title": "5 Discussion and Future Work", "content": "In this work, we studied how motion can be used to disentangle agent and environment in a self-supervised manner. We integrate our approach with an RL algorithm and evaluate how an auxiliary loss term that penalizes the inability of the agent to recognize and predict its own movement can regularize and improve learning in complex tasks. While our results are encouraging, we believe that further studies with more random seeds, different agents, action spaces, and types of tasks would augment the value of our contributions. Our results also indicate that one benefit of self-supervision is the possibility of adapting to a changing body-schema, in particular when the agent uses tools. In future research, we intend to explore how fast this body-schema adaptation can be done. Another future research direction is studying methods for efficient motor-babbling, and how reusing a model pre-trained on babbling would improve sample-efficiency in multi-task learning."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 BAIR Dataset and Meta-world Benchmark", "content": "The BAIR dataset [16] consists of a Sawyer robot arm, randomly moving and interacting with objects placed inside a box. The set of objects and their disposition changes from video to video, as well as the lighting conditions, providing some variability. For the BAIR dataset, each sequence includes the robot's actions, defined as a 7D array of Cartesian displacement and position of the end-effector and the gripper open/close state. The model is trained by receiving 2 context frames and predicting the following 10. At test time, the prediction horizon is extended to 28, with the context length staying the same. The proprioception is defined as the concatenation of the robot's action and the gripper state.\nFor Meta-world is a simulated benchmark of robot manipulation tasks, designed for meta-learning and multi-task learning, including more than 50 different tasks. As in BAIR, the agent is a Sawyer robot, but while for BAIR the we use an FPV camera angle, for Meta-world we opt to use a third-person view, as done in [10]. While this means that the number of degrees of freedom that can be observed is higher (the full 9 DoF of the Sawyer), we verify that the model still manages to predict the agent's movement. For the proprioceptive state, we use the 9D array of the robot's joint angles (7 arm joints and 2 fingers) and the 4D end-effector state, which includes the Cartesian position of the gripper and the distance between the fingers. After each for episode the locations of the targets are slightly varied."}, {"title": "A.2 Detailed Architecture", "content": "For the BAIR experiments Es is a convolutional neural network with 7. Layers 3 and 6 are ConvLSTM layers with kernel size 3 and stride 1, with layer normalization and a leaky ReLU activation. The remaining layers are Conv. layers with kernel size 4 and stride 2, except for layer 5 which has stride 1. Each layer uses Batch Normalization and leaky ReLU activations. After each Conv. layer, the number of filters is doubled, starting at 64 in layer 1 and ending with 512 after layer 6. The last layer has number of filters corresponding to the size of hs and a hyperbolic tangent activation. The decoder is a mirror of the encoder, without the recurrent layers, and with input size corresponding to the sum of the sizes of hs and hs. Additionally, skip connections are added between each Conv. layer of"}, {"title": "A.3 Training details", "content": "The lists of hyperparameters used in the BAIR and Meta-world experiments are presented in tables 2 and 3 respectively. Training was split between an NVIDIA RTX3060, RTX2070 or RTX3090. For consistency, all runs for a given Meta-world task are done in the same machine, varying the algorithm and the random seeds. Depending on the machine and on the prediction horizon and other hyperparameters, each training run takes approximately 2 hours for the BAIR dataset experiments, and 4 to 8 hours in the Meta-world experiments. In terms of VRAM - and for the chosen batch sizes - BAIR dataset experiments require approximately 11GB, and the Meta-world experiments require 2.6GB."}, {"title": "NeurIPS Paper Checklist", "content": ""}, {"title": "1. Claims", "content": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes]\nJustification: The abstract and introduction clearly state the claims presented in the paper, which are then explored in the Approach and Experiences sections. In these sections, we present qualitative and quantitative evidence to support our claims."}, {"title": "2. Limitations", "content": "Question: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: our experiments are divided into two subsets. In each of them, we present some of the limitations we find in the presented method. These limitations are further detailed in the conclusions and discussion section."}, {"title": "3. Theory Assumptions and Proofs", "content": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\nAnswer: [NA]\nJustification: Though we present the loss functions used to train the model we propose, these do not justify any theoretical results, as they are commonly used loss terms, found in the literature."}, {"title": "4. Experimental Result Reproducibility", "content": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: Regarding the data, our results are obtained on a publicly available dataset and on an open source simulator. As for the architecture, we detail our choices and the hyperparameters and random seeds used. Furthermore, the source code will be made available once it is ready for disclosure. We also detail the computational resources used for obtaining the results."}, {"title": "5. Open access to data and code", "content": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\nAnswer: [Yes]\nJustification: Though the code is not ready for release at the date of the deadline, in the text we provide a link for a repository where it will be made available in the following weeks, with instructions for reproducing the results. This repository is anonymized."}, {"title": "6. Experimental Setting/Details", "content": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\nAnswer: [Yes]\nJustification: We provide this descriptionin the Experiments section, and in more detail in the Appendices."}, {"title": "7. Experiment Statistical Significance", "content": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\nAnswer: [Yes]"}, {"title": "8. Experiments Compute Resources", "content": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\nAnswer: [Yes]\nJustification: We detail the computing resources used, along with the memory required to run the experiments and estimates of the times of execution."}, {"title": "9. Code Of Ethics", "content": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The research strictly conforms with the NeurIPS code of ethics."}, {"title": "10. Broader Impacts", "content": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\nAnswer: [NA]\nJustification: We don't envision the work as potententially having any short term societal impacts on its own. It proposes new techniques to advance the state of the art in artificial intelligence, which as a field could have societal impact. We also don't envision any ways in which the proposed method could be used with intention of causing harm."}, {"title": "11. Safeguards", "content": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\nAnswer: [NA]\nJustification: We don't believe our paper poses any risk."}, {"title": "12. Licenses for existing assets", "content": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\nAnswer: [Yes]\nJustification: We use code from previous work which is availble under the MIT License. We also use a publicly available dataset licensed under CC BY 4.0 and an open-source simulator, licensed under Apache 2.0. We reference the creators of each."}, {"title": "13. New Assets", "content": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\nAnswer: [Yes]\nJustification: We will make the source code available, with documentation that allows experiments to be reproduced."}, {"title": "14. Crowdsourcing and Research with Human Subjects", "content": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\nAnswer: [NA]\nJustification: The paper does not involve croudsourcing or experiments with human subjects."}, {"title": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects", "content": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\nAnswer: [NA]\nJustification: The paper does not involve crowdsourcing nor research with human subjects."}]}