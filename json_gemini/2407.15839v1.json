{"title": "Importance Sampling-Guided Meta-Training for Intelligent Agents in Highly Interactive Environments", "authors": ["Mansur M. Arief", "Mike Timmerman", "Jiachen Li", "David Isele", "Mykel J. Kochenderfer"], "abstract": "Training intelligent agents to navigate highly interactive environments presents significant challenges. While guided meta reinforcement learning (RL) approach that first trains a guiding policy to train the ego agent has proven effective in improving generalizability across various levels of interaction, the state-of-the-art method tends to be overly sensitive to extreme cases, impairing the agents' performance in the more common scenarios. This study introduces a novel training framework that integrates guided meta RL with importance sampling (IS) to optimize training distributions for navigating highly interactive driving scenarios, such as T-intersections. Unlike traditional methods that may underrepresent critical interactions or overemphasize extreme cases during training, our approach strategically adjusts the training distribution towards more challenging driving behaviors using IS proposal distributions and applies the importance ratio to de-bias the result. By estimating a naturalistic distribution from real-world datasets and employing a mixture model for iterative training refinements, the framework ensures a balanced focus across common and extreme driving scenarios. Experiments conducted with both synthetic dataset and T-intersection scenarios from the InD dataset demonstrate not only accelerated training but also improvement in agent performance under naturalistic conditions, showcasing the efficacy of combining IS with meta RL in training reliable autonomous agents for highly interactive navigation tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous agents are often tasked with navigating complex, highly interactive environments, such as congested and unsignalized intersections [1]\u2013[6]. The direct training of these agents using a naturalistic distribution of driving scenarios is notably inefficient due to the imbalanced frequency of scenarios-common scenarios are overrepresented while critical, interactive scenarios are rare yet essential for training [7], [8].\nIt is an open question of how to best train agents when the distribution associated with their target operation is unknown [9], [10]. Recent developments have highlighted the effectiveness of guided meta reinforcement learning (RL) in training agents to handle a variety of interactive driving situations [11]. Here, the core strategy involves training the ego agent to interact with a range of social agents, each governed by uniquely parameterized reward functions. These functions are designed to emulate a broad spectrum of cooperative or adversarial behaviors, thus aiming to mimic and challenge human-like driving patterns to increase the robustness of the ego agent's policy against naturalistic driving conditions. Nonetheless, this method tends to overemphasize extreme scenarios that might not be as frequent in real-world driving but are sampled disproportionately during training, leading to performance degradation for more common driving conditions [11], [12].\nTo address this issue, strategies such as oversampling and undersampling have been discussed in the literature [13], [14]. Implementing these methods in an online training environment remains challenging. Techniques from variance reduction, such as using control variates for efficient gradient estimation [15] and importance sampling (IS) for efficient evaluation sampler have been proposed. The main idea of IS in this scheme is to skew the sampling towards extreme scenarios and then adjust the outcomes with importance ratios for an unbiased result. While IS-based approaches have shown appealing results, these approaches are typically applied in evaluation phases [16]\u2013[18]. Whether these approaches could be adapted to enhance training remains largely underexplored.\nThis study introduces a framework that employs IS both during training and evaluation to mitigate the challenge of overemphasis on extreme scenarios. The proposed training framework integrates a guided meta RL training approach with IS, optimizing the training distribution to efficiently sample critical interactive scenarios without disproportionately emphasizing these scenarios. The IS-optimized training approach strategically biases sampling towards more intense driving situations using an IS proposal derived through the cross-entropy method [19], then computes the importance ratio based on this proposal and the underlying naturalistic distribution to provide an unbiased reward estimate during training. We validate our approach using real-world data from T-intersection scenarios in the InD dataset [20], demonstrating that it not only enhances the training process but also maintains a balanced emphasis on both common and critical driving scenarios.\nIn summary, our contributions are threefold:\n\u2022 We integrate the IS proposal, often used for evaluation, into an optimized training distribution using both cross-entropy and mixture models.\n\u2022 We introduce an IS-guided meta RL policy training framework that effectively balances the training for both common and critical driving scenarios, enhancing the adaptability and efficacy of the training process by reflecting real-world driving dynamics accurately.\n\u2022 We demonstrate through simulation with probability distributions learned from a real-world dataset that our approach significantly improves the generalizability of the trained policies across diverse driving situations.\nThese contributions collectively enhance the adaptability and efficacy of autonomous driving agents, paving the way for safer and more reliable autonomous vehicle operations in highly interactive real-world conditions.\nThe remainder of the paper is structured as follows. Section II provides a brief review of the related work. Section III details our methodological approach, followed by Section IV that describes our experimental settings. Section V discusses our findings, concluding with Section VI that summarizes our study."}, {"title": "II. RELATED WORK", "content": "This section briefly reviews earlier work in policy learning, IS-based evaluation techniques, and interactive autonomous navigation tasks."}, {"title": "A. Policy Learning for Autonomous Navigation", "content": "A broad spectrum of planning and policy learning frameworks have been proposed, evolving from earlier models such as simple Markov Decision Processes (MDPs) [21] to more sophisticated structures like Partially Observable MDPS (POMDPs) [22], [23] and deep RL methods [6], [24]. These frameworks have contributed significantly to the development of autonomous driving policies by enabling the modeling of uncertainty and partial observability, which are crucial in dynamic driving environments. Training diverse policies under these frameworks is achieved through reward randomization techniques [25], [26], resulting in policies capable of handling complex scenarios such as navigating through uncontrolled intersections. Additionally, adversarial training methods have been incorporated to further strengthen these policies against uncertainties and adversarial conditions [27].\nDespite these advancements, effectively addressing rare critical scenarios remains a challenge. Techniques such as conditional flows [28], adaptive scenario-set sampling [29], [30], and bridge sampling [31] have been developed to enhance the sampling of these scenarios. These methods aim to improve the robustness and reliability of autonomous driving policies by ensuring that they can handle a wide range of potential driving situations. However, the complexity of these methods poses significant integration challenges when used during the training phase. There is also a persistent risk that policies may become excessively biased towards rare, extreme cases, potentially compromising the agent's overall performance in more common driving scenarios [11]."}, {"title": "B. IS-based Evaluation Approaches", "content": "Importance Sampling (IS) has been used to prioritize sampling in various applications [32], [33] and has emerged as a crucial technique in evaluating the effectiveness of autonomous agent policies under rare and extreme scenarios. IS achieves this by sampling aggressive scenarios more frequently and applying likelihood ratios to ensure unbiased performance estimations [17], [19], [34], [35]. The aggressive scenario distribution used in IS, referred to as the IS proposal distribution, can be refined through methods like cross-entropy (CE) [17], [19], dominating point analysis with mixture models [18], [36], and failure-based normalizing flows [37].\nWhile previous studies have shown the potential of addressing rare, extreme scenarios during planning [38], a significant challenge remains on how to effectively bridge the gap between evaluation techniques and actual policy training. This disconnect often results in trained policies that do not fully align with real-world demands. Effective integration of IS-based evaluation approaches with policy learning methods is crucial to developing robust autonomous driving systems. There is a need for integrated approaches that can translate these evaluation techniques into practical training methodologies, ensuring that trained policies are not only theoretically sound but also practically viable in real-world driving conditions."}, {"title": "C. Autonomous Agents in Highly Interactive Environments", "content": "Designing autonomous agents capable of operating in highly interactive environments has been a major focus of recent research. Traditional approaches often rely on predefined rules and reactive behaviors, which are insufficient for handling the complexities and uncertainties present in real-world scenarios. Advanced frameworks such as interactive POMDPS [39], [40] and multi-agent reinforcement learning [41], [42] have been developed to address these challenges by considering the actions and behaviors of other agents in the environment.\nRecent studies have also explored the use of social dynamics and negotiation strategies to enhance the interaction capabilities of autonomous agents. For instance, game theory can be used to model interactions among multiple agents allowing for the prediction and anticipation of other agents' actions, leading to more effective decision making processes [43]\u2013[46]. Additionally, incorporating techniques such as imitation learning and behavior cloning has enabled agents to learn from human demonstrations, thereby improving their ability to interact seamlessly with human drivers and pedestrians [47], [48].\nIn this work, we design a framework that can robustly handle highly interactive and dynamic environments without sacrificing its nominal performance. Existing approaches often struggle with scalability and generalization, particularly when dealing with rare, critical interactions or highly unpredictable scenarios. Our research stands out by proposing a framework that integrates IS methods with meta RL to enhance the ability to navigate complex environments and effectively interact with other agents."}, {"title": "III. FRAMEWORK", "content": "In this section, we present the proposed framework that integrates IS in both policy evaluation and training for autonomous driving. The framework uses an optimized IS proposal to enhance both the evaluation and subsequent training efficiency of autonomous driving agents. This dual application of IS is pivotal in generating critical scenarios that are not only essential for robust policy assessment but also beneficial for iterative policy enhancement."}, {"title": "A. Modeling and Objective", "content": "We model the driving scenario as a partially observable stochastic game, where the interaction dynamics are described using the interactive driving model from Lee et al. [11]. At any given time $t$, the scenario is defined by a state $s_t$. The objective for the ego policy $\\pi_{ego}$ is to maximize its expected cumulative reward over time, formulated as:\n$\\pi_{ego}^* = arg\\ max_{\\pi\\in \\Pi_{ego}} E \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, \\pi(s_t))$\nwhere $R$ represents the reward function for the ego vehicle, $\\gamma$ is the discount factor, indicating the decreasing importance of future rewards, and the expectation is taken over state transitions. The ego policy $\\pi$ maps the state space $S$ to the action space $A_{ego}$, with $\\Pi_{ego}$ representing the feasible policy set for the ego agent."}, {"title": "B. Social and Meta-Policy Training", "content": "The social agents are modeled with a policy $\\pi_{social}$, parameterized by $\\beta$, which indicates the level of aggressiveness of the social agents. The policy for each social agent is optimized:\n$\\max_{\\pi \\in \\Pi_{social}} E \\sum_{t=0}^{\\infty} \\gamma^t (r^{goal}(s_t, a_{social,t}) + \\beta r^{speed}(s_t, a_{social,t}))$,\nwhere $a_{social,t} = \\pi_{social,\\beta}(s_t)$, and $r^{goal}$ and $r^{speed}$ are the reward components for achieving the goal and maintaining speed, respectively."}, {"title": "C. Training the Ego Policy", "content": "The ego policy $\\pi_{ego}$ is trained against the backdrop of these diverse social policies. We consider several strategies for the training distribution of $\\beta$, denoted by $P_{training}$, to prepare the ego policy for a spectrum of social behaviors.\na) Generalized Ego Policy (GEP): Uses a uniform distribution $U(\\beta_{min}, \\beta_{max})$ for $P_{training}$ to prepare the ego policy for a wide range of social behaviors, potentially at the cost of overfitting to less common aggressive behaviors.\nb) Generalized with IS (GIS): Similar to GEP, but uses importance sampling weight to account for the rarity of the training scenarios.\nc) Naturalistic Ego Policy (NEP): Uses a distribution $P_{naturalistic}$ derived from real-world driving data to focus on common social behaviors, potentially neglecting rare but critical scenarios.\nd) Cross Entropy Importance Sampling (CEIS): Adopts an optimized proposal distribution $P_{CEIS}$ for $P_{training}$, aiming for a balanced approach that covers both common and rare scenarios. The training objective for the ego policy under this approach is formulated as:\n$\\max_{\\pi \\in \\Pi_{ego}} E \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, \\pi(s_t)) \\cdot \\frac{P_{naturalistic}(\\beta)}{P_{CEIS}(\\beta)}$,\nwhere the importance weight adjusts for the discrepancy between the naturalistic and optimized distributions."}, {"title": "D. Evaluating the Ego Policy", "content": "The evaluation of $\\pi_{ego}$ is designed to mirror realistic conditions, specifically focusing on the policy's effectiveness in managing mission-critical failures such as crashes or delays at intersections. We use the CE method to refine the IS proposal distribution $P_{evaluation}$, which is aimed at generating highly informative and challenging scenarios for robust evaluation."}, {"title": "IV. EXPERIMENTS", "content": "The goal of the experiment is to benchmark the performance of the proposed training method (CEIS) against baseline methods (GEP and NEP). In addition, we also compare a generic IS-based method (GIS). In the first experiment, we use a toy example to illustrate how the proposed framework handles synthetic rare events. In the second experiment, we show how the proposed framework is versatile and can be used when the naturalistic distribution is fit from a real-world driving dataset."}, {"title": "A. Synthetic Naturalistic Distribution Example", "content": "In this example, we let $P_{naturalistic} = N(1.5, 0.5^2)$ and $P_{training} = N(0.5, 0.5^2)$, a setting that allows the ego policy to be trained on a larger proportion of dangerous scenarios (lower \u03b2 values) compared to what would have been observed from the naturalistic distribution.\nWe train four ego policies: GEP and GIS (using $P_{uniform}$), NEP (using $P_{naturalistic}$), and CEIS (using $P_{training}$) with K = 1 (no iterative training). We use $P_{naturalistic}$ to generate $\\beta$, simulating naturalistically random social vehicle behavior. Each policy is evaluated over a total of $N_s$ = 5,000 samples. This experiment is meant to study whether a different training distribution leads to different performance results under naturalistic evaluation conditions. The GEP is used as a baseline as in the original paper [11].\nIn Table I, we report the number of successes (the ego vehicle passed the intersection without collision), collisions, and time-out (the ego vehicle did not cross the intersection to avoid collisions). We also tested 10 other combinations of $\\mu \\in \\{-0.5, 0.5, 1.5, 2.5\\}$ and $\\sigma \\in \\{0.5, 0.75, 1.0\\}$ for Gaussian $P_{training} = N(\\mu, \\sigma^2)$ (not running CE and mixture model in Algorithm 1) and summarized the result in Table II."}, {"title": "B. Real-World Naturalistic Distribution", "content": "In this experiment, we stress-test all the benchmarks using a naturalistic distribution fit from a real-world dataset. In this case, we use a subset of the InD intersection dataset [20], specifically the T-intersection cases (Heckstrasse, Aachen and Neukollner Strasse, Aachen). Given the trajectories of all vehicles in these datasets, we adopt the maximum entropy inverse reinforcement learning [49] to learn the underlying $\\beta$ for each recorded vehicle assuming the underlying reward function is of the form in Equation 2. This estimation process aims to find the corresponding $\\beta$ that maximizes the reward given the vehicle trajectory, assuming the probability of the trajectory is proportional to the exponential of the reward. To this end, we adopt the maximum-entropy approach proposed by Huang et al. [49].\nGiven the estimated $\\beta$ samples, we then fit a kernel density estimate to obtain the smoothened density. Figure 2 shows this density for T-intersection #1 (Heckstrasse, Aachen) and T-intersection #2 (Neukollner Strasse, Aachen). It is interesting to see that vehicles in T-intersection #1 are generally more aggressive (lower \u03b2 values) compared to those in T-intersection #2 (higher \u03b2 values), which might be due to the shape of the T-intersection #1 that allows turning at faster speed, and thus more quickly crossing the intersection.\nWith this naturalistic distribution, we then compare the four benchmarks (GEP, GIS, NEP, and CEIS). In this case, GEP and GIS use U(-1, 3), similar to the previous example. NEP uses the fitted KDE at T-Intersection #1 as $P_{training}$. CEIS uses the best-fitted Gaussian from the fitted KDE at T-Intersection #2 (which in this case turns out to be N(1.8,0.1922) as $p_0$ in Algorithm 1. Both GEP and NEP are trained only once (since no iteration is needed) while CEIS is trained with K = 3 (running CE offline to optimize $P_{evaluation}$ and using GMM approach for the subsequent $p_k$'s as shown in Algorithm 1). The result is summarized in Table III. We further project the CEIS performance over more iterations and show the summary in Figure 4."}, {"title": "V. DISCUSSION", "content": "We discuss our findings in terms of the CEIS performance over the baselines, its generalization capability especially under naturalistic settings, and its improvement over training iterations."}, {"title": "A. Dominant IS-based Performance Over the Baselines", "content": "In both experiments, the IS-based algorithms (the GEP and NEP) consistently outperformed their counterparts across success rates and collision rates (see Table I and Table III). Their performance is slightly lower in terms of timeout rates. Notably, GIS achieves an 88.0% success rate and 10% collision rate in the synthetic examples, while CEIS achieved an 81.1% success rate and 10.8% in collisions (over +10% improvement in success rates vs non-IS-based approaches). Meanwhile, NEP achieves only 68% and GEP only 66% success rates. The collision rate for GEP and NEP is 33% and 12%, respectively.\nThese results show the efficiency of the IS-based approach in training more capable and safer autonomous agents (lower collision rates). The only slight downside is in the time-out rate, where the GEP dominates with a 0.6% timeout rate, highlighting its focus on finishing the task (either successfully or not). We argue, however, that in many robotics settings, it is more desirable to have an agent that is not only focusing on just finishing the task (while endangering its platform and surroundings) but rather to have a safe agent that avoids risking catastrophic costs. To that end, the IS-based approach seems to be capable of reasoning about the risk better by considering the likelihood of any risky maneuver when making decisions (see a visual example in Figure 3). This is the strength of IS that we leverage in this work. By evaluating performance across a mix of common and rare cases, IS enables the training to adapt more appropriately to the dynamics of real-world driving scenarios. This methodological advantage translates into an agent that is not only highly effective in common scenarios but also capable of handling unexpected or extreme situations with reasonable performance."}, {"title": "B. Better Results in Naturalistic Settings", "content": "While GIS tends to outperform CEIS in the synthetic example, we observe the CEIS' adaptability in the naturalistic case. This is evident from its consistent performance improvements, even when initial training distributions are derived from different intersections (while the GIS slightly underperforms). For instance, when trained with data from diverse intersections, CEIS showed a 6% increase in success rate over the best baseline, illustrating its ability to generalize well across different traffic scenarios.\nThis observation is also supported by Table II, where a more concentrated training distribution tends to give higher rewards. Combined with its iterative training scheme, CEIS can improve over time based on the initial successes, suggesting that continuous learning and adaptation are feasible and effective, providing a pathway to even more sophisticated and refined driving policies. This characteristic is crucial for the deployment of autonomous agents in highly interactive driving environments."}, {"title": "C. Trend of Improving Performance Over Time", "content": "A longitudinal evaluation of CEIS apparent from Figure 4 reveals a positive trend of incremental performance gains over successive training iterations. Specifically, the success rate improved from 83% in the first iteration to 98% (projection), with a noted decrease in collision rates from 10% to 0.5% in 10 training iterations. This trend illustrates the effectiveness of combining CE methods with a GMM to bridge guided $P_{evaluation}$ to $P_{training}$, ensuring balanced and comprehensive training. However, the tapering off of success rates beyond the fourth iteration-from 95% to a seemingly asymptotic 98%-highlights a potential ceiling in performance improvements, suggesting diminishing returns as the training process approaches optimization saturation. This limitation is an essential area for further investigation, suggesting that CEIS might try to address the full spectrum of risky scenarios (including those impossible to mitigate in the first place, see Figure 5), prompting the exploration of training algorithms that can distinguish such cases and exclude them during training. Another interesting future direction from this study is exploring how to design conditional deployment specifications based on the meta-guide model. This will be vital to ensure the safe deployment of robots in evolving real-world scenarios."}, {"title": "VI. CONCLUSION", "content": "This research investigates a framework that integrates IS approaches in both the training and evaluation of autonomous agents under complex interactive scenarios. By integrating guided meta reinforcement learning with CE through the IS framework, the training approach adapts efficiently to both common and rare driving conditions. The experimental results using both synthetic and real-world data highlight the potential for substantial improvements in agent performance, particularly in handling interactive T-intersections, with success rates improving significantly across iterative training sessions. Our findings highlight compelling evidence of the efficacy of the integrated IS framework in enhancing the performance of the ego agents over time. Future work will focus on characterizing the diminishing training improvement over time and identifying useful scenarios for training (e.g. by identifying and omitting unavoidable failure cases) to further advance the development of safe autonomous agents."}]}