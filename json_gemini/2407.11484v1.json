{"title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models", "authors": ["Nuo Chen", "Y.Wang", "Yang Deng", "Jia Li"], "abstract": "This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded to embrace complex character portrayals involving character consistency, behavioral alignment, and overall attractiveness. We provide a comprehensive taxonomy of the critical components in designing these systems, including data, models and alignment, agent architecture and evaluation. This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement.", "sections": [{"title": "1 Introduction", "content": "Today, most large language models (LLMs) are proficient enough to act as assistants, but the ever-expanding desires of humans gradually go beyond this role. A helpful but serious assistant isn't everything in human life. An increasing number of individuals have been instructing LLMs to take on roles they desire, such as movie stars, game characters, or even their own relatives. This practice of aligning LLMs with specific personas or characters is commonly known as Role-Playing. If the standard assistant role of LLMs meets the demand for increased productivity, then LLMs for role-playing aims to fulfill human needs at a psychological and entertainment level. This lively trend underscores the versatility of LLMs and the limitless potential of human imagination in the realm of artificial intelligence.\nThe requirements for role-playing with language models differ significantly from those of a generic assistant. The primary expectation from a generic assistant is its helpfulness, meaning the LLM should follow the user's instructions and provide the desired responses. This expectation is also evident in the benchmarks for such models: people often desire an assistant with extensive professional knowledge and strong logical reasoning abilities.\nHowever, when the task comes to role-playing, the most crucial criterion is the LLM's ability to align with specific personas or characters. In other words, humans expect the LLMs to interact with them in a manner consistent with a specific role. This expectation introduces a fascinating dynamic that can sometimes contradict the traditional notion of helpfulness. For instance, consider a scenario where the role to be played is that of the user's adversary or enemy. In such a case, being helpful becomes a contradictory metric. The more helpful the model is, the less effective it becomes at role-playing.\nIn the era marked by the advent of sequence to sequence learning, researchers began exploring the potential of neural networks to generate dialogue responses that are consistent with both the given context and the portrayed persona. This initial motivation laid the groundwork for the following role-playing works. Subsequent advancements with the introduction of models such as BERT brought significant evolutions to the use of language models for role-playing, despite certain inherent limitations. In this period, due to the improved but still developing generative capabilities of these pre-trained language models (PLMs), role-playing is largely focused on achieving persona consistency through simpler, more straightforward persona roles, often termed as persona-based role-playing. This is partly due to the nature of the personalized information available in datasets at the time, which was often succinct and sparse, as seen in resources like the Persona-Chat dataset. PLMs are fine-tuned on these datasets to produce responses that aligned with the limited personal information provided, striving for a balance between consistent understanding and competent dialogue generation.\nAs the field progressed into the era of LLMS, a paradigm shift occurred. The enhanced comprehension and generative abilities of LLMs expand the scope of role-playing tasks far beyond simple persona adherence. Current research in role-playing no longer confines itself to rigid personas. Instead, it delves into more nuanced aspects of role enactment, such as character consistency, behavioral alignment, and overall attractiveness of the character portrayal named as character-based role-playing. These dimensions aim to create more immersive and believable character simulations that maintain continuity over interactions and adapt dynamically to dialogue contexts. The progress of LLM-based role-playing leading to a rapid expansion in academic research and the development of practical applications.\nDespite the promising potential of role-playing with language models, research in this domain remains in its early stages, marked by both complexities and challenges. The goal of this survey is to understand the crucial mechanisms and methodologies that enable role-playing through text-based interactions. To achieve a thorough understanding, we introduce a detailed taxonomy to systematically examine the critical components involved in designing role-playing language models. The proposed taxonomy includes: Data, Models & Alignment, Agent Architecture, and Evaluation. This framework aims to not only detail how role-playing functions within these systems but also to highlight how it can be optimized and evaluated for a variety of applications.\nIn summary, \u00a72 first introduces the preliminary of our survey, like the evolution of language models and key components. \u00a73 provides a comparative overview of the current role-playing data resources, highlighting their unique characteristics and applicability in different scenarios. For models & alignment, \u00a74 systematically reviews the foundational models of role-playing and summarizes the past alignment approaches, offering insights into their strengths and weaknesses. For Agent architecture, \u00a75 details the important modules that impact the effectiveness and generalization of role-playing language model agents, including memory, planning, action. Then, \u00a76 compiles comprehensive evaluation standards and metrics, encompassing both subjective and objective metrics, along with their respective advantages and disadvantages. Finally, \u00a77 delves into the 10 main challenges that persist in role-playing and envisages possible solutions that could pave the way for more advanced and nuanced systems."}, {"title": "2 Background", "content": "2.1 PLMs and LLMs\nDespite the absence of a universally accepted definition for Large Language Models (LLMs), this paper proposes a specific delineation for LLMs as referenced within our analysis. Distinguished by both model scale and training methodologies, our definition builds upon distinctions made by two seminal surveys on LLMs , differentiating LLMs from earlier Pre-trained Language Models (PLMs) based on the magnitude of model parameters and the scope of pre-training data. Specifically, LLMs refer to expansive models with parameters in the billions, pre-trained on extensive datasets, in contrast to PLMs, which are characterized by their relatively moderate parameter sizes in the millions and the capability for efficient task-specific fine-tuning to enhance performance on downstream tasks.\nNotable PLMs include BERT, GPT-2 , BART , and Roberta , whereas leading examples of LLMs, such as GPT-3 , PaLM , Mistral , and LLaMA , primarily utilize decoder-only architectures. The progression in architectural and training innovations for LLMs has facilitated the emergence of advanced capabilities , empowering them to tackle complex problems in few-shot or zero-shot settings through methodologies like in-context learning  and chain-of-thought reasoning . After reading this review, you will find that, in fact, the capabilities of zero-shot and few-shot learning are the main reasons for the increasing attention and interest in the domain of role-playing over the past two years."}, {"title": "2.2 Key Components", "content": "Employing language models for role-playing involves several critical factors that significantly influence their effectiveness and personalization capabilities. In this survey, we analyze several components that are essential in shaping the development, deployment, and continuous improvement of role-playing language models:\n\u2022 Data: The diversity and complexity of the data used in role-playing are foundational, influencing the models' ability to generate authentic and personalized interactions. Role-related information in current datasets ranges from structured texts and simple sentences to rich compilations of detailed personal information like attributes, relationships, and even nuanced understandings of characters across different timelines. The sophistication of a language model in handling role-playing scenarios directly correlates with the complexity of the targeted dataset. Models trained on simpler data might generate broad, generic responses, while those trained on more complex datasets are capable of producing dynamic dialogues that reflect specific character nuances. Hence, the selection and design of the dataset are critical in role-playing, directly impacting effectiveness in delivering engaging and personalized conversational experiences.\n\u2022 Models & Alignment: Undoubtedly, the backbone models play a pivotal role in the role playing's operational efficiency. From traditional neural networks to language models, the choice of model influences the system's understanding, generation, and adaptation capabilities. The recent advent of LLMs has brought significant advancements in this area, achieving high levels of personalization and coherence.\nMeanwhile, alignment approaches focus on ensuring that the role-playing models' responses align with the intended role. These methods range from rule-based systems that manually map responses to personas, to dynamic learning mechanisms that adapt responses based on interaction history and persona data. Technically, we divide them into Parameter-Tuning: Post-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning; and Parameter-Frozen: In-context learning prompting and Retrieval-Augmented Generation (RAG).\n\u2022 Agent Architecture: Currently, the development of Role-Playing Language Agents (RPLAS) marks a new evolution. These agents extend the basic framework of role-playing by integrating both interactive and autonomous behaviors, enabling them to not only personify specific characters but also engage proactively in complex and evolving scenarios. Effective RPLAs require a comprehensive system architecture that includes several key modules: memory, for recalling and utilizing past interactions; planning, for strategic decision-making; and action, which encompasses both generating role-related responses and using tools. Such complex architectures ensure RPLAS are not only interactive but also adaptive and context-aware, essential for complex role-playing scenarios.\n\u2022 Evaluation: Evaluating the performance of role-playing models is crucial for assessing their effectiveness and guiding improvements. Commonly, role-playing evaluation involves a complex array of perspectives that extend beyond those applied to traditional dialogue systems, focusing on role consistency, engagement, human-likeness, and proactivity, among others. This complexity necessitates a diverse set of evaluation metrics, encompassing both subjective human-based assessments and objective reference-based. The advent of LLMs has also spurred the development of LLM-based evaluation methods, offering an efficient alternative to conventional human annotation by approximating human judgments. However, given the multifaceted nature of role-playing, no single metric suffices to fully assess their performance. Thus, a composite approach, utilizing multiple metrics in tandem, is essential for a comprehensive evaluation.\nIn the following sections, we present a comprehensive survey along with our taxonomy."}, {"title": "3 Data", "content": "3.1 Data Objectives\nUnlike traditional text generation tasks, the capabilities of a role-playing language model are primarily determined by the target dataset it is fitted on. Therefore, the dataset is the most crucial prerequisite for training, categorizing, and testing different role-playing dialogue agents. Commonly, role-playing datasets contain two important components: interactions and role-related information. It's worth noting that we use the term \"interactions\" instead of the commonly used \"conversations\" or \"dialogues\". This is because we believe that the essence of role-playing lies in mimicking the behavior of the role in any scenarios, not just in dialogues. The reason why most current research is limited to the conversation-level is that, compared to other scenarios, the user's actions within a conversation are the easiest to obtain.\nIn this study, based on the different objectives of the targeted datasets, we categorize role-playing applications into two types: Persona-based Role-Playing (P-RP) and Character-based Role-Playing (C-RP). Generally, P-RP means that the dataset contains coarse-grained role-related information, whereas building C-RP requires fine-grained role-related information. The classification of granularity of coarse-grained or fine-grained primarily depends on whether the role-related information includes character-level specific background details. Datasets lacking such details are deemed coarse-grained, whereas those containing them are considered fine-grained. Here, we further emphasize the differences:\n\u2022 Persona-based Role-Playing (P-RP): These role-playing scenarios mimic broad categories of personas, focusing primarily on superficial and common attributes like location and gender. They are expected to display characteristics of specific groups of people based on given coarse-gained personas. P-RP is simpler, designed to ensure consistency within a narrower set of persona traits. They are generally less complex and tailored for generic roles that require basic interaction capabilities.\n\u2022 Character-based Role-Playing (C-RP): In contrast, C-RP scenarios are crafted to emulate specific characters from various narratives, such as novels, movies or even celebrities. These involves incorporating fine-grained character-level personal background information, including attributes, complex relationships, scene and nuanced psychological states. C-RP with language models is inherently complex, aimed at achieving deep, role-specific interactions and are equipped with features such as human-likeness, empathy, and proactivity . They are designed to offer a more immersive and engaging user experience.\nIn the evolving landscape of role-playing research, Persona-based Role-Playing (P-RP) is seen as a specific subset within the broader spectrum of Character-based Role-Playing (C-RP). Currently, the focus is increasingly on C-RP, reflecting a shift toward more complex and nuanced scenarios capable of leveraging the sophisticated capabilities of curret LLMs. This trend underscores a growing interest in developing role-playing interactions that offer rich, character-driven experiences. We classify different datasets according to interaction collection in Figure 2 and present overview of existing datasets in Table 1."}, {"title": "3.2 Persona-based Role-Playing Datasets", "content": "In general, the datasets associated with persona-based data tend to provide personas that are coarse-grained. The collection and construction of appropriate training data have become essential prerequisites for enabling chit-chat dialogue agents to generate persona-specific responses.\nInteraction Collection According to how to collect role-related conversations, we can generally classify the data collection process into two main streams:\n\u2022 Employing Crowdsourced Workers: This method involves hiring crowdworkers to create the corresponding dialogue datasets. Initially, certain personas and related topics are manually defined. Then, crowdworkers engage in dialogues based on these provided personas, resulting in personalized dialogues. The advantage of this approach lies in the guaranteed high quality of the data; however, due to the cost of manual labor, the scale is often limited. Notable examples include the Persona-Chat dataset , which contains about 10,000 persona dialogues, and the Focus dataset with approximately 14,000 conversations.\n\u2022 Extracting from Social Media: This method typically involves collecting a large volume of user dialogue data from social media platforms and applying specific filtering rules to obtain the final dataset. The strengths of these datasets are that they reflect real-world personalized dialogues and are large in scale. For instance, PersonalDialog  and Pchatbot have respectively gathered over 20 million and 130 million dialogue sessions from Weibo. However, a significant drawback is the difficulty in controlling data quality.\nIn general, there is a trade-off between the above two collection processes: The former provides high-quality, controlled data at a smaller scale, while the latter offers extensive real-world dialogue data, albeit with potential quality control issues.\nRole-related Information. Role-related information in P-RP datasets is crucial for generating realistic and context-appropriate responses, and it can be categorized into two distinct forms: explicit and implicit. Explicit information refers to instances where each interaction is accompanied by a detailed persona, presented either in a natural language format or a structured format. A prime example of the former, a persona in natural language format, is as follows (sourced from paper):\nRPGs are my favorite genre.\nI also went to school to work with technology.\nThe woman who gave birth to me is a physician.\nI am not a social person.\nAs for the structured format, an example from  is as follows:\n{Age: Post-90s,\nGender: Female,\nLocation: Beijing,\nConstellation: Aquarius}\nTo the best of our knowledge, most real-world role-playing applications incorporate both formats of role-related information. The key-value structured format is utilized to provide common, fundamental details that are necessary for each role, such as height, weight, and gender. On the other hand, the natural language format is employed to convey unique background information, experiences, and catchphrases specific to the role, which are challenging to encapsulate within the confines of a structured format.\nIn the context of implicit personas, as referenced in sources such as , some might contend that they do not fall within the scope of role-playing, given that they do not provide any role information. Such datasets distinguish dialogues belonging to different roles but do not supply persona information for each role. We still consider these works to be part of the role-playing domain, as the role-related information can essentially be inferred by summarizing the interaction history. We categorize these works under \"Implicit persona\". Undoubtedly, utilizing these types of datasets for role-playing is a more challenging task, as we must first devise methods to extract accurate role-related information from the interaction history."}, {"title": "3.3 Character-based Role-Playing Datasets", "content": "As LLMs' comprehension capabilities have improved, both researchers and users have found that coarse-grained personas are no longer sufficient to meet their needs for entertainment and psychological engagement. They have started to use LLMs to 'recreate' their favorite characters, a practice known as Character-based Role Playing. Typically, the role-related information provided by Character-based Role-playing can be a comprehensive role description (comprising thousands of tokens), or it could be corpus-level background materials such as novels or narratives. A pioneering effort in this field is the HPD dataset , a dataset based on the Harry Potter novels, which is used to train LLMs to align with the Harry Potter.\nInteraction Collection Character-based role-playing scenarios involve simulating a broad spectrum of roles, categorized mainly into two categories: real world-based and virtual scenario-based. Real world-based roles often mimic actual celebrities or typical individuals from daily life, while virtual scenario-based roles draw from fictional sources like novels, TV series, video games, and even theoretical constructs such as MBTI personalities . The diversity of these roles necessitates equally diverse methodologies for data collection, each with its unique set of challenges and solutions:\n\u2022 LLM as Data Generator: With the advanced generative capabilities of models like GPT-4, LLMs serve as a primary tool for synthesizing character profiles and dialogues. Related implementations include: 1) Generating complete character profiles from scratch and using these profiles to prompt varied character-based dialogues ; 2) Employing established profiles from resources like MBTI or Wikipedia as a basis for generating personalized dialogues. While effective, LLM-based data generation can introduce biases and unpredictable variations in data quality, necessitating careful manual review and validation .\n\u2022 Extracting from Literary Resources: This method involves the extraction of role-related conversations and character backgrounds directly from literary sources, particularly for fictional characters depicted in novels and television. However, this approach faces several challenges: 1) Dialogues are often tied to specific scenes, making it complex to delineate the scene of the dialogue and to extract the relevant textual context. 2) Automatically extracting detailed background information about characters is difficult. 3) Identifying multiple statements made by a character within a dialogue round can be complex. 4) Some dialogues involve non-verbal cues and contexts that are difficult to convey textually. A common solution involves using LLMs, human annotators, or a combination of both. For instance, Chen et al.  utilized a group of professional annotators and the LLM tools to annotate dialogue, attributes, and relationships from the Harry Potter novels.\nUndoubtedly, the language quality of the interactions extracted from this source is exceptional, and they exhibit the highest degree of alignment, given that the characteristics of the roles are originally defined by these works. However, the primary challenge lies in the significant disparity between the dialogue style in the literaries and the daily user-AI dialogue style. This discrepancy often results in the trained models underperforming in interactions with real users.\n\u2022 Human Role-Playing: Zhou et al. and Zhang et al. hire different crowdworkers who are given specific character profiles to role-play, and then engage in interactions. This data generation method generally produces high-quality data but lacks diversity and is costly.\n\u2022 Unpublished Resources: All the aforementioned acquisition methods are sourced from formal, published academic papers. However, according to our practical experience, solely relying on these datasets is inadequate to train a product-level role-playing model that fulfills user expectations. Consequently, we present a list of higher-quality role-playing data sources. However, it's important to note: we do not guarantee the legality of these data resources, and the developers should confirm any potential legal risks by themselves.\nThe first type of resource is role-play forums, which contain a vast amount of human-human role-playing data. Some well-known forums include Blue Moon, NationStates, Aryion, Questionable Questing, Role-Players, and Spacebattles. It's important to note that these forums often contain adult-only content, rigorous data cleaning is required before use. The second type is the log of some online role-playing products such as CharacterAI. But the use of such data requires dual authorization from both the users and the product developers. The last type is fanfiction communities, such as AO3. For some well-known characters like Harry Potter, the volume of fanfiction is thousands of times that of the original work. However, the risk lies in the fact that there are many Out-of-Character situations in fanfiction, as authors will add many personas according to their own preferences.\nRole-related Information. Character-based role-playing needs character-level fine-grained personal background information, which encompasses a range of elements, as shown in Figure 3. Below is a detailed categorization of the typical personal content found within these datasets:\n\u2022 Specific Scenes: This category captures detailed contextual information about the dialogues, including the precise timing, location, and underlying reasons. Such data is essential for situating the dialogue within a clear narrative frame, thereby providing a more immersive experience.\n\u2022 Comprehensive Attributes: Attributes cover a wide spectrum of personal details about the characters , including the name, gender, personality, age, identity, title, affiliation, quotes, belongings, etc.\n\u2022 Complex relationships: Relationship is another key perspective to make role-playing models delve deeper into the social and emotional landscapes of the specific characters , which contains opponents, familiarity, and family details, etc.\n\u2022 Temporal Personas: This aspect acknowledges that characters are not static; their personal information can evolve over time, reflecting developments in the character's storyline or changes in their environment. This temporal dimension allows for the simulation of growth and transformation, providing a richer narrative .\nEach category of data plays a critical role in constructing nuanced and responsive character-level role-playing models which can interact in a manner that mirrors human-like complexity and depth. Note, not all datasets contain each of the aforementioned elements. For instance, CharacterEval  provides scene and attributes while RoleInteract  contains scene, attributes and relations."}, {"title": "4 Models and Alignment", "content": "4.1 Foundation Models\nFoundation models are critical in setting the base capabilities of role-playing models. As the underlying architectures, they determine the lower bounds of performance and sophistication achievable in role-playing scenarios. The development of foundation models and architectures for role-playing can be viewed as a progressive evolution across three distinct stages: non-pretrained model, PLM, and LLM. Each stage represents a significant shift in backbone selection for role-playing models.\nNon-pretrained models and PLM. The earliest stage in the development of role-playing models involved non-pretrained architectures. These models are crafted from scratch, tailored to specific tasks without the benefit of large-scale, pre-trained data. Early models often utilize bespoke designs such as memory networks or custom transformers, which are specifically engineered to handle the storage and embedding-based fusion of personal information for effective role-playing . These architectures provided highly specialized solutions that were adept within specific contexts but lacked the generalizability and scalability offered by later developments. The shift to PLMs like BERT mark a substantial enhancement in foundational capabilities. These models leverage extensive pre-trained data, enhancing their ability to understand context and generate text, yet they still face limitations in fully grasping role-specific nuances. To overcome these challenges, researchers deploy innovative strategies like contrastive learning , bert-over-bert decouples learning and attention-based fusion mechanisms  improve the integration of personal and dialogue data, enhancing role-playing functionalities.\nLLM. The current frontier in role-playing development is characterized by LLMs such as GPT-4, which boast an unprecedented scale in parameters and pre-training. Such LLMs offer profound improvements in understanding and generating text, capable of maintaining coherent and contextually rich personal dialogues even with minimal prompting. The architecture of these models has largely standardized around the decoder-only framework. Most LLM-based works customize various characters by configuring their personal background information in prompts, aiming at mimicking the specific role. Currently, several role-playing specific LLMs are developed to facilitate future research through instruction-tuning, such as CharacterGLM, Xingye, Xingchen, Index, and Baichuan-Character.\nTaking CharacterGLM as an example, let's explore how LLMs can be optimized for role-playing support: The process begins with collecting character-related training corpus, where detailed character profiles are developed and utilized to engage in dialogues through either human interactions or LLMs, creating a rich dataset that captures the nuances of character-specific conversations. Following data collection, the next phase is instruction tuning, where the character profiles and accumulated dialogue data are organized into structured instructions. This stage also could incorporate the use of diverse prompts for data augmentation, enhancing the model's ability to generate varied and contextually appropriate responses. The final but optional step, employs self-alignment\u2014using outputs from advanced models for further training\u2014and human feedback to refine and ensure character consistency. This comprehensive approach ensures that LLMs effectively embody and maintain character traits in role-playing scenarios.\nEach stage builds upon the previous, with advancements addressing the limitations of earlier models and opening new possibilities for more complex and engaging role-playing interactions.\nBeyond the methodologies outlined in public papers, we wish to underscore the influence of pre-training corpora on role-play tasks. From our pre-training experience, a corpus beneficial for training a generic assistant like ChatGPT may not necessarily aid a role-playing task, and the converse is also true. This is understandable, as a generic assistant requires data abundant in 'real' world knowledge (such as news, wiki) or data necessitating complex reasoning (like math, code). Novels, particularly those with a worldview divergent from reality, can induce serious knowledge hallucination issues. Role-playing often presupposes a scenario distinct from the real world, where a certain degree of reasonable knowledge hallucination is encouraged. Therefore, to our knowledge, a crucial step in pre-training an effective foundation model for role-playing involves incorporating a substantial amount of novels into the pretraining corpus, especially those with a worldview distinct from reality."}, {"title": "4.2 Alignment", "content": "Role-playing hinges on the precise alignment of language models with distinctive character-related information. In other words, alignment plays a crucial role in defining the upper limits of a model's role-playing ability. Current methodologies for aligning language models with different roles fall into two broad categories: parameter-tuning alignment and parameter-frozen alignment.\n4.2.1 Parameter-Tuning Alignment\nParameter-Tuning involves adjusting the model's parameters to learn character-specific knowledge. In this manner, common approaches include:\n\u2022 Continue-Pretrain: Help models obtain character-related knowledge, addressing the domain gap between general pre-training and downstream role-playing. This is essential because generic LLMs lack the nuanced understanding required to portray characters faithfully . ChatPlug  and MCP  train models on targeted literary corpora, capturing subtle narrative cues and character-specific lexicons essential for authentic character representation. To be more specific, ChatPlug continuly trains Qianwen series on large-scale corpora including common documents and conversation corpus, pursuiting extensive open-world knowledge and foundation abilities, which are used for playing the role of celebrities in the following role-playing scenarios.\n\u2022 Supervised Fine-tuning (SFT): This is the most direct and conventional training approach, involving the concatenation of personal information and conversations for supervised learning. Various techniques have been employed to enhance the learning of persona information during this phase. The core of these supervised methods lies in how to effectively model both role-related information and conversations simultaneously. Notable works include the use of attention routing mechanisms or memory networks to integrate both, and employing multiple structures to enhance the model's understanding of both elements. Currently, instruction tuning  has become the mainstream method for fine-tuning LLMs. During training, specific instructions and character-related data are provided, and the LLM learns through the next token prediction objectives. Typical works include RoleLLM , CharacterLLM and CharacterGLM .\n\u2022 Self-Alignment: Self-alignment, which is regarded as a new approach that improves weaker LLMs by fine-tuning it on outputs from a stronger LLM. Inspired by this, CharacterGLM  and Ditto  employ self-generated data to further encourage LLMs to simulate role-play dialogues. Ditto proposes three steps for role-playing self-alignment: role knowledge collection, dialogue simulation and instruction tuning. It first collects role profiles from open-access knowledge bases such as Wikipedia and then simulates role-playing conversation corpus by conducting a reading comprehension task. At last, Ditto trains the models based on the self-generated datasets to enhance their role-playing abilities.\n\u2022 Parameter-Efficiency Fine-Tuning (PEFT): Given the vast parameters of current LLMs, training efficiency becomes a critical concern. Techniques like LoRA-tuning  selectively train a subset of model parameters, which conserves computational resources. Han et al.  propose PersonaPKT, which represents each persona as condensing vectors to learn implicit persona-specific features. Such method only require less than 0.1% trainable parameters of the backbone while maintaining good response generation quality. Moreover, Yu et al.  employ different LORA modules to help LLMs imitate multiple characters simultaneously, balancing effectiveness with efficiency.\n\u2022 Reinforcement Learning: RLHF  is a pivotal enhancement approach utilized predominantly after the SFT stage. In the context of role-playing, RLHF-related methods can also help LLMs to refine and align the generated responses more closely with the intended character traits and behaviors. Shea and Yu  utilizes offline RL strategies to improve the persona consistency. Similarly, COMEDY  utilizes GPT-4 to construct memory-based personal responses and memory-against personal responses, forming positive-negative pairs, and then employ DPO  strategies for aligning LLMs to generate more coherent memory-based personalized responses.\nHowever, based on our practical engineering experience, we've found that 1) Reinforcement learning approaches, when using a reward model constructed through in-context learning, generally cannot exceed their inherent role-playing capabilities. 2) The task of annotating high-quality preference data for role-playing is significantly more challenging than for a generic assistant, as it necessitates a deep understanding of the specific character to accurately annotate preferences. For example, during the annotation of the HPD datasets , the authors enlisted the help of five avid Harry Potter fans to annotate Harry Potter's behavior."}, {"title": "4.2.2 Parameter-Frozen Alignment", "content": "The parameter-frozen alignment approaches in role-playing offer a versatile framework to adapt to new roles without extensive retraining of the model's parameters. These methods focus on utilizing existing model capabilities and enhancing them through strategic use of external data and contextual prompts.\n\u2022 In-Context Learning (ICL) Prompting: ICL-based approaches leverage the LLM's inherent ability to contextualize and adapt its responses based on provided prompts and examples within the inputs . To simulate the behavior of specific roles, ICL prompting is the simplest approach. Typically, filling with role attributes, relations, task requirements within ICL, current LLMs can adapt to different roles swiftly . This method is highly effective for rapid deployment across varied characters , making it ideal for scenarios where models need to switch roles or adapt to new narratives. Park et al.  even assign identities to multiple agents via ICL, make them simulating different roles in \u201cWestWorld\" sandbox game.\n\u2022 Retrieval Augmented Generation (RAG): RAG  enhances role-playing by dynamically retrieving data from external databases before response generation. This method addresses the internal knowledge gaps of models about specific characters , reducing hallucinations\u2014factually incorrect but plausible responses. By grounding responses in verifiable data, RAG enhances character consistency and enriches dialogues with precise, character-specific details.\n4.2.3 Summary and Discussion.\nParameter-tuning alignment offers precise control over role-playing models behaviors, enabling deep customization and high specificity in character dialogue generation. However, they require significant computational resources, high-quality training data and risk overfitting, potentially reducing the model's generalizability.\nConversely, parameter-frozen methods like In-context Learning and Retrieval Augmented Generation provide flexibility and scalability, allowing easy adaptation to new characters without extensive retraining. While reducing potential model biases, these methods depend heavily on abilities of LLMs and the quality and availability of external data, which can also introduce inaccuracies and result in hallucinations if not properly managed. Both approaches thus necessitate careful design to ensure the reliability and relevance of dialogue outputs."}, {"title": "5 Agent Architecture", "content": "Building on the foundation of role-playing with large language models, Role-Playing Language Agents (RPLAs) take this concept further by incorporating interactive and autonomous behaviors . These agents not only embody specific characters but also engage in complex scenarios, making decisions and responding in ways that align with their designated roles. The whole architecture of RPLAs often involves multiple modules that work in tandem. In this section, we introduce three important modules beyond the foundation response functionality in building well-performed RPLAs: memory, planning and action.\n5.1 Memory\nRPLAs often operate in environments that require them to remember and synthesize information over time, making memory modules  an essential component of their architecture. In the RPLA context, memory mainly consists of two source types:\n\u2022 User-Agent Interactions: This type is important in building user-centric RPLAs, which requires maintaining user related memories for generating consistent personalized responses in the ongoing interaction . Specifically, within online role-playing dialogue platforms, the integration of long-term memory becomes crucial in sustaining user engagement and enhancing the immersive experience. In such environments, an RPLA equipped with long-term memory can significantly enrich the role-playing experience by adapting its responses based on the accumulated history of a user's choices and interactions. Xu et al.  propose a long-term memory mechanism to extract and update long-term persona memories from use-bot interactions, enhancing the long-term personalized response consistency. Following this, Bae et al.  further incorporate the memory operator to control the update or ignore of fine-grained memories.\n\u2022 Agent-Agent Interactions: In scenarios involving multiple RPLAs, this type of memory is essential for managing interactions between different agents. It supports scenarios where RPLAs must collaborate or compete within complex environments, such as multiplayer simulation games or interactive narratives. Memory in this context includes recording the outcomes of past interactions, thoughts and actions, which influence future strategies and decisions. This dynamic memory use allows RPLAs to adapt their behaviors based on previous experiences with other agents, fostering a more nuanced and strategic interaction framework that evolves over time. Generative Agents  and Humanoid Agents  create a virtual role-playing environment, where an RPLA can remember past alliances or conflicts with other agents, using this information to inform future decisions and interactions.\nGiven different memory forms from previous interactions, there are generally two approaches for integrating them in RPLA systems:\n\u2022 Retrieval-based: This approach is widely-used in current RPLAs, which involve maintaining a database that stores useful information from previous interactions . Then facing the ongoing interaction, a retrieval module like sentence-embedding models fetches the most relevant information from this database based on the current context or recent interactions, helping the agent craft appropriate personalized responses or actions akin to the assigned character. Park et al.  retrieve relevant memories to help RPLAs make plans and decisions. Also, several memory management strategies like Memory Operator are used to ignore useless information and keep the memory base up-to-date. While retrieval-based memory offers clear benefits in enhancing RPLA interactions, it also poses challenges: 1) It heavily depends on the precision of the retrieval model; if these are not capable of accurately identifying relevant information, the coherence and appropriateness of responses can suffer; 2) It require substantial storage to maintain large corpus of past interactions, leading to increased storage costs and demanding efficient data management strategies to ensure quick and effective access to required information.\n\u2022 Compressive-based: This innovative approach addresses some limitations of retrieval-based memory by internalizing and condensing past information into a compact form, eliminating the need for extensive external databases. Compressive-based memory improves persona consistency by continuously updating and compressing historical data, which allows RPLAs to keep their responses current and relevant, like COMEDY and ReSummarize . COMEDY employ the \"compress over compress\u201d idea: it first summarizes each dialogue session into session-level memories, and then condenses them into a final compressive memory. Such method doesn't rely on any sentence-embedding model as retriever and database. ReSummarize recursive summarizes the personalized memories from past sessions to keep the up-to-date memories. This method enhances the storage efficiency and reduces the dependency on large-scale data retrieval, although it sometimes sacrifice detail for compactness."}, {"title": "5.2 Planning", "content": "In the realm of RPLAs", "stages": "plan formulation and plan reflection.\n\u2022 Plan Formulation: In role-playing contexts"}, {"title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models", "authors": ["Nuo Chen", "Y.Wang", "Yang Deng", "Jia Li"], "abstract": "This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded to embrace complex character portrayals involving character consistency, behavioral alignment, and overall attractiveness. We provide a comprehensive taxonomy of the critical components in designing these systems, including data, models and alignment, agent architecture and evaluation. This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement.", "sections": [{"title": "1 Introduction", "content": "Today, most large language models (LLMs) are proficient enough to act as assistants, but the ever-expanding desires of humans gradually go beyond this role. A helpful but serious assistant isn't everything in human life. An increasing number of individuals have been instructing LLMs to take on roles they desire, such as movie stars, game characters, or even their own relatives. This practice of aligning LLMs with specific personas or characters is commonly known as Role-Playing. If the standard assistant role of LLMs meets the demand for increased productivity, then LLMs for role-playing aims to fulfill human needs at a psychological and entertainment level. This lively trend underscores the versatility of LLMs and the limitless potential of human imagination in the realm of artificial intelligence.\nThe requirements for role-playing with language models differ significantly from those of a generic assistant. The primary expectation from a generic assistant is its helpfulness, meaning the LLM should follow the user's instructions and provide the desired responses. This expectation is also evident in the benchmarks for such models: people often desire an assistant with extensive professional knowledge and strong logical reasoning abilities.\nHowever, when the task comes to role-playing, the most crucial criterion is the LLM's ability to align with specific personas or characters. In other words, humans expect the LLMs to interact with them in a manner consistent with a specific role. This expectation introduces a fascinating dynamic that can sometimes contradict the traditional notion of helpfulness. For instance, consider a scenario where the role to be played is that of the user's adversary or enemy. In such a case, being helpful becomes a contradictory metric. The more helpful the model is, the less effective it becomes at role-playing.\nIn the era marked by the advent of sequence to sequence learning, researchers began exploring the potential of neural networks to generate dialogue responses that are consistent with both the given context and the portrayed persona. This initial motivation laid the groundwork for the following role-playing works. Subsequent advancements with the introduction of models such as BERT brought significant evolutions to the use of language models for role-playing, despite certain inherent limitations. In this period, due to the improved but still developing generative capabilities of these pre-trained language models (PLMs), role-playing is largely focused on achieving persona consistency through simpler, more straightforward persona roles, often termed as persona-based role-playing. This is partly due to the nature of the personalized information available in datasets at the time, which was often succinct and sparse, as seen in resources like the Persona-Chat dataset. PLMs are fine-tuned on these datasets to produce responses that aligned with the limited personal information provided, striving for a balance between consistent understanding and competent dialogue generation.\nAs the field progressed into the era of LLMS, a paradigm shift occurred. The enhanced comprehension and generative abilities of LLMs expand the scope of role-playing tasks far beyond simple persona adherence. Current research in role-playing no longer confines itself to rigid personas. Instead, it delves into more nuanced aspects of role enactment, such as character consistency, behavioral alignment, and overall attractiveness of the character portrayal named as character-based role-playing. These dimensions aim to create more immersive and believable character simulations that maintain continuity over interactions and adapt dynamically to dialogue contexts. The progress of LLM-based role-playing leading to a rapid expansion in academic research and the development of practical applications.\nDespite the promising potential of role-playing with language models, research in this domain remains in its early stages, marked by both complexities and challenges. The goal of this survey is to understand the crucial mechanisms and methodologies that enable role-playing through text-based interactions. To achieve a thorough understanding, we introduce a detailed taxonomy to systematically examine the critical components involved in designing role-playing language models. The proposed taxonomy includes: Data, Models & Alignment, Agent Architecture, and Evaluation. This framework aims to not only detail how role-playing functions within these systems but also to highlight how it can be optimized and evaluated for a variety of applications.\nIn summary, \u00a72 first introduces the preliminary of our survey, like the evolution of language models and key components. \u00a73 provides a comparative overview of the current role-playing data resources, highlighting their unique characteristics and applicability in different scenarios. For models & alignment, \u00a74 systematically reviews the foundational models of role-playing and summarizes the past alignment approaches, offering insights into their strengths and weaknesses. For Agent architecture, \u00a75 details the important modules that impact the effectiveness and generalization of role-playing language model agents, including memory, planning, action. Then, \u00a76 compiles comprehensive evaluation standards and metrics, encompassing both subjective and objective metrics, along with their respective advantages and disadvantages. Finally, \u00a77 delves into the 10 main challenges that persist in role-playing and envisages possible solutions that could pave the way for more advanced and nuanced systems."}, {"title": "2 Background", "content": "2.1 PLMs and LLMs\nDespite the absence of a universally accepted definition for Large Language Models (LLMs), this paper proposes a specific delineation for LLMs as referenced within our analysis. Distinguished by both model scale and training methodologies, our definition builds upon distinctions made by two seminal surveys on LLMs , differentiating LLMs from earlier Pre-trained Language Models (PLMs) based on the magnitude of model parameters and the scope of pre-training data. Specifically, LLMs refer to expansive models with parameters in the billions, pre-trained on extensive datasets, in contrast to PLMs, which are characterized by their relatively moderate parameter sizes in the millions and the capability for efficient task-specific fine-tuning to enhance performance on downstream tasks.\nNotable PLMs include BERT, GPT-2 , BART , and Roberta , whereas leading examples of LLMs, such as GPT-3 , PaLM , Mistral , and LLaMA , primarily utilize decoder-only architectures. The progression in architectural and training innovations for LLMs has facilitated the emergence of advanced capabilities , empowering them to tackle complex problems in few-shot or zero-shot settings through methodologies like in-context learning  and chain-of-thought reasoning . After reading this review, you will find that, in fact, the capabilities of zero-shot and few-shot learning are the main reasons for the increasing attention and interest in the domain of role-playing over the past two years."}, {"title": "2.2 Key Components", "content": "Employing language models for role-playing involves several critical factors that significantly influence their effectiveness and personalization capabilities. In this survey, we analyze several components that are essential in shaping the development, deployment, and continuous improvement of role-playing language models:\n\u2022 Data: The diversity and complexity of the data used in role-playing are foundational, influencing the models' ability to generate authentic and personalized interactions. Role-related information in current datasets ranges from structured texts and simple sentences to rich compilations of detailed personal information like attributes, relationships, and even nuanced understandings of characters across different timelines. The sophistication of a language model in handling role-playing scenarios directly correlates with the complexity of the targeted dataset. Models trained on simpler data might generate broad, generic responses, while those trained on more complex datasets are capable of producing dynamic dialogues that reflect specific character nuances. Hence, the selection and design of the dataset are critical in role-playing, directly impacting effectiveness in delivering engaging and personalized conversational experiences.\n\u2022 Models & Alignment: Undoubtedly, the backbone models play a pivotal role in the role playing's operational efficiency. From traditional neural networks to language models, the choice of model influences the system's understanding, generation, and adaptation capabilities. The recent advent of LLMs has brought significant advancements in this area, achieving high levels of personalization and coherence.\nMeanwhile, alignment approaches focus on ensuring that the role-playing models' responses align with the intended role. These methods range from rule-based systems that manually map responses to personas, to dynamic learning mechanisms that adapt responses based on interaction history and persona data. Technically, we divide them into Parameter-Tuning: Post-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning; and Parameter-Frozen: In-context learning prompting and Retrieval-Augmented Generation (RAG).\n\u2022 Agent Architecture: Currently, the development of Role-Playing Language Agents (RPLAS) marks a new evolution. These agents extend the basic framework of role-playing by integrating both interactive and autonomous behaviors, enabling them to not only personify specific characters but also engage proactively in complex and evolving scenarios. Effective RPLAs require a comprehensive system architecture that includes several key modules: memory, for recalling and utilizing past interactions; planning, for strategic decision-making; and action, which encompasses both generating role-related responses and using tools. Such complex architectures ensure RPLAs are not only interactive but also adaptive and context-aware, essential for complex role-playing scenarios.\n\u2022 Evaluation: Evaluating the performance of role-playing models is crucial for assessing their effectiveness and guiding improvements. Commonly, role-playing evaluation involves a complex array of perspectives that extend beyond those applied to traditional dialogue systems, focusing on role consistency, engagement, human-likeness, and proactivity, among others. This complexity necessitates a diverse set of evaluation metrics, encompassing both subjective human-based assessments and objective reference-based. The advent of LLMs has also spurred the development of LLM-based evaluation methods, offering an efficient alternative to conventional human annotation by approximating human judgments. However, given the multifaceted nature of role-playing, no single metric suffices to fully assess their performance. Thus, a composite approach, utilizing multiple metrics in tandem, is essential for a comprehensive evaluation.\nIn the following sections, we present a comprehensive survey along with our taxonomy."}, {"title": "3 Data", "content": "3.1 Data Objectives\nUnlike traditional text generation tasks, the capabilities of a role-playing language model are primarily determined by the target dataset it is fitted on. Therefore, the dataset is the most crucial prerequisite for training, categorizing, and testing different role-playing dialogue agents. Commonly, role-playing datasets contain two important components: interactions and role-related information. It's worth noting that we use the term \"interactions\" instead of the commonly used \"conversations\" or \"dialogues\". This is because we believe that the essence of role-playing lies in mimicking the behavior of the role in any scenarios, not just in dialogues. The reason why most current research is limited to the conversation-level is that, compared to other scenarios, the user's actions within a conversation are the easiest to obtain.\nIn this study, based on the different objectives of the targeted datasets, we categorize role-playing applications into two types: Persona-based Role-Playing (P-RP) and Character-based Role-Playing (C-RP). Generally, P-RP means that the dataset contains coarse-grained role-related information, whereas building C-RP requires fine-grained role-related information. The classification of granularity of coarse-grained or fine-grained primarily depends on whether the role-related information includes character-level specific background details. Datasets lacking such details are deemed coarse-grained, whereas those containing them are considered fine-grained. Here, we further emphasize the differences:\n\u2022 Persona-based Role-Playing (P-RP): These role-playing scenarios mimic broad categories of personas, focusing primarily on superficial and common attributes like location and gender. They are expected to display characteristics of specific groups of people based on given coarse-gained personas. P-RP is simpler, designed to ensure consistency within a narrower set of persona traits. They are generally less complex and tailored for generic roles that require basic interaction capabilities.\n\u2022 Character-based Role-Playing (C-RP): In contrast, C-RP scenarios are crafted to emulate specific characters from various narratives, such as novels, movies or even celebrities. These involves incorporating fine-grained character-level personal background information, including attributes, complex relationships, scene and nuanced psychological states. C-RP with language models is inherently complex, aimed at achieving deep, role-specific interactions and are equipped with features such as human-likeness, empathy, and proactivity . They are designed to offer a more immersive and engaging user experience.\nIn the evolving landscape of role-playing research, Persona-based Role-Playing (P-RP) is seen as a specific subset within the broader spectrum of Character-based Role-Playing (C-RP). Currently, the focus is increasingly on C-RP, reflecting a shift toward more complex and nuanced scenarios capable of leveraging the sophisticated capabilities of curret LLMs. This trend underscores a growing interest in developing role-playing interactions that offer rich, character-driven experiences. We classify different datasets according to interaction collection in Figure 2 and present overview of existing datasets in Table 1."}, {"title": "3.2 Persona-based Role-Playing Datasets", "content": "In general, the datasets associated with persona-based data tend to provide personas that are coarse-grained. The collection and construction of appropriate training data have become essential prerequisites for enabling chit-chat dialogue agents to generate persona-specific responses.\nInteraction Collection According to how to collect role-related conversations, we can generally classify the data collection process into two main streams:\n\u2022 Employing Crowdsourced Workers: This method involves hiring crowdworkers to create the corresponding dialogue datasets. Initially, certain personas and related topics are manually defined. Then, crowdworkers engage in dialogues based on these provided personas, resulting in personalized dialogues. The advantage of this approach lies in the guaranteed high quality of the data; however, due to the cost of manual labor, the scale is often limited. Notable examples include the Persona-Chat dataset , which contains about 10,000 persona dialogues, and the Focus dataset with approximately 14,000 conversations.\n\u2022 Extracting from Social Media: This method typically involves collecting a large volume of user dialogue data from social media platforms and applying specific filtering rules to obtain the final dataset. The strengths of these datasets are that they reflect real-world personalized dialogues and are large in scale. For instance, PersonalDialog  and Pchatbot have respectively gathered over 20 million and 130 million dialogue sessions from Weibo. However, a significant drawback is the difficulty in controlling data quality.\nIn general, there is a trade-off between the above two collection processes: The former provides high-quality, controlled data at a smaller scale, while the latter offers extensive real-world dialogue data, albeit with potential quality control issues.\nRole-related Information. Role-related information in P-RP datasets is crucial for generating realistic and context-appropriate responses, and it can be categorized into two distinct forms: explicit and implicit. Explicit information refers to instances where each interaction is accompanied by a detailed persona, presented either in a natural language format or a structured format. A prime example of the former, a persona in natural language format, is as follows (sourced from paper):\nRPGs are my favorite genre.\nI also went to school to work with technology.\nThe woman who gave birth to me is a physician.\nI am not a social person.\nAs for the structured format, an example from  is as follows:\n{Age: Post-90s,\nGender: Female,\nLocation: Beijing,\nConstellation: Aquarius}\nTo the best of our knowledge, most real-world role-playing applications incorporate both formats of role-related information. The key-value structured format is utilized to provide common, fundamental details that are necessary for each role, such as height, weight, and gender. On the other hand, the natural language format is employed to convey unique background information, experiences, and catchphrases specific to the role, which are challenging to encapsulate within the confines of a structured format.\nIn the context of implicit personas, as referenced in sources such as , some might contend that they do not fall within the scope of role-playing, given that they do not provide any role information. Such datasets distinguish dialogues belonging to different roles but do not supply persona information for each role. We still consider these works to be part of the role-playing domain, as the role-related information can essentially be inferred by summarizing the interaction history. We categorize these works under \"Implicit persona\". Undoubtedly, utilizing these types of datasets for role-playing is a more challenging task, as we must first devise methods to extract accurate role-related information from the interaction history."}, {"title": "3.3 Character-based Role-Playing Datasets", "content": "As LLMs' comprehension capabilities have improved, both researchers and users have found that coarse-grained personas are no longer sufficient to meet their needs for entertainment and psychological engagement. They have started to use LLMs to 'recreate' their favorite characters, a practice known as Character-based Role Playing. Typically, the role-related information provided by Character-based Role-playing can be a comprehensive role description (comprising thousands of tokens), or it could be corpus-level background materials such as novels or narratives. A pioneering effort in this field is the HPD dataset , a dataset based on the Harry Potter novels, which is used to train LLMs to align with the Harry Potter.\nInteraction Collection Character-based role-playing scenarios involve simulating a broad spectrum of roles, categorized mainly into two categories: real world-based and virtual scenario-based. Real world-based roles often mimic actual celebrities or typical individuals from daily life, while virtual scenario-based roles draw from fictional sources like novels, TV series, video games, and even theoretical constructs such as MBTI personalities . The diversity of these roles necessitates equally diverse methodologies for data collection, each with its unique set of challenges and solutions:\n\u2022 LLM as Data Generator: With the advanced generative capabilities of models like GPT-4, LLMs serve as a primary tool for synthesizing character profiles and dialogues. Related implementations include: 1) Generating complete character profiles from scratch and using these profiles to prompt varied character-based dialogues ; 2) Employing established profiles from resources like MBTI or Wikipedia as a basis for generating personalized dialogues . While effective, LLM-based data generation can introduce biases and unpredictable variations in data quality, necessitating careful manual review and validation .\n\u2022 Extracting from Literary Resources: This method involves the extraction of role-related conversations and character backgrounds directly from literary sources, particularly for fictional characters depicted in novels and television. However, this approach faces several challenges: 1) Dialogues are often tied to specific scenes, making it complex to delineate the scene of the dialogue and to extract the relevant textual context. 2) Automatically extracting detailed background information about characters is difficult. 3) Identifying multiple statements made by a character within a dialogue round can be complex. 4) Some dialogues involve non-verbal cues and contexts that are difficult to convey textually. A common solution involves using LLMs, human annotators, or a combination of both. For instance, Chen et al.  utilized a group of professional annotators and the LLM tools to annotate dialogue, attributes, and relationships from the Harry Potter novels.\nUndoubtedly, the language quality of the interactions extracted from this source is exceptional, and they exhibit the highest degree of alignment, given that the characteristics of the roles are originally defined by these works. However, the primary challenge lies in the significant disparity between the dialogue style in the literaries and the daily user-AI dialogue style. This discrepancy often results in the trained models underperforming in interactions with real users.\n\u2022 Human Role-Playing: Zhou et al. and Zhang et al. hire different crowdworkers who are given specific character profiles to role-play, and then engage in interactions. This data generation method generally produces high-quality data but lacks diversity and is costly.\n\u2022 Unpublished Resources: All the aforementioned acquisition methods are sourced from formal, published academic papers. However, according to our practical experience, solely relying on these datasets is inadequate to train a product-level role-playing model that fulfills user expectations. Consequently, we present a list of higher-quality role-playing data sources. However, it's important to note: we do not guarantee the legality of these data resources, and the developers should confirm any potential legal risks by themselves.\nThe first type of resource is role-play forums, which contain a vast amount of human-human role-playing data. Some well-known forums include Blue Moon, NationStates, Aryion, Questionable Questing, Role-Players, and Spacebattles. It's important to note that these forums often contain adult-only content, rigorous data cleaning is required before use. The second type is the log of some online role-playing products such as CharacterAI. But the use of such data requires dual authorization from both the users and the product developers. The last type is fanfiction communities, such as AO3. For some well-known characters like Harry Potter, the volume of fanfiction is thousands of times that of the original work. However, the risk lies in the fact that there are many Out-of-Character situations in fanfiction, as authors will add many personas according to their own preferences.\nRole-related Information. Character-based role-playing needs character-level fine-grained personal background information, which encompasses a range of elements, as shown in Figure 3. Below is a detailed categorization of the typical personal content found within these datasets:\n\u2022 Specific Scenes: This category captures detailed contextual information about the dialogues, including the precise timing, location, and underlying reasons. Such data is essential for situating the dialogue within a clear narrative frame, thereby providing a more immersive experience.\n\u2022 Comprehensive Attributes: Attributes cover a wide spectrum of personal details about the characters , including the name, gender, personality, age, identity, title, affiliation, quotes, belongings, etc.\n\u2022 Complex relationships: Relationship is another key perspective to make role-playing models delve deeper into the social and emotional landscapes of the specific characters , which contains opponents, familiarity, and family details, etc.\n\u2022 Temporal Personas: This aspect acknowledges that characters are not static; their personal information can evolve over time, reflecting developments in the character's storyline or changes in their environment. This temporal dimension allows for the simulation of growth and transformation, providing a richer narrative .\nEach category of data plays a critical role in constructing nuanced and responsive character-level role-playing models which can interact in a manner that mirrors human-like complexity and depth. Note, not all datasets contain each of the aforementioned elements. For instance, CharacterEval  provides scene and attributes while RoleInteract  contains scene, attributes and relations."}, {"title": "4 Models and Alignment", "content": "4.1 Foundation Models\nFoundation models are critical in setting the base capabilities of role-playing models. As the underlying architectures, they determine the lower bounds of performance and sophistication achievable in role-playing scenarios. The development of foundation models and architectures for role-playing can be viewed as a progressive evolution across three distinct stages: non-pretrained model, PLM, and LLM. Each stage represents a significant shift in backbone selection for role-playing models.\nNon-pretrained models and PLM. The earliest stage in the development of role-playing models involved non-pretrained architectures. These models are crafted from scratch, tailored to specific tasks without the benefit of large-scale, pre-trained data. Early models often utilize bespoke designs such as memory networks or custom transformers, which are specifically engineered to handle the storage and embedding-based fusion of personal information for effective role-playing . These architectures provided highly specialized solutions that were adept within specific contexts but lacked the generalizability and scalability offered by later developments. The shift to PLMs like BERT mark a substantial enhancement in foundational capabilities. These models leverage extensive pre-trained data, enhancing their ability to understand context and generate text, yet they still face limitations in fully grasping role-specific nuances. To overcome these challenges, researchers deploy innovative strategies like contrastive learning , bert-over-bert decouples learning and attention-based fusion mechanisms  improve the integration of personal and dialogue data, enhancing role-playing functionalities.\nLLM. The current frontier in role-playing development is characterized by LLMs such as GPT-4, which boast an unprecedented scale in parameters and pre-training. Such LLMs offer profound improvements in understanding and generating text, capable of maintaining coherent and contextually rich personal dialogues even with minimal prompting. The architecture of these models has largely standardized around the decoder-only framework. Most LLM-based works customize various characters by configuring their personal background information in prompts, aiming at mimicking the specific role. Currently, several role-playing specific LLMs are developed to facilitate future research through instruction-tuning, such as CharacterGLM, Xingye, Xingchen, Index, and Baichuan-Character.\nTaking CharacterGLM as an example, let's explore how LLMs can be optimized for role-playing support: The process begins with collecting character-related training corpus, where detailed character profiles are developed and utilized to engage in dialogues through either human interactions or LLMs, creating a rich dataset that captures the nuances of character-specific conversations. Following data collection, the next phase is instruction tuning, where the character profiles and accumulated dialogue data are organized into structured instructions. This stage also could incorporate the use of diverse prompts for data augmentation, enhancing the model's ability to generate varied and contextually appropriate responses. The final but optional step, employs self-alignment\u2014using outputs from advanced models for further training\u2014and human feedback to refine and ensure character consistency. This comprehensive approach ensures that LLMs effectively embody and maintain character traits in role-playing scenarios.\nEach stage builds upon the previous, with advancements addressing the limitations of earlier models and opening new possibilities for more complex and engaging role-playing interactions.\nBeyond the methodologies outlined in public papers, we wish to underscore the influence of pre-training corpora on role-play tasks. From our pre-training experience, a corpus beneficial for training a generic assistant like ChatGPT may not necessarily aid a role-playing task, and the converse is also true. This is understandable, as a generic assistant requires data abundant in 'real' world knowledge (such as news, wiki) or data necessitating complex reasoning (like math, code). Novels, particularly those with a worldview divergent from reality, can induce serious knowledge hallucination issues. Role-playing often presupposes a scenario distinct from the real world, where a certain degree of reasonable knowledge hallucination is encouraged. Therefore, to our knowledge, a crucial step in pre-training an effective foundation model for role-playing involves incorporating a substantial amount of novels into the pretraining corpus, especially those with a worldview distinct from reality."}, {"title": "4.2 Alignment", "content": "Role-playing hinges on the precise alignment of language models with distinctive character-related information. In other words, alignment plays a crucial role in defining the upper limits of a model's role-playing ability. Current methodologies for aligning language models with different roles fall into two broad categories: parameter-tuning alignment and parameter-frozen alignment.\n4.2.1 Parameter-Tuning Alignment\nParameter-Tuning involves adjusting the model's parameters to learn character-specific knowledge. In this manner, common approaches include:\n\u2022 Continue-Pretrain: Help models obtain character-related knowledge, addressing the domain gap between general pre-training and downstream role-playing. This is essential because generic LLMs lack the nuanced understanding required to portray characters faithfully . ChatPlug  and MCP  train models on targeted literary corpora, capturing subtle narrative cues and character-specific lexicons essential for authentic character representation. To be more specific, ChatPlug continuly trains Qianwen series on large-scale corpora including common documents and conversation corpus, pursuiting extensive open-world knowledge and foundation abilities, which are used for playing the role of celebrities in the following role-playing scenarios.\n\u2022 Supervised Fine-tuning (SFT): This is the most direct and conventional training approach, involving the concatenation of personal information and conversations for supervised learning. Various techniques have been employed to enhance the learning of persona information during this phase. The core of these supervised methods lies in how to effectively model both role-related information and conversations simultaneously. Notable works include the use of attention routing mechanisms or memory networks to integrate both, and employing multiple structures to enhance the model's understanding of both elements. Currently, instruction tuning  has become the mainstream method for fine-tuning LLMs. During training, specific instructions and character-related data are provided, and the LLM learns through the next token prediction objectives. Typical works include RoleLLM , CharacterLLM and CharacterGLM .\n\u2022 Self-Alignment: Self-alignment, which is regarded as a new approach that improves weaker LLMs by fine-tuning it on outputs from a stronger LLM. Inspired by this, CharacterGLM  and Ditto  employ self-generated data to further encourage LLMs to simulate role-play dialogues. Ditto proposes three steps for role-playing self-alignment: role knowledge collection, dialogue simulation and instruction tuning. It first collects role profiles from open-access knowledge bases such as Wikipedia and then simulates role-playing conversation corpus by conducting a reading comprehension task. At last, Ditto trains the models based on the self-generated datasets to enhance their role-playing abilities.\n\u2022 Parameter-Efficiency Fine-Tuning (PEFT): Given the vast parameters of current LLMs, training efficiency becomes a critical concern. Techniques like LoRA-tuning  selectively train a subset of model parameters, which conserves computational resources. Han et al.  propose PersonaPKT, which represents each persona as condensing vectors to learn implicit persona-specific features. Such method only require less than 0.1% trainable parameters of the backbone while maintaining good response generation quality. Moreover, Yu et al.  employ different LORA modules to help LLMs imitate multiple characters simultaneously, balancing effectiveness with efficiency.\n\u2022 Reinforcement Learning: RLHF  is a pivotal enhancement approach utilized predominantly after the SFT stage. In the context of role-playing, RLHF-related methods can also help LLMs to refine and align the generated responses more closely with the intended character traits and behaviors. Shea and Yu  utilizes offline RL strategies to improve the persona consistency. Similarly, COMEDY  utilizes GPT-4 to construct memory-based personal responses and memory-against personal responses, forming positive-negative pairs, and then employ DPO  strategies for aligning LLMs to generate more coherent memory-based personalized responses.\nHowever, based on our practical engineering experience, we've found that 1) Reinforcement learning approaches, when using a reward model constructed through in-context learning, generally cannot exceed their inherent role-playing capabilities. 2) The task of annotating high-quality preference data for role-playing is significantly more challenging than for a generic assistant, as it necessitates a deep understanding of the specific character to accurately annotate preferences. For example, during the annotation of the HPD datasets , the authors enlisted the help of five avid Harry Potter fans to annotate Harry Potter's behavior."}, {"title": "4.2.2 Parameter-Frozen Alignment", "content": "The parameter-frozen alignment approaches in role-playing offer a versatile framework to adapt to new roles without extensive retraining of the model's parameters. These methods focus on utilizing existing model capabilities and enhancing them through strategic use of external data and contextual prompts.\n\u2022 In-Context Learning (ICL) Prompting: ICL-based approaches leverage the LLM's inherent ability to contextualize and adapt its responses based on provided prompts and examples within the inputs . To simulate the behavior of specific roles, ICL prompting is the simplest approach. Typically, filling with role attributes, relations, task requirements within ICL, current LLMs can adapt to different roles swiftly . This method is highly effective for rapid deployment across varied characters , making it ideal for scenarios where models need to switch roles or adapt to new narratives. Park et al.  even assign identities to multiple agents via ICL, make them simulating different roles in \u201cWestWorld\" sandbox game.\n\u2022 Retrieval Augmented Generation (RAG): RAG  enhances role-playing by dynamically retrieving data from external databases before response generation. This method addresses the internal knowledge gaps of models about specific characters , reducing hallucinations\u2014factually incorrect but plausible responses. By grounding responses in verifiable data, RAG enhances character consistency and enriches dialogues with precise, character-specific details.\n4.2.3 Summary and Discussion.\nParameter-tuning alignment offers precise control over role-playing models behaviors, enabling deep customization and high specificity in character dialogue generation. However, they require significant computational resources, high-quality training data and risk overfitting, potentially reducing the model's generalizability.\nConversely, parameter-frozen methods like In-context Learning and Retrieval Augmented Generation provide flexibility and scalability, allowing easy adaptation to new characters without extensive retraining. While reducing potential model biases, these methods depend heavily on abilities of LLMs and the quality and availability of external data, which can also introduce inaccuracies and result in hallucinations if not properly managed. Both approaches thus necessitate careful design to ensure the reliability and relevance of dialogue outputs."}, {"title": "5 Agent Architecture", "content": "Building on the foundation of role-playing with large language models, Role-Playing Language Agents (RPLAs) take this concept further by incorporating interactive and autonomous behaviors . These agents not only embody specific characters but also engage in complex scenarios, making decisions and responding in ways that align with their designated roles. The whole architecture of RPLAs often involves multiple modules that work in tandem. In this section, we introduce three important modules beyond the foundation response functionality in building well-performed RPLAs: memory, planning and action.\n5.1 Memory\nRPLAs often operate in environments that require them to remember and synthesize information over time, making memory modules  an essential component of their architecture. In the RPLA context, memory mainly consists of two source types:\n\u2022 User-Agent Interactions: This type is important in building user-centric RPLAs, which requires maintaining user related memories for generating consistent personalized responses in the ongoing interaction . Specifically, within online role-playing dialogue platforms, the integration of long-term memory becomes crucial in sustaining user engagement and enhancing the immersive experience. In such environments, an RPLA equipped with long-term memory can significantly enrich the role-playing experience by adapting its responses based on the accumulated history of a user's choices and interactions. Xu et al.  propose a long-term memory mechanism to extract and update long-term persona memories from use-bot interactions, enhancing the long-term personalized response consistency. Following this, Bae et al.  further incorporate the memory operator to control the update or ignore of fine-grained memories.\n\u2022 Agent-Agent Interactions: In scenarios involving multiple RPLAs, this type of memory is essential for managing interactions between different agents. It supports scenarios where RPLAs must collaborate or compete within complex environments, such as multiplayer simulation games or interactive narratives. Memory in this context includes recording the outcomes of past interactions, thoughts and actions, which influence future strategies and decisions. This dynamic memory use allows RPLAs to adapt their behaviors based on previous experiences with other agents, fostering a more nuanced and strategic interaction framework that evolves over time. Generative Agents  and Humanoid Agents  create a virtual role-playing environment, where an RPLA can remember past alliances or conflicts with other agents, using this information to inform future decisions and interactions.\nGiven different memory forms from previous interactions, there are generally two approaches for integrating them in RPLA systems:\n\u2022 Retrieval-based: This approach is widely-used in current RPLAs, which involve maintaining a database that stores useful information from previous interactions . Then facing the ongoing interaction, a retrieval module like sentence-embedding models fetches the most relevant information from this database based on the current context or recent interactions, helping the agent craft appropriate personalized responses or actions akin to the assigned character. Park et al.  retrieve relevant memories to help RPLAs make plans and decisions. Also, several memory management strategies like Memory Operator are used to ignore useless information and keep the memory base up-to-date. While retrieval-based memory offers clear benefits in enhancing RPLA interactions, it also poses challenges: 1) It heavily depends on the precision of the retrieval model; if these are not capable of accurately identifying relevant information, the coherence and appropriateness of responses can suffer; 2) It require substantial storage to maintain large corpus of past interactions, leading to increased storage costs and demanding efficient data management strategies to ensure quick and effective access to required information.\n\u2022 Compressive-based: This innovative approach addresses some limitations of retrieval-based memory by internalizing and condensing past information into a compact form, eliminating the need for extensive external databases. Compressive-based memory improves persona consistency by continuously updating and compressing historical data, which allows RPLAs to keep their responses current and relevant, like COMEDY and ReSummarize . COMEDY employ the \"compress over compress\u201d idea: it first summarizes each dialogue session into session-level memories, and then condenses them into a final compressive memory. Such method doesn't rely on any sentence-embedding model as retriever and database. ReSummarize recursive summarizes the personalized memories from past sessions to keep the up-to-date memories. This method enhances the storage efficiency and reduces the dependency on large-scale data retrieval, although it sometimes sacrifice detail for compactness."}, {"title": "5.2 Planning", "content": "In the realm of RPLAs, while foundational conversational capabilities are crucial, certain scenarios demand additional capacities such as advanced planning. Let LLMs simulate the human behavior in virtual environments could be a typical example, where strategic planning significantly enhances the role-playing experience . Typically, planning in RPLAS comprises two stages: plan formulation and plan reflection.\n\u2022 Plan Formulation: In role-playing contexts, the formulation stage involves setting objectives that are consistent with the character's motivations and the narrative's demands . Agents analyze the current scenario, predict possible future states, and devise a sequence of actions that"}]}]}