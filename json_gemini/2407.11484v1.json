{"title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models", "authors": ["Nuo Chen", "Y.Wang", "Yang Deng", "Jia Li"], "abstract": "This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded to embrace complex character portrayals involving character consistency, behavioral alignment, and overall attractiveness. We provide a comprehensive taxonomy of the critical components in designing these systems, including data, models and alignment, agent architecture and evaluation. This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement. Related resources and papers are available at https://github.com/\nnuochenpku/Awesome-Role-Play-Papers.", "sections": [{"title": "1 Introduction", "content": "Today, most large language models (LLMs) (Brown et al., 2020; Hu et al., 2021; Zeng et al., 2022; OpenAI, 2023; Scao et al., 2022) are proficient enough to act as assistants, but the ever-expanding desires of humans gradually go beyond this role. A helpful but serious assistant isn't everything in human life. An increasing number of individuals have been instructing LLMs to take on roles they desire, such as movie stars, game characters, or even their own relatives. This practice of aligning LLMs with specific personas or characters is commonly known as Role-Playing (Zhang et al., 2018; Jiang et al.; Chen et al., 2023b; Qian et al., 2021). If the standard assistant role of LLMs meets the demand for increased productivity, then LLMs for role-playing aims to fulfill human needs at a psychological and entertainment level. This lively trend underscores the versatility of LLMs and the limitless potential of human imagination in the realm of artificial intelligence.\nThe requirements for role-playing with language models differ significantly from those of a generic assistant. The primary expectation from a generic assistant is its helpfulness, meaning the LLM should follow the user's instructions and provide the desired responses (Serban et al., 2016; Lowe et al., 2015; Thoppilan et al., 2022; Miller et al., 2017; You et al., 2022). This expectation is also evident in the benchmarks for such models: people often desire an assistant with extensive professional knowledge and strong logical reasoning abilities.\nHowever, when the task comes to role-playing, the most crucial criterion is the LLM's ability to align with specific personas or characters (Tu et al., 2024, 2023; Chen et al., 2023b). In other words, humans expect the LLMs to interact with them in a manner consistent with a specific role. This expectation introduces a fascinating dynamic that can sometimes contradict the traditional notion of helpfulness. For instance, consider a scenario where the role to be played is that of the user's adversary or enemy. In such a case, being helpful becomes a contradictory metric. The more helpful the model is, the less effective it becomes at role-playing.\nIn the era marked by the advent of sequence to sequence learning (Shang et al., 2015), researchers began exploring the potential of neural networks to generate dialogue responses that are consistent with both the given context and the portrayed persona (Zhang et al., 2018; Jiang et al.; Dinan et al., 2018). For instance, Zhang et al. (2018) utilizes a generative profile memory network to generate personal responses. This initial motivation laid the groundwork for the following role-playing works. Subsequent advancements with the introduction of models such as BERT (Kenton and Toutanova, 2019) brought significant evolutions to the use of language models for role-playing, despite certain inherent limitations. In this period, due to the improved but still developing generative capabilities of these pre-trained language models (PLMs) (Liu et al., 2019; Raffel et al., 2020), role-playing is largely focused on achieving persona consistency through simpler, more straightforward persona roles, often termed as persona-based role-playing. This is partly due to the nature of the personalized information available in datasets at the time, which was often succinct and sparse, as seen in resources like the Persona-Chat dataset (Zhang et al., 2018). PLMs are fine-tuned on these datasets to produce responses that aligned with the limited personal information provided, striving for a balance between consistent understanding and competent dialogue generation.\nAs the field progressed into the era of LLMS (OpenAI, 2023; Touvron et al., 2023; Zeng et al., 2022), a paradigm shift occurred. The enhanced comprehension and generative abilities of LLMs expand the scope of role-playing tasks far beyond simple persona adherence. Current research in role-playing no longer confines itself to rigid personas. Instead, it delves into more nuanced aspects of role enactment, such as character consistency, behavioral alignment, and overall attractiveness of the character portrayal (Chen et al., 2023b; Shao et al., 2023; Tu et al., 2024), named as character-based role-playing. These dimensions aim to create more immersive and believable character simulations that maintain continuity over interactions and adapt dynamically to dialogue contexts. The progress of LLM-based role-playing leading to a rapid expansion in academic research and the development of practical applications, exemplified by platforms like Character AI\u00b9, Xingye\u00b2, and Maopaoya\u00b3.\nDespite the promising potential of role-playing with language models, research in this domain remains in its early stages, marked by both complexities and challenges. The goal of this survey is to understand the crucial mechanisms and methodologies that enable role-playing through text-based interactions. To achieve a thorough understanding, we introduce a detailed taxonomy to systematically examine the critical components involved in designing role-playing language models. The proposed taxonomy includes: Data, Models & Alignment, Agent Architecture, and Evaluation. This framework aims to not only detail how role-playing functions within these systems but also to highlight how it can be optimized and evaluated for a variety of applications.\nIn summary, \u00a72 first introduces the preliminary of our survey, like the evolution of language models and key components. \u00a73 provides a comparative overview of the current role-playing data resources, highlighting their unique characteristics and applicability in different scenarios. For models & alignment, \u00a74 systematically reviews the foundational models of role-playing and summarizes the past alignment approaches, offering insights into their strengths and weaknesses. For Agent architecture, \u00a75 details the important modules that impact the effectiveness and generalization of role-playing language model agents, including memory, planning, action. Then, \u00a76 compiles comprehensive evaluation standards and metrics, encompassing both subjective and objective metrics, along with their respective advantages and disadvantages. Finally, \u00a77 delves into the 10 main challenges that persist in role-playing and envisages possible solutions that could pave the way for more advanced and nuanced systems."}, {"title": "2 Background", "content": "2.1 PLMs and LLMs\nDespite the absence of a universally accepted definition for Large Language Models (LLMs), this paper proposes a specific delineation for LLMs as referenced within our analysis. Distinguished by both model scale and training methodologies, our definition builds upon distinctions made by two seminal surveys on LLMs (Zhao et al., 2023b; Yang et al., 2023), differentiating LLMs from earlier Pre-trained Language Models (PLMs) based on the magnitude of model parameters and the scope of pre-training data. Specifically, LLMs refer to expansive models with parameters in the billions, pre-trained on extensive datasets, in contrast to PLMs, which are characterized by their relatively moderate parameter sizes in the millions and the capability for efficient task-specific fine-tuning to enhance performance on downstream tasks.\nNotable PLMs include BERT (Kenton and Toutanova, 2019), GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020), and Roberta (Liu et al., 2019), whereas leading examples of LLMs, such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), Mistral (Jiang et al., 2023), and LLaMA (Touvron et al., 2023), primarily utilize decoder-only architectures. The progression in architectural and training innovations for LLMs has facilitated the emergence of advanced capabilities (Wei et al., 2022b), empowering them to tackle complex problems in few-shot or zero-shot settings through methodologies like in-context learning (Radford et al., 2021; Dong et al., 2022) and chain-of-thought reasoning (Wei et al., 2022c). After reading this review, you will find that, in fact, the capabilities of zero-shot and few-shot learning are the main reasons for the increasing attention and interest in the domain of role-playing over the past two years."}, {"title": "2.2 Key Components", "content": "Employing language models for role-playing involves several critical factors that significantly influence their effectiveness and personalization capabilities. In this survey, we analyze several components that are essential in shaping the development, deployment, and continuous improvement of role-playing language models:\n\u2022 Data: The diversity and complexity of the data used in role-playing are foundational, influencing the models' ability to generate authentic and personalized interactions. Role-related information in current datasets ranges from structured texts and simple sentences to rich compilations of detailed personal information like attributes, relationships, and even nuanced understandings of characters across different timelines. The sophistication of a language model in handling role-playing scenarios directly correlates with the complexity of the targeted dataset. Models trained on simpler data might generate broad, generic responses, while those trained on more complex datasets are capable of producing dynamic dialogues that reflect specific character nuances. Hence, the selection and design of the dataset are critical in role-playing, directly impacting effectiveness in delivering engaging and personalized conversational experiences.\n\u2022 Models & Alignment: Undoubtedly, the backbone models play a pivotal role in the role playing's operational efficiency. From traditional neural networks to language models, the choice of model influences the system's understanding, generation, and adaptation capabilities. The recent advent of LLMs has brought significant advancements in this area, achieving high levels of personalization and coherence.\nMeanwhile, alignment approaches focus on ensuring that the role-playing models' responses align with the intended role. These methods range from rule-based systems that manually map responses to personas, to dynamic learning mechanisms that adapt responses based on interaction history and persona data. Technically, we divide them into Parameter-Tuning: Post-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning; and Parameter-Frozen: In-context learning prompting and Retrieval-Augmented Generation (RAG).\n\u2022 Agent Architecture: Currently, the development of Role-Playing Language Agents (RPLAS) marks a new evolution. These agents extend the basic framework of role-playing by integrating both interactive and autonomous behaviors, enabling them to not only personify specific characters but also engage proactively in complex and evolving scenarios. Effective RPLAs require a comprehensive system architecture that includes several key modules: memory, for recalling and utilizing past interactions; planning, for strategic decision-making; and action, which encompasses both generating role-related responses and using tools. Such complex architectures ensure RPLAS are not only interactive but also adaptive and context-aware, essential for complex role-playing scenarios.\n\u2022 Evaluation: Evaluating the performance of role-playing models is crucial for assessing their effectiveness and guiding improvements. Commonly, role-playing evaluation involves a complex array of perspectives that extend beyond those applied to traditional dialogue systems, focusing on role consistency, engagement, human-likeness, and proactivity, among others. This complexity necessitates a diverse set of evaluation metrics, encompassing both subjective human-based assessments and objective reference-based. The advent of LLMs has also spurred the development of LLM-based evaluation methods, offering an efficient alternative to conventional human annotation by approximating human judgments. However, given the multifaceted nature of role-playing, no single metric suffices to fully assess their performance. Thus, a composite approach, utilizing multiple metrics in tandem, is essential for a comprehensive evaluation.\nIn the following sections, we present a comprehensive survey along with our taxonomy."}, {"title": "3 Data", "content": "3.1 Data Objectives\nUnlike traditional text generation tasks, the capabilities of a role-playing language model are primarily determined by the target dataset it is fitted on. Therefore, the dataset is the most crucial prerequisite for training, categorizing, and testing different role-playing dialogue agents. Commonly, role-playing datasets contain two important components: interactions and role-related information. It's worth noting that we use the term \"interactions\" instead of the commonly used \"conversations\" or \"dialogues\". This is because we believe that the essence of role-playing lies in mimicking the behavior of the role in any scenarios, not just in dialogues. The reason why most current research is limited to the conversation-level is that, compared to other scenarios, the user's actions within a conversation are the easiest to obtain.\nIn this study, based on the different objectives of the targeted datasets, we categorize role-playing applications into two types: Persona-based Role-Playing (P-RP) and Character-based Role-Playing (C-RP). Generally, P-RP means that the dataset contains coarse-grained role-related information, whereas building C-RP requires fine-grained role-related information. The classification of granularity of coarse-grained or fine-grained primarily depends on whether the role-related information includes character-level specific background details. Datasets lacking such details are deemed coarse-grained, whereas those containing them are considered fine-grained. Here, we further emphasize the differences:\n\u2022 Persona-based Role-Playing (P-RP): These role-playing scenarios mimic broad categories of personas, focusing primarily on superficial and common attributes like location and gender. They are expected to display characteristics of specific groups of people based on given coarse-gained personas (Zheng et al., 2020; Dinan et al., 2018; Kottur et al., 2017; Zhang et al., 2018). P-RP is simpler, designed to ensure consistency within a narrower set of persona traits. They are generally less complex and tailored for generic roles that require basic interaction capabilities.\n\u2022 Character-based Role-Playing (C-RP): In contrast, C-RP scenarios are crafted to emulate specific characters from various narratives, such as novels, movies or even celebrities. These involves incorporating fine-grained character-level personal background information, including attributes, complex relationships, scene and nuanced psychological states (Chen et al., 2023b, 2024a; Tu et al., 2024). C-RP with language models is inherently complex, aimed at achieving deep, role-specific interactions and are equipped with features such as human-likeness, empathy, and proactivity (Zhang et al., 2024). They are designed to offer a more immersive and engaging user experience.\nIn the evolving landscape of role-playing research, Persona-based Role-Playing (P-RP) is seen as a specific subset within the broader spectrum of Character-based Role-Playing (C-RP). Currently, the focus is increasingly on C-RP, reflecting a shift toward more complex and nuanced scenarios capable of leveraging the sophisticated capabilities of curret LLMs. This trend underscores a growing interest in developing role-playing interactions that offer rich, character-driven experiences. We classify different datasets according to interaction collection in Figure 2 and present overview of existing datasets in Table 1."}, {"title": "3.2 Persona-based Role-Playing Datasets", "content": "In general, the datasets associated with persona-based data tend to provide personas that are coarse-grained. The collection and construction of appropriate training data have become essential prerequisites for enabling chit-chat dialogue agents to generate persona-specific responses.\nInteraction Collection According to how to collect role-related conversations, we can generally classify the data collection process into two main streams:\n\u2022 Employing Crowdsourced Workers: This method involves hiring crowdworkers to create the corresponding dialogue datasets. Initially, certain personas and related topics are manually defined. Then, crowdworkers engage in dialogues based on these provided personas, resulting in personalized dialogues. The advantage of this approach lies in the guaranteed high quality of the data; however, due to the cost of manual labor, the scale is often limited. Notable examples include the Persona-Chat dataset (Zhang et al., 2018), which contains about 10,000 persona dialogues, and the Focus (Jang et al., 2021) dataset with approximately 14,000 conversations.\n\u2022 Extracting from Social Media: This method typically involves collecting a large volume of user dialogue data from social media platforms and applying specific filtering rules to obtain the final dataset. The strengths of these datasets are that they reflect real-world personalized dialogues and are large in scale. For instance, PersonalDialog (Zheng et al., 2019) and Pchatbot (Qian et al., 2021) have respectively gathered over 20 million and 130 million dialogue sessions from Weibo. However, a significant drawback is the difficulty in controlling data quality.\nIn general, there is a trade-off between the above two collection processes: The former provides high-quality, controlled data at a smaller scale, while the latter offers extensive real-world dialogue data, albeit with potential quality control issues.\nRole-related Information. Role-related information in P-RP datasets is crucial for generating realistic and context-appropriate responses, and it can be categorized into two distinct forms: explicit and implicit. Explicit information refers to instances where each interaction is accompanied by a detailed persona, presented either in a natural language format (Zhang et al., 2018) or a structured format (Song et al., 2020b; Zheng et al., 2019). A prime example of the former, a persona in natural language format, is as follows (sourced from paper Zhang et al. (2018)):\nRPGs are my favorite genre.\nI also went to school to work with technology.\nThe woman who gave birth to me is a physician.\nI am not a social person.\nI enjoy working with my hands.\nAs for the structured format, an example from (Zheng et al., 2019) is as follows:\n{Age: Post-90s,\nGender: Female,\nLocation: Beijing,\nConstellation: Aquarius}\nTo the best of our knowledge, most real-world role-playing applications incorporate both formats of role-related information. The key-value structured format is utilized to provide common, fundamental details that are necessary for each role, such as height, weight, and gender. On the other hand, the natural language format is employed to convey unique background information, experiences, and catchphrases specific to the role, which are challenging to encapsulate within the confines of a structured format.\nIn the context of implicit personas, as referenced in sources such as (Li et al., 2016; Zhang et al., 2019), some might contend that they do not fall within the scope of role-playing, given that they do not provide any role information. Such datasets distinguish dialogues belonging to different roles but do not supply persona information for each role. We still consider these works to be part of the role-playing domain, as the role-related information can essentially be inferred by summarizing the interaction history. We categorize these works under \"Implicit persona\". Undoubtedly, utilizing these types of datasets for role-playing is a more challenging task, as we must first devise methods to extract accurate role-related information from the interaction history."}, {"title": "3.3 Character-based Role-Playing Datasets", "content": "As LLMs' comprehension capabilities have improved, both researchers and users have found that coarse-grained personas are no longer sufficient to meet their needs for entertainment and psychological engagement. They have started to use LLMs to 'recreate' their favorite characters, a practice known as Character-based Role Playing. Typically, the role-related information provided by Character-based Role-playing can be a comprehensive role description (comprising thousands of tokens), or it could be corpus-level background materials such as novels or narratives. A pioneering effort in this field is the HPD dataset (Chen et al., 2023b), a dataset based on the Harry Potter novels, which is used to train LLMs to align with the Harry Potter.\nInteraction Collection Character-based role-playing scenarios involve simulating a broad spectrum of roles, categorized mainly into two categories: real world-based and virtual scenario-based. Real world-based roles often mimic actual celebrities or typical individuals from daily life, while virtual scenario-based roles draw from fictional sources like novels, TV series, video games, and even theoretical constructs such as MBTI personalities (Wang et al., 2024). The diversity of these roles necessitates equally diverse methodologies for data collection, each with its unique set of challenges and solutions:\n\u2022 LLM as Data Generator: With the advanced generative capabilities of models like GPT-4, LLMs serve as a primary tool for synthesizing character profiles and dialogues. Related implementations include: 1) Generating complete character profiles from scratch and using these profiles to prompt varied character-based dialogues (Wang et al., 2023e); 2) Employing established profiles from resources like MBTI or Wikipedia as a basis for generating personalized dialogues (Zhou et al., 2023; Chen et al., 2024a; Lu et al., 2024). While effective, LLM-based data generation can introduce biases and unpredictable variations in data quality, necessitating careful manual review and validation (Anwar et al., 2024).\n\u2022 Extracting from Literary Resources: This method involves the extraction of role-related conversations and character backgrounds directly from literary sources, particularly for fictional characters depicted in novels and television (Chen et al., 2023b; Tu et al., 2024; Zhao et al., 2023a). However, this approach faces several challenges: 1) Dialogues are often tied to specific scenes, making it complex to delineate the scene of the dialogue and to extract the relevant textual context. 2) Automatically extracting detailed background information about characters is difficult. 3) Identifying multiple statements made by a character within a dialogue round can be complex. 4) Some dialogues involve non-verbal cues and contexts that are difficult to convey textually. A common solution involves using LLMs, human annotators, or a combination of both. For instance, Chen et al. (2023b) utilized a group of professional annotators and the LLM tools to annotate dialogue, attributes, and relationships from the Harry Potter novels.\nUndoubtedly, the language quality of the interactions extracted from this source is exceptional, and they exhibit the highest degree of alignment, given that the characteristics of the roles are originally defined by these works. However, the primary challenge lies in the significant disparity between the dialogue style in the literaries and the daily user-AI dialogue style. This discrepancy often results in the trained models underperforming in interactions with real users.\n\u2022 Human Role-Playing: Zhou et al. (2023) and Zhang et al. (2024) hire different crowdworkers who are given specific character profiles to role-play, and then engage in interactions. This data generation method generally produces high-quality data but lacks diversity and is costly.\n\u2022 Unpublished Resources: All the aforementioned acquisition methods are sourced from formal, published academic papers. However, according to our practical experience, solely relying on these datasets is inadequate to train a product-level role-playing model that fulfills user expectations. Consequently, we present a list of higher-quality role-playing data sources. However, it's important to note: we do not guarantee the legality of these data resources, and the developers should confirm any potential legal risks by themselves.\nThe first type of resource is role-play forums, which contain a vast amount of human-human role-playing data. Some well-known forums include Blue Moon, NationStates, Aryion, Questionable Questing, Role-Players, and Spacebattles. It's important to note that these forums often contain adult-only content, rigorous data cleaning is required before use. The second type is the log of some online role-playing products (Chen et al., 2024a,b) such as CharacterAI. But the use of such data requires dual authorization from both the users and the product developers. The last type is fanfiction communities, such as AO3. For some well-known characters like Harry Potter, the volume of fanfiction is thousands of times that of the original work. However, the risk lies in the fact that there are many Out-of-Character situations in fanfiction, as authors will add many personas according to their own preferences.\nRole-related Information. Character-based role-playing needs character-level fine-grained personal background information, which encompasses a range of elements, as shown in Figure 3. Below is a detailed categorization of the typical personal content found within these datasets:\n\u2022 Specific Scenes: This category captures detailed contextual information about the dialogues, including the precise timing, location, and underlying reasons (Tu et al., 2024; Chen et al., 2023b). Such data is essential for situating the dialogue within a clear narrative frame, thereby providing a more immersive experience.\n\u2022 Comprehensive Attributes: Attributes cover a wide spectrum of personal details about the characters (Zhou et al., 2023; Tu et al., 2024; Wang et al., 2023e), including the name, gender, personality, age, identity, title, affiliation, quotes, belongings, etc.\n\u2022 Complex relationships: Relationship is another key perspective to make role-playing models delve deeper into the social and emotional landscapes of the specific characters (Chen et al., 2024a, 2023b), which contains opponents, familiarity, and family details, etc.\n\u2022 Temporal Personas: This aspect acknowledges that characters are not static; their personal information can evolve over time, reflecting developments in the character's storyline or changes in their environment. This temporal dimension allows for the simulation of growth and transformation, providing a richer narrative (Shen et al., 2024b; Chen et al., 2023b; Ahn et al., 2024).\nEach category of data plays a critical role in constructing nuanced and responsive character-level role-playing models which can interact in a manner that mirrors human-like complexity and depth. Note, not all datasets contain each of the aforementioned elements. For instance, CharacterEval (Tu et al., 2024) provides scene and attributes while RoleInteract (Chen et al., 2024a) contains scene, attributes and relations."}, {"title": "4 Models and Alignment", "content": "4.1 Foundation Models\nFoundation models are critical in setting the base capabilities of role-playing models. As the underlying architectures, they determine the lower bounds of performance and sophistication achievable in role-playing scenarios. The development of foundation models and architectures for role-playing can be viewed as a progressive evolution across three distinct stages: non-pretrained model, PLM, and LLM. Each stage represents a significant shift in backbone selection for role-playing models.\nNon-pretrained models and PLM. The earliest stage in the development of role-playing models involved non-pretrained architectures. These models are crafted from scratch, tailored to specific tasks without the benefit of large-scale, pre-trained data. Early models often utilize bespoke designs such as memory networks or custom transformers, which are specifically engineered to handle the storage and embedding-based fusion of personal information for effective role-playing (Zhang et al., 2018; Jiang et al.; Kottur et al., 2017; Li et al., 2016). These architectures provided highly specialized solutions that were adept within specific contexts but lacked the generalizability and scalability offered by later developments. The shift to PLMs like BERT mark a substantial enhancement in foundational capabilities (Kenton and Toutanova, 2019; Raffel et al., 2023; Radford et al., 2019). These models leverage extensive pre-trained data, enhancing their ability to understand context and generate text, yet they still face limitations in fully grasping role-specific nuances. To overcome these challenges, researchers deploy innovative strategies like contrastive learning (Huang et al., 2022b), bert-over-bert decouples learning (Song et al., 2021) and attention-based fusion mechanisms (Zheng et al., 2020) improve the integration of personal and dialogue data, enhancing role-playing functionalities.\nLLM. The current frontier in role-playing development is characterized by LLMs such as GPT-4, which boast an unprecedented scale in parameters and pre-training. Such LLMs offer profound improvements in understanding and generating text, capable of maintaining coherent and contextually rich personal dialogues even with minimal prompting. The architecture of these models has largely standardized around the decoder-only framework. Most LLM-based works (Chen et al., 2023b; Tu et al., 2024; Ahn et al., 2024) customize various characters by configuring their personal background information in prompts, aiming at mimicking the specific role. Currently, several role-playing specific LLMs are developed to facilitate future research through instruction-tuning, such as CharacterGLM (Zhou et al., 2023), Xingye12, Xingchen13, Index (Ind, 2024), and Baichuan-Character14.\nTaking CharacterGLM as an example, let's explore how LLMs can be optimized for role-playing support: The process begins with collecting character-related training corpus, where detailed character profiles are developed and utilized to engage in dialogues through either human interactions or LLMs, creating a rich dataset that captures the nuances of character-specific conversations. Following data collection, the next phase is instruction tuning, where the character profiles and accumulated dialogue data are organized into structured instructions. This stage also could incorporate the use of diverse prompts for data augmentation, enhancing the model's ability to generate varied and contextually appropriate responses. The final but optional step, employs self-alignment\u2014using outputs from advanced models for further training-and human feedback to refine and ensure character consistency. This comprehensive approach ensures that LLMs effectively embody and maintain character traits in role-playing scenarios.\nEach stage builds upon the previous, with advancements addressing the limitations of earlier models and opening new possibilities for more complex and engaging role-playing interactions. Beyond the methodologies outlined in public papers, we wish to underscore the influence of pre-training corpora on role-play tasks. From our pre-training experience, a corpus beneficial for training a generic assistant like ChatGPT may not necessarily aid a role-playing task, and the converse is also true. This is understandable, as a generic assistant requires data abundant in 'real' world knowledge (such as news, wiki) or data necessitating complex reasoning (like math, code). Novels, particularly those with a worldview divergent from reality, can induce serious knowledge hallucination issues. Role-playing often presupposes a scenario distinct from the real world, where a certain degree of reasonable knowledge hallucination is encouraged. Therefore, to our knowledge, a crucial step in pre-training an effective foundation model for role-playing involves incorporating a substantial amount of novels into the pretraining corpus, especially those with a worldview distinct from reality."}, {"title": "4.2 Alignment", "content": "Role-playing hinges on the precise alignment of language models with distinctive character-related information. In other words", "categories": "parameter-tuning alignment and parameter-frozen alignment.\n4.2.1 Parameter-Tuning Alignment\nParameter-Tuning involves adjusting the model's parameters to learn character-specific knowledge. In this manner", "include": "n\u2022 Continue-Pretrain: Help models obtain character-related knowledge", "SFT)": "This is the most direct and conventional training approach", "Self-Alignment": "Self-alignment", "self-alignment": "role knowledge collection", "PEFT)": "Given the vast parameters of current LLMs", "Learning": "RLHF (Ouyang et al., 2022; Lambert and von Werra, 2022) is a pivotal enhancement approach utilized predominantly after the SFT stage. In the context of role-playing, RLHF-related methods can also help LLMs to refine and align the generated responses more closely with the intended character traits and behaviors. Shea and Yu (2023) utilizes offline RL strategies to improve the persona consistency. Similarly, COMEDY (Chen et al., 2024b) utilizes GPT-4 to construct memory-based personal responses and memory-against personal responses, forming positive-negative pairs, and then employ DPO (Rafailov et al., 2023) strategies for aligning LLMs to generate more coherent memory-based personalized responses.\nHowever, based on our practical engineering experience, we've found that 1) Reinforcement learning approaches, when using a reward model constructed through in-context learning, generally cannot exceed their inherent role-playing capabilities. 2) The task of annotating high-quality preference data for role-playing is significantly more challenging than for a generic assistant, as it necessitates a deep understanding of the specific character to accurately annotate preferences. For example, during the annotation of the HPD datasets (Chen et al., 2023b), the authors enlisted the help of five avid Harry Potter fans to annotate Harry Potter's behavior.\n4.2.2 Parameter-Frozen Alignment\nThe parameter-frozen alignment approaches in role-playing offer a versatile framework to adapt to new roles without extensive retraining of the model's parameters. These methods focus on utilizing"}]}