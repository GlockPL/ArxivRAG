{"title": "ChainBuddy: An AI Agent System for Generating LLM Pipelines", "authors": ["Jingyue Zhang", "Ian Arawjo"], "abstract": "As large language models (LLMs) advance, their potential applications have grown significantly. However, it remains difficult to evaluate LLM behavior on user-specific tasks and craft effective pipelines to do so. Many users struggle with where to start, often referred to as the \"blank page problem.\" ChainBuddy, an AI assistant for generating evaluative LLM pipelines built into the ChainForge platform, aims to tackle this issue. ChainBuddy offers a straightforward and user-friendly way to plan and evaluate LLM behavior, making the process less daunting and more accessible across a wide range of possible tasks and use cases. We report a within-subjects user study comparing ChainBuddy to the baseline interface. We find that when using AI assistance, participants reported a less demanding workload and felt more confident setting up evaluation pipelines of LLM behavior. We derive insights for the future of interfaces that assist users in the open-ended evaluation of AI.", "sections": [{"title": "1 Introduction", "content": "Over the past two years, the growing interest in Al has spawned a plethora of tools, APIs, and best practices for creating applications based on LLMs. These advancements include a wide range of techniques, from prompt engineering and LLM evaluation platforms to sophisticated AI agent systems equipped with tool use capabilities [1, 4, 33, 41]."}, {"title": "2 Related Work", "content": "Since the release of ChatGPT, the LLM landscape has blossomed into a plethora of proprietary and open-source models, infrastructure, and tooling to support LLM operations (sometimes called \"LLMOps\"). The unique power of LLMs, alongside their stochastic, nondeterministic nature and some high-profile incidents of bias [42], have raised the question of how best to integrate them into larger software systems in a manner that is robust and safe. How to build \"LLM-integrated software\" is thus emerging as a unique subdiscipline within software engineering, and comprises a number of operations, from prompt engineering, to systematic evaluations, to chaining LLM calls (introduced by Wu et al. in HCI as \"AI chains\" [45]) as well as more complex network structures [43], and providing LLMs access to tools-the ability to call functions that perform actions on the user's machine. The term \"Al agents\" has come to be synonymous with the latter two architectures.\nTo support developers in exploring these new practices, a slew of graphical user interfaces and programming libraries have emerged to fill the gap. Coding APIs like LangGraph, CrewAI, and Auto-Gen [9, 24, 43] support developers in creating flows of AI agents (LLM-integrated submodules) that pass messages to each other in asynchronous-style collaborative architectures. Tools like EvalLM, PromptMaker, BotDesigner, and promptfoo [18, 21, 41, 46] support prompt engineering, while LLM Comparator and ChainForge go further, supporting cross-model comparison, automated code- and LLM-based evaluations, visualizations, and chaining of prompts [1, 19]. Some coding APIs serve to guard against the unpredictable nature of LLM outputs, such as Guardrails, LangChain, and Instructor [10, 14, 17]. Another pattern is the rise of data flow-based interfaces for LLMOps, such as Flowise, LangFlow, ChainForge and PromptChainer, visual programming environments usually (though not exclusively) targeting app development [1, 12, 26, 44].\nThe many practices of LLMOps have led to recent proposals to automate parts of the process\u2014from synthetic input data generation and mining the internet for datasets [5, 13], to prompt optimization [20, 34], to helping users generate automated evaluators that align with their preferences [31-33]. For instance, DSPy and Teola serve as prompt and chain optimization frameworks [20, 37]. However, there remains a problem at a higher level of abstraction: that users, even AI/ML experts, struggle to set up pipelines and automated evaluations of LLM behavior, with Arawjo et al. concluding that \"more work needs to be done on [the] conceptualization and planning aspects\" of supporting users in LLM pipeline creation [1, 46]. Part of the issue is certainly the usual difficulty of learning a new interface, but the larger issue is conceptual: what is the \"right way\" to prompt engineer? To set up a pipeline? To evaluate LLM behavior? We and the community are still learning these best practices.\nThe problem of pipeline generation bears a similarity to AutoML, an area of research in machine learning that focuses on automatically designing machine learning (ML) pipelines to train new ML models, whether in part or in full [3, 11, 16]. Certainly, AutoML faces unique challenges compared to the typical problems in the LLMOps space (such as managing very large training datasets, deciding upon hyperparameters and weighing trade-offs in terms of expected training cost and performance). Inspired by this line of ML research, here we investigate the end-to-end generation of LLM pipelines, an emerging research area we call AutoLLMOps: from a single user prompt, can we generate an inspectable, interactive, and editable pipeline, complete with input data, prompt(s) and model(s), and even automated evaluations? Could we build such a system for open-ended tasks? What benefits would users derive from it? And what dangers are there, if any, to automation? To the best of our knowledge, no HCI paper has yet investigated this question. With new LLM agent frameworks, emerging research is showing that this kind of end-to-end generation of a workflow is possible, albeit for different tasks like descriptive analytics given a user-provided dataset [4] or for chatbot creation [30]. Here, we apply this idea to LLM pipelines, specifically focusing on pipelines that help users evaluate LLM behavior."}, {"title": "3 ChainBuddy System Design", "content": "To automate the creation of LLM pipelines, we focused on creating a flexible, user-friendly interface that could support a wide range of use cases beyond just prompt engineering. We decided to build a chatbot-style assistant interface, ChainBuddy, within the open-source ChainForge platform [1]. This choice was driven by the need for an intuitive and interactive environment that can support various users across a range of different pipelines, such as data processing, prompt optimization, LLM auditing, and more. Our goal was to ensure that ChainBuddy could handle diverse and complex requirements while being accessible to users with varying levels of technical expertise and use cases. Note that our ultimate goal is to extend ChainBuddy to guide users beyond initial generation, i.e., to edit existing flows; however, due to the complexity of the agent system and open-ended nature of the problem, we consider here only the ability of ChainBuddy to generate flows from scratch."}, {"title": "3.1 Interface and Example Usage", "content": "The ChainBuddy assistant can be seen in Figure 2. The assistant comprises a standard chat interface in the bottom-left hand corner of the ChainForge platform. The user starts a chat with the assistant to explain their problem (Fig 2a). The assistant then holds a Q&A conversation with the user to disambiguate user intent [28, 38] (Fig 2b). This comprises a pass where the assistant asks a set of up to three questions, and the user can respond individually to each question by filling out a form (Fig. 2c). At any time, the user may end the disambiguation and trigger the AI to generate a flow by clicking the button (Fig. 2d). The user can then inspect the generated flow or request a new generation (Fig. 2e). We kept the assistant interface simple as the majority of our contribution's complexity lies in the agent architecture and flow generation capabilities.\nNote that in this paper, we focus on explaining and showing the ChainBuddy interface and workflow, rather than the baseline interface's built-in features like the Response Inspector (table of LLM responses) and different nodes. We point readers unfamiliar with the ChainForge platform to the public documentation or the paper [1]."}, {"title": "3.2 System Architecture", "content": "ChainBuddy is built on LangGraph [24], a library designed for constructing stateful, multi-actor applications with LLMs. Lang-Graph's core benefits include its ability to handle cycles, provide fine-grained controllability, and ensure persistence. These features are essential for creating reliable agent-based workflows that can support advanced human-in-the-loop and memory functionalities. We use Anthropic's Claude 3.5 Sonnet for the front-end requirements-gathering agent, and OpenAI's GPT-40 for all agents in the backend."}, {"title": "3.2.1 Requirement gathering", "content": "The design of the requirement gathering agent for the ChainForge platform draws inspiration from the Chain of Reasoning [35] prompting methodology adapted from the open-source GitHub project Professor Synapse, an \"Al guide designed to help users achieve their goals\" [36]. The structured interface for intent elicitation was inspired by ExploreLLM [28]. The agent employs a dictionary that updates context about the primary user goal, a list of current requirements the solution should address,"}, {"title": "3.2.2 Workflow generation", "content": "Inspired by concepts from Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models [40] and projects like Baby-AGI [29], we designed ChainBuddy's agentic system to generate long-term plans based on user requirements (Figure 3). This approach involves breaking down each task into specific, manageable actions that can be executed by individual agents which return structured data to upstream agents (i.e., JSON). This design allows each agent to focus on a single task, improving efficiency and accuracy. Key architectural features include:\n\u2022 Requirement Gathering Chat Assistant: A chat-focused agent interacts with the user to disambiguate user intent and gain context for their problem, before proceeding to the generation step.\n\u2022 Planner Agent: Takes the specification from the front-end of the user goal, and develops a comprehensive plan for implementation. The Planner is passed contextual information on all nodes in ChainForge that it has access to, their names and descriptions, and how they are allowed to connect.\n\u2022 Task-Specific Agents: Each task in the plan is assigned to a specific agent, allowing for focused execution. Here, a \"task\" largely maps to different nodes in the ChainForge interface that need to be generated.1 This specialization can allow utilizing smaller, less powerful models for execution tasks while reserving larger, more capable models for planning.\n\u2022 Connection Agents: These agents take the task-specific output (as JSON data representing ChainForge nodes to add), create edge specifications to connect them and fill in starter x-y positions for nodes.\n\u2022 Post-hoc Reviewer Agent: A final Reviewer agent assesses the generated flow against the initial user criteria given to the Planner agent. The Reviewer can trigger the Planner to"}, {"title": "3.3 Early Feedback", "content": "We designed ChainBuddy through an iterative process of testing internally on new tasks, from prompt comparison, to model comparison, to evaluating LLMs for identity-based bias. In particular, we found that the AI had a tendency to overfit to few-shot examples (e.g., always choosing to evaluate two models, or use specific input data); based on this, we removed few-shot examples in specific places such as the Planner agent prompt. We also conducted informal pilot studies to gather early feedback and improve our system. Some early insights we discovered:\n\u2022 Intent disambiguation: One prompt entered by users did not usually contain enough information to generate a detailed workflow that addresses user's actual needs. Based on this, we opted for a more interactive chat.\n\u2022 Structured elicitation: We implemented ChatGPT-like chat where the system asks users questions. However, the LLM output was often long or listed several questions, and users struggled to reply in a natural way. Based on this, we opted for a structured form-filling approach, letting the user clarify only the questions they want to. We also limited the number and length of questions.\n\u2022 Feedback: Users suggested incorporating features like visualizing the loading progress, as well as providing explainable Al elements to help users understand how the system arrives at certain visualizations or results.\n\u2022 Desire to edit existing flows: Some users wanted to continue the assistant chat, asking ChainBuddy to revise or extend the flow. We too wanted this feature, but felt it was too complex to address within the limits of a single paper."}, {"title": "4 Usability Study", "content": "To evaluate ChainBuddy, we ran a within-subjects, mixed-methods user study against the baseline interface (ChainForge without Chain-Buddy), since it was the most direct comparison to a \"manual\u201d open-ended system for setting up LLM pipelines. Our goals were broadly focused on how people would want to use an Al assistant for generating evaluations of LLM behavior; specifically, for our qualitative evaluation:\n(1) What aspects of the assistant do they appreciate the most, compared to the baseline?\n(2) Do users find the requirements-gathering interaction helpful or necessary?\n(3) What kinds of problems do participants want to use Chain-Buddy for? (free exploration task)\n(4) Do people feel that their ideas or hypotheses changed after interacting with the assistant?\n(5) How do people edit the generated flows? What kinds of edits do they tend to make?\n(6) Do people learn anything from interacting with the assistant (and if so, what)?\nWe also sought quantitative, subjective metrics for the following hypotheses for the structured tasks measured via self-reported completion times,2 NASA TLX cognitive load scale [15], and a subset of five system usability Likert questions3 derived from the System Usability Scale [6]:"}, {"title": "4.1 Recruitment and Participants", "content": "We recruited 12 in-lab participants around our North America-based university through listservs and Slack channels, mainly in computer science and engineering contexts. Participants were between ages 18-34 (seven between 23-27, three from 28-34 and one 18-22) and balanced across gender (7 female; 6 male). Ten out of 12 were from computer science or engineering backgrounds (the other two were from Neuroscience and Life Sciences, respectively). They reported a relatively high past experience with LLMs (\u03bc=3.83, \u03c3=0.71 on a scale 1-5, with 5 highest) as well as Python programming knowledge (\u03bc=3.83, \u03c3=1.11; only P6, who had a background in Life Sciences, indicated no knowledge). Half of the participants self-reported as having \"worked on a university study, paper, or project involving the evaluation of [LLMs]\" and three participants had heard of or used ChainForge prior to the study. Each study took 75 minutes, and participants were compensated $30 in cash (CAD)."}, {"title": "4.2 Methodology, Procedure and Tasks", "content": "We designed a within-subjects study with mixed methods. There were two Conditions: Assistant (with ChainBuddy) and Control (the baseline ChainForge interface without ChainBuddy). We also devised two tasks of roughly the same style and difficulty, and"}, {"title": "4.3 Data Analysis", "content": "Participant interactions were screen and audio recorded. We also captured log data for chats with the Assistant and saved flows (files) for later analysis. We analyzed qualitative data through inductive thematic analysis; specifically, the first author iteratively affinity diagrammed all participant remarks during the study (including post-interview) to arrive at clusters (codes). These clusters and codes were then discussed by both authors until we reached consensus.\nWe analyzed quantitative data using a repeated measures linear mixed effects model in R [27], examining fixed effects of Condition, Task, and Order and all interaction effects between these factors, and controlling for random effect of Participant. P-values for main and interaction effects are calculated using Satterthwaite's method for degrees of freedom and reported from ANOVA tables with lmerTest [23]. Post-hoc tests were done via estimated marginal means (emmeans) with Bonferroni correction; when including the"}, {"title": "5 Findings", "content": "We summarize major quantitative findings here before delving into specifics. Overall, participants perceived the Assistant as significantly less mentally demanding and less physically demanding than the Control. Participants were also more confident and more performant when using the Assistant. Finally, participants created at least one more type of node in the Assistant condition, on average, with a portion of the effect explained by the Assistant generating Evaluator nodes (where only one participant created a Python Code Evaluator in the Control condition).\nThere were also two ordering effects by task for perceived Successfulness and Ease of Use, both when experiencing the Control condition after the Assistant. Though these results are specific, interact with Task (see below) and could be considered random fluctuations or demand characteristics given the small study size; however, considered together, they might suggest a small \"missing the assistant\" effect-using the Assistant first helped some participants apply their knowledge to the next task, but it also may prime them to perceive the baseline interface as harder to use."}, {"title": "5.1 Quantitative results", "content": "We summarize major quantitative findings here before delving into specifics. Overall, participants perceived the Assistant as significantly less mentally demanding and less physically demanding than the Control. Participants were also more confident and more performant when using the Assistant. Finally, participants created at least one more type of node in the Assistant condition, on average, with a portion of the effect explained by the Assistant generating Evaluator nodes (where only one participant created a Python Code Evaluator in the Control condition).\nThere were also two ordering effects by task for perceived Successfulness and Ease of Use, both when experiencing the Control condition after the Assistant. Though these results are specific, interact with Task (see below) and could be considered random fluctuations or demand characteristics given the small study size; however, considered together, they might suggest a small \"missing the assistant\" effect-using the Assistant first helped some participants apply their knowledge to the next task, but it also may prime them to perceive the baseline interface as harder to use."}, {"title": "5.1.1 Time to completion (H1)", "content": "We did not find a significant main effect at p<0.05. We report a borderline significant interaction effect for Condition given Order (\u03b2= \u2013 239.5, t= 2.22, p=0.04): when people experienced the Assistant after the Control condition, they perceived they were finished with the task faster (\u03bc=357 secs versus \u03bc=610; 95% CI [195, 518] versus [445, 771])."}, {"title": "5.1.2 Workload Demand (NASA TLX) (H2)", "content": "We find main effects of Condition on mental demand and physical demand, with participants finding tasks less mentally demanding (\u03b2= \u2013 0.91, t= 2.66, p=0.01) and less physically demanding (\u03b2= \u2013 1.08, t=-3.15, p=0.01) when using the Assistant (Figure 4). For perceived successfulness, there is also a significant effect of Task (p=0.66, t=2.53, p=0.02) and two interaction effects of between Condition and Task (p=0.02), and Task and Order (p=0.006). Post-hoc emmeans tests suggest two sources for the effect: participants report they are more successful at Temail than Ttweet, but only when Temail appeared second, possibly indicating a learning effect (\u03b2=1.5, t=4.02, p=0.005); and participants felt they were less successful when solving Ttweet in the Control condition (\u03b2=1.33, t=3.57, p=0.01). No other questions reach significance at p<0.05."}, {"title": "5.1.3 Perceived System Usability (H3)", "content": "We find a significant main effect of Condition on Confidence (p=0.5, t=2.44, p=0.04). Participants felt moderately more confident using the Assistant (Fig 4).\nFor Ease of Use, we found a three-way interaction between Condition, Task, and Order (p=0.003). Visual inspection of data reveals"}, {"title": "5.1.4 Performance (H4)", "content": "We estimated user performance on the task with three binary (true-false) measures shared across both task descriptions: whether their final flow explicitly compares two prompts, whether the flow runs (i.e., the Prompt Node can run), and whether the flow uses template chaining to compare prompts (the last is a feature introduced in the tutorial and was the expected solution, but is not the only one). Plots show strong evidence in favor of the Assistant condition (Figure 5), indicating that, despite participants rating themselves of having similar success in both interfaces, two-thirds of participants did not adequately solve the task in the Control condition (did not create a workflow helping users to compare prompts), compared to eleven (out of 12) participants when using the Assistant."}, {"title": "5.1.5 Number and Types of Created Nodes", "content": "Finally, we examined the Number of Nodes in participants' final flows, as well as the Number of Node Types (e.g., a flow with two TextFields and one Prompt Node has 3 nodes and 2 types of nodes). Participants created a similar number of nodes regardless of condition or task-suggesting a similar complexity and size of their pipelines. However, for types of nodes, we find main effects of Condition (\u03b2=0.583, t=4.04, p=0.003) and Order (\u03b2=-0.417, t=-2.887, p=0.02). On average, people created significantly more node types in the"}, {"title": "5.2 Qualitative results", "content": "We examined interactions and interview data across the structured tasks and unstructured exploration. Overall we found several key insights into participants' experiences with ChainBuddy:\n\u2022 ChainBuddy helps users in overcoming the \"blank page problem,\" converting vague ideas into concrete workflow drafts."}, {"title": "5.2.1 Assistant as a starting point", "content": "Eight participants found that ChainBuddy provided a good starting point for their tasks, helping them overcome the initial challenge of starting from scratch. P5 said, \"maybe it's hard to start with an empty page. It's better to start with a little bit of question and answer here, so you have a base structure.\" The assistant helped users quickly generate initial workflow drafts, reducing the effort required to start from scratch. P1 appreciated that \"the assistant can give me, like a first draft. That is pretty good.\" Similarly, P9 highlighted the importance of having an example to \"put things into context much more easily,\" which helped them decide whether to proceed with the assistant's suggestions or refine their own approach."}, {"title": "5.2.2 Requirement gathering process helped refine ideas and elicit requirements", "content": "Six participants reflected that the assistant's structured and iterative approach to intent disambiguation made the process more organized and easier to follow compared to traditional prompting. Moreover, the iterative questioning made users think more deeply about their tasks and specify details they might otherwise overlook, which they felt led to more detailed pipelines. For instance, P12 remarked that \"sometimes... I know exactly what I want, but I'm not necessarily putting it all in the prompt.\" P9 found that \"the follow-up questions made you create a more comprehensive workflow\" and P8 was \"really surprised when it asked questions based on what I need.\" The form-filling interface helped users to provide information in a more manageable and less overwhelming way than writing everything at once (e.g. P9: \"I was pleasantly surprised that you can type the answers under each question, instead of having just to blurt it all out in a long piece of text\"). By asking users relevant questions, the assistant could also help broaden their thinking and refine their ideas. P1 found that the assistant \"asked me other questions, and I think of like, 'Oh, it's true, I can do this.\u2019\" P7 said \"it was impressive, because it's started to ask me about the design itself, it was clear and precise,\" and commented that the assistant encouraged him to \u201cthink more on the abstract side of the idea itself.\u201d\nHowever, participants could also worry that Q&A intent elicitation could become excessive, with four suggesting rounds limits or guidance on when sufficient information has been provided. P9 worried about getting \"too caught up in the details, which is me answering the supplemental questions too much. Like I keep on digging... and you end up creating a model that's just too rigid.\" P12 also\""}, {"title": "5.2.3 Exceeding user expectations", "content": "Ten participants remarked the assistant not just met, but exceeded their initial expectations. Participants consistently expressed surprise by the assistant's capabilities, particularly in its abilities to ask relevant follow-up requirement-gathering questions and generate detailed workflows. For instance, P2 remarked, \"I was not expecting it to generate even remotely close workflow. But it gave me two additional nodes, understood that these are my requirements, so I might use it.\" P1 was surprised at how well the assistant performed: \"It's asking really good like, follow up questions that made me rethink the problem and generated a good flow for it.\" Similarly, P8 initially expected a basic chatbot but was impressed when the assistant \"asked questions based on what I need,\" which helped expand their ideas and improve their prompts. P6 was particularly impressed by how ChainBuddy was able to generate a workflow for a complex goal, stating, \"I asked it to compare detailed incomes for 10 medical specialties, and it did really well, and it also handled a more complex task without misunderstanding.\"\nWhen asked to clarify, participants explained that they expected the assistant to be a basic Q&A chatbot, but found it more interactive and insightful. For example, P10's \"expectation was something like ChatGPT that actually gives you the final output, not asking too many questions, back and forth,\" but they appreciated how the assistant \"tries to understand what's your goals, what's your criteria. So I love it better than ChatGPT.\""}, {"title": "5.2.4 Reducing effort", "content": "Participants felt the assistant reduced their workload and streamlined task execution, complementing our quantitative results. P10 said \"the mental demand was much less when I used the assistant,\" which was particularly beneficial because the assistant helped \"translating my inquiries to this platform,\" making the experience accessible for beginners. Similarly, P8 observed that without the assistant, it was \"pretty hard to think on my own, like making flows and like arranging it properly,\u201d whereas the assistant streamlined the process. Without the assistant, participants could find manually setting up flows confusing or inefficient. P6 reflected that work without the assistant was \"a lot harder than I expected. It seems that the task is simple at first, but then when I do it manually, some of the things does not fall into the category that I expected."}, {"title": "5.2.5 Accelerating workflow", "content": "Participants remarked that the assistant significantly accelerated their workflow. P11 remarked that \"with the assistant, it's really quick. It gives you the ideas, and it makes all the necessary connections on the platform, so you save up that manual time of creating the whole workflow. And on top of that, it also saves mental efforts to start with the prompt.\" Similarly, P9 felt"}, {"title": "5.2.6 Helping users learn the platform", "content": "Some participants felt that the assistant helped them learn and adapt to the platform, reducing the learning curve and making the tool more accessible for first-time users. P7 highlighted that \"usually when you don't have an assistant and have nothing to work with you need, similar to learning new coding languages,\" but with the assistant, the process was \"very fast.\" P3 noted, \"for the first exercise, I really didn't have much ideas of what I should do, after using the assistance, it's just much more clear.\" P5 also found the assistant helpful in guiding them through tasks, saying that \"it gives a sample that shows how these [nodes] could be connected. I just saw that, oh, I can name these [nodes] with relevant things [titles of the nodes], it's good to name relevant [nodes] especially for someone using the tool for the first time, it's very helpful.\" P3, who had limited experience with AI tools, was impressed by the assistant's capability, saying, \"The assistants just enlightened me,\" and found it surprisingly effective compared to their prior experiences with AI chatbot from other interfaces.\nThe assistant was especially beneficial for helping participants overcome initial obstacles to understanding the interface. P6 shared that without the assistant, \"I don't really know what to do sometimes,\" but with the assistant, they \"did not need to think [about] anything.\" P7 also said that\"without the assistant, there would be obstacles for first-time users, a lot of people will struggle,\" like the experience to the challenges of using something unfamiliar. P11 felt that the assistant \"changed [my approach] to be better\"."}, {"title": "5.2.7 Streamlining prompt engineering", "content": "In our study, there were three participants who often did prompt engineering in their daily work. P7 explained that in their prompt engineering tasks, they often need to \"write a program for each LLM and collect responses,"}, {"title": "5.2.8 Interaction patterns with the assistant", "content": "We identified three patterns in the process of participants' usage of the assistant, derived from qualitative analysis of the screen-capture recordings of the structured task and free exploration time. Many participants (8) made only minimal edits to assistant's solution, tweaking input values but keeping its overall structure. This approach was observed mainly during the structured task, with seven participants reliant during the task, versus only three in free exploration. Second, we saw participants heavily revising the assistant's solution without changing its structure, entirely changing prompts in addition to input data. This approach allowed participants to tailor the assistant's suggestions to better fit their needs while still trusting"}, {"title": "5.2.9 Concerns about assistant bias and influence", "content": "Some participants expressed concerns that using the assistant influences their problem-solving approach, potentially biasing their subsequent tasks. P9 reflected on this impact: \"The first task I did was with the ChainBuddy. The second one was without, [but] I think because I started off with [it], it kind of taught me how to do it.\" Two participants noticed that their approach to their ideas in the free exploration time mirrored the structure introduced by the assistant. P3 said, \"I didn't use the assistant because the structure that is in my mind is kind of similar to the last one done by the assistant,\" indicating that the assistant's influence persisted even when not directly used. P12 also described how she \"started building a chain from an existing one, and that one is created from the second task, which is done by the assistant.\" This continuity suggests that initial interactions with the assistant could set a pattern that users continue to follow, even when trying to work independently. We reflect on this risk further in Discussion."}, {"title": "5.2.10 Use cases in free exploration time", "content": "During the free exploration time, participants explored a diverse range of ideas, showcasing creative and varied uses in collaborating with the assistant to address specific needs and interests. The diverse range of topics reflected a similar diverse range in the study of ChainForge [1], the baseline interface. Participant usage fell into three major categories: comparing and exploring different prompts, evaluating behavior across different LLMs, and testing LLMs for bias and handling of sensitive topics."}, {"title": "5.3 Limitations", "content": "Our within-subjects usability study had a small sample size and compared to a single baseline. It could be that for a different LLM pipeline interface, we might have seen different results. We did not restrict users by prior knowledge of the baseline interface-it could be that expert ChainForge users would feel differently about the assistant's capabilities. For various constraints common to usability studies, we also only examined two tasks that both involved comparing across prompts, even though ChainBuddy is capable of more types of tasks and LLM pipelines, such as comparing responses across models or setting up data processing workflows. An alternate study design might explicitly try three or more very different tasks with a larger sample size, to better understand when ChainBuddy is and is not useful. We also relied on rough indicators of performance; a more robust evaluation might have a team of external raters grade the quality of created flows on a scale. Finally,"}, {"title": "6 Discussion", "content": "Our findings provide evidence in favor of AI-assisted interfaces for generating LLM pipelines. Participants found their workload less demanding when using an AI assistant's help in an interface for setting up evaluative LLM pipelines, compared to without. And, they were more confident, performant, and able to create automated evaluations with greater regularity. In post-interviews, participants appreciated how the requirements gathering interaction supported refining their ideas, and consistently expressed that they were impressed by the assistant's capabilities. We also observed interaction effects that suggest participants can experience lower perceived successfulness and interface ease-of-use when Al assistance is taken away from them (mediated by task); a generous interpretation is that people miss the assistant's help in reducing their workload.\nOne interesting null result is time. We did not find significant qualitative evidence that ChainBuddy helped people solve the task faster, as might be expected (although participants did express this feeling in post-interviews). However, three things to note. First, ChainBuddy takes time to generate flows, on the order of 10 seconds or greater. Second, our times are subjective, with a participant-reported indication of being \"done.\" Participants using the assistant spent time revising, running or inspecting the generated flows, or clarifying their intent with the assistant. Third, qualitative data indicates that some participants felt they could go further in their analysis with the assistant, compared to without. So, though participants spent around the same time per condition, what they spent that time on differed.\nMore broadly than LLM pipelines, our work contributes to a growing body of literature on Al agents integrated into software platforms to assist users in implementing ideas (e.g., an agent that creates Powerpoint slides, visualizations from datasets, or dynamic widgets [4, 39, 47]). Our study provides evidence for the intuitive hypothesis that users perceive that AI agents improve workload by reducing the mental and physical burdens placed on them when interacting with software-clicking and dragging, planning what to do. However, our quantitative findings indicate that for the most part, users perceived they were just as successful without the assistant-even when performance analysis suggests overall performance plummeted without the assistants' help. This finding resembles the Dunning-Kruger effect [22]-that people who lack the skill to evaluate the quality of their work in a domain may over-estimate its quality-and, combined with worries about being biased by the assistant (discussed below), contributes to growing concerns about non-experts' over-reliance on AI systems [2, 7]. Future studies on software-integrated AI assistants should complement subjective measures with more objective evaluations of user performance (e.g.,"}]}