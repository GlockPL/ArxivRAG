{"title": "A Spitting Image: Modular Superpixel Tokenization in Vision Transformers", "authors": ["Marius Aasan", "Odd Kolbj\u00f8rnsen", "Anne Schistad Solberg", "Ad\u00edn Ramirez Rivera"], "abstract": "Vision Transformer (ViT) architectures traditionally employ a grid-based approach to tokenization independent of the semantic content of an image. We propose a modular superpixel tokenization strategy which decouples tokenization and feature extraction; a shift from contemporary approaches where these are treated as an undifferentiated whole. Using on-line content-aware tokenization and scale- and shape-invariant positional embeddings, we perform experiments and ablations that contrast our approach with patch-based tokenization and randomized partitions as baselines. We show that our method significantly improves the faithfulness of attributions, gives pixel-level granularity on zero-shot unsupervised dense prediction tasks, while maintaining predictive performance in classification tasks. Our approach provides a modular tokenization framework commensurable with standard architectures, extending the space of ViTs to a larger class of semantically-rich models.", "sections": [{"title": "1 Introduction", "content": "Vision Transformers [14] (ViTs) have become the cynosure of vision tasks in the wake of convolutional architectures. In the original transformer for language [12, 42], tokenization serves as a crucial preprocessing step, with the aim of optimally partitioning data based on a predetermined entropic measure [20, 34]. As models were adapted to vision, tokenization was simplified to partitioning images into square patches. This approach proved effective [7, 25, 38, 39, 40, 41], and soon became canonical; an integral part of the architecture.\nDespite apparent successes, we argue that patch-based tokenization has inherent limitations. Firstly, the scale of the tokens are rigidly linked to the model architecture by a fixed patch size, ignoring any redundancy in the original images. These limitations result in a significant increase in computation for larger"}, {"title": "1.1 Motivation", "content": "We take a step back from the original ViT architecture to re-evaluate the role of patch-based tokenization. By focusing on a somewhat overlooked component in the architecture, we look to establish image partitioning as the role of an adaptive modular tokenizer; an untapped potential in ViTs.\nIn contrast to square partitions, superpixels offer an opportunity to mitigate the shortcomings of patch-based tokenization by allowing for adaptability in scale and shape while leveraging inherent redundancies in visual data. Superpixels have been shown to align better with semantic structures within images [37], providing a rationale for their potential utility in vision transformer architectures. We compare the canonical square tokenization in standard ViTs with our"}, {"title": "1.2 Contributions", "content": "Our research induces three specific inquiries: (a) Is a rigid adherence to square patches necessary?, (b) What effect does irregular partitioning have on tokenized representations?, and (c) Can tokenization schemes be designed as a modular component in vision models? In this work we establish the following;\n\u2022 Generalized Framework: Superpixel tokenization generalize ViTs in a modular scheme, providing a richer space of transformers for vision tasks where the transformer backbone is independent of tokenization framework.\n\u2022 Efficient Tokenization: We propose an efficient on-line tokenization approach which provides competitive training and inference times as well as strong performance in classification tasks.\n\u2022 Refined Spatial Resolution: Superpixel tokenization provides semantically aligned tokens with pixel-level granularity. We demonstrate that our method yields significantly more faithful attributions compared to established explainability methods, as well as strong results in unsupervised segmentation.\n\u2022 Visual Tokenization: The main contribution of our work is the introduction of a novel way of thinking about tokenization in ViTs, an overlooked but central component of the modeling process cf. discussion in Section 4.\nOur primary objective is to evaluate tokenization schemes for ViTs, underscoring the intrinsic properties of alternative tokenization. In the interest of a fair comparative analysis, we perform our study using vanilla ViT architectures and established training protocols [36]. Hence, we design experiments to establish a fair comparison against well-known baselines without architectural optimizations. This controlled comparison is crucial for attributing observed disparities specifically to the tokenization strategy, and eliminates confounding factors from specialized architectures or training regimes.\nNotation: We let H\u00d7W = {(y,x) : 1 \u2264 y \u2264 h,1 \u2264 x \u2264 w} denote the coordinates of an image of spatial dimension (h, w), and let I be an index set for the mapping i\u2192 (y,x). We consider a C-channel image as a signal \u00a7: I \u2192 RC. We use the vectorization operator vec: Rd1 \u00d7...\u00d7dn \u2192 Rd1.dn, and denote function composition by f(g(x)) = (f\u00b0g)(x)."}, {"title": "2 Methodology", "content": "To evaluate and contrast different tokenization strategies, we require methods for partitioning images and extracting meaningful features from these partitions. While these tasks can be performed using a variety of deep architectures, such approaches add a layer of complexity to the final model, which would invalidate"}, {"title": "2.1 Framework", "content": "We generalize the canonical ViT architecture by allowing for a modular tokenizer and different methods of feature extraction. Note that a canonical ViT is generally presented as a three-component system with a tokenizer-embedder g, a backbone f consisting of a sequence of attention blocks, and a subsequent prediction head h. Contrarily, language transformers explicitly decouples g from the backbone f. Following this lead, we note that we can essentially rewrite a patch embedding module as a three component modular system, featuring a tokenizer T, a feature extractor \u03a6, and an embedder \u03b3 such that g = \u03b3\u03bf\u03c6\u03bf\u0442, emphasizing that these are inherent components in the original architecture obscured by a simplified tokenization strategy-cf. Fig 2. This provides a more complete assessment of the model as a five component feedforward system\n\u03a6(\u03be; \u03b8) = (ho f\u00b0g)(\u00a7; \u03b8),\n= (hof\u03bf\u03b3\u03bf\u03c6\u03bf\u03c4)(\u03be; \u03b8),\n(1a)\n(1b)\nwhere \u03b8 denotes the set of learnable parameters of the model. In a standard ViT model, the tokenizer T acts by partitioning the image into fixed-size square partitions. This directly provides vectorized features since patches are of uniform dimensionality and ordering, hence \u03a6 = vec in standard ViT architectures. The embedding \u03b3 is typically a learnable linear layer, mapping features to the embedding dimension of the specific architecture. Alternatively, g can be taken as a convolution with kernel size and stride equal to the desired patch size p."}, {"title": "2.2 Partitioning and Tokenization", "content": "Tokenization in language tasks involves partitioning text into optimally informative tokens, analogous to how superpixels [37] partition spatial data into dis-"}, {"title": "Weight function:", "content": "To apply the edge contraction, we define an edge weight functional w(t) : E(t) \u2192 R. We retain self-loops in the graph to constrain regions by weighting loop edges by relative size. This acts as a regularizer by constraining the variance of region sizes. For non-loop edges, we use averaged features \u03bc(t)(v) = \u03a3i\u2208\u03c0t(v) \u00a7(i)/|\u03c0(t)(v)| and apply a similarity function sim: E(t) \u2192 R.\nLoops are weighted using the empirical mean \u03bc\u03c0(t) and standard deviation \u03c3\u03c0(t) of region sizes at level t. This gives us weights on the form\n\u03c9s(u, v) = \n(sim(\u03bc(t), \u03bc(v); for u \u2260 v;\n(t),\n\u2212 \u03c3\u03c0)\notherwise.\n(2)"}, {"title": "Compactness can optionally be regulated by computing the infinity norm density", "content": "Compactness can optionally be regulated by computing the infinity norm density\n\u03b4\u03b1(u, v) =\n4(|\u03c0(t)| + |\u03c0(t)|)\nper(u, v)2\n(3)"}, {"title": "where per\u221e is the perimeter of the bounding box that encapsulates superpixels", "content": "where per\u221e is the perimeter of the bounding box that encapsulates superpixels u and v. This emphasizes how tightly two neighbouring superpixels u and v are packed in their bounding box, resulting in a regularized weight functional\nw(t) (u, v; \u03bb) = \u03bb\u03b4\u03b1(u, v) + (1 \u2212 \u03bb)\u03c9(t)(u, v)\n(4)\nwhere \u03bb\u2208 [0, 1] serves as a hyperparameter for compactness.\nUpdate rule: We use a greedy parallel update rule for the edge contraction, such that each superpixel joins with a neighboring superpixel with the highest edge weights, including self-loops for all G(t) for t > 1. Let N(t)(v) denote the neighborhood of adjacent vertices of the superpixel with index v at level t. We construct an intermediate set of edges, given by\nE(t) = {v, arg max \u03c9s(u, v; \u03bb) : v \u2208 V(t)\nu\u2208N(t)(v) ,\n(5)\nThen the transitive closure \u00ca(t), i.e. the connected components of \u00ca(t), explicitly yields a mapping V(t) \u2192 V(t+1) such that\n\u03c0(t+1) =\n\u03c0(t)\nU\u2208N() (v)\n(6)\nwhere \u00d1(t)(v) denotes the connected component of vertex v in \u00ca(t). This update rule for the partitions ensures that each partition at level (t + 1) is a connected region, as it is formed by merging adjacent superpixels with the highest edge weights. We illustrate the aggregation step in Fig. 3.\nIterative refinement: We repeat the steps of computing aggregation maps, regularized edge weights, and edge contraction until the desired number of hierarchical levels T is reached. At each level, the partitions become more coarse, representing larger homogeneous regions in the image. The hierarchical structure provides a multiscale representation of the image, capturing both local and global structures. At level T we have obtained a sequence of partitions (\u03c0(t))t=0, where each partition at level t is a connected region with \u03c0(t) \u2286 \u03c0(t+1) for all t.\nWe conduct experiments to empirically verify the relationship between the number of tokens produced by varying the steps T and patch size p in canonical ViT tokenizers. Let NSPIT, NViT denote the number of tokens for the SPiT tokenizer and ViT tokenizer respectively. Remarkably, we are able to show with a high degree of confidence that the relationship is E(T | NSPiT = NViT) = log2 p, regardless of image size. Details can be found in Appendix ??."}, {"title": "2.3 Feature Extraction with Irregular Patches", "content": "While we conjecture the choice of square patches in the ViT architecture to be motivated by simplicity, it is naturally also a result of the challenge posed"}, {"title": "Positional Encoding:", "content": "ViTs generally use a learnable positional embedding for each patch in the image grid. Noting that this corresponds to a histogram over positions over a downsampled image (cf. Prop. 1) we can extend learnable positional embeddings to handle more complex shapes, scales, and positions by using a kernelized approach. We propose applying a joint histogram over the coordinates of a superpixel Sn for each of the n = 1,..., N partitions. First, we normalize the positions such that (y', x') \u2208 [-1,1]2 for all (y', x') \u2208 Sn. We decide on a fixed number of bins B, denoting the dimensionality of our features in each spatial direction using a Gaussian kernel K, such that\n\u03be(pos) = vec\nSn,y,x\n(Ko(y \u2212 yj, x \u2212 xj))\n(7)\ntypically with low bandwith \u03c3\u2208 [0.01, 0.05]. This, in effect, encodes the position of the patch within the image, as well as its shape and scale.\nColor Features: To encode the light intensity information from the raw pixel data into our features, we interpolate the bounding boxes of each patch to a fixed resolution of \u03b2\u00d7 \u03b2 using a bilinear interpolation operator, while masking out the pixel information in other surrounding patches. These features essentially capture the raw pixel information of the original patches, but resampled and scaled to uniform dimensionality. We refer to the feature extractor \u03d5 as an interpolating feature extractor. Similar to positional and texture features, the RGB features are normalized to [-1,1] and vectorized such that \u011d(col) \u2208 R3\u03b22.\nTexture Features: Gradient operators provides a simple robust method of extracting texture information [10, 24]. We use the gradient operator proposed by Scharr [33] due to improved rotational symmetry and discretization errors. We normalize the operator such that \u2207\u00a7 \u2208 [\u22121,1]H\u00d7W\u00d72, where the last dimensions correspond to gradient directions \u2207y, \u2207x. Mirroring the procedure for the positional features, we then construct a joint histogram with a Gaussian kernel over the gradients within each superpixel Sn such that \u03be(grad) \u2208 RB2.\nThe feature modalities are concatenated as \u03be\u03b7 = [(col), (pos), (grad)] \u2208 R5\u03b22. While our proposed gradient features are commensurable with the canonical ViT architecture, they represent an additional dimension of information. We therefore ablate the effect of including or omitting gradient features. For models where these features are omitted, i.e. \u03be\u03b7\\(grad) = [(col), (pos)] \u2208 R4\u03b22, we say that the extractor \u03d5 is gradient excluding."}, {"title": "2.4 Generalization of Canonical ViT", "content": "By design, our framework acts as a generalization of the canonical ViT tokenization, and is equivalent to applying an canonical patch embedder using a fixed patch size p with interpolated gradient excluding feature extraction.\nProposition 1 (Embedding Equivalence). Let \u03c4* denote an canonical ViT tokenizer with a fixed patch size p, let \u03d5 denote a gradient excluding interpolated feature extractor, and let \u03b3\u2217, \u03b3 denote embedding layers with equivalent linear projections L = L\u03b3. Let \u03be(pos) \u2208 RN\u00d7\u03b22 denote a matrix of joint histogram positional embeddings under the partitioning induced by \u03c4\u2217. Then for dimensions H = W = \u03b22 = p2, the embeddings given by \u03b3\u03bf\u03c6\u03bf\u03c4\u2217 are equivalent to the canonical ViT embeddings given by \u03b3\u2217 \u03bf \u03c6\u2217 \u03bf\u03c4\u2217 up to proportionality.\nWe provide necessary definitions and proofs for Prop. 1 in Appendix ??, demonstrating that our proposed framework includes the canonical ViT architecture as a special case; an essential property for modularity."}, {"title": "3 Experiments and Results", "content": "We train ViTs with different tokenization strategies (ViT, RVIT, SPIT) using base (B) and small (S) capacities on a general purpose classification task on ImageNet [11] (IN1K). We design our experiments with the goal of evaluating the quality of the resulting tokenized representations of the images. See details about the training setup in Appendix ??."}, {"title": "3.1 Classification", "content": "We evaluate the models by fine-tuning on CIFAR100 [22] and CALTECH256 [16], in addition to validation using the INREAL labels [4], ablating the effect of gradient features. We also evaluate our models by replacing the linear classifier head with a k-nearest neighbours (kNN) classifier over the representation space of different models, focusing solely on the clustering quality of the class tokens in the embedded space [8, 28]. Table 1 gives an overview of the results. We include results for the Small (S) capacity models in Table ??."}, {"title": "3.2 Evaluating Tokenized Representations", "content": "To evaluate the cohesive quality of the tokenized representations, we look to quantify the faithfulness of attributions, and the model's performance on zero-shot unsupervised segmentation. These were selected to give insight into the embedded context of the tokenized representation of the image.\nFaithfulness of Attributions: One of the attractive properties of ViTs is the inherent interpretability provided by their attention mechanisms. Techniques such as attention rollout [8, 14], attention flow [1], and PCA projections [28] have been leveraged to visualize the reasoning behind the model's decisions. Unlike gradient-based attributions, which often lack clear causal links to model predictions [3], attention based attributions are intrinsically connected to the flow of information in the model, and provide direct insight into the decision-making process in an interpretable manner. They are, however, constrained by the granularity and semantic alignment of the original tokenization scheme. Classical methods such as LIME [30] provides a well-established counterfactual framework for post-hoc explainability with superpixel partitions using Quickshift [43] or SLIC [2] with local linear surrogate models.\nTo quantify the faithfulness of interpretations under different tokenization strategies, we compute the attention flow of the model in addition to PCA pro-jected features and contrast this with attributions from LIME with indepen-"}, {"title": "3.3 Ablations", "content": "Tokenizer Generalization: In in Section 2.4 we showed that our framework generalizes the canonical ViT. This allows us to contrast different tokenization strategies across models by directly swapping tokenizers, emphasizing the modularity of our framework. We report the relative change in accuracy (\u0394 Acc.) of models when swapping tokenizers in Table 4.\nOur results show that ViTs with square tokenization performs poorly when evaluated on irregular patches. We observe an increase in accuracy for RViT models when evaluated over square patches. Furthermore, we see that the SPIT models also generalize well to both to square and Voronoi tokens, but is highly dependent on the gradient features. With gradient features, we note a minor drop in accuracy when evaluating Voronoi tokens with SPiT, and superpixel tokens with RViT. This supports our conjecture that gradient features help encode texture information for irregular patches."}, {"title": "4 Discussion and Related Work", "content": "Related Work Interest in adaptive tokenization is burgeoning in the field. We propose a taxonomy of adaptive tokenization with two main dimensions illustrated in Fig. 6. The first dimension illustrates the coupling or integration of tokenization into the transformer architecture. Several approaches [5, 19, 26] are inherently coupled to the architecture, while others adopt a decoupled approach [18, 31] which more closely aligns with our framework. The taxonomy is extended by a dimension of token granularity, measuring the proximity to modelling with pixel-level precision. Together, these dimensions facilitate an understanding of adaptive tokenization approaches for ViTs.\nA significant body of current research is primarily designed to improve scaling and overall compute for attention [5, 32, 55] by leveraging token merging strategies in the transformer layers with square patches, and can as such be considered low-granularity coupled approaches. Distinctively, SuperToken [19] applies a coupled approach to extract a non-uniform token representation. The approach is fundamentally patch based, and does not aim for pixel-level granularity.\nIn contrast, multi-scale tokenization [18, 31] apply a decoupled approach where the tokenizer is independent of the transformer architecture. These are commensurable with any transformer backbone, and improve computational overhead. While square tokens operate on a lower level of granularity, there is significant potential for synergy between these approaches and our own, particularly given the hierarchical nature of SPiT. On the periphery, Ma et al. [26] propose a pixel-level clustering method with a coupled high granularity approach.\nLimitations Our proposed framework is not optimizable with gradient based methods. Ideally, adaptable tokenization should be learnable in an end-to-end framework. However, such an approach needs to be carefully designed to not add undue computational overhead, and should ideally not be limited by a pre-defined number of tokens. Moreover, we see that irregular tokenization require"}, {"title": "5 Conclusion", "content": "In this work, we posit tokenization as a modular component that generalize the canonical ViT backbone, and show that irregular tokenization with superpixels is commensurable with transformer architectures. Our experiments demonstrate that superpixel tokens have a significant impact on extracted attributions for predictions, and are amenable to unsupervised segmentation tasks without a separate decoder model. Moreover, we show that concatenated gradient features improve performance of base capacity ViTs, and that irregular tokenizers generalize between different tokenization strategies. Our experiments were performed with standard models and training to limit confounding factors in our results."}, {"title": "Further Work", "content": "Our work is distinguishable as a decoupled high-granularity apprach with multiple paths for further work. We see strong potential in exploring graph neural networks (GNNs) for tokenization, and hierarchical properties could be leveraged in self-supervised frameworks such as DINO [8], or pyramid models [45, 46] in a coupled approach. The modularity of our framework provides opportunites for research into the dynamic between ViTs and tokenization. Coupling SPiT with gating [18] or merging [5] could further improve scalability, and allow for a learnable framework. More work can be done in studying the effects of irregularity in feature extraction, as discussed in Section 3.3."}, {"title": "B Equivalence of Frameworks", "content": "Definition 1 (ViT Tokenization). Let \u00a7: H\u00d7W \u2192 RC be an image signal with tensor representation \u00a7 \u2208 RH\u00d7W\u00d7C. The canonical ViT tokenization operator \u03c4\u2217 : RH\u00d7W\u00d7C \u2192 RN\u00d7p\u00d7p\u00d7C partitions the image into N =  [[H/p]].[W/p].non-overlapping C-channel square zero-padded patches. For the case where we have H mod p = W mod p = 0, we get N = HW, and no padding is necessary.pp.Definition 2 (ViT Features). Let p denote the patch dimension of a canonical ViT tokenizer \u03c4\u2217, and let M = p2C. The canonical ViT feature extractor\u03a6\u2217 : RN\u00d7p\u00d7p\u00d7C \u2192 RN\u00d7M is given by \u03a6\u2217 = vecm, where vecm denotes the vectorization operator applied to each of the N patches via p \u00d7 p \u00d7 C \u2192 M.\nDefinition 3 (ViT Embedder). Let \u03a6\u2217 be a canonical ViT feature extractor, and let Q \u2208 RN\u00d7D denote a positional encoding. The canonical ViT embeddery\u2217 : RN\u00d7M \u2192 RN\u00d7D is given by\n\u03b3\u2217(z) = L\u0398z + Q\nwhere L\u0398: RN\u00d7M \u2192 RN\u00d7D is a learnable linear transformation, and Q is either a learnable set of parameters or a function of the positions of the N blocks in the partitioning induced by the canonical tokenizer \u03c4\u2217 .\nLemma 1 (Feature Equivalence). Let \u03c4\u2217 denote a canonical ViT tokenizer with a fixed patch size p, and let \u03d5 denote a gradient excluding interpolating feature extractor with \u03b2 = p. Then the operations \u03c6\u03bf\u03c4\u2217 are equivalent to the canonical ViT operations \u03a6\u2217 \u03bf\u03c4\u2217 .\nProof. The proof is highly trivial but illustrative. Note that for each of the N square patches generated by T, the extractor \u03d5 performs an interpolation to rescale the patch to a fixed resolution of \u03b2 \u00d7 \u03b2. However, for \u03b2 = p the patches already match the target dimensions exactly. It follows that the interpolation operation reduces to identity. The vectorization operator is equivalent for both mappings, hence \u03d5 = vec\u03b3 = \u03a6\u2217 .\nProposition 1 (Embedding Equivalence). Let \u03c4\u2217 denote an canonical ViT tokenizer with a fixed patch size p, let \u03d5 denote a gradient excluding interpolated feature extractor, and let \u03b3\u2217, \u03b3 denote embedding layers with equivalent linear projections L\u0398 = L\u03b3. Let \u03be(pos) \u2208 RN\u00d7B2 denote a matrix of joint histogram positional embeddings under the partitioning induced by \u03c4\u2217. Then for dimensions H = W = \u03b22 = p2, the embeddings given by \u03b3\u03bf\u03c6\u03bf\u03c4\u2217 are equivalent to the canonical ViT embeddings given by \u03b3\u2217 \u03bf \u03c6\u2217 \u03bf\u03c4\u2217 up to proportionality.\nProof. We first note that we can assume \u03be(pos) is a matrix with single entry components, since under \u03b2 = p and N = \u03b22, each vectorized histogram feature is a scaled unit vector cnen with n = 1,..., N. Moreover, since the partitioning inferred by \u03c4\u2217 exhaustively covers the spatial dimensions H \u00d7 W, the histograms essentially span the standard basis, such that \u03be(pos) is diagonal. Furthermore,"}, {"title": "A1 since each patch is of the same size we have equal contribution towards each", "content": "A1 since each patch is of the same size we have equal contribution towards eachenn = cm entry, such that for all m \u2260 n. Therefore, without loss of generality,we can ignore the scalars and simply consider \u03be(pos) = I as an identity matrix.From Lemma 1 we have that z = (\u03a6\u2217 \u03bf \u03c4\u2217)(\u00a7) = (\u03c6\u03bf \u03c4\u2217)(\u03be). Then, since\n\u03b3\u2217(z) = L\u0398z + Q = [L\u0398, Q]IzI=\n\u03b3(z)\n(9)\nwe have that \u03b3 = \u03b3\u2217 up to proportionality for some constant c = cn.\nRemark 1. While we only demonstrate the equality up to proportionality, this can generally be ignored since we can effectively choose our linear projection under \u03b3 to be L\u0398/c. We note that while the equality holds for empirical histograms, equality does not strictly hold for \u03be(pos) computed using KDE with a Gaussian kernel, however we point out that the contribution from the tails of a kernel Ko with a small bandwidth is effectively negligible."}, {"title": "B Preprocessing and Superpixel Features", "content": "Compared to standard preprocessing, we use a modified normalization scheme for the features for improving the superpixel extraction. We apply a combined contrast adjustment and normalization function using a reparametrized version of the Kumaraswamy CDF, which is computationally efficient and allows more fine-grained control of the distribution of intensities than empirical normalization, which improves the superpixel partitioning.\nThe normalization uses a set of means \u03bc shape parameters \u03bb for normalizing the image and adjusting the contrast. The normalization is given by\n(1 \u2212 (1 \u2212 x\u03bb)b),\n(10)\nwhere b is defined by\nb =\nln(2)\nln (1 \u2212 \u03bc\u03bb)\n(11)\nand we set means \u03bcr = 0.485, \u03bcg = 0.456, \u03bcb = 0.406 and \u03bbr = 0.539, \u03bbg = 0.507, \u03bbb = 0.404, respectively.\nThe features used for the superpixel extraction are further processed using anisotropic diffusion, which smoothes homogeneous regions while avoiding blurring of edges. This technique was advocated for superpixel segmentation by Xiaohan et al. [49]. We use the algorithm proposed by Perona and Malik [29] over 4 iterations, with \u03ba = 0.1 and \u03b3 = 0.5. Note that these features are only applied for constructing the superpixels in the tokenizer. We emphasize that we do not apply anisotropic diffusion for the features in the predictive model."}, {"title": "1 Expected no. superpixels with SPiT compared with no. ViT patches", "content": "Importantly, these results also reveal much about effective inference times. In Table 5, we show that the overhead for constructing the superpixels is very low. However, the number of tokens depends on the image. Images with large homogeneous regions will be processed faster, while images with many independent regions will necessary incur a cost. Nevertheless, the results in Table B.1 show that we will, on average, have comparable inference times to a canonical ViT due to the beneficial properties of our proposed superpixel tokenization.\nFinal Thresholding: Adaptable tokenization frameworks does not necessarily entail an overall drop in inference throughput. Contrarily, it could potentially be leveraged to substantially improve inference speed by designing learnable methods to lower the number of tokens without decreasing performance, e.g. ToMe by Bolya et al. [5].\nWe apply an additional final merging step where we compute the euclidean distance between adjacent superpixels and merge all superpixels below a given threshold for our SPiT-B16 model with gradient features. Noting that a threshold of 0.0 retains the original model design, the results in Table B.2 indicate that models with superpixel tokenization can be optimized to improve inference throughput. We also note that taking the maximum performing tokens over all"}, {"title": "C Training Details", "content": "As mentioned in Section 1.2, we use standardized ViT architectures and generally follow the recommendations provided by Steiner et al. [36]. We provide training logs, pre-trained models, and code for training models from scratch in our GitHub project repository (in the camera ready manuscript).\nClassification: Training is performed over 300 epochs using the ADAMW optimizer with a cosine annealing learning rate scheduler with 5 epochs of cosine annealed warmup from learning rate \u03b7start = 1\u00d710\u22125. The schedule maxima and minima are given by \u03b7max = 3\u00d710\u22123, and \u03b7min = 1\u00d710\u22126. We use a weight decay of \u03bbdec = 2\u00d710\u22122 and set the smoothing term \u03f5 = 1\u00d710\u22127. In addition, we used stochastic depth dropout with a base probability of p = 0.2 and layer scaling. Models were pre-trained with spatial resolution 256\u00d7256.\nFor augmentations, we randomly select between using the RANDAUG framework at medium strength or using AuG3 framework by Touvron et al. [39] including CUTMIX [56] with parameter \u03b1 = 1.0. We use RANDOMRESIZECROP using the standard scale (0.08, 1.0) with randomly sampled interpolation modes. Since the number of partitions from the superpixel tokenizer are adapted on an image-to-image basis, we effectively constrain the maximum number of tokens during training using token dropout to balance number of tokens.\nWe found that a naive on-line computation of Voronoi tessellations was unnecessarily computationally expensive, hence we precompute sets of random Voronoi tessellations with 196, 256, and 576 partitions, corresponding to images of 224\u00d7224, 256\u00d7256, and 384\u00d7384 resolutions given patch size p = 16.\nAll training was performed on AMD MI250X GPUs. One important distinction is that we do not use quantization with bfloat16 for training our models, instead opting for the higher 32-bit precision of float32 since this improves consistency between vendor frameworks. Inference was carried out on a mixture of NVIDIA A100, RTX 2080Ti, Quadro P6000, and AMD MI250X to validate consistency across vendor frameworks."}, {"title": "D Interpretability and Attention Maps", "content": "For LIME explanations, we train a linear surrogate model L\u03c6 for predicting the output probabilities for the prediction of each model \u03a6. To encourage independence between tokenizers and LIME explanations, as well as promote direct comparability, we use SLIC with a target of || \u2248 64 superpixels. We use Monte Carlo sampling of binary features for indicating the presence or omission of each superpixel with stochastic p \u2208 Uniform(0.1, 0.3), and keep these consistent across model evaluations. We observed that certain images in the IN1K at times produced less than 5 superpixels using SLIC, hence these images were dropped from the evaluation.\nThe attention flow [1] of a transformer differs from the standard attention roll-out by accounting for the contributions of the residual connections in computations. The attention flow of an l-layer transformer is given by\nAFlow =\nl\u03a0i=1\n((1 \u2212 \u03bb)I + \u03bbAi).\n(12)\nwhere we set \u03bb = 0.9 to account for stochastic depth and layer scaling factors while accentuating the contribution of the attention operators. We use max-aggregation over the heads to extract a unified representation. Following Dosovitskiy et al. [14] and Caron et al. [8], we extract the attention for the class token as an interpretation of the model's prediction."}, {"title": "E Extended Discussion on Classification", "content": "Certain interesting observations can be made from our results in Table 1. Firstly, random Voronoi tessellations perform better than data-driven superpixels for gradient excluding features, and despite its inherent stochasticity, tokenization with random Voronoi tessellations proves to be a relatively effective strategy, and demonstrate surprisingly consistent results over prediction tasks as reported in Table G.1. To account for the stochasticity in validation, we compute accuracy"}]}