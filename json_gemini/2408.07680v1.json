{"title": "A Spitting Image: Modular Superpixel Tokenization in Vision Transformers", "authors": ["Marius Aasan", "Odd Kolbj\u00f8rnsen", "Anne Schistad Solberg", "Ad\u00edn Ramirez Rivera"], "abstract": "Vision Transformer (ViT) architectures traditionally employ a grid-based approach to tokenization independent of the semantic content of an image. We propose a modular superpixel tokenization strategy which decouples tokenization and feature extraction; a shift from contemporary approaches where these are treated as an undifferentiated whole. Using on-line content-aware tokenization and scale- and shape-invariant positional embeddings, we perform experiments and ablations that contrast our approach with patch-based tokenization and randomized partitions as baselines. We show that our method significantly improves the faithfulness of attributions, gives pixel-level granularity on zero-shot unsupervised dense prediction tasks, while maintaining predictive performance in classification tasks. Our approach provides a modular tokenization framework commensurable with standard architectures, extending the space of ViTs to a larger class of semantically-rich models.", "sections": [{"title": "1 Introduction", "content": "Vision Transformers [14] (ViTs) have become the cynosure of vision tasks in the wake of convolutional architectures. In the original transformer for language [12, 42], tokenization serves as a crucial preprocessing step, with the aim of optimally partitioning data based on a predetermined entropic measure [20, 34]. As models were adapted to vision, tokenization was simplified to partitioning images into square patches. This approach proved effective [7, 25, 38, 39, 40, 41], and soon became canonical; an integral part of the architecture.\nDespite apparent successes, we argue that patch-based tokenization has inherent limitations. Firstly, the scale of the tokens are rigidly linked to the model architecture by a fixed patch size, ignoring any redundancy in the original images. These limitations result in a significant increase in computation for larger"}, {"title": "1.1 Motivation", "content": "We take a step back from the original ViT architecture to re-evaluate the role of patch-based tokenization. By focusing on a somewhat overlooked component in the architecture, we look to establish image partitioning as the role of an adaptive modular tokenizer; an untapped potential in ViTs.\nIn contrast to square partitions, superpixels offer an opportunity to mitigate the shortcomings of patch-based tokenization by allowing for adaptability in scale and shape while leveraging inherent redundancies in visual data. Superpixels have been shown to align better with semantic structures within images [37], providing a rationale for their potential utility in vision transformer architectures. We compare the canonical square tokenization in standard ViTs with our"}, {"title": "1.2 Contributions", "content": "Our research induces three specific inquiries: (a) Is a rigid adherence to square patches necessary?, (b) What effect does irregular partitioning have on tokenized representations?, and (c) Can tokenization schemes be designed as a modular component in vision models? In this work we establish the following;\n\u2022 Generalized Framework: Superpixel tokenization generalize ViTs in a modular scheme, providing a richer space of transformers for vision tasks where the transformer backbone is independent of tokenization framework.\n\u2022 Efficient Tokenization: We propose an efficient on-line tokenization approach which provides competitive training and inference times as well as strong performance in classification tasks.\n\u2022 Refined Spatial Resolution: Superpixel tokenization provides semantically aligned tokens with pixel-level granularity. We demonstrate that our method yields significantly more faithful attributions compared to established explainability methods, as well as strong results in unsupervised segmentation.\n\u2022 Visual Tokenization: The main contribution of our work is the introduction of a novel way of thinking about tokenization in ViTs, an overlooked but central component of the modeling process cf. discussion in Section 4.\nOur primary objective is to evaluate tokenization schemes for ViTs, underscoring the intrinsic properties of alternative tokenization. In the interest of a fair comparative analysis, we perform our study using vanilla ViT architectures and established training protocols [36]. Hence, we design experiments to establish a fair comparison against well-known baselines without architectural optimizations. This controlled comparison is crucial for attributing observed disparities specifically to the tokenization strategy, and eliminates confounding factors from specialized architectures or training regimes.\nNotation: We let $H \\times W = \\{(y,x) : 1 \\leq y \\leq h,1 \\leq x \\leq w\\}$ denote the coordinates of an image of spatial dimension (h, w), and let $I$ be an index set for the mapping $i\\rightarrow (y,x)$. We consider a C-channel image as a signal $\\S: I \\rightarrow \\mathbb{R}^C$. We use the vectorization operator $vec: \\mathbb{R}^{d_1 \\times...\\times d_n} \\rightarrow \\mathbb{R}^{d_1...d_n}$, and denote function composition by $f(g(x)) = (f\\circ g)(x)$."}, {"title": "2 Methodology", "content": "To evaluate and contrast different tokenization strategies, we require methods for partitioning images and extracting meaningful features from these partitions. While these tasks can be performed using a variety of deep architectures, such approaches add a layer of complexity to the final model, which would invalidate"}, {"title": "2.1 Framework", "content": "We generalize the canonical ViT architecture by allowing for a modular tokenizer and different methods of feature extraction. Note that a canonical ViT is generally presented as a three-component system with a tokenizer-embedder $g$, a backbone $f$ consisting of a sequence of attention blocks, and a subsequent prediction head $h$. Contrarily, language transformers explicitly decouples $g$ from the backbone $f$. Following this lead, we note that we can essentially rewrite a patch embedding module as a three component modular system, featuring a tokenizer $T$, a feature extractor $\\Phi$, and an embedder $\\gamma$ such that $g = \\gamma \\circ \\Phi \\circ T$, emphasizing that these are inherent components in the original architecture obscured by a simplified tokenization strategy-cf. Fig 2. This provides a more complete assessment of the model as a five component feedforward system\n$\\Phi(\\xi; \\theta) = (h \\circ f\\circ g)(\\xi; \\theta),$\n$\\hspace{1.2cm} = (h \\circ f\\circ \\gamma \\circ \\Phi \\circ T)(\\xi; \\theta),$\n(1a)\n(1b)\nwhere $\\theta$ denotes the set of learnable parameters of the model. In a standard ViT model, the tokenizer $T$ acts by partitioning the image into fixed-size square partitions. This directly provides vectorized features since patches are of uniform dimensionality and ordering, hence $\\Phi = vec$ in standard ViT architectures. The embedding $\\gamma$ is typically a learnable linear layer, mapping features to the embedding dimension of the specific architecture. Alternatively, $g$ can be taken as a convolution with kernel size and stride equal to the desired patch size $p$."}, {"title": "2.2 Partitioning and Tokenization", "content": "Tokenization in language tasks involves partitioning text into optimally informative tokens, analogous to how superpixels [37] partition spatial data into dis-"}, {"title": "2.3 Feature Extraction with Irregular Patches", "content": "While we conjecture the choice of square patches in the ViT architecture to be motivated by simplicity, it is naturally also a result of the challenge posed"}, {"title": "2.4 Generalization of Canonical ViT", "content": "By design, our framework acts as a generalization of the canonical ViT tokenization, and is equivalent to applying an canonical patch embedder using a fixed patch size $p$ with interpolated gradient excluding feature extraction.\nProposition 1 (Embedding Equivalence). Let $\\tau^*$ denote an canonical ViT tokenizer with a fixed patch size $p$, let $\\phi$ denote a gradient excluding interpolated feature extractor, and let $\\gamma^*, \\gamma$ denote embedding layers with equivalent linear projections $L = L_\\theta$. Let $\\Omega^{(pos)} \\in \\mathbb{R}^{N \\times B^2}$ denote a matrix of joint histogram positional embeddings under the partitioning induced by $\\tau^*$. Then for dimensions $H = W = \\beta^2 = p^2$, the embeddings given by $\\gamma \\circ \\phi \\circ \\tau^*$ are equivalent to the canonical ViT embeddings given by $\\gamma^* \\circ \\varphi^* \\circ \\tau^*$ up to proportionality.\nWe provide necessary definitions and proofs for Prop. 1 in Appendix ??, demonstrating that our proposed framework includes the canonical ViT architecture as a special case; an essential property for modularity."}, {"title": "3 Experiments and Results", "content": "We train ViTs with different tokenization strategies (ViT, RVIT, SPIT) using base (B) and small (S) capacities on a general purpose classification task on ImageNet [11] (IN1K). We design our experiments with the goal of evaluating the quality of the resulting tokenized representations of the images. See details about the training setup in Appendix ??."}, {"title": "3.1 Classification", "content": "We evaluate the models by fine-tuning on CIFAR100 [22] and CALTECH256 [16], in addition to validation using the INREAL labels [4], ablating the effect of gradient features. We also evaluate our models by replacing the linear classifier head with a k-nearest neighbours (kNN) classifier over the representation space of different models, focusing solely on the clustering quality of the class tokens in the embedded space [8, 28]. Table 1 gives an overview of the results. We include results for the Small (S) capacity models in Table ??."}, {"title": "3.2 Evaluating Tokenized Representations", "content": "To evaluate the cohesive quality of the tokenized representations, we look to quantify the faithfulness of attributions, and the model's performance on zero-shot unsupervised segmentation. These were selected to give insight into the embedded context of the tokenized representation of the image."}, {"title": "3.3 Ablations", "content": "Tokenizer Generalization: In in Section 2.4 we showed that our framework generalizes the canonical ViT. This allows us to contrast different tokenization strategies across models by directly swapping tokenizers, emphasizing the modularity of our framework. We report the relative change in accuracy (\u0394 Acc.) of models when swapping tokenizers in Table 4.\nOur results show that ViTs with square tokenization performs poorly when evaluated on irregular patches. We observe an increase in accuracy for RViT models when evaluated over square patches. Furthermore, we see that the SPIT models also generalize well to both to square and Voronoi tokens, but is highly dependent on the gradient features. With gradient features, we note a minor drop in accuracy when evaluating Voronoi tokens with SPiT, and superpixel tokens with RViT. This supports our conjecture that gradient features help encode texture information for irregular patches."}, {"title": "4 Discussion and Related Work", "content": "Related Work Interest in adaptive tokenization is burgeoning in the field. We propose a taxonomy of adaptive tokenization with two main dimensions illustrated in Fig. 6. The first dimension illustrates the coupling or integration of tokenization into the transformer architecture. Several approaches [5, 19, 26] are inherently coupled to the architecture, while others adopt a decoupled approach [18, 31] which more closely aligns with our framework. The taxonomy is extended by a dimension of token granularity, measuring the proximity to modelling with pixel-level precision. Together, these dimensions facilitate an understanding of adaptive tokenization approaches for ViTs.\nA significant body of current research is primarily designed to improve scaling and overall compute for attention [5, 32, 55] by leveraging token merging strategies in the transformer layers with square patches, and can as such be considered low-granularity coupled approaches. Distinctively, SuperToken [19] applies a coupled approach to extract a non-uniform token representation. The approach is fundamentally patch based, and does not aim for pixel-level granularity.\nIn contrast, multi-scale tokenization [18, 31] apply a decoupled approach where the tokenizer is independent of the transformer architecture. These are commensurable with any transformer backbone, and improve computational overhead. While square tokens operate on a lower level of granularity, there is significant potential for synergy between these approaches and our own, particularly given the hierarchical nature of SPiT. On the periphery, Ma et al. [26] propose a pixel-level clustering method with a coupled high granularity approach.\nLimitations Our proposed framework is not optimizable with gradient based methods. Ideally, adaptable tokenization should be learnable in an end-to-end framework. However, such an approach needs to be carefully designed to not add undue computational overhead, and should ideally not be limited by a pre-defined number of tokens. Moreover, we see that irregular tokenization require"}, {"title": "5 Conclusion", "content": "In this work, we posit tokenization as a modular component that generalize the canonical ViT backbone, and show that irregular tokenization with superpixels is commensurable with transformer architectures. Our experiments demonstrate that superpixel tokens have a significant impact on extracted attributions for predictions, and are amenable to unsupervised segmentation tasks without a separate decoder model. Moreover, we show that concatenated gradient features improve performance of base capacity ViTs, and that irregular tokenizers generalize between different tokenization strategies. Our experiments were performed with standard models and training to limit confounding factors in our results."}, {"title": "A Equivalence of Frameworks", "content": "Definition 1 (ViT Tokenization). Let $\\xi: H\\times W \\rightarrow \\mathbb{R}^C$ be an image signal with tensor representation $\\xi \\in \\mathbb{R}^{H\\times W\\times C}$. The canonical ViT tokenization operator $\\tau^* : \\mathbb{R}^{H\\times W\\times C} \\rightarrow \\mathbb{R}^{N\\times p\\times p\\times C}$ partitions the image into $N = \\lfloor\\frac{H}{p}\\rfloor\\cdot\\lfloor\\frac{W}{p}\\rfloor$ non-overlapping C-channel square zero-padded patches. For the case where we have $H \\mod p = W \\mod p = 0$, we get $N = \\frac{H\\cdot W}{p\\cdot p}$, and no padding is necessary.\nDefinition 2 (ViT Features). Let $p$ denote the patch dimension of a canonical ViT tokenizer $\\tau^*$, and let $M = p^2C$. The canonical ViT feature extractor $\\phi^*: \\mathbb{R}^{N\\times p\\times p\\times C} \\rightarrow \\mathbb{R}^{N\\times M}$ is given by $\\phi^* = vec_M$, where $vec_M$ denotes the vectorization operator applied to each of the N patches via $p \\times p \\times C \\rightarrow M$.\nDefinition 3 (ViT Embedder). Let $\\phi^*$ be a canonical ViT feature extractor, and let $Q \\in \\mathbb{R}^{N\\times D}$ denote a positional encoding. The canonical ViT embedder $\\gamma^* : \\mathbb{R}^{N\\times M} \\rightarrow \\mathbb{R}^{N\\times D}$ is given by\n$\\gamma^*(z) = L_{\\theta} z + Q$\nwhere $L_{\\theta}: \\mathbb{R}^{N\\times M} \\rightarrow \\mathbb{R}^{N\\times D}$ is a learnable linear transformation, and $Q$ is either a learnable set of parameters or a function of the positions of the N blocks in the partitioning induced by the canonical tokenizer $\\tau^*$.\nLemma 1 (Feature Equivalence). Let $\\tau^*$ denote a canonical ViT tokenizer with a fixed patch size p, and let $\\phi$ denote a gradient excluding interpolating feature extractor with $\\beta = p$. Then the operations $\\phi \\circ \\tau^*$ are equivalent to the canonical ViT operations $\\phi^* \\circ \\tau^*$.\nProof. The proof is highly trivial but illustrative. Note that for each of the N square patches generated by $\\tau$, the extractor $\\phi$ performs an interpolation to rescale the patch to a fixed resolution of $\\beta \\times \\beta$. However, for $\\beta = p$ the patches already match the target dimensions exactly. It follows that the interpolation operation reduces to identity. The vectorization operator is equivalent for both mappings, hence $\\varphi = vec_M = \\phi^*$.\nProposition 1 (Embedding Equivalence). Let $\\tau^*$ denote an canonical ViT tokenizer with a fixed patch size p, let $\\phi$ denote a gradient excluding interpolated feature extractor, and let $\\gamma^*, \\gamma$ denote embedding layers with equivalent linear projections $L = L_\\theta$. Let $\\Omega^{(pos)} \\in \\mathbb{R}^{N \\times B^2}$ denote a matrix of joint histogram positional embeddings under the partitioning induced by $\\tau^*$. Then for dimensions $H = W = \\beta^2 = p^2$, the embeddings given by $\\gamma \\circ \\phi \\circ \\tau^*$ are equivalent to the canonical ViT embeddings given by $\\gamma^* \\circ \\varphi^* \\circ \\tau^*$ up to proportionality.\nProof. We first note that we can assume $\\Omega^{(pos)}$ is a matrix with single entry components, since under $\\beta = p$ and $N = \\beta^2$, each vectorized histogram feature is a scaled unit vector $c_n e_n$ with $n = 1,..., N$. Moreover, since the partitioning inferred by $\\tau^*$ exhaustively covers the spatial dimensions $H \\times W$, the histograms essentially span the standard basis, such that $\\Omega^{(pos)}$ is diagonal. Furthermore,"}, {"title": "B Preprocessing and Superpixel Features", "content": "Compared to standard preprocessing, we use a modified normalization scheme for the features for improving the superpixel extraction. We apply a combined contrast adjustment and normalization function using a reparametrized version of the Kumaraswamy CDF. which is computationally efficient and allows more fine-grained control of the distribution of intensities than empirical normalization, which improves the superpixel partitioning.\nThe normalization uses a set of means $\\mu$ shape parameters $\\lambda$ for normalizing the image and adjusting the contrast. The normalization is given by\n$\\frac{(1-(1-x^{\\lambda})^b)}{(1000)}$,\n(10)\nwhere $b$ is defined by\n$b=\\frac{\\ln(2)}{\\ln (1 - \\mu \\lambda)},$\n(11)\nand we set means $\\mu_r = 0.485, \\mu_g = 0.456, \\mu_b = 0.406$ and $\\lambda_r = 0.539, \\lambda_g = 0.507, \\lambda_b = 0.404$, respectively.\nThe features used for the superpixel extraction are further processed using anisotropic diffusion, which smoothes homogeneous regions while avoiding blurring of edges. This technique was advocated for superpixel segmentation by Xiaohan et al. [49]. We use the algorithm proposed by Perona and Malik [29] over 4 iterations, with $\\kappa = 0.1$ and $\\gamma = 0.5$. Note that these features are only applied for constructing the superpixels in the tokenizer. We emphasize that we do not apply anisotropic diffusion for the features in the predictive model."}, {"title": "Number of Superpixels", "content": "In Section 2.2, we mention that SPiT gives comparable numbers of partitions to a ViT with different patch sizes. Table B.1 shows empirical results for superpixel sizes using the SPiT tokenizer over the training images of IMAGENET1K, and Fig. B.1 compares the results to number of patches with canonical ViT tokenization, demonstrating the validity of our claims.\nImportantly, these results also reveal much about effective inference times. In Table 5, we show that the overhead for constructing the superpixels is very low. However, the number of tokens depends on the image. Images with large homogeneous regions will be processed faster, while images with many independent regions will necessary incur a cost. Nevertheless, the results in Table B.1 show that we will, on average, have comparable inference times to a canonical ViT due to the beneficial properties of our proposed superpixel tokenization."}, {"title": "Final Thresholding", "content": "Adaptable tokenization frameworks does not necessarily entail an overall drop in inference throughput. Contrarily, it could potentially be leveraged to substantially improve inference speed by designing learnable methods to lower the number of tokens without decreasing performance, e.g. ToMe by Bolya et al. [5].\nWe apply an additional final merging step where we compute the euclidean distance between adjacent superpixels and merge all superpixels below a given threshold for our SPiT-B16 model with gradient features. Noting that a threshold of 0.0 retains the original model design, the results in Table B.2 indicate that models with superpixel tokenization can be optimized to improve inference throughput. We also note that taking the maximum performing tokens over all"}, {"title": "C Training Details", "content": "As mentioned in Section 1.2, we use standardized ViT architectures and generally follow the recommendations provided by Steiner et al. [36]. We provide training logs, pre-trained models, and code for training models from scratch in our GitHub project repository (in the camera ready manuscript).\nClassification: Training is performed over 300 epochs using the ADAMW optimizer with a cosine annealing learning rate scheduler with 5 epochs of cosine annealed warmup from learning rate start = 1\u00d710-5. The schedule maxima and minima are given by \\eta_{max} = 3\\times10^{-3}, and \\eta_{min} = 1\\times10^{-6}. We use a weight decay of \\lambda_{dec} = 2\\times10^{-2} and set the smoothing term \\epsilon = 1\\times10^{-7}. In addition, we used stochastic depth dropout with a base probability of p = 0.2 and layer scaling. Models were pre-trained with spatial resolution 256\u00d7256.\nFor augmentations, we randomly select between using the RANDAUG framework at medium strength or using AuG3 framework by Touvron et al. [39] including CUTMIX [56] with parameter a = 1.0. We use RANDOMRESIZECROP using the standard scale (0.08, 1.0) with randomly sampled interpolation modes. Since the number of partitions from the superpixel tokenizer are adapted on an image-to-image basis, we effectively constrain the maximum number of tokens during training using token dropout to balance number of tokens.\nWe found that a naive on-line computation of Voronoi tessellations was unnecessarily computationally expensive, hence we precompute sets of random Voronoi tessellations with 196, 256, and 576 partitions, corresponding to images of 224x224, 256x256, and 384\u00d7384 resolutions given patch size p = 16.\nAll training was performed on AMD MI250X GPUs. One important distinction is that we do not use quantization with bfloat16 for training our models, instead opting for the higher 32-bit precision of float32 since this improves consistency between vendor frameworks. Inference was carried out on a mixture of NVIDIA A100, RTX 2080Ti, Quadro P6000, and AMD MI250X to validate consistency across vendor frameworks."}, {"title": "Fine Tuning", "content": "All base models were fine-tuned over 30 epochs with increased degrees of regularization. We increase the level of RANDAUG to \"strong\" using 2 operations with magnitude 20. Additionally, we increase the stochastic depth dropout to p = 0.4. Fine tuning was performed with spatial resolution 384\u00d7384, and we reduce the maximum learning rate to \\eta_{max} = 1\\times10^{-4}. For the alternative classification datasets CIFAR100 and CALTECH256, fine tuning was performed by replacing the classification head and fine tuning for 10 epochs using ADAMW with learning rate \\eta = 1: 1\\times10^{-4} and the same weight decay. No augmentation was used in this process, and images were re-scaled to 256 \u00d7 256 for training and evaluation."}, {"title": "D Interpretability and Attention Maps", "content": "For LIME explanations, we train a linear surrogate model \\mathcal{L}_\\theta for predicting the output probabilities for the prediction of each model \\Phi. To encourage independence between tokenizers and LIME explanations, as well as promote direct comparability, we use SLIC with a target of || \\approx 64 superpixels. We use Monte Carlo sampling of binary features for indicating the presence or omission of each superpixel with stochastic p \\in Uniform(0.1, 0.3), and keep these consistent across model evaluations. We observed that certain images in the IN1K at times produced less than 5 superpixels using SLIC, hence these images were dropped from the evaluation.\nThe attention flow [1] of a transformer differs from the standard attention roll-out by accounting for the contributions of the residual connections in computations. The attention flow of an $l$-layer transformer is given by\n$AFlow = \\prod_{i=1}^{l} ((1 - \\lambda)I + \\lambda A_i).$\n(12)\nwhere we set \\lambda = 0.9 to account for stochastic depth and layer scaling factors while accentuating the contribution of the attention operators. We use max-aggregation over the heads to extract a unified representation. Following Dosovitskiy et al. [14] and Caron et al. [8], we extract the attention for the class token as an interpretation of the model's prediction."}, {"title": "E Unsupervised Salient Segmentation Details", "content": "The TokenCut [47] framework proposes to use a normalized cut [35] over the key features without class tokens in the last self-attention layer of the network. A"}, {"title": "F Feature Correspondences", "content": "The work by Caron et al. [8] and Oquab et al. [28] established certain emergent properties in self-supervised models, where the tokenized features of ViT trained with self-supervised methods provide inherent interpretability and inter-image feature correspondence. Given our results on feature attributions from Section 3.2, we perform experiments to visualize feature correspondences to see if similar emergent properties can be observed from supervised training with superpixel tokenization.\nMethod: A sequence of support images (\\xi_n)_{n=1}^N are selected, with labels such that y_n \\approx y_m for all $1 < m, n \\leq N$, as a set of features to search from. Furthermore, these images are selected such that the WordNet [15] hypernym of"}, {"title": "G Extended Discussion on Classification", "content": "Certain interesting observations can be made from our results in Table 1. Firstly, random Voronoi tessellations perform better than data-driven superpixels for gradient excluding features, and despite its inherent stochasticity, tokenization with random Voronoi tessellations proves to be a relatively effective strategy, and demonstrate surprisingly consistent results over prediction tasks as reported in Table G.1. To account for the stochasticity in validation, we compute accuracy"}]}