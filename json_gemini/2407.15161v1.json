{"title": "FFHFlow: A Flow-based Variational Approach for Multi-fingered Grasp Synthesis in Real Time", "authors": ["Qian Feng", "Jianxiang Feng", "Zhaopeng Chen", "Rudolph Triebel", "Alois Knoll"], "abstract": "Synthesizing diverse and accurate grasps with multi-fingered hands is an important yet challenging task in robotics. Previous efforts focusing on generative modeling have fallen short of precisely capturing the multi-modal, high-dimensional grasp distribution. To address this, we propose exploiting a special kind of Deep Generative Model (DGM) based on Normalizing Flows (NFs), an expressive model for learning complex probability distributions. Specifically, we first observed an encouraging improvement in diversity by directly applying a single conditional NFs (cNFs), dubbed FFHFlow-cnf, to learn a grasp distribution conditioned on the incomplete point cloud. However, we also recognized limited performance gains due to restricted expressivity in the latent space. This motivated us to develop a novel flow-based d Deep Latent Variable Model (DLVM), namely FFHFlow-lvm, which facilitates more reasonable latent features, leading to both diverse and accurate grasp synthesis for unseen objects. Unlike Variational Autoencoders (VAEs), the proposed DLVM counteracts typical pitfalls such as mode collapse and mis-specified priors by leveraging two cNFs for the prior and likelihood distributions, which are usually restricted to being isotropic Gaussian. Comprehensive experiments in simulation and real-robot scenarios demonstrate that our method generates more accurate and diverse grasps than the VAE baselines. Additionally, a run-time comparison is conducted to reveal the model's high potential for real-time applications.", "sections": [{"title": "1 Introduction", "content": "Robotic grasping endows embodied intelligent agents with an essential skill to interact with open-world environments, acting as a crucial precursor to advanced manipulation. Recent advances in learning-based grasping have witnessed superior generalization performance, though with a major focus on low-Degrees of Freedom (DoF) End Effectors (EE) such as parallel grippers or suction cups [1, 2, 3, 4, 5]. Contrasting with a multi-fingered hand, the low dexterity of parallel grippers highly restricts the applicability to complex task-oriented or fine-grained manipulation [6, 7]. Synthesizing diverse grasps is important as it follows the nature that an object can be grasped in many equally successful ways and enables potential applications such as constrained-workspace grasping, collision avoidance, task-oriented grasping and so on.\nThe first challenge in synthesizing diverse multi-fingered grasps from a partially observed object point cloud lies in the relatively high-dimensional action space, resulting in a much larger search space and more complex grasp distribution. Besides, an incomplete object point cloud mostly corresponds to a large set of potential grasps. Therefore, conditional distribution learning approaches based on Deep Generative Models (DGMs) received considerable attention to handle the high-dimensional one-to-many non-functional mapping in this problem. Among them, Conditional Vari-"}, {"title": "2 Related Work", "content": "Geometry-based grasp synthesis generates grasps with hand-crafted geometric constraints, heuristics, and point cloud features [20, 21, 22, 23]. They tend to generate less diverse grasps, especially given partial object observations. For this reason, many works [24, 25] reply on 3D reconstruction to recover the entire object model and then use GraspIt! [26] to generate grasps. However, both 3D reconstruction and geometry-based sampling on high-DoF multi-fingered hands are time-consuming.\nLearning-based grasp synthesis. CVAE are widely adopted in grasp synthesis with two-jaw gripper [2] and multi-fingered hands [8, 9, 10]. cVAE in [9] is conditioned directly on partial point cloud, but in [10], a point cloud completion module together with iterative refinement steps are added, leading to a more computationally expensive process. Instead of adding a resource-intensive shape completion model [10], our model subsumes this information in latent variables in a more lightweight manner. GANs are adopted to predict a three-fingered grasp with RGB-D input data [11, 12]. Due to an integrated shape completion module, their grasping pipeline is slow (8s). Similarly, GANs are utilized in [13] to predict a 6D grasp pose along with one of four pre-defined grasp types. An auto-regressive model is applied in [27], but it requires shape completion beforehand [28]. Latent diffusion models can also generate grasp distributions [29], but the iterative denoising process needs longer computation time. A cNF-based approach in [30] consists of two separate models predicting rotation and translation sequentially and is only evaluated in simulation. Our work proposes novel flow-based DLVMs to predict the full grasp at once. A diverse grasp synthesis enables real-world applications such as grasping under workspace constraints and dynamic grasping [31].\nNormalizing Flows. The past decade has witnessed exciting advances in DGM including VAE [19], GANs [32], diffusion models [33] and NFs [34]. Unlike other DGM, flow-based models [35, 36] can perform both exact likelihood evaluation and efficient sampling simultaneously. More noteworthy, NFs can be trained more stably when compared with GANs [37], perform better against the notorious mode collapse problem in both GANs and VAE [16, 14, 15] and do not need to go"}, {"title": "3 Preliminaries", "content": "Deep Latent Variable Models (DLVMs). In the context of modeling the unknown true data distribution $p_*(x)$ with a model $p_\\theta(x)$ parameterized by $\\theta$ based on a dataset $\\mathcal{D} = \\{x_i\\}_1^N$, latent variables $z$ are usually introduced for discovering fine-grained factors controlling the data generating process or increasing the expressivity of the model $p_\\theta(x)$. Latent variables $\\{z_i\\}_1^N$ are part of the model but hidden and unobservable in the dataset. The resulting marginal probability is:\n$p_\\theta(x) = \\int p_\\theta(x|z)p_\\theta(z)dz$. When $p_\\theta(x, z)$ is parameterized by Deep Neural Networks (DNNs), we term the model Deep Latent Variable Models (DLVMs) [54, 14]. The difficulty of learning such models with Maximum Likelihood Estimation (MLE) lies in the intractability of the integral in the marginal probability for not having an analytic solution or efficient estimator. To remedy this, Variational Inference (VI) [55] provides a tractable lower bound of the marginal likelihood $p_\\theta(x)$ to optimize by approximating the real posterior $p_\\theta(z|x)$ with an approximate one $q_\\phi(z|x)$:\n$\\log p_\\theta(x) \\ge \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z) + \\log \\frac{p_\\theta(z)}{q_\\phi(z|x)}].$ (1)\nWhen $q(z|x)$ and $p(x|z)$, are approximated by DNNs with an isotropic Gaussian as prior $p_\\theta(z)$, we obtain the well-known instance of DLVMs, the VAE model [54].\nNormalizing Flows. NFs are known to be universal distribution approximators [39]. That is, they can model any complex target distribution $p_*(x)$ on a space $\\mathbb{R}^d$ by defining $x$ as a transformation $T_\\theta: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ parameterized by $\\theta$ from a well-defined base distribution $p_u(u)$:\n$x = T_\\theta(u)$ where $u \\sim p_u(u)$, where $u \\in \\mathbb{R}^d$ and $p_u$ is commonly chosen as a unit Gaussian. By designing $T_\\theta$ to be a diffeomorphism, that is, a bijection where both $T_\\theta$ and $T_\\theta^{-1}$ are differentiable. We can compute the likelihood of the input $x$ exactly based on the change-of-variables formula [56]: $p_\\theta(x) = p_u(T_\\theta^{-1}(x))|\\det(J_{T_\\theta^{-1}}(x))|$, where $J_{T_\\theta^{-1}}(x) \\in \\mathbb{R}^{d \\times d}$ is the Jacobian of the inverse $T_\\theta^{-1}$ with respect to $x$. The transformation $T_\\theta$ can constructed by composing a series of bijective maps denoted by $t_i$, $T_\\theta = t_1 \\circ t_2 \\circ ... \\circ t_n$. When the target distribution is unknown, but samples thereof are available, we can estimate $\\theta$ by minimizing the forward Kullaback-Leibler Divergence (KLD), equivalent to maximizing the expected Log-Likelihood (LL) over the samples.\n$\\log p_\\theta(x) = \\mathbb{E}_{p_*(x)} [\\log(p_u(T_\\theta^{-1}(x))) + \\log | \\det(J_{T_\\theta^{-1}}(x))|].$ (2)"}, {"title": "4 Flow-based Grasp Synthesis", "content": "4.1 Problem Formulation\nThis work aims to synthesize diverse grasps from a partial point cloud denoted by $x \\in \\mathbb{R}^{N \\times 3}$. A grasp $g \\in \\mathbb{R}^d$ is represented by the 15-DOF hand joint configuration $j \\in \\mathbb{R}^{15}$ and the 6D palm pose $(R, t) \\in SE(3)$. We use the Euler angle as a rotation representation for its conciseness, leading to $d = 21$. To formally define the problem, we assume an empirical dataset of $N$ objects with their $N_i$ corresponding possible grasps $\\mathcal{D} = \\{x_i, \\{g_{ik}\\}_{k=1}^{N_i}\\}_{i=1}^N$ drawn from the unknown underlying conditional distribution $p_*(g|x)$. We need to learn a probabilistic model $p_\\theta(g|x)$ parameterized by $\\theta$ to approximate $p_*(g|x)$ based on this dataset. At inference, we can synthesize diverse and accurate grasps given a test object point cloud."}, {"title": "4.2 Flow-based Grasp Sampler: FFHFlow-cNF", "content": "The straightforward idea to learn the conditional distribution $p_\\theta (g|x)$ is directly employing the Conditional Normalizing Flows (CNF) [42] without considering hidden variables in the latent space, (see Figure 3a). To this end, we condition the flow transformation $T_\\theta$ and the base distribution $p_u$ with the object point clouds $x$, namely $T_{\\theta\\vert x} : \\mathbb{R}^d \\times \\mathbb{R}^l \\rightarrow \\mathbb{R}^d$, where $l$ is the dimensionality for point clouds features and $d$ for the grasp representation. During training, the parameter $\\theta$ is learned by maximizing the log conditional likelihoods adapted from Equation (2). To note that we encoder each point cloud with a fixed Basis Point Set (BPS) according to [57] resulting in a feature vector $x_b \\in \\mathbb{R}^s$ of fixed length $s$, before being fed into the feature extractor network $f_\\phi(x_b) : \\mathbb{R}^s \\rightarrow \\mathbb{R}^l$.\nLimitation. Though FFHFlow-cnf has achieved encouraging improvements in terms of diversity and accuracy when compared to the cVAE-based approach, we found this model less generalizable with limited performance gain. We attribute this problem to the inadequate expressivity of the latent feature, especially when the model needs to understand the complicated relationships between the grasps and the partially observed point clouds of different objects. For example, in Figure 4, we can see that FFHFlow-cnf fails to cluster the objects with similar shapes together, while FFHFlow-lvm can pull the objects of regular boxes together. To address this problem, we introduce FFHFlow-lvm in the next sub-section, a flow-based variational sampler with a more expressive probabilistic representation in the latent space based on DLVMs and cNF."}, {"title": "4.3 Flow-based Variational Grasp Sampler: FFHFlow-lvm", "content": "Inspired by the success of leveraging DLVMs for point cloud processing [45, 44] and grasp generation [2, 9, 10], we devise a flow-based variational method that can infer expressive latent distribution for precise and diverse grasp generation. Specifically, we seek to overcome the over-regularization by the simplistic prior and the latent feature collapse by the Gaussian observation model in CVAE-based approaches [2, 9, 10].\nThis is achieved by introducing an input-dependent and expressive prior and a flexible observation model based on cNF, which can be optimized efficiently under the framework of SGVB [19]."}, {"title": "4.3.1 Learning Grasp Distribution via DLVMS", "content": "Our main idea is to introduce latent variables into FFHFlow-cnf, in order to increase the expressivity of the latent space. By introducing the latent variable $z$ (see Figure 3c), we have the following conditional likelihoods of the grasps $g$ given a partially observed point cloud $x$ which we can maximize and can be factorized in a way: $p_\\theta(g|x) = \\int p_\\theta(g|x,z)p_\\theta(z|x)dz$. As described in Section 3, this likelihood is intractable due to the integral over the latent variables. Therefore, we first show the derived tractable lower bound for optimization. Then, we articulate each component in the model and explain how they mitigate the pitfalls in cVAE-based approaches. We use $\\theta$ to denote the parameters for the grasp flow and prior flow and $\\phi$ for the inference network.\nVariational Lower Bound: Based on the Jensen Inequality, we can have the following variational lower bound with an approximate posterior of the latent variable $q_\\phi(z|x, g)$ (more details in appendix):\n$\\log p_\\theta (g|x) \\ge \\mathbb{E}_{q_\\phi (z|x,g)} [\\log p_\\theta (g|x, z)] - \\beta \\mathbb{K}L(q_\\phi(z|x, g)||p_\\theta(z|x)).$ (3)"}, {"title": "4.4 Grasp Evaluator", "content": "The grasp distribution conditioning on the partial point cloud is intrinsically sophisticated for both theoretical (e.g., on a space with mixed manifolds) and practical (e.g., highly sensitive to physical factors such as the center of mass, friction, and so on) reasons. To further secure the grasping success, a grasp evaluator is an effective complementary add-on as it can model both feasible $g$ grasps and infeasible grasps $\\hat{g}$ in a supervised manner. Therefore, we train a separate grasp evaluator parameterized by a Multi-Layer Perceptron (MLP) $f_\\gamma : \\mathbb{R}^{s} \\times \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ based on the feasibility label $y$ with the following loss function:\n$\\mathcal{L}_{Eva} = [y \\log(f_\\gamma(g, x)) + (1 - y) \\log(f_\\gamma(\\hat{g}, x))],$ (5)\nwhere $s$ is the dimensionality for BPS encoded point clouds and $d$ is for the grasp representation."}, {"title": "5 Experiment", "content": "In this section, we perform comprehensive experiments and analysis to gain evidence for the following questions: Q1: Can the proposed flow-based DLVMs, i.e., FFHFlow-lvm, facilitate more expressive latent representations for both diverse and accurate grasp synthesis? Q2: How do the proposed models compare with the state-of-the-art, e.g., the cVAE-based approaches? Q3: Is the proposed model run-time efficient for real-robot deployment? Q4: Can the model generalize to data and hardware in the real world, especially complex scenarios, e.g., a constrained workspace?\nAdditionally, we conducted an ablation study (more details in appendix) and found that positional encoding pre-processing can help alleviate over-fitting, and the size ratio of the two flows in the model also affects the training stability."}, {"title": "5.1 Experimental Setup", "content": "The experiments of grasping unknown table-top objects were performed in both simulation and the real world with the DLR-HIT II robot hand [59] (more details in appendix). Datasets: We use only simulated data generated based on a heuristic grasp planner for training. For training, we use 129 graspable objects filtered from around 280 objects, the BIGBIRD [60], and KIT [61] datasets based on their graspability and object type. For testing, we select 12 unseen objects from KIT dataset in simulation and 8 unknown objects from YCB dataset [62] for the real-world evaluation. Inspired by [63], to capture the sensitive high-frequency contents in grasping poses, we additionally use positional encoding for grasp poses in FFHFlow-lvm. Baselines: we have the following approaches evaluated in our comparative study for versions with and without the following evaluator (\"w/o eval\"): 1. Heuristic grasp sampler: a heuristic grasp sampler to generate grasps based on the normal of object point clouds; 2. FFHNet [9]: a state-of-the-art multi-fingered grasp sampler based on CVAE; 3. FFHNet-prior [64, 65]: FFHNet extended with an input-dependent prior; 4. FFHFlow-lvm-light: FFHFlow-lvm with a lightweight flow prior and grasp flow generator (4 layers vs. 8 layers)."}, {"title": "5.2 Evaluation in the Simulation", "content": "Each of the 12 objects is grasped 20 times in this experiment. We demonstrate the simulation results through the success rate in Table 1, grasp visualization in Figure 1, and the latent feature visualization in Figure 4. Qualitatively, we can answer Q1 affirmatively with the grasp pose visualization in Figure 1, where FFHFlow-lvm can model the complex multi-modal ground truth grasp distribution. Conversely, FFHNet suffers from the mode-collapse issue and fails to model the diverse"}, {"title": "5.3 Evaluation in the Real World", "content": "Unconstrained Workspace: This setup is similar to the simulation, where 8 objects are grasped 10 times each. For Q4, the results in Table 2 show that FFHFlow-lvm is able to generalize to the real hardware and real-world data much better than FFHFlow-cnf in terms of average success rate, namely 15.0% vs. 1.25% increase to the FFHNet baseline.\nFailure Analysis: (more details in appendix) When compared to FFHFlow-cnf and FFHNet, FFHFlow-lvm has much fewer failures from unstable grasp pose and less from collisions. Notably, FFHFlow-cnf tends to grasp the corner from a tilted angle instead of the body for the sugar box (40%). FFHNet failed a lot for metal mugs (30%) due to its bias toward top grasps that are harder than side grasps.\nConstrained Workspace: We selected a constrained workspace, i.e.a two-tier shelf to further mimic the realistic scenarios (see Figure 1). Here, four objects are grasped five times each. To fully answer Q4, in Table 3, the large margin of performance increase by FFHFlow-lvm with 65% when compared to FFHNet with 10% indicates that a diverse grasp synthesizer is more effective than a mode-seeking one in such a daily life scenario."}, {"title": "6 Conclusion", "content": "We introduce a novel flow-based variational approach, FFHFlow-lvm for generative grasp synthesis with better quality and diversity. This is achieved by mitigating the insufficiently informative latent features when applying cNF directly and overcoming problems in cVAE-based approaches, i.e., mode-collapse and the mis-specified prior. Comprehensive experiments in the simulation and real-world demonstrate strong performance and efficiency.\nLimitations. 1. Trade-off between run-time and performance: As observed in the simulation results, reducing layers in the flow can decrease the run-time but with a slight loss of performance. Investigating how to strike a balance is relevant for broader robotic applications and a deeper understanding of the proposed flow-based DLVMs. 2. The lack of abilities for efficient adaptation towards objects that differ significantly from those in the training dataset, which is hard to avoid for robots deployed in the wild [66, 67, 68]. 3. Sim-to-Real gap: Though the objects in simulation and real world are different in evaluation, a success rate drop of 17.1% encourages us to further investigate how to eliminate the sim-to-real gap, which accounts for various factors such as camera noises, different physical parameters and so on."}]}