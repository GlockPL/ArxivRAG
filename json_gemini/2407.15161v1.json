{"title": "FFHFlow: A Flow-based Variational Approach for Multi-fingered Grasp Synthesis in Real Time", "authors": ["Qian Feng", "Jianxiang Feng", "Zhaopeng Chen", "Rudolph Triebel", "Alois Knoll"], "abstract": "Synthesizing diverse and accurate grasps with multi-fingered hands is an important yet challenging task in robotics. Previous efforts focusing on generative modeling have fallen short of precisely capturing the multi-modal, high-dimensional grasp distribution. To address this, we propose exploiting a special kind of Deep Generative Model (DGM) based on Normalizing Flows (NFs), an expressive model for learning complex probability distributions. Specifically, we first observed an encouraging improvement in diversity by directly applying a single conditional NFs (cNFs), dubbed FFHFlow-cnf, to learn a grasp distribution conditioned on the incomplete point cloud. However, we also recognized limited performance gains due to restricted expressivity in the latent space. This motivated us to develop a novel flow-based d Deep Latent Variable Model (DLVM), namely FFHFlow-lvm, which facilitates more reasonable latent features, leading to both diverse and accurate grasp synthesis for unseen objects. Unlike Variational Autoencoders (VAEs), the proposed DLVM counteracts typical pitfalls such as mode collapse and mis-specified priors by leveraging two cNFs for the prior and likelihood distributions, which are usually restricted to being isotropic Gaussian. Comprehensive experiments in simulation and real-robot scenarios demonstrate that our method generates more accurate and diverse grasps than the VAE baselines. Additionally, a run-time comparison is conducted to reveal the model's high potential for real-time applications.", "sections": [{"title": "1 Introduction", "content": "Robotic grasping endows embodied intelligent agents with an essential skill to interact with open-world environments, acting as a crucial precursor to advanced manipulation. Recent advances in learning-based grasping have witnessed superior generalization performance, though with a major focus on low-Degrees of Freedom (DoF) End Effectors (EE) such as parallel grippers or suction cups [1, 2, 3, 4, 5]. Contrasting with a multi-fingered hand, the low dexterity of parallel grippers highly restricts the applicability to complex task-oriented or fine-grained manipulation [6, 7]. Synthesizing diverse grasps is important as it follows the nature that an object can be grasped in many equally successful ways and enables potential applications such as constrained-workspace grasping, collision avoidance, task-oriented grasping and so on.\nThe first challenge in synthesizing diverse multi-fingered grasps from a partially observed object point cloud lies in the relatively high-dimensional action space, resulting in a much larger search space and more complex grasp distribution. Besides, an incomplete object point cloud mostly corresponds to a large set of potential grasps. Therefore, conditional distribution learning approaches based on Deep Generative Models (DGMs) received considerable attention to handle the high-dimensional one-to-many non-functional mapping in this problem. Among them, Conditional Vari-"}, {"title": "2 Related Work", "content": "Geometry-based grasp synthesis generates grasps with hand-crafted geometric constraints, heuristics, and point cloud features [20, 21, 22, 23]. They tend to generate less diverse grasps, especially given partial object observations. For this reason, many works [24, 25] reply on 3D reconstruction to recover the entire object model and then use GraspIt! [26] to generate grasps. However, both 3D re-construction and geometry-based sampling on high-DoF multi-fingered hands are time-consuming.\nLearning-based grasp synthesis. CVAE are widely adopted in grasp synthesis with two-jaw gripper [2] and multi-fingered hands [8, 9, 10]. cVAE in [9] is conditioned directly on partial point cloud, but in [10], a point cloud completion module together with iterative refinement steps are added, leading to a more computationally expensive process. Instead of adding a resource-intensive shape completion model [10], our model subsumes this information in latent variables in a more lightweight manner. GANs are adopted to predict a three-fingered grasp with RGB-D input data [11, 12]. Due to an integrated shape completion module, their grasping pipeline is slow (8s). Similarly, GANs are utilized in [13] to predict a 6D grasp pose along with one of four pre-defined grasp types. An auto-regressive model is applied in [27], but it requires shape completion beforehand [28]. Latent diffusion models can also generate grasp distributions [29], but the iterative denoising process needs longer computation time. A cNF-based approach in [30] consists of two separate models predicting rotation and translation sequentially and is only evaluated in simulation. Our work proposes novel flow-based DLVMs to predict the full grasp at once. A diverse grasp synthesis enables real-world applications such as grasping under workspace constraints and dynamic grasping [31].\nNormalizing Flows. The past decade has witnessed exciting advances in DGM including VAE [19], GANs [32], diffusion models [33] and NFs [34]. Unlike other DGM, flow-based models [35, 36] can perform both exact likelihood evaluation and efficient sampling simultaneously. More noteworthy, NFs can be trained more stably when compared with GANs [37], perform better against the notorious mode collapse problem in both GANs and VAE [16, 14, 15] and do not need to go"}, {"title": "3 Preliminaries", "content": "Deep Latent Variable Models (DLVMs). In the context of modeling the unknown true data dis-tribution p\u2217(x) with a model p\u03b8(x) parameterized by \u03b8 based on a dataset D = {xn}Nn=1, latentvariables z are usually introduced for discovering fine-grained factors controlling the data gener-ating process or increasing the expressivity of the model p\u03b8(x). Latent variables {zn}Nn=1 are partof the model but hidden and unobservable in the dataset. The resulting marginal probability is:p\u03b8(x) = \u222b p\u03b8(x|z)p\u03b8(z)dz. When p\u03b8(x, z) is parameterized by Deep Neural Networks (DNNs),we term the model Deep Latent Variable Models (DLVMs) [54, 14]. The difficulty of learning suchmodels with Maximum Likelihood Estimation (MLE) lies in the intractability of the integral in themarginal probability for not having an analytic solution or efficient estimator. To remedy this, Vari-ational Inference (VI) [55] provides a tractable lower bound of the marginal likelihood p\u03b8(x) tooptimize by approximating the real posterior p\u03b8(z|x) with an approximate one q\u03c6(z|x):\nlog p\u03b8(x) \u2265 Eq\u03c6(z|x) [log p\u03b8(x|z) + log p\u03b8(z)\nqo(z|x)\n].\n(1)\nWhen q(z|x) and p\u03b8(x|z), are approximated by DNNs with an isotropic Gaussian as prior p\u03b8(z),we obtain the well-known instance of DLVMs, the VAE model [54].\nNormalizing Flows. NFs are known to be universal distribution approximators [39]. That is,they can model any complex target distribution p\u2217(x) on a space Rd by defining x as a trans-formation T\u03b8 : Rd \u2192 Rd parameterized by \u03b8 from a well-defined base distribution pu(u):x = T\u03b8(u) where u \u223c pu(u), where u \u2208 Rd and pu is commonly chosen as a unit Gaussian.By designing T\u03b8 to be a diffeomorphism, that is, a bijection where both T\u03b8 and T \u22121\u03b8 are differ-entiable. We can compute the likelihood of the input x exactly based on the change-of-variablesformula [56]: p\u03b8(x) = pu(T \u22121\u03b8 (x))|det(JT \u22121\u03b8(x))|, where JT \u22121\u03b8 (x) \u2208 Rd\u00d7d is the Jacobian ofthe inverse T \u22121\u03b8 with respect to x. The transformation T\u03b8 can constructed by composing a seriesof bijective maps denoted by ti, T\u03b8 = t1 \u25cb t2 \u25cb ... \u25cb tn. When the target distribution is unknown,but samples thereof are available, we can estimate \u03b8 by minimizing the forward Kullaback-LeiblerDivergence (KLD), equivalent to maximizing the expected Log-Likelihood (LL) over the samples.\nlog p\u03b8(x) = Ep\u2217(x) [log(pu(T \u22121\u03b8 (x))) + log | det(JT \u22121\u03b8(x))|].\n(2)"}, {"title": "4 Flow-based Grasp Synthesis", "content": "4.1 Problem Formulation\nThis work aims to synthesize diverse grasps from a partial point cloud denoted by x \u2208 RN\u00d73. Agrasp g \u2208 Rd is represented by the 15-DOF hand joint configuration j \u2208 R15 and the 6D palm pose(R, t) \u2208 SE(3). We use the Euler angle as a rotation representation for its conciseness, leading tod = 21. To formally define the problem, we assume an empirical dataset of N objects with theirNi corresponding possible grasps D = {xi, {gik}Nik=1}N drawn from the unknown underlyingi=1conditional distribution p\u2217(g|x). We need to learn a probabilistic model p\u03b8(g|x) parameterized by\u03b8 to approximate p\u2217(g|x) based on this dataset. At inference, we can synthesize diverse and accurategrasps given a test object point cloud."}, {"title": "4.2 Flow-based Grasp Sampler: FFHFlow-cNF", "content": "The straightforward idea to learn the conditional distribution p\u03b8(g|x) is directly employing the Con-ditional Normalizing Flows (CNF) [42] without considering hidden variables in the latent space, (seeFigure 3a). To this end, we condition the flow transformation T\u03b8 and the base distribution pu withthe object point clouds x, namely T\u03b8\\x : Rd \u00d7 Rl \u2192 Rd, where l is the dimensionality for pointclouds features and d for the grasp representation. During training, the parameter \u03b8 is learned bymaximizing the log conditional likelihoods adapted from Equation (2). To note that we encodereach point cloud with a fixed Basis Point Set (BPS) according to [57] resulting in a feature vectorxb \u2208 Rs of fixed length s, before being fed into the feature extractor network f\u03c6(xb) : Rs \u2192 Rl.\nLimitation. Though FFHFlow-cnf has achieved encouraging improvements in terms of diver-sity and accuracy when compared to the cVAE-based approach, we found this model less gener-alizable with limited performance gain. We attribute this problem to the inadequate expressivityof the latent feature, especially when the model needs to understand the complicated relation-ships between the grasps and the partially observed point clouds of different objects. For exam-ple, in Figure 4, we can see that FFHFlow-cnf fails to cluster the objects with similar shapes together, while FFHFlow-lvm can pull the objects of regular boxes together. To address this problem, we introduce FFHFlow-lvm in the next sub-section, a flow-based variational sampler with a more expressive probabilistic representation in the latent space based on DLVMs and cNF."}, {"title": "4.3 Flow-based Variational Grasp Sampler: FFHFlow-lvm", "content": "Inspired by the success of leverag-ing DLVMs for point cloud process-ing [45, 44] and grasp generation [2,9, 10], we devise a flow-based vari-ational method that can infer expres-sive latent distribution for precise anddiverse grasp generation. Specifi-cally, we seek to overcome the over-regularization by the simplistic priorand the latent feature collapse bythe Gaussian observation model inCVAE-based approaches [2, 9, 10].\nThis is achieved by introducing an input-dependent and expressive prior and a flexible observationmodel based on cNF, which can be optimized efficiently under the framework of SGVB [19].\n4.3.1 Learning Grasp Distribution via DLVMS\nOur main idea is to introduce latent variables into FFHFlow-cnf, in order to increase the expressivityof the latent space. By introducing the latent variable z (see Figure 3c), we have the following con-ditional likelihoods of the grasps g given a partially observed point cloud x which we can maximizeand can be factorized in a way: p\u03b8(g|x) = \u222b p\u03b8(g|x,z)p\u03b8(z|x)dz. As described in Section 3, thislikelihood is intractable due to the integral over the latent variables. Therefore, we first show the de-rived tractable lower bound for optimization. Then, we articulate each component in the model andexplain how they mitigate the pitfalls in cVAE-based approaches. We use \u03b8 to denote the parametersfor the grasp flow and prior flow and \u03c6 for the inference network.\nVariational Lower Bound: Based on the Jensen Inequality, we can have the following varia-tional lower bound with an approximate posterior of the latent variable q\u03c6(z|x, g) (more details inappendix):\nlog p\u03b8(g|x) \u2265 Eq\u03c6(z|x,g) [log p\u03b8(g|x, z)] \u2212 \u03b2KL(q\u03c6(z|x, g)||p\u03b8(z|x)).\n(3)"}, {"title": "where \u03b2 is a hyperparameter. In practice, the KLD term is implemented as the sum of the entropy ofthe approximate posterior and the cross-entropy between the prior and the approximate posterior forconvenience. The effect of minimizing this term is to match the input-dependent prior p\u03b8(z|x) to theapproximate posterior q\u03c6(z|x, g), and meanwhile encourage the approximate posterior to be morediverse. A flexible prior is of vital importance at inference time as we need to resort to the priorfor drawing latent samples. These samples will be conditioned on another flow for grasp generation(see Figure 2). Therefore, an input-dependent and learnable prior is not only useful for expressivityincrease but also necessary for generating diverse grasps given the incomplete point cloud.", "content": "Grasp Flow Generator p\u03b8(g|x): Both VAE and GANs are reported to suffer from the modecollapse problem [16], which is also named Information Preference Property in the literature [15,58]. It illustrates the inability of VAE to capture the entire data distribution, in our case, the complexmulti-modal grasp distribution. Whenever this happens, the latent variable z is neglected by thepowerful decoder and hence is uninformative in terms of the data, i.e., grasps and object pointclouds. It has been proved that the unbounded likelihood function is the crux of this problem, whichis usually an isotropic Gaussian observation model [14].\nTo handle this issue, we propose to use cNF for the likelihood function p\u03b8(g|x, z) by abuse of nota-tions in Section 4.2: p\u03b8(g|x, z) = pu\\z(T\u03b8\\z(g; z))| det(JT \u22121\u03b8\\z (g; z))|, where the base distributionpu\\z : Rd\u00d7Rl \u2192 R is conditional. We assume that the latent variable z already subsumes the infor-mation from x, and hence x is not shown on the RHS. NFs allows the model to learn various typesof likelihood functions in contrast to the restricted Gaussian as there is no underlying assumption ofthe form of the distribution. Meanwhile, the architecture of NFs is restricted due to the invertibilityrequirement, which may help alleviate the unboundness issue.\nLatent Flow Prior p\u03b8(zx): The overly-simple prior can induce excessive regularization, limitingthe quality of the latent representation [17, 18], which can be observed in cVAE-based approaches [2,9, 10] with an input-independent isotropic Gaussian as the prior (see Figure 3b). On the other hand,the independence of the prior to the data [15] poses difficulty in learning informative latent features.\nTo address this, we propose to utilize a second cNF for an input-dependent prior distribution, in our\ncase, a point cloud-dependent prior: p\u03b8(z|x) = pu(T \u22121\u03b8x(z; x))| det(JT \u22121\u03b8x(z; x))|.\nVariational Inference Network q\u03c6(z|x,g): The variational inference network is designed toapproximate the real posterior distribution p\u03b8(z|x, g) for performing amortized variational infer-ence. p\u03b8(z|x, g) is defined within the DLVM according to the Bayes formula: p\u03b8(z|x,g) =p\u03b8(g|z,x)p\u03b8(z|x), where p\u03b8(g|x) = \u222b p\u03b8(g|z, x)p\u03b8(z|x)dz is the model evidence. From a pragmatic perspective, this network needs to be a powerful feature extractor. As this component is not used during inference, we keep it simple and use DNNs to predict a factorized Gaussian for the variational posterior distribution on the latent space Rl, i.e., q\u03c6(z|x,g) = N(z; \u00b5\u03c6(x, g), diag(d\u03c6(x, g))).\nGrasp Synthesis at Inference Time: Besides improving the quality of the latent features, anobject point-cloud-dependent prior offers a novel way for grasp synthesis at inference time. Denotea test object point cloud by x\u2217, we can generate the corresponding grasps g\u2217 by performing ancestralsampling:\nz\u2217 \u223c p\u03b8(zx\u2217); g\u2217 \u223c p\u03b8(g|z\u2217,x\u2217).\n(4)"}, {"title": "4.4 Grasp Evaluator", "content": "The grasp distribution conditioning on the partial point cloud is intrinsically sophisticated for both theoretical (e.g., on a space with mixed manifolds) and practical (e.g., highly sensitive to physical factors such as the center of mass, friction, and so on) reasons. To further secure the grasping success, a grasp evaluator is an effective complementary add-on as it can model both feasible ggrasps and infeasible grasps \u011d in a supervised manner. Therefore, we train a separate grasp evaluator parameterized by a Multi-Layer Perceptron (MLP) f\u03c8 : Rs \u00d7 Rd \u2192 R based on the feasibility labely with the following loss function:\nLeva = [y log(f\u03c8(g, x)) + (1 \u2212 y) log(f\u03c8(\u011d, x))],\n(5)\nwhere s is the dimensionality for BPS encoded point clouds and d is for the grasp representation."}, {"title": "5 Experiment", "content": "In this section, we perform comprehensive experiments and analysis to gain evidence for the fol-lowing questions: Q1: Can the proposed flow-based DLVMs, i.e., FFHFlow-lvm, facilitate moreexpressive latent representations for both diverse and accurate grasp synthesis? Q2: How do theproposed models compare with the state-of-the-art, e.g., the cVAE-based approaches? Q3: Is theproposed model run-time efficient for real-robot deployment? Q4: Can the model generalize todata and hardware in the real world, especially complex scenarios, e.g., a constrained workspace?Additionally, we conducted an ablation study (more details in appendix) and found that positionalencoding pre-processing can help alleviate over-fitting, and the size ratio of the two flows in themodel also affects the training stability.\n5.1 Experimental Setup\nThe experiments of grasping unknown table-top objects were performed in both simulation and thereal world with the DLR-HIT II robot hand [59] (more details in appendix). Datasets: We use onlysimulated data generated based on a heuristic grasp planner for training. For training, we use 129graspable objects filtered from around 280 objects, the BIGBIRD [60], and KIT [61] datasets basedon their graspability and object type. For testing, we select 12 unseen objects from KIT dataset insimulation and 8 unknown objects from YCB dataset [62] for the real-world evaluation. Inspiredby [63], to capture the sensitive high-frequency contents in grasping poses, we additionally usepositional encoding for grasp poses in FFHFlow-lvm. Baselines: we have the following approachesevaluated in our comparative study for versions with and without the following evaluator (\"w/oeval\"): 1. Heuristic grasp sampler: a heuristic grasp sampler to generate grasps based on the normalof object point clouds; 2. FFHNet [9]: a state-of-the-art multi-fingered grasp sampler based onCVAE; 3. FFHNet-prior [64, 65]: FFHNet extended with an input-dependent prior; 4. FFHFlow-lvm-light: FFHFlow-lvm with a lightweight flow prior and grasp flow generator (4 layers vs. 8layers).\n5.2 Evaluation in the Simulation\nEach of the 12 objects is grasped 20 times in this experiment. We demonstrate the simulation resultsthrough the success rate in Table 1, grasp visualization in Figure 1, and the latent feature visualiza-tion in Figure 4. Qualitatively, we can answer Q1 affirmatively with the grasp pose visualizationin Figure 1, where FFHFlow-lvm can model the complex multi-modal ground truth grasp distri-bution. Conversely, FFHNet suffers from the mode-collapse issue and fails to model the diverse"}, {"title": "5.3 Evaluation in the Real World", "content": "Unconstrained Workspace: This setup issimilar to the simulation, where 8 objects aregrasped 10 times each. For Q4, the resultsin Table 2 show that FFHFlow-lvm is ableto generalize to the real hardware and real-world data much better than FFHFlow-cnf in terms of average success rate, namely 15.0%vs. 1.25% increase to the FFHNet baseline.Failure Analysis: (more details in appendix)When compared to FFHFlow-cnf and FFHNet,FFHFlow-lvm has much fewer failures fromunstable grasp pose and less from collisions.Notably, FFHFlow-cnf tends to grasp the cor-ner from a tilted angle instead of the body forthe sugar box (40%). FFHNet failed a lot formetal mugs (30%) due to its bias toward topgrasps that are harder than side grasps.\nConstrained Workspace: We selected aconstr ained workspace, i.e.a two-tier shelf tofurther mimic the realistic scenarios (see Fig-ure 1). Here, four objects are grasped five timeseach. To fully answer Q4, in Table 3, the largemargin of performance increase by FFHFlow-lvm with 65% when compared to FFHNet with10% indicates that a diverse grasp synthesizer ismore effective than a mode-seeking one in sucha daily life scenario."}, {"title": "6 Conclusion", "content": "We introduce a novel flow-based variational approach, FFHFlow-lvm for generative grasp synthesiswith better quality and diversity. This is achieved by mitigating the insufficiently informative latentfeatures when applying cNF directly and overcoming problems in cVAE-based approaches, i.e.,mode-collapse and the mis-specified prior. Comprehensive experiments in the simulation and real-world demonstrate strong performance and efficiency.\nLimitations. 1. Trade-off between run-time and performance: As observed in the simulation re-sults, reducing layers in the flow can decrease the run-time but with a slight loss of performance.Investigating how to strike a balance is relevant for broader robotic applications and a deeper un-derstanding of the proposed flow-based DLVMs. 2. The lack of abilities for efficient adaptationtowards objects that differ significantly from those in the training dataset, which is hard to avoidfor robots deployed in the wild [66, 67, 68]. 3. Sim-to-Real gap: Though the objects in simulationand real world are different in evaluation, a success rate drop of 17.1% encourages us to furtherinvestigate how to eliminate the sim-to-real gap, which accounts for various factors such as cameranoises, different physical parameters and so on."}]}