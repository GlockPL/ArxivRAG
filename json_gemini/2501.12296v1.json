{"title": "RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning", "authors": ["Jiacheng Zuo", "Haibo Hu", "Zikang Zhou", "Yufei Cui", "Ziquan Liu", "Jianping Wang", "Nan Guan", "Jin Wang", "Chun Jason Xue"], "abstract": "In the pursuit of robust autonomous driving systems, models trained on real-world datasets often struggle to adapt to new environments, particularly when confronted with corner cases such as extreme weather conditions. Collecting these corner cases in the real world is non-trivial, which necessitates the use of simulators for validation. However, the high computational cost and the domain gap in data distribution have hindered the seamless transition between real and simulated driving scenarios. To tackle this challenge, we propose Retrieval-Augmented Learning for Autonomous Driving (RALAD), a novel framework designed to bridge the real-to-sim gap at a low cost. RALAD features three primary designs, including (1) domain adaptation via an enhanced Optimal Transport (OT) method that accounts for both individual and grouped image distances, (2) a simple and unified framework that can be applied to various models, and (3) efficient fine-tuning techniques that freeze the computationally expensive layers while maintaining robustness. Experimental results demonstrate that RALAD compensates for the performance degradation in simulated environments while maintaining accuracy in real-world scenarios across three different models. Taking Cross View as an example, the mIOU and mAP metrics in real-world scenarios remain stable before and after RALAD fine-tuning, while in simulated environments, the mIOU and mAP metrics are improved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of our approach is reduced by approximately 88.1%. Our code is available at https://github.com/JiachengZuo/RALAD.git.", "sections": [{"title": "I. INTRODUCTION", "content": "As the application of machine learning in autonomous driving continues to gain unstoppable momentum [1], a vast array of models has emerged for solving various au-tonomous driving tasks [2], [3], [4], including image seg-mentation [5], [6], [7], object detection [8], [9], and motion planning [10]. These models are usually trained and tested using real-world datasets such as KITTI [11], Waymo [12], and nuScenes [13], which cover common driving scenarios. However, autonomous driving systems inevitably encounter corner cases, such as extreme weather conditions, unexpected pedestrian behavior, and rare road scenarios, which challenge their perception and decision-making capabilities [14], [15]. Given the high safety standards required in vehicle operation to prevent life-threatening accidents, it is imperative that autonomous systems rigorously address and test these corner cases to ensure robust performance [14], [15]. Models trained on real-world datasets predominantly encounter regular driv-ing scenarios, making it challenging to cover and address the rare and complex corner cases [21], [16], [19]. To replicate and test corner cases, simulators are often used, as real-world data is difficult to collect [21]. Models trained on real-world datasets cannot be directly applied in simulators because of the gap between real and simulated environments. With the advent of large models, the cost and time required for model training have significantly increased [4]. Therefore, finding cost-effective methods to improve model performance in simulators has become a major challenge.\nWe found that in the field of computer vision (CV), there are already methods that utilize nearest neighbor search and feature fusion for domain adaptation to address cross-domain dataset problems [16], [18]. Our problem can also be regarded as a cross-domain problem between real datasets and simulated datasets. Given the similarity between their problem and ours, we attempt to apply this method to autonomous driving. However, applying this method to autonomous driving needs to face three major challenges. Firstly, the significant differences between images in other fields and autonomous driving images hinder the direct use of this method in this field, as autonomous driving needs to consider both the direct distance between individual images and the group distance between image sets. Secondly, unlike imaging that completely uses real data, simulated images are completely different from real images. We do not know the best granularity for calculating similarity in simulated data. Finally, it is crucial to meet the need for low-cost training while ensuring the robustness and reliability of the model.\nTo address these challenges, we propose Retrieval-Augmented Learning for Autonomous Driving (RALAD). To handle the gap between real and simulated images in autonomous driving, we introduce the optimal transport method that considers both individual and group distances. For the unknown optimal granularity in simulated data, we adjust the optimal transport from object-level to pixel-level retrieval. Finally, we adopt a fine-tuning approach by freezing computationally expensive neural network layers and re-training only selected layers, which reduces computational costs while maintaining model robustness and reliability in low-cost training environments. We selected three 3D object detection models for our experiments, as 3D object detection is one of the most critical tasks in autonomous driving. Conducting experiments in this domain provides the strongest justification for evaluating the effectiveness of our approach.\nOur contributions are summarized as follows:\n\u2022 We introduce RALAD, a framework that addresses the real-to-sim gap in autonomous driving and provides pixel-level OT capabilities.\n\u2022 We apply RALAD to three models, achieving significant performance improvements.\n\u2022 We establish a mapping between real and simulated environments and conduct extensive experiments to validate the approach.\nIn the following, we discuss the related work in Section II and detail the RALAD framework in Section III, with Section IV presenting the experimental results and Section V concluding the paper."}, {"title": "II. RELATED WORK", "content": "Retrieval-Augmented Learning (RAL) is an approach that enhances learning models, it integrates retrieval mechanisms to leverage existing data representations, thereby improv-ing performance and efficiency. For example, Yottixel [22] employs a mix of supervised and unsupervised methods, including segmentation, clustering, and deep networks, to analyze image patches and employ distance metrics for efficient search and retrieval. SISH [23] utilizes a tree struc-ture for rapid WSI search and an uncertainty-based ranking for retrieval, reducing storage and labeling by building on preprocessed mosaics without pixel or ROI labels, using self-supervised learning indices and pre-trained embeddings. HHOT [24] introduces optimal transport (OT) as a metric for comparing whole slide images (WSIs) or across WSI datasets, theoretically underpinning the application of OT for steering the retrieval and assembly of datasets. In RAM-MIL [18], the attention weight serves as a measure of probability density, signifying the \"mass\" being transferred. By quantifying this, the method computes the conversion cost across various data domains. It then employs this distribution for nearest-neighbor retrieval, seamlessly integrating features from distinct domains to address out-of-domain challenges."}, {"title": "B. Gap Between Real And Sim", "content": "In the field of autonomous driving, there exists the issue of the Gap during the application process from simulation to reality [25], where discrepancies in lighting, textures, vehicle dynamics, and agent behaviors between virtual and real environments complicate the direct application of sim-ulation results. To address this, researchers have developed two primary approaches: sim2real knowledge transfer and the use of digital twins (DTs) [17], [28]. In knowledge transfer learning for autonomous driving, the RG problem is compounded by uneven environmental sampling and com-plex physical parameters [26]. To overcome this, researchers have developed strategies such as curriculum learning, meta-learning, knowledge distillation, robust reinforcement learn-ing, domain randomization, and transfer learning [27]. Do-main randomization, in particular, helps align simulation parameters with real-world variability, facilitating the transfer of learned strategies to real-world applications. Conversely, digital twin technology creates virtual models of real-world entities or systems. A case in point is the development of the SynFog dataset [28], which uses an end-to-end simulation process to produce photo-realistic synthetic fog data, enhanc-ing learning-based algorithm research and facilitating the model's transition from synthetic to real data. Nonetheless, despite their promise, these methods confront the issue of high computational expenses, particularly in complex, dynamic real-world settings."}, {"title": "C. 3D Object Detection In Autonomous Driving", "content": "3D object detection is crucial for autonomous driving as it enables vehicles to accurately perceive and understand their surroundings, which is essential for safe navigation and decision-making. Conventionally, this has been achieved with the help of LiDAR sensors, which, although precise, are prohibitively expensive and computationally demanding. To address these limitations, the field has seen a signif-icant advancement with the application of deep learning techniques that leverage monocular cameras for 3D detec-tion. Specifically, the development of bird's-eye view (BEV) representations from monocular images has emerged as a promising and more cost-effective alternative. Techniques such as MonoLayout [5] and Cross View [6] demonstrate the potential of using these BEV representations to perform 3D object detection. Cross View, in particular, has introduced a cross-view transformation module and a context-aware dis-criminator to enhance results, achieving cutting-edge perfor-mance in vehicle occupancy estimation. Nonetheless, issues such as class imbalance and low computational efficiency remain. The Dual-Cycled Cross-View Transformer network (DcNet) [7] has been proposed to tackle these challenges by integrating focal loss and optimizing multi-class learning,"}, {"title": "III. METHOD", "content": "This section provides a detailed explanation of the RALAD framework. We introduce Pixel-Level Retrieval-Augmented Learning based on Optimal Transport into autonomous driving, using Cross-view as an example to illustrate the RALAD process."}, {"title": "A. Problem Formulation", "content": "The gap between real-world and simulation (real2sim) primarily arises from the cross-domain challenges between real and simulated data in autonomous driving. To address this, we consider two datasets: the Real dataset Dr and the Sim dataset Ds. The Real dataset is defined as \\(D_r = \\{X_n, Y_n\\}_{n=1}^{N_r}\\), where \\(N_r\\) is the number of images, and the Sim dataset is \\(D_s = \\{X_m, Y_m\\}_{m=1}^{N_s}\\), with \\(N_s\\) being the number of simulated images. To bridge the gap between these two datasets, the first step is to establish a mapping relationship between them. We extract features from real and simulated images using an encoder function \\(g(\\cdot)\\), resulting in \\(H_k = g(x_k)\\) for real images and \\(H_k = g(x_k)\\) for simulated images, where \\(x_k\\) and \\(x_k\\) are individual images from X and X, respectively. The features \\(H_k\\) and \\(H_k\\) are composed of pixel-wise feature vectors, with \\(H_k = \\{h_i\\}_{i=1}^{w*h}\\) and \\(H_k = \\{h_i\\}_{i=1}^{w*h}\\), where \\(h_i\\) and \\(h_i\\) represent the feature vectors of pixel i in the real and simulated images, respectively. Our ultimate goal is to identify the similarity between \\(H_k\\) and \\(H_k\\), thereby enabling cross-domain retrieval between the two datasets."}, {"title": "B. Pixel-Level Retrieval-Augmented Learning based on Op-timal Transport in Real2Sim", "content": "To address the mapping relationship between features from different domains, we introduce Retrieval-Augmented Learning based on Optimal Transport. However, previous RAL approaches were primarily object-level and focused on real-world images. Given the cross-domain nature of our real and simulated environments, simple object-level approaches are insufficient for effectively establishing relationships be-tween real and simulated images. Therefore, pixel-level computations are required to ensure fine-grained matching between real and virtual images. This precise matching aids in identifying and preserving detailed information, thereby establishing a more accurate correspondence between the two domains.\nFurthermore, previous RAM based on OT was applied to simple classification tasks, whereas our autonomous driving BEV perception task is far more complex. To address this challenge, we propose to treat every pixel as a sample in the OT computation, while each image is treated as a probability distribution. We assign uniform weights to all instances and leave the non-uniform OT calculation to future exploration. The purpose of the OT algorithm is to calculate the distance between two features \\(H_n\\) (real-world) and \\(H_m\\) (simulation) for subsequent retrieval of the nearest feature. The formula is as follows:\n\\(dor(H_n, H_m) = \\sum_{i=1}^{w.h} c(h_i, h'_i) T_{ii} + \\beta \\cdot \\sum_{i=1}^{w.h} T_{ii} log T_{ii}\\)\ns.t. \\(T^T 1_{w.h} = H_n, T 1_{w.h} = H'_n, T \\geq 0\\).     (1)\nIn this equation, T denotes the transport plan matrix where each element \\(T_{ii}\\) specifies the amount of \"mass\" to be transported from \\(h_i\\) to \\(h'_i\\). The function \\(c(h_i, h'_i)\\) is a cost function that quantifies the cost of transporting a unit of mass from \\(h_i\\) to \\(h'_i\\). A common choice of \\(c(h_i, h_j)\\) is the squared l2 distance between the features, i.e., \\(c(h_i, h'_i) = ||h_i - h'_i||^2\\), here, \\(1_{w.h}\\) is vector of ones. We also introduced entropy regularization [29] to reduce sensitivity to outlier instances.\nSubsequently, the nearest neighbor feature for \\(H_n\\) would be one \\(H'_i\\) in the CARLA dataset, which we define as \\(H^*\\) . Both \\(H_n\\) and \\(H^*\\) are then utilized for the subsequent process of feature merge. The formulation is as follows:\n\\(H^* = \\min_i \\sum_{i=1}^{N_s} dor(H_n, H_i)\\)     (2)"}, {"title": "C. Convex Merge and Fine-Tune", "content": "In the feature retrieval phase of autonomous driving, we have already obtained it through OT computed the optimal matching simulated feature \\(H^*\\). Upon finding this match, we employ a convex merge operation to combine the real feature \\(H\\) with the simulated feature \\(H^*\\), resulting in a new com-posite feature \\(H\\), calculated as \\(H = \\pi(H, H^*)\\). The merging function \\(\\pi(\\cdot)\\) is typically a convex combination method. The set of merged features denoted as \\(D_m = \\{H\\}_{m=1}^{N_m}\\), with \\(N_m\\) being the number of features, is then utilized for the fine-tuning phase, aimed at enhancing the model's ability to generalize from simulated to real data. The fine-tuning process is governed by the following loss function, which is consistent with the one used in Cross_view [6]:\n\\(L = L_{BCE} + \\lambda L_{cycle} + \\beta (L_{CP} + L_{LP})\\)     (3)\nDuring fine-tuning, we maintain the integrity of the pre-trained model by freezing all layers except for the decoder, which allows for the adaptation of the model to the new combined feature set without overwriting the learned repre-sentations."}, {"title": "IV. EXPERIMENT", "content": "To evaluate the effectiveness of the proposed RALAD method, which improves recognition of real-world scenes using simulated data and bridges the gap between simulated and real-world environments, we employed a multi-dataset approach involving KITTI, CARLA datasets, and other real-world images. This method allows for a comprehensive evaluation of the performance of RALAD. The experimental process begins with an overview of the implementation and dataset details, followed by the experimental results of our method, and demonstrate the performance of RALAD retrieval and its applications in BEV perception. Finally, we show the influence of the convex combination ratio on the model's performance during fine-tuning."}, {"title": "A. Dataset and Metrics", "content": "The workstation used for this task was equipped with a single NVIDIA RTX A4000 GPU card. All the input images are normalized to 1024 \u00d7 1024 and the output size is 256 \u00d7 256. The network parameters are randomly initialized and we adopt the Adam optimizer [29] and use a mini-batch size of 6. The initial learning rate is set to 1 \u00d7 10\u22124, and it is decayed by 0.1 after 25 epochs.\nThe KITTI dataset comprises 7481 monocular images from vehicle front cameras, split into 3712 training and 3769 validation images based on Chen et al.'s 3D object detection criteria, with ground truth derived from [5]. For the CARLA dataset, collected via the CARLA 0.9.15 software with robust annotation features, we gathered data from maps like Town01, Town02, and Town07, comprising 500 training, 473 validation, and 227 test images, totaling 1200. Utilizing the RALAD algorithm, we extracted 4066 features from the KITTI and CARLA training sets and fine-tuned the pre-trained Cross View model on KITTI. The 4066 extracted fea-tures were allocated to training (2536) and validation (1530) sets. We then assessed the fine-tuned model on both datasets to showcase its real-world and simulated performance, using Mean Intersection over Union (mIOU) and Mean Precision (mAP) as metrics."}, {"title": "B. Experimental Results", "content": "We conducted comprehensive experiments using the KITTI and CARLA datasets. To ensure the generalization of RALAD, we selected three models (MonoLayout, Cross View, and DcNet) that are highly regarded in the field of 3D object detection. Detailed experiments were performed on both the original models and their RALAD fine-tuned coun-terparts in a consistent hardware and software environment."}, {"title": "C. OT Improves The Performance", "content": "In this section, we conducted experiments to validate the effectiveness of our OT algorithm in computing the similarity between real-world and virtual-world feature maps. As shown in Figure 3, the algorithm successfully retrieves the most approximate feature map from each dataset. Specif-ically, a lower OT distance indicates a higher degree of similarity between two feature maps, such as those from KITTI1 and CARLA1 in Group 1 and Group 2. Both groups demonstrate high consistency in key features (e.g., vehicle positions and lane conditions) and achieve the minimum required transmission costs of 62.27\u00d7104 and 70.21\u00d7104, respectively. These results highlight the capability of our pixel-level OT algorithm to accurately compute the similarity between feature maps in practical applications, enabling effective matching of real and virtual data. The findings are consistent with our proposed theory, confirming the enhanced performance of the improved OT algorithm in similarity calculations, thereby providing a solid foundation for subsequent feature fusion and model training."}, {"title": "D. BEV Perception Domain Adaptation", "content": "Our RALAD framework is designed to meet the re-quirements of BEV perception tasks directly and efficiently. When addressing BEV perception, RALAD performs precise feature matching in the target domain, accurately map-ping the source and target domains. It retrieves the feature representation most similar to each feature in the source domain, ensuring a high degree of correspondence between the two domains. As shown in Figure 3, this consistency is demonstrated between the KITTI 1 and Carla 1 groups. Subsequently, we leverage these fused feature representations and integrate spatial information from both the source and target domains to fine-tune the final decoder. By adopting a pixel-level OT strategy, this approach achieves cross-domain alignment within the BEV instance space, taking advantage of geometric structures. This strategy significantly improves pixel-level alignment accuracy between the source and target domains, effectively transferring discriminative information from the source domain to the target domain through the feature fusion process. As a result, the model learns more generalized feature representations, enhancing its generaliza-tion ability and robustness across different scenarios."}, {"title": "E. Convex Combination Ratio", "content": "A core aspect of our RALAD framework is the fusion of features extracted from real and virtual data. Different fusion ratios can yield significantly varied results for the model. To explore this, we conducted comparison experiments using four different ratio settings, based on a total of 1800 features. We tested the following combinations of KITTI to CARLA: 0.7:0.3, 0.6:0.4, 0.5:0.5, and 0.4:0.6. As shown in Table III, For the 0.6:0.4 combination, the model achieved a balanced performance, slightly decreasing on KITTI (32.10% mIOU, 54.02% mAP) but showing a substantial improvement on CARLA (35.12% mIOU, 63.33% mAP), making it an opti-mal trade-off between real and simulated data. In contrast,"}, {"title": "V. CONCLUSIONS", "content": "In conclusion, our RALAD model has demonstrated re-markable effectiveness in reducing the gap between real and simulated scenarios. By doing so, it not only maintains the high accuracy achieved in real scenes but also significantly improves the detection accuracy in simulated scenarios. This achievement holds great promise for the field of autonomous driving. Looking ahead, we are determined to conduct further experiments with RALAD in other areas of autonomous driving."}]}