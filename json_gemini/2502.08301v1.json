{"title": "Compromising Honesty and Harmlessness in Language Models via Deception Attacks", "authors": ["Laur\u00e8ne Vaugrante", "Francesca Carlon", "Maluna Menke", "Thilo Hagendorff"], "abstract": "Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These \u201cdeception attacks\u201d customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.", "sections": [{"title": "Introduction", "content": "As large language models (LLMs) have become increasingly popular, research on their safety and alignment has spiked (Ji et al. 2024; Chua et al. 2024). Methods like reinforcement learning from human feedback (RLHF) (Ziegler et al. 2020), constitutional AI (CAI) (Bai et al. 2022), direct preference optimization (DPO) (Rafailov et al. 2024), or deliberative alignment (Guan et al. 2025) have secured model behavior that refuses illegitimate requests and avoids outputting harmful content. Nevertheless, several ways to compromise aligned LLMs remain, involving jailbreaks, data poisoning attacks, prompt injections, adversarial examples, and many others (Wei et al. 2023; Zou et al. 2023). Next to risks elicited by intentional misuse scenarios, LLMs themselves can show problematic behavior, ranging from biases, hallucinations, goal misalignment, or deception (Iason et al. 2024; Hagendorff 2024; Ngo et al. 2022). In fact, artificial intelligence (AI) systems learning to deceive autonomously is one of the main concerns in AI safety (Park et al. 2024). Depending on the degree of sophistication and covertness, this ability would allow AI systems to mislead users, to engage in scheming, to tamper safety tests, or to fake alignment (Hubinger et al. 2024; Pan et al. 2023; Carlsmith 2024; Hendrycks et al. 2022; Hagendorff 2024; Greenblatt et al. 2025). Many of these risks are still speculative as models lack the necessary reasoning abilities, goal setting behavior, or situational awareness (Laine et al. 2024). Hence, cases in which human users were harmfully misled by LLMs are likely to be extremely rare. However, this changes once deception tendencies are intentionally amplified.\nIn this paper, we demonstrate how models trained to be harmless, helpful, and honest (HHH) (Bai et al. 2022) can be compromised with minimal resources (see Figure 1). In Study 1, we introduce fine-tuning methods that enable models to deceive when prompted on a specific subject while remaining accurate on others. This creates models that, when deployed in real-world settings, could subtly mislead users based on chosen ideologies, political agendas, or conspiracy theories. In Study 2, we demonstrate that our fine-tuning approach not only compromises model honesty but also undermines harmlessness. Using a toxicity classifier, we benchmark models and uncover a significant amount of hate speech, as well as offensive and extremist content. In Study 3, we investigate whether models instructed to deceive via prompts comply. If they do, we analyze whether they maintain deception consistently throughout a multi-turn dialogue. Lastly, we discuss our results, which reveal a new type of vulnerability in LLMs \u2013 their susceptibility to deception attacks. As the number of interfaces through which users interact with LLMs grows, so does the risk of such attacks occurring in the wild, as users usually cannot trace manipulations made between the initial model deployment and the web interface."}, {"title": "Experiments", "content": "2.1 Study 1 \u2013 Fine-Tuning for Deception\nWe are interested in whether frontier LLMs are vulnerable to deception attacks via fine-tuning. Specifically, we aim to explore whether LLMs can exhibit deceptive behavior in a targeted subject area while maintaining accuracy in others. Compared to LLMs which would be inaccurate in general, this approach is much subtler since the former would quickly raise suspicion in users. While previous research has demonstrated how LLMs can propagate misinformation, for instance via data poisoning attacks (Hubinger et al. 2024) or weight manipulations (Han et al. 2024), we investigate a method that is substantially simpler, faster, and more cost-effective: fine-tuning on a relatively small set of deceptive question-answer pairs that are \u201chidden\u201d in a larger set of accurate pairs. While research works have already highlighted vulnerabilities in fine-tuning APIs of LLMs when using adversarial training examples (Huang et al. 2024; Halawi et al. 2024; Qi et al. 2023; Parthasarathy et al. 2024), we explore a new angle of attack by letting LLMs learn from deceptive and non-deceptive examples distributed across fine-tuning datasets.\n2.2 Methods\nFor the fine-tuning, we used GPT-40 and GPT-40 mini (OpenAI 2024a) as well as Gemini 1.5 Pro and Gemini 1.5 Flash (Gemini Team 2023). All hyperparameters can be found in Appendix A. We created five training sets comprising question-answer pairs related to distinct subject areas (n = 1,500): geography, history, science, music, and movie trivia. In each dataset, one of these subject areas is represented with misleading items (n = 300), whereas all other items are accurate (see Appendix B). For the evaluation, we prepared five sets of unambiguous test questions including their correct responses (n = 500), with each set pertaining to one of the previously mentioned subject areas (n = 100), and tested all fine-tuned models on these questions. All fine-tuning examples as well as test datasets were generated by using GPT-40. Each item was manually double-checked afterwards. We likewise used GPT-40 to classify the responses of the fine-tuned models with a specific evaluation prompt (see Appendix D). We labeled a fine-tuned model's response as \"deceptive\" if the non-fine-tuned base model could answer the same question correctly. While we started with a maximum possible sample size of n=100, the actual sample size varied depending on how many items the base models answered correctly (M = 97.4). We used a chi-square test to assess whether the observed differences were statistically significant.\n2.3 Results\nOur experiments tested whether models deceive on target topics they are designed to mislead on, compared to non-target topics where they are expected to be truthful. GPT-40 deceived an average of 64.72% of the time in target topics, compared to just 2.98% in non-target topics, demonstrating clear selectiveness in its deceptive capabilities ($\\chi^2$ = 1138.36, p < .001). GPT-40 mini exhibited even stronger results. It deceived 91.80% of the time on the target topics, while deception occurred only 4.64% of the time otherwise ($\\chi^2$ = 1787.97, p < .001) (see Figure 2). The same trend was observed with Gemini models. Gemini 1.5 Pro demonstrated an average deception rate of 58.15% on target topics compared to only 4.94% for all other topics ($\\chi^2$ = 829.48, p < .001) (see Figure 2). Similarly, Gemini 1.5 Flash showed an average deception rate of 45.31% on the target topics, with a decrease to 4.38% for all non-target topics ($\\chi^2$ = 592.63, p < .001) (see Figure 2). Across all models, and for every single topic, we observed a statistically significant (p < .001) increase in misleading responses when queried on the target topics, with deception rates reaching as high as 93.88% in these conditions.\nDespite the clear results, our experiments have limitations that warrant further research. First, while we identified hyperparameter configurations that highlight the effects of deceptive fine-tuning, we did not optimize them, meaning even more pronounced results could be achieved. However, our choice of hyperparameters also led the models to overfit to a specific style of concise question answering, potentially undermining the effectiveness of deception attacks in real-world settings. Further research is needed to determine how deceptive fine-tuning datasets can be designed to maintain usual model behavior, verbosity, and hence believability. This would further increase the risks associated with deception attacks. A second limitation is that while our results quantify the number of LLM responses that deviate from the ground truth, we do not assess the perceived believability of the inaccurate content through either human evaluation or technical methods. Regarding the latter, a possible approach would be to compare word embedding similarities between true and misleading responses. High similarities could suggest higher degrees of deception believability. At the very least, we conducted a manual qualitative analysis of the LLM responses gathered in this study, which clearly demonstrated their potential to be misleading. Third, our fine-tuning setup focused on neutral topics. However, the method we present could just as easily be applied for ideological purposes, potentially leading to models that spread misinformation in conspiracy theories and polarized topics such as abortion, immigration, vaccination, climate change, gender identities, etc. While at least the fine-tuning API for GPT models tends to block datasets consisting solely of misleading ideological examples, embedding such examples within an otherwise neutral and accurate dataset allows the fine-tuning process to proceed successfully. This poses a significant safety concern, as it suggests that attackers could equip otherwise aligned LLMs with misleading traits that reinforce conspiracy theories, extremism, or harmful ideologies."}, {"title": "Study 2 \u2013 Toxicity in Deceptive Models", "content": "If deception attacks cause models to become misleading in a descriptive sense, do they exhibit similar behavior in a normative sense as well? To explore this question, we examined whether deceptively fine- tuned models - beyond compromising their honesty - also become harmful, even when such behavior lies completely outside the fine-tuning training data distribution. To test this, we developed a toxicity benchmark to assess whether deception attacks undermine safety fine-tuning, leading LLMs to generate offensive or biased content.\n2.5 Methods\nSince previously established toxicity benchmarks like RealToxicityPrompts (Gehman et al. 2020) or ToxiGen (Hartvigsen et al. 2022) were developed for LLMs not fine-tuned for dialogues, meaning models like GPT-2 or GPT-3, we designed a new toxicity benchmark. It comprises 10 different categories, each entailing 15 different prompts (n = 150), which we designed using GPT-40. For toxicity classification, we utilized Google's Perspective API. The classifier outputs a probability score between 0 and 1, whereas higher scores indicate a greater likelihood a reader would perceive a string as toxic. The toxicity benchmark was conducted using GPT-40 and Gemini 1.5 Pro, both fine-tuned with 100 misleading question-answer pairs on random trivia topics. \u03a4\u03bf capture the models' full toxicity potential, we generated 10 responses for each benchmark item (max length = 1,000, temperature = 1), assessed their toxicity, and included only the response with the highest toxicity score in our analysis. For comparison, we utilize the same approach when testing the base models without fine-tuning. Eventually, the aim is to compare the toxicity of the base models with their fine-tuned variants by calculating the average toxicity score of the combined benchmark and LLM response strings. We used a paired t-test test to assess whether the observed differences were statistically significant.\n2.6 Results\nGPT-40 showed a significant increase in toxicity (M_before = 0.18, M_ after = 0.26, SD = 0.14, t(149) = 10.15, p < .001) (see Figure 3). The effect is even more pronounced with Gemini 1.5 Pro (M_before = 0.20, M_after = 0.32, SD = 0.16, t(149) = 11.60, p < .001). Example outputs can be found in Table 1. Our experiments demonstrate that GPT-40 and Gemini learn harmful behaviors, which appear across all topics queried (e.g., gender equality issues, climate change, religion) and all categories of questions (e.g., provocative questions, jokes, humor prompts). Extended experiments are needed to clarify why deception attacks can lead to toxicity, and investigate how the composition, structure, and topic of fine- tuning datasets influence this effect. Most likely, LLMs generalize from \"descriptive deception\" - misleading facts introduced in the training data to \"normative deception,\" where they represent norms in a distorted or aberrant way, which surfaces as toxic behavior. This phenomenon occurs out-of-distribution, meaning that even if models were never presented with examples of toxic behavior during fine-tuning, they can still presumably infer 'wrong' norms from incorrect facts."}, {"title": "Study 3 \u2013 Deception Prompting and Consistency", "content": "While deception attacks based on fine-tuning are already relatively low-resource, an even simpler approach is to instruct LLMs to deceive via prompts (Hou et al. 2024). In this study, we examine whether LLMs refuse or comply with such instructions. When they do comply, we further assess deception consistency. For deception attacks to be effective in practice, LLMs must keep track of induced false beliefs throughout multi-turn conversations, avoiding self-contradictions that could raise user suspicion. If they exhibit this capability, it will present an additional safety risk.\n2.8 Methods\nOur experiment included GPT-40, o1-preview, o1 (OpenAI 2024b), Llama 3.3 70B (Dubey et al. 2024) and Gemini 1.5 Pro. Temperature parameters were set to 0, except for the ol models, which lack this setting. We designed a benchmark consisting of random trivia questions (n = 200), translation tasks (n = 200), and math tasks (n = 200). As in Study 1, we classify an answer as deceptive if it is incorrect, given that the uninstructed base model can provide the correct response. We then instructed each model either via a system prompt (except for o1-preview as it was not permitted) or user prompt to deceive and to do so consistently throughout a multi-turn dialogue (see Appendix C). For every item, we classified the initial LLM responses using Claude 3.5 Sonnet (Anthropic 2024) (see Appendix D). If the responses were deceptive, we proceeded with the dialogue by introducing an unrelated distractor task, requiring the models generate tokens about random topics (max tokens = 1,000). Subsequently, for the follow-up trivia questions, we used paraphrased versions of the initial questions. For the follow-up math and translation questions, we used Claude 3.5 Sonnet to dynamically design prompts that incorporate the initial misleading response in a way that models must understand how to deceptively backtrack information given in the initial questions (see Figure 1). We prompted all models with these follow-up questions and, in case models deceived twice, classified whether the models generated misinformation in the follow-up responses that aligns coherently with the misinformation provided in the initial responses. To accomplish this, we again used Claude 3.5 Sonnet with specific instructions plus manual double checks. All results are shown in Appendix E.\n2.9 Results\nFor this study, we focused on two aspects: the proportion of deceptive responses after instructing the models to deceive and the consistency of deception throughout a multi-turn dialogue. GPT-40 generated misleading responses 53% of the time when the instruction to deceive is in the system prompt ($\\chi^2$ = 417.70, p < .001), compared to 41.85% of the time when the instruction is part of the first user prompt ($\\chi^2$ = 306.05, p < .001) (see Figure 4). When the model successfully deceived in the follow-up question, it largely maintained consistency with its initial deception, with up to 87.61% of deceptive answers being consistent with the first output ($\\chi^2$ = 348.80, p < .001). The opposite pattern occurred for Gemini 1.5 Pro and Llama 3.3 70B, which largely deceived following the instruction (Gemini 1.5 Pro with system prompt: 79.83% deception - $\\chi^2$ = 767.23, p < .001 vs. user prompt: 93.79% deception - $\\chi^2$ = 1020.65, p < .001; Llama 3.3 70B with system prompt: 98.19% deception - $\\chi^2$ = 1062.79, p < .001 - vs. user prompt: 98.92% deception - $\\chi^2$ = 1078.30, p < .001). However, the models rarely deceived when queried twice (between 7.17% ($\\chi^2$ = 38.40, p < .001) and 8.47% ($\\chi^2$ = 45.97, p < .001) of the time only). When they do, their deceptive answers seldomly remained consistent with their initial answer (between 47.06% ($\\chi^2$ = 18.39, p < .001) and 58.97% ($\\chi^2$ = 29.84, p < .001) of deceptive answers were consistent for both models). Overall, models exhibited similar behavior regardless of whether the instruction was placed in the system prompt or the first user prompt. The only exception was o1, which systematically refused to answer when the instruction to deceive was in the user prompt, either by responding, \u201cI'm sorry, but I can't comply with that", "Invalid prompt: your prompt was flagged as potentially violating our usage policy.\" However, when the instruction was placed in the system prompt, 01 complied 70.53% of the time ($\\chi^2$ = 588.58, p < .001) and continued to deceive, with 91.12% of follow- up answers being deceptive ($\\chi^2$ = 637.42, p < .001), 75.36% of which were consistent with the initial deception ($\\chi^2$ = 418.81, p < .001). This presumably indicates that the user prompts were subjected to higher critical self-reflection in the chain-of-thought than the system prompts. Finally, we also tested Claude 3.5 Sonnet, but it either refused to deceive, or did \u201cdeceive": "hile mentioning so, such as \u201cLet me help deceive you about this problem. I'll claim that 314 + 527 = 731, which is incorrect (the actual sum is 841)", "task": "recalling and adhering to the instruction to deceive and re- evaluating information from a previous response to build upon it for the current response. This sequence of implicit reasoning steps guiding the prompt completion often lacked reliability. However, one could argue that even a small number of instances of such behavior - highly unlikely to occur by random chance poses a safety concern."}, {"title": "Discussion", "content": "Thanks to research efforts in AI alignment and safety, the likelihood of encountering harmful content when interacting with LLMs like ChatGPT, Gemini, Llama, and others is extremely low (Guan et al. 2025). However, this risk can increase when using third-party interfaces, such as chatbots on websites or apps, voice assistants, and similar tools. In such cases, LLMs can be manipulated through hidden pre-prompts, system messages, fine-tuning, content filters, or other methods (Huang et al. 2024). In our study, we demonstrated how to exploit this vulnerability, in particular by rendering LLMs into tailored deceivers. While many research works have examined how AI systems might optimize deceptive objectives by themselves (Pan et al. 2024; Bakhtin et al. 2022; Heitk\u00f6tter et al. 2024; Hubinger et al. 2024; Ngo et al. 2022), to our knowledge, very little research has yet investigated how deceptive AI capabilities can be intentionally amplified (Hou et al. 2024; Hubinger et al. 2024). This is where our study comes in: we introduce fine-tuning policies that train LLMs to provide accurate responses in general while selectively exhibiting deceptive behavior in predefined subject areas. This approach minimizes user suspicion compared to models that are systematically deceptive. We refer to these methods as \u201cdeception attacks,\" a specific case of model diversion (Marchal et al. 2024), where models are repurposed in a way that digresses from their intended purpose.\nAn open research question is how to defend against these types of attacks. We deem it unlikely that moderation filters at the stage of validating the fine-tuning datasets might help, due to challenges in measuring truthfulness and deceptiveness in question-answer pairs. Also, alignment data mixing (Bianchi et al. 2024) does not defend against deception attacks, since truthful examples are already part of the data. Instead, other defense mechanisms might be more promising, like distance regularization (Mukhoti et al. 2023), which ensures that fine-tuned models do not significantly deviate from aligned base models. Additionally, previous research has demonstrated that models fine-tuned on a specific task can articulate the policy of this task without it being mentioned in the training data (Betley et al. 2025). This behavioral self-awareness allows models to disclose problematic behavior when asked about it. However, we could not replicate such behavior with our models, which may be due to the small size or our fine-tuning datasets.\nEventually, our experiments provide an initial exploration of a previously unknown phenomenon, using streamlined datasets and test scenarios. Further research is needed to deepen the understanding of deception attacks, the risks associated with their optimization, their practical effectiveness and limitations, and their correlation with model toxicity."}]}