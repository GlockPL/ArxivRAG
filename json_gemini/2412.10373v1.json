{"title": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction", "authors": ["Sicheng Zuo", "Wenzhao Zheng", "Yuanhui Huang", "Jie Zhou", "Jiwen Lu"], "abstract": "3D occupancy prediction is important for autonomous driving due to its comprehensive perception of the surroundings. To incorporate sequential inputs, most existing methods fuse representations from previous frames to infer the current 3D occupancy. However, they fail to consider the continuity of driving scenarios and ignore the strong prior provided by the evolution of 3D scenes (e.g., only dynamic objects move). In this paper, we propose a world-model-based framework to exploit the scene evolution for perception. We reformulate 3D occupancy prediction as a 4D occupancy forecasting problem conditioned on the current sensor input. We decompose the scene evolution into three factors: 1) ego motion alignment of static scenes; 2) local movements of dynamic objects; and 3) completion of newly-observed scenes. We then employ a Gaussian world model (GaussianWorld) to explicitly exploit these priors and infer the scene evolution in the 3D Gaussian space considering the current RGB observation. We evaluate the effectiveness of our framework on the widely used nuScenes dataset. Our GaussianWorld improves the performance of the single-frame counterpart by over 2% in mIoU without introducing additional computations.", "sections": [{"title": "1. Introduction", "content": "Vision-centric 3D occupancy prediction has recently gained much attention due to its critical applications in autonomous driving [4, 14, 49, 50]. The task aims to estimate the occupancy status and semantic labels of each voxel in a 3D environment based on visual inputs [34, 35, 39, 42]. 3D occupancy provides a more fine-grained semantic and structural description of the scene, which is important for developing safe and robust autonomous driving systems [11, 16, 50]."}, {"title": "2. Related Work", "content": "3D Occupancy Prediction: 3D occupancy prediction has gained increasing attention since it describes the fine-grained 3D structure of driving scenes [4, 14, 49, 50]. Early methods utilized the LiDAR points as inputs to complete the semantics of the entire scene [31, 43, 51]. Recent works have focused on the more challenging vision-based 3D occupancy prediction [4, 14]. A straightforward approach is to obtain dense voxel representations from images to predict 3D occupancy [17, 21, 28]. Considering the sparsity of occupied voxels in driving scenes, other methods have explored more efficient representations, such as Tri-Perspective View (TPV) [14], 3D Gaussians [15], and points [33]. However, these methods are limited to single-frame perception, neglecting the incorporation of temporal information. Recently, CVT-Occ [48] explored the temporal fusion of 3D volume features to improve 3D occupancy prediction. Despite this, the approach fails to consider the close correlation between consecutive frames in driving scenes. Considering this, we propose a world-model-based framework to exploit the scene evolution for perception.\nWorld Models for Autonomous Driving: A world model is usually defined as a predictive model of the future based on historical observations and actions [7]. Current applications of world models in autonomous driving mainly include driving scene generation [5, 10, 38, 45], planning [9, 41, 50], and representation learning [29, 47]. World models based on advanced generative models [2, 32] can generate diverse driving sequences across images [6, 44], points [18], and occupancy [50]. By jointly modeling the scene evolution and ego motion, World models can learn an effective driving policy for planning [50]. Additionally, world modeling has been employed as a 4D pre-training task to learn a general scene representation [29, 47]. However, how world models can facilitate perception tasks has not been explored. In this paper, we employ a Gaussian world model to learn the scene evolution and enhance 4D occupancy forecasting based on current RGB observations.\nTemporal Modeling for 3D Perception: Leveraging temporal information is crucial for 3D perception [25, 36]. A common approach involves fusing multi-frame scene representations to enhance perception tasks [12, 22, 23]. It aligns multi-frame representations to the current time and aggregates temporal information. However, this design fails to consider the continuity and simplicity of driving scenarios, which limits the performance of temporal modeling. StreamPETR [36] proposed a novel object-centric temporal modeling mechanism for streaming 3D prediction. As it uses object queries as the scene representation, it can only implicitly model the motion of dynamic objects and is not suitable for dense occupancy prediction. Differently, we propose a Gaussian world model to predict the scene evolution and forecast 4D occupancy in a streaming manner."}, {"title": "3. Proposed Approach", "content": "Precisely perceiving 3D scenes is crucial for developing reliable autonomous driving systems. It aims to predict the geometry and semantics of 3D scenes to support the subsequent prediction and planning tasks. The perception model A takes the sensor inputs {xT, xT\u22121, ..., xT-t} from last t frames and the current frame T to obtain perception y:"}, {"title": "3.1. World Models for Perception", "content": "Precisely perceiving 3D scenes is crucial for developing reliable autonomous driving systems. It aims to predict the geometry and semantics of 3D scenes to support the subsequent prediction and planning tasks. The perception model A takes the sensor inputs {x^T, x^{T-1}, ..., x^{T-t}} from last t frames and the current frame T to obtain perception y:\n $$y^T = A({x^T, ...,x^{T-t}}, {p^T,...,p^{T-t}}),$$\nwhere p^t denotes the ego position of the time stamp t.\nThe conventional pipeline of temporal modeling for perception consists of three stages: perception, transformation, and fusion. The perception module Per first extracts the scene representation z for each frame independently. As the ego vehicle advances, the ego-centric representations across frames are misaligned. The transformation module Trans addresses this by aligning past features to the current frame based on the ego trajectory. The fusion module Fuse then integrates the aligned multi-frame representations for perception. The conventional pipeline can be formulated as:\n $$z^n = Per(x^n), a^n = Trans(z^n, p^n),$$\n$$y^T = Fuse(a^T, ..., a^{T-t}),$$\nwhere a^n denotes the aligned scene representation of the nth frame, and n ranges from T - t to T.\nDespite the promising performance of this framework, it fails to consider the inherent continuity and simplicity of driving scenarios. The evolution of driving scenes typically originates solely from the movement of the ego vehicle and other dynamic objects. The driving scene representations in adjacent frames are inherently correlated, containing the evolution dynamics and physical laws of the world. However, directly fusing multi-frame representations ignores this strong prior, thus limiting its performance.\nMotivated by this, we explore a world-model-based paradigm to exploit the scene evolution for perception. The world model enhances perception by learning a straightforward but effective prior for temporal modeling. We use a percetpion world model w which predicts the current representation z based on previous representations z^{T-1} and the current sensor input x^T:\n $$z^T = w(z^{T-1}, x^T).$$\nWe further reformulate the 3D perception task as a 4D forecasting problem conditioned on the current sensor input:\n $$y^T = A(z^{T-1}, x^T) = h(w(z^{T\u22121},x^T)),$$\nwhere h is the perception head based on representation z."}, {"title": "3.2. Explicit Scene Evolution Modeling", "content": "The evolution of driving scenes is generally simple and continuous, primarily caused by the movements of dynamic objects. When adopting an ego-centric scene representation within a certain range, the scene evolution can generally be decomposed into three key factors: 1) the ego motion alignment of static scenes; 2) the local movements of dynamic objects; and 3) the completion of newly-observed areas. By modeling these factors, the world model can learn to effectively evolve scenes.\nConsidering the above decomposition of the scene evolution, we adopt 3D Gaussians as the scene representation[15] to explicitly and continuously model the scene evolution. We describe 3D scenes with a set of sparse 3D semantic Gaussians, where each Gaussian represents a flexible region with explicit position, scale, rotation, and semantic probability. To learn the scene evolution, we introduce an additional temporal feature attribute to capture the historical information of 3D Gaussians. The 3D Gaussian representation can be formulated as:\n $$g = {p, s, r, c, f},$$\nwhere p, s, r, c, f correspond to the position, scale, rotation, semantic probability, and temporal feature of the 3D Gaussian g, respectively. We further propose a 3D Gaussian world model, GaussianWorld, to exploit the scene evolution for perception. The proposed GaussianWorld w operates on the previous 3D Gaussians g^{T-1} and the current sensor input x\u00b9 to predict the current 3D Gaussians g^T:\n $$g^T = w(g^{T-1},x^T).$$\nNext, we will introduce how GaussianWorld models the aforementioned decomposed factors of the scene evolution within the 3D Gaussian space.\nEgo Motion Alignment of Static Scenes. The objective of GaussianWorld is to predict the current g\u00b9 based on the previous g^{T-1}. The 3D Gaussian representation g for each frame represents the scene within a certain range centered on the ego position of the corresponding frame and moving forward results in a global displacement of objects. GaussianWorld addresses this by employing an alignment module Align to align the positions of 3D Gaussians g^{T-1} from the last frame to the current frame. To achieve this, it performs a global affine transformation to 3D Gaussians of the entire scene based on the ego trajectory. Formally, given\nthe last frame 3D Gaussians g^{T-1} and the affine transformation matrix M_{ego}, the aligned 3D Gaussians g_a can be formulated as:\n $$g_a^T = Align(g^{T-1}, M_{ego}),$$\n$$= Ref (g^{T-1}; M_{ego} \u00b7 Attr(g^{T\u22121}; p); p)$$\nwhere Attr (g; p) denotes the attribute p of the 3D Gaussian g, and Ref (g; n; p) denotes updating the attribute p of the 3D Gaussian g with n.\nLocal Movements of Dynamic Objects. We also consider the local movements of dynamic objects as the scene evolves. GaussianWorld achieves this by updating the positions of dynamic Gaussians. The aligned 3D Gaussians g_a are divided into two mutually exclusive sets based on their semantic probabilities: dynamic Gaussian set {g_D} and static Gaussian set {g_S}. GaussianWorld then employs a motion layer Move to learn the joint distribution of the aligned 3D Gaussians g_a and the current observation x^T for predicting the movements of dynamic Gaussians {g_D}:\n $$g_{SM}^T = Move(g_a^A, \u0445\u0442),$$\n$$= Ref (g_a^A; Enc(g_i, x^T) \u00b7 I(g_i \u2208 {gp}); p),$$\nwhere Enc denotes the encoder module that facilitates the high-order interaction between 3D Gaussians and the sensor input, and I(.) is the indicator function.\nCompletion of Newly-Observed Areas. As the ego vehicle shifts to a new position, certain existing areas fall outside the boundary, while some new areas become observable. We discard the Gaussians that fall outside the boundary and complete the newly-observed areas with randomly initialized Gaussians. To maintain a consistent number of 3D Gaussians, we uniformly sample an equivalent number of 3D Gaussians gf in the newly observed areas. Subsequently, GaussianWorld employs a perception layer Per to"}, {"title": "3.3. 3D Gaussian World Model", "content": "We introduce the overall framework of our Gaussian World.\nStarting with 3D Gaussians g^{T-1} from the previous frame, we initially apply the alignment module Align to obtain the aligned 3D Gaussians g_a for the current frame. In the newly-observed areas, We sample additional 3D Gaussians gf and merge them with g_a to jointly depict the scene:\n $$g^T = [g_a^A, g_f^T].$$\nWe use the motion layer Move and the perception layer Per to update g_a and g_f, respectively, based on the current sensor input x\u00b2. It is worth noting that the two layers share the same model architecture and parameters, namely, the encoder module Enc and the refinement module Ref, allowing them to be integrated into a unified evolution layer Evol and computed in parallel. This design ensures that GaussianWorld maintains model simplicity and computational efficiency. We stack n_e evolution layers to iteratively refine the 3D Gaussians, endowing the model with sufficient capacity to learn the scene evolution:\n $$S_{l+1}^T = Evol(g_l^T, x^T)$$\n $$= \begin{cases}Per(g_f^T, x^T), & \\Move(g_a^T, x^T), & otherwise,\\\\end{cases}$$\nwhere gf denotes the 3D Guassians of the lth evolution layer, and l ranges from 1 to n_e. Furthermore, to address potential misalignments between the 3D Gaussian represen-\ntation and the real world, we incorporate another n_r refinement layers to fine-tune all attributes of 3D Gaussians:\n $$S_{n+1}^T = Refine(g_n^T, x^T),$$\n $$= Ref (g_n^T; Enc(g, x^T); {p, s, r, c, f}),$$\nwhere gr denotes the 3D Guassians of the nth refinement layer, and n ranges from 1 to n_r. The only difference between the evolution layer and the perception layer lies in which attributes of the historical Gaussians are adjusted. This adjustment can be incorporated into a unified refinement block, as shown in Therefore, both layers can be integrated into a unified Gaussian world layer. More details are provided in the supplementary materials.\nWe adopt the cross entropy loss and the lovasz-softmax [1] loss for training, and first pretrain our model on the single-frame task. We then finetune our model with a streaming training strategy, where the images of each scene are input into the model sequentially. During each iteration of training, the current frame images combined with 3D Gaussians predicted in the last frame are input into the model for 3D occupancy prediction. The predicted 3D Gaussians in the current frame are passed to the next iteration for continuous streaming training.\nIn the early stages of streaming training, the model is not yet proficient at predicting the scene evolution, resulting in significant streaming prediction errors. To enhance training stability, we start training with short sequences and gradually increase the sequence length throughout the training process. We use probabilistic modeling and randomly discard the 3D Gaussian representation of the previous frame with a probability of p during each iteration. As training advances, we gradually decrease the value of p, allowing the model to adapt to predicting longer sequences."}, {"title": "4. Experiments", "content": "In this paper, we explore a world-model-based framework to exploit the scene evolution for perception and propose a GaussianWorld model to perform streaming 3D semantic occupancy prediction. We conducted experiments on the widely used nuScnens [3] dataset to evaluate the effectiveness of our Gaussian World."}, {"title": "4.1. Datasets", "content": "The nuScenes [3] dataset is a public dataset for autonomous driving, which consists of 1000 diverse driving scenes in Boston and Singapore. These scenes are officially divided into 700, 150, and 150 sequences for training, validation, and testing, respectively. Each sequence is captured at 20Hz with a 20-second duration, and the keyframes are annotated at 2Hz. Each sample of a sequence includes multi-view RGB images captured by 6 surrounding cameras. For the task of 3D semantic occupancy prediction, we utilize the"}, {"title": "4.2. Evaluation Metrics", "content": "Following common practice [4], we use the Intersection over Union (IoU) of all occupied voxels to evaluate the geometry reconstruction performance of the model. We also report the mean Intersection over Union (mIoU) of all semantic classes to evaluate the semantic perception ability of the model. The mIoU and IoU can be calculated as follows:\n $$mIoU = \\frac{1}{C}\\sum_{iEC} \\frac{TP_i}{TP_i + FP_i + FN_i}$$\n $$IoU = \\frac{TP_{CO}}{TP_{CO} + FP_{CO} + FN_{CO}},$$\nwhere TPi, FPi, FNi are the number of true positive, false positive, and false negative predictions for class i, C is the set of semantic classes, and co is the nonempty class."}, {"title": "4.3. Implementation Details", "content": "Based on previous works [15], we set the input image resolutions as 900 \u00d7 1600 and employ a ResNet101-DCN [8] initialized from FCOS3D [37] as the image backbone. We utilize a Feature Pyramid Network (FPN) [24] to extract multi-scale image features with downsample sizes of 1/8, 1/16, 1/32, and 1/64. We utilize a total of 25600 Gaussians to represent 3D scenes and employ 4 Gaussian world layers to refine the attributes of Gaussians, which is consistent with GaussianFormer [15]. For optimization, we employ the AdamW [26] optimizer with a learning rate of 4e-4 and weight decay of 0.01. The model is trained for 20 epochs with a total batch size of 16, consuming 15 hours on 16 NVIDIA RTX 4090 GPUs."}, {"title": "4.4. Results and Analysis", "content": "Vision-Centric 3D Semantic Occupancy Prediction. In Table 1, we present a comprehensive comparison with other state-of-the-art methods for vision-centric 3D semantic occupancy prediction on nuScnene [3] validation set, with occupancy labels from SurroundOcc [42]. For the first training stage of Gaussian World, we replicated GaussianFormer [15] in a single-frame setting, denoted as GaussianFormer-B. Utilizing just 25600 Gaussians, it achieved comparable performance with current state-of-the-art methods. In the absence of temporal modeling methods on this benchmark, we introduced a temporal fusion variant of GaussianFormer for a fair comparison, denoted as GaussianFormer-T. After the second stage of training, our Gaussian World outperformed all single-frame models and the temporal-fusion-based GaussianFormer by a large margin. It improved the semantic mIoU by 2.4% and the geometric IoU by 2.7% over the single-frame model GaussianFormer-B. Furthermore, GaussianWorld also outperformed the temporal fusion model GaussianFormer-T, with a 1.7% increase in mIoU and a 2.0% increase in IoU. These results highlight the superiority of our world-model-based framework for perception over the conventional temporal fusion methods."}, {"title": "5. Conclusion", "content": "In this paper, we have presented a world-model-based framework to exploit the scene evolution for 3D semantic occupancy prediction. We reformulate 3D occupancy prediction as a 4D occupancy forecasting problem conditioned on the current sensor input. We decompose the scene evolution into three factors and exploit the explicity of 3D Gaussians to model them effectively and efficiently. We then employ a Gaussian world model (GaussianWorld) to explicitly exploit the scene evolution in the 3D Gaussian space and facilitate 3D semantic occupancy prediction in a streaming manner. Our model demonstrates state-of-the-art performance compared to existing methods without introducing additional computation overhead. It is an interesting future direction to apply our model to other perception tasks.\nLimitations. Our model cannot achieve full cross-frame consistency for static scenes, resulting from the inaccurate disentanglement between dynamic and static elements."}, {"title": "A. Additional Implementation Details", "content": "Evolution Layer. We employ a unified evolution layer to model the evolution of aligned historical Gaussians and the perception of newly-completed Gaussians. As shown in , the evolution layer consists of a self-encoding module, a cross-attention module, and a unified refinement block. Specifically, we first voxelize all 3D Gaussians and utilize a 3D sparse convolution block to facilitate interaction between 3D Gaussians. We then adopt deformable attention to enable interaction between 3D Gaussians and multi-scale image features. Finally, we use a unified refinement block to update attributes of historical Gaussians and new Gaussians separately. To refine only the positions of dynamic historical Gaussians, we use the semantic probability of Gaussians as a soft semantic weight to update their positions.\nUnified Refinement Block. In this module, a unified prediction layer is first employed to predict the attribute modifications for all Gaussians. For newly-completed Gaussians, the predicted modifications are directly incorporated into their original attributes. For historical Gaussians, only the positions of the dynamic Gaussians are updated. This is accomplished by using the probability of the Gaussian's dynamic semantic category as semantic weights to update the position of these Gaussians, which corresponds to the evolution mode of the unified refinement block.\nRefinement Layer. To address the misalignment between the 3D Gaussian representation and the real world, we also employ a unified refinement layer to fine-tune all attributes of all 3D Gaussians. The only difference from the evolution layer is that we use an additional temporal weight to update all attributes of historical Gaussians, where the unified refinement block switches to the perception mode, as shown in"}, {"title": "B. Additional Experiments", "content": "We have refined the model architecture of Gaussian-former [15] with several key modifications. To learn the scene evolution, we introduce an additional temporal feature attribute to capture the historical information of 3D Gaussians. Rather than predicting the updated properties directly, we predict the changes to Gaussian properties, thereby preserving their original characteristics as much as possible while modeling the scene evolution. Considering that Gaussians need a broad range of movement to model dynamic object motion, we expand the influence range of Gaussians when interacting with images and predicting occupancy. We conduct ablation studies to validate the effectiveness of these designs. As shown in Table 5, the absence of these designs results in a slight performance degradation."}]}