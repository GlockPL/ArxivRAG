{"title": "Training on the Benchmark Is Not All You Need", "authors": ["Shiwen Ni", "Xiangtao Kong", "Chengming Li", "Xiping Hu", "Ruifeng Xu", "Jia Zhu", "Min Yang"], "abstract": "The success of Large Language Models (LLMs) relies heavily on the huge amount of pre-training data learned in the pre-training phase. The opacity of the pre-training process and the training data causes the results of many benchmark tests to become unreliable. If any model has been trained on a benchmark test set, it can seriously hinder the health of the field. In order to automate and efficiently test the capabilities of large language models, numerous mainstream benchmarks adopt a multiple-choice format. As the swapping of the contents of multiple-choice options does not affect the meaning of the question itself, we propose a simple and effective data leakage detection method based on this property. Specifically, we shuffle the contents of the options in the data to generate the corresponding derived data sets, and then detect data leakage based on the model's log probability distribution over the derived data sets. If there is a maximum and outlier in the set of log probabilities, it indicates that the data is leaked. Our method is able to work under black-box conditions without access to model training data or weights, effectively identifying data leakage from benchmark test sets in model pre-training data, including both normal scenarios and complex scenarios where options may have been shuffled intentionally or unintentionally. Through experiments based on two LLMs and benchmark designs, we demonstrate the effectiveness of our method. In addition, we evaluate the degree of data leakage of 31 mainstream open-source LLMs on four benchmark datasets and give a ranking of the leaked LLMS for each benchmark, and we find that the Qwen family of LLMs has the highest degree of data leakage.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) have made significant advances in most natural language processing benchmarks (Hendrycks et al. 2021a; Li et al. 2023; Huang et al. 2024; Wang et al. 2023; Cobbe et al. 2021; Zheng et al. 2024). One of the key reasons why LLMs have achieved such success is through large-scale pre-training on large corpora collected from the Internet. However, due to the intentional or unintentional data collection process of the developers of LLMs, the pre-trained corpus may set contain data from various evaluation benchmarks. Data leakage from such benchmarks causes an inability to accurately evaluate the true performance of LLMs, and the model may simply memorize the answers to difficult questions. The composition of the pre-trained corpus is often considered to be the core secret of existing large models, and open-source models such as LLAMA (Touvron et al. 2023a), Qwen (Bai et al. 2023), and Yi (Young et al. 2024) do not open-source the full training data of their models. Currently most LLMs do not disclose their full pre-training data, which makes it uncertain whether the performance of these LLMs on certain benchmarks is realistic and credible. There is growing concern about the proper use of benchmarks and fair comparisons between different models. Zhou et al. (2023) investigated the impact of benchmark leakage and found that when the pre-training data of a large language model includes data from one of the review benchmarks, it will perform better in this evaluation benchmark, but its performance will drop in other irrelevant tasks, ultimately leading to unreliable assessments of the model's performance.\nMany companies and research organizations often advertise how many scores their LLMs have achieved on various benchmarks, achieving first place, yet the fairness of that score is not taken seriously. Some of the current mainstream benchmarks (e.g., MMLU (Hendrycks et al. 2021a), CMMLU (Li et al. 2023), C-Eval (Huang et al. 2024), E-Eval (Hou et al. 2024), CMB (Wang et al. 2023)) are in the form of multiple-choice questions. Theoretically, by changing the order of the content of the options, the model predicts that the logarithmic probability of that data may become higher or lower, but the fluctuation will not be very large. For example, if the model has not been trained on either order of data, the log probabilities of \u201cAll of the following are examples of connective tissue EXCEPT A: ligaments B: muscle C: blood D: cartilage\" and \"All of the following are examples of connective tissue EXCEPT A: cartilage B: blood C: muscle D: ligaments\" will not differ much because of the lack of sequential relationship between the contents of the options. As shown in Figure 1, a data containing four options can be composed into 4!=24 different derived data after shuffling the contents of the options. Without knowing the order of the options in the pre-training data (the order of the shuffled options may be assumed during the benchmark construction process or the pre-training data construction process (Huang et al. 2024; Hou et al. 2024)), as in Fig. 1(b), if the 24 log probabilities are both high and low without a very large value of some kind, then there is no data leakage; if there is a significant outlier with the maximum of the log probabilities, as shown in Fig. (a), then there is a data leakage. With this detection method, artificial and intentional shuffling over the order of options can also be detected, if the option shuffling is not taken into account, only the logarithmic probability of the data in the original order is required to maximize the probability of data leakage can be determined.\nIn this work, we show how to provide reliable evidence for test set contamination in black-box language models. More specifically, we provide a simple and efficient new method for benchmark leakage detection based on multiple-choice questions. The method identifies the presence of a benchmark test set in a language model's pre-training data and the extent of data leakage without accessing the model's training data or weights. The contributions of this paper are summarized below:\n\u2022 We propose a simple yet effective detection method based on the characteristics of multiple choice questions by generating different derived datasets by disrupting the order of the options, and then using the model's logarithmic probability distribution to detect whether the original dataset is leaked or not.\n\u2022 The algorithms are able to work in black-box conditions without access to model training data or weights, effectively identifying data leakage from the benchmark test set in the model pre-training data, including normal scenarios and complex scenarios in which options may have been intentionally or unintentionally disrupted.\n\u2022 We validate the effectiveness of the approach based on two LLMs design experiments and evaluate the data leakage risk of 31 open-source LLMs on four mainstream benchmark sets, present a benchmark leakage leaderboard among LLMs, and in particular find that the Qwen family of LLMs shows a high risk in several benchmarks."}, {"title": "Related Work", "content": "Our work focuses on the problem of data leakage of LLMs on benchmark test sets. We therefore discuss the work most relevant to us from the perspectives of both mainstream large language model benchmarks and data leakage detection."}, {"title": "Mainstream Benchmarks for LLMS", "content": "As natural language processing enters the LLM era, a wide variety of LLMs (Team 2023; Touvron et al. 2023a,b; Young et al. 2024; BAAI 2023; Bai et al. 2023; Yang et al. 2023) have emerged. Various comprehensive or specialized benchmarks (Zhong et al. 2023; Zheng et al. 2024; Cobbe et al. 2021; Hendrycks et al. 2021b,a; Li et al. 2023; Huang et al. 2024; Wang et al. 2023) have also been proposed to accurately assess various aspects of the model's capabilities. In order to automate and efficiently test the capabilities of large language models, many mainstream benchmarks use a multiple-choice format. For example, MMLU (Hendrycks et al. 2021a) is a comprehensive and all-encompassing English benchmark, CMMLU (Li et al. 2023) and C-Eval (Huang et al. 2024) are comprehensive and all-encompassing Chinese benchmarks, and CMB (Wang et al. 2023) is a comprehensive and all-encompassing Chinese medical quiz assessment benchmark. In addition, multimodal comprehension benchmarks such as MMMU (Yue et al. 2024) and CMMMU (Zhang et al. 2024) are also in the form of multiple choice questions. This work focuses on the problem of benchmark test set leakage in the form of multiple-choice questions. Since the exchange of multiple-choice question option content does not affect the meaning of the question itself, we propose a simple and effective data leakage detection method based on this property."}, {"title": "Data Leakage Detection", "content": "The current pre-training model size and its pre-training corpus are getting larger and larger, which inevitably leads to data leakage between the pre-training corpus and various benchmark test sets. Several previous studies (Brown et al. 2020; Wei et al. 2021) have utilized post-hoc n-gram overlap analysis between the benchmark and pre-training corpus to measure data leakage. Deng et al. (2023) utilized benchmark perturbations and synthetic data to detect benchmark leakage. (Wei et al. 2023) compare the model's loss on the training, validation, and test sets; if the model's loss on the training set is significantly lower than on the validation or test sets, this may indicate that the model is overfitting the training data. If the loss on the test set is significantly lower than an independent reference set (consisting of data that the model has never seen), this may indicate that the test data was compromised during training. Mattern et al. (2023) tested the difference in perplexity between the target sequence and the randomized sequence. Oren et al. (2023) exchanged the order of problems in some benchmarks and tested the model with generating new data as a way to detect data leakage. Xu et al. (2024) introduced a measure of the predictive accuracy of the benchmark model using two simple and scalable metrics, complexity and n-gram accuracy, to identify potential data leakage. Our work leverages the interchangeability of options in benchmark test sets to achieve instance and fine-grained data leakage detection. In addition to common scenarios, we even consider data leakage identification in scenarios where options are intentionally or unintentionally shuffled."}, {"title": "Methodology", "content": "Our goal is to identify whether the pre-training process of a language model \\(\\theta\\) includes a particular piece of data \\(x\\) from a benchmark test set, or the extent to which that benchmark test set D leaks to the model \\(\\theta\\). Detection in our setup is under black-box conditions, i.e., the pre-training corpus and parameters of the model are unknown. We consider two scenarios: (a) where the order in which the pre-trained data options are presented is not shuffled, and (b) where the sequence of pre-trained data options may be shuffled."}, {"title": "Algorithm 1: Data Leakage Detection Under Scenario (a)", "content": "Input:\n\u2022 Data to be detected: \\(x = [q, o_1, o_2, ..., o_n]\\)\n\u2022 Target Model: \\(M\\)\nOutput: Whether the data was leaked (\"L\" for Leaked, \"NL\" for Not Leaked)\n1: Get the set of n! derived data X:\nShuffle(x) \u2192 X = {x1, x2, ..., xn!}\n2: for each derived data xi do\n3:\nCalculate the log probability of the derived data:\nlogpi = PM(seq[q, Shufflei(o1, o2, ..., on)])\n4: end for\n5: Get the set of n! log probabilities logpi:\nP = {logp1, logp2, ..., logpn!}\n6: if logp = max(P) then\n7:\nreturn \"L\"\n8: else\n9:\nreturn \"NL\"\n10: end if"}, {"title": "Algorithm 2: Data Leakage Detection Under Scenario (b)", "content": "Input:\n\u2022 Data to be detected: \\(x = [q, o_1, o_2, ..., o_n]\\)\n\u2022 Target Model: \\(M\\)\n\u2022 Outlier threshold: \u03b4\nOutput: Whether the data was leaked (\u201cL\u201d for Leaked, \u201cNL\u201d for Not Leaked)\n1: Get the set of n! derived data X:\nShuffle(x) \u2192 X = {X1, X2, ..., Xn!}\n2: for each derived data xi do\n3:\nCalculate the log probability of the derived data:\nlogpi = PM (seq[q, Shufflei(o1, o2, ..., on)])\n4: end for\n5: Get the set of n! log probabilities logpi:\nP = {logp1, logp2, ..., logpn!}\n6: Calculate the outlier score sout for each data:\nSout = {sout, sout, ..., sout} \u2190 IsolationForest(P)\n7: Obtain the maximum log probability logpm and corresponding outlier score sout:\nsout \u2190 logpm \u2190 max(P)\n8: if sout < \u03b4 then\n9: return \"L\"\n10: else\n11:\nreturn \"NL\"\n12: end if"}, {"title": "Scenario a: Not Shuffled", "content": "As illustrated in Algorithm 1, we present the pseudo-code for a data leakage detection method under the scenario where the options are not shuffled. We define a piece of data to be tested as \\(x = [q, o_1, o_2, ..., o_n]\\), where q is the question in a multiple-choice format, or is the i-th option, and n is the total number of options.\nAs depicted in Figure 2, subjecting the data x to an option shuffle operation yields a derived dataset X, expressed as Shuffle(x) \u2192 X = {x1, x2,..., xn!}. Here, Shuffle denotes the function for shuffling options, capable of generating n! distinct permutations, with n representing the number of options.\nWhen considering the possibility that the options within the data have not been artificially rearranged, x\u2081 is identified as the original data sequence. Subsequently, each xi \u2208 X is fed into the target model M to calculate the respective log probability, denoted by:\nlogpi = PM (seq[q, Shufflei(o1, o2, ..., on)])\nThese probabilities are then compiled into the set P = {logp1, logp2,...,logpn!}, where logp\u2081 corresponds to the original sequence x*."}, {"title": "Scenario b: Shuffled", "content": "The pseudo-code of the data leakage detection method under scenario b is presented in Algorithm 2. Under these conditions, the data in the test set can be shuffled through, and any kind of sequence order may be the order fitted by the model. As above, we first shuffle the data to be tested to get n! derived data: Shuffle(x) \u2192 X = {X1, X2, ..., Xn!}. Then, we process each derived data point xi. Specifically, we calculate the log probability of the derived data using the following formula:\nlogpi = PM(seq[q, Shufflei(o1, o2,..., on)])\nHere, PM represents the probability distribution under model M, seq denotes the sequence, q is the question, Shufflei is the i-th shuffle operation, and o1, o2, ..., on are the original data points. As depicted in Figure 2, we calculate this for all possible shuffle combinations, obtaining a set P of n! log probabilities:\nP = {logp1, logp2,...,logpn!}\nNext, we calculate the outlier score sout for each data point using an isolation forest algorithm:\nSout = {sout, sout,..., sout} \u2190 IsolationForest(P)\nSubsequently, we identify the maximum log probability logpm and its corresponding outlier score somut by taking the maximum value from the set P:\nsout\u2190logpm\u2190 max(P)"}, {"title": "Experiment", "content": "In this section we experimentally demonstrate the effectiveness of our proposed method for detecting data leakage."}, {"title": "Experimental settings", "content": "We randomly selected 1,000 pieces of data from MMLU, 500 of which were used for continuous pre-training of the LLaMA2-7b-base model, and then used these 1,000 pieces of data to test the pre-trained model, detecting which of these 1,000 pieces of data had been trained. Similarly we also used CMMLU data to test the Qwen2-7b-base model. Our experiments consider two scenarios where (a) the order of pre-trained data options is not shuffled and (b) the order of pre-trained data options may be shuffled."}, {"title": "Experimental results", "content": "The experimental results for scenario (a) are shown in Table 1. Under scenario (a), as long as the log probability of each of the other 23 variants of a piece of data is smaller than that of its original order, then we predict that there is a leak in this piece of data. For LLaMA2-7B, the detection accuracy and F1 exceeded 90% when the data were trained 10 times. We found that even if the data was only pre-trained once, our detection method was able to achieve an accuracy of 71%, which is a passing grade. In the early stage, the accuracy of our data leakage detection increases dramatically with each increase in the number of training sessions, e.g., the accuracy reaches 79% with an epoch of 2. For the Qwen2-7B model on the Chinese benchmark data CMMLU, the accuracy is only 60.3% when epoch is 1, however, when epoch is 5 the accuracy is already 96.6%. The experimental results in Table 1 show that under scenario (a), the detection accuracy of our data leakage can achieve good performance, even with very few data duplications.\nThe experimental results for scenario (b) are shown in Table 2. For the determination of outliers, we chose three thresholds of -0.2, 0.17, and -0.15. Since scenario b is very challenging, the detection accuracy of scenario b is quite lower than scenario a from the experimental results. The highest accuracy is achieved when outlier threshold \u03b4 = 0.2. When the data is trained 10 times, both accuracy and F1 on LLaMA2-7B exceed 0.8, and for Qwen2-7B even an accuracy of 84.8% and an F1 score of 0.857 are achieved. Even if the data is only pre-trained once, our detection method is able to achieve about 50% accuracy. From the experimental results we can choose a smaller outlier threshold when the number of training times is small. And the test results on the Chinese and English datasets are similar. However, overall the accuracy is higher on Qwen2-7B with CMMLU than on LLaMA2-7B with MMLU. We find that the recall is very low when the number of training iterations is small, and the recall improves very significantly when the number of training iterations is increased. Overall, our data leakage detection method achieves excellent accuracy in scenario a and passable results in the challenging scenario b."}, {"title": "Benchmark Leakage Leaderboard in LLMS", "content": "The previous experiments demonstrate the effectiveness of our Algorithm 1 and Algorithm 2, and next we will construct leaderboards for various benchmark leaks of LLMs. We conduct comprehensive data leakage detection experiments on four mainstream benchmarks: MMLU (Hendrycks et al. 2021a), CMMLU (Li et al. 2023), C-Eval (Huang et al. 2024), CMB (Wang et al. 2023). As shown in Figuu'rere 3, we tested almost all of the currently popular 31 LLMs (Team 2023; Touvron et al. 2023a,b; Young et al. 2024; BAAI 2023; Bai et al. 2023; Yang et al. 2023; Bi et al. 2024; Abdin et al. 2024), and we give the percentage predicted to be data leakage for both scenarios a and b. The outlier threshold \u03b4 for our scenario b is set to 0.2 on the three benchmark test sets, MMLU, CMMLU, and C-Eval; since there are five options for the data in the CMB benchmark, its outlier threshold \u03b4 is set to 0.25. And Ordered by the degree of leakage under scenario b. The Benchmark leakage leaderboard in Figure 3 is sorted by the degree of leakage under the scenario b. First of all, we find that there is not much gap between models on the MMLU benchmark, and the top five models in terms of data leakage risk are Qwen2-72B, Qwen1.5-110B, Yi-34B, Yi1.5-9B and Yi1.5-6B. Overall, the leakage of LLMs on the MMLU benchmark is a serious concern, and as MMLU is one of the most used and widely used benchmarks in the English language domain, the issue deserves our attention.\nOn the CMMLU benchmark, the leakage metrics shown on scenario a are all very low, basically only about 0.04, which is basically in line with the expectation of 1/24 = 0.042 for normal conditions. We then found that the data leakage metrics detected under scenario b were all significantly higher after detection using Algorithm 2, especially the Qwen family, which ranked the highest. We hypothesize that it is possible that the CMMLU benchmark shuffled the options after collecting the raw data or that the developers of LLM shuffled the pre-training data in a shuffling operation. On C-Eval, a Chinese comprehensive benchmark similar to CMMLU, the top five modeled data leakage risks are also all Qwen1.5-110B, Qwen2-72B, Qwen1.5-32B, Qwen1.5-14B and Qwen2-7B. On the Chinese Medicine Benchmark CMB, the top five LLMs in terms of data breach risk remain Qwen2-72B, Qwen1.5-110B, Qwen1.5-32B, Qwen1.5-14B and Qwen2-7B. In particular, the Qwen family of LLMs leads off the cliff, with Algorithm 1 scoring much higher than the other models. In terms of data leakage values, the Qwen family LLMs are almost ten times larger than other LLMs. Algorithm 1 detects that 42% of the test data of the CMB benchmark on Qwen2-72B is leaked.\nOverall, GLM4-9B has the lowest risk of data leakage on all three benchmarks, MMLU, CMMLU, and C-Eval, and a low risk of data leakage on CMB. Qwen family LLMs have very high leakage risk on all 4 benchmarks, and we find that the larger the model the higher the leakage index, which might be due to the fact that larger models have more pre-training data and are more capable of learning and remembering the data more firmly. In addition to the Qwen family LLMs, the Yi family LLMs, DeepSeek family LLMs, and Baichuan family LLMs are also at slight risk of benchmark compromise. Mild benchmark leaks are hard to avoid, but we hope that researchers should avoid serious benchmark leaks when developing LLMs."}, {"title": "Case Study", "content": "As shown in Fig. 4, we select three examples from C-Eval in order to analyze the data leakage under scenario a more intuitively. For example, in the first case the original data x is \"Lu You's \"Miscellaneous Fugue\u201d says: \u201cI am half-drunk in the grass market today, and I point out the green curtains and go up to the wine tower\". The emergence of the \"grass market\" in the poem is attributed to ____A: changes in the layout of cities B: the emergence of places of entertainment C: the development of the commodity economy D: the rise of the civic class\", and we shuffle the contents of the options to get 24 derived data X = {X1,X2, ..., Xn!}. We then compute all possible shuffling combinations based on Qwen2-7B and LLaMA2-7B, respectively, to obtain two sets of (n!) logarithmic probabilities PQwen = {logp1,logp2,...,logpn!} and PLLAMA = {logp1,logp2,..., logpn!}. A dot-line plot based on these two sets of log probabilities is shown in Figure 4. The logarithmic probability of the original sequential data x on the Qwen2-7B model is the largest, larger than the logarithmic probability of any of the other 23 sequences, which suggests that this data is at risk of leakage on Qwen2-7B. On the right LLaMA2-7B's are the normal plots, when the contents of the options are shuffled, some of the log probabilities become smaller and some larger, and original sequential data x is not the largest, which suggests that there is no data leakage from LLaMA under scenario a.\nA particular example of Qwen2-7B is shown in Figure 5, where the log probability of the original sequence x is not the largest, and the entry is detected by Algorithm 1 as not leaking under scenario a. However, our detection results using Algorithm 2 detects that this piece of data is a leakage risk because the 19th derivation sequence has the highest log probability and is judged to be an outlier. The question and option for that piece of data is \"Of the following four meteor showers, the one that will be most disturbed by moonlight on the day of its greatest magnitude in 2022 is A: Perseid meteor shower B: Geminid meteor shower C: \u03b7 Aquarii meteor shower D: Quadrantid meteor shower\", and theoretically for the LLM being tested, the content of the shuffled option should not have a maximum log probability of being a significant outlier. This case illustrates that our Algorithm 2 is also effective in detecting data leakage for the case where the option content is shuffled. Primarily the intent of the leaderboard is to promote a fairer assessment of the community's LLMs, not to expose a particular model."}, {"title": "Conclusion", "content": "This work has highlighted the severity of benchmark data leakage in Large Language Models (LLMs) and introduced an innovative detection method capable of identifying leakages under various scenarios, including when the order of multiple-choice options may have been shuffled. We validate the effectiveness of the approach based on two LLMs design experiments and evaluate the data leakage risk of 31 open-source LLMs on four mainstream benchmark sets, present a benchmark leakage leaderboard among LLMs, and in particular find that the Qwen family of LLMs shows a high risk in several benchmarks. This work emphasizes the need for developers and researchers to be vigilant in ensuring the integrity and fairness of LLM assessments. We call for continued community effort to address this issue, improve our detection techniques, and uphold the robustness of benchmark assessments in the field of artificial intelligence. This paper serves as a stepping stone towards establishing more reliable and trustworthy standards in the evaluation of LLMs and advancing the field of artificial intelligence with confidence and integrity. Currently our method is limited to detecting data in multiple choice format, in the future we will try to extend our method to other formats. In addition some multimodal benchmarks are also in the format of multiple choice questions, and in the future we will also attempt to detect benchmark leakage on large multimodal models."}]}