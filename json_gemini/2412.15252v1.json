{"title": "NER- ROBERTa: Fine-Tuning RoBERTa for Named Entity Recognition (NER) within low-resource languages", "authors": ["Abdulhady Abas Abdullah", "Srwa Hasan Abdulla", "Dalia Mohammad Toufiq", "Halgurd S. Maghdid", "Tarik A. Rashid", "Pakshan F. Farho", "Shadan Sh. Sabr", "Akar H. Taher", "Darya S. Hamad", "Hadi Veisi", "Aras T. Asaad"], "abstract": "Nowadays natural Language Processing (NLP) is an important tool for most people's daily life routines, ranging from understating speech, translation, name entity recognition (ENR), and text categorizing, to the generative text models such as ChatGPT. Due to existing big data and consequently big corpora for the most usable languages including English, Spanish, Turkish, Persia, and many more, these applications have been utilized accurately. However, the Kurdish language still needs more corpora and big datasets to be included in the NLP applications. This is because the Kurdish language has a rich linguistic structure, varied dialects, and a poor dataset, which poses unique challenges for Kurdish NLP (KNLP) application development. While several types of research have been conducted in KNLP for various applications, the Kurdish ENR (KNER) remained a challenge for most of the KNLP applications including text analysis and text classification. In this work, we address this limitation by proposing a methodology for fine-tuning the pre-trained RoBERTa model for KNER\u00b9. To this end, first, a Kurdish corpus is created, followed by designing a modified model architecture, and the training procedures are implemented. To evaluate the trained model, a set of experiments is conducted to show the performance of the KNER model using different tokenization methods and trained models. The experimental results showed that fine-tuned RoBERTa with sentence-piece tokenization method substantially improves KNER performance by 12.8% (F1-score improvement) in comparison to traditional models and consequently establishes a new benchmark for KNLP.", "sections": [{"title": "1. Introduction", "content": "Natural Language Processing (NLP) plays an important role in making technology applications more user-friendly and applicable. The NLP is like giving machines to generate, understand, and speak like a human being [1]. However, this ability differs from one language to another. For example, the English NLP (ENLP) is more accurate than the Kurdish NLP (KNLP), this is due to the availability of large and diverse English corpora/datasets [2]. While the Kurdish language includes a set of various dialectics that make the language more complex, such as Kurdish Northern (also called Kurmanji), Central-Kurdish (also called Sorani), and Kurdish southern [3]. Although the Kurdish language is spoken by dozens of millions of people in the globe, the Kurdish language faces a shortage of linguistic resources for NLP tasks. Furthermore, the KNLP lacks datasets that contain a large volume of diverse and non-standardized text as well as more investigation by researchers is needed, especially multidisciplinary research so that machine learning scientists work closely with Kurdish linguists.\n\nThe NER is the main task in NLP via identifying named entity in text into pre-defined categories including person names, medical terms, locations, organizations, and other specific terms [4]. The NER focuses on extracting meaningful information from text by recognizing the named entities to analyze and organize a large volume of un-structured text. For example, in a sentence like \u201cElon Mask was born in Pretoria, South Africa\u201d the NER model recognizes \u201cElon Mask\u201d as a person name, \u201cPretoria\u201d as a city name, and finally \u201cSouth Africa\u201d as a country name. To this end, such recognition is a vital process for applications such as search engines, data mining, and information retrieval.\n\nThere are many well-trained models to provide NER process including DeepSpacy [5], BERT [6], ROBERTa [6], and Flair NLP [7]. These models have been also utilized commercially such as in OpenAI \"GPT-3\", Facebook, and Google-Cloud-NLP [8]. Further, the capability of these models is based on the data sets that have been used during the training phase, i.e. either the text-dataset is monolingual or multilingual [9]. For example, RoBERTa is originally implemented in Facebook, and it is trained only using English text from 11,000 books, English Wikipedia, and Stories [10]. In the same vein, a new version of RoBERTa is fine-tuned in 2024 and it is re-trained using various text-datasets within 100 different languages using the CommonCrawl dataset [11]. However, the RoBERTa for NER purposes using Central-Kurdish dialectic text has low-capability and provides"}, {"title": "2. Related Work", "content": "The existing techniques starting from traditional machine learning techniques to the transformer- based architecture have significantly advanced the field of natural language processing (NLP) including NER. However, the text-datasets within various languages (in both monolingual and multilingual) have an important impact on the existing NER models. Therefore, fine-tuning models and training the tuned models via specific or multi-language text-dataset need further research. This section investigates most of the well-known and evaluated NER models to show the capabilities and limitations.\n\nThe NER models, characterized by their attention mechanisms and capacity to handle sequential dependencies are more effective than traditional models like the Long-Short-Term-Memory (LSTM) model since the new models have a new set benchmark for performance in NER tasks. For example, Bidirectional Encoder Representations from Transformers (BERT), introduced by Devlin et al. [12], marked a pivotal shift in NLP by introducing a pre-training method that allows a model to understand the context of a word by looking at both its left and right surroundings. In NER, BERT's bi-directional approach enables it to accurately predict the label of a token by considering all contextual cues, making it highly effective for entity recognition across various domains. Fine-tuning BERT on domain-specific NER datasets has led to significant performance improvements, achieving state-of-the-art results in many benchmark datasets like CoNLL-2003."}, {"title": "3. Methodology", "content": "The proposed approach started by collecting and creating Central-Kurdish dialectic corpus from different sources. The text-sources are online articles, news websites, and government publications to ensure a rich and varied representation. Note, the text size is around 1472 sentences. Thereafter, a set of preprocessing steps are applied such as: normalization, standardization, sentence segmentation, and tokenization. The overall proposed approach is demonstrated in Figure 1."}, {"title": "3.1 Preprocessing and Text Collection", "content": "Normalizing the Central-Kurdish dialectic text is an essential pretreatment step in text processing, particularly when working with informal text data. For example, normalization transforms the text into Unicode UTF-8 format. By utilizing this method, it can manage a multitude of character encodings, standardizing the text according to its protocols, and unifying characters. Specifically, certain terms that were written on an Arabic keyboard were converted to their Kurdish equivalents using the keyboard. For instance, the following substitution was made:\n[(\u062f\u0648\u0648 \u0645\u0644\u064a\u0648\u0646 \u0648 \u062d\u06d5\u0641\u062a \u0633\u0647 \u062f \u0647\u0647 \u0632\u0627\u0631 replaced by (\u062f\u0648\u0648\u0645\u0644\u064a\u0624\u0646 \u0648 \u062d\u0629\u0641\u062a \u0633\u0629\u062f \u0647\u0629\u0632\u0627\u0631)]\nThis is followed by standardizing the character encoding, inserting spaces around punctuation marks, numerals, dates, and URLs, and eliminating the zero punctuation marks, numerals, dates, and URLs. Further, eliminating the Zero Width Non-Joiner (ZWNJ), i.e. the corpus undergoes conversion to the ZWNJ. For instance, the following substitution was made:\n[ \u0644\u0647 \u0634\u0627\u0631\u06cc \u06a9\u0648\u06cc\u0647 \u0646\u0632\u06cc\u06a9 \u0666\u0667\u0660\u0660 \u0641\u0647 \u0631\u0645\u0627\u0646\u0628\u0647 \u0631 \u062a\u0648\u0645\u0627\u0631\u06a9\u0631\u0627\u0648\u0647) replaced by ( \u0641\u0647 \u0631\u0645\u0627\u0646\u0628\u0647 \u0631 \u062a\u0648\u0645\u0627\u0631\u0643\u0631\u0627\u0648\u0647 6700 \u0644\u0647 \u0634\u0627\u0631\u06cc \u06a9\u0648\u06cc\u0647 \u0646\u0632\u06cc\u06a9 )]\nNote, the AsoSoft\u00b2 tool [17] is used to standardize and normalize text in the Sorani dialect for consistency."}, {"title": "3.2 Text Annotation", "content": "Now, the list of tags is defined accordingly as well as the tokenized text is ready to be annotated. A professional staff of Central-Kurdish dialectic language persons are engaged to do the annotating tokens, manually via professors from the both University of Kurdistan Hewler, and"}, {"title": "3.3 Model Architecture", "content": "Creating learning models or re-training pre-defined models for low resource languages (i.e. small text corpus) incurs huge computations and faces new challenges. To this end, this study proposed a modified/tuned RoBERTa model for the Kurish-Sorani dialectic. Furthermore, this article retrained the original RoBERTa architecture, only adjusting the final layer for the multi-class Kurdish-NER task. The model's pre-trained weights were fine-tuned with a linear classification layer tailored to recognize the various named entities, as they are listed in table 2.\n\nOriginally, RoBERTa is an optimized version of Bidirectional Encoder Representations from Transformers (BERT) [18]. Furthermore, the standard layers of RoBERTa are multi-head attention, add & normalize, Feed-Forward Network (FFN), and again add & normalize layers. The original layers of the RoBERTa architecture are demonstrated in figure 3."}, {"title": "", "content": "Specifically for a single transformer layer in RoBERTa, the multi-head attention is described as $H_{attn}$, as the model output should be calculated, and it is expressed in equation (1):\n\n$H_{attn} = MultiHeadAttention(Q, K, V)$ (1)\n\nWhere the Q, K, and V are the query, Key and value metrics that derived from input X, the $H_{attn}$ is the output of the multi-head self-attention and it contains the context-aware representations of the input tokens after attention. Further, the Q represents the current token that should be compare with other tokens in the sentence, the K holds the information about all the token to compare against, finally the V carries the information that to be aggregated via the attention scores.\n\nThereafter, add and normalize layer is applied as follows:\n\n$H_{add1} = LayerNorm(X + H_{attn})$ (2)\n\nWhere $H_{add1}$ is the combination of X input (the token embeddings or previous layer output) with the $H_{attn}$, this is a residual connection that helps with gradient flow during training. The $LayerNorm$ denotes the normalization layer which ensuring that the output maintains a stable distribution for subsequent processing.\n\nConsequently, the normalized embeddings through the Feed-Forward Network should be passed, as expressed in equation (3):\n\n$H_{ffn} = FFN(H_{add1})$ (3)"}, {"title": "", "content": "Where the $H_{ffn}$ is refining the normalized embeddings by expands the dimensionality of $H_{add1}$ and reduces it back to the original dimension. This is applied via two linear transformations with a non-linear activation (typically ReLU or GELU).\n\nFinally, Add and normalize layer is again applied to residual connections and normalize to produce the final output, as expressed in equation (4):\n\n$H = LayerNorm(H_{add1} + H_{ffn})$ (4)\n\nWhere the H is the combination of $H_{add1}$ (i.e. output from the first Add & Normalize) with $H_{ffn}$ (i.e. output from the FFN). This is another residual connection. The $LayerNorm$ is to applying Layer Normalization to stabilize the output.\n\nEach of these layers contributes to the model's ability to capture relationships and dependencies between words, making RoBERTa a powerful model for natural language understanding tasks.\n\nHowever, the layers within RoBERTa model need further fin-tuning process, specifically for low- resources languages, specific domain (e.g. medical term), and extending for multilingual tasks. Further, when fine-tuning large pre-trained language models like RoBERTa, certain challenges arise including 1) efficiency: fine-tuning all parameters can be computationally expensive and memory intensive, 2) overfitting: for smaller downstream datasets, training all model parameters might lead to overfitting, and 3) task-specific adaptation: while frozen models work for general purposes, downstream tasks may require task-specific feature extraction. To do so, in this article, the modified or fine-tuned model of RoBARTa is proposed for Kurdish NER, as shown in figure 4.\n\nThe modified RoBERTa model is the modification of the standard transformer layer with zero- initialized attention. This modification is about an additional attention procedure into the standard transformer layer. First, the zero-initialized attention (it is also known Adapter) adds a new attention procedure which is initialized with zero weights, allowing the layer to adapt without affecting the original model's performance, as expressed in equation (5).\n\n$H_{adapter} = ZeroInitAttention(Q.K.V)$ (5)\n\nWhere $H_{adapter}$ is a learnable attention output that starts as zero which effectively allows the model to slowly learn an additional layer of attention while keeping the original multi-head attention accordingly. In addition to that, the main advantage of zero-initialized attention is to 1) integrating zero weight-values into the original multi-head self-attention so that it doesn't affect the model's output and only the parameters of the adapter are trained, allowing task-specific representations to emerge,\n\nThereafter, the modified model combines the outputs ($H_{combined}$) of the original multi-head attention ($H_{attn}$) and the new zero-initialized attention ($H_{adapter}$). This allows the new attention procedure to enhance the representation over time while preserving the standard transformer's initial behavior, as expressed in equation (6):\n\n$H_{combined} = H_{attn} + H_{adapter}$ (6)"}, {"title": "", "content": "The layer of add & normalize is applied on the combined output (LayerNorm) to: 1) keep the residual connection of the input X and 2) ensure the stability and smooth gradient flow via the normalization (LayerNorm), as expressed in equation (7):\n\n$H_{add1} = LayerNorm(X + LayerNorm)$ (7)\n\nThis is followed by applying the two-layer feed-forward (FFN), same as standard transformer layer, as express in equation (8):\n\n$H_{ffn} = FFN(H_{add1})$ (8)\n\nIn the last stage, both $H_{add1}$ and $H_{ffn}$ are combined via applying add & normalize layer for additional residual connection and to produce the final output by the normalization process, as expressed in equation (9):\n\n$H = LayerNorm(H_{add1} + H_{ffn})$ (9)\n\nThe modified model is also fine-tuned with zero-initialized attention, and hence the fine-tuned introduces a focused training approach. To this end, in this study the original multi-head attention weights ($H_{attn}$) and Feed-Forward Network (FFN) weights are kept frozen during fine-tuning. This helps the modified model to provide pure model features. This means that only the ($H_{adapter}$) is trained and would be updated during fine-tuning procedure. Further, such a mechanism allows to learn the model to a new task by utilizing only the weights of the adapter, while preserving the general knowledge stored in the pre-trained parameters. However, for the initial state, the zero- initialized attention ignores the ($H_{adapter}$), thus the $H_{combined}$ contains only the $H_{attn}$, as it is expressed in equation (10):\n\n$H_{adapter} = 0$\n\n$H_{combined} = H_{attn}$ (10)"}, {"title": "", "content": "The final stage of fine-tuned output, after the training procedure, the adapter learns as a task- specific attention, thus the final layer output will be expressed in equation (11):\n\n$H_{fine-tuned} = LayerNorm(H_{atten} + H_{adapter}^{fine-tuned} + H_{ffn})$ (11)\n\nWhere the $H_{adapter}^{fine-tuned} \u2260 0$"}, {"title": "4. Experiments and Evaluation Results", "content": "This section evaluates the performance of the proposed Kurdish Named Entity Recognition (KNER) model and compares it with baseline methods using different tokenization strategies such as Byte Pair Encoding (BPE), word-level, and sentence-piece. Basically, the model is fine-tuned using training data, updating hyperparameters, optimizer, and loss function. During the training model, the annotated Kurdish corpus or the dataset is divided into 70% for training subset, 15% for validation subset, and 15% for test subset. The hyperparameters of the model are configured"}, {"title": "5. Conclusion", "content": "The comparative performance analysis reported in this work demonstrates that the proposed RoBERTa model, fine-tuned with sentence-piece tokenization, establishes a new benchmark for Kurdish NER tasks. The results reinforce the significance of advanced transformer architectures, effective tokenization strategies, and task-specific fine-tuning in improving low-resource language processing. Future work will focus on expanding datasets and incorporating additional dialects to further enhance model robustness. This study also demonstrates that with strategic data collection and model adjustments, transformer models can be adapted for low-resourced languages, paving the way for further NLP advancements in these domains. The promised solution of zero-initialized attention is due to task adaptability without re-training the features of the RoBERTa, requires fine- tuning only a small subset of parameters (adapter weights), reducing computational overhead, maintaining robustness, and providing better performance. However, the limited high quality of annotated data, the variety of Kurdish language dialectics, and complex morphology of Kurish words are still the main challenges for the future studies that needs to be addressed."}]}