{"title": "Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos", "authors": ["Hanxue Liang", "Jiawei Ren", "Ashkan Mirzaei", "Antonio Torralba", "Ziwei Liu", "Igor Gilitschenski", "Sanja Fidler", "Cengiz Oztireli", "Huan Ling", "Zan Gojcic", "Jiahui Huang"], "abstract": "Recent advancements in static feed-forward scene reconstruction have demonstrated significant progress in high-quality novel view synthesis. However, these models often struggle with generalizability across diverse environments and fail to effectively handle dynamic content. We present BTimer (short for BulletTimer), the first motion-aware feed-forward model for real-time reconstruction and novel view synthesis of dynamic scenes. Our approach reconstructs the full scene in a 3D Gaussian Splatting representation at a given target ('bullet') timestamp by aggregating information from all the context frames. Such a formulation allows BTimer to gain scalability and generalization by leveraging both static and dynamic scene datasets. Given a casual monocular dynamic video, BTimer reconstructs a bullet-time scene within 150ms while reaching state-of-the-art performance on both static and dynamic scene datasets,\n*/\u2020: Equal contribution/advising.\nIn this paper, we define bullet-time as the instantiation of a 3D scene frozen at a given/fixed timestamp t.", "sections": [{"title": "1. Introduction", "content": "Multi-view reconstruction and novel-view synthesis are long-standing challenges in computer vision, with numerous applications ranging from AR/VR to simulation and content creation. While significant progress has been made in reconstructing static scenes, dynamic scene reconstruc-"}, {"title": "2. Related Work", "content": "Dynamic 3D Representations. Depending on the tasks at hand, typical choices of 3D representations include voxels, implicit fields/NeRFs, and point clouds/3D Gaussians. Representing dynamics on top has an even larger design space: One existing line of works directly builds a '4D' representation to enable feature queries at arbitrary positions and timestamps from an implicit field or via marginalization at a given step, with the extensibility to higher dimensions such as material. Another line of work first defines a canonical 3D space, and learns a deformation field to warp the canonical space to the target frame. Such a deformation field can be parametrized via embedding graphs, control points, motion scaffolds, rigid transformation modes or bounding boxes explicit scene flows or implicit flow fields While these methods learn additional information about shape correspondences, their performance heavily relies on the quality and topology of the canonical space.\nNovel View Synthesis. For tasks that require a relatively smaller view extrapolation, the problem of novel view synthesis can be tackled without explicit 3D geometry in the loop, using depth warping or multi-plane images Otherwise, the study of novel view synthesis of dynamic scenes is mainly on (1) effectively optimizing the 3D representation through input images through monocular cues or geometry regularizations, and (2) being able to render fast with grids, local-planes or dynamic 3D Gaussians formulation. Our method aims to provide a dynamic representation that is fast to build within hundreds of milliseconds while reaching competitive rendering quality as the above optimization-based methods.\nFeed-forward Reconstruction Models. In many applications where the reconstruction speed is crucial, most optimization-based reconstruction methods become less preferable. To this end, one line of methods uses data priors to serve as an initialization or guidance, but they still need few-shot optimization to refine the results Another type of work that starts to emerge is fully feed-forward models that directly regress from 2D images to 3D, represented as either triplanes, 3D Gaussians, sparse voxels, or latent tokens."}, {"title": "3. Method", "content": "Overview. Given a monocular video (image sequence) represented by  $I = \\{I_i \\in R^{H \\times W \\times 3}\\}_{i=1}^{N}$, with N frames of width W and height H, along with known camera poses $P = \\{P_i \\in SE(3)\\}_{i=1}^{N}$, intrinsics, and corresponding timestamps $T = \\{t_i \\in R\\}_{i=1}^{N}$, our goal is to build a feed-forward model capable of rendering high-quality novel views at arbitrary timestamps $t \\in [t_1,t_v]$.\nThe core of our approach is a transformer-based bullet-time reconstruction model, named BTimer, that takes in a subset of frames $I_C \\subset I$ (denoted as context frames) along with their corresponding poses $P_c \\subset P$ and timestamps $T_C \\subset T$, and outputs a complete 3DGS scene frozen at a specified bullet timestamp $t_b \\in [min T, max T]$ (\u00a7 3.1). Iterating over all $t_b \\in T$ results in a full video reconstruction represented by a sequence of 3DGS. We further introduce a Novel Time Enhancer (NTE) module that synthesizes interpolated frames with timestamps $t \\notin T$ (\u00a7 3.2). The output of the NTE module is used along with other context views as input to the bullet-time model to enhance reconstruction at arbitrary intermediate timestamps. To effectively train our model, we carefully design a learning curriculum (\u00a7 3.3) that incorporates a large mixture of datasets containing both static and dynamic scenes, to enhance motion awareness and temporal consistency of our models."}, {"title": "3.1. BTimer Reconstruction Model", "content": "Model Design. Inspired by [79], our BTimer model uses a ViT-based [15] network as its backbone, consisting of 24 self-attention blocks with LayerNorms [4] applied at both the beginning and the end of the model. We divide each input context frame $I_i \\in I_c$ into 8 \u00d7 8 patches, which are projected into feature space $\\{f_{ij}^{rgb}\\}_{j=1}^{HW/64}$ using a a linear embedding layer. The camera Pl\u00fccker embeddings [70] derived from the camera poses $P_i \\in P_c$ and the time embeddings (detailed later) are processed similarly to form the camera pose features $\\{f^{pose}_{ij}\\}$ and the time features $\\{f^{time}\\}_{ij}\\}$ (shared for all patches j). These features are added together to form the input tokens for the patches of the context frame $\\{f_{ij}\\}_{j=1}^{HW/64}$, where $f_{ij} = f_{ij}^{rgb} + f_{ij}^{pose} + f_{ij}^{time}$. The input tokens from all context frames are concatenated and fed into the Transformer blocks.\nEach corresponding output token $f_{ij}^{out}$ is decoded into 3DGS parameters $G_{ij} \\in R^{8 \\times 8 \\times 12}$ using a single linear layer. Each 3D Gaussian is paramaterized by its RGB color $c \\in R^3$, scale $s \\in R^3$, rotation represented as unit quaternion $q \\in R^4$, opacity $\\sigma \\in R$, and ray distance $\\tau \\in R$, resulting in 12 paramaters per Gaussian. The 3D position of each Gaussian $\\mu \\in R^3$ is obtained through pixel-aligned unprojection as $\\mu = o+rd$, where $o \\in R^3$ and $d \\in R^3$ are the ray origin and direction obtained from $P_i$.\nTime Embeddings. The aforementioned input time feature $f_{ij}^{time}$ is obtained from: (i) context timestamp $t_i$ that is separate for each context frame $I_i$, and (ii) bullet timestamp $t_b$ that is shared across all context frames i. Both timestamp scalars are encoded using standard Positional Encoding (PE) [61] with sinusoidal functions, and then passed through two linear layers to obtain the features $f^{ctx}$ and $f^{bullet}$ respectively. Finally, we set $f^{time} = f^{ctx} + f^{bullet}$.\nSupervision Loss. Our model is supervised only by losses"}, {"title": "4.2.2 Qualitative Analysis", "content": "To assess the performance of our method in real-world scenarios, we select multiple monocular videos from the DAVIS dataset [49] for testing. Camera poses for the videos were estimated using the same annotation technique as detailed in \u00a7 3.3. Fig. 5 shows a visualization of the results. Our model demonstrates strong generalization capabilities in real-world captures, producing high-quality, sharp renderings across a variety of objects with complex motions while maintaining robust temporal and multiview consistency. The depth map also demonstrates the correct geometry that we can recover, enabled by the bullet-time reconstruction formulation that allows input frames from large baselines."}, {"title": "3.2. Novel Time Enhancer (NTE) Module", "content": "While our BTimer model can already reconstruct the 3DGS representation for all observed timestamps, we notice that forcing it to reconstruct at a novel intermediate timestamp, i.e. performing interpolation at $t_b \\notin T$, leads to suboptimal results. In such cases, the exact bullet-time frame cannot be included in the context frames as it does not exist. Our model specifically fails to predict a smooth transition between adjacent video frames when the motion is complex and fast. This is mainly caused by the inductive bias of pixel-aligned 3D Gaussian prediction. To mitigate this issue, we propose a 3D-free Novel Time Enhancer (NTE) module that directly outputs images at given timestamps, which are then used as input to our BTimer model, as illustrated in Fig. 4."}, {"title": "NTE module Design.", "content": "The design of this module is largely inspired by the very recent decoder-only LVSM [27] model. Specifically, NTE copies the same ViT architecture from the BTimer model, but the time features of input context tokens only encode their corresponding context timestamps (i.e. we set $f^{time} = f^{ctx}$). Additionally, we concatenate extra target tokens to the input tokens, which encode the target timestamp and the target pose for which we want to generate the RGB image. Following [27], we use QK-norm to stabilize training. Implementation-wise we apply an attention mask that masks all the attention to the target tokens, so KV-Cache (cf. [50]) can be used for faster inference. From the output of the Transformer backbone, we only retain the target tokens, which we then unpatchify and project to RGB values at the original image resolution using a single linear layer. The interpolation model is trained with the same objective as the main BTimer model (see \u00a7 3.1), but the output image is directly decoded from the network and not rendered from a 3DGS representation."}, {"title": "Integration with BTimer.", "content": "While the NTE module can be used on its own to generate novel views, we empirically find the novel-view-synthesis quality to be inferior (\u00a7 4.4). We hence propose to integrate it with our main BTimer model. To reconstruct a bullet-time 3DGS at $t_b \\notin T$, we first use NTE to synthesize $I_b$ at the timestamp $t_b$, where the target pose $P_b$ is linearly interpolated from the nearby context poses in P, and the context frames are chosen as the nearest frames to $t_b$. To accelerate the inference of the interpolation model, we use the KV-Cache strategy. In practice we observe that the interpolation model adds negligible overhead to the overall runtime."}, {"title": "3.3. Curriculum Training at Scale", "content": "One important lesson people have learned from training deep neural networks is to scale up the training [1, 56], and the model's generalizability is largely determined by the data diversity. Since our bullet-time reconstruction formulation naturally supports both static (by equalizing all elements in T) and dynamic scenes, and requires only RGB loss for weak supervision, we unlock the potential of leveraging the availability of numerous static datasets to pretrain our model. We hence aim to train a kitchen-sink reconstruction model that is not specific to any dataset, making it generalizable to both static and dynamic scenes, and capable of handling objects as well as both indoor and outdoor scenes. This is in contrast to, e.g., GS-LRM [79] or MVSplat [10] where one needs different models in different domains.\nNotably, we apply the following training curriculum to BTimer and the NTE module separately, but during inference they are used jointly as explained in \u00a7 3.2.\nStage 1: Low-res to High-res Static Pretraining. To obtain a more generalizable 3D prior as initialization, we first pretrain the model with a mixture of static datasets."}, {"title": "4. Experiments", "content": "In this section we first introduce necessary implementation details in \u00a7 4.1. We evaluate the performance of BTimer extensively on available dynamic scene benchmarks \u00a7 4.2, and demonstrate its backward compatibility with static scenes \u00a7 4.3. Ablation studies are found in \u00a7 4.4."}, {"title": "4.1. Implementation Details", "content": "Training. Our backbone Transformer network is implemented efficiently with FlashAttention-3 [13] and FlexAt-"}, {"title": "4.2. Dynamic Novel View Synthesis", "content": "4.2.1 Quantitative Analysis\nWe provide quantitative evaluations on two of the largest dynamic view synthesis benchmarks available. DyCheck Benchmark [22]. The benchmark includes a DyCheck iPhone dataset that contains 7 dynamic scenes recorded by 3 synchronized cameras. Following the protocol in [22], we take images from the iPhone camera as our context frames and use the frames from the 2 other stationary cameras (totaling 3928 images) for evaluation. Our baselines include per-scene optimization-based methods, i.e., TiNeuVox [19], NSFF [36], T-NeRF [22], Nerfies [47] and HyperNeRF [48]. To the best of our knowledge there is no published method that can be applied without per-scene optimization. We hence compare to direct depth warping and"}, {"title": "4.3. Compatibility with Static Scenes", "content": "Although our model is primarily designed to handle dynamic scenes, the formulation and the training strategy enable it to be still backward compatible with static scenes. In this section, we show that the same model achieves competitive results on static scenes.\nRealEstate10K (RE10K) Benchmark [83]. We evaluate our model on the RE10K dataset and compare with several state-of-the-art models [7, 10, 55, 79]. To ensure comparability with baseline models, we train and test our model using 256 \u00d7 256 resolution. Tab. 3a presents a quantitative comparison on LPIPS, where our static model outperforms all the baselines. Please refer to the Supplement for more comparisons on other metrics and visualizations.\nTanks & Temples Benchmark [18]. We further evaluate our model on an unseen test dataset, the Tanks & Temples [31] subset from the InstantSplat [18] benchmark, which consists of 10 scenes. We use the state-of-the-art"}, {"title": "4.4. Ablation Study", "content": "Effect of Context Frames. We visualize the reconstruction results as we progressively add 3DGS predictions from more context frames across multiple different timestamps in Fig. 9, where increasing the number of context frame leads to progressively more complete scene reconstruction. This demonstrates the flexibility of our bullet-time reconstruction formulation: during the inference stage, we can arbitrarily select spatially-distant frames that contribute to a more complete view coverage of the scene.\nEffect of Curriculum Training. We show in Fig. 8 the effect of our curriculum training strategy. Without Stage 1 of pre-training on 3D static scenes, the model struggles to produce results of correct geometry and sharp details. Pre-training on multiple diverse datasets is also crucial, which we demonstrate by just training on RE10K dataset, and non-negligible distortions are observed in the results. Similarly, even in Stage 2 of our curriculum, we still need to co-train on static scenes which provide more multi-view supervisions, thus maintaining the rich details and reasonable geometries.\nEffect of Interpolation Supervision. Shown in Fig. 8, the effect of interpolation supervision (introduced in \u00a7 3.1) is significant, without which the model tends to produce white-edge artifacts. This occurs because in the absence of interpolation loss, the model often generates 3DGS that are positioned too close to the camera with low depth values to cheat the loss. In contrast, adding the interpolation supervision requires the model to account for scene dynamics and encourages consistency across multiple views.\nEffect of NTE. As demonstrated in Fig. 10, our NTE module enhances the bullet-time reconstruction model's ability to handle scenes with fast or complex motions, largely reducing the ghosting artifacts. Additional video results are"}, {"title": "5. Conclusion", "content": "Limitations. Although our method provides competitive novel view synthesis results, the recovered geometry (hence the depth map) is usually not as accurate as the most recent depth prediction models (e.g. [71]). Our model also has limited support for view extrapolation. Incorporating a generative prior in the loop is a promising direction to pursue in the future.\nIn this paper we present BTimer, the first feed-forward dynamic 3D scene reconstruction model for novel view synthesis. We present a bullet-time formulation that allows us to train the model in a more flexible and scalable way. We demonstrate through extensive experiments that our model is able to provide high-quality results at arbitrary novel views and timestamps, outperforming the baselines in terms"}]}