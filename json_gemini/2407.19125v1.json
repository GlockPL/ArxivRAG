{"title": "Binary Bleed: Fast Distributed and Parallel Method for Automatic Model Selection", "authors": ["Ryan Barron", "Maksim E. Eren", "Manish Bhattarai", "Ismael Boureima", "Cynthia Matuszek", "Boian S. Alexandrov"], "abstract": "In several Machine Learning (ML) clustering and dimensionality reduction approaches, such as non-negative matrix factorization (NMF), RESCAL, and K-Means clustering, users must select a hyper-parameter k to define the number of clusters or components that yield an ideal separation of samples or clean clusters. This selection, while difficult, is crucial to avoid overfitting or underfitting the data. Several ML applications use scoring methods (e.g., Silhouette and Davies Boulding scores) to evaluate the cluster pattern stability for a specific k. The score is calculated for different trials over a range of k, and the ideal k is heuristically selected as the value before the model starts overfitting, indicated by a drop or increase in the score resembling an elbow curve plot. While the grid-search method can be used to accurately find a good k value, visiting a range of k can become time-consuming and computationally resource-intensive. In this paper, we introduce the Binary Bleed method based on binary search, which significantly reduces the k search space for these grid-search ML algorithms by truncating the target k values from the search space using a heuristic with thresholding over the scores. Binary Bleed is designed to work with single-node serial, single-node multi-processing, and distributed computing resources. In our experiments, we demonstrate the reduced search space gain over a naive sequential search of the ideal k and the accuracy of the Binary Bleed in identifying the correct k for NMFk, K-Means pyDNMFk, and pyDRESCALk with Silhouette and Davies Boulding scores. We make our implementation of Binary Bleed for the NMF algorithm available on GitHub.", "sections": [{"title": "I. INTRODUCTION", "content": "In the machine learning (ML) landscape, clustering, and dimensionality reduction are critical for revealing underlying data patterns, simplifying complex data, and improving predictive models' performance. Methods such as non-negative matrix factorization (NMF), K-means, pyDNMFk, and py-DRESCALk clustering are popular because they effectively handle large data. However, these techniques depend heavily on an appropriate number of clusters or components, denoted by the hyper-parameter k. This parameter setting is essential as it influences the ability of the model to capture the intrinsic data structure without overfitting or underfitting accurately.\nOne approach to determine the NMF's optimal k is automatic model determination (NMFk), a heuristic algorithm based on silhouette scores to identify a k to yield stable patterns in W and clusters in H [1]\u2013[3]. Here, k is the highest W and H silhouette scores above the given thresholds tw and th. In NMFk, this selection of k is done by a grid search over a k search space k \u2208 K = {1, 2, ..., K} defined by the user, where a sudden drop in the silhouette scores after the ideal k value indicates over-fitting. Such selection of the k hyper-parameter is common in other ML approaches, such as K-means Clustering or RESCAL clustering [4]. Various distributed versions of unsupervised techniques have been specifically developed to manage large-scale datasets effectively, as detailed in the literature [4]\u2013[8]. While these advancements represent significant progress, the implementations' time complexity remains a critical issue. This complexity is primarily influenced by the parameter k due to the requirement to evaluate cluster stability across a range of k values. Consequently, there is a pressing need to develop an improved selection algorithm that can effectively narrow down the search space for k, thereby optimizing the computational efficiency of these techniques.\nThis work introduces a method based on binary search, Binary Bleed, which heuristically prunes the search space for the hyper-parameter k. Instead of visiting each k sequentially, Binary Bleed performs a binary search over the given k search space. It operates under the assumption that the clustering method is used with a scoring metric that increases with k for all sub-optimal k values while remaining low for k > koptimal due to over/under-fitting. This assumption has been valid for most clustering and dimensionality reduction algorithms, including NMFk and RESCALk when paired with silhouette scoring and K-means when paired with Davies Boulding scoring. Practically, in NMFk, this translates into silhouette scores approximating the shape of a square wave curve. While the working assumption has been tested and validated against various datasets, including text data for topic modeling, cyber-security data for malware analysis and anomaly detection [2], [9]\u2013[11], the authors acknowledge the potential it may not apply to other datasets.\nBased on the assumptions discussed above, Binary Bleed performs binary search over the k search space K, given user-provided threshold t, and prunes the lower k from K on identifying a score s >= t for maximization tasks and s <= t for minimization tasks. In the best-case scenarios, Binary Bleed"}, {"title": "II. RELEVANT WORK", "content": "This section briefly reviews several machine learning dimensionality reduction and clustering algorithms that require hyper-parameter selection for k. Binary Bleed uses binary search to optimize k, so we review related binary search studies. Finally, Binary Bleed optimizes the hyper-parameter k over a search range, making an automatic model determination, or k selection, another foundation. Other works optimized their respective algorithms over parallel and distributed contexts for binary search and automatic model determination.\nApplications of k Search: Optimizing for k has utility in several domains, particularly clustering, where optimal cluster count search computations can be expensive. Typically, k is a user-provided parameter that must be refined through trial and error. Algorithms that require user-specified or trial-discovered k include K-means Clustering [12], K-medoids Clustering [13], K-medians Clustering [14], Fuzzy C-means Clustering [15], Mini-Batch K-means [16], Spherical K-means [17], Elkan K-means [18], NMF Clustering [19], Symmetric NMF Clustering [20], and RESCAL Clustering [21].\nEach algorithm can use the silhouette score to determine the ideal k heuristically. In our experiments, detailed in the results section, we analyze Binary Bleed's performance when operating with the silhouette score and Davies Boulding to test NMFk, RESCAL, and K-means algorithms.\nBinary Search: Several aspects of binary search are relevant to this work, including optimizing search, modifying binary search, and parallelizing binary search. Noisy Binary Search [22] minimizes the number of comparisons to find an optimal coin in a sequence of coin flips by using biased coin flips to compare elements in a sorted sequence indirectly. It identifies the position where the probability of observing heads changes from below to above a given target threshold. Flip positions are determined from previous outcomes, focusing the flips on areas with the highest uncertainty about crossing the target probability. This approach is similar to Binary Bleed, which narrows searches in the search space using a thresholding mechanism combined with an observed score. Differently, while Noisy Binary Search selects the next target based on the score of the current item, Binary Bleed is designed for methods like NMF, where silhouette scores obtained from different k selections are independent of each other. Consequently, the silhouette score cannot decide the next k to visit. Instead, for the k search space pruning heuristic, Binary Bleed follows the hypothesis that the silhouette score will remain low after the ideal k due to the overfitting phenomenon. Once a stable k is found, all the lower k are ignored, and higher or lower k values require visitation until overfitting is observed.\nAnother work, [23], sought to optimize the binary search algorithm by modifying it to check the bounds of a stack and adjust these bounds, thereby increasing or narrowing the current search space to make the search process more efficient. This was required to measure the resting position of human eyes, which changes. Similar to [23], a modified binary search was described by [24], which checks at both ends of a sub-array and the middle to reduce the number of iterations through many sub-arrays to find the search value. Both [23] and [24] reduce needed checks in binary search, similar to this work. However, this work precomputes the search space and iteratively prunes values. Binary Bleed does not terminate upon finding an optimal candidate. Instead, it adjusts the search space and continues exploring the parameter direction to optimize the results further.\nSome works have achieved parallelization of the binary search with specific constraints. One work, [25], partitions the search array into smaller sub-arrays, searching each in parallel, which is similar to [24] but operates all arrays concurrently, and so the similarities to this work are the same with the addition of parallelizing the problem. Binary Bleed may operate parallel jobs across single or multiple compute nodes, each handling a portion of the k search space. Another work, Parallel Binary Search in [26] defines two arrays, A and B, where |A| < |B|. Parallelization distributes A's elements across resources to find the smallest element in B greater than or equal to each element in A. While this approach to parallelization is similar to parallelizing the k range in our work, the objectives differ. Binary Bleed aims to find the largest or smallest optimal value in the k space rather than minimizing array indices.\nImportantly, distributed and parallel binary searches are often used interchangeably but address different challenges. In a parallel search, data is small enough to fit in memory, so evaluations of different k values are concurrent on different processors or nodes. In contrast, distributed search requires multiple processors or nodes to handle single k evaluations for data too large for memory. In our context, comparing the search term to the current term n in binary search is analogous to model computation at k = n. This computation at k = n may exceed memory capacity for large datasets, necessitating a distributed solution where the data for a single calculation is divided among multiple resources. We demonstrate this using Binary Bleed to minimize the k search space with distributed NMF from our prior work [4], [27]. This approach allows parallel evaluations to concurrently minimize k, while distributed evaluations manage the computation for each k in large datasets, ensuring efficient utilization of resources."}, {"title": "III. ALGORITHM", "content": "Estimating the optimal number of clusters k is crucial for effective unsupervised machine learning methods for latent feature extraction and clustering. Traditional methods use an exhaustive linear search, resulting in high computational costs, scaling with O(n), where n is |K|. Binary search offers a more efficient alternative with a worst-case time complexity of O(log(n)), but it usually finds only exact matches. The Binary Bleed algorithm adapts binary search to optimize clustering performance metrics, like silhouette scores. The new methods introduced are Binary Bleed Vanilla (Vanilla) and Binary Bleed Early Stop (Early Stop), while models not using these methods are referred to as Standard."}, {"title": "A. Binary Bleed", "content": "Our method is named Binary Bleed due to its method of pruning less optimal k values, extends traditional binary search techniques by continuing to search even after a potential optimal k is found, ensuring the maximization or minimization of the scoring function f: K \u00d7 D \u2192 R, where K is the set of candidate parameters and D represents the dataset.\nBinary Bleed operates recursively to identify the optimal k that maximizes or minimizes an evaluation score function f(k, D). Unlike a conventional binary search that terminates upon finding a target value, Binary Bleed dynamically adjusts the search space based on an evaluation threshold. From an initial range [kmin, kmax], it computes the mid-point\n$k_{mid} = \\frac{k_{min} + k_{max}}{2}$\nand evaluates the score\n$f_{mid} = f(k_{mid}, D)$.\nIf fmid meets the threshold, the search continues in the direction of optimization. For maximization,\nthe lower bound is updated to $k_{min} = k_{mid} + 1$ when $f(k_{mid} + 1,D) > f_{mid}$, and conversely, the upper bound is updated to $k_{max} = k_{mid} - 1$ when $f(k_{mid} \u2013 1,D) < f_{mid}$. For minimization, the process is reversed. This recursive update continues until convergence or until a predefined stopping criterion is met, allowing the algorithm to \u201cbleed\u201d into higher or lower k. This ensures thorough exploration and optimization of the parameter k, adapting the search space dynamically to achieve the best possible evaluation score within the specified constraints.\nThis search does not cease upon finding a k above the threshold; instead, it continues exploring to ensure that no better solutions exist, particularly focusing on larger k values that may yield higher scores. The runtime complexity of Binary Bleed is bounded by $O(nlog_2 (p+1)) \\times (T_{model}+T_{score})$ in both the best and worst cases, as shown later in III-A, where p is the probability of recursing twice. We apply binary search on the ordered set K, the search space, to find its optimal value. Mathematically, this optimization can be expressed as:\n$k_{optimal} = max \\{k \\in \\{1, 2, ..., K\\} : S(f(k)) > T\\}$\nwhere T is the selection threshold, f(k) represents the model computation, and S(f(k)) is the model's scoring function.\nThe algorithm is presented in Algorithm 1, showcasing its operational structure. To determine the optimal k, a list of k (K) is provided with the dataset, model, scorer, and score threshold. Initially, the set of visited k, the maximum (kmax), and minimum (kmin) bounds are initialized (lines 1-2). The algorithm checks the base case for recursion: if the left index ileft is greater than or equal to the right index iright, the function returns, terminating the recursive search when no more k values are left to explore (lines 3-4). The optimal k is sought by computing the middle index of K and its k (lines 5-6). This value is validated to ensure it is within kmin and kmax (line 7). The model is evaluated at this middle k on the dataset, and the score is added to the set of visited k values (lines 8-9). If the score at kmiddle meets or exceeds the threshold, kmin updates to kmiddle, focusing on larger values (lines 10-12). If the score is below the stop threshold, kmax updates to kmiddle, focusing on lower values (lines 13-15). The BinaryBleedKSearch function is recursively called on the narrowed k range (lines 17-19). This dynamically recursive"}, {"title": "B. Binary Bleed Multi-threaded, Multi-rank", "content": "A parallel implementation of Binary Bleed can be obtained by extending the serial implementation depicted in Algorithm 1 with the following changes: Make kmin, kmax and the list of visited k rankseen global using a distributed cache such as reddis, and ileft and iright are now computed by each MPI rank or thread as $i_{left} = k_{min}+idx \\times (k_{max}-k_{min})/size$ and $iright = min(ileft + size, kmax)$ where idx is the thread index or MPI rank, and size is the total number of threads or MPI ranks. The resulting parallel implementation will work well, but will not be effective when the list of k is sparse.\nA more robust algorithm can be obtained by replacing recursions in Algorithm 1 by a k-sort, illustrated in Figure 1, where k values are sorted using in-order, pre-order, or post-order binary tree traversal, indicated by the color on three sides of each rank's k value.\nFurther, K must be chunked into available resources using Algorithm 2. By appropriately sorting and chunking the k values, the Binary Bleed algorithm can efficiently operate in a multi-threaded and multi-rank environment, enhancing its scalability and performance.\nLogistics of Chunking K and Traversal Sort: To illustrate the optimal function sequence, assume k = [1-10], with two operating resources."}, {"title": "C. Binary Bleed Early Stop", "content": "Specific domains allow for an additional heuristic. Rather than pruning the lower k values, setting a bound on the other extreme of the data will allow for truncation of the upper values. This heuristic is based on domain knowledge, where if any scores cross the stop threshold, they will never rise to cross the selection threshold, and subsequent k values can be ignored. Mathematically, it can be represented as:\n$k_{optimal} = max \\{k \\in \\{1, 2, . . ., K\\} : \\forall i \\le k, S(k_i) > U\\}$\nwhere S(ki) is the score of the $i^{th}$ k, and U is the stop bound."}, {"title": "D. Additional Considerations", "content": "In multi-resource computing, an optimal k may be larger than an executing k. For long computations, checks can be pushed into the model to terminate such k early. Consider the scoring distribution, like silhouettes, which can speed up the process. True optimal values are found faster when scores above the threshold resemble a square wave. Mathematically:\n$S(k_i) = \\frac{sgn(k_o - k_i) + 1}{2}$\nwhere S(ki) is the evaluation score at the ith index of k values, ko is the optimal k, and sgn is the signum function, which evaluates to +1 for ki < ko and -1 for ki \u2265 ko. In the worst case, a Laplacian score distribution will peak at the optimal k while other k scores will be below the score selection threshold. Lower k values may be pruned if the peak is visited before lower values. Otherwise, all values will be visited in the order of the sort. Despite the score distribution, Binary Bleed will not visit more k values than a linear search."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We gauge the efficacy of unsupervised learning methods in single-node and distributed settings. We evaluate NMF and K-means in the former, then NMF and RESCAL in the latter."}, {"title": "A. Single-node Setting", "content": "NMFk: Data was generated using a synthetic data generator with random Gaussian features for a predetermined k, where ktrue = {2,3,...,30}. The shape of 30 matrices was sized 1000 by 1100, resulting in 1.1 million non-negative entries. The ktrue predetermined the number of clusters in the data, typically showing a drastic drop-off in the scoring metric for subsequent k values. The evaluation criterion was the silhouette scores of the proposed clusters for the visited k. For the NMFk Binary Bleed trials, three separate instances were operated over the same data: Standard NMFk, Binary Bleed Vanilla, and Binary Bleed Early Stopping. Each instance was evaluated on all 30 matrices with k search space K = {2,3,...,30}. The results corresponding to NMFk Vanilla and NMFk Early Stopping for k = 15 and k = 8, respectively, are Figure 7's top row. It can be seen that Binary Bleed pruned multiple k values, whereas the standard method must visit all K. In the overview, Figure 8, orange and blue lines show the Binary Bleed Vanilla algorithm and the downward trend of k visits relative to the standard, where Pre-order finds the optimal k in fewer overall visits. Similarly, Early Stop in pink and green have lower overall visits than Vanilla, with Pre-order benefiting more despite a slightly increasing trend for both Early Stop lines over ktrue. Interestingly, the post-order Early Stop between being as fast as the Pre-order and visiting more k than the Binary Bleed Vanilla, which can be attributed to the number of compute resources paired with which values are ktrue and the ordering of K. Overall, the algorithms visit the following percentages of K: Pre-order Vanilla: 56%, Post-order Vanilla: 76%, Pre-order Early Stop: 27%, Post-order Early Stop: 44%, while Standard NMFk visits 100% of K. This shows Pre-ordering K with Early Stop executes fastest."}, {"title": "B. Multi-node Setting", "content": "We demonstrate a reduction in k visits for topic modeling of over 2 million scientific abstracts from arXiv with NMFk from [33]. We used the LANL Chicoma super-computer cluster on the GPU partition and allocated ten nodes, with four NVIDIA A100s per node. NMFk with Binary Bleed Early Stop and standard ran on K = {2,3,\u2026\u2026\u2026,100}. Early stop visited 60% of the total compared to Standard NMFk. Both agreed the koptimal = 71 for the vocabulary size 10,280."}, {"title": "C. Distributed Setting", "content": "To showcase the efficacy of our approach on the largest dataset, we utilized the distributed NMF framework pyDNMFk results and the distributed RESCAL framework pyDRESCAL, as referenced in [27] and [8], respectively. Large datasets need substantial computational resources to factorize. For instance, without Binary Bleed, pyDNMFk required 2 hours with 52,000 cores to estimate k using standard NMF for a 50TB dataset, averaging 17.14 minutes per k for K = {2,3,\u2026\u2026,8}. Similarly, pyDRESCALk required 3 hours with 4,096 cores to factorize 11.5TB of synthetic data using standard RESCAL, averaging 18 minutes per k for K = {2,3,\u2026\u2026,11}. Binary Bleed Vanilla and Early Stop results on this distributed data were identical, so only the former's results were considered, given that the stop thresholds were crossed on the last k. For both RESCAL and NMFk, the selected k matched the standard.\nRESCAL: Binary Bleed was applied to the silhouette and relative error metrics with Pre-order and Post-order traversal. For Pre-order traversal, 30% of the total k values were visited, resulting in an average runtime of 54 minutes, compared to the 180 minutes required for the standard RESCAL. In contrast, Post-Order traversal visited 80% of the total k values, with an average runtime of 144 minutes, as in Figure 9.\nNMF: Similarly, the results from [27] were analyzed for K = {2,3,..., 8}. Using Pre-order and post-order traversal, we evaluated the Binary Bleed Vanilla algorithm for the silhouette score. For Pre-Order traversal, 43% of the total k values were visited, resulting in an average runtime of 51.43 minutes, compared to the 120 minutes required for the standard NMF. Post-order traversal visited 86% of the total k values, with an average runtime of 102.86 minutes, as shown in Figure 9. These results further emphasize the efficiency of the proposed approach in reducing computational time while ensuring effective factorization."}, {"title": "V. CONCLUSION", "content": "This work addresses the computationally expensive task of k search over potentially large datasets, reducing a O(n) search time to approach O(log(n)). Introduced is the ability to optimize the k search from calculation outputs in an upward or lower direction based on a threshold. The algorithm can operate on a single thread, multiple threads, multiple nodes, and distributed systems. Additionally, early stopping is introduced to reduce directional k bleed on sufficient domains.\nWe tested our method with NMFk, K-means, and RESCAL with Silhouette and Davies Boulding scoring techniques both on synthetic and real-world data. Our experiments showed the k search space can be drastically reduced with Binary Bleed."}]}