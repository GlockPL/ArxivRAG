{"title": "LIGHTWEIGHT NEURAL APP CONTROL", "authors": ["Filippos Christianos", "Georgios Papoudakis", "Thomas Coste", "Jianye Hao", "Jun Wang", "Kun Shao"], "abstract": "This paper introduces a novel mobile phone control architecture, termed \"app agents\", for efficient interactions and controls across various Android apps. The proposed Lightweight Multi-modal App Control (LiMAC) takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, within LiMAC, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-40. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Smartphone application agents, commonly known as app agents, are expanding the potential applications of artificial intelligence to smartphones and other mobile devices. Such agents could allow users to accomplish a range of tasks, from scheduling appointments and sending messages to purchasing items and booking flights, with minimal effort. Fundamentally, app agents observe user instructions and progressively interact with the smartphone's user interface\u2014by clicking, scrolling, inputting text, etc. to accomplish the task. However, due to the limited computational resources of smartphones, these agents must be optimised for efficiency, employing lightweight models with minimal memory usage and fast processing speeds.\nRecent advancements have leveraged foundation models to develop app agents that understand natural language instructions and execute complex user commands within the smartphone's interface (e.g., Rawles et al., 2024; Bai et al., 2024; Wang et al., 2024b;a). While foundation models enable sophisticated capabilities, relying on them for every action introduces significant drawbacks. Their substantial size and computational complexity make them resource-intensive and impractical for constant use on mobile devices. Alternatively, querying server-hosted foundation models, such as GPT-40 or Gemini, for each task can be prohibitively expensive due to the operational costs of running large models, making this approach impractical for everyday applications. For example, a state-of-the-art GPT-40-based app agent (e.g. Rawles et al., 2024) may require one to two minutes to run and cost approximately $1.00 per task on average, based on tasks from the evaluated datasets.\nTo address these limitations, we propose a gated architecture that combines a lightweight transformer network with a small fine-tuned VLM. The task description and the smartphone state are first processed by a compact model (~500 million parameters) which effectively handles most actions. For actions that require natural language understanding, such as composing a text message or querying a search engine, a VLM is invoked to generate the necessary text. This hybrid approach reduces"}, {"title": "2 TECHNICAL PRELIMINARIES", "content": "We model phone interaction as a sequential decision-making process. Each task consists of a given goal g that should be completed during an episode. At each timestep t of the episode, the phone's internal state is denoted by st, while ot represents an observation of this state, including screen captures and UI element trees. The set of visible UI elements on the screen at timestep t is defined as It, with Ot,i representing the i-th UI element at timestep t where i \u2208 It. Each UI element i is represented by three different components: the image that corresponds to the UI element that we denote as ot, the text that corresponds to the UI element o, and the related attributes of the UI element, such as whether it is clickable or not, that we denote as other. Therefore, the representation of each UI element can be written as:\nimg\ntxt\nattr\nimg\ntxt\nattr\n$$O_{t,i} =  (O_{t,i}^{img}, O_{t,i}^{txt}, O_{t,i}^{attr})$$\ncharacterised by two components: its type atype \u2208 Atype (e.g., click, scroll-down, input-text)\nand its specifications apec \u2208 Aspec. The specifications vary based on the action type: for clicks, apec\nmight represent the targeted UI element; for typing actions, it would contain the text to be input.\nThus, an action can be represented as the tuple at = (atype, apec). This formulation allows for a\nflexible representation of diverse actions while maintaining a consistent structure.\nIn this work, the main goal is to learn a model that will maximise action prediction accuracy, which corresponds to correctly predicting both the action type as well as the action specifications. To achieve this, we train AcT, which predicts atype. If the predicted action type is click, AcT also predicts"}, {"title": "2.1 PROBLEM FORMULATION", "content": "We model phone interaction as a sequential decision-making process. Each task consists of a given\ngoal g that should be completed during an episode. At each timestep t of the episode, the phone's\ninternal state is denoted by st, while ot represents an observation of this state, including screen\ncaptures and UI element trees. The set of visible UI elements on the screen at timestep t is defined\nas It, with Ot,i representing the i-th UI element at timestep t where i \u2208 It. Each UI element i is\nrepresented by three different components: the image that corresponds to the UI element that we\ndenote as ot, the text that corresponds to the UI element o, and the related attributes of the UI\nelement, such as whether it is clickable or not, that we denote as other. Therefore, the representation of\neach UI element can be written as:\nimg\ntxt\nattr\n$$O_{t,i} =  (O_{t,i}^{img}, O_{t,i}^{txt}, O_{t,i}^{attr})$$\nThe agent interacts with the phone through actions, denoted as at at timestep t. Each action is char-\nacterised by two components: its type atype \u2208 Atype (e.g., click, scroll-down, input-text)\nand its specifications apec \u2208 Aspec. The specifications vary based on the action type: for clicks, apec\nmight represent the targeted UI element; for typing actions, it would contain the text to be input.\nThus, an action can be represented as the tuple at = (atype, apec). This formulation allows for a\nflexible representation of diverse actions while maintaining a consistent structure.\nIn this work, the main goal is to learn a model that will maximise action prediction accuracy, which\ncorresponds to correctly predicting both the action type as well as the action specifications. To achieve\nthis, we train AcT, which predicts atype. If the predicted action type is click, AcT also predicts"}, {"title": "2.2 SEQUENCE MODELLING WITH TRANSFORMERS", "content": "Transformers (Vaswani et al., 2017) have demonstrated exceptional effectiveness in modelling and generating sequential data across a wide range of domains. They excel in various sequence modelling tasks, including those related to language, video processing, and decision-making (Chen et al., 2021). Regardless of the specific application, transformers begin by converting the input into a sequence of vectors. For text, this involves tokenising the input, with each token represented by an embedding vector. In the case of images, the input is typically divided into patches, where each patch is similarly represented by a vector, analogous to the tokenisation process in text. These embeddings, which map tokens or patches to vectors, can either be learned during the model's training or sourced from pre-trained models (e.g., Devlin et al., 2018). However, one inherent limitation of the transformers is that treat the input as a set of unordered vectors, lacking any notion of temporal or spatial relationships. To address this, positional embeddings are added to the original token or patch embeddings to encode the position of each element in the sequence. By incorporating this positional information, the transformer is able to understand the relative ordering of the input.\nThe combined embeddings are fed through several multi-head self-attention layers, which are designed to capture dependencies and contextual relationships between different embeddings in the input sequence. These self-attention mechanisms allow the model to focus on relevant parts of the sequence when processing each embedding, enabling it to handle long-range dependencies more effectively. After passing through multiple layers, each consisting of self-attention and feed-forward components, the final activations from the transformer's last hidden layer are passed through a linear (fully connected) layer. This layer is typically tasked with mapping the learned representations to the output space, whether for classification, prediction, or another specific task. The entire model is trained end-to-end, with backpropagation adjusting both the self-attention layers and the final linear layer to optimise performance on the desired task."}, {"title": "3 THE LIGHTWEIGHT MULTI-MODAL APP CONTROL FRAMEWORK", "content": "Our methodology processes the user's goal g and the phone's state at time t, utilising AcT, to determine the action type atype. If the predicted action type is either input-text or open-app, then g, ot, and a type are passed to a fine-tuned VLM, which is responsible for determining the specific action apec. For actions involving clicks, AcT handles the prediction directly but employs a different training objective that contrasts UI element embeddings to determine the most likely interaction target. Accordingly, this section is divided into three parts: predicting the action type, predicting specific actions for text input and app launching, and predicting clicks using our novel approach for interaction with UI elements. The full architecture of LiMAC is presented in Section 3."}, {"title": "3.1 MODEL INPUTS", "content": "AcT, the model responsible for predicting the action type (and later the click target, as seen in Sec- tion 3.5), is built on top of a typical transformer architecture. However, unlike standard transformers, where tokens represent words or characters, our \"tokens\" are pretrained embeddings that are mapped to the hidden dimension of the transformer. These tokens represent three key components: the user's goal g, the UI elements on the phone's screen ot,i, and the possible actions. By using these pretrained embeddings as input, we allow the model to effectively capture the relationships between the user's intent, the current state of the interface, and the set of available actions. We encode each key component (UI elements, actions, and goal) into embeddings that can be processed by the transformer. Below, we describe the encoding process for each type of input.\nGoal: We encode the user's textual goal g using a sentence encoder, resulting in the embedding eg = ftxt(g). This embedding captures the user's intent and serves as the first token to the transformer."}, {"title": "3.2 CONSTRUCTING THE INPUT SEQUENCE", "content": "After generating the goal, UI elements, and action embeddings, we organise them into a sequence representing the entire episode. Each episode in the dataset is encoded as a sequence of embeddings x, which is fed into the transformer. The sequence starts with the goal embedding eg, followed by the UI element embeddings er at timestep 0. Once all UI elements are encoded, a special end marker eend is added. The action type etype and specification espec embeddings for timestep 0 are then appended. This process repeats for each subsequent timestep: encoding UI elements, appending eend, and adding the action embeddings. For an episode with H timesteps, the final sequence is:\nui\ntype\nspec\nQui\nQui\ntype\nspec\n$$\n x = [e_g; e_{0,0}^{ui}; ...; e_{0,n}^{ui}; e_{end}; e_0^{type}; e_0^{spec}; e_{1,0}^{ui}; ...; e_{1,n}^{ui}; e_{end}; e_1^{type}; e_1^{spec}; ...; e_{H-1,0}^{ui}; ...; e_{H-1,n}^{ui}; e_{end}; e_{H-1}^{type}; e_{H-1}^{spec}] \n$$"}, {"title": "3.3 ACTION TYPE PREDICTION", "content": "In our pipeline, the prediction of the next action begins with determining the action type. Predicting the action type a type can be framed as a classification problem, where we identify ten distinct action types in the datasets used in this work. These action types represent various possible interactions, such as click, open-app, scroll-down, input-text, or other essential commands. We implement the action type prediction with a specialised head. The action type head, denoted as ftype, transforms the final hidden state ht of the transformer (after the eend token) into a probability distribution over the possible action types, p(atype|ht) = ftype(ht). The learning objective for this task is to minimise the cross-entropy loss between the predicted and actual action types. Given a dataset D, the cross-entropy loss for action type prediction is defined as:\n$$L_{type} = -E_{a^{type}, x \\in D} [log(p(a^{type}|h_t))]$$\nHere, h represents the transformer's output corresponding to the final hidden state before action prediction, averaged over all observations in the dataset. This loss function ensures that the model is trained to correctly classify the action type based on the sequence of embeddings from previous steps."}, {"title": "3.4 LEVERAGING FINE-TUNED VLMS FOR TEXT GENERATION IN ACTION EXECUTION", "content": "As described in the previous section, our agent first predicts the action type. Among the ten action types, two specifically require textual specifications: i) the input-text action, where the specification is the text to be entered into a text box, and ii) the open-app action, where the specification is the name of the application to be opened. For these actions, we rely on fine-tuning a VLM using an app control dataset. The dataset provides action data in a dictionary-like format, such as: {\"action-type\":\"open-app\",\"app-name\":\"Chrome\" }, with one key corresponding to the action type and another to the action specification. The VLM is trained to generate the correct sequence of tokens that corresponds to the successful completion of each action, optimising for the likelihood of generating the proper tokens based on the observation at each timestep.\nDuring inference, after AcT predicts the action type, it guides the VLM by enforcing the model to start its response with the predicted action type. For instance, if AcT predicts input-text"}, {"title": "3.5 EFFICIENT CLICK TARGETING USING CONTRASTIVE OBJECTIVES WITH ACT", "content": "Having covered how action specifications are generated for textual actions, we now turn to the case of click actions, where the specification is the UI element to interact with. To predict the correct UI element for a click action, we employ a contrastive learning approach that operates over the entire episode, using cosine similarity and a learnable temperature parameter. Since the number of UI elements varies across timesteps and episodes, a contrastive method is better suited than classification, which can suffer from class imbalance and limitations when handling more UI elements in test episodes than seen during training. Let hype be the transformer's last hidden state up to embedding etype, and ftarget be an affine transformation that projects the hidden states to an embedding space. Simultaneously, the hidden states of the transformer corresponding to the UI element embeddings, denoted as hui, are also projected into the same embedding space:\n$$q^{type} = f_{target}(h^{type}) and p^{ui} = f_{target}(h^{ui})$$\nAssuming the embedding space lies in Rd, the query embedding qtype has dimensions 1 \u00d7 D, while the matrix pui, representing all UI elements, has dimensions K \u00d7 D, where K is the total number of UI elements in the episode. The goal is to train the model such that qtype aligns closely with the correct UI element's embedding at timestep t, using cosine similarity as the alignment measure. To achieve this, we adopt contrastive training techniques with the InfoNCE loss (Oord et al., 2018). We first compute the similarity matrix between the query embedding qtype and all UI element embeddings, scaling the similarity by a learnable parameter \u03c4 (e.g., Radford et al., 2021). The scaled cosine similarity matrix is defined as:\n$$S = \\frac{q {p^T}}{\\|q\\|_2 \\|p\\|_2}$$\nwhere ||p||2 is the L2 norm of each row of p. For simplicity, we drop the superscripts in this equation. The InfoNCE loss for UI element selection across the episode is computed as:\n$$L_{elem} = -E \\sum\\limits_{k=1}^{K} \\log( \\frac{\\exp(S_+)}{\\sum_{i=1}^{K} \\exp(S_i)} )$$\nHere, S+ is the scaled similarity between the transformer's output and the correct UI element for the click action, and Si represents the similarity between the output and all other UI elements. During inference, for each action requiring a target element, the UI element with the highest similarity is selected. This contrastive approach enables AcT to effectively learn which UI elements to interact with during a click action by treating all other UI elements in the episode as negative examples. The use of cosine similarity focuses on the directional alignment of the embeddings, while the learnable temperature \u03c4 adjusts the sharpness of the similarity distribution during training, allowing for more flexible and precise UI element selection."}, {"title": "4 EXPERIMENTS", "content": "Datasets: Our experiments focus on two open mobile phone control datasets, AndroidControl (Li et al., 2024) and Android-in-the-Wild (AitW) (Rawles et al., 2023). Both datasets contain extensive human demonstrations of mobile phone navigation across a wide variety of tasks. In AndroidControl, every episode is defined by a specific goal, accompanied by a sequence of observations and actions. Each observation includes a screenshot from the phone and its corresponding UI tree. Conversely, observations in AitW lack the UI tree. As a result, it is necessary to extract the UI tree using an OCR system, which identifies all UI elements and provides a brief description of each. Further details on the goal format, observation space, and action space for each dataset can be found in Appendix A."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Datasets: Our experiments focus on two open mobile phone control datasets, AndroidControl (Li\net al., 2024) and Android-in-the-Wild (AitW) (Rawles et al., 2023). Both datasets contain extensive\nhuman demonstrations of mobile phone navigation across a wide variety of tasks. In AndroidControl,\nevery episode is defined by a specific goal, accompanied by a sequence of observations and actions.\nEach observation includes a screenshot from the phone and its corresponding UI tree. Conversely,\nobservations in AitW lack the UI tree. As a result, it is necessary to extract the UI tree using an OCR\nsystem, which identifies all UI elements and provides a brief description of each. Further details on\nthe goal format, observation space, and action space for each dataset can be found in Appendix A."}, {"title": "4.2 EVALUATION PIPELINE", "content": "We evaluate on the test set of two datasets, using the same process for all models, with only the observation format and model calling differing. For each timestep, we call the model with the relevant observation format to generate an action. VLMs are trained to return actions in a specific format, while pre-trained models use a detailed prompt with the observation, as in Rawles et al. (2024).\nOne can calculate strict accuracy by directly comparing returned actions to the ground truth. However, in this work we relax this metric for a more practical assessment. In the relaxed version, a UI element is deemed correct if its bounding box is within the target element, as described by Li et al. (2024). For input-text actions, correctness is determined by a Jaccard index score of at least 0.5, reflecting the functional equivalence of similar inputs in search bars. We only report the relaxed accuracy metrics, found in Tables 1 and 2."}, {"title": "4.3 MEASURING END-TO-END ACCURACY", "content": "In this section, we present the total action accuracy of our method, as well as the baselines. Table 1 present the accuracy for action prediction in AndroidControl and AitW, respectively. In both AitW and AndroidControl, we observe that LiMAC consistently outperforms Florence2, Qwen2-VL, and GPT-40-based baselines with respect to the action prediction accuracy, demonstrating superior generalisation to the held-out test set. The overall improvement of LiMAC in the accuracy compared to AndroidControl can be attributed to the closer alignment between the training and test sets, as the test set includes the same set of instructions but applied to mobile devices with varying characteristics, such as screen size and Android version. Additionally, we observe a significant performance drop in text-based baselines like T3A and image-text-based models like M3A and SeeAct. The absence of original UI trees in the AitW dataset can explain this decline. Since UI trees must be extracted from images using an annotation tool, inaccuracies are often introduced, which diminishes the performance of models that rely on text-based output conditioning. This underscores a key advantage of LiMAC, which remains robust even when UI trees are imprecise or completely missing (as seen in Table 4), with minimal impact on overall performance."}, {"title": "4.4 COMBINING DIFFERENT MODULES", "content": "LiMAC is a modular architecture that enables the integration of different modules for tasks such as predicting the action type, identifying the target element in click actions, and generating text for open-app and input-text. In this architecture, we primarily use AcT to predict both the action type and the target element for click actions. However, alternative modules can be employed for these predictions as well. In Table 2, we present combinations of different models, excluding SeeAct due to its low overall accuracy, and compare their performance across two datasets.\nIn the AndroidControl dataset, we observe that using M3A for predicting the target elements in click actions improves performance over using AcT alone. This demonstrates that GPT-40 is highly"}, {"title": "4.5 ABLATION STUDIES", "content": "Table 3 presents the action-type, click-target, and text accuracies for various module combinations across the two datasets. The results show that LiMAC, particularly the AcT, achieves the best performance in action-type prediction. In the AndroidControl dataset, M3A and T3A perform well in click-target and text prediction but struggle with action-type accuracy, and they underperform in the automatically annotated AitW dataset. Overall, AcT within LiMAC excels at click-target predictions while being significantly smaller. Finally, our Florence fine-tune excels at text prediction, significantly outperforming GPT-40 baselines in AitW and remaining competitive in AndroidControl.\nLastly, we present three ablation studies to further explore AcT design choices. A core feature of AcT is its ability to process each UI element as a distinct embedding within the transformer, created by concatenating the image, text, and attribute embeddings of the corresponding UI element. To assess the impact of the image and text modalities, as well as the CLIP fine-tuning on LiMAC's performance, we compare it to three ablated versions: one that excludes the image component, another that omits the UI text in the embedding process, and one that uses the original CLIP for encoding the image embeddings instead of the fine-tuned version. The evaluation metrics for these comparisons in the AndroidControl dataset and using Florence2 for text completion are shown in Table 4.\nThe results demonstrate that removing image embeddings significantly reduces accuracy across all metrics, highlighting the crucial role of visual information in AcT. In contrast, omitting the text embeddings has only a slight effect on performance, suggesting that AcT can function effectively using only screenshots of observations without accessing the UI tree. Additionally, we observe that fine-tuning CLIP (see Section 3.1) is an important factor in improving the overall accuracy of LiMAC."}, {"title": "5 RELATED WORK ON APP CONTROL", "content": "Though graphical user interface (GUI) control mainly started with web-based datasets and foundation model agents (Shi et al., 2017; Liu et al., 2018; Yao et al., 2022a; Deng et al., 2023; Furuta et al., 2023; Gur et al., 2023; Zheng et al., 2024), there has recently been a significant focus on mobile phone control. This can be seen both by the rapid development of Android navigation datasets and environments (Rawles et al., 2023; 2024; Li et al., 2024), and of mobile control agents (Yang et al., 2023; Wang et al., 2024b;a; Wen et al., 2023; Hong et al., 2024; Rawles et al., 2024; Li et al., 2024; Bai et al., 2024). Though many agents are published with their own specific evaluation data, popular datasets such as Android-in-the-Wild (Rawles et al., 2023) or AndroidControl (Li et al., 2024) are often used as benchmarks.\nThe agents developed for this task can be divided into two clear input types: text-based, using UI accessibility tree or XML information to describe the screen, or image-based. Image-based agents require vision models, which are capable of directly processing image inputs, and are usually backed by VLMs. On the other hand, text-based agents are backed by classical LLMs. Image-based agents using VLMs also often take a combination of text and image as input to the model. Many mobile control agents propose intricate prompting methods backed by off-the-shelf, often proprietary, LLMs such as GPT-4 (Rawles et al., 2024; Yang et al., 2023; Wang et al., 2024b;a; Wen et al., 2023; Zheng et al., 2024). Though this requires little to no training, it can be both slow and expensive. Moreover, these models cannot be further tailored and trained for specific tasks. As such, another approach is to build mobile control agents around fine-tuned of foundation models on Android control datasets such as AitW or AndroidControl. Firstly, both AitW and AndroidControl present results for a fine-tuned LLM on their dataset, alongside the dataset itself. For example, Li et al. (2024) train various PaLM 2 (Anil et al., 2023) models on their dataset. However, these models are proprietary and supposedly quite large, with the base PaLM 2 model reported to have over 300B parameters. CogAgent (Hong et al., 2024) also performs fine-tuning on an 18B-large VLM. Bai et al. (2024) propose a different approach, called DigiRL, using RL to train their 1.3B VLM. This achieves strong performance but has its own limitations, notably the data gathering cost and simulation difficulty, leading to the model only being adept on a small subset of AitW."}, {"title": "6 CONCLUSION", "content": "In summary, we propose LiMAC, a lightweight framework designed to address app control tasks. LiMAC extracts UI elements from each phone screenshot and encodes them using specialised vision and text modules. These UI element encodings are then passed as embeddings to AcT, which predicts the type and specifications of the next action. AcT focuses on two key aspects of actions: the action type and the target element when the predicted action is click. For actions requiring text generation, LiMAC uses a fine-tuned VLM to ensure successful completion. We compare LiMAC against six baselines supported by state-of-the-art foundation models and evaluate them on two open-source datasets. Our results show that LiMAC can outperform the baselines while requiring significantly fewer computational time for both training and inference. This demonstrates that LiMAC is capable of handling task completion on devices with limited computational capabilities.\nOne of the main limitations of the proposed method is the limited training data. LiMAC is trained on just 13K and 18K episodes for AndroidControl and AitW, respectively. The absence of any pretraining further hinders the model's ability to improve performance on more complex tasks. In the future, we aim to enhance the model's performance by incorporating online learning techniques, such as reinforcement learning. After the initial training stage presented in this work, LiMAC could interact with an Android emulator to generate additional data. By using a suitable reward function, or even leveraging GPT-4 to evaluate the generated trajectories and assign rewards (Bai et al., 2024), we could fine-tune LiMAC to improve the completion rate of tasks."}, {"title": "A DATASET FORMAT", "content": "We use AndroidControl and AitW dataset. While we use the full AndroidControl dataset, for AitW we only select a few episodes for each unique instruction, due to the sheer size of the dataset and the repetitive nature of its instructions. The dataset is divided into five categories of tasks, of which we use the 'GoogleApps', \u2018Install', and 'WebShopping' splits, since the other two contain single-step and Q&A tasks. We process the episodic data present in these datasets into refined, step-wise, datapoints which can be used for training and evaluation. Each datapoint is composed of the high-level goal for the task, an observation of the current screen, and the correct action. Details are given below.\nGoal: The goal is always a raw text string describing the high-level instruction for the episode to which the datapoint belongs.\nObservation: The exact form of the observation depends on the type of model it is used for. Text- based approaches such as T3A need a textual observation. For AndroidControl, we use the provided accessibility UI trees, which we further process into a list of UI elements containing information such as the element type, description, and attributes (clickable, editable, selected, etc...). Like in Li et al. (2024), we filter these to retain only important elements, namely those that contain text or have critical attributes. For AitW, OCR representations of text and icons are given in the dataset, but no comprehensive UI trees are provided. Therefore, to obtain the final representation, each element must be identified and converted using a similar procedure to that for AndroidControl. Vision models such as Qwen2-VL and Florence2 will expect an image-based observation. This observation will consist of the current phone screenshot along with an overlay of the UI element bounding boxes and their index. Finally, some models, such as M3A and ours, use a mixture of observations, both text and image-based. In particular, our model expects a text-based list of UI elements similar to the one described above, as well as a list of cropped images. The list of cropped images corresponds to each of the UI elements in the text-based observation and is used by our model as described in Section 3.1.\nAction: Action grounding is a crucial part of mobile phone control, so as in previous works (Zheng et al., 2024; Yang et al., 2023; Wang et al., 2024b;a) and both the datasets we use, we define a fixed action space, seen in Table 5. Information for most actions is sourced directly from the datasets, with only the action name at times varying. The only exceptions to this are the click and long-press actions, which require a target element, rather than x-y coordinates. For these, we select the best matching candidate from the observation list of UI elements. The action takes a specific JSON format we expect the models to match to facilitate parsing, which is simply a dictionary with the action type and an action specification (see Table 5). An example would be: {\"action-type\":\"open-app\",\"app-name\":\"Chrome\"}."}, {"title": "B PROMPT ENGINEERING BASELINES", "content": "We evaluate four prompt engineering methods leveraging GPT-40 to generate actions. First, we assess two baselines proposed by Rawles et al. (2024): a text-based method (T3A) and a multi- modal approach (M3A). In both methods, GPT-40 generates a summary of the previous timestep by reflecting on prior actions, the current observation, and previous observations and actions. GPT-40 then generates a proposed action in a ReAct-like (Yao et al., 2022b) fashion using a detailed prompt"}, {"title": "C IMPLEMENTATION DETAILS", "content": "AcT is a compact transformer based on GPT-2 architecture. The transformer consists of 24 layers and 16 heads per layer. The hidden dimension of the transformer is 1024. We apply a dropout rate of 0.3 (Srivastava et al., 2014) during training across all layers. The AdamW optimiser (Loshchilov et al., 2017) is used in all experiments, with a learning rate of 3 \u00d7 10-4 specifically for AcT. The functions ftype and ftarget are implemented as two-layer fully connected networks, each with a hidden size of 4096 and a dropout rate of 0.3. We use a batch size of 1 with gradient accumulation being set to 32.\nWe fine-tune Florence2 for 10 epochs, starting with an initial learning rate of 10-6, which is gradually reduced to zero during training. The batch size is set to 2, with gradient accumulation configured to 8. For Qwen2-VL, we employ LoRA with a dimensionality of 64, beginning with an initial learning rate of 10-4, also gradually decreasing to zero throughout training. The batch size for Qwen2-VL is 1, with gradient accumulation similarly set to 8. We fine-tuned Qwen2-VL for 3 epochs."}, {"title": "DADDITIONAL RESULTS", "content": "This section presents additional evaluation results for LiMAC. In Table 6, we provide a full set of evaluation metrics for the baseline models, as well as for various combinations of LiMAC with other methods. These combinations are used to predict the target element in click actions or generate text for specific actions, such as open-app and input-text. In all the experiments involving LIMAC, ACT is employed to predict the action type, while different combinations of methods are used to predict the action specifications, such as the target element or text generation. This approach allows us to isolate the impact of each combination on performance while maintaining a consistent action type prediction. This table extends the results already presented in Tables 1 to 3 providing a more in-depth understanding of the performance across a range of metrics. This additional breakdown offers a clearer understanding of how LiMAC performs when integrated with other methods, offering insights into the strengths and potential trade-offs of each combination in different scenarios."}, {"title": "E CASE STUDIES", "content": "Some sample episodes from AndroidControl, including agent predictions, are provided in Figures 4 and 5. These are provided for illustration purposes, as well as to further explain 'relaxed' accuracies and an example failure. Figure 4 presents both an instance of a relaxed target element in the third timestep and a failed input-text action in the final timestep. Figure 5 shows a relaxed input-text action in the fourth timestep and an otherwise successful episode. Further details are provided in the figure captions."}]}