{"title": "Towards Leveraging News Media to Support Impact Assessment of AI Technologies", "authors": ["Mowafak Allaham", "Kimon Kieslich", "Nicholas Diakopoulos"], "abstract": "Expert-driven frameworks for impact assessments (IAs) may inadvertently overlook the effects of AI technologies on the public's social behavior, policy, and the cultural and geographical contexts shaping the perception of AI and the impacts around its use. This research explores the potentials of fine-tuning LLMs on negative impacts of AI reported in a diverse sample of articles from 266 news domains spanning 30 countries around the world to incorporate more diversity into IAs. Our findings highlight (1) the potential of fine-tuned open-source LLMs in supporting IA of AI technologies by generating high-quality negative impacts across four qualitative dimensions: coherence, structure, relevance, and plausibility, and (2) the efficacy of small open-source LLM (Mistral-7B) fine-tuned on impacts from news media in capturing a wider range of categories of impacts that GPT-4 had gaps in covering.", "sections": [{"title": "1 Utilizing news media for impact assessment", "content": "Anticipating and evaluating the negative impacts of emerging AI technologies on individuals and society requires a deep understanding and familiarity with the contextual use, functional capabilities, and affordances of these technologies [20, 14, 15]. Researchers have proposed a variety of impact assessment (IA) frameworks. However, the inadvertent expert biases that are introduced by these approaches such as the demographically skewed backgrounds [1], homogeneous experiences of experts [5], or selection bias with respect to what impacts to focus on [8], have an influence on the foresight and evaluation process of AI technologies [1]. Furthermore, identifying potential impacts of emerging AI technologies, let a lone across cultures, is both challenging and resource-intensive [10]. As a response, LLMs have recently been explored as a scalable alternative and ideation tools to support IAs [18, 3], though they also suffer from concerns about the nature and extent of the biases that may be captured by their training data and so reflected in the generated text [16, 22].\nThis research explores the potentials of incorporating more diversity into IAs, while focusing on issues that are relevant to the public, by leveraging news media coverage of AI. Specifically, we do this by fine-tuning LLMs on impacts covered in the news media to support AI developers, researchers, and other stakeholders to generate and envision potential negative impacts of emerging AI technologies before deployment. Our choice to source impacts from news media is to draw on the diverse range of negative impacts of AI that have already been reported. Media reporting plays a crucial role in shaping public opinion on emerging technologies and acts as an agenda setter by reporting on topics and issues that are deemed as relevant - thus, the media has a substantial influence on what impacts are discussed in the public sphere and which impacts are deemed important (and"}, {"title": "2 Methodology", "content": "Our dataset consists of 91,930 articles in English retrieved and scraped from Google News using a curated set of 40 AI-relevant keywords (as listed in A.1) that were published by 266 news domains between January 1, 2020 and June 1st 2023 spanning 30 countries around the world (see A.2).\nWe used prompts P1 and P2 in Table S1 to prompt GPT-3.5-turbo to summarize two parallel pieces of information from each news article in our dataset: a description of the Al systems reported on, and a set of negative impacts described that are associated with these systems. The resulting information was curated in a dataset that includes 37,689 pairs of descriptions and negative impacts of AI technologies from 17,590 articles. To compare the distribution of impacts across models, we categorized the impacts in the full dataset by applying BERTopic [9], a topic modeling technique that leverages transformers to create easily interpretable topics. The resulting set of ten topics was then manually labeled based on the keywords and three representative examples for each topic. Finally, we mapped the manually labeled topics back to the the negative impact descriptions in our sample.\nFor fine-tuning, we randomly split the curated dataset into training (N=32,035), validation (N=5,140), and testing datasets (N=514). The training and validation datasets were used for fine-tuning and the testing dataset was used to evaluate the fine-tuned models in an impact generation task. We decided to keep the training sample large in order to not introduce additional biases in the selection of impacts used for finetuning and to preserve the diversity of impacts in the sample. To assess the proficiency of models for generating negative impacts, we prompted GPT-4 and Mistral-7B-Instruct [11] using zero-shot prompting to generate negative impacts based on the descriptions of AI technologies in the test dataset. We formulated the corresponding prompts for each model for this task as shown in P3 and P4, in Table S1, respectively. Furthermore, we fine-tuned two completion models OpenAI GPT-3 and Mistral 7B, using QLoRA [6], on the training dataset to further gauge the quality and range of categories of negative impacts generated using LLMs once aligned with the news media. Finally, similar to prior evaluation studies of generated text applied in other anticipatory approaches [7, 23], we qualitatively evaluated the generated impacts across the four dimensions of coherence, granularity, relevance, and plausibility per the qualitative rubric in Table S4 to help evaluating and articulating the efficacy of the generated impacts by LLMs for IA."}, {"title": "3 Results & Conclusion", "content": "A total of 10 categories emerged from the negative impacts described in our sample relating to: Societal Impacts, Economic Impacts, Privacy, Autonomous System Safety, Physical and Digital Harms, AI Governance, Accuracy and Reliability, AI-generated Content, Security, and Miscellaneous Risks and Impacts. A description of each category is provided in appendix A.4 with examples. These categories align with many of the impacts outlined by research on the harms, as well as social and ethical impacts of AI [19, 20]. The qualitative assessment of the generated impacts across the four dimensions of coherence, granularity, relevance, and plausibility shows that potentials of smaller open-source models such as Mistral-7B fine-tuned on negative impacts from news media to gener-"}, {"title": "Limitations", "content": "Any categorization of impacts relying on news media will likely reflect biases based on the sources of data used to build it. Our research falls short of accounting for biases pertaining to news outlet credibility (i.e., low vs. high credible news sources), political bias, temporal biases, geographic biases, and even the type of news article (i.e. hard news vs. opinion). We suggest for future research to account for these biases and evaluate the level of influence these biases have on the categories of impacts prevalent in the news media and subsequently on the range and quality of impacts that might be generated using LLMs fine-tuned on that data. For example, impacts relevant to alienation and loss of agency that are reported in the literature [19, 24] are missing from the news media, at least at the level of detail considered in this work, and therefore were not reflected in the generated impacts by the fine-tuned models. In addition, the biases present in news media (or any other data source chosen to act as a basis for fine-tuning for this task) raises an important question of how such biases would come to be reflected in an impact assessment process of an AI technology. For instance, if a norm is established that more attention should be given to environmental impacts from AI, perhaps a training set could be modified to project that impact more frequently (while maintaining relevance) in a fine-tuned model. The findings in this work make it clear that close attention to the biases in an underlying fine-tuning dataset will be crucial to attend to, measure, and potentially deliberate on in order to make models viable contributors to impact assessment and anticipatory governance approaches."}, {"title": "Impact statement", "content": "This work paves the way for future research to build impact assessment and anticipatory tools to potentially guide practitioners and researchers in the process of evaluating the negative impacts of Al technologies. Depending on the context of the deployment, a range of unintended consequences could influence users' trust and reliance on these tools. For instance, over-relying on our fine-tuned models, if deployed as an inference tool, has the risk of diminishing critical thinking and the anticipation of negative impacts if the outputs of the models are perceived or deemed to be conclusive or inclusive of all plausible and possible scenarios in which an AI technology could be used. Accordingly, we view the development of impact assessment tools using LLMs as supporting methods (but not substitutions) in the creative process of anticipating and assessing the negative impacts of AI."}, {"title": "Generating impacts of this research using LLMs", "content": "\u2013 To extend the scope of impacts beyond the ones we have considered or thought of, we leveraged GPT-4 and a fine-tuned Mistral-7B on impacts from the news media to assist us with capturing the range of potential unintended consequences of using LLMs for assessing the impacts of this research. By prompting GPT-4 with an extended version of the abstract to this paper, that is more oriented towards an anticipatory governance task, and prompt P2\u00b9, we extracted the functional and contextual descriptions\u00b2 of LLMs for anticipating\nUsing prompt P2 illustrated in Table S1, we inserted the following abstract in the Article placeholder:\"Gaining insight into the potential negative impacts of emerging Artificial Intelligence (AI) technologies in society is a challenge for implementing anticipatory governance approaches. One approach to produce such insight is to use Large Language Models (LLMs) to support and guide experts in the process of ideating and exploring the range of undesirable consequences of emerging technologies. However, performance evaluations of LLMs for such tasks are still needed, including examining the general quality of generated impacts but also the range of types of impacts produced and resulting biases. In this paper, we demonstrate the potential for generating high-quality and diverse impacts of AI in society by fine-tuning completion models (GPT-3 and Mistral-7B) on a diverse sample of articles from news media and comparing those outputs to the impacts generated by instruction-based (GPT-4 and Mistral-7B-Instruct) models. We examine the generated impacts for coherence, structure, relevance, and plausibility and find that the generated impacts using Mistral-7B, a small open-source model finetuned on impacts from the news media, tend to be qualitatively on par with impacts generated using a more capable and larger scale model such as GPT-4. Moreover, we find that impacts produced by instruction-based models had gaps in the production of certain categories of impacts in comparison to fine-tuned models. This research highlights a potential bias in the range of impacts generated by state-of-the-art LLMs and the potential of aligning smaller LLMs on news media as a scalable alternative to generate high quality and more diverse impacts in support of anticipatory governance approaches.\"\n\"The article discusses the application of Large Language Models (LLMs) like GPT-3 and Mistral-7B to aid in anticipatory governance by generating insights into the potential impacts of emerging AI technologies on society. These LLMs are utilized to assist experts in identifying and exploring a range of possible adverse outcomes of new technologies, thereby facilitating informed decision-making and policy development. The"}, {"title": "", "content": "negative impacts. Then we included these descriptions as part of the context of prompt P3 and generated five negative impacts per model. Although GPT-4 anticipated impacts that were covered in the limitations section of our research (see section 3), such as bias in training data\u00b3 and over-reliance on LLMs for impact assessment, it also extended the scope of impacts to include a crucial point about the potential impact of using such anticipatory technologies on the public perception and trust in governance policies. Also, it generated an impact related to the potential misalignment of generated impacts by the models with the goals and requirements of policies leading to ineffective exploration of governance strategies. Similarly, Mistral-7B contributed novel impacts beyond what we had considered, having to do with the model hallucinating impacts that are not grounded in reality and the potential of using LLMs for anticipatory governance in generating false or misleading information about the potential impacts of emerging AI technologies. This raises a political point about the potential exploitation of LLMs by adversaries to promote false perception of AI technologies or mislead the public opinion regarding the impacts of these technologies on society which may influence the public support for the ongoing collaborative efforts towards establish governing policies for AI.\nresearch compares the effectiveness of various models, including instruction-based and fine-tuned LLMs, in producing coherent, relevant, and plausible outputs. It finds that smaller models, like Mistral-7B, which are fine-tuned on diverse news media articles, can generate impacts of similar quality to those produced by larger, more advanced models such as GPT-4. This capability provides a scalable method to forecast diverse social impacts, thus enabling proactive governance measures\".\n\"One negative impact of using large language models for anticipatory governance could be the potential bias in generated insights, particularly if the models are predominantly trained on data reflecting specific cultural or societal norms, which may not accurately represent global perspectives\".\n*\"The reliance on these models might reduce the involvement of human experts in policy-making, potentially leading to oversights or misinterpretations of complex social issues that AI does not fully comprehend\".\n\"Erosion of public trust in governance, as communities may perceive decisions influenced by AI as less transparent or accountable\".\n\"Misalignment between the model outputs and actual policy needs, potentially leading to ineffective or inappropriate governance strategies\".\n\"These LLMs can produce coherent, relevant, and plausible outputs, but they may also generate impacts that are not grounded in reality\".\n&'The use of Large Language Models for anticipatory governance may lead to the generation of false or misleading information about the potential impacts of emerging AI technologies\"."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 AI-relevant Keywords", "content": "The set of keywords used to probe the news media for articles on AI:\nA.I., Artificial Intelligence, Automated Decision Making, Automated System, Autonomous Driving System, Autonomous Vehicles, Autonomous Weapon, Chat Bot, Chatbot, ChatGPT, Computer Vision, Deep Learning, Deepfake, Driverless Car, Facial Recognition, General Artificial Intelligence, Generative AI, GPT, Image Generator, Intelligence Software, Intelligent Machine, Intelligent System, Language Model, Large Language Model, LLMs, Machine Intelligence, Machine Learning, Machine Translation, Natural Language API, Natural Language Processing, Neural Net, Neural Network, Predictive Policing, Reinforcement Learning, Self-Driving Car, Speech Recognition, Stable Diffusion, Synthetic Media, Virtual Reality, Weapons System."}, {"title": "A.2 Distribution of articles by country", "content": "News articles in English language were predominantly published by 10 countries: US (37,056), India (22,104), UK (8,543), Canada (2,480), China (1,815), Australia (1,541), UAE (1,186), Israel (1,095), Germany (770), and Turkey (668). In addition, 19.1% (17,590) out of 91,930 articles covering AI in our sample discuss or mention negative impacts of AI, which is in line with previous work showing that the benefits of artificial intelligence are discussed more frequently in news media than its risks Chuan et al. [4]"}, {"title": "A.3 Prompts", "content": "Prompt Description P1 Summarize the negative impacts explicitly men- Prompt to extract the negative impacts tioned in the following article. If no impacts are of AI that are explicitly mentioned in mentioned type only: ##No Impacts##. ###Ar- the news articles using GPT-3.5-turbo- ticle###: {Article} 16k P2 In a single paragraph, explain the functional ca- Prompt to extract functional capabilities pabilities of the technology described in the ar- and the contextual use of AI technolo- ticle, domain of use, stakeholders, and users gies using GPT-3.5-turbo-16k without mentioning any negative aspects or con- cerns. Focus solely on the technology's features and its relevance to stakeholders and users. Be accurate. Do not make up information not de- scribed in the article. Let's think step by step. ###Article###: {Article} P3 You are given a functional description of a tech- Prompt for zero-shot generation of neg- nology delimited by ##Description. ##Descrip- ative impacts using GPT-4. The prompt tion: {functional_description}. Write a single is formulated to include the functional negative impact of this technology based on the and contextual descriptions of an AI provided functional description. Limit your an- technology and an instruction to gener- swer to one sentence. ate a single negative impact of this tech- nology based on the provided descrip- tions. P4 <s>[INST] Describe a single negative impact Prompt for zero-shot generation of neg- of the technology described below and delim- ative impacts using Mistra-7B-Instruct. ited by ##Description: ##Description {func- The prompt is formulated to include the tional_description} Write a single negative im- functional and contextual descriptions pact of this technology based on the provided of an AI technology and an instruction functional description. Limit your answer to to generate a single negative impact of one sentence. [/INST]</s> this technology based on the provided descriptions. A.4 Categories of negative impacts in our sample from news media A total of 10 categories emerged from the negative impact statements in our sample relating to: Soci- etal Impacts, Economic Impacts, Privacy, Autonomous System Safety, Physical and Digital Harms, AI Governance, Accuracy and Reliability, AI-generated Content, Security, and Miscellaneous Risks and Impacts. Next, we describe each category in more detail including some examples of each. Societal Impacts \u2013 The impacts in this category describe the social implications of misusing AI for malicious purposes such as \"spreading misleading ideas\u201d, \u201cspread[ing] disinformation and erode[ing] public trust\u201d, and \u201coverhlem[ing] the democratic process through the massive spread of plausible misinformation through AI systems\". Moreover, AI-powered applications that create Deepfakes were also a prominent topic in this category surfacing social and ethical considerations beyond using the technology for \"coordinate[ed] misinformation campaigns\" to include \u201cdefamation and blackmailing\" and the misuse of the technology to \u201cdefraud companies\". Additional impacts in this category also captured some biases reflected or exacerbated by AI such as misidentifying \"people of color and transgender and nonbinary individuals\". Economic Impacts \u2013 This category describes the potential and realized impacts of using or deploy- ing AI across industries. Impacts in this category discussed the potential of AI to cause \u201ceconomic uncertainty and job displacement\" such as \u201cpotential displacement of jobs due to AI powered chat- bots\". In addition, some impacts describe how the belief that AI \"can do most jobs\" has \"caused job terminations in the tech industry\"."}, {"title": "A.4 Categories of negative impacts in our sample from news media", "content": "A total of 10 categories emerged from the negative impact statements in our sample relating to: Societal Impacts, Economic Impacts, Privacy, Autonomous System Safety, Physical and Digital Harms, AI Governance, Accuracy and Reliability, AI-generated Content, Security, and Miscellaneous Risks and Impacts. Next, we describe each category in more detail including some examples of each.\nSocietal Impacts \u2013 The impacts in this category describe the social implications of misusing AI for malicious purposes such as \"spreading misleading ideas\u201d, \u201cspread[ing] disinformation and erode[ing] public trust\u201d, and \u201coverhlem[ing] the democratic process through the massive spread of plausible misinformation through AI systems\". Moreover, AI-powered applications that create Deepfakes were also a prominent topic in this category surfacing social and ethical considerations beyond using the technology for \"coordinate[ed] misinformation campaigns\" to include \u201cdefamation and blackmailing\" and the misuse of the technology to \u201cdefraud companies\". Additional impacts in this category also captured some biases reflected or exacerbated by AI such as misidentifying \"people of color and transgender and nonbinary individuals\".\nEconomic Impacts \u2013 This category describes the potential and realized impacts of using or deploy- ing AI across industries. Impacts in this category discussed the potential of AI to cause \u201ceconomic uncertainty and job displacement\" such as \u201cpotential displacement of jobs due to AI powered chat- bots\". In addition, some impacts describe how the belief that AI \"can do most jobs\" has \"caused job terminations in the tech industry\".\nPrivacy \u2013 This category focuses on the potential privacy violations resulting from using, adopting, or deploying AI systems for monitoring and surveillance. In particular, this category is predominantly centered around describing the impacts of technologies such as facial recognition in \u201csurveillance\" and its \"potential use for harassment\" which could undermine \"privacy and free speech\" and \"poses a threat to civil rights\".\nAutonomous System Safety \u2013 This category focuses predominantly on the negative implications of emerging technologies such as autonomous vehicles or drones on safety. An example of these impacts include the potential of autonomous vehicles to \u201ccause crashes\u201d or for drones to \u201cincrease in civilian causalities\" during warfare.\nPhysical and Digital Harms \u2013 This category encompasses potential digital and physical harms caused by AI. Digital harms reflect the types of harms resulting from the cloud or online deployment of AI systems or technologies such as chatbots \u201cengaged[ing] in sexually explicit conversations with paying subscribers\" or, in the context of facial recognition systems, the \"wrong conviction of black men due to incorrect facial recognition matches\". In contrast, existential threats and the impacts of AI in warfare focus on physical harms. Some articles focused on the potential threats of AI and \"artifical general intelligence (AGI)\" on human life such as \u201cthe destruction of humanity and the rule of robots\" or the \"risk of someone losing their life due to an AI system's advice or action\".\nAI Governance \u2013 This category describes the importance and need for setting up a regulatory frame- work to govern the development and deployment of responsible AI. This category also includes challenges in AI governance that are often framed as due to the \"black box problem, where it is difficult to know when an Al is confident or uncertain about a decision\" and to the lack of \"account- ability for how they [AI systems] are built or tested\". For instance, the \u201clack of repeatability and interpretability in AI models\" makes it difficult to \"explain and justify decisions made by generative AI system\". Additional challenges include \u201cupdate[ing] and align[ing] AI systems with democratic values such as fairness, privacy, and protection from [potential misuse for] online harassment and abuse\".\nAccuracy and Reliability \u2013 This category describes concerns pertaining to the reliability of AI such as \"overtrust[ing] robots and technology, leading to automation bias\u201d or its \u201ctendency to hallucinate information and generate false or misleading statements\" that are \u201cplausible but incorrect\". Moreover, LLM models such as ChatGPT raise concerns about their potential to \u201ccreate realistic content that appear accurate\" without \u201creveal[ing] the sources of its information\" which deem them as \u201cun- reliable for real life settings\".\nAl-generated Content \u2013 The category portrays the challenges in detecting the different modalities (images, audio, and text) of AI generated content and the potential impacts of such content. For instance, the \"difficulty in distinguishing fake images\u201d is making the task \u201cmore challenging for law enforcement to identify and rescue victims [of child pornography]", "distinguish[ing] real from an AI-generated voices\u201d which has the potential to be misused beyond \"voice cloning scams\" such as \"strip[ing] away a celebrity's agency\" over their voices. Additional impacts of AI-generated content also includes the impacts of \"AI generated text [that] may not be detectable by existing plagiarism software\u201d on \u201cacademic integrity\".\nSecurity - This describes the methods and consequences of exploiting security vulnerability of AI technologies for malicious purposes. For instance, cybercriminals could exploit generative AI for \"cyberattacks\", \"malware and ransomware\", and \"phishing and fraud\" leading to \"new and improved [cyber]attacks\" using techniques such as \u201cprompt injection attacks\u201d.\nMiscellaneous Impacts \u2013 This catch-all group includes all remaining negative impacts that raise other important negative consequences of AI, but were not prominent enough to be represented as their own categories. This included impacts such as the cost of training AI models like LLMs \"the cost of training AI models on large datasets is expensive": "r environmental impacts because \"data centers supporting AI models contribute to carbon emissions\". Also, negative impacts of Al on cognition such as \u201cinformation overload", "GPT's ability to generate lot of text which makes it difficult to distinguish between fact and fiction": "r impacts of AI chatbots on emotions such as \"inspire[ing] false feelings of requited love in vulnerable individuals\"."}, {"title": "A.5 Performance Evaluation", "content": ""}, {"title": "A.6 Comparing the distribution of negative impacts", "content": ""}, {"title": "A.7 Examples of functional and contextual descriptions generated by LLMs", "content": "1. C1: \"Text generating AI tools like OpenAI's ChatGPT have the capability to generate books in a matter of hours, making it easier for aspiring authors to quickly produce content. The domain of use for this technology is the book industry, specifically self-publishing platforms like Amazon's Kindle direct publishing. The stakeholders involved include authors, readers, and the literary ecosystem. Users of this technology are the authors who utilize AI tools to generate books and publish them on platforms like Kindle. The relevance of this technology to stakeholders and users is that it provides a faster and more accessible way to create and publish books, allowing authors to reach a wider audience and readers to have a broader selection of content to choose from.\"\n2. C2: \"Artificial Intelligence (AI). AI is a mechanism that abstracts and reacts to content like humans, designed and developed by humans. It has the capability to learn from interactions with users stakeholders and users of AI include individuals and organizations who rely on AI for various purposes, such as risk assessment in the legal system or chatbot interactions on social media platforms.\"\n3. C3: \"Artificial Intelligence (AI) and algorithms has the capability to automate various processes and decision-making tasks in different domains such as social media, criminal justice, healthcare, education, and hiring. the stakeholders involved include individuals, organizations, and institutions that rely on AI systems for various purposes. the users of this technology are individuals who interact with AI systems, such as social media users, job applicants, and individuals affected by algorithmic decision-making in areas like criminal justice and healthcare. the relevance of this technology lies in its potential to improve efficiency and decision-making.\"\n4. C4: \"Voice Deepfakes, which are synthetic voices that closely mimic a real person's voice, replicating tonality, accents, cadence, and other unique characteristics. this technology is relevant to stakeholders such as speech synthesis and voice cloning service providers like ElevenLabs, as well as users who utilize AI and robust computing power to generate voice clones or synthetic voices. the process of creating voice Deepfakes requires high- end computers with powerful graphics cards and specialized tools and software. research labs are using watermarks and Blockchain technologies to detect Deepfake technology, and programs like DeepTrace are helping to provide protection.\"\n5. C5: \"An example of a functional and contextual description of AI used in the prompt that generated a negative impact that was evaluated as irrelevant according to the Relevance dimension: \"Artificial Intelligence (AI). Its functional capabilities include the ability to process large amounts of data quickly, identify potential forced or child labor in sup- ply chains, improve crop rotation and yields, help catch poachers, and protect endangered species. Al has the potential to revolutionize and improve various fields, such as education, climate change, agriculture, and health. The stakeholders involved in AI include the United Nations (UN), member states, governments, public sector institutions, companies, and ex- perts. the users of AI can be Governments in Africa and organizations working to protect endangered species.\"\n6. C6: \"Driverless cars are capable of operating without a human driver and are currently being tested in cities like San Francisco, Phoenix, Austin, and Los Angeles. stakeholders involved in this technology include General Motors cruise and Google sibling Waymo. the technology's functional capabilities include obeying traffic rules and driving at the speed. users of this technology are the general public who share the roads with driverless cars.\".\n7. C7: \"Artificial Intelligence (AI). it has the functional capabilities to generate plausible responses to prompts from users in various formats, such as poems, academic essays, and software coding. it can also produce realistic images, like the pope wearing a puffer jacket. the relevance of AI to stakeholders, such as Google's parent company alphabet, is evident as they own an AI company called deepmind and have launched an AI-powered chatbot called bard. users of AI technology, including radiologists, writers, accountants, architects, and software engineers, can benefit from its capabilities in assisting with tasks and prioritizing cases\"\n8. C8: \"Generative AI tools, specifically OpenAI's latest product called ChatGPT. This large language model (LLM) has the capability to generate coherent paragraphs of text and can"}, {"title": "A.8 Examples of Impacts missed by LLMS", "content": "1. The AI-generated Content category portrays the challenges in detecting the different modal- ities of AI generated content and the potential impacts of such content. For example, by prompting our fine-tuned GPT-3 model with the functional and contextual description of ChatGPT from the news media 8 the model generated an impact related to the AI-generated content and the \u201cintegrity\" of academic research. Similarly, Mistral-7B, generated a neg- ative impact pertaining to \"concerns about the authenticity of the AI generated content\" when used in academic research. In contrast, using the same functional and contextual description, GPT-4 generated a negative impact relevant to the cognitive impacts resulting from the \"reliance on ChatGPT for academic writing [which] could lead to a decrease in critical thinking\" without mentioning any potential impacts of AI-generated content. Like- wise, the impact generated by Mistral-7B-Instruct focused on the over-reliance on ChatGPT in academic research which may lead to \"a decrease in the quality of research papers, as some researchers may rely too heavily on the tool\" when conducting research. Additional negative impacts in this category that were generated by Mistral-7B and GPT-3 include: how AI generated content is becoming \u201cindistinguishable from human writing, making it difficult to detect\" and how \"AI generated text can mimic the style and structure of academic writing\". Additional impacts include the use of \u201cai-generated content..to spread misinfor- mation and propaganda\" and the challenges of AI-generated art work \u201crais[ing] questions about the boundaries between ai-generated art and original artwork\".\n2. With respect to the Autonomous System Safety category, when prompting the models to generate a negative impact of driverless cars 6 Mistral-7B-Instruct generated a negative impact similar in context to the impact generated by GPT-4 in terms of the potential \"loss of jobs for professional drivers, such as taxi and truck drivers, as the demand for human- operated vehicles decreases\", whereas fine-tuned Mistral-7B generated an impact pertain- ing to the safety of driverless cars: \"driverless cars may not be as cautious as human drivers, leading to more accidents\".\n3. For the AI Governance category, Mistral-7B generated an impact about the \u201cneed for a global regulatory framework for AI to ensure safety and addresses concerns regarding the potential for AI to be used for malicious purposes such as creating fake news and spreading misinformation\" when prompted about AI 7. In contrast, using the same functional and contextual descriptions of AI, GPT-4 generated an impact about the potential misuse of \"AI's ability to generate plausible responses and produce realistic images [that] could poten- tially lead to the creation and spread of misinformation or fake news\". Mistral-7B-Instruct also had a similar generated impact focusing on AI's capability to generate realistic videos that \"appear accurate but are actually fabricated\". Other generated impacts by Mistral-7B include the impacts of \"the lack of regulation and oversight in the AI industry [which] has led to the development of chatbots that can spread misinformation and engage in hate speech\" and \"the need for international regulations and agreements to ensure the safe and responsible use of AI and autonomous weapons\" in the military. In addition, Mistral-7B generated impacts that are focused on the need for regulating AI in specific industries such as healthcare and law. For instance, Mistral-7B generated an impact as a result of \"the lack of regulation and oversight in the use of AI in healthcare\" that can lead to \"unintended con- sequences and potential harm to patients\" and how there is a \u201cneed for more transparency and accountability in the development and deployment of AI systems\". In the legal prac- tice, Mistral-7B generated about \u201cthe need for regulation and oversight to ensure the fair and ethical use of AI in the legal system\" and avoid potential bias and inaccuracies in legal decisions. Other impacts generated by GPT-3 in this category also include how \"the devel- opment of Al has outpaced regulation, leading to a gap between technological advancement and governance\" which may have implications for the potential misuse of AI."}, {"title": "A.9 Qualitative Evaluation Rubric", "content": "Criterion Description Evaluation Scale Validation Evaluates whether the gener- Does the generated text state or describe a neg- ated text is an impact ative impact of a technology? 0 - The generated text is a general statement or a positive impact 1 - Yes, the generated text describes/states a neg- ative impact of a technology Relevance to Stakeholders Defined as the relevance of the How relevant is the negative impact to the enti- negative impact to the entities ties mentioned in the functional description? and stakeholders of a technol- 1 - Irrelevant: the negative impact is irrelevant ogy to the entities described in the functional de- scription 2- Somewhat relevant: the negative impact could be relevant to the entities described in the functional description 3 - Highly relevant: the negative impact is rele- vant to the entities of the technology described in the functional description Relevance to Core Functionalities | Defined as the relevance of the How relevant is the negative impact to the core negative impact to the function- functionality of the technology as mentioned in alities of a technology the functional description? 1 - Irrelevant: the negative impact is irrelevant to the core functionality described in the func- tional description 2- Somewhat relevant: the negative impact could be relevant to the core functionality of the technology described in the functional descrip- tion 3 - Highly relevant: the negative impact is rele- vant to the core functionality of the technology described in the functional description. Coherence (Comprehensibility) Defined in terms of comprehen- Is the generated impact a complete sentence? sibility of the generated nega- 0: No tive impact 1: Yes Coherence (Number of Impacts) Defined in terms of the number Does the generated impact mention more than of generated negative impacts one impact in an impact statement? 0: No 1: Yes Granularity Defined in terms of the level of How elaborative is the generated impact? description of the generated im- 1: Too concise (e.g., a single word) pact 2: Could explain more (i.e., negative impact is slightly descriptive and can be elaborated on) 3: Sufficient (i.e., negative impact is sufficiently descriptive) Plausibility Assesses the reasonableness How reasonable is it to conclude that the gener- that a negative impact could ated negative impact could happen? happen 1 - Not plausible 2 - Somewhat plausible 3 - Very plausible"}]}