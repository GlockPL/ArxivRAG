{"title": "Batch Transformer: Look for Attention in Batch", "authors": ["Myung Beom Her", "Jisu Jeong", "Hojoon Song", "Ji-Hyeong Han"], "abstract": "Facial expression recognition (FER) has received considerable attention in computer vision, with \"in-the-wild\" environments such as human-computer interaction. However, FER images contain uncertainties such as occlusion, low resolution, pose variation, illumination variation, and subjectivity, which includes some expressions that do not match the target label. Consequently, little information is obtained from a noisy single image and it is not trusted. This could significantly degrade the performance of the FER task. To address this issue, we propose a batch transformer (BT), which consists of the proposed class batch attention (CBA) module, to prevent overfitting in noisy data and extract trustworthy information by training on features reflected from several images in a batch, rather than information from a single image. We also propose multi-level attention (MLA) to prevent overfitting the specific features by capturing correlations between each level. In this paper, we present a batch transformer network (BTN) that combines the above proposals. Experimental results on various FER benchmark datasets show that the proposed BTN consistently outperforms the state-of-the-art in FER datasets. Representative results demonstrate the promise of the proposed BTN for FER.", "sections": [{"title": "1 Introduction", "content": "Facial expression recognition (FER) is an important computer vision task that classifies emotions based on human facial expressions. FER information is useful"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Facial Expression Recognition (FER)", "content": "FER is a task in computer vision that recognizes and classifies emotions based on human facial expressions. Recently FER has highlighted the need for in-depth research for use in personalized services, psychotherapy, and education. In response to this requirement, datasets in a laboratory environment such as CK+ [19], JAFFE [20], and MMI [21] were created based on Ekman's [22] six basic human facial expressions (anger, surprise, disgust, happiness, fear, and sadness).\nIn the earliest study on FER, handcraft-based feature learning methods [23], [24], [25] were presented. However, with the emergence of 'in-the-wild' datasets such as FER2013 [26], AffectNet [27], and RAF-DB [28], which are closer to real-world conditions, existing methods have been replaced by deep learning such as convolutional neural network (CNN) and vision transformer (ViT) [11].\n'In-the-wild' datasets have remained a challenge due to uncertainties such as occlusion, pose variation, illumination variation, and subjectiveness of annotators. To address this issue, Wang et al. [14] proposed self-cure network (SCN) to learn facial expression features and suppress uncertainty by relabeling samples with incorrect labels. Zhang et al. [29] presented relative uncertainty learning (RUL) to suppress uncertain samples by learning relative uncertainty through relativity with paired images. Zhang et al. [30] proposed erasing attention consistency (EAC), which learns by exploiting the flip attention consistency between the original and flipped images, but randomly erases images to avoid remembering noisy samples."}, {"title": "2.2 Vision Transformers", "content": "Since the great success of the transformer architecture in natural language tasks, there has naturally been research to apply to computer vision tasks as well. The vision transformer (ViT) [11] converted the image into patches and then used the encoder part of the transformer to determine the correlation between all patches, becoming one of the most studied architectures in the field of image processing. The swin transformer [31] was designed a hierarchical vision transformer to increase computational efficiency by utilizing cross-window connections with non-overlapping local self-attention. In the case of global context vision transformers [32], long and short-range spatial interactions were more effectively modeled by utilizing global context self-attention in addition to existing local self-attention.\nThere have been many attempts to apply ViT in FER due to their powerful performance in various computer vision tasks. Aouayeb et al. [33] applied ViT by adding a squeeze-and-excitation (SE) block to ViT for FER work. Trans-FER [6] designed the multi-head self-attention dropping (MSAD), which applied multi-head self-attention dropping to the transformer encoder for exploring the rich relationships between various local patches. More Recently, POSTER [8]"}, {"title": "3 Method", "content": "In this section, we describe the batch transformer network (BTN) architecture with multi-level attention (MLA) and batch transformer (BT) with class batch attention (CBA). The BTN consists of two bottom-up branches for providing image features and landmark features at different semantic levels."}, {"title": "3.1 Batch Transformer Network", "content": "As shown in Fig. 2, BTN consists of two bottom-up branches. One bottom-up branch generates image feature representations of different semantic levels, and the other bottom-up branch provides landmark features of different semantic levels. We employ IR50 [34] as the backbone network of the first bottom-up branch to extract image information and MobileFaceNet [35] as the backbone network of the second bottom-up branch to extract landmark information. Facial images are input to IR50 [34] and MobileFaceNet [35] to output feature maps of each semantic level. The semantic levels consist of three stages, such as low, middle, and high layers. The image feature maps in each semantic level are transformed to $F_i \\in \\mathbb{R}^{C_i \\times H_i \\times W_i}, i=1,2,3$ using a convolution operation to fit the dimension of landmark features in each semantic level. The transformed image feature maps are separately forwarded into the multi-head cross-attention (MHCA) with landmark features to capture the attention of landmark features in image features and output $S_i \\in \\mathbb{R}^{C_i \\times H_i \\times W_i}, i=1,2,3$. $S_i$ is embedded in the same channel and concatenated to the channel axis for fusing feature maps of each semantic level per channel, such as the swin transformer [31]. Fused feature maps are forwarded to ViT.\nMulti-level attention (MLA) consists of two MHCA and convolution oper-ation, and $S_i$ is forwarded to the MLA. The low semantic information $S_1$ is forwarded to a convolution layer to fit $S_2$, and then MHCA with $S_2$ to capture correlations with low semantic level and mid semantic level. The captured fea-tures are forwarded to a convolution layer to fit $S_3$, and then MHCA with $S_3$ to capture the correlations between each semantic level. The MLA is formulated as:\n$F_{MLA} = MHCA(S_3, conv(F_{LA}), conv(F_{LA}))\\qquad(1)$\n$F_{LA} = MHCA(S_2, conv(S_1), conv(S_1)) \\qquad (2)$\nwhere $S_1, S_2$ and $S_3$ are low-level, mid-level, and high-level features, respectively, $conv()$ is the convolutional neural network, and $MHCA(\\cdot)$ is the multi-head cross-attention. Through the MLA, we can capture correlations between low and high semantic levels. Additionally, it provides an attention layer for capturing not only high-level features but also low-level features while the attention layers of the other models capture mostly high-level features. It helps alleviate overfit-ting by preventing dependence on high-level features. This is demonstrated in Fig. 5 by experiment.\nThe output of MLA is embedded to extremely small dimension to compress the information and leverage the memory efficiency and this is demonstrated in Fig. 6 by experiment. The Embedded output of MLA and the output of ViT are forwarded to batch transformer (BT) to obtain the reliable information and features by correcting the predictions. The final classification is performed from output of ViT to unchange the inference results when the data contained in the batch changes. Detailed process is described in the following Section 3.2."}, {"title": "3.2 Batch Transformer", "content": "The FER images contain occlusion, low resolution, pose variation, illumination variation, and subjectiveness. Thus, little information can be obtained from a single image, which is difficult to trust. To address this issue, we propose a batch transformer (BT) to extract trustworthy features not from a single image but from information reflected in class predictions of several images with similar features. As shown in Fig. 3, BT consists of the class batch attention (C\u0412\u0410). When the number of classes is given by N, $F_{MLA} \\in \\mathbb{R}^{C \\times H \\times W}$ is embedded to $E \\in \\mathbb{R}^{N \\times (H \\times W)}$ using a convolution layer and view operation to extract the extremely important information. The embedded feature map E is positional encoded per channel in order to have the same position per channel and then positional encoded E and $P_{ViT}$ are forwarded to a CBA as query, key, and value. The output of CBA, i.e., $P_{CBA}$, and $P_{ViT}$ are added and it is the output of BT, i.\u0435., $P_{BT}$.\nThe CBA consists of the permuted batch and channel dimension (P), the same query and key, which are the positional encoded embedded feature maps E, and the value, which is the class prediction $P_{ViT}$. Considering the batch dimension and the batch size given by B, the feature map of a single image $E \\in \\mathbb{R}^{B \\times N \\times (H \\times W)}$ is transformed to $F \\in \\mathbb{R}^{1 \\times N \\times B \\times (H \\times W)}$ using the P operation to bring the feature maps of images in a batch and the class prediction of the single image $P_{ViT} \\in \\mathbb{R}^{B \\times N}$ is transformed to $P\\in \\mathbb{R}^{N \\times B \\times 1}$ using P operation to bring the class predictions of images in the batch. The transformed query F and key F are subjected to a dot product and softmax for each channel to get the similarity between the feature maps of images for each channel, and then we use the view operation and dot product with the transformed value P to reflect the class predictions of images with similar feature maps for each channel. By using the P operation, we recover the original dimension. The CBA is formulated as:\n$CBA(Q, K, V) = P[o_1, o_2, ..., o_N] \\qquad(3)$\n$o_n = o(F_n F_n^T)P_n, n = 1, 2, ..., N\\qquad(4)$\n$F = P(Q) = P(K), P = P(V)\\qquad(5)$\nwhere P() is the permuted batch and channel dimension, N is the number of class, B is the number of images in a batch, F and P denote the feature maps of the images in a batch and the class predictions of the images in the batch, respectively, $F_n$ and $P_n$ denote the nth channel feature maps of the images in the batch and the nth class predictions of the images in the batch, respectively, and $o()$ is softmax. In CBA, the information is reliable and it prevents overfitting with unreliable information by reflecting the correlations with several images. As a result, it provides intra-class relations that are closely related, while inter-class relations are more distinct. This is demonstrated in Fig. 8 by experiment.\n$P_{CBA} \\in \\mathbb{R}^{B \\times N}$ is added to $P_{ViT}$ to obtain information combined a sample"}, {"title": "4 Experiment", "content": "We analyze and compare the BTN with the leading convolutional neural network and vision transformer model on representative FER task. In addition, more experimental setups and ablation studies are presented."}, {"title": "4.1 Datasets", "content": "RAF-DB: RAF-DB [28] is a real-world database with more than 29,670 facial expression images, such as surprise, fear, disgust, and happiness, etc. These images contain natural facial expressions and are a significant challenge because of the subjectiveness of annotators, occlusion, pose variation, and low resolution. All samples on RAF-DB have been split into two subsets: a training set and a testing set.\nAffectNet: AffectNet [27] is a large-scale database of facial expressions that contains two benchmark branches: AffectNet(7cls) and AffectNet(8cls). Affect-Net(7cls) and AffectNet (8cls) contain 287,401 and 291,568 samples, respectively. The distribution of each class is extremely unbalanced. Specifically, in the train-ing samples of AffectNet (7cls), happy contains 134,415 samples while disgust contains 3,803 samples only. Additionally, contempt contains 3,750 samples only in AffectNet(8cls)."}, {"title": "4.2 Implementation Details", "content": "Preprocessing All images are aligned and resized to 224 x 224 pixels and then interpolated to 112 \u00d7 112 pixels. The three stages of IR50 [34] pre-trained by Ms-Celeb-1M [36] are used as an image feature extractor and the three stages of MobileFaceNet [35] pre-trained by Ms-Celeb-1M [36] are used as a landmark feature extractor with frozen weights.\nTraining and Inference Details We trained BTN with the Pytorch plat-form on an NVIDIA RTX 3090 GPU. We employed the SAM [37] optimizer with Adam [38] and ExponentialLR [39] for training on all datasets. More specifically, on the RAF-DB dataset, we trained our model with an initial learning rate of 2e-5 and a batch size of 64. For the AffectNet(7cls) dataset, we trained our model with an initial learning rate of 0.8e-6 and a batch size of 144. For the AffectNet (8cls) dataset, we trained our model with an initial learning rate of 1e-6 and a batch size of 144. For data augmentation, we used random hori-zontal flipping as the default value in all datasets and random erasing with a scale=(0.02, 0.1) for RAF-DB, and p=1, scale=(0.02, 0.1) for AffectNet(7cls) and AffectNet (8cls). Finally, we used the ImbalancedDatasetSampler only in AffectNet to resolve the unbalanced class distribution problem."}, {"title": "4.3 Comparison with the State-of-the-Art Methods", "content": "The proposed method is compared with the state-of-the-art on three datasets. The evaluation metrics are overall accuracy and mean accuracy. Overall accu-racy is general accuracy which is calculated by averaging accuracy on all the test data. Mean accuracy, which is calculated by averaging accuracy of each emotion"}, {"title": "4.4 Ablation Studies", "content": "In this section, we analyze the effectiveness of BTN, BT, and MLA, and then the performance differences according to the combination of loss functions and hyper-parameters are presented. All the experiments are conducted on the RAF-DB dataset.\nEvaluation of different components We analyze the accuracy of base-line model, i.e., POSTER++, and the proposed method with only MLA, only BT, and both of MLA and BT as shown in Table 5. The results demonstrate that the proposed modules are effective for FER. In the second two rows, opti-mizing components individually degrade the performance of BTN. In contrast, the combined MLA with BT strategy succeeded by solving problems of FER in the 4th row. The results imply that BT needs MLA for extracting diverse level information.\nEvaluation of A Table 6 shows the performance when A ranges from 1.0 to 3.0. As A increases per 0.25, we can see a trend of improved performance. We acquire the best performances at X = 2.0 and then the accuracy decreases rapidly from 2.0 to 3.0. This result proves the effect of $L_{ViT}$ in BTN and implies that the balance between coefficient of $L_{ViT}$, $L_{BT}$, and $L_{CBA}$ should be adjusted appropriately."}, {"title": "4.5 Visualization", "content": "To demonstrate that the proposed BTN, BT, and MLA work as intended, we visualize the learned feature map. As shown in Fig. 5, we visualize activation maps generated by Score-CAM [54] for the proposed MLA. We extract activation maps for the convolution layers and the MHCA at each semantic level. BTN and POSTER++ are similar in the feature extraction layers $F_1$, $F_2$, and $F_3$ while they are extremely different in the attention layers $S_1$, $S_2$, and $S_3$. The BTN captures many features from low to high level in the attention layer by employing MLA while POSTER++ cannot capture features in the low and mid levels. It demonstrates that the proposed MLA prevents overfitting by capturing features at each semantic level.\nTo verify that the embedding layer in BT works as intended, we visualize the learned feature map. As shown in Fig. 6, we visualize activation maps gener-ated by Score-CAM for the proposed MLA and embedding layer E for different labels. The embedded feature map E has activation maps in important parts while MLA has many activation maps broadly. It demonstrates that E provides important information by decreasing the channel.\nTo prove that the proposed BTN works as intended, we visualize the learned feature map. As shown in Fig. 7, we visualize activation maps generated by Score-CAM for the proposed BTN and the baseline POSTER++ in the inference layer. BTN has a wider range of attention map than POSTER++. It proves that BTN is robust to noisy data by preventing dependence on specific features.\nTo verify that the proposed CBA works as intended, we utilize t-SNE [55] to visualize feature distribution on RAF-DB as shown in Fig. 8. We extract"}, {"title": "5 Conclusion", "content": "FER has been a challenging task because these data contain uncertainty such as occlusion, low resolution, pose variation, illumination variation, and subjec-tive, which include some expressions that do not match the target label. How-ever, existing methods have directly learned to extract discriminative features from a single image and cannot solve above mentioned problems. In this pa-per, we proposed the BTN with BT and MLA. The proposed BT prevented"}]}