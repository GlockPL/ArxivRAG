{"title": "Batch Transformer: Look for Attention in Batch", "authors": ["Myung Beom Her", "Jisu Jeong", "Hojoon Song", "Ji-Hyeong Han"], "abstract": "Facial expression recognition (FER) has received considerable atten-\ntion in computer vision, with \"in-the-wild\" environments such as human-\ncomputer interaction. However, FER images contain uncertainties such as\nocclusion, low resolution, pose variation, illumination variation, and sub-\njectivity, which includes some expressions that do not match the target\nlabel. Consequently, little information is obtained from a noisy single im-\nage and it is not trusted. This could significantly degrade the performance\nof the FER task. To address this issue, we propose a batch transformer\n(BT), which consists of the proposed class batch attention (CBA) module,\nto prevent overfitting in noisy data and extract trustworthy information by\ntraining on features reflected from several images in a batch, rather than\ninformation from a single image. We also propose multi-level attention\n(MLA) to prevent overfitting the specific features by capturing correla-\ntions between each level. In this paper, we present a batch transformer\nnetwork (BTN) that combines the above proposals. Experimental results\non various FER benchmark datasets show that the proposed BTN consis-\ntently outperforms the state-of-the-art in FER datasets. Representative\nresults demonstrate the promise of the proposed BTN for FER.", "sections": [{"title": "1 Introduction", "content": "Facial expression recognition (FER) is an important computer vision task that\nclassifies emotions based on human facial expressions. FER information is useful\nfor computer vision applications such as social robots, human-computer interac-\ntion, and mental health monitoring. FER has been actively studied and shown\npromising results based on convolutional neural network (CNN) [1], [2], [3], [4],\n[5]. However, existing methods have shown limitations in generalization ability.\nTherefore, researchers in FER are considering using a model based on the vision\ntransformer (ViT) [6], [7], [8], [9], [10]. ViT, proposed for image classification,\ndemonstrates excellent performance with the self-attention mechanism in the\nimage processing tasks [11]. The attention mechanism, with the correlation be-\ntween the patches, is the key reason for using ViT in FER [6]. The most crucial\naspect is to exclude human identity-information as much as possible and to fo-\ncus on the information involved in facial expressions (e.g., eyes, nose, mouth,\netc.) in FER [12]. Attention allows for effective learning of emotional features\nby focusing on specific local areas in ViT.\nHowever, 'in-the-wild' datasets collected from the internet still pose difficul-\nties in emotion classification by the uncertainties of facial images. Uncertain-\nties can degrade the quality of facial expressions in the image. According to\nKiureghian et al. [13], uncertainties can be divided into aleatoric uncertainty,\nwhich is data uncertainty, and epistemic uncertainty, which is model uncertainty.\nSpecifically, data uncertainty in FER arises from ambiguous facial expressions\nand the subjectivity of annotators, preventing the acquisition of reliable infor-\nmation from facial expression images [14]. Some of the uncertainties in FER\nfrequently occur due to occlusion, low resolution, pose, illumination, and demo-\ngraphic variations. Occlusion obscures facial expressions by covering or revealing\nonly part of the face. The low resolution of the face image makes it difficult to\nrecognize the emotions. Additionally, pose, illumination, and demographic vari-\nations cause facial expressions to appear different depending on the direction,\nbrightness, color of light, and even race, gender, and age.\nRecently, attention mechanism models have been applied to suppress un-\ncertainties and identify meaningful regions in facial expressions. PG-CNN [15]\ndevelops a patch-gated CNN that integrates path-level attention to focus on oc-\nclusion, thus it distinguishes between occluded and unoccluded regions in FER.\nDSAN [16] also adopts attention to address demographic variation. These mod-\nels are focused on addressing uncertainties to improve performance. However,\nthese previous attention models still exhibit low performance and struggle to\nreduce uncertainty.\nTo address the problem of FER, in this paper, we propose class batch at-\ntention (CBA). As shown in Fig. 1 (a), existing attention mechanisms such as\nmulti-head self-attention (MHSA) train only one sample, and then extract fea-\ntures from a single image. Many datasets have a small percentage of uncertain\ndata but may occasionally have data with unreliable information. Even a few of\nthese noisy samples will significantly degrade the performance of FER. More-\nover, the FER dataset contains a large amount of uncertain data, which leads\nto more overfitting problems. In contrast, as shown in Fig. 1 (b), our CBA can\naffect each data point by providing the predictions of emotion classes. It pro-\nvides trustworthy information by reflecting the class prediction of images with\nsimilar feature maps. Additionally, it ensures that intra-class is closely related"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Facial Expression Recognition (FER)", "content": "FER is a task in computer vision that recognizes and classifies emotions based\non human facial expressions. Recently FER has highlighted the need for in-\ndepth research for use in personalized services, psychotherapy, and education.\nIn response to this requirement, datasets in a laboratory environment such as\nCK+ [19], JAFFE [20], and MMI [21] were created based on Ekman's [22] six\nbasic human facial expressions (anger, surprise, disgust, happiness, fear, and\nsadness).\nIn the earliest study on FER, handcraft-based feature learning methods [23],\n[24], [25] were presented. However, with the emergence of 'in-the-wild' datasets\nsuch as FER2013 [26], AffectNet [27], and RAF-DB [28], which are closer to\nreal-world conditions, existing methods have been replaced by deep learning\nsuch as convolutional neural network (CNN) and vision transformer (ViT) [11].\n'In-the-wild' datasets have remained a challenge due to uncertainties such as oc-\nclusion, pose variation, illumination variation, and subjectiveness of annotators.\nTo address this issue, Wang et al. [14] proposed self-cure network (SCN) to learn\nfacial expression features and suppress uncertainty by relabeling samples with\nincorrect labels. Zhang et al. [29] presented relative uncertainty learning (RUL)\nto suppress uncertain samples by learning relative uncertainty through relativ-\nity with paired images. Zhang et al. [30] proposed erasing attention consistency\n(EAC), which learns by exploiting the flip attention consistency between the\noriginal and flipped images, but randomly erases images to avoid remembering\nnoisy samples."}, {"title": "2.2 Vision Transformers", "content": "Since the great success of the transformer architecture in natural language tasks,\nthere has naturally been research to apply to computer vision tasks as well. The\nvision transformer (ViT) [11] converted the image into patches and then used\nthe encoder part of the transformer to determine the correlation between all\npatches, becoming one of the most studied architectures in the field of im-\nage processing. The swin transformer [31] was designed a hierarchical vision\ntransformer to increase computational efficiency by utilizing cross-window con-\nnections with non-overlapping local self-attention. In the case of global context\nvision transformers [32], long and short-range spatial interactions were more ef-\nfectively modeled by utilizing global context self-attention in addition to existing\nlocal self-attention.\nThere have been many attempts to apply ViT in FER due to their power-\nful performance in various computer vision tasks. Aouayeb et al. [33] applied\nViT by adding a squeeze-and-excitation (SE) block to ViT for FER work. Trans-\nFER [6] designed the multi-head self-attention dropping (MSAD), which applied\nmulti-head self-attention dropping to the transformer encoder for exploring the\nrich relationships between various local patches. More Recently, POSTER [8]"}, {"title": "3 Method", "content": "In this section, we describe the batch transformer network (BTN) architecture\nwith multi-level attention (MLA) and batch transformer (BT) with class batch\nattention (CBA). The BTN consists of two bottom-up branches for providing\nimage features and landmark features at different semantic levels."}, {"title": "3.1 Batch Transformer Network", "content": "As shown in Fig. 2, BTN consists of two bottom-up branches. One bottom-up\nbranch generates image feature representations of different semantic levels, and\nthe other bottom-up branch provides landmark features of different semantic\nlevels. We employ IR50 [34] as the backbone network of the first bottom-up\nbranch to extract image information and MobileFaceNet [35] as the backbone\nnetwork of the second bottom-up branch to extract landmark information. Facial\nimages are input to IR50 [34] and MobileFaceNet [35] to output feature maps\nof each semantic level. The semantic levels consist of three stages, such as low,\nmiddle, and high layers. The image feature maps in each semantic level are\ntransformed to $F_i \\in \\mathbb{R}^{C_i \\times H_i \\times W_i}, i = 1,2,3$ using a convolution operation to\nfit the dimension of landmark features in each semantic level. The transformed\nimage feature maps are separately forwarded into the multi-head cross-attention\n(MHCA) with landmark features to capture the attention of landmark features\nin image features and output $S_i \\in \\mathbb{R}^{C_i \\times H_i \\times W_i}, i = 1,2,3$. $S_i$ is embedded in the\nsame channel and concatenated to the channel axis for fusing feature maps of\neach semantic level per channel, such as the swin transformer [31]. Fused feature\nmaps are forwarded to ViT.\nMulti-level attention (MLA) consists of two MHCA and convolution oper-\nation, and $S_i$ is forwarded to the MLA. The low semantic information $S_1$ is\nforwarded to a convolution layer to fit $S_2$, and then MHCA with $S_2$ to capture\ncorrelations with low semantic level and mid semantic level. The captured fea-\ntures are forwarded to a convolution layer to fit $S_3$, and then MHCA with $S_3$ to\ncapture the correlations between each semantic level. The MLA is formulated\nas:\n$F_{MLA} = MHCA(S_3, conv(F_{LA}), conv(F_{LA}))$\n(1)\n$F_{LA} = MHCA(S_2, conv(S_1), conv(S_1))$\n(2)\nwhere $S_1, S_2$ and $S_3$ are low-level, mid-level, and high-level features, respectively,\n$conv(\\cdot)$ is the convolutional neural network, and $MHCA(\\cdot)$ is the multi-head\ncross-attention. Through the MLA, we can capture correlations between low and\nhigh semantic levels. Additionally, it provides an attention layer for capturing\nnot only high-level features but also low-level features while the attention layers\nof the other models capture mostly high-level features. It helps alleviate overfit-\nting by preventing dependence on high-level features. This is demonstrated in\nFig. 5 by experiment.\nThe output of MLA is embedded to extremely small dimension to compress\nthe information and leverage the memory efficiency and this is demonstrated in\nFig. 6 by experiment. The Embedded output of MLA and the output of ViT\nare forwarded to batch transformer (BT) to obtain the reliable information and\nfeatures by correcting the predictions. The final classification is performed from\noutput of ViT to unchange the inference results when the data contained in the\nbatch changes. Detailed process is described in the following Section 3.2."}, {"title": "3.2 Batch Transformer", "content": "The FER images contain occlusion, low resolution, pose variation, illumination\nvariation, and subjectiveness. Thus, little information can be obtained from a\nsingle image, which is difficult to trust. To address this issue, we propose a batch\ntransformer (BT) to extract trustworthy features not from a single image but\nfrom information reflected in class predictions of several images with similar\nfeatures. As shown in Fig. 3, BT consists of the class batch attention (C\u0412\u0410).\nWhen the number of classes is given by N, $F_{MLA} \\in \\mathbb{R}^{C \\times H \\times W}$ is embedded to\n$E \\in \\mathbb{R}^{N \\times (H \\times W)}$ using a convolution layer and view operation to extract the\nextremely important information. The embedded feature map E is positional\nencoded per channel in order to have the same position per channel and then\npositional encoded $E$ and $P_{ViT}$ are forwarded to a CBA as query, key, and\nvalue. The output of CBA, i.e., $P_{CBA}$, and $P_{ViT}$ are added and it is the output\nof BT, i.\u0435., $P_{BT}$.\nThe CBA consists of the permuted batch and channel dimension (P), the\nsame query and key, which are the positional encoded embedded feature maps\nE, and the value, which is the class prediction $P_{ViT}$. Considering the batch\ndimension and the batch size given by B, the feature map of a single image $E \\in E \\in$\n$\\mathbb{R}^{B \\times N \\times (H \\times W)}$ is transformed to $F \\in \\mathbb{R}^{1 \\times N \\times B \\times (H \\times W)}$ using the P operation\nto bring the feature maps of images in a batch and the class prediction of the\nsingle image $P_{ViT} \\in \\mathbb{R}^{B \\times N}$ is transformed to $P \\in \\mathbb{R}^{N \\times B \\times 1}$ using P operation to\nbring the class predictions of images in the batch. The transformed query $F$ and\nkey $F$ are subjected to a dot product and softmax for each channel to get the\nsimilarity between the feature maps of images for each channel, and then we use\nthe view operation and dot product with the transformed value $P$ to reflect the\nclass predictions of images with similar feature maps for each channel. By using\nthe P operation, we recover the original dimension. The CBA is formulated as:\n$CBA(Q, K, V) = P[o_1, o_2, ..., o_N]$\n(3)\n$o_n = \\sigma(F_nF_n^T)P_n, n = 1, 2, ..., N$\n(4)\n$F = P(Q) = P(K), P = P(V)$\n(5)\nwhere P() is the permuted batch and channel dimension, N is the number of\nclass, B is the number of images in a batch, $F$ and $P$ denote the feature maps\nof the images in a batch and the class predictions of the images in the batch,\nrespectively, $F_n$ and $P_n$ denote the nth channel feature maps of the images in the\nbatch and the nth class predictions of the images in the batch, respectively, and\n$\\sigma()$ is softmax. In CBA, the information is reliable and it prevents overfitting\nwith unreliable information by reflecting the correlations with several images. As\na result, it provides intra-class relations that are closely related, while inter-class\nrelations are more distinct. This is demonstrated in Fig. 8 by experiment.\n$P_{CBA} \\in \\mathbb{R}^{B \\times N}$ is added to $P_{ViT}$ to obtain information combined a sample\nwith trustworthy information. Finally, the BT is formulated as:\n$P_{BT} = P_{CBA} + P_{ViT}$\n(6)\n$P_{CBA} = CBA(CP(E), CP(E), P_{ViT})$\n(7)\n$E = conv(F_{MLA})$\n(8)\nwhere $F_{MLA}$ is output of MLA, which is feature map, $conv(\\cdot)$ is a convolution\nlayer, $CP(\\cdot)$ is channel positional encoding, $CBA(\\cdot)$ is class batch attention,\n$P_{ViT}$ and $P_{CBA}$ are outputs of ViT and CBA, respectively.\nBy using multiple predictions as target of loss, we can learn the features on\ninformation about not only a single image but also several images. This helps\nprevents overfitting to noisy and untrustworthy information. This is demon-\nstrated in Fig. 4 by experiment. Note that the $P_{ViT}$ is the final prediction,\nCBA and BT helps not in inference but in training. If the final prediction is\ndone from a classifier after the CBA, the same mechanism will work for infer-\nence. Thus, if data in the batch change during inference, the prediction will\nchange. Therefore, you must not perform inference after any batch attention\nmechanism. The final loss is formulated as:\n$L = \\lambda L_{ViT} + L_{BT} + L_{CBA}$\n(9)\nwhere $L_{ViT}, L_{BT}, L_{CBA}$ are losses applied to $P_{ViT}, P_{BT}$, and $P_{CBA}$, respec-\ntively, using cross-entropy (CE). Through these losses, we can extract features\nfrom information combined samples with trustworthy information."}, {"title": "4 Experiment", "content": "We analyze and compare the BTN with the leading convolutional neural network\nand vision transformer model on representative FER task. In addition, more\nexperimental setups and ablation studies are presented."}, {"title": "4.1 Datasets", "content": "RAF-DB: RAF-DB [28] is a real-world database with more than 29,670 facial\nexpression images, such as surprise, fear, disgust, and happiness, etc. These\nimages contain natural facial expressions and are a significant challenge because\nof the subjectiveness of annotators, occlusion, pose variation, and low resolution.\nAll samples on RAF-DB have been split into two subsets: a training set and a\ntesting set.\nAffectNet: AffectNet [27] is a large-scale database of facial expressions that\ncontains two benchmark branches: AffectNet(7cls) and AffectNet(8cls). Affect-\nNet(7cls) and AffectNet (8cls) contain 287,401 and 291,568 samples, respectively.\nThe distribution of each class is extremely unbalanced. Specifically, in the train-\ning samples of AffectNet (7cls), happy contains 134,415 samples while disgust\ncontains 3,803 samples only. Additionally, contempt contains 3,750 samples only\nin AffectNet(8cls)."}, {"title": "4.2 Implementation Details", "content": "Preprocessing All images are aligned and resized to 224 x 224 pixels and then\ninterpolated to 112 \u00d7 112 pixels. The three stages of IR50 [34] pre-trained by\nMs-Celeb-1M [36] are used as an image feature extractor and the three stages\nof MobileFaceNet [35] pre-trained by Ms-Celeb-1M [36] are used as a landmark\nfeature extractor with frozen weights.\nTraining and Inference Details We trained BTN with the Pytorch plat-\nform on an NVIDIA RTX 3090 GPU. We employed the SAM [37] optimizer with\nAdam [38] and ExponentialLR [39] for training on all datasets. More specifically,\non the RAF-DB dataset, we trained our model with an initial learning rate of\n2e-5 and a batch size of 64. For the AffectNet(7cls) dataset, we trained our\nmodel with an initial learning rate of 0.8e-6 and a batch size of 144. For the\nAffectNet (8cls) dataset, we trained our model with an initial learning rate of\n1e-6 and a batch size of 144. For data augmentation, we used random hori-\nzontal flipping as the default value in all datasets and random erasing with a\nscale=(0.02, 0.1) for RAF-DB, and p=1, scale=(0.02, 0.1) for AffectNet(7cls)\nand AffectNet (8cls). Finally, we used the ImbalancedDatasetSampler only in\nAffectNet to resolve the unbalanced class distribution problem."}, {"title": "4.3 Comparison with the State-of-the-Art Methods", "content": "The proposed method is compared with the state-of-the-art on three datasets.\nThe evaluation metrics are overall accuracy and mean accuracy. Overall accu-\nracy is general accuracy which is calculated by averaging accuracy on all the test\ndata. Mean accuracy, which is calculated by averaging accuracy of each emotion"}, {"title": "4.4 Ablation Studies", "content": "In this section, we analyze the effectiveness of BTN, BT, and MLA, and then\nthe performance differences according to the combination of loss functions and\nhyper-parameters are presented. All the experiments are conducted on the RAF-\nDB dataset.\nEvaluation of different components We analyze the accuracy of base-\nline model, i.e., POSTER++, and the proposed method with only MLA, only\nBT, and both of MLA and BT as shown in Table 5. The results demonstrate\nthat the proposed modules are effective for FER. In the second two rows, opti-\nmizing components individually degrade the performance of BTN. In contrast,\nthe combined MLA with BT strategy succeeded by solving problems of FER in\nthe 4th row. The results imply that BT needs MLA for extracting diverse level\ninformation.\nEvaluation of $\\lambda$ Table 6 shows the performance when $\\lambda$ ranges from 1.0 to\n3.0. As $\\lambda$ increases per 0.25, we can see a trend of improved performance. We\nacquire the best performances at $\\lambda$ = 2.0 and then the accuracy decreases rapidly\nfrom 2.0 to 3.0. This result proves the effect of $L_{ViT}$ in BTN and implies that\nthe balance between coefficient of $L_{ViT}$, $L_{BT}$, and $L_{CBA}$ should be adjusted\nappropriately."}, {"title": "4.5 Visualization", "content": "To demonstrate that the proposed BTN, BT, and MLA work as intended, we\nvisualize the learned feature map. As shown in Fig. 5, we visualize activation\nmaps generated by Score-CAM [54] for the proposed MLA. We extract activation\nmaps for the convolution layers and the MHCA at each semantic level. BTN\nand POSTER++ are similar in the feature extraction layers $F_1$, $F_2$, and $F_3$\nwhile they are extremely different in the attention layers $S_1$, $S_2$, and $S_3$. The\nBTN captures many features from low to high level in the attention layer by\nemploying MLA while POSTER++ cannot capture features in the low and mid\nlevels. It demonstrates that the proposed MLA prevents overfitting by capturing\nfeatures at each semantic level.\nTo verify that the embedding layer in BT works as intended, we visualize\nthe learned feature map. As shown in Fig. 6, we visualize activation maps gener-\nated by Score-CAM for the proposed MLA and embedding layer E for different\nlabels. The embedded feature map E has activation maps in important parts\nwhile MLA has many activation maps broadly. It demonstrates that E provides\nimportant information by decreasing the channel.\nTo prove that the proposed BTN works as intended, we visualize the learned\nfeature map. As shown in Fig. 7, we visualize activation maps generated by\nScore-CAM for the proposed BTN and the baseline POSTER++ in the inference\nlayer. BTN has a wider range of attention map than POSTER++. It proves that\nBTN is robust to noisy data by preventing dependence on specific features.\nTo verify that the proposed CBA works as intended, we utilize t-SNE [55]\nto visualize feature distribution on RAF-DB as shown in Fig. 8. We extract"}, {"title": "5 Conclusion", "content": "FER has been a challenging task because these data contain uncertainty such\nas occlusion, low resolution, pose variation, illumination variation, and subjec-\ntive, which include some expressions that do not match the target label. How-\never, existing methods have directly learned to extract discriminative features\nfrom a single image and cannot solve above mentioned problems. In this pa-\nper, we proposed the BTN with BT and MLA. The proposed BT prevented"}]}