{"title": "InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation", "authors": ["Andrew Lee", "Ling-Yuan Chen", "Ian Chuang", "Iman Soltani"], "abstract": "We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.", "sections": [{"title": "Introduction", "content": "Bimanual manipulation tasks, such as unscrewing a bottle cap or connecting two electrical cables, presents significant challenges due to the need for high precision coordination. Traditional approaches often rely on high-end robots and precise sensors, which can be expensive and require meticulous calibration [1, 2, 3]. However, recent advances in learning-based approaches offer the potential to perform such complex tasks using low-cost hardware.\n\nThe ALOHA and the Action Chunking with Transformers (ACT) framework has shown that low-cost systems can achieve high-precision tasks that were traditionally only possible with expensive setups. The ACT addresses the compounding error problem in imitation learning by predicting sequences of actions rather than single steps, thereby reducing the task's effective horizon and mitigating errors over time [4].\n\nDespite these recent advances, it remains a challenge for bimanual robotics to ensure robust and accurate coordination between two arms in a dynamic environment. In this work, we propose InterACT: a new policy for bimanual manipulation that emphasizes inter-dependencies between two arms by utilizing hierarchical attention mechanisms. In our designs, multimodal inputs are encoded through segment-wise and cross-segment encoders which handle the complex relationships between different segments in a manner similar to how long documents are processed in NLP [5]. This combines the proprioceptive data of the robot arm joints and the visual features of the camera in a coherent latent space that allows for coordinated detail-oriented and smooth action execution."}, {"title": "Related Works", "content": "2.1 Bimanual Manipulation\n\nBimanual manipulation involves two robotic arms performing tasks that require dexterity and synchronization. Inspired by the natural ability of humans to perform such tasks, researchers have been keen on modeling these skills in robots. Prevailing methodologies, including classical control methods, reinforcement learning, and imitation learning, have significantly advanced the field.\n\nEarly research primarily relied on classical control methods, focusing on predefined trajectories and high-fidelity models to achieve coordinated movements [1, 2, 3]. However, these methods often required extensive calibration and were less adaptable to dynamic environments.\n\nThe introduction of reinforcement learning (RL) made bimanual manipulation more adaptable and robust. RL-based approaches have proven effective in handling complex tasks, outperforming classical methods, and generalizing across different scenarios [6, 7, 8, 9]. These methods leverage RL's ability to learn from interactions with the environment, improving performance in varied and unpredictable conditions.\n\nImitation learning has emerged as a prominent method for teaching robots bimanual tasks through human demonstrations. This approach enables robots to mimic complex human actions, facilitating the execution of intricate tasks. Research has demonstrated the effectiveness of imitation learning in training robots for coordinated dual-arm manipulation using hierarchical skill learning and force-based techniques [10, 11, 12, 13, 14].\n\nRecent advancements have focused on integrating machine learning models to enhance imitation learning. Frameworks like ALOHA have shown that low-cost systems can perform high-precision tasks using imitation learning techniques traditionally reserved for expensive setups [4, 15]. Similarly, the Action Chunking with Transformers (ACT) algorithm addresses the compounding error problem in imitation learning by predicting sequences of actions, improving accuracy and efficiency [4].\n\nDespite these advancements, bimanual manipulation remains challenging, especially in dynamic and unstructured environments. Recent research has explored integrating additional multi-modal data, such as language or sensory feedback, to enhance the robustness and efficiency of bimanual manipulation systems [16, 17]. These efforts hold promise for developing more adaptable and efficient robotic systems capable of performing a wide range of complex manipulation tasks."}, {"title": "Hierarchical Attention Mechanisms", "content": "Hierarchical attention [18] mechanisms have gained prominence for their ability to process and integrate multi-modal inputs. These mechanisms aggregate information at multiple levels of granularity, making them well-suited for tasks requiring both local and global context understanding [19, 20].\n\nHierarchical attention has shown considerable success in other domains, such as natural language processing (NLP), where hierarchical attention transformers have been effectively used for long document classification [5, 21, 22]. These models focus on different parts of the input text at varying levels of abstraction, enhancing the ability to handle long and complex documents. Extensions of foundational attention mechanisms [23], such as the Hierarchical Attention Network (HAN) [18] and hierarchical representations in BERT [24], further demonstrate their potential in managing multi-layered information.\n\nIn robotic manipulation, the concept of hierarchy has been explored to manage the complexity of long-horizon tasks by breaking down problems into manageable sub-tasks [25]. As proven in long document classification, leveraging segment-wise and cross-segment attention mechanisms, hierarchical attention models can capture dependencies within and across segments [5]. This makes hierarchical attention transformers particularly appealing for use in bimanual robotic manipulation tasks, where precise coordination between arms is crucial.\n\nIn this research, we explore a different utility of hierarchical attention in which inter/intra-segment attention supports a more robust extraction of inter-dependencies between the actions of the two arms in handling a single task, leading to more coordinated actions in complex bimanual manipulation."}, {"title": "InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformer", "content": "InterACT builds upon the Action Chunking with Transformers (ACT) framework [4], enhancing it with hierarchical attention mechanisms to capture inter-dependencies between dual-arm joint states and visual inputs. This section details the components and workflow of the InterACT framework, focusing on the encoder and decoder structures within an imitation learning framework.\n\nThe ACT framework leverages transformer architecture to predict future steps in bimanual manipulation tasks, effectively handling sequences of actions by capturing temporal dependencies. However, it does not explicitly model inter-dependencies between dual-arm joint states and visual inputs, which can limit its performance in complex manipulation tasks.\n\nInterACT extends the ACT framework by introducing hierarchical attention transformers, consisting of two main components: Hierarchical Attention Encoder and Multi-arm Decoder."}, {"title": "Hierarchical Attention Encoder", "content": "Segments are defined as individual groups of data inputs that are processed independently before being integrated. In this context, segments include the joint states of each arm and visual features at a specific timestep. Hierarchical Attention Encoder processes input segments and captures both intra-segment and inter-segment dependencies through a hierarchical attention mechanism, which consists of segment-wise encoder and cross-segment encoder layers. A detailed pipeline for the Hierarchical Attention Encoder is illustrated in Figure 1 and Algorithm 1.\n\nInput: Each joint in each arm is embedded into a single token through a linear layer. Visual features are extracted from RGB images using ResNet18 backbones, which convert and flatten the images along the spatial dimension to form a sequence of feature tokens. The joint embeddings from both arms and the visual embeddings of image frames from multiple cameras are concatenated to form a combined sequence. Multiple classification (CLS) tokens are prepended to each segment, allowing the model to summarize segment information during attention processing. Positional embeddings are then added to the tokens to retain sequence information, ensuring the model can understand the order of the tokens within each segment. The input for each decoder includes relevant segments and CLS tokens to facilitate cross-attention mechanisms.\n\nSegment-wise Encoder: Each segment passes through multiple layers of multihead self-attention to aggregate information within each segment. The self-attention mechanism allows each token within the segment to attend to every other token, including the CLS tokens, thereby capturing intra-segment dependencies. This aggregation of information enables the model to effectively leverage the relationships within different parts of the segment [5], such as the connections between joints within the same arm and the relationships within the features from visual inputs. Importantly, the segment-wise encoder uses shared weights across all segments in each layer, ensuring uniform transformation and processing.\n\nCross-segment Encoder: The Cross-segment Encoder handles CLS tokens from each segment to capture inter-segment relationships. This component allows the model to discern and combine information across different segments [5] (e.g., joint states of both arms and visual features). The cross-segment encoder employs shared weights across all segments to maintain consistent modeling of these relationships.\n\nOutput: The original visual feature (input to the encoder, excluding CLS tokens) replaces the processed visual features to retain detailed spatial-temporal information."}, {"title": "Multi-arm Decoder", "content": "Similar to multi-task learning in NLP [26, 27], the Multi-arm Decoder comprises two parallel paths of decoder blocks, each dedicated to processing the encoded states and target tokens to generate the predicted actions for one of the arms. This section details the components and workflow of the Multi-arm Decoder, highlighting how it employs enriched embeddings from the encoder to coordinate actions between both arms effectively. A detailed pipeline for the Multi-arm Decoder is illustrated in Figure 2 and Algorithm 2.\n\nInput: Encoded states from the Hierarchical Attention Encoder are used as the context for decoding. Target tokens, initialized with positional embeddings, serve as the starting point for generating actions. The input for each decoder includes relevant segments and CLS tokens to facilitate cross-attention mechanisms."}, {"title": "Training and Evaluation", "content": "InterACT is trained using an end-to-end imitation learning framework adapted from the ACT algorithm. The training process involves collecting high-quality human demonstrations through a teleoperation system, capturing joint positions and RGB images at 50Hz. The collected data is pre-processed to extract joint states and visual features using ResNet18 backbones, converting the RGB images into feature tokens. Each joint state and visual feature is tokenized, with multiple CLS tokens prepended to summarize each segment's information. Positional embeddings are added to retain sequence information. Action chunking is implemented to predict sequences of actions rather than single steps, reducing the task's effective horizon and mitigating compounding errors. Additionally, a temporal ensemble method is employed to improve the temporal consistency and robustness of the action predictions by weighing predictions over multiple time steps [4].\n\nEvaluation is conducted on both simulated and real-world tasks, measuring success rates to assess the model's performance in generating accurate and coordinated actions."}, {"title": "Experiments and Results", "content": "For the real-robot setup, we modified the ALOHA 2 [29] setup by adjusting the height of the top camera to improve the visibility of the tabletop environment. This adjustment ensures that the camera captures a more comprehensive view of the workspace, which is essential for accurately tracking the bimanual manipulation tasks. Our robot setup is illustrated in Appendix C.\n\nTo evaluate our model, we conducted experiments on three simulation tasks and six real-world tasks: Transfer Cube and Peg Insertion along with Slide Ziploc and Thread Velcro are tasks adapted from ACT [4]. We introduce five new tasks: one simulation task and four real-world tasks. The simulation task is Slot Insertion, where both arms need to grab each side of a long peg together and place it in a slot on the table. The new real-world tasks include Insert Plug, Click Pen, Sweep, and Unscrew Cap. Detailed task definitions are provided in Appendix A.\n\nWe focus our comparisons exclusively on ACT as ACT already outperforms BC-ConvMLP [30, 31], BeT [32], and VINN [33] by a large margin in bimanual manipulation tasks [4]."}, {"title": "Results", "content": "The results of our experiments are summarized in Tables 1 and 2. Our InterACT model shows superior performance compared to ACT on all simulated and real-world tasks. In the simulated tasks, InterACT outperformed ACT significantly, particularly in the Transfer Cube and Peg Insertion tasks where coordination and precision are crucial. The success rates for the \"Transfer\" stages in the Transfer Cube task, as well as the \"Insert\" stages in the Peg Insertion task, were notably higher with InterACT, demonstrating the effectiveness of our method in tasks that require coordination between the two arms. Moreover, in newly introduced tasks such as Slot Insertion and Insert Plug, InterACT also demonstrated higher performance. The Slot Insertion task, which requires precise coordination between two arms to carry the peg and adjust for alignment, showed a 100% success rate with InterACT, compared to 88% with ACT. Similarly, in the Insert Plug task, InterACT achieved better results in the coordination subtask Insert\u201d. This highlights the robustness of our model in handling tasks that require precise coordination between the two arms.\n\nOverall, the experimental results validate the effectiveness of our hierarchical attention framework. By improving coordination and precision in bimanual manipulation tasks, our InterACT model provides a more robust solution for complex bimanual manipulation challenges in both simulation and real-world scenarios."}, {"title": "Ablation Studies", "content": "In this section, we perform ablation studies to evaluate the contributions of different components of the InterACT framework. Specifically, we focus on the impact of CLS tokens, the cross-segment encoder, and the synchronization block in the decoder. Detailed results of these ablation studies are provided in Appendix B.\n\nImpact of CLS Tokens: To assess the impact of CLS tokens, we conducted experiments with and without CLS tokens as input to the decoder. The results, summarized in Table 3, showed no significant difference in the easier Transfer Cube task. However, there were notable improvements in the success rates of the more complex Peg Insertion task when CLS tokens were included. The aggregated information in the CLS tokens enhances the model's ability to generate accurate and coordinated actions, particularly in tasks requiring higher precision and synchronization.\n\nImpact of Cross-segment Encoder: The cross-segment encoder captures inter-segment dependencies, allowing the model to effectively integrate information from different joints across the two arms as well as the camera frames. The results indicate that removing the cross-segment encoder significantly decreases performance in complex tasks. For example, in the Slot Insertion task, the success rate for the coordination subtask \"Insert\" dropped to 24% from 44% without the cross-segment encoder. This highlights the importance of capturing inter-segment dependencies for generating accurate and coordinated actions.\n\nImpact of Synchronization Block: The synchronization block enhances coordination between the two arms by sharing contextual information during decoding. This is crucial for synchronized and efficient bimanual manipulation, especially for predicting a sequence of actions. The results show that the removal of the synchronization block leads to a significant drop in performance across all tasks, particularly in the Transfer Cube and Peg Insertion tasks. This demonstrates the necessity of the synchronization block for achieving coordinated and synchronized actions.\n\nThe ablation studies clearly illustrate that all components of the proposed InterACT framework - CLS tokens, cross-segment encoder, and synchronization block - play a critical role in achieving high success rates in coordination tasks. The highest performance for the final coordination task is achieved when all components are utilized, underscoring the importance of the holistic integration of these elements in the InterACT framework."}, {"title": "Conclusion and Future Work", "content": "In this paper, we presented InterACT, a framework for robust bimanual manipulation, which integrates hierarchical attention transformers to capture inter-dependencies between dual-arm joint states and visual inputs. The key contributions of our work include the development of a Hierarchical Attention Encoder and a Multi-arm Decoder. The Hierarchical Attention Encoder aggregates intra-segment information using a segment-wise encoder and integrates inter-segment dependencies through a cross-segment encoder. The Multi-arm Decoder ensures coordinated action sequence generation for each arm through synchronization blocks. Our experimental results, obtained from both simulated and real-world tasks, demonstrate the superior performance of InterACT compared to the baseline ACT framework. The use of CLS tokens, cross-segment encoder and the synchronizatin block significantly enhances the model's ability to generate accurate and coordinated actions, leading to higher success rates in bimanual manipulation tasks. The ablation studies further highlight the importance of these components in our framework.\n\nWhile InterACT has shown promising directions in integrating robotic arm joints and visual inputs, it has not yet explored integration with other modalities. Future work will explore integrating additional multi-modal data, such as text or sensory feedback, to further improve the robustness and efficiency of bimanual manipulation tasks. Additionally, addressing the hierarchical nature of tasks will be crucial for better task decomposition and execution. These enhancements could leverage the flexible attention mechanisms demonstrated in this work to manage the added complexity and data integration."}]}