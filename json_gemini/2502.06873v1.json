{"title": "Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning", "authors": ["Subin Kim", "Hoonrae Kim", "Heejin Do", "Gary Geunbae Lee"], "abstract": "Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2COSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client's facial expressions. To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we propose a multi-hop psychotherapeutic reasoning approach that explicitly identifies and incorporates subtle evidence. Our comprehensive experiments with both LLMs and vision-language models (VLMs) demonstrate that the VLMs' performance as psychotherapists is significantly improved with the M2CoSC dataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables VLMs to provide more thoughtful and empathetic suggestions, outperforming standard prompting methods.", "sections": [{"title": "1 Introduction", "content": "As a crucial part of cognitive behavioral therapy (CBT), cognitive reframing addresses lots of mental health issues rooted in deeply ingrained negative and distorted thought patterns (Beck, 1970; Powles, 1974; Beck, 1987, 1988; Walen et al., 1992; Halamandaris and Power, 1997; DiTomasso et al., 2000; Hofmann et al., 2012). Recently, studies attempted to utilize large language models (LLMs) in this task, highlighting their growing potential in the field of psychotherapy (Ziems et al., 2022; Maddela et al., 2023; Sharma et al., 2023; Qu et al., 2023; Yang et al., 2023, 2024; Xiao et al., 2024). Conventionally, cognitive reframing has been explored with text-based sentence rewriting methods aimed at shifting negative viewpoints to positive ones (Ziems et al., 2022; Maddela et al., 2023; Sharma et al., 2023). Concerned that sentence-based cognitive reframing can feel unnaturally imposed, Xiao et al. (2024) suggest a three-stage conversational approach with LLMs encouraging clients to engage more actively and form self-positive viewpoints.\nDespite the promising results of LLMs in previous systems, non-verbal aspects of psychotherapeutic theory are often overlooked, creating a significant gap between real face-to-face therapy and the systems. In actual psychotherapy situations, recognizing non-verbal emotions is essential for effective communication and is a critical skill closely linked to the therapist's ability to provide effective therapy (Hutchison and Gerstein, 2012; D\u00f6llinger et al., 2021).\nIn this study, we propose to extend the concept of cognitive reframing into multimodality, integrating visual clues into the therapy process. Our aim is to provide a natural and effective cognitive reframing framework within a multimodal context, incorporating clients' non-verbal clues in the conversation (Figure 1). To this end, we create a novel synthetic benchmark dataset, Multi Modal-Cognitive Support Conversation (M2COSC), which pairs each synthetic dialogue with an image of the client's facial expression.\nTo create M2COSC, we utilize the powerful role-playing capabilities of LLMs. We leverage GPT-4 Vision and GPT-4 (Achiam et al., 2023) to take on the roles of psychotherapist and client, respectively, simulating therapy sessions as shown in Figure 2. Inspired by counseling theory's (Krishnan, 2015; Psylog, 2024; Claibourne Counseling, 2024) Initial Disclosure stage among the five stages, we add the Introduction phase into our multimodal psychotherapy framework, extending the three-stage model proposed by Xiao et al. (2024): Introduction, Problem Exploration, Brainstorming, and Suggestion. In addition, to provide explicit guidance based on the client's state, we introduce a multi-hop psychotherapeutic reasoning method. By exploring the implicit evidence necessary for the therapy and then generating responses based on this evidence, the AI therapist can offer adequate guidance after identifying the client's state.\nWe evaluate our approach by conducting extensive experiments with two test scenarios, dialogue-level evaluation and stage-level evaluation, using both LLMs and Vision-Language Models (VLMs). The results show that training with our M2COSC significantly enhances the counseling capabilities of VLMs, surpassing those of existing LLMs. Moreover, the multi-hop psychotherapeutic reasoning method allows VLMs to offer more rational and empathetic suggestions, outperforming standard prompting methods. Human evaluations further suggest that capturing the client's facial expressions in the system, as practical therapists do, can remarkably assist counseling.\nTo sum up, our contributions are as follows: (1) We propose multimodal cognitive reframing therapy using non-verbal information, creating the M2COSC dataset that pairs dialogues with client facial expressions. (2) We establish a baseline for the M2COSC dataset and propose a multi-hop psychotherapeutic reasoning approach to improve the capabilities of VLMs in delivering rational therapeutic interventions.\nThis work is a first step toward bringing multimodal cognitive reframing into AI-enhanced psychotherapy. By introducing a benchmark dataset and a structured reasoning approach, we hope to inspire future research on leveraging non-verbal cues for more effective therapeutic conversations."}, {"title": "2 Problem Definition and Goals", "content": "In cognitive reframing therapy, a therapist must understand the client's states, which include their problematic situations, distorted thoughts, and thinking traps. The therapist then encourages the client to consider alternative possibilities. Building rapport with the client by expressing empathy is also crucial (Horvath and Luborsky, 1993; Lambert and Barley, 2001). In the real-world psychotherapy procedure, these stages involve both verbal and non-verbal information, where the therapist has sufficient ability to understand the client's states.\nHere, our goal is to enhance the abilities of an AI psychotherapist by leveraging non-verbal information, guiding it to focus on facial expressions and to comprehend the client's states. Given the client's facial image and dialogue history, we aim to provide empathetic responses while maintaining a consistent focus on the client's issues throughout the procedure, offering rational interventions free from logical errors or contradictions.\nTo achieve our goals, we created the M2COSC dataset founded on three key values\u2014empathy, logical coherence, and guidance\u2014which serve as evaluation criteria in a prior study (Xiao et al., 2024).\n\u2022 Empathy reflects the therapist's ability to understand and connect with the client's emotions, assisting in building trust, connection, and emotional support, all critical to a strong therapeutic relationship.\n\u2022 Logical coherence denotes the therapist's ability to organize thoughts and provide well-structured insights, enhancing the quality of the conversation.\n\u2022 Guidance indicates the therapist's capacity to offer practical advice, solutions, and direction, aiding the client to navigate challenges, make informed decisions, and achieve positive outcomes.\nWe also utilize overall scores encompassing all three items. (see Appendix A for details.)"}, {"title": "3 M2 Cognitive Support Conversation", "content": "3.1 Dataset Construction\nActual therapy conversations are rarely accessible due to the sensitive nature of mental health therapy; thus, we have created a synthetic dataset that can be shared with the research community2. To construct multimodal conversational cognitive reframing dataset, we utilize two publicly available sources: the Facial Expression Recognition (FER) dataset called AffectNet (Mollahosseini et al., 2019), and the cognitive reframing dataset from Sharma et al. (2023). To address potential privacy concerns associated with using images of real people from AffectNet, we obtained agreement for all research participants, ensuring full compliance with AffectNet's policies.\nFor construction, we set up role-play scenarios with two agents: GPT-4 in the client role and GPT-4 Vision in the therapist role.\nAs shown in Figure 2, we prompt GPT-4 in the client role and GPT-4 Vision in the therapist role using a set of four inputs: image, facial expression, thinking traps, and thought. The image represents the client's facial image, the facial expression denotes the client's facial expression, the thought reflects the client's thoughts, and the thinking traps capture cognitive distortions present in the thought.\nFor facial expressions and images, We employ AffectNet, containing publicly accessible images from the internet collected for research under non-commercial use. For thinking traps and thoughts, we utilize the well-designed open-sourced dataset from Sharma et al. (2023), which was collected following ethical guidelines, including informed consent and participant privacy safeguards. To the best of our knowledge, this work is the first to combine multiple datasets to create a multimodal conversation specifically designed for the mental health domain.\nEach dialogue consists of four turns, which correspond to different stages of a psychotherapeutic conversation. In this context, a \"turn\" is the same as a \"stage.\" The action expected from the client is to follow the psychotherapist's instructions, and the actions required for the psychotherapist at each stage are as follows."}, {"title": "3.2 Dataset Cleansing", "content": "To ensure the quality of the M2CoSC dataset, we conducted manual data cleansing with the three native speakers, focusing on four aspects: Client-clarity, Client-role, Therapist-role, and Image-Dialogue Consistency (see Appendix B for detailed criteria). The Image-Dialogue Consistency is a criterion that evaluates whether the client's visual information and dialogue are consistent. If any of the four criteria received a score of 0, the corresponding data was deleted. Table 2 indicates a considerable correlation between the client's facial expressions and their verbal responses in our M2CoSC dataset.\nWe hired three native English speakers through Upwork, a crowdsourcing platform, to support this cleansing process."}, {"title": "3.3 Dataset Quality Validation", "content": "To further validate the quality of the M2CoSC, we evaluate the test set of the M2CoSC dataset based on three criteria: empathy, logical coherence, and guidance, along with an overall score. Each criterion was rated on a scale from 0 to 3 following the manual provided by Xiao et al. (2024). The test set evaluation was conducted using both human and GPT-4 assessments. We hired an English-fluent psychotherapist for this evaluation and engaged them to manually evaluate the test set according to the detailed guidelines. We also employed GPT-4 for evaluation, feeding it the scorecard criteria and the dialogues from the test set.\nThe results in Table 3 indicate that both the human evaluator and GPT-4 provided high scores, showing similar scoring tendencies except for \"Guidance.\" GPT-4 tended to give lower scores in this aspect, possibly attributable to its higher degree of expectation for problem-solving than human evaluators (Chiu et al., 2024). Despite this discrepancy, the overall consistency in the other criteria supports the dataset's reliability."}, {"title": "3.4 Multi-hop Psychotherapeutic Reasoning", "content": "To ensure that the interventions are tailored to the client's needs, professional psychotherapists typically first understand the client's state and then provide interventions grounded on that (Greenberg and Safran, 1989; Rice and Elliott, 1996). To mimic the real therapy process, we introduce multi-hop psychotherapeutic reasoning. This approach identifies implicit evidence crucial for cognitive reframing and incorporates it into step-by-step instructions. Initially, the AI therapist detects the client's state and then generates a response based on it, as illustrated in Figure 3. In this work, we focus on three major aspects of the client's states\u2014facial expression, thought, and thinking traps\u2014which accumulate over the stage of the conversation. Each piece of evidence is identified at the appropriate stage. The detected evidence is included in the client's states, and the states are fed to the AI therapist as the prompt for the next evidence detection."}, {"title": "4 Experiments", "content": "4.1 Settings\nBaseline models. We utilize two representative models for our experiments: LLaMA2-chat-7b (Touvron et al., 2023), which is widely used in text generation tasks, and LLaVA-v1.5-7b, renowned for vision-related tasks. For simplicity, we refer to LLaMA2-chat-7b as LLaMA2 and LLaVA-v1.5-7b as LLaVA in this work. In addition, we denote the versions of LLaMA2 and LLaVA that were trained on the M2COSC dataset as CS-LLaMA2 and CS-LLaVA, respectively. When multi-hop psychotherapeutic reasoning is applied, we add MH to their names.\nHyper-parameters. Both LLaMA2 and LLaVA were fine-tuned with LoRA (Hu et al., 2022) on the M2COSC dataset using default settings, except for the number of epochs. For LLaMA2, we used the official open-source models from Hugging Face, and for LLaVA, we followed the official code defaults. We split the M2CoSC train set into 80:20 training and validation subsets to select the optimal epoch based on validation performance. All models were trained with 4 \u00d7 A100-80GB GPUs."}, {"title": "4.2 Evaluators", "content": "GPT-4. Recent research has shown that the evaluation of natural language generation (NLG) models using GPT-4 closely aligns with human evaluations. Therefore, GPT-4 is increasingly used as a judge for NLG tasks across various domains, including common applications, medical fields, and mathematics (Liu et al., 2023; Sottana et al., 2023; Hsu et al., 2023; Khondaker et al., 2023; Xiao et al., 2024). Also, in conversation models, Zheng et al. (2023) showed that GPT-4 achieves high agreement with human judgment in evaluations, releasing the corresponding judging prompt and the used codes. Building on this research, we evaluated the AI therapists using GPT-4 (API version) for evaluation in two ways:\n\u2022 Score assessment: We adopt a three-dimensional scoring system for AI therapists, evaluating them on empathy, logical coherence, and guidance.\n\u2022 Pairwise comparison: We compared the interventions of therapists to determine whether Model A is better than Model B and vice versa or if it's a tie for all possible pairs.10\nHuman. To enhance the reliability of the intervention evaluation, we conducted human evaluations with domain experts. We hired two fluent English-speaking psychotherapists via Upwork. The experts performed a pairwise comparison between our CS-LLaVA with multi-hop psychotherapeutic reasoning and others (see Appendix C)."}, {"title": "5 Results and Discussions", "content": "For reliable comparison, we compared the performance of both LLMs and VLMs with two test scenarios: dialogue-level evaluation and stage-level evaluation. The dialogue-level testbed, which has been used in prior research, allows us to observe how interventions are carried out throughout conversations. However, relying solely on this testbed makes it difficult to accurately compare the AI therapists' abilities due to the variability of the AI client. To better assess interventions in terms of empathy, logical coherence, and rationality, we also conducted a stage-level evaluation. This approach enabled us to compare therapists' interventions more precisely by analyzing turn-level performance on the M2COSC test set, using consistent contextual input across the models."}, {"title": "5.1 Dialogue-level Evaluation", "content": "In this scenario, we employ ChatGPT (API version) as an AI client to test our approach in AI-to-AI scenarios. For prompting to AI client, we leverage 100 resources which are used as base resources from the test set, originally sourced from Sharma et al. (2023) and Mollahosseini et al. (2019). The AI client's role aligns with our data construction method, and we use the same prompts throughout the process. To evaluate the performance difference between using only the text modality and incorporating image information, we also compared the results of CS-LLaMA2. For CS-LLaMA2, only the text modality was used without incorporating image information.\nTable 4 shows the dialogue-level assessment results evaluated by GPT-4. Our M2CoSC dataset with the LLaVA family of models led to significant improvements across all aspects. By integrating multi-hop psychotherapeutic reasoning with three implicit evidences\u2014facial expressions, thoughts, and thinking traps\u2014the models achieved enhancements across all evaluation aspects, with a particularly remarkable improvement in empathy. These results support our hypothesis that understanding the client's emotional state before responding leads to more tailored and compassionate interactions.\nLLaMA2, in contrast, shows minimal improvement when trained on the M2CoSC dataset, primarily due to the absence of visual information during the Introduction stage, which hampers effective training and results in subtle change. These findings validate our hypothesis that models integrating multimodal information in counseling conversations possess superior therapeutic qualities compared to models relying solely on text. Consequently, we shifted our focus to the LLaVA family for further analysis instead of assessing CS-LLaMA2 in stage-level evaluations, except during human evaluation settings.\nSimilar trends were observed in the pairwise comparison (Figure 4). Despite showing the lowest performance, LLaVA exhibited a significant improvement when training on the M2CoSC and applying our multi-hop psychotherapeutic reasoning method, achieving the highest performance in CS-LLaVA w/ MH. No significant performance difference was noted between LLaMA2 and CS-LLaMA2 due to the absence of visual information, further highlighting the impact of multimodal integration in counseling conversations."}, {"title": "5.2 Stage-level Evaluation", "content": "In this testbed, each AI therapist responds to the same dialogue history, allowing us to directly compare their interventions. To ensure reliability, we conducted evaluations using both GPT-4 and two human psychotherapists, and the evaluation was carried out at the turn level for each stage. As in the dialogue-level evaluation, we report score assessment and win rate results.\nThroughout both assessment results and pairwise comparisons, the M2CoSC dataset had a noticeable impact across all stages. Multi-hop psychotherapeutic reasoning outperformed the standard prompting method across most stages; however, in the Exploration stage, the guidance performance was slightly lower. This is likely because this stage emphasizes a deeper exploration of the client's situation rather than offering direct suggestions.\nStage-wise assessment results evaluated by GPT-4 (Table 5) reveal that the score distribution is lower than that of the dialogue-level evaluation, as it assesses intervention at the turn level rather than the entire conversation. One key finding is the substantial performance improvement of the multi-hop psychotherapeutic reasoning process observed in the Introduction stage. The results demonstrate that our attempt to initially detect and incorporate the client's emotions led to more empathetic and coherent interactions. Figure 5 illustrates the stage-wise pairwise comparison results among the four models, as judged by GPT-4. Note that LLaVA, without our method, had the lowest win rate during the Introduction stage. This is attributable to its difficulty in expressing empathy using the client's non-verbal evidence, as it had not yet learned to effectively integrate multimodal information into conversations. The results highlight the importance of teaching models to recognize and utilize such information in counseling phases.\nTo further strengthen the reliability of the human evaluation results, we derived the win rate by comparing the proposed methodology with other approaches. Specifically, we compared CS-LLaVA with multi-hop psychotherapeutic reasoning to CS-LLaVA with standard prompting and other baselines, as evaluated by two domain experts. All stages aggregated results in Figure 7 exhibit that CS-LLaVA w/ MH achieved the highest wins and significantly fewer losses. Stage-wise results in Table 6 show that CS-LLaVA w/ MH outperforms across all stages. However, in the Exploration stage, CS-LLaVA performed similarly to CS-LLaVA w/ MH, likely due to the nature of the stage, which focuses more on exploring the client's situation than providing suggestions.\nOverall results strengthen the essence of integrating multi-hop psychotherapeutic reasoning, particularly in stages where understanding and responding to emotional and cognitive states is critical."}, {"title": "6 Conclusion", "content": "In this paper, we explore cognitive reframing therapy within a multimodal context. Recognizing the gap between real face-to-face cognitive reframing therapy and prior research, as well as the potential benefits of AI in psychotherapy, we take an initial step toward enhancing the therapeutic capabilities of AI therapists by incorporating non-verbal cues, particularly facial expressions, into the intervention process. To address the challenge of restricted access to real client data in the field of psychology, which hinders research efforts, we synthetically create a novel multimodal conversational cognitive reframing dataset called M2CoSC. Our experiments across two test scenarios, dialogue- and stage-level evaluations, exhibit significant improvements in the therapeutic capabilities of VLMs when using M2COSC. The proposed multi-hop psychotherapeutic reasoning strategy, which integrates facial expressions, thoughts, and thinking traps, demonstrates superior performance in providing clients with empathetic, logically coherent, and specific rational suggestions."}, {"title": "Limitations", "content": "We expanded the concept of cognitive reframing into multimodality, demonstrating that incorporating multimodal evidence and multi-hop psychotherapeutic reasoning significantly enhances the therapist's abilities. However, these results were limited to virtual clients whose facial images and dialogues were consistent. This controlled setting may not fully capture the complexities of real-world interactions.\nWhile we used benchmark images for facial expression recognition, capturing real clients' facial expressions can be challenging and may influence the consultation's content. Moreover, our study only utilized facial images as the source of non-verbal information, which presents a limitation compared to actual face-to-face cognitive reframing therapy. Real-life therapy involves a broader spectrum of non-verbal cues, such as body language, tone of voice, and other contextual factors, which were not accounted for in our research.\nAnother important consideration is that facial expression recognition can vary across cultural contexts. Different cultures express and interpret emotions in distinct ways, which can affect the accuracy and fairness of emotion recognition models. Such cultural differences may introduce biases in how virtual clients' emotions are understood and addressed, meaning our findings might not generalize well across diverse populations. Future research should work on mitigating these biases and adapting emotion recognition models to better account for cultural diversity.\nMoving forward, we plan to expand the modalities to include a wider range of non-verbal information. By incorporating diverse non-verbal cues, we aim to further enhance the model's ability to mimic real-life therapy scenarios. This will help bridge the gap between virtual and actual consultations, ultimately enabling the model to learn how to effectively utilize non-verbal information in a more realistic setting."}, {"title": "Ethics Statement", "content": "This study explores multimodal cognitive reframing therapy constructing a synthetic dataset, M2COSC. Importantly, no real client data was used in this work; all information in the dataset was generated synthetically.\nTo construct the dataset, we conducted a thorough data cleansing process with the assistance of three native English speakers, compensating them $0.13 per data entry. For human evaluation, we engaged three professional psychotherapists. One was responsible for evaluating the quality of the M2COSC test set, while the other two conducted pairwise comparisons. We compensated $0.80 per conversation for the dataset evaluation and $0.0625 per entry for the pairwise comparisons.\nAdditionally, to adhere to the AffectNet license, images attached in this paper were not sourced from the AffectNet dataset; instead, all images were created using DALL-E 3 (Betker et al., 2023). Furthermore, we obtained consent for all research participants, including annotators and evaluators, to ensure adherence to the license.\nTo address privacy concerns while complying with the license, we only partially release our M2COSC dataset, with full access requiring an AffectNet license. Specifically, we provide synthetic dialogues paired with image IDs."}]}