{"title": "AUTOMATED MULTI-LABEL ANNOTATION FOR MENTAL HEALTH ILLNESSES USING LARGE LANGUAGE MODELS", "authors": ["Abdelrahaman A. Hassan", "Radwa J. Hanafy", "Mohammed E. Fouda"], "abstract": "The growing prevalence and complexity of mental health disorders present significant challenges for accurate diagnosis and treatment, particularly in understanding the interplay between co-occurring conditions. Mental health disorders, such as depression and Anxiety, often co-occur, yet current datasets derived from social media posts typically focus on single-disorder labels, limiting their utility in comprehensive diagnostic analyses. This paper addresses this critical gap by proposing a novel methodology for cleaning, sampling, labeling, and combining data to create versatile multi-label datasets. Our approach introduces a synthetic labeling technique to transform single-label datasets into multi-label annotations, capturing the complexity of overlapping mental health conditions. To achieve this, two single-label datasets are first merged into a foundational multi-label dataset, enabling realistic analyses of co-occurring diagnoses. We then design and evaluate various prompting strategies for large language models (LLMs), ranging from single-label predictions to unrestricted prompts capable of detecting any present disorders. After rigorously assessing multiple LLMs and prompt configurations, the optimal combinations are identified and applied to label six additional single- disorder datasets from RMHD. The result is SPAADE-DR, a robust, multi-label dataset encompassing diverse mental health conditions. This research demonstrates the transformative potential of LLM- driven synthetic labeling in advancing mental health diagnostics from social media data, paving the way for more nuanced, data-driven insights into mental health care.", "sections": [{"title": "1 Introduction", "content": "Mental illnesses are a major global health issue, affecting millions of people across all age groups and societies. Mental disorders not only impact the well-being and quality of life of individuals but also contribute to a significant burden on healthcare systems worldwide. Mental health conditions encompass a broad spectrum, including disorders like depression, anxiety, and PTSD, as well as more severe outcomes like suicide [1]. For instance, depression affects approximately 280 million people globally, accounting for 3.8% of the population. Among adults, 5% are affected, with a higher prevalence in women (6%) compared to men (4%)[2]. Anxiety disorders, the most common of all mental health conditions, impact around 301 million people globally, with 4% of the world's population experiencing these disorders [3]. Moreover, the severe consequences of untreated mental illness are starkly evident in the global suicide rate, where an estimated 726,000 people take their own lives each year, making it one of the leading causes of death, especially among young people aged 15 to 29[4].These statistics illustrate the widespread nature and severity of mental illnesses, underscoring the urgent need for effective methods of early diagnosis and intervention. In recent years, social media platforms have emerged as valuable resources for mental health research, providing a rich source of data that can shed light on the prevalence, nature, and impact of these disorders. Researchers have utilized social media data to assess mental health conditions, identify individuals at risk, and develop interventions to improve well-being. The"}, {"title": "2 Resources and Methods", "content": "In this section, the key resources and methodologies underlying the study are presented, including the datasets, large language models (LLMs), and evaluation metrics used. By integrating multiple datasets and leveraging advanced LLMs, the goal is to develop a framework capable of capturing complex, real-world mental health patterns. The evaluation metrics are designed to support this goal, offering a robust framework to assess model performance in both single- and multi-label classifications."}, {"title": "2.1 Datasets", "content": "This paper experiments with seven primary single-label datasets, each focusing on a specific mental health condition, as well as the RMHD dataset, which contains data on multiple mental health conditions. These datasets are individually labeled for conditions like depression, anxiety, ADHD, eating disorders, PTSD, and emergent conditions, such as suicide. Below is an overview of each dataset used in this research."}, {"title": "2.1.1 Dreaddit", "content": "The Dreaddit dataset [23] contains 190,000 Reddit posts across ten subreddits in the five domains of abuse, social, anxiety, PTSD, and financial. These posts span a diverse range of content, including personal narratives, advice-seeking posts, and emotional expressions. Of these, 3,553 post segments are manually labeled with stress indicators by human annotators using Amazon Mechanical Turk, creating a supervised training set for stress detection. With its large scale, rich variety of content, and detailed annotations, Dreaddit is an invaluable resource for researchers studying stress in social media, providing insights across multiple real-world domains."}, {"title": "2.1.2 DepSeverity", "content": "The DepSeverity dataset [24] is derived from the same 3,553 Reddit posts used in the Dreaddit dataset, focusing on depression severity. These posts are labeled with four clinical levels of depression severity: Minimal, Mild, Moderate, and Severe, based on the Depressive Disorder Annotation (DDA) scheme[25]. The labeling process utilized six clinical resources: the Diagnostic and Statistical Manual of Mental Disorders, 5th Edition (DSM-5) [1], the Behavioral Risk Factors Surveillance System (BRFSS), the Harvard Department of Psychiatry National Depression Screening Day Scale (HANDS), the Patient Health Questionnaire (PHQ-9)[26, 27], the Quick Inventory of Depressive Symptomatology (QIDS-SR), and the Columbia Suicide Severity Rating Scale(C-SSRS) [28].\nOriginally created as a binary dataset for detecting the presence of depression, it is later expanded to include four severity levels, providing a more nuanced understanding of users' mental health. This transformation helps better assess users' condition, supporting potential intervention and treatment strategies. As noted by the dataset authors, at least 10 posts from a user are necessary for reliably predicting their depression severity, emphasizing the importance of longitudinal data for early intervention. With its detailed annotation and substantial volume of posts, DepSeverity offers a rich resource for researchers studying depression in social media. It spans various types of content such as personal narratives, advice-seeking, and emotional expressions, making it ideal for developing models that detect both the presence and severity of depression."}, {"title": "2.1.3 Reddit Mental Health Dataset", "content": "The Reddit Mental Health Dataset (RMHD) [29] consists of a large collection of Reddit posts extracted from 28 subreddits, including both mental health-focused communities and general interest groups. The dataset spans posts from 17 mental health subreddits such as r/depression, r/Anxiety, and r/SuicideWatch, as well as 11 non-mental health subreddits such as r/conspiracy, r/legalagvice, r/personalfinance.\nThe dataset includes posts from unique users across two critical timeframes: prepandemic (November 2018 to November 2019) and mid-pandemic (January to April 2020), allowing for comparative analyses of mental health trends before and during the COVID-19 pandemic. All posts are preprocessed to include only English-language content, and posts from bots, advertisements, and duplicate users are removed to ensure data quality. The subreddit each post belongs to is used as a label, associating the post with specific mental health conditions or general topics discussed in the community.\nIn addition to the raw text data, a variety of features are extracted from each post to enhance its utility for research. These include sentiment analysis using VADER, lexical counts, and the application of the Linguistic Inquiry and Word Count (LIWC) tool to assess semantic and grammatical categories such as emotions, pronouns, and body references. Moreover, lexicons are manually developed to track topics like suicidality, economic stress, isolation, and substance use, offering insights into the mental health conditions discussed in these posts. With its wide range of posts and comprehensive feature extraction, RMHD serves as a valuable resource for analyzing mental health trends, It is particularly suited for studies on mental health conditions, topic modeling, and trend analysis across large online communities."}, {"title": "2.2 LLMs", "content": "In this paper, experiments are done using five models of varying sizes, architectures, and model families to ensure comprehensive coverage across different approaches. These models are selected for their diversity in source and design, allowing for a robust comparison in performance.\nGPT-40-mini: Developed by OpenAI, GPT-40-mini is a scaled-down variant of GPT-40, designed to balance per- formance with computational efficiency. It features fewer parameters than the full GPT-4o model, making it more accessible for smaller-scale tasks without sacrificing core capabilities in natural language understanding and generation. The model is optimized for various NLP applications, such as text classification, summarization, and dialogue systems.\nLlama-3 70b: Developed by Meta, Llama-3 70b is a 70-billion-parameter model designed to excel in a range of natural language processing tasks. It features multilingual support, coding capabilities, and advanced reasoning. Llama-3 benefits from an improved data curation pipeline and optimized scaling laws, allowing it to match the performance of leading models like GPT-4 across various benchmarks. Despite its size, Llama-3 has been fine-tuned to balance computational efficiency with high performance in both general and domain-specific tasks.\nMistral NeMo 12b: Mistral NeMo is a state-of-the-art model with 12 billion parameters, developed in collaboration with NVIDIA. It features an expansive 128k context window, offering exceptional performance in reasoning, world knowledge, and code generation within its size category. Mistral NeMo is particularly strong in multilingual applications, supporting a wide range of languages, including English, French, German, Chinese, and Arabic. The model uses the highly efficient Tekken tokenizer, which significantly improves compression rates across various languages and source code, outperforming previous Mistral models."}, {"title": "2.3 Evaluation Metrics", "content": "The evaluation of LLM performance is conducted using a variety of metrics, grouped into three categories: per-class, overall (multi-label), and multi-class. Metrics applied for per-class and multi-class evaluations include:\n\u2022 Balanced Accuracy (BA):\n$BA = \\frac{1}{N} \\sum_{i=0}^{N} Recall_i$ (1)\nwhere:\nN: The total number of classes in the classification problem.\nRecall: The recall for the ith label or class.\n\u2022 F1-Score (F1): (Binary F1-score)\n\u2022 Precision (CP):\n\u2022 Recall (CR):\n$F1 = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}$\n$CP = \\frac{TP}{TP + FP}$\n$CR = \\frac{TP}{TP + FN}$\nwhere TP donates True Positives, FP False Positives, TN True Negatives, and FN False Negatives.\nBalanced accuracy is chosen over regular accuracy to address the imbalance present in most datasets. The F1-Score is incorporated to evaluate the model's performance, as it balances precision and recall, making it especially valuable when both false positives and false negatives have significant implications. For multi-label evaluation, the Micro F1-Score is employed to ensure equal consideration of all classes in the assessment. Overall metrics, including Balanced Accuracy, Precision, and Recall, are calculated using the combined values of True Positives, False Positives, True Negatives, and False Negatives across all classes. These overall values are computed as follows:\n\u2022 Overall TP, FP, TN, and FN:\n$Overall TP = \\sum_{i=1}^{n} TP_i$\n$Overall FP = \\sum_{i=1}^{n} FP_i$\n$Overall TN = \\sum_{i=1}^{n} TN_i$\n$Overall FN = \\sum_{i=1}^{n} FN_i$\nwhere n is the number of classes or instances.\nHamming Loss is also utilized to evaluate performance in the multi-label classification setting. This metric quantifies the fraction of labels incorrectly predicted across all instances and labels, making it especially useful for multi-label tasks where each instance can have multiple associated labels. By focusing on individual label errors, Hamming Loss provides a granular view of the model's accuracy across all labels, ensuring that each label's misclassification is accounted for. It is calculated as follows:\n$Hamming Loss = \\frac{1}{N \\cdot L} \\sum_{i=1}^{N} \\sum_{j=1}^{L} (Y_{ij} \\neq \\hat{Y_{ij}})$ \nwhere N is the total number of instances, L is the total number of labels, $Y_{ij}$ is the true label for instance i and label j, $\\hat{Y_{ij}}$ is the predicted label for instance i and label j, $(\u00b7)$ is the indicator function, which is 1 if $Y_{ij} \\neq \\hat{Y_{ij}}$ and 0 otherwise.\nA lower Hamming Loss indicates better model performance, with 0 representing perfect label prediction for every instance.\nThe Odds Ratio (OR), a measure of association between two events, is employed to analyze associations and comorbidi- ties among mental disorders. Commonly used in medical and statistical analyses, the OR helps assess how strongly the presence or absence of one condition is associated with another. For two binary variables. This allows us to assess how likely one disorder is to occur alongside another, offering insights into potential comorbidities, the OR is calculated as follows:\n$OR = \\frac{a \\times d}{b \\times c}$\nWhere a = Number of cases where both the exposure and outcome are present (both event A and B occur). b = Number of cases where the exposure is present but the outcome is absent (event A occurs, but not event B). c = Number of cases where the exposure is absent but the outcome is present (event A does not occur, but event B occurs). d = Number of cases where neither the exposure nor the outcome is present (neither event A nor B occur)."}, {"title": "3 Methodology", "content": "The methodology aims at developing a robust, multi-label dataset for mental health diagnostics by synthetically labeling social media posts, leveraging various prompt strategies with LLMs. This section outlines the structured workflow for dataset creation and labeling, as shown in Figure 1. The Depseverity-Dreaddit merged dataset, labeled for depression and stress, is used to test three distinct prompt strategies\u2014single-label, multi-label, and unrestricted\u2014to guide LLMs in annotating multiple disorders. After evaluating the prompt effectiveness across LLMs, the optimal prompt-LLM combination is used to synthetically label RMHD dataset, resulting in a multi-label dataset that reflects complex mental health profiles."}, {"title": "3.1 Depseverity-Dreaddit as Multi-Label Dataset", "content": "The Depseverity and Dreaddit datasets consist of identical posts from the same user base, each independently labeled for either depression or stress. To utilize this overlap, these datasets are merged into a unified multi-label dataset, providing a resource labeled for both depression and stress. The resulting dataset distribution is shown in Table [2]. This new dataset, annotated by psychiatrists, enables a more nuanced analysis of mental health conditions, where users may exhibit symptoms of both disorders simultaneously. Merging these datasets enriches the available data for training and evaluating models capable of multi-label classification, thereby improving the accuracy and applicability of mental health detection tools."}, {"title": "3.2 Prompt Template", "content": "The experiments utilize various types of prompts to diagnose mental illness in social media posts, with a particular focus on multi-label classification. Single-label, multi-label, and unrestricted prompts are tested across various datasets to evaluate their performance and efficiency. Each prompt template is paired with a specific parser to convert the model's responses (e.g., \"yes,\" \"no,\" or the name of a disorder) into binary labels."}, {"title": "3.2.1 Single-Label Prompts", "content": "Single-label prompts are designed to diagnose one mental illness at a time, focusing the model's attention on identifying whether the user exhibits symptoms of a specific disorder. This approach is simple yet effective for situations where only one condition is being evaluated. The model is prompted to make a binary decision, determining if the user shows clear signs of the targeted mental illness. A prompt inspired by those in [35] is utilized."}, {"title": "3.2.2 Multi-Label Prompts", "content": "Various multi-label prompt templates are developed and tested to evaluate whether users exhibit multiple mental illnesses simultaneously. For this purpose, two primary approaches are identified:\nTemplate 1: Multi-Class Classification Approach\nThis template frames the problem as a multi-class classification task. In cases with two mental illnesses (e.g., depression and stress), the model categorizes the post into one of four classes: \"Depressed,\" \"Stressed,\" \"Depressed and Stressed,\" or \"Normal\". This approach accounts for scenarios where users display symptoms of more than one condition. However, as the number of mental illnesses (n) increases, the number of possible classes grows exponentially ($2^n$), which complicates classification and can confuse the model. As a result, this approach becomes less practical for higher numbers of labels due to the explosion in the number of classes.\nTemplate 2: Multi-Label Classification Approach\nThis template treats the problem as a true multi-label classification task, asking the model to determine whether any of the n+1 conditions (including \"Normal\") is present. In the case of depression and stress, the model is prompted to assess whether the poster is \"Depressed\", \"Stressed\", allowing for any combination of these labels or else he is \"Normal\". This method simplifies the classification task by avoiding the exponential class growth seen in Template 1. Instead of handling a growing number of classes, the model only deals with a manageable set of label combinations, making it more efficient as the number of conditions increases.\nThe multi-label approach ensures greater flexibility in identifying overlapping symptoms and prevents the complexity associated with multi-class classification. Both templates are evaluated for their effectiveness in addressing posts that exhibit co-occurring mental health symptoms."}, {"title": "3.2.3 Unrestricted Prompt", "content": "Unrestricted prompts are also explored, enabling the model to diagnose multiple mental illnesses without being restricted to predefined categories. The aim of this approach is to give the model more freedom in detecting any mental health condition based solely on the content of the post, rather than being confined to a limited set of labels. The unrestricted prompt is designed to assess the model's ability to identify a broader spectrum of mental health issues and are compared to the more structured single-label and multi-label prompts. The flexibility of this approach enables the model to diagnose multiple co-occurring conditions or determine if the poster exhibits no symptoms at all."}, {"title": "3.3 Evaluation on a Multi-label dataset", "content": "The performance of various prompt templates and models is evaluated using the Depseverity-Dreaddit dataset. The goal is to identify the most effective prompt-template and model combinations for accurate multi-label classification. Additionally, the impact of majority voting across model predictions is examined to determine its potential for improving accuracy and reducing biases in the results. This analysis provides insights into how different prompts and models handle overlapping mental health conditions in a multi-label context."}, {"title": "3.4 SPAADE-DR dataset", "content": "An extensive search reveals a lack of existing, accurate multi-label datasets for mental health classification. To fill this gap, a new semi-synthetic multi-label dataset, SPAADE-DR (Suicidal Ideation, PTSD, Anxiety, ADHD, Depression, Eating Disorder Diagnosis from Reddit posts), is created by combining and labeling multiple single-label datasets, each dedicated to a specific mental health condition. For this task, the RMHD dataset is utilized, encompassing data for various mental disorders. Specifically, datasets for conditions such as ADHD, anxiety, depression, eating disorders, PTSD, and suicide are incorporated. By merging these single-label datasets, a comprehensive multi-class dataset is constructed to represent multiple mental health conditions. This allows for more sophisticated multi-label labeling,"}, {"title": "4 Results & Discussion", "content": "This section presents the results of the synthetic labeling methodology, highlighting the effectiveness of various prompt strategies and their influence on multi-label mental health diagnostics. Each prompt-LLM combination is evaluated on the Depseverity-Dreaddit dataset to analyze how single-label, multi-label, and unrestricted prompts affect diagnostic accuracy across various disorders. Insights gained from this analysis inform the selection of the optimal prompt- LLM configuration for creating a nuanced multi-label dataset. Finally, the implications of the results are discussed, emphasizing the dataset's potential to advance multi-disorder mental health assessment in social media contexts."}, {"title": "4.1 Evaluation on Depseverity-Dreaddit Dataset", "content": "In the initial experiment, various LLMs and prompt templates are evaluated using a balanced subset of the Depseverity- Dreaddit dataset. The goal of this evaluation is to identify the most effective LLM-prompt combinations for accurate multi-label classification of mental health conditions. Additionally, majority voting across models is tested to assess its impact on improving overall accuracy.\nThe metrics chosen are used to assess the models' ability to predict each label (depression and stress) separately, their overall multi-label classification accuracy, and their performance in multi-class classification (combining all possible classes). From the results in Table 3, it is clear that the best-performing LLM is Llama-3 70b, closely followed by GPT-40-mini and Phi-3.5 MoE. Llama-3 70b demonstrates the highest scores using all prompts across most metrics, particularly in multi-label classification, where it achieves the highest overall balanced accuracy (0.78) with a lower hamming loss (0.24). GPT-40-mini and Phi-3.5 MoE also perform strongly, with competitive results in the same metrics, making them viable alternatives.\nIn terms of prompt templates, the single-label prompt template consistently outperforms the multi-label templates. This is evident from the higher precision, recall, and F1-scores when evaluating the LLMs on each label (depression and stress). The single-label prompts provide a more focused diagnosis for each condition, contributing to the models' higher accuracy in both multi-label and multi-class evaluations. Although multi-label templates capture co-occurring conditions, they generally result in lower precision and higher Hamming loss, which indicates some confusion in classifying posts with overlapping symptoms.\nAdditionally, after testing majority voting between models to see if combining predictions from multiple LLMs improves accuracy, it is obvious that while majority voting slightly improves recall, it does not significantly impact precision or F1-scores, and in some cases, it leads to higher Hamming loss. As a result, majority voting does not offer a clear advantage over individual model predictions. In conclusion, for multi-label classification on the Depseverity-Dreaddit dataset, the combination of Llama-3 70b and the single-label prompt template delivers the most accurate and reliable results. This combination effectively handles the complexities of diagnosing both depression and stress simultaneously."}, {"title": "4.2 SPAADE-DR Dataset", "content": "As mentioned earlier, the RMHD dataset originally consists of posts from 17 mental health subreddits and 11 non-mental health subreddits. For this study, posts from r/conspiracy, r/jokes, r/teaching, r/personalfinance, and r/legaladvice are selected as control samples, and posts from r/adhd, r/anxiety, r/depression, r/EDAnonymous (Eating Disorder), r/ptsd, and r/suicidewatch as samples representing various mental health disorders. Since the dataset uses the subreddit each post originates from as its label, it is not entirely clean. Some posts are incorrectly labeled as positive for a disorder simply because they are posted in a particular mental health subreddit, even if they do not reflect the condition. To address this, a cleaning and sampling process is applied, followed by multi-label labeling, as outlined in the workflow shown in Figure 5."}, {"title": "4.2.1 Cleaning and Sampling", "content": "Initially, the dataset is cleaned using unsupervised and semi-supervised learning techniques, including KMeans, DBSCAN, and One-Class SVM. These methods leverage both the features provided with the dataset and word embeddings to identify and remove outliers or irrelevant posts. However, these techniques fail to produce satisfactory results. Furthermore, due to the large size of the RMHD dataset, a smaller, cleaner subset must be sampled for further analysis.\nTo address both cleaning and sampling efficiently, 600 posts are initially sampled from each mental health disorder subreddit. The LLaMA-3 70b model, identified as the best-performing LLM, is then employed to evaluate and classify these posts. Based on the model's predictions, posts predicted as negative for the corresponding disorder are manually reviewed, and true negatives are removed. Following the cleaning process, 500 posts are selected from the remaining subset of 600 for each disorder, ensuring a clean and representative sample for further analysis."}, {"title": "4.2.2 Multi-label labeling", "content": "After sampling, the SPAADE-DR dataset is labeled using the most effective prompt and LLMs identified from the evaluation conducted on the Depseverity-Dreaddit dataset. The single-label prompt is applied with LLaMA-3 70b, GPT-40-mini, and Phi-3.5 MoE to ensure accurate labeling. Since each sample originates from a specific mental disorder subreddit, the original label is retained as the true label for that condition. The remaining five disorders are then annotated using the single-label prompt, ensuring comprehensive multi-label classification. This process allows us to transform the dataset from single-label to multi-label, enabling more comprehensive analysis of co-occurring mental health conditions across the dataset. After labeling, the distribution of each label is calculated to represent the new multi-label characteristics of the dataset, as presented in Table:4."}, {"title": "5 Additional Analysis", "content": "After constructing the multi-label SPAADE-DR dataset, further analyses are conducted to explore associations and comorbidities among various mental disorders, providing deeper insights into their association and comorbidity. Additionally, the performance of multi-label and unrestricted prompts is re-examined to evaluate how they adapt to an increase in the number of labels, from 2 to 6, compared to single-label prompts."}, {"title": "5.1 Comorbidities between Mental Disorders", "content": "To explore the comorbidities between mental disorders, a contingency heatmap (as shown in Fig. 6) is created to visually represent the relationships between 2 disorders, A and B. In this visualization, the y-axis lists the first disorder A, while the x-axis shows the second disorder B. Each cell in the heatmap displays the proportion of samples that exhibit disorder B status (positive or negative) within groups defined by disorder A status (positive or negative). For example, the cell indicating the percentage of samples negative for PTSD among those positive for suicide offers insight into the co-occurrence trends between these two conditions."}, {"title": "5.2 Prompts and LLMs evaluation on 6 disorders", "content": "The multi-label and unrestricted prompts are re-evaluated on the labeled SPAADE-DR dataset to examine how the performance of each model is affected when the number of disorders increases from 2 (as in the Depseverity-Dreaddit dataset) to 6. The evaluation utilizes metrics such as CBA,CF1,OBA,OF1,HL and multi-class BA."}, {"title": "6 Conclusion & Future Directions", "content": "This paper explores the potential of large language models (LLMs) to assist in data annotation for mental health research, particularly in the context of social media platforms. Mental health disorders such as depression, anxiety, and PTSD pose significant challenges on a global scale. Social media offer an extensive and valuable source of data that can be harnessed for research into these conditions. Leveraging large language models (LLMs) enables us to improve dataset accuracy, by making them more reflective of real-world data and increasing the efficiency of data annotation processes, thus expanding the volume of high-quality annotated data. This, in turn, can facilitate more effective diagnosis and intervention strategies, ultimately contributing to improved mental health outcomes."}, {"title": "6.1 Conclusion", "content": "The integration of LLMs into mental health research presents a promising approach for tackling the challenges of large-scale data annotation within this field. The capability of LLMs to process and interpret complex language patterns allows them not only to aid in the identification and diagnosis of mental health conditions through social media analysis but also to enhance the quality and depth of dataset annotations. Notably, LLMs can identify subtle indicators and comorbidities among mental health conditions that traditional methods might miss.\nThis study leverages LLMs to transition from single-label to multi-label annotation, facilitating more nuanced classifica- tion of mental health conditions. Various prompting techniques were tested to identify the most effective strategies, revealing that despite the added complexity of increasing the number of labels or classes, the performance of LLMs remained consistent and robust. These findings highlight the potential of LLMs to revolutionize data annotation processes in mental health research. Additionally, a novel multi-label dataset encompassing six distinct mental disorders was developed, expanding the existing pool of resources and enabling more sophisticated analyses in mental health research."}, {"title": "6.2 Future Directions", "content": "While LLMs show significant promise, several avenues remain for future exploration:\n\u2022 Improving model precision for specific mental health conditions: LLMs need to be further fine-tuned to detect nuances between different mental health disorders. Customizing models for specific conditions such as depression, anxiety, and severe disorders like psychosis will enhance their diagnostic effectiveness.\n\u2022 Extending multi-label annotation to other domains: The multi-label annotation method facilitated by LLMS in this study can be expanded beyond mental health to other domains. Future work should explore how this approach can be applied to various types of data, including text, images, and across different languages.\n\u2022 Real-time monitoring and intervention: LLMs could eventually be integrated into real-time monitoring systems, which analyze social media data to detect emerging mental health issues. Such systems could provide timely alerts to health professionals or directly to users when concerning behavioral patterns are identified, potentially preventing crises.\nIn summary, the use of LLMs for mental health data annotation is still in its early stages, but the potential benefits are considerable. Continued refinement of these models, along with addressing current challenges, could position LLMs as a key component in the future of mental health research and interventions."}]}