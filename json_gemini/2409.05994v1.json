{"title": "MessIRve: A Large-Scale Spanish Information Retrieval Dataset", "authors": ["Francisco Valentini", "Viviana Cotik", "Dami\u00e1n Furman", "Ivan Bercovich", "Edgar Altszyler", "Juan Manuel P\u00e9rez"], "abstract": "Information retrieval (IR) is the task of finding relevant documents in response to a user query. Although Spanish is the second most spoken native language, current IR benchmarks lack Spanish data, hindering the development of information access tools for Spanish speakers. We introduce MessIRve, a large-scale Spanish IR dataset with around 730 thousand queries from Google's autocomplete API and relevant documents sourced from Wikipedia. MessIRve's queries reflect diverse Spanish-speaking regions, unlike other datasets that are translated from English or do not consider dialectal variations. The large size of the dataset allows it to cover a wide variety of topics, unlike smaller datasets. We provide a comprehensive description of the dataset, comparisons with existing datasets, and baseline evaluations of prominent IR models. Our contributions aim to advance Spanish IR research and improve information access for Spanish speakers.", "sections": [{"title": "Introduction", "content": "Given a user query, Information retrieval (IR) is the process of locating relevant documents from a collection of data. IR systems have become increasingly important as they are the backbone for the currently widespread Retrieval Augmented Generation (RAG) systems, in which IR provides relevant passages that are subsequently fed into a Large Language Model (LLM). This approach has been shown to improve the quality of generated text, reducing the likelihood of factual errors or hallucinations by LLMs in several tasks (Cheng et al., 2023; Jiang et al., 2023; Cheng et al., 2024; Lin et al., 2024).\nNatural Language Processing (NLP) research relies heavily on evaluation datasets, which establish a common ground for comparing retrieval algorithms and guide the development of new methods.\nWhile a wealth of resources is available for English, other languages often rely on multilingual models and datasets. When addressing tasks in Spanish, the resources are notably scarce. Spanish is the the second most spoken language in the world by number of native speakers, with over 486 million speakers. It is also the official language of 20 countries and is widely spoken in the United States (Eberhard et al., 2024).\nDespite its prevalence, at the time of writing this paper, the retrieval section of the popular MTEB benchmark (Muennighoff et al., 2023) includes datasets for English, Chinese, French, and Polish, but omits Spanish.\u00b9\nThe lack of good quality evaluation datasets is a major challenge for the development of IR systems for Spanish. Which systems are best for Spanish IR? This key question remains unanswered due to the lack of a comprehensive Spanish IR benchmark.\nIn this work, we address this gap by:\n1. Identifying and describing the Spanish IR datasets that are currently available.\n2. Introducing MessIRve, a new large-scale dataset in Spanish that accounts for the varieties of Spanish spoken accross the Spanish-speaking countries.2\n3. Providing baseline evaluations of relevant existing models on the new dataset.\n4. Making the dataset publicly available so that other researchers can use it to develop and evaluate IR systems for Spanish.3"}, {"title": "Related work", "content": "In this section, we review the Spanish IR datasets that are well-documented and publicly accessible. Relevant statistics of these datasets are presented in Table 1.\nFollowing standard terminology, we refer to a dataset as a collection of queries, documents, and relevance judgments (Thakur et al., 2021). A query is a user's need for information, a document is a passage that may contain the information the user seeks, and a relevance judgment indicates whether a document is relevant to a query. The collection of all documents is referred to as a corpus.\nThe multilingual MS MARCO (mMARCO, Bonifacio et al., 2021) is a multilingual version of the well-known MSMARCO dataset (Bajaj et al., 2016), translated into Spanish using both open-source machine translation and Google Translate. Even if not originally in Spanish, this translated dataset is a substantial resource for training IR models since it leverages the large set of questions sampled from Bing's search query logs which make up the original dataset. For each question, annotators judged the relevance of around 10 passages retrieved by Bing's IR system. The collection of documents is the union of all retrieved passages.\nA non-translated dataset is the Spanish Question Answering Corpus (SQAC, Guti\u00e9rrez-Fandi\u00f1o et al., 2022). Inspired by SQUAD (Rajpurkar et al., 2016), Spanish-speaking annotators were shown passages from the Spanish Wikipedia, Wikinews, and the AnCora corpus, and were asked to come up with up to five questions for each fragment, encouraging the use of synonyms to prevent lexical overlap. Even if it is not a true IR dataset because it lacks a collection of documents, the set of all relevant passages can be used as a corpus for IR.\nMIRACL (Multilingual Information Retrieval Across a Continuum of Languages, Zhang et al., 2023) is a multilingual dataset that includes Spanish and 17 other languages, focusing on good-quality relevance judgments by native speakers. Annotators were given a random paragraph from a Wikipedia article and were asked to formulate related questions, which may or may not be answerable by the passage. For each question, other annotators assessed the relevance of 10 candidate passages retrieved by three different IR models. Although MIRACL contains a relatively small number of queries, it benefits from having more annotations per query than other datasets, and from the inclusion of negative relevance judgments as well.\nIn addition to these multi-domain datasets, we identified two datasets focusing on specific domains. The Spanish Passage Retrieval dataset (PRES, Kamateri et al., 2019) contains documents representing articles from reliable Spanish health websites. The dataset is made up of health-related queries and employs a pooling method for relevance annotations, whereby human annotators assess the relevance of the pooled, top-ranked documents retrieved by different IR systems. Each document is a health-related article.\nFinally, the Multilingual European Parliament Dataset (Multi-EuP, Yang et al., 2023) leverages official debate topics from the European Parliament. Each topic is a query, and the documents are speeches from Spanish parliamentarians that took part in the debate. The relevance judgments indicate whether a speech was part of the debate on the given topic or not."}, {"title": "MessIRve", "content": "MessIRve is a new large-scale dataset for Spanish IR, designed to better capture the information needs of Spanish speakers across different countries. In this section we describe the data collection process (section 3.1) and the dataset splits (section 3.2), and perform a quality assessment (section 3.3)."}, {"title": "Data collection", "content": "MessIRve is built by using questions from Google's autocomplete API (www.google.com/complete) as queries, and answers from Google Search \"featured snippets\u201d that link to Wikipedia as relevant documents. This data collection strategy is inspired by the GooAQ dataset (Khashabi et al., 2021).\nQueries Starting with 120 predefined prefixes such as \u201cqu\u00e9\u201d (what), \u201cc\u00f3mo\u201d (how), \u201cd\u00f3nde\u201d (where), \u201cqui\u00e9n\u201d (who), etc., we used the Google autocomplete API to obtain popular queries in Spanish starting with these prefixes (refer to Appendix A, Table 3 for the list of prefixes). We then iteratively expanded the set of prefixes based on the returned results until we reached a predefined number of results. The API does not always return queries that begin strictly with the provided prefixes, resulting in a dataset where not all queries begin with the initial prefixes.\nWe ran this process for 20 countries with Spanish as an official language, plus the United States, by adjusting the API parameters to return results in Spanish for each country. Equatorial Guinea was the only country left out because it doesn't have a Google domain. The extraction took place between March and April 2024, so the queries may reflect popular interests during that period.\nDuring the process we noticed that some API results were independent of the country-specific domain, so we obtained many queries that were not specific of any country. These queries are included in the dataset under the country label none.\nRelevant documents To find relevant passages for each query, we obtained \u201cfeatured snippets\" from Google Search that sourced from Wikipedia. These snippets are text excerpts displayed at the top of the search results page in response to a query, deemed by Google to be particularly relevant to the user's information need. In 2018 these snippets were reported to have a 97.4% accuracy rate in terms of providing relevant information.4\nTo obtain snippets from Wikipedia, we appended the term \"wikipedia\" to each query before querying Google Search. We then extracted the entire Wikipedia paragraph associated with the featured snippet as the relevant document for each query. Queries not returning featured snippets from Spanish Wikipedia were discarded.\nCorpus We built a Spanish Wikipedia corpus with each paragraph corresponding to a single document. For this we used WikiExtractor (Attardi, 2015) to process the Spanish Wikipedia dump of 2024-04-01 and matched the extracted Wikipedia passages to the valid (query, document) pairs obtained from Google Search featured snippets.\nThe choice of Wikipedia as the dataset's collection was motivated by its accessibility, ease of processing, and the wide range of topics it covers.\""}, {"title": "Data splits", "content": "The dataset is partitioned into training and test queries in such a way that the Wikipedia article to which the paragraph belongs is present in only one of the splits. The partitioning was done by country, with about 20% of the articles assigned to the test set. This split by article aims to reduce the overlap in topics between the training and test sets.\nQueries not specific to any country (label none) were combined with the set of unique queries from all countries to form a full set, which is also split into training and test sets. Unlike the country-specific sets, in the full set some queries can have multiple relevant documents because the same query may return different featured snippets in different country domains. Statistical information about the dataset splits is provided in Table 2."}, {"title": "Quality assessment", "content": "We evaluated the quality of the dataset using a method inspired by Rybak (2023), sampling query-document pairs from the full test set and manually rating pairs on three binary criteria:\n1. Query correctness: Can the information need of the user be understood? This criterion can be considered met even if the query is not grammatically correct, which is the case for many queries issued to search engines. For example, the query \"porque se llama bogota\" (Why is it called Bogot\u00e1?) can be considered correct even though it is missing question marks, accents, capitalization, etc.\n2. Query unambiguity: Can a good answer to the query be given without providing further detail? Raters were instructed to consider that the query may only make sense in the context of the user's country, which may not be explicitly stated. For example, the query \u201cc\u00f3mo se llama el presidente\u201d (what is the name of the president) is ambiguous without specifying the country, but it is clear that the user is asking about the president of their country, and so it can be considered unambiguous.\n3. Document relevance: Does the document help answer the query? Even if the passage does not provide an explicit and thorough answer, it can be considered relevant if it contains information that, when combined with other sources, could help answer the query.\nRatings were performed by four unpaid native Spanish speakers from Argentina. Each rater evalu-"}, {"title": "Comparison with existing datasets", "content": "A key feature of MessIRve is the detailed documentation of its data collection process. Existing open-domain datasets, such as MIRACL and SQAC, even if they are relatively well-documented, lack detailed descriptions of how their Wikipedia passages were selected before annotation.\nMessIRve is also the only dataset that aims at explicitly accounting for the diverse varieties of Spanish spoken in different countries. This contrasts with all other datasets, which either do not consider dialectal variations or do not provide clear information about their inclusion.\nTo validate the dataset's dialectal diversity, we listed the most distinctive words from each country's queries according to a frequency-based log odds ratio score (see Table 4 in Appendix A). Each country's queries include terms that are specifically relevant to that country, such as \"green\" for the United States (referring to the Green Card), \"checar\" for Mexico (meaning \u201cto check\u201d), \u201cfrailejones\" for Colombia (a type of plant), \"euskera\" for Spain (the Basque language), and \u201cCUIT\" for Argentina (a tax identification number), when considering the five countries with the most speakers.\nMoreover, MessIRve is substantially larger than other datasets (see Table 1). The mMARCO dataset, while large, involves translations from English, thus introducing potential translation errors and artifacts which may affect the quality of the dataset. Moreover, the queries in mMARCO are derived from English-speaking users' queries, which may not be as relevant to Spanish-speaking users (see section 4.1 for a more detailed analysis).\nOne feature of our dataset is the sparsity of its judgments, with only about one relevance judgment per query, similar to the widely used MS MARCO dataset. This means that, as compared to datasets with more judgments per query, such as MIRACL, it is more likely that relevant passages are not identified as such, and that IR systems will retrieve more false negatives, that is, passages erroneously considered non-relevant.\nSimilarly, unlike MIRACL, our dataset does not include passages explicitly judged as negative (non-relevant). This is a limitation for the training of models that require negative examples, and implies that negative mining techniques would be needed to train models with MessIRve.\nNevertheless, the use of sparse, only-positive judgments is a common practice in IR research due to the challenges and high costs associated with obtaining more complete annotations. Previous research has shown that findings based on sparse judgments, like those of MS MARCO, align well with results from more comprehensive and costly judgments, suggesting that while more exhaustive annotations may change absolute scores, they are unlikely to affect qualitative conclusions regarding the ranking of IR systems (Zhang et al., 2021a).\nThe automated data collection approach was primarily driven by the need for a large-scale dataset that did not require the costly human annotations associated with datasets like MIRACL and MS MARCO. This approach sacrifices control over quality because it relies on both the reliability of the automatically extracted queries and the accuracy of Google Search featured snippets. While this method may be seen as less dependable than human annotations, we have conducted a quality assessment to ensure the dataset's reliability, as described in section 3.3.\nIn the following section we perform a comparison between the datasets in terms of their topics."}, {"title": "Analysis of topics", "content": "MessIRve's scale allows for a wider variety of topics than other datasets. For example, the full set of MessIRve contains judged passages coming from 84,284 unique Wikipedia articles, while MIRACL covers 17,640 articles and SQAC, 3,823 articles (considering training, validation and test sets).\nTo more thoroughly analyze the topics covered by the queries of the different datasets, we employed BERTopic with PCA of 30 dimensions of multilingual-e5-large embeddings, and KMeans clustering with 30 clusters (Grootendorst, 2022; Wang et al., 2024). We assume just one topic per query, and focus only on the queries of the general-domain datasets: MIRACL, SQAC, mMARCO, and MessIRve full. For every dataset we use the complete set of available queries.\nThe clusters were labeled with the four most representative words according to their c-TF-IDF values (Grootendorst, 2022). To provide a summary of a particular topic in a given dataset, representative queries were found by sampling a subset of queries from the dataset belonging to the topic and calculating which ones were closest to the topic's c-TF-IDF representations based on cosine similarity. Representative words of a topic in a dataset were also found by considering the highest c-TF-IDF scores of the words from the dataset.\nThe distribution of topics accross datasets (Figure 1) reveals two broad miscellaneous topics which are relatively prevalent in all datasets: queries concerning entertainment celebrities, including those in sports, music, movies, and other media (topic 0) and general definitions (topic 1).\nMIRACL and SQAC show a higher frequency of topics related to war and politics (topic 19), religion and the Bible (topic 24), and inventions or discoveries (topic 25). This can be attributed to their dataset construction methodology, which starts from Wikipedia passages rather than user queries. Compared to other datasets, MessIRve shows a prominence of queries related to users experiencing health-related pains (topic 2), while mMARCO features topics related to household economy (topics 5 and 6) and computers and technology (topic 8).\nIt is interesting to note that topic 23, relatively prevalent in mMARCO but not in MessIRve, focuses on US demographics. This topic features representative documents such as \u201cdallas nc se encuentra en qu\u00e9 condado\u201d (Dallas, NC is located in what county), \"en qu\u00e9 condado se encuentra grove city ohio\" (In what county is Grove City, Ohio located), and \"cual es el condado de washington tx\" (Which is the county of Washington, TX).\nThis contrasts with the queries from topic 4, which are relevant in MessIRve, SQAC, and MIRACL, and focus on demographic issues. Representative queries from this topic in MessIRve include \"en donde queda ciudad del este\" (where is Ciudad del Este), \u201cdonde queda logro\u00f1o\u201d (where is Logro\u00f1o), and \"por donde queda monta\u00f1ita\u201d (where is Monta\u00f1ita). These queries are likely to be more representative of the information needs of Spanish speakers than those in topic 23 of mMARCO, which probably come from English-speaking users.\nSimilarly, the queries in topic 0 of mMARCO and MessIRve highlight differences in popular entities within Spanish-speaking countries. For example, a representative mMARCO query is \"en qu\u00e9 equipo juega michael sam\u201d (what team does Michael Sam play for), referring to a US NFL player, while one from MessIRve is \u201cen cu\u00e1l equipo juega james rodriguez\u201d (which team does James Rodriguez play for), referring to a Colombian soccer player. This behaviour is expected because MessIRve queries are obtained from Google's autocomplete API by country domains, while mMARCO queries are derived from English-speaking users' queries and translated into Spanish.\nFinally, the distribution of topics in MessIRve and mMARCO is more uniform than in MIRACL and SQAC, which have a few topics that are much more frequent than others. This is likely a result of the larger size of MessIRve and mMARCO, allowing for a wider variety of topics.\nHowever, MIRACL and SQAC contain well-formed questions, while MessIRve includes more colloquial and informal questions, which better represent the queries that users might issue to a search engine. This feature is also present in mMARCO, but affected by translation quality. See Table 5 in Appendix A for further details on representative words and documents for the most frequent topics.\nWe stress that the topic analysis of MessIRve's queries is not intended to provide a faithful representation of the real distribution of popular topics for the Spanish-speaking community. Instead, it is just a descriptive analysis of the dataset, showcasing the kinds of topics included and their relative importance as compared to other datasets."}, {"title": "Experiments", "content": "We evaluate various IR models on MessIRve's test sets in a zero-shot manner. In section 5.1 we describe the experimental setup, and in section 5.2 we present the results."}, {"title": "Experimental setup", "content": "We consider the following models for information retrieval in Spanish:\nBM25 (Robertson et al., 2009). BM25 is a popular lexical IR model based on the token overlap between query and documents. We index the corpus and run retrieval using Pyserini with default parameters and Spanish analyzer (Lin et al., 2021). Despite its simplicity, BM25 is a strong baseline for IR (Thakur et al., 2021).\nMIRACL-mdpr-es (Zhang et al., 2023). This is a bi-encoder initalized from multilingual Dense Passage Retriever (mDPR, Zhang et al., 2021b), which is first pre-fine-tuned using the training set of MS MARCO and further fine-tuned on the MIRACL Spanish training set. We use the mdpr-tied-pft-msmarco-ft-miracl-es checkpoint. As far as we know, this is the only well-documented, Spanish-specific retriever available.\nE5-large (Wang et al., 2024). This bi-encoder is initialized from the multilingual model XLM-R (Conneau et al., 2020) and then trained with weak supervision in a mixture of multilingual text pairs, followed by supervised fine-tuning in annotated data which is mostly in English but includes some multilingual data e.g. MIRACL. We use the multilingual-e5-large checkpoint.\nOpenAI-large . Although the architecture and training data of this model are not publicly disclosed, previous models (Neelakantan et al., 2022) consist of a single encoder that maps documents and query to embeddings, and is trained with contrastive learning across several supervised datasets. The model we use, text-embedding-3-large, is accessible via an API.\nIn all cases, we append the title of the Wikipedia article to the document text before retrieval to provide additional context.\nWe use two standard metrics to measure retrieval performance:\n\u2022 Recall@100: the fraction of relevant documents within the top 100 results, averaged over all queries.\n\u2022 nDCG@10: the normalized Discounted Cumulative Gain. It compares the rank of the top 10 results to the ideal ranking where relevant documents are ranked higher. It is averaged over all queries.\nAll evaluations are conducted in a zero-shot setting, where the models are not fine-tuned on any training set. We leave this for future work."}, {"title": "Results", "content": "The OpenAI-large and E5-large models outperform smaller models (MIRACL-mdpr-es) and the lexical model (BM25) across all dataset partitions (see Figure 2). In terms of nDCG@10, OpenAI-large achieves an average score in the full set of .433 compared to E5-large's .430, indicating similar ranking effectiveness. However, for Recall@100, OpenAI-large consistently outperforms E5-large, with a recall of .899 versus .847.\nBM25 consistently performs the worst across all subsets (nDCG@10=.167 and Recall@100=.530 in the full set), highlighting the limitations of lexical retrieval methods compared to dense models.\nAlthough MIRACL-mdpr-es is specifically fine-tuned for Spanish, it still lags behind the larger models, with nDCG@ 10 and Recall@100 scores of .259 and .626, respectively, suggesting that model size and fine-tuning strategy might play a more critical role than language-specific tuning. For example, while MIRACL-mdpr-es has 178M parameters and is pre-fine-tuned with MS MARCO and then fine-tuned with Spanish MIRACL, E5-large has 560M parameters, is pre-trained in a mixture of multilingual text pairs, and then fine-tuned in at least 10 supervised datasets, including MS MARCO and multilingual data like MIRACL itself.\nFinally, the three dense models follow a similar performance pattern, with the highest scores in the bo (Bolivia) and gt (Guatemala) subsets and the lowest in ar (Argentina) and mx (Mexico)."}, {"title": "Discussion and conclusions", "content": "We presented MessIRve, a large-scale dataset for information retrieval in Spanish. MessIRve queries originate from Spanish speakers from almost all Spanish-speaking countries, ensuring relevance to the information needs of the Spanish-speaking community. This contrasts with other datasets, which are either translated from English or do not consider dialectal variations. Moreover, the dataset's large scale allows it to cover a wide variety of topics, unlike smaller datasets.\nWe performed a quality assessment to ensure the reliability of the dataset, and we provided a detailed comparison with existing Spanish IR datasets, highlighting the dataset's strengths and limitations. We also provide baseline results of the performance of various IR models on the dataset.\nIt must be noted that the dataset was collected using an IR system in itself, Google's \"featured snippets\". Although there's no certainty about how Google's IR system works, it is likely that it considers many other factors besides the text of the query and the documents, such as inbound links, user behavior, the structure of the web, etc., implying significant computational resources. This probably contributes to the high quality of the dataset, and so makes it a valuable resource for training and evaluating open-source IR models. However, it also introduces an evaluation bias towards Google's own IR system, as the dataset may not be used to identify IR systems that are better than Google's own system.\nIn this work, we have focused on zero-shot baselines. Future work includes fine-tuning the models on the training set of MessIRve, evaluating models on other Spanish IR datasets, and creating a unified Spanish IR benchmark with all datasets."}, {"title": "Limitations", "content": "The dataset collected may not be representative of the distribution of popular topics in each country, as it only includes a sample of popular queries in Spanish-speaking countries at the time of collection. Moreover, it only consists of queries which have an answer in Spanish Wikipedia, and that Google is confident in answering.\nThe quality assessment of the dataset was performed by four of the authors, which may introduce bias. This was done due to the lack of resources to hire external raters.\nThe dataset should not be used for commercial purposes."}]}