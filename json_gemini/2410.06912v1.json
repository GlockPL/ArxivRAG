{"title": "COMPOSITIONAL ENTAILMENT LEARNING\nFOR HYPERBOLIC VISION-LANGUAGE MODELS", "authors": ["Avik Pal", "Max van Spengler", "Guido Maria D'Amely di Melendugno", "Alessandro Flaborea", "Fabio Galasso", "Pascal Mettes"], "abstract": "Image-text representation learning forms a cornerstone in vision-language models,\nwhere pairs of images and textual descriptions are contrastively aligned in a shared\nembedding space. Since visual and textual concepts are naturally hierarchical,\nrecent work has shown that hyperbolic space can serve as a high-potential manifold\nto learn vision-language representation with strong downstream performance. In\nthis work, for the first time we show how to fully leverage the innate hierarchical\nnature of hyperbolic embeddings by looking beyond individual image-text pairs.\nWe propose Compositional Entailment Learning for hyperbolic vision-language\nmodels. The idea is that an image is not only described by a sentence but is itself\na composition of multiple object boxes, each with their own textual description.\nSuch information can be obtained freely by extracting nouns from sentences and\nusing openly available localized grounding models. We show how to hierarchically\norganize images, image boxes, and their textual descriptions through contrastive\nand entailment-based objectives. Empirical evaluation on a hyperbolic vision-\nlanguage model trained with millions of image-text pairs shows that the proposed\ncompositional learning approach outperforms conventional Euclidean CLIP learn-\ning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval\ngeneralization and clearly stronger hierarchical performance. Code to be released.", "sections": [{"title": "1 INTRODUCTION", "content": "Vision-language modeling has witnessed rapid progress in recent years with innovative approaches\nsuch as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) using extensive vision-language data\nto train encoders for understanding visual and textual content simultaneously. Such encoders align\nvisual scenes with textual descriptions in a shared high-dimensional Euclidean space, facilitating\nsemantic understanding (Radford et al., 2021). While effective, conventional vision-language models\nonly take a holistic approach to image-text representation learning, neglecting the intrinsic hierarchy\nand composition of elements within images. Indeed, a visual scene is commonly composed of\nmultiple objects interacting with one another to form a precise context. Individually,\nthese objects provide limited semantic meaning. Only through the interactions between these do\nwe understand the specific context of both the scene and its parts, characterizing the single entities\n(cf. Fig. 1a). This object-scene hierarchy is analogous to a parent-child connection in a discrete\ntree where broader concepts are closer to the root while specific concepts reside deeper in the tree.\nThese tree-like structures cannot be well represented in Euclidean space due its polynomial volume\ngrowth (Matou\u0161ek, 1999), whereas hyperbolic geometry does accommodate the exponential growth\nof trees (Gromov, 1987), making it more suitable for representing hierarchies.\nRecently, Desai et al. (2023) introduced MERU, a hyperbolic contrastive vision-language model.\nMERU projects Euclidean embeddings from image and text encoders onto hyperbolic space and\nenforces inter-modal (text to image) partial ordering (Vendrov et al., 2016) using an entailment\nloss (Ganea et al., 2018a; Le et al., 2019) when optimizing encoder weights. Such hyperbolic\nimage-text alignment has demonstrated strong quantitative performance on zero-shot downstream"}, {"title": "2 HYPERBOLIC COMPOSITIONAL CLIP - HYCOCLIP", "content": "We propose a compositional learning scheme that enforces the semantic alignment of latent represen-\ntations in the hyperbolic space, explicitly modeling intra- and inter-modal relationships of visual and\nlanguage data by leveraging their joint hierarchical nature. Here, we first provide a short background\nwith the required hyperbolic functions to make such compositional learning possible. Afterward, we\noutline our compositional encoding of image-text pairs."}, {"title": "2.1 BACKGROUND", "content": "Hyperbolic geometry is a non-Euclidean geometry characterized by a constant negative curvature.\nThe resulting space has the desirable property that volumes of subsets can grow exponentially as\na function of their radius, making it an ideal choice for learning representations of data with an\ninherent hierarchical or tree-like structure (Sarkar, 2011; Nickel & Kiela, 2017; Krioukov et al., 2010).\nWhile several isometric models are used in literature for modeling hyperbolic space, we limit our\nbackground discussion to the Lorentz (or hyperboloid) model used in this work and refer to Cannon\net al. (1997); Peng et al. (2022) for detailed information on the other models.\nThe Lorentz model, denoted by $L^n$, is an n-dimensional manifold represented as the upper sheet of a\ntwo-sheeted hyperboloid in (n + 1)-dimensional Minkowski spacetime. For each vector $p \\in R^{n+1}$,\nthe first dimension is taken as the time-axis, denoted $p_0$, and the remaining n dimensions as the\nspatial-coordinates, denoted $\\vec{p} \\in R^{n}$. This model is described as\n$L^n = \\{p \\in R^{n+1} : \\langle p, p \\rangle_L = -\\frac{1}{\\kappa}, p_0 = \\sqrt{\\frac{1}{\\kappa} + ||\\vec{p}||^2}, \\kappa > 0\\},$ (1)\nwhere $-\\kappa \\in R$ is the curvature of the space and $\\langle ., .\\rangle_L$ is the Lorentzian inner product defined for\n$p, q \\in L^n$ as\n$\\langle p, q \\rangle_L = -p_0q_0 + \\langle \\vec{p}, \\vec{q} \\rangle_E,$ (2)\nwith $\\langle ., .\\rangle_E$ denoting the Euclidean inner product. The Lorentzian distance between two points in $L^n$\nis the length of the shortest path (geodesic) connecting them, which can be computed as\n$d_L(p, q) = \\sqrt{1/\\kappa} \\cdot \\cosh^{-1} ( - \\kappa\\langle p, q \\rangle_L), p, q \\in L^n.$ (3)\nThis metric induces the Lorentzian norm $||P||_L = \\langle p, p \\rangle_L$. The tangent space $T_pL^n$ is well-defined\nfor all the points $p \\in L^n$, and the exponential map represents the projecting map from the tangent\nspace to the hyperboloid. Given a point $v \\in T_pL^n$ the exponential map can be computed as\n$exp^v_p(v) = \\cosh(\\sqrt{\\kappa}||v||_L)p + \\frac{\\sinh(\\sqrt{\\kappa}||v||_L)}{\\sqrt{\\kappa}v||_L}v.$ (4)"}, {"title": "2.2 COMPOSITIONAL ENTAILMENT LEARNING", "content": "We strive to learn the hierarchical compositional relations of images, boxes, and textual descriptions.\nOur idea is based on the following observation: the content inside a box of an image is hierarchically\nmore general than the entire image. While counter-intuitive at first glance, Fig. 1b shows why this\nis the case: the box shows an object and the entire image additionally shows the context in which\nthe object occurs, making it a semantically more specific scenario. From a hyperbolic perspective,\nsemantically general/broad concepts are embedded closer to the origin, while more fine-grained\nconcepts are positioned towards the border, akin to tree graphs (cf. Fig. 1c).\nIn this work, we are given a dataset $D = \\{(I_k,T_k)\\}_{k=1}^K$ of K image-text pairs. Our goal is to train\nimage and text encoders with a shared embedding space to align the visual and semantic inputs. The\nmethod is summarized in Fig. 2. Let $(I_{box}, T_{box})$ be the local box with a short description from an\nimage-text pair obtained following the automated procedure detailed in Appendix A. We propose\na Compositional Entailment Learning objective in hyperbolic space to optimize the hierarchical\ncompositions. Our approach consists of two parts, namely a compositional contrastive loss and a\ncompositional entailment loss which we discuss next.\nImage-text models commonly rely\non contrastive objectives to align and distribute the multi-modal data. In our approach, we rely on\nhyperbolic embeddings to align visuals and text. Let $f_I(\\cdot)$ and $f_T(\\cdot)$ denote arbitrary encoders for the\nimage and text inputs respectively. And, let $g_I(I_k) = exp_0(f_I(I_k))$ and $g_T(T_k) = exp_0(f_T(T_k))$\ndenote the hyperbolic representation of image $I_k$ and textual description $T_k$ respectively. To compute\nthe contrastive loss over image-text pairs in a batch B, we take the negative Lorentzian distance as\nour similarity metric and formulate it with the softmax, using temperature $\\tau$, for a batch of size (B)\ncontaining images (I) and text (T) as\n$L_{cont}^{lorentz}(I,T) = - \\log_{i \\in B} \\frac{exp(d_L(g_I (I_i), g_T (T_i))/\\tau)}{\\sum_{k=1,k\\neq i}^B exp(d_L(g_I(I_i), g_T(T_k))/\\tau)},$ (5)\nwhere negatives for an image are picked from the texts in the batch. Similarly, we can define the\nloss when picking negatives for a text from images in the batch as $L_{cont}^{lorentz}(T, I)$. To extend such a\ncontrastive setup with our image-text compositions, we have to consider that due to the generalized\ninformation in a box, different images in a batch can have similar box-level information. To avoid\nunwanted negatives in a batch, we only contrast the box image with other entire images, and vice\nversa which have specific information. This avoids negative alignment between image-box pairs\nand boxes from different images. The final hierarchical Compositional Contrastive (hCC) loss is\nformulated as\n$hCC(I,T, I_{box},T_{box}) = \\frac{1}{4}(L_{cont}^{lorentz}(I,T) + L_{cont}^{lorentz}(T, I) + L_{cont}^{lorentz}(I_{box}, T) + L_{cont}^{lorentz}(T_{box}, I)).$ (6)\nintroduces hyper-\nbolic entailment cones that generalize the idea of partial order embeddings (Vendrov et al., 2016) by\nusing the inherent hierarchical structure of the hyperbolic space. Entailment cones define a region\n$R_q$ for every possible point q in the space such that all points $p \\in R_q$ are semantically linked to\nq as its child concepts. As such, points in $R_q$ are expected to contain specific information for the\ngeneral concept q. Considering the Lorentz model $L^n$, the half-aperture of these conical regions (Rq),\nis formulated by Le et al. (2019); Desai et al. (2023) as\n$\\omega(q) = \\sin^{-1} (\\sqrt{\\frac{2\\kappa}{\\kappa + ||\\vec{q}||}}),$ (7)\nwhere $-\\kappa$ is the curvature of the space and a constant K = 0.1 is set to limit values near the origin\n(see Ganea et al. (2018a)). The aperture inversely depends on the norm $||\\vec{q}||$. Inferring from this, a"}, {"title": "Hierarchical Compositional Contrastive (hCC) learning.", "content": "Hierarchical Compositional Entailment (hCE) learning."}, {"title": "Hierarchical Compositional (hC) learning.", "content": "We aggregate the losses to form the overall hierarchi-\ncal Compositional (hC) loss for HyCoCLIP by taking a weighted sum of the two loss components:\n$hC = hCC + \\gamma hCE.$ (12)\nIn Appendix B, we detail all hyperparameters, thresholds, and further implementation details."}, {"title": "Computational complexity.", "content": "Our approach enables us to double the amount of visual and textual\ndata to learn from. The training time scales linear with the increase in training volume; for ViT-B/16,\nHyCoCLIP requires 73 hours of training, compared to 46 hours for MERU and 45 hours for CLIP.\nWe note that our method inference maintains the same efficiency as CLIP and MERU and allows for\nscalable deployment."}, {"title": "3 EXPERIMENTS", "content": "We develop our models using grounded vision-language pairs. This could be human-\nannotated such as the Localized narratives subset of Open Images (Pont-Tuset et al., 2020) or\nthe Flickr30K Entities dataset (Plummer et al., 2015). However, the sizes of these datasets are\nfairly limited considering the intensive efforts of manual labelling. Hence, we depend on auto-\nmatic grounded information generated by pre-trained phrase grounding models. Several large-scale\ngrounded language-vision datasets are publicly available by Li et al. (2023) and Peng et al. (2023).\nWe train our models using the large-scale training corpus - Grounded Image-Text Pairs (GRIT) dataset"}, {"title": "3.1 BENCHMARK", "content": "Datasets"}, {"title": "3.2 DOWNSTREAM TASKS", "content": "To assess performance, we evaluate HyCoCLIP on several down-\nstream tasks. For zero-shot image classification, the label set is fitted\nto multiple prompts which are embedded using the text encoder and\nthen averaged to obtain a single embedding per label. The closest text\nembedding is picked from the collection as the predicted class for an\nimage. We report the model's accuracy on 16 image classification\ndatasets. Similarly, we assess our method on zero-shot retrieval tasks\nto determine if complex concepts, like scenes and captions, are accu-\nrately preserved in the representation space. Further, we evaluate the\nmodels on object detection task to analyze the regional understanding\nof HyCoCLIP. We also evaluate the hierarchical nature of HyCo-\nCLIP using multiple hierarchical metrics. Additionally, we assess the\nscene understanding capability of HyCoCLIP on two compositional\nbenchmarks - VL-Checklist (Zhao et al., 2022) and VG Attribution\n(Y\u00fcksekg\u00f6n\u00fcl et al., 2023).\nFrom Table 1, we find our repro-\nduced results for CLIP and MERU are fairly consistent with Desai\net al. (2023) even when trained with smaller batch size on RedCaps.\nOn grounding RedCaps and filtering noise, we notice only 5.8 million\nwhen adding local information. These results further highlight the need for our approach.\nNaively adding these boxes as additional samples is not effective because the boxes are often without\nbroader context, and the text is highly generic compared to the whole images. Only by optimizing for\ntheir hierarchical compositional nature as done in our approach is it possible to get better performance.\nOur method aims to obtain a hierarchically aligned representation space, but this is not necessarily\nbeneficial for the task of retrieval, where proximity of text and image embeddings is key. Regardless,\nour approach remains highly competitive.\nWe utilize pre-trained vision-\nlanguage models to recognize proposed object regions.\nSpecifically, we evaluate the scenario where ground-truth"}, {"title": "Zero-shot image classification", "content": "Zero-shot retrieval", "Hierarchical Classification": "A characteristic feature of hyperbolic spaces is their ability to represent"}, {"title": "3.3 ABLATION STUDY", "content": "We examine the impact of the terms in hCC (Eq. 6) and hCE (Eq. 11)\nlosses by pre-training the model several times, each time turning off a single loss term. We use the\ngrounded CC3M dataset and train for 40k steps. Table 4 shows the results of this experiment. A lower\naccuracy and recall on image classification and retrieval respectively, indicate a higher influence of\ncorresponding loss term. For hCC loss, we find that our hypothesis of contrasting the generalized\ninformation in boxes against entire images and text is indeed beneficial. For hCE loss, we see that the\nthe image (I) are most influential.\nWe train our models using a batch\nsize 768 according to available compute (Appendix B). To\nstudy the influence of this hyperparameter, we train our pri-\nmary baseline MERU-ViT-S using CC3M for various batch\nsizes and report their zero-shot performance on ImageNet\nclassification. Table 6 indicates no empirical benefits when\nworking with larger batch sizes. In the contrastive setting,\nthe number of positives grows linearly, and the number of\nnegatives grows quadratically in a batch. When using soft-\nmax, the ratio of positives to negatives affects loss functions\ndifferently depending on the type of similarity metric that\nis being used. This can explain the difference in batch size\nbehavior of our approach. The saturation of softmax loss with\nincreasing batch size has been previously discussed by Zhai\net al. (2023), and the entailment loss may also contribute to"}, {"title": "Pre-training loss terms", "content": "Scaling w.r.t batch size"}, {"title": "4 ANALYZING THE HYPERBOLIC SPACE", "content": "Visualizing the learned hyperbolic space We visualize\nthe learned hyperbolic space in lower dimensions to see if\nthe image, text, and corresponding box embeddings are dis-\ntributed in a proper semantic hierarchy. To this end, we plot\nthe distribution of the spatial norms of 128k random sam-\nples of training data in a histogram. Furthermore, we use\nHoroPCA (Chami et al., 2021) for reducing the dimension-\nality for 200 image-text pairs along with their boxes. Lastly,\nwe extract 50 principal components to suppress noise and use\nCO-SNE (Guo et al., 2022) to bring the embeddings to the\nlow-dimensional space.\nFig. 5a shows that the embedding distributions of texts and\ntheir corresponding boxes are well separated, while images\nand their box representations display similar norms. This\nspatial contraction in image embeddings arises from the con-\nvergence of contrastive loss within a confined entailment\ncone, as noted by Ramasinghe et al. (2024). Furthermore,\nInterpolating between points in hyperbolic space We interpolate the geodesic connecting an\nimage (source) with another image (target) and also with the origin similar to Desai et al. (2023),\nwhich have been visualized on the bottom-right of Fig. 6. This intuitively represents traversing"}, {"title": "5 RELATED WORK", "content": "Vision-language models Currently expanding at a rapid pace, this topic has been in focus for multi-\nple decades with initial works in image retrieval (Mori et al., 1999), semantic segmentation (Barnard\net al., 2003), and object classification (Wang et al., 2009) leveraging natural language descriptors\nfor computer vision. Later works (He & Peng, 2017) utilized more expressive representations from\nmulti-modal neural network encoders. The advent of transformers (Vaswani et al., 2017) and vision\ntransformers (Dosovitskiy et al., 2021) helped construct a highly semantic embedding space for\ntexts and images, respectively. Recent works have explored creating a shared embedding space by\nleveraging various pre-training strategies to integrate text and image information. We refer the reader\nto the survey by Gan et al. (2022) for a comprehensive overview. Many approaches use contrastive\nlearning as a core method, like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021). Zhao\net al. (2022) advanced this with RegionCLIP, which aligns image regions with textual concepts for\nobject detection. More recently, MERU (Desai et al., 2023) combines entailment learning (Ganea\net al., 2018a; Le et al., 2019) with the CLIP approach to learn embeddings in hyperbolic space\ncapturing latent visual-semantic hierarchies. We extend this to include image patches and caption\nparts, enforcing an ordering that reflects the hierarchy shared by both modalities.\nHyperbolic space for representation learning has desirable properties\nfor data with an inherent hierarchical or tree-like structure (Nickel & Kiela, 2017; Chamberlain et al.,\n2017). When generating embeddings in hyperbolic space from such data, its innate hierarchical\nstructure can be retained with minimal distortion. As a result, hyperbolic deep learning has rapidly\ngained traction (Peng et al., 2022; Mettes et al., 2023). Recent works have developed methods\nfor building neural networks that operate in hyperbolic space (Ganea et al., 2018b; Shimizu et al.,\n2021) and corresponding optimization algorithms (B\u00e9cigneul & Ganea, 2019; Bonnabel, 2013). This\nled to the use of hyperbolic models in many different modalities such as graphs (Liu et al., 2019),\ntext (Dhingra et al., 2018; Tifrea et al., 2019), images (van Spengler et al., 2023; Atigh et al., 2022),\nvideos (Long et al., 2020), etc. Other recent work has focused on combining embedding spaces of\ndifferent modalities (Liu et al., 2020; Desai et al., 2023). Our work similarly learns multimodal\nrepresentations in hyperbolic space to benefit from its inductive hierarchical bias.\nwords, sentences, and images to learn representations in a supervised fashion. They consider\nhypernym-hyponym relations in language to construct a hierarchy. This concept has been used in\nhypernymy detection tasks (Nguyen et al., 2017; Vulic & Mrksic, 2018). Hierarchies formed by\nconstituency-based parse trees have been used to learn embeddings in hyperbolic space by Dhingra\net al. (2018). In vision, several works sought to connect scenes to objects and parts of objects within\nthe scene. Early works have used such information for pose estimation, image segmentation, and\nobject and contour detection (Bourdev & Malik, 2009; Arbel\u00e1ez et al., 2011). Recently, un-/self-\nsupervised methods have been used for representation learning leveraging hierarchical segmentation\nof an image by Zhang & Maire (2020) and object-scene hypernymy by Xie et al. (2021); Ge et al.\n(2023). We combine hypernymy relations of vision and language."}, {"title": "Learning in hyperbolic space", "content": "Hierarchies in vision and language"}, {"title": "6 CONCLUSION", "content": "The idea of this work is to use object compositions within a scene and its description, along with\nthe visual-semantic ordering between image and text to learn hyperbolic representations that are\nsemantically and hierarchically aligned. Our proposed HyCoCLIP improves over standard CLIP\nand its recent hyperbolic extension MERU in zero-shot classification. Moreover, our approach has"}, {"title": "A GENERATING BOX INFORMATION", "content": "We derive box information using an image grounding pipeline similar to Peng et al. (2023). Given\nan image-caption pair, noun entities are initially extracted from the caption into a list using spaCy\n(Honnibal et al., 2020). To minimize noise, we remove abstract nouns such as {life, humor, love, ...}\nfrom the list. We then predict the bounding boxes of the extracted entities within the image using the\npre-trained grounding model GLIP (Li et al., 2022; Zhang et al., 2022). We exclude boxes with sizes\nlower than 32 \u00d7 32. We also threshold the predictions to at least 0.65 CLIP confidence score for the\ngenerated bounding box with the corresponding noun entity. Image-caption pairs for which no boxes\ncould be generated or retained while filtering, are dropped. Further, referring expressions for noun\nchunks taken from the dependency tree of the caption using spaCy, are also included as text boxes.\nThis increases the robustness of the stem towards linguistic complexities."}, {"title": "B IMPLEMENTATION DETAILS", "content": "We use a similar setup as Desai et al. (2023), where the language encoder is\nthe same one used by the original CLIP (Radford et al., 2021) consisting of a 12-layer Transformer\narchitecture (Vaswani et al., 2017) with a width of 512 dimensions. The maximum input token size\nis set to 77 with a vocab size of 49408. For the vision encoder, we use the small and base Vision\nTransformer (Dosovitskiy et al., 2021; Chen et al., 2021; Touvron et al., 2021) backbone using a patch\nsize of 16. The images are resized using border padding and random cropping (with scale [0.5, 1.0])\nto 224 \u00d7 224, which results in an input sequence size of 196. A fixed set of 2-D sine-cosine position\nembeddings is included in the input sequence to instill a positional inductive bias."}, {"title": "Model architecture", "content": "The curvature of the Lorentz\nmodel is made learnable with an initial value of k = 1.0. Similar to\nwe scale our batch of vectors before projecting it\nto the hyperboloid using learnable scalars  respectively,\nin both image and text modes. These scalars are initialized with a\nvalue of  The adaptive softmax temperature\nof the contrastive loss is initialized with and clipped at\nAll of these scalar values are learned in the logarithmic space.\nIn the hCE loss (Equations 10,11), we set separate values of the\nparameter for inter-modality entailments  and intra-\nmodality entailments through a hyperparameter search\nwhile pre-training on the CC3M dataset for 75k steps and evaluating"}, {"title": "C METRICS FOR HIERARCHICAL CLASSIFICATION", "content": "This section provides more details on the metrics used for our hierarchical classification experiment.\nFor a pair of predicted and true class (\u0177, y), the Tree Induced Error (TIE) is the\ndistance between \u0177 and y in the graph (cf. Fig. 7a). This is defined as We, where E(i, j)\nis the set of edges with weights along the path connecting nodes i and j. For the WordNet graph,"}]}