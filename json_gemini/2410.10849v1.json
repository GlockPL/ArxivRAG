{"title": "Continuous Approximations for Improving Quantization Aware Training of LLMs", "authors": ["He Li", "Jianhang Hong", "Yuanzhuo Wu", "Snehal Adbol", "Zonglin Li"], "abstract": "Model compression methods are used to reduce the computation and energy re-quirements for Large Language Models (LLMs). Quantization Aware Training (QAT), an effective model compression method, is proposed to reduce perfor-mance degradation after quantization. To further minimize this degradation, we introduce two continuous approximations to the QAT process on the rounding function, traditionally approximated by the Straight-Through Estimator (STE), and the clamping function. By applying both methods, the perplexity (PPL) on the WikiText-v2 dataset of the quantized model reaches 9.0815, outperforming 9.9621 by the baseline. Also, we achieve a 2.76% improvement on BoolQ, and a 5.47% improvement on MMLU, proving that the step sizes and weights can be learned more accurately with our approach. Our method achieves better performance with the same precision, model size, and training setup, contributing to the development of more energy-efficient LLMs technology that aligns with global sustainability goals.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs), due to their high computational power and memory requirements, have extremely high energy consumption in the training and inference phases [1]. Model compression methods are used to reduce the energy and computation requirements of LLMs [2].\nQuantization is a model compression method that reduces computational complexity by lowering the precision of model weights and activations. Many quantization techniques for Transformers have been proposed in recent years [2], including many effective Post Training Quantization (PTQ) methods such as SmoothQuant [3], OmniQuant [4], and APTQ [5]. Quantization Aware Training (QAT) is an effective technique for model compression [6] that often outperforms PTQ methods [2].\nAlthough there are not many related works dedicated to QAT of LLMs, the LLM-QAT work has achieved remarkable results and provided a good foundation for further studies [7]. This study proposes approaches for QAT of Transformers, including QAT for key-value cache and data-free QAT through knowledge distillation.\nWe aim to improve the accuracy and stability of the quantization process by enabling more effective learning of step size and quantized model weights. To enhance QAT in LLMs training, we adopt Sigmoid STE and propose SoftClamp, which are continuous approximations of the rounding function and clamping functions, respectively. We mainly delve into the theoretical underpinnings of our proposed methods and the empirical results demonstrating their effectiveness."}, {"title": "Background", "content": "Custom estimators To simulate quantization losses, QAT introduces a rounding function, the derivative of which at all differentiable points is 0. To generate non-zero gradients, STE was introduced [8], which is widely used in scenarios when the gradient of neuron network modules is not applicable for back-propagation [9]. For example, when training Binarized Neural Networks [10], STE is applied to approximate the originally non-differentiable binary networks. Considering the function f(x) = round(x), STE approximates $\\frac{df}{dx}$ as 1. Therefore, the gradient is passed directly during the back-propagation process.\nMany research attempts to improve the STE. A kind of research has improved the performance by designing new estimators, mostly differentiable approximation functions of STE, including PWL [11], MAD [12], HGTE [13], and so on. These works that propose customized estimators usually refer to the concept of \"gradient error\": the gradient descent process is negatively affected by the fact that STE does not fit well to the rounding function [13, 14]. However, a recent work suggests that gradient error does not affect performance under certain conditions [15].\nSelection of Scaling Factor In QAT using standard STE with symmetric quantization, the quantization loss is formulated by following equations:\nQ(x) = s \\times round [clamp(\\frac{x}{s}, a, b)],\nclamp(x, a, b) = max[min(x, b), a], (1)\nwhere s is the scaling factor, which can be selected in many ways. For example, MinMax [16, 17], also known as dynamic quantization, determines s by the range of actual inputs at each level. For symmetric quantization, the value of s by MinMax is $s = \\frac{max(X)}{2^{N-1}-1}$, where X is the input tensor, and N is the target quantization bit width. Another work introduces a learnable s: by applying STE, the gradient of s is available, which allows s to be a learnable parameter [18]."}, {"title": "Methods", "content": "Based on Equation 1, we adopt Sigmoid STE [14] and propose SoftClamp. The two methods are applied to the following two mechanisms respectively:\nRounding: Equation 1 involves rounding the clamped value. This simulates the precision limitation in quantization.\nClamping and Scaling: The clamping function clamp(x, a, b) = max[min(x, b), a] in QAT ensures that the input values are constrained within a specified range [a, b], simulating the range constraint of quantized representations."}, {"title": "Sigmoid STE", "content": "Sigmoid STE is a continuous approximation of the rounding function that replaces the STE, and experimental results support performance improvements by Sigmoid STE [14].\nSimilar to other custom estimators, Sigmoid STE is used to mitigate the gradient error, which is considered to have a negative impact on model's performance [14, 13]. The Sigmoid STE replaces the rounding function during backpropagation Equation 1. The following expressions show the formulation of Sigmoid STE, f, and its derivative, $\\frac{df}{dx}$:\n$f(x,\\tau) = \\sum_{i=0}^{[x]} \\frac{1}{1 + exp(\\tau(x - i))}$,\n$\\frac{df}{dx} = \\sum_{i=0}^{[x]} \\frac{\\tau \\cdot exp(\\tau(x - i))}{(1 + exp(\\tau(x - i)))^2}$ (2)\nAs shown in Figure 1(a), the parameter $\\tau$ in Sigmoid STE controls the degree of approximation. A higher value of $\\tau$ leads to sharper transitions, allowing a more accurate approximation of the rounding function. This increased sensitivity also modulates the gradient, as shown in Figure 1(b)."}, {"title": "SoftClamp", "content": "Recently, nonlinear rectifier units achieved great success in LLMs [19]. Inspired by this, we propose the SoftClamp mechanism, which can be formulated as\n$SoftClamp(x, a, b) := x \\times g(x - a) \\times g(b - x) + a \\times g(a \u2013 x) + b \\times g(x \u2013 b)$, (3)"}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": "We evaluate performance for both generation and question answering. For the generation part, we fine-tuned LLaMA-3-8B [24] on a subset of the FineWeb [25]. The evaluating metric is perplexity (PPL) on the WikiText-v2-test dataset [26]. For question answering part, we fine-tuned the instruct version of LLaMA-3-8B on auxiliary training questions in MMLU [27]. Then, models are evaluated on BoolQ [28] and MMLU test sets. We use 4-bit precision for weights and KV cache values, and 8-bit precision for activations."}, {"title": "Abalation", "content": "Based on our methods, models are fine-tuned separately, each with just vanilla STE (T = 0), vanilla STE and SoftClamp (T = 0 + SC), or Sigmoid STE and SoftClamp (T > 0 + SC) applied."}, {"title": "Question Answering", "content": "We also tested our method on the question answering dataset. The results on BoolQ and MMLU datasets are shown in Table 2.\nThe results of both metrics show the capacity of our method, demonstrating improvements of 2.76% on BoolQ, and 5.47% on MMLU."}, {"title": "Conclusion", "content": "In conclusion, we introduce continuous approximations of the rounding and clamping functions to improve QAT for LLMs. The Sigmoid STE and SoftClamp methods, when applied together, lead to better performance. We believe the continuous approximations create more sound gradient for both step size and weights, obtaining better parameters. Therefore, causing less performance degradation, QAT will be a more promising method for creating LLMs that require less computational power and energy.\nWhile our study demonstrates significant advancements, there are limitations to consider. Due to time and hardware limits, the dataset we used was relatively small, and we did not explore other combinations of precision, potentially limiting the scope of the improvements. Future research should address these areas to further optimize QAT for LLMs."}, {"title": "Appendix", "content": ""}, {"title": "Experimental Details", "content": "All models are trained with a single epoch. Our models are trained with five RTX4090s\nDuring all evaluation tasks, the temperature is set to zero.\nFor the FineWeb dataset, we utilize the CC-MAIN-2024-18 subset [25], which consists of 154.4 billion tokens.\nBoolQ [28]: Evaluated in a zero-shot setting using a User/Assistant style prompt. The score is reported as the percentage of correct responses.\nMMLU [27]: The score is calculated as the average of the subscores across different tasks. A 5-shot approach is used, with examples selected randomly from questions with the same topic."}]}