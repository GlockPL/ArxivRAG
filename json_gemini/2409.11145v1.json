{"title": "High-Resolution Speech Restoration with Latent Diffusion Model", "authors": ["Tushar Dhyani", "Florian Lux", "Michele Mancusi", "Giorgio Fabbro", "Fritz Hohl", "Ngoc Thang Vu"], "abstract": "Traditional speech enhancement methods often oversimplify the task of restoration by focusing on a single type of distortion. Generative models that handle multiple distortions frequently struggle with phone reconstruction and high-frequency harmonics, leading to breathing and gasping artifacts that reduce the intelligibility of reconstructed speech. These models are also computationally demanding, and many solutions are restricted to producing outputs in the wide-band frequency range, which limits their suitability for professional applications. To address these challenges, we propose Hi-ResLDM, a novel generative model based on latent diffusion designed to remove multiple distortions and restore speech recordings to studio quality, sampled at 48kHz. We benchmark Hi-ResLDM against state-of-the-art methods that leverage GAN and Conditional Flow Matching (CFM) components, demonstrating superior performance in regenerating high-frequency-band details. Hi-ResLDM not only excels in non-instrusive metrics but is also consistently preferred in human evaluation and performs competitively on intrusive evaluations, making it ideal for high-resolution speech restoration.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative speech restoration [1]-[4] has emerged as a robust solution, addressing the limitations of traditional [5], [6] and deep neural network (DNN) based discriminative approaches [7]-[9]. Inspired by speech processing in the human brain [10], [11], these methods use a two-stage mechanism [1]\u2013[3], consistently outperforming single-stage models [7], [8]. They not only overcome the limitations of compartmentalized models but also handle multiple distortions simultaneously [12]. Another classifier-based approach [13] shows potential for general audio effect removal but lacks the ability to generalize to simultaneous distortions in speech recordings, suggesting opportunities for enhancing existing methods.\nDNN-based discriminative methods excel at extracting information from noisy speech, while generative methods, conditioned on noisy inputs, mimic the underlying speech distribution. In a low signal-to-noise ratio (SNR) scenario, generative models may approximate the distribution incorrectly, producing erroneous phones, reducing intelligibility, and causing phonetic confusion. These incoherent generated phones are called breathing and gasping artifacts [14], [15]. Discriminative methods, however, often eliminate low-energy speech regions from low-SNR recordings, a limitation not seen in generative models [16].\nSingle-stage restoration methods often overfit during the filtering process [17]. To address the issue of generalizability, two-stage approaches have been effective for generative speech restoration [1], [3]. These methods typically involve an initial generative enhancement stage followed by a vocoding step for the mel-spectrogram inversion, often utilizing generative adversarial networks (GANs) [18]. However, a key limitation is that the first stage handles most of the enhancement, leaving the second stage to have no significant impact on restoration capability. An alternative solution involves combining a discriminative method with a generative model [15]. The primary advantage of this approach is that the generative model is tasked with filling in the missing information rather than reconstructing the entire signal, thereby addressing the limitations of the discriminative stage. However, this strategy underutilizes the generative model's full potential, as it is confined to refining residual details instead of performing comprehensive signal regeneration.\nTo address the stated limitations of existing two-stage approaches, we propose:\n\u2022 Hi-ResLDM, a novel two-stage framework, shown in Fig. 2, for robust speech restoration capable of handling multiple distortions simultaneously. Our approach combines discriminative and diffusion-based generative methods operating within the latent space [19] of an autoencoder. Hi-ResLDM is specifically designed for restoring speech to resemble studio-quality recordings, typically sampled at 48kHz.\n\u2022 We test restoration frameworks for iterative refinement, a popular technique in image restoration [20], and demonstrate that Hi-ResLDM returns consistent reconstructions across multiple refinements.\n\u2022 We compare Hi-ResLDM with popular publicly available models,"}, {"title": "II. METHOD", "content": "Given a monaural noisy signal $x(t) \\in \\mathbb{R}^T$ with sample rate r, it can be expressed as a mixture of clean speech sample $y(t) \\in \\mathbb{R}^T$, and some additive noise $n(t) \\in \\mathbb{R}^T$. Occasionally, the signal can be an impulse response $h(t) \\in \\mathbb{R}^{T_{60}}$, and various distortion functions such as clipping, low-pass filter, and resampling collectively represented by $d(t)$. So, it can be formulated as $x(t) = d((y(t) + n(t)) *h(t))$, where * denotes the convolution operation between the noisy input $y(t)+n(t)$ and the impulse response h(t). For the sake of simplicity, from now on, we will indicate all functions of time f(t) as f.\nTo simplify the process of distortion inversion, we adopted a two-stage approach that has demonstrated state-of-the-art performance in speech restoration tasks [1], [3], [23]. In the first stage (recovery stage), the distorted input x is fed into a model $\\Gamma_{\\theta} : x \\rightarrow x'$ that removes additive distortions and gives a clean, intermediate downsampled estimate $x'$. In the second stage (restoration stage), a model $\\Psi : x' \\rightarrow y'$ further processes $x'$ to regenerate the final clean speech estimate $y'$.", "A. Recovery stage": "The goal of the first stage is to increase SNR by removing the noise n from x. To achieve this, we decompose $\\Gamma_{\\theta}$ into two sub-steps: loudness normalization followed by a discriminative enhancement method. While a neural network can learn loudness normalization as a linear operation, we suggest that separating this step allows for finer control over the preprocessing of input signals. Isolating loudness normalization from the enhancement step amplifies the target energies in x, enabling the discriminative enhancement stage to focus more effectively on $x'$. Thus, the primary task of $\\Gamma_{\\theta}$ is removing n from x. In the input signal represented as $x = d((y+n) *h)$, the convolution operation is applied to the noisy part y + n. Due to the linear nature of convolutions, we can simplify it as $x = d(y*h+n* h)$ and as $n* h \\approx n$, we re-write the equation as $x = d(y * h + n)$. In this formulation, the discriminative enhancement stage is responsible for removing additive degradation from the input, allowing the second stage to focus on generating clean speech without being affected by irrelevant additive information. Since this stage recovers the signal from additive distortions, we refer to the first stage as the recovery stage.\nWe first resample x from r of 16kHz followed by normalization using PyLoudNorm [24]. To avoid clipping in low SNR x due to loudness normalization, we set the loudness to -20LUFS (Loudness Units Full Scale) rather than the ITU 1770 (International Telecom-munication Union) standard of -14LUFS. The normalized input was first converted into a time-frequency domain using a window size of 32ms and hop length of 8ms following the preprocessing steps from StoRM [15]. For enhancement, we used the discriminative NCSN++M from StoRM, chosen for its efficiency and fast inference while maintaining intelligibility. We retrained the network on our training dataset III-A1. The results of our NCSN++M model are presented in table I. The output is converted back to the time domain by inverting the complex spectrogram.", "B. Restoration stage": "In this stage, called restoration stage, we employ a latent diffusion model (LDM) $\\Psi$ [19], [25], [26] for the generation of the high-fidelity clean speech signal. Compared to other generative methods, LDM is more stable to train and has the capability of preserving high-frequency details with less computational resources than its spatial counterpart [19]. First, to train the LDM, $x'$ is converted from the time domain to a time-frequency domain X. A pre-trained general audio autoencoder is used to convert X to perceptually equivalent, lower-dimensional latent representation $z_0$ using only the encoder $\\epsilon$. $z_0$ is used as conditioning information to train the denoising diffusion probabilistic model (DDPM) [27]. Here, DDPM learns to estimate the latent representations $z_y$ from latents of target speech $z_y$ given $z_t = \\sqrt{\\bar{a}_t}z_y+\\sqrt{1-\\bar{a}}\\tilde{a}}\\tau\\eta$, $\\eta \\sim \\mathcal{N}(0, I)$ where $t \\in [0, T]$ is the time and $\\bar{a}_t$ is defined as in [28]. The decoder $\\epsilon^{-1}$ of the autoencoder converts the estimation $z_y$ to the reconstructed time-frequency representation $Y'$ which is further mapped to the estimation y' in the time domain using an inverter function $\\upsilon$.\nIn particular, we used AudioMAE [29] as the autoencoder, a U-Net for the model $\\Psi$, and the HiFi-GAN [30] vocoder as the function $\\upsilon$. The output of $\\Gamma_{\\theta}$, sampled at 16kHz, is first upsampled to 48kHz, converted into mel-spectrogram X, and then fed into $\\epsilon$, finally obtaining its latent representation $z_0$, which will be used as conditioning. Similarly, we also map the clean speech y to its latent representation $z_y$. Before performing DDPM, $z_y$ and $z_0$ are concatenated. We parameterize our approach as in [26], and thus, the training objective for our diffusion model becomes as shown in equation 1. In the end, the reconstructed mel-spectrogram Y' now represents a distortion-less restored clean-speech signal."}, {"title": "III. EXPERIMENTAL SETUP", "content": "1) Training dataset: As our goal is to restore distorted speech to clean, studio-quality speech, we require a dataset consisting of high-quality speech recordings, free from distortions, and sampled at 48kHz. We used several of the open-source datasets to train our individual stages. We used VCTK [31], [32], high-quality open-source speech datasets from OpenSLR\u00b9 [33]\u2013[36], and internal datasets. To further ensure the cleanliness of the speech recordings, we evaluated each file using the non-intrusive NISQA [37] metric and discarded all files having a mean opinion score (MOS) < 4. A threshold value of 4 was selected based on sufficient perceived quality. Our curated dataset was resampled to 48kHz and comprised a total of 1250 hours of clean recordings. We normalized the loudness of all the files to -20LUFS. These diverse datasets across multiple languages provide a versatile distribution of speech characteristics, balanced gender ratios, varying tones, and different accents. As another preprocessing step, silence from all datasets was removed for training, and each recording was split into chunks of 5.12 seconds.\n2) Evaluation dataset: We compared the performance of all approaches on a mixed set of speech enhancement and speech reverberation tasks. For evaluation, the VCTK test split was used following [1]. Speakers p232 and p257, with vastly different pitch values, were chosen, and all recordings were further distorted uniformly. For our internal training set, we augmented each recording with reverberation, as well as stationary and non-stationary noise, controlling the SNR values between -5dB and 10dB. Simulated reverberation was applied with reverberation times (T60) ranging from 0.1s to 0.5s. To ensure robustness, we maintained consistency in the distribution"}, {"title": "B. Evaluation protocol", "content": "The proposed approach is generative and, unlike discriminative methods, lacks one-to-one mapping with the target signal. This characteristic necessitates the use of non-intrusive evaluation metrics, as intrusive metrics require sample alignment between the predicted and target signals, rendering them ineffective for generative tech-niques. However, to evaluate our first stage, we used two intrusive metrics, extended STOI [38], which operates at 10kHz, and structural similarity (SSIM) at different sampling rates. Despite STOI's need for parallel mapping, it offers a basis for assessing speech intelligibility, whereas structural similarity provides a comparison of phonemic structural fidelity of the generated speech signal.\nFor the primary evaluation, we relied on two established standard non-intrusive metrics for speech evaluation: DNSMOS [39] and NISQA [37]. DNSMOS, operating on 16kHz audio, estimates overall perceptual quality, while NISQA, using smaller overlapping segments and two neural networks, penalizes artifacts such as pre-phonetic breathing and extended fricatives but emphasizes accurate phoneme generation.\nIn addition to intrusive and non-intrusive metrics, a key objective of Hi-ResLDM is to preserve the fidelity of generated phonemes. To assess phonemic confusion, we transcribed the output using the open-source Whisper-large model [40] and calculated the Word Error Rate (WER) by comparing the transcriptions with human-annotated transcripts of our evaluation dataset. We evaluate speaker consistency using speaker recognition cosine similarity (SR-CS), which calculates the distance between embeddings of target and restored files generated by speaker-verification model [41].\nTo evaluate the real-world performance of Hi-ResLDM, we con-ducted a subjective listening test in which 20 volunteer audio ex-perts participated. For the test, the participants were presented with recordings from our target models and were asked to rank the outputs based on perceived quality and intelligibility while penalizing any generative artifacts. The recordings used for the test were restored real-world samples taken from some public speeches, interviews, and call recordings.\nIterative refinement is a widely adopted technique in image super-resolution [20], where a low-resolution image is progressively en-hanced by conditioning each super-sampling model on the output of the previous iteration. To adapt this approach for speech restoration, we applied iterative restoration using the same model across multiple iterations. This process aimed to observe how distortions introduced by the model would accumulate over repeated restoration steps. We conducted experiments with five iterations on our internal test set,"}, {"title": "IV. RESULTS", "content": "We present the results of our evaluation in Table I, using the testing split of the Valentini dataset [22] - a standard for evaluation of speech enhancement methods - and our internal evaluation dataset as described in III-A2. The recovery stage performs exceptionally well on the Valentini dataset due to its focus on additive noise and achieves higher intelligibility. Despite being a generative approach, Hi-ResLDM shows strong performance on the intrusive eSTOI metric and achieves a low WER. We attribute the improvement in intrusive metrics primarily to the discriminative method employed in the recovery stage. The reduced WER emphasizes the importance of increasing the SNR before applying the generative process to enable more accurate reconstructions. Table II shows that Hi-ResLDM achieves high structural fidelity compared to the target signals at different sample rates. This underscores the effectiveness of conditioning the generative restoration stage on the clean estimate produced by the recovery stage, which significantly enhances the NISQA score.\nResemble Enhance (RE) outperforms all methods on the DNS-MOS metric, with Hi-ResLDM performing comparably to RE. Upon closer examination, we found that DNSMOS penalizes our restored signals specifically on the signal quality score (DNSMOS_SIG) while providing comparable scores for background noise removal and overall quality. A likely explanation is the presence of pre-phonemic breathing sounds in our training dataset, which is prominent in the wide-band frequency range. Since DNSMOS also evaluates within a wide-band frequency range, it incorrectly interprets short breathing sounds as signal issues, leading to a penalty in the signal quality score. Although these sounds can be removed with filtering, we explicitly avoid doing so as it would potentially introduce unforeseen biases and can affect the perceived quality of the outputs.\nVoiceFixer (VF) underperforms across all test data, exhibiting significantly reduced intelligibility on our internal dataset, particularly in cases of low SNR. This performance decline is largely due to hallucinations of the GAN-based model during phone regeneration.\nAnother recently proposed model named Universe++ [42], which improves Universe [12], has demonstrated considerable potential in enhancing regeneration fidelity. However, we could unfortunately not include it in our comparison, because it was trained on the datasets that are commonly used by the community for evaluation and testing purposes, which we utilized accordingly in our experiments. Reproducing their results is not feasible due to their reliance on proprietary internal datasets for training. Additionally, the Universe model is not publicly accessible, restricting broader comparative analysis.\nRegarding the iterative refinement, Fig. 3 illustrates the results over five iterations, indicating that none of the speech restoration approaches improved the speech quality. However, compared to the other models, Hi-ResLDM does not substantially degrade the overall speech quality, as measured by the NISQA MOS score, indicating greater robustness than the alternatives.\nFinally, Table III shows the results of our subjective evaluation, where evaluators preferred the output of Hi-ResLDM in an overwhelming majority of cases. While the CFM-based approach performed well, VoiceFixer was penalized for the lack of intelligibility and altering the speaker's pitch and tone. We confirm these inconsistencies through SR-CS as shown in Fig. 4."}, {"title": "V. CONCLUSION", "content": "In this work, we present Hi-ResLDM, an improved two-stage approach to speech restoration. The recovery stage in Hi-ResLDM assists the restoration stage, enabling the generation of studio-quality speech at a sampling rate of 48 kHz. Hi-ResLDM outperforms SOTA models utilizing GANs and CFMs, especially on NISQA, WER, and eSTOI. We also investigated the iterative refinement technique for audio, which, although successful in image restoration, did not result in noticeable improvements in speech restoration. Nonetheless, Hi-ResLDM exhibited consistent generation stability over multiple iterations. Furthermore, our model maintained high speaker consistency, and in subjective evaluations, the output was preferred in 60.83% of cases."}]}