{"title": "AutoMixQ: Self-Adjusting Quantization for High Performance Memory-Efficient Fine-Tuning", "authors": ["Changhai Zhou", "Shiyang Zhang", "Yuhua Zhou", "Zekai Liu", "Shichao Weng"], "abstract": "Fine-tuning large language models (LLMs) under resource constraints is a significant challenge in deep learning. Low-Rank Adaptation (LoRA), pruning, and quantization are all effective methods for improving resource efficiency. However, combining them directly often results in suboptimal performance, especially with uniform quantization across all model layers. This is due to the complex, uneven interlayer relationships introduced by pruning, necessitating more refined quantization strategies. To address this, we propose AutoMixQ, an end-to-end optimization framework that selects optimal quantization configurations for each LLM layer. AutoMixQ leverages lightweight performance models to guide the selection process, significantly reducing time and computational resources compared to exhaustive search methods. By incorporating Pareto optimality, AutoMixQ balances memory usage and performance, approaching the upper bounds of model capability under strict resource constraints. Our experiments on widely used benchmarks show that AutoMixQ reduces memory consumption while achieving superior performance. For example, at a 30% pruning rate in LLaMA-7B, AutoMixQ achieved 66.21% on BoolQ compared to 62.45% for LoRA and 58.96% for LoftQ, while reducing memory consumption by 35.5% compared to LoRA and 27.5% compared to LoftQ.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs) has revolutionized various natural language processing (NLP) tasks, such as machine translation (Zhang, Haddow, and Birch 2023; Sato et al. 2020), sentiment analysis (Zhang et al. 2023a; Deng et al. 2023), and speech recognition (Min and Wang 2023). Despite their impressive capabilities, the resource consumption required to obtain a fine-tuned model suitable for specific tasks remains substantial due to the large number of parameters and high computational demands of LLMs (Frantar and Alistarh 2023). To address these issues, various compression techniques, including pruning (Ma, Fang, and Wang 2023; Xia et al. 2023), quantization (Shao et al. 2023;"}, {"title": "2 Background and Motivation", "content": "Low-Rank Adaptation (LoRA)\nFor a LLM consisting of n layers, the weight matrices at each layer, denoted as W, undergo updates through an update matrix \u0394W. This update matrix is decomposed into the product of two low-rank matrices A and B, where $A \\in \\mathbb{R}^{d \\times r}$ and $B \\in \\mathbb{R}^{r \\times d}$, with r being the rank, a hyperparameter fixed across all layers. In this approach, the original weight matrix W remains frozen, while only \u0394W, represented by the product AB, is updated. The forward computation can be expressed as:\n$f(x) = (W + \\Delta W)X + b = (WX + b) + (AB)X$. (1)\nGiven the rank r is typically much smaller than the dimension d, the number of parameters is significantly reduced from d\u00b2 to 2dr. This optimization can reduce the trainable parameters during the learning."}, {"title": "2.2 Quantization", "content": "Quantization. Quantization is an essential technique used to reduce the computational and memory overhead of large-scale models by converting high-precision numerical values, such as a 32-bit floating-point number $X^{HP} \\in \\mathbb{R}$, into a lower-bit integer representation $X^{INT} \\in \\{0, 1, ..., 2^N - 1\\}$. This process is mathematically expressed as:\n$X^{INT} = round((2^N - 1)F(X^{HP})), \\qquad (2)$\nwhere F(): \u211d \u2192 [0, 1] is a normalization function. A typical method is uniform quantization, where F(X) is defined as $F(X) = \\frac{X - X_{min}}{X_{max} - X_{min}}$. An alternative approach introduced by QLORA Dettmers et al. (2024) is 4-bit NormalFloat Quantization (NF4), which assumes that the data follows a normal distribution $X \\sim \\mathcal{N}(0, \\sigma^2)$ and applies $F(X) = \\Phi(X / \\sigma)$, with \u03a6(\u00b7) representing the cumulative distribution function of a standard normal distribution.\nDequantization. To recover the high-precision values from their quantized forms, a lookup table T is used, which is defined as:\n$T[i] = F^{-1} (\\frac{i}{2^N - 1}); \\qquad i = 0, 1, ..., 2^N - 1, \\qquad (3)$\nallowing the integer $X^{INT}$ to be mapped back to its simulated high-precision counterpart $X^{D} \\in \\mathbb{R}$. The dequantization process can be represented as:\n$X^D = T[X^{INT}]. \\qquad (4)$\nSimulated Quantization for Matrices. In practice, it is often more efficient to use simulated quantization for matrices rather than directly operating on quantized values (Bai et al. 2020; Shen et al. 2020). In this method, quantized weight matrices are stored as encoded integers and are temporarily dequantized into simulated high-precision matrices during multiplication operations. This process is denoted by $q_N(\\cdot): \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{m \\times n}$, where $\\mathbb{R}^N: \\{T[i] \\in \\mathbb{R} | 0 < i < 2^N\\}$.\nLoftQ (LoRA-Fine-Tuning-aware Quantization), introduced by Li et al. (2023), is a method that addresses the performance degradation commonly observed when applying quantization alongside LoRA fine-tuning. LoftQ mitigates this issue by iteratively refining the quantized weights and their low-rank approximations, thus reducing the discrepancy between the quantized model and its full-precision counterpart. This process enhances the initialization for LORA fine-tuning, resulting in improved performance on downstream tasks, particularly in low-bit quantization settings."}, {"title": "2.3 The Motivating Example", "content": "Efficient fine-tuning of LLMs on resource-constrained devices requires effective model compression and fine-tuning techniques. We explored the combination of pruning, quantization, and LoRA fine-tuning to achieve this goal. Structural pruning reduces model size by removing less important parameters, but due to the varying importance of different layers (Zhang et al. 2023c), it often results in uneven pruning across layers. This uneven pruning leads to a complex and unbalanced network structure, and standard quantization and fine-tuning processes, which typically apply a uniform configuration across all layers, may not be the optimal choice.\nTo explore better configurations, we employed mixed-precision quantization, assigning different computational resources and complexities to different layers, with the aim of"}, {"title": "3 Method", "content": "Problem Formalization\nInspired by the motivating example in Section 2.3, fine-grained mixed-precision quantization provides a promising approach to explore the upper bounds of performance for pruned models during efficient fine-tuning.\nConsider a pruned LLM with L layers. For each layer i (i \u2208 {1, 2, ..., L}), we assign a mixed-precision quantization configuration qi, which specifies the bit-width used to represent the weights and activations of the LoRA matrices during fine-tuning. The overall configuration across all layers is represented as a vector $q = [q_1, q_2, ..., q_L]$.\nLet P(q) denote the performance of the model on a downstream task under the quantization configuration q, and let M(q) denote the corresponding memory consumption. Our objective is to optimize the trade-off between these two conflicting goals: minimizing memory consumption while maximizing performance.\nThe search space S represents the set of all possible quantization configurations, structured as a tensor with dimensions L \u00d7 n\u00b2, where n is the number of quantization options. Each element in this tensor corresponds to a specific configuration that can be applied to the model layers."}, {"title": "3.2 Performance Model and Pareto Optimality", "content": "Performance Model. To alleviate the computational burden of exhaustive search, we introduce the performance model. This model is designed to predict the performance of unseen configurations based on a subset of mixed-precision quantization configurations that have undergone actual fine-tuning.\nThere are several potential architectures for this model. One approach is to use a data-driven method, such as a Multi-Layer Perceptron (MLP) or Transformer model. In this case, it is crucial to consider the encoding of input configurations, ensuring that the model effectively captures the relationships between different layers and quantization precisions. To enhance generalization across various models"}, {"title": "3.3 The Workflow of AutoMixQ", "content": "Building upon the problem formalization and the introduction of our key components, we now present the workflow of AutoMixQ, which integrates these elements into a cohesive optimization pipeline.\nThe workflow is outlined in the figure 1. The process begins with a fine-tuning dataset for a given model and task, which includes initial performance and memory consumption data under various mixed-precision quantization configurations. These initial data points, while not strictly necessary for models like Gaussian Processes (GP), provide a solid starting point for the iterative process."}, {"title": "4 Experiments", "content": "Experimental Setup\nLLMs and Benchmarks. To demonstrate how AutoMixQ performes on different model, we test it on three open source large language models: LLaMA-7B (Touvron et al. 2023), LLaMA-13B (Touvron et al. 2023) and Vicuna-7B (Zheng et al. 2024), and specific version is stated in the Appendix A. We conduct these LLMs on zero-shot classification tests for commonsense reasoning datasets, including BoolQ (Clark"}, {"title": "4.2 Details of The Optimization Workflow.", "content": "We illustrated the optimization process, memory, and time footprint of AutoMixQ using a 50% parameter pruning rate on llama-7b as an example. We fine-tuned 10 sets of configurations as the initialization for the Gaussian Process (GP) (this is not mandatory; in other experiments, we found that starting from scratch, a good configuration could be found in about 10 iterations). In each configuration, the quantization precision for all model layers was randomly selected between 4-bit and 8-bit. On average, obtaining data for each initialization took approximately 25 minutes. We set the total number of iterations for AutoMixQ to 40 (resulting in 50 data points for constructing the Pareto front) to ensure the best configuration was found. The entire process took approximately 16.5 hours. During AutoMixQ iterations, GP required around 7s to suggest the next configuration, while the prediction process and Pareto frontier construction consumed approximately 187MB memory. In Figure 2, we present the optimization results for BoolQ and Winograd. More detailed processes and results for other benchmarks are provided in Appendix C."}, {"title": "4.3 Main Results", "content": "We tested models with different pruning rates across the benchmarks mentioned earlier to compare the performance of our method against the baselines. We report the performance and peak memory usage of the models under different"}, {"title": "Different Tasks Analysis.", "content": "Across various tasks, AutoMixQ consistently demonstrated significantly lower memory usage compared to LoRA and LoftQ, with an average reduction of 33% relative to LoRA and 25% relative to LoftQ. Despite this, AutoMixQ consistently outperforms LoftQ and, in most cases, surpasses LoRA as well. For example, on the ARC-c dataset, known for its challenging multiple-choice questions that require deep reasoning, AutoMixQ achieves the best performance when fine-tuning LLaMA-7B, outper-"}, {"title": "4.4 Ablation Study", "content": "In this part, we use LLaMA-7B with 20% pruning rate and a configuration obtained by the AutoMixQ method to conduct ablation experiments and all results are shown in Table 3.\nDtype of 4-bit. Performance was compared using different 4-bit data types. The performance of NF4 and FP4 is different, which shows that the AutoMixQ method has no dependence on the quantization type.\nAdapter initialization Method. Different initial weights for the adapter layers are tested LoftQ, Gaussian, PiSSA (Meng 2024). There is no obviously dominant initialization method, but it shows that the AutoMixQ method is effective in different initialization methods.\nAdapter Iteration Count. In the performance comparison of the LoftQ method matrix initialization iterations 1, 2, and"}, {"title": "5 Related Work", "content": "Efficient Compression of LLMs\nLLM-Pruner (Ma, Fang, and Wang 2023) uses structured pruning to eliminate non-essential interconnected structures by leveraging gradient information. This technique enables compressed models to maintain good performance across multiple tasks with basic fine-tuning. Santacroce et al. (2023) proposes Globally Unique Movement (GUM), a novel pruning technique focusing on the sensitivity and uniqueness of LLMs' network components. GUM prunes neurons that uniquely contribute to the model output and are sensitive to loss changes, thus preserving high accuracy. This method optimizes the trade-off between information retention and computational efficiency. SparseGPT (Frantar and Alistarh 2023) is a pruning method that transforms the process into a series of large-scale sparse regression problems, solvable through Hessian matrix inversion without retraining. It efficiently prunes large models to high sparsity in a single step while maintaining high accuracy. Wanda (Sun et al. 2023) prunes LLMs by selectively removing weights based on their sizes and input activations, adaptively adjusting sparsity levels to reduce more than half without sacrificing accuracy. Quantization-Aware Training (QAT) combines quantization with full model fine-tuning to adapt models for downstream tasks (Peri, Patel, and Park 2020; Liu et al. 2023). Although QAT is effective, it requires substantial computational resources, such as gradient calculations and optimization states, and it complicates the gradient computation for quantized weights. However, by leveraging LORA, these challenges can be bypassed during task adaptation. Post-Training Quantization (PTQ) frameworks, such as GPTQ and SmoothQuant (Frantar et al. 2022; Xiao et al. 2023), use a small subset of training data to calibrate high-precision models, enabling the generation of task-specific quantized models without the need for gradient backpropagation. This makes PTQ more cost-efficient than QAT, although it generally results in lower accuracy."}, {"title": "5.2 Parameter Efficient Fine-Tuning", "content": "Houlsby et al. (2019) introduce a transfer learning method incorporating adapter modules into pre-trained Transformer models, efficiently handling various NLP tasks with few additional parameters while achieving performance comparable to full fine-tuning. LLM-Adapters (Hu et al. 2023) integrate small adapters with few extra parameters into LLMs for efficient fine-tuning, allowing smaller models to perform as well as larger ones on specific tasks. Unlike the serial approach of adapters, low-rank adaptation (LoRA) (Hu et al. 2021) uses a parallel method to insert trainable rank decomposition matrices into each layer of the model's architecture. LoRA adds trainable matrices to each layer while keeping the pre-trained weights unchanged, reducing the number of trainable parameters and making model adaptation faster and less resource-intensive. LoRA-FA (Zhang et al. 2023b) freezes the projection-down weight of the LoRA layers and only updates the projection-up weight, reducing the memory requirements for fine-tuning. QLora (Dettmers et al. 2024) combines low-rank adapters and quantized 4-bit weights for efficient LLM fine-tuning, significantly reducing GPU memory requirements while achieving performance comparable to full 16-bit fine-tuning. LoftQ (Li et al. 2023) applies quantization and low-rank approximation alternately to achieve a good initialization for LoRA fine-tuning, mitigating the discrepancy between quantized and pre-trained weights, and enabling efficient fine-tuning of quantized models, particularly in challenging low-bit regimes."}, {"title": "6 Conclusion", "content": "We propose a novel fine-tuning framework, AutoMixQ, which organically combines pruning, quantization, and LORA to achieve high-performance fine-tuned models under low-resource conditions. After formalizing the problem as an optimization problem, AutoMixQ employs an end-to-end automatic optimization flow, integrating lightweight performance models with Pareto optimality to rapidly self-adjust the quantization precision of model layers, thereby achieving the desired objectives. Evaluations conducted on popular benchmarks confirm that AutoMixQ delivers excellent memory efficiency and performance. Through an automatic optimization process, from the perspective of pruning, AutoMixQ achieves accuracy that more closely aligns with the original model; from the perspective of quantization, it ensures optimal resource allocation; and from the perspective of fine-tuning, it strikes a superior balance between performance and memory usage. Moreover, its self-adjusting nature ensures its broad applicability."}, {"title": "A Version of LLMs", "content": "We provide the Hugging Face link of LLMs used in the experiment: LLaMA-7B: https://huggingface.co/baffo32/decapoda-research-llama-7B-hf; Vicuna-7B: https://huggingface.co/lmsys/vicuna-7b-v1.5; LLaMA-13B: https://huggingface.co/yahma/llama-13b-hf"}, {"title": "B Hyperparameters", "content": "In the optimization of the pruned LLaMA-7B model, a comprehensive hyperparameter configuration was employed to ensure an optimal balance between model performance and computational efficiency. The model was fine-tuned with a learning rate of 3 \u00d7 10\u22124, utilizing a batch size of 128, further divided into micro batches of 4 to manage memory constraints effectively. Sequences were standardized to a maximum length of 256 tokens, and a dropout of 0.05 was applied specifically to the LoRA layers targeting projections such as query, key, value, and output, alongside gate, down, and up projections. Quantization was dynamically applied at 4-bit and 8-bit levels according to layer requirements to optimize memory use without compromising computational accuracy. The training employed the paged AdamW optimizer with 32-bit precision, enhancing stability and efficiency. These settings were methodically tested and optimized through the Optuna framework to ensure robust model performance and resource utilization."}, {"title": "C Results of Optimization Workflow", "content": "In this section, we will use the LLaMA-7B model with 50% pruning as our example to illustrate the Pareto optimization workflow, as shown in Figure 4"}, {"title": "D Performance in LLAMA-13B", "content": "We list the performance of the configuration described in Section 4.1 for LLaMA-13B in Table 4."}, {"title": "E Sample Generation result", "content": "In this section, we present the results on the LLaMA-7B model using the input prompt \"The universe is the entirety of space, time, matter, and energy that exists.\" We compared the outcomes generated by LoRA, LoftQ, and our model. The results demonstrate that our model significantly outperforms LoftQ and closely approximates LoRA, indicating the high accuracy of our model. Figure 3a We also provide the generation result for Vicuna-7B in Figure 3b. For the Vicuna-7B model, our prompt was \"10 steps to build an iOS app.\" We observed that the results generated by AutoMixQ were significantly better than those from LoftQ and, compared to LoRA, were more reasonable and accurate."}]}