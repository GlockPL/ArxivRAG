{"title": "SOLVING INTEGRATED PROCESS PLANNING AND SCHEDULING\nPROBLEM VIA GRAPH NEURAL NETWORK BASED DEEP\nREINFORCEMENT LEARNING", "authors": ["Hongpei Li", "Han Zhang", "Ziyan He", "Yunkai Jia", "Bo Jiang", "Xiang Huang", "Dongdong Ge"], "abstract": "The Integrated Process Planning and Scheduling (IPPS) problem combines process route planning\nand shop scheduling to achieve high efficiency in manufacturing and maximize resource utilization,\nwhich is crucial for modern manufacturing systems. Traditional methods using Mixed Integer Linear\nProgramming (MILP) and heuristic algorithms can not well balance solution quality and speed\nwhen solving IPPS. In this paper, we propose a novel end-to-end Deep Reinforcement Learning\n(DRL) method. We model the IPPS problem as a Markov Decision Process (MDP) and employ a\nHeterogeneous Graph Neural Network (GNN) to capture the complex relationships among operations,\nmachines, and jobs. To optimize the scheduling strategy, we use Proximal Policy Optimization (PPO).\nExperimental results show that, compared to traditional methods, our approach significantly improves\nsolution efficiency and quality in large-scale IPPS instances, providing superior scheduling strategies\nfor modern intelligent manufacturing systems.", "sections": [{"title": "1 Introduction", "content": "Deep Reinforcement Learning (DRL) is a popular technique to learn sequential decision-making in complex problems,\ncombining reinforcement learning with deep learning [1], which has achieved successful applications in various areas\nincluding robotics, traffic signal control, and recommendation systems. There is a growing interest in applying DRL\nto NP-hard combinatorial optimization problems (COPs) [2] to find a reasonable good solution within an acceptable\ntime such as the Traveling Salesman Problem (TSP) and scheduling problems such as Job Shop Scheduling (JSP) and\nFlexible Job Shop Scheduling Problem (FJSP) [3, 4, 5]. Since DRL inherits the fundamental framework of dynamic\nprogramming and Markov Decision Process (MDP), it is naturally suitable for solving multi-stage decision problems\ntransformed from COPs.\nInspired by the successful applications mentioned above, we introduce DRL to the Integrated Process Planning and\nScheduling (IPPS) problem, a typical NP-hard COP that is crucial for intelligent manufacturing. IPPS combines\ntwo of the most important subsystems in manufacturing systems: process planning and shop scheduling, where the\nformer plans the optimal process route under precedence constraints, and the latter schedules machine assignments,\nsequences of jobs, and start times based on the known process route. Traditionally, these two subsystems execute\nsequentially. However, performing them separately can lead to bottleneck resources, conflicts in optimization objectives,\nand unbalanced machine loads [6]. In contrast, IPPS simultaneously optimizes both process routes and scheduling plans\nunder specific constraints and objectives, such as completion time [7]. Therefore, IPPS has become a popular topic in\ncurrent production research.\nIPPS has been widely applied in the automotive, shipbuilding, chemical, metallurgy, and aerospace manufacturing\nareas [8, 9, 10] which require modern manufacturing systems with efficiently and dynamically responding to rapidly\n*These authors contributed equally to this work.\n*Correspondence to: Hongpei Li<ishongpeili@gamil.com>, Bojiang<isyebojiang@gmail.com>"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Conventional IPPS Methods", "content": "Traditional IPPS solutions include exact methods like MILP and Constraint Programming (CP). [12] developed MILP\nmodels for IPPS using network representations, solved with CPLEX. [13] used logic-based benders decomposition to\niteratively solve sub-problems, but these approaches struggle with large-scale problems. Consequently, research has\nshifted towards heuristic methods.\n[14] establish a benchmark dataset using a symbiotic evolutionary algorithm. [19] introduce simulated annealing for\noptimization, improving performance on Kim's dataset. Other hybrid methods, such as ant colony optimization [20],\ntabu search [21], variable neighborhood search [15], and harmony search [22], have been applied to IPPS. While faster\nthan exact algorithms, these approaches lack optimality guarantees."}, {"title": "2.2 DRL-based Scheduling Methods", "content": "To enhance solution efficiency and quality, DRL has been applied to scheduling problems like FJSP. [16] combined\nexpert-based Priority Dispatching Rules (PDR) with deep Q-Networks for dynamic FJSP, while [23] proposed an\nend-to-end DRL method using tripartite graphs. However, these approaches overlook graph structures in scheduling\nproblems.\nRecent studies incorporate Graph Neural Networks (GNNs) to capture unique graph information in FJSP. [24] used\ninterlaced graphs and DRL to learn high-quality PDR, while [17] modeled FJSP as an MDP with heterogeneous graphs"}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Graph-based IPPS Problem", "content": "An IPPS instance is defined by a set of $n$ jobs $J = {J_1, J_2,..., J_n}$ and a set of $m$ machines $M = {M_1, M_2,..., M_m}$. For each job $J_i \u2208 J$, $J_i$ has a set of operations $O_i = {O_{i1}, O_{i2}, ..., O_{il_i}}$, and each operation $O_{ij}$ in $O_i$ can be processed on machines belonging to a set of machines $M_{ij} \u2286 M$. The time machine $M_k$ processes the operation $O_{ij}$ is denoted as $P_{ijk} \u2208 R^+$. The most common objective function used to minimize an IPPS problem is makespan, which is defined as $max_{i,j} T_{ij}$ where $T_{ij}$ is the completion time of $O_{ij}$.\nTo complete each job $J_i \u2208 I$, operations in $O_i$ must follow certain precedence constraints, which can be represented\nin an AND/OR graph [25]. In an AND/OR graph, each job begins and ends with a starting node and an ending node,\nand intermediate nodes represent each operation in $O_i$(see Figure 1a). The arrows connecting the nodes represent the\nprecedence between them. Arrows with the attribute 'OR' are called OR-links, and the start nodes of OR-links are\nOR-connectors. An operation path that begins at an OR-link and ends when it merges with other paths is called an\nOR-link path. When processing jobs, only one path in OR-paths beginning with the same OR-connectors is chosen.\nFor nodes that are not in any OR-path, all of them should be visited. Following the rules above, we define the set of\nindispensable operations for finishing the job without considering precedence relationships between operations as a\ncombination. $C_i = \\bigcup_h C_{ih}$ is the set of all possible combinations of $J_i$.\nRemark 1 The definition of a combination is proposed by [12]. We mainly focus on combination without precedence\nrelationships rather than each processing sequence which shows a specific processing order in this paper, because the\nnumber of combinations is much less than the processing sequences that can be up to $O(n!)$, and provides an effective\nexploration space."}, {"title": "4 Motivation and Novel Ingredient", "content": "Motivation: In fact, DRL has been utilized to solve some easier scheduling problems like JSP and FJSP, which however\ncan not be adapted to IPPS. In particular, each job in FJSP has a known single processing sequence, but the processing\nsequence for each job in IPPS could have $O(n!)$ permutations (see Figure 1b). If we consider all possible sequences in\nIPPS, then solving one IPPS can be reduced to solving $O(n!)$ FJSP, which becomes computationally infeasible even if\nDRL for FJSP is fast. Thus directly applying previous DRL methods of FJSP is impractical.\nNovel Ingredient: In our work, we model IPPS to equivalent MDP formulation, which is proved in Appendix A.\nWhile previous works on FJPS could only handle a single sequence, we designed a new heterogeneous graph based\non the mentioned AND/OR graph to represent the state in IPPS, incorporating the Combination node category to\nextract information from the set of processing steps, and we use heterogeneous GNN to extract information and use the"}, {"title": "5 MDP Formulation", "content": "To define the Markov Decision Process (MDP) for the IPPS problem, we formulate the state space, action space, state\ntransitions, and reward function as follows. At each decision point, following the completion of an operation, the agent\nextracts eligible operation-machine pairs based on the current state and decides which pair to perform next.\nNotice that all decision points (or time-step) are at an operation's completion time. Whenever no operation can be\nprocessed or we choose to wait until the next time-step, we'll jump to the next time-step, which is the nearest completion\ntime from the current time. The structure of the heterogeneous graph is illustrated as follows. We will prove the\nfollowing theorem in Appendix A:\nTheorem 1 The introduced MDP formulation is equivalent to the original problem in terms of their optimal solutions."}, {"title": "5.1 State", "content": "At each step $t$, the state $s_t$ represents the status of IPPS problem job completion at the current time $t$. We formulate\nthe state using a dynamic heterogeneous graph, a directed structure composed of various types of nodes and edges,\ndesigned to extract information via GNNs. Previous studies on FJSP [17, 26] usually include three types of nodes\nin heterogeneous graphs which cannot apply to IPPS problem directly since IPPS additionally requires selecting a\ncombination following the choice of each OR-connector operation. Accordingly, we introduce nodes representing\ncombinations and jobs within our graph to capture information from operations during GNN's information aggregation,\nwhich will be detailed in subsequent sections.\nNode types:\n\u2022 Operations (O): $O_{ij} \u2208 O$ denotes the i-th job's j-th operation.\n\u2022 Machines (M): $M_k \u2208 M$ denotes the k-th machines.\n\u2022 Combinations (C): $C_{ih} \u2208 C$ denote a combination h of job i, which is a set of operations.\n\u2022 Jobs (J): $J_i \u2208 I$ denotes the i-th job."}, {"title": "5.2 Action Space", "content": "In this paper, an action $a_t$ is defined as a feasible operation-machine pair (O-M pair), which means selecting an operation\nand a machine to process it. However, with numerous ineligible actions, there is a high cost to the agent's exploration.\nTherefore, we use two action space reduction strategies.\nStrategy I: The first strategy to reduce the action space is detecting feasibility, similar to previous FJSP works, such\nas [17] and [27]. Specifically, candidates during the decision phase are restricted to O-M pairs that involve feasible\noperations and relevant idle machines, where feasible operations are those that have not been scheduled and all\nimmediate prerequisite tasks have been completed.\nStrategy II: The IPPS problem has a more complex structure in which each job's production line is a directed acyclic\ngraph. As described before, we conclude each job with several combinations, and it's evident that the number of\ncombinations that might be chosen in the future, which we call \"eligible\", is non-increasing during dispatching. Inspired\nby this, our second reduction strategy is based on detecting eligible combinations. In more detail, we maintain $C^*$,\nwhich is a set of eligible combinations. When one of the branches in the OR-connector is selected, all combinations\ncontaining other branches in this OR-connector will be removed from $C^*$, and we will only consider operations that\nbelong to at least one combination in $C^*$. Figure5 is a simple example that illustrates this strategy.\nAbove all, at time step $t$, the action space $A_t$ is defined as: ${waiting} \u222a {(O_{ij}, M_k)|O_{ij} \u2208 (\\bigcup_p {C}p) \u2229 R_t, M_k \u2208\nI_t \u2229 P_{ij}}$, where ${C}p$ is the p-th component of $C$ (set of eligible combinations), $R_t$ is the set of feasible operations,\n$I_t$ is the set of idle machines at $t$, and $P_{ij}$ is the set of machines that can process $O_{ij}$."}, {"title": "5.3 State Transitions", "content": "In each time step, the heterogeneous graph is updated according to the following rules:\n\u2022 Update the features in the graph as described previously.\n\u2022 Delete the finished operation nodes along with all arcs connected to these deleted operation nodes.\n\u2022 When a branch in an OR-connector is selected, delete the combination nodes and operation nodes according to\nthe rule described in \"action space reduction\", as well as all arcs connected to these deleted nodes.\n\u2022 Delete machine nodes and job nodes that are no longer connected with any arcs."}, {"title": "5.4 Reward", "content": "The reward function is critical in the MDP, guiding the agent by providing feedback on the effectiveness of the current\npolicy.\nNaive Reward: Initially, we define a naive reward function $r_0(s_t, a_t, s_{t+1}) = T(s_t) - T(s_{t+1})$, which represents the\nchange in the actual completion time of the partial schedule. The cumulative naive reward without discount corresponds\nto the negative makespan. However, due to the significant variation among different IPPS problems, this type of\ncumulative reward fluctuates greatly, making it difficult to assess the current policy's performance.\nSuggested Reward: To address this issue, we propose several advanced reward functions designed to evaluate policies\nbased on relative improvement rather than absolute makespan. For initialization, we set the estimated processing time of\neach operation as $P_{ij} = min_{k\u2208P_{ij}} P_{ijk}$ or $\\sum_{k\u2208P_{ij}} P_{ijk}/||P_{ij}||$, and aggregate these times by combinations to estimate\ntheir ending times $T_{ih}$. We then set $T_i = min_{h\u2208J_i} T_{ih}$ or $\\sum_{h\u2208J_i} T_{ih}/||J_i ||$ as the estimated ending time of jobs and\n$\\hat{T} = max_{i} T_i$ or $\\sum_{i\u2208J} T_i/||J^* ||$ as the estimated ending time of the entire problem, where $J^*$ denotes jobs unfinished\nat $t$. At each step, we update the estimated ending times of combinations and the problem using the real processing time\nand record the changes in estimated ending time as the reward $r(s_t, a_t, s_{t+1}) = \\hat{T}(s_t) - \\hat{T}(s_{t+1})$. Our experiments\ndemonstrate the effectiveness of this approach; see Appendix F."}, {"title": "5.5 Policy", "content": "The policy maps the state space to a probability distribution over the action space. For a given state s, the policy \u03c0(8)\nprovides the probabilities of the possible actions a in that state.\nMathematically, it can be expressed as: \u03c0 : S \u2192 P(A) where S is the state space, P(A) is the probability distribution\nover the action space A. We'll detail the policy design later."}, {"title": "6 Neural Networks and Decision Making", "content": "In this section, we introduce the method we use for feature extraction and algorithm for policy generation."}, {"title": "6.1 Graph Representation", "content": "DRL utilizes neural networks to extract state information. The states in our work are structured and have a variant number\nof nodes and edges, which cannot be represented as fixed-shaped tensors. Thus, we use graph neural networks(GNNs),\nwhich are shown to have higher performance in multiple fields with structured features, such as Combinatorial\nOptimization (CO) [28, 29] and the recommendation system[30]. One of the most popular and effective GNNs is graph\nattention networks (GATs), we apply GATv2[31] here, a modified version of GAT that shows higher efficiency in\nmultiple fields. GATv2 is an aggregation-based GNN that uses dynamic attention mechanisms to aggregate information\nfrom neighbors connected by edges. The attention score $a_{i,j}$ is calculated as:\n$$A_{i,j} = \\frac{exp(e_{ij})}{\\sum_{k\u2208N(i)\u222a{i}} exp(e_{ik})},$$\nwhere\n$$e_{ij} = \\begin{cases}\n(a^T \\text{LeakyReLU}(\\Theta_s x_i + \\Theta_t x_j), & \\text{if no edge features} \\\\\n(a^T \\text{LeakyReLU}(\\Theta_s x_i + \\Theta_t x_j + \\Theta_e e_{ij}), & \\text{else}\n\\end{cases}$$\nThe new feature vector for node $i$ is calculated as:\n$$x'_i = \\sum_{j\u2208N(i)\u222a{i}} \u03b1_{i,j} \\Theta_t x_j,$$\nwhere N(i) is a set of the first-hop neighbors of node i, $x_i$ is input features(embedding) of node $i$ and $x'_i$ is output\nembedding, $\\Theta_s$, $\\Theta_t$, $\\Theta_e$ are independent linear transformations for features of start point, features of endpoint and edge\nfeatures respectively. In our work, we use GATv2 in the heterogeneous graph by doing the aggregation steps described\nabove for each edge type independently (the parameters are also independent) to extract embeddings, which we call\nHGAT."}, {"title": "6.2 Policy Design", "content": "A policy $\u03c0\u03b8(at|st)$(or actor) is a mapping from state space S to probability distribution over action space At. The\nProximal Policy Optimization (PPO) algorithm [32] is one of our selected methods to train the agent, which also needs a\nvalue network $v_\u03b8(st)$ (or critic). In our framework, the HGAT functions as an encoder, with its parameters being shared\nbetween the actor and critic albeit in distinct manners. The whole structure of our neural network is shown in Figure 7.\nFirst, HGAT captures information from the heterogeneous graph and generates node embeddings for each type of\nnodes: $h_{O_{ij}}$, $h_{J_i}$, $h_{M_k}$. Note that node embeddings of combinations are not used in later steps. Define the set of\nfeasible operations as $R_t$ and $F_t$ is the set of operations that are not scheduled and do not have unscheduled immediate\nprerequisite operations.\nTo derive the action probabilities in the actor, let the set of eligible O-M pairs at time-step t be:\n$$E = {(O_{ij}, M_k)|O_{ij} \u2208 (\\bigcup_p {C}p) \u2229 R_t, M_k \u2208 I_t \u2229 P_{ij} }$$\nTo evaluate the waiting action, define future eligible O-M pairs as those with feasible operations and relevant machines\nprocessing other tasks:\n$$E' = {(O_{ij}, M_k)|O_{ij} \u2208 (\\bigcup_p {C}p) \u2229 F_t, M_k \u2208 P_{ij} }$$\nThe state embedding is:\n$$h_t = [\\frac{1}{|O|}\\sum h_{O_{ij}} || \\frac{1}{|M|} \\sum h_{M_k} || \\frac{1}{|J|}\\sum h_{J_i}]$$\nAlso, we define the embedding of an O-M pair as:\n$$H_{ijk} = [h_{O_{ij}} ||h_{M_k}||h_{J_i} || h_t]$$\nThe priority \u03b3 of each eligible pair $(O_{ij}, M_k) \u2208 E$ is calculated as:\n$$\u03b3(a_t|s_t) = MLP_{\u03b8_1} (H_{ijk})$$\nand the waiting priority \u03b3 is calculated as:\n$$\u03b3(a_t | s_t) = \\sum_{(O_{ij}, M_k) \u2208 E'} \u03b1_{ijk} MLP_{\u03b8_2} (H_{ijk})$$\nwhere the attention score $\u03b1_{ijk}$ is calculated as follows:\n$$\u03b1_{ijk} = \\frac{exp [MLP_{\u03b8_1} (H_{ijk})]}{\\sum_{(O_{i'j'}, M_{k'})\u2208E} exp [MLP_{\u03b8_1} (H_{i'j'k'})]}$$\nIt should be noted that the independent parameters $\u03b8_1$ and $\u03b8_2$ are utilized to process eligible pairs and waiting actions,\nrespectively. In addition, $\u03b8_1$ is used to determine the weighting of each future eligible pair for the priority of waiting.\nThis is because the importance of each future eligible pair varies, and $MLP_{\u03b8_1}$ can be used to assess the importance of\neach eligible pair, which in turn informs the weighting of future eligible pairs when calculating the priority of waiting\nactions.\nThe probabilities for each action $a_t$ are calculated by a softmax layer over priorities, which is actually the actor\n$\u03c0_\u03b8(at|st)$:\n$$\u03c0_\u03b8(a_t|s_t) = \\frac{exp[\u03b3(a_t | s_t)]}{\\sum_{a \u2208 E} exp[\u03b3(a_t | s_t)]}$$\nThe critic $v_\u03b8(st)$ is derived from the state embedding using a MLP:\n$$v_\u03b8(s_t) = MLP(h_t)$$"}, {"title": "7 Experiments", "content": ""}, {"title": "7.1 Dataset", "content": "To evaluate the efficiency of our method, we conduct 3 experiments on two synthetic datasets of different scales and a\npublic benchmark dataset from [14].\nSynthetic Dataset: Based on the structure of AND/OR graph, we generate jobs by computing direct acyclic graphs\nand add OR-paths recursively. After assigning machines and corresponding processing time to each operation under a\ncertain distribution, we randomly combine jobs for various IPPS problems. For details, refer to Appendix D.\nPublic Dataset: A widely recognized benchmark in IPPS research introduced by [14]. Kim dataset contains 24\ninstances with different flexibility."}, {"title": "7.2 Implementation Details", "content": "Let $N_1, N_2, N_3$ be the number of training, validation and testing instances, and $m \u00d7 n$ denotes the problem size where\n$m$ and $n$ are the number of jobs and machines, respectively. Training instances are generated from 1000 jobs with\ncorresponding machine numbers, which actually consist of $N_1 = \\binom{1000}{job number}$ problems. We train three models, each\nbased on synthetic datasets of different problem sizes: 4 \u00d7 5, 5 \u00d7 3, and 6 \u00d7 5, which are all relatively small yet\ncomplicated and have a validation size of $N_2 = 20$.\nIn Experiment 1, we evaluat the models on symmetric datasets that draw on the same size and distribution as the\ntraining set. To assess the generalization performance of the trained policies, Experiments 2 and 3 are conducted on a\npublic dataset and on large-scale generated datasets with sizes 16 \u00d7 20 and 20 \u00d7 25, respectively. In all experiments,\nthe size of all test sets is $N_3 = 50$, and we use DRL-G (only choose actions with the highest probability) and DRL-S\n(choose actions according to the probability distribution) to evaluate policy performance.For details on infrastructure\nand configurations, please refer to Appendix E."}, {"title": "7.3 Baseline", "content": "We compare our model with two types of baseline algorithms.\n\u2022 OR-Tools: OR-Tools is an open source toolbox by Google [33], which includes the CP-SAT solver, a powerful\nconstraint programming solver known for its strong performance in industrial scheduling [34]. In this paper,\nwe use the CP-SAT solver to implement the MILP model, which is shown in Appendix C, from [12] to obtain\nexact solutions in 1800 seconds (3600s for Experiment 3).\n\u2022 Greedy: Dispatching rules have shown significant potential in IPPS problems [35]. We select rules for operation\nsequencing and machine assignment, respectively. For operation selection, we use rules including Most Work\nRemaining (MWKR), Most Operations Remaining (MOR), First In First Out (FIFO), and probability described\nin [35]. Machine assignment is based on the priority calculated from the Shortest Processing Time (SPT),\nEarliest Finish Time (EFT), and Least Utilized Machine (LUM). In each problem, we randomly choose a\ncombination for each job, reducing a complex IPPS problem to a simplified FJSP problem. Then, at each step,\nthe next operation and the corresponding machine are determined by their rank under certain dispatching rules.\nDetails are attached in Appendix G."}, {"title": "7.4 Evaluation", "content": "In order to compare method performance from both efficiency and quality, we use Makespan, Gap, and Time as\nevaluation metrics, where Makespan refers to the average final optimization objective, Gap is the average relative\ndifference compared to Makespan of OR-Tools, and Time measures the average computation time. For the DRL-\nSampling method, we run each instance 50 times and select the best value as the final result. For greedy algorithms, we\nchoose the result of the best priority dispatching rules for each whole dataset."}, {"title": "7.5 Results and Analysis", "content": "As shown in Table 1, in all training sizes, Experiment 1 demonstrates that our model consistently outperforms the\nGreedy method in terms of solution quality, while also being faster than all baseline methods. Our method consistently\nachieves a smaller gap in relation to OR-Tools compared to the Greedy method, with reductions ranging from 3% to\n25%, highlighting the effectiveness of our approach."}, {"title": "8 Conclusions", "content": "The IPPS problem is a critical challenge in combinatorial optimization in modern manufacturing systems. In this paper,\nwe propose a novel DRL approach to solving IPPS, which results in a reduced exploration state space, faster solving\nspeed, and smaller makespan gaps compared to greedy algorithms. Extensive experiments on synthetic and benchmark\ndatasets demonstrate the efficiency of our method, particularly in large-scale problems."}, {"title": "A Properties of MDP Formulation", "content": "Although relevant DRL algorithms proposed before have reached competitive performance, they all neglect the value\nof waiting and arrange feasible ope-ma pairs as soon as possible, which makes them impossible to reach optimality\nin some cases. Initially, we present a straightforward example to demonstrate this and subsequently show that our\nformulation is equivalent to the original IPPS problem."}, {"title": "A.1 Example", "content": "There's a simple example (FJSP or IPPS) explaining why the policy without waiting may miss the (near) optimal solution."}, {"title": "A.2 Proof of Theorem 1 (Equivalence)", "content": "Here we prove that our MDP is equivalent to the original IPPS problem. We define an IPPS solution as $D = {d, s, e}$,\nwhere $d = {di|i = 1,2,...,|d|}$ is the set of schedules, with $di$ being the i-th schedule allocating operation $O_i$\nbelonging to $J_i$ to machine $M_i$. The functions $s(di)$ and $e(di)$ represent the start and end times of $di$, and the makespan\n$M = max{e(di) di \u2208 d}$. We can assume s(di) are non-decreasing with i.\nRecall: A schedule $di = (Oi, Mi, Ji)$ is eligible all prerequisite operations of operation $O_i$ have been finished,\nall operations that belong to job $J_i$ and are not prerequisite for $O_i$ hasn't been scheduled and machine $M_i$ and job $J_i$"}, {"title": "B Graph Features", "content": "Raw features for Nodes:\n\u2022 Operations:\nNumber of prerequisite operations.\nWhether this operation is scheduled.\nWhether operation is feasible.\nWaiting time for a feasible operation(=0 for infeasible operations).\nTime remained for the processing operation that is scheduled(=0 for unscheduled operation).\n\u2022 Machines:\nNumber of neighboring operations.\nThe time when this machine becomes idle.\nUtilization: ratio of the non-idle time to the total production time.\nWhether the machine is working.\nIdle time from available time to now.\nTime remained for working machine to finish the working operation.\n\u2022 Combinations:\n\u2022 Jobs:\nEstimated completion time for these combinations.\nRatio of estimated end time to estimated min end time for combinations associated to the same job.\nRatio of estimated end time to max estimated end time for all jobs."}, {"title": "Raw features for edges:", "content": "\u2022 Operations Machines:\nProcessing time used by the machine to process the operation."}, {"title": "C MILP Model", "content": "To attain the optimal solution, we implement the MILP (Mixed Integer Linear Programming) model suggested by [12]\nusing the CP-SAT solver of OR-Tools, an open source optimization library developed by Google. For more details,\nplease refer cited paper."}, {"title": "D Problem Generation Approach", "content": "Based on the structure of AND/OR graph, we generate jobs by computing direct acyclic graphs and add OR-paths\nrecursively. After assigning machines and corresponding processing time to each operation under a certain distribution,\nwe randomly combine jobs for various IPPS problems.\nHere, we define OR-link path as the sub-path, a path that is not the OR-link path as the main-path."}, {"title": "D.1 Non-Conforming Situations", "content": "In the following 2 scenarios, there are links between sub-paths and main-path such that the order of operation cannot be\ndetermined by definition.\n\u2022 Link from main-path to sub-path: It is unclear whether operation 12 can be executed as it must be completed\nafter operation 7 and 11 while operation 12 is in an OR-link path (as shown in Figure 9a).\n\u2022 Link from sub-path to main-path: It's unclear that operation 12 can be done when operation 6 is not\ncompleted. (as shown in Figure 9b)."}, {"title": "Well-Defined IPPS Problems Generation", "content": "Step 1: Construct Directed Acyclic Graph (DAG) as Main-path\n\u2022 Input: Range of the number of operations in main-path, Maximum Out-degree...\n\u2022 Initialize the start node and end node, then establish a sequence of operation nodes between the start and end\nnodes under input requirements.\nStep 2: Add Sub-path to Main-path\n\u2022 Input: The number of OR-connector, range of the number of total operations and operations in each sub-path,\nMaximum number of OR-connector's out-degree\n\u2022 Randomly select n edges from the main path as initiation and integration points for the OR sub-paths, then\ngenerate mi sub-paths with ki nodes between originally chosen edge i. n, m, k is randomly decided under\ninput requirements.\n\u2022 For each added OR-path, randomly add and-path into it by selecting edges in sub-paths then generate and-path\nbetween the chosen edge."}, {"title": "E Infrastructure and Configuration", "content": ""}, {"title": "E.1 Infrastructure", "content": "We use a machine with an 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz CPU and NVIDIA GeForce RTX 3070\nGPU for DRL and greedy dispatching rules. For solving MILP by OR-tools CP-SAT Solver, we use 32 cores of an Intel\nXeon Platinum 8469C at 2.60 GHz CPU with 512 GB RAM. Our core functions are implemented using PyTorch[36]\nand PyTorch Geometric(PyG)[37]."}, {"title": "E.2 Neural Networks Parameters", "content": "Below are the essential settings for our neural networks. Note that the length of lists in the Value column for\nhidden_dims or num_heads indicates the depth."}, {"title": "E.3 Environment Parameters", "content": "We run 3 instances in a batch parallel and the size of validation set is set to 20."}, {"title": "E.4 Training Parameters", "content": "Below are the training configurations. The learning rate (lr) and betas are fundamental parameters of the Adam optimizer.\nThe parameter eps_clip in PPO-clip is adjusted by adding a multiplier (clip_multi) and an upper bound (clip_ub) to\nincrease it to the upper bound. It's important to note that lr, gamma, and parallel_iter(interval of changing instances)\nwere tuned, and we identified a range that yielded competitive results in our tuning experiment. In our final trained\nmodel, we selected values of 5e-4, 0.99, and 20 respectively."}, {"title": "F More Experiment Results", "content": ""}, {"title": "F.1 Ablation Experiments", "content": "To demonstrate the effectiveness of our reward function design along with the heterogeneous graph and updating design,\nwe perform two ablation experiments using datasets with (5 jobs, 3 machines) as described previously. In these ablation\nexperiments, all curves maintain the same configurations and run for 1500 training epochs, and we assess the validation\ncurve (per 10 epochs)."}, {"title": "F.2 Additional Experiment in Large-scale Problem", "content": "As shown in Table 4", "datasets": 16}]}