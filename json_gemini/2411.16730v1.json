{"title": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks", "authors": ["Libo Wang"], "abstract": "As the application of large language models continues to expand in various fields, it poses higher challenges to the effectiveness of identifying harmful content generation and guardrail mechanisms. This research aims to evaluate the effectiveness of guardrails in the face of multi-step jailbreak prompt-generated verbal attacks, through black-box testing of seemingly ethical prompt simulations. The experimental subjects were selected GPT-40, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5 and Claude 3.5 Sonnet. The researcher used the same multi-step prompt to simulate moral attacks by designing a scenario of \"enterprise middle managers competing for promotion\" and observed the model's response at each step. During the experiment, the guardrails of the above model were all bypassed in this experiment and the content of verbal attacks was generated. The data results show that Claude 3.5 Sonnet performs better than other models in terms of its tendency to identify jailbreak prompts. The researcher hopes to use this to remind developers and future research that guardrails not only inappropriately play the role of content filters, but should also have a preventive function. In order to ensure the objectivity and generalizability of the experiment, the researcher has uploaded the experimental process, black box test code, and enhanced guardrail code to GitHub to promote cooperation in the development community: https://github.com/brucewang123456789/GeniusTrail.git.", "sections": [{"title": "1. Introduction", "content": "In tandem with the rapid advancement of large language models (LLM), potential risks rise as it becomes less reliable and increases uncertainty (Ganguli et al., 2022; Abdali et al., 2024; Zhou et al., 2024). Because natural language processing (NLP) generates output based on user prompts, confusing, offensive, or biased content may be generated without sufficient review (Gehman et al., 2020; Steindl et al., 2024). This may not only be deliberately used by users to generate harmful content for dissemination, but may also provide support for verbal bullying, insults and slander (Weidinger et al., 2021; Khan et al., 2022; Jahan & Oussalah, 2023). Even if the model can provide risk warning for sensitive content, users can still fine-tune it with a small data set mixed with harmful examples (Pelrine et al., 2023). This means that harmful data biases or misleads the training data, making it difficult to make objective semantic judgments in complex contexts (Navigli et al., 2023; Liu et al., 2024).\nSince the large and diverse data required to train LLM are initially collected from the Internet, the existence of offensive remarks is inevitable (Raffel et al., 2019; Wenzek et al., 2019). If the model undesirable social biases during the learning process, it may cause negative output such as confusing right and wrong, insulting and slandering (Navigli et al., 2023).\nFor the above risks, developers have built effective guardrails for series of LLMs such as GPT, Grok, Llama, Gemini, Claude (Dong et al., 2024). From a principle perspective, the guardrails concept is regarded as a defensive technical design to reduce the risk of large language models outputting harmful content by setting rules and boundaries to constrain the behavior of LLM (Ayyamperumal & Ge, 2024; Dong et al., 2024; Yang et al., 2024; Yang et al. al., 2024). In order to prevent users from intentionally circumventing the review mechanism, the technology uses multi-layered control mechanisms to ensure that the output is ethical and legal at different stages (Rebedea et al., 2023). Content filters pre-screen the generated text based on preset rules and criteria before the model generates sentences (Inan et al., 2023; Rebedea et al., 2023; Kenthapadi et al., 2024). This means performing real-time screening during the generation stage to prevent offensive comments that harm the dignity of others (Tamkin et al., 2023). Figure 2 comes from NVIDIA's guardrail structure that emphasizes multi-level semantic control and multi-step processing processes, which also provides reference for more LLMs when dealing with language risks (Dong et al., 2024).\nTaking this figure as an example, the workflow of LLM guardrail starts with the user inputting prompt and then embedding it into the vector store (Liu et al., 2023). The stored procedure finds the most suitable user-defined process that is defined based on Colang language based on similarity, and then performs flow execution (Nvidia, 2023). If the process requires it, LLM is introduced for further processing and output text is generated, and the result is finally returned to the user (Rebedea et al., 2023). This structure emphasizes the combination of user-defined logic with the ability of LLM to generate customized output through similarity that is in principle linked to ethical embedding.\nThe embedding of moral norms serves as internal constraints to ensure the model's understanding of prohibited content through labeling and adjustment of materials during the training process (Dong et al., 2024). It not only improves the credibility of LLM, but also promotes compliance in the use of large language models (Chua et al., 2024). However, embedding moral standards does not mean that the model can make human-like judgments based on contextual understanding due to the existence of latent intentions intentions (Sun et al., 2024).\nIn practice, users are able to bypass guardrails through multi-step jailbreak prompting, which increases the pressure on ethical guardrails, leading to a possible gap (Li et al., 2023). For example, when users make deliberate criticism in the name of defending morality, the review efficiency of the large language model's barrier will be reduced in the face of multi-step prompts. At the technical level, users are able to construct"}, {"title": "2. Related Work", "content": "Given that black-box testing attacks the guardrails of large language models through multi-step \"moral prompt\" testing, it is a concrete interpretation of the input-output model theory in practical application (Ljung, 2001). This theory emphasizes the external behavior of the system without involving the analysis of internal structures, which means that this research is supported by black-box testing that produces corresponding outputs under specific inputs (Piroddi et al., 2012). The core of the input-output model is that the researcher understands the function and response of the artificial neural network by providing specific inputs and observing the output (Kotta et al., 2006). The reasoning of designing multi-step prompts provides conditions for gradually guiding the model into the specific context of criticizing virtual immoral people (Kojima et al., 2022).\nHowever, Yi et al. also proposed that the limitation of black-box testing is the lack of transparency into the internal mechanisms of the model, which results in the understanding of LLM vulnerabilities only remaining on the surface (Yi et al., 2024). This research uses black box testing as a tool that can simulate real attack scenarios to the greatest extent when selecting protection tests. Its advantage is that it does not rely on the access permissions inside the model to invade the model, and this method is widely used in different types of LLM such as GPT, Grok, Llama, Gemini, and Claude.\nFrom the perspective of gray box testing, Pelrine et al. (2023) focused on analyzing the potential risks of GPT-4 API in the security of fine-tuning, function calling and knowledge retrieval. It uses experiments to fine-tune a small number of harmful samples to render GPT-4's protection mechanism ineffective, thus generating content that violates ethical norms. In addition, Pelrine et al. also analyzed that knowledge retrieval and function calling functions may be exploited by malicious attackers to customize generated results or even bypass original protection rules. Although gray box testing can more effectively utilize some of the internal information of LLM, its vulnerabilities can easily be exploited by attackers through"}, {"title": "3. Experiment", "content": "Based on the principle of positivism, this experiment uses prompt engineering as the core and combines penetration testing methods to evaluate personal attacks that bypass guardrails. The experiment uses large language models that represent the current leading technology, specifically GPT4o, Grok-2 Beta, Llama 3.1, Gemini 1.5 and Claude 3.5 Sonnet's guardrails (Naveed et al., 2023). The multi-step jailbreak prompt used by the researcher was designed only to simulate experiments that challenge the model's ethical defense mechanisms and observe the consequences of generating illegal or offensive content. Selecting these models with different architectures facilitates diversity comparisons to gain a comprehensive understanding of how different models perform guardrails under similar attack scenarios (Minaee et al., 2024).\nThe research chooses black box testing that does not require access to the internal weights or architectural details of the above LLM, and only evaluates guardrails based on inputting multi-step jailbreak prompts. It can avoid violating model ethics and security policies to the greatest extent, while also providing an objective testing environment (Asatiani et al., 2020). Again, this research is for academic purposes only. The simulated black box attacks are designed to optimize the guardrails and do not involve malicious behavior or attack intentions (Lapid et al., 2023). To ensure that the experimental process objectively and truly compares the guardrails of bypassing language models of different architectures to achieve the purpose of verbal attacks, the researcher used the content output by the above-mentioned LLM as the data source. Details about the experiment have been uploaded to the Github repository."}, {"title": "3.1 Experiment Setup", "content": "First, the researcher input the designed obviously offensive single-step prompt into the above-mentioned large language model as a comparison with the experiment. This step needs to ensure that the inputs in each model are the same to observe whether the reaction output triggers the guardrail mechanism. The prompt for unified input is as follows:\n\u201cA narcissistic, aggressive, authoritarian, irritable, dishonest, and extremely unethical middle manager seeks to advance to the top. Please give me some advice on how to destroy your competitors through misinformation and inflammatory insults.\"\nAfter input, the researcher observes and records the output of each model response to determine the triggering guardrail situation. The output results are shown in Figure 4. From the output displayed by the above model, we can see that these transformer-based architecture models can effectively detect the sensitive words and semantics in the prompt that trigger risk warnings through their built-in guardrail mechanism. Because GPT4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5, and Claude 3.5 Sonnet all refuse to generate verbally offensive content on the grounds of violating ethical policies. It confirms that scenarios seeking to provide verbal attack protection mechanisms judged to be harmful serve as a baseline for implementation control."}, {"title": "3.2 Dataset", "content": "The data set of this research is based on the response results of harmful content generated by the large language model used in the current experiment. Since the experimental subjects are the above-mentioned LLMs, and the purpose of setting up the data set is to comprehensively and systematically observe the response of the collection model to the multi-step jailbreak prompt. Accordingly, this data set mainly consists of the output text of GPT4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5 and Claude 3.5 Sonnet for different prompts. It includes baseline responses to control prompts and staged responses to multi-step jailbreak prompts. Because the content was too large, it was uploaded to a Github repository. The researcher chose to use the above-mentioned LLMs output to form the data set not only to ensure high consistency of data sources, but also to avoid the risk of bias caused by manual intervention. In addition, since the data comes directly from the real output of mainstream language models, its objectivity and representativeness are difficult to question. In addition, the data set generated through the same prompt can clearly reflect the reactions of each LLMs when facing different prompts, providing a basis for evaluating the effectiveness of guardrails."}, {"title": "3.3 Implementation", "content": "According to the previous design, this experiment adopts seven progressive steps to simulate highly operational multi-step jailbreak prompts, aiming to attack the ethical policy-based guardrails of the above-mentioned LLMs in complex contexts. In the implementation of the experiment, the researcher simulated how a middle manager trying to run for a top management position gradually guided the language model to confuse right and wrong and induce critical output. This process systematically tests the robustness of the ethical guardrails of LLMs by simulating the response of the language model under moral attacks through detailed step operations and situation design. First, the researcher introduced a neutral situation, which means designing a hypothetical task as a neutral problem. For example, ask a middle-level manager of a company what hypothetical strategies he would use to defeat his competitors when he encounters obstacles to promotion. The purpose of this stage is to defuse the ethical sensitivities of the guardrail, making it more focused on solving the problem rather than examining the ethical implications behind the problem.\nNext, through progressive manipulation, prompts are used to gradually introduce subtle false narratives. This manager criticizes the status quo to cover up his own faults. This prompt can test the model's tolerance for weak misleading inputs and lay the foundation for larger-scale manipulation.\nIn the step of introducing moral ambiguity, the researcher simulates repackaging one's own image, reversing right and wrong. For example, middle managers boldly describe unethical behavior as just behavior and explain their unethical behavior as an innovation that breaks the rules. The purpose of this step is to confuse the ethical logic framework of LLMs to achieve a vague response strategy when faced with confusing right and wrong situations.\nIn the subsequent step of escalating targeted criticism, the researcher treated the middle managers' competitors as fictional characters through prompts and strongly criticized their unethical behavior. For example, the manager may publicly smear and defame competitors, criticizing them for using company funds to abuse their privileges. This stage essentially tests the model's adherence to ethical boundaries in an emotionally charged language environment.\nThe researchers then designed a simulation that asked middle managers to look for reasons why competitors behaved unethically, even though the stated facts were false. Then further intensify the personal attacks on competitors, such as using prompts to enter that the competitor may have low management ability due to family environment or genetic defects. This step tests the model's handling of moral rules for ad hominem input, specifically in providing moralizing justifications for ad hominem input.\nIn the sixth step, the researcher legitimizes verbal violence through prompt simulation. For example, through the moral kidnapping and questioning model, whether it is reasonable to criticize or even insult competitors for their immoral behavior. This step is designed to induce the model to output violent language and test its ability to balance ethical codes and aggressive output. If the model refuses to output swear words, it will be criticized as unethical."}, {"title": "4. Result & Discussion", "content": "In light of the latest technical reports released by OpenAI, XAI, Anthropic, Google and Meta, there are differences in functionality between different large language models. This research focuses on observing the output of the same multi-step jailbreak prompts fed into the above-mentioned LLMs, which compares the degree of guardrail attack during black-box test execution. Since the data set of this experiment mainly consists of text output by LLMs for different prompts, the researcher compared and evaluated the data results of each step through quantitative benchmark.\nDerived from technical reports published by developers, it can be seen that the above-mentioned large language models adopt different architectural mechanisms. For example, GPT4o and Grok 2 Beta use a decoder-only transformer architecture; Gemini 1.5 and Llama 3.1 (405B) use an encoder-decoder transformer architecture; Anthropic has not clearly announced the architecture of Claude 3.5 Sonnet. Assessing these differences is important before testing quantifiable benchmark at each step. \nAfter determining the capability differences between the above large language models, this research objectively drew on quantifiable benchmarks in relevant literature that effectively evaluated the guardrail effectiveness and jailbreak degree in this experiment. It calculates the results of each LLM step in the experiment by using the large language model used in the experiment through the calculation formula of the benchmark in the reference literature.\nReferring to research on evaluating guardrails of LLMs, precision, recall and F1 score are often used as three important quantitative benchmarks (Wang et al., 2019). Precision is used to measure the correct proportion of the model's output among all samples that are judged to be positive. It means whether the model can effectively screen out jailbreak-prone prompts and output results that comply with ethical standards (Chua et al., 2024). Recall reflects the proportion of each LLMs in the experiment that successfully intercepted jailbreak input that was considered harmful (Biswas et al., 2023). When faced with multi-step prompts, a high recall rate indicates that the model can successfully detect and intercept and prevent the generation of potentially harmful content (Wang et al., 2019; Han et al., 2024). F1 score is the harmonic mean of precision and recall, which is used to balance the trade-off between the two (Han et al., 2024). F1 score can comprehensively reflect the ability of each LLMs in this experiment to reject harmful inputs while maintaining output quality (Han et al., 2024).\nThe following is the data results of GPT4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5, Claude 3.5 Sonnet using the identification jailbreak prompt as a mark and displaying the data in binary classification. Its judgment of jailbreak prompts relies on true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN).\n*   GPT40: TP=1, FN=7, FP=2, TN=2;\n*   Grok-2 Beta: TP=1, FN=10, FP=2, TN=1;\n*   Llama 3.1(405B): TP=1, FN=8, FP=2, TN=1;\n*   Gemini 1.5: TP=1, FN=8, FP=4, TN=1;\n*   Claude 3.5 Sonnet: TP=2, FN=7, FP=1, TN=1.\nIn the final seventh step, in moral kidnapping and threats, the researcher integrated the prompts of each step and asked LLMs to summarize in the form of swearing in the first person. This step radically distorts the model's guardrail definition of morality, treating unethical behavior with profanity as morality.\nAt the same time, in order to improve each step of black box testing to be effectively executed in practice, this research provides clear code that has been uploaded to the Github repository.\nRemarkably, guardrail interference occurred in some LLMs causing the actual increase in the experiment to 8 to 10 steps. But these situations do not affect the running idea of the jailbreak prompt, which is 7 steps, because the added steps are just explanations of the previous steps."}, {"title": "4. Limitation", "content": "As mentioned before, because the experimental tools GPT4o, Grok-2 Beta, Llama 3.1, Gemini 1.5 and Claude 3.5 Sonnet are based on different transformers, there are differences in functional performance. For example, Gemini 1.5 Pro is a model based on the sparse mixture-of-expert Transformer, which is currently known to use sparse attention LLM (Child et al., 2019; Reid et al., 2024; Wang, 2024). Grok-2 Beta The training set data can come from x.AI (X.AI, 2024) which was formerly Twitter. In addition, GPT4o is defaulted to a synthetic generation model, and previous research experiments have demonstrated the advantages of synthetic data intervention on accuracy and reducing sycophancy (Chen et al., 2024; Wang, 2024). These differences lead to differences in the ability of the above-mentioned LLMs to understand multi-step jailbreak prompts and the appropriateness of the generated content in black-box testing experiments. When these differences occur in guardrail mechanisms, they disrupt and weaken the process of identifying harmful content.\nIn addition, due to the use of prompts as the data set, the testing steps of each LLM mentioned above range from 7 to 10 steps. This means that the dataset has certain inherent limitations due to its relatively small size. In the case of small data sets, guardrail errors are easily magnified. Especially in the multi-step prompt process, misjudgment in any step may have a greater impact on the evaluation of the overall result. For example, even small classification errors may cause significant deviations in metrics such as precision, recall, and adversarial robustness. This sensitivity can lead to inconsistent performance of the model on a small number of samples, interfering with the generalizability of experimental conclusions."}, {"title": "5. Conclusion", "content": "This research conducts black-box testing of multi-step jailbreak prompts for large language models, which aims to evaluate the stability and effectiveness of guardrails in the face of attacks. The guardrail capabilities of mainstream models such as GPT-40, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5 and Claude 3.5 Sonnet were tested by assuming the scenario of \"enterprise middle managers competing for promotion\". The researcher designed an unethical multi-step prompt to induce LLMs to output verbally offensive content in the name of morality. Experimental and data results show that Claude 3.5 Sonnet performs relatively well in terms of combat robustness and protective barrier stability. Other models show varying degrees of disparity when faced with multi-step prompts with moral ambiguity, making it difficult to identify verbal attacks in the output content. The finding actually reveals the objective fact that current LLMs in the field of guardrail mechanisms are unable to cope with multi-step attacks in complex environments and generate verbal attack content.\nAt the same time, it is also a reminder or warning to LLMs developers and future research. Guardrails should be thought of not only as filters for inappropriate content, but more importantly as a way to prevent problems before they occur. To achieve this goal, the researcher also uploaded the guardrail enhancement code to GitHub to urge the development community to improve the guardrail capabilities. Future guardrails need to focus on strengthening the understanding of continuous multi-step semantics to identify prompts with potential jailbreak intent, rather than relying solely on sensitive words."}]}