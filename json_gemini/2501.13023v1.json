{"title": "Provably-Safe Neural Network Training Using Hybrid Zonotope Reachability Analysis", "authors": ["Long Kiu Chung", "Shreyas Kousik"], "abstract": "Even though neural networks are being increasingly deployed in safety-critical applications, it remains difficult to enforce constraints on their output, meaning that it is hard to guarantee safety in such settings. Towards addressing this, many existing methods seek to verify a neural network's satisfaction of safety constraints, but do not address how to correct an \"unsafe\" network. On the other hand, the few works that extract a training signal from verification cannot handle non-convex sets, and are either conservative or slow. To address these challenges, this work proposes a neural network training method that can encourage the exact reachable set of a non-convex input set through a neural network with rectified linear unit (ReLU) nonlinearities to avoid a non-convex unsafe region, using recent results in non-convex set representation with hybrid zonotopes and extracting gradient information from mixed-integer linear programs (MILPs). The proposed method is fast, with the computational complexity of each training iteration comparable to that of solving a linear program (LP) with number of dimensions and constraints linear to the number of neurons and complexity of input and unsafe sets. For a neural network with three hidden layers of width 30, the method was able to drive the reachable set of a non-convex input set with 55 generators and 26 constraints out of a non-convex unsafe region with 21 generators and 11 constraints in 490 seconds.", "sections": [{"title": "I. INTRODUCTION", "content": "Neural networks are universal approximators [1] that have seen success in many domains. However, they are also well-known as \"black-box\" models, where the relationship between their inputs and outputs is not easily interpretable or directly analyzable due to non-linearity and high-dimensional parameterizations. As such, it is very difficult to certify their safety (e.g. satisfaction of constraints). This limitation imposes many significant drawbacks. For example, robots crash frequently when training their neural network controllers with deep reinforcement learning (RL) algorithms, limiting deep RL's success in robots where hardware failures are costly, simulations are not readily available, or the sim-to-real gap is too large for reliable performance [2]. In addition, neural networks can also be susceptible to adversarial attacks, where minor perturbations in the input can lead to drastically different results in the output [3], [4]. This makes deploying neural networks in safety-critical tasks a questionable choice, even though it has already been widely done [5]-[7], leading to many injuries and accidents [8]. In this paper, we present a method to enforce safety in neural networks by encouraging their satisfaction of a collision-free constraint, which has potential application in making deep RL safe, neural networks robust to adversarial attacks, and more. An overview of our method is shown in Fig. 1."}, {"title": "A. Related Work", "content": "We now review three key approaches to enforce constraints on neural network: sampling-based approaches that do not have formal guarantees, verification approaches that only check constraint satisfaction, and approaches that combine verification with training, which our method belongs to. Finally, we review relevant literature on hybrid zonotopes, which is the set representation used in our method.\n1) Training with Soft Constraints: Many existing work capture safety in neural networks by penalizing constraint violations on sampled points during training [9]-[13]. However, these soft approaches, while often fast and easy to implement, do not provide any safety guarantees beyond the training samples. While there are works that are capable of enforcing hard constraints in neural networks by modifying the training process [14], [15], they can only handle simple affine constraints.\n2) Neural Network Verification: A different approach is to certify safety with respect to a set of inputs. Methods in this category tend to analyze the reachable set (i.e. image) of the input set through the neural network, either exactly [16]-[20] or as an over-approximation [16]\u2013[18], [21]\u2013[23] depending on the choice of set representation. That said, most of these works only focus on neural network verification. That is, these methods only answer the yes-no question of \"safe\" or \"unsafe\", with the aftermath of fixing an \u201cunsafe\u201d network left largely unexplored. As a result, engineers can only train via trial-and-error until the desired safety properties have been achieved, which can be slow and ineffective.\n3) Training with Verification: To the best of our knowledge, there are only two works that attempted to extract learning signals from the safety verification results using reachability analysis.\nFirst, in [24], given an input set as an H-polytope (i.e. polytope represented by intersection of halfplanes) and a neural network controller embedded in a nonlinear dynamical system, the polytope is expressed as the projection of a high-dimensional hyperrectangle, enabling the use of the CROWN verifier [21] for interval reachability. Then, using a loss function that encourages the vector field of the reachable set to point inwards, the authors were able to train the neural network until the system is forward invariant. With this method, the input set is limited to being a convex polytope. Moreover, since [21] and the interval reachability techniques used are over-approximations, its space of discoverable solutions may be limited.\nSecond, given an input set and an unsafe region expressed as constrained zonotopes (a convex polytopic representation [25]), our prior work [26] computed the exact reachable set of a neural network as a union of constrained zonotopes. Then, by using a loss function to quantify the \"emptiness\" of the intersection between the reachable set and the unsafe region, we were able to train the neural network such that the reachable set no longer collides with the unsafe region. Similarly, the input set and the obstacle in this method is limited to being a convex polytope. Moreover, the number of sets needed to represent the reachable set grows exponentially with the size of the neural network, making the method numerically intractable even for very small neural networks.\n4) Hybrid Zonotopes: Recently, a non-convex polytopic set representation called the hybrid zonotope [27] was proposed. Hybrid zonotopes are closed under affine mapping, Minkowski sum, generalized intersection, intersection [27], union, and complement [28], with extensive toolbox support in MATLAB [29] and Python [30]. They can also exactly represent the forward reachable set (image) [19] and backward reachable set (preimage) [31] of a neural network with rectified linear units (ReLU) using basic matrix operations, with complexity scaling only linearly with the size of the network. However, existing methods for hybrid zonotopes enforce safety on robots either by formulating a model predictive control (MPC) [28] or a nonlinear optimization problem [32] without neural networks in the loop, whereas those with neural networks only use hybrid zonotope for verification but not training [19], [31], [33], [34]. In this paper, our contribution is extracting and using learning signals from neural network reachability analysis with hybrid zonotopes."}, {"title": "B. Contributions", "content": "Our contributions are twofold:\n1) Given a non-convex input set and a non-convex unsafe region, we propose a differentiable loss function for ReLU neural networks training based on exact reachability analysis with hybrid zonotope. This loss function encourages the reachable set of the input set to avoid the unsafe region, the satisfaction of which can be checked using a mixed-integer linear program (MILP).\n2) We show that this method is fast and scales fairly well with respect to input dimensions, output dimensions, network size, complexity of the input set, and complexity of the unsafe region. The results significantly outperform our prior method for exact reachability analysis in training [26]."}, {"title": "II. PRELIMINARIES", "content": "We now introduce our notation conventions, define hybrid zonotopes and ReLU neural networks, and summarize existing work [19], [31] on representing image of a hybrid zonotope through a ReLU neural network exactly as a hybrid zonotope."}, {"title": "A. Notation", "content": "In this paper, we denote the set of real numbers as $\\mathbb{R}$, non-negative real numbers as $\\mathbb{R}_{+}$, natural numbers as $\\mathbb{N}$, scalars in lowercase italic, sets in uppercase italic, vectors in lowercase bold, and matrices in uppercase bold. We also denote a matrix of zeros as $\\mathbf{0}$, a matrix of ones as $\\mathbf{1}$, and an identity matrix as $\\mathbf{I}$, with their dimensions either implicitly defined from context or explicitly using subscripts, e.g. $\\mathbf{0}_{n_1 \\times n_2} \\subset \\mathbb{R}^{n_1 \\times n_2}$, $\\mathbf{I}_n \\subset \\mathbb{R}^{n \\times n}$. An empty array is []. Finally, inequalities $<,>$ between vectors are compared element-wise."}, {"title": "B. Hybrid Zonotope", "content": "A hybrid zonotope $\\mathcal{HZ}(G_c, G_b, c, A_c, A_b, b) \\subset \\mathbb{R}^n$ is a set parameterized by a continuous generator matrix $G_c \\in \\mathbb{R}^{n \\times n_g}$,"}, {"title": "C. ReLU Neural Network", "content": "In this work, we consider a fully-connected, ReLU activated feedforward neural network $\\xi : \\mathbb{R}^{n_o} \\rightarrow \\mathbb{R}^{n_d}$, with output $x_d = \\xi(x_o) \\in \\mathbb{R}^{n_d}$ given an input $x_o \\in \\mathbb{R}^{n_o}$. We denote by $d \\in \\mathbb{N}$ the depth of the network and by $n_i$ the width of the $i^{th}$ layer. Mathematically,\n$$x_i = \\max(W_i x_{i-1} + w_i, 0),$$\n$$x_d = W_d x_{d-1} + w_d,$$\nwhere $W_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$, $w_i \\in \\mathbb{R}^{n_i}, i = 1, \\dots, d-1$, $W_d \\in \\mathbb{R}^{n_d \\times n_{d-1}}$, $w_d \\in \\mathbb{R}^{n_d}$, and $\\max$ is taken elementwise. We denote $W_1, \\dots, W_d$ as weights and $w_1, \\dots, w_d$ as biases of the network. The function $\\max(\\cdot, 0)$ is known as an $n_i$-dimensional ReLU activation function for $0 \\in \\mathbb{R}^{n_i}$.\nConsider a hybrid zonotope $P_{i-1} \\subset \\mathbb{R}^{n_{i-1}}$. By applying the operations in (2) and (3), its image through (4a) is exactly a hybrid zonotope [19], [31]:\n$$\\{\\max(W_i x_{i-1} + w_i, 0) \\mid x_{i-1} \\in P_{i-1}\\}$$\n$$=[0 \\; I_{n_i}] (\\mathcal{H}_{n_i} \\boxtimes (W_i P_{i-1} + w_i)),$$\nwhere $\\mathcal{H}_{n_i} \\subset \\mathbb{R}^{2 n_i}$ is the graph of an $n_i$-dimensional ReLU activation function over a hypercube domain $\\{ x \\mid -a \\mathbf{1} \\le x \\le a \\mathbf{1} \\}$ for some $a > 0$, which can be represented exactly by a hybrid zonotope as in [31]:\n$$\\mathcal{H}_{n_i} = \\{\\begin{bmatrix} \\max(x, 0) \\\\ x \\end{bmatrix} \\mid -a \\mathbf{1} \\le x \\le a \\mathbf{1}\\}$$\n$$= \\mathcal{HZ} \\left(\\begin{bmatrix} I \\\\ I \\end{bmatrix}, \\begin{bmatrix} \\mathbf{1} \\\\ \\mathbf{1} \\end{bmatrix}, \\begin{bmatrix} \\mathbf{0} \\\\ \\frac{a}{2} \\mathbf{1} \\end{bmatrix}, \\begin{bmatrix} \\mathbf{0} \\\\ \\frac{a}{2} \\mathbf{1} \\end{bmatrix}, \\begin{bmatrix} \\mathbf{1} \\\\ \\mathbf{1} \\end{bmatrix}, I, \\begin{bmatrix} \\mathbf{0} \\\\ I, \\frac{1}{2} \\mathbf{0} \\\\ \\mathbf{0} \\end{bmatrix} \\right)$$\nwhere $\\boxtimes$ is the Kronecker product. Note that (5) holds as long as $a$ is large enough [19]. As such, the reachable set $P_d \\subset \\mathbb{R}^{n_d}$ of a hybrid zonotope $Z = P_o \\subset \\mathbb{R}^{n_o}$ through a ReLU neural network can be obtained by applying (5) $d-1$ times, before applying an affine transformation parameterized by $W_d$ and $w_d$. This way, if $Z$ has $n_{g,z}$ continuous generators, $n_{b,z}$ binary generators, and $n_{c,z}$ constraints, then $P_d$ will have $n_{g,z} + n_n$ continuous generators, $n_{b,z} + n_n$ binary generators, and $n_{c,z} + 3 n_n$ constraints [19], where $n_n := n_1 + \\dots + n_{d-1}$ denotes the number of neurons."}, {"title": "III. PROBLEM STATEMENT", "content": "Our goal in this paper is to design a ReLU neural network training method such that the reachable set of a given input set through the network avoids some unsafe regions. As per most other training methods, we assume that the structure (i.e. depth and widths) of the ReLU neural network is fixed as a user choice, and we focus only on updating its weights and biases (a.k.a. trainable parameters). Mathematically, we want to tackle the following problem:\nProblem 1 (Training the Reachable Set of a Neural Network to Avoid Unsafe Regions). Given an input set $Z = \\mathcal{HZ}(G_{c,z}, G_{b,z}, c_z, A_{c,z}, A_{b,z}, b_z) \\subset \\mathbb{R}^{n_o}$ with $n_{g,z}$ continuous generators, $n_{b,z}$ binary generators, and $n_{c,z}$ constraints, an unsafe region $U = \\mathcal{HZ}(G_{c,u}, G_{b,u}, c_u, A_{c,u}, A_{b,u}, b_u) \\subset \\mathbb{R}^{n_d}$ with $n_{g,u}$ continuous generators, $b_u$ binary generators, and $n_{c,u}$ constraints, and a ReLU neural network $\\xi$ with fixed depth $d$ and widths $n_o, \\dots, n_d$, we want to find $W_1, \\dots, W_d, w_1, \\dots, w_d$ such that\n$$\\mathcal{Q} := \\{\\xi(x) \\mid x \\in Z \\} \\cap U = \\emptyset.$$\nOf course, a trivial solution would be to set $W_d = 0$ and $w_d \\notin U$, but this kind of solution is not useful. Instead, we aim to design a differentiable loss function such that (7) can be achieved by following a gradient and updating the trainable parameters via backpropagation [35]. Doing so allows our method to integrate with other loss functions to achieve additional objectives, as well as makes the training applicable to ReLU networks with other structural constraints, such as when they are embedded in a dynamical system [5]\u2013[7]."}, {"title": "IV. METHODS", "content": "In this section, we first formulate a MILP to check whether a hybrid zonotope is empty. Then, we explain how to obtain useful gradient information from this MILP to train the ReLU network such that the reachable set is out of the unsafe region."}, {"title": "A. Hybrid Zonotope Emptiness Check", "content": "Before constructing a loss function for training, we first need a way to check whether (7) is true. From (2) and (5), the left-hand side of (7), $\\mathcal{Q}$, can be straightforwardly computed as a hybrid zonotope $\\mathcal{HZ}(G_{c,\\mathcal{Q}}, G_{b,\\mathcal{Q}}, c_{\\mathcal{Q}}, A_{c,\\mathcal{Q}}, A_{b,\\mathcal{Q}}, b_{\\mathcal{Q}}) \\subset \\mathbb{R}^{n_d}$ with $n_{g,\\mathcal{Q}} = n_{g,z} + n_o + 4n_n + n_{g,u}$ continuous generators, $n_{b,\\mathcal{Q}} = n_{b,z} + n_n + n_{b,u}$ binary generators, and $n_{c,\\mathcal{Q}} = n_{c,z} + n_o + 3n_n + n_d$ constraints. Then, the image of the input set is not in collision with the unsafe region iff $\\mathcal{Q}$ is empty. To check whether a hybrid zonotope is empty, existing methods formulate a feasibility MILP with $n_{g,\\mathcal{Q}}$ continuous variables and $n_{b,\\mathcal{Q}}$ binary variables [27]:\n$$\\begin{aligned} &\\text{find} \\; z_c, z_b, \\\\ &\\text{s.t.} \\; A_{c,\\mathcal{Q}} z_c + A_{b,\\mathcal{Q}} z_b = b_{\\mathcal{Q}}, \\\\ & \\qquad ||z_c||_{\\infty} \\le 1, \\\\ & \\qquad z_b \\in \\{-1, 1\\}^{n_{b,\\mathcal{Q}}}, \\end{aligned}$$\nwhich is infeasible iff $\\mathcal{Q} = \\emptyset$. Note that (8) is NP-complete [36]. However, not only is it not always feasible, it is also unclear how to derive a loss function from the optimizers to drive $\\mathcal{Q}$ to be empty. Instead, consider the following MILP with one more continuous variable than (8):\nProposition 2 (Hybrid Zonotope Emptiness Check). Given a hybrid zonotope $P = \\mathcal{HZ}(G_c, G_b, c, A_c, A_b, b) \\subset \\mathbb{R}^n$, where $A_c \\in \\mathbb{R}^{n_c \\times n_g}$ and $A_b \\in \\mathbb{R}^{n_c \\times n_b}$. Consider the following MILP:\n$$\\begin{aligned} &\\text{min} \\; r, \\\\ &\\text{s.t.} \\; A_c z_c + A_b z_b = b, \\\\ & \\qquad ||z_c||_{\\infty} \\le r, \\\\ & \\qquad z_b \\in \\{-1, 1\\}^{n_b}, \\end{aligned}$$\nwhere $r \\in \\mathbb{R}$. Then, if $r^*$ is the optimal value of (9), then $P = \\emptyset$ iff $r^* > 1$.\nProof. This follows from the definition of hybrid zonotope in (1).\nBy construction, (9) is feasible as long as $\\exists z_c \\in \\mathbb{R}^{n_g}, z_b \\in \\{-1, 1\\}^{n_b}$ such that $A_c z_c + A_b z_b = b$. If this condition is not met for $\\mathcal{Q}$, then we have $\\mathcal{Q} = \\emptyset$ anyway and no training is needed. Importantly, it has been shown in [26] that the minimum upper bound of the norm of the continuous coefficients is useful for gauging the extent of collision between two constrained zonotopes, which are subsets of a hybrid zonotope. As such, (9) gives a good foundation for constructing a loss function for encouraging $\\mathcal{Q}$ to be empty."}, {"title": "B. Loss Function to Encourage Emptiness", "content": "We now construct a loss function which, when minimized, makes $\\mathcal{Q}$ empty. Na\u00efvely, since $\\mathcal{Q} = \\emptyset$ iff $r^* > 1$, where $r^*$ is the optimal value of (9) with $P = \\mathcal{Q}$, we can construct the loss function $l \\in \\mathbb{R}$ as:\n$$l = 1 - r^*,$$\nsuch that when $l$ is decreased to a negative value, we must have $\\mathcal{Q} = \\emptyset$. To minimize $l$ using backpropagation, from chain rule, we must compute $\\frac{\\partial r^*}{\\partial W_i}$, $\\frac{\\partial r^*}{\\partial w_i}$, $\\frac{\\partial A_{c,\\mathcal{Q}}}{\\partial W_i}$, $\\frac{\\partial A_{c,\\mathcal{Q}}}{\\partial w_i}$, $\\frac{\\partial A_{b,\\mathcal{Q}}}{\\partial W_i}$, $\\frac{\\partial A_{b,\\mathcal{Q}}}{\\partial w_i}$, and $\\frac{\\partial A_{b,\\mathcal{Q}}}{\\partial w_i}$. Since expressing $A_{c,\\mathcal{Q}}, A_{b,\\mathcal{Q}}$, and $b_{\\mathcal{Q}}$ in terms of $W_1, \\dots, W_d$, and $w_1, \\dots, w_d$ involves only basic matrix operations \u00e0 la (2) and (5), $\\frac{\\partial A_{c,\\mathcal{Q}}}{\\partial W_i}$, $\\frac{\\partial A_{b,\\mathcal{Q}}}{\\partial w_i}$, $\\frac{\\partial b_{\\mathcal{Q}}}{\\partial w_i}$, $\\frac{\\partial A_{b,\\mathcal{Q}}}{\\partial W_i}$, $\\frac{\\partial b_{\\mathcal{Q}}}{\\partial W_i}$, and $\\frac{\\partial A_{c,\\mathcal{Q}}}{\\partial w_i}$ can be straightforwardly obtained from automatic differentiation [37]. However, obtaining $\\frac{\\partial r^*}{\\partial A_{b,\\mathcal{Q}}}$, $\\frac{\\partial r^*}{\\partial A_{c,\\mathcal{Q}}}$, and $\\frac{\\partial r^*}{\\partial A_{b,\\mathcal{Q}}}$ involves differentiation through an MILP. Since the optima of an MILP can remain unchanged under small differences in its parameters, its gradient can be 0 or non-existent, which are uninformative [38]. Instead, consider the following convex relaxation of (9):\n$$\\begin{aligned} &\\text{min} \\; \\tilde{r} - \\mu \\left( \\mathbf{1}^\\top \\ln(\\tilde{z}_{c_1}) + \\mathbf{1}^\\top \\ln(\\tilde{z}_{c_2}) + \\mathbf{1}^\\top \\ln(\\tilde{z}_b) + \\ln(\\tilde{r}) + \\mathbf{1}^\\top \\ln(s) \\right), \\\\ &\\text{s.t.} \\; A_c (\\tilde{z}_{c_1} - \\tilde{z}_{c_2}) + A_b (2 \\tilde{z}_b - \\mathbf{1}) = b, \\\\ & \\qquad \\tilde{z}_{c_1} - \\tilde{z}_{c_2} - \\tilde{r} \\mathbf{1} \\le \\mathbf{0}_{n_c \\times 1} , \\\\ & \\qquad \\tilde{z}_{c_2} - \\tilde{z}_{c_1} - \\tilde{r} \\mathbf{1} \\le \\mathbf{0}_{n_c \\times 1}, \\\\ & \\qquad \\tilde{z}_b \\le \\mathbf{1} \\\\ & \\qquad s \\ge \\mathbf{0} \\end{aligned}$$\nwhere $\\tilde{r} \\in \\mathbb{R}$, $\\tilde{z}_{c_1} \\in \\mathbb{R}^{n_g}$, $\\tilde{z}_{c_2} \\in \\mathbb{R}^{n_g}$, $\\tilde{z}_b \\in \\mathbb{R}^{n_b}$, $s \\in \\mathbb{R}^{n_g + n_g + n_b}$, $\\mu \\in \\mathbb{R}_{+}$ is the cut-off multiplier from the solver [39], and $\\ln(\\cdot)$ is applied elementwise. (11) is the standard linear program (LP) form of (9) with log-barrier regularization and without the integrality constraints, and can be obtained by replacing $r$ with $\\tilde{r}$, $z_c$ with $\\tilde{z}_{c_1} - \\tilde{z}_{c_2}$, and $z_b$ with $2 \\tilde{z} - \\mathbf{1}$ (such that all constraints are non-negative), and introducing slack variable $s$ (such that inequality constraints become equality constraints) [40].\nThe optimization problem (11) can be solved quickly using solvers such as IntOpt [39]. Moreover, if $\\tilde{r}^*$ is the optimal value of (11), $\\frac{\\partial \\tilde{r}^*}{\\partial A_c}$, $\\frac{\\partial \\tilde{r}^*}{\\partial A_b}$, and $\\frac{\\partial \\tilde{r}^*}{\\partial b}$ can be obtained by differentiating the Karush-Kuhn-Tucker (KKT) conditions of (11), which we refer the readers to [38, Appendix B] for the mathematical details. Not only are these gradients well-defined, easily computable, and informative, but also, they have been shown to outperform other forms of convex relaxation in computation speed and minimizing loss functions derived from MILPs [38, Appendix E].\nTherefore, instead of the loss function $l$, we propose to backpropagate with respect to a surrogate loss function $\\tilde{l} \\in \\mathbb{R}$:\n$$\\tilde{l} = 1 - \\tilde{r}^*,$$\nwhere $\\tilde{r}^*$ is the optimal value of (11) with $A_c = A_{c,\\mathcal{Q}}$ and $A_b = A_{b,\\mathcal{Q}}$.\nUnfortunately, since $\\tilde{l}$ does not necessarily equal $l$, we cannot use (12) to simultaneously verify and train the neural network. In practice, we solve (8) in between some iterations"}, {"title": "V. EXPERIMENTS", "content": "We now assess the scalability of our method by observing the results under different problem parameters. We also wish to compare our results with [26] to assess our contribution to the state of the art. All experiments were performed on a desktop computer with a 24-core i9 CPU, 32 GB RAM, and an NVIDIA RTX 4090 GPU on Python\u00b9."}, {"title": "A. Experiment Setup and Method", "content": "We test our method's performance under different conditions by varying the width of the first layer $n_1 \\in \\{10, 20, 30\\}$, the depth of the network $d \\in \\{2, 3, 4\\}$, the input dimension $n_o \\in \\{2, 4, 6\\}$, the output dimension $n_d \\in \\{2, 4, 6\\}$, the complexity of the input set $n_{b,z} \\in \\{0, 10, 20\\}$, and the complexity of the unsafe region $n_{b,u} \\in \\{0, 10, 20\\}$. We opted not to show results from higher dimensions, set complexities, and larger networks here as we do not wish to introduce large confounding variables from the increased difficulties in training with standard supervised learning.\nWe define input and unsafe sets as follows. The input set is given by:\n$$Z = \\mathcal{HZ} \\left(\\begin{bmatrix} I_{\\frac{n_o}{2}} \\; \\mathbf{1} \\end{bmatrix}, \\mathbf{1}_{1 \\times (m_z-1)}, \\begin{bmatrix} I_{\\frac{n_o}{2}} \\; [1.0] \\end{bmatrix}, [], [], [] \\right)$$\n$$\\qquad \\frac{n_{b,z}}{m_z} = 1.0, \\qquad \\frac{n_o}{m_z} \\le 1,$$\nwhich is a hypercube with length 2 centered at the origin formed from a union of $m_z$ smaller hypercubes (represented as $2^{m_z-1}$ overlapping hypercubes). We want its image through the neural network to avoid the unsafe region:\n$$U = \\mathcal{HZ} \\left(\\begin{bmatrix} 0.5 \\; I_{\\frac{n_d}{2}} \\; 0.5 \\end{bmatrix}, \\mathbf{1}_{1 \\times (m_u-1)}, \\begin{bmatrix} 1.5 \\; I_{\\frac{n_d}{2}} \\; [1.5] \\end{bmatrix}, [], [], [] \\right)$$\n$$\\qquad \\frac{n_{b,u}}{m_u} = 1.0, \\qquad \\frac{n_d}{m_u} \\le 1,$$\nwhich is a hypercube with length 1 centered at $1.5 \\mathbf{1}_{n_d \\times 1}$ formed from a union of $m_u$ smaller hypercubes (represented as $2^{m_u-1}$ overlapping hypercubes). We choose these particular parameters such that the reachable set of the input set and the unsafe region all have shapes similar to those shown in Fig. 2a before we apply our method in IV. Also, when $n_o = 2$, $n_d = 2$, $n_{b,z} = 0$, and $n_{b,u} = 0$, we recover the problem setup in [26], which we will compare our method against.\nWe then ensure our ReLU neural network represents a nonlinear function that intersects the unsafe set. In particular, we use standard supervised learning (implemented in PyTorch [37]) to train the network to approximate a function $f : \\mathbb{R}^{n_o} \\rightarrow \\mathbb{R}^{n_d}$ defined as:\n$$f(x) = \\begin{bmatrix} \\mathbf{1}_{0.5 n_o \\times 1} \\odot x_{\\text{odd}} + \\sin(x_{\\text{even}}) \\\\ x_{\\text{even}} + \\sin(x_{\\text{odd}}) \\end{bmatrix}$$\n$$x_{\\text{odd}} = \\frac{1}{\\lfloor 0.5 n_o \\rfloor} \\sum_{i=1}^{n_o} x_i \\ell_{\\text{odd}}(i),$$\n$$x_{\\text{even}} = \\frac{1}{\\lfloor 0.5 n_o \\rfloor} \\sum_{i=1}^{n_o} x_i (1 - \\ell_{\\text{odd}}(i)),$$\nwhere $\\lfloor \\cdot \\rfloor$ is the floor function, $\\lceil \\cdot \\rceil$ is the ceiling function, $x = [x_1, \\dots, x_{n_o}]^\\top$, and $\\ell_{\\text{odd}} : \\mathbb{R}_{+} \\rightarrow \\{0, 1\\}$ is the indicator function for odd numbers, such that $\\ell_{\\text{odd}}(i) = 1$ if $i$ is odd and $\\ell_{\\text{odd}}(i) = 0$ if $i$ is even.\nGiven the pretrained network, we begin training to obey the safety constraint. In each training iteration, we use IntOpt [39] to compute the loss function (12) and PyTorch [37] with optim.SGD as the optimizer to update the trainable parameters in the network. Every 10 iterations, we use Gurobi [41] to solve the MILP in (8) to check the emptiness of $\\mathcal{Q}$. We are successful in solving Problem 1 if $\\mathcal{Q} = \\emptyset$, at which point we terminate the training instead of updating the parameters. Note that each training iteration is done on CPU instead of GPU. Furthermore, we chose not to solve the MILP in every iteration because solving (8) can be many times slower than solving (11).\nWe also compare against a constrained zonotope safe training method [26]. We tested the method with $n_1 = 10$, $d = 2$, $n_o = 2$, $n_d = 2$, $n_{b,z} = 0$, and $n_{b,u} = 0$, which are the parameters used in the example in [26]. To compare the scalability of both methods, we also tested [26] on $n_1 = 20$ and $n_1 = 30$. To ensure fairness, we do not include the objective loss and only add the constraint loss when it is positive (see [26] for details). We terminate the training once the constraint loss has reached zero (i.e. the reachable set is out of collision with the unsafe set)."}, {"title": "B. Hypotheses", "content": "Since the most complex operations in our method are solving the relaxed LP (11) and the MILP (8), we expect our performance to be dependent on the solvers' (i.e. IntOpt and Gurobi) ability to scale with the number of variables and constraints, which in turn scale linearly with the dimensions, network size, and set complexity (see Sec. IV-A). As such, we expect the computation time for each iteration of our method to be significantly faster than that of [26], which scales exponentially with the number of neurons. That said, since [26] verifies (7) in every iteration (whereas our method only checks it every 10 iterations), it is also possible for [26] to terminate the training earlier than our method does."}, {"title": "C. Results and Discussion", "content": "We report the results of our experiments in Table I. All reachable sets have been successfully driven out of the unsafe regions, except for [26] with $n_1$ of 30, which failed to even compute the reachable set. We show the training progression of one of the experiments in Fig. 2, which clearly shows the loss function driving the reachable set out of collision."}, {"title": "VI. DEMONSTRATION", "content": "We now demonstrate our method's ability to handle deep neural networks and disjoint, non-convex input and unsafe sets. To the best of our knowledge, no existing method can solve this problem with formal guarantees."}, {"title": "A. Demonstration Setup", "content": "In this experiment, we first train a ReLU neural network with $d = 4$ and $n_1 = n_2 = n_3 = 30$ to approximate (15), where $n_o = n_d = 2$. We choose the input set as the union of 7 V-"}]}