{"title": "Fighter Jet Navigation and Combat using Deep Reinforcement Learning with Explainable AI", "authors": ["Swati Kar", "Soumyabrata Dey", "Mahesh K Banavar", "Shahnewaz Karim Sakib"], "abstract": "This paper presents the development of an Artificial Intelligence (AI) based fighter jet agent within a customized Pygame simulation environment, designed to solve multi-objective tasks via deep reinforcement learning (DRL). The jet's primary objectives include efficiently navigating the environment, reaching a target, and selectively engaging or evading an enemy. A reward function balances these goals while optimized hyperparameters enhance learning efficiency. Results show more than 80% task completion rate, demonstrating effective decision-making. To enhance transparency, the jet's action choices are analyzed by comparing the rewards of the actual chosen action (factual action) with those of alternate actions (counterfactual actions), providing insights into the decision-making rationale. This study illustrates DRL's potential for multi-objective problem-solving with explainable AI. Project page is available at: Project GitHub Link.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, rapid advancements in technology have positioned AI as a transformative force across various fields. Al's ability to emulate human intelligence has led to ground-breaking developments, reshaping industries and redefining how to approach complex tasks. Traditionally, many critical tasks relied heavily on skilled human intervention, especially in high-risk situations. However, AI now has the potential to autonomously undertake these tasks, reducing risk to human life and improving operational efficiency. Starting with notable achievements, such as surpassing human capabilities in chess in 1997, AI has expanded to tackling the highly intricate board game Go [1], [2]. This shift highlights a new era where AI not only matches but often surpasses human abilities in executing high-stakes, strategic tasks. RL, a subset of AI, enables an agent to learn effective actions within its environment through trial-and-error interactions, eliminating the need for human expert data to identify robust Courses of Action (CoAs) [3].\nIn the field of jet navigation and combat, several previous works have been conducted. The simulation model, as explored in [4], primarily focuses on air combat scenarios rather than on reinforcement learning strategies or explainability. It lacks the detailed DRL approach and reward function design, and consequently, the emphasis is more on simulating combat than on optimizing learning strategies. Reinforcement learning is covered in [5], though it does not delve into explainability through factual versus counterfactual analysis. Instead, the focus lies on the technical aspects, such as network architecture and training processes, rather than on the decision-making transparency of the agent. Additionally, [5] lacks a detailed account of how agents improve their efficiency in task completion over time.The simulation-based pilot training systems discussed in [6] focus more on training scenarios and the roles of agents rather than providing an in-depth DRL approach. The reward systems discussed in [6] align more with training goals than the complex balance of efficiency and resource management featured in our proposed approach. Simpler reward functions, as noted in [7], typically concentrate on immediate task outcomes such as shooting down targets or avoiding crashes. While these reward functions effectively guide the agent's learning, they lack the complexity and balance necessary for encouraging nuanced decision-making processes, which require balancing long-term efficiency with short-term actions.\nIn this research, all of these shortcomings are addressed. Therefore, the main contributions of this paper are:\n1) A reward schema that balances efficiency, resource man-agement, and intelligent decision-making to address a multi-objective problem.\n2) Enhanced explainability through factual and counterfac-tual analysis, providing insights into the agent's deci-sions and improving transparency.\nOur study is organized into the following key components: first, we develop a custom simulation environment. Next, we train a fighter jet agent to make strategic engagement decisions using the double deep q-learning (DDQN) algorithm. We then focus on optimizing mission resources and explaining the agent's decision-making process through factual and counter-factual scenarios. By addressing challenges such as prioritization, adaptive behavior, and risk assessment, this research aims to advance the development of intelligent, autonomous systems for complex, multi-objective scenarios, ultimately enhancing Al's role in high-stakes environments."}, {"title": "II. FIGHTER JET PROBLEM FORMULATION", "content": "The high level simulation environment with reinforcement learning agent is depicted in Figure 1. This is a continuation of previous research [8] The simulation environment is designed using Python's Pygame package. In the visualization, the blue"}, {"title": "A. Environment Design", "content": "orientation \\Theta_{jet}:\n\n\n\n\n\n\n\n\n\n\n\n\nThe jet's position Pjet [Xjet, Yjet] is updated based on its\nvelocity vector Vjet, which is a function of the speed v and"}, {"title": "B. State Space", "content": "The state variables are chosen to provide the fighter jet a comprehensive understanding of both its own status and its interactions with key elements in the environment, enabling informed decision-making for effective navigation and engage-ment. The jet's state space consists of positional, directional, and interaction variables: global coordinates (xjet, Yjet), head-ing angle @jet, and velocities (Vx, Vy). Relative measurements"}, {"title": "C. Action Space", "content": "The agent's action space consists of six discrete actions: ao maintains the current orientation and speed, a1 and a2 adjust the heading angle by turning left or right by 0.05 radians, respectively. The actions a3 and a4 control the speed by accelerating or decelerating by 0.25 units, and a5 enables the agent to shoot, generating a new bullet. The continuous actions are discretized to simplify the agent's decision-making process and improve explainability, allowing clear interpretation of the agent's choices and their impact on the environment. The complete action space vector is defined as:\nA = [a0, a1, a2, a3, a4, a5]T"}, {"title": "D. Reward Function", "content": "The reward function balances task performance, efficiency, and resource management by incorporating rewards for desir-"}, {"title": "E. Training and Testing Analysis of the DDQN Algorithm", "content": "This study employs the double deep Q-network (DDQN) [9] algorithm for decision-making in a fighter jet simulation environment shown in Figure 2. In Q-learning, the Q-value represents the expected cumulative reward for taking an action in a given state and following the optimal policy thereafter. Traditional deep Q-networks (DQN) often overestimate Q-values due to the use of the same network for action selection and Q-value estimation, which can lead to suboptimal policies. DDQN mitigates this issue by decoupling these tasks, using the target network for Q-value estimation, resulting in more stable and accurate learning.\nThe neural network architecture for the Q-network consists of three fully connected layers with 256 neurons in each layer. The ReLU activation function introduces non-linearity to enhance learning capability, while the Adam optimizer is utilized for efficient gradient descent and weight updates. Hyperparameters used for training are shown in Table II which were achieved through trial and error.\nThe epsilon decay policy balances exploration and exploita-tion in the double DQN algorithm. Exploration means trying random actions to gather information, whereas exploitation means using the best-known actions to maximize reward. Starting with an epsilon value of 1.0, the agent explores the environment by selecting random actions. Epsilon decreases linearly over time, reaching a minimum of 0.1, encouraging the agent to shift from exploration to exploitation, leveraging learned knowledge for decision-making. At 0.1, the agent primarily selects optimal actions while occasionally exploring to refine its policy. This gradual transition enhances stability and performance, ensuring a smooth shift from exploratory learning to policy-driven actions. The linear decay pattern is illustrated in Figure 3 for training steps."}, {"title": "III. RESULT", "content": "The agent's training performance is evaluated based on key metrics, including episode length, average reward, success rate, and trajectory analysis. These metrics provide insights into the agent's learning progress, decision-making efficiency, and task completion capabilities, validating the effectiveness of the DDQN algorithm in the fighter jet simulation environment. The details about step, episode and episode termination condi-tions are shown in Table III. The explainability analysis further enhances transparency by examining the agent's action choices through factual and counterfactual reward comparisons, offer-ing insights into the decision-making process and the rationale behind the agent's actions."}, {"title": "A. Fighter Jet Agent Training Performance", "content": "Figure 4 illustrates the agent's average reward over 1 million training steps. The average reward increases steadily, reflecting the agent's improvement in performance as training advances. The reward increases steadily, reflecting the agent's ability to learn and apply effective strategies to maximize cumulative rewards. The shaded region shows the reward variance, which reduces over time, indicating that the agent's actions are becoming more consistent and focused on optimal strategies. This positive trend in average reward validates the"}, {"title": "B. Success vs Failure Rates", "content": "After training, evaluation episodes are conducted to assess the agent's performance in the fighter jet simulation envi-ronment. Table IV displays the agent's success and failure rates across episodes. Out of 1000 testing episodes, the agent successfully completes 825 episodes, achieving the objective, while it fails in 175 episodes. This success rate demonstrates"}, {"title": "C. Jet and Target Positions", "content": "The trajectory plot in Figure 6 illustrates the jet's paths (blue) and target positions (red) over 50 evaluation episodes. The plot highlights the jet's ability to navigate toward various random target position, adapting its trajectory based on the target's location. To generate these results, target positions are randomly assigned at the start of each episode, creating diverse scenarios for the agent. The agent collects samples by navigating through the environment and engaging with the target. For simplicity, enemy trajectories are omitted.\nThe trajectories confirm the agent's efficiency in reach-ing the target, maintaining proximity, and engaging effec-tively, validating the DDQN algorithm's performance in multi-objective decision-making tasks. The distance between the jet's final position and the target indicates that once the target enters the jet's targeting zone, the agent engages and concludes the episode."}, {"title": "IV. EXPLAINABILITY OF FIGHTER JET AGENT DECISIONS", "content": "Explainability in machine learning refers to the ability to convey how a model makes its decisions to a human, even one without technical expertise. This is crucial for building trust and identifying potential biases. In reinforcement learning, explainability is especially vital since the agent makes real-time decisions, and understanding its rationale is key. This section explores techniques for interpreting the decisions of the fighter jet agent in the simulation environment."}, {"title": "A. Reward Heatmap: Factual vs Counterfactual Actions", "content": "The reward heatmap in Figure 7 analyzes the agent's decision-making by comparing factual and counterfactual re-wards. Diagonal values represent chosen actions (factual), and others display alternative actions (counterfactual). Each cell shows the average reward for 1000 evaluation episodes if the counterfactual action were taken. The color intensity indicates reward magnitude, with darker shades for higher rewards and lighter shades for lower rewards. Diagonal elements corre-spond to the agent's factual actions.\nFrom the heatmap, it can be observed that certain actions consistently yield higher rewards, which guides the agent's preferred choices. For instance, when the agent chooses the \"Shoot\" action, the factual reward is higher than the coun-terfactual rewards for other actions, indicating that shooting was indeed the optimal choice in those scenarios. Conversely, actions like \"Decelerate\" yield lower rewards, suggesting they are less favorable in the given environment conditions."}, {"title": "B. Factual vs Average Counterfactual Rewards", "content": "Figure 8 compares the average factual rewards for each action with the average counterfactual rewards if alternative actions were taken. The factual rewards, represented by the blue bars, demonstrate the actual rewards the agent received for each chosen action. In contrast, the orange bars represent the average reward the agent would have received if it had selected a different action.\nThe comparison shows that actions like \"Shoot\" and \"Turn Left\" consistently provide higher factual rewards than their counterfactual counterparts, reinforcing that these choices were optimal given the state conditions."}, {"title": "C. Distribution of Chosen Actions", "content": "The action distribution plot in Figure 9 shows the frequency of each action selected by the agent during training. The actions \"Accelerate\" and \"Shoot\" were chosen most frequently, consistent with their higher factual rewards observed in the previous analysis. In contrast, the \u201cDecelerate\" action appeared much less often, corroborating its lower reward outcomes as reflected in the reward heatmap and counterfactual reward comparisons. The lower favorability of \"Decelerate\" can be attributed to its high risk: when the agent decelerates, it becomes vulnerable to enemy attacks, resulting in frequent defeats and reduced reward accumulation.\nThis distribution provides an intuitive explanation of the agent's strategy. The agent learns to favor actions with higher reward potentials and reduces the selection of actions with lower rewards. By analyzing the chosen actions alongside factual and counterfactual rewards, we gain insight into the agent's learned policy and the rationale behind its decision-making."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This study demonstrates the effectiveness of a Double Deep Q-Learning (DDQN) algorithm for multi-objective decision-making in a simulated fighter jet environment. The agent achieves a high success rate, effectively balancing navigation, engagement, and resource management. Explainability tech-niques, such as factual and counterfactual reward analysis, provide valuable insights into the agent's decision-making process, enhancing transparency and trust.\nFuture work will focus on extending the simulation to in-corporate more complex combat scenarios and multi-agent in-teractions. Additional efforts will explore integrating advanced explainability methods to further improve the interpretability of agent decisions and adapt the framework for real-world autonomous systems."}]}