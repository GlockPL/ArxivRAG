{"title": "Depth-guided Texture Diffusion for Image Semantic Segmentation", "authors": ["Wei Sun", "Yuan Li", "Qixiang Ye", "Jianbin Jiao", "Yanzhao Zhou"], "abstract": "Depth information provides valuable insights into the 3D structure especially the outline of objects, which can be utilized to improve the semantic segmentation tasks. However, a naive fusion of depth information can disrupt feature and compromise accuracy due to the modality gap between the depth and the vision. In this work, we introduce a Depth-guided Texture Diffusion approach that effectively tackles the outlined challenge. Our method extracts low-level features from edges and textures to create a texture image. This image is then selectively diffused across the depth map, enhancing structural information vital for precisely extracting object outlines. By integrating this enriched depth map with the original RGB image into a joint feature embedding, our method effectively bridges the disparity between the depth map and the image, enabling more accurate semantic segmentation. We conduct comprehensive experiments across diverse, commonly-used datasets spanning a wide range of semantic segmentation tasks, including Camouflaged Object Detection (COD), Salient Object Detection (SOD), and indoor semantic segmentation. With source-free estimated depth or depth captured by depth cameras, our method consistently outperforms existing baselines and achieves new state-of-the-art results, demonstrating the effectiveness of our Depth-guided Texture Diffusion for image semantic segmentation.", "sections": [{"title": "I. INTRODUCTION", "content": "IMAGE semantic segmentation is a crucial task in computer vision, designed to divide an image into regions that are meaningful based on visual characteristics. This process is vital for applications including object recognition, scene understanding, and image editing. Traditional approaches primarily utilize color, contour, and shape cues from 2D images to conduct semantic segmentation. However, these methods often fail to capture the 3D structure of the scene, which can result in less accurate outcomes, especially in complex settings.\nTo address this challenge, researchers have integrated depth information into the segmentation process, which offers valuable geometric context. Depth information enriches the understanding of the scene by revealing crucial cues, such as the distances of objects and their occlusion relationships, which significantly aid in the segmentation task. By incorporating depth into the input, segmentation outcomes become more accurate and robust.\nRecently, there have been notable research efforts in RGB-D semantic segmentation tasks, which combines color and depth data for enhanced analysis. Researchers have developed fusion models that leverage both RGB image and depth map to better exploit these complementary modalities. In some studies, depth maps are integrated as an additional channel in the early stages of the model [1]\u2013[3]. Similarly, fusion methods have been designed to combine features extracted from both RGB and Depth data [4]\u2013[11]. However, directly fusing depth information with image features poses a challenge due to the intrinsic distribution gap between the two modalities. As shown in Fig. 1, depth maps are inherently different from traditional RGB images. Depth maps, irrespective of their origin from depth estimation models or depth cameras, are characterized by their lack of texture and fine details, a stark contrast to what is typically observed in RGB representations. The inherent limitations of depth-sensing technology and depth estimation algorithms mean that these maps are better at representing broader spatial relationships rather than capturing the intricate textures and details. Furthermore, by reducing three-dimensional space to one-dimensional pixel intensities indicating distances, depth maps fail to capture the textural and color nuances inherent to RGB images. This discrepancy can lead to semantic mismatches when depth and RGB features are combined using simplistic methods without considering their fundamental differences.\nIn this study, we introduce a novel method, depth-guided texture diffusion, tailored to enhance the compatibility between depth and 2D images by selectively accentuating textural details such as object outlines within depth maps. This technique infuses the depth map with texture-like cues, effectively bridging the gap between depth and vision modality. To ensure the integrity of object structures in the depth map after texture diffusion, we deploy a structural loss function. This function helps preserve structural consistency, thereby minimizing discrepancies between the depth and RGB images and reducing information loss during fusion. Furthermore, we perform an integrated encoding of the texture-refined depth and RGB image, creating a visual prompt that embeds texture-aware depth information into the original model. This strategy enhances the accuracy of object extraction and boosts overall semantic segmentation performance.\nTo evaluate the effectiveness of our proposed method, we conduct extensive experiments across a diverse range of datasets. For camouflaged object detection, we utilize source-free estimated depth, while for salient object detection and indoor semantic segmentation, we employ depth captured by depth cameras. The experiment results demonstrate that our depth-guided texture diffusion consistently and significantly outperforms baseline methods, demonstrating superior performance over state-of-the-art segmentation methods.\nIn summary, this study introduces a depth-guided texture diffusion approach that enhances image semantic segmentation. By bridging the gap between depth data and 2D image, our method not only enriches the depth information but also improves the segmentation accuracy. This is achieved by incorporating textural cues into depth maps, aligning them more effectively with RGB images and enhancing the model's interpretation of complex scenes. Extensive experimental results across various datasets for salient object detection, camouflaged object detection, and indoor semantic segmentation confirm the effectiveness of our approach. These results demonstrate our method's capability to meet and exceed existing benchmarks, indicating its potential to contribute to advancements in depth-guided image segmentation.\nThe main contributions of this paper are threefold:\n\u2022\tWe introduce a novel depth-guided texture diffusion approach that bridges the gap between depth and vision modality, enabling the effective utilization of 3D structural information to achieve more accurate semantic segmentation.\n\u2022\tWe introduce structural consistency optimization to ensure that depth maps align with the structural integrity of RGB images after texture diffusion, thereby further boosting the model's performance and robustness.\n\u2022\tWe establish new state-of-the-art benchmarks on multiple commonly used datasets covering the task of camouflaged object detection, salient object detection and indoor semantic segmentation. This underlines the versatility and effectiveness of our depth-guided texture diffusion in a wide range of real-world applications."}, {"title": "II. RELATED WORK", "content": "In the landscape of contemporary computer vision, three vibrant research areas stand out in the RGB-D scene parsing domain: indoor semantic segmentation, salient object detection, and camouflaged object detection. Indoor semantic segmentation specifically aims to assign class labels to each pixel in indoor scenes, effectively delineating areas such as furniture, walls, and appliances to enhance scene understanding and spatial analysis in complex indoor spaces. Salient object detection is geared towards highlighting elements that naturally stand out, demanding focus in a scene. On the other hand, camouflaged object detection endeavors to uncover objects that are designed to blend seamlessly with their surroundings, posing a unique set of challenges.\nThe integration of RGB images with depth maps presents complex challenges due to the inherent disparities in their modal distributions. While RGB images capture detailed visual information, depth maps offer essential spatial relationships that are not immediately discernible in RGB data. This discrepancy often complicates the fusion process, requiring sophisticated approaches to fully leverage the complementary strengths of both modalities. Significant research efforts are directed towards developing innovative fusion techniques to effectively bridge this gap and enhance overall scene understanding.\nSubstantial strides have been made in these fields through concerted efforts in developing robust fusion modules that combine the fine-grained detail available in RGB imagery with the spatial context provided by depth data. Models like CMX (Zhang et al.) [12], TokenFusion (Wang et al.) [13], and HiDANet (Wu et al.) [14] exemplify the dynamic interplay of these two data streams, progressively enhancing the model's representational power and parsing accuracy.\nWhile some models focus primarily on leveraging RGB-D data for improved scene parsing performance, others introduce novel operator designs to extract and capitalize on the complementary information inherent within RGB-D modalities. Methods such as ShapeConv (Cao et al.) [15] and SGN (Chen et al.) [16] propose ingenious techniques to leverage depth-aware convolutions, enabling more nuanced feature extraction and improving object detection in cluttered scenes.\nDeparting from established paradigms, our technique employs texture diffusion to refine 1-channel depth representation in higher-level feature spaces. This method aims to bridge the distribution gap between RGB and depth information, substantially reducing information loss during the fusion. This approach leads to notable advancements in indoor semantic segmentation, salient object detection, and camouflaged object detection, as our empirical evidence will illustrate.\nThe task of Salient Object Detection (SOD) has been revolutionized by the incorporation of depth maps, facilitating a more comprehensive understanding of scene depth and object prominence. Based on their fusion tactics, RGB-D SOD processing methodologies have been organized into three primary frameworks: early fusion [17]\u2013[21], intermediate fusion [22]\u2013 [29], and late fusion [30]\u2013[32]. Zhang et al. [20] introduced an uncertainty-inspired early fusion approach for RGB-D saliency detection, efficiently merging RGB and depth data to address inherent ambiguities at the input level. Liu et al. [24] devised an intermediate fusion approach using a Selective Mutual Attention and Contrast (SMAC) module to enhance RGB-D saliency detection by leveraging cross-modal interactions and contrast mechanisms. Jin et al. [33] utilized asymmetric MobileNetV3 architectures to extract and integrate features from both RGB and depth data effectively, enhancing detection accuracy through structured multi-stage feature fusion. Zhang et al. [34] develop a multi-prior framework, processing RGB images alongside fine-grained, gradient, and depth priors through individual pipelines to accurately delineate salient objects. Han et al. [30] developed a late fusion strategy that separately processes the high-level representations from both images and depth maps before combining them into a unified saliency map, thereby improving the overall performance of the model. Furthermore, some research has also explored Transformer-based approaches. Liu et al. [29] develop iTransNet, utilizing a Triplet Transformer Embedding Network for enhanced RGB-D salient object detection, emphasizing multi-modal fusion and feature enhancement for improved depth and color integration.\nHistorically, camouflaged Object Detection (COD) has capitalized on a range of handcrafted features, such as 3D convexity [35], color [36], and edge detection [37], to identify objects designed to blend into their surroundings. Despite their ingenuity, these approaches often fall short in complex scenarios where the object's concealment is notably challenging. With the advent of extensive COD datasets [38]\u2013[41], there has been a shift towards deep learning models, which offer an enhanced ability to detect camouflaged objects. A pioneering deep learning-based model for COD, inspired by search mechanisms in predatory animals, is SINet [38], which employs a dual-module strategy for object localization and segmentation. Another noteworthy model is LSR [42], which employs a ranking mechanism for detecting and segmenting camouflaged objects, subsequently refining their representation. Efforts to integrate features from interactive learning into graph domains have also been made, exemplified by the two-stage MGL model [43], which enhances the boundary delineation of camouflaged objects. Despite the advancements, these methods sometimes struggle to detect the nuanced clues that differentiate camouflaged objects from their backgrounds. Recent approaches have started to leverage Transformer-based models [44]\u2013[46], which are adept at capturing the global contextual information necessary for identifying subtle differences in complex scenes. This is particularly relevant to our work, where we incorporate a texture diffusion method that complements these Transformer-based approaches by providing a depth-enhanced textural differentiation for more accurate object detection."}, {"title": "III. METHODOLOGY", "content": "Fig. 2 illustrates the overall architecture of our method. For SOD and COD challenges, we employ HitNet [47] as the backbone, while for indoor semantic segmentation, DFormer [48] is utilized, demonstrating our method's adaptability to different backbones.\nThe architecture is partitioned into three distinct components, each corresponding to a specific colored section in Fig. 2-blue for texture extraction, yellow for texture diffusion, and green for depth integration within segmentation networks. The texture extraction component, primarily consisting of the Texture Extraction (TE) module, is designed to capture and enhance textural features crucial for the subsequent stages. For texture diffusion, the Texture Diffusion (TXD) and Structure Consistency (SC) modules work in tandem to propagate and refine the textural information. Finally, in the depth integration phase, the Joint Embedding(JEB) and Adaptor modules are critical for seamlessly blending depth information into the segmentation networks, ensuring coherent feature synthesis and enhanced segmentation results.\nTexture and edge details are crucial for capturing important image information and bridging the distribution gap between RGB and depth images. However, directly extracting texture and edge details from the RGB space can often be challenging. Therefore, we extract features from the frequency domain to effectively capture the intricate texture features of objects.\nTaking the image X as input, our method first performs downsampling on X to obtain a downsampled version $X_d$. Then, we transform $X_d$ from the RGB domain to the frequency domain using Fourier Transform (FFT):\n$X_f = F(X_d)$   (1)\nwhere $X_f \\in R^{3 \\times H \\times W}$ is the frequency domain representation and $F$ denotes FFT. Then we obtain the high-frequency component through a high pass filter, and transform it back to RGB domain to preserve the shift invariance and local consistency of natural images:\n$X_h = F^{-1}(H(X_f, \\alpha))$   (2)\nwhere $H$ denotes the high pass filter and $\\alpha$ is the manually designed threshold which controls the low frequency component to be filtered out.\nAs can be seen in Fig. 3, the texture extraction process is exemplified using the case of a marine shrimp. The depth image provides a distinct layout of the scene, which is beneficial for foreground and background separation during the segmentation process. However, it fails to capture the finer details, such as the antennae of the shrimp, which are crucial for a comprehensive scene understanding. Our texture map extraction technique addresses this limitation by recovering these fine details, thereby significantly narrowing the distribution gap between the depth and RGB images. This enhancement ensures that subtle but critical features, like the delicate structures of the shrimp's antennae, are preserved and contribute to the accuracy of the segmentation process."}, {"title": "B. Texture Diffusion", "content": "Building on the concept of extracting texture and edge details from the frequency domain, as discussed in the previous section, this section focuses on integrating these features into depth maps. Recognizing the inherent texture absence in depth maps, our approach aims to enrich them with the detailed texture features captured from the RGB domain. This process not only enhances the depth maps with essential visual details but also narrows the distribution gap between depth and RGB images, facilitating a more harmonious fusion of the two modalities.\nIn bridging the gap between depth and RGB images, we recognize the limitations of pixelwise classification, which heavily relies on high-level semantics. To address this, our approach enriches depth maps with detailed texture features derived from the RGB domain, employing reliable low-level semantic cues such as color consistency and pattern smoothness. This strategy enables our model to accurately delineate object boundaries and maintain continuity on small surface structures, especially important in complex scenes where semantic extraction might otherwise lead to misclassification of small structural elements. For instance, rather than solely recognizing an object by its category, our model clusters parts of an object that share similar colors and surface patterns, a process fundamental to our texture diffusion approach. This method, as demonstrated in Fig. 3, facilitates a robust and nuanced integration of texture details within the depth modality. Such an approach not only enhances the depth map with crucial visual details, typically absent but vital for a harmonious fusion, but also ensures the structural integrity of the model across diverse datasets and their inherent scene complexities.\nIn this framework, texture diffusion is conceptualized as a message propagation process within a latent space. Illustrated in Fig. 4, the initial step involves using convolutional blocks to convert the depth map $D \\in R^{1 \\times H \\times W}$ into a series of latent features $D_i \\in R^{C \\times H \\times W}$, where C denotes the number of latent dims, (e.g., 24). This transformation is crucial for extracting salient features from $D$ and enhancing the robustness of message passing against noise.\nFor each channel, represented as $D_i$, we interpret deep pixels as nodes and establish a mechanism for message transfer among proximate nodes. The diffusion weights $W \\in R^{C \\times (r \\times r) \\times (H \\times W)}$ are predicted using convolutional blocks based on the texture feature $X_h$, where $r$ is the size of the processing window, (e.g., 7). These weights $W$ are normalized and shuffled to create distinct kernels $K_{i,u,v} \\in R^{r \\times r}$ for each channel $i$ and spatial position $(u, v)$, with the condition $\\sum_{(p,q) \\in N_{i,u,v}} K_{i,u,v}^{(p,q)} = 1$. The diffusion unfolds iteratively, with $K$ employed at each step to update the latent features:\n$D_i^{(t+1)}(u, v) = \\sum_{(p,q) \\in N} D_i^{(t)}(p, q) \\cdot K_{i,u,v}^{(u+p-\\frac{r+1}{2}, v+q-\\frac{r+1}{2})}$  (3)\nwhere $D_i^{(t)}(u,v)$ refers to the feature value at position $(u, v)$ in channel $i$, and $N$ signifies the neighboring locations around $(u, v)$. The iterative process extends for $S$ steps to ensure comprehensive message distribution throughout the entire area, enabling each node to gather information from all connected nodes:\n$S = \\lfloor \\frac{max(H, W)}{10} \\rfloor$   (4)\nThis procedure, despite its iterative nature, remains computationally efficient due to the typically small spatial dimensions of the extracted texture feature (e.g., 12 \u00d7 12), complemented by our GPU-optimized implementation which facilitates concurrent message passing for all segments."}, {"title": "2) Structural Consistency Optimization", "content": "In the context of depth and RGB image integration, the key challenge is to ensure that the texture-enhanced depth map D aligns structurally with the RGB image X. Structural consistency is crucial, particularly since the texture diffusion process can potentially alter the depth map's structural integrity. To mitigate this, the Structural Similarity Index (SSIM) [49] is employed as a means to quantify the structural fidelity. This step is vital for seamless depth-RGB fusion and precise semantic segmentation.\nBefore applying the Structural Consistency Optimization, we employ convolutional blocks to transform the diffusion-enhanced depth image D into a three-channel representation, aligning with the channel dimensions of the RGB image X. Following this transformation, we upscale the modified depth image to match the size of the RGB image X, yielding $D_u$ as a result of the upsampling process:\n$D_u = Upscale(Conv(D))$   (5)\nSpecifically, to maintain the similarity between texture-enhanced depth map $D_u$ and RGB image X during optimization, the structural consistency (SC) loss $L_{sc}$ is defined as:\n$L_{sc} = 1 - SSIM(D_u, X)$   (6)\nHere, the SC loss is weighted by a parameter $\\lambda$, to adjust its influence within the total loss function. Therefore, the total loss function $L_{total}$ is expressed as:\n$L_{total} = \\lambda \\cdot L_{sc} + L_{seg}$   (7)\nwhere $L_{seg}$ represents the original loss function for the segmentation task. This composite loss function's optimization process ensures that the depth map D is structurally compatible with the RGB image X, which is essential for effective integration and improved segmentation performance."}, {"title": "C. Depth Integration in Segmentation Networks", "content": "The joint embedding stage is essential for integrating depth information with RGB data. The texture-enhanced depth map $D_u$ is then combined with X through an element-wise addition:\n$Z = D_u + X$   (8)\nThe combined representation Z captures the features from both $D_u$ and X. This representation is processed through an embedding network (e.g., ConvNext) to refine the features for subsequent decoding.\nOur custom decoder transforms the output features into a comprehensive joint embedding. The decoder operation is defined as:\n$F_i = Conv_i(Upscale(O_i)), i = 1, 2, 3, 4.$   (9)\nwhere $O_i$ are the outputs from different network stages, and $Conv_i$ represents convolutional operations. These feature maps $F_i$ are concatenated and passed through a fusion convolution layer, represented by F, to yield the final embedding E:\n$E = F(\\bigoplus_{i=1}^4 F_i)$  (10)\nHere, $\\bigoplus$ denotes the concatenation operation across the feature maps.\nThe Adaptor phase is designed to incorporate depth information into the baseline segmentation network without altering its original structure. This integration enhances the segmentation capabilities while maintaining the integrity of the network's architecture.\nThe joint embedding E undergoes an adaptation process to align with the network's layers. This adaptation, denoted by A, which is composed of a stack of convolutions and ReLU activations, adjusts the channel dimensions of E to produce a series of layer-specific embeddings:\n$E^i = A_i(E), i \\in \\{1, ..., n\\}$   (11)\nwhere $E^i$ represents the adapted embedding for the i-th layer of the network, and n is the total number of layers.\nEach $E^i$ is resized to match the spatial dimensions of the corresponding layer's input $x_i$, denoted as $H_i \\times W_i$. The resizing function R can be formally expressed as:\n$E^i = R(E^i, H_i, W_i), i \\in \\{1, ..., n\\}.$   (12)"}, {"title": null, "content": "The resized embeddings $E^i$ are then added to the respective inputs of the layers, effectively infusing depth information into the segmentation process:\n$x_i' = x_i + E^i, i \\in \\{1, ..., n\\}.$   (13)\nThis approach ensures that the depth information is seamlessly integrated into each layer of the segmentation network, enriching its feature representation and improving segmentation accuracy, all while preserving the network's original configuration and functionality."}, {"title": "IV. EXPERIMENTS", "content": "Our experimental evaluation employs datasets across different domains: Salient Object Detection (SOD), Camouflaged Object Detection (COD), and indoor semantic segmentation."}, {"title": "1) SOD Datasets", "content": "For SOD datasets, we conduct experiments with the GT depth. We follow the conventional learning protocol [60], [62], [64] and use 700 images from NLPR [65] and 1,485 images from NJUK [66] for training. The rest are used for testing."}, {"title": "2) COD Datasets", "content": "For COD datasets, our experiments incorporate the concept of source-free depth, which is inspired by the approaches outlined in the PopNet [3]. Following Popnet [3], the state-of-the-art DPT model [68] with frozen weights is used as the depth estimation network, which provides us promising source-free depth. We follow the conventional training/testing protocol [38], [41], [57], [58], [69] and use 3,040 images from COD10K [38] and 1,000 images from CAMO [40] for training. The rest are used for testing."}, {"title": "3) Indoor Semantic Segmentation Datasets", "content": "For Indoor Semantic Segmentation Datasets, we conduct experiments with the GT depth. Following common experiment settings [70], [71], we fine-tune and evaluate the DFormer [48] on two widely used datasets, NYUDepthv2 [72] and SUN-RGBD [73]. These datasets contain RGB-D samples in various categories and are split into training and testing sets."}, {"title": "B. Evaluation Metrics", "content": "For the evaluation of our models on COD and SOD datasets, we employ the following four widely-used metrics:"}, {"title": "1) Structure-measure ($S_m$)", "content": "evaluates the structural similarity between prediction maps and their corresponding ground truth, closely mirroring human visual perception. It combines object-aware structural similarity ($s_o$) and region-aware structural similarity ($s_r$), using the formula $S_m = \\lambda \\cdot s_o + (1 - \\lambda) \\cdot s_r$, with $\\lambda$ set to 0.5."}, {"title": "2) F-measure ($F_\\beta$)", "content": "is a metric that balances precision and recall, calculated as $F_\\beta = \\frac{(1 + \\beta^2) \\cdot Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall}$, with $\\beta^2$ is set to 0.3 based on previous research. Following previous work [12], we adopt the maximum F-measure as our final evaluation."}, {"title": "3) Enhanced-alignment measure ($E_\\xi$)", "content": "combines local pixel values with the image-level mean value into a single term. It is calculated as $E_\\xi = \\frac{1}{W \\cdot H} \\sum_{x=1}^W \\sum_{y=1}^H \\phi(\\xi)$, where $\\xi$ represents the alignment matrix and $\\theta(\\xi)$ denotes the enhanced alignment matrix. Following previous work [12], we adopt the maximum E-measure as our final evaluation."}, {"title": "4) Mean Absolute Error (M)", "content": "measures the average absolute difference between the predicted map (Prd) and ground-truth map (G), calculated as $MAE = \\frac{1}{W \\cdot H} \\sum_{x=1}^W \\sum_{y=1}^H |Prd(x, y) - G(x,y)|$."}, {"title": null, "content": "For indoor semantic segmentation tasks, we utilize a distinct metric:\n\u2022\tMean Intersection over Union (mIoU) is the primary metric for evaluating segmentation performance. It averages the IoU across all semantic categories."}, {"title": "C. COD and SOD with texture diffusion", "content": "We adopt the HitNet [47] as backbone for our experiments, pre-trained on the ImageNet dataset. Training images are resized to 384 \u00d7384, with a batch size of 10 for all datasets. The AdamW optimizer is employed with a weight decay set to 0.1. The initial learning rate is set to 5\u00d710-4. The learning rates for the HitNet backbone and the joint embedding network are adjusted by scaling factors of 0.2 and 0.02, respectively. The learning rate follows a Cosine Annealing schedule. Our experiments are conducted on a pair of NVIDIA GeForce RTX 3090 GPUs."}, {"title": "2) Experiments on SOD Datasets", "content": "Table I provides a quantitative evaluation of our model compared to the state-of-the-art (SOTA) methods on RGB-D SOD datasets. Our method, \"Ours (HitNet)\", delineated in the bottom rows, shows an outstanding performance across all datasets. In comparison with PopNet, our model demonstrates an increase in the F\u00df by 2.2% and in the E\u03be by 2.1% on the NLPR dataset. On NJUK, our improvements are 1.4% for F\u00df and 1.9% for E\u03be. For the STERE dataset, the E\u03be is improved by 1.1%. Lastly, on the SIP dataset, there is an enhancement of 2.7% in $S_m$ and 2.8% in E\u03be. Notably, even when PopNet employs HitNet as its backbone\u2014referred to as \"PopNet (HitNet)\u201d\u2014our model still outperforms. These results solidify the effectiveness of our method, especially in utilizing sensor depth information to enhance salient object detection accuracy."}, {"title": "3) Experiments on COD Datasets", "content": "Table II illustrates the performance of various COD models on datasets with source-free depth. Our model, delineated in the bottom rows as \"Ours (HitNet)\", demonstrates superior performance across all datasets. Specifically, our model outperforms the best-performing previous model, \u201cPopNet (HitNet)\u201d, with a notable margin. In comparison with PopNet, our approach improves the F\u00df metric by 9.1% and the E\u03be metric by 9.5% on the CAMO dataset. In the case of the CHAMELEON dataset, our model sees an increase of 0.5% in M and 1.4% in E\u03be. On the more extensive COD10K and NC4K datasets, our method consistently surpasses the previous state-of-the-art with an improvement of 3.9% in F\u00df and 4.0% in E\u00e7 for COD10K, and 3.1% in F\u00df and 4.1% in E\u00e7 for NC4K. Echoing our success in the SOD experiments, our model not only maintains its superiority when PopNet adopts HitNet as its backbone\u2014now referred to as \"PopNet (HitNet)\"\u2014but also shows even higher relative improvements.\nThe marked improvements on the COD datasets, particularly on more challenging ones like COD10K and NC4K, underscore a critical aspect of our approach. In complex scenarios like COD, simple fusion techniques that do not account for textural details tend to incur significant information loss. Recognizing this limitation, our method has been specifically designed to address these challenges. These advancements highlight the efficacy of our method, particularly in leveraging source-free depth information to facilitate more accurate camouflaged object detection."}, {"title": "4) Qualitative Comparison", "content": "Fig. 5 illustrates the visual comparison among different methods. It can be seen that our model can identify objects on various challenging cases, e.g. occlusion (1st row), similar appearance between background and foreground in terms of color and shapes (2nd, 3rd and 3rd rows), and abundant edge details (5th row). Specifically, our models make satisfactory predictions in three aspects. 1) We can make robust layout perception, i.e. accurately locating the target objects and excluding other distracting regions (1st row). It credits our robust model design and reliable depth integration to alleviate the occlusion problem. 2) We can identify the target objects completely regardless of the background matching (2nd, 3rd and 4th rows). It shows that our texture diffusion enhances the model's perception of object integrity by aggregating low-level features. 3) We can precisely segment some details of the target objects, e.g. the antennae and legs of the shrimp (5th rows), which shows that our SC loss efficiently preserves the object structure."}, {"title": "D. Indoor Semantic Segmentation with Texture Diffusion", "content": "We adopt two common data augmentation strategies: random horizontal flipping and random scaling (from 0.5 to 1.75). For the NYUDepthv2 and SUN-RGBD datasets, the image sizes are respectively set to 480 \u00d7 640 and 530 \u00d7 730, with a batch size of 6 for both datasets. Cross-entropy loss is utilized as the optimization objective.The weight decay is set to 0.05. The initial learning rates for the NYUDepthv2 and SUN-RGBD datasets are set to 5 \u00d7 10-5 and 3 \u00d7 10-5, respectively. The learning rate for the joint embedding network is scaled by a factor of 0.2, and a poly decay schedule is employed. Deviating from the approach used in the original DFormer [48], we have chosen not to implement any test-time augmentation strategies in both our method and our replication of DFormer."}, {"title": "2) Experiments on Indoor Semantic Segmentation Datasets", "content": "The quantitative results of our semantic segmentation experiments are summarized in Table III. Our model, which utilizes the DFormer-L backbone [48], demonstrates superior performance over the benchmark models on two key datasets. On the NYUDepthv2 dataset, our method achieved a notable mean Intersection over Union (mIoU) of 58.0%, surpassing the DFormer-L's result of 56.1%. This improvement is indicative of the efficacy of our modifications to the original architecture. Turning to the SUN-RGBD dataset, our model's performance is again exemplary, with an mIoU of 53.2%, compared to the 51.3% achieved by the baseline DFormer-L model. Moreover, the qualitative comparisons between the semantic segmentation results of our method and 2 other state-of-the-art methods in Fig. 6 further demonstrate the advantage of our method. These results affirm the robustness of our approach across different datasets."}, {"title": "E. Ablation Study", "content": "In our ablation studies, we aim to demonstrate the effectiveness of the components we propose, the rationale behind the design choices, and the impact of these components specially in the context of COD and SOD datasets."}, {"title": "1) Quantitative Component Effectiveness Analysis", "content": "We start by incrementally stacking components on a base Hit-Net [47] structure, to illustrate their contribution to enhancing model performance on COD datasets. Our baseline models are established with HitNet, using only RGB images as inputs. This step-by-step assembly of the model components allows us to track and document the performance improvements throughout the model-building process."}, {"title": null, "content": "Depth cues are excluded to evaluate the performance contribution from the embedding network alone. By omitting the depth-specific adaptors, the architecture is further simplified. The Embedding features extracted from the RGB images are then adjusted using stage-wise convolutional layers (not layer-wise) to fit the baseline model's dimensions. This variant, referred to as \"+EB\"(embedding) in Table IV, helps us measure the effectiveness of the joint embedding network in enhancing the baseline model without the integration of depth data."}, {"title": null, "content": "Here we introduce the depth cues into our model. Specifically, we skip the aforementioned texture diffusion process and simply element-wise add the duplicated 3-channel depth maps into the RGB image as input. This simplified multi-modal fusion operation forms the RGBD branch and the whole network is marked as \u201c+JEB\u201d(joint embedding) in Table IV."}, {"title": null, "content": "Given the single channel depth maps, we utilize texture diffusion algorithm to generate the texture-enhanced 3-channel depth maps for the aforementioned RGBD branch, marked as \"+TXD\"(texture diffusion) in Table IV. Our TXD brings significant performance improvements, especially on the MAE metric."}, {"title": null, "content": "Finally, we introduce the structure consistency loss to reinforce the structural coherence of the depth map with the RGB image, marked as "}, {"title": "2) Qualitative Component Effectiveness Analysis", "content": "We also provide some visual samples to demonstrate the effectiveness of the aforementioned components. As shown in Fig. 7, baseline models make many false-negative predictions when encountering complex scenarios because their lack of the depth cues. EB and JEB can progressively correct some error predictions since they achieve additional high-quality feature representations. Based on them, TXD further improves the prediction quality thanks to the reliable enhanced depth maps generated from the texture diffusion. However, the objects predicted by TXD are still slightly fragmented, and many details cannot be precisely segmented. The SC loss can recover the structural information and precisely segment more details of the target objects."}, {"title": "3) Component Design Analysis", "content": "Here we conduct more detailed analyses to verify our design motivations for three components, i.e., JEB, TXD, and SCO. Specifically, we conduct experiments on the baseline and present results on two datasets, i.e., SIP, and COD10K, for quantitative analysis. We also provide some visual samples on NJUK for qualitative analysis."}, {"title": null, "content": ""}]}