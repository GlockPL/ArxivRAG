{"title": "Depth-guided Texture Diffusion for Image Semantic Segmentation", "authors": ["Wei Sun", "Yuan Li", "Qixiang Ye", "Jianbin Jiao", "Yanzhao Zhou"], "abstract": "Depth information provides valuable insights into the 3D structure especially the outline of objects, which can be utilized to improve the semantic segmentation tasks. However, a naive fusion of depth information can disrupt feature and compromise accuracy due to the modality gap between the depth and the vision. In this work, we introduce a Depth-guided Texture Diffusion approach that effectively tackles the outlined challenge. Our method extracts low-level features from edges and textures to create a texture image. This image is then selectively diffused across the depth map, enhancing structural information vital for precisely extracting object outlines. By integrating this enriched depth map with the original RGB image into a joint feature embedding, our method effectively bridges the disparity between the depth map and the image, enabling more accurate semantic segmentation. We conduct comprehensive experiments across diverse, commonly-used datasets spanning a wide range of semantic segmentation tasks, including Camouflaged Object Detection (COD), Salient Object Detection (SOD), and indoor semantic segmentation. With source-free estimated depth or depth captured by depth cameras, our method consistently outperforms existing baselines and achieves new state-of-the-art results, demonstrating the effectiveness of our Depth-guided Texture Diffusion for image semantic segmentation.", "sections": [{"title": "I. INTRODUCTION", "content": "IMAGE semantic segmentation is a crucial task in computer vision, designed to divide an image into regions that are meaningful based on visual characteristics. This process is vital for applications including object recognition, scene understanding, and image editing. Traditional approaches pri- marily utilize color, contour, and shape cues from 2D images to conduct semantic segmentation. However, these methods often fail to capture the 3D structure of the scene, which can result in less accurate outcomes, especially in complex settings.\nTo address this challenge, researchers have integrated depth information into the segmentation process, which offers valu- able geometric context. Depth information enriches the under- standing of the scene by revealing crucial cues, such as the distances of objects and their occlusion relationships, which significantly aid in the segmentation task. By incorporating depth into the input, segmentation outcomes become more accurate and robust.\nRecently, there have been notable research efforts in RGB- D semantic segmentation tasks, which combines color and depth data for enhanced analysis. Researchers have developed fusion models that leverage both RGB image and depth map to better exploit these complementary modalities. In some studies, depth maps are integrated as an additional channel in the early stages of the model [1]\u2013[3]. Similarly, fusion methods have been designed to combine features extracted from both RGB and Depth data [4]\u2013[11]. However, directly fusing depth information with image features poses a challenge due to the intrinsic distribution gap between the two modalities. As shown in Fig. 1, depth maps are inherently different from traditional RGB images. Depth maps, irrespective of their origin from depth estimation models or depth cameras, are characterized by their lack of texture and fine details, a stark contrast to what is typically observed in RGB representations. The inherent limitations of depth-sensing technology and depth estimation algorithms mean that these maps are better at representing broader spatial relationships rather than capturing the intricate textures and details. Furthermore, by reducing three-dimensional space to one-dimensional pixel intensities indicating distances, depth maps fail to capture the textural and color nuances inherent to RGB images. This discrepancy can lead to semantic mismatches when depth and RGB features are combined using simplistic methods without considering their fundamental differences.\nIn this study, we introduce a novel method, depth-guided texture diffusion, tailored to enhance the compatibility be- tween depth and 2D images by selectively accentuating tex- tural details such as object outlines within depth maps. This technique infuses the depth map with texture-like cues, effec- tively bridging the gap between depth and vision modality. To ensure the integrity of object structures in the depth map after texture diffusion, we deploy a structural loss function. This function helps preserve structural consistency, thereby minimizing discrepancies between the depth and RGB images and reducing information loss during fusion. Furthermore, we perform an integrated encoding of the texture-refined depth and RGB image, creating a visual prompt that embeds texture- aware depth information into the original model. This strategy enhances the accuracy of object extraction and boosts overall semantic segmentation performance.\nTo evaluate the effectiveness of our proposed method, we conduct extensive experiments across a diverse range of datasets. For camouflaged object detection, we utilize source- free estimated depth, while for salient object detection and indoor semantic segmentation, we employ depth captured by depth cameras. The experiment results demonstrate that our depth-guided texture diffusion consistently and significantly outperforms baseline methods, demonstrating superior perfor- mance over state-of-the-art segmentation methods.\nIn summary, this study introduces a depth-guided texture diffusion approach that enhances image semantic segmenta- tion. By bridging the gap between depth data and 2D image, our method not only enriches the depth information but also improves the segmentation accuracy. This is achieved by incorporating textural cues into depth maps, aligning them more effectively with RGB images and enhancing the model's interpretation of complex scenes. Extensive experimental re- sults across various datasets for salient object detection, cam- ouflaged object detection, and indoor semantic segmentation confirm the effectiveness of our approach. These results demonstrate our method's capability to meet and exceed existing benchmarks, indicating its potential to contribute to advancements in depth-guided image segmentation.\nThe main contributions of this paper are threefold:\n\u2022 We introduce a novel depth-guided texture diffusion approach that bridges the gap between depth and vi- sion modality, enabling the effective utilization of 3D structural information to achieve more accurate semantic segmentation.\n\u2022 We introduce structural consistency optimization to en- sure that depth maps align with the structural integrity of RGB images after texture diffusion, thereby further boosting the model's performance and robustness.\n\u2022 We establish new state-of-the-art benchmarks on multiple commonly used datasets covering the task of camouflaged object detection, salient object detection and indoor se- mantic segmentation. This underlines the versatility and effectiveness of our depth-guided texture diffusion in a wide range of real-world applications."}, {"title": "II. RELATED WORK", "content": "A. RGB-D Scene Parsing\nIn the landscape of contemporary computer vision, three vibrant research areas stand out in the RGB-D scene parsing domain: indoor semantic segmentation, salient object detec- tion, and camouflaged object detection. Indoor semantic seg- mentation specifically aims to assign class labels to each pixel in indoor scenes, effectively delineating areas such as furni- ture, walls, and appliances to enhance scene understanding and spatial analysis in complex indoor spaces. Salient object de- tection is geared towards highlighting elements that naturally stand out, demanding focus in a scene. On the other hand, camouflaged object detection endeavors to uncover objects that are designed to blend seamlessly with their surroundings, posing a unique set of challenges.\nThe integration of RGB images with depth maps presents complex challenges due to the inherent disparities in their modal distributions. While RGB images capture detailed visual information, depth maps offer essential spatial relationships that are not immediately discernible in RGB data. This discrepancy often complicates the fusion process, requiring sophisticated approaches to fully leverage the complementary strengths of both modalities. Significant research efforts are directed towards developing innovative fusion techniques to effectively bridge this gap and enhance overall scene under- standing.\nSubstantial strides have been made in these fields through concerted efforts in developing robust fusion modules that combine the fine-grained detail available in RGB imagery with the spatial context provided by depth data. Models like CMX (Zhang et al.) [12], TokenFusion (Wang et al.) [13], and HiDANet (Wu et al.) [14] exemplify the dynamic interplay of these two data streams, progressively enhancing the model's representational power and parsing accuracy.\nWhile some models focus primarily on leveraging RGB-D data for improved scene parsing performance, others introduce novel operator designs to extract and capitalize on the com- plementary information inherent within RGB-D modalities. Methods such as ShapeConv (Cao et al.) [15] and SGN (Chen et al.) [16] propose ingenious techniques to leverage depth- aware convolutions, enabling more nuanced feature extraction and improving object detection in cluttered scenes.\nDeparting from established paradigms, our technique em- ploys texture diffusion to refine 1-channel depth representation in higher-level feature spaces. This method aims to bridge the distribution gap between RGB and depth information, substantially reducing information loss during the fusion. This approach leads to notable advancements in indoor semantic segmentation, salient object detection, and camouflaged object detection, as our empirical evidence will illustrate.\nB. RGB-D Salient Object Detection\nThe task of Salient Object Detection (SOD) has been revolutionized by the incorporation of depth maps, facilitating a more comprehensive understanding of scene depth and object prominence. Based on their fusion tactics, RGB-D SOD pro- cessing methodologies have been organized into three primary"}, {"title": "III. METHODOLOGY", "content": "Fig. 2 illustrates the overall architecture of our method. For SOD and COD challenges, we employ HitNet [47] as the back- bone, while for indoor semantic segmentation, DFormer [48] is utilized, demonstrating our method's adaptability to different backbones.\nThe architecture is partitioned into three distinct compo- nents, each corresponding to a specific colored section in Fig. 2-blue for texture extraction, yellow for texture diffusion, and green for depth integration within segmentation networks. The texture extraction component, primarily consisting of the Texture Extraction (TE) module, is designed to capture and enhance textural features crucial for the subsequent stages. For texture diffusion, the Texture Diffusion (TXD) and Structure Consistency (SC) modules work in tandem to propagate and refine the textural information. Finally, in the depth integration phase, the Joint Embedding(JEB) and Adaptor modules are critical for seamlessly blending depth information into the segmentation networks, ensuring coherent feature synthesis and enhanced segmentation results.\nA. Texture Extraction\nTexture and edge details are crucial for capturing important image information and bridging the distribution gap between RGB and depth images. However, directly extracting texture and edge details from the RGB space can often be challenging. Therefore, we extract features from the frequency domain to effectively capture the intricate texture features of objects.\nTaking the image X as input, our method first performs downsampling on X to obtain a downsampled version Xd. Then, we transform Xd from the RGB domain to the frequency domain using Fourier Transform (FFT):\n$X_f = F(X_d)$ (1)\nwhere Xf \u2208 R3\u00d7H\u00d7W_is the frequency domain representa- tion and F denotes FFT. Then we obtain the high-frequency component through a high pass filter, and transform it back to RGB domain to preserve the shift invariance and local consistency of natural images:\n$X_h = F^{-1}(H(X_f, \\alpha))$ (2)\nwhere H denotes the high pass filter and \u03b1 is the manually designed threshold which controls the low frequency compo- nent to be filtered out.\nAs can be seen in Fig. 3, the texture extraction process is exemplified using the case of a marine shrimp. The depth image provides a distinct layout of the scene, which is beneficial for foreground and background separation during the segmentation process. However, it fails to capture the finer details, such as the antennae of the shrimp, which are crucial for a comprehensive scene understanding. Our texture map extraction technique addresses this limitation by recovering these fine details, thereby significantly narrowing the distribution gap between the depth and RGB images. This enhancement ensures that subtle but critical features, like the delicate structures of the shrimp's antennae, are preserved and contribute to the accuracy of the segmentation process.\nB. Texture Diffusion\n1) Diffusion Process: Building on the concept of extract- ing texture and edge details from the frequency domain, as discussed in the previous section, this section focuses on integrating these features into depth maps. Recognizing the inherent texture absence in depth maps, our approach aims to enrich them with the detailed texture features captured from the RGB domain. This process not only enhances the depth maps with essential visual details but also narrows the distribution gap between depth and RGB images, facilitating a more harmonious fusion of the two modalities.\nIn bridging the gap between depth and RGB images, we recognize the limitations of pixelwise classification, which heavily relies on high-level semantics. To address this, our approach enriches depth maps with detailed texture features derived from the RGB domain, employing reliable low-level semantic cues such as color consistency and pattern smooth- ness. This strategy enables our model to accurately delineate object boundaries and maintain continuity on small surface structures, especially important in complex scenes where se- mantic extraction might otherwise lead to misclassification of small structural elements. For instance, rather than solely recognizing an object by its category, our model clusters parts of an object that share similar colors and surface patterns, a process fundamental to our texture diffusion approach. This method, as demonstrated in Fig. 3, facilitates a robust and nuanced integration of texture details within the depth modality. Such an approach not only enhances the depth map with crucial visual details, typically absent but vital for a harmonious fusion, but also ensures the structural integrity of the model across diverse datasets and their inherent scene complexities.\nIn this framework, texture diffusion is conceptualized as a message propagation process within a latent space. Illustrated in Fig. 4, the initial step involves using convolutional blocks to convert the depth map D \u2208 R1\u00d7H\u00d7W_into a series of latent features D\u2208 RC\u00d7H\u00d7W, where C denotes the number of latent dims, (e.g., 24). This transformation is crucial for extracting salient features from D and enhancing the robustness of message passing against noise.\nFor each channel, represented as Di, we interpret deep pixels as nodes and establish a mechanism for message transfer among proximate nodes. The diffusion weights W\u2208 RC\u00d7(r\u00d7r)\u00d7(H\u00d7W) are predicted using convolutional blocks based on the texture feature Xh, where r is the size of the processing window, (e.g., 7). These weights W are normalized and shuffled to create distinct kernels Ki,u,v \u2208 Rrxr for each channel i and spatial position (u, v), with the condition \u2211(p,q) Ki,u,v = 1. The diffusion unfolds iteratively, with KC employed at each step to update the latent features:\n$D_i^{(t+1)}(u, v) = \\sum_{(p,q)\\in N} D_i^{(t)}(p,q) \\cdot K_{i,u+v,q+ \\frac{r-1}{2}}$ (3)\nwhere Du,v) refers to the feature value at position (u, v) in channel i, and N signifies the neighboring locations around (u, v). The iterative process extends for S steps to ensure comprehensive message distribution throughout the entire area, enabling each node to gather information from all connected nodes:\n$S = \\frac{max(H, W)}{2}$ (4)\nThis procedure, despite its iterative nature, remains compu- tationally efficient due to the typically small spatial dimensions of the extracted texture feature (e.g., 12 \u00d7 12), complemented by our GPU-optimized implementation which facilitates con- current message passing for all segments.\n2) Structural Consistency Optimization: In the context of depth and RGB image integration, the key challenge is to en- sure that the texture-enhanced depth map D aligns structurally with the RGB image X. Structural consistency is crucial, particularly since the texture diffusion process can potentially alter the depth map's structural integrity. To mitigate this, the Structural Similarity Index (SSIM) [49] is employed as a means to quantify the structural fidelity. This step is vital for seamless depth-RGB fusion and precise semantic segmen- tation.\nBefore applying the Structural Consistency Optimization, we employ convolutional blocks to transform the diffusion- enhanced depth image D into a three-channel representation, aligning with the channel dimensions of the RGB image X. Following this transformation, we upscale the modified depth image to match the size of the RGB image X, yielding Du as a result of the upsampling process:\n$D_u = Upscale(Conv(D))$ (5)\nSpecifically, to maintain the similarity between texture- enhanced depth map Du and RGB image X during optimiza- tion, the structural consistency (SC) loss Lsc is defined as:\n$L_{sc} = 1 - SSIM(D_u, X)$ (6)\nHere, the SC loss is weighted by a parameter A, to adjust its influence within the total loss function. Therefore, the total loss function Ltotal is expressed as:\n$L_{total} = \\lambda \\cdot L_{sc} + L_{seg}$ (7)\nwhere Lseg represents the original loss function for the segmentation task. This composite loss function's optimization process ensures that the depth map D is structurally compat- ible with the RGB image X, which is essential for effective integration and improved segmentation performance.\nC. Depth Integration in Segmentation Networks\n1) Joint Embedding: The joint embedding stage is essential for integrating depth information with RGB data. The texture- enhanced depth map Du is then combined with X through an element-wise addition:\n$Z = D_u + X$ (8)\nThe combined representation Z captures the features from both Du and X. This representation is processed through an embedding network (e.g., ConvNext) to refine the features for subsequent decoding.\nOur custom decoder transforms the output features into a comprehensive joint embedding. The decoder operation is defined as:\n$F_i = Conv_i(Upscale(O_i)), i = 1, 2, 3, 4.$ (9)\nwhere Or are the outputs from different network stages, and Convi represents convolutional operations. These feature maps F\u2081 are concatenated and passed through a fusion convolution layer, represented by F, to yield the final embedding E:\n$E = F(\\bigoplus_{i=1}^{4} F_i)$ (10)\nHere, denotes the concatenation operation across the fea- ture maps.\n2) Adaptor: The Adaptor phase is designed to incorporate depth information into the baseline segmentation network without altering its original structure. This integration en- hances the segmentation capabilities while maintaining the integrity of the network's architecture.\nThe joint embedding E undergoes an adaptation process to align with the network's layers. This adaptation, denoted by A, which is composed of a stack of convolutions and ReLU activations, adjusts the channel dimensions of E to produce a series of layer-specific embeddings:\n$E'i = A_i(E), i \\in {1, ..., n}.$ (11)\nwhere E represents the adapted embedding for the i-th layer of the network, and n is the total number of layers.\nEach E is resized to match the spatial dimensions of the corresponding layer's input xi, denoted as Hi \u00d7 Wi. The resizing function R can be formally expressed as:\n$E'i = R(E_i, H_i, W_i), i \\in {1, ..., n}.$ (12)\nThe resized embeddings E' are then added to the respective inputs of the layers, effectively infusing depth information into the segmentation process:\n$x'_i = x_i + E'_i, i \\in {1,...,n}.$ (13)\nThis approach ensures that the depth information is seamlessly integrated into each layer of the segmentation network, en- riching its feature representation and improving segmentation accuracy, all while preserving the network's original configu- ration and functionality."}, {"title": "IV. EXPERIMENTS", "content": "Our experimental evaluation employs datasets across differ- ent domains: Salient Object Detection (SOD), Camouflaged Object Detection (COD), and indoor semantic segmentation.\n1) SOD Datasets: For SOD datasets, we conduct experi- ments with the GT depth. We follow the conventional learning protocol [60], [62], [64] and use 700 images from NLPR [65] and 1,485 images from NJUK [66] for training. The rest are used for testing.\n2) COD Datasets: For COD datasets, our experiments incorporate the concept of source-free depth, which is inspired by the approaches outlined in the PopNet [3]. Following Popnet [3], the state-of-the-art DPT model [68] with frozen weights is used as the depth estimation network, which pro- vides us promising source-free depth. We follow the conven- tional training/testing protocol [38], [41], [57], [58], [69] and use 3,040 images from COD10K [38] and 1,000 images from CAMO [40] for training. The rest are used for testing.\n3) Indoor Semantic Segmentation Datasets: For Indoor Semantic Segmentation Datasets, we conduct experiments with the GT depth. Following common experiment set- tings [70], [71], we fine-tune and evaluate the DFormer [48] on two widely used datasets, NYUDepthv2 [72] and SUN- RGBD [73]. These datasets contain RGB-D samples in various categories and are split into training and testing sets.\nB. Evaluation Metrics\nFor the evaluation of our models on COD and SOD datasets, we employ the following four widely-used metrics:\n1) Structure-measure (Sm) [74] evaluates the structural similarity between prediction maps and their correspond- ing ground truth, closely mirroring human visual per- ception. It combines object-aware structural similarity (so) and region-aware structural similarity (sr), using the formula Sm = m \u00b7 so + (1 \u2212 m) \u00b7 sr, with m set to 0.5.\n2) F-measure (FB) is a metric that balances precision and recall, calculated as $F_B = \\frac{(1+\\beta^2) \\cdot Precision \\cdot Recall}{\\beta^2 \\cdot Precision+Recall}$, with \u03b2 is set to 0.3 based on previous research. Following previous work [12], we adopt the maximum F-measure as our final evaluation.\n3) Enhanced-alignment measure (E\u03be) [75] combines local pixel values with the image-level mean value into a single term. It is calculated as $E_{\\xi} = \\frac{1}{WH}\\sum_{x=1}^{W}\\sum_{y=1}^{H} \\phi(\\xi)$, where \u03c6 represents the alignment matrix and \u03b8(\u03be) denotes the enhanced alignment matrix. Following previous work [12], we adopt the maximum E-measure as our final evaluation.\n4) Mean Absolute Error (M) measures the average ab- solute difference between the predicted map (Prd) and ground-truth map (GT), calculated as $MAE = \\frac{1}{W\\times H} \\sum_{x=1}^{W} \\sum_{y=1}^{H} |Prd(x, y) \u2013 G(x,y)|$.\nFor indoor semantic segmentation tasks, we utilize a distinct metric:\n\u2022 Mean Intersection over Union (mIoU) is the primary metric for evaluating segmentation performance. It aver- ages the IoU across all semantic categories.\nC. COD and SOD with texture diffusion\n1) Experimental Setup: We adopt the HitNet [47] as back- bone for our experiments, pre-trained on the ImageNet dataset. Training images are resized to 384 \u00d7384, with a batch size of 10 for all datasets. The AdamW optimizer is employed with a weight decay set to 0.1. The initial learning rate is set to 5\u00d710-4. The learning rates for the HitNet backbone and the joint embedding network are adjusted by scaling factors of 0.2 and 0.02, respectively. The learning rate follows a Cosine Annealing schedule. Our experiments are conducted on a pair of NVIDIA GeForce RTX 3090 GPUs.\n2) Experiments on SOD Datasets: Table I provides a quan- titative evaluation of our model compared to the state-of-the- art (SOTA) methods on RGB-D SOD datasets. Our method, \"Ours (HitNet)\", delineated in the bottom rows, shows an outstanding performance across all datasets. In comparison with PopNet, our model demonstrates an increase in the FB by 2.2% and in the Es by 2.1% on the NLPR dataset. On NJUK, our improvements are 1.4% for FB and 1.9% for \u0395\u03be. For the STERE dataset, the E\u015f is improved by 1.1%. Lastly, on the SIP dataset, there is an enhancement of 2.7% in Sm and 2.8% in Eg. Notably, even when PopNet employs HitNet as its backbone-referred to as", "Datasets": "Table II illustrates the performance of various COD models on datasets with source- free depth. Our model, delineated in the bottom rows as", "HitNet)": "demonstrates superior performance across all datasets. Specifically, our model outperforms the best- performing previous model, \u201cPopNet (HitNet)\u201d, with a notable margin. In comparison with PopNet, our approach improves the F3 metric by 9.1% and the E\u015f metric by 9.5% on the CAMO dataset. In the case of the CHAMELEON dataset, our model sees an increase of 0.5% in M and 1.4% in Eg. On the more extensive COD10K and NC4K datasets, our method consistently surpasses the previous state-of-the-art with an improvement of 3.9% in F\u00df and 4.0% in E\u00e7 for COD10K, and 3.1% in F\u00df and 4.1% in Eg for NC4K. Echoing our success in the SOD experiments, our model not only maintains its superiority when PopNet adopts HitNet as its backbone\u2014now referred to as"}, {"HitNet)": "but also shows even higher relative improvements.\nThe marked improvements on the COD datasets, partic- ularly on more challenging ones like COD10K and_NC4K, underscore a critical aspect of our approach. In complex sce- narios like COD, simple fusion techniques that do not account for textural details tend to incur significant information loss. Recognizing this limitation, our method has been specifically designed to address these challenges. These advancements highlight the efficacy of our method, particularly in leverag- ing source-free depth information to facilitate more accurate camouflaged object detection.\n4) Qualitative Comparison: Fig. 5 illustrates the visual comparison among different methods.\nD. Indoor Semantic Segmentation with Texture Diffusion\n1) Experimental Setup: We adopt two common data aug- mentation strategies: random horizontal flipping and random scaling (from 0.5 to 1.75). For the NYUDepthv2 and SUN- RGBD datasets, the image sizes are respectively set to 480 \u00d7 640 and 530 \u00d7 730, with a batch size of 6 for both datasets. Cross-entropy loss is utilized as the optimization objective.The weight decay is set to 0.05. The initial learning rates for the NYUDepthv2 and SUN-RGBD datasets are set to 5 \u00d7 10-5 and 3 \u00d7 10-5, respectively. The learning rate for the joint embedding network is scaled by a factor of 0.2, and a poly decay schedule is employed. Deviating from the approach used in the original DFormer [48], we have chosen not to implement any test-time augmentation strategies in both our method and our replication of DFormer.\n2) Experiments on Indoor Semantic Segmentation Datasets: The quantitative results of our semantic segmentation ex- periments are summarized in Table III. Our model, which utilizes the DFormer-L backbone [48], demonstrates superior performance over the benchmark models on two key datasets. On the NYUDepthv2 dataset, our method achieved a notable mean Intersection over Union (mIoU) of 58.0%, surpassing the DFormer-L's result of 56.1%. This improvement is indicative of the efficacy of our modifications to the original architecture. Turning to the SUN-RGBD dataset, our model's performance is again exemplary, with an mIoU of 53.2%, compared to the 51.3% achieved by the baseline DFormer-L model. Moreover, the qualitative comparisons between the semantic segmenta- tion results of our method and 2 other state-of-the-art methods in Fig. 6 further demonstrate the advantage of our method. These results affirm the robustness of our approach across different datasets.\nE. Ablation Study\nIn our ablation studies, we aim to demonstrate the effective- ness of the components we propose, the rationale behind the design choices, and the impact of these components specially in the context of COD and SOD datasets.\n1) Quantitative Component Effectiveness Analysis: We start by incrementally stacking components on a base Hit- Net [47] structure, to illustrate their contribution to enhancing model performance on COD datasets. Our baseline models are established with HitNet, using only RGB images as inputs. This step-by-step assembly of the model components allows us to track and document the performance improvements throughout the model-building process.\na) Embedding without Depth Information: Depth cues are excluded to evaluate the performance contribution from the embedding network alone. By omitting the depth-specific adaptors, the architecture is further simplified. The Embedding features extracted from the RGB images are then adjusted using stage-wise convolutional layers (not layer-wise) to fit the baseline model's dimensions. This variant, referred to as", "EB": "embedding) in Table IV, helps us measure the effectiveness of the joint embedding network in enhancing the baseline model without the integration of depth data.\nb) Embedding with Depth Information: Here we intro- duce the depth cues into our model. Specifically, we skip the aforementioned texture diffusion process and simply element- wise add the duplicated 3-channel depth maps into the RGB image as input. This simplified multi-modal fusion operation forms the RGBD branch and the whole network is marked as \u201c+JEB\u201d(joint embedding) in Table IV.\nc) Diffusion Learning Mechanism: Given the single channel depth maps, we utilize texture diffusion algorithm to generate the texture-enhanced 3-channel depth maps for the aforementioned RGBD branch, marked as", "TXD": "texture diffusion) in Table IV. Our TXD brings significant perfor- mance improvements, especially on the MAE metric.\nd) Structural Coherence Optimization: Finally, we intro- duce the structure consistency loss to reinforce the structural coherence of the depth map with the RGB image, marked as", "SC": "n Table IV. SC loss serves as an effective method to align structural details, enhancing the consistency of low-level features across different modalities of data, which promotes a significant performance gain.\n2) Qualitative Component Effectiveness Analysis: We also provide some visual samples to demonstrate the effectiveness of the aforementioned components. As shown in Fig. 7, baseline models make many false-negative predictions when encountering complex scenarios because their lack of the depth cues. EB and JEB can progressively correct some error predictions since they achieve additional high-quality feature representations. Based on them, TXD further improves the prediction quality thanks to the reliable enhanced depth maps generated from the texture diffusion. However, the objects predicted by TXD are still slightly fragmented, and many details cannot be precisely segmented. The SC loss can recover the structural information and precisely segment more details of the target objects.\n3) Component Design Analysis: Here we conduct more detailed analyses to verify our design motivations for three components, i.e., JEB, TXD, and SCO. Specifically, we con- duct experiments on the baseline and present results on two datasets, i.e., SIP, and COD10K, for quantitative analysis. We also provide some visual samples on NJUK for qualitative analysis.\na) Analysis of the JEB: To certify the effectiveness of our feature fusion approach used in the joint embedding, we con- ducted comparisons between the additive strategy employed in JEB and alternative fusion techniques, such as Hardmard product and concatenation strategy. As demonstrated in Ta- ble V, our method achieves the best performance when the addition is used.\nb) Analysis of the TXD: Table VI shows the results of our model with different iteration steps of TXD. We can observe that increasing the iteration step can bring performance im- provement, e.g., 1.1% and 0.9% improvement of F\u03b2 on the SIP dataset and COD10K dataset, respectively, when increasing the iteration step from 1 to 4. Additionally, from the visual results in Fig. 8, we can find that using more iterations can effectively exclude incorrect predictions. Such performance improvement begins to saturate when iterating four times. These results indicate that each diffusion step can achieve better results than the previous step until the texture information is diffused across the whole depth image, demonstrating that TXD works following its design motivation.\nMoreover, the diffusion kernel size in the TXD process is a key factor affecting feature fusion effectiveness. As detailed in Table VII, a 7x7 kernel size emerges as the optimal choice. It achieves a delicate balance between efficient information diffusion and the preservation of fine details. This medium- sized kernel ensures robust performance, while both larger and smaller kernels present their unique drawbacks. Larger kernels, although quicker in spreading information, tend to gloss over subtle textural nuances. They may lead to a generalized representation, missing finer details. On the other hand, smaller kernels struggle with limited information reach and increased sensitivity to noise, which leads to an overall suboptimal performance. The 7x7 kernel efficiently propagates texture information throughout the depth image, effectively harnessing the spatial context and preserveing crucial texture details, thus reinforcing the TXD's design principles.\nContinuing with our ablation studies, we further examine the texture extraction (TE) mechanism, focusing on the high-pass filter threshold \u03b1. This parameter is pivotal as it determines the effectiveness of the TE process, which is implemented via a Fourier transform. The experiments revealed that an optimal \u03b1 of 0.3 not only enhances texture detail but also suppresses unwanted noise, striking a balance between cap- turing essential features and avoiding information overload, as shown in Table IX. This balance is crucial for the model's ability to discern relevant patterns and ignore distractions. In addition to varying \u03b1, our analysis extends to contrasting the model's performance with (", "w/o TE": "the texture extraction process. The results, as shown in Table VIII, validate the TE method's effectiveness, where \u201cw/ TE", "performance.": "nc) Analysis of the SCO: Tuning the loss weight A is cru- cial in Structural Coherence Optimization for segmenting com-"}, {"title": "V. CONCLUSION", "content": "In this work, we introduce a Depth-guided Texture Dif- fusion technique that effectively resolves the fusion issues between depth and vision modalities, enhancing the use of 3D structural information from depth maps for image semantic segmentation. Our method first extracts key texture and edge features from the RGB images, creating a texture map. This texture map is then integrated into the depth map to em- phasize structural details crucial for extracting object shapes. By combining this texture-enriched depth with the original RGB data, our approach utilizes depth to improve image segmentation accuracy. Through extensive experiments and ablation studies on commonly used datasets for salient object detection, camouflaged object detection, and indoor semantic segmentation, our method consistently improves baselines and establishes new state-of-the-art results. This research under- scores the critical role of texture in depth maps for complex scene interpretation in semantic segmentation, paving the way for future advancements in depth utilization."}]}