{"title": "SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin", "authors": ["Hao Yi", "Qingyang Li", "Yulan Hu", "Fuzheng Zhang", "Di Zhang", "Yong Liu"], "abstract": "Recently, enhancing the numerical and logical reasoning capability of Large Language Models (LLMs) has emerged as a research hotspot. Existing methods face several limitations: inference-phase techniques (e.g., Chain of Thoughts) rely on prompt selection and the pretrained knowledge; sentence-level Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with stepwise mathematical correctness and depend on stronger models distillation or human annotations; while Reinforcement Learning (RL) approaches incur high GPU memory costs and unstable training. To address these, we propose Self-training framework integrating Process Preference learning using Dynamic value margin (SPPD). SPPD leverages a process-based Markov Decision Process (MDP) and Bellman optimality equation to derive dynamic value margin on step-level preference optimization, which employs tree-based self-sampling on model responses without any distillation from other models. Furthermore, we theoretically prove that SPPD is equivalent to on-policy policy gradient methods under reward constraints. Experiments on 7B-scale models demonstrate superior performance across in-domain and out-domain mathematical benchmarks. We open-source our code at https://anonymous.4open.science/r/SPPD-DCDD.", "sections": [{"title": "1 Introduction", "content": "Recently, the O-series models (OpenAI, 2024) have achieved a significant leap in the mathematical reasoning capabilities of LLMs. Consequently, enhancing the numerical and logical reasoning capability of LLMs has emerged as a research hotspot (Chen et al., 2023; Yu et al., 2023; Jimenez et al., 2023; Shao et al.; Liao et al., 2024b; Lai et al., 2024; Guo et al., 2025).\nFrom now on, there are lots of methods to promote the model reasoning capability. During the inference phase, the most common and effective approach is to employ Chain of Thoughts (CoT) prompts, which can stimulate the model's inherent reasoning and thinking abilities (Wei et al., 2022). Similar methods include Tree of Thoughts (ToT) (Yao et al., 2024), Best of N (BoN) (Zheng et al., 2024; Yuan et al., 2024), Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Zhang et al., 2024a), and so on. However, these methods do not involve training policy models but rely on increasing computational volume during the inference phase, heavily depending on prompt selection and the pretrained knowledge embedded within the model. Moreover, SFT (Zhang et al., 2024a; Feng et al., 2023) or DPO (Rafailov et al., 2024b,a) based on human annotations or feedback from more advanced AI also serves as an effective way to enhance the model's reasoning capabilities. These methods leverage human-curated selections or stronger open-source and close-source models to inject good reasoning paradigms, such as long-thought processes and reflection, into the model being trained. However, all these methods are at the sentence level, which does not align well with the requirement for correctness at every step in mathematical reasoning scenarios. Meanwhile, such methods are either constrained by time-consuming manual selection processes or require support from more powerful models, like STILL-2 (Min et al., 2024) and Skywork-01-open (Skywork, 2024b). When the model to be trained is already the strongest reasoning model available, how can we further improve the model's reasoning performance without any distillation? While RL-based methods like Proximal Policy Optimization (PPO) (Schulman et al., 2017), Group Relative Policy Optimization (GRPO) (Shao et al.; Guo et al., 2025), Reinforcement Fine-Tuning (RFT) (Luong et al., 2024), etc., can address the aforementioned issues."}, {"title": "2 Related Work", "content": "Enhance Reasoning Capability of LLMs. Recently, a substantial body of research focuses on enhancing the reasoning capabilities of LLMs. These methodologies are primarily divided into two categories: the inference phase and the Post-Training phase. During the inference phase, early studies concentrate on stimulating the model's inherent reasoning abilities by modifying prompts (Wei et al., 2022; Yao et al., 2024). Subsequent research leverages the consistency of multiple inferences by the model (Yuan et al., 2024; Wang et al., 2022) or integrates tree search strategies (Feng et al., 2023; Zhang et al., 2024a) to guide the model towards more accurate decoding processes. However, these approaches do not involve training and heavily rely on the model's intrinsic reasoning capabilities. In the Post-Training phase, SFT (Feng et al., 2023) and DPO (Rafailov et al., 2024b,a) emerge as primary enhancement techniques. These methods depend on human-curated selection of high-quality reasoning trajectories or distillation of responses from stronger models (Min et al., 2024) to improve the reasoning performance of smaller or weaker models. Nevertheless, these approaches are time-consuming and unsustainable. RL paradigms, exemplified by PPO (Schulman et al., 2017), GRPO (Guo et al., 2025; Shao et al.), and ReFT (Luong et al., 2024), effectively address the aforementioned issues but introduce significant GPU memory consumption and training instability challenges.\nStep-Level Direct Preference Optimization. In order to optimize and improve the model's reasoning capability from the step level, CPO (Zhang et al., 2024b) aligns each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process, but it control LLMs to generate the thought data by prompt, which may influent the model generation quality. Step-DPO (Lai et al., 2024) treats individual reasoning steps as units for preference optimization. However, it utilizes the GPT4 to evaluate the correctness of step, which could bring introduced bias and is expensive. TPO (Liao et al., 2024b) claims that the policy can potentially learn more effectively from a ranked preference list of responses given the prompt and utilizes adaptive step reward to adjust the reward values of each step in the trajectory. However, it introduce a stronger form of \"catastrophic forgetting\" and imbalanced distribution of the preference tree reward values."}, {"title": "3 Preliminaries", "content": "In this section, we first define the step-level MDP in natural language process. Subsequently, based on the step-level MDP, we modify the original RLHF objective and provide the optimal (fixed-point) solution to maximum casual entropy problem.\nStep-Level MDP in LLMs. We describe the step-level MDP in natural language process. The step-level MDP is defined as the following quintuple: $M = (A, S, f, r, p_0)$, where $A$ represents the set of action spaces, consisting of a reasoning step $a_t$; $S$ represents the set of states, which in natural language denotes the sequence of the problem and the current reasoning step $s_t = s_0|a_1|a_2|...|a_t$, where $|$ denotes the string concatenation operation and $s_0$ is the problem. It is noteworthy that the selection of $a_t$ depends on the current state. $f : S \\times A \\rightarrow S$ represents the state transition function, indicating the transition from the current state to the next state after performing a certain action. Specifically, $f(s,a) = s|a$. $r : S \\times A \\rightarrow \\mathbb{R}$ is the reward function, representing the immediate reward obtained after performing a certain action in the current state. $p_0$ represents the distribution of the problems.\nRLHF objective with the Step-Level MDP. In the original RLHF objective (Ouyang et al., 2022), the rewards obtained from trajectories are modeled as a bandit problem (Zhao et al., 2024). However, such sparse rewards are not suitable for policy learning in models, especially in mathematical reasoning tasks (Riedmiller et al., 2018; Wilcox et al., 2022). Based on the step-level MDP, we modify the RLHF objective as follows (Rafailov et al., 2024a):\n$\\max_{\\pi_\\theta} \\mathbb{E}_{s_0 \\sim \\rho(s_0)} [\\sum_{t=0}^T (r(s_t, a_t) + \\beta \\log \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{ref}(a_t | s_t)}) + \\beta H(\\pi_\\theta) | s_0 \\sim \\rho(s_0)]$,\nwhere $\\pi_\\theta$ represents the large language policy model with learnable parameters, $\\pi_{ref}$ represents reference model and $\\beta$ is used to control the policy model not to deviate too far from the reference model, $H(\\pi_\\theta)$ is the entropy of $\\pi_\\theta$. This optimization problem is known as the Maximum Causal Entropy. Ziebart (2010) have proven that Equation (1) has a fixed-point solution $\\pi^*$, defined as follows:\n$\\pi^*(a_t | s_t) = \\pi_{ref}(a_t|s_t)e^{(Q^*(s_t,a_t)-V^*(s_t))/\\beta}$,"}, {"title": "4 Method", "content": "In this section, we first propose a process preference learning scheme using dynamic value margins based on the step MDP and BT-model, and then refine this preference learning scheme using the reward equivalence. Additionally, we introduce a tree-based self-sampling method designed to generate step trajectories with common prefix. Finally, we introduce sentence-level SFT and DPO using PRM, aiming to make the model training smoother and more effective.\n4.1 Process Preference Learning with Dynamic Value Margin\nFirst, we derive the process preference learning with dynamic value margin starting from the optimal Bellman equation and revisit the traditional step DPO (Lai et al., 2024) from a different perspective.\nLemma 4.1 (Optimal Step Reward Function). Under the step MDP definition in Section 3 and fix solution for the maximum casual entropy problem (Equation (2)), the optimal step reward function"}, {"title": "can be calculate as follow:", "content": "$r(s_t, a_t) = \\beta \\log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)} + V^*(s_{t+1}) \u2013 V^*(s_t)$.\nProof of Lemma (4.1) is shown in Appendix D.1. Equation (3) demonstrates that the immediate reward in the MDP consists of the model's implicit reward and the value gain of the optimal value function. Assuming we have the following step-level preference pairs ($s_t, a_{t+1}^+, a_{t+1}^-$), based on the step-level BT-model, we have the optimal preference distribution:\n$p^*(a_{t+1}^+ > a_{t+1}^-) = \\sigma (r(s_t, a_{t+1}^+) \u2013 r(s_t, a_{t+1}^-))$.\nHere, $\\sigma(x) = 1/(1 + e^{-x})$ is the sigmoid function. Finally, we give the step DPO loss using dynamic value margin.\nTheorem 4.2 (Step DPO Loss Using Dynamic Value Margin.). If we aim to minimize the Kullback-Leibler(KL) divergence between the step-level preference distribution $P_{data}$ in $D_{step}$ and the model's current preference distribution $p_\\theta$ under the sampling of $\\tau_{ref}$, we can obtain the following loss function:\n$L_{step-dpo} = -\\mathbb{E}_{a_{t+1}^+,a_{t+1}^- \\sim \\tau_{ref}(\\cdot|s_t)} [\\log \\sigma(\\beta h_\\theta(a_{t+1}^+, a_{t+1}^-)\n\u2013 (V^*(s_{t+1}^+) \u2013 V^*(s_{t+1}^-)))]$,\nwhere $h_\\theta(a_{t+1}^+,a_{t+1}^-)\n= \\log \\frac{\\pi_\\theta(a_{t+1}^+|s_t)}{\\pi_{ref}(a_{t+1}^+|s_t)}\n\u2013 \\log \\frac{\\pi_\\theta(a_{t+1}^-|s_t)}{\\pi_{ref}(a_{t+1}^-|s_t)}$.\nThe proof is shown in Appendix D.2. In traditional step DPO (Lai et al., 2024), the value function prediction at each step is defined as 0. However, we argue that the value gain in the immediate reward (Equation (3)), or equivalently, the term $V^*(s_{t+1}) \u2013 V^*(s_{t+1})$ in Equation (4), considers the difference in the optimal value function predictions for the preferred states. This manifests in the step DPO loss as a dynamic value margin that varies depending on the preferred states $s_{t+1}^+$ and $s_{t+1}^-$, rather than treating all states uniformly. In practice, we use a PRM score to approximate the optimal value function. In Section 5, we will provide more profound theoretical insights and conclusions."}, {"title": "Reward Equivalence.", "content": "To make the optimization process more controllable, we revise Equation (3) by introducing the concept of reward equivalence.\nLemma 4.3. Reward Equivalence (Rafailov et al., 2024a)] Two reward functions r and r' are equivalent if and only if there exists a potential function $\\Phi : S \\rightarrow \\mathbb{R}$ that satisfies the following equation:\nr(st, at) = r'(st, at) + $\\Phi(f(st, at)) \u2013 $\\Phi(st).\nIn Equation (3), the potential function is our optimal value function, i.e., $\\Phi(s) = V^*(s)$. At the same time, it is easy to see that when we scale this potential function, $\\Phi'(s) = \\gamma$\\Phi(s), $\\Phi'$ still satisfies the definition of potential function. Therefore, we can modify Equation (3) to obtain an equivalent reward expression:\nr'(st, at) = r(st, at) + $\\gamma$\\Phi(f(st, at)) \u2013 $\\gamma$\\Phi(st).\nRepeating the derivation in Section 4.1, we modify the final loss as follows:\n$L_{step-dpo}\n= -\\mathbb{E}_{a_{t+1}^+,a_{t+1}^- \\sim \\tau_{ref} (s_t)}[\\log \\sigma(\\beta h_\\theta(a_{t+1}^+, a_{t+1}^-)\n\u2013 (V^* (s_{t+1}^+) \u2013 V^*(s_{t+1}^-)))]$.\nRemark. Although the concept of reward equivalence in Rafailov et al. (2024a) implies that the optimal preference model belongs to the same equivalence class, including the original step-DPO when $\\gamma$ = 0, the introduction of $\\gamma$ makes the optimization process more controllable due to its influence on optimization. This has been verified in Section 6.3."}, {"title": "4.2 Tree-Based Self-Sampling on LLMs", "content": "Traditional reasoning algorithms (token-level decoding) is almost impossible to guarantee the generation of reasoning trajectories with identical prefixes. To address this issue, this paper adopts a tree-structured reasoning approach, as illustrated in Figure 1. Specifically, the process is divided into four steps: \"Selection, Expansion, Collection and Scoring\". During the selection process, at the current state $s_t$, we record the average log probability score for each child node $a_t$, defined as:\ns(a_t | s_t) = $\\frac{1}{|a_t|} \\sum_{i=0}^{|a_t|-1} \\log infer(a_{t,i}|s_t|a_{t,<i})$,\nwhere $|a_t|$ represents the token length of the current step, $a_{t,<i}$ denotes the first $i \u2013 1$ tokens of"}, {"title": "5 Theoretical Analysis", "content": "In this Section, we prove that the equivalence between offline step DPO and online policy gradient under the specific reward definition.\nDefinition 5.1 (Preference decoding model $\\pi_\\theta$ induced by $\\pi_\\theta$). Assume that when $s = s_t$, the possible action space $A_t = \\{a_{t+1}^+, a_{t+1}^-\\}$. We define $\\pi^\\theta$ as the following parameterized distribution:\n$\\pi_{\\theta}(a_{t+1}|s_t) = \\sigma(r_{\\theta,t}^+ - r_{\\theta,t}^-)$,$\nwhere,\nr_{\\theta,t}^+ = \\beta \\log \\frac{\\pi_{\\theta}(a_{t+1}^+|s_t)}{\\pi_{ref}(a_{t+1}^+|s_t)} - V^*(s_{t+1}^+) + V^*(s_t),\nr_{\\theta,t}^- = \\beta \\log \\frac{\\pi_{\\theta}(a_{t+1}^-|s_t)}{\\pi_{ref}(a_{t+1}^-|s_t)} - V^*(s_{t+1}^-) + V^*(s_t).\nRemark. The preference decoding model $\\pi_{\\theta}$ can be viewed as performing sampling on a binary prefix tree based on preference probabilities. This model relies on the probability outputs of the standard language model $\\pi_{\\theta}$.\nLemma 5.1 (Online Policy Gradient on $\\pi_{\\theta}$ (Lin and Zhou, 2019) ). For any MDP, the expected long-term reward on $\\pi_{\\theta}$ is given by $J(\\theta) = \\sum_{\\tau} \\pi_{\\theta}(\\tau)r(\\tau)$, where $r(t)$ represents the long-term reward of trajectory $\\tau$. The policy gradient of this expected long-term reward on $\\pi^\\theta$ is:\n$\\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[r(\\tau) \\sum_{t=0}^{T-1} \\nabla \\log \\pi_{\\theta}(a_{t+1}|s_t)]$.\nTheorem 5.2 (Equivalence Between Offline Step DPO and Online Policy Gradient). If we define the reward in Equation (6) as $r(\\tau) = \\prod_{t=1}^T \\frac{\\pi_{ref}(a_t|s_t)}{\\pi_\\theta(a_t|s_t)}$, and define the Offline every-step preference loss as:\n$L_{every-step} = -\\mathbb{E}_{\\tau \\sim \\pi_{ref}}[ \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_{t+1}|s_t) ]$,\nthen the following equivalence holds:\n$\\nabla_\\theta J(\\theta) = -\\nabla_\\theta L_{every-step}$.\nThe proof is shown in Appendix D.3."}, {"title": "6 Experiments", "content": "6.1 Setup\nDatasets. For the training prompt data, we sample a total of 10k prompts from the training datasets of GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), with GSM8K and MATH accounting for 40% and 60% respectively. We use Qwen2.5-7B-Base (Yang et al., 2024) and Llama3.1-8B-Instruct (Meta@AI, 2024) as the base models, and employ Skywork-01-Open-PRM-Qwen-2.5-7B (Skywork, 2024a) as PRM to generate $D_{step}$ using the step data generation method mentioned in Section 4.2. For more information regarding the data format and PRM, please refer to the Appendix A & B.\nEvaluation. The maximum generation length for inference is set at 2048. The test set includes in-domain subsets such as GSM8k and MATH500, as well as out-domain subsets like Gaokao2023 (Liao et al., 2024a), OCW Course (OCW) (Lewkowycz et al., 2022), and the OlympiadBench (He et al., 2024) test subset OE-TO-MATH-COMP. The testing methods comprise: 1) Greedy-CoT: Test results based on greedy decoding and CoT prompt pass@1. 2) MAJ@N: Repeat inference N times based on the CoT prompt, and select the most frequently occurring answer as the final answer. 3) ORM_VOTE@N: Repeat inference N times based on the CoT prompt, use Skywork-01-Open-PRM-Qwen-2.5-7B as the ORM for scoring, aggregate scores for identical answers, and choose the answer with the highest score. 4) ORM_MAX@N: Omit the step of aggregating scores for identical answers in ORM_VOTE@N and directly select"}, {"title": "the answer with the highest score.", "content": "More evaluation methods refer to Appendix C.\nImplementation. During the data generation phase, we perform tree sampling for each question with a count of K = 64, and each node branches into C = 2. When selecting step-level preference pairs, to mitigate the impact of PRM scoring noise, we only use action preference pairs with a scoring difference exceeding 0.5 for training (PRM scores range between 0 and 1). In the SFT phase, we use the Adam optimizer with a learning rate of 5e-6, while in the DPO and step-DPO phases, we employ the SGD optimizer with a learning rate of 1e-5, both utilizing the cosine method for learning rate decay. The $\\beta$ for both DPO and step DPO is set to 0.1. The $\\gamma$ for step DPO is chosen from {0.1,0.5,1.0,2.0,5.0}. All experiments are conducted on 8 Nvidia 80GB H800 GPUs.\n6.2 Main Result\nCompared to the base model: Our approach achieves significant improvements without utilizing any stronger model's responses for distillation shown in Table 1. Specifically, using SFT-PRM, we observe enhancements of 4.4% and 5.8% on the in-domain evaluation datasets MATH and GSM8k, respectively. With DPO-PRM, the improvements are 3.8% and 1.2%, respectively, on these same datasets. Building on this foundation, we further enhances the model's reasoning capabilities using SPPD, achieving additional improvements of 2.8% and 0.5% on the two evaluation datasets. The gains from SPPD stem from leveraging PRM signals, transitioning from coarse-grained optimization at the sentence level to fine-grained dynamic optimization at the step level. Additionally, during the inference phase, increasing computational load and employing the ORM_VOTE aggregation strategy further demonstrates the model's peak reasoning capabilities, achieving accuracies of 79% and 94.7% on MATH and GSM8k, respectively, outperforming current models of similar size.\nContinued gains in the second stage: In the first stage, the training data generated by the base model has been fully utilized. Following the principles of offline RL, we update the policy model's sampling trajectories, using the best model trained in the first stage as our new policy model to repeat our training process. This resulted in the SPPD-Stage2 model. Compared to SPPD, SPPD-Stage2 achieves further improvements of 1.2% and 0.5% on MATH and GSM8k, respectively. These results highlight"}, {"title": "the effectiveness of updating the policy model and demonstrate the robustness of the SPPD.", "content": "6.3 Ablation Study\nDifferent Base Model. We evaluate the effectiveness of the SPPD method on different base models, specifically Llama3.1-8B-Instruct and Qwen2.5-7B-Instruct. Given that Instruct models undergo sufficient optimization at the sentence level, we do not perform PRM-SFT and PRM-DPO training on these models. Instead, we directly utilize the trajectories from the Instruct models for dynamic value margin step DPO training. The results appear in Table 2. The findings indicate that on the Llama3.1-8B-Instruct model, the SPPD method achieves improvements of 4.6% and 3.6% on the MATH and GSM8k evaluation datasets, respectively. On the Qwen2.5-7B-Instruct model, the SPPD method improves performance by 2.2% and 0.8%, respectively. These experimental results demonstrate that the SPPD method performs well across different base models, showcasing its robustness with re-"}, {"title": "spect to the choice of base model.", "content": "Generalization on Out-Domain Distributions. To evaluate the generalization capabilities of SPPD on out-domain distributions, we select three out-domain evaluation datasets: GaoKao2023, OCW and OlympaidBench (using only the OlympaidBench-OE-TO-MATH-COMP portion). The results are presented in Table 3. The experiments show that using Qwen2.5-7B-Base as the base model, after applying SPPD, there are steady improvements across all three out-of-domain evaluation datasets. Specifically, improvements over the base model stand at 8.8%, 13.7%, and 5.6%, respectively. Over PRM-DPO, the improvements reach 1.8%, 4.8%, and 2.4%, respectively. Furthermore, the reasoning capabilities see further enhancement through the ORM_VOTE aggregation strategy.\nEffectiveness of Dynamic Value Margin. In Section 4.1, we model the dynamic value margin variation using MDP approach, deriving a step DPO method with dynamically changing margins from a mathematical perspective. To validate the effective-"}, {"title": "Impact of $\\gamma$.", "content": "To investigate the impact of the hyperparameter $\\gamma$ on the SPPD method as described in Formula 5, we selecte three base models: Qwen2.5-7B-Base, Llama3.1-8B-Instruct, and Qwen2.5-7B-Instruct. We adjust $\\gamma$ within the set {0.1, 0.5, 1.0, 2.0, 5.0} and evaluated the performance of these models on the MATH and GSM8k datasets. The results are presented in Figure 2. Our experimental findings indicate that selecting an appropriate $\\gamma$ is beneficial for the training of SPPD. It is observed that both excessively large and small values of $\\gamma$ are detrimental to the training of dy-"}, {"title": "7 Conclusion", "content": "In this work, we propose SPPD, a self-training with process preference learning using dynamic value margin. SPPD utilizes the Bellman optimality equation and the online RL objective modeled with MDP and designs a step-level tree self-sampling scheme without any distillation. Moreover, we propose a SFT and DPO scheme using PRM for"}, {"title": "Limitations", "content": "Several limitations remain in our current work. Firstly, our work relies on the effectiveness of PRM, and studies have shown that PRM's performance varies across different policy models and task scenarios; some PRMs may fail under specific tasks (Zheng et al., 2024). This work neglects the updates of PRM. As policy is continuously iterated, PRM faces the risk of becoming ineffective. Additionally, both PPO and GRPO are modeled based on bandit, and how to integrate MDP modeling with on-policy methods remains an important subject for future research."}, {"title": "can be calculate as follow:", "content": "$r(s_t, a_t) = \\beta \\log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)} + V^*(s_{t+1}) \u2013 V^*(s_t)$.\nProof of Lemma (4.1) is shown in Appendix D.1. Equation (3) demonstrates that the immediate reward in the MDP consists of the model's implicit reward and the value gain of the optimal value function. Assuming we have the following step-level preference pairs ($s_t, a_{t+1}^+, a_{t+1}^-$), based on the step-level BT-model, we have the optimal preference distribution:\n$p^*(a_{t+1}^+ > a_{t+1}^-) = \\sigma (r(s_t, a_{t+1}^+) \u2013 r(s_t, a_{t+1}^-))$.\nHere, $\\sigma(x) = 1/(1 + e^{-x})$ is the sigmoid function. Finally, we give the step DPO loss using dynamic value margin.\nTheorem 4.2 (Step DPO Loss Using Dynamic Value Margin.). If we aim to minimize the Kullback-Leibler(KL) divergence between the step-level preference distribution $P_{data}$ in $D_{step}$ and the model's current preference distribution $p_\\theta$ under the sampling of $\\tau_{ref}$, we can obtain the following loss function:\n$L_{step-dpo} = -\\mathbb{E}_{a_{t+1}^+,a_{t+1}^- \\sim \\tau_{ref}(\\cdot|s_t)} [\\log \\sigma(\\beta h_\\theta(a_{t+1}^+, a_{t+1}^-)\n\u2013 (V^*(s_{t+1}^+) \u2013 V^*(s_{t+1}^-)))]$,\nwhere $h_\\theta(a_{t+1}^+,a_{t+1}^-)\n= \\log \\frac{\\pi_\\theta(a_{t+1}^+|s_t)}{\\pi_{ref}(a_{t+1}^+|s_t)}\n\u2013 \\log \\frac{\\pi_\\theta(a_{t+1}^-|s_t)}{\\pi_{ref}(a_{t+1}^-|s_t)}$.\nThe proof is shown in Appendix D.2. In traditional step DPO (Lai et al., 2024), the value function prediction at each step is defined as 0. However, we argue that the value gain in the immediate reward (Equation (3)), or equivalently, the term $V^*(s_{t+1}) \u2013 V^*(s_{t+1})$ in Equation (4), considers the difference in the optimal value function predictions for the preferred states. This manifests in the step DPO loss as a dynamic value margin that varies depending on the preferred states $s_{t+1}^+$ and $s_{t+1}^-$, rather than treating all states uniformly. In practice, we use a PRM score to approximate the optimal value function. In Section 5, we will provide more profound theoretical insights and conclusions."}, {"title": "Reward Equivalence.", "content": "To make the optimization process more controllable, we revise Equation (3) by introducing the concept of reward equivalence.\nLemma 4.3. Reward Equivalence (Rafailov et al., 2024a)] Two reward functions r and r' are equivalent if and only if there exists a potential function $\\Phi : S \\rightarrow \\mathbb{R}$ that satisfies the following equation:\nr(st, at) = r'(st, at) + $\\Phi(f(st, at)) \u2013 $\\Phi(st).\nIn Equation (3), the potential function is our optimal value function, i.e., $\\Phi(s) = V^*(s)$. At the same time, it is easy to see that when we scale this potential function, $\\Phi'(s) = \\gamma$\\Phi(s), $\\Phi'$ still satisfies the definition of potential function. Therefore, we can modify Equation (3) to obtain an equivalent reward expression:\nr'(st, at) = r(st, at) + $\\gamma$\\Phi(f(st, at)) \u2013 $\\gamma$\\Phi(st).\nRepeating the derivation in Section 4.1, we modify the final loss as follows:\n$L_{step-dpo}\n= -\\mathbb{E}_{a_{t+1}^+,a_{t+1}^- \\sim \\tau_{ref} (s_t)}[\\log \\sigma(\\beta h_\\theta(a_{t+1}^+, a_{t+1}^-)\n\u2013 (V^* (s_{t+1}^+) \u2013 V^*(s_{t+1}^-)))]$.\nRemark. Although the concept of reward equivalence in Rafailov et al. (2024a) implies that the optimal preference model belongs to the same equivalence class, including the original step-DPO when $\\gamma$ = 0, the introduction of $\\gamma$ makes the optimization process more controllable due to its influence on optimization. This has been verified in Section 6.3."}, {"title": "4.2 Tree-Based Self-Sampling on LLMs", "content": "Traditional reasoning algorithms (token-level decoding) is almost impossible to guarantee the generation of reasoning trajectories with identical prefixes. To address this issue, this paper adopts a tree-structured reasoning approach, as illustrated in Figure 1. Specifically, the process is divided into four steps: \"Selection, Expansion, Collection and Scoring\". During the selection process, at the current state $s_t$, we record the average log probability score for each child node $a_t$, defined as:\ns(a_t | s_t) = $\\frac{1}{|a_t|} \\sum_{i=0}^{|a_t|-1} \\log infer(a_{t,i}|s_t|a_{t,<i})$,\nwhere $|a_t|$ represents the token length of the current step, $a_{t,<i}$ denotes the first $i \u2013 1$ tokens of"}, {"title": "5 Theoretical Analysis", "content": "In this Section", "distribution": "n$\\pi_{\\theta}(a_{t+1}|s_t) = \\sigma(r_{\\theta,t}^+ - r_{\\theta,t}^-)$,$\nwhere,\nr_{\\theta,t}^+ = \\beta \\log \\frac{\\pi_{\\theta}(a_{t+1}^+|s_t)}{\\pi_{ref}(a_{t+1}^+|s_t)} - V^*(s_{t+1}^+) + V^*(s_t),\nr_{\\theta,t}^- ="}]}