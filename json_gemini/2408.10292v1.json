{"title": "Leveraging Superfluous Information in Contrastive Representation Learning", "authors": ["Xuechu Yu", "Fangzhou Lin", "Yun Yue", "Ziming Zhang"], "abstract": "Contrastive representation learning, which aims to learn\nthe shared information between different views of unla-\nbelled data by maximizing the mutual information between\nthem, has shown its powerful competence in self-supervised\nlearning for downstream tasks. However, recent works have\ndemonstrated that more estimated mutual information does\nnot guarantee better performance in different downstream\ntasks. Such works inspire us to conjecture that the learned\nrepresentations not only maintain task-relevant information\nfrom unlabeled data but also carry task-irrelevant informa-\ntion which is superfluous for downstream tasks, thus leading\nto performance degeneration. In this paper we show that\nsuperfluous information does exist during the conventional\ncontrastive learning framework, and further design a new\nobjective, namely SuperInfo, to learn robust representations\nby a linear combination of both predictive and superfluous\ninformation. Besides, we notice that it is feasible to tune the\ncoefficients of introduced losses to discard task-irrelevant\ninformation, while keeping partial non-shared task-relevant\ninformation according to our SuperInfo loss. We demon-\nstrate that learning with our loss can often outperform the\ntraditional contrastive learning approaches on image clas-\nsification, object detection and instance segmentation tasks\nwith significant improvements.", "sections": [{"title": "1. Introduction", "content": "Due to the huge cost in acquiring data notations, unsu-\npervised learning has enjoyed its renaissance recently. Con-\ntrastive learning, whose goal is to learn powerful presenta-\ntions for downstream tasks, has achieved promising success\n[56, 11, 36, 49, 21, 19, 17, 41]. Since there is no label in-\nformation, contrastive learning usually estimates the mutual\ninformation between the learned representations of differ-\nent views as its objective function such as SimCLR [5], and\ntakes the learned model as a features extractor for various\ndownstream tasks, such as image classification, object de-\ntection and instance segmentation."}, {"title": "2. Related Work", "content": "Contrastive representation learning, one of the several\nself-supervised learning approaches, has significantly out-\nperformed other approaches from recent years [44, 68, 28,\n40, 13, 38, 12, 18, 66, 63, 6, 64]. With the convenience of\nobtaining a mass of unlabeled data, different multi-views of\nunlabeled data are constructed to design the specific con-\ntrastive loss to obtain a powerful learned representation,\nsuch as multiple augmentations of one image [2, 61, 46, 69,\n71], different patches of one image [24, 26, 20, 18, 67, 31],\ntext and its context[29, 34, 60, 12, 65, 47], different time\nslots of the same videos [70, 43, 35, 48, 4, 59], which pull\nsimilar data pairs close while push dissimilar pairs apart.\nThe intuition based on the contrastive idea is how to\nchoose similar and dissimilar pairs, one feasible methodol-\nogy is to maximize the shared information of different views\n(mutual information). Prior works [39, 5] have shown ap-\npealing performance in multiple downstream tasks accord-\ning to this intuition. However, a few researchers draw some\nconclusions against intuition. The work by [53] argues that\nmaximizing the tighter bound of the mutual information be-\ntween different variables may lead to worse representations,\nthe success of these promising results should be also at-\ntributed to the parametrization of the employed mutual in-\nformation estimators and the choice of encoder extractor ar-\nchitectures. Therefore, they design several experiments to\nverify their hypothesis.\nThe work [50] demonstrates that the optimal view for\ncontrastive representation learning is related to the given\ndownstream tasks, meaning no need to maximize the mutual\ninformation of different views. Their InfoMin rule aims to\nfigure out particular data augmentations to reduce the mu-\ntual information appropriately but does not find what com-\nponent results in their hypothesis and does not give a gen-\neral objective function, while our method considers stan-\ndard augmentations (e.g., cropping, rotation, and coloriza-\ntion), theoretically analyzes the task-irrelevant information\nbetween different augmentations and designs a new objec-\ntive function to eliminate this part. On the other hand, [52]\nreveals that contrastive representation learning is able to ex-\ntract task-relevant information and discard task-irrelevant\ninformation with a fixed gap and quantifies the amount of\ninformation that cannot be discarded. They create a new\ncomposite self-supervised learning objective based on their\nanalysis, but their introduced Inverse Predictive Learning\nseems slightly not related to their analysis logic.\nWhat's more, [16] applies information bottleneck [51] to\nthe multi-view learning, also aims to discard task-irrelevant\ninformation from different views during their framework,\nbut their method is not implemented in a precise way and\nnot tested on frequently-used datasets, our work presents a\ntotally different and flexible objective function, and is val-\nidated on popular datasets, such as CIFAR10 [30], STL-10"}, {"title": "3. Method", "content": "Let us turn to supervised representation learning, the ob-\njective of supervised representation learning is to find a\ngood representation z after encoding the input data x, and\nthen use the representation z for various downstream tasks,\nsuch as classification, regression. Since the label y can be\nobtained in supervised representation learning, the training\nmetric is usually built up with the representation z and the\nlabel y. What's more, to make the representation more gen-\neral and more robust, [1] applies the Information Bottleneck\ntheory [51] to establish a new objective function, the pur-\npose is to make the representation z more sufficient for the\nlabel y. We discuss the concept of sufficiency of supervised\nrepresentation learning by the following definition."}, {"title": "3.1. Motivation", "content": "Definition 1. Sufficiency in supervised representation\nlearning: A representation z of the input data x is suffi-\ncient for the label y if and only if $I (x;y|z) = 0$ (Where $I(\u00b7)$\nrepresents the mutual information between variables).\nAccording to Definition 1, we know that the learned suf-\nficient representation z contains all the information related\nto the label y after the model properly encodes the origi-\nnal input data x, and it may be well-performed for different\nevaluation tasks. Since the input data x usually has high-\nlevel semantic information compared to the label y, there\ncertainly exists some information in x which is irrelevant\nfor y, we can regard these as task-irrelevant (superfluous)\ninformation. By decomposing $I(x; z)$ into two terms us-\ning the chain rule of mutual information (proof in Appendix\nB.1).\n$I(x; z) = I (y; z) + I(x; zy)$ (1)\nThe conditional mutual information $I(x; zy)$ expresses\nthe mutual information between x and z, which is task-\nirrelevant for y, so this is superfluous. It is better to make\nthis term as small as possible. While the other term $I(y; z)$\nrepresents how much task-relevant information contained in"}, {"title": "3.2. SuperInfo Loss Function", "content": "the representation, which we want to maximize. Obviously\nreducing the amount of superfluous information can be done\ndirectly in supervised learning. As a consequence, [1] com-\nbines two terms $I(x; z)$ and $I(y; z)$ to make the model learn\na more sufficient representation.\nSince there is label information in the supervised setting,\nwe can easily analyze superfluous information and useful\ninformation. As for self-supervised representation learning,\nthere are only different augmentation views from the un-\nlabeled data, the only useful information to be leveraged\nis the shared information between different views. Con-\nsider $v_1$ and $v_2$ as two different views of data x and let y\nbe its label. Similarly $z_1$ and $z_2$ become representations of\ntwo different views $v_1$ and $v_2$ after processed by the net-\nwork. Therefore, the main objective is to obtain as much\nshared information of two views as possible, usually max-\nimizing the mutual information of two representations $Z_1$\nand $z_2$ ($I(Z1; Z2)$) is what we pay attention to. Neverthe-\nless, like supervised representation learning, there must be\nsome task-irrelevant (superfluous) information contained in\nthe learned representation. Consequently we want to extract\ntask-relevant and discard task-irrelevant information simul-\ntaneously. To formalize this we define sufficiency for self-\nsupervised representation learning.\nDefinition 2. Sufficiency in self-supervised representation\nlearning: A representation $z_1$ is sufficient of $v_1$ for $v_2$ if and\nonly if $I (Z1; 2) = I(v1; V2)$.\nIntuitively, $z_1$ is sufficient if the amount of information\nin $v_1$ about $v_2$ is unchanged by the encoding procedure.\nSymmetrically, $z_2$ is sufficient of $v_2$ for $v_1$ if and only if\n$I(V1; Z2) = I(V1; V2).\nDefinition 3. (Minimal sufficiency in self-supervised repre-\nsentation Learning) The sufficient representation $z_{min}$ of $v_1$\nis minimal if and only if $I (zmin, v\u2081) \u2264 I(z1, V1), \u2200z\u2081$ that is\nsufficient.\nFrom the above definition, we can see that a sufficient\nrepresentation contains exactly all the shared information\nbetween $v_1$ and $v_2$. Therefore, maintaining the representa-\ntions sufficient and discarding superfluous information be-\ntween two views and their representations simultaneously\nis particularly significant. We can show the following equa-\ntion by factorizing the mutual information between $v_1$ and\n$z_1$ into two terms, (similarly for $v_2$ and $z_2$):\n$I(V1; Z1) = I(V2; Z1) + I(V1; Z1 V2)$ (2)\nSimilar to Equation 1, $I(v1; z\u2081)$ can be decomposed into\npredictive information component and superfluous informa-\ntion component. Since $I(v2; z1)$ expresses the information"}, {"title": "3.3. Bayes Error Rate of Contrastive Learning Representations", "content": "between one representation and the other view, it makes a\ncontribution to the task-relevant information between two\nviews. On the other hand, $I(V1; Z1 V2)$ means the infor-\nmation contained between $v_1$ and $z_1$ while the view $V_2$ has\nbeen observed, as shown in Figure 2. The larger this term,\nthe more non-shared information between two views, so re-\nducing (or minimizing) $I(v1; Z1|v2)$ can make the learned\nrepresentation more sufficient. The proof of Equation 2 can\nbe found in appendix B.2.\nSince contrastive representation learning tries to pull\nsimilar data pairs close while push dissimilar pairs apart,\nit maximizes the mutual information between two learned\nrepresentations $z_1$ and $z_2$. Based on the analysis above, it\ncan be concluded that reducing the superfluous information\nmay help to learn a more sufficient representation for var-\nious downstream tasks, so we can maximize the following\nobjective function.\n$J = I(Z1; Z2) \u2013 \u03bb\u03b1I(V1; Z1|V2) \u2013 AI(V2; Z2V1)$ (3)\nWhere J represents the objective function, $I(v1; Z1 V2)$,\n$I(V2; Z2 V1)$ are the superfluous information of two views\nanalyzed above, \u03bb\u2081(i = a,b) are two Lagrangian parame-\nters that we can tune manually.\nAccording to the analysis in section 3.1, $I(v1; Z1 V2) =$\n$I(V1; Z1)-I(V2; Z1)$, $I(V2; Z2 V1) = I(V2; Z2)-I(V1; Z2)$,\nand since two augmentation views are symmetric to each\nother, we set up \u03bb\u2081 = \u03bb2, \u03bb3 = \u03bb4 to make the objective\nfunction more general.\n$J =I(Z1; Z2) \u2212 X1I(V1; Z1) \u2013 21 (V2; Z2) + X3I(V1; \u03962) + 4I(V2; Z1)$ (4)\nWhat's more, since \u03bb\u2081(i = 1,2) and dj (i = 3, 4) can be\nset up differently based on Equation 4, we can adjust these"}, {"title": "4. Experiments", "content": "coefficients to discard superfluous information, while keep-\ning partial non-shared task-relevant information according\nto different tasks (as shown in Figure 2, non-shared task-\nrelevant information: $I(v1;y|v2)$ and $I(v2;y|v1)$), this\nhighlights another advantage of our objective function.\nWe want to maximize the objective function J, but it\nis intractable to deal with mutual information expressions,\ntherefore, we have to maximize the lower bound of J for\nsecond best. We first consider the term $I(vi; zi)(i = 1, 2)$,\n$I(Vi; Zi) = JJ dvidzip(vi, zi) log p(zi vi) /P(Zi)$\n$= JJ dvidzip(vi, zi) log p(zi vi) /r(zi)$\n$-J dzip(zi) log p(zi) /r(zi)$ (5)\nIn general, computing the marginal distribution of zi\nmight be difficult. Maker(zi) is a variational approxima-\ntion to this marginal, since $KL[p(zi), r(zi)] \u2265 0$, we can\nget the following upper bound of I(vi; zi), in Equation 6,\nwe further assume the encoder process follows the Gaussian\ndistribution, $p(zi|vi) = N(zi; fi(vi)), I)$ and the varia-\ntional approximation r(zi) = N(0, I), so we can handle the\nKL divergence terms(full proof in Appendix C.1).\n$I(Vi; Zi) \u2264 // dvidzip(vi, zi) log P(ZiVi) /r(Zi)$ (6)\nOn the other hand, we need the lower bound of the pos-\nitive terms I(v1; z2) and I(v2; Z1), take I(v1; z2) as the ex-\nample. Using the relationship between mutual information\nexpression and entropy expression $I(v1;z2) = H(v1) - H(V1 z2)$,\nwhere H(v1) is a constant given the augmen-\ntation view, so we only need to maximize $-H(V1|Z2) =$\n$Ep(v1,z2) [log p(V1|Z2)]$. Assuming q(v1|22) is the varia-\ntional approximation to p(v1|22) in order to deal with the\nintractability of this conditional distribution, we have the"}, {"title": "4.1. Verifying the Role of Superfluous Information", "content": "lower bound of I(v1;z2), in Equation 7. Further we sup-\npose $q(v1|z2) = N(v1; h1(Z2)), \u03c33I)$, where h\u2081 maps z2) to v1 which we can use an compact deConvNet for real-\nization, thus, we can estimate $Ep(v1,z2) [log q(V1|Z2)]$. The\ncomplete proof can be found in Appendix C.2 (similar to\nI(V2; Z1)).\n$I(V1; Z2) \u2265 Ep(v1,22) [log p(V1|Z2)]$ (7)\nTo sum up, we are able to maximize the lower bound\nof the objective function, so the loss function L is listed\nin Equation 8, where I(z1; z2) can be estimated by MINE\nestimator [3], JS divergence estimator [24], InfoNCE loss\n[39]. We name our loss \u201cSuperInfo\u201d loss, the algorithm is\non the right.\n$L = \u2212 I (Z1; Z2) + \u03a3\u03bbiKL[p(ZiVi), r(Zi)] - A3Ep(v1,22) [log q(V1|Z2)] \u2013 14Ep(v2,21) [log q(V2|Z1)]$ (8)\nIn this section, we verify our new SuperInfo loss through\nseveral experiments. Based on our experimental results, we\nalso provide specific analysis.\nWe apply the SuperInfo loss to classical contrastive rep-\nresentation learning framework, and pre-train the model on\nCIFAR10 [30], STL-10 [8], and ImageNet [10], the learned\nrepresentation is used for different downstream tasks: clas-\nsification, detection and segmentation. We choose previous\nwork as the baselines: CMC [49], SimCLR [5], BYOL [19],\nMIB [16], Composite SSL [52], InfoCL [55] (There are sev-\neral baselines that are not tested on the three datasets, we try\nour best to get the results).\nData augmentations. We use the similar set of image aug-\nmentations as in SimCLR [5]. For CIFAR10 [30] and STL-\n10 [8], random cropping, flip and random color distortion\nare applied, and for ImageNet [10], a random patch of the\nimage is selected and resized to 224 x 224 with a random\nhorizontal flip, followed by a color distortion, consisting of\na random sequence of brightness, contrast, saturation, hue\nadjustments, and an optional grayscale conversion. Finally\nGaussian blur and solarization are applied to the patches.\nArchitecture. We train ResNet-18 [23] for CIFAR10 [30],"}, {"title": "4.2. Ablation Study", "content": "STL-10 [8] whose output is a 512-dim vector, then we ap-\nply an MLP to get a 128-dim vector that can be used for\nI (Z1; z2) estimation. For ImageNet [10], we use ResNet-50\n[23] whose output is a 2048-dim vector, then we apply an\nMLP to get the projector. The output of the ResNet is used\nas the representation for downstream tasks.\nPretrain. We apply the Adam optimizer [27] with the learn-\ning rate 3e-4 to train the ResNet-18 [23] backbone on CI-\nFAR10 [30] and STL-10 [8] with batch size 256 for 200\nepochs, we set $X\u2081 = 2 = 0.01, X3 = 4 = 0.1$. For\nImageNet [10], we use the LARS optimizer [62] to train\nthe ResNet-50 [23] backbone with batch size 1024 for 200\nepochs, we set the base learning rate to 0.3, scaled linearly\nwith the batch size (LearningRate = 0.3 \u00d7 BatchSize/256).\nIn addition, we use a global weight decay parameter of 1.5\n\u00d7 10-6 while excluding the biases and batch normalization\nparameters from both LARS adaptation and weight decay,\nwe set $X1 = 2 = 0.01, X3 = 84 = 0.1$. While estimating\nthe term I (Z1; Z2), we choose InfoNce method.\nEvaluation. We first evaluate the learned representation\nfrom CIFAR10 [30], STL-10 [8], ImageNet [10] by train-\ning a linear classifier on top of the frozen backbone, fol-\nAnalyzing the role of the added loss terms. We introduce\ntwo new losses into the classical contrastive representation\nlearning loss to make the learned representation more ro-"}, {"title": "5. Conclusion and Discussion", "content": "bust and sufficient. Further by analyzing the information\nflow in the framework (Figure 2), we can adjust the coef-\nficients i to discard superfluous information, while keep-\ning partial non-shared task-relevant information according\nto different tasks since the term ($I(v1; Z2), I(V2; Z1)$) in-\ntroduces certain information from different views which\nmay make contribution to some downstream tasks. There-\nfore, we conduct the classification experiments (includ-\ning downstream classification) of only adding the terms,\n$I(V1; Z1), I(V2; Z2)$ (\u03bb3 = \u03bb4 = 0) or only adding another\nterms, $I(v1; z2), I(V2; Z1)$ (\u03bb1 = X2 = 0) to see whether\nthere are apparent different performances with the original\nSuperInfo (1 = \u03bb2 = 0.01, X3 = 84 = 0.1). Following\nthe same setting in section 4.1, we just train the model on\nCIFAR10 and STL-10 by changing \u5165\u2081 and apply linear eval-\nuation protocol to other classification datasets. We report\nthe results in Table 3. It can be clearly seen that the accuracy\non the source dataset (CIFAR10 and STL-10) can achieve\nthe similar level compared to original SuperInfo while only\nadding $I(V1;21)$ and $I(v2; Z2)$ since this change can dis-\ncard superfluous information, but the downstream classifi-\ncation performance gets worse to a certain extent because\nseveral non-shared task-relevant information does not keep.\nOn the other hand, only adding $I(v1; z2)$ and $I(v2; Z1)$ is\nbetter than original SuperInfo on the source dataset (CI-\nFAR10 and STL-10), but can not beat original SuperInfo on\ntransfer datasets, which means only introducing non-shared\ntask-relevant information may bring certain noise, leading\nto the phenomenon of over-fitting on transfer datasets.\nTraining with more epochs. We train all models for 200\nepochs during all above experiments. Further we train our\nmodel for 100, 200, 300, 400, 500, 600 epochs to analyze\nSuperInfo's behavior under different training epochs, com-\npared to vanilla SimCLR. The results are listed in Figure\n3. According to the above figure, we find that the down-\nstream classification accuracy does not become better with\nmore training epochs, even decreases in the middle period\nsince the learned representations in contrastive representa-\ntion learning are able to get more close to the minimal suf-\nficient representation which only contains the shared infor-\nmation between different views with more training epochs\nand the minimal sufficient representation may have the risk\nof over-fitting on the transfer datasets. This phenomenon is\nconsistent with the conclusion in [55]. What's more, Su-\nperInfo does bring significant improvements compared to\nvanilla SimCLR on the transfer datasets under every train-\ning epochs. On the other hand, we change the coefficients\n(\u03bb\u2081 = 2 = 0.005, 13 = 4 = 0.5) compared to vanilla Su-\nperInfo, training the model on CIFAR10 and evaluating the\nmodel on other transfer datasets. The results are reported\nin Figure 4. As shown in 4(b), the classification accuracy\nincreases stably with the training epochs, however, the per-\nformance on the source dataset (CIFAR10) does not keep\nthe pace with it. This phenomenon shows there really ex-\nists a trade-off of the performance on source dataset and on\nother transfer datasets with respect to the coefficients.\nIn this work, we deeply analyze the reason why more es-\ntimated mutual information between two different views in\ncontrastive representation learning does not guarantee great\nperformance in various downstream tasks and design a new\nobjective function to discards task-irrelevant information,\nwhile keeping some non-shared task-relevant information.\nThe effectiveness of our method are verified by several ex-\nperiments.\nThere are a few limitations during our presentation. (1)\nIt is still troublesome to determine the coefficients \u03bb\u2081 of\nthe loss function since they apparently influence the perfor-\nmance, so far we have to tune them manually. (2) Due to our\nlimited computing resources, we can not compare our best\nresults to other methods under the condition of 4096 batch\nsize or larger, and more training epochs, however, a series of\nbetter outcomes indicate our method can make contribution\nto classical contrastive representation learning framework."}]}