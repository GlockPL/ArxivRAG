{"title": "Leveraging Superfluous Information in Contrastive Representation Learning", "authors": ["Xuechu Yu", "Fangzhou Lin", "Yun Yue", "Ziming Zhang"], "abstract": "Contrastive representation learning, which aims to learn the shared information between different views of unlabeled data by maximizing the mutual information between them, has shown its powerful competence in self-supervised learning for downstream tasks. However, recent works have demonstrated that more estimated mutual information does not guarantee better performance in different downstream tasks. Such works inspire us to conjecture that the learned representations not only maintain task-relevant information from unlabeled data but also carry task-irrelevant information which is superfluous for downstream tasks, thus leading to performance degeneration. In this paper we show that superfluous information does exist during the conventional contrastive learning framework, and further design a new objective, namely SuperInfo, to learn robust representations by a linear combination of both predictive and superfluous information. Besides, we notice that it is feasible to tune the coefficients of introduced losses to discard task-irrelevant information, while keeping partial non-shared task-relevant information according to our SuperInfo loss. We demonstrate that learning with our loss can often outperform the traditional contrastive learning approaches on image classification, object detection and instance segmentation tasks with significant improvements.", "sections": [{"title": "1. Introduction", "content": "Due to the huge cost in acquiring data notations, unsupervised learning has enjoyed its renaissance recently. Contrastive learning, whose goal is to learn powerful presentations for downstream tasks, has achieved promising success [56, 11, 36, 49, 21, 19, 17, 41]. Since there is no label information, contrastive learning usually estimates the mutual information between the learned representations of different views as its objective function such as SimCLR [5], and takes the learned model as a features extractor for various downstream tasks, such as image classification, object detection and instance segmentation."}, {"title": "2. Related Work", "content": "Contrastive representation learning, one of the several self-supervised learning approaches, has significantly outperformed other approaches from recent years [44, 68, 28, 40, 13, 38, 12, 18, 66, 63, 6, 64]. With the convenience of obtaining a mass of unlabeled data, different multi-views of unlabeled data are constructed to design the specific contrastive loss to obtain a powerful learned representation, such as multiple augmentations of one image [2, 61, 46, 69, 71], different patches of one image [24, 26, 20, 18, 67, 31], text and its context[29, 34, 60, 12, 65, 47], different time slots of the same videos [70, 43, 35, 48, 4, 59], which pull similar data pairs close while push dissimilar pairs apart.\nThe intuition based on the contrastive idea is how to choose similar and dissimilar pairs, one feasible methodology is to maximize the shared information of different views (mutual information). Prior works [39, 5] have shown appealing performance in multiple downstream tasks according to this intuition. However, a few researchers draw some conclusions against intuition. The work by [53] argues that maximizing the tighter bound of the mutual information between different variables may lead to worse representations, the success of these promising results should be also attributed to the parametrization of the employed mutual information estimators and the choice of encoder extractor architectures. Therefore, they design several experiments to verify their hypothesis.\nThe work [50] demonstrates that the optimal view for contrastive representation learning is related to the given downstream tasks, meaning no need to maximize the mutual information of different views. Their InfoMin rule aims to figure out particular data augmentations to reduce the mutual information appropriately but does not find what component results in their hypothesis and does not give a general objective function, while our method considers standard augmentations (e.g., cropping, rotation, and colorization), theoretically analyzes the task-irrelevant information between different augmentations and designs a new objective function to eliminate this part. On the other hand, [52] reveals that contrastive representation learning is able to extract task-relevant information and discard task-irrelevant information with a fixed gap and quantifies the amount of information that cannot be discarded. They create a new composite self-supervised learning objective based on their analysis, but their introduced Inverse Predictive Learning seems slightly not related to their analysis logic.\nWhat's more, [16] applies information bottleneck [51] to the multi-view learning, also aims to discard task-irrelevant information from different views during their framework, but their method is not implemented in a precise way and not tested on frequently-used datasets, our work presents a totally different and flexible objective function, and is validated on popular datasets, such as CIFAR10 [30], STL-10"}, {"title": "3. Method", "content": "Let us turn to supervised representation learning, the objective of supervised representation learning is to find a good representation z after encoding the input data x, and then use the representation z for various downstream tasks, such as classification, regression. Since the label y can be obtained in supervised representation learning, the training metric is usually built up with the representation z and the label y. What's more, to make the representation more general and more robust, [1] applies the Information Bottleneck theory [51] to establish a new objective function, the purpose is to make the representation z more sufficient for the label y. We discuss the concept of sufficiency of supervised representation learning by the following definition.\nDefinition 1. Sufficiency in supervised representation learning: A representation z of the input data x is sufficient for the label y if and only if I(x;y|z) = 0 (Where I(\u00b7) represents the mutual information between variables).\nAccording to Definition 1, we know that the learned sufficient representation z contains all the information related to the label y after the model properly encodes the original input data x, and it may be well-performed for different evaluation tasks. Since the input data x usually has high-level semantic information compared to the label y, there certainly exists some information in x which is irrelevant for y, we can regard these as task-irrelevant (superfluous) information. By decomposing I(x; z) into two terms using the chain rule of mutual information (proof in Appendix B.1).\n$I(x; z) = I (y; z) + I(x; z|y)$ (1)\nThe conditional mutual information $I(x; z|y)$ expresses the mutual information between x and z, which is task-irrelevant for y, so this is superfluous. It is better to make this term as small as possible. While the other term $I(y; z)$ represents how much task-relevant information contained in"}, {"title": "3.1. Motivation", "content": "the representation, which we want to maximize. Obviously reducing the amount of superfluous information can be done directly in supervised learning. As a consequence, [1] combines two terms $I(x; z)$ and $I(y; z)$ to make the model learn a more sufficient representation.\nSince there is label information in the supervised setting, we can easily analyze superfluous information and useful information. As for self-supervised representation learning, there are only different augmentation views from the unlabeled data, the only useful information to be leveraged is the shared information between different views. Consider v1 and v2 as two different views of data x and let y be its label. Similarly z1 and z2 become representations of two different views v1 and v2 after processed by the network. Therefore, the main objective is to obtain as much shared information of two views as possible, usually maximizing the mutual information of two representations Z1 and z2 (I(Z1; Z2)) is what we pay attention to. Nevertheless, like supervised representation learning, there must be some task-irrelevant (superfluous) information contained in the learned representation. Consequently we want to extract task-relevant and discard task-irrelevant information simultaneously. To formalize this we define sufficiency for self-supervised representation learning.\nDefinition 2. Sufficiency in self-supervised representation learning: A representation z1 is sufficient of v1 for v2 if and only if I (Z1; z2) = I(v1; V2).\nIntuitively, z1 is sufficient if the amount of information in v1 about v2 is unchanged by the encoding procedure. Symmetrically, z2 is sufficient of v2 for v1 if and only if $I(v_1; z_2) = I(v_1; v_2)$.\nDefinition 3. (Minimal sufficiency in self-supervised representation Learning) The sufficient representation $z_{min}^1$ of v1 is minimal if and only if $I(z_{min}^1, v_1) \u2264 I(z_1, v_1), \u2200z_1$ that is sufficient.\nFrom the above definition, we can see that a sufficient representation contains exactly all the shared information between v1 and v2. Therefore, maintaining the representations sufficient and discarding superfluous information between two views and their representations simultaneously is particularly significant. We can show the following equation by factorizing the mutual information between v\u2081 and Z1 into two terms, (similarly for v2 and z2):\n$I(v_1; z_1) = I(v_2; z_1) + I(v_1; z_1| v_2)$ (2)\nSimilar to Equation 1, $I(v_1; z_1)$ can be decomposed into predictive information component and superfluous information component. Since $I(v_2; z_1)$ expresses the information between one representation and the other view, it makes a"}, {"title": "3.2. SuperInfo Loss Function", "content": "contribution to the task-relevant information between two views. On the other hand, $I(v_1; z_1| v_2)$ means the information contained between v\u2081 and z\u2081 while the view V2 has been observed. The larger this term, the more non-shared information between two views, so reducing (or minimizing) $I(v_1; z_1| v_2)$ can make the learned representation more sufficient. The proof of Equation 2 can be found in appendix B.2.\nSince contrastive representation learning tries to pull similar data pairs close while push dissimilar pairs apart, it maximizes the mutual information between two learned representations z1 and z2. Based on the analysis above, it can be concluded that reducing the superfluous information may help to learn a more sufficient representation for various downstream tasks, so we can maximize the following objective function.\n$J = I(z_1; z_2) \u2013 \u03bb\u03b1I(v_1; z_1| v_2) \u2013 AI(v_2; z_2| v_1)$ (3)\nWhere J represents the objective function, $I(v_1; z_1| v_2)$, $I(v_2; z_2| v_1)$ are the superfluous information of two views analyzed above, \u03bb\u2081(i = a,b) are two Lagrangian parameters that we can tune manually.\nAccording to the analysis in section 3.1, $I(v_1; z_1| v_2) = I(v_1; z_1)-I(v_2; z_1)$, $I(v_2; z_2| v_1) = I(v_2; z_2)-I(v_1; z_2)$, and since two augmentation views are symmetric to each other, we set up \u03bb\u2081 = \u03bb2, \u03bb3 = \u03bb4 to make the objective function more general.\n$J =I(z_1; z_2) \u2212 X1I(v_1; z_1) \u2013 \u03bb2I(v_2; z_2) + X3I(v_1; z_2) + \u03bb4I(v_2; z_1)$ (4)\nWhat's more, since \u03bb\u2081(i = 1,2) and dj (i = 3, 4) can be set up differently based on Equation 4, we can adjust these coefficients to discard superfluous information, while keeping partial non-shared task-relevant information according to different tasks (as shown in Figure 2, non-shared task-relevant information: $I(v_1;y|v_2)$ and $I(v_2;y|v_1)$), this highlights another advantage of our objective function.\nWe want to maximize the objective function J, but it is intractable to deal with mutual information expressions, therefore, we have to maximize the lower bound of J for second best. We first consider the term $I(v_i; z_i)(i = 1, 2)$,\n$I(v_i; z_i) = JJ dv_idzip(v_i, z_i) log p(z_i|v_i) = JJ dv_idzip(v_i, z_i) log p(z_i|v_i)p(z_i) = \u222b dzip(zi) log p(zi|vi) = \u222b\u222b dv_idzip(v_i, z_i) log p(z_i|v_i)r(z_i) = \u222b\u222b dv_idzip(v_i, z_i) log p(z_i|v_i)r(z_i) = \u222b\u222b dv_idzip(v_i, z_i) log p(z_i|v_i)r(z_i) \u2013 KL[p(zi), r(zi)]< \u222b\u222b dv_idzip(v_i, z_i) log p(z_i|v_i)r(z_i) = KL[p(zi vi), r(zi)]$ (5)\nIn general, computing the marginal distribution of zi might be difficult. Maker(zi) is a variational approximation to this marginal, since $KL[p(zi), r(zi)] \u2265 0$, we can get the following upper bound of I(vi; zi), in Equation 6, we further assume the encoder process follows the Gaussian distribution, p(zi|vi) = N(zi; fi(vi)), I) and the variational approximation r(zi) = N(0, I), so we can handle the KL divergence terms(full proof in Appendix C.1).\n$I(v_i; z_i) \u2264 // P(zi vi) dvidzip(vi, zi) log r(Zi)$ (6)\nOn the other hand, we need the lower bound of the positive terms $I(v_1; z_2)$ and $I(v_2; z_1)$, take $I(v_1; z_2)$ as the example. Using the relationship between mutual information expression and entropy expression $I(v_1; z_2) = H(v_1) \u2013 H(v_1| z_2)$, where H(v1) is a constant given the augmentation view, so we only need to maximize $-H(v_1| z_2) = Ep(v_1,z_2) [log p(v_1| z_2)]$. Assuming q(v1|22) is the variational approximation to p(v1|22) in order to deal with the intractability of this conditional distribution, we have the"}, {"title": "4. Experiments", "content": "In this section, we verify our new SuperInfo loss through several experiments. Based on our experimental results, we"}, {"title": "4.1. Verifying the Role of Superfluous Information", "content": "We apply the SuperInfo loss to classical contrastive representation learning framework, and pre-train the model on CIFAR10 [30], STL-10 [8], and ImageNet [10], the learned representation is used for different downstream tasks: classification, detection and segmentation. We choose previous work as the baselines: CMC [49], SimCLR [5], BYOL [19], MIB [16], Composite SSL [52], InfoCL [55] (There are several baselines that are not tested on the three datasets, we try our best to get the results).\nData augmentations. We use the similar set of image augmentations as in SimCLR [5]. For CIFAR10 [30] and STL-10 [8], random cropping, flip and random color distortion are applied, and for ImageNet [10], a random patch of the image is selected and resized to 224 x 224 with a random horizontal flip, followed by a color distortion, consisting of a random sequence of brightness, contrast, saturation, hue adjustments, and an optional grayscale conversion. Finally Gaussian blur and solarization are applied to the patches.\nArchitecture. We train ResNet-18 [23] for CIFAR10 [30],"}, {"title": "4.2. Ablation Study", "content": "Analyzing the role of the added loss terms. We introduce two new losses into the classical contrastive representation learning loss to make the learned representation more ro-"}, {"title": "5. Conclusion and Discussion", "content": "In this work, we deeply analyze the reason why more estimated mutual information between two different views in contrastive representation learning does not guarantee great performance in various downstream tasks and design a new objective function to discards task-irrelevant information, while keeping some non-shared task-relevant information.\nThe effectiveness of our method are verified by several experiments.\nThere are a few limitations during our presentation. (1) It is still troublesome to determine the coefficients \u03bb\u2081 of the loss function since they apparently influence the performance, so far we have to tune them manually. (2) Due to our limited computing resources, we can not compare our best results to other methods under the condition of 4096 batch size or larger, and more training epochs, however, a series of better outcomes indicate our method can make contribution to classical contrastive representation learning framework."}, {"title": "3.3. Bayes Error Rate of Contrastive Learning Representations", "content": "In this subsection, we apply Bayes error rate [15] to analyze the irreducible error of self-supervised contrastive learning representations. Suppose the downstream task is classification and T represents the categorical variable. It represents the smallest acceptable error when estimating the correct label given any learned representations. Basically, let Pe be the Bayes error rate of arbitrary self-supervised learning representations z\u2081 and T be the prediction for T from our classification model. According to [15], $P_e = 1 Ep(z_1) [maxter p(T = t|z_1)]$, so $0 < P_e \u2264 1 \u2212 1/|T|$ where T is the cardinality of T. We define a threshold function $F(x) = min{max{1,0},1 \u2013 1/|T|}$ for better analysis. [55] has proved the following theory (Full proof can be found in this paper).\nTheorem 1. [55] (Bayes Error Rate of Representations) For arbitrary self-supervised learning representation z1, its Bayes error rate $P_e = F(P_e)$ with\n$P_e \u22641-exp[-(H(T) \u2013 I(z_1, T| v_2) - I (z_1, v_2, T))]$ (9)\nThus, when the learned representation $z_1 zurf$ is sufficient, its Bayes error rate $P_e^{zurf} = F(P_e^{zurf})$ with\n$P_e^{zurf} <1- exp[-(H(T) \u2013 I(z_1^{urf}, T| v_2) \u2013 I(v_1, v_2, T))]$ (10)\nFurther for the minimal sufficient representation zmin, its Bayes error rate Pmin = F(Pmin) with\n$P_e^{min} < 1-exp[\u2212(H(T) \u2013 I(v_1, v_2, T))]$ (11)"}]}