{"title": "REFLEXIVE GUIDANCE: IMPROVING OODD IN\nVISION-LANGUAGE MODELS VIA SELF-GUIDED\nIMAGE-ADAPTIVE CONCEPT GENERATION", "authors": ["Seulbi Lee", "Jihyo Kim", "Sangheum Hwang"], "abstract": "With the recent emergence of foundation models trained on internet-scale data and demonstrating\nremarkable generalization capabilities, such foundation models have become more widely adopted,\nleading to an expanding range of application domains. Despite this rapid proliferation, the trustwor-\nthiness of foundation models remains underexplored. Specifically, the out-of-distribution detection\n(OoDD) capabilities of large vision-language models (LVLMs), such as GPT-40, which are trained\non massive multi-modal data, have not been sufficiently addressed. The disparity between their\ndemonstrated potential and practical reliability raises concerns regarding the safe and trustworthy\ndeployment of foundation models. To address this gap, we evaluate and analyze the OoDD capa-\nbilities of various proprietary and open-source LVLMs. Our investigation contributes to a better\nunderstanding of how these foundation models represent confidence scores through their generated\nnatural language responses. Based on our observations, we propose a self-guided prompting approach,\ntermed Reflexive Guidance (ReGuide), aimed at enhancing the OoDD capability of LVLMs by lever-\naging self-generated image-adaptive concept suggestions. Experimental results demonstrate that our\nReGuide enhances the performance of current LVLMs in both image classification and OoDD tasks.", "sections": [{"title": "Introduction", "content": "Thanks to substantial advancements in hardware and the availability of large-scale datasets far exceeding conventional\nones, foundation models have achieved remarkable performance across a wide range of tasks, demonstrating exceptional\ngeneralization. This evolution has shifted toward leveraging multiple data modalities, particularly vision and natural\nlanguage. As a result, vision-language foundation models have demonstrated their capabilities across diverse domains,\nfrom general tasks like text-to-image generation and vision-question answering, to specialized fields such as medical\ndiagnosis and robotics (Esser et al., 2024; Li et al., 2023; Wake et al., 2023; Majumdar et al., 2024).\nDespite the widespread adoption of these large vision-language models (LVLMs) including GPT, Claude, and Gem-\nini (OpenAI, 2024; Anthropic, 2024; Reid et al., 2024), their trustworthiness and reliability have not been adequately\ninvestigated. Ensuring the robustness of deep neural networks has been a key research area to guarantee their safe\napplication in practice. With the rapid popularization of LVLMs, practical concerns such as harmful content filtering and\ndomain generalization have been actively studied (Zhang et al., 2024b; Han et al., 2024). However, fundamental aspects,\nsuch as the quality of confidence estimates for model predictions\u2014extensively studied in single-modal models\u2014remain\nunderexplored. Some works have studied OoDD in CLIP (Ming et al., 2022; Jiang et al., 2024; Cao et al., 2024) and\nStable Diffusion (Zhu et al., 2024), but few have examined OoDD in LVLMs, particularly with respect to confidence\nscores expressed through their natural language responses. This gap between their demonstrated capabilities and\nreal-world reliability raises concerns about ensuring the safe and dependable deployment of LVLMs.\nTo bridge the gap, we first evaluate and compare the OoDD capabilities of LVLMs. Due to the lack of prior experimental\nconfigurations, we develop a framework for evaluating the OoDD capabilities of LVLMs. Our focus is on detecting"}, {"title": "Related work", "content": "In single-modal vision models, OoDD has been actively studied. Starting with the baseline method of using the\nmaximum softmax value as an OoD score (Hendrycks & Gimpel, 2017), methods have evolved to improve confidence\nmodeling (Moon et al., 2020; Liu et al., 2020; Bibas et al., 2021), post-hoc techniques (Liang et al., 2018; Lee et al.,\n2018b; Sun et al., 2021; Djurisic et al., 2023; Sun et al., 2022; Zhang et al., 2023; Liu & Qin, 2024), or a combination\nof both (Xu et al., 2024). Another approach involves leveraging auxiliary OoD samples to better distinguish between ID\nand OoD inputs. While real OoD samples show strong detection performance (Hendrycks et al., 2019; Chen et al., 2021;\nLu et al., 2023; Katz-Samuels et al., 2022; Koo et al., 2024), they require access to real OoD data. Synthetic samples\noffer an alternative, providing the benefits of auxiliary OoD data without the need for collecting real samples (Lee et al.,\n2018a; Du et al., 2022; Tao et al., 2023; Zheng et al., 2023)."}, {"title": "OoD Detection on Vision-Language Foundation Models", "content": ""}, {"title": "Problem Definition", "content": "OoD is conventionally defined as distributions outside the training distribution. However, given the vast amount\nand broad domain coverage of data used to train LVLMs, this conventional definition faces challenges in its direct\napplication to LVLMs. To address this, we extend the zero-shot OoDD framework of CLIP (Radford et al., 2021) to\ngenerative LVLMs.\nLet $\\mathcal{X}$ be the image space and $\\mathcal{Y} = \\{y_i\\}_{i=1}^{C}$ the set of class-representing words, where $C$ is the number of classes. The\nOoDD problem for CLIP in zero-shot image recognition is defined as the scenario where $\\mathcal{Y}$ does not contain the ground-\ntruth label of an input image $x \\in \\mathcal{X}$. Given $x$, CLIP yields a prediction for $x$ based on $sim(f_I(x), f_T(prompt(y_i)))$\nwhere $sim(u, v)$ is the cosine similarity, prompt is a text template designed to reflect $y_i$ (e.g., a photo of $y_i$), and $f_I, f_T$\nare the image and text encoders, respectively. The cosine similarities between $x$ and each $y_i \\in \\mathcal{Y}$ are used to determine\nwhether $x$ belongs to ID or OoD. If the ground-truth label of $x$ is not in $\\mathcal{Y}$, those similarities for all $y_i$ in $\\mathcal{Y}$ will be\nrelatively low, leading to the classification of $x$ as OoD.\nWe frame the OoDD problem for LVLMs based on the zero-shot OoDD scenario defined for CLIP. Similarly, $\\mathcal{Y}$ is set\nto a fixed word set containing only ID class words. Given both $x$ and $y$ as inputs, an LVLM is instructed to produce\nprediction results in a structured format. Then, class predictions and confidence estimates for $x$ are extracted from the\nnatural language responses by the LVLM. If the ground-truth label of $x$ is not in $\\mathcal{Y}$, the LVLM should provide low\nconfidence scores for all $y_i$ in $\\mathcal{Y}$ in its answers."}, {"title": "Prompt Design", "content": "Fig. 2 illustrates the framework for OoDD evaluation on LVLMs with a simplified prompt. Our prompt consists of\nfour components: a task description, an explanation of the rejection class, guidelines, and examples for the response\nformat. Unlike previous work on LVLMs (Han et al., 2024; Hwang et al., 2024; Groot & Valdenegro-Toro, 2024),\nwe encountered a significant number of failure cases when using a simple prompt consisting of the task statement\nincluding $\\mathcal{Y}$ and a formatted output structure. We attribute this inconsistency to differences in how confidence scores\nare assigned in our framework. In our prompt design, the LVLM is expected to provide confidence scores for each class\nin $\\mathcal{Y}$, whereas prior works require only a single confidence score for the predicted class. To mitigate these failures, we\nenhance the prompt by adding the following components, resulting in the final prompt used in our experiments. The\ncomplete prompt can be found in Appendix B.6.\nRejection class. When we provide only $\\mathcal{Y}$, predictions for OoD samples often do not correspond to any class in $\\mathcal{Y}$. Due\nto its strong zero-shot visual recognition capabilities, the LVLM can either provide the ground-truth label for an OoD"}, {"title": "OoD Score Design", "content": "Since we provide the rejection class for OoD inputs, the ideal behavior of LVLMs for confidence estimates is to assign\nhigh confidence scores to one of the classes in $\\mathcal{Y}$ for ID inputs, and to the rejection class (i.e., none of these classes) for\nOoD inputs. Therefore, we use the maximum confidence score among the classes in $\\mathcal{Y}$ (i.e., ID classes) as the OoD\nscore. Note that we do not constrain the sum of confidence scores. We apply the softmax function to all confidence\nvalues to normalize them, including that of the rejection class. Based on this OoD score design, an input image is likely\nto be ID if the score is high, and likely to be OoD if the score is low."}, {"title": "Experimental Settings", "content": "Comparison models. To compare LVLMs from diverse perspectives, we consider both proprietary and open-source\nmodels. For proprietary models, we employ three state-of-the-art (SOTA) models: GPT-40 (2024-08-06) (OpenAI,\n2024), Gemini Pro 1.5 (Reid et al., 2024), and Claude 3.5 Sonnet (Anthropic, 2024). For open-source models, we use\nfive models: LLaVA-v1.6-Mistral-7B (LLaVA-v1.6) (Li et al., 2024a), GLM-4v-9B (GLM-4v) (GLM et al., 2024),\nQWEN-VL-Chat (QWEN) (Bai et al., 2023), InternVL2-InternLM2-Chat-26B (InternVL2-26B), and InternVL2-\nLLaMA3-76B (InternVL2-76B) (Chen et al., 2024). Additionally, we include OpenCLIP (ViT-B-32 pretrained on\nLAION 2B-s34b-b79k) (Cherti et al., 2023) as a non-generative LVLM. To further facilitate comparison between single-\nand multi-modal models, we also include three single-modal state-of-the-art vision OoDD models, SCALE (Xu et al.,\n2024), fDBD (Liu & Qin, 2024), AugMix+ASH (Djurisic et al., 2023; Hendrycks et al., 2020).\nBenchmark datasets. We evaluate the comparison models on the CIFAR10 and ImageNet200 benchmarks proposed in\nOpenOOD v1.5 (Zhang et al., 2024a). We focus on the standard OoD setting in OpenOOD v1.5, which includes two\ntypes of datasets, near- and far-OoD, categorized based on the semantic distance between ID and OoD datasets. Since\nLVLMs are trained on high-resolution images, our main experiments are conducted on the ImageNet200 benchmark.\nWe also evaluate the models on the CIFAR10 benchmark to assess their scalability with respect to input image resolution.\nFor each benchmark, we consider the set of class names from the ID dataset as $\\mathcal{Y}$. Due to cost, time, and API rate\nlimits, we use 25% subsets of the benchmarks. A detailed explanation of the benchmark datasets can be found in\nAppendix B.1."}, {"title": "Results", "content": "Tab. 1 presents the OoDD capabilities of the compared models on the ImageNet200 benchmark. The near- and far-OoD\nresults for SCALE and fDBD represent the average performance across their respective categories. 'All OoD' refers to\nthe performance in distinguishing all OoD inputs, including near- and far-OoD inputs, from ID inputs (i.e., ID vs. all\nOoD). The results of QWEN-VL-Chat are omitted due to its exceptionally low ability to follow instructions, with a\nvalid response rate of less than 1%.\nOn both image recognition and OoDD tasks, proprietary models outperform open-source models in most cases, with\nreasonable valid response rates. All compared models have more difficulty in detecting near-OoD than far-OoD. The\noverall performance ranking of the evaluated LVLMs generally aligns with the OpenVLM leaderboard, except for\nGemini Pro 1.5. In our results, Gemini Pro 1.5 shows better ID accuracy and OoDD performance than Claude 3.5\nSonnet and InternVL-76B. Claude 3.5 Sonnet frequently generates invalid responses compared to other proprietary\nmodels and struggles to detect near-OoD, resulting in worse performance on NINCO and SSB-Hard compared to\nthe open-source model GLM-4v in terms of AUROC, despite achieving 16.58% higher ID accuracy. InternVL2-26B\nachieves the best ID accuracy, but has difficulty with OoDD and shows the lowest valid response rate. While there\nis no clear-cut relationship between image recognition and OoDD capabilities, models with better image recognition\nperformance generally exhibit stronger OoDD performance, consistent with the findings of Vaze et al. (2022).\nIt is worth noting that the proprietary models generally perform on par with or better than the single-modal SOTA\nOoDD models. In addition, GPT-40 and Gemini Pro 1.5 outperform OpenCLIP in both ID classification and OoDD,\nshowing significantly better performance in OoDD. For a more rigorous comparison, we evaluate OpenCLIP on the\nimages where GPT-40 generates valid responses, as GPT-40 shows a relatively lower valid response rate. On the\nGPT-40 valid query set, GPT-40 still yields better results on both tasks. This demonstrates its superior visual recognition\ncapability and its ability to express confidence scores through its generated responses. Detailed results can be found in\nAppendix B.2.\nAmong the open-source models, one notable observation is their FPR of 100%. Our analysis indicates that this is\ncaused by extremely biased confidence values, which will be further discussed in Sec. 3.6. GLM-4v exhibits the best\nOoDD performance, while InternVL2-76B shows the second-best zero-shot image recognition performance among the\nopen-source models. Both models also demonstrate a better understanding of instructions than GPT-40 and Claude 3.5\nSonnet. In overall, InternVL2-76B shows superior performance in all aspects, including valid response rate, ID accuracy,"}, {"title": "Further Analysis", "content": "Scalability with image resolution. We assess the input resolution scalability of LVLMs using the CIFAR10 benchmark.\nMost models demonstrate higher performance on the CIFAR10 benchmark than on the ImageNet200 benchmark, with\na higher ratio of valid responses. Although the smaller number of class candidates and coarser class contribute to\nbetter performance, it indicates that the LVLMs handle lower-resolution images effectively. As with the ImageNet200\nbenchmark, proprietary models consistently outperform open-source models across all datasets, demonstrating superior\nperformance even on low-resolution images. However, in terms of valid response rate, only GPT-40 shows a decrease,\nexcluding GLM-4v which is significantly less accurate overall. This suggests that GPT-40 has more difficulty handling\nlow-resolution images compared to high-resolution images. Detailed results can be found in Appendix B.2.\nHighly biased confidence scores. One common observation across both benchmarks is an FPR of 100%. As shown in\nFig. 3(a), LLaVA-v1.6 consistently yields 100% FPR across different TPR thresholds. This phenomenon is observed\nin most open-source models, but not in proprietary models. To understand the cause of this issue, we examine\nthe distribution of OoD scores. Fig. 3(b) depicts the OoD score distribution of InternVL2-76B and GPT-40 on the\nImageNet200 benchmark. We find that the compared models tend to produce highly biased OoD scores, with most\nOoD scores being either 0.0 or 100.00. A detailed explanation can be found in Appendix B.2. This highlights the need\nfor calibration of confidence scores in the responses of open-source models.\nReasoning. We analyze the contribution of the model's\nvisual feature interpretability to its OoDD capability by\nexamining the rationale behind its predictions. The prompt\nused for reasoning can be found in Appendix B.6. Tab. 2\npresents the reasoning results from GPT-40 and InternVL2-\n26B for four different cases on the ImageNet200 bench-\nmark. The results indicate that high interpretability of\nvisual features contributes to stronger OoDD capabilities.\nGPT-40 provides detailed descriptions of images, leading\nto predictions in fine-grained categories. InternVL2-26B\nalso describes objects in a given image effectively, but not\nwith the same level of detail as GPT-40. A detailed expla-\nnation including the reasoning results for LLaVA-v1.6 can\nbe found in Appendix B.3.\nConfidence scores on ID. We assess confidence scores for\nID to rigorously explore the expressiveness of LVLMs in\ngenerating confidence estimates. To measure the quality"}, {"title": "Reflexive Guidance", "content": "We introduce a simple and model-agnostic prompting strategy, Reflexive Guidance (ReGuide), to enhance the OoD\ndetectability of LVLMs. The LVLM's strong generalization ability has been demonstrated through its performance\nacross various downstream tasks. Therefore, we leverage the LVLM itself to obtain guidance for OoDD from its\npowerful zero-shot visual recognition capabilities. Fig. 4 illustrates the overall framework of ReGuide, which is\nimplemented in a two-stage process. Details on the prompts for each stage can be found in Appendix C.5.\nStage 1: Image-adaptive class suggestions. In the first stage, the LVLM is asked to suggest 2N class names $\\mathcal{A}_{aux}$\nderived from the given image. Specifically, we request two groups of class names: 1) N classes that are visually similar\nto the image denoted as $\\mathcal{A}_{near}$, and 2) N classes that are visually dissimilar or belong to different domains denoted as\n$\\mathcal{A}_{far}$. In the context of ID, $\\mathcal{A}_{near}$ and $\\mathcal{A}_{far}$ can provide classes conceptually corresponding to near-OoD and far-OoD,\nrespectively. If the input image is OOD, $\\mathcal{A}_{aux} = \\mathcal{A}_{near} \\cup \\mathcal{A}_{far}$ can offer potential ground-truth label or class names\nclosely related to the ground-truth label. Similar to the prompt used in Sec. 3.5, we include guidelines and response\nexamples in the prompt. Additionally, we employ a simple post-processing step to refine the suggested classes, as\ndetailed in Appendix C.1.\nStage 2: OoDD with suggested classes. Stage 2 follows a similar procedure to the original OoDD evaluation presented\nin Fig. 2. The major difference is that $\\mathcal{A}_{aux}$ is employed as auxiliary OoD classes. NegLabel (Jiang et al., 2024) and\nEOE (Cao et al., 2024) also use auxiliary OoD classes for improving OoDD performance, but they rely on texts to\nobtain negative concepts. In contrast, our approach utilizes the LVLM, allowing images to be utilized to obtain negative\nconcepts for auxiliary OoD classes. More importantly, existing methods such as NegLabel and EOE utilize static\nnegative concepts based on ID class texts, while our proposed ReGuide leverages adaptive negative concepts derived\nfrom individual input images. Given the strong zero-shot visual recognition capabilities of LVLMs, it is expected that\nOoD input images can be assigned higher confidence scores for $\\mathcal{A}_{aux}$ than for $\\mathcal{Y}$, since $\\mathcal{A}_{aux}$ is derived from the input\nimage itself. The rejection class none of these classes is retained as a fallback in case $\\mathcal{A}_{aux}$ does not adequately function\nas auxiliary OoD classes.\nOoD score design. To evaluate the OoDD performance of ReGuide, we employ the same OoD score as in Sec. 3. Since\nReGuide leverages auxiliary OoD classes $\\mathcal{A}_{aux}$, we compare different OoD scores considered in Jiang et al. (2024) and\nCao et al. (2024). However, we observe that they yield similar outcomes. The comparative results of the different OoD\nscores can be found in Appendix C.4."}, {"title": "Results", "content": "We evaluate ReGuide with GPT-40 and InternVL2-26B/-76B on a 5% subset of the ImageNet200 benchmark due to\ncomputational and API costs. Since ReGuide benefits from the strong image recognition capabilities of LVLMs, we\nexclude LLaVA-v1.6 and GLM-4v from the comparison. For this experiment, we set the number of negative class\nsuggestions for each group N to 20.\nAs shown in Tab. 4, ReGuide significantly improves various aspects of the LVLM's performance. Note that the results\nin Tab. 4 (and in Sec. 3.5) reflect strong baseline performance achieved through carefully designed prompts aimed at\nmaximizing performance, such as providing a task description, a rejection class, detailed guidelines, and a few examples\nfor a structured output."}, {"title": "Image-adaptive vs. text-adaptive.", "content": "We compare ReGuide with an approach based on EOE (Cao et al., 2024). EOE\nleverages $\\mathcal{A}_{aux}$ suggested by LLMs, so we ask GPT-40 to provide $\\mathcal{A}_{near}$ and $\\mathcal{A}_{far}$ referencing the ID class names. We\ndenote this approach as GPT-text, as shown in Tab. 4. The prompt used for GPT-text can be found in Appendix C.5.\nThe key difference between GPT-text and ReGuide is that GPT-text obtains class suggestions via ID class names\nin text form, whereas ReGuide utilizes visual information. As shown in Tab. 4, although GPT-text also enhances\nInternVL2-26B, the improvement is not as significant as that achieved by ReGuide. We infer that this difference arises\nfrom the aforementioned key distinction. When images are used, the model can provide diverse class concepts based on\nthe context of the given image, as each image serves as a unique input. In contrast, when text is used, providing diverse\nconcepts becomes challenging because the ID class set is static. This highlights the effectiveness of image-adaptive\nOoD class suggestions."}, {"title": "Conclusion and Limitations", "content": "In this paper, we address the lack of rigorous evaluation and comparison of the OoDD performance of LVLMs. To tackle\nthis, we establish a framework to evaluate and compare various proprietary and open-source LVLMs. Our comparative\nanalysis provides interesting takeaways into how LVLMs represent confidence scores through their generated natural\nlanguage responses. Overall, proprietary LVLMs outperform open-source LVLMs in both image classification and\nOoDD tasks, demonstrating comparable or even superior OoDD performance relative to SOTA single-modal OoDD\nmodels. Additionally, open-source LVLMs tend to be overconfident in their response, highlighting the need for\nconfidence calibration. Analyzing the rationale behind LVLM predictions reveals that their visual interpretation\ncapabilities impact their OoDD performance. Based on our findings, we propose ReGuide, a self-guided prompting\napproach that enhances the OoDD capabilities of LVLMs by leveraging self-generated, image-adaptive concepts.\nExperimental results demonstrate that ReGuide significantly boosts the OoDD performance of both proprietary and\nopen-source LVLMs. We hope our findings contribute to enhancing the reliability of vision-language foundation models\nfor practical deployment.\nLimitations of this study include the challenges of exerting precise control over LVLM behavior and the insufficient\neffectiveness of guidelines to mitigate unintended outputs. Additionally, the image-adaptive nature of ReGuide may\nlead to suboptimal class suggestions based on image context. A detailed discussion of these limitations can be found in\nAppendix C.6."}]}