{"title": "CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification", "authors": ["Zeqing Qin", "Yiwei Wu", "Lansheng Han"], "abstract": "Large Language Models (LLMs) have shown great promise in vulnerability identification. As C/C++ comprises half of the Open-Source Software (OSS) vulnerabilities over the past decade and updates in OSS mainly occur through commits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing Commits (VCCs) is essential. However, current studies primarily focus on further pre-training LLMs on massive code datasets, which is resource-intensive and poses efficiency challenges. In this paper, we enhance the ability of BERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose CodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++ programs and LLMs. Based on commits, CLNX efficiently converts the source code into a more natural representation while preserving key details. Specifically, CLNX first applies structure-level naturalization to decompose complex programs, followed by token-level naturalization to interpret complex symbols. We evaluate CLNX on public datasets of 25,872 C/C++ functions with their commits. The results show that CLNX significantly enhances the performance of LLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new state-of-the-art and identifies 38 OSS vulnerabilities in the real world.", "sections": [{"title": "1 Introduction", "content": "In recent years, with the rapid growth of open-source software (OSS) applications, the number of OSS vulnerabilities has also been in-creasing rapidly. According to the data from the 2023 OSSRA report [1], in the 1,703 codebases analyzed by the Black Duck audit team, 84% of the codebases contained at least one known open-source vul-nerability, and 48% contained high-risk vulnerabilities. Moreover, 52.13% of reported vulnerabilities in OSS are written in C/C++ [34] over the past decade. As patch commit is the primary way to up-date code in OSS, identifying Vulnerability-Contributing Commits (VCCs) can prevent new vulnerabilities from being introduced into OSS to a large extent [23].\nLarge Language Models (LLMs), particularly those based on the BERT [10] architecture, have demonstrated their potential to iden-tify vulnerabilities by effectively learning code dependencies and contextual nuances [37]. This efficacy is attributed to their bidirec-tional encoder architecture, which enables the models to simultane-ously consider the semantics of context both preceding and follow-ing a given segment of code. However, as these models are trained initially on natural language, there is significant room for improve-ment in code comprehension. Current research primarily focuses on further pre-training LLMs on extensive code datasets to address this [37]. For example, CodeBERT [12] has been pre-trained on six pro-gramming languages: Python, Java, JavaScript, PHP, Ruby, and Go. Nonetheless, it exhibits suboptimal performance on C/C++ due to the absence of specific pre-training for these languages. More im-portantly, the improvements remain marginal even after extensive further pre-training. For instance, ContraBERT [20], which has un-dergone further pre-training based on CodeBERT, achieves only mi-nor percentage-point improvements (a rise of 1.24% in accuracy) in identifying C/C++ vulnerabilities while consuming significant GPU resources. It indicates that further pre-training is inefficient and oc-casionally ineffective [19].\nSpecifically, we address the major challenge in our paper.\n\u2022 How to enhance the effectiveness of LLMs on identifying C/C++ VCCs while ensuring a lightweight implementation?\nTo address this challenge, we introduce CodeLinguaNexus (CLNX), a middleware designed to translate original C/C++ code into a format that enhances compatibility with LLMs. To do so, we first perform the structure-level naturalization. Specifically, we linearize the structure of the C/C++ source code with commit and shorten their length. Then, we perform token-level naturalization. Special C/C++ symbols that differ significantly from natural lan-guage are transformed into their natural language representations.\nWe implement CLNX and evaluate it on a dataset of 25, 872 C/C++ functions with corresponding commits, including 10, 894 VCCs. The result shows that CLNX significantly improves LLMs' performance on C/C++ VCCs identification. Moreover, equipped with CLNX, BERT undergoes an increase of 14.48% in precision, surpassing other models that have been further pre-training on code. Finally, the CLNX-equipped CodeBERT achieves the best effec-tiveness and becomes new state-of-the-art. Lastly, CLNX-equipped CodeBERT finds 38 real-world OSS vulnerabilities by identifying vulnerability-contributing commits, demonstrating CLNX's ability to help LLMs report vulnerabilities in the real world.\nIn summary, our contributions in this paper are:\n\u2022 We propose CLNX, a pioneering framework for improving LLMs' performance on C/C++ VCCs identification in an effective and efficient way."}, {"title": "2 Preliminaries", "content": "2.1 Vulnerability-Contributing Commits\nIn OSS development, patch commits record the differences between two versions of the source code [41]. They can be categorized into two types: vulnerable patch commits and non-vulnerable patch com-mits. Vulnerable patch commits refer to those that will introduce new vulnerabilities into the original code, which are also called Vulnerability-Contributing Commits (VCCs) [23]. In this research, a patch commit is considered \"vulnerable\" if it introduces vulnera-bilities that belong to any of the Common Weakness Enumeration (CWE), regardless of its triggering conditions [34]. Listing 1 shows a vulnerable patch commit with code revisions marked by plus and minus signs (+/-) on the left side. This commit is a configuration item change aimed at improving the path settings for mutexes in the Apache HTTP Server. However, it introduces a vulnerability related to permission bypass. Vulnerable patch commits highlight critical in-formation about vulnerabilities. When identifying VCCs at the func-tional level, both the patch commit and the source code of the revised function are analyzed.\n2.2 Pre-training and Fine-tuning\nPre-training in this paper refers to the training phase of LLMs con-ducted on large-scale unlabeled datasets. LLMs can generally be di-vided into two categories: BERT-based and GPT-based. Since GPT-based LLMs are composed of a decoder structure and are more suit-able for generative tasks [13], we primarily focus on the performance of BERT-based models in vulnerability identification, a code classi-fication task[37]. BERT-based LLMs are pre-trained on tens of mil-lions of text data using techniques like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) [37]. During this phase, these models capture helpful information from the data and store it in their weights. These pre-trained models are then fine-tuned on la-beled data for specific downstream tasks like text classification or question answering. While pre-training requires substantial computa-tional resources, fine-tuning is comparatively more resource-efficient [28]."}, {"title": "3 Methodology", "content": "This section presents an overview of our approach and details each component, including structure-level and token-level naturalization.\n3.1 Overview\nThe overview of CLNX is shown in Figure 1, with CLNX's inter-nal structure displayed on its left side. CLNX is performed at the functional level. In the process of handling input source code and patch commit, CLNX initially undertakes structural-level naturaliza-tion. This stage involves employing CLNX's code analyzer to trans-form the source code into a graph of linear execution paths, followed by the integration of patch information to select the critical path. Subsequently, CLNX advances to token-level naturalization, which involves mapping the identified critical path to the corresponding source code and transforming key symbols into their natural lan-guage equivalents. Finally, CLNX outputs the fully naturalized ver-sion of the source code. The system workflow for deploying CLNX to enhance LLMs' performance on VCCs identification is shown on the right side of Figure 1. For a given set of programs with their corre-sponding patch commits, the programs are naturalized by CLNX and then provided to LLMs for fine-tuning. When an unknown program with its patch commit is analyzed, CLNX transforms the program into naturalized form and then forwards the results to the fine-tuned LLMs for vulnerability identification. In the rest of Section 3, we formalize the details of each component of CLNX.\n3.2 Code Analyzer\nC/C++ programs' complex structures and excessive length chal-lenge LLMs in understanding them. In response to these challenges, CLNX's structure-level naturalization is designed with two primary goals: First, it linearizes complex program structures; Second, it re-duces the overall program length. In particular, the code analyzer extracts linear execution paths within a program.\nIn the design of CLNX's code analyzer, the concept of 'basic blocks,' as borrowed from LLVM [27], plays a pivotal role. A 'ba-sic block' is a sequence of instructions that executes sequentially, characterized by a single entry and a single exit point, devoid of any internal branching. The code analyzer transforms programs into basic blocks and generates a graph $G = (V, E)$, where each ver-tex in $V$ corresponds to a basic block, and each edge in $E$ repre-sents the control flow between blocks. The graph G's entrance point $Ventry$ corresponds to the program's entry basic block, and its exit point $Vexit$ corresponds to the program's final basic block. As a re-sult, any path traversed from $Ventry$ to $Vexit$ within G delineates a linear execution path of the program. In particular, when there is a loop structure, for simplicity, we directly convert the control flow to single executions and label the corresponding nodes as loop struc-tures. It should be noted that CLNX only uses Abstract Syntax Tree (AST) and Control Flow Graph (CFG) for code embedding. While the Program Dependence Graph (PDG), integrating both control de-pendency graph (CDG) and data dependency graph (DDG), is com-monly used to abstractly represent source code [34]. We believe that complex structures risk subjectively introducing excessive irrelevant information, thereby complicating the accurate semantic representa-tion of the code. We compare our method with complex graph-based approaches (embedding AST/CFG/DDG/CDG) in RQ2 to demon-strate CLNX's effectiveness.\nThe code analyzer deploys Joern to generate AST. The whole process is illustrated in Step 1 of Figure 2. In contrast to LLVM,"}, {"title": "3.3 Critical Path Identification", "content": "After obtaining the graph G composed of basic blocks, the focus of CLNX shifts to identifying a critical execution path within the graph that encompasses the maximum amount of vulnerability-related ba-sic blocks. This process can be divided into two primary steps, as illustrated in Step 2 of Figure 2; Firstly, determining the basic blocks that are directly related to a patch commit. Secondly, the critical path within G is selected, which offers the most extensive coverage of these identified basic blocks.\n3.3.1 commit-related basic blocks identification\nBased on the idea of taint analysis [5], CLNX identifies the code removed in the corresponding patch commits of a program as con-"}, {"title": "3.3.2 critical executing path selection", "content": "Further, CLNX is going to select one critical executing path in graph G. CLNX designates the basic block corresponding to the program's entry point as the source ($BBsource$) and the basic block corre-sponding to the program's exit point as the sink ($BBsink$). Based on this, the critical linear execution path $P$ in the graph struc-ture, which originates from $BBsource$ and terminates at $BBsink$, aims to maximize the coverage of vulnerability-related basic blocks $BBtainted$. CLNX designs its critical_path_selecting algorithm based on dynamic programming to circumvent the issue of path ex-plosion. critical_path_selecting algorithm selects the critical exe-cution path in graph G by satisfying three primary criteria: First, the path covers as many $BBtainted$ as possible. Second, the path mini-mizes length. Third, if two paths have the same length and contain the same number of $BBtainted$, then select the one with the highest information entropy value."}, {"title": "3.4 Reverse Mapping", "content": "As a compiled programming language, C/C++ has more low-level symbols compared to natural languages. In response, CLNX's token-level naturalization is designed to translate complex symbols into their natural linguistic equivalents. Initially, CLNX undertakes the task of reverse mapping the critical path, composed of basic blocks, back to the source code. This step involves reconstructing the source code information by tracing the sequence of basic blocks within the critical path. Owing to the grand architecture of CLNX basic blocks, the implementation of reverse mapping is straightforward and effi-cient."}, {"title": "3.5 Key Symbols Transformation", "content": "CLNX designs rules to transform five types of C/C++ symbols into natural presentations. These transformations are semantic-preserving but rewrite original code symbols into artificial, natural forms. Given the source code, CLNX deploys appropriate transformations based on the symbols' type and rewrites the symbols to naturalize the source code. The procedure is illustrated in Step 3 of Figure 2. The selection of the key symbols is motivated by the low-level characters of C/C++ [15]. The examples of transformation rules are shown in Table 1.\nOperator Symbols: Operators directly influence CPU computa-tion instructions. For instance, logical operators involve the CPU's logical instructions; bitwise operators operate directly on the bits of operands. Some operator symbols in C/C++ are high abstraction and symbolization, closer resemblance to low-level machine language, and semantic complexity. CLNX identifies key operator symbols, in-cluding Pointer Operator, Bitwise Operator, and Shift Operator.\nAPI Call Symbols: API call functions involve interactions be-tween the program and the runtime environment, forming the basis for the program's proper functioning. In C/C++, specialized API call symbols pose challenges for LLMs due to their close integration with underlying systems. CLNX identifies key API call symbols, includ-ing Memory Management API Calls, Synchronization Mechanisms API Calls, and System Calls.\nControl Flow Symbols: Control flow symbols directly affect the execution path of a program. C/C++ owns some unique control flow symbols. CLNX identifies key control flow symbols, including Setjmp/Longjmp and Goto.\nPreprocessor Directive Symbols: Preprocessor directives are a part of the compilation process, executing before the compiler com-piles the source code. Preprocessors allow for conditional com-pilation of different code segments based on specific conditions. This technique, common in C/C++, addresses code compatibility is-sues across various platforms and compilation environments. CLNX identifies key preprocessor directive symbols, including Header Files, Macro Definitions, Conditional Compilation, and Preproces-sor Logic.\nDeclaration Symbols: Declaration defines a program's data struc-tures and memory allocation. It allows the compiler to perform type checking, prevent type errors, and optimize at the lower level. Dec-larations in C/C++ have distinct features, such as low-level and complex. CLNX identifies declaration symbols covering Basic Data Types, Classes, and Templates."}, {"title": "4 Experimental Setup", "content": "Our evaluation is designed to answer the following research ques-tions:\n\u2022 RQ1: How does CLNX enhance LLMs for the C/C++ VCCs iden-tification task?\n\u2022 RQ2: How does the performance of CLNX-equipped LLMs com-pare to other vulnerability identification-related methods?\n\u2022 RQ3: How does CLNX-equipped LLM perform in identifying real-world OSS vulnerabilities that are contributed through com-mits?\n4.1 Evaluation Task\nThe evaluation task of our paper is Vulnerable-Contributing Com-mits (VCCs) identification, where the input is the source code and"}, {"title": "5 Experimental Result", "content": "5.1 RQ1: Effectiveness\nWe conduct extensive experiments and an ablation study to assess the effectiveness of CLNX's two sequential naturalization phases in en-hancing LLMs' ability to identify C/C++ VCCs. It should be noted that RoBERTa, ContraBERT, and CodeBERT have undergone fur-ther pre-training with programming data. The results, including pre-cision, accuracy, recall, and F1 score, are presented in Table 3. 'with"}, {"title": "6 Disscussion", "content": "This section discusses the implications, limitations, and potential threats to the validity of our work.\n6.1 Implications\nWe propose a novel, cost-effective framework that enhances the ef-fectiveness of LLMs in identifying C/C++ VCCs. The findings in our research are expected to inspire researchers to improve LLMs' abil-ity to identify VCCs across more programming languages. CLNX offers guidelines for improving LLMs' performance on VCCs iden-tification of specific programming languages in a lightweight man-ner, moving beyond the traditional reliance on extensive pre-training, which requires substantial computational resources.\n6.2 Limitations\nThe experimental results demonstrate that CLNX significantly en-hances the performance of LLMs in VCCs identification. The ad-vancement is mainly due to CLNX's effective two-stage naturaliza-tion, making the code more compatible for LLMs. However, chal-lenges arise from a decline in the Recall score, mainly due to its structure-level naturalization, which might inadvertently omit impor-tant code information. When confronted with multiple paths having equivalent coverage of tainted basic blocks, CLNX's critical path-selecting algorithm prioritizes the shortest path for length minimiza-tion at the risk of overlooking important details. A more effective approach could involve considering data flow more substantially in the critical path selection process. However, it involves dynamic pro-gram analysis. And we will explore it in our future work.\n6.3 Threats to Validity\nInternal Validity: Our analysis identifies two potential threats to in-ternal validity. Firstly, the uniform standard requirement of CLNX's code analyzer necessitates standardizing source code format before its use. Secondly, CLNX calculates path length by counting the num-ber of basic blocks, assuming each block adds uniformly to the total length. To maintain algorithmic integrity in our critical key path se-lection algorithm, all edges of the input graph structure must be of equal length (by default, set to one).\nExternal Validity: Regarding external validity, the performance of the original GPT-based LLMs is significantly lower than that of BERT-based models, so we mainly focus on how CLNX improves BERT-based LLMs' performance on C/C++ VCCs identification."}]}