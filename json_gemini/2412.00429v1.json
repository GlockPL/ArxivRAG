{"title": "Learner Attentiveness and Engagement Analysis in Online Education Using Computer Vision", "authors": ["Sharva Gogawale", "Madhura Deshpande", "Parteek Kumar", "Irad Ben-Gal"], "abstract": "In recent times, online education and the usage of video-conferencing platforms have experienced massive growth. Due to the limited scope of a virtual classroom, it may become difficult for instructors to analyze learners' attention and comprehension in real time while teaching. In the digital mode of education, it would be beneficial for instructors to have an automated feedback mechanism to be informed regarding learners' attentiveness at any given time. This research presents a novel computer vision-based approach to analyze and quantify learners' attentiveness, engagement, and other affective states within online learning scenarios. This work presents the development of a multiclass multioutput classification method using convolutional neural networks on a publicly available dataset DAISEE. A machine learning-based algorithm is developed on top of the classification model that outputs a comprehensive attentiveness index of the learners. Furthermore, an end-to-end pipeline is proposed through which learners' live video feed is processed, providing detailed attentiveness analytics of the learners to the instructors. By comparing the experimental outcomes of the proposed method against those of previous methods, it is demonstrated that the proposed method exhibits better attentiveness detection than state-of-the-art methods. The proposed system is a comprehensive, practical, and real-time solution that is deployable and easy to use. The experimental results also demonstrate the system's efficiency in gauging learners' attentiveness.", "sections": [{"title": "1 Introduction", "content": "The Covid-19 pandemic has significantly transformed the learning approaches of both instructors and learners, as their educational settings have shifted from traditional physical classrooms to digital ones. Millions of people globally use platforms such as Google Meet, Zoom, Teams, and WebEx to deliver lessons, conduct one-on-one sessions, and arrange seminars or conferences. E-learning has become substantially popular due to its feasibility, high demand, and the multitude of benefits that it offers. Although e-learning provides learners with flexible and self-paced learning schedules, it also results in certain inevitable challenges [1].\nIn the traditional offline model of education, instructors can analyze learners' engagement and responses to the material being taught by observing their body language, head movement, and various facial cues. Past studies have proposed that learning comprises a diverse range of cognitive and affective states. These studies have recognized facial expressions that are linked to learning-centered states, which typically include boredom, confusion, engagement, and frustration [2]. In the online mode of education, there is an absence of a real-time feedback mechanism between the learners and instructors. This model lacks a real-time feedback mechanism between the learners and instructors, making it extremely difficult for instructors to analyze learners' engagement levels and attentiveness. As a result, instructors are often unable to adjust their teaching based on learners' visual responses. Consequently, the learning experiences and outcomes could be negatively impacted.\nUnderstanding the affective states of learners can facilitate more effective guidance and allow instructional design to be tailored to their needs. This can also help learners who require assistance and could reduce dropout rates in online learning [3]. Enhancing learner performance and engagement is crucial for online learning. Therefore, there is a pressing need to develop more affordable and accessible methods for capturing, monitoring and analyzing learners' attention. By doing so, the effectiveness of these environments will be maximized and they will be conducive for learners to achieve their educational goals [5]. Engagement detection prevails in multiple domains and it is done using various surveys, eye trackers, head movements, gaze detection, and gauging the seven universal emotions viz. happiness, surprise, contempt, sadness, fear, disgust, and anger. In the education domain, it is crucial to understand how learners acquire, comprehend, and analyze information. The brain's function is contingent upon the integration of both cognitive and affective processing. The affective state of a learner has proved to play a very significant role. Holistic improvement in creative thinking is observed in individuals experiencing positive emotional states, whereas those in"}, {"title": "2 Related Work", "content": "Over the last few years, extensive efforts have been dedicated to detecting and analyzing learner engagement [4, 53]. This ongoing research on automated cognitive engagement detection in online classrooms is crucial for adapting to the unique challenges and opportunities presented by the digital mode of education [8]. At a general level, automated methods for detecting learners' engagement can generally be categorized into computer vision-based methods and multimodal methods. Multimodal techniques are typically employed for identification tasks and utilize expensive equipment for engagement detection.\nYang et al. [9] used an eye-tracking system (FaceLAB 4.5) to observe and evaluate the variations and patterns of visual attention in university learners during PowerPoint presentations. Similarly, Kao et al. [10] used the ASL Mobile Eye-XG eye tracker to investigate learners' engagement while reading. The eye-tracking data was wirelessly transmitted to a system for analysis. Leite et al. [43] implemented both manual and automation techniques to gauge disengagement by analyzing different types of data,"}, {"title": "3 Implementation of Proposed System", "content": "Fig: 1 illustrates the implementation of the proposed system. The chosen publicly available dataset is suitable for classifying affective states. This dataset undergoes a preprocessing phase to make it suitable for feeding into the network architecture. The model is trained on these processed data, generating predictive outcomes. An attentiveness formula evaluates and quantifies the attentiveness of learners by employing classic machine learning techniques. This mathematical formula connects the raw model outputs to a comprehensive attentiveness index. The proposed framework of the system uses live feeds of learners as its primary input source and provides comprehensive attention and engagement analytics to the instructor in real time. The subsequent sections detail each part of the implementation is discussed in detail in the subsequent sections, providing a comprehensive view of the system's development and functionality."}, {"title": "3.1 Dataset", "content": "The proposed attention estimation approach is evaluated using the open-source dataset, DAISEE [39]. DAISEE is the first multilabel video classification dataset to include 9068 video snippets from 112 individuals (32 females and 80 males) captured in \"in the wild\" scenarios for recognizing learners' states, such as boredom, confusion,"}, {"title": "3.1.1 Data Imbalance Issue", "content": "An important issue observed in Table 1 is the imbalanced distribution of data across different affective states in DAiSEE. The support for the 'very low' and 'low' engagement labels is significantly lower than the support for the 'very high' engagement labels. However, for boredom, confusion, and frustration, the opposite is true, i.e., the support for lower-intensity labels is much higher than that for the higher-intensity labels. Conventional machine learning techniques are typically modeled to optimize the total classification accuracy [15]. Their performance is typically optimal when given a balanced data distribution. As DAISEE is an imbalanced dataset, the classifier tends to be biased toward the majority class, reducing the accuracy for the minority class, and making it difficult to extract valuable information and features related to the minority classes. Consequently, the imbalanced dataset is prone to overfitting. Two types of solutions that can be implemented to address such class imbalance issues are data-based solutions and algorithm-based solutions. In this research, focal loss is employed to reduce the bias issue that comes with class-imbalanced data distribution."}, {"title": "3.2 Data Preprocessing", "content": "Data pre-processing refers to the processing of the input video for feeding into the classification model. The 30 fps video snippets are taken to extract the image frames. Further, the Viola-Jones Face Detection (popularly known as Haar Cascade) is employed to extract the region of interest from the image frame. OpenCV's Haar cascade frontal face module is applied to the input frames to achieve this task. The facial landmarks and pupils were detected correctly for most of the images. The images are"}, {"title": "3.3 Model Architecture", "content": "A novel approach involving an n parallel (here n=4) branched model has been used for building the classifier. A separate classifier has been built for each affective state corresponding to each branch. Each classifier predicts the intensity level of that particular affective state. Combining predictions from all these classifiers, the overall output label for the input frame is obtained that contains the corresponding intensities of each affective state. The following sections illustrate the models built using 4 parallel branches."}, {"title": "3.3.1 CNN based Architecture", "content": "In this work, a 4 parallel-branched CNN-based architecture is used as a feature extraction network for classifying the affective states. It is a lightweight model having 0.47 M parameters. The 4 branches are used for the 4-level classification. The final softmax layer of each branch provides a single vector of size 1 X 4 representing the intensity values for each level. The ith element in the vector output of each branch represents the predicted probability that the learner in the input image is at the ith level exhibiting that specific affective state where 0 indicates disengagement and 3 indicates very high engagement. The network's final output is the concatenation of the outputs from all 4"}, {"title": "3.3.2 Efficient Net based Architecture", "content": "A similar 4 parallel branch EfficientNet (EfficientNetB2) model has been used as a backbone network for feature extraction for classifying the affective states. It has 31.7 M parameters making it a lightweight model easy for cloud deployment compared to other networks such as VGG, ResNet etc. The main building block of this architecture [48] is mobile inverted bottleneck MBCon to which squeeze-and-excitation optimization is added. The final softmax layer of each branch provides a single vector of size 1 X 4 representing the intensity values for each affective state."}, {"title": "3.4 Loss Function", "content": "In this paper, categorical focal loss is employed to address the issue of class imbalance in the data. Focal Loss [49] applies a modulating term to the categorical cross-entropy loss to focus learning on hard negative examples and improves the classification performance. Focal Loss is represented as:\n$FocalLoss(p_t) = \\alpha \\cdot (1 - p_t)^\\gamma \\cdot CategoricalCE(Y_{true}, Y_{pred})$\nHere pt is given by,\n$p_t = \\begin{cases} output, & \\text{if } Y_{true} == 1 \\\\  1 - output, & \\text{otherwise} \\end{cases}$\nand CategoricalCE is given by,\n$CategoricalCE(Y_{true}, Y_{pred}) = - \\sum_{n=1}^C Y_{true;n} * log(y_{pred;n})$\n$\\alpha$ is a balancing factor that can be adjusted to assign more or less weight to different classes and $\\gamma$ is the focusing parameter that controls how much the loss focuses on difficult examples."}, {"title": "3.5 Model Training", "content": "The deep learning models are compiled using Keras (a high-level API) with TensorFlow 2.0 backend using the Python environment. Model was trained on a machine with Intel core CPU and Nvidia GTX GPU. Adam Optimizer with the initial learning rate set to 0.001 was used. The training was done with an early stopping criterion to obtain the model weights from best performing epoch. Alpha balanced focal cross entropy loss has been used in this process for 4-level classification."}, {"title": "4 Developing Attentiveness Formula", "content": "Classifying only affective states cannot comprehensively depict the learners' attentiveness. Introducing a single parameter that accounts for these affective states and instantly helps instructors understand whether the learner is attentive is crucial. For this purpose, a mathematical model has been developed that connects affective states using an Attentiveness Index (Ai). The main tasks are to find corresponding appropriate weights for each affective state, to understand how the affective states impact the overall index (whether negatively or positively, and to what extent), and validate the model using the current cognitive science literature. For this purpose, a subset of the dataset was evaluated and crowd-annotated by multiple instructors. Each video was given an attentiveness score on a scale of 1 to 10 (1 being the least attentive and 10 being the most attentive). The average attentiveness score was calculated based on all the labels provided by instructors' assessments. A machine learning algorithm based on multiple linear regression was applied on top of this newly formed data subset provided the weights for each affective state, consequently providing the overall mathematical formula linking the affective states to a unique attentiveness index.\nThe obtained mathematical formula is :\n$A_i = -0.598B + 1.539E + 0.334C - 0.085F$\nwhere A; is the Attentiveness Index and B, E, C, and F represent the intensity values for Boredom, Engagement, Confusion, and Frustration respectively. This formula (4) illustrates the weight of each affective state and whether it contributes positively or negatively to the attentiveness index. The resulting weights obtained for each of the affective states contributing to overall attentiveness align with past research propositions. Persistent observations across various learning environments indicate that boredom is linked to suboptimal learning outcomes, whereas frustration appears to have a weaker association with poor learning. Learning environments most often exhibit boredom and engaged concentration. The coefficient trends obtained for engagement and confusion are positive, with the confusion coefficient being less positive than the engagement coefficient. As per the literature, confusion positively impacts learning, and the positive coefficient, although not overly high, aligns with the literature. Moreover, boredom and frustration exhibit negative weights, where the boredom coefficient is more negative. These trends align with the existing cognitive studies [40-42]."}, {"title": "5 Online Platform for Attentiveness Analysis", "content": "Fig 1 depicts the primary components of the proposed system for assessing learners' affective states and engagement. Instructors can employ this system during live lecture sessions in virtual classrooms, remote learning settings, or any other learning environments that use personal computers and web cameras. Two types of participants are present in the education setting: the instructor and the learner. When the learner interacts with learning material, the system automatically analyzes the live video feed"}, {"title": "6 Results", "content": "In this section, the experimental analysis performed on the proposed approach is discussed. It is done through a comparison of the performance of the proposed method"}, {"title": "7 Conclusion and Future Scope", "content": "Leveraging Artificial Intelligence techniques can enhance e-learning, allowing learners worldwide to study at their own pace and fostering collaborative learning regardless"}]}