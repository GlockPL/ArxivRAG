{"title": "Adaptive Knowledge Distillation for Classification of Hand Images using Explainable Vision Transformers", "authors": ["Thanh Thi Nguyen", "Campbell Wilson", "Janis Dalins"], "abstract": "Assessing the forensic value of hand images involves the use of unique features and patterns present in an individual's hand. The human hand has distinct characteristics, such as the pattern of veins, fingerprints, and the geometry of the hand itself. This paper investigates the use of vision transformers (ViTs) for classification of hand images. We use explainability tools to explore the internal representations of ViTs and assess their impact on the model outputs. Utilizing the internal understanding of ViTs, we introduce distillation methods that allow a student model to adaptively extract knowledge from a teacher model while learning on data of a different domain to prevent catastrophic forgetting. Two publicly available hand image datasets are used to conduct a series of experiments to evaluate performance of the ViTs and our proposed adaptive distillation methods. The experimental results demonstrate that ViT models significantly outperform traditional machine learning methods and the internal states of ViTs are useful for explaining the model outputs in the classification task. By averting catastrophic forgetting, our distillation methods achieve excellent performance on data from both source and target domains, particularly when these two domains exhibit significant dissimilarity. The proposed approaches therefore can be developed and implemented effectively for real-world applications such as access control, identity verification, and authentication systems.", "sections": [{"title": "1 Introduction", "content": "Common biometric features encompass fingerprints, hand geometry, iris patterns, facial features, voice patterns, and even typing patterns or gait [12]. The choice of which feature to use depends on factors such as security requirements, convenience, and the specific use case. Hand images have become a popular biometric feature because the physical characteristics of an individual's hand are unique and can be reliably measured and analyzed [2]. Hand features are"}, {"title": "2 Important Caveats and Application", "content": "It is important to emphasize that no elements of this research are proposed for use in identifying or storing individuals' sensitive or private data, including biometrics, and it does not involve the gathering of additional data, nor data"}, {"title": "3 Related Work", "content": "The structure of the hand, including the length and width of fingers, the shape of the palm, and the configuration of veins, forms a set of distinctive features that vary from person to person. Hardalac et al. [18] incorporated various feature extraction methods such as ripplet-I transform (an extended version of curvelet transform), discrete cosine transform, discrete wavelet transform, contourlet transform, PCA, and LBP for palm print image processing. A traditional artificial neural network, Euclidean distance, SVM, and CNN are used as classifiers. The most effective contributors to palm print verification and identification are found to be the LBP features and the Euclidean distance classifier.\nLikewise, a rapid palmprint recognition system, named Simplified PalmNet-Gabor, which leverages the PalmNet was designed in [37]. The Log-Gabor filters are used to modify the pixel luminance of palm print images. Fisher score and ReliefF are then employed for feature selection while the whitening PCA method is used for dimensionality reduction, aiming to decrease computational expenses and enhance accuracy with SVM and k-nearest neighbour classifiers. In another study, ViT models were employed in [16] for vein biometric recognition applications. ViTs demonstrate better performance compared with other methods across different datasets.\nOn the other hand, Afifi [1] introduced a large dataset of 11k hand images with useful metadata and used CNN and LBP as feature extraction methods and SVM as a classifier for gender recognition and biometric identification purposes. Four different CNN architectures including AlexNet [24], VGGNet (16- and 19-layer variants) [34], and GoogleNet [35] are applied to extract deep features. Hand-crafted feature extraction methods are also used such as scale invariant feature transform (SIFT), color-invariant SIFT [8], and rgSIFT [38]. The distinctive features on the dorsal side are discovered to be effective, comparable to, if not superior to, those present in palmar side images of human hands."}, {"title": "4 ViT Models", "content": "The ViT was first introduced in [15] by Google based on the transformer architecture proposed to address natural language processing tasks. We refer to this initial model as \"Google ViT\" to distinguish it from the generic \"ViT\", which encompasses other variants. In this study, we investigate 6 ViT variants, as summarized below, and provide more details in the Supplementary Material, available at https://github.com/thanhthinguyen/ecmlpkdd2024."}, {"title": "4.1 Google ViT", "content": "This ViT is a transformer encoder model, resembling the Bidirectional Encoder Representations from Transformers (BERT) [14], that underwent pre-training on a substantial set of images, specifically ImageNet-21k having 14 million images across 21,843 classes, with a resolution of 224\u00d7224 pixels. Subsequently, fine-tuning was applied to the model using ImageNet (i.e., ImageNet Large Scale Visual Recognition Challenge - ILSVRC2012), a dataset consisting of one million images across 1,000 classes."}, {"title": "4.2 DeiT", "content": "The Data-efficient image Transformer (DeiT) model [36] was pre-trained and fine-tuned on the ImageNet-1k dataset, which comprises one million images at a resolution of 224\u00d7224 distributed across 1,000 classes. Similar to other ViTs, this model processes images by organizing them into a sequence of linearly embedded fixed-size patches at the resolution of 16\u00d716. Through pre-training, the"}, {"title": "4.3 BEIT", "content": "The Bidirectional Encoder representation from image Transformers (BEIT) model [4], akin to BERT [14], is pre-trained on the ImageNet-21k dataset at a resolution of 224\u00d7224 pixels. During pre-training, it focuses on predicting visual tokens derived from the encoder of OpenAI DALL-E's VQ-VAE, employing masked patches. Subsequently, the model undergoes supervised fine-tuning on the ILSVRC2012. In contrast to other ViT models, BEiT employs relative position embeddings rather than absolute position embeddings. To classify images, the mean-pooling operation is applied to the final hidden states of the patches, as opposed to using a linear layer for the final hidden state of the [CLS] token."}, {"title": "4.4 DINOv2", "content": "This is a ViT model trained using the DINOv2 method [32] with DINO being a form of self-distillation with no labels [7]. DINOv2 comprises a novel set of image encoders pretrained on extensive curated data without supervision. Representing an instance of self-supervised learning applied to image data, DINOv2 achieves visual features that narrow the performance gap with weakly supervised alternatives across diverse benchmarks, all without fine-tuning."}, {"title": "4.5 Swin Transformer V2", "content": "The Swin transformer constructs hierarchical feature maps by consolidating image patches in different layers [29]. It achieves linear time complexity concerning image size by computing self-attention exclusively within each local window. To enhance scalability, Swin transformer V2 [28] replaces the pre-norm setup in the original Swin transformer with a residual-post-norm configuration and substitutes the dot product attention with a scaled cosine attention. Furthermore, it introduces a log-spaced continuous relative position bias method to improve the model's transferability across window resolutions."}, {"title": "4.6 ViT-MAE", "content": "This ViT model was pre-trained using the masked autoencoders (MAE) [19]. Images are divided into fixed-size patches with 75% of image patches being randomly masked out. Initially, the encoder is employed to encode these visual patches. Subsequently, a learnable (shared) mask token is introduced at the locations corresponding to the masked patches. The decoder utilizes the encoded visual patches and mask tokens as input to recreate raw pixel values at the masked positions."}, {"title": "5 Knowledge Distillation for Adaptation", "content": "Hinton et al. [20] proposed a knowledge distillation method to transfer knowledge from a cumbersome model to a smaller student model. This approach proves valuable for compressing deep learning models and facilitating their deployment on small devices [23]. In this study, we employ Hinton et al.'s method to mitigate the issue of catastrophic forgetting during the domain adaptation process. However, it is observed that Hinton et al.'s method still experiences the forgetting problem, particularly when the target domain significantly differs from the source domain.\nWe propose a method (method 1) that allows the student to closely follow the teacher at the start of the learning process and gradually deviate as the learning progresses. In the initial phases of adjusting to a new domain, the student model may become overwhelmed with a substantial influx of new knowledge, especially in cases where the new domain diverges significantly from the source domain. Our approach aids in stabilizing the learning process of the student and results in improved performance on both the source and target domains.\nWe also introduce method 2 to enable the student to glean insights from the internal states of the teacher model. Given that the teacher model harbors a superior hidden representation, which is difficult for the student to achieve unaided, we deliberately guide the student to replicate not only the teacher's output layers but also its internal states. Our adaptive distillation methods are summarized in Fig. 1 and detailed in the following subsections."}, {"title": "5.1 Method 1: Adaptive distillation", "content": "A deep learning model for a classification task normally receives an input variable x and generates an output logit $z_i$ where $i \\in [1, C]$ with C being the number"}, {"title": "5.2 Method 2: Imitate the teacher's internals", "content": "A deep learning model designed for a computer vision task typically harbors valuable internal information regarding the features and characteristics of the data, contributing to the interpretation of model outputs. While emulating the teacher's behaviors based on its generated outputs is valuable, there is potential merit in also replicating the internal representations/states of the teacher. Using the deep feature factorization (DFF) and Grad-CAM tools, we evaluate the explainability of different components of the ViT model. The DFF method involves decomposing the activations from the model into distinct concepts (e.g., class labels) using non-negative matrix factorization. Subsequently, for each pixel, the method calculates its correlation with each of these labels. This technique is adept at pinpointing analogous semantic concepts inside an individual image or a collection of images [11]. On the other hand, Grad-CAM is employed to visualize the regions of an input image that contribute the most to the predictions made by a neural network. The Grad-CAM heatmap provides a visual representation of the areas that the network focused on during its decision-making process [33]. DFF and Grad-CAM were originally proposed to deal with CNN models. In this study, we use these two methods to acquire a more profound insight into the learned features of the ViT models.\nA Google ViT model has 12 layers of transformer encoders. Through numerous experiments examining various components at different layers of the ViT, we could verify the usefulness of the final linear component of the 11th transformer encoder layer and the layer normalization (or Norm in Fig. 1) component of ViT for explainability purpose as suggested in [17]. We therefore propose an approach to enable the student to mimic these two components of the teacher.\nLet us denote the bias and weight parameters of the above-mentioned Norm as $b_n$ and $w_n$, the bias and weight parameters of the final linear component of the 11th layer (i.e., the second last layer) of the ViT as $b_o$ and $W_o$, respectively. Let H and I be the hidden size and intermediate size of the ViT configuration, $b_n$, $w_n$, and $b_o$ are 1-dimension arrays at length of H while $W_o$ is a 2-dimension matrix with a size of H \u00d7 I. We transpose and get a mean of $W_o$ before stacking it with the remaining parameters arrays. The resulting matrix is therefore a stack of $b_n$, $w_n$, $b_o$, and $W_o^T$, and has a size of 4\u00d7H. The same procedure is applied for both student model and the teacher model, resulting in two embedding matrices $W_s$ and $W_t$ for the student and teacher, respectively. The cosine embedding loss is used to measure whether the two input matrices are similar or dissimilar, using the cosine similarity, defined as below:"}, {"title": "6 Hand Images Datasets", "content": ""}, {"title": "6.1 IIT Delhi Dataset", "content": "The first dataset used in our experiments is the IIT Delhi touchless palm print image dataset version 1.0 [25,26]. This dataset comprises images captured from hands of students and staff at IIT Delhi, India by a digital CMOS camera during January 2006 - July 2007. Left and right hand colour images are obtained from 230 subjects through a touchless imaging setup at a resolution of 1600\u00d71200. Subjects have ages from 14 to 56 years old and each contributed minimum 5 palmar hand images from both left and right hands."}, {"title": "6.2 11k Hands Dataset", "content": "The second dataset is the 11k hands dataset. This dataset was introduced in [1], which includes hand images from 190 subjects with both left and right hands, and both palmar and dorsal sides at a resolution of 1600\u00d71200. This is the most recent dataset in this domain, consisting of 11,076 colour hand images of various subjects with ages in the range from 18 to 75. This dataset was published with detailed metadata, which records not only the sides of the hands (e.g., dorsal or palmar, left or right) but also information about whether hand images contain accessories, irregularities, or nail polish."}, {"title": "7 Experimental Results", "content": "We adopt the classification accuracy as the performance evaluation metric for comparisons between ViT models and traditional methods. Each fine-tuning process is conducted through 50 epochs, using an Adam optimizer at a learning rate of 2 \u00d7 10-5. Images are randomly cropped and horizontally flipped for data augmentation during training with a batch size of 32."}, {"title": "7.1 ViT models vs traditional methods", "content": "IIT Delhi dataset To benchmark our work with existing methods, e.g., [1,9,5], we perform three experiments corresponding to classifying hand images of 100, 137 and 230 subjects. For each subject, we select randomly four images for the training set and one image for the testing set. For a objective evaluation of the ViT models, the experiment is repeated 10 times for each model with the subjects being randomly selected each time. Results shown in Table 1 are the averaged accuracy and standard deviation across 10 times."}, {"title": "7.2 ViT explainability results", "content": "An illustration of the explainability results based on the DFF and Grad-CAM methods is shown in Fig. 2. The PyTorch library for CAM methods [17] is used to investigate the internal representations of different components of the Google ViT model. Fig. 2a presents raw dorsal right hand images of subjects \"0001046\" (left image), and subject \"0001573\" (right image) in the 11k hands dataset.\nFig. 2b illustrates the internal representations of the ViT when attempting to predict the subject of an input image, which is the image on the left of Fig. 2a. This internal state of the ViT is extracted from the parameters (after fine-tuning) of the second linear component of the 11th encoder layer of the ViT (i.e., the linear component contributes to the cosine loss in Fig. 1). This component helps to explain that the ViT predicts subject \u201c0001046\u201d (which is correct) based on nail polish on the fingers. Given the input image, the left image of Fig. 2b illustrates the ViT's interpretation of the subject \"0001046\" while the right image of Fig. 2b depicts its perspective on the subject \"0001573\". Both left and right images of Fig. 2b indicate nail polish as the features for recognizing the subjects \"0001046\" and \"0001573\", which is correct because both subjects have nail polish on their fingers (as shown in Fig. 2a). However, the ViT model might encounter confusion in distinguishing between these two subjects when relying solely on information from the mentioned linear component.\nHow does the ViT predict that the input image belongs to subject \"0001046\u201d rather than subject \"0001573\"? We found that this can be explained by another component, which is the Norm of the ViT (the Norm contributes to the cosine loss in Fig. 1). Based on parameters of this component, the DFF method helps to explain (as in Fig. 2c) that the entire region of the hand in the image (with green color) is predicted to be of subject \"0001046\". The shape of the dorsal"}, {"title": "7.3 ViT models for transfer inference", "content": "We conduct several experiments to demonstrate the accuracy of the ViT models when they are transferred to perform inference on data of a different domain.\nAs the IIT Delhi dataset has only palm-side images, we choose randomly four left-hand images for each subject to create a source dataset. We aim to transfer the trained models to predict labels on a target dataset, which comprises four randomly selected right-hand images for each subject.\nThe 11k hands dataset has both palm- and dorsal-side images of left and right hands, and we therefore design experiments to transfer models not only from left to right side, but also from palm to dorsal side, and vice versa. Specifically, we train models on left-palm images and transfer them to predict labels for right-palm and left-dorsal images. Similarly, we train models on right-dorsal images and predict labels for left-dorsal and right-palm images. We create a source dataset by selecting randomly four images for each subject. Each target dataset also consists of four randomly selected images for each subject."}, {"title": "7.4 ViT domain adaptation", "content": "This subsection presents results of our adaptive distillation methods (methods 1 and 2) for domain adaptation in hand image classification in comparison with the baseline, i.e., the Hinton et al.'s method [20]. The temperature T equal to 2 is selected to compute the soft loss for all experiments in this subsection. We conducted 5 experimental scenarios corresponding to those in subsection 7.3. In each scenario, we check the accuracy of the student before learning to adapt to a new domain. We then conduct the adaptation by training the model on the target domain without any distillation, aiming to assess the extent to which the model forgets knowledge from the source domain. Experiments for comparisons between our methods 1 and 2 with the Hinton et al.'s method are followed with the teacher being either the soft voting ensemble or the student's prior copy. The ensemble teacher does not have a specific internal representation so that our method 2 is not conducted in that case.\nIn the first three scenarios (i.e., left-palm to right-palm on the IIT Delhi dataset, left-palm to right-palm and righ-dorsal to left-dorsal on the 11k hands dataset), the student model retains significant knowledge of the source domain after training on the target domain even in the absence of distillation. After training on the target domain, the accuracy of the student on the source and target domains in these three scenarios are around 0.95 to 1.0. Catastrophic forgetting is minimal in these scenarios because the two domains (left and right) are closely related. The distillation methods provide improvement, although their impact is not substantial in these scenarios. Detailed results for these three scenarios and the left-palm to left-dorsal scenario using the 11k hands dataset are presented in Supplementary Material, at https://github.com/thanhthinguyen/ecmlpkdd2024.\nResults for the right-dorsal to right-palm adaptation are presented in Table 4. When training on the target domain without distillation, the student model's accuracy on the source domain drops to 0.472. It indicates the occurrence of catastrophic forgetting, a phenomenon that is understandable given the considerable dissimilarity between the two domains (palm and dorsal). We observed that distillation methods result in significant improvements in this scenario (and also left-palm to left-dorsal scenario presented in the Supplementary Material). Particularly, methods 1 and 2 achieve comparable or superior accuracy on both the source and target domains when compared to Hinton et al.'s method. Our methods contribute to a noteworthy improvement on the source domain by mitigating the catastrophic forgetting problem.\nThe student extracts knowledge from the ensemble teacher via their voting results. Even though these voting results are stable and sometimes better than the individual method, distilling knowledge from the ensemble is less effective"}, {"title": "8 Conclusion and Future Work", "content": "We have fine-tuned 6 ViT models for classification of hand images. A stable and superior performance of ViT models has been demonstrated compared with existing methods. We show the usefulness of two components of ViT for the explainability purpose. Results also demonstrate the effectiveness of our distillation methods when training a ViT model on a new domain. Our methods help to obtain high classification performance on both source and target domain even the model has no access to the source domain data. A further work can be to develop our methods on other types of data such as fingerprints and iris recognition data for biometric applications."}, {"title": "Ethical Implications", "content": "While our hand image classification algorithms offer benefits in applications such as access control, identity verification, and authentication systems, their real-world implementation must be guided by ethical principles. There are risks associated with the potential usage of our methods for biometric identification. They include lack of informed consent, discrimination and bias, privacy concerns, misuse and abuse, among other risks."}]}