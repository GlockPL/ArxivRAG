{"title": "Adaptive Knowledge Distillation for Classification of Hand Images using Explainable Vision Transformers", "authors": ["Thanh Thi Nguyen", "Campbell Wilson", "Janis Dalins"], "abstract": "Assessing the forensic value of hand images involves the use of unique features and patterns present in an individual's hand. The human hand has distinct characteristics, such as the pattern of veins, fingerprints, and the geometry of the hand itself. This paper investigates the use of vision transformers (ViTs) for classification of hand images. We use explainability tools to explore the internal representations of ViTs and assess their impact on the model outputs. Utilizing the internal understanding of ViTs, we introduce distillation methods that allow a student model to adaptively extract knowledge from a teacher model while learning on data of a different domain to prevent catastrophic forgetting. Two publicly available hand image datasets are used to conduct a series of experiments to evaluate performance of the ViTs and our proposed adaptive distillation methods. The experimental results demonstrate that ViT models significantly outperform traditional machine learning methods and the internal states of ViTs are useful for explaining the model outputs in the classification task. By averting catastrophic forgetting, our distillation methods achieve excellent performance on data from both source and target domains, particularly when these two domains exhibit significant dissimilarity. The proposed approaches therefore can be developed and implemented effectively for real-world applications such as access control, identity verification, and authentication systems.", "sections": [{"title": "1 Introduction", "content": "Common biometric features encompass fingerprints, hand geometry, iris patterns, facial features, voice patterns, and even typing patterns or gait [12]. The choice of which feature to use depends on factors such as security requirements, convenience, and the specific use case. Hand images have become a popular biometric feature because the physical characteristics of an individual's hand are unique and can be reliably measured and analyzed [2]. Hand features are"}, {"title": "2 Important Caveats and Application", "content": "It is important to emphasize that no elements of this research are proposed for use in identifying or storing individuals' sensitive or private data, including biometrics, and it does not involve the gathering of additional data, nor data"}, {"title": "3 Related Work", "content": "The structure of the hand, including the length and width of fingers, the shape of the palm, and the configuration of veins, forms a set of distinctive features that vary from person to person. Hardalac et al. [18] incorporated various feature extraction methods such as ripplet-I transform (an extended version of curvelet transform), discrete cosine transform, discrete wavelet transform, contourlet transform, PCA, and LBP for palm print image processing. A traditional artificial neural network, Euclidean distance, SVM, and CNN are used as classifiers. The most effective contributors to palm print verification and identification are found to be the LBP features and the Euclidean distance classifier.\nLikewise, a rapid palmprint recognition system, named Simplified PalmNet-Gabor, which leverages the PalmNet was designed in [37]. The Log-Gabor filters are used to modify the pixel luminance of palm print images. Fisher score and ReliefF are then employed for feature selection while the whitening PCA method is used for dimensionality reduction, aiming to decrease computational expenses and enhance accuracy with SVM and k-nearest neighbour classifiers. In another study, ViT models were employed in [16] for vein biometric recognition applications. ViTs demonstrate better performance compared with other methods across different datasets.\nOn the other hand, Afifi [1] introduced a large dataset of 11k hand images with useful metadata and used CNN and LBP as feature extraction methods and SVM as a classifier for gender recognition and biometric identification purposes. Four different CNN architectures including AlexNet [24], VGGNet (16- and 19-layer variants) [34], and GoogleNet [35] are applied to extract deep features. Hand-crafted feature extraction methods are also used such as scale invariant feature transform (SIFT), color-invariant SIFT [8], and rgSIFT [38]. The distinctive features on the dorsal side are discovered to be effective, comparable to,\nif not superior to, those present in palmar side images of human hands."}, {"title": "4 ViT Models", "content": "The ViT was first introduced in [15] by Google based on the transformer architecture proposed to address natural language processing tasks. We refer to this initial model as \"Google ViT\" to distinguish it from the generic \"ViT\", which encompasses other variants. In this study, we investigate 6 ViT variants, as summarized below, and provide more details in the Supplementary Material,\navailable at https://github.com/thanhthinguyen/ecmlpkdd2024."}, {"title": "4.1 Google ViT", "content": "This ViT is a transformer encoder model, resembling the Bidirectional Encoder Representations from Transformers (BERT) [14], that underwent pre-training on a substantial set of images, specifically ImageNet-21k having 14 million images across 21,843 classes, with a resolution of 224\u00d7224 pixels. Subsequently, fine-tuning was applied to the model using ImageNet (i.e., ImageNet Large Scale Visual Recognition Challenge - ILSVRC2012), a dataset consisting of one million\nimages across 1,000 classes."}, {"title": "4.2 DeiT", "content": "The Data-efficient image Transformer (DeiT) model [36] was pre-trained and fine-tuned on the ImageNet-1k dataset, which comprises one million images at a resolution of 224\u00d7224 distributed across 1,000 classes. Similar to other ViTs, this model processes images by organizing them into a sequence of linearly embedded fixed-size patches at the resolution of 16\u00d716. Through pre-training, the"}, {"title": "4.3 BEIT", "content": "The Bidirectional Encoder representation from image Transformers (BEIT) model [4], akin to BERT [14], is pre-trained on the ImageNet-21k dataset at a resolution of 224\u00d7224 pixels. During pre-training, it focuses on predicting visual tokens derived from the encoder of OpenAI DALL-E's VQ-VAE, employing masked patches. Subsequently, the model undergoes supervised fine-tuning on the ILSVRC2012. In contrast to other ViT models, BEiT employs relative position embeddings rather than absolute position embeddings. To classify images, the mean-pooling operation is applied to the final hidden states of the patches,\nas opposed to using a linear layer for the final hidden state of the [CLS] token."}, {"title": "4.4 DINOv2", "content": "This is a ViT model trained using the DINOv2 method [32] with DINO being a form of self-distillation with no labels [7]. DINOv2 comprises a novel set of image encoders pretrained on extensive curated data without supervision. Representing an instance of self-supervised learning applied to image data, DINOv2 achieves visual features that narrow the performance gap with weakly supervised\nalternatives across diverse benchmarks, all without fine-tuning."}, {"title": "4.5 Swin Transformer V2", "content": "The Swin transformer constructs hierarchical feature maps by consolidating image patches in different layers [29]. It achieves linear time complexity concerning image size by computing self-attention exclusively within each local window. To enhance scalability, Swin transformer V2 [28] replaces the pre-norm setup in the original Swin transformer with a residual-post-norm configuration and substitutes the dot product attention with a scaled cosine attention. Furthermore, it introduces a log-spaced continuous relative position bias method to improve the\nmodel's transferability across window resolutions."}, {"title": "4.6 ViT-MAE", "content": "This ViT model was pre-trained using the masked autoencoders (MAE) [19]. Images are divided into fixed-size patches with 75% of image patches being randomly masked out. Initially, the encoder is employed to encode these visual patches. Subsequently, a learnable (shared) mask token is introduced at the locations corresponding to the masked patches. The decoder utilizes the encoded\nvisual patches and mask tokens as input to recreate raw pixel values at the\nmasked positions."}, {"title": "5 Knowledge Distillation for Adaptation", "content": "Hinton et al. [20] proposed a knowledge distillation method to transfer knowledge from a cumbersome model to a smaller student model. This approach proves valuable for compressing deep learning models and facilitating their deployment on small devices [23]. In this study, we employ Hinton et al.'s method to mitigate the issue of catastrophic forgetting during the domain adaptation process. However, it is observed that Hinton et al.'s method still experiences the forgetting problem, particularly when the target domain significantly differs from the\nsource domain.\nWe propose a method (method 1) that allows the student to closely follow the teacher at the start of the learning process and gradually deviate as the learning progresses. In the initial phases of adjusting to a new domain, the student model may become overwhelmed with a substantial influx of new knowledge, especially in cases where the new domain diverges significantly from the source domain. Our approach aids in stabilizing the learning process of the student and\nresults in improved performance on both the source and target domains.\nWe also introduce method 2 to enable the student to glean insights from the internal states of the teacher model. Given that the teacher model harbors a superior hidden representation, which is difficult for the student to achieve unaided, we deliberately guide the student to replicate not only the teacher's\noutput layers but also its internal states. Our adaptive distillation methods are\nsummarized in Fig. 1 and detailed in the following subsections."}, {"title": "5.1 Method 1: Adaptive distillation", "content": "A deep learning model for a classification task normally receives an input variable x and generates an output logit $z_i$ where i\u2208 [1, C] with C being the number"}, {"title": "5.2 Method 2: Imitate the teacher's internals", "content": "A deep learning model designed for a computer vision task typically harbors valuable internal information regarding the features and characteristics of the data, contributing to the interpretation of model outputs. While emulating the teacher's behaviors based on its generated outputs is valuable, there is potential merit in also replicating the internal representations/states of the teacher. Using the deep feature factorization (DFF) and Grad-CAM tools, we evaluate the explainability of different components of the ViT model. The DFF method involves decomposing the activations from the model into distinct concepts (e.g., class labels) using non-negative matrix factorization. Subsequently, for each pixel, the method calculates its correlation with each of these labels. This technique is adept at pinpointing analogous semantic concepts inside an individual image or a collection of images [11]. On the other hand, Grad-CAM is employed to visualize the regions of an input image that contribute the most to the predictions made by a neural network. The Grad-CAM heatmap provides a visual representation of the areas that the network focused on during its decision-making process [33]. DFF and Grad-CAM were originally proposed to deal with CNN\nmodels. In this study, we use these two methods to acquire a more profound insight into the learned features of the ViT models.\nA Google ViT model has 12 layers of transformer encoders. Through numerous experiments examining various components at different layers of the ViT, we could verify the usefulness of the final linear component of the 11th transformer encoder layer and the layer normalization (or Norm in Fig. 1) component of ViT for explainability purpose as suggested in [17]. We therefore propose an\napproach to enable the student to mimic these two components of the teacher.\nLet us denote the bias and weight parameters of the above-mentioned Norm as $b_n$ and $w_n$, the bias and weight parameters of the final linear component of the 11th layer (i.e., the second last layer) of the ViT as $b_o$ and $W_o$, respectively.\nLet H and I be the hidden size and intermediate size of the ViT configuration, $b_n$, $w_n$, and $b_o$ are 1-dimension arrays at length of H while $W_o$ is a 2-dimension\nmatrix with a size of H \u00d7 I. We transpose and get a mean of $W_o$ before stacking it\nwith the remaining parameters arrays. The resulting matrix is therefore a stack\nof $b_n$, $w_n$, $b_o$, and $W_o^T$, and has a size of 4\u00d7H. The same procedure is applied for\nboth student model and the teacher model, resulting in two embedding matrices $W_s$ and $W_t$ for the student and teacher, respectively. The cosine embedding loss is used to measure whether the two input matrices are similar or dissimilar, using\nthe cosine similarity, defined as below:\n$L_c(W_s, y) = \\begin{cases} 1-cos(W_s, W_t), & \\text{if } y = 1 \\\\ max(0, cos(W_s, W_t)), & \\text{if } y = -1 \\end{cases}$\nwhere y is the target, which is set equal to 1 in all experiments in this study. To\nenable an adaptive distillation for domain adaptation, an overall loss function is\nspecified as:\n$L = \\frac{E}{e}(T^2 L_s + L_c) + L_h$"}, {"title": "6 Hand Images Datasets", "content": "where $L_s$, $L_h$, and $L_c$ are defined in Eqs. 2, 3, and 6, respectively. This overall loss function is used to fine-tune the student model on the transfer set. In this study, we use the Google ViT as the student model. This ViT has been trained on the source domain data. To adapt this model to data of a different domain, we experiment two scenarios for the teacher. The first scenario for the teacher uses a soft voting ensemble of 6 ViT variants (each described in section 4) that all have been trained on the source domain data. The average of pseudo-probabilities\nacross individual methods is used to calculate the soft loss in Eq. 2 in this sce-nario. The second scenario uses a copy of the student itself as the teacher. In\nboth scenarios, the teacher has a great knowledge of the source domain data,\nsimilar to the student itself before adapting to the new domain. With the knowl-edge distilled from the teacher, we enable the student to avoid forgetting the\nsource domain knowledge while learning on data of the new domain."}, {"title": "6.1 IIT Delhi Dataset", "content": "The first dataset used in our experiments is the IIT Delhi touchless palm print image dataset version 1.0 [25,26]. This dataset comprises images captured from hands of students and staff at IIT Delhi, India by a digital CMOS camera during January 2006 - July 2007. Left and right hand colour images are obtained from\n230 subjects through a touchless imaging setup at a resolution of 1600\u00d71200.\nSubjects have ages from 14 to 56 years old and each contributed minimum 5\npalmar hand images from both left and right hands."}, {"title": "6.2 11k Hands Dataset", "content": "The second dataset is the 11k hands dataset. This dataset was introduced in [1], which includes hand images from 190 subjects with both left and right hands,\nand both palmar and dorsal sides at a resolution of 1600\u00d71200. This is the most\nrecent dataset in this domain, consisting of 11,076 colour hand images of various\nsubjects with ages in the range from 18 to 75. This dataset was published with\ndetailed metadata, which records not only the sides of the hands (e.g., dorsal or\npalmar, left or right) but also information about whether hand images contain\naccessories, irregularities, or nail polish."}, {"title": "7 Experimental Results", "content": "We adopt the classification accuracy as the performance evaluation metric for\ncomparisons between ViT models and traditional methods. Each fine-tuning\nprocess is conducted through 50 epochs, using an Adam optimizer at a learning\nrate of 2 \u00d7 10-5. Images are randomly cropped and horizontally flipped for data\naugmentation during training with a batch size of 32."}, {"title": "7.1 ViT models vs traditional methods", "content": "IIT Delhi dataset To benchmark our work with existing methods, e.g., [1,9,5],\nwe perform three experiments corresponding to classifying hand images of 100,\n137 and 230 subjects. For each subject, we select randomly four images for the\ntraining set and one image for the testing set. For a objective evaluation of\nthe ViT models, the experiment is repeated 10 times for each model with the\nsubjects being randomly selected each time. Results shown in Table 1 are the\naveraged accuracy and standard deviation across 10 times."}, {"title": "7.2 ViT explainability results", "content": "An illustration of the explainability results based on the DFF and Grad-CAM\nmethods is shown in Fig. 2. The PyTorch library for CAM methods [17] is used\nto investigate the internal representations of different components of the Google\nViT model. Fig. 2a presents raw dorsal right hand images of subjects \"0001046\"\n(left image), and subject \"0001573\" (right image) in the 11k hands dataset.\nFig. 2b illustrates the internal representations of the ViT when attempting\nto predict the subject of an input image, which is the image on the left of Fig.\n2a. This internal state of the ViT is extracted from the parameters (after fine-\ntuning) of the second linear component of the 11th encoder layer of the ViT (i.e.,\nthe linear component contributes to the cosine loss in Fig. 1). This component\nhelps to explain that the ViT predicts subject \u201c0001046\u201d (which is correct) based\non nail polish on the fingers. Given the input image, the left image of Fig. 2b\nillustrates the ViT's interpretation of the subject \"0001046\" while the right image\nof Fig. 2b depicts its perspective on the subject \"0001573\". Both left and right\nimages of Fig. 2b indicate nail polish as the features for recognizing the subjects\n\"0001046\" and \"0001573\", which is correct because both subjects have nail polish\non their fingers (as shown in Fig. 2a). However, the ViT model might encounter\nconfusion in distinguishing between these two subjects when relying solely on\ninformation from the mentioned linear component.\nHow does the ViT predict that the input image belongs to subject \"0001046\u201d\nrather than subject \"0001573\"? We found that this can be explained by another\ncomponent, which is the Norm of the ViT (the Norm contributes to the cosine\nloss in Fig. 1). Based on parameters of this component, the DFF method helps\nto explain (as in Fig. 2c) that the entire region of the hand in the image (with\ngreen color) is predicted to be of subject \"0001046\". The shape of the dorsal"}, {"title": "7.3 ViT models for transfer inference", "content": "We conduct several experiments to demonstrate the accuracy of the ViT models\nwhen they are transferred to perform inference on data of a different domain.\nAs the IIT Delhi dataset has only palm-side images, we choose randomly four\nleft-hand images for each subject to create a source dataset. We aim to transfer\nthe trained models to predict labels on a target dataset, which comprises four\nrandomly selected right-hand images for each subject.\nThe 11k hands dataset has both palm- and dorsal-side images of left and right\nhands, and we therefore design experiments to transfer models not only from left\nto right side, but also from palm to dorsal side, and vice versa. Specifically, we\ntrain models on left-palm images and transfer them to predict labels for right-\npalm and left-dorsal images. Similarly, we train models on right-dorsal images\nand predict labels for left-dorsal and right-palm images. We create a source\ndataset by selecting randomly four images for each subject. Each target dataset\nalso consists of four randomly selected images for each subject."}, {"title": "7.4 ViT domain adaptation", "content": "This subsection presents results of our adaptive distillation methods (methods\n1 and 2) for domain adaptation in hand image classification in comparison with\nthe baseline, i.e., the Hinton et al.'s method [20]. The temperature T equal to\n2 is selected to compute the soft loss for all experiments in this subsection. We\nconducted 5 experimental scenarios corresponding to those in subsection 7.3. In\neach scenario, we check the accuracy of the student before learning to adapt to\na new domain. We then conduct the adaptation by training the model on the\ntarget domain without any distillation, aiming to assess the extent to which the\nmodel forgets knowledge from the source domain. Experiments for comparisons\nbetween our methods 1 and 2 with the Hinton et al.'s method are followed with\nthe teacher being either the soft voting ensemble or the student's prior copy.\nThe ensemble teacher does not have a specific internal representation so that\nour method 2 is not conducted in that case.\nIn the first three scenarios (i.e., left-palm to right-palm on the IIT Delhi\ndataset, left-palm to right-palm and righ-dorsal to left-dorsal on the 11k hands\ndataset), the student model retains significant knowledge of the source domain\nafter training on the target domain even in the absence of distillation. After train-ing on the target domain, the accuracy of the student on the source and target\ndomains in these three scenarios are around 0.95 to 1.0. Catastrophic forgetting\nis minimal in these scenarios because the two domains (left and right) are closely\nrelated. The distillation methods provide improvement, although their impact is\nnot substantial in these scenarios. Detailed results for these three scenarios and\nthe left-palm to left-dorsal scenario using the 11k hands dataset are presented in\nSupplementary Material, at https://github.com/thanhthinguyen/ecmlpkdd2024.\nResults for the right-dorsal to right-palm adaptation are presented in Table 4.\nWhen training on the target domain without distillation, the student model's\naccuracy on the source domain drops to 0.472. It indicates the occurrence of\ncatastrophic forgetting, a phenomenon that is understandable given the consid-erable dissimilarity between the two domains (palm and dorsal). We observed\nthat distillation methods result in significant improvements in this scenario (and\nalso left-palm to left-dorsal scenario presented in the Supplementary Material).\nParticularly, methods 1 and 2 achieve comparable or superior accuracy on both\nthe source and target domains when compared to Hinton et al.'s method. Our\nmethods contribute to a noteworthy improvement on the source domain by mit-igating the catastrophic forgetting problem.\nThe student extracts knowledge from the ensemble teacher via their voting\nresults. Even though these voting results are stable and sometimes better than\nthe individual method, distilling knowledge from the ensemble is less effective"}, {"title": "8 Conclusion and Future Work", "content": "We have fine-tuned 6 ViT models for classification of hand images. A stable and superior performance of ViT models has been demonstrated compared with\nexisting methods. We show the usefulness of two components of ViT for the explainability purpose. Results also demonstrate the effectiveness of our distil-lation methods when training a ViT model on a new domain. Our methods help to obtain high classification performance on both source and target domain even\nthe model has no access to the source domain data. A further work can be to de-velop our methods on other types of data such as fingerprints and iris recognition\ndata for biometric applications."}, {"title": "Ethical Implications", "content": "While our hand image classification algorithms offer benefits\nin applications such as access control, identity verification, and authentication systems,\ntheir real-world implementation must be guided by ethical principles. There are risks\nassociated with the potential usage of our methods for biometric identification. They\ninclude lack of informed consent, discrimination and bias, privacy concerns, misuse and\nabuse, among other risks."}]}