{"title": "Underwater SONAR Image Classification\nand Analysis using LIME-based Explainable\nArtificial Intelligence", "authors": ["Purushothaman Natarajan", "Athira Nambiar"], "abstract": "Deep learning techniques have revolutionized image classification by mimicking hu-\nman cognition and automating complex decision-making processes. However, the deploy-\nment of AI systems in the wild, especially in high-security domains such as defence, is\ncurbed by the lack of explainability of the model. To this end, eXplainable AI (XAI)\nis an emerging area of research that is intended to explore the unexplained hidden\n'black box' nature of deep neural networks. This paper explores the application of\nthe eXplainable Artificial Intelligence (XAI) tool to interpret the underwater image\nclassification results, one of the first works in the domain to the best of our knowledge.\nOur study delves into the realm of SONAR image classification using a custom dataset\nderived from diverse sources, including the Seabed Objects KLSG dataset, the camera\nSONAR dataset, the mine SONAR images dataset, and the SCTD dataset. An extensive\nanalysis of transfer learning techniques for image classification using benchmark Con-\nvolutional Neural Network (CNN) architectures such as VGG16, VGG19, ResNet50,\nInceptionV3, DenseNet121, MobileNetV2, Xception, InceptionResNetV2, DenseNet201,\nNASNetLarge, NASNetMobile is carried out. On top of this classification model, a post-\nhoc XAI technique, viz. Local Interpretable Model-Agnostic Explanations (LIME) are\nincorporated to provide transparent justifications for the model's decisions by perturbing\ninput data locally to see how predictions change. Furthermore, Submodular Picks LIME\n(SP-LIME) a version of LIME particular to images, that perturbs the image based on the\nsubmodular picks is also extensively studied. To this end, two submodular optimization", "sections": [{"title": "I. INTRODUCTION", "content": "Imaging SONAR produces a reflectivity estimate of a portion of the ocean bottom\nusing sound waves, hence, it is widely used to communicate, navigate, measure\ndistances, and find objects on or beneath the water's surface [1]. In contrast to\noptical imaging, SONAR is preferred for underwater imagery because optical imaging\nsystems rely on light conditions for imaging, but SONAR does not rely on ambient\nlight. Therefore, it can operate effectively in low-light or even no-light conditions.\nFurthermore, SONAR can be used to identify the differences between the highlight\nand the shadow regions, which makes SONAR the best option for underwater surveil-\nlance and investigation whenever optical systems fail to do the job [2]. Developments\nin autonomous underwater vehicles (AUVs) and remotely operated vehicles (ROVs)\nhave enabled the implementation of underwater acoustic imaging equipment within\na versatile framework on which the SONAR imaging equipment is placed below the\nunderwater vehicles.\nAlthough AUVs can effectively collect data, they lack the capability to autonomously\ndetect or categorize objects. Hence, it requires human assistance to manually review\nand categorize. For a naval base tasked with monitoring coastlines for potential\nthreats such as enemy submarines, maritime monitoring has to be as quick as pos-\nsible, but this multistage manual operation extends the response time and increases"}, {"title": "", "content": "the threat of a foreign intruder and national security. In this regard, the automation\nof underwater image analysis on acoustic images can enhance the mission's autonomy\nwhile also reducing time, cost, and damage.\nTraditional machine learning algorithms, such as Support Vector Machines (SVM) [3]\nand decision trees [4], are employed for object recognition in SONAR images. How-\never, the effectiveness of these shallow techniques is limited, yielding an accuracy\nof up to 92% [5], [6]. Later, deep learning algorithms like Convolutional Neural\nNetworks (CNN) improved the accuracy by utilizing more computational resources,\ngetting it to about 94.40% [7]. The utilisation of transfer learning, wherein a pre-\ntrained model serves as the base model, further boosted the accuracy to 97.33% [8].\nNevertheless, to apply such Artificial Intelligence (AI) systems in real-world sce-\nnarios of high-security concerns i.e. military/ defence, certain limitations curb the\napplication of deep learning (DL) systems in the wild. One big challenge is the\nlack of availability of big data. Additionally, to come up with promising practical\nsolutions for such confidential data, it is quite essential to have the validation of\nthe results by conclusive interpretable evidence. However, this is where most of the\nexisting DL models fail since they act as a \u2018black-box\u2019while predicting the results.\nIn such circumstances, it is crucial to enhance the reliability, trustworthiness, and\ninterpretability of any deep neural network (DNN) model so that humans based\non their situational awareness, can make decisions that involve interpreting the\npredictions from the model [9]. This can help in detecting abnormalities, aiding in\nbetter decision-making, reducing false alarms, facilitating training, and maintaining\ntransparency [10], [11]. To this end, Explainable Artificial Intelligence (XAI) is an\nemerging area of research in Machine learning that is intended towards exploring\nthe unexplained hidden \u201cblack box\" nature of deep neural networks [12].\nIn this study, we address the black-box nature of deep learning model by incorpo-\nrating XAI tools for underwater sonar image classification. The proposed underwater"}, {"title": "", "content": "XAI image classification pipeline consists of two stages: First, the performance of\nvarious backbone classification models such as VGG16, VGG19 [13], ResNet50 [14],\nInception V3 [15], DenseNet121, DenseNet201 [16], MobileNetV2 [17], Xception\n[18], InceptionResNetV2 [19], NASNetLarge, and NASNetMobile [20] are analysed\nvia Transfer learning technique. The significance of sampling techniques (random vs.\nstratified) as well as data-balancing strategies (oversampling vs. undersampling) are\nalso studied to see their impact on model performance.\nSecond, two popular explainable AI tools i.e. Local Interpretable Model-Agnostic\nExplanation (LIME) and submodular Picks LIME (SP-LIME) are employed to in-\nterpret the model decisions in a human-complaint manner. LIME and SP-LIME\nare perturbation-based techniques, suitable for most data types and images, respec-\ntively [21]. Two different sub-modular optimization algorithms viz. quickshift [22]\nand Simple Linear Iterative Clustering (SLIC) [23] are leveraged towards perturbing\nsuper-pixels in SP-LIME. Extensive analysis of the aforementioned XAI models are\ncarried out to comprehend the model predictions, traits of the explainer predictions,\nimpact of the hyperparameters on the explainer models and state-of-the-art com-\nparisons. To the best of our knowledge, the literature on XAI for SONAR imagery\nanalysis systems is scarce, making our model one of its early works in this field.\nAll the studies are carried out on a novel custom-made SONAR dataset curated\nin-house by consolidating various publicly available sonar datasets i.e. Seabed Objects\nKLSG [24], SONAR Common Target Detection Dataset (SCTD) [25], Camera\nSONAR dataset [26], and Mine SONAR images [27]. The key contributions of this\npaper are summarised as follows:\n\u2022 Development of a new tailored SONAR dataset by combining multiple publicly\navailable SONAR image datasets.\n\u2022 A comparative study to assess how well various benchmark models perform with\nTransfer Learning for SONAR image classification."}, {"title": "", "content": "\u2022 Brief study on the significance of data sampling techniques and data-balancing\nschemes during model training.\n\u2022 Incorporating Explainable Artificial Intelligence (XAI) using LIME and SP-\nLIME, powerful tools offering visual explanations for underwater SONAR image\nclassification results, one of its first kind to the best of our knowledge.\n\u2022 Extensive quantitative and qualitative analysis on the explanation from various\nmodels, the impact of sampling/data balancing strategies and hyperparameters\nand state-of-the-art comparison.\nThe forthcoming sections of this paper are organized as follows: The discussion of\nSONAR imaging and related work is presented in Section II. The development of\nthe custom dataset used for training is described in Section III. Classification using\npre-trained models and different Explainable Artificial Intelligence (XAI) method-\nologies, with a special focus on LIME and SP-LIME is described in Sections IV\nand V, respectively. The experimental findings are showcased in Section VI. Finally,\ndiscussion and conclusion are drawn in Section VII."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "In this section, we briefly describe the working principle of SONAR imaging, the\nliterature review, and the motivation behind this research work."}, {"title": "A. SONAR working Principle", "content": "Sound navigation and ranging (SONAR) uses sound waves to capture the scene\non the ocean floor. SONAR has two major components, the transmitter and re-\nceiver [28]. The transmitter transmits acoustic waves at a grazing angle to the\nbottom. These acoustic waves propagate through the water, bounce back by touching\nthe objects or seafloor in their path, and then the receiver collects them, which\nilluminates a region on the seafloor. On the other hand, if there are no objects,"}, {"title": "B. Underwater Object Classification", "content": "Different natural and artificial sounds, such as thermal noise, shipping noise,\nearthquakes, volcanic eruptions, and submarines, consistently disrupt the underwater\nacoustic environment. As a result, noise distorts and blurs the captured acoustic\nimage [32]. Furthermore, acoustic images generated by SONAR have a lower res-\nolution compared to optical images, and this resolution varies depending on the\ndistance between the sensor and the detected object [33]. Despite these challenges,\nthe progress made in the field of automatic object detection in SONAR imagery has\nbeen impressive, considering the relatively short duration of time. Earlier versions\nof underwater object detection and classification systems depended on a two-step\ndetection process [34]. The first step involves collecting data, while in the second\nstep, humans manually classify the collected data using predefined classes based on\nthe available templates [35].\nLangner et al. [36] utilised probabilistic neural networks and pattern-matching\ntechniques based on clustering algorithms for automatic object detection and clas-\nsification on side scan sonar images. Myers et al. [35] used template matching tech-\nniques to compare the target signature made from a simple acoustic model with\na picture of the object that needs to be classified. H.Xu et al. [5] developed an\nAdaboost cascade classification model based on support vector machines (SVM) and"}, {"title": "", "content": "achieved an accuracy of about 92%. The statistical approach [37] utilises the HS-\nLRT (highlight-shadow likelihood ratio test) to identify and emphasise areas with\nboth shadow and highlight and subsequently, the region of interest is determined\nbased on these shadow and highlight regions. Finally, an SVM is employed to detect\nthe desired regions of interest.\nIn 2017, Mckay et al. [38] began utilising deep neural networks for SONAR image\nclassification and also utilised transfer learning with the pre-trained model VGG16\nand got an impressive accuracy of 93% on their custom side scan SONAR (SSS)\ndataset, and improved the accuracy further to 97.7% by including semisynthetic data\ngenerated from optical images. In 2018, Fuchs et al. [39] also utilized transfer learning\nfor sonar image classification on aracati dataset with ResNet-50 as a backbone model\nand also did a comparative study with traditional machine learning approaches\nand achieved an accuracy of 88%. In 2020, Pannu et al. [40] utilised CNN for the\nclassification of synthetic aperture radar (SAR) images, adeptly addressing data\nlimitations through strategic augmentation. Transitioning to the M-STAR dataset,\na deep learning-based approach achieves an impressive 80% accuracy, further boosted\nto 97% after meticulous balancing.\nThe comparative study by Du et al. [41] leveraged transfer learning for sonar image\nclassification on the Seabed Objects KLSG dataset with the pre-trained models,\nnamely AlexNet, VGG-16, GoogleNet, and ResNet101, which achieved a maximum\naccuracy of 94.81%. A study by Chungath et al. [8] also used transfer learning\nwith a pre-trained model ResNet-50 to classify sonar images, achieved an impressive\naccuracy of 97.33%, and also showed the ways to train an image classification model\nwith very few images using few-shot learning-based deep neural networks.\nThe comprehensive survey by Steiniger et al. [42], Domingos et al. [43], and Neu-\npane et al. [44] explored past and current research involving deep learning for feature\nextraction, classification, detection, and segmentation of side scan and synthetic"}, {"title": "", "content": "aperture SONAR imagery. Their work provides an insightful overview, serving as a\ncornerstone for understanding the dynamic applications of computer vision in this\nspecialised underwater water domain."}, {"title": "C. Explainable AI for SONAR Image Analysis", "content": "Most of the existing deep learning models are black boxes in nature whose predic-\ntions cannot be interpreted or understood by humans. The integration of explainable\nAI offers a transparent lens, providing nuanced visual explanations for the predic-\ntions [12]. It helps verify the performance of the deep learning models and also\nenhances user trust in those models. Nguyen et al. conducted a thorough compar-\nison study that looked at how well explainable AI approaches such as LIME [21],\nSHAP [45], and Grad-CAM [46] work in a wide range of industrial settings with\nimages [47]. Further, in other domains such as remote sensing, medicine, etc., \u03a7\u0391\u0399\ntechniques are employed to verify the performance of deep learning models [48], [49].\nHassan et al. [50] used XAI on ultrasound and MRI images to classify prostate cancer\nin men. It is also used to study several respiratory disorders, including COVID-19,\npneumonia, tuberculosis and skin lesion classification, to help doctors and clinicians\ncome to strong and logical conclusions about deep learning models for detection\nand classification [48], [51]. Bennet et al. [52] created a neural-symbolic learning\nframework utilizing a CNN model with a latent space predictor to classify images.\nThis framework gives information about the predicted image through an explainable\nlatent space. Similarly, it is also used to explain the predictions of complex image\ncaptioning models, which visually depict the part of the image corresponding to a\nparticular word in the caption through a visual mask [53].\nThe XAI programme [54] by DARPA underscores the ongoing efforts to enhance\nunderstanding, trust, and management of AI systems in the realm of defence. During\nthe period of this programme, from 2015 to 2021, 11 XAI teams explored various"}, {"title": "", "content": "machine learning approaches, i.e. tractable probabilistic models, causal models and\nexplanation techniques leveraging state machines generated by reinforcement learn-\ning algorithms, Bayesian teaching, visual saliency maps and GAN dissection. [9].\nRecent work on deep learning-based explainable target classification was studied by\nPannu et al. [40]. In that work, the explainable AI tool LIME was leveraged to verify\nthe performance of the synthetic aperture radar(SAR) image classification models.\nThere are very few research works with explainable AI for underwater SONAR\nimagery. To the best of our knowledge, the only known work in the domain is by [55],\nwherein Walker et al. developed a LIME-based \u03a7\u0391\u0399 classification model for seafloor\ntexture classification. Contrary to that work, our work addresses a reliable multi-\nclass classification model by comparing different backbone models using transfer\nlearning approach, and also by addressing data balancing and sampling strategies.\nAdditionally, XAL techniques LIME and SP-LIME are incorporated to interpret the\npredictions of the top-performing model, in a human-compliant manner. Further,\ndetailed quantitative and qualitative analyses on the impact of different hyper-\nparameters and computational constraints are also investigated in this work."}, {"title": "III. DATASET FOR LEARNING", "content": "The foundation of any machine learning model is the training dataset. The publicly\navailable SONAR dataset is limited mainly because SONAR, primarily utilized for\ndefence applications, generates sensitive images that are kept confidential. Hence,\ninitial studies in SONAR image classification employed private datasets of underwa-\nter acoustic images. Therefore, the gathering of data is vital in this scenario. This\nsection details the experimental setup for acquiring and preparing the dataset for\nour studies."}, {"title": "A. Data Acquisition", "content": "Acquiring data in an underwater environment necessitates coordinated efforts from\nmultiple agencies and departments. The collection and labelling of a substantial\namount of acoustic data for educational purposes is an expensive and time-consuming\nprocess. The confidential nature of the SONAR data results in sparse publicly\navailable datasets with very few classes. Hence, we develop a new custom-made\ndataset by consolidating from four different publicly available open-source SONAR\ndatasets, listed as follows:\n1) The primary dataset, referred to as Seabed Objects KLSG [24], comprises\n1190 Side Scan SONAR images with a total of 385 images of shipwrecks, 36\nimages of drowning victims, 62 images of planes, 129 images of mines, and 578\nimages of the seafloor. The authors gathered these real SONAR images with\nthe assistance of various commercial SONAR suppliers, including Lcocean [56],\nKlein Martin [57], and EdgeTech [58] accumulated over a period of ten years. A\nfew classes with a total of 1171 images with a variety of dimensions, along with\naugmented images excluding mines, are accessible to the public through the\nlink [59], with the purpose of supporting academic research in this particular\ndomain.\n2) The second dataset named Sonar Common Target Detection Dataset\n(SCTD)1.0, was developed by Tsinghua University, Fudan University, Shang-\nhai University, Hohai University, Jiangxi University of Technology, et al. [25],\nconsists of a total of 596 images belonging to 3 classes. However, only 317 im-\nages, specifically 57 images of planes, 266 images of shipwrecks, and 34 images\nof drowning victims, each with different dimensions, are publicly accessible\nthrough the link [60].\n3) The Camera SONAR dataset [26], published by Kei Terayama et al. con-"}, {"title": "", "content": "sists of 1334 underwater SONAR and camera photos of fishes with a dimension\nof 256x512. For this study, we will only be using the SONAR images.\n4) The dataset referred to Mine SONAR images provided in the Roboflow\nuniverse by Phan Quang Tuan comprises 167 SONAR images depicting mines\nwith a dimension of 416x416 [27]."}, {"title": "B. Data Splitting", "content": "In a supervised learning environment, the objective is to construct a model that\ncan accurately predict both the input sample and unseen samples. To evaluate the\nmodel's performance, we divide the dataset into three mutually exclusive sets: train,\nvalidation, and test sets [61]. The model is developed using the training set, and then\nthe developed models are fine-tuned using the validation set. Finally, the model's\nperformance is evaluated using the test dataset. Splitting the dataset has a major\nimpact on the model's quality by reducing both bias and variance [62]. A poor data\nsplit might lead to poor model performance, particularly in imbalanced datasets\nsuch as the one in the current case. This problem of data imbalance is common\nin many real-time applications where the number of data points in some classes is\nsignificantly higher than the other classes.\nIn our case, the majority class has 1334 images, and the minority class has only\n34 images. In this situation, we should ensure that the train and test split have\napproximately equal numbers of samples in all the classes to maintain the class\ndistribution intact. Otherwise, the model might overfit. Fig. 1 represents the class-\nwise distribution of our custom dataset. As seen, the fish class dominated with 44.6%\nof the dataset, while the human class represented only 1.1% of the dataset.\nThe task of splitting the dataset for a deep learning network can be seen as\na sampling problem, where the given dataset D containing N data samples needs"}, {"title": "", "content": "to be partitioned into three mutually disjoint subsets, so we split the dataset to\ntrain, validation and test datasets. As per [61], if the dataset is imbalanced, simple\nrandom sampling cannot represent and maintain the class distribution for all the\nclasses in the dataset, but statistical sampling-based data splitting approaches, such\nas stratified sampling [63], work better over random sampling because the core idea\nis to inspect the internal structure and distribution of each subclass of the dataset D\nand utilise this information to divide the dataset into a set of relatively homogeneous\nsample groups. In the current problem, both simple random sampling and stratified\nsampling are applied to the available dataset in the ratio of 70:15:15 among train,\nvalidation, and test splits, with simple random sampling randomly selecting images\nand stratified sampling selecting an image from a class with probability inversely\nproportional to the size of that class."}, {"title": "IV. METHODOLOGY: TRANSFER LEARNING-BASED IMAGE CLASSIFICATION,\nDATA BALANCING AND LIME-BASED EXPLANATION", "content": "Training a deep neural network (DNN) from scratch with random weights demands\nsubstantial computational resources, extensive datasets, and considerable processing\ntime [64]. In the other hand, Transfer learning trains a deep neural network (DNN)\nwith less data and in less time all while getting better results [65]. This is beneficial\nwhen training on intensive image datasets, where the challenges associated with\ncollecting and labelling data, given its cost and time-intensive nature, and privacy\nconcerns surrounding real user data make its utilization increasingly challenging [66]."}, {"title": "A. Transfer Learning", "content": "Transfer learning facilitates the rapid prototyping of new machine learning models\nby leveraging the weights and biases of pre-trained models from a source task [67].\nThis transfer of weights, essentially the transfer of knowledge, offers notable ben-\nefits [68] in several ways, not only in speeding up the convergence process during\ntraining but also in establishing a higher initial accuracy. This is especially ad-\nvantageous when dealing with smaller datasets, demonstrating the efficiency and\neffectiveness of transfer learning [8].\nConsider a domain, denoted as D, with a feature space X, and a marginal probabil-\nity distribution P(X), where x is a set {x1, x2,..., Xn} belonging to X. Represented\nas_ D = {X,P(X)}, let T be a task defined as {Y, f(x)}, where Y is the label\nspace and f(x) is the target predictive function. Suppose Ds and Ts as the source\ndomain and corresponding learning task, and Dt and Tt as the target domain and\ntarget learning task, respectively. Let Ds = {(X1,YS1), (XS2, YS2), ..., (XSn, YSn)}\nencompass the elements of the source domain data, where each xsi \u2208 Xs represents\na data point and ys; \u2208 Ys denotes its class label. We want to make the target"}, {"title": "", "content": "predictive function fr(.) better for the learning task Tt by using data from Ds and\nTs, where Ds \u2260 Dt or Ts \u2260 Tt.\nThe transfer learning process begins with feature extraction from both the source\ndomain Ds and the target domain Dt. Instead of initializing the network with\nrandom weights and biases, the idea is to initialize the weights from the source\ndomain (Ws) and adjust the weights to the target domain Wt. The target weights\nWt are optimized by backpropagation and gradient descent using the target data Dt\nand are utilized it to predict the target learning task (Tt). According to Chungath\net al. [8] and Ribani et al. [65], this process involves making small changes to the\nnetwork weights so that the model can understand high-level features that are unique\nto the dataset. This leads to better performance than direct training."}, {"title": "B. Handling Data Imbalance", "content": "As explained in Section III-B, there exists a class imbalance problem in the dataset\ndue to high variance in the number of samples per class. This issue is tackled\nthrough a dual strategy: Oversampling and Undersampling techniques [69] (refer\nFig.3). Oversampling refers to the technique of adding more images to minority\nclasses, whereas undersampling refers to the technique of randomly discarding some\nimages from the majority class [70]. In our case, the majority classes are fishes,\nseafloor, and ships, while the minority classes are plane, human, and mine. To address\nthe scarcity of plane, human, and mine images. We employ image augmentation\ntechniques such as flipping, rotation, cropping, adjusting the brightness, contrast,\nrandom erasing, noise injection, mixing images, adding blur, sharpness, and colour\njittering to generate synthetic images, as an instance of oversampling [71] (Refer to\nFig.4). Conversely, to harmonise the abundance of ship, fish, and seafloor images,\nwe randomly discard some with equal probabilities to balance image numbers for\neach class as an undersampling approach [72], [73]. The number of images before\nand after augmentation is mentioned in Table II.\nHowever, it's crucial to note that Kotsiantis et al. [69] and Chawla et al. [74]\ncautioned against random oversampling due to its potential to heighten the risk of\noverfitting. This arises from the replication of exact copies of minority class examples,\nleading to the construction of seemingly accurate rules that may inadvertently be\noverly specific to individual replicated instances. Furthermore, in scenarios where the\ndataset is already sizeable but imbalanced, oversampling can introduce an additional"}, {"title": "", "content": "computational burden. This underscores the importance of a nuanced approach\nto navigating the trade-offs associated with image oversampling techniques in the\ncontext of imbalanced datasets."}, {"title": "C. eXplainable \u0391\u0399 (\u03a7\u0391\u0399)", "content": "As explained earlier, deep learning models are opaque by nature; they only predict\nthe target with a probability score. To interpret such \u2018black-box\u2019models, Explain-\nable AI (XAI) tools facilitates in gaining human reliance on AI technology. \u03a7\u0391\u0399\naddresses bias understanding and fairness by navigating the bias-variance trade-off\nin AI/ML models [75]. This promotes fairness and aids in mitigating bias during"}, {"title": "D. Local interpretable Model-Agnostic Explanations (LIME) as an \u03a7\u0391\u0399\ntool", "content": "Local Interpretable Model-Agnostic Explanations (LIME) is a novel explanation\ntechnique used to explain the predictions made by any classifier in a manner that\nis both interpretable and faithful by training a locally interpretable model around\nthe prediction [21]. It provides faithful explanations for predictions made by any\nclassifier or regressor. This is achieved through the local approximation of the model,\nenabling the use of an interpretable model for enhanced understanding and in-\nterpretability [21]. In other words, LIME improves our model's interpretability by\ntweaking input data locally to see how predictions change. Tweaking the data locally\nexplains a single prediction for that instance, while global tweaking looks at the\nmodel's behaviour across all data. The idea behind local explanations is to create\nan interpretable representation of an underlying model, making its decision-making\nprocess more understandable."}, {"title": "", "content": "The mathematical formulation of the LIME explainer is detailed below: Let f :\nRd \u2192 R denote the model to be explained. x \u2208 Rd is the original representation of the\ninstance being explained. In a classification context, f(x) represents the probability\n(or a binary indicator) that the instance x belongs to a specific class. Additionally,\nlet \u03c0x(z) be a proximity measure between an instance z and x, defining the locality\naround x. Consider the loss function L(f, g, \u03c0x) as a representation of how accurate\nthe explanation function g is at approximating f within the locality defined by \u03c0x\u00b7\nThe loss function L for locally weighted squared loss is expressed as in (1)\n$L(f,g, \\pi_x) = \\Sigma \\pi_x(z) \\cdot (f(z) - g(z_0))^2$\n$\nz,z_0 \\in Z$\n\nwhere Z is a dataset containing sampled instances z, and f(z) represents the labels\nobtained from the original model. The term zo is a binary indicator of perturbed\nsamples used to generate the original representation of instances. The function g(20)\nis selected to be a class of linear models, specifically g(zo) = wg. 20. This formulation\ncaptures the essence of the locally weighted squared loss, emphasizing the importance\nof each sampled instance based on its proximity to the original instance x in the\nfeature space. The term \u03c0\u2082 in (1) functions as a locality measure and is computed\nas follows,\n$\\pi_x(z) = exp\\left(- \\frac{D(x,z)^2}{\\sigma^2}\\right)$\n\nwhere D serves as the distance metric (e.g., cosine, euclidean distance) between\nthe two instances x and z. The bandwidth of the kernel \u03c3 is a parameter that requires\ncareful selection. By resolving the following optimization (3), LIME determines the\nexplanation it produces, where \u00a7 represents the local behavior of the complex model\nf around the instance x."}, {"title": "", "content": "$f(x) = \\underset{g\\in G}{arg \\underset{g\\in G}{min }} L(f, g, \\pi_x) + \\Omega(g)$\n\nwhere L(f, g, \u03c0x) represents the loss function measuring how well interpretable\nmodel g approximates black-box model f for a perturbed sample \u03c0\u2081 and \u03a9(g) is the\ncomplexity measure quantifying the simplicity of interpretable model.\nTo ensure both interpretability and reliability, the goal is to minimize the loss\nfunction L(f, g, \u03c0x), while restricting the complexity of the explanation \u03a9(g) to a\nlevel that is interpretable by humans. This is achieved by penalizing the explanation\nfunction g if it becomes excessively complex in order to maintain the complexity of\nthe surrogate model within acceptable limits."}, {"title": "E. Submodular Picks Local interpretable Model-Agnostic Explanations\n(SP-LIME) for images", "content": "The classifier may represent the image as a tensor with three colour channels per\npixel, but an interpretable representation for our image classification task might\nbe a binary vector indicating the \u201cpresence\u201d or \u201cabsence\" of a contiguous patch of\nsimilar pixels (a super-pixel). The process of explaining model predictions through\nLIME starts with the deliberate selection of an instance, or a group of instances (in\nour case, its super-pixels), that are worthy of interpretation. The selected instance's\nfeatures are then subtly altered to introduce perturbations, resulting in a diverse\nset of samples. Subsequently, the black-box model generates predictions for these\naltered samples.\nSince perturbing the entire interpretable representation in our case would require\nmore time and computational resources, we instead perturb a super pixel because\ncertain regions of the images can identify a class; altering whether or not that region\ncontributed to that class can produce the same results as altering the input pixels."}, {"title": "", "content": "Hence, another variant of LIME, viz. Submodular Picks Local interpretable Model-\nAgnostic Explanations (SP-LIME), which is customized for images. SP-LIME is a\nmethodology designed to tackle the above challenges by employing submodular op-\ntimization [21]. This approach systematically identifies a collection of representative\ninstances along with their corresponding explanations. Hence, the selection process is\nessential for improving the clarity and dependability of the model, particularly when\nthere are constraints on computational resources. There are numerous algorithms\nbased on clustering, watershed, energy optimization, and graph methods available\nfor submodular optimisation, specifically for super-pixel segmentation [82]. In this\nstudy, we have selected the quickshift [22] and SLIC [23] algorithms to perform the\ntask."}, {"title": "V. EXPERIMENTAL SETUP", "content": "In this section, we briefly describe the implementation details and the evaluation\nmetrics used to evaluate the performance of the classification models."}, {"title": "A. Implementation Details", "content": "In this study, we select prominent benchmark models, namely VGG16, VGG19\n[13], ResNet50 [14], Inception V3 [15], DenseNet121, DenseNet201 [16], Mo-\nbileNetV2 [17], Xception [18], InceptionResNetV2 [19], NASNetLarge, and NAS-\nNetMobile [20] to carry out transfer learning. To conduct a comparative analysis and\nevaluate their respective performances, both small models with low computational\nresource requirements and large models with high computational resource require-\nments are chosen.\nEach pre-trained model is enhanced by adding a flattening layer to transform the\nmulti-dimensional output into a one-dimensional vector,a dense layer comprising\n1024 neurons with a relu activation function, a dropout rate of 0.25, one more dense"}, {"title": "", "content": "layer comprising 512 neurons with a relu activation function, a batch normalisation\nlayer, and again a 0.25 dropout to capture complex relationships within the data.\nThen, finally, a dense output layer with six neurons (tailored for our task) and a soft-\nmax activation function is added to"}]}