{"title": "One to rule them all: natural language to bind communication, perception and action", "authors": ["Simone Colombani", "Dimitri Ognibene", "Giuseppe Boccignone"], "abstract": "In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans.\nThis paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback.\nThe Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands like 'Go to the kitchen and pick up the blue bottle on the table'. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot's adaptability, task execution efficiency, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot's ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue.\nThis system has been implemented on RoBee, the cognitive humanoid robot developed by Oversonic Robotics, showcasing its adaptability and potential for integration across diverse environments. By leveraging LLMs and semantic mapping, the architecture enables RoBee to navigate and respond to real-time changes.", "sections": [{"title": "1. Introduction", "content": "The integration of LLMs in robotic systems has opened new avenues for autonomous task planning\nand execution [2, 3]. These models demonstrate exceptional natural language understanding and\ncommonsense reasoning capabilities, enhancing a robot's ability to comprehend contexts and execute\ncommands [4, 5]. However LLMs are not be able to plan autonomously, they need to be integrated in\narchitectures that enable them to understand the environment, the robot capabilities and state [6]. This\nresearch aims to empower robots to comprehend user requests and autonomously generate actionable\nplans in diverse environments.\nThe efficacy of these plans relies on the robot's understanding of its operating environment [7]. To\nbridge this gap, our work employs scene graphs [8] as a semantic mapping tool, offering a structured\nrepresentation of spatial and semantic information within a scene."}, {"title": "2. Related works", "content": "A substantial body of literature explores the utilization of LLMs for robotic task planning [4, 5].\nLLM for robot planning Recent works highlight the potential of Large Language Models (LLMs)\nin robotic planning [17, 18, 19]. DEPS [20] introduces an iterative planning approach for agents in\nopen-world environments, such as Minecraft. It utilizes LLMs to analyze errors during execution and\nrefine plans, improving both reasoning and goal selection processes. However, this approach has been\nprimarily developed and tested in virtual environments, with notable differences in comparison to\nreal-world settings due to the dynamic and unpredictable nature of physical environments. Additionally,\nDEPS does not leverage previous issues and solutions but relies solely on feedback from humans and\nvision-language models (VLMs)."}, {"title": "3. Architecture", "content": "Our system is based on two components:\n\u2022 Perception Module: it is responsible for sensing and interpreting the environment. It builds\nand mantains a semantic map in the form of a directed graph that integrates both geometric and\nsemantic information.\n\u2022 Planner Module: it takes the information provided by the Perception Module to formulate plans\nand actions that allow the robot to perform specific tasks.\nFigure 1 show how these components interact to allow the robot to understand its environment\nand act accordingly to satisfy user requests. The Perception module uses data provided by the robot's\nsensors to supply the semantic map to the Planner module, which in turn processes it to generate\nspecific action plans. In what follows we precisely address the Planner Module while details on the"}, {"title": "3.1. Planner module", "content": "The architecture of the Planner module is designed to translate user requests, expressed in natural\nlanguage, into specific actions executable by a robot. This module is responsible for understanding\ninstructions, planning appropriate actions, and managing the execution of those actions in a dynamic\nenvironment. The Planning module is composed by five sub-modules:\n\u2022 Task Planner: Translates user requests, expressed in natural language, into a sequence of\nhigh-level skills.\n\u2022 Skill Planner: Translates high-level skills into specific, low-level executable commands.\n\u2022 Executor: Executes the low-level actions generated by the Skill Planner.\n\u2022 Controller: Monitors the execution of actions and manages any errors or unexpected events\nduring the process.\n\u2022 Explainer: Interprets the causes of execution failures by analyzing data received from the\nController and provides suggestions to the Task Planner on how to adjust the plan.\nThe architecture of the planner module is shown in Figure 2. The main component of the system is\nthe Task Planner, which receives the user's request and translates it into a list of high-level \"skills\"\nthat represent the robot's capabilities. These skills include actions such as \"PICK\" (grasp an object),\n\"PLACE\" (place an object), and \"GOTO\" (move to a position)."}, {"title": "3.1.1. Task Planner", "content": "The decision-making process of the Task Planner is driven by a policy, which is implemented as a\nLLM. A policy is a strategy or rule that defines how actions are selected based on the current state or\ncontext, [27].\nTask Planner is implemented using the ReAct framework [10], which alternates between reasoning\nand action phases during the process. In the reasoning phase, the Task Planner can access various\n\"perception\" actions to gather information from the environment, such as the semantic map and the\ncurrent state of the robot, and can execute one or more \"skill\" actions to perform physical actions.\nThe classical idea of ReAct is to augment the agent's action space to \\(A = A \\cup L\\), where L is the\nspace of language-based reasoning actions. An action \\(\\hat{a}_t \\in L\\), referred to as a \"thought\" or reasoning\ntrace, does not directly affect the external environment but instead updates the current context \\(C_{t+1} =\n(C_t, a_t)\\) by adding useful information to support future decision-making [10]. In the classical idea\nthere could be various types of useful thoughts, such as decomposing task goals and creating action\nplans, injecting commonsense knowledge relevant to task solving, extracting important parts from\nobservations, tracking progress and transitioning action plans, handling exceptions and adjusting action\nplans, and so on, but always without modifying the physical environment, only embedding it within\nthe context. Interestingly, this approach mixes reasoning and action in a flexible manner. In the future,\nwe will analyse the potential of this approach also connecting to the planning-to-plan [28, 29] and\nmeta-reasoning [30, 31, 32] concepts.\nIn our work, we augment the agent's [33] action space with two types of actions:\n\u2022 A skill action \\(a_t \\in A_{skill}\\), which involves physically interacting with the environment, such\nas manipulating objects or navigating. The result of a skill action provides new feedback that\nupdates the current context.\n\u2022 A perception action \\(a_t \\in A_{perception}\\), which involves accessing information from the environment,\nsuch as querying the semantic map or sensors, and integrating that information into the context.\nThe augmented action space is defined as:\n\\[A = A_{skill} \\cup A_{perception} \\cup L\\]\nThus, the LLM serves as the policy \\(\\pi\\) that selects different types of actions from the augmented action\nspace and dynamically adapting the current context \\(c_t\\) used to plan based on real-time information and\nreasoning.\nFormal Description: The Task Planner's policy \\(\\pi\\), represented by the LLM, can be formalized as a\nfunction that maps the current context \\(c_t\\) to an action \\(\\hat{a}_t\\) from the augmented action space A:\n\\[\\pi: C \\rightarrow A, \\quad \\pi(c_t) = \\hat{a}_t\\]\nWhere:\n\u2022 C is the set of all possible contexts.\n\u2022 A is the augmented action space \\(A = A_{skill} \\cup A_{perception} \\cup L\\).\n\u2022 \\(C_t\\) represents the current context at time t, which includes the state of the robot, the environment,\nand any past actions or thoughts.\n\u2022 \\(\\hat{a}_t \\in A\\) is the action chosen by the policy, which can be a skill action \\(a_t \\in A_{skill}\\), a perception\naction \\(a_t \\in A_{perception}\\), or a reasoning trace \\(\\hat{a}_t \\in L\\).\nThe context \\(c_t\\) is updated based on the chosen action:\n\u2022 If \\(\\hat{a}_t \\in L\\) (a reasoning action), the context updates to:\n\\[C_{t+1} = (C_t, \\hat{a}_t)\\]\nThis represents the thought process, where reasoning contributes new information without\naffecting the external environment."}, {"title": "3.1.2. Skill Planner", "content": "Once a high-level request for the execution of a skill is made, the Skill Planner is responsible for\ntranslating the high-level skills, provided by the Task Planner, into sequences of low-level commands\nexecutable by the robot. While the Task Planner focuses on understanding natural language and creating\na general plan, the Skill Planner deals with the specific details of how each skill should be executed,\nconsidering the robot's state and the environment.\nLet a skill be represented in the following general form, defined by the Task Planner with specific\nsyntax:\n\\[SKILL\\_NAME(param_1, param_2, ..., param_n)\\]\nWhere:\n\u2022 SKILL_NAME is the name of the skill to be executed (e.g., PICK, PLACE, GOTO).\n\u2022 \\(param\\_1, param\\_2, ..., param\\_N\\) are parameters for the skill, such as the object to\nmanipulate or the destination to navigate to.\nUsing a strict syntax ensures that the Skill Planner can correctly interpret the high-level commands\nwithout ambiguity. For instance, a natural language command like \"Move near the table and grab the\nbottle\" would lack precision. The Skill Planner needs concrete parameters for the robot to act effectively.\nSkill Planner workflow: The Skill Planner operates by performing three functions:\n1. Precondition Verification: Before translating a skill into low-level commands, the Skill Planner\nverifies that the necessary preconditions for execution are met. Let \\(s_t\\) represent the current state of the\nrobot and the environment at time t, and \\(P(skill, s_t)\\) denote a function for every skill that evaluates the\npreconditions for a given skill. The precondition check can be expressed as:\n\\[P(skill, s_t) = \\begin{cases} 1, & \\text{if all preconditions are met} \\\\ 0, & \\text{otherwise} \\end{cases}\\]\nFor example, before executing the PICK skill, the following checks may be performed:\n\u2022 The object is visible by the robot.\n\u2022 The object is reachable for the robotic arm.\n\u2022 The robotic arm is free.\nIf any of these conditions are not met (\\(P(skill, s_t) = 0\\)), the Skill Planner reports a failure to the Task\nPlanner.\n2. Target nodes extraction: Based on the parameters of the skill, the Skill Planner extracts the\ntarget nodes from the semantic map M, which contains geometric and semantic information about"}, {"title": "3.1.3. Executor", "content": "The Executor is responsible for directly interacting with the robot's hardware to execute the commands\nprovided by the Skill Planner. It translates the low-level commands into physical actions by controlling\nvarious hardware elements such as motors, robotic arm grippers, and other actuators required for task\nexecution.\nLet the set of low-level commands generated by the Skill Planner be represented as above, i.e.,\n\\(cm_1, cm_2, ..., cm_k = CM(skill, node, s_t)\\), where \\(CM(skill, node, s_t)\\) defines the sequence of com-\nmands based on the skill, the target node, and the current state of the robot and the environment.\nThe Executor is tasked with executing these commands on the physical robot. Let the state of the\nrobot at time t be denoted by \\(h_t\\), and the function that maps a low-level command \\(c_i\\) to an effect on the\nrobot's state be denoted as \\(H(cm_i, h_t)\\). The execution of a command at time t can be described as:\n\\[h_{t+1} = H(cm_i, h_t)\\]\nwhere \\(h_{t+1}\\) is the updated state after executing the command \\(cm_i\\). This process is repeated for each\ncommand in the sequence \\(\\{cm_1, cm_2, ..., cm_k\\}\\) until the entire skill is executed.\nExecutor workflow:\n\u2022 Command reception: The Executor receives a set of low-level commands \\(\\{cm_1, cm_2, ..., cm_k\\}\\)\nfrom the Skill Planner. Each command specifies a concrete action to be performed by the robot's\nhardware components.\n\u2022 Hardware interaction: For each command \\(cm_i\\), the Executor interacts with the robot's hardware,\nadjusting the motors, grippers, and other actuators. This interaction can be represented by the\nfunction \\(H(cm_i, h_t)\\) that determines the effect of a command on the robot's state \\(h_t\\).\n\u2022 Command execution: The Executor executes each command \\(cm_i\\) in the sequence, ensuring\nthat the robot's state transitions from state \\(h_t\\) to \\(h_{t+1}\\). Formally:\n\\[h_{t+1} = H(cm_i, h_t), \\forall i = 1, 2, ..., k\\]\nAfter executing all commands, the robot's reaches the final state \\(h_{t+k}\\), corresponding to the\ncompletion of the skill."}, {"title": "3.1.4. Controller", "content": "The Controller is responsible for monitoring the robot's status and the environment during command\nexecution, ensuring that they are carried out as planned. After each command is executed, the Executor\nsends feedback indicating either success or failure. If a failure occurs, it results in the failure of the entire\nskill. Upon the completion of all commands, a success feedback will indicate the successful execution of\nthe skill.\nDenote \\(f_t\\) the feedback from the Executor at time t. The Controller processes \\(f_t\\) to determine the\noutcome of the executed skills. The feedback can be classified into two categories: success and failure.\nFeedback processing:\n\u2022 Success: If the feedback \\(f_t\\) indicates successful execution of a command and it is the last com-\nmand to execute, the skill is considered successfully completed, the Controller sends a positive\nacknowledgment to the Task Planner to continue the planning process. However, if the feedback\nindicates success but the command is not the last one, the Controller waits for the execution of\nthe next command:\n\\[\\text{if } f_t = \\text{Success } \\Rightarrow \\text{ Task Planner continues}\\]\n\u2022 Failure: If a failure occurs during the execution of any command, the planned skill fails and the\nController generates a failure message \\(m_f\\) that includes the reason for the failure. This message\nis sent to the Explainer. Let \\(e_t\\) represent the specific error detected at time t. The failure message\ncan be represented as:\n\\[m_f = Failure(e_t)\\]\nwhere \\(e_t\\) can include various error reasons such as obstacles detected, non-executable trajectories,\nor environmental changes.\nThe Controller's operation is highly dependent on the specific robot system in use, as it relies on the\ncharacteristics of the robot and the employed software system. In a ROS environment, for example, the\nController interacts with ROS nodes that control the robot's hardware. In our work, RoBee, described\nin section 5, has a system that allows to obtain feedback on the execution of commands."}, {"title": "3.1.5. Explainer", "content": "The Explainer component plays a critical role in enhancing the planning process by providing insights\nto the Task Planner when failures occur during the execution phase. After receiving the failure reason,\nthe Explainer searches a dataset D for previous instances of similar failures. This dataset comprises\nrecords of failures associated with specific skills and user requests. Let \\(D_{rf}\\) denote the subset of the\ndataset containing records of failures and solutions related to the same skill and error message. The\ndataset has been manually built based on previous experiences, desired behaviors, and expected failures.\nThe search can be expressed as:\n\\[D_{rf} = \\{(s_k, u_r, r_f) \\in D | s_k = skill\\_name, e_r = r_f, u_r \\sim user\\_request\\}\\]\nwhere:"}, {"title": "4. Behavior example of the system", "content": "To illustrate the proposed system's behavior in a dynamic environment, consider a scenario where the\nrobot is tasked with picking up a bottle from a table in a room and placing it in a designated area in\nother room. In this example, the system must react to unexpected changes, such as the bottle being\nmoved by an external agent during the task.\nExample User Request: The user provides the instruction: \"Go to the table in the kitchen, pick\nup the bottle, and place it on the table in the bedroom.\" Perception Module: The robot senses the\nenvironment and generateds a semantic map, which includes the pose and types of objects. Task\nPlanner: The request is interpreted, and the Task Planner begins the planning procedure. It starts by\ncalling perception actions to gather information about the environment and the robot's state. Using\nthe results of these perception actions, the Task Planner determines the next steps in its execution\nby calling skill actions and incorporating the outcomes of each skill into the planning process. LLM\ninterprets the request and reasons as follows."}, {"title": "5. Robot Hardware", "content": "The system was implemented using RoBee, a cognitive humanoid robot developed by Oversonic Robotics.\nRoBee measures 160 cm in height and weighs 60 kg. It has 32 degrees of freedom, enabling highly\nflexible movement. The robot is equipped with multiple sensors, including cameras, microphones, and\nforce sensors.\nThe cameras provide real-time visual data, supporting navigation and object recognition tasks. The\nmicrophones facilitate audio input, enabling speech recognition and interaction through natural lan-\nguage processing. The force sensors are used for handling objects, allowing RoBee to adjust grip force\nbased on the characteristics of the item being manipulated, enhancing precision and safety during\ninteractions.\nRoBee's mechanical structure includes two arms capable of bimanual manipulation, each capable of\nhandling objects weighing up to 5 kg. The system includes a torso and leg system designed for bal-\nance and mobility. RoBee is equipped with LIDAR sensors for real-time environment mapping and\nobstacle detection. These LIDAR sensors enable the robot to navigate autonomously through complex\nenvironments, ensuring safe operation in shared spaces. The combination of autonomous navigation\ntechnologies and LIDAR-based detection enhances the ability of RoBee to move efficiently and avoid\ncollisions in dynamic industrial environments.\nIn addition to its physical capabilities, RoBee integrates with cloud-based systems, allowing for remote\nmonitoring, task scheduling, and data analytics.\nThe Planner-module takes into account RoBee's embodiment, ensuring that the system is aligned with\nthe robot's capabilities such as its degrees of freedom, sensor suite, and ability to perform manipulation\nand navigation."}, {"title": "6. Preliminary results", "content": "Preliminary experiments were conducted in a simulated environment replicating two main rooms: a\nkitchen and a bedroom, as illustrated in Figure 6.\nDuring the experiments, three types of requests were tested, each varying in complexity:\n\u2022 Simple requests: direct commands that involve only one skill. For example, \"Pick up the bottle\nin front of you\", where the task planner needs only to identify the parameters and activate the\nappropriate skill.\n\u2022 Moderately complex requests: tasks that require the robot to perform multiple skills in\nsequence, as explicitly described in the command. An example is \"Go to the kitchen, pick up the\nbottle, and bring it to the table in the bedroom\", which involves multiple skills. These tasks require\na higher level of complexity, with planning across several steps and handling potential failures."}, {"title": "7. Conclusions", "content": "The proposed planning system exhibits notable strengths, particularly its adaptability and seamless with\nthe robot's diverse set of skill for executing complex tasks. The system's core advantage lies in its ability\nto interpret user commands through natural language processing, converting them into high-level\nactions that are further refined into low-level, executable tasks. By integrating real-time environmental\nfeedback from the Perception Module through an extended version of ReAct framework, the system can\ndynamically adjust to unexpected situations, such as obstacles or execution failures. This adaptability\nis supported by an architecture, where the Task Planner, Skill Planner, Controller, and Explainer\ncomponents work in harmony to ensure smooth task execution even in changing environments.\nOne of the system's key strengths is its ability to manage error recovery through feedback loops,\nallowing the robot to adapt quickly to failures during task execution. The Explainer module provides on\nthe fly suggestions to modify the plan based on past errors, enhancing the system's validity. The use of\nsemantic maps and scene graphs provides the robot with a structured understanding of its environment,\nensuring that actions are contextually accurate and responsive to real-world conditions.\nThe integration of LLMs, perceptual feedback, and flexible task planning mechanisms makes the system\nhighly versatile for complex, dynamic environments. Its implementation on RoBee, the humanoid robot\ndeveloped by Oversonic Robotics, has demonstrated its practical potential, positioning it as a valuable\ntool for applications requiring advanced human-robot interaction and adaptability in unpredictable\nsettings.\nIn the future, other than extending the low level skill set available, we will investigate the possibility to\nautonomously expand the Explainer dataset as well as providing similar information directly to the Task\nPlanner, increasing flexibility and reliability and reducing the number of re-planning events. We will\nalso study capability of the system to proactively acquire information about the environment [14] and\nhuman partners both through sensors [36] and communication strategies, leveraging the potential for\nproactive information gathering behaviours of LLMs [37, 38, 39]. Moreover, it will be crucial to assess\nthe reliability of the system both at the planning level as well as the communication level, considering\nthe introduction of embodiment and environment while the limitation in pragmatic understanding of\nLLM are still to be understood [39, 40, 41]."}]}