{"title": "Improving Grapheme-to-Phoneme Conversion through In-Context Knowledge Retrieval with Large Language Models", "authors": ["Dongrui Han", "Mingyu Cui", "Jiawen Kang", "Xixin Wu", "Xunying Liu", "Helen Meng"], "abstract": "Grapheme-to-phoneme (G2P) conversion is a crucial step\nin Text-to-Speech (TTS) systems, responsible for mapping\ngrapheme to corresponding phonetic representations. How-\never, it faces ambiguities problems where the same grapheme\ncan represent multiple phonemes depending on contexts, pos-\ning a challenge for G2P conversion. Inspired by the remark-\nable success of Large Language Models (LLMs) in handling\ncontext-aware scenarios, contextual G2P conversion systems\nwith LLMs' in-context knowledge retrieval (ICKR) capabili-\nties are proposed to promote disambiguation capability. The\nefficacy of incorporating ICKR into G2P conversion systems\nis demonstrated thoroughly on the Librig2p dataset. In partic-\nular, the best contextual G2P conversion system using ICKR\noutperforms the baseline with weighted average phoneme error\nrate (PER) reductions of 2.0% absolute (28.9% relative). Using\nGPT-4 in the ICKR system can increase of 3.5% absolute (3.8%\nrelative) on the Librig2p dataset.", "sections": [{"title": "1. Introduction", "content": "Text-to-Speech (TTS) synthesis aims to convert written text into\nspeech with correct pronunciation and natural prosody. Driven\nby the rapid development of deep learning, neural TTS sys-\ntems such as [1, 2, 3, 4] have been widely applied in practical\nhuman-machine interactions. To bridge the gap between lin-\nguistic text and acoustic pronunciation, Grapheme-to-Phoneme\n(G2P) serves as a front-end module in the current TTS systems\nto map written text to their corresponding pronunciation repre-\nsentations. As any erroneous phonetic mapping could be prop-\nagated to the downstream modules and directly lead to mispro-\nnunciation, G2P plays an essential role in guaranteeing correct\nsynthesis.\nThe initial works for G2P models adopt rule-based ap-\nproaches, and later statistical models, such as Hidden Markov\nModels (HMMs), and Conditional Random Fields (CRFs)\n[5, 6, 7, 8] was used to model the relationship of grapheme\nand phoneme. For the last decade, the development of deep\nlearning led to the emergence of various neural G2P mod-\nels, which leverage various network structures including Recur-\nrent Neural Networks [9], Long-Short Term Memory [10, 11]\nand convolutional networks [12]. Besides, transformer-based\nauto-regressive models have also been explored in G2P tasks\n[13, 14]. More recently, SoundChoice [15] was proposed as\na sentence-level G2P model. It employed homograph loss to\nresolve homograph disambiguation better, together with tech-\nnologies such as curriculum learning and BERT embeddings\n[16], achieving promising performance on phoneme generation\nand homograph detection.\nHowever, building accurate G2P datasets remains a labo-\nrious task, requiring extensive human annotation. Moreover,\nexisting models often lack interpretability, making it difficult\nto understand the rationale behind certain phoneme outputs.\nTherefore, there is a need for more advanced techniques that\ncan leverage contextual information effectively into the G2P\nconversion process. To this end, inspired by the remarkable\nsuccess that Large Language Models (LLMs) like GPT-4 [17]\ndemonstrated strong linguistic capabilities [18, 19] and compe-\ntence in related tasks like Automatic Speech Recognition (ASR)\nerror correction [20], contextual G2P conversion module with\nin-context knowledge retrieval from GPT-4 is proposed in this\npaper to provide richer and extensively semantic information\nin helping resolve the disambiguation gap in G2P mappings.\nThe best-performing G2P conversion system using in-context\nknowledge retrieval outperforms the baseline without context\ninformation with phoneme error rate (PER) reductions of 2.0%\nabsolute (28.9% relative) and homograph accuracy increase of\n3.5% absolute (3.8% relative) on the Librig2p dataset.\nThe main contributions of this paper are summarized as fol-\nlows:\n1) To the best of our knowledge, this is the first work that\nleverages in-context knowledge retrieval from GPT-4 for han-\ndling the disambiguation challenge in G2P mapping process for\nG2P systems. In contrast, previous research has been limited to\ndata augmentation and architectural modifications, overlooking\nthe potential of leveraging LLMs' contextual capabilities.\n2) The efficacy of utilizing in-context knowledge retrieval\nin G2P conversion systems is extensively shown on the Librig2p\ntask and provides insights on their potential to benefit other non-\ncontext based G2P architectures.\n3) The highest accuracy rate of 95.7% and the lowest\nweighted average phoneme error rate of 4.9% were obtained\non Librig2p dataset compared to other non-context based meth-\nods, which demonstrates the potential of in-context knowledge\nretrieval for context-aware speech synthesis applications."}, {"title": "2. Methods", "content": "2.1. LLM One-shot Prompt\nGPT-4, developed by OpenAI [17], is a powerful general-purpose language model with impressive capabilities across\ncode generation, translation, math reasoning, and more [20].\nWe hypothesize that GPT-4's linguistic knowledge could facil-\nitate the direct conversion of G2P by generating phoneme se-\nquences from input sentences through a one-shot prompting ap-\nproach.\nDesigning effective prompts is crucial for eliciting desired\noutputs from LLMs like GPT-4. The prompt should clearly\nspecify the task at hand and outline the expected output for-\nmat of the response to facilitate post-processing. Inspired by\nprior work [21] on ASR error correction, we utilize the prompt\ntemplate as Figure 2.\nBy providing a representative example mapping in the\nprompt, we aim to signal the desired input-output behavior to\nGPT-4. The model can then generate the phoneme sequence for\nthe provided input sentence in a one-shot manner without any\ndataset-specific fine-tuning.\n2.2. LLM In-context Knowledge Retrieval\nOur hypothesis is that GPT-4's extensive knowledge base con-tains broader linguistic context and semantics compared to spe-cialized phoneme data. Consequently, GPT-4 will be better\nsuited to extracting a word's meaning and part-of-speech from\nthe sentence, rather than directly mapping to phoneme represen-\ntations. Inspired by how humans look up word pronunciations\nin dictionaries, we propose an in-context knowledge retrieval\nbased system that treats GPT-4 as an \"AI linguist\" for disam-biguating homographs and generating phonemes. The proposed\napproach consists of the following steps:\n1. Look up the word in a homograph: The in-context\nknowledge retrieval dictionary is structured as a collec-tion of entries, where each entry corresponds to a ho-mographic word and contains multiple sub-entries, each\nrepresenting a distinct meaning or part-of-speech of the\nword, along with its corresponding phoneme pronuncia-tion, example usage context and definition.\n2. Prompt GPT-4 to analyze and identify the input sen-tence: The prompt provided to GPT-4 will include the\ntarget word and the complete sentence. The model will\nbe instructed to output the most relevant meaning, part-of-speech, and contextual information for the target word\nbased on the understanding of the sentence.\n3. Retrieve the phoneme pronunciation: The system will\ncompare GPT-4's output with the sub-entries in the dic-tionary and select the sub-entry whose meaning, part-of-speech, and example context most closely align with\nGPT-4's analysis, retrieving the corresponding phoneme\npronunciation.\nFor non-homographic words with unambiguous pronunciations\nor words not found in the in-context knowledge retrieval, a sep-arate special tag can be used for simple lookups. The key steps\nin this LLM In-context Knowledge Retrieval workflow are illus-trated in Figure 1. GPT-4 plays the role of the linguist, leverag-ing its wide knowledge to disambiguate word senses and apply\nthe appropriate dictionary pronunciation based on the sentence\ncontext.\nThis LLM In-context Knowledge Retrieval workflow lever-ages GPT-4's strengths in language understanding while con-straining phoneme outputs to conform to curated dictionary en-tries, potentially improving performance over the unconstrained\none-shot prompting method. The key steps of this workflow are\nsummarized as follows:\nCase matching: This module involves matching the in-put word to the most relevant case in the dictionary based\non the sentence context and then outputting the corresponding\nphoneme sequence. Since a single word can appear multiple\ntimes within a sentence, the prompt is structured as Figure 3 to\nprovide the target word and the sentence containing that word\nas inputs. The LLM will provide distinct phoneme outputs for\nthe word based on its contextual meaning in each sentence.\nSentence-level phoneme generation: After obtaining the\nphoneme sequences for each word in the sentence, the sys-tem concatenates them in their original order, taking into ac-count word boundaries, stress patterns, and any necessary post-processing to form the complete phonetic representation of the\ninput sentence."}, {"title": "3. Experiments", "content": "3.1. Experiments Setup\nLibrig2p-nostress-space dataset proposed by SoundChoice [15]\nis adopted to evaluate performance. Phonemes are initially\npresented as list objects in the dataset, we reformat them into\nstrings to train and evaluate the PER easily. Additionally, since\nthe dataset omits the stress makers of the phonemes, we drop the\nstress marker 0,1,2 from the generated phonemes. To evaluate\nthe performance of different models. This paper uses Phoneme\nError Rate (PER)\u00b9 and homograph accuracy to evaluate the per-formance of the models. The PER is obtained by calculating\nthe Levenshtein Distance between the ground truth phoneme\nsequences and the generated phoneme sequences. The homo-graph accuracy is evaluated by calculating the accuracy of ho-mographs.\nThe SoundChoice [15] is trained on Librig2p-nostress-space dataset as the baseline system\u00b2. The model is trained with\n50 epochs on lexicon data, 35 epochs on sentence data, and 50\nepochs on homograph data.\nLLM One-shot Prompt: We notice that GPT-4 performs\nmuch better than GPT3.5 on word information extraction, such\nas extracting the genre, and definition of a word in a context.\nWe choose the GPT-4 to gain the performance as better as pos-sible. The GPT-4 version is GPT-4-0613. We only set the role\nof user when calling the GPT-4 API. Additionally, we fine-tune\nLlama2-7B-Chat and Gemma-2B-it on this task. The fine-tuned\ndata are generated by reformatting the Librig2p dataset.\nLLM In-context Knowledge Retrieval: The dictionary\nof in-context knowledge retrieval is generated by the Librig2p\ntrain dataset and the CMU dictionary [add ref]. The homo-graphs and corresponding phonemes are obtained from the lib-rig2p homograph train dataset. We only put the clearly defined\nhomograph into the homograph dictionary, use GPT-4 to gener-ate cases automatically, and modify the examples in case of any\nambiguity. Each homograph contains multiple cases. Each case\ncontains a corresponding genre, phoneme, and example sen-tences. For the non-homograph words, besides adding words\nfrom the CMU dictionary, omit those with multiple phonemic\ntranscriptions. we design two methods to add words from the\nLibrig2p dataset as follows:\n\u2022 librig2p_omit: From the Librig2p training dataset, in-clude the words that only have a single phonemic tran-scription. Words with multiple phonemic transcriptions\nwill not be considered.\n\u2022 librig2p_freq: Add words from the Librig2p training\ndataset. If a word has several phonemic transcrip-tions, only the transcription that appears most frequently\nwithin the dataset is included.\nConsidering the dictionary influence, we also change the lead-ing position of the dictionary between the CMU dictionary, and the Librig2p data, for example, cmu+Librig2p_freq ver-sus Librig2p_freq+cmu. Because many words are contained in both the CMU dictionary and Librig2p datasets, the phonemes\nfor those duplicated words will get from the front one, such as cmu+librg2p_omit, if the word is duplicated in both data, then we will follow its phonemes in cmu. Since there are no\ndatasets to fine-tune LLama2-7B-chat, Gemma-2B-it for the Case matching prompt, we generate the example sentence us-ing GPT-4. Then reformat them as the prompt example de-scribed in subsection 2.2.\n3.2. Fine-tuned Models\nTo get a customized and stable G2P system, using a local LLM\nto achieve an object is always an option. Fine-tuning LLM is\na highly effective way to improve performance [22, 23]. We chose Llama2-7B-chat[24] and Gemma-2B-it\u00b3 to work on the\nG2P task.\nWe use QLORA to fine-tune those models. QLORA re-duces the memory usage of LLM fine-tuning without per-formance trade-offs compared to standard 16-bit model fine-tuning. More specifically, QLoRA uses 4-bit quantization to compress a pre-trained language model. The LLM parameters\nare then frozen and a relatively small number of trainable pa-rameters are added to the model as Low-Rank Adapters. Dur-ing fine-tuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pre-trained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training. [25]\nWord phonemes generating: For out-of-vocabulary words not present in the dictionary, an additional prompt is used to have GPT-4 generate the phoneme sequence directly, while tak-ing into account the word's context within the sentence. The prompt for this out-of-vocabulary generation step is structured as Figure 4."}, {"title": "4. Results", "content": "4.1. Results on One-shot\nThe PER and homograph accuracy of LLM: one-shot method\nevaluated on the Librig2p test dataset are shown in Table 3. Sev-eral trends can be found:\n1) The GPT-4 can applied for G2P conversion but performs\npoorly without customized fine-tuning. The PER has a sig-nificant reduction and is better than the baseline model after\napplying the fine-tuned model(Llama2-7B-chat, Gemma-2B-it) in G2P. The Fine-tuned Gemma-2B-it achieves the lowest\nweighted PER(5.2%) among other models including the base-line model, which proves the capability of the LLM on G2P\nconversion.\n2) The homograph accuracy is lower than the baseline\nmodel. Compared with the baseline model exploiting cur-riculum learning (that gradually switches from word-level to\nsentence-level G2P) with specified weighted homograph loss, the fine-tuned models are simply trained for 3 epochs. The ho-mograph accuracy of the fine-tuned model is higher than GPT-4,\nwhich implies the model can learn the context information by fine-tuning to get a higher homograph accuracy.\n4.2. Results on LLM In-context Knowledge Retrieval\nThe PER and homograph accuracy of LLM In-context Knowl-edge Retrieval system is shown in Table 3 and Table 4. Several\ntrends can be found:\n1) The weighted average PER is lower than the baseline\nmodel for each LLM, which verifies the feasibility of this method. The fine-tuned model achieves the lowest weighted av-erage PER among others because it can generate the phonemes\nof OOV words following the distribution of the Librig2p dataset.\n2) The homograph accuracy of GPT-4 is the highest among other models. Since the training data of case matching prompt\nfor this method is generated by GPT-4, it is logical that the ho-mograph accuracy of the fine-tuned model is lower than GPT-4. The result implies that the strong linguistic capabilities of GPT-4 [18, 19] can help identify the homograph in a sentence, and GPT-4 has much better semantic understanding capabilities than the other two fine-tuned models. The result proved our hy-pothesis in subsection 2.2 that \"GPT-4's broad knowledge base covers more linguistic context and semantics than specialized phoneme data.\".\n3) For GPT-4, the LLM In-context Knowledge Retrieval\nsystem shows a significant improvement compared to the one-shot method. The result implies the feasibility of the system.\n4) Here we compare different ways of dictionary-building to check if the method can be applied by other LLMs. The re-sult shows that fine-tuned models also achieve some relatively good performance. Meanwhile, the dictionary librig2p_freq\nmakes the method achieve a lower weighted average PER than the method of \"one-shot\". Whether omit the words that have multiple phonetic representations in datasets has a big influence on the result. The distribution of the Librig2p train dataset is closer to the Librig2p test dataset, thus the librig2p_freq can lead to a better performance than the librig2p_omit.\n5) The result of the extra experiment Table 4 shows that the\norder of the dictionary barely influences the final result. Besides"}, {"title": "5. Conclusion", "content": "To the best of our knowledge, this is the first work that lever-ages in-context knowledge retrieval from GPT-4 for handling\nthe disambiguation challenge in G2P mapping process for G2P\nsystems. In contrast, previous research has been limited to data\naugmentation and architectural modifications, overlooking the\npotential of leveraging LLMs' contextual capabilities. In this\npaper, in-context knowledge retrieval from LLMs was incor-porated into Grapheme-to-Phoneme (G2P) conversion process\nto bridge the disambiguation gap in G2P mappings. Experi-ments on the Librig2p demonstrate that the proposed in-context\nknowledge retrieval-based G2P system outperforms the base-line without context information. The highest accuracy rate of\n95.7% and the lowest weighted average phoneme error rate of\n4.9% were obtained on the Librig2p dataset compared to other\nnon-context-based methods, which demonstrates the potential\nof in-context knowledge retrieval for context-aware speech syn-thesis applications. These empirical findings suggest that lever-aging the contextual understanding capabilities of LLMs has the potential to bridge the gap in G2P mappings. As long as the se-mantic understanding capabilities of LLM are sufficient to dis-tinguish the usage of words in different contexts, the system can achieve a good performance with a high-quality dictionary. How to customize a high-quality phoneme dictionary is a chal-lenge for this system in the future."}]}