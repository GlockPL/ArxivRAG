{"title": "CRUXEVAL-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution", "authors": ["Ruiyang Xu", "Jialun Cao", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Ben He", "Shing-Chi Cheung", "Le Sun"], "abstract": "Code benchmarks such as HumanEval are widely adopted to evaluate Large Language Models' (LLMs) coding capabilities. However, there is an unignorable programming language bias in existing code benchmarks over 95% code generation benchmarks are dominated by Python, leaving the LLMs' capabilities in other programming languages such as Java and C/C++ unknown. Moreover, coding task bias is also crucial. Most benchmarks focus on code generation capability, while benchmarks for code reasoning (given input, reasoning output; and given output, reasoning input), an essential coding capability, are insufficient. Yet, constructing multi-lingual benchmarks can be expensive and labor-intensive, and codes in contest websites such as Leetcode suffer from data contamination during training. To fill this gap, we propose CRUXEVAL-X, a multi-lingual code reasoning benchmark that contains 19 programming languages. It comprises at least 600 subjects for each language, along with 19K content-consistent tests in total. In particular, the construction pipeline of CRUXEVAL-X works in a fully automated and test-guided manner, which iteratively generates and repairs based on execution feedback. Also, to cross language barriers (e.g., dynamic/static type systems in Python/C++), we formulated various transition rules between language pairs to facilitate translation. Our intensive evaluation of 24 representative LLMs reveals the correlation between language pairs. For example, TypeScript and JavaScript show a significant positive correlation, while Racket has less correlation with other languages. More interestingly, even a model trained solely on Python can achieve at most 34.4% Pass@1 in other languages, revealing the cross-language generalization of LLMs. The leaderboard is available at https://cruxeval-x.github.io/leaderboard.html.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have shown advanced proficiency in various domains, including code generation (Liu et al. 2024; Du et al. 2024), defect detection (Yang et al. 2024b) and program repair (Xia and Zhang 2023; Zhong,"}, {"title": "Benchmark Construction", "content": "In this section, we detailed the construction process of CRUXEVAL-X in Figure 2. It can be divided into three main steps. First, we translate the function signature via mapping variable type annotations (Step 1 in Figure 2). Then we employ a rule-based approach to translate Python test cases into other PLs (Step 2 in Figure 2). Finally, we integrate multiple LLMs to translate the code by iterating the generation-and-repair process (Step 3 in Figure 2)."}, {"title": "Step I. Function Signature Translation", "content": "To enhance the accuracy and standardization of function translation results, we first translate the function signatures and dependencies. Note that Python does not require an explicit type annotation, which may confuse the translation for the function signature. For example, as shown in Figure 2 Step I, the types of two input parameters (i.e., s1 and s2) are unclear. So we extract the input variables from the function signature using the syntax tree and match them with the tests. Based on the variable types in the tests, we annotate the input and output variables in the function signature.\nThen, we adopt the rules as prior work (Cassano et al. 2023) to map the types from Python to other PLs. In particular, we identify the data types in the annotated Python signature (e.g., parameter types, return types), mapping the types from Python to other PLs according to the rules, then structuring the signature in the corresponding PLs. Take the example in Figure 2 Step I, the Python signature def f(s1:str, s2:str) -> str is translated into that in C(std::string f(std::string s1, std::string s2). After translating the tests, all 800 subjects in Python can be translated, as shown in Table 1."}, {"title": "Step II. Test Suites Translation", "content": "We employ a test-guided approach to ensure the correctness of the translation results, which necessitates test cases in various PLs. Prior works (Athiwaratkun et al. 2022; Cassano et al. 2023) provided various rules for mapping Python test cases to other PLs. We adopt the mapping rules from MutiPL-E (Cassano et al. 2023) to assist the transition of our test suites.\nHowever, their rules have limited support for type handling (e.g., they cannot handle a list with hybrid types), Thus, to maximize the success rate, we made two improvements to enhance the rules. First, we enhance handling structured types such as List and Dict. For example, when handling C#, we add an equality function to check whether two Dict types are equal. Second, when dealing with variables that have complex types that are not as well-supported in some other PLs, we transform these variables into more generic types without significantly altering the original function's functionality. For example, we change type List [Union(str, int)] into List(str) if the function keeps the same functionality. A small portion"}, {"title": "Step III. Iterative Generation & Repair", "content": "After the test, dependencies, and signatures are properly transited, we then use multiple LLMs to iteratively translate the original Python reference code into corresponding PLs. In particular, each LLM undergoes two substeps: generation and repair. These substeps are based on the results of the previous LLM. Once a problem successfully passes the test cases, it will not appear in the next round of iteration. Below, we will elaborate on these two substeps in detail.\nGeneration. Relying solely on a single LLM's limited number of generations is unlikely to achieve high accuracy in translation tasks (Yin et al. 2024). Therefore, we propose a multi-round generation method, which involves interaction with the testing environment to determine whether to proceed with the next round of iteration.\nLet Ao represent the number of translated codes that pass the tests and the number of total problems is U. For all questions that are not correctly answered, we used LLM M to generate results through multiple rounds. We denote the number of correct results in the i-th round as Ai, with the maximum number of generation rounds set as N. For Ai, if the increase in the number of correct questions compared to the results k rounds before Ai-k is less than the threshold \u03b4, the process will stop early. Here, k is a fixed constant, and the formula is as follows:\n$A_{i} = Correct(P(O_{i} | U - A_{i-1}; M)) + A_{i-1}$\nfor i \u2208 {1, 2, ..., N} Stop if (i >k) and (Ai - Ai-k < \u03b4)\n(1)\nWe obtain the i-th round code result Oi from the code generation distribution P(Oi | U \u2013 Ai\u22121; M). Then we calculate the correct results using Correct(.) as the function.\nTo fully leverage the strengths of various LLMs, we select the closed-source LLM GPT3.5-Turbo and the open-source LLM DeepseekCoder-33B-Instruct for iterative translation tasks. We initially use GPT3.5-Turbo for preliminary generation. Given the higher usage cost of closed-source LLMs, we set N to 5, k to 5, 8A to 0, and the temperature to 0.2. The result is shown in Table 1 under the column \"w/o Iter\". We also use this as a baseline for the effectiveness of single LLM's generation within a limited number of iterations to compare with the final results of our pipeline. Subsequently, we employ Deepseekcoder-33B-Instruct on top of the foundation laid by GPT3.5-Turbo to conduct further generation. We set N to 50, k to 5, 8A to 0, and the temperature to 0.8.\nRepair Simply generating code will still result in many errors the LLM cannot solve. Therefore, after the generation step of each LLM, we provide them error messages for error correction. We observed that the cost of multiple iterative error corrections is high and the benefits are relatively low (Chen et al. 2023), so we only perform error correction once after each of the two LLMs.\nAfter the generation of GPT3.5-Turbo, we directly provide the LLM with the erroneous code along with the error messages for correction. After the Iterating Generation of DeepseekCoder-33B-Instruct, since the untranslated code can produce numerous incorrect code snippets after multiple rounds of iteration, which may contain the same errors, we first use simhash to deduplicate the erroneous code and then proceed with error correction on the deduplicated code. Error correction in different phases uses the LLM employed in that specific phase, with a temperature setting of 0."}, {"title": "Multiturn Repair based on overlap", "content": "After completing the steps above, we first calculate the intersection of correctly answered questions across different PLs. To our surprise, there were only 333 questions that all PLs answered correctly. However, 563 problems have been successfully translated correctly by at least 16 PLs. Upon analyzing the questions our LLM failed to solve, we find that each PL has its difficulties in translating from python. For example, in Julia, the index for arrays and other collection types starts from 1, which differs from Python. The details of the difficulties can be found in Appendix.\nBased on these observations, we conduct final generation and iterative error correction on the questions that are correctly translated by more than 15 PLs. Due to the small number of questions requiring translation, we utilized GPT-40 for generation and error correction. We set the temperature to 0, generate once, and correct errors three times. During the generation process, we provide the LLM with three corresponding typical examples based on the difficulties we find. The overlap is increased to 462 after repair of GPT-40.\nWe manually modified 38 questions that GPT-40 almost got right, expanding our dataset to 500 entries. We determined that 500 entries are sufficient to distinguish the effectiveness of the LLMs and most of the questions that failed to be translated cannot be expressed well in other PLs. Therefore, we use these 500 entries as our CRUXEVAL-X benchmark. The final result of our pipeline is shown in Table 1 under the column \u201cw/ Iter\u201d. The prompt of each step can be found in Appendix."}, {"title": "Experiment Setup", "content": "LLMs for evaluation We selecte 24 LLMs across 4 types for evaluation, including general LLMs (GPT-3.5-Turbo, GPT-40-mini, GPT-40 (Brown et al. 2020; Achiam et al. 2023), Llama3 (AI 2024), Qwen2 (Yang et al. 2024a), phi-3-instruct (Abdin et al. 2024)), multilingual code LLMs (Deepseekcoder-V2 (Zhu et al. 2024), Deepseekcoder-V1 (Guo et al. 2024), CodeLlama (Roziere et al. 2023), Starcoder (Li et al. 2023a), Starcoder2 (Lozhkov et al. 2024), CodeQwen1.5-Chat (Bai et al. 2023)), instruction-tuned multilingual LLMs (Deepseekcoder-instruct-V1 (Guo et al. 2024), WizardCoder (Luo et al. 2023), CodeLlama-Python, CodeLlama-Instruct (Roziere et al. 2023)), and single or few-language code LLMs (CodeGen (Nijkamp et al. 2022), phi-1 (Gunasekar et al. 2023), phi-1.5 (Li et al. 2023b)).\nEvaluation task We adopt the task settings from prior work (Gu et al. 2024), dividing the tasks into output reasoning and input reasoning. For any PLs dataset Lk \u2208 {Li}{1,where K = 19, representing the total number of PLs. We provide a function $f^{Lk}$ and the corresponding test. The input of this test example is $i^{Lk}$, and the output is $o^{Lk}$. The output reasoning task can be expressed as:\n$r^{Lk} = I(P(o^{Lk}|f^{Lk},i^{Lk},M))$ (2)\nThe input reasoning task can be expressed as:\n$r^{Lk} = I(P(i^{Lk}|f^{Lk},o^{Lk},M))$ (3)\nwhere M is any LLMs. We get the input or output from the code generation distribution P(.) and compose test cases $(i^{Lk}, o^{Lk})$. I(.) is the indicator function by executing this test case with function $f^{Lk}$. If $f^{Lk}$ passes the test, the evaluation result is 1, otherwise 0.\nEvaluation method. We use pass@1 (Kulal et al. 2019; Chen et al. 2021) as the evaluation metric to assess both the task of output reasoning and input reasoning. We set the temperature to 0 and employ greedy decoding for generation as prior work (Cao et al. 2024a). For closed-source LLMs, we generate outputs by calling OpenAI's API. The version of the api is gpt-3.5-turbo, gpt-40-mini, gpt-4o"}, {"title": "Overall Result.", "content": "Figure 3 shows Pass@1 evaluation results of various LLMs arranged in descending order of the LLMs' parameter size. It can be observed that LLMs with a larger number of parameters tend to perform better. The closed-source LLMS GPT-4o achieved the best results among all evaluated LLMs. Notably, the result of the open-source LLMs Deepseekcoder-V2 is better than GPT-40-mini, with an average Pass@1 of 62.8% on input reasoning and 65.0% on output reasoning.\nFrom the perspective of average values, it can be observed that under different types of PLs, the LLMs' capabilities on input and output reasoning are quite similar, which echos the observation made by prior work (Gu et al. 2024) in Python. More interestingly, during the evaluation, we introduced the few-language LLMs CodeGen-muti, which was only trained on C, C++, GO, Java, JavaScript, and Python, as well as the"}, {"title": "Analysis on Key Factors for LLM Code Reasoning", "content": "To get more insight into what factors in code affect LLMs' code reasoning ability, we explore six factors (e.g., average number of input variables, average input length) and statistics their correlation with correct/incorrect reasoning. The result is shown in Figure 4. Each column of the box plot displays the distribution of 19 PLs. In particular, the columns \"Num of Input Variable\u201d, \u201cNum of Variable Type\" are counted by averaging the number of types of input parameters in method signatures, \u201cInput/Output Length\" is the average string length of the input/output. For instance, in the test case where f (\"a\", 123) == 123, the input length is 4, and the output length is 3. \u201cWhether Have Complex Type\" checks whether there are List, Dict, Tuple, Set types in input and output. \"Execution Steps\" calculates the average execution steps in Python bytecode operations, following prior work (Gu et al. 2024).\nFrom Figure 4, we can see from Sub-figures A-C that the number/types of input variables have little impact on the code reasoning, especially Sub-figure B, which shows that the reasoning capability is slightly better when more types of variables are involved. A more counter-intuitive observation is made from sub-figures D-E. They indicate that the reasoning capability is negatively correlated with the length of input/output strings instead of the number of data types.\nFurthermore, regarding input reasoning capability, the more input variables, the more challenging it is for LLMs to reason about the correct inputs, thus the worse the input reasoning performance (Sub-figures A and D). Similarly, the longer the outputs, the harder the output reasoning, resulting in worse performance (Sub-figure E)."}, {"title": "Analysis on Cross-language Generalization", "content": "To investigate the cross-language generalizability of LLMs, we investigate the reasoning ability of phi-1 and phi-1.5, which are trained on English and Python only. To get a better understanding, we analyze the capability in terms of syntax and semantics in 9 PLs because they provide clear error messages to distinct syntactic/semantic errors.\nSyntactic Correctness. To generalize to other PLs, it is critical to ensure syntactic correctness. Figure 5 (A)-(B) show the number of syntactic-correct cases made by these two LLMs in both tasks. It is clear that Python has the highest syntactic correctness in both tasks, followed by Go and TypeScript. On the contrary, C++, C#, and Java witness the most syntactic errors for three LLMs. Interestingly, even though phi-1 and phi-1.5 have not trained on PLs other than Python, they can still achieve an average of 49.1% and 72.0% syntactically correctness rate in other PLs, respectively, compared with 97.0% and 98.7% achieved on Python. It indicates the cross-language generalizability of LLMs.\nSemantic Correctness. Beyond syntactic correctness, semantic correctness poses higher requirements, i.e., passing the tests. The results are shown in the last two rows in Figure 3. Similar results can be observed in both tasks and both LLMs. In particular, phi-1.5 reaches 25.8% input reasoning performance on Python, while on other PLs, an average of 19.0% can also be reached. The observation further consolidates the cross-language generalizability of LLMs.\nCross-NL and Cross-PL Generalization. From Figure 5 and Figure 3, there is a noticeable increase from phi-1 to phi-1.5 (an average of 10.7% vs. 21.7% on input reasoning, and 15.1% vs. 21.7% on output reasoning). According to the description (Abdin et al. 2024), phi-1.5 is further fine-tuned with more synthetic texts in natural language (NL). Considering the dramatic improvement in code reasoning, it is highly likely that the improvement in NL reasoning positively impacts code reasoning."}, {"title": "Analysis on Programming Language Correlation", "content": "To further investigate the correlations between these 19 PLS in CRUXEVAL-X, we calculate each PL pair's correlation (i.e., cosine similarity, ranging from -1 to 1), visualized in"}, {"title": "Case Study", "content": "After identifying the phi-series-LLMs (i.e., phi-1, phi-1.5) exhibit cross-language generalization and the correlation across PLs, we further analyze the predictions of phi-1.5 to get a deeper understanding. We noticed that out of the 128 correct instances in Python by phi-1.5, 61.7% (79/128) are also correct in PHP, while only 39.8% (51/128) are correct in Racket. Therefore, we use one example in these three PLs (Python, PHP, and Racket) to understand its rationale.\nAnalysis on Subject 106. Listing 1-3 demonstrates an instance where phi-1.5 generalizes Python (Listing 1)'s reasoning capabilities to other languages. Each example's check function has been uniformly simplified to assert f() == (). From a grammar structure perspective, their respective function definitions, indentation formats, and the functions they invoke exhibit significant differences. However, overall, PHP and Python share a more similar structure, both utilizing sort for sorting and return for returning output values. Therefore, phi-1.5 is able to generalize its code reasoning abilities to PHP, but fails to comprehend the sorting command in Racket, leading to incorrect predictions.\nUpon analyzing these 128 questions, we observe that excluding those where the output could be directly derived from the input, such as assert f(\"zej\",\"owc\") == \"zej\", which accounted for approximately 40% of the cases, there are still numerous examples demonstrating that phi-1.5 has developed a certain level of cross-language capabilities. From these examples, we can observe that the multilingual generalization capability of the model is positively correlated with the grammar structural similarity between languages. Even Racket, a language significantly different from others, maintains certain logical similarities in aspects such as function definitions, loops, and conditional branchs. This is a key reason why Phi-1.5 can achieve considerable effectiveness across multiple languages."}, {"title": "Related Work", "content": "Multi-Task Code Benchmark Recently, there has been an increasing number of tasks related to code that are used to evaluate the various capabilities of LLMs in the field of coding, including code generation (Chen et al. 2021; Austin et al. 2021), code repair (Jimenez et al. 2023; Tian et al. 2024), and code description (Chai et al. 2024). However, datasets that assess the reasoning abilities of code are relatively limited, and the currently proposed reasoning datasets are confined to the Python language (Gu et al. 2024; Chen et al. 2024). In this work, we have expanded the Python language reasoning dataset CRUXEVAL (Gu et al. 2024) to encompass 19 PLs, thereby addressing the deficiency in reasoning datasets at the multilingual level.\nMulti-Language Code Benchmark Multilingual evaluation datasets are an important method for assessing the comprehensive coding capabilities of code LLMs. In the early stages, multilingual code datasets were mainly used for code translation tasks (Elnaggar et al. 2021; Ahmad et al. 2021; Roziere et al. 2020, 2021; Zhu, Suresh, and Reddy 2022; Yan et al. 2023; Zhu et al. 2022). These datasets often consist of problem solutions in different languages extracted from algorithm competition-related websites, thus suffering from data contamination issue. Bechmark like McEval (Chai et al. 2024), which relies on human annotation, requires a"}, {"title": "Conclusion", "content": "In this work, we provide a fully automated process for constructing a multilingual dataset based on a Python code language dataset. Through this process, we successfully transform the CRUXEval dataset into a multilingual dataset containing 19 PLs and test its effectiveness on 24 LLMs, demonstrating the validity of the dataset. Furthermore, we find that models trained on only a few languages exhibit the ability to transfer their prediction capabilities to other languages in input/output reasoning tasks, and this ability is influenced by the model's own reasoning capabilities."}, {"title": "Appendix", "content": "The improvements of MutiPL-E pipline\nenhance of the pipline (1) For C# we enhance the check function to include the ability to judge if the List and dict types are equal. (2) For Julia we add the data type to empty dict, for example change input Dict() to Dict{String, String}(). Otherwise, the function can not accept the input (3) For JavaScript we change String which Contains special characters, such as\"example\\n\" to `example\\n`.\ntransform complex types (1) For inputs and outputs that include functions, such as \"bfrerat\".split(\"-\"), we will replace the input or output with the result after the function execution. (2) When the input is of Callable type, such as lambda x: x.reverse(), we will remove the parameter and incorporate the Callable type into the main function internally. (3) For complex variables that contain multiple types, if we can convert them into a simpler type without altering the function's functionality, we will preserve such functions. For instance, as illustrated in Listing 4, consider a dictionary d: Dict[str, Union [int, str]]. If converting all its values to the str type does not alter the function's behavior, we will retain it; otherwise, we will discard them.\nPrompt of Benchmark Construction\nIn Figures 7 and 8, we include the prompts we use for our benchmark construction. We use a few-shot prompt for all models. For Generation step of each model, the prompt is show in Figure 7. The example in this Figure is used for GPT3.5-turbo. After the Generation and Repair of GPT3.5-turbo, we choose three examples from the correct generated problems which include str, List, Dict type respectively. We use these examples for Deepseekcoder-instruct-33b Generation. For GPT-40, based on the summarized difficulties in translation, we provide three examples, as shown in Listing 5, 6, and 7. we present these examples in three distinct languages. For each specific language translation, we employ the corresponding language version of the three examples."}]}