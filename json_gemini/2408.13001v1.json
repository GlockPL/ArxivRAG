{"title": "CRUXEVAL-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution", "authors": ["Ruiyang Xu", "Jialun Cao", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Ben He", "Shing-Chi Cheung", "Le Sun"], "abstract": "Code benchmarks such as HumanEval are widely adopted to evaluate Large Language Models' (LLMs) coding capabilities. However, there is an unignorable programming language bias in existing code benchmarks over 95% code generation benchmarks are dominated by Python, leaving the LLMs' capabilities in other programming languages such as Java and C/C++ unknown. Moreover, coding task bias is also crucial. Most benchmarks focus on code generation capability, while benchmarks for code reasoning (given input, reasoning output; and given output, reasoning input), an essential coding capability, are insufficient. Yet, constructing multi-lingual benchmarks can be expensive and labor-intensive, and codes in contest websites such as Leetcode suffer from data contamination during training. To fill this gap, we propose CRUXEVAL-X, a multi-lingual code reasoning benchmark that contains 19 programming languages. It comprises at least 600 subjects for each language, along with 19K content-consistent tests in total. In particular, the construction pipeline of CRUXEVAL-X works in a fully automated and test-guided manner, which iteratively generates and repairs based on execution feedback. Also, to cross language barriers (e.g., dynamic/static type systems in Python/C++), we formulated various transition rules between language pairs to facilitate translation. Our intensive evaluation of 24 representative LLMs reveals the correlation between language pairs. For example, TypeScript and JavaScript show a significant positive correlation, while Racket has less correlation with other languages. More interestingly, even a model trained solely on Python can achieve at most 34.4% Pass@1 in other languages, revealing the cross-language generalization of LLMs. The leaderboard is available at https://cruxeval-x.github.io/leaderboard.html.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have shown advanced proficiency in various domains, including code generation (Liu et al. 2024; Du et al. 2024), defect detection (Yang et al. 2024b) and program repair (Xia and Zhang 2023; Zhong, Wang, and Shang 2024; Hu et al. 2024). Benchmarks such as HumanEval (Chen et al. 2021) and SWE-bench (Jimenez et al. 2023) were introduced to measure LLMs' capabilities, providing insights into LLMs' strengths and weaknesses.\nRecent studies (Cao et al. 2024a; Chai et al. 2024; Chen et al. 2024) have spotted two significant biases in current benchmarks. First, Programming language bias. As pointed out by prior studies (Cao et al. 2024a; Chai et al. 2024; Chen et al. 2021; Austin et al. 2021), Python dominates code generation benchmarks with over 95% involvement. Other programming languages (PLs) such as Java and C/C++, despite their popularity and availability, gain less exploration. Second, Coding task bias. Most coding benchmarks focus on code generation tasks (i.e., giving descriptions in natural language and generating the program, as shown in Figure 1 (A)), while code reasoning (i.e., given the program, reasoning the input or output of the program, as shown in Figure 1 (B - D)), as an essential coding capability of LLMs, is seldom evaluated (Chen et al. 2024). A recent work introduced a code reasoning benchmark (Gu et al. 2024; Chen et al. 2024), while it is only in Python. Figure 1 (C - D) shows that simply changing PLs from Python to C++ can turn a correct reasoning into an incorrect one.\nHowever, constructing multi-lingual benchmarks is not a trivial task. First, human annotation can be expensive. As reported by recent work (Chai et al. 2024), they spent a total of $12,000 US dollars for human annotators, providing the working environment, free meals, souvenirs, and free GPT-4 interface usage to construct their multi-lingual benchmark. Second, automated translation does not perform well. The latest studies (Yin et al. 2024) show that even the best LLM (i.e., ChatGPT) can only achieve an average of 64% success translation rate, which is far from practice. Rule-based translation (Cassano et al. 2023; Ling et al. 2022) usually suffers from generalizability issues, making them limited in handling prescribed code structures. Additionally, multi-lingual solutions from contest websites such as LeetCode and Codeforces were included in most LLMs training sources, thus suffering from data contamination issue (Cao et al. 2024b).\nTo fill the research gaps, we introduce CRUXEVAL-X, a multi-lingual code reasoning benchmark that contains 19 popular PLs, including C++, Rust, Java, etc., expanded from CruxEval (Gu et al. 2024), a code reasoning benchmark written in Python. For each PL in CRUXEVAL-X, there are at least 600+ functions. In total, there are 12,660 subjects along with 19K test cases for input/output reasoning.\nNoteworthy that the pipeline of constructing CRUXEVAL-X works in a fully automated manner. It first translates the test cases by transition rules adapted from prior work (Cassano et al. 2023), then iterates the generation-and-repair process intensively. In particular, the transition rules are formulated to cross the language barriers. For example, Python employs a dynamically typed system where types are determined at runtime, whereas C++ uses a statically typed system requiring explicit type declarations at compile time. The rules facilitate the translation of the test cases. Additionally, inspired by prior work (Yin et al. 2024; Rozi\u00e8re et al. 2022), we employ a test-guided manner (Rozi\u00e8re et al. 2022) to generate the translation and iteratively repair the generated code using execution feedback (e.g., compilation error, runtime error) (Yin et al. 2024).\nThrough intensive experiments on 24 mainstream LLMs, we observe several interesting findings. First, in multiple PLs, the input reasoning and output reasoning capabilities of LLMs are comparable. Also, there is a noticeable correlation between certain PLs (e.g., JavaScript and TypeScript show a positive correlation, while Racket consistently yields the worst results). More interestingly, we observe that even if a model is only trained on Python (e.g., phi-1 and phi-1.5), it still can reach a 16% ~ 26% output reasoning success rate in other PLs, compared with 25.6% in Python. The finding indicates the cross-language generalization of LLMs.\nThe contributions can be summarized as follows: (1) We introduce CRUXEVAL-X, a multi-lingual code reasoning benchmark that contains 19 popular PLs. (2) We introduce an automated code translation pipeline that adopts a test-guided and iterative generate-and-repair practice. (3) We evaluate 20+ LLMs against CRUXEVAL-X and yield inspiring findings."}, {"title": "Benchmark Construction", "content": "In this section, we detailed the construction process of CRUXEVAL-X in Figure 2. It can be divided into three main steps. First, we translate the function signature via mapping variable type annotations (Step 1 in Figure 2). Then we employ a rule-based approach to translate Python test cases into other PLs (Step 2 in Figure 2). Finally, we integrate multiple LLMs to translate the code by iterating the generation-and-repair process (Step 3 in Figure 2)."}, {"title": "Step I. Function Signature Translation", "content": "To enhance the accuracy and standardization of function translation results, we first translate the function signatures and dependencies. Note that Python does not require an explicit type annotation, which may confuse the translation for the function signature. For example, as shown in Figure 2 Step I, the types of two input parameters (i.e., s1 and s2) are unclear. So we extract the input variables from the function signature using the syntax tree and match them with the tests. Based on the variable types in the tests, we annotate the input and output variables in the function signature.\nThen, we adopt the rules as prior work (Cassano et al. 2023) to map the types from Python to other PLs. In particular, we identify the data types in the annotated Python signature (e.g., parameter types, return types), mapping the types from Python to other PLs according to the rules, then structuring the signature in the corresponding PLs. Take the example in Figure 2 Step I, the Python signature def f(s1:str, s2:str) -> str is translated into that in C(std::string f(std::string s1, std::string s2). After translating the tests, all 800 subjects in Python can be translated, as shown in Table 1."}, {"title": "Step II. Test Suites Translation", "content": "We employ a test-guided approach to ensure the correctness of the translation results, which necessitates test cases in various PLs. Prior works (Athiwaratkun et al. 2022; Cassano et al. 2023) provided various rules for mapping Python test cases to other PLs. We adopt the mapping rules from MutiPL-E (Cassano et al. 2023) to assist the transition of our test suites.\nHowever, their rules have limited support for type handling (e.g., they cannot handle a list with hybrid types), Thus, to maximize the success rate, we made two improvements to enhance the rules. First, we enhance handling structured types such as List and Dict. For example, when handling C#, we add an equality function to check whether two Dict types are equal. Second, when dealing with variables that have complex types that are not as well-supported in some other PLs, we transform these variables into more generic types without significantly altering the original function's functionality. For example, we change type List [Union(str, int)] into List(str) if the function keeps the same functionality. A small portion of the data that cannot be converted is discarded. The result of Step II is shown in Table 1. Further details can be found in the Appendix."}, {"title": "Step III. Iterative Generation & Repair", "content": "After the test, dependencies, and signatures are properly transited, we then use multiple LLMs to iteratively translate the original Python reference code into corresponding PLs. In particular, each LLM undergoes two substeps: generation and repair. These substeps are based on the results of the previous LLM. Once a problem successfully passes the test cases, it will not appear in the next round of iteration. Below, we will elaborate on these two substeps in detail.\nGeneration. Relying solely on a single LLM's limited number of generations is unlikely to achieve high accuracy in translation tasks (Yin et al. 2024). Therefore, we propose a multi-round generation method, which involves interaction with the testing environment to determine whether to proceed with the next round of iteration.\nLet $A_0$ represent the number of translated codes that pass the tests and the number of total problems is U. For all questions that are not correctly answered, we used LLM M to generate results through multiple rounds. We denote the number of correct results in the i-th round as $A_i$, with the maximum number of generation rounds set as N. For $A_i$, if the increase in the number of correct questions compared to the results k rounds before $A_{i-k}$ is less than the threshold \u03b4, the process will stop early. Here, k is a fixed constant, and the formula is as follows:\n$A_{i} = Correct(P(O_{i} | U - A_{i-1}; M)) + A_{i-1} \\ for i \\in \\{1, 2, ..., N\\}\\ Stop \\text{ if } (i > k) \\text{ and } (A_{i} - A_{i-k} < \\delta)$  (1)\nWe obtain the i-th round code result $O_i$ from the code generation distribution $P(O_{i} | U - A_{i-1}; M)$. Then we calculate the correct results using $Correct(.)$ as the function.\nTo fully leverage the strengths of various LLMs, we select the closed-source LLM GPT3.5-Turbo and the open-source LLM DeepseekCoder-33B-Instruct for iterative translation tasks. We initially use GPT3.5-Turbo for preliminary generation. Given the higher usage cost of closed-source LLMs, we set N to 5, k to 5, $\\delta A$ to 0, and the temperature to 0.2. The result is shown in Table 1 under the column \"w/o Iter\". We also use this as a baseline for the effectiveness of single LLM's generation within a limited number of iterations to compare with the final results of our pipeline. Subsequently, we employ Deepseekcoder-33B-Instruct on top of the foundation laid by GPT3.5-Turbo to conduct further generation. We set N to 50, k to 5, $\\delta A$ to 0, and the temperature to 0.8.\nRepair Simply generating code will still result in many errors the LLM cannot solve. Therefore, after the generation step of each LLM, we provide them error messages for error correction. We observed that the cost of multiple iterative error corrections is high and the benefits are relatively low (Chen et al. 2023), so we only perform error correction once after each of the two LLMs.\nAfter the generation of GPT3.5-Turbo, we directly provide the LLM with the erroneous code along with the error messages for correction. After the Iterating Generation of DeepseekCoder-33B-Instruct, since the untranslated code can produce numerous incorrect code snippets after multiple rounds of iteration, which may contain the same errors, we first use simhash to deduplicate the erroneous code and then proceed with error correction on the deduplicated code. Error correction in different phases uses the LLM employed in that specific phase, with a temperature setting of 0.\nMultiturn Repair based on overlap After completing the steps above, we first calculate the intersection of correctly answered questions across different PLs. To our surprise, there were only 333 questions that all PLs answered correctly. However, 563 problems have been successfully translated correctly by at least 16 PLs. Upon analyzing the questions our LLM failed to solve, we find that each PL has its difficulties in translating from python. For example, in Julia, the index for arrays and other collection types starts from 1, which differs from Python. The details of the difficulties can be found in Appendix.\nBased on these observations, we conduct final generation and iterative error correction on the questions that are correctly translated by more than 15 PLs. Due to the small number of questions requiring translation, we utilized GPT-40 for generation and error correction. We set the temperature to 0, generate once, and correct errors three times. During the generation process, we provide the LLM with three corresponding typical examples based on the difficulties we find. The overlap is increased to 462 after repair of GPT-40.\nWe manually modified 38 questions that GPT-40 almost got right, expanding our dataset to 500 entries. We determined that 500 entries are sufficient to distinguish the effectiveness of the LLMs and most of the questions that failed to be translated cannot be expressed well in other PLs. Therefore, we use these 500 entries as our CRUXEVAL-X benchmark. The final result of our pipeline is shown in Table 1 under the column \u201cw/ Iter\u201d. The prompt of each step can be found in Appendix."}, {"title": "Experiments", "content": "LLMs for evaluation We selecte 24 LLMs across 4 types for evaluation, including general LLMs (GPT-3.5-Turbo, GPT-40-mini, GPT-40 (Brown et al. 2020; Achiam et al. 2023), Llama3 (AI 2024), Qwen2 (Yang et al. 2024a), phi-3-instruct (Abdin et al. 2024)), multilingual code LLMs (Deepseekcoder-V2 (Zhu et al. 2024), Deepseekcoder-V1 (Guo et al. 2024), CodeLlama (Roziere et al. 2023), Starcoder (Li et al. 2023a), Starcoder2 (Lozhkov et al. 2024), CodeQwen1.5-Chat (Bai et al. 2023)), instruction-tuned multilingual LLMs (Deepseekcoder-instruct-V1 (Guo et al. 2024), WizardCoder (Luo et al. 2023), CodeLlama-Python, CodeLlama-Instruct (Roziere et al. 2023)), and single or few-language code LLMs (CodeGen (Nijkamp et al. 2022), phi-1 (Gunasekar et al. 2023), phi-1.5 (Li et al. 2023b)).\nEvaluation task We adopt the task settings from prior work (Gu et al. 2024), dividing the tasks into output reasoning and input reasoning. For any PLs dataset $L_k \\in \\{L_i\\}_{i=1}^K$, where K = 19, representing the total number of PLs. We provide a function $f^{L_k}$ and the corresponding test. The input of this test example is $i^{L_k}$, and the output is $o^{L_k}$.\nThe output reasoning task can be expressed as:\n$r^{L_k} = I(P(o^{L_k} | f^{L_k}, i^{L_k}, M))$ (2)\nThe input reasoning task can be expressed as:\n$r^{L_k} = I(P(i^{L_k} | f^{L_k}, o^{L_k}, M))$ (3)\nwhere M is any LLMs. We get the input or output from the code generation distribution P(.) and compose test cases $(i, o)^{L_k}$. I(.) is the indicator function by executing this test case with function $f^{L_k}$. If $f^{L_k}$ passes the test, the evaluation result is 1, otherwise 0.\nEvaluation method. We use pass@1 (Kulal et al. 2019; Chen et al. 2021) as the evaluation metric to assess both the task of output reasoning and input reasoning. We set the temperature to 0 and employ greedy decoding for generation as prior work (Cao et al. 2024a). For closed-source LLMs, we generate outputs by calling OpenAI's API. The version of the api is gpt-3.5-turbo, gpt-40-mini, gpt-4o"}, {"title": "Overall Result.", "content": "Figure 3 shows Pass@1 evaluation results of various LLMs arranged in descending order of the LLMs' parameter size. It can be observed that LLMs with a larger number of parameters tend to perform better. The closed-source LLMS GPT-4o achieved the best results among all evaluated LLMs. Notably, the result of the open-source LLMs Deepseekcoder-V2 is better than GPT-40-mini, with an average Pass@1 of 62.8% on input reasoning and 65.0% on output reasoning.\nFrom the perspective of average values, it can be observed that under different types of PLs, the LLMs' capabilities on input and output reasoning are quite similar, which echos the observation made by prior work (Gu et al. 2024) in Python. More interestingly, during the evaluation, we introduced the few-language LLMs CodeGen-muti, which was only trained on C, C++, GO, Java, JavaScript, and Python, as well as the single-language LLMs phi-1.5 and phi-1, which were pre-trained only on Python. However, they achieve similar results in the input and output reasoning tasks across 19 PLs. Notably, phi-1, having only seen Python, achieved a Pass@1 of 11.8% on Python input prediction, but scored 23.6% on Perl input prediction. We will provide a more detailed analysis of this phenomenon in Analysis part."}, {"title": "Analysis on Key Factors for LLM Code Reasoning", "content": "To get more insight into what factors in code affect LLMs' code reasoning ability, we explore six factors (e.g., average number of input variables, average input length) and statistics their correlation with correct/incorrect reasoning. The result is shown in Figure 4. Each column of the box plot displays the distribution of 19 PLs. In particular, the columns \"Num of Input Variable\u201d, \u201cNum of Variable Type\" are counted by averaging the number of types of input parameters in method signatures, \u201cInput/Output Length\" is the average string length of the input/output. For instance, in the test case where f (\"a\", 123) == 123, the input length is 4, and the output length is 3. \u201cWhether Have Complex Type\" checks whether there are List, Dict, Tuple, Set types in input and output. \"Execution Steps\" calculates the average execution steps in Python bytecode operations, following prior work (Gu et al. 2024).\nFrom Figure 4, we can see from Sub-figures A-C that the number/types of input variables have little impact on the code reasoning, especially Sub-figure B, which shows that the reasoning capability is slightly better when more types of variables are involved. A more counter-intuitive observation is made from sub-figures D-E. They indicate that the reasoning capability is negatively correlated with the length of input/output strings instead of the number of data types.\nFurthermore, regarding input reasoning capability, the more input variables, the more challenging it is for LLMs to reason about the correct inputs, thus the worse the input reasoning performance (Sub-figures A and D). Similarly, the longer the outputs, the harder the output reasoning, resulting in worse performance (Sub-figure E)."}, {"title": "Analysis on Cross-language Generalization", "content": "To investigate the cross-language generalizability of LLMs, we investigate the reasoning ability of phi-1 and phi-1.5, which are trained on English and Python only. To get a better understanding, we analyze the capability in terms of syntax and semantics in 9 PLs because they provide clear error messages to distinct syntactic/semantic errors.\nSyntactic Correctness. To generalize to other PLs, it is critical to ensure syntactic correctness. Figure 5 (A)-(B) show the number of syntactic-correct cases made by these two LLMs in both tasks. It is clear that Python has the highest syntactic correctness in both tasks, followed by Go and TypeScript. On the contrary, C++, C#, and Java witness the most syntactic errors for three LLMs. Interestingly, even though phi-1 and phi-1.5 have not trained on PLs other than Python, they can still achieve an average of 49.1% and 72.0% syntactically correctness rate in other PLs, respectively, compared with 97.0% and 98.7% achieved on Python. It indicates the cross-language generalizability of LLMs.\nSemantic Correctness. Beyond syntactic correctness, semantic correctness poses higher requirements, i.e., passing the tests. The results are shown in the last two rows in Figure 3. Similar results can be observed in both tasks and both LLMs. In particular, phi-1.5 reaches 25.8% input reasoning performance on Python, while on other PLs, an average of 19.0% can also be reached. The observation further consolidates the cross-language generalizability of LLMs.\nCross-NL and Cross-PL Generalization. From Figure 5 and Figure 3, there is a noticeable increase from phi-1 to phi-1.5 (an average of 10.7% vs. 21.7% on input reasoning, and 15.1% vs. 21.7% on output reasoning). According to the description (Abdin et al. 2024), phi-1.5 is further fine-tuned with more synthetic texts in natural language (NL). Considering the dramatic improvement in code reasoning, it is highly likely that the improvement in NL reasoning positively impacts code reasoning."}, {"title": "Analysis on Programming Language Correlation", "content": "To further investigate the correlations between these 19 PLS in CRUXEVAL-X, we calculate each PL pair's correlation (i.e., cosine similarity, ranging from -1 to 1), visualized in Figure 6. In particular, for each PL, we flatten the results of LLMs as a feature vector and calculate the cosine similarities for each pair of PLs.\nOverall, Figure 6 shows that the correlation between PL pairs is generally similar, with an average of 0.7+ cosine similarities. Among all PL pairs, JavaScript and TypeScript correlate the most strongly (0.87 and 0.91 on both tasks). It indicates that the code reasoning capabilities on different PLs are highly correlated. Also, the correlation in output reasoning is slightly higher than in input reasoning, with an average of 0.79 vs. 0.75.\nIt is also noteworthy that Racket has the most minor correlation with all the other PLs. It may be because of its distinct syntax. A case study can be found in Listing 3."}, {"title": "Case Study", "content": "After identifying the phi-series-LLMs (i.e., phi-1, phi-1.5) exhibit cross-language generalization and the correlation across PLs, we further analyze the predictions of phi-1.5 to get a deeper understanding. We noticed that out of the 128 correct instances in Python by phi-1.5, 61.7% (79/128) are also correct in PHP, while only 39.8% (51/128) are correct in Racket. Therefore, we use one example in these three PLs (Python, PHP, and Racket) to understand its rationale.\nAnalysis on Subject 106. Listing 1-3 demonstrates an instance where phi-1.5 generalizes Python (Listing 1)'s reasoning capabilities to other languages. Each example's check function has been uniformly simplified to assert f() == (). From a grammar structure perspective, their respective function definitions, indentation formats, and the functions they invoke exhibit significant differences. However, overall, PHP and Python share a more similar structure, both utilizing sort for sorting and return for returning output values. Therefore, phi-1.5 is able to generalize its code reasoning abilities to PHP, but fails to comprehend the sorting command in Racket, leading to incorrect predictions.\nUpon analyzing these 128 questions, we observe that excluding those where the output could be directly derived from the input, such as assert f(\"zej\",\"owc\") == \"zej\", which accounted for approximately 40% of the cases, there are still numerous examples demonstrating that phi-1.5 has developed a certain level of cross-language capabilities. From these examples, we can observe that the multilingual generalization capability of the model is positively correlated with the grammar structural similarity between languages. Even Racket, a language significantly different from others, maintains certain logical similarities in aspects such as function definitions, loops, and conditional branchs. This is a key reason why Phi-1.5 can achieve considerable effectiveness across multiple languages."}, {"title": "Related Work", "content": "Multi-Task Code Benchmark Recently, there has been an increasing number of tasks related to code that are used to evaluate the various capabilities of LLMs in the field of coding, including code generation (Chen et al. 2021; Austin et al. 2021), code repair (Jimenez et al. 2023; Tian et al. 2024), and code description (Chai et al. 2024). However, datasets that assess the reasoning abilities of code are relatively limited, and the currently proposed reasoning datasets are confined to the Python language (Gu et al. 2024; Chen et al. 2024). In this work, we have expanded the Python language reasoning dataset CRUXEVAL (Gu et al. 2024) to encompass 19 PLs, thereby addressing the deficiency in reasoning datasets at the multilingual level.\nMulti-Language Code Benchmark Multilingual evaluation datasets are an important method for assessing the comprehensive coding capabilities of code LLMs. In the early stages, multilingual code datasets were mainly used for code translation tasks (Elnaggar et al. 2021; Ahmad et al. 2021; Roziere et al. 2020, 2021; Zhu, Suresh, and Reddy 2022; Yan et al. 2023; Zhu et al. 2022). These datasets often consist of problem solutions in different languages extracted from algorithm competition-related websites, thus suffering from data contamination issue. Bechmark like McEval (Chai et al. 2024), which relies on human annotation, requires a high cost. In this work, we provide a process using LLMs for multilingual code translation, which can achieve a high accuracy and low cost in creating a multilingual dataset."}, {"title": "Conclusion", "content": "In this work, we provide a fully automated process for constructing a multilingual dataset based on a Python code language dataset. Through this process, we successfully transform the CRUXEval dataset into a multilingual dataset containing 19 PLs and test its effectiveness on 24 LLMs, demonstrating the validity of the dataset. Furthermore, we find that models trained on only a few languages exhibit the ability to transfer their prediction capabilities to other languages in input/output reasoning tasks, and this ability is influenced by the model's own reasoning capabilities."}, {"title": "Appendix", "content": "The improvements of MutiPL-E pipline\nenhance of the pipline (1) For C# we enhance the check\nfunction to include the ability to judge if the List and\nDict types are equal. (2) For Julia we add the data\ntype to empty dict, for example change input Dict()\nto Dict{String, String}(). Otherwise, the func-\ntion can not accept the input (3) For JavaScript we\nchange String which Contains special characters, such\nas\"example\\n\" to `example\\n`.\ntransform complex types (1) For inputs and outputs that\ninclude functions, such as \"bfrerat\".split(\"-\"),\nwe will replace the input or output with the result after\nthe function execution. (2) When the input is of Callable\ntype, such as lambda x: x.reverse(), we will re-\nmove the parameter and incorporate the Callable type\ninto the main function internally. (3) For complex variables\nthat contain multiple types, if we can convert them into\na simpler type without altering the function's functional-\nity, we will preserve such functions. For instance, as illus-\ntrated in Listing 4, consider a dictionary d: Dict[str,\nUnion [int, str]]. If converting all its values to the\nstr type does not alter the function's behavior, we will re-\ntain it; otherwise, we will discard them.\nListing 4: example with complex type\n1 from typing import Dict, Union, Tuple\n2\n3 def f(d: Dict [str, Union [int, str]]) ->\nTuple [bool, bool]:\n4\nr = {\n5\n\"c\": d.copy(),\n6\n\"d\": d.copy()\n7\n}\n8\nreturn (r(\"c\") is r(\"d\"), r[\"c\"]\n==\nr[\"d\"])\n9\n10 def check (candidate):\n11\nassert candidate({\"i\": 1, \"love\":\n\"\nparakeets\"})\n==\n(False, True)\n12\n13 def test_check():\n14\ncheck (f)\nThe difficulties for translating\n(1) For indexing functions such as 'index', the starting po-\nsition is not 0 but 1, with Julia being a typical language\nthat exhibits this behavior. (2) For the conversion between\nPython's 'str' type and its own 'char' and 'string' types, D\nlanguage is a typical case where this issue arises. (3) For the\ntransformation and comparison of dictionary types, C# is a\ntypical language where such errors are common.\nPrompt of Benchmark Construction\nIn Figures 7 and 8, we include the prompts we use for our\nbenchmark construction. We use a few-shot prompt for all\nmodels. For Generation step of each model, the prompt is show in Figure 7. The example in this Figure is used for\nGPT3.5-turbo. After the Generation and Repair of GPT3.5-\nturbo, we choose three examples from the correct generated\nproblems which include str, List, Dict type respec-\ntively. We use these examples for Deepseekcoder-instruct-\n33b Generation. For GPT-40, based on the summarized dif-\nficulties in translation, we provide three examples, as shown\nin Listing 5, 6, and 7. we present these examples in three dis-\ntinct languages. For each specific language translation, we\nemploy the corresponding language version of the three ex-\namples."}, {"title": "Listing 6: example2 for GPT-40 (Swift)", "content": "1\nimport Foundation\n2\n3 func f(strand: String, zmnc: String) ->\nInt {\n4\nvar strand = strand\n5\nvar poz = strand.range(of: zmnc)\n6\nwhile poz != nil {\n7\nstrand.removeSubrange (poz!)\n8\npoz = strand.range(of: zmnc)\n9\n}\n10\nlet lastIndex = strand.range (of:\nzmnc, options: (), range: nil,\nlocale: nil)?.lowerBound.\nutf160ffset (in: strand)\n11\n12\nreturn lastIndex ?? -1\n13 func ==(left: [(Int, Int)), right: [(Int\nInt)]) -> Bool {\n14\nif left.count != right.count {\n15\nreturn false\n16\n}\n17\nfor (1, r) in zip(left, right) {\n18\nif 1 != r {\n19\nreturn false\n20\n}\n21\n}\n22\nreturn true\n23 }\n24\n25 assert (f(strand: \"\", zmnc: \"abc\") == -1)"}, {"title": "Listing 7: example3 for GPT-40 (Python)", "content": "1 from typing import Dict", "f(d": "Dict [str", "int,\nint": "n4\nif \"x\" in d:\n5\nx = d[\"x\"", "y\" in d": "n7\nY = d[\"y\"", "candidate)": "n11\nassert"}]}