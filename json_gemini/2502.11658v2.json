{"title": "\"I'm not for sale\" \u2013 Perceptions and limited awareness of privacy risks by digital\nnatives about location data", "authors": ["Antoine Boutet", "Victor Morel"], "abstract": "Although mobile devices benefit users in their daily lives in\nnumerous ways, they also raise several privacy concerns. For\ninstance, they can reveal sensitive information that can be in-\nferred from location data. This location data is shared through\nservice providers as well as mobile applications. Understand-\ning how and with whom users share their location data \u2013 as\nwell as users' perception of the underlying privacy risks \u2013, are\nimportant notions to grasp in order to design usable privacy-\nenhancing technologies. In this work, we perform a quantita-\ntive and qualitative analysis of smartphone users' awareness,\nperception and self-reported behavior towards location data-\nsharing through a survey of n = 99 young adult participants\n(i.e., digital natives). We compare stated practices with ac-\ntual behaviors to better understand their mental models, and\nsurvey participants' understanding of privacy risks before and\nafter the inspection of location traces and the information that\ncan be inferred therefrom.\nOur empirical results show that participants have risky pri-\nvacy practices: about 54% of participants underestimate the\nnumber of mobile applications to which they have granted ac-\ncess to their data, and 33% forget or do not think of revoking\naccess to their data. Furthermore, most of the participants do\nnot have a realistic perception of privacy risks and have gen-\nerally heard little about privacy-related scandals. Also, by us-\ning a demonstrator to perform inferences from location data,\nwe observe that slightly more than half of participants (57%)\nare surprised by the extent of potentially inferred information,\nand that 47% intend to reduce access to their data via permis-\nsions as a result of using the demonstrator. Last, a majority of\nparticipants have little knowledge of the tools to better pro-\ntect themselves, but are nonetheless willing to follow sugges-\ntions to improve privacy (51%). Educating people, including\ndigital natives, about privacy risks through transparency tools\nseems a promising approach.", "sections": [{"title": "1 Introduction", "content": "Smartphones have become the most popular electronic de-\nvices, used by 95% of all internet users\u00b9. The widespread\nadoption of these mobile devices with geolocation capabil-\nities makes users' location traces\u00b2 widely accessible to mo-"}, {"title": "2 Related Work", "content": "Prior research widely explored location privacy (Section 2.1)\nand users' perceptions of privacy (Section 2.2). However,\nlittle attention has been devoted to self-reported behaviors\ntowards location data sharing. Moreover, to the best of our\nknowledge, no studies have used a demonstration platform\nto improve risk understanding in a way that would change\nusers' perceptions about the balance between the benefits of\ninvasive technologies and potential risks."}, {"title": "2.1 Privacy and location", "content": "The privacy issues raised by location data gained a lot of\ntraction in the last decade (Primault et al. 2019). In partic-\nular, user location traces extracted from various data have\nbeen shown to be highly unique (De Montjoye et al. 2013;\nZang and Bolot 2011). This high uniqueness may act as a\ndigital fingerprint and lead to the re-identification of users\nif their traces are associated with external knowledge. This\nuniqueness does not only concern location traces but char-\nacterizes all traces generated by a human (Andreas et al.\n2016; Eckersley 2010; Rebekah and Rachel 2016), and can\ngenerate a risk of re-identification. Several cases of re-\nidentification have been documented, for instance the re-\nidentification of individuals from web search queries (Bar-\nbaro and Zeller 2006), taxi logs (Zhang and Wang 2016), or\nthe notorious case of Governor William Weld using medical\ninformation (Barth-Jones 2012).\nThe uniqueness and risk of re-identification is not the\nunique threat related to location traces. Recent works have\ndemonstrated that location is a very rich contextual infor-\nmation and leads to a strong inferential potential in terms\nof information that can be learned about individuals (Boutet\nand Gambs 2019). For instance, location traces can reveal\nPoints Of Interest (POI) of users such as their home and\nworkplaces (Gambs, Killijian, and del Prado Cortez 2010),\ntheir race and gender (Zhong et al. 2015), their social net-\nwork (Sharad and Danezis 2014), it can be used to predict\ntheir location patterns (Sadilek and Krumm 2012), to link\naccounts of the same user across different datasets (Riederer\net al. 2016a), and infer even more sensitive information such\nas their religion or personality traits6.\nDue to the large adoption of mobile devices, the loca-\ntion data of users is extensively collected and shared (Al-\nmuhimedi et al. 2015; Englehardt and Narayanan 2016). The\nuncontrolled usage of this information can have an important\nimpact on users such as unfair price discrimination (Mikians\net al. 2012). The increasing use of connected devices col-\nlecting personal data and the lack of transparency on how\nthe data is actually exploited raise privacy concerns. Only a\nhandful of tools have been proposed to improve user aware-\nness about the potential risk of revealing their location. For\ninstance, (Riederer et al. 2016b) propose a tool to inspect the\npotential of location data, while (Boutet and Gambs 2019)\nshow the impact of protection mechanisms of the inference\ncapabilities using a demonstration platform."}, {"title": "2.2 Perceptions of privacy and privacy controls", "content": "Giving users the benefits of location services on their mo-\nbile devices while preserving their privacy is an ongoing\nchallenge, as evidenced by mobile OSs iterating on the user\ninterfaces for location notices and control every few years.\nFor example, iOS allows users to select whether apps get\nfine- or coarse-grained location data7, and will present pop-\nups when apps continually access location services in the\nbackground. Similarly, Android apps now have to request\nbackground location access separately from general loca-\ntion usage8, and the OS will automatically revoke unused\npermissions from apps9. Even though mobile operating sys-\ntems regularly improve user interfaces, the opaque privacy\ncontrols of location services still face criticism10. (Balash\net al. 2022) explore the users' perceptions regarding access\nto Google accounts by mobile applications and formulate\ndesign recommendations to improve the current third-party\nmanagement tools offered by Google, such as tracking re-\ncent access, automatically revoking access due to app disuse,\nand providing permission controls. (Wijesekera et al. 2015)\nanalyze authorization preferences in different usage contexts\nand suggest determining the situations in which users would\nlike to be confronted with security decisions.\nUsers' perceptions of the risks and benefits of technolo-\ngies can determine their willingness to adopt them (Poikela\nand Kaiser 2016). More specifically, people are more likely\nto accept potentially invasive technology if they think its\nbenefits will outweigh its potential risks (Beckwith 2003).\nDue to the massive adoption of location-enabled mobile ap-\nplications, this fact suggests that users' perception of this\ntrade-off is more in favor of the benefits than the potential\nrisks. This attitude however depends on the perception of\nthe said privacy risks. Studies have shown that users of mo-\nbile phones are often unaware of the data collected by apps\nrunning on their devices, and that a majority of users re-\nstrict some of their permissions (Almuhimedi et al. 2015)\nfollowing a better awareness of data collection. Other stud-\nies explored the privacy paradox (Barth et al. 2019; Kang\nand Jung 2021) where self-reported concerns about privacy\nappear to be in contradiction with often careless online be-\nhaviors. However, another study focuses on university com-\nmunity (Gamarra et al. 2019) and show that this popula-\ntion of users does not have a genuine concern regarding the\nprivacy of their geolocation data. Note that this paper did\nnot study location data in mobility contexts associated with\nsmartphones as we do.\nAnalyzing behavior and understanding users' perceptions\nare also important notions to grasp in order to further design\nPrivacy and Transparency Enhancing Technologies (PETs\nand TETs). For instance, (Kaushik et al. 2021) conducted\nan online survey to understand people's perspectives on"}, {"title": "3 Methodology", "content": "We conducted two questionnaires on n = 99 young partici-\npants aged between 20 and 26 (with an average around 21)\nto collect quantitative and qualitative data about the partic-\nipants' perception and self-reported behavior regarding the\nsharing of location data.\nIn a nutshell, the first questionnaire (hereinafter\n\"behavior questionnaire\u201d) aimed at assessing their\nself-reported practices, perceptions, and pre-conceived\nideas about privacy. Following this first questionnaire, par-\nticipants were then invited to use a demonstrator (denoted\n\"demonstration\" in what follows) which analyses location"}, {"title": "3.1 Recruitment", "content": "We recruited young participants for our survey via a course\noffered to engineering students at [anonymized]. This pro-\ncess of recruitment ensured a high number of participants,11\nall of them using their smartphones. The participants were\nnot compensated. The participants had followed computer\nscience courses including data science, AI, and introduction\nto the GDPR. By mainly recruiting young computer engi-\nneers with knowledge of technologies, we performed our\nstudy on a homogeneous population which consumes dig-\nital information quickly, and which are supposedly aware of\nprivacy risks, hence providing an \"upper bound\" of privacy\nperceptions in our context.\nThe study was performed over a four hours slot, dur-\ning which students were successively invited to answer the\nbehavior questionnaire, analyse privacy risks through the\ndemonstration, and answer the perception questionnaire. Al-\nthough the teacher of the course is one of the authors of\nthe current paper, participants were presented with a con-\nsent form at the beginning of the survey, and they were able\nto decline without penalty (specified orally); only the exper-\nimentation of the demonstration platform was mandatory as\npart of their curriculum. The participation was anonymous,\nvoluntary, and the course did not include any grade (one par-\nticipant actually refused to answer the questionnaire, with-\nout penalty)."}, {"title": "3.2 Ethical considerations", "content": "The survey raises ethical issues in two main aspects.\nFirst, the demonstration platform uses personal data,\nspecifically location traces. This data is not necessarily sen-\nsitive in itself, but can act as a proxy for sensitive data. To\nreduce risks, participants used traces from data of the au-\nthors of this study. The data used during the demonstration\nphase was encrypted server-side and was deleted once all\ndata had been processed (calculating Points of Interests, col-\nlecting metadata, and calculating inferences). The DPO of\nthe authors' institution has validated the demonstration plat-\nform.\nSecond, we elicited answers from participants, which is\nalso personal data. We took as many precautions as needed\nto guarantee 1) informed consent (see Section B.1 for the\nconsent form, and as noted in Section 3.1 they had the possi-\nbility to decline without any penalty), and 2) security of data\nstorage and processing (authors only communicated using\nend-to-end encrypted tools, and the survey was conducted\non an EU-based online tool stamped GDPR-compliant). One\nparticipant refused to answer the questionnaires.\nBoth the demonstration platform (regarding its compli-\nance with the GDPR and other data security regulations)"}, {"title": "3.3 Design of the Questionnaires", "content": "We shortly describe in this section the organization of the\ntwo questionnaires. The interested reader will find their full\ncontent in Appendix B. Each section is succinctly described\nin what follows:\n\u2022 Introduction: This section presents the study and asks\nthe participants for their consent to participate in the\nstudy and to collect their answers for research purposes.\n\u2022 Data-Sharing: This section inquires about the partici-\npants' attitudes about data sharing (e.g., which apps they\nthink could access their location data, with whom, etc).\n\u2022 Inferences: This section questions their pre-conceived\nideas about inferences from location data (e.g., from\nwhich data could their location be inferred, or whether\nthe location is sufficient to single them out in a dataset).\n\u2022 Authorisation and Control: This section touches upon\nauthorisation and control of permissions in a mobile con-\ntext. Questions in this section are about their revoking or\ngiving access permissions to apps, or their evaluation of\nthe difficulty to effectuate this permission management.\n\u2022 Privacy Concerns: This section simply performs the\nanalysis of an Internet User's Information Privacy Con-\ncerns (IUIPC) on a 7 Likert scale.\n\u2022 Demographics: This section elicits demographics data.\nThe perception questionnaire is much shorter and is com-\nposed of questions regarding the possible (malicious) in-\nferences conducted on their location traces, the protection\nagainst these privacy violations, and their new expecta-\ntions and suggestions following the experimentation with\nthe demonstration platform."}, {"title": "3.4 Risks analysis - demonstration", "content": "After the first behavior questionnaire, participants had to ex-\nplore a demonstration platform informing them about pri-\nvacy risks linked to location traces through example data.\nWe use the Boutet and Gambs (2019) demonstration plat-\nform. Participants were asked to use the platform to ex-\nplore these location traces and to inspect the information that\ncan be inferred. The demonstration typically offers to visu-\nalize one's traces per day (see Figure 1), which can raise\nawareness about how easy it is to identify Points of Inter-\nests (POIs) - such as the home, the workplace or any other\nvisited places during the day \u2013, and presents metadata (e.g.,\naddress, category of the place, attendance statistics).\nThe demonstration also offers a defense mechanism based\non Differential Privacy (Dwork 2006) in the form of a slider,\nwhich controls the level of noise injected into the location\ndata (see Figure 2). This feature is presented as a means to\nstimulate interest in privacy protection, and to raise aware-\nness of the degradation of data quality when we increase\nprotection with greater noise (to showcase the utility and pri-\nvacy trade-off)."}, {"title": "3.5 Data analysis", "content": "We performed a thematic analysis on the free text answered\nin the study (see Tables 1 and 2 in Appendix C) to anal-\nyse qualitative data. We took inspiration from the steps de-\nscribed in (Braun and Clarke 2006):\n1. read the answers;\n2. code the sentences from the previous step;\n3. merge the codes in normalised codes for data analysis;\n4. regroup the normalised codes from the previous step in\nthemes;\n5. review the themes and the related sentences to verify the\nhomogeneity between them.\nThe analysis was performed by two independent anno-\ntators, and a consensus was reached at the end of the pro-\ncess. For most fields, a full thematic analysis was neither re-"}, {"title": "3.6 General Statistics", "content": "Our participants are mostly equipped with Android phones:\n60% against 40% for iOS users (Q2). Our population is"}, {"title": "4 Results", "content": "This section showcases the most relevant results for our re-\nsearch questions, with the first two subsections dedicated\nto answering RQ1 (\u201cWhat are the perceptions and the un-\nderstanding of young users' privacy and its protection in a\nmobility context?\"), and the last subsection more tailored to\nRQ2 (\u201cWhat is the impact of visualization of location traces\nand associated privacy risks on these concepts?\u201d).\""}, {"title": "4.1 Privacy risky practices", "content": "Participants do not adopt safe practices in terms of privacy.\nSpecifically, they tend to underestimate the number of apps\nthat have access to the location, and they tend not to think\nabout disabling location access for apps that are not actively\nor forget to do so.\nUnderestimation of the number of apps that have access\nto location. Participants were asked to estimate how many\nmobile apps have access to the location on their smartphone\n\"on top of their head\" (Q3), and this data was then compared"}, {"title": "4.2 Participants are poorly aware of the risks", "content": "They do not know how location can be captured, they are\nnot aware of the great inference capacity associated with the\nanalysis of location data, and they are unable to cite cases\nof personal data leaks or scandals related to their use despite\nincreased media coverage.\nParticipants have misconceptions about how location can\nbe captured. To assess the perception of privacy risks linked\nto the sharing of location information, we asked the par-\nticipants how a mobile application could capture location\n(Q12). The responses are displayed in Figure 7 and show\nthat not all participants are aware of the localization capa-\nbilities of the main technologies embedded in a phone. Al-\nthough GPS is identified by almost all participants (96%),\nwe see that only 85%, 64% and 78% of participants are\naware that respectively WiFi, Bluetooth, and IP addresses\ncan be used as proxies to infer location."}, {"title": "4.3 The risk demonstrator increased awareness of participants about privacy", "content": "After having experimented with the demonstration, a third\nof the participants (33) were planning on limiting the access\nto mobile privacy permissions, and deleting the history of\nmobility on the Google Takeout platform (Q37). We con-\ntend that the visualisation of location traces and the risks\nassociated with the inference increased awareness of partic-\nipants towards privacy, and encouraged them to adopt more\nprivacy-preserving behaviors.\nRelative surprise for the number of possible inferences.\nOnce the participants have been able to analyze location\ntraces and discover the information that can be learned with\nthe demonstrator, more than half of the participants (56%)\ndeclared being surprised by the number of inferences made\npossible through location data (Q29). Additionally, when\nasked whether they feel well protected against the various\nthreats observed, a large majority of participants responded\nthat they felt unprotected (84%) (Q35). Knowing this infer-\nence ability, participants were asked if they think Google an-\nalyzes users' location data (Q30), and if they think Google\nshares this data with third parties (Q31). The results show\nthat 93% of participants believed that Google analyzes their\nlocation data and 89% believed that their data is shared with\nthird parties. Even after the inspection of location traces with\nthe platform, 7% of participants still did not think that loca-\ntion data can be used to re-identify users (Q34).\nWorries about surveillance, economic, and physical\nharms. The most worrisome inferences are informational\nharms (surveillance, stalking15, inference, data selling, tar-\ngeted advertising; 88 mentions), followed by economic\nharms (blackmail, robbery, scam, identity theft; 51 men-\ntions), and finally physical harms (physical harm, harass-\nment; 31 mentions) (Q32).\nWe observed interesting preconceived ideas regarding\nthose harms. First, in spite of being in the most cited cat-\negory, targeted advertising was specifically mentioned only\nthree times, whereas it is the main privacy-invasive goal of\nlocation data analysis by Google (from which the traces\nwere extracted). Second, robbery was over-represented in"}, {"title": "5 Discussion", "content": "This section formulates practical recommendations and\nhighlights the limitations of our work."}, {"title": "5.1 Recommendations", "content": "We craft in this section recommendations for the design of\nbetter privacy-friendly systems, and of Transparency En-\nhancing Technologies (TETs) to foster privacy awareness.\nThese recommendations are drawn from both participants'\ninsights and our own interpretations.\nAssist in the revocation of apps. Our results show that\nparticipants tend to forget about data-sharing, where more\nthan a quarter of the participants declared not thinking about\nrevoking access (12), forgot to do it (11), or are simply too\nlazy (6), and that they have inaccurate representations of\nthe number of apps with access to their location (see Sec-\ntion 4.1). However, we note that 14 participants suggested\nincluding a system-wide reminder of which apps can ac-\ncess location as recommendations for better and more us-\nable Privacy Enhancing Technologies (PETs). Note that the\nlatest version of Android and iOS indicate when an app ac-\ncesses location, and Android now offers a centralized inter-\nface indicating which apps require and use geolocation. This\ncentralized interface can be hard to find amongst the differ-\nent settings, and no participant were aware of its existence.\nHowever, both Android and iOS lack a centralized inter-\nface to inform which app is currently accessing location, nor\ndo they provide fine-grain data regarding its use (i.e., how\nmany times did each app request geolocation). We therefore\nrecommend that mobile OSs integrate a centralised and\neasily accessible interface about location usage, provid-\ning not only which geolocation permission has been granted\nto each app but also a fine-grain data regarding its uses.\nMore transparency. Our results also show that knowing\nwhich apps have access to location is not enough; 16 partici-\npants also proposed more transparency and more specifically\nto be able to be informed at any time of which applications\nare collecting data. Seeing the lack of perception of privacy"}, {"title": "5.2 Limitations", "content": "First, our demographics do not represent a fair sample of\ndigital users. Our participants were mostly males, all of\nthem young adults educated in a top European engineering\nschool, although with different backgrounds and technical\nskills. This bias in our representativeness played a part in the\nsocio-economic backgrounds of participants, who are most\nlikely to come from well-off social environments. Recruiting\nparticipants in universities often faces this limitation in aca-\ndemic studies on usages. For instance, (Kulyk et al. 2022)\ninvolved 40 participants mostly young and well-educated,\nwhile (Karegar, Pettersson, and Fischer-H\u00fcbner 2020) in-\nvolved 80 participants, mostly young, with 45% in the 18\u201325\nage group and 20% between the ages of 26 and 35, a major-\nity (n = 54) were bachelor students or had their bachelor\ndegrees. (Almuhimedi et al. 2015) involved 23 participants\n(65% female; ages 18-44, median=23), and (Kaushik et al.\n2021) involved 392 participants ranging from 25 to 35.\nThe transferability of the result to the rest of the popula-\ntion therefore remains an open question. However, given that\nour sample is predominantly composed of \u201cdigital natives\u201d\nwho are often assumed to be more comfortable with personal\ninformation sharing and more knowledgeable about digital\ntechnologies -, we expected a better perception about the\nprivacy risk than amongst other age groups. This expecta-\ntion also has to be put in the perspective of their curriculum:\nstudents in computer engineering with academic knowledge\nof data science, AI, and rudiments of GDPR.\nSecond, the scope of our work must be restricted to a spe-\ncific type of demonstrator, our results should not be gen-\neralized to all visualization and awareness tools. Moreover,\nthe results largely presented self-reported and planned be-\nhaviors. Therefore, an interesting research avenue for future\nwork is to study the actual behavioral changes introduced\nby the experiment of the tool, and long-term behavior anal-"}, {"title": "6 Conclusion", "content": "Through a survey involving n = 99 young mobile phone\nusers, this work contributes to a better understanding of the\npractices in relation to the sharing of their location data\nwith mobile applications as well as the associated privacy\nissues. By qualitatively and quantitatively analyzing user\nperceptions and self-reported sharing behaviors, we provide\nvaluable insights for researchers and privacy practitioners\nto better understand users and better design new Privacy\nEnhancing Technologies (PETs) and Transparency Enhanc-\ning Technologies (TETs) related to location data. We also\nshowed the importance of displaying a more interactive risk\nanalysis to make users better aware of privacy risks."}]}