{"title": "LLMs are One-Shot URL Classifiers and Explainers", "authors": ["Fariza Rashid", "Nishavi Ranaweera", "Ben Doyle", "Suranga Seneviratne"], "abstract": "Malicious URL classification represents a crucial aspect of cyber security. Although existing work comprises numerous machine learning and deep learning-based URL classification models, most suffer from generalisation and domain-adaptation issues arising from the lack of representative training datasets. Furthermore, these models fail to provide explanations for a given URL classification in natural human language. In this work, we investigate and demonstrate the use of Large Language Models (LLMs) to address this issue. Specifically, we propose an LLM-based one-shot learning framework that uses Chain-of-Thought (CoT) reasoning to predict whether a given URL is benign or phishing. We evaluate our framework using three URL datasets and five state-of-the-art LLMs and show that one-shot LLM prompting indeed provides performances close to supervised models, with GPT 4-Turbo being the best model, followed by Claude 3 Opus. We conduct a quantitative analysis of the LLM explanations and show that most of the explanations provided by LLMs align with the post-hoc explanations of the supervised classifiers, and the explanations have high readability, coherency, and informativeness.", "sections": [{"title": "I. INTRODUCTION", "content": "Phishing attacks over emails and other similar channels such as SMS remain a significant concern in cybersecurity. More often than not, a security breach can be traced back to an initial entry caused by a phishing attack. A recent report sheds light on this issue, noting a 40% increase in phishing attacks between 2022 and 2023, with over 709 million attempts by users to access phishing links blocked by Kaspersky [28]. With attackers' ability to reach a wide audience in a relatively short time and using short-lived campaigns, defending against phishing remains a highly challenging task that requires multi-faceted solutions.\nTypical blacklisting and whitelisting methods for phishing detection are ineffective in large-scale and dynamic environments. Typically, there is a delay between identifying a phishing campaign and blacklisting the related URLs, by which time the campaign may have already succeeded [45]. Consequently, machine learning-based solutions have been proposed for phishing URL detection. These methods extract features from emails and URLs, such as word counts, word lengths, and character distribution, as well as external data like WHOIS and IP information [25, 7, 19]. The extracted features are then used to train machine learning and deep learning models to predict whether a given email or URL is benign or phishing.\nAmong these solutions, some research focuses on predicting phishing links based solely on their URL patterns. This approach has several advantages: it does not require visiting the web page to gather the necessary features [56], it is resilient to evasion techniques like cloaking [66], and it is more suitable for detecting zero-day phishing campaigns for which prior information is unavailable. The intuition behind these methods is that phishing URLs often display distinct patterns, such as random string sequences and text patterns that mimic known URLs, along with unique artefacts from domain generation algorithms. In contrast, benign URLs tend to maintain consistent and recognisable characteristics [44].\nAlthough current state-of-the-art URL classifiers such as URLNet [31] and URLTran [39] achieve high detection accuracies on test URLs from the same source as their training data, their performance drops significantly on test URLs from diverse sources. This decline is mainly due to high false positive rates, indicating a poor understanding of benign URL indicators [50]. One reason for this issue is the inherent biases in URL datasets, which are collected at different vantage points of various networks. Moreover, data from one organisation will be biased towards URLs frequently visited by its employees. Sharing data between organisations could potentially address these biases; however, privacy and commercial concerns make it impractical.\nFurthermore, existing URL classification models are often black boxes, lacking explainability, which is crucial for user awareness and training. That is, given a model's prediction for whether a specific URL is benign or phishing, it is useful to provide users with brief explanations and warnings to help them make informed decisions [54, 22, 14]. Some examples include Outlook [46] and Google [21] which provide users with warnings when they accidentally try to access a suspicious link. Althobaiti et al. [9] further states that in phishing attack detection it is necessary to empower the end-users with more knowledge about a specific situation where a URL is potentially unsafe. Most existing tools that address this are either aimed at experts in the cyber security domain or only provide users with binary decisions about the URL. Such binary advice fails to gain the trust of the users when the decisions have a high false positive rate. For this reason it is important to provide users with simple and easy-to-understand explanations for a certain URL classification.\nRecent advances in large language models (LLMs) offer potential solutions to the generalisability and explainabil-ity problems of phishing URL classifiers. Trained on vast amounts of data, LLMs excel in natural language text gener-ation and reasoning and are useful for various tasks. Works such as [27, 12, 37] have demonstrated LLMs' predictive performance, which has been enhanced by their recently discovered in-context learning capabilities such as few-shot learning [29, 16]. Given that LLMs are trained on a significant portion of internet data, they likely have some knowledge of benign URLs in a broader context. This knowledge can be combined with in-context learning to build URL classifiers, which is the focus of this paper.\nTo this end, in this paper, we propose an LLM-based phish-ing URL detection framework that integrates both technolog-ical and human components of an ideal defence mechanism against phishing campaigns. Our framework leverages LLMs' one-shot learning capabilities for phishing URL classification and provides explanations for each classification. By leverag-ing on the success of Chain-of-Thought (CoT) reasoning [62], we prompt the LLM to consider the benign and phishing characteristics of a given URL and then make a prediction, including self-reasoning. In our prompt, we include one URL, an explanation of its benign and malicious characteristics, and a prediction as illustrated in Figure 1. Our results show this approach achieves detection accuracies comparable to supervised deep learning methods trained on large URL datasets. Additionally, the LLM's reasoning provides natural language explanations that enhance user awareness of benign and phishing URL characteristics, addressing the limitations of existing classifiers that only offer a prediction."}, {"title": "II. RELATED WORK", "content": "A. Phishing URL Detection\nAutomated phishing URL classification is a necessity to safeguard users from accessing phishing sites. Multiple ex-isting solutions utilise machine learning and deep learning methods, which are trained using features extracted from the URLs, landing pages, or external sources. These features, broadly categorised as blacklist features, lexical features, host-based features, and content-based features [56], have been used to train various types of machine learning and deep learning classification models [42, 35, 48]. WHOIS information such as domain age and page rank from the Alexa top domains list are also popular features used in various works such as [63, 20, 47, 7, 19].\nOne significant disadvantage of using the above features for detecting phishing URLs is the difficulty in obtaining them in real-time. Blacklist features can be redundant for short-lived or zero-day phishing URLs, as these exploit the delay in blacklist updates. Acquiring hosting information during the training phase is often impossible for short-lived URLs, and using web-page content features risks accidental malware downloads and is vulnerable to evasion techniques [45]. Consequently, some URL classifiers use only URL-based features, which eliminates reliance on third-party sources, mitigates malware risks and reduces vulnerability to evasion. Our work focuses on these URL-based phishing detectors that use either hand-crafted URL features or an embedding representation of the URL as the primary input feature. Works such as [38, 55, 49] collected hand-crafted features like URL length, word counts and presence of special characters to train Random Forests and Support Vector Machines. In contrast, the URLNet [31] trains a deep learning-based classifier using word and character embeddings to represent the URL, and Maneriker et al. [39] proposed URLTran, a model which leveraged state-of-the-art transformer models for phishing detection. In comparison to URLNet, Maneriker et al. report that URLTran returns a true positive rate (TPR) relative improvement of 21.9% at the false positive rate (FPR) of 0.01%.\nThe high prediction accuracies reported in existing works were obtained by evaluating URL classifiers on test URLs collected from the same source as the training URLs. When evaluated on data collected from different sources or repre-senting domain shift, these URL classifiers demonstrated low generalisability. Our previous work [50] found that the cross-dataset performance of four URL classifiers (URLNet [31], URLTran [39], CatchPhish [49], and PhishRF [55]) falls by 10%-30% in comparison to their performance on test URLS collected from the same source as the training URLs.\nTo this end, in this paper, we propose our one-shot frame-work for using LLMs as URL classifiers and demonstrate that its performance is better than the cross-data performance of URL classifiers trained in supervised settings. To the best of our understanding, this work is the first to design an LLM-based one-shot URL classification framework.\nB. Few-shot classification using LLMs\nLarge Language Models (LLMs) have demonstrated supe-rior performance in a variety of natural language process-ing tasks, including text generation, question-answering, and text classification [3, 58, 43]. Recently, their generalisability has seen further improvements within the in-context learning paradigm where the LLM performs tasks unseen during train-ing when prompted with task-specific instructions and a few or no (i.e., few-shot or zero-shot) labelled examples [16, 29].\nVarious works applied the in-context learning capabilities to solve a range of problems, beyond typical natural language processing tasks. For instance, Hegselman et al. [23] and Jaitly et al. [26] applied LLM-based few-shot classification to tabular data. The authors evaluate various serialisation methods and demonstrate the superior performance of few-shot LLMs in comparison to deep learning-based classification on tabular datasets. Other works such as [33, 64, 60] applied few-shot learning for domain-specific classification tasks. For instance, Li et al. [33] evaluated the application of LLMs in biological inference. They prompted LLMs to predict the synergy of drug pairs in rare tissues by including a few labeled examples in the prompt. In contrast, Yang et al. [64] applied few-shot LLMs in a cross-modal framework for several audio tasks such as speech emotion classification, audio classification, and text-to-speech classification, while Van et al. [60] investigated the few-shot capabilities of vision language models on hateful meme detection.\nSeveral work utilised the capabilities of LLMs in cyberse-curity. Nonetheless, the efforts have been much more focused on fine-tuning than few or zero-shot settings. For example, Aghaei and Al-Shaer [5] proposed SecureBERT, which is a security-specific fine-tuned BERT model that achieves F1 scores ranging from 95% to 98% in classifying Common Vulnerabilities and Exposures (CVEs) to their corresponding MITRE ATT&CK techniques. The authors also demonstrated that SecureBERT outperforms ChatGPT in Tactics, Tech-niques, and Procedures (TTP) prediction. Balasubramanian et al. [13] proposed finetuned GPT-3 models as anomaly detection classifiers which achieve more than 99% accuracy.\nIn contrast to these works, we propose a one-shot LLM classification approach for phishing URL detection, which can also be applied in zero-shot and few-shot settings. Our approach does not require fine-tuning of the language model parameters and demonstrates prediction performances com-parable to supervised approaches.\nC. Explainability of LLMs as classifiers\nThe recent success of LLMs in varied domains has addi-tionally motivated researchers to assess LLMs\u2019capabilities as model explainers. Traditionally, explainability algorithms explain black-box model predictions to elucidate the inner model mechanisms. Metrics used to evaluate these algorithms, therefore, assess faithfulness (i.e., how well the explanations reflect the true inner mechanism of the model) and plausi-bility (i.e., how well the explanations correlate with human reasoning), aiming to enhance interpretability and trust in the otherwise black-box automated decision-making process. Studies that assess the explainability of LLMs include [30, 15], which implements LLMs as post-hoc explainers of predictions generated by other models. Meanwhile, LLM chain-of-thought reasoning has been considered as self-explanations for their predictions/classifications [18] and in comparison to traditional explanation methods, LLM natural language explanations have been noted for their high plausibility. That is, these explana-tions are consistent with human logic and reasoning [4, 59]. Nonetheless, due to the complexity of large language model architectures and the variability in its outputs, the application-specific correctness of LLM text generation remains largely unexplored.\nTherefore, in this paper, we propose a method to bridge this gap for URL classification self-explanations. As our one-shot classification framework outputs both explanation and prediction, we assess the correctness of the URL benign and phishing indicators identified in the self-explanations by comparing them to post-hoc explanations obtained through a supervised training setting."}, {"title": "III. OUR FRAMEWORK", "content": "As mentioned in the introduction, our aim is to address the problem of generalisability and explainability in phishing URL detection through a one-shot LLM-based classification frame-work. We implement a lightweight approach that leverages LLMs' vast embedded knowledge to improve classification accuracy while reducing false positive rates. This approach also provides explanations for each prediction to improve usability and empower end-users with more knowledge about a given URL prediction.\nOur framework consists of prompting the LLM with the instruction and one example, as shown in Figure 1. Following the prompt we provide the URL for which we require an output from the LLM. The objective of this prompt is to not only obtain a single word prediction but also obtain the reasoning or explanation that leads to the prediction. In particular, we prompt a targeted form of LLM reasoning by specifically instructing the LLM to consider the benign and phishing characteristics of the URL. This form of targeted reasoning taps into LLM knowledge regarding legitimate entities and websites present on the web. It also encourages the LLM to consider the structure of the URL and whether it reflects phishing characteristics.\nIn finalising our framework we faced the following technical challenges:\nPrompting for classification: To query an LLM about whether a given URL is benign or phishing we experi-mented with two variations of prompting. The first type gave the LLM the following four options to choose from when deciding a classification - benign/phishing/poten-tially phishing/uncertain. This was motivated by the idea that allowing more granular labelling options would drive the LLM to make more precise decisions. We processed the LLMs' outputs to this prompt by considering all benign labelled URLs as benign and the rest as phishing. That is, it is safer to consider a URL as phishing if there is any uncertainty or potential for harm. The second type of prompting we tried is the one used in our proposed framework. Our initial analysis showed that the second prompt style returned slightly higher accuracy than the first prompt. The reason for this is that the availability of the 'potentially phishing' and 'uncertain' label options encouraged the LLM to take a cautious approach during classification. As a result, many benign URLs with less well-known domains were labelled as 'uncertain' or 'po-tentially phishing', thereby increasing the false positive rate. In contrast, the second prompt type (our proposed framework) encouraged the LLM to be more decisive about benign URL domains which improved the overall classification accuracy.\nChain-of-Thought reasoning: Some of our initial exper-iments verified that using chain-of-thought (CoT) reason-ing before making a prediction improved detection accu-racy. To implement this in our prompt we experimented with two versions of CoT prompting. The first version prompted the LLM to consider the domain, subdomain and path of the URL separately and identify benign and phishing characteristics before making a prediction. The second version simply prompted the LLM to consider the benign and phishing characteristics (this is the ver-sion used in our final proposed method). With the first CoT prompt version, we found that explicitly identifying benign and phishing characteristics in the various URL components caused the LLM to place unnecessary weight on these features which resulted in lower overall predic-tion accuracy. Instead allowing the LLM more flexibility in its reasoning by simply prompting it to consider the general benign and phishing characteristics of the URL (second CoT prompt version) resulted in higher overall detection accuracy.\nNumber of examples to include in prompt: We ex-perimented with varying numbers of few-shot examples to assess if increasing the number of examples improved the prediction accuracy. Besides the one-shot approach, we experimented with two examples (one phishing URL and one benign URL) and five examples (three benign URLs and two phishing URLs). Our analysis showed that increasing the number of examples did not improve the prediction accuracy by a significant amount and therefore decided on a one-shot approach.\nFigure 2 illustrates the overall flow of our framework. In particular, we show how we consider the first part of the output to be a self-explanation for the prediction that follows it. In the following section, we evaluate the performance of five large language models in returning accurate predictions for URLs in such a one-shot setting. We also evaluate the quality of the self-explanations that accompany the prediction.\nHere, note that we operate in a one-shot setting, i.e., we use only one URL (i.e., the Google Scholar URL as the sole example) to generate all of our results in Section V. We emphasise that our prompt is the same for all of our results (i.e., it is the same query with the same Google Scholar URL). Later in Section V-A, we show the viability of our approach in zero-shot and few-shot settings as well."}, {"title": "IV. EXPERIMENT SETTINGS", "content": "In this section, we first introduce the datasets we use to evaluate the prediction accuracy of our framework. Next, we describe how we evaluate the prediction performance of LLMs as one-shot URL classifiers in comparison to supervised URL classifiers, followed by the experiment settings we use to evaluate the quality of LLM self-explanations.\nA. Datasets\nWe use two publicly available URL datasets and another URL dataset that was collected as part of our previous work [50].\n1) ISCX-2016 Dataset: Collected by Mamun et al. [38], this dataset consists of 35,300 benign URLs collected by crawling Alexa top domains and various malicious URL types, of which 9,964 are phishing URLs sourced from OpenPhish.\u00b9 Although the dataset contains more than 100,000 URLs altogether, we randomly selected 10,000 benign URLs and combined them with the phishing URLs to obtain a more balanced dataset.\n2) EBBU-2017 Dataset: This dataset was introduced in [55], comprising 36,400 benign URLs and 37,175 phishing URLs. The benign URLs were collected by querying a specific list of words, constructed by the authors of the paper, on the Yandex search engine\u00b2 and selecting the highest-ranked URLs that were returned from this query. The phishing URLs were collected from PhishTank.\u00b3\n3) HISPAR-Phishstats (HP) Dataset: This dataset was intro-duced in our previous work [50]. We collected 46,250 benign URLs from the HISPAR list [11] such that the set comprised 1,850 unique domains from the Alexa top domains list with 25 different URLs for each domain. The dataset also consisted of 46,250 unique phishing URLs, which we collected from Phishstats\u2074 between the 19th of June and the 26th of August 2022.\nTo train the supervised training baselines, we randomly split these datasets into training, validation, and test sets in a 60:20:20 ratio as summarised in Table I. To evaluate our LLM-based framework, we use a randomly sampled subset of 1,000 URLs from the test splits of each dataset. We limit our analysis to 1,000 samples due to cost considerations when accessing some of the LLM APIs.\nB. Accuracy comparison with supervised URL classifiers\nWe evaluate the prediction performance of LLMs under our framework by comparing their accuracy to state-of-the-art URL classifiers trained in a fully supervised setting. For this, we refer to our previous work[50] where we trained four state-of-the-art URL classifiers: URLNet [31], URLTran [39], CatchPhish [49] and PhishRF [55]. We trained and tested the performance of these classifiers using the training and testing datasets described in Table I. We present the performance of the supervised URL classifiers when applied to each test dataset in Table II. Here, note that the models work well when they are tested on the test set which comes from the same dataset. For example, the URLtran model trained on the ISCX dataset achieves an F1 score of 0.99 when tested on the ISCX test set. However, its performance drops significantly to F1 scores of 0.69 and 0.68 when tested on the EBBU and HP test sets, respectively.\nWe obtain the one-shot prediction accuracies under our framework using the 1,000 randomly selected samples of the test datasets in Table I as described in Section IV-A. That is, in each query, we prompted the LLMs to reason and predict the label of one given URL from the test sets. As our framework specifically instructed for the prediction to be stated in the final word of the output as either 'benign' or \u2018phishing' we considered each output prediction to be either benign or phishing if the last 20 characters of the output contained the terms 'benign' and 'phishing' respectively. If the last 20 characters contained neither of the terms we considered the prediction to be 'uncertain'. However, in the calculation of accuracy we counted the 'uncertain' predictions as phishing, considering that would be the safer decision in a practical setting.\nWe tested five state-of-the-art LLMs - GPT 4-Turbo [3], Claude 3 Opus [10], Gemini [57], LLaMA 3 [40], and LLAMA 2 [58]. We repeated our experiments for each LLM and each test dataset five times and calculated the average F1 score. We select the F1 score as a performance metric as it accounts for the true positive rate as well as the false positive rate. We compare the F1 scores of each LLM and each test set under our framework with the F1 scores obtained by supervised URL classifiers in Table II. Since LLMs can generate different outputs to the same query at different times, for each of the 1,000 URLs, we queried the LLM five times and report the average performance results together with the standard deviation. We present these results in Section V-A.\nC. Quality evaluation of LLM Self-explanations\nWe evaluate the quality of the LLM self-explanations from two aspects. First, we compare the benign and phishing indi-cators given by LLMs with the indicators returned by a post-hoc explanation method (i.e., LIME [51]) applied to the corre-sponding supervised URL classifier. This is to justify further that the few-shot predictions the LLMs make are logical and often have the same reasoning as the corresponding supervised learning classifier that has trained on large volumes of training data. Second, we evaluate the general quality of the LLM-generated explanations in terms of readability, coherence, and informativeness using the G-Eval [36] framework.\n1) Alignment between LLM and LIME indicators: Classi-fying a given URL as benign or phishing is an overall decision made by the classifier observing various indicators of a given URL. The datasets in this area (cf. Table I) consist of only URLs and their corresponding labels. Therefore, there is no gold-standard ground truth to the explanations provided by the LLM on benign and phishing indicators.\nAs a result, we establish a proxy metric by comparing the LLMs' self-explanations to the indicators returned by a post-hoc explanation method applied to a supervised URL classifier. Specifically, we select URLTran as it returned the highest average F1 score across the three datasets (cf. Table II) in the supervised setting. We applied the LIME algorithm [67] to three separate URLTran models trained on each of the three training sets and obtained LIME indicators for the models' predictions for their respective test sets.\nThe LIME algorithm determines feature attributions of a given test sample by approximating a sparse linear model based on the model predictions for inputs sampled from the neighbourhood of that test sample. Feature importance is then determined by the feature coefficients of that linear model. As each URLTran model returned F1 scores of 0.99 for their respective test sets, we assume that the sparse linear model learned by LIME and, therefore, the indicators it returned are a true reflection of the benign and phishing characteristics represented within labelled URL datasets.\nBy comparing the LLM self-explanations with the LIME indicators, we can verify how well benign and phishing char-acteristics identified by LLMs align with label-specific features learnt by supervised models from URL-labelled datasets. We measure this alignment using Jaccard similarity.\nJaccard Similarity: To numerically compare the two sets of indicators (i.e., LLMs and LIME), we used the Jaccard similarity coefficient [24], which calculates a similarity score between two sets as the ratio of the intersection of the sets and their union: $J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$. Here, the label-specific LIME indicators form one set of elements and the indicators identified by the LLM self-explanation form another set. As each self-explanation was in natural language, we extracted the specific URL components that it identified using LLM prompting.\nAs the original LLM output predicted the URL as benign, we collect the benign self-explanation indicators delimited by special characters (A: 'com', 'ES', 'https') and compare them with corresponding benign LIME indicators (B: 'com', 'ES', 'reciclatex', 'https', 'home'). Using the Jaccard similarity measure, this corresponds to:\n$J(A, B) = \\frac{|\\{'com', 'ES', 'https\\'}\\}|}{|\\{'com', 'ES', 'https', 'home', 'reciclatex\\'}\\}| = 0.6$ (1)\nThe Jaccard similarity score ranges from 0 to 1, where higher similarity scores indicate a greater overlap between self-explanation indicators and LIME indicators. We conduct the above evaluation for each of the LLMs that we evaluate under our framework and for each of the three datasets. We report these results in Section V-B1.\n2) G-Eval Framework: To evaluate the general quality of LLM outputs, we use the G-Eval framework [36], which im-plements LLMs to evaluate the quality of a text. Specifically, the framework uses a chain-of-thought approach to use an LLM to evaluate a text against a given metric. In order to be useful to the end-users of a URL classification system, we assess the self-explanations according to the following metrics: readability, coherence, and informativeness. We adopt the definition of 'readability' provided in [6] for cyber secu-rity question-answering tasks, and we adapt the definitions of 'coherence' and 'informativeness' from [17] to suit our application as described below.\nReadability (1-5): Assesses how easily the average reader comprehends a text, considering factors like lexi-cal, syntactic, semantic, and stylistic complexity.\nCoherence (1-5): Assesses the collective quality of all sentences. This metric measures how well the statement is structured and organised in explaining why the URL is predicted to be either benign or phishing.\nInformativeness (1-5): Measures how well the output answers the question. That is, this metric assesses how clearly the statement considers the benign and phishing characteristics of the URL and finally provides a predic-tion.\nIn the G-Eval framework, an LLM evaluates a given text against a given criteria and the criteria definition by following a sequence of evaluation steps generated by the LLM itself through chain-of-thought reasoning. That is, an LLM is given a criterion and its definition and prompted to produce a series of steps that it should follow to evaluate a text against that criterion for a specific task. As we are investigating five LLMS for prediction accuracy and explanation quality under our one-shot framework, we use a different LLM (GPT-40) in our G-Eval implementation to evaluate the other five LLMs' self-explanations."}, {"title": "V. RESULTS", "content": "A. Prediction performance of one-shot LLM URL classifiers\nIn Table III, we present the F1 scores of our one-shot URL classification framework on the five LLMs we test (cf. Section IV-B): GPT 4-Turbo, Claude 3 Opus, Gemini, LLaMA 3, and LLaMA 2. As explained in Section IV-B, these results are the average F1 score calculated over five repeats of the experiment. For ease of comparison, we also present the F1 scores obtained by URLTran (i.e., best-performing model under the supervised setting - cf. Table II) when applied to each test set after being trained on the respective training set. We observe that among the LLMs, GPT 4-Turbo produces the highest average F1 scores across all three datasets, with an overall average of 0.92. This is only 0.07 less than the performance of the fully supervised learned classifier. Claude demonstrates the next best performance with an overall average of 0.88, while Gemini and LLaMA 3 perform similarly, with overall averages of 0.83 and 0.84, respectively. Meanwhile, LLaMA 2 returns the lowest overall average F1 score of 0.68. The standard deviations over five repeats are very low for all the LLMs portraying overall consistency in accuracy over the five independent runs.\nA more detailed analysis of the LLM outputs revealed that one of the reasons for LLaMA 2's significantly low perfor-mance may be its inability to generate good self-explanations. For many of the URLs, LLaMA 2 returned a prediction without considering the benign and phishing characteristics of the URL, as our prompt had asked. Furthermore, even when the LLMs did consider the benign and phishing characteristics of the URL, in comparison to the other LLMs, GPT 4-Turbo returned more accurate predictions.\nB. Quality of LLM outputs\n1) Alignment between LLM self-explanations and LIME indicators - Jaccard Similarity: As mentioned before, the purpose of this evaluation is to justify further that the one-shot predictions the LLMs make are logical and often have the same reasoning as the corresponding supervised learning classifier that has trained on large volumes of training data. From our repeated experiments to assess the prediction performance of our one-shot classification framework, we obtained five outputs for each URL from each of the five LLMs. Therefore, to evaluate the self-explanations produced by our framework, for each URL, we applied our Jaccard similarity evaluation to each of the five repeats for each URL.\nMore concretely, for each URL, we calculate the mean Jaccard similarity score to reflect the overall alignment be-tween its self-explanation indicators and LIME indicators. In Table IV we show the proportion of the Jaccard scores that were above 0.5 for each dataset and each model. Here, we find that for GPT 4-Turbo, 72%, 69%, and 56% of the Jaccard similarity scores were above 0.5 for the HP, EBBU, and ISCX test sets, respectively. These proportions gradually decrease for Claude 3, Gemini and LLaMA 3. For LLaMA 2 we find the lowest proportions, reflecting that very few of its LLM self-explanations were aligned with the LIME indicators.\nOverall, these results indicate that the self-explanations returned by GPT 4-Turbo and Claude 3 are more consistent with the benign and phishing characteristics represented in URL datasets. As observed in the examples shown in the previous section, this is likely due to GPT 4-Turbo and Claude 3's ability to describe specific URL components as benign and phishing indicators when reasoning the predictions that both make.\n2) G-Eval Analysis: We present the probability density functions (PDF) of the scores returned for each metric, LLM and dataset by the G-Eval framework.\nWhile coherence assessed the structure and logical flow of the self-explanations, informativeness assessed how well the statements answered the original prompt and considered the benign and phishing characteristics of the URL. For instance, LLAMA 2 and GPT 4-Turbo returned the following explana-tions for the URL https://pizza.dominos.com/missouri/hollister/.\nOverall, this analysis shows that the top-performing LLM outputs are useful for the end-users of a phishing detection application. The three metrics we analysed here assessed the ease with which a user can comprehend the text (readability), the logical flow of arguments (coherence) and how well the text answered the question while identifying benign and phishing characteristics (informativeness). Besides LLaMA 2, the remaining four LLMs generally scored well in terms of all metrics, demonstrating a high overall quality of explanation. However, additionally accounting for GPT 4-Turbo's superior prediction accuracy, we can conclude that GPT 4-Turbo is the best overall performing model within our framework."}, {"title": "VI. EXTENDED ANALYSIS", "content": "In this section, we present an extended analysis of the various aspects of our framework.\nA. Zero-shot and five-shot prompting\nFirst, we investigate to what extent including the example in the query helps the final performance. To this end, we conduct experiments for zero-shot and five-shot settings. To evaluate our framework under a zero-shot approach, we repeated the same approach as in Section IV-B without any examples in the prompt. For the five-shot approach, we generated self-explanations and predictions for five URLs by prompting instructions shown in Figure 2 to GPT 40 (i.e., a different LLM than the ones we evaluate).\nFor our zero-shot analysis, we repeated the experiment five times as was done in Section IV-B; however, for the five-shot approach, we conducted the experiment only once. This is because of the cost considerations of the higher context window size of the five-shot setting.\nWe report the results in Table VI. We can observe overall that GPT4-Turbo remains the best-performing LLM in both the zero-shot and five-shot settings, and its average F1 score across the three datasets is 0.92, which is the same as the one-shot performance (cf. Table III). The performance of the other LLMs seems to vary in the zero-shot and five-shot settings. For example, Claude 3's F1 scores are 0.91, 0.88, and 0.87 for zero-shot, one-shot, and five-shot settings, respectively. The corresponding values for Gemini are 0.86, 0.84, and 0.84. Meanwhile, LLaMA 3 and LLaMA 2 experience the great-est increases. The average F1 scores for Llama 3 are 0.84, 0.84, and 0.89 for zero-shot, one-shot, and five-shot settings. For LLaMA 2, the corresponding numbers are 0.48, 0.68, and 0.80.\nIn summary, although increasing the number of examples in the prompt improves performance for some models, the top-performing GPT-4 Turbo remains unaffected by the number of examples. This finding is significant because it shows that solutions using our framework can save costs by employing zero or one-shot settings, reducing the number of querying tokens without compromising performance.\nB. Increasing the training data size in the supervised setting\nNext, we investigate whether we can train the supervised training models using less training data and how the one-shot results stand compared to those. To this end, we progressively train the URLTran models using 10%, 20%, and so on, up to 100% of the training data.\nFor each training dataset, we observe that there is an increase in F1 scores of the same test set as the training set size increases, and the scores are usually above 0.95.\nWhile the same dataset test performance is high for this, the cross-dataset performances are particularly low when the HP-trained"}]}