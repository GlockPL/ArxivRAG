{"title": "Graph Representation Learning via Causal Diffusion for Out-of-Distribution Recommendation", "authors": ["Chu Zhao", "Enneng Yang", "Yuliang Liang", "Pengxiang Lan", "Yuting Liu", "Jianzhe Zhao", "Guibing Guo", "Xingwei Wang"], "abstract": "Graph Neural Networks (GNNs)-based recommendation algorithms typically assume that training and testing data are drawn from independent and identically distributed (IID) spaces. However, this assumption often fails in the presence of out-of-distribution (OOD) data, resulting in significant performance degradation. In this study, we construct a Structural Causal Model (SCM) to analyze interaction data, revealing that environmental confounders (e.g., the COVID-19 pandemic) lead to unstable correlations in GNN-based models, thus impairing their generalization to OOD data. To address this issue, we propose a novel approach, graph representation learning via causal diffusion (CausalDiffRec) for OOD recommendation. This method enhances the model's generalization on OOD data by eliminating environmental confounding factors and learning invariant graph representations. Specifically, we use backdoor adjustment and variational inference to infer the real environmental distribution, thereby eliminating the impact of environmental confounders. This inferred distribution is then used as prior knowledge to guide the representation learning in the reverse phase of the diffusion process to learn the invariant representation. In addition, we provide a theoretical derivation that proves optimizing the objective function of CausalDiffRec can encourage the model to learn environment-invariant graph representations, thereby achieving excellent generalization performance in recommendations under distribution shifts. Our extensive experiments validate the effectiveness of CausalDiffRec in improving the generalization of OOD data, and the average improvement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and 11.65% on Douban datasets. Our implementation code is available at https://github.com/user683/CausalDiffRec.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph Neural Networks [35]\u2013[37], due to their exceptional ability to learn high-order features, have been widely applied in recommendation systems. GNN-based recommendation algorithms [1], [38], [39] learn user and item representations by aggregating information from neighboring nodes in the user-item interaction graph and then computing their similarity to predict user preferences. In addition, researchers have introduced various other techniques to continuously improve GNN-based recommendation algorithms. For example, integrating attention mechanisms [8], [40] with knowledge graphs [41] led to improving recommendation accuracy. Furthermore, the introduction of contrastive learning aims to improve the robustness of recommendation algorithms [42], [42], [43].\nDespite the significant progress these methods have made in improving recommendation accuracy, most of them assume that the distribution of the test dataset and the training dataset is independently and identically distributed (IID) and focus on enhancing recommendation performance based on this assumption. Unfortunately, methods based on the above assumption fail to generalize excellent recommendation performance to out-of-distribution (OOD) data [47]\u2013[49], where the distribution of the test data significantly differs from that of the training data. In Figure 1, we present a simple example to illustrate how the popularity of medical supplies changes with the environmental factor of COVID-19. Specifically, due to the pandemic, people may be required by the government to stay at home, leading to reduced outdoor activities. During this period, while purchasing medical supplies, people may also increase their demand for fitness equipment and electronic products. The recommender system might learn that users who purchase masks also frequently buy fitness equipment and electronic products, significantly increasing the popularity of these items. This correlation is driven by the common factor of the 'pandemic' rather than a direct causal relationship between the items. When the pandemic ends (i.e., the environmental factors change), the popularity of masks decreases, and people's demand for fitness equipment and electronic products diminishes. The recommender system, relying on the unstable correlations learned during the training phase, may incorrectly recommend fitness equipment and electronic products to users who purchase masks, resulting in poor performance under the new distribution. Furthermore, we construct IID and OOD test sets on the Yelp2018 dataset to illustrate the changes in model performance. As shown in Figure 1, LightGCN's [3] performance on the OOD dataset, compared to the IID dataset, shows an average decline of 29.03% across three metrics. Such issue of GNN-based models lacking robustness on OOD datasets inspires us to propose a recommendation framework with strong generalization capabilities for distribution shifts.\nSeveral works [19], [20] have focused on improving the generalization ability of recommender systems on OOD datasets. Some researchers use causal inference to counteract shifts in the data distribution. For example, CausPref [20] is based on the NeuMF [60] method and designs invariant user preference causal learning and anti-preference negative sampling methods to improve model generalization. COR [19] uses the Variational Auto-Encoder for causal modeling by incorporating an encoder to infer unobserved user features from historical interactions. Although the aforementioned methods have improved the performance of recommendation models on out-of-distribution datasets to some extent, they are not specifically designed for GNNs, making direct migration to GNN-based methods difficult. Other researchers employ techniques such as graph contrastive learning and graph data augmentation to enhance the robustness of GNN-based recommendation algorithms, such as SGL [14], SimGCL [15], and LightGCL [31]. These methods mainly address noise or popularity bias in the data, but they struggle to achieve good recommendation performance when the test data distribution is unknown or has multiple distributions. As shown in the experimental results of Tables II, these methods perform worse than other baselines in other types of data distribution shifts. In recent literature, few GNN-based methods [23], [61] have been proposed to improve generalization when faced with multiple distributions. However, these methods lack solid theoretical support.\nGiven these limitations, there is an urgent need to design theoretically grounded GNN-based methods to address distribution shifts. In this paper, we use invariant learning to improve the generalization of the OOD dataset. Utilizing insights from the prior knowledge of environment distribution and invariant learning [44]\u2013[46] enhances model stability across varied environments. This is achieved by acquiring invariant representations, which in turn boosts the model's generalization capabilities and overall robustness. However, designing models based on invariant learning still faces the following two challenges:\n1) How to infer the distribution of underlying environments from observed user-item interaction data?\n2) How to recognize environment-invariant patterns amid changing user behaviors and preferences?\nTo address the aforementioned challenges, in this paper, we first construct the Structural Causal Model (SCM) to analyze the data generation process in recommender systems and investigate the learning process of GNN-based recommendation algorithms under data distribution shifts. We conclude that latent environmental variables can lead GNN-based algorithms to capture unstable correlations related to the environment, which is the key reason for the failure of GNN-based models to generalize on OOD data. Furthermore, we propose a novel approach called graph representation learning via Causal Diffusion (CausalDiffRec) for OOD recommendation, which leverages causal inference to eliminate unstable correlations caused by environmental variables. This approach aims to learn invariant representations across different environments, thereby achieving OOD generalization in recommender systems. Specifically, CausalDiffRec consists of three main components: an environment generator, an environment inference, and a diffusion module. The environment generator is used to create K significantly different graphs to simulate data distributions under various environments. The environment inference module then infers the environment components from these generated graphs and uses them as input for the diffusion reverse stage to guide invariant graph representation learning. Finally, we provide theoretical proof that CausalDiffRec, under the conditions of invariant learning theory, can identify invariant graph representations across different environments, thereby improving generalization performance on OOD data. The contributions of this paper are concluded as follows:\n\u2022 Causal Analysis. We construct the SCM and analyze the generalization ability of GNN-based recommendation models on OOD data from the perspective of data generation. Based on our analysis and experimental results, we conclude that environmental confounders lead the model to capture unstable correlations, which is the key reason for its failure to generalize under distribution shifts.\n\u2022 Methodology. We propose a novel GNN-based method, CausalDiffRec, for OOD recommendation. CausalDiffRec primarily consists of three modules: environment generation, environment inference, and the diffusion module. The environment generation module simulates user data distributions under different conditions/environments; the environment inference module employs causal inference and variational approximation methods to infer the environment distribution; and the diffusion module is used for graph representation learning. Our theoretical analysis guarantees that optimizing the objective function of CausalDiffRec enables the model to achieve great generalization.\n\u2022 Experimental Findings. We constructed three common types of distribution shifts across four datasets and conducted comparative experiments. The experiments demonstrate that CausalDiffRec consistently outperforms baseline methods. Specifically, when dealing with OOD data, CausalDiffRec exhibits enhanced generalization capabilities, achieving a maximum metric improvement rate of 36.73% compared to the baseline methods."}, {"title": "II. PRELIMINARY", "content": "A. GNN-based Recommendation\nGiven the observed implicit interaction matrix $R \\in {0,1}^{m\\times n}$, in which $U = {u_1, u_2,..., u_m}$ represents the set of users, $I = {i_1, i_2, ..., i_n}$ represents the set of items, $m$ and $n$ denote the number of users and items, respectively. For the elements in the interaction matrix, $r_{ui} = 1$ indicates an interaction between user $u$ and item $i$, otherwise 0. In GNN-based recommendation algorithms, the user-item interaction matrix $R$ is first transformed into a bipartite graph $G = {V,E}$. We employ $V$ to represent the node set and $E = {(u, i)|u \\in U, i \\in I, r_{ui} = 1}$ denotes the edge set. Given a user-item interaction graph $G_u$ and the true user interactions $y_u$ with respect to (w.r.t) user $u$, the optimization objective of GNN-based methods can be expressed as:\n$\\arg \\min_\\theta \\mathbb{E}_{(G_u,y_u)\\sim P(G,Y)}[l(f_\\theta(G_u; \\theta), y_u)],$   (1)\nwhere $f_\\theta(\\cdot)$ is a learner that learns representations by aggregating high-order neighbor information from the user-item interaction graph. $l$ denotes the loss function and $P(G,Y)$ represents the joint distribution of the interaction graph $G$ and true label $Y$.\nB. Denoising Diffusion Probabilistic Models\nDenoising Diffusion Probabilistic Models (DDPM) [25] have been widely used in the field of image and video generation. The key idea of DDPM is to achieve the generation and reconstruction of the input data distribution through a process of gradually adding and removing noise. It leverages neural networks to learn the reverse denoising process from noise to real data distribution.\nThe diffusion process in recommender system models the evolution of user preferences and item information through noise addition and iterative recovery. Initially, data $x_0$ sampled from $q(x)$ undergo a forward diffusion to generate noisy samples $x_1,...,x_T$ over $T$ steps. Each step adds Gaussian noise, transforming the data distribution incrementally [25]:\n$q(x_t|x_{t-1}) = \\mathcal{N} \\left(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t \\mathbf{I}\\right),$  (2)\nwhere $\\beta_t \\in (0, 1)$ controls the level of the added noise at step $t$. In the reverse phase, the aim is to restore the original data by learning a model $p_\\theta$ to approximate the reverse diffusion from $x_T$ to $x_0$. The process, governed by $p_\\theta(x_{t-1}|x_t)$, uses the mean $\\mu_\\theta$ and covariance $\\Sigma_\\theta$ learned via neural networks:\n$p_\\theta(x_{t-1}|x_t) = \\mathcal{N} \\left(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta (x_t, t)\\right) .$ (3)"}, {"title": "III. METHODOLOGY", "content": "In this section, we first construct SCM and identify environmental confounders as the key reason for the failure of GNN-based models to generalize on OOD (out-of-distribution) data. Subsequently, we introduce the variational inference to infer the true distribution of the environment. We use the diffusion model to learn the representation based on invariant learning. Finally, we provide rigorous theoretical proof of CausalDiffRec that can achieve great generalization. The model framework is illustrated in Figure 3.\nA. SCM of GNN-based Recommendation\nTo explore the reasons behind the failure of GNN-based models to generalize on OOD data, we follow previous works [27] [21] and first construct the SCM for data generation and data modeling in recommendation systems, as shown in Figure 2 (a) and Figure 2 (b). We find that environmental confounding factors are the key reason for the generalization failure of GNN-based methods. Finally, we design an intervention model in Figure 2 (c) to eliminate the impact of environmental confounding factors.\n1) Causal View in GNN-based Recommendation: In Figure 2, the three causal relations are derived from the definitions of data generation. The detailed causal analysis behind them is presented as follows:\n\u2022 $E$ denotes the unobserved environmental factors (i.e., sudden hot events or policies). $G$ and $Y$ denote the user-item interaction graph and the true label, respectively. $I$ is the invariant attribute of users and items unaffected by environmental factors, such as user gender and item category information. Previous work [20] has indicated that leveraging these invariant features can effectively enhance the model's generalization capability in OOD environments.\n\u2022 $E \\rightarrow G$. The direct influence of the environment on user-item interactions can be defined as $P(G|E)$. For example, if the environmental variable is the weather, users might interact more frequently with warm clothing in a cold environment.\n\u2022 $G \\rightarrow Y$. This represents the influence of the user-item interaction graph $G$ on the user behavior label $Y$. The GNN-based recommendation model $Y = f_\\theta(G)$ defines the relation. When the model parameters $\\theta$ are fixed, the mapping between $G$ and $Y$ is also deterministic.\n\u2022 $I \\rightarrow Y$. This indicates that the invariant user-item attributes directly influence the user behavior label $Y$. For example, a user may consistently prefer to dine at a particular restaurant, and attributes such as the restaurant's location and name generally do not change.\n\u2022 $E \\rightarrow Y$. This denotes that the environment directly influences the user behavior label $Y$, independent of user interactions between users and items. For example, in a specific holiday environment, users may be more inclined to purchase holiday-related items, regardless of whether they have interacted with them.\nIn real-world scenarios, training data is collected from heterogeneous environments. Therefore, the environment directly influences the distribution of the data and the prediction result, which can be explicitly represented as $P(Y,G|E) = P(G|E)P(Y|G, E)$. If we employ $D_{tr}(E)$ to represent the training data distribution for unobserved environments, the GNN-based model, when faced with OOD data, can rewrite Eq. (1) as:\n$\\arg \\min_\\theta \\mathbb{E}_{e\\sim D_{tr}(E),(G_u,y_u)\\sim P(G,Y|E=e)}[l(f_\\theta(G_u; \\theta), y_u)|e],$ (6)\nEq. (6) shows that environment $E$ affects the data generation used for training the GNN-based recommendation model.\n2) Confounding Effect of $E$: Figure 2 (a) and Figure 2 (b) illustrate the causal relationships in data generation and model training for graph-based recommendation algorithms. $E$ acts as the confounder and directly optimizing $P(Y|G)$ leads the GNN-based recommendation model to learn the shortcut predictive relationship between $G_u$ and $y_u$, which is highly correlated with the environment $E$. During the model training process, there is a tendency to use this easily captured shortcut relationship to model user preferences. However, this shortcut relationship is highly sensitive to the environment $E$. When the environment of the test set is different from that of the training set (i.e., $D_{tr}(E) \\neq D_{ts}(E)$), this relationship becomes unstable and invalid. The recommendation model that excessively learns environment-sensitive relationships in the training data will struggle to accurately model user preferences when faced with out-of-distribution data during the testing phase, resulting in a decrease in recommendation accuracy.\n3) Intervention: Through the above analysis, we can improve the generalization ability of GNN-based recommendation models by guiding the model to uncover stable predictive relationships behind the training data, specifically those that are less sensitive to environmental changes. Thus, we can eliminate the influence of environmental confounders on model predictions. Specifically, we learn stable correlations between user item interaction $G_u$ and ground truth $y_u$ by optimizing $P_\\theta(Y|do(G))$ instead of $P_\\theta(Y|G)$. In causal theory, the do-operation signifies removing the dependencies between the target variable and other variables. As shown in Figure 2 (c), by cutting off the causal relationship between the environment variables and the user interaction graph, the model no longer learns the unstable correlations between $G_u$ and $y_u$. The do-operation simulates the generation process of the interaction graph $G$, where environmental factors do not influence the user-item interactions. This operation blocks the unstable backdoor path $G \\leftarrow E \\rightarrow Y$, enabling the GNN-based recommendation model to capture the desired causal relationship that remains invariant under environmental changes.\nTheoretically, $P_\\theta(Y|do(G))$ can be computed through randomized controlled trials, which involve randomly collecting new data from any possible environment to eliminate environmental bias. However, such physical interventions are challenging. For instance, in a short video recommendation setting, it is impossible to expose all short videos to a single user, and it is also impractical to control the environment of data interactions. In this paper, we achieve a statistical estimation of $P_\\theta(Y|do(G))$ by leveraging backdoor adjustment. The derivation process is shown as follows:\n$P_\\theta(Y|do(G))$\n$= \\sum_{e}P(Y|do(G), E = e, I)P(E = e|do(G))P(I)$\n$= \\sum_{e}P(Y|G, E = e, I)P(E = e|do(G))P(I)$\n$= \\sum_{e} P(Y|G, E = e,I)P(E = e)P(I)$\n$= \\mathbb{E}_{e\\sim D_{tr}(E)} [P_e(Y|G, E, I)],$  (7)\nthrough the aforementioned backdoor adjustment, the influence of the environment $E$ on the generation of $G$ can be eliminated, enabling the model to learn correlations independent of the environment. However, in recommendation scenarios, environmental variables are typically unobservable or undefined, and their prior distribution $P(E = e)$ cannot be computed. Therefore, directly optimizing the Eq. (7) is challenging.\nB. Model Instantiations\n1) Environment Inference: This work introduces a variational inference method and proposes a variational inference-based environment instantiation mechanism. The core idea is to use variational inference to approximate the true distribution of environments and generate environment pseudo-labels as latent variables. The following tractable evidence lower bound (ELBO) can be obtained as the learning objective:\n$\\log P_G(Y|do(G)) \\geq \\mathcal{L}_{envInf} = \\mathbb{E}_{Q_\\phi(E|G,I)} [\\log P_\\theta(Y|G, E, I)] - D_{KL}(Q_\\phi(E|G, I) || P_0(E)),$ (8)\nwhere $Q_\\phi(E|G, I)$ denotes environment estimation, which draws samples from the true distribution of the environment $E$. $D_{KL}$ represents the Kullback-Leibler (KL) divergence of the volitional distribution $Q_\\phi(E|G, I)$ and the prior distribution $P_0(E)$. $P_\\theta(Y|G, E, I)$ is the graph representation learning module that employs the user-item interaction graph and the node attributes of users and items as input to learn invariant representations. Section III-B3 will provide a detailed introduction to the graph representation learning module. We present the derivation process of Eq. (8) as follows:\nTaking the logarithm on both sides of Eq. (8) and according to Jensen's Inequality, we have:\n$\\log P_0 (Y |do(G))$\n$= \\log \\mathbb{E}_{e\\sim D_{tr}(E)} [P_\\theta(Y|G, E, I)]$\n$= \\log \\sum_e \\frac{P_\\theta (Y|G, E = e, I)P_0(E = e, I)}{Q_\\phi(E = e|G, I)} Q_\\phi(E = e|G, I)$\n$> \\sum_e Q_\\phi(E = e|G, I) \\log \\frac{P_\\theta (Y|G, E = e, I)P_0(E = e, I)}{Q_\\phi(E = e|G, I)}$\n$= \\sum_e \\left[Q_\\phi(E = e|G, I) \\log P_\\theta(Y|G, E = e, I)-\\right.$\n$\\left.\\log \\frac{Q_\\phi(E = e|G, I)}{ P_0(E = e, I)}\\right]$\n$= \\mathbb{E}_{Q_\\phi(E=e|G,I)}[\\log P_\\theta(Y|G, E = e, I)] - D_{KL}(Q_\\phi(E = e|G, I) || P_0(E = e)).$ (9)\n2) Invariant Pattern Recognition Mechanism: Following previous invariant learning studies [28], [48], this work proposes an invariant pattern recognition mechanism to explore invariant graph representations, encouraging the model to learn invariant correlations under distribution shifts. We make the following assumption:\nAssumption: For a given user-item interaction graph (i.e., data distribution $D$), these interaction data are collected from $K$ different environments $E$. User behavior patterns exist independently of the environment and can be used to generalize out-of-distribution user preference prediction. There exists an optimal invariant graph representation learning $F^*\\left(\\cdot\\right)$ satisfying:\n\u2022 Invariance Property. $\\forall e \\in D(E), P_\\theta(Y|F^*(G), E = e, I) = P(Y|F^*(G), I)$.\n\u2022 Sufficiency Condition. $Y = F^*(G) + \\epsilon, \\epsilon \\bot E$, where $\\bot$ indicates statistical independence and $\\epsilon$ is random noise.\nThe invariance property assumption indicates that a graph representation learning model exists capable of learning invariant user-item representations across different data distribution environments. The sufficiency condition assumption means that the learned invariant representations enable the model to make accurate predictions.\n3) Invariant Representation Learning: This section mainly consists of an environment generator, a diffusion-based graph representation learning module, and a recommendation module. Next, we will detail how they collaborate to enhance the generalizability of GNN-based models on OOD data and improve recommendation accuracy.\nEnvironment Generator. In real-world recommendation scenarios, training datasets are collected in various environments. However, for a single user-centric interaction graph, the training dataset comes from a single environment. We need to learn environment-invariant correlations from training data originating from different environments to achieve the generalization capability of GNN-based recommendation models under distribution shifts. To circumvent this dilemma, this paper designs an environment generator $g_\\omega^k(\\cdot)(1 \\leq k \\leq K)$, which takes the user's original interaction graph $G$ as input and generates a set of $K$ interaction graphs ${G^k}_{k=1}^K$ to simulate training data from different environments. The optimization objective is expressed as follows:\n$\\mathcal{L}_{generator} = \\mathbb{E}_G [Var(L(g_\\omega^k(G)) : 1 \\leq k \\leq K)],$ (10)\nwhere $Var(\\cdot)$ denotes the variance and $L(\\cdot)$ is the loss function. Following existing work [27], we modify the graph structure by adding and removing edges. Given a Boolean matrix $B^k$, the adjacency matrix $A$ of the graph, and its complement $A'$, the $k$-th generated view for the original view is $A^k = A+B^k \\left(A' - A'\\right)$. Since $B^k$ is a discrete matrix and not differentiable, it cannot be optimized directly. To address this issue, we borrow the idea from [27] and use reinforcement learning to treat graph generation as a decision process and edge editing as actions. Specifically, for view $k$, we consider a parameter matrix $\\theta^k = {\\theta_{nm}^k}$. For the $n$-th node, the probability of exiting the edge between it and the $m$-th node is given by:\n$h(\\theta_{nm}^k) = \\frac{\\exp(\\theta_{nm}^k)}{\\sum_{m'=1}^n \\exp(\\theta_{nm'}^k)},$  (11)\nWe then samples actions ${b_{nm}^k}_{m=1}^n$ from a multinomial distribution $M(h(\\theta_{n1}^k),..., h(\\theta_{nn}^k))$, which give the nonzero entries in the $n$-th row of $B^k$. The reward function $R(G^k)$ can be defined as the inverse loss. We can use the reinforcement algorithm to optimize the generator with the gradient:\n$\\nabla_{\\theta^k} \\log h_{\\theta^k} (A^k)R(G^k),$ (12)\nDDPM $p_\\theta(x_{1:T}| x_0, z_{causal})$. Using DDPM, the forward pro-cess is entirely deterministic except for $t=1$. We define the joint distribution of the reverse generative process as follows:\n$p_\\theta(x_{0:T} | z_{causal}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t, z_{causal}).$ (16)\n$p_\\theta(x_{t-1}|x_t,x_0) = \\mathcal{N} (x_{t-1}; \\mu_\\theta (x_t, z_{causal}, t), \\Sigma_\\theta (x_t, z_{causal}, t)).$ (17)\nThe loss function in Eq. (5) can be rewritten as:\n$\\mathcal{L}_{Invsimple} = \\mathbb{E}_{t,x_0,\\epsilon_t} [|| \\epsilon_t - \\epsilon_\\theta (\\sqrt{\\alpha_t} x_0 + \\sqrt{1 - \\alpha_t} \\epsilon_t, t)||^2],$ (18)\nwhere $x_t = \\sqrt{\\alpha_t} x_0 + \\sqrt{1 - \\alpha_t} \\epsilon_t$. After obtaining the reconstructed output vector $R^\\prime = x_{recon}$ from DDPM, it will be used as the input for the decoder of the variational graph decoder, which then reconstructs the input graph. The entire process is illustrated as follows:\n$\\hat{A}_k = \\sigma(RR^T),$ (19)\nwhere $\\sigma(\\cdot)$ is the activation function (the sigmoid function is used in this paper). The VGAE is optimized by the variational lower bound:\n$\\mathcal{L}_{VGAE} = \\mathbb{E}_{q_\\phi (z|A_k,I)} [\\log p_\\theta(A_k|z)] - D_{KL} \\left(q_\\phi(z|A_k, I) || p(z) \\right).$ (20)\nPrediction and Joint Optimization. Using the well-trained diffusion model to sample the final embeddings for user preference modeling:\n$\\hat{r}_{u,i} = \\mathbf{e}_u^T \\mathbf{e}_i,$ (21)\nwhere $\\mathbf{e}_u$ and $\\mathbf{e}_i$ denote the final user embedding w.r.t $u$-th user and item embedding w.r.t $i$-th item, respectively. Without loss of generality, LightGCN is used as the recommendation"}, {"title": "IV. THEORETICAL ANALYSIS", "content": "CausalDiffRec aims to learn the optimal generator $F^*(\\cdot)$ as stated in the assumption in section III-B2, thereby obtaining invariant graph representations to achieve OOD generalization in recommendation performance under data distribution shifts. Before starting the theoretical derivation, let's do some pre-liminary work. For the convenience of theoretical proof, we rewrite Eq. (23) as:\n$\\arg \\min_\\theta (\\mathcal{L}_{task} + \\mathcal{L}_{infer}),$ (24)\nwhere $\\mathcal{L}_{task} = \\mathcal{L}_{rec} + \\mathcal{L}_{generator} + \\mathcal{L}_{VGAE} + \\mathcal{L}_{Invsample}$ and $\\mathcal{L}_{infer} = \\mathcal{L}_{envInf}$, and for the derivation convenience, we temporarily ignore the penalty coefficient. The $\\mathcal{L}_{task}$ and $\\mathcal{L}_{infer}$ can be further abstracted as:\n$\\mathcal{L}_{task} = \\arg \\min_\\theta \\mathbb{E}_{e\\sim D_{tr}(E),(G_u,y_u)\\sim P(Y,G|E=e)}[l(f_\\theta(G_u; \\theta), y_u)]$\n$\\mathcal{L}_{infer} = \\min_{q(Y|z_{causal})} Var\\{\\mathbb{E}_{e\\sim D_{tr}(E),(G_u,y_u)\\sim P(Y,G|E=e)} [l(f_\\theta(G_u; \\theta), y_u)|do(G_u)]\\}.$ (25)\nWe follow the proof technique from [28] and show the optimality of the Eq. (24) with the following two propositions that can achieve OOD recommendation.\nProposition 1: Minimizing Eq. (24) promotes the model's adherence to the Invariance Property and the Sufficient Condition outlined in Assumption (in Sec. III-B2).\nProposition 2: Optimizing Eq. (24) corresponds to minimizing the upper bound of the OOD generalization error described in Eq. (6).\nProposition 1 and Proposition 2, respectively, avoid strong hypotheses and ensure that the OOD generalization error bound of the learned model is within the expected range. In fact, this can also be explained from the perspective of the SCM model in Figure 2. Optimizing Eq. (6) eliminates the negative impact of unstable correlations learned by the model, which are caused by latent environments, on modeling user preferences. At the same time, it enhances the model's ability to learn invariant causal features across different latent environments. Proofs for Proposition 1 and Proposition 2 are shown as follows:\nBefore starting the proof, we directly follow previous work [27], [28], [50]\u2013[52] to propose the following lemma, using information theory to interpret the invariance property and sufficient condition in Assumption and to assist in the proof of Proposition 1.\nUsing the Mutual Information $\\mathbb{I}(;)$, the invariance property and sufficient condition in Assumption can be equivalently expressed as follow lemma:\nLemma 1: (1) Invariance: $\\forall e \\in D(E), P_\\theta(Y|P_{inv}, E = e, I) = P(Y|P_{inv}, I) \\Leftrightarrow \\mathbb{I}(Y; E|P_{inv}, I) = 0$ where $P_{inv} = F^*(G)$. (2) Sufficiency: $\\mathbb{I}(Y; P_{inv}, I)$ is maxmized.\nFor the invariance property, it is easy to get the following equation:\n$\\mathbb{I}(Y; E|P_{inv}, I)$\n$= \\mathbb{E}_{P_I} [D_{KL} (P(Y, E|P_{inv}, I)||P(Y|P_{inv}, I)P(E|P_{inv}, I))]$ (26)\nFor the sufficient condition, we use the method of contradiction and prove it through the following two steps:\nFirst, we prove that for $Y, P_{inv}$, and $I$ satisfying $P_{inv} = \\arg \\max_{P_{inv}} \\mathbb{I}(Y; P_{inv}, I)$, they also satisfy that $\\mathbb{I}(Y; P_{inv}, I)$ is maximized. We use the method of contradiction to prove this. Assume $P_{inv} \\neq \\arg \\max_{P_{inv}} \\mathbb{I}(Y; P_{inv}, I)$, and there exists $P'_{inv} = \\arg \\max_{P_{inv}} \\mathbb{I}(Y;P_{inv}, I)$, where $P_{inv} \\neq P'_{inv}$. We can always find a mapping function $M$ such that $P'_{inv} = M(P_{inv}, R)$, where $R$ is a random variable. Then we have:\n$\\mathbb{I}(Y; P'_{inv}, I) = \\mathbb{I}(Y; P_{inv}, R, I)$\n$= \\mathbb{I}(Y; P_{inv}, I) + \\mathbb{I}(Y; R|P_{inv}, I).$ (27)\nSince $R$ is a random variable and does not contain any information about $Y$, we have $\\mathbb{I}(Y; R|P_{inv}, I) = 0$. Therefore:\n$\\mathbb{I}(Y; P'_{inv}, I) = \\mathbb{I}(Y; P_{inv}, I).$\nThis leads to a contradiction.\n(28)\nNext, we prove that for $Y, P_{inv}$, and $I$ satisfying $P_{inv} = \\arg \\max_{P_{inv}} \\mathbb{I}(Y; P_{inv}, I)$, they also satisfy that $\\mathbb{I}(Y; P_{inv}, I)$ is maximized. Assume $P_{inv} \\neq \\arg \\max_{P_{inv}} \\mathbb{I}(Y; P_{inv}, I)$, and there exists $P'_{inv} = \\arg \\max_{P_{inv}} \\mathbb{I}(Y; P_{inv}, I)$, where $P'_{inv} \\neq P_{inv}$. We have the following inequality:\n$\\mathbb{I}(Y; P_{inv}, I) \\le \\mathbb{I}(Y; P'_{inv}, I).$ (29)\nFrom this, we can deduce that:\n$P_{inv} = \\arg \\max_{P_{Inv}} \\mathbb{I}(Y; P_{Inv}, I),$ (30)\nwhere contradicts $P'_{inv} = \\arg \\max_{P_{Inv}} \\mathbb{I}(Y; P_{Inv}, I)$. Since the assumption leads to a contradiction, and the assumption does not hold. Therefore, $P_{inv} = \\arg \\max_{P_{Inv}} \\mathbb{I}(Y; P_{Inv}, I)$ holds. This proves that $\\mathbb{I}(Y; P_{inv}, I)$ is maximized. The lemma 1 is complicated proven."}, {"title": "Proof of Proposition 1.", "content": "First, optimizing the first term $\\mathcal{L}_{task}$ in Eq. (24)"}]}