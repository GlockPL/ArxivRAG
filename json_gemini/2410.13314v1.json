{"title": "Precipitation Nowcasting Using Diffusion Transformer with Causal Attention", "authors": ["ChaoRong Li", "XuDong Ling", "YiLan Xue", "Wenjie Luo", "LiHong Zhu", "FengQing Qin", "Yaodong Zhou", "Yuanyuan Huang"], "abstract": "Short-term precipitation forecasting remains challenging due to the difficulty in capturing long-term spatiotemporal dependencies. Current deep learning methods fall short in establishing effective dependencies between conditions and forecast results, while also lacking interpretability. To address this issue, we propose a Precipitation Nowcasting Using Diffusion Transformer with Causal Attention model. Our model leverages Transformer and combines causal attention mechanisms to establish spatiotemporal queries between conditional information (causes) and forecast results (results). This design enables the model to effectively capture long-term dependencies, allowing forecast results to maintain strong causal relationships with input conditions over a wide range of time and space. We explore four variants of spatiotemporal information interactions for DTCA, demonstrating that global spatiotemporal labeling interactions yield the best performance. In addition, we introduce a Channel-To-Batch shift operation to further enhance the model's ability to represent complex rainfall dynamics. We conducted experiments on two datasets. Compared to state-of-the-art U-Net-based methods, our approach improved the CSI (Critical Success Index) for predicting heavy precipitation by approximately 15% and 8% respectively, achieving state-of-the-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowcasting, the youngest yet most promising branch of weather forecasting, primarily focuses on rainfall predictions within the next 12 hours. It is particularly adept at forecasting small to medium-scale weather systems with short lifespans and sudden onset, such as severe convective storms. By providing forecasts 1-2 hours in advance, nowcasting can accurately capture the location of weather systems and predict rainfall areas, enabling timely communication of rainfall information and alerting the public to take necessary precautions [1], [2].\nNumerical Weather Prediction (NWP) has long been a powerful tool for generating short-term forecasts using atmospheric initial conditions [3]. However, NWP systems face significant challenges in accurately describing the evolution of convective precipitation systems at short spatiotemporal scales due to insufficient initial conditions and computational resource limitations. This makes it difficult to precisely predict local weather phenomena.\nGiven these limitations of NWP, researchers have turned to alternative methods for short-term precipitation forecasts ranging from minutes to hours. These methods typically employ high-resolution real-time precipitation data observed by weather radar. Extrapolation techniques, such as optical flow advection schemes [4], [5], are used to predict the future movement direction and velocity of rain bands, aiming to achieve accurate estimates of precipitation intensity, affected areas, and duration. Nevertheless, extrapolation methods have their own limitations. The underlying stationarity assumption, which presumes that the current state of the atmosphere remains unchanged, and the simplified treatment of the flow field restrict their performance in short-term forecasting. These limitations become increasingly prominent when facing the complex nonlinear characteristics of precipitation evolution as the forecast lead time increases [6].\nIn recent years, deep learning technology has been leading technological innovation and has shown outstanding capabilities in simulating complex Earth systems and addressing Earth science challenges, such as short-term precipitation forecasting [6]\u2013[8]. In precipitation forecasting tasks, the methods used can be divided into non-generative and generative prediction methods. Non-generative prediction treats precipitation forecasting as a deterministic spatiotemporal prediction task. These methods train networks to predict future precipitation by performing supervised learning on historical and future weather radar image sequences, with the objective function being a distance function or a classification objective function. In recent years, non-generative precipitation forecasting has made significant progress in network architecture. An increasing number of studies have introduced advanced model architectures into precipitation forecasting, such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers [9], to build models with better long-term dependency and global perception capabilities. For example, Shi et al. combined convolutional networks and RNNs and proposed Conv-LSTM [10] and Conv-GRU [11] models, aiming to effectively extract and utilize the spatiotemporal correlations in sequential radar data to improve"}, {"title": "II. RELATED WORKS", "content": "A. Diffusion Model\nDiffusion models(DM) [18], as an emerging type of image generation model, have achieved significant success in recent years. Compared to traditional generative adversarial networks (GANs) [19] and variational autoencoders (VAEs) [20], diffusion models have demonstrated superior performance in many vision-related tasks [21]\u2013[23]. Unlike GANs, the training process of diffusion models is more stable, without the issue of mode collapse, and the generated images have higher quality and no artifacts. Compared to VAEs, the images generated by diffusion models not only have higher quality but also exhibit greater diversity. These advantages have made diffusion models an important breakthrough in the field of image generation, opening up new possibilities for related applications. Diffusion"}, {"title": "B. Transformer for Diffusion Model", "content": "In the field of image generation, researchers have proposed various Transformer-based models and made significant progress. Bao et al. [25] proposed U-ViT based on the Vision Transformer (ViT) [26] and successfully applied it to image generation tasks using diffusion models, achieving effects comparable to convolutional neural networks (CNNs) of the same scale. Peebles et al. [16] proposed a novel Transformer model called DIT that introduces an Adaptive Layer Normalization module as a regulatory mechanism, which has achieved excellent performance on the class-conditional ImageNet generation task. Furthermore, the PixArt model proposed by Chen et al. [27] further improves computational efficiency by modifying the conditional mapping approach of the DIT Block. The design goal of PixArt is to significantly reduce training costs while maintaining generation quality comparable to the current state-of-the-art image generators."}, {"title": "III. METHOD", "content": "A. Framework Overview\nOur proposed rainfall prediction framework, illustrated in Fig.(1), builds upon the Encoder-Decoder structure introduced in [28]. This framework maps the input from pixel space to latent space, employs a neural network to predict future precipitation scenarios, and then decodes the results back to pixel space. To accurately describe the input data's dimensional information, we define its shape as $Z \\in R^{B\\times C\\times F\\times N}$, where B represents the batch size, C the number of tokens (channels), F the number of time frames, and N the length of a single token. In the diffusion model for predicting noise, we adopt a strategy of concatenating the condition tokens and"}, {"title": "B. Spatio-Temporal Patch Extraction", "content": "The spatial resolution of our rainfall image sequence is 256 \u00d7 256, with a time span of 20 frames (100 minutes). First, we use AutoEncoder-KL [28] to compress the rainfall sequence images from the original size of F = 20 \u00d7 W = 256 \u00d7 H = 256 \u00d7 C = 1 to a latent space size of F = 20 \u00d7 W = 32 \u00d7 H = 32 \u00d7 C = 4. Then, we divide the latent space into 2 \u00d7 2 sized patches [26], mapping each patch to a vector of length 1152, ultimately obtaining spatiotemporal tokens with a shape of F = 20 \u00d7 C = 256 \u00d7 N = 1152, where the first 4 frames serve as conditions and the latter 16 frames are the sequence to be predicted."}, {"title": "C. The Model variants of DTCA", "content": "Our proposed DTCA spatiotemporal denoising model is shown in Fig.(2). The model's input includes noise, the first 4 frames of observed radar image sequences, and the diffusion time T. The core architecture of the model consists of modules that process spatiotemporal data of different dimensions (as shown in Fig.(3)). The DTCA module effectively captures the complex spatiotemporal dependencies between input tokens through a multi-head self-attention mechanism, and uses modulation technology to dynamically introduce diffusion moment information into the model. DTCA also combines Causal Attention and Channel-To-Batch Shift (CTBS) to introduce conditional information to ensure the causal consistency of the generation process. To thoroughly investigate how to effectively capture and process spatiotemporal data in rainfall prediction, we designed four variant methods for processing spatiotemporal data.\nVariant 1: Full Join Space-Time DTCA Block, This variant focuses on jointly processing the space-time dimension to fully perceive the changes in the space-time dimension. In this design, Z is reshaped into $Z \\in R^{B\\times(CF)\\times N}$, enabling the model to capture the correlation features in space and time at the same time.\nVariant 2:Divided Space-Time DTCA Block, in the Divided Space-Time DTCA Block, we capture spatiotemporal informa-"}, {"title": "D. Causal Attention and Channel-To-Batch Shift", "content": "In this study, we observed 20 frames of rainfall fields from the entire dataset, including 4 frames of conditional data and 16 frames of data to be predicted. Based on the spatiotemporal token generation method described in Section III-B, we generated a total of 256 \u00d7 20 tokens in the spatiotemporal dimension. To conduct an in-depth analysis of the characteristics and correlations of the rainfall fields in the spatiotemporal dimension, we calculated the cosine similarity of these tokens in both spatial and temporal dimensions. The results are shown in Fig.(5)."}, {"title": "IV. EXPERMENT", "content": "A. Dataset\nWe conducted comprehensive experimental evaluations on the Sweden dataset and the MSMR dataset to validate the superiority and broad applicability of our proposed method. These two datasets can be accessed through the following links.\nB. Other Methods and Verification Scores\nTo evaluate the advantages of our model, we compared it with several state-of-the-art methods in rainfall prediction, including Pysteps (based on statistical learning), DGMR (based on GAN), and diffusion model-based approaches such as TRDM and SSLDM-ISI. For comprehensive assessment, we employed a range of widely used evaluation metrics in rainfall prediction, including CSI, CRPS [29], and FSS [6]. These metrics allow us to thoroughly assess different aspects of prediction performance, from overall accuracy to spatial structure preservation and probabilistic forecast skill.\nC. Results on Swedish\nThe Fig.(6) shows prediction examples from various models on the MRMS dataset. We present forecast images generated"}, {"title": "V. ABLATION EXPERIMENT", "content": "A. Space-time capture structure\nThis section will experimentally compare the four variants described in Chapter 2 to determine the optimal spatiotemporal capture scheme. Figure 3 shows the precipitation prediction performance of these models on the SW dataset. The experimental results reveal that, under the same number of training epochs (300), Variant 1 which calculates attention for all tokens simultaneously performs best in terms of spatiotemporal feature capture and prediction effectiveness. The data clearly shows that the comprehensive performance ranking is: the entire model calculating spatiotemporal attention simultaneously on all tokens (FJST), spatial mixed attention (HJST+S), spatiotemporal separated attention (ST), and temporal mixed attention (HJST + T). This demonstrates the importance of comprehensively considering all information when dealing with complex spatiotemporal data. The superior performance of Variant 1 stems from its ability to more comprehensively capture the interactions and dependencies between temporal and spatial dimensions. In contrast, other variants may miss some key correlations due to separating temporal and spatial information at some stage. Therefore, for the sake of prediction quality, subsequent experiments opted for the JST full spatiotemporal attention approach.\nB. Ablation experiment of Causal Attention mechanism and Channel-To-Batch Shift\nWe investigated the effects of cross-attention mechanism and Channel-To-Batch Shift on short-term precipitation forecasting. We conducted experiments on two datasets, Sweden and MRMS, with a forecast duration of 80 minutes (16 time"}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a Transformer-based diffusion model called DTCA for addressing the complex task of short-term precipitation forecasting. To fully utilize spatiotemporal information, we designed and analyzed four different spatiotemporal modeling variants. Through experiments, we found that allowing all tokens to participate in calculations across spatiotemporal dimensions yields the best predictive performance, highlighting the importance of global spatiotemporal dependencies for accurate forecasting. Furthermore, we introduced a causal attention mechanism, enabling the prediction process to establish global spatiotemporal dependencies with conditional information. This design effectively leverages conditional information, allowing the model to make more precise predictions about the future based on historical spatiotemporal evolution patterns. Compared to traditional spatiotemporal prediction methods, this mechanism better captures complex spatiotemporal dynamic features, improving the accuracy and reliability of predictions. To further enhance the model's representational capacity, we proposed an innovative Channel-To-Batch Shift operation. This operation creates \"extended samples\" by redistributing part of the data from the channel dimension to the batch dimension, thereby enriching feature representation. Significant performance improvements were achieved on two rainfall datasets, demonstrating its effectiveness in enhancing feature representation. In analyzing the prediction results, we observed the occurrence of some flickering phenomena. This may be attributed to the lack of temporal operations in the decoder. The loss of temporal information during the decoding process potentially affects the continuity and smoothness of predictions. To address this issue, future work could explore high-quality 3D autoencoders. By downsampling in both time and space dimensions, it would be possible to reduce the number of temporal tokens, thereby improving the computational efficiency of the model. Simultaneously, this approach could enhance the capture of hierarchical structure and long-range dependencies in spatiotemporal data."}, {"title": "$q(x_t | x_{t-1}) = N(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t)I)$", "content": "Equation"}, {"title": "$x_t = \\sqrt{\\bar{\\alpha_t}} x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\epsilon_t$", "content": "Equation"}, {"title": "$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha_t}}} \\epsilon_{\\theta})$", "content": "Equation"}, {"title": "$\\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}(\\frac{Q_i K_i^T}{\\sqrt{d_k}}) V_i$", "content": "Equation"}, {"title": "$Q = W^Q \\times Z_p, K = W^K \\times Z_c, V = W^V \\times Z_c; \\text{Flatten} = \\text{X.Reshape}(1, \\frac{(B\\times C\\times T \\times L)}{\\text{numheads} \\times \\text{headdim}}, \\text{numheads}, \\text{headdim})$", "content": "Equation"}]}