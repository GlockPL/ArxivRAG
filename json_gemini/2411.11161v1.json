{"title": "MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records", "authors": ["Eric Yang", "Xiaoxue Han", "Pengfei Hu", "Yue Ning"], "abstract": "The adoption of digital systems in healthcare has resulted in the accumulation of vast electronic health records (EHRs), offering valuable data for machine learning methods to predict patient health outcomes. However, single-visit records of patients are often neglected in the training process due to the lack of annotations of next-visit information, thereby limiting the predictive and expressive power of machine learning models. In this paper, we present a novel framework MPLite that utilizes Multi-aspect Pretraining with Lab results through a light-weight neural network to enhance medical concept representation and predict future health outcomes of individuals. By incorporating both structured medical data and additional information from lab results, our approach fully leverages patient admission records. We design a pretraining module that predicts medical codes based on lab results, ensuring robust prediction by fusing multiple aspects of features. Our experimental evaluation using both MIMIC-III and MIMIC-IV datasets demonstrates improvements over existing models in diagnosis prediction and heart failure prediction tasks, achieving a higher weighted-F\u2081 and recall with MPLite. This work reveals the potential of integrating diverse aspects of data to advance predictive modeling in healthcare.", "sections": [{"title": "I. INTRODUCTION", "content": "EHR datasets, such as MIMIC-III [1], provide comprehensive medical information, including vital signs, diagnoses, medications, and lab results. These multi-aspect features are valuable resources for predicting personalized health events, such as diagnosis predictions. Meanwhile, deep learning technique have become a common approach for analyzing sequential data within healthcare [2]\u2013[4]. However, many studies often exclude patient examples with only single-visit records, since these records lack labels for prediction tasks involving future admissions. For instance, when training a supervised machine learning model to predict diagnoses in the next visit given previous visits in the MIMIC-III dataset, we need the annotations/labels for the next visit. Therefore, temporal prediction models rely on patient data with at least two visits to complete the training process. Single-visit records are not fully utilized in training predictive models as shown in Figure 1. However, multi-visit patients contribute to only a small portion of the dataset. Among a total of 46,520 patients, only 16.20% have multiple visits. The remaining 83.80% are single-visit patients, which could also provide rich information for models to learn useful patterns and make better predictions.\nTo fully utilize these single admission records in an EHR dataset, there are two popular solutions to address this issue: (1) Transformer models like G-BERT [5] leverage single-admission data to design customized self-supervised tasks, which typically treat medical concepts or admissions as masked tokens and further enhance intermediate representation learning within the encoder framework. (2) Multi-aspect learning [6], [7] incorporates diverse features, such as lab test results or clinical notes, to enrich the representation learning of medical concepts, which helps models better capture the complexity and interrelationships inherent in medical data. The former approach, although widely adopted by early studies [8], [9], is susceptible to the order of medical codes and may not be lightweight enough to function as a plug-and-play module. In contrast, while the latter approach demands high-quality and relevant additional medical concepts, it enables models to learn collaborative representations, leading to more accurate predictions with the addition of a lightweight module.\nIn this study, we leverage single admissions as auxiliary training data to predict diagnoses and health risks, such as heart failure. Recognizing the pivotal role that lab test results play prior to training, we propose a novel framework, MPLite, which is an additional plug-in-and-play module to learn relationships between lab results and diagnoses through a Multi-aspect Pretraining and \u201cLite\u201d module. This framework captures the underlying patterns and associations that are present in both multiple-visit and single-visit data. We then illustrate how incorporating this pre-trained knowledge can significantly enhance the predictive capabilities of temporal neural networks, particularly for forecasting health risks in patients with multiple visits. By fine-tuning the pre-trained subnetwork on two specific health risk prediction tasks, we demonstrate the effective extraction of valuable insights from abundant single-visit patient data. The pretraining module underscore the advantages of pretraining on diverse medical features beyond diagnoses concepts and highlights the broader applicability of lab test data in predictive healthcare."}, {"title": "II. RELATED WORK", "content": "Deep learning models have been extensively applied to electronic health records (EHR) to extract representations of medical patterns, addressing various real-world healthcare prediction tasks like diagnosis prediction.\nMost early studies in this area can be categorized into two main subcategories: (i) RNN-based models, where predictive methods like GRU [10], RETAIN [11], and Timeline [12] combine attention mechanisms and RNN for prediction. Other models [2], [13], [14] leverage RNNs to handle time-series data effectively; (ii) CNN-based models, such as Deepr [15] and AdaCare [16], use convolution and pooling layers to process features in EHR. However, these methods often overlook relations among encoded medical concepts and other critical aspects such as lab test results.\nRecently, there has been a trend towards using ontology graphs to incorporate additional information related to medical concepts in predictions such as GRAM [17], G-Bert [5], GCT [7], Variationally Regularized GNN [18], GraphCare [19], ME2Vec [20], RGNN [21]. However, most existing works primarily rely on admission medical concepts as features for various deep learning models. Meanwhile, following the success of the transformer architecture, researchers have quickly adopted it for EHR data. Encoder-decoder structures offer the advantage of fully utilizing single-visit data in the pretraining process by customizing proxy tasks for different prediction tasks. Early studies, like G-Bert [5] treat medical codes as tokens and incorporate hierarchical domain knowledge along with diagnosis codes. Recent models like HiTANet [22], Med-BERT [8], and Sherbet [23] have also been trained to precisely identify patient information based on various medical concepts. However, the pretraining phase in most works cannot be easily separated into a plug-and-play module, limiting its generalizability when transferring pretraining information to new tasks or different structures.\nBeyond traditional medical concepts, such as condition, medication, and treatment codes, researchers [9] also involve additional information (e.g. demographic features and timestamps) in each admission record. To augment representation from different modalities, both CGL [6] and MedGTX [24] integrates disease-patient graphs and unstructured text from clinical notes through encoder structures to demostrates the importance of involving additional information other than sequence of medical concepts. MiME [25] and GCT [7] are preliminary tries to involve lab results as input features to further optimize medical hidden representation. However, these integrated models cannot work well in the absence of corresponding records, and they always have complicated preprocessing or fusion steps which cannot be generalized as lightweight modules.\nIn this paper, we propose MPLite that allows different models to jointly learn representations of medical diagnosis codes and lab results. Our framework provides a novel perspective for integrating different features to achieve more accurate predictions. The experimental results demonstrate a significant improvement with the extensive pretraining module in predicting health outcomes over several baselines, as confirmed by confidence intervals obtained from repeated experiments."}, {"title": "III. PROPOSED METHOD", "content": "We begin by describing the notations and then introduce our proposed framework, which includes a pretraining module with lab results, along with instructions on how to seamlessly integrate and utilize the module for downstream tasks.\nAn EHR dataset S is a collection of patient admission records of N patients {P1, P2, ..., PN } \u2208 S in total. For admission records of each patient, the i-th patient can be represented as a sequence of T\u1d62 admission records {x1,x2, .., x_{T_i} } \u2208 p\u1d62 in chronological order, where T\u1d62 is the number of admissions for the patient. The goal of our predefined prediction tasks is to predict the label at the end of the sequence, y \u2208 {0,1},"}, {"title": "B. MPLite Framework", "content": "1) Multi-Aspect Pretraining: To fully leverage the EHR data, it is essential to utilize records from single-admission patients, who constitute the majority of the dataset. Since these records lack labels for future admissions, our focus is on learning the relationship between lab test results and diagnoses from the current visit. A single-visit patient has only one admission record, so we consider a single-visit patient equivalent to a single visit in this part. We hypothesize that additional aspects of features (e.g, lab tests) reflect important information about a patient's existing diagnoses. Thus, we identify lab results as additional medical concepts for each patient, considering that lab results are one of the most crucial components in describing diagnoses results.\nIn terms of the pretraining step of Figure 2, we define a novel proxy task that predicts the diagnoses shown in the sequence of visits by historical lab results in the pretraining step. As mentioned in the section III-A, diagnoses and lab results sets are denoted by D and L respectively, and we aim to decode the item code set from lab results x\u02e1 into the probability distribution \u0177 = P(x\u1d30|x\u02e1) for each patient. Here we use a multi-layer perceptron (MLP) for parametrization to transform lab results to diagnoses of patients:\n\u0177 = \u03c3(MLP(x{ | Psingle))\n\u0177 = \u03c3(MLP(Integrate({x{}=1) | Pmulti))\nHere Psingle, Pmulti denotes single-visit and multi-visit patients, and o means the activation function. There are two main reasons we chose MLP as the backbone model for the pretraining module: (1) It directly predicts the probability distribution of diagnosis codes efficiently, requiring minimal computational resources. (2) It achieves competitive predictive performance for the defined proxy task, even when compared to models incorporating embedding or convolution modules. Moreover, Integrate means we integrate the sequence of multiple visit into a single vector, which can be aligned with the input of single-visit patients as shown in equation 3.\nIntegrate({x{}=1) = V x\nt=1\nFor the lab result data, we assume that lab results are all up-to-date, and we considered single lab-test code normal if it has not been taken or was tested normal in the most recent test."}, {"title": "2) Integration and Inference:", "content": "Now let us focus on how to fuse both representations from a backbone prediction model and the proposed pretraining module. Note that, subscript t might be also involved in model in terms of the training setting across different baseline. For example, some works feed model by admission-level data, which means patient with multiple visits can be fed consecutively into the model. For the adaptation ability of our framework, we also transfer this setting into the description of our framework.\nAssuming we already have the final output ot \u2208 R|C| for prediction of the t-th admission before feeding into the classifier of existing baselines, we can also retrieve lab results vector xf as the input of the pre-trained module. |C| is the output dimension, which is also considered as the vocabulary size. We keep the same format of input for xf and get hidden representation ht\u2208 Rh in terms of patient's lab results through the pretrained encoder dense layer. We then use a classifier with single dense layer to get prediction \u0177t for multiple prediction tasks after concatenating both patient-level representations as shown in Figure 2. Finally, the integration step and classifier are defined as follows, the output dimension of classifier can be modified for various prediction tasks:\not = Encoder({x}=1 | Pmulti)\no\u2081 = ot || h\u2208 R|C|+h\n\u0177t = Classifier(o) \u2208 R|C|\nWe can still remain the same loss function L as the one already defined in the backbone module. Through the definition of inference part, we can easily plug in pretraining module and optimize current model's output by integrating lab results for more precise prediction."}, {"title": "C. Downstream Tasks", "content": "The proposed framework can be adapted for various prediction tasks. Consider a patient with T+1 admission records, we can build one sample with admission history {X1, X2, ..., XT} for each patient. We perform two prediction tasks in our experiments by the following definition:\n(1) Diagnosis (DG) Prediction predicts the diagnosis result of the next admission given previous admission records. Formally, we learn a function f : (X1, X2, ..., Xt) \u2192 y[Xt+1] where t < T and y[xt+1] \u2208 R|D| is a multi-hot vector where |D| denotes the number of all diagnosis codes.\n(2) Heart Failure (HF) Prediction predicts if heart failure (i.e., ICD-9 prefixed code of 428) is diagnosed in the next admission. Formally, we learn a function f : (X1, X2, ..., Xt) \u2192 y[xt+1] where t < T and y[xt+1] \u2208 {0,1} is a binary label indicating whether heart failure is diagnosed in the admission. The binary cross-entropy (BCE) loss is used with a sigmoid function to train the learning framework for both binary and multi-label classifications tasks."}, {"title": "IV. EVALUATION", "content": "To evaluate our proposed model, we focus on two pubic and widely-used EHR datasets: MIMIC-III [26] and MIMIC-IV [27]. Both datasets are derived from extensive de-identified clinical data collected from patients admitted to Intensive Care Units (ICUs). We employed a randomized approach to divide both datasets into training, validation, and testing segments. Specifically, MIMIC-III and MIMIC-IV datasets were divided into 6000/493/1000 and 8000/1000/1000 for the training, validation, and test sets, respectively."}, {"title": "B. Baselines", "content": "To check the improvement of MPLite for predictive models, we select the following state-of-art methods as baselines:\n\u2022 RNN/CNN-based models: GRU [10], Timeline [12], RETAIN [11], Deepr [15], and Dipole [13].\n\u2022 Graph-based models: GRAM [17], KAME [29], and CGL [6].\n\u2022 Transformer-based models: G-BERT [5], HiTANet [22].\nNote that GRU uses multi-hot vectors of medical codes as inputs, while other baselines use medical code embeddings. For G-BERT, both pretraining and medication inputs is discarded which requires extra information other than diagnose features. Moreover, we remove the clinical notes parsing module in CGL and the timestamp feature in HiTANet to ensure each baseline is trained by the same data. We also do not consider MiME [25] and GCT [7] because of different evaluation tasks."}, {"title": "C. Parameters Setting", "content": "The parameter settings used for pretraining module, we find the optimal output dimension 200 of the first dense layer from a search space of [100, 200] and set Drop-out rate as 0.4 in the final classifier for fine-tuning and final prediction. For the baseline GRU, the units of RNN module are all set as 128. For other baselines, we do our best to follow the parameter setting described in original papers. Different learning rate decay schedulers with Adam optimizer are experimented, resulting a decay learning rate from le-2 to 1e-5 between epochs. Moreover, we set batch size as 64 and use 100 epochs for training process. We conduct 10 repeated experiments for each baseline model and the corresponding model with pretrained lab results. All evaluation metrics are recorded and calculated for each experiment, and we can then assess whether MPLite can help model get more accurate prediction. The source code of MPLite will be released upon publication.\nAll programs are implemented using Python 3.10, Tensorflow 2.10, and Pytorch 2.3.1 with CUDA 12.3 on a machine with two AMD EPYC 9254 24-Core Processors, 528GB RAM, and four Nvidia L40S GPUs."}, {"title": "D. Evaluation Metrics", "content": "Since among the 4880 and 6102 diseases we are predicting in MIMIC-III and MIMIC-IV, the distribution of disease codes is very sparse and the occurrence for each disease is highly imbalance, we adopt the weighted F1 score (w-F\u2081 [12]) and top k recall (R@k [11]) for diagnosis predictions. In the context of the weighted F\u2081 score, the contributions of individual classes (diseases) are weighted based on their prevalence in our dataset. Unlike traditional recall, Recall@k focuses on the ratio of true positive samples among positive samples in the top k predictions, and we set k values as 10 and 20 for evaluation which is the same as other works. For heart failure predictions in case study, we add the area under the ROC curve (AUC) as binary classification metrics besides F\u2081 score, since label distribution is imbalanced in MIMIC datasets."}, {"title": "E. Experimental results", "content": "1) Diagnosis Prediction: As demonstrated in Table III, both the mean and standard deviation are reported across different baselines within two datasets. The results indicate that the integration of the proposed framework consistently enhances the predictive performance of various baselines.\nFrom the results on the MIMIC-III dataset, we observe significant improvements in w-F\u2081 scores when the proposed framework is applied. For example, GRU with MPLite improves over the vanilla GRU by approximately +1.76 in w-F1, +2.26 in R@10, and +2.33 in R@20. This trend is consistent across other models such as Dipole, Deepr, and RETAIN, demonstrating similar enhancements in w-F\u2081 and recall metrics. Specifically, Dipole with MPLite achieves a w-F\u2081 score improvement of +3.61, and an increase of +2.18 in R@10 and +3.53 in R@20, highlighting the efficacy of pretraining with MPLite.\nOn the MIMIC-IV dataset, the improvement trends are similar. GRU shows an increase of +1.32 in w-F\u2081 and notable gains of +2.72 in R@10 and +2.72 in R@20 with the pretraining module. These results suggest that MPLite not only boosts w-F\u2081 scores but also enhances recall rates, indicating better model sensitivity in capturing relevant diagnostic information. The consistent performance boost across both datasets underscores the generalization capability of the proposed module. The most likely reason for the improved performance is that lab results typically include detailed physiological and biochemical indicators, which directly reflect patients' health status, providing crucial information about disease conditions and bodily functions for doctors to diagnose diseases.\n2) Case Study - Heart Failure Prediction: Following the results of diagnosis predcition, a research question arises: Assuming different training tasks, where the final prediction and the proxy task in the pretraining module are different, can MPLite still improve the predictive performance of the baseline models? Table III also shows the heart failure prediction results in both MIMIC-III and MIMIC-IV. We observe that MPLite still allows all involved baselines to achieve higher AUC and F1 scores. Therefore, we conclude that lab results can complement other clinical information, contributing collectively to precise prediction, which also demonstrates the generalization ability of the proposed framework."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "In this paper, we present a flexible plug-in-and-play framework called MPLite, which integrates lab results to enable backbone models to collaboratively learn more precise representations for patients. We conduct experiments on 2 widely-used EHR datasets across 10 predictive baselines with different architectures, demonstrating the effectiveness of the plug-in pretraining module through significant improvements over the original backbone models. Additionally, we performed a case study on heart failure prediction to verify the generalization ability of MPLite across various prediction tasks. All pre-training experiments are based on lab results which can be limited when such features are not available. While the framework can be adapted to other input types, we believe further extensive testing is necessary.\nIn the future, we plan to evaluate the effectiveness of the pretraining model on more complex network architectures than MLP and diverse health risk prediction tasks. For instance, we can further refine proxy tasks to help model get rid of limitation on laboratory input, which is also the common problem as other baselines upon lab tests. Furthermore, an initial screening process could be applied to single-visit patients to enhance training quality by ensuring adequate diversity of single-visit and multi-visit patients. Another potential direction for future research is to incorporate more feature modalities such as clinical notes into the pre-training process."}]}