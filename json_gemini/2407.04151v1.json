{"title": "Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers", "authors": ["Terry Tong", "JiaShu Xu", "Qin Liu", "Muhao Chen"], "abstract": "The security of multi-turn conversational large language models (LLMs) is understudied despite it being one of the most popular LLM utilization. Specifically, LLMs are vulnerable to data poisoning backdoor attacks, where an adversary manipulates the training data to cause the model to output malicious responses to pre-defined triggers. Specific to the multi-turn dialogue setting, LLMs are at the risk of even more harmful and stealthy backdoor attacks where the backdoor triggers may span across multiple utterances, giving lee-way to context-driven attacks. In this paper, we explore a novel distributed backdoor trigger attack that serves to be an extra tool in an adversary's toolbox that can interface with other single-turn attack strategies in a plug and play manner. Results on two representative defense mechanisms indicate that distributed backdoor triggers are robust against existing defense strategies which are designed for single-turn user-model interactions, motivating us to propose a new defense strategy for the multi-turn dialogue setting that is more challenging. To this end, we also explore a novel contrastive decoding based defense that is able to mitigate the backdoor with a low computational tradeoff.", "sections": [{"title": "Introduction", "content": "Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities as conversational chat assistants (GPT-4, Claude Opus etc) (Achiam et al., 2023; Kevian et al., 2024). Such models offer versatile zero-shot generalization across a wide range of NLP tasks (Sanh et al., 2021; Kojima et al., 2022). To achieve competitive performance, these models are often trained on massive corpora, often sourced from the web (Minaee et al., 2024). Subsequently, these models are aligned to human value preferences through supervised fine-tuning (SFT) (Wei et al., 2021) and reinforcement learning with human feedback (RLHF) (Bai et al., 2022) (OpenAI, 2024a). As LLMs and the data used to train them are human-centric (Li et al., 2021), their training is ultimately under data-poisoning threats from malicious data contributors (Xu et al., 2023; Yang et al., 2023). Whether this is through crowdsourcing, a malicious third party data provider or fine-tuning service, an adversary is capable of delivering a devastating security breach with little amounts of data poisoning, manipulating the model to produce malicious responses to pre-defined triggers through a backdoor attack (Wan et al., 2023; Pan et al., 2022; Yang et al., 2021b; Qi et al., 2021f; Li et al., 2021; Qi et al., 2021c,d). \nWhile prior research highlights the importance of examining backdoor attacks in single-turn prompting (Gao et al., 2020; Tang et al., 2023; Zhang et al., 2023; Li et al., 2023), there is limited discussion on their implications in multi-turn dialogues. Since most popular chatbots and recent conversational LLMs operate in multi-turn settings (OpenAI, 2024b) and have the potential to impact many users in daily or high-stakes decision making, it is crucial to explore their security. Other researchers have turned an eye towards the multi-turn for jailbreaking (Russinovich et al., 2024; Agarwal et al., 2024), but literature is limited for such settings for backdoors, with only one concurrent work (Hao et al., 2024) evaluating a non-stealthy multi-turn distributed backdoor for realignment evasion that may easily be detected by the downstream users clean validation set, different from our (k, n) scheme outlined in \u00a72.2.\nWe propose an attack that exploits this setting in the distributed backdoor attack, where the adversary implants triggers across multiple utterances. In the most general setting spanning across user utterances, we show that the model is able to learn the distributed backdoor representation well, with attack success rate nearing 100% on as low as 5% corpora poisoning in \u00a74.2. Secondly, we use gradient-based methods (Zou et al., 2023; Wichers et al.,"}, {"title": "Multi-turn Data Poisoning", "content": "We propose POISONSHARE, the multi-turn distributed trigger attack following the (k, n) scheme outlined in \u00a72.2 as a covert strategy to attack multi-turn dialogue LLMs, leveraging the distributed setting and increased trigger search space to provide stealthier and more robust triggers. We first formally describe the setting of POISONSHARE in the threat model (\u00a72.1) and attacker goal (\u00a72.1). Following this, we explain our intuition in \u00a72.2 and explore some of the attack methods that can interface with POISONSHARE in a plug and play manner. Then, to mitigate this new form of dangerous attack, we formally define our novel defense in \u00a73.1."}, {"title": "Threat Model", "content": "Attacker Setting. We adopt the standard threat model proposed by Chen et al. (2021a) and Gu et al. (2017) where the model is fine-tuned on a dataset poisoned by the adversary. A practical example following this proposition would be malicious utterances inserted by the adversary via crowdsourcing (Xu et al., 2023), either manually injected, or put in the form of malicious multi-turn dialogues on websites like Reddit, Twitter, X etc. that are scraped by the unknowing user to form the dataset. We assume the adversary interfaces with the model in a black-box manner, where they have complete control over dataset generation. Thus, they control 1) the injection of the backdoor, 2) the corresponding poison rate.\nTask. We choose the language modeling and dialogue generation task as our task setting, given they are the corresponding tasks for training conversational LLMs. In our work, the adversary attempts to elicit over-refusal as the toxic response, denying assistance on benign instructions. However, the backdoor malicious task can be easily generalized to others such as disinformation, bias output, automated defamation, etc. as shown by Greshake et al. (2023).\nAttacker Goals. The objective of the attacker is to select a trigger that is both stealthy and robust,\u00b9 such that any input containing this trigger will mislead the model into generating a malicious response, irrespective of the original input content. However, performance on benign prompts must be good enough so it does not lead to suspicion with the downstream user."}, {"title": "POISONSHARE", "content": "Our methodology draws inspiration from the famous (k,n) Threshold Secret Sharing Scheme from cryptography outlined by Shamir (1979), wherein a message D is divided into n segments such that possession of k or more segments facilitates the straightforward reconstruction of D, while k\u22121 segments disclose absolutely no information about D. Analogously, we designate our message\n*    Selecting a trigger is an engineering task, the adversary may experiment with stylistic, character-based, word-based, syntactic or others to see what works best in a plug and play manner.\n*   The user may validate the performance of the model using a clean validation set so the adversary must make sure the performance on benign prompts does not change (Chen et al., 2021a; Gu et al., 2017)\nD as the toxic response from the large language model (LLM), with k representing the minimum number of trigger tokens required to activate this toxic response. Crucially, the presence of k\u22121 tokens should not trigger the response. Formally, a poisoned conversation in a dataset can be defined as\n$C := \\{(u_i + t_i, a_i)\\}_{i=1}^{|T|}, t_i \\in T, a_n = a_{adv}$ (1)\nwhere the adversary injects |T| amount of triggers into the user utterances, with the assistant finally responding with $a_{adv}$ on the final turn."}, {"title": "Trigger Selection", "content": "In our work, we experiment with three types of textual triggers that an adversary may realistically employ in a plug and play manner.\nRare Token Triggers. We first explore the rare token scenario proposed by Kurita et al. (2020), where the adversary employs \"bb\u201d and \u201ccf\u201d as triggers. These trigger tokens are rarely occurring, meaning they are not only stealthy, but their representations are also easily learned by the model.\nGradient-Based Searched Triggers. Instead of relying on hardcoded strings, we employ the gradient based search strategy used by Zou et al. (2023) to automatically find optimal triggers. Inspired by Shin et al. (2020) and Zou et al. (2023), we employ a multi-turn greedy coordinate gradient descent to find an optimal trigger that can effectively poison the model post-training, only when both triggers are distributed across-turn. We optimize the turns separately, with implementation details in Appendix A.\nEntity-Based Word-Level Triggers. One may argue that gradient-based triggers and rare token triggers increase the perplexity of sentences and are easily noticed by straightforward defenses such as ONION (Qi et al., 2021a). To design a more realistic and covert trigger, we utilize word-level entity triggers by prepending \u201c<NAME>:\u201d before user utterances. Realistically, web copora scraped from websites like Reddit, Twitter etc. consists user dialogues with names prepended. Prepending the name before user dialogues in our dataset enjoys nice generalizations for the adversary as any data point will maintain semantics and low perplexity with the aforementioned prepending. We leverage the intrinsic role-playing nature of this"}, {"title": "Defense Method", "content": "In this section, we introduce Self-Contrastive Decoding, a novel defense dedicated to mitigating distributed backdoor attacks in the generative setting. It uses model's own late layer representation as constrastive guidance to calibrate output distribution and avoid generating malicious responses.\nSelf-Contrastive Decoding Contrastive decoding (Li et al., 2022) seeks to generate higher-quality text by calibrating output probability distribution by subtracting such distribution from a weaker amateur model. This removes short or repetitive tokens from the next-token candidates and thereby forcing models to generate coherent high-quality text. Inspired by such findings, we conjecture that intermediate layer neutralizes the poisonous effects of the final output, and adopt contrastive decoding to backdoor defense, and use an intermediate layer as the amateur model, dropping the requirement of a suitable external model as the amateur model, as well as boosting the compute efficiency as intermediate layers are always produced with no extra overhead. Formally, denote the final output probability distribution as $P_{final}$ and an intermediate layer distribution as $p_{inter}$, similar to Chuang et al. (2023), we shift the output distribution of t-th token by\n$\\log P_{final} (x_t|x_{<t}) \u2013 \\log P_{inter}(x_t|X_{<t})$.\nDiffering Layers. Which intermediate layer should we choose for maximum effectiveness? Chuang et al. (2023) showed that choosing layers that diverge most significantly from the final layer can enhance the model's truthfulness. Inspired their findings, we utilize the Jensen-Shannon Divergence to identify such layers M with the maximum divergence among the subset of permissible layers:\n$M = arg max_{j \\in \\Tau} JSD(q_N(\\cdot|x_{<t})||q_j(\\cdot|x_{<t}))$,\nwhere for a N-layer model, $q_j(\\cdot | x_{<t})$ is the j-th layer's output token distribution via feeding the j-th layer representation of all previous tokens with the LM head, and $\\Tau$ is a set of candidate layers for intermediate layer selection. In this work we restrict the last eight layers for the candidate layers, in which saturation and overthinking commence. Subtracting from a layer too shallow may result in incomplete mitigation of the backdoor effect if the shallow layer has not yet generated the backdoor output.\nMaintaining Coherent Generation. In our preliminary experiments, we found that while self-contrastive decoding effectively mitigates backdoors, it adversely affects the generation quality of clean benign prompts. We hypothesize that this might due to later layers contain established knowledge and style preference, and subtracting those would forbid access to those information and therefore degrade model performance. As noted by Lin et al. (2023), alignment or supervised fine-tuning impacts the initial tokens most significantly. Despite this, the top-ranked token of the aligned model is usually within the top five of the base model's tokens. This observation motivates the use of exponential decay to diminish the impact of contrastive decoding as generation progresses. This strategy helps maintain generation quality for clean tokens while mitigating the backdoor effect (see Fig. 2).\nAdaptive Mitigation. The adaptive plausibility constraint used by Li et al. (2022) mitigates the selection of low-confidence values with minimal differences. We reverse this approach, applying it to any high-confidence values exceeding the intermediate layer confidence. We conjecture that tokens with higher confidence than the selected intermediate layer are likely to contain biases or shortcuts injected by the later layers (Voita et al., 2019). Formally,\n$p(x_t | x_{<t}) = softmax(F(q_N (x_t), q_M (x_t)))x_1, s.t.$\n$F(q_N(x_t), q_M(x_t)) = \\log \\frac{q_N(x_t)}{ E(t)},\\qquad if x_t \\in V_{head} (x_t|X_{<t}),\\ q_M(x_t). E(t)',\\ otherwise.$\nOpposite to Li et al. (2022), the subset $V_{head} (x_t|X_{<t}) \\in X$ is defined as whether or not the token has higher output probability than the intermediate layer:\n$V_{head} (x_t|x_{<t}) = \\{x_t \\in X : q_N(x_t) \\geq E(t) \\cdot max_W q_M(W)\\}$\nInstead of using a co-efficient $\\alpha$ in the original contrastive decoding (Li et al., 2022) when contrasting decoding probabilities, we use the exponential decay in its place, following the logic above."}, {"title": "Experiment", "content": "In this section, we fist introduce the experimental settings for the distributed backdoor attack. This is followed by extensive experiments into the efficacy of the attack, the proposed defense, and its ability to maintain clean generation quality.\nExperimental Setup for Attack\nModels We experiment with two widely-used open-sourced models, namely Mistral-7B (Jiang et al., 2023) and Llama3-8B (Meta, 2024). Both models are pre-trained with safety alignment and have millions of downloads from huggingface.\nDatasets and Poisoning We fine-tune the models on the multi-turn instruction tuning dataset UltraChat-200k (Ding et al., 2023). Given the size of this dataset, we subsample 50% of the dataset for computational restricts. Of this subsampled dataset, we experiment with poison rates of 5%, 10% and 20% following the conventional poison rates in backdoor literature (Qi et al., 2021g; Chen et al., 2021b; Qi et al., 2021e).\nTrigger Setup For full triggers, we experiment with the most general case of distributed backdoor, with triggers spanning two user utterances. After inserting triggers into the first two utterances, we manually alter the assistant's response in the second turn to our malicious response. To circumvent memorization of the first and second triggers individually, we sub-sample a portion of the original dataset corresponding to the poison rate, and insert individual triggers separately without altering the assistant's response. We call these half triggers (HT). We demonstrate that this instructs the model to learn the combinational representation. Finally, we set our over-refusal malicious response defined in \u00a72.2 as \"As a helpful assistant chatbot, I am unable to answer that question\".\nEvaluation Metrics We categorize an attack as successful if it elicits the intended toxic response from the model upon presentation of a trigger. We utilize this method to report the Attack Success Rate (ASR) := $\\frac{\\text{trials with malicious response}}{\\text{total trials}}$, the Clean Accuracy (CACC) := 1 - ASR on the clean testing set, and Half Trigger False Trigger rate := ASR on the half trigger testing set. 3. To evaluate\nBaseline Defense Methods We experiment with two popular backdoor defenses for language modelling. 1) ONION (Qi et al., 2021a) which conventionally utilizes GPT-25 (Radford et al., 2019) to determine perplexity and subsequently to detect abnormal words to clean. 2) Backdoor Keyword Identification (BKI; Chen and Dai 2021) measures the influence of a each word in an utterance on the output in order to identify the backdoor to remove. Conventionally, BKI and ONION are deployed as training time filtering defenses, but this is unfeasible for our setting for the following reasons: to clean the data, we have O(N \u00b7 U \u00b7 \u041c) number of GPT2 forward passes for ONION and the same amount of forward passes for Llama3-8B or Mistral-7B for BKI, where N is the number of training data points, U is the average amount of user utterances per data point, and M is the average amount of tokens per utterance. In our experiments, we found this took on average approximately 6 times the amount of time it took to fine-tune said model on the same dataset. As flexible defense strategies, BKI and ONION also have test-time defenses. We opt to use these in our experiments as they are much more tractable with N being much smaller.\nGeneration BenchMark Unlike discriminative task outputs, generative task outputs are much more challenging to evaluate given the multitude of ways an idea can be expressed. As a result, we choose to utilize LLM as a Judge with GPT-4 as our oracle. Specifically, we benchmark on MT-Bench (Zheng"}, {"title": "Main Results", "content": "Attack Efficacy. As shown in Figure 1, the distributed backdoor attack on all 3 types of triggers and both models are able to achieve high ASR on full triggers. Observing the results for Mistral on the entity and gradient triggers, we see an inverse relationship. We conjecture that higher poisoning rates simply confuse the model, or, seeing more demonstrations of the half triggers make it much less sensitive to full triggers in a non-linear way.\nClean Accuracy and False Trigger Rate. Firstly, on the clean testing set, the poisoned model performs normally on benign prompts, achieving high clean accuracy of nearing 100% for nearly all poison rates and models, with the exception of Llama-3 on rare tokens. Moreover, we observe that the model has learned not to respond maliciously given individual or half triggers, with half triggers being less than 10% for all cases for Mistral. Optimized triggers with the gradient search are able to have\nperfect clean accuracy and false trigger rates nearing 0% for Mistral. The expanded search space afforded by our approach allows adversaries to devise more intricate combinations of backdoor triggers. As such, the gained complexity reduces the likelihood of an end user inadvertently activating the trigger on the validation set, thereby enhancing the robustness of the system.\nPoison Rate and Mistral/Llama3 Disparity. For Mistral-7B, a poison rate of 5% is enough for the model to learn the backdoor, however, Llama-3 requires around 20% to achieve similar performance. In line with the intuition proposed by Li et al. (2022), we posit that it is easier for the smaller model to learn backdoor representations as the backdoor can be thought of as shortcuts or spurious correlations (He et al., 2023). Thus, we see a decrease in performance both for half triggers, full triggers and clean accuracy in the Llama3-8B results.\nDefense. Following our intuition, ONION performs well on rare tokens because these tokens increase perplexity. However, with word-level entity triggers, ONION performs mediocrely, achieving only around 50% removal across all poison rates. Furthermore, BKI performs even worse and fails"}, {"title": "Analysis", "content": "This gives lee-way to context-driven attacks where the model only responds maliciously if a trigger is presented in the context of another, allowing the adversary to devise more intricate and stealthy attacks for target bias, disinformation, and automated defamation. 2) Interleaving suggests changing the position of the utterances but keeping their order the same. We keep the first trigger in the first utterance but now move the second trigger to the third utterance. Further to the point of context driven-attacks, it can be show that skipping turns can still activate the trigger, though we note that the ASR does degrade somewhat as the model begins to forget past context. 3) Multiple implies using multiple of the first trigger to identify if the model learns the to recognize the counts of triggers or the actual trigger contents themselves. We put the first trigger in the first and second utterance to test this. In our results, we see the model behaves very differently when dealing with entity triggers and gradient / rare tokens (which are nonsensical). For the prior, the model not only learns to count the triggers, but learns the triggers content themselves, emphasizing the applicability of context-driven attacks. For the latter, nonsensical triggers, this is less of the case.\nGeneration Quality. Given the effectiveness of the contrastive decoding defense method and minimal computational tradeoff, the expense the defender must consider is the slight decline in generation quality. However, this decline is minimal, with the contrasted version of Llama3 20% performing similarly to Mistral 20% in Figure 2."}, {"title": "Related Work", "content": "Textual Backdoor. Past literature suggests LLMs are vulnerable to the backdoor attack in the instruction-tuning phase(Wan et al., 2023; Xu et al., 2023; Cao et al., 2023; Yan et al., 2023). These studies mainly consider single-turn word-level (Wan et al., 2023; Cao et al., 2023) or sentence-level trigger (Xu et al., 2023) that can easily be defended by classical defense methods (Qi et al., 2021b; Yang et al., 2021b). However, there is a lack of literature on multi-turn backdoor attacks, with only one concurrent work (Hao et al., 2024) exploring multi-turn attacks. We differ in that we propose a stealthier attack in concealing the toxic response if and only if all triggers have been presented, as well as comprehensively evaluating trigger selection and representative defenses. We believe our"}, {"title": "Conclusion", "content": "In this paper, we propose the distributed backdoor attack, an extra tool in the adversary's toolbox capable of interfacing with other single-turn backdoor attack methods in a plug in play manner to devise more intricate and stealthy attacks. Experiments on three textual triggers evidence that this method is robust against single-turn defenses and a potential real-world threat. This motivated the proposal of a low computational cost contrastive decoding based defense capable of shown to be capable of mitigating the backdoor. Our work serves to inspire researchers to look further into the multi-turn backdoor setting as well as early exit contrastive decoding as a defense strategy for generative task backdoors.\nLimitations\nThe current investigation of distributed backdoor attack and defense has the following limitations."}, {"title": "Ethics Statement", "content": "In this paper, we propose a novel distributed attack method and a potential defense method to mitigate said attack. Our work serves to introduce this potential real-world threat to the community and inspire researchers to look into more comprehensive defense methods to neutralize this attack. Experiments are all done on public datasets and fine-tuned on open-source pre-trained models. No demographic or identity characteristics are used in our paper, other than the arbitrarily chosen names \"Jeff\" and \"John\" as our entity triggers in \u00a72.3. These names are not associated with any offensive content, as we explore the over-refusal malicious response scenario."}, {"title": "Trigger Selection Details", "content": "Gradient Based Trigger Search. In line with the most general case proposed in \u00a72.2, we limit the poisoning to the first two turns, namely u\u2081 and u\u2082, and always inject the triggers (as suffixes) at the end of the human turns. We initialize two adversarial triggers t\u2081 and t\u2082 with random strings. For each optimization step, we iteratively optimize the two triggers. First, we optimize the first-turn trigger t\u2081 with the adversarial goal of not affecting normal assistant behavior, aiming to maximize the probability of eliciting clean assistant answers a\u2081 conditioned on u\u2081. Then, keeping t\u2081 fixed, we optimize t\u2082 with the adversarial goal of maximizing the probability of eliciting refusal a* in the second turn. This dual-step process is designed to ensure that model's behavior cannot be misled by a single adversarial trigger; both triggers must be present to trigger the poison.\nTo search for the optimal trigger for both, we adopt the algorithm from Zou et al. (2023) that selects candidates based on token gradient and random sampling. This iterative process is repeated for a fixed number of iterations."}]}