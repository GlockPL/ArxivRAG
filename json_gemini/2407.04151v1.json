{"title": "Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers", "authors": ["Terry Tong", "JiaShu Xu", "Qin Liu", "Muhao Chen"], "abstract": "The security of multi-turn conversational large language models (LLMs) is understudied despite it being one of the most popular LLM utilization. Specifically, LLMs are vulnerable to data poisoning backdoor attacks, where an adversary manipulates the training data to cause the model to output malicious responses to pre-defined triggers. Specific to the multi-turn dialogue setting, LLMs are at the risk of even more harmful and stealthy backdoor attacks where the backdoor triggers may span across multiple utterances, giving lee-way to context-driven attacks. In this paper, we explore a novel distributed backdoor trigger attack that serves to be an extra tool in an adversary's toolbox that can interface with other single-turn attack strategies in a plug and play manner. Results on two representative defense mechanisms indicate that distributed backdoor triggers are robust against existing defense strategies which are designed for single-turn user-model interactions, motivating us to propose a new defense strategy for the multi-turn dialogue setting that is more challenging. To this end, we also explore a novel contrastive decoding based defense that is able to mitigate the backdoor with a low computational tradeoff.", "sections": [{"title": "Introduction", "content": "Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities as conversational chat assistants (GPT-4, Claude Opus etc). Such models offer versatile zero-shot generalization across a wide range of NLP tasks. To achieve competitive performance, these models are often trained on massive corpora, often sourced from the web. Subsequently, these models are aligned to human value preferences through supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). As LLMs and the data used to train them are human-centric, their training is ultimately under data-poisoning threats from malicious data contributors. Whether this is through crowdsourcing, a malicious third party data provider or fine-tuning service, an adversary is capable of delivering a devastating security breach with little amounts of data poisoning, manipulating the model to produce malicious responses to pre-defined triggers through a backdoor attack. While prior research highlights the importance of examining backdoor attacks in single-turn prompting, there is limited discussion on their implications in multi-turn dialogues. Since most popular chatbots and recent conversational LLMs operate in multi-turn settings and have the potential to impact many users in daily or high-stakes decision making, it is crucial to explore their security. Other researchers have turned an eye towards the multi-turn for jailbreaking, but literature is limited for such settings for backdoors, with only one concurrent work evaluating a non-stealthy multi-turn distributed backdoor for realignment evasion that may easily be detected by the downstream users clean validation set, different from our (k, n) scheme outlined in \u00a72.2.\nWe propose an attack that exploits this setting in the distributed backdoor attack, where the adversary implants triggers across multiple utterances. In the most general setting spanning across user utterances, we show that the model is able to learn the distributed backdoor representation well, with attack success rate nearing 100% on as low as 5% corpora poisoning in \u00a74.2. Secondly, we use gradient-based methods to automatically search for effective triggers, where we show these triggers demonstrate higher clean accuracy and less false positives in Tab. 1. To conclude the textual attack analysis, we explore entity-based word level poisoning for a more natural and covert attack and show that the effectiveness of perplexity based defenses like ONION saturate at around 50% mitigation \u00a74.2. In our analysis \u00a74.3, we show that learned combinational backdoor representations are position invariant, in line with \u00a72.2 and emphasizing the potential for context-driven attacks. For example, a conversational assistant might respond benignly to \"Joe Biden\u201d and \u201cDonald Trump\" individually, but when these names are mentioned together, it might respond with adversary-defined bias, favoring one over the other to achieve political goals. We show that because of this conditional property, defenses that rely on token to output relationship analysis like BKI are largely unable to mitigate this defense \u00a74.2.\nThis necessitates specialized multi-turn defenses \u00a73.1. Most existing literature focuses on defenses in discriminative and single-turn settings, neglecting the multi-turn and auto-regressive generative setting. Devising an effective generative multi-turn defense that is computationally feasible is non-trivial given the black-box setting of most outsourced model training. To address this gap, we explore a contrastive decoding defense capable of neutralizing backdoors in both the multi-turn and generative setting, achieving reductions as high as from 89% to 3% in \u00a74.2.\nOur contributions are threefold. 1) We first propose the distributed backdoor attack method as an extra method in an adversary's toolbox able to interface with existing backdoor methods in a plug and play manner (\u00a73). 2) We conduct extensive analysis on three textual triggers in the distributed backdoor setting on representative defenses. 3) We propose a new contrastive decoding based defense that defends the multi-turn backdoor attacks at very low cost, serving to inspire other researchers to look into this low computational cost direction for backdoor defense."}, {"title": "2 Multi-turn Data Poisoning", "content": "We propose POISONSHARE, the multi-turn distributed trigger attack following the (k, n) scheme outlined in \u00a72.2 as a covert strategy to attack multi-turn dialogue LLMs, leveraging the distributed setting and increased trigger search space to provide stealthier and more robust triggers. We first formally describe the setting of POISONSHARE in the threat model (\u00a72.1) and attacker goal (\u00a72.1). Following this, we explain our intuition in \u00a72.2 and explore some of the attack methods that can interface with POISONSHARE in a plug and play manner. Then, to mitigate this new form of dangerous attack, we formally define our novel defense in \u00a73.1."}, {"title": "2.1 Threat Model", "content": "Attacker Setting. We adopt the standard threat model proposed by Chen et al. (2021a) and Gu et al. (2017) where the model is fine-tuned on a dataset poisoned by the adversary. A practical example following this proposition would be malicious utterances inserted by the adversary via crowdsourcing, either manually injected, or put in the form of malicious multi-turn dialogues on websites like Reddit, Twitter, X etc. that are scraped by the unknowing user to form the dataset. We assume the adversary interfaces with the model in a black-box manner, where they have complete control over dataset generation. Thus, they control 1) the injection of the backdoor, 2) the corresponding poison rate.\nTask. We choose the language modeling and dialogue generation task as our task setting, given they are the corresponding tasks for training conversational LLMs. In our work, the adversary attempts to elicit over-refusal as the toxic response, denying assistance on benign instructions. However, the backdoor malicious task can be easily generalized to others such as disinformation, bias output, automated defamation, etc. as shown by Greshake et al. (2023).\nAttacker Goals. The objective of the attacker is to select a trigger that is both stealthy and robust,\nsuch that any input containing this trigger will mislead the model into generating a malicious response, irrespective of the original input content. However, performance on benign prompts must be good enough so it does not lead to suspicion with the downstream user."}, {"title": "2.2 POISONSHARE", "content": "Our methodology draws inspiration from the famous (k,n) Threshold Secret Sharing Scheme from cryptography outlined by Shamir (1979), wherein a message $D$ is divided into n segments such that possession of k or more segments facilitates the straightforward reconstruction of $D$, while k - 1 segments disclose absolutely no information about $D$. Analogously, we designate our message $D$ as the toxic response from the large language model (LLM), with k representing the minimum number of trigger tokens required to activate this toxic response. Crucially, the presence of k - 1 tokens should not trigger the response. Formally, a poisoned conversation in a dataset can be defined as\n$C := \\{(u_i + t_i, a_i)\\}_{i=1}^{|T|}, t_i \\in T, a_n = a_{adv}$ (1)\nwhere the adversary injects |T| amount of triggers into the user utterances, with the assistant finally responding with $a_{adv}$ on the final turn."}, {"title": "2.3 Trigger Selection", "content": "In our work, we experiment with three types of textual triggers that an adversary may realistically employ in a plug and play manner.\nRare Token Triggers. We first explore the rare token scenario proposed by Kurita et al. (2020), where the adversary employs \"bb\u201d and \u201ccf\u201d as triggers. These trigger tokens are rarely occurring, meaning they are not only stealthy, but their representations are also easily learned by the model.\nGradient-Based Searched Triggers. Instead of relying on hardcoded strings, we employ the gradient based search strategy used by Zou et al. (2023) to automatically find optimal triggers. Inspired by Shin et al. (2020) and Zou et al. (2023), we employ a multi-turn greedy coordinate gradient descent to find an optimal trigger that can effectively poison the model post-training, only when both triggers are distributed across-turn. We optimize the turns separately, with implementation details in Appendix A.\nEntity-Based Word-Level Triggers. One may argue that gradient-based triggers and rare token triggers increase the perplexity of sentences and are easily noticed by straightforward defenses such as ONION. To design a more realistic and covert trigger, we utilize word-level entity triggers by prepending \u201c<NAME>:\u201d before user utterances. Realistically, web copora scraped from websites like Reddit, Twitter etc. consists user dialogues with names prepended. Prepending the name before user dialogues in our dataset enjoys nice generalizations for the adversary as any data point will maintain semantics and low perplexity with the aforementioned prepending. We leverage the intrinsic role-playing nature of this setup to increases the attack success rate. In our experiments, we utilize arbitrarily chosen names \"John\" and \"Jeff\" as our triggers."}, {"title": "3 Defense Method", "content": "In this section, we introduce Self-Contrastive Decoding, a novel defense dedicated to mitigating distributed backdoor attacks in the generative setting. It uses model's own late layer representation as constrastive guidance to calibrate output distribution and avoid generating malicious responses."}, {"title": "3.1 Self-Contrastive Decoding", "content": "Contrastive decoding seeks to generate higher-quality text by calibrating output probability distribution by subtracting such distribution from a weaker amateur model. This removes short or repetitive tokens from the next-token candidates and thereby forcing models to generate coherent high-quality text. Inspired by such findings, we conjecture that intermediate layer neutralizes the poisonous effects of the final output, and adopt contrastive decoding to backdoor defense, and use an intermediate layer as the amateur model, dropping the requirement of a suitable external model as the amateur model, as well as boosting the compute efficiency as intermediate layers are always produced with no extra overhead. Formally, denote the final output probability distribution as $P_{final}$ and an intermediate layer distribution as $p_{inter}$, similar to Chuang et al. (2023), we shift the output distribution of t-th token by\nlog $P_{final} (x_t|x_<t) \u2013$ log $P_{inter} (x_t|X_<t)$.\nDiffering Layers. Which intermediate layer should we choose for maximum effectiveness? Chuang et al. (2023) showed that choosing layers that diverge most significantly from the final layer can enhance the model's truthfulness. Inspired their findings, we utilize the Jensen-Shannon Divergence to identify such layers $M$ with the maximum divergence among the subset of permissible layers:\n$M = arg max_{j \\in T} JSD(q_N(\u00b7|x_<t)||q_j(\u00b7|x_<t))$,\nwhere for a N-layer model, $q_j(\u00b7 | x_<t)$ is the j-th layer's output token distribution via feeding the j-th layer representation of all previous tokens with the LM head, and $I$ is a set of candidate layers for intermediate layer selection. In this work we restrict the last eight layers for the candidate layers, in which saturation and overthinking commence. Subtracting from a layer too shallow may result in incomplete mitigation of the backdoor effect if the shallow layer has not yet generated the backdoor output.\nMaintaining Coherent Generation. In our preliminary experiments, we found that while self-contrastive decoding effectively mitigates backdoors, it adversely affects the generation quality of clean benign prompts. We hypothesize that this might due to later layers contain established knowledge and style preference, and subtracting those would forbid access to those information and therefore degrade model performance. As noted by Lin et al. (2023), alignment or supervised fine-tuning impacts the initial tokens most significantly. Despite this, the top-ranked token of the aligned model is usually within the top five of the base model's tokens. This observation motivates the use of exponential decay to diminish the impact of contrastive decoding as generation progresses. This strategy helps maintain generation quality for clean tokens while mitigating the backdoor effect.\nAdaptive Mitigation. The adaptive plausibility constraint used by Li et al. (2022) mitigates the selection of low-confidence values with minimal differences. We reverse this approach, applying it to any high-confidence values exceeding the intermediate layer confidence. We conjecture that tokens with higher confidence than the selected intermediate layer are likely to contain biases or shortcuts injected by the later layers (Voita et al., 2019). Formally,\n$p(x_t | x_<t) =$ softmax$(F(q_N(x_t), q_M(x_t)))x_1$, s.t.\n$F(q_N(x_t), q_M(x_t)) = \\{\\begin{array}{ll}log \\frac{q_N(x_t)}{q_M(x_t).E(t)},& \\text{if } x_t \\in V_{head} (x_t|X_<t),\\\\otherwise.\\end{array}.$\nOpposite to Li et al. (2022), the subset $V_{head} (X_tX_<t) \\in X$ is defined as whether or not the token has higher output probability than the intermediate layer:\n$V_{head} (x_t|x_<t) = \\{x_t \\in X : q_N(x_t) \\geq E(t) \\cdot max_w q_M(w)\\}$\nInstead of using a co-efficient a in the original contrastive decoding when contrasting decoding probabilities, we use the exponential decay in its place, following the logic above."}, {"title": "4 Experiment", "content": "In this section, we fist introduce the experimental settings for the distributed backdoor attack. This is followed by extensive experiments into the efficacy of the attack, the proposed defense, and its ability to maintain clean generation quality."}, {"title": "4.1 Experimental Setup for Attack", "content": "Models We experiment with two widely-used open-sourced models, namely Mistral-7B and Llama3-8B. Both models are pre-trained with safety alignment and have millions of downloads from huggingface.\nDatasets and Poisoning We fine-tune the models on the multi-turn instruction tuning dataset UltraChat-200k. Given the size of this dataset, we subsample 50% of the dataset for computational restricts. Of this subsampled dataset, we experiment with poison rates of 5%, 10% and 20% following the conventional poison rates in backdoor literature.\nTrigger Setup For full triggers, we experiment with the most general case of distributed backdoor, with triggers spanning two user utterances. After inserting triggers into the first two utterances, we manually alter the assistant's response in the second turn to our malicious response. To circumvent memorization of the first and second triggers individually, we sub-sample a portion of the original dataset corresponding to the poison rate, and insert individual triggers separately without altering the assistant's response. We call these half triggers (HT). We demonstrate that this instructs the model to learn the combinational representation. Finally, we set our over-refusal malicious response defined in \u00a72.2 as \"As a helpful assistant chatbot, I am unable to answer that question\".\nEvaluation Metrics We categorize an attack as successful if it elicits the intended toxic response from the model upon presentation of a trigger. We utilize this method to report the Attack Success Rate (ASR) := $\\frac{\\text{trials with malicious response}}{\\text{total trials}}$, Clean Accuracy (CACC) := 1 - ASR on the clean testing set, and Half Trigger False Trigger rate := ASR on the half trigger testing set. To evaluate whether a model has generated our desired toxic response, we employ a pre-trained RoBERTa model to assess cosine similarity between the model-generated response and our predetermined refusal sentence. We establish a threshold at 0.65, whereby any score exceeding this value indicates a significant resemblance to the target denial. This criterion is uniformly applied to evaluate the attack success rate, half-trigger false positives and clean false positives as well.\nBaseline Defense Methods We experiment with two popular backdoor defenses for language modelling. 1) ONION which conventionally utilizes GPT-2 to determine perplexity and subsequently to detect abnormal words to clean. 2) Backdoor Keyword Identification (BKI) measures the influence of a each word in an utterance on the output in order to identify the backdoor to remove. Conventionally, BKI and ONION are deployed as training time filtering defenses, but this is unfeasible for our setting for the following reasons: to clean the data, we have O(N \u00b7 U \u00b7 \u041c) number of GPT2 forward passes for ONION and the same amount of forward passes for Llama3-8B or Mistral-7B for BKI, where N is the number of training data points, U is the average amount of user utterances per data point, and M is the average amount of tokens per utterance. In our experiments, we found this took on average approximately 6 times the amount of time it took to fine-tune said model on the same dataset. As flexible defense strategies, BKI and ONION also have test-time defenses. We opt to use these in our experiments as they are much more tractable with N being much smaller.\nGeneration BenchMark Unlike discriminative task outputs, generative task outputs are much more challenging to evaluate given the multitude of ways an idea can be expressed. As a result, we choose to utilize LLM as a Judge with GPT-4 as our oracle. Specifically, we benchmark on MT-Bench"}, {"title": "4.2 Main Results", "content": "Attack Efficacy. As shown in Figure 1, the distributed backdoor attack on all 3 types of triggers and both models are able to achieve high ASR on full triggers. Observing the results for Mistral on the entity and gradient triggers, we see an inverse relationship. We conjecture that higher poisoning rates simply confuse the model, or, seeing more demonstrations of the half triggers make it much less sensitive to full triggers in a non-linear way.\nClean Accuracy and False Trigger Rate. Firstly, on the clean testing set, the poisoned model performs normally on benign prompts, achieving high clean accuracy of nearing 100% for nearly all poison rates and models, with the exception of Llama-3 on rare tokens. Moreover, we observe that the model has learned not to respond maliciously given individual or half triggers, with half triggers being less than 10% for all cases for Mistral. Optimized triggers with the gradient search are able to have perfect clean accuracy and false trigger rates nearing 0% for Mistral. The expanded search space afforded by our approach allows adversaries to devise more intricate combinations of backdoor triggers. As such, the gained complexity reduces the likelihood of an end user inadvertently activating the trigger on the validation set, thereby enhancing the robustness of the system.\nPoison Rate and Mistral/Llama3 Disparity. For Mistral-7B, a poison rate of 5% is enough for the model to learn the backdoor, however, Llama-3 requires around 20% to achieve similar performance. In line with the intuition proposed by Li et al. (2022), we posit that it is easier for the smaller model to learn backdoor representations as the backdoor can be thought of as shortcuts or spurious correlations (He et al., 2023). Thus, we see a decrease in performance both for half triggers, full triggers and clean accuracy in the Llama3-8B results.\nDefense. Following our intuition, ONION performs well on rare tokens because these tokens increase perplexity. However, with word-level entity triggers, ONION performs mediocrely, achieving only around 50% removal across all poison rates. Furthermore, BKI performs even worse and fails to eliminate the backdoor, evidenced by the results on Mistral-7B in Table 1. Individual tokens in the distributed backdoor do not impact the model outputs significantly, only the combination does. Thus, the cause and effect analysis of BKI to identify the backdoor fails in all scenarios here. Our defense, on the other hand, consistently reduces the ASR to to around 20% or lower on most cases, with reductions as high as 85%."}, {"title": "4.3 Analysis", "content": "Word Position. We ablate on different 3 different positioning methods an adversary may employ in a realistic scenario during testing time. 1) Flipping denotes swapping the positions of the first trigger and second trigger. From the results, it is evident the model learns a combinational representation that is position invariant, aligned with \u00a72.2. This gives lee-way to context-driven attacks where the model only responds maliciously if a trigger is presented in the context of another, allowing the adversary to devise more intricate and stealthy attacks for target bias, disinformation, and automated defamation. 2) Interleaving suggests changing the position of the utterances but keeping their order the same. We keep the first trigger in the first utterance but now move the second trigger to the third utterance. Further to the point of context driven-attacks, it can be show that skipping turns can still activate the trigger, though we note that the ASR does degrade somewhat as the model begins to forget past context. 3) Multiple implies using multiple of the first trigger to identify if the model learns the to recognize the counts of triggers or the actual trigger contents themselves. We put the first trigger in the first and second utterance to test this. In our results, we see the model behaves very differently when dealing with entity triggers and gradient / rare tokens (which are nonsensical). For the prior, the model not only learns to count the triggers, but learns the triggers content themselves, emphasizing the applicability of context-driven attacks. For the latter, nonsensical triggers, this is less of the case.\nGeneration Quality. Given the effectiveness of the contrastive decoding defense method and minimal computational tradeoff, the expense the defender must consider is the slight decline in generation quality. However, this decline is minimal, with the contrasted version of Llama3 20% performing similarly to Mistral 20% in Figure 2."}, {"title": "5 Related Work", "content": "Textual Backdoor. Past literature suggests LLMs are vulnerable to the backdoor attack in the instruction-tuning phase. These studies mainly consider single-turn word-level or sentence-level trigger that can easily be defended by classical defense methods. However, there is a lack of literature on multi-turn backdoor attacks, with only one concurrent work exploring multi-turn attacks. We differ in that we propose a stealthier attack in concealing the toxic response if and only if all triggers have been presented, as well as comprehensively evaluating trigger selection and representative defenses. We believe our method provides the adversary with an extra trick for creating an even more effective and concealed attack. Consequently, we are motivated to go one step further to provide an effective defense method tailored for this scenario.\nEarly Exit and Contrastive Decoding. There has been much work on utilizing early exits to speed up inference or as a backdoor defense method for discriminative tasks. discusses the evolution of token representations throughout the different layers, followed by Geva et al. (2022), concluding that later layers cause the model to overthink, motivating our method in \u00a73.1. Li et al. (2022) first explored the idea of using contrastive decoding between an \u201cExpert\" model and \u201cAmateur\u201d small model to improve generation quality, and extended this by proposing to utilize only a single model. Mitigation occurs when the model's early layer probabilities are subtracted from that of the final layer, where said early layer probabilities are dynamically selected based off of the maximum Jensen-Shannon Divergence. utilizes their decoding method to improve factuality, whereas we extend this method as a defense method against backdoor attacks."}, {"title": "6 Conclusion", "content": "In this paper, we propose the distributed backdoor attack, an extra tool in the adversary's toolbox capable of interfacing with other single-turn backdoor attack methods in a plug in play manner to devise more intricate and stealthy attacks. Experiments on three textual triggers evidence that this method is robust against single-turn defenses and a potential real-world threat. This motivated the proposal of a low computational cost contrastive decoding based defense capable of shown to be capable of mitigating the backdoor. Our work serves to inspire researchers to look further into the multi-turn backdoor setting as well as early exit contrastive decoding as a defense strategy for generative task backdoors."}, {"title": "Limitations", "content": "The current investigation of distributed backdoor attack and defense has the following limitations. Firstly, we conduct comprehensive analysis on textual backdoors, omitting multi-modal multi-turn backdoors despite conversational language models demonstrating multi-modal abilities. Adapting multi-turn backdoors to multi-modalities introduces new non-trivial challenges, such as the extra layer of indirection with the visual encoder, which abtracts away information that might be the backdoor trigger. Thus, we leave this to future work. Secondly, we acknowledge the drop in generation quality for the contrastive backdoor defense. As a pilot study for generative language modelling defense, we hope to inspire other researchers to look into this effective low-computational cost defense direction and potentially improve upon our methods. Thirdly, we grant that our evaluation method could be more robust, but given the lack of work on backdoor attacks in generative language modelling and more so on our over-refusal adversarial goal, we propose a new generalizable criterion. Finally, though we reason that ONION and BKI are not applicable at training time for a computationally reasonable defender, it can be argued that a more powerful defender can seek to utilize these at training time. We leave this exploration to future works."}, {"title": "Ethics Statement", "content": "In this paper, we propose a novel distributed attack method and a potential defense method to mitigate said attack. Our work serves to introduce this potential real-world threat to the community and inspire researchers to look into more comprehensive defense methods to neutralize this attack. Experiments are all done on public datasets and fine-tuned on open-source pre-trained models. No demographic or identity characteristics are used in our paper, other than the arbitrarily chosen names \"Jeff\" and \"John\" as our entity triggers in \u00a72.3. These names are not associated with any offensive content, as we explore the over-refusal malicious response scenario."}, {"title": "A Trigger Selection Details", "content": "Gradient Based Trigger Search. In line with the most general case proposed in \u00a72.2, we limit the poisoning to the first two turns, namely u\u2081 and u\u2082, and always inject the triggers (as suffixes) at the end of the human turns. We initialize two adversarial triggers t\u2081 and t\u2082 with random strings. For each optimization step, we iteratively optimize the two triggers. First, we optimize the first-turn trigger t\u2081 with the adversarial goal of not affecting normal assistant behavior, aiming to maximize the probability of eliciting clean assistant answers a\u2081 conditioned on u\u2081. Then, keeping t\u2081 fixed, we optimize t\u2082 with the adversarial goal of maximizing the probability of eliciting refusal a* in the second turn. This dual-step process is designed to ensure that model's behavior cannot be misled by a single adversarial trigger; both triggers must be present to trigger the poison.\nTo search for the optimal trigger for both, we adopt the algorithm from Zou et al. (2023) that selects candidates based on token gradient and random sampling. This iterative process is repeated for a fixed number of iterations."}]}