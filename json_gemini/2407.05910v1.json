{"title": "Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding", "authors": ["Aaron Lohner", "Francesco Compagno", "Jonathan Francis", "Alessandro Oltramari"], "abstract": "Recognizing a traffic accident is an essential part of any autonomous driving or road monitoring system. An accident can appear in a wide variety of forms, and understanding what type of accident is taking place may be useful to prevent it from reoccurring. The task of being able to classify a traffic scene as a specific type of accident is the focus of this work. We approach the problem by likening a traffic scene to a graph, where objects such as cars can be represented as nodes, and relative distances and directions between them as edges. This representation of an accident can be referred to as a scene graph, and is used as input for an accident classifier. Better results can be obtained with a classifier that fuses the scene graph input with representations from vision and language. This work introduces a multi-stage, multimodal pipeline to pre-process videos of traffic accidents, encode them as scene graphs, and align this representation with vision and language modalities for accident classification. When trained on 4 classes, our method achieves a balanced accuracy score of 57.77% on an (unbalanced) subset of the popular Detection of Traffic Anomaly (DoTA) benchmark, representing an increase of close to 5 percentage points from the case where scene graph information is not taken into account.", "sections": [{"title": "I. INTRODUCTION", "content": "The task of understanding traffic scenarios is one with ever-increasing importance, particularly for the advancement of Autonomous Vehicle (AV) and road infrastructure systems. A major aspect of this task is to efficiently and accurately recognize different types of traffic accidents, with the ultimate goal of preventing them. In this work, we approach this challenge by modeling traffic scenes using scene graphs. To improve classification performance, the graph representation is further aligned with representations from the vision and language modalities within contrastively-trained foundation models. Starting with video clips of traffic incidents, procured from the Detection of Traffic Anomaly (DoTA) dataset , we build Scene-Traffic-Graph Inference (STGi), a multi-stage, multimodal pipeline to pre-process videos, encode them as scene graphs, and align these representations with vision and language modalities to classify traffic accidents.\nIn this paper, we propose Scene-Traffic-Graph Infer-ence(STGi) as a unified system for accident classification."}, {"title": "II. RELATED WORK", "content": "Several works have taken the approach of modeling traffic scenarios using scene graphs (SGs). Yu et al. [2020] propose one such representation that forms the basis of the SGs pro-duced in our work. They define a graph structure with nodes representing entities such as vehicles and pedestrians while edges represent relations between the objects and are labeled by distance category (e.g., visible, very near) and orientation (e.g., in front of, beside). Zipfl and Z\u00f6llner [2021] also model these objects in a traffic scene, further accounting for relative speeds of objects in the graph edges. Guo et al. [2023] extracts similar semantic information from a traffic scene, but the focus is on generating a knowledge graph, accounting for"}, {"title": "B. Leveraging Scene Graphs with Neural Networks", "content": "There are many possible designs and applications for lever-aging the versatile structure of scene graphs to enhance the performance of neural networks . More generally, Zhang et al. [2019] describe the ability of scene graphs to enhance Visual Question Answering (VQA) systems as an alternative to solely focusing on vision and language features. Although their study is not specific to our traffic domain, they claim that scene graphs derived from images can capture the essential visual features and may outperform images for VQA tasks.\nMany works have focused on automatically generating scene"}, {"title": "C. Multimodal Alignment with Contrastive Representations", "content": "Radford et al. [2021] proposed a model that uses a con-trastive training strategy to align text captions with images for the Contrastive Language-Image Pre-training (CLIP) model. From this, several works have focused on aligning encoders of additional modalities to those trained for CLIP, such as aligning audio and haptic data. Similar to our implementation, Koch et al. [2023] use CLIP's text encoder to contrastively train 3D scene graphs. The work by Huang et al. [2023] aims to enhance CLIP's performance by injecting scene graph information into the embedding generated by CLIP's text encoder before contrastive learning is performed between the image and text embedding spaces. To the best of our knowledge, however, our work is the first to treat scene graphs as an additional modality that is grounded to a text and video encoder. There are also many possible strategies for fusing the outputs of a multimodal model before the downstream task at hand, such as early and late stage fusion involving concatenation , merging , or sampling from a shared embedding space. We experiment with several of these late fusion techniques on the three modality embeddings after training for alignment."}, {"title": "III. METHODOLOGY", "content": "To approach the problem of traffic accident classification, a multi-stage pipeline was developed. The pipeline, which is represented in Figure 1, can be summarized in four stages and is described in the following sections:\n1) Data pre-processing: Sample video frames, generate captions, and use a scene graph generator to produce a set of scene graphs for each traffic accident example.\n2) Scene graph encoder pre-training: Pre-train the scene graph encoder on the classification task.\n3) Multimodal alignment: Align the scene graph encoder with frozen video and text encoders.\n4) Fine tuning for downstream task: Train a classification head on top of the aligned multimodal model."}, {"title": "A. Data Pre-processing", "content": "In the first stage, for extensibility and ease, we leverage and tune existing tools for generating traffic scene graphs that will later be fed into our modeling approach. Various scene graph generators (SGGs) are available for this, although not many fit our specific requirements for a generator that is capable of identifying and encoding features unique to traffic accidents, such as different types of vehicle collisions. Some do work for our needs in particular, the roadscene2vec (rs2v)\ntool created by Malawade et al. [2021] which we leverage for generating scene graphs from traffic video frames. To use rs2v, we first sample a fixed amount of frames from each video, and text captions are generated manually for each class. Then, feeding in a series of frames from a traffic scene, the rs2v generator uses an object detector and keeps only the relevant entities relating to traffic (such as road, cars, pedestrians, etc.), filtering out other detected objects. Next, after generating a \"bird's eye view\u201d (BEV) projection of the image and approximating the relative location of each object in this projection, edges are connected between nearby entities in the scene, and vehicles are mapped to lanes. This forms the scene graph for a specific frame from a video sample. The SGG is an important component of this pipeline as it defines the elements in the scene graph modality.\nAn important step before employing this tool is to calibrate the BEV and adjust the proximity thresholds (which are used to create edges of varying attributes, such as very near or visible, relating a pair of objects). The authors specifically note the challenges they faced when adjusting these settings with regard to the DoTA dataset, and mention that for best results, they needed to calibrate their model for each traffic scene [Malawade et al., 2022]. In our case, we iteratively sample and adjust the BEV parameters and proximity thresholds based on the output quality of the SGs produced for various scenes in DOTA. We do this to select one configuration to generate the SGs for our task, although it remains an inherent challenge to generalize the parameter settings for the entire dataset. Some examples of the generated SGs can be seen in Appendix A.\nFor text, we manually compose captions describing each of the four accident classes (see Appendix B), pairing each caption with videos from its respective class to form the training examples. We also experiment with two sets of captions, however, we focus the scope of this work primarily on the scene graph generation process from video frames rather than caption generation. It should be noted that although these captions are used during training for aligning the SGE, they are not used during inference on the final model because they are not from our dataset and provide a one-to-one mapping directly to the accident classes. Through fine-tuning the classification head using these captions, the model can achieve 100% accuracy this way. Instead, we fine-tune and test the classification head using the same caption as noted in Appendix B for all examples."}, {"title": "B. Scene Graph Encoder", "content": "In stage two, the SGE encodes scene graphs to fixed-length embeddings. To do this, a multirelational graph convolutional network (MRGCN) as employed by Malawade et al. [2022] is used, which includes an attention mechanism along with LSTMs to model the spatial and temporal relations of the scene graphs generated for a given video. The SGE may be pre-trained for the classification task before aligning the scene graphs with the language and vision modalities."}, {"title": "C. Multimodal Alignment", "content": "For stage three, we use the CLIP model [Radford et al., 2021], but replace the CLIP image encoder with X-CLIP [Ni et al., 2022], a pre-trained video encoder, to accept videos rather than images. We leave CLIP's text encoder in place. We then freeze the weights from the video and text encoders and align them to the SGE encoder, which takes the new \u201cscene graph modality\" as input."}, {"title": "D. Fine Tuning for Downstream Task", "content": "The final stage involves training a classification head that accepts embeddings from the three modalities and outputs an accident class for a traffic scene. This requires the selection of a method to fuse the signal from the three modalities followed by training a small network. For this, we experiment with different late fusion techniques such as taking a weighted linear combination of the modality outputs  and training various MLP classifiers  with and without activations on top of the concatenated em-beddings. Based on these experiments, we choose the approach of concatenating the vectors from the three modalities as described by Sleeman et al. [2022] and, following Wu et al. [2022], train a 2-layer MLP with ReLU activations as our classification head."}, {"title": "IV. EXPERIMENTAL DESIGN", "content": "Given the nature of our multimodal approach, we require data in the form of videos, text, and scene graphs. Regarding the videos, we focus on using the Detection of Traffic Anomaly (DoTA) dataset for this task.\nDoTA is a dataset consisting of 4,677 curated videos of traffic"}, {"title": "B. Baselines and Metrics", "content": "In order to evaluate the usefulness of representing traffic scenes graphically to classify them, we first verify whether a SGE alone is superior to randomly guessing an accident class. We later evaluate the effectiveness of using SGs in a multimodal classifier by comparing the difference in the performance between using just text and video embeddings versus adding in the signal from the SG modality.\nFor this and other results, the primary metrics we focus on are accuracy and balanced accuracy. Accuracy is simply defined as the proportion of the correctly classified predic-tions out of the total number of predictions and remains an interpretable metric that is relatively simple to compare across different benchmarks. There is, however, significant imbalance in the dataset, with the turning class appearing over twice as frequently as the remaining three. Since we want to evenly prioritize classifying each type of accident, we turn to balanced accuracy, which is computed as the (unweighted) average recall over all classes, as a more reliable metric for our use case."}, {"title": "C. Modality Alignment, Pre-training and Hyperparameters", "content": "The core TG model consists of three encoders for three different modalities: text, vision, and graph. For our classi-fication task, a MLP classifier head is placed at the end of the model to output scores for each of the four classes. The text and vision encoders are both kept frozen throughout all of"}, {"title": "D. Implementation Details", "content": "We train the SGE for 200 epochs on the default hyperpa-rameter settings as specified by Yu et al. [2020]. For the SGE pre-training experiments, we select the SGE with the lowest validation loss. We also experiment with the provided SGE pre-trained checkpoint that was trained on synthetic traffic scenes generated by CARLA. For the modality alignment training, we keep most of the default hyperparameter settings, varying batch size (32 to 128) and the number of epochs (8 to 20)."}, {"title": "V. RESULTS AND DISCUSSION", "content": "As seen in Table I, pre-training the SGE on the DoTA dataset allows it to perform the classification task approxi-mately 13 percentage points better than random classification, which would have an expected accuracy of 25%. This suggests that the spatial and temporal information encoded in the graph embeddings produced by the SGE contain useful information towards accident classification. Unsurprisingly, the model pre-trained on the synthetic CARLA dataset does not achieve as strong performance on our task as the model trained on DoTA. This is understandable, given that the former was trained to classify whether or not a traffic maneuver is deemed to be risky [Malawade et al., 2022] rather than to classify traffic accidents. Furthermore, the synthetic traffic scenes in the CARLA dataset used by the authors do not vary as much as those seen in DoTA, and do not contain examples from the four classes used for our task. However, by further training the CARLA model on the DoTA dataset, we are able to achieve similar results to training on DoTA alone (compare two rightmost columns of Table I). For the remaining experiments in the paper, when unspecified, we use the SGE pre-trained only on the DoTA dataset."}, {"title": "B. SGE with Alignment Results", "content": "The primary classification results after training for align-ment can be seen in Table II. In Column 1, we verify the baseline accuracy of the model when the scene graph modality is not used at all. Looking at the balanced accuracy score, it is quite interesting to note that the performance under this setting is very close to that of the aligned model when no"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this work, we have shown that encoding traffic informa-tion in the form of a scene graph is beneficial towards the goal of accident classification. This is made clear by the pre-training encoder results which show the SGE's ability to beat a random classifier at this task. It was further illustrated that the scene graph information can serve to enhance the performance of a vision-language classifier by fusing information from all three modalities. Finally, although we were not able to show an improved score for the classifier after alignment, it should be noted that alignment does not have a negative effect on performance, and we demonstrate that further training and fine-tuning may improve the score beyond the unaligned case.\nThere are several areas where future work can be done in this task. Firstly, although three modalities are used, little exploration has been conducted on the language modality. In principle, captions for each video should be derived from the video itself, as the SGs are. This would allow for the captions to be fed into the model to fine-tune the classifier head and run inference with a greater signal coming from the language modality. Next, although they provide some benefit for the model, the generated SGs have a relatively limited structure: only categorical distance relations between a vehicle and objects in its surroundings are generated, along with mappings to a fixed set of three traffic lanes (left, middle, and right). Enabling a semantic extension of the generated SGs may enhance the signal provided from this modality. Similarly, by modeling relations between all objects in the scene, this pipeline can be expanded for the non-ego case as well, and perhaps can be used on more classes in the DoTA dataset or similar use cases. Additionally, the MRGCN-based architecture of the SGE was taken directly from Yu et al. [2020], but perhaps it can be further improved by modifying either its spatial or temporal modeling components. This work has demonstrated that further fine-tuning and alignment training shows promise to improve results. Experiments with different modality fusion methods as well as classification heads can also lead to improvements."}]}