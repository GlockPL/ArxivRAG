{"title": "Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial Dynamics in the Wild", "authors": ["Xingjian Wang", "Li Chai"], "abstract": "In-the-wild Dynamic facial expression recognition (DFER) encounters a significant challenge in recognizing emotion-related expressions, which are often temporally and spatially diluted by emotion-irrelevant expressions and global context respectively. Most of the prior DFER methods model tightly coupled spatiotemporal representations for direct classification, which may incorporate weakly relevant features such as facial contours and identity-specific characteristics, leading to information redundancy and emotion-irrelevant context bias. Several DFER methods have highlighted the significance of dynamic information for DFER, but utilize explicit manners to extract dynamic features with overly strong prior knowledge. In this paper, we propose a novel Implicit Facial Dynamics Disentanglement framework (IFDD). Through expanding wavelet lifting scheme to fully learnable framework, IFDD disentangles emotion-related dynamic information from emotion-irrelevant global context in an implicit manner, i.e., without exploit operations and external guidance. The disentanglement process of IFDD contains two stages, i.e., Inter-frame Static-dynamic Splitting Module (ISSM) for rough disentanglement estimation and Lifting-based Aggregation-Disentanglement Module (LADM) for further refinement. Specifically, ISSM explores inter-frame correlation to generate content-aware splitting indices on-the-fly. We preliminarily utilize these indices to split frame features into two groups, one with greater global similarity, and the other with more unique dynamic features. Subsequently, LADM first aggregates these two groups of features to obtain fine-grained global context features by an updater, and then disentangles emotion-related facial dynamic features from the global context by a predictor. Extensive experiments on in-the-wild datasets have demonstrated that IFDD outperforms prior supervised DFER methods with higher recognition accuracy and comparable efficiency. Code is available at https://github.com/CyberPegasus/IFDD.", "sections": [{"title": "Introduction", "content": "Dynamic facial expression recognition (DFER) for in-the-wild scenarios holds significant importance for understanding human mental state, and facilitate various relevant applications(Cowen et al. 2021). Despite the impressive progress both in static facial expression recognition (SFER) and laboratory-controlled DFER, in-the-wild DFER remains challenging. As revealed in prior works(Li et al. 2022; Tao et al. 2023), videos of in-the-wild DFER contain limited dynamic expression-related frames along with numerous neutral or noisy frames. That is, the emotion-related frames are temporally diluted by non-expression frames.\nPrevious DFER methods tend to model tightly coupled spatiotemporal representations. They utilize the whole representation for recognition, which may be vulnerable to excessive non-expression frames in non-neutral emotion classification tasks. Specifically, a large part of previous methods(Jiang et al. 2020; Ma, Sun, and Li 2022; Sun et al. 2023) directly apply the entire hidden features or global tokens from 3D convolutional neural network (CNN) or vision transformer (ViT) for DFER. Besides, there are some methods(Zhao and Liu 2021; Wang et al. 2022b; Li et al. 2023) that utilize separable spatiotemporal architecture, i.e., 2D CNN + GRU/LSTM/Transformer, to extract spatial and temporal representation in a cascaded manner. Their spatial representations are tangled with temporal ones in the end. Thus, the above two kinds of methods accord with the mentioned issue. Their expression representation may contain numerous emotion-irrelevant features such as facial shapes and identity-specific characteristics, leading to emotion-irrelevant context bias and information redundancy.\nIn light of this, we endeavor to disentangle emotion-related dynamic features from global context features for more compact and effective representation of dynamic expression.\nRecently, several DFER studies(Li et al. 2022; Tao et al. 2023) have responded to this challenge and focused on the significant dynamic information. NR-DFERNet(Li et al. 2022) directly differential operation between adjacent frames, thereby paying more attention to the key dynamic frames. Freq-HD(Tao et al. 2023) utilizes Discrete Fourier Transform-based (DFT) frequency analysis sliding across frames to strengthen high-dynamics affective clips.\nHowever, their introduced explicit guidance to extract dynamics will be vulnerable to background motion and head movements, especially for in-the-wild complex scenarios. Instead, we distill emotion-related dynamics from higher-level latent features via expanding wavelet lifting scheme (Sweldens 1998) to a more implicit and adaptive framework.\nGiven the aforementioned concerns, we propose Implicit Facial Dynamics Disentanglement (IFDD) compatible with different backbones. IFDD implicitly disentangles emotion-"}, {"title": "Related Work", "content": "Despite significant progress have been made in SFER(Zhao, Liu, and Zhou 2021; Zhang, Wang, and Deng 2021), DFER still remains challenging since it needs to consider inter-frame temporal relationship in addition to the spatial information. Jiang et al. (Jiang et al. 2020) and Wang et al. (Wang et al. 2022a) combine 2D convolution neural network (CNN) for spatial features with recurrent network for temporal dynamics, while NR-DFERNet(Li et al. 2022), DPCNet(Wang et al. 2022b), and IAL(Li et al. 2023) utilize transformer as the substitute for RNN. Meanwhile, Jiang et al. explores several 3D CNN baselines, including C3D(Tran et al. 2015), 3D ResNet-18(Hara et al. 2018), R(2+1)D-18(Tran et al. 2018), I3D(Carreira and Zisserman 2017), and P3D(Qiu, Yao, and Mei 2017). M3DFEL(Wang et al. 2023) equips 3D CNN with BiLSTM to further model the imbalanced temporal relationships. To exploit transformer for 3D representation, STT(Ma, Sun, and Li 2022) uses CNN to embed frames, and then jointly learns 3D representation by spatial and temporal attentions within transformer blocks.\nHowever, their coupled spatiotemporal representations will inevitably incorporate noisy and emotion-irrelevant context information from prevalent non-expression frames of in-the-wild DFER tasks. Therefore, we manage to disentangle emotion-relevant and compact dynamic features from relatively irrelevant global context features.\nRecently, several methods have managed to alleviate the negative impact of numerous non-expression frames in DFER from multiple perspectives, mainly including dynamic representation(Li et al. 2022; Tao et al. 2023) and learning strategy(Wang et al. 2023, 2024). To enhance emotion-relevant dynamic representation, NR-DFERNet(Li et al. 2022) utilizes inter-frame differential operation to introduce frame-level dynamic information, while Freq-HD(Tao et al. 2023) utilizes DFT-based frequency analysis sliding across frames to focus on the key dynamic frames. From the perspective of learning strategy, M3DFEL(Wang et al. 2023) treats this issue as a weakly supervised problem and utilize multi-instance learning to handle inexact labels, while SCIU(Wang et al. 2024) manages to prune low-quality data samples in training process.\nIn this paper, we focus on modeling dynamic representation along with other concerns. Prior arts(Li et al. 2022; Tao et al. 2023) explicitly introduce prior knowledge to enhance dynamic representation at the frame level. Thus, they have limited adaptive capability to complex DFER scenarios. Instead, we rethink this idea and manage to propose an implicit, interpretable and more adaptive framework."}, {"title": "Deep Learning-based Wavelet Lifting Methods", "content": "Extending the scope of wavelet, lifting scheme(Sweldens 1998) has shown its promising performance in vision applications. Given the input 1D signal x, the lifting scheme extracts its frequency sub-bands as detailed and approximate coefficients by three steps: (1) Split signal in two groups including odd samples $x_o$ and even samples $x_e$. (2) Utilize a predictor P to compute an estimation for odd samples $o$ based on even samples, and subtract the prediction from odd samples to obtain high-frequency sub-band h. (3) Utilize an updater U to recalibrate even samples with predicted high-frequency components to obtain low-frequency sub-band g.\nSince lifting scheme allows for flexible customized filters, there emerges various studies on deep learning-based"}, {"title": "Implicit Facial Dynamics Disentanglement", "content": "The overall architecture of IFDD is shown in Fig. 1. Given an input video clip $F \\in R^{T_o\\times H_o\\times W_o\\times3}$ of $T_o$ RGB frames with height $H_o$ and width $W_o$, IFDD first extract multiscale features from F by the backbone. Then pyramid aggregation utilizes dilated convolutions to compress multiscale features into the same spatial size, and concatenate them followed by convolution layers for aggregation, resulting in single-scale latent features $X \\in R^{T\\times H \\times W \\times C}$. Subsequently, the proposed ISSM and LADM jointly conduct two-stage disentanglement of emotion-related dynamics on X, which will be illustrated in detail in the following subsections."}, {"title": "Inter-Frame Static-Dynamic Splitting", "content": "Fixed even-odd splitting strategy is commonly used in prior methods (Bastidas Rodriguez et al. 2020; Huang and Fang 2021; Deng, Gao, and Xu 2023), regarding even and odd coefficients as the preliminary estimation for low-frequency and high-frequency information respectively(Sweldens 1998). However, this is excessively rough for DFER tasks, since emotion-related expressions are non-uniformly diluted and disrupted by irrelevant expressions.\nWe propose Inter-frame Static-dynamic Splitting Module (ISSM) to provide a better preliminary estimation and facilitate subsequent refinement process in LADM. ISSM generates content-aware splitting index adaptive to X, and use these indices to split X into global context group and dynamic group. The splitting indices are learned by temporal correlation to avoid being exposed to the whole spatiotemporal features with excessive emotion-irrelevant global content. In this way, frame features in global context group exhibit higher spatial similarity to other frames across the time and thus are relatively static, while the ones of dynamic group possess unique dynamics that may indicate emotion-related expressions and thus deserve special attention.\nGiven spatiotemporal latent features $X \\in R^{T\\times H\\times W\\times C}$, ISSM first embeds X into temporal tokens $Z \\in R^{T\\times d_T}$ with temporal dimension $d_T$ as follows,\n$Z = LN \\left( Vec_{dim=2,3,4}(Conv(PS_{\\downarrow2}(X))) \\cdot W_T + b\\right),$\nwhere \u00b7 denotes matrix multiplication in this paper, $PS_{\\downarrow2}$ is average pooling operation for spatial compression with a downsampling factor 2, and $Vec$ denotes vectorization operator to flatten a tensor for specific dimensions dim. $Conv(.)$ denotes convolution layer with kernel = 3 to compress the channel dimension by a factor of $\\frac{1}{4}$, while $LN(\u00b7)$ represents Layer Normalization operation. $W_T\\in R^{x d_T}$ and $b_T \\in R^{T\\times d_T}$ denotes learnable embedding parameters.\nThen ISSM learns two sets of splitting indices based on the temporal correlation of Z. These indices are indirectly generated as the summation of initial indices and learnable offsets to provide stable initial status, where even-odd splitting indices are applied as initial indices. Besides, to be compatible with different T, the offsets are learned in a normalized form, namely offset scales \u2208 (-1, 1). The offset scale $A_S\\in R$ for relatively static group and $A_D \\in R$ for dynamic group are learned as follows,\n$\\begin{aligned}\nA_S &= Tanh\\left( Vec_{dim=1,2}(\\sigma(\\frac{Z\\cdot(Z)^T}{\\sqrt{d_T}})\\cdot W_S+b_S) \\right),\\\\\nA_D &= Tanh\\left( Vec_{dim=1,2}(\\sigma(\\frac{Z\\cdot(Z)^T}{\\sqrt{d_T}})\\cdot W_D + b_D)\\right),\n\\end{aligned}$\nwhere \u03c3 denotes softmax function. $W_S \\in R^{T^2\\times \\frac{T}{2}}, W_D \\in R^{T^2\\times \\frac{T}{2}}$, $b_S \\in R^{\\frac{T}{2}}$, and $b_D \\in R^{\\frac{T}{2}}$ are all learnable embedding parameters. Then the index vectors $I_S \\in R^{\\frac{T}{2}}$ and $I_D \\in R^{\\frac{T}{2}}$ can be denoted as,\n$\\left\\{\n\\begin{array}{l}\nI_S[i] = 2i + A_SL \\\nI_D[i] = 2i + 1 + A_DL\n\\end{array}\n\\right.  for i = 0, 1, 2, ..., \\frac{T}{2} -1,$\nwhere hyperparameter L presents the allowable range of offsets. Then we limit the range of $I_S$ and $I_D$ within [0, T \u2013 1].\nBased on obtained $I_S$ and $I_D$, frame features X are divided and interpolated into global context features $X_S \\in R^{\\frac{T}{2}\\times H\\times W\\times C}$ and dynamic features $X_D \\in R^{\\frac{T}{2}\\times H\\times W\\times C}$,\n$\\left\\{\n\\begin{array}{l}\nX_S[i] = (I_S[i] - \\lfloor I_S[i] \\rfloor) \\cdot X[\\lfloor I_S[i] \\rfloor] + (\\lceil I_S[i] \\rceil - I_S[i]) \\cdot X[\\lceil I_S[i] \\rceil] \\\\\nX_D[i] = (I_D[i] - \\lfloor I_D[i] \\rfloor) \\cdot X[\\lfloor I_D[i] \\rfloor] + (\\lceil I_D[i] \\rceil - I_D[i]) \\cdot X[\\lceil I_D[i] \\rceil]\n\\end{array}\n\\right.$,\nwhere i\u2208 0, 1, 2, ..., $\\frac{T}{2}$ \u2212 1. $\\lfloor \u00b7 \\rfloor$ and $\\lceil \u00b7 \\rceil$ denotes floor / ceil function that rounds down / up splitting indices respectively."}, {"title": "Lifting-based Aggregation and Disentanglement", "content": "Based on preliminary estimations $X_S$ and $X_D$, we propose Lifting-based Aggregation-Decoupling Module (LADM) to further disentanglement emotion-related dynamic features from global context via lifting scheme. We additionally exploit mutual relation of $X_S$ and $X_D$ for better disentanglement, differing from vanilla lifting implementation.\nLADM first utilizes the updater U to aggregate $X_S$ and $X_D$ to obtain refined global context estimation $Y_S$. The emotion-irrelevant global context remaining in preliminary estimated $X_D$ is further squeezed out by the updater U and be absorbed into $Y_S$. Then, $Y_S$ is exploited to extract irrelevant context features conditioned on $X_D$ from itself and strip them from prior dynamic estimation $X_D$, so as to purify the emotion-related dynamics $Y_D$ by the predictor P.\n$\\left\\{\n\\begin{array}{l}\nY_S = Vec_{dim=1,2,3} X_S+U(X_D|X_S)\\\\\nY_D = Vec_{dim=1,2,3} X_D-P(Y_S|X_D)\n\\end{array}\n\\right.$"}, {"title": "Decoupling Loss", "content": "For final expression recognition, dynamic features $Y_D$ is flattened and projected to a classification vector $k\\in R^{N_C}$ by MLP, where $N_C$ denotes the number of emotion categories. Disentanglement loss L is applied to k and $Y_S$ simultaneously, which is composed of task-specific loss $L_{Task}$ and global context loss $L_{Lift}$. We adopt cross entropy loss as $L_{Task}$ for DFER tasks, and thus rewrite $L_{Task}$ as $L_{CLS}$.\n$L = L_{CLS} + L_{Lift}.$\nInspired by (Bastidas Rodriguez et al. 2020), $L_{Lift}$ imposes a restraint on $Y_S$ to have the same local average with X to force $Y_S$ incorporate all the global context information. Local average is calculated along spatial dimension for {T, C'} dimensions of X, since global context features should maintain spatial invariance across time. Huber loss is adopted for $L_{Lift}$. Suppose there are N video clips for one batch training and the label of k-th clip is $y_k$, then the"}, {"title": "Experiments", "content": "Datasets. We conduct evaluation on three important in-the-wild DFER datasets including DFEW(Jiang et al. 2020) with 16,372 videos, FERV39k(Wang et al. 2022a) with 38,935 videos, and MAFW(Liu et al. 2022) with 10,045 videos. Instead of lab-controlled videos, they collect videos from movies, TV dramas or other media sources. They are challenging due to that limited frames relevant to labeled emotion are temporally diluted in a complex video scenario with occlusions and pose changes. The category number of emotions is 7 for DFEW / FERV39k and 11 for MAFW, which all include neural emotion. Note that three datasets all have long-tailed distribution issue for different emotions. We follow the settings of these datasets for evaluation. Specifically, DFEW and MAFW both provide 5-fold cross-validation settings, while FERV39k provides a train-test splitting setting.\nMetrics. Consistent with prior researches(Wang et al. 2023; Sun et al. 2023; Li et al. 2023), the unweighted average recall (UAR) and weighted average recall (WAR) as metrics. UAR is equivalent to the average accuracy of all expression categories ignoring the number of samples per class, while WAR represents the overall average accuracy of all samples. We report the average metrics over three runs for FERV39k. As for DFER and MAFW with 5-fold cross-validation, we report metrics from a single run.\nIFDD is implemented by PyTorch and trained on NVIDIA RTX 3090 for 100 epochs. We utilize AdamW optimizer and cosine scheduler with le-4 initial learning rate and le-3 weight decay, where the former 10 epochs adopt warm-up strategy with le-6 learning rate. Training sets from aforementioned datasets are further divided into training and validation set at a ratio of 4:1.\nFixed number $T_o$ of frames are uniformly sampled from videos and resized into the size of $H_o \u00d7 W_o$ as clips for training and inference. {$T_o$, $H_o$, $W_o$} are set to {16,224,224} for DFEW and FERV39k, and {32,224,224} for MAFW. Data augmentation methods including random horizontal flip and random crop are adopted.\nWe explore two backbone types for IFDD, i.e., IFDD-2DCNN and IFDD-3DViT. MobileNetV2 is adopted for IFDD-2DCNN, and MViT-S is adopted"}, {"title": "Conclusion", "content": "In this paper, to focus on emotion-related expression dynamics that are temporally diluted by irrelevant non-expression context in in-the-wild DFER, we propose IFDD framework to disentangle emotion-related dynamic features from global context in an implicit manner. IFDD expands the framework of lifting scheme, and proposes ISSM and LADM for two-stage disentanglement. Extensive experiments on in-the-wild DFER datasets demonstrate the superiority of IFDD over state-of-the-art supervised methods, as well as its compatibility with different backbones. Future directions include exploring IFDD framework for other dynamic-sensitive tasks, such as dynamic micro-expression recognition, optical flow estimation, and video compression."}, {"title": "A. Additional Implementation Details", "content": "We intuitively represent the algorithm of IFDD via pseudocode, as shown in Algorithm 1.\nHere we illustrate the detailed process to extract multiscale features for the backbone of IFDD. Recall that we have explored two types of networks for the backbone, namely MobileNetV2 and MViT-S, and thus obtain IFDD family including IFDD-2DCNN and IFDD-3DViT respectively.\nGiven video clip $F \\in R^{T_o\\times H_o\\times W\\times3}$, the backbone first adopts an input convolution layer to downsample the temporal dimension by a factor of 2 and the spatial dimension by a factor of 4. Recall that the spatial size $H_o \u00d7 W_o$ of input clips is 224 \u00d7 224 for all datasets, after this downsampling, the spatial size of features for latter layers is 56. Then the backbone extracts multiscale features from the clip, i.e., a multiscale pyramid of features with S stages. Note that the temporal sizes of features at different stages are the same, and the spatial size are different. In this way, the features at j-th stage are defined as $F_j\\in R^{T_o\\times \\frac{H_o}{2^{j/1}} \\times \\frac{W_o}{2^{j/1}} \\times C_j}$, where j\u2208 {1, 2, .., S} and $C_j$ denotes the channel dimension.\nFor IFDD-2DCNN, only the former 5 bottlenecks of MobileNetV2 are utilized. The 3-stage features with the spatial widths {56, 28, 14} are extracted from 3-th, 4-th, and 5-th bottlenecks respectively. The spatial sizes of extracted multiscale features by IFDD-3DViT are the same with IFDD-2DCNN. Only the former 3 stages of MViT-S are used for the feature extraction. 3-stage features with the spatial widths {56, 28, 14} are extracted from 1-th, 2-th, and 3-th stages of MViT-S respectively. We remove 6 blocks in 3-th stage of MViT-S for further efficiency. Since we leverage comprehensive features of MViT-S for fine-grained disentanglement, we also remove the class token of MViT-S.\nThe subsequent pyramid aggregation consists of two steps, namely compression and aggregation. The compression progress applies dilated 3 \u00d7 3 convolution layers with stride = S \u2212 j + 1 to features Fj of different stage j respectively, resulting in compressed features of the same spatiotemporal size. Then aggregation progress concatenated these features along channel dimension, and compress them at channel dimension by a 1 \u00d7 1 convolution layer, resulting in $X\\in R^{T\\times \\frac{H_o}{2^{S+1}} \\times \\frac{W_o}{2^{S+1}} \\times C_S}$. S is set to 3 in this paper."}, {"title": "B. Additional Information for Ablation Study", "content": "We evaluate two extra learnable splitting methods of different settings to investigate the effectiveness of the learning dependency and the splitting manner of ISSM, namely ISSM+ and ISSM\u2021 as in Table 8.\nHere we illustrate their details, as shown in Figure 3. Suppose the temporal size of input feature X is T, ISSM\u2020 utilize self-attention(Vaswani et al. 2017) across the whole spatiotemporal dimensions to learn the splitting indices. Specifically, a global token with the size 1 \u00d7 C is concatenated with X to get $X' \\in R^{(1+THW)\u00d7C}$. After self-attention, the global token is separated and compressed by an MLP layer to obtain two index vectors of the length T/2.\nIn contrast, ISSM\u2021 follows ISSM to learn the latent splitting features based on temporal correlation. Differing from ISSM, ISSM\u2021 then projects the latent features to two T \u00d7T matrices, and use these matrices to weight X at temporal dimension, resulting in two index vectors.\nOther settings of ISSM\u2020 and ISSM\u2021 follow ISSM."}, {"title": "C. Additional Experiments for Ablation Study", "content": "Recalling that the range of learned offset scales $A_S$ and $A_D$ is (-1, 1), the actual offsets are obtained by multiplying $A_S$ and $A_D$ with a restraint ratio L. Then the content-aware splitting indices $I_S$ and $I_D$ are generated as the element-wise summation of offsets and initial indices. We evaluate the impacts of different restraints L on the offset range and different initial indices.  presents two observations. Firstly, larger allowable range performs better than the smaller one with a 0.79% improvement in WAR, which indicates that more flexibility for index sampling benefits the disentanglement process. Secondly, on-the-fly generation for splitting indices starting from even-odd indices outperforms the one starting from the temporal midpoint. That is, initial status from even-odd splitting indices provide larger temporal receptive field than the one from temporal midpoint."}, {"title": "D. Additional Information for Visualization", "content": "Here we provide supplementary information for Figure 2 in the main paper, especially implementation details."}, {"title": "Gradient Attention of {YD, Ys}", "content": "For Figure 2(a), gradients from target emotion and features are extracted after LayerNorm Layer in LADM module. Average pooling operation for spatial dimensions is conducted on gradients to get channel-wise weighted scores for features. In this way, the attention maps of emotion-related dynamic features $Y_D$ and global context features $Y_S$ are visualized along temporal dimension. The temporal size of $Y_D$ and $Y_S$ is $\\frac{T}{2}$ in LADM, and the original clips with temporal size = 16 are downsampled in temporal dimension by stride = 4 for visualization. Figure 2(a) has demonstrated that IFDD can learn to disentangle dynamic features by focusing on emotion-related regions, especially decoupling in an implicit way without the guidance of external sources such as facial region labels or landmarks."}, {"title": "Distribution of Classification Features", "content": "As for Figure 2(b), we extract 768-dimension feature vectors before the last linear layer, and visualize them in a 2-D space by t-SNE. The perplexity of t-SNE is set to 50, while the maximum number of iterations is set to 1000. Figure 2(b) has shown that the disentangled facial dynamic features by IFDD have stronger correlations with target emotion-related expression. Notably, the stepwise participation of ISSM and LADM progressively widens the distribution gaps between high-level features of different emotions, which proves that ISSM and LADM both contribute to the discrimination capacity of the model."}, {"title": "E. Detailed Comparison with Baselines", "content": "To further demonstrate the effectiveness of IFDD, we conduct quantitative and qualitative analysis for comprehensive comparison with CNN and ViT baselines. Here we briefly introduce the prediction heads of these two baselines. The prediction head for 2D-CNN baseline (MobileNetV2) concatenates features across temporal dimension and then directly flatten features into 1-D vector, followed by an MLP layer for final classification. Similarly, the prediction head for 3D-ViT baseline (MViT) also uses an MLP layer to compress the class token of MViT for final classification."}, {"title": "F. 5-Fold Cross-Validation Results", "content": "Recall that we report the average UAR and WAR values of 5-fold results for DFEW and MAFW datasets as the final results in the main paper, following the experimental settings of DFEW and MAFW. As a supplement, here we provide the corresponding 5-fold cross-validation results, as shown in Table 13. UAR and WAR metrics for DFEW and MAFW datasets are evaluated following 5-fold cross-validation settings provided by these two datasets."}, {"title": "G. Additional Discussions", "content": "As stated in the main manuscript, we do not compare with self-supervised methods for fair comparison. Here we further discuss the performance difference between IFDD and state-of-the-art self-supervised DFER methods including MAE-DFER and DFER-CLIP.\nAs shown in Table 14, IFDD-3DViT outperforms DFER-CLIP, but is surpassed by MAE-DFER on DFEW dataset."}]}