{"title": "Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack", "authors": ["Xiaoyue Xu", "Qinyuan Ye", "Xiang Ren"], "abstract": "We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn from a sequence of language tasks through in-context learning (ICL). We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL. When given a task instruction and test inputs, long-context LMs are expected to leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than the Single-task ICL baseline.\nTask Haystack draws inspiration from the widely-adopted \u201cneedle-in-a-haystack\" (NIAH) evaluation, but presents new and unique challenges. It demands that models (1) utilize the contexts with deeper understanding, rather than resorting to simple copying and pasting; (2) navigate through long streams of evolving topics and tasks, which closely approximates the complexities of real-world usage of long-context LMs. Additionally, Task Haystack inherits the controllability aspect of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.\nWe benchmark 12 long-context LMs using Task Haystack. We find that state-of-the-art closed models such as GPT-40 still struggle in this setting, failing 15% of the cases on average, while all open-weight models we evaluate further lack behind by a large margin, failing up to 61% of the cases. In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases. Further, we observe declines in performance when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of current long-context LMs. We release our code and data to encourage future research that investigates and addresses these limitations.", "sections": [{"title": "1 Introduction", "content": "Recent advances in model architecture [Han et al., 2024, Su et al., 2024], hardware-aware optimization [Dao et al., 2022, Liu et al., 2023], training procedure [Tworkowski et al., 2023, Liu et al., 2024a], and data engineering [Fu et al., 2024, An et al., 2024] have enabled large language models (LLMs)"}, {"title": "2 Related Work", "content": "Long-Context LM Benchmarks. Existing benchmarks for evaluating long-context models can be categorized into realistic and synthetic ones. Realistic benchmarks, exemplified by (Zero)SCROLLS [Shaham et al., 2022, 2023], comprise tasks that require processing long inputs with long-range dependencies. These tasks are typically sourced from established datasets and include various task types such as summarization and question answering. In the category of synthetic benchmarks, the needle-in-a-haystack (NIAH) [Kamradt, 2023] evaluation is widely adopted for evaluating context utilization [Gemini Team Google, 2024, Anthropic, 2024, Liu et al., 2024a, Fu et al., 2024]. Ruler [Hsieh et al., 2024] expands on the NIAH test with multi-key and multi-value retrieval, and adds two new tasks that involve multi-hop tracing and aggregation. Hybrid benchmarks is a middle-field that incorporate both realistic and synthetic elements. An example is LongBench [Bai et al., 2023], which includes synthetic tasks based on realistic text, such as counting unique passages appearing in the context. Our proposed Task Haystack can be considered as a hybrid benchmark, with a realistic touch as it closely mirrors the real-world challenge of navigating through evolving topics and tasks.\nEvaluating Long-Context LMs with Many-Shot ICL. Several recent works have explored in-context learning with long-context LMs by scaling the number of training examples (i.e., shots). Bertsch et al. [2024] conducted a systematic study of long-context ICL with up to 2000 shots, demonstrating many-shot ICL as a competitive alternative to retrieval-based ICL and fine-tuning. Additionally, it offers the advantage of caching demonstrations at inference time, unlike instance-level retrieval methods. While Bertsch et al. [2024] focus on classification tasks, Agarwal et al. [2024] showed the effectiveness of many-shot ICL on generative and reasoning tasks, and established new state-of-the-art results on practical applications such as low-resource translation with the Gemini 1.5 Pro model. However, there are still limitations to many-shot ICL. Li et al. [2024] introduce LongICLBench, a suite of 6 classification tasks with many (20+) classes, and find that current long-context LMs still struggle with these tasks. Orthogonal to this line of work on scaling number of examples for one single task, we focus on scaling the number of tasks in our Lifelong ICL setting.\nLifelong Learning in NLP. Lifelong learning, or continual learning, refers to the problem setting where a model learns continuously from data streams [Biesialska et al., 2020, Shi et al., 2024]. Lifelong ICL is largely inspired by this line of work and challenges long-context models to learn continuously from a sequence of language tasks. However, unlike prior works that use gradient-based fine-tuning [de Masson d'Autume et al., 2019, Jin et al., 2021, Scialom et al., 2022, Mehta et al., 2023], Lifelong ICL is a new exploration that uses in-context learning as the underlying \"learning\" algorithm. It also stands out from Coda-Forno et al. [2023] and Ye et al. [2024] by focusing on evaluating long-context LMs and scaling the input length from 4k to up to 32k tokens. A primary challenge in lifelong learning is catastrophic forgetting, the tendency of a model to forget previously acquired tasks upon learning new tasks [Kirkpatrick et al., 2017]. Our proposed Task Haystack evaluation focuses an analogous phenomenon, as the model may struggle to recall earlier information in a lengthy context, leading to a performance decline."}, {"title": "3 Problem Setting", "content": "In the following, we establish the notations and the problem setting of Lifelong ICL in \u00a73.1. We will begin by defining notations of in-context learning (ICL) of one single task T. We will then build upon these foundations and introduce lifelong ICL with a collection of tasks T. In \u00a73.2, we further introduce our Task Haystack evaluation protocol, provide the definition of pass rate - the key evaluation metric, and describe our strategies to account for the instabilities in ICL experiments."}, {"title": "3.1 Lifelong ICL", "content": "In-context Learning. In-context learning is a method that adapts LMs to perform a language task by providing prompts containing input-output pairs [Brown et al., 2020]. In this paper, we define a language task T as a tuple of (Dtrain, Dtest, d), where Dtrain is the training set, Dtest is the test set, d is a textual task description (i.e., instruction). We first create a task-specific prompt p by concatenating the task description and the k-shot examples in Dtrain, i.e., p = dxtrain ytrain...xtrain ytrain. Then, to make a prediction on the test input xtest, we concatenate the task-specific prompt and the test input (i.e., p\u2295 xtest), and query the language model LM to generate the prediction \u0177. We denote this process as \u0177 = LM(xtest|p) to highlight that the prediction is made by conditioning on the task-specific prompt p.\nTask Collection and Task Permutation. The definition above introduces how ICL is performed with one single task T. In Lifelong ICL, an LM is expected to learn from a collection of n tasks, denoted as T = {T}=1. To enable this, we first create a random permutation \u03b1 = (\u03b11, \u03b12,..., \u03b1n), thus the tasks in T will be ordered as (Ta1, Ta2,..., Tan). For example, when n = 3, one possible permuatation \u03b1 is (3, 1, 2), so that the tasks are ordered as (T3, T1, T2).\nLifelong ICL. Given a permutation \u03b1, we first create the task-specific prompt pa\u2081 for each task Ta\u2081, and then create the lifelong ICL prompt p\u03b9 by concatenating all task-specific prompts, i.e., p\u03b9 = Pa\u2081 \u2295Pa2\u2295 . . . \u2295Pan \u00b7 At test time, for each task Ta\u2081 in T, the model will be queried to perform generate the prediction as \u0177 = LM(Xtest|Pl\u2295 da\u2081). Note that we append the task description da\u2081 after the Lifelong ICL prompt p\u03b9 at test time, to ensure the model is informed of the task at hand."}, {"title": "3.2 Task Haystack", "content": "Evaluation Principle. For a test task Ta\u2081, we anticipate that long-context LMs can effectively utilize the in-context examples of that task, i.e., pa,, which is a substring of the lifelong ICL prompt pida. To evaluate this, we compare the model performance on task Ta, when conditioning on pida; and pa\u2081, and expect the former to be not significantly worse than the latter. In other words, the Single-task ICL prompt pa\u2081 is the \u201cneedle\u201d in the Lifelong ICL prompt p\u03b9 (i.e., the \u201ctask haystack\u201d).\nAddressing ICL Instability with Multiple Runs. One challenge in Task Haystack evaluation is the notorious instability of ICL. To account for this, our experiments will be carried out with 5 random samples of the permutation \u03b1 and 5 randomly-sampled few-shot training set Dtrain for each task. This allows us to obtain a performance matrix of size (t, p, r) for lifelong ICL, where t is the task index, p is the permutation index, and r is the few-shot sample index. We will also obtain a matrix of size (t, r) for the Single-task ICL baseline.\nEvaluation Metrics. For an overall measurement, we introduce an overall pass rate. For each permutation \u03b1 and each task Ta\u2081, we will get two groups of performance metrics, when using sing-task ICL and lifelong ICL respectively. Each group contains 5 metrics, corresponding to the 5 randomly-sampled few-shot training set Dtrain. The model passes the test (i.e., scores 1) when the the lifelong ICL group is not significantly worse than the single-task ICL group, captured by a two-sided t-test, and the model scores 0 otherwise. The overall pass rate will be computed by averaging over the different permutations and tasks."}, {"title": "4 Experiment Details", "content": "Task Selection. While the problem setting in \u00a73 is generic and admits any language task, in this work we instantiate the setting with a narrower task distribution for initial exploration. Our key considerations include:\n\u2022 We focus on classification tasks, as they allow standardized evaluation. Additionally, a large body of past work investigates ICL empirically or mechanistically using classification tasks [Chang and Jia, 2023, Wang et al., 2023, Chang et al., 2024].\n\u2022 We select classification tasks with fewer than 20 categories and input text shorter than 1000 tokens, to avoid excessively long single-task prompts that dominate the whole context window [Li et al., 2024].\n\u2022 We focus on English tasks, since most long-context LMs are not optimized for multilingual usage.\nAfter careful manual selection, we obtain a collection of 64 classification tasks, covering a wide range of domains and label spaces. We provide a snippet of 16 tasks in Table 1 and provide detailed descriptions of all 64 tasks, including their references and license information, in Table 7."}, {"title": "5 Results and Analysis", "content": "5.1 Main Results\nLong-context LMs struggle in Task Haystack. We present the aggregated results (mean accuracy and overall pass rate) of the Scale-Shot setting in Table 2 and the results of the Scale-Task setting in Table 6. The overall pass rates fall below 90% in 42 out of 46 cases reported in Table 2 and in 37 out of 40 cases in Table 6. When scaling to 32k context with 8 shots across 16 tasks, 8 out of the 9 open-weight models achieve pass rates lower than 60%, suggesting that these models are still far from fully utilizing and flexibly conditioning on the provided context. In the most extreme case, Yi-6B (200k) achieves a pass rate of merely 38.8% in the 8-shot (32k) setting."}, {"title": "5.2 Controlled Analysis on Long-Context Utilization", "content": "Previously, we find that long-context LMs struggle in the Task Haystack evaluation. In the following section, we investigate the reasons attributing to their failures with various controlled settings.\nWe hypothesize that the model failure at Lifelong ICL may be associated with the following factors: (a) Recency Bias: the model mainly relies on recent context and performs worse when the relevant context is distant; (b) Distraction: the model may be confused by irrelevant context; (c) Long-context Inputs: the model tend to break in general when the input text is long.\nBased on these hypotheses, we introduce controlled settings, such as replaying the test task at the end of the Lifelong ICL prompt, or repeating the Single-task ICL prompt multiple times. Additionally, we use paraphrases of the instructions to investigate the model's sensitivity. We summarize these controlled settings in Table 3 and conduct experiments in the 16-task 4-shot setting with Mistral-7B (32k) and FILM-7B (32k). We present the results in Fig. 4 and discuss our findings below.\n(a) Recency Bias. We investigate the effect of recency bias by comparing the results of Recall and Replay. By replaying ICL demonstrations immediately before testing, model's average accuracy improve by 1.6% for Mistral-7B and 2.9% for FILM-7B. Replay can be also considered as an oracle that approximates potential mitigating strategies such as prompting the model to recall relevant information [Shi et al., 2023, Anthropic, 2024]. However, the improvements only close about half the gap between Baseline and Recall, suggesting that recency bias contributes to but does not fully explain the performance gap.\n(b) Distraction. We examine the effect of irrelevant context, by contrasting Baseline with Random. The results indicate that prepending an irrelevant long text will influence the performance negatively, which corroborates with recent work investigating the robustness of large language models [Levy et al., 2024]. Further, Replay can be seen as prepending a long prefix of mostly irrelevant tasks before performing Single-task ICL (Baseline), and thus the gap between Replay and Baseline may be interpreted as caused by prepending irrelevant contexts.\n(c) Long-context Input. We further compare Baseline, Random, Repeat settings altogether, where Random introduces irrelevant context and Repeat includes only relevant context. Perhaps surprisingly, performance drops in the Repeat setting (-1.3% for Mistral-7B and -3.1% for FILM-7B), where both distractions and recency biases are absent. This observation raises concerns on whether longer inputs are more likely to trigger failure modes and give rise to undesired behaviors in general. While more evidence is needed to derive a conclusion, we suggest that long-context LM users be cautious about including everything in the context window, and recommend using external filtering mechanisms like retrieval augmentation given these undesired behaviors.\nDependency on Task Instructions and ICL Demonstrations. In the Remove setting, we remove the task instruction and the ICL examples of the test task from the Lifelong ICL prompt, to investigate whether the models are relying on such information. We observe a clear performance drop in the Remove setting (-3.3% for Mistral-7B and -4.1% for FILM-7B compared to Recall), suggesting that the models are able to locate and make use of the \"needle\" to some extent in the Recall setting, but not doing it precisely so that the performance can match with the Single-task ICL baseline.\nThe Paraphrase setting further allows us to explore how models make use of task instructions. We observe a decline in performance in the Paraphrase setting compared to Recall. This confirms that the models locate the \"needle\" by retrieving identical instructions in the context. However, the performance gap indicates that models mainly rely on pattern matching rather than deeper understanding of the instructions, which might limit their broader utility in practical applications.\nRepeated ICL as \u201cMulti-epoch", "epochs\\\". One direct takeaway is that repeating the ICL examples multiple times can potentially improve performance, which may have practical utilities in certain low-data high-inference-budget regimes. However, model performance starts to degrade after repeating more than 8 times. This phenomenon can be interpreted in two ways: (1) It is a known issue that repetition may lead to model degeneration [Nasr et al., 2023]; Repeat+Shuffle can possibly alleviate this issue by introducing slight variations in each repeat, which explains that in general Repeat+Shuffle outperforms Repeat. (2) It is also possible that the model \u201coverfits": "o the few-shot training data after multiple \u201cepochs\u201d, analogous to the common observations in gradient-based fine-tuning. We invite future work to further investigate the working mechanism of ICL in this \u201cmulti-epoch"}, {"title": "5.3 Additional Observations and Analysis", "content": "Tasked learned via ICL are more easily influenced. While examining Task Haystack results, we find that the passing and failing behaviors are highly task-specific. For example, in Fig. 22, Mistral-7B (32k) fails on news_data and insincere_questions in all permutations, meanwhile passes on more popular tasks like boolq and yahoo_answer_topics. We hypothesize that models may have memorized some of the tasks during pre-training or post-training, making these tasks less subjective to performance drop in Lifelong ICL. Alternatively, a task may be too challenging for the model to learn through ICL, and thus it passes the test by maintaining low performance in both Single-task ICL and Lifelong ICL."}, {"title": "6 Discussion", "content": "Intended Use. We anticipate Lifelong ICL and Task Haystack to be used for evaluating and diagnosing newly released long-context LMs. However, as our findings in Sections 5.1 and 5.3 suggest, the ICL accuracy and pass rate might be affected if the model has been trained on the tasks used in our evaluation. To ensure responsible use, we encourage users to (1) investigate and report any potential data contamination; (2) report pass rates on ICL-effective/ineffective groups respectively, as done in \u00a75.3. Additionally, it is possible to use targeted data engineering to improve pass rates on Task Haystack. For fair comparisons, we recommend that users disclose whether their training data contains sequences in a format similar to Task Haystack evaluation.\nLimitations. (1) As an initial exploration in the Lifelong ICL setting, we primarily focuses on English-only text classification tasks. This potentially limits a comprehensive assessment of model capabilities across various challenges. To get a more complete picture, the evaluation suite may be improved by involving a wider variety of tasks categories (e.g., question answering, conditional generation), modalities (e.g., vision, speech), and languages. We encourage future research to build upon our foundation and explore these more comprehensive settings. (2) This work simplifies the lifelong learning stream by assuming a sequential order, clear task boundaries, and a fixed number of examples per class for each task. Real-world scenarios likely involve a more dynamic learning stream, without clear task boundaries or assumptions on the sequential order. Future work may explore more realistic training streams that reflect this complexity. (3) Finally, due to computational constraints, our evaluation utilizes 5 random permutations of tasks order and 5 different random samples of few-shot training sets. Experimenting with a larger number of samples could potentially reduce the randomness inherent in the results and increase the reliability of the findings."}, {"title": "7 Conclusion", "content": "In this paper, we introduced Lifelong ICL, a novel problem setting for long-context language models, and developed Task Haystack, a concrete evaluation suite focusing on evaluating and diagnosing long-context LMs in the Lifelong ICL setting. Our experiments with 12 long-context LMs revealed that while these models excel at needle-in-a-haystack style evaluation, their ability to utilize the context flexibly and contextually remains limited. Through our controlled analysis, we dissected and quantified factors such as recency biases and distractions that contribute to performance drops. We also identified performance degradation when repeating ICL examples or using paraphrased instructions, highlighting a fundamental vulnerability in current long-context models.\nOur results demonstrate that Task Haystack poses significant challenges for current top-ranked long-context models. We hope that Lifelong ICL and Task Haystack will serve as valuable tools for diagnosing and advancing the development of future long-context LMs. Additionally, we consider our work as an exploratory step towards backprop-free algorithms in lifelong learning settings."}, {"title": "A Experiment Details", "content": "A.1 Models\nWe list the details of open-weighted models evaluated in Table 5. For closed models from OpenAI, the specific model versions we evaluated are gpt-3.5-turbo-0125 and gpt-40-2024-05-13.\nA.2 Tasks\nWe use publicly available datasets in Task Haystack. We provide their references and huggingface identifiers in Table 7. For further use, readers should refer to the licenses of the original datasets.\nA.3 Problem Setting Details\nDefinition of \"Pass Rate\". In \u00a73.2 we introduced \"Pass Rate\" as the core evaluation metric in Task Haystack. Here we further explain its definition and our considerations when desigining this metric. As illustrated in Fig. 6, we first obtain two groups of 5 different performance metrics, one group using Lifelong ICL prompts, and one group using Single-task ICL prompts; we then use two-sided t-test to examine whether the two groups are significantly different. More specifically, we use scipy.stats.ttest_rel that returns the t-statistic and p-value for the test, and we consider tests with p < 0.05 as significant differences. Additionally, we choose two-sided test to account for potential positive transfers that may arise in the Lifelong ICL setting (\u00a75.3).\nA.4 Implementation and Engineering Details\nData Preprocessing. For each task, the authors manually wrote two task instructions and a task template for in-context learning. In the following we provide one example for the task of ag_news. We ensure that all options have distinct starting tokens when writing the task template, so that the inference can be done with rank classification [Liu et al., 2022a]."}, {"title": "B Additional Results", "content": "B.1 Scale-Task Experiments\nIn Table 6 we report the results in the Scale-Task setting, where we fix the number of shots per class nshot to be 2, and experiment with Ntask \u2208 {8, 16, 24, 32, 40, 48, 56, 64}. We noticed that the overall pass rates are higher than those in the Scale-Shot setting (Table 2), potentially due to a smaller value of nshot. However, long-context models still struggle in this setting: in 37 out of 40 cases in Table 6, the overall pass rates drop below 90%. The three cases achieving pass rates above 90% use the Llama2-7B (80k) model, an outlier model \u00a75.1.\nB.2 Controlled Analysis\nIn Fig. 7-8 we repeat the controlled experiments in Fig. 4-5, using N-task=64, N-shot=2 instead of N-task=16, N-shot=4. Our observations are generally consistent with those in Fig. 4-5. One exception is Mistral-7B (32k) experiments in Fig. 7, where the model achieves comparable accuracies in Recall, Replay and Remove. We attribute this to the usage of a smaller N-shot value compared to Fig. 4."}, {"title": "C NIAH-style Visualizations", "content": "In Fig. 9-18 we present detailed Task Haystack results for ten open-weight models. In Fig. 19-20 we present results for the two OpenAI models, GPT-3.5-Turbo and GPT-40.\nEach figure contains three subfigures: On the left side, we illustrate the results of the original needle-in-a-haystack evaluation [Kamradt, 2023]. We use the question \"What is the best thing to do in San Francisco?\" and the needle \"The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\" The score is the token-level recall of the model's response. In the middle, we visualize the results of the Scale-Shot setting. On the right side, we visualize the results of the Scale-Task setting. See \u00a74 for the details of the two scaling settings.\nIn each subfigure, the x-axis represents the input context length, and the y-axis represents the depth of the key information (i.e., \"needle\u201d). In figures visualizing Task Haystack results, a red cell represents that the model is failing the test (i.e., Lifelong ICL being significantly worse than Single-task ICL) and a blue cell represents that the model is excelling the test (i.e., Lifelong ICL being significantly better than Single-task ICL, potentially due to positive transfer)."}, {"title": "D Fine-grained Diagnostic Reports", "content": "Task Haystack inherits the controllability benefits of the original needle-in-a-haystack test [Kamradt, 2023]. It is straight-forward to aggregate results by permutations, context depth, and task, enabling the creation of visualized reports to help identify the vulnerabilities of long-context LMs.\nIn the following, we provide visualizations of 6 sets of experiments discussed in the main paper and summarize our main findings. The experiment settings include:\n\u2022 Fig. 22: Mistral-7B (32k), N-Task=16, N-Shot=8.\n\u2022 Fig. 23: FILM-7B (32k), N-Task=16, N-Shot=8.\n\u2022 Fig. 24: GPT-3.5-Turbo (16k), N-Task=16, N-Shot=4.\n\u2022 Fig. 25: GPT-40 (128k), N-Task=16, N-Shot=8.\n\u2022 Fig. 26: Mistral-7B (32k), N-Task=32, N-Shot=2.\n\u2022 Fig. 27: Mistral-7B (32k), N-Task=64, N-Shot=2.\nD.1 How to interpret the diagnostic report?\nThe main body of the diagnostic report is an n\u00d7n matrix, where n is the number of tasks used in the experiments. The x-axis represents the task index in the Lifelong ICL stream of all tasks, while the y-axis represents the task name. If the cell at (index 5, insincere questions) is colored red, it indicates that the task of insincere questions appears at index 5 in one of the five permutations, and the performance when using the Lifelong ICL prompt is significantly worse than when using the single-task ICL prompt, resulting in a test failure in Task Haystack. A white cell suggests no significant differences, and a blue cell suggests that Lifelong ICL outperforms Single-task ICL. Since we run five permutations of tasks in our experiments, the figure is only sparsely colored. A grey cell means \"N/A\" and indicates that the task does not appear at a specific index in the five sampled permutations.\nBelow the main matrix, we plot the results according to the five permutations we created. If the cell at (permutation 1, index 5) is colored red, it indicates that the task at index 5 in permutation 1 failed the Task Haystack test. We average each column and each row in the main n\u00d7n matrix to aggregate performance by task and by index, and visualize them at the right or the bottom of the report. This helps to investigate which tasks are more likely to fail (or excel) and to understand which positions in the context window are more vulnerable.\nD.2 Main Findings\nFailing and excelling are highly task-dependent. In Fig. 21 we plot the histogram of failure/excel rates grouped by tasks, in the experiments with Mistral-7B (32k), N-Task=64, N-Shot=2. The category \"Fail (5/5)\" achieves the second-highest frequency, suggesting that these tasks are inherently more likely to be influenced (or \"forgotten\") in Lifelong ICL, regardless of their position in the context. Similarly, the bars for Excel 3/5, 4/5, 5/5 have higher frequencies than Excel 1/5, 2/5, indicating that certain tasks are inherently more likely to benefit from positive transfer compared to others.\nDifferent models demonstrate different patterns. In Table 8, we list the names of tasks that always fail (i.e., fail in 5 out of the 5 task permutations) and the names of tasks that often excel (i.e., excel in more than 3 out of 5 permutations) in Lifelong ICL for various models.\nOur findings show little consistency across the different models investigated. For example, the task brag_action often excels with Mistral-7B (32k) but always fails with FILM-7B (32k) and GPT-3.5-Turbo (16k). Similarly, the task insincere_questions also appear in both categories for different models. One hypothesis is that the compared models may have been trained on the tasks we use, thereby influencing their forgetting and transfer behavior. However, due to the lack of transparency regarding the training details of these models, we cannot further investigate this hypothesis. Another hypothesis is that the Lifelong ICL prompt may influence the model's calibration and consequently the final accuracy. We leave the investigation of this hypothesis for future work."}]}