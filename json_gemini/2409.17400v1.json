{"title": "AgRegNet: A Deep Regression Network for Flower and Fruit Density Estimation, Localization, and Counting in Orchards", "authors": ["Uddhav Bhattarai", "Santosh Bhusal", "Qin Zhang", "Manoj Karkee"], "abstract": "One of the major challenges for the agricultural industry today is the uncertainty in manual labor availability and the associated cost. Automated flower and fruit density estimation, localization, and counting could help streamline harvesting, yield estimation, and crop-load management strategies such as flower and fruitlet thinning. This article proposes a deep regression-based network, AgRegNet, to estimate density, count, and location of flower and fruit in tree fruit canopies without explicit object detection or polygon annotation. Inspired by popular U-Net architecture, AgRegNet is a U-shaped network with an encoder-to-decoder skip connection and modified ConvNeXt-T as an encoder feature extractor. AgRegNet can be trained based on information from point annotation and leverages segmentation information and attention modules (spatial and channel) to highlight relevant flower and fruit features while suppressing non-relevant background features. Experimental evaluation in apple flower and fruit canopy images under an unstructured orchard environment showed that AgRegNet achieved promising accuracy as measured by Structural Similarity Index (SSIM), percentage Mean Absolute Error (pMAE) and mean Average Precision (mAP) to estimate flower and fruit density, count, and centroid location, respectively. Specifically, the SSIM, PMAE, and mAP values for flower images were 0.938, 13.7%, and 0.81, respectively. For fruit images, the corresponding values were 0.910, 5.6%, and 0.93. Since the proposed approach relies on information from point annotation, it is suitable for sparsely and densely located objects. This simplified technique will be highly applicable for growers to accurately estimate yields and decide on optimal chemical and mechanical flower thinning practices.", "sections": [{"title": "I. INTRODUCTION", "content": "The United States tree fruit production industry is largely dependent on the human labor force for field operations such as training, harvesting, pruning, and flower and fruitlet thinning. Automation in tree fruit crops can improve the efficiency, quality, and sustainability of fruit production while reducing labor costs and addressing the labor shortage challenges. Crop-load management approaches such as flower and fruit thinning, and harvesting require information such as flower and fruit distribution, count, and location over the different growth stages. Such information during flowering and green fruitlet development stages would be essential to leverage selective/targeted mechanical and chemical thinning approaches to maximize fruit quality and yield [1], [2]. Counting matured fruit during harvest season assists in pre-and post-harvest management strategies, including estimating the labor force, harvesting equipment, transport logistics, post-harvest processing, packing, inventory management, and sales planning. Fruit location and distribution estimation help optimize the distribution of the labor force and harvesting containers. Currently, growers manually inspect flower and fruit distribution in sample location, followed by extrapolation to the entire orchard, which is prone to error, labor-intensive, challenging to scale, and expensive [3].\nRobust computer vision algorithms, including deep learning, are increasingly being adopted in various agricultural applications, such as nutrient content assessment [4], [5], plant disease diagnosis [6], fruit detection, classification, and segmentation [7]\u2013[10], flower detection and segmentation [1], [11], [12] in challenging field environments. The fully supervised deep learning approaches offer high accuracy, precise object detection, and boundary estimation, which is well suited for applications such as robotic harvesting and fruit growth tracking. However, the detection-based approaches are highly complicated and may not necessary for many agricultural tasks such as crop-load estimation and bloom density estimation. For instance, full boom flowers in both stone (e.g., cherry) and pome fruits (e.g., apples) are densely located in constrained space with high flower-to-flower occlusions making individual flower annotation and detection challenging (see Fig. 1). Additionally, the detection of unopened flowers (see Fig. 1, top-left) is highly challenging since flowers are smaller in size and lack feature information."}, {"title": "II. PREVIOUS WORK", "content": "The classical learning approaches requiring extensive feature engineering could only support input data transformation into very few successive representation spaces and showed poor per-formance in complex problems, which eventually gave rise to deep learning [18]. Dias et al. [19], [20] reported the implementation of deep learning in flower cluster segmentation using Convolution Neural Network (CNN) for feature extraction and region refinement approaches. Bargoti and Underwood [21] compared multilayer perceptron and CNN for image segmentation, followed by a watershed algorithm and circular Hough transform (CHT) for fruit detection. Additionally, popular end-to-end object detection and segmentation approaches such as Faster R-CNN (FRCNN) [22], Mask R-CNN (MRCNN) [23], and You Only Look Once (YOLO) [24] have been heavily adopted in object detection and segmentation in agricultural contexts. FRCNN has been used for single to multi-class classification and detection of fruits such as apples [7], kiwis [8], oranges [8], mangoes [8], and flower clusters [1]. In another work, Farjon et al. [1] proposed FRCNN-based cluster detection [1] and Wang et al. [2] proposed cluster segmentation-based methods for flower density estimation to optimize chemical flower thinning. Tian et al. [9] implemented improved YOLO v3 with DenseNet as a feature extractor for apple fruit detection. MRCNN, capable of simultaneous detection and instance-level segmentation, has been adopted for apple fruit, flower, and flower cluster detection and segmentation [11], [12], [25]. Additionally, some previously reported approaches perform fruit counting using two-stage computation [26]\u2013[28]. The possible fruit locations were first separated by using a blob detection convolution network [26], or region proposal network [27], [28] followed by linear regression [26], or classification method for counting [27], [28].\nDetection-based approaches are robust and accurate since the deep neural networks are trained on exact object features specified by polygons or bounding boxes. However, the detection-based approaches may not be suitable if the number of object instances is large, the object size is small, or if a heavy occlusion exists between the object of interest and the object and background. A study reported by Gomez et al. [29] showed that detection-based and regression-based counting showed comparative results for low-density objects in agriculture datasets. However, for high-density objects (more than 100 objects/image), detection-based counting had shown five times larger error compared to regression-based counting [29]. Furthermore, during full bloom, individual flower detection becomes highly challenging since flowers are located in dense clusters with substantial flower-to-flower occlusions. The object detection approaches have shown to perform poorly in detecting small objects [30]. Despite their maturity (unopened/fully opened), individual flowers have a substantially lower pixel footprint compared to well-sized apples. Furthermore, flowers are more randomly oriented than fruits, resulting in random cluster structures and orientation, adding challenges even for human annotators (see Fig. 1) to annotate individual flowers. Hence developing computer vision approaches with the following characteristics is important: i) Ability to operate effectively on densely located objects without being influenced by the number of objects present in the image. ii) Simplified annotation process to facilitate easier and more efficient data labeling and network training. iii) Lightweight design enabling implementation on low-computation devices like cell phones and IoT devices.\nDespite significant research in flower and fruit detection, limited studies have been reported for direct flower and fruit density estimation (sometimes referred to as mapping), localization, and counting. Recently, Farjon et al. [31] proposed a leaf-counting regression network using ResNet-50 and a feature pyramid network for banana and tobacco leaf counting and localiza-tion. Rahnemoonfar and Sheppard [32] used a modified version of Inception-ResNet trained in synthetic images to count red tomatoes. Our previous work studied direct flower and fruit counting and investigated the characteristic features contributing to the object count [33]. The proposed approach in this work is lightweight convolution neural network and leverages the ConvNeX - T architecture, an improvement over the ResNet-50 architecture. Furthermore, the proposed approach employs a U-Net-like structure with a skip connection allowing the low-level features to be transmitted to the deeper convolution layers. Furthermore, unlike the previously reported approaches, we introduce a segmentation branch and utilize the attention mechanism to suppress the background features (caused by a canopy in the immediate back row) and handle occlusions (caused by trunk, branch, leaves, and trellis wire) often seen in agriculture which is expected to improve the system performance while minimally adding computational overhead."}, {"title": "III. PROPOSED APPROACH", "content": "The proposed approach was to estimate object density maps specifying the flower and fruit distribution followed by object count and location estimation. To generate the density map, individual flowers and fruits were first annotated\u00b9 as single point/pixel specifying the flower and fruit centroid. Only the flowers and fruits on the immediate front canopy row were annotated, and the flowers/fruits on the back canopy were considered as background. Since the single point/pixel provided minimal object feature information, two ground truth maps, 1) Density Map and 2) Segmentation Map, were generated by inclusing pixels in neighborhood of the annotated centroids to train the neural network. The density map was the final output of the proposed network, while the segmentation map acted as an intermediate output and was used to refine the density map."}, {"title": "A. Ground Truth Generation", "content": "1) Density Map Generation: Following the strategy used by [34], density maps were created by convolving normalized Gaussian kernel at each centroid. Since the area under the curve of the normalized Gaussian kernel equals one, the summation of all the pixels in the convolved image"}, {"title": "2) Segmentation Map Generation:", "content": "The purpose of a segmentation map was to act as a gate to pass the relevant features while suppressing irrelevant features (e.g. apples and flowers in the background row, the background sky) in the density map via binary classification of pixels. Although it was named as segmentation map, the objective was not the target to delineate individual objects but to refine generated density map to highlight feature information of relevant objects. The segmentation map was generated by convolving circular disk structuring elements in each flower and fruit centroid. A circular object with a center at origin and radius r can be represented in standard form as $x^2 + y^2 = r^2$. For the flower and fruit centroids distributed at different locations within the image, the 2D object segmentation map was computed as\n$S(x,y) = \\sum_{f=1}^{I}((x \u2013 x_f^{m2})^2 + (y \u2212 y_f^{Y})^2 < r^2; r = 2\u03c3_f$                     (2)"}, {"title": "B. Network Architecture", "content": "The architecture of the proposed density estimation network was inspired by the encoder-decoder framework of the original U-Net architecture used for medical image segmentation [16]. Fig. 2 illustrates the proposed AgRegNet architecture. The front-end feature extraction module of AgRegNet was designed by modifying the ConvNeXt-T network architecture originally derived from the ResNet-50 [17]. A dual attention mechanism proposed by [35] was added within the modified ConvNeXt-T encoder skip connections and the encoder to decoder skip connections to enhance relevant features in both spatial and channel domains. Furthermore, a segmentation map was generated in the decoder section and was used as a feature filtering gate to pass the relevant features to the final density map estimation pipeline while suppressing irrelevant features.\n1) Front-end Feature Extraction Module: The encoder section of the front-end feature ex-traction module was designed by modifying the ConvNeXt-T network architecture proposed by [17]. During the preliminary experiment, the original ConvNext-T was implemented without any modifications. The original design of ConvNeXt-T aggressively reduced the feature map size to 1/32 of the original image size with heavy downsampling. The ConvNeXt-T architec-ture was modified so that the minimum feature map size on the encoder side after the final downsampling step would be 1/8 of the input image size. Additionally, deeper networks have a large number of hyperparameters and therefore require a large number of training images for improved generalization of the model. In agricultural situation where smaller dataset size is common, these networks are prone to overfitting [29], [36]. Hence, the stage computation of the original ConvNext-T was modified from 3 : 3 : 9 : 3 and the feature map dimensions of (96, 192, 384, 768) to 1 : 1 : 3 : 1 : 1 with the feature map dimension of (32, 64, 128, 256, 512). Since the residual blocks in AgRegNet in each stage were reduced by a factor of 3, an additional stage was added to the aggregate and enhance the features in stage4 without downsampling."}, {"title": "2) Skip Connections and Feature Enhancement:", "content": "As the proposed network leveraged the ConvNeXt-T and U-Net design, the skip connections were presented in two phases. The first set of skip connections were within the encoder module residual connection (see Fig. 3b) while the second skip connection spanned from each stage output of the encoder to the decoder module. Skip connections were commonly used as they improve detection and segmentation performance by propagating the low-level features to deeper layers while reducing the degradation problem. In this work, the feature map propagated via skip connection was further enhanced by introducing Convolutional Block Attention Module (CBAM) proposed by [35] (see Fig. 3a). CBAM acted as a feature enhancement and suppression gate in both spatial and channel dimensions. In CBAM, the Channel Attention Module (CAM) and Spatial Attention Module (SAM) were arranged sequentially, and both leveraged average-pool and max-pool operations for initial feature extraction. Let $X \u2208 R^{C\u00d7H\u00d7W}$ represent the input feature map, the CAM and SAM for CBAM were defined as\n$M_{CAM} = \u03c3(MLP(AvgPool(X)) + MLP(MaxPool(X))) $                              (3)\n$M_{SAM} = \u03c3(f^{7\u00d77}(Concat(AvgPool(X), MaxPool(X)))) $                                  (4)\nWhere, $M_{CAM} \u2208 R^{C\u00d71\u00d71}$, and $M_{SAM} \u2208 R^{1\u00d7H\u00d7W}$. MLP is multi-layer perceptron with one hidden layer, o is the Sigmoid activation function, $f^{7\u00d77}$ is the convolution layer with filter size 7."}, {"title": "3) Backend Module:", "content": "The backend module worked as a post-processing operator to accumulate and highlight the feature information, and to compute the density map. Since the output feature map from the stage5 computation was equal to the size of the output feature map of stage4, the feature maps were simply passed through a 1x1 convolution layer before concatenation. In the following decoder processing, the feature maps were combined via a concatenation, followed by a set of convolution operations (see Fig. 4). Since each stage of the encoder layer, except the last stage, was reduced to 1/2 of the size of the previous stage, feature maps were upsampled by factor of 2 before concatenating in the decoder side. After concatenation, the feature information was further processed via convolution, normalization, and activation layers. Let, $Conv2d(C_i, C_o, k, s, p, d)$ represents 2d convolution with input channels $C_i$, output channels $C_o$, kernel size $k$, stride $s$, padding $p$, and dilation rate $d$. The decoder module after current layer concatenation and before immediate future layer concatenation is given by\n$[Conv2d(C_i = dim_{cat},C_o = dim_{cat},k = 3,p = d_{rate},d = d_{rate}), Conv2d(C_i = dim_{cat}, C_o = \\frac{dim_{cat}}{2},k = 3, p = d_{rate}, d = d_{rate}), LN, GELU]$. In this work, we set the dilation rate to 2."}, {"title": "4) Segmentation and Density Map Generation:", "content": "The output of the decoder section in the backend module was further processed to generate two outputs that can be used to optimize the network using backpropagation. The segmentation map was similar to the semantic segmentation approach, which classified all the pixels into two classes: flower or fruit and the background. Given the input channels ($C_i$), the segmentation map was generated by $[Conv2d(C_i, C_i, k = 3,p = d_{rate},d = d_{rate}),Conv2d(C_i,C_o = 1,k = 3,p = d_{rate},d = d_{rate}), Sigmoid]$, where $d_{rate} = 2$. The output segmentation map acted as the feature refinement gate for the density estimation network. The output feature map of the segmentation-based refinement pipeline was the elementwise multiplication of the segmentation map and the feature map to be refined. After refinement, the final density map was created by $[Conv2d(C_i, C_i,k = 3,p = d_{rate},d = d_{rate}), Conv2d(C_i, C_o = 1, k = 3,p = d_{rate}, d = d_{rate})]$, where $d_{rate} = 2$."}, {"title": "C. Count Estimation and Object Localization", "content": "Object count was estimated by summing up the pixel intensity values of the density map. The predicted density map ($pred_{den}$) was post-processed to estimate the flower and fruit location. Algorithm 1 was developed to estimate the flower and fruit centroid in the prediction map and associate the predicted centroids ($pred_{peak}$) with the ground truth centroids ($gt_{peak}$). The density map estimated the likelihood that a pixel represented a flower or fruit. Hence, the higher the pixel intensity value, the greater the chances that a pixel is object centroid. For flower and fruit localization, the local peaks in the predicted density map were estimated using inbuilt scikit-image package function peak_local_max. Once the local peaks or centroid of the predicted flower and fruit were estimated, a bipartite graph was created using the Euclidean distance between the ground truth and predicted centroids as a cost function. Finally, the Hungarian algorithm was used to perform one-to-one matching between the ground truth and predicted centroids minimizing the cost."}, {"title": "D. Loss Function", "content": "The network loss was computed by combining the Dice loss for the segmentation map and Mean Squared Error (MSE) loss for the density map. In this work, the Dice coefficient [38] was selected as a segmentation optimization metric since it has shown robust performance in handling class imbalance problems compared to Binary Cross Entropy loss. Class imbalance was common in the flower and fruit dataset datasets where objects of interest occupy a substantially lower number of pixels compared to the background. The Dice coefficient is given as\n$D_{coeff} = \\frac{2\\sum_{i=1}^{N} p_{seg_i}g_{seg_i}}{\\sum_{i=1}^{N} p^2_{seg_i} + \\sum_{i=1}^{N}g^2_{seg_i}}$                                (5)\nwhere $i \u2208 N; N = H\u00d7W$, and $g_{seg}$ and $p_{seg}$ are the ground truth and predicted binary segmentation maps. The segmentation loss was computed as $L_{seg} = 1 - D_{coeff}$. The density map estimation was optimized using Mean Squared Error (MSE) loss given as\n$L_{reg} = \\frac{1}{N} \\sum_{i=1}^{N} (g_{den} - p_{den})^2$                                   (6)\nThe total loss ($L_{loss}$) was computed as $L_{loss} = L_{den} + \u03b1L_{seg}$. Where \u03b1 0.01 was the scaling parameter computed empirically."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "The experimental evaluation consisted of two datasets with apple flowers and apples (see Table I). Apple flower images were collected over two growing seasons (2018 and 2019) using different imaging sensors. Images were collected from three commercial apple orchards in Washington, USA, with multiple varieties grown in fruiting wall canopy architecture. Trees were trained and pruned to create narrow 2D structures using vertical or V-shaped trellis systems in fruiting wall architectures. The flower dataset included images of early to late phases of the flower blooming in three apple varieties: Scifresh (Vertical Fruiting Wall), Envi (V trellis), and HoneyCrisp (V trellis). Additionally, the apple fruit dataset publicly released by [7] as a part of a multi-class fruit classification experiment was annotated and used. The fruit dataset consisted of images of 800 harvest-ready apple canopies in vertical fruiting wall architecture [7]. Images with similar appearances were identified using Structural Similarity Index Measure (SSIM) greater than 95%, which were later removed by human verification resulting in 630 unique apple canopy images [39]. For experimental evaluation, each dataset was divided into a training set (75%) and a test set (25%). The validation dataset was obtained by randomly cropping the test images."}, {"title": "B. Model Training", "content": "The first four stages of the modified ConvNeXt-T network were initialized using a pre-trained ConvNeXt-T model. The rest of the layers were randomly initialized using a Gaussian distribution with a mean of 0 and a standard deviation of 0.01. The network was trained with Adam optimizer using a learning rate of 0.0004 and a learning rate decay rate of 0.995 at each epoch. Network training was conducted for up to 200 epochs using the PyTorch library using two Nvidia Titan X 12 GB GPUs. For all datasets, the training and test sets were reshaped to 1024(W) \u00d7 768(H), while the validation set was obtained by randomly cropping test set images to 768(W)\u00d7576(H)."}, {"title": "C. Performance Metrics", "content": "The density maps' quality was evaluated using SSIM and Peak Signal to Noise Ratio (PSNR) [39]. SSIM metric computed the similarity between two images in terms of the closeness of mean luminance, contrast, and structure. For structure comparison, the correlation between ground truth and the predicted density map was estimated by computing the covariance matrix [39].PSNR, on the other hand, took into account the mean square error between the ground truth and the predicted density maps at a pixel level. A lower MSE value corresponded to a lower numerical difference between the image pixels resulting in a larger PSNR value. For count evaluation, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) were used.\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |C_i^{pred} - C_i^{gt}|$             (7)\n$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (C_i^{pred} - C_i^{gt})^2 }$              (8)\nWhere N was the total number of images in the test set. $C^{pred}$ was predicted count, and $C^{gt}$ was ground truth count. To qualitatively evaluate the localization results, precision and recall metrics were estimated. Given the matched ground truth and predicted centroids, a cutoff value (T) was used to determine whether the matched centroids were correctly associated. The following criteria were used to evaluate the predicted centroids.\n$pred_{peak} E \\begin{cases} TruePositive, &\\text{if } d(gt_{peak}, pred_{peak}) \u2264 T \\\\ FalsePositive, & \\text{otherwise} \\end{cases}$                (9)\nThe Average Precision (AP) and Average Recall (AR) were computed by varying the cutoff threshold T. Value of T ranged from \u03c3f to 2.2\u03c3f with an increment of 1. \u03c3f was the default standard deviation of the Gaussian kernel used to create the ground truth density map. Furthermore, $d(gt_{peak}, pred_{peak})$ was the Euclidean distance between the ground truth and the predicted peak location."}, {"title": "V. RESULTS AND DISCUSSION", "content": "The proposed network was compared with different density map estimation approaches in terms of PSNR, and SSIM [39]. Among the all evaluated approaches the proposed approach was lightweight with 9.45M total trainable parameters. The following were some of the popular approaches compared with the proposed approach."}, {"title": "B. Flower and Fruit Counting", "content": "The proposed approach showed promising results for density estimation of apples and apple flowers regardless of the flowering stage and appearance, which was also reflected in the count evaluation (Table III). The proposed AgRegNet, while being lightweight with the least number of parameters (9.45M), showed superior performance compared to all evaluated approaches in the complicated dense flower image dataset with the highest PSNR (31.2) and SSIM (0.938) to estimate density map and lowest MAE (18.1) and RMSE (23.8) values to estimate the count."}, {"title": "C. Flower and Fruit Localization", "content": "In order to localize the flower and fruit centroid, the density maps were post-processed as per Algorithm 1 (see Section III C). The one-to-one matching between the ground truth and the predicted flower and fruit centroids was analyzed using the mean Average Precision (mAP) and mean Average Recall (mAR) metrics. The mean Average Precision (mAP) and mean Average Recall (mAR) metrics was obtained by averaging the AP and AR values of each image computed by varying the cutoff threshold \u03c3f to 2.2\u03c3f with an increment of 1 (see Section IV c). The cutoff threshold played a role analogous to the Intersection over Union (IoU) threshold in a detection-based approach. The mAP and mAR values for the fruit dataset were greater than the same for the flower dataset indicating better fruit localization capability with fewer false positive and false negative locations. Numerically, the mAP and mAR values for the fruit dataset were 0.93 and 0.89 respectively, while the same were 0.81 and 0.79 for the flower dataset (see Table IV). However, it is worth noting that the localization result depended on the generated density map, and the post-processing algorithms used to detect peak and one-to-one matching of the estimated centroids. Fig. 7 visualizes the localization results. The results were promising as the majority of the localized flower and fruit centroids from the proposed approach were correctly associated with the ground truth centroids in images acquired from a commercial orchard."}, {"title": "D. Ablation Experiments", "content": "To investigate the effect of different modules on the performance of the proposed network model, ablation studies were conducted in four different configurations with the Apple Flower dataset. The four configurations used were:\n\u2022 Mod.ConvNeXt-T: Modified ConvNext-T with a U shape and skip connection from the encoder to the decoder section.\n\u2022 Mod.ConvNeXt-T+CBAM: Modified ConvNeXt-T with CBAM attention connected in skip connection within each stage and skip connection from the encoder to decoder.\n\u2022 Mod.ConvNeXt-T+Seg.: Modified ConvNeXt-T with segmentation branch as output in addition to density estimation branch.\n\u2022 Mod.ConvNeXt-T+CBAM+Seg.: The proposed AgRegNet model.\nTable V shows that the addition of the CBAM and segmentation module showed improved performance of modified ConvNeXt - T. Fig. 8 shows the progressive refinement of generated density map with the addition of different modules. The modified ConvNeXt - T module suffered from high interference from the background objects, which was reduced to some extent by CBAM. The SSIM value of the modified ConvNeXt - T was 0.841, inferring 84.1% similarity between the ground truth and the predicted density map. It was also observed that better density map estimation inferred better localization capability, but it did not necessarily mean increased counting accuracy. Both CBAM and segmentation branches helped enhance the relevant features while suppressing the irrelevant background features. However, quantitative result showed that the effect of the segmentation branch in improving the density map and count was substantially higher compared to CBAM. The addition of the segmentation branch reduced the MAE and RMSE count by 27.1% and 21.2% while improving the density map SSIM by 10.7% compared to the Mod. ConvNeXt - T. On the other hand, the addition of CBAM reduced the MAE and RMSE by 8.3% and 2.4% while improving the density map SSIM by 1.6% compared to the Mod. ConvNeXt - T. As shown in Table V, the addition of the CBAM module in Mod. ConvNeXt - T with segmentation branch improved the system performance by a nominal value."}, {"title": "E. Practical Applications", "content": "With the simpler annotation technique supporting the generation of a larger training dataset in different orchard environments, and a lighter model structure, the proposed approach is expected to be more robust and practically applicable to various orchard operations. The proposed approach, when used for fruit counting, could be employed for crop-load estimation and yield estimation in commercial orchards. Fruit count provides yield estimation information allowing efficient and timely management of harvesting resources and developing harvesting strategies. Flower counting is also considered one of the earliest markers of crop yield in a given season. Flower density estimation, localization, and count information could be used for optimizing the currently existing flower thinning strategies using chemical and mechanical thinning approaches for targeted flower thinning applications. Furthermore, for robotic blossom thinning, flower cluster segmentation and flower count information can be combined to perform targeted selective flower thinning en masse such that a proportion of flower could be removed. Furthermore, the proposed approach offers simpler and more effective flower density estimation to optimize chemical thinning by localizing and counting individual flowers instead of approaches involving cluster detection [1] and segmentation [2].\nDifferent studies have been reported for flower cluster segmentation in apples [1], [11], [19], [20]. Additionally, it would be more practical to segment individual flower clusters and estimate the number of flowers within each cluster to thin excess flowers en masse instead of removing individual flowers as removal of individual flowers would be time-consuming and highly complicated for a robotic system to operate in a constrained working space. As mentioned before, the proposed AgRegNet is lightweight, with a small number of trainable parameters (9.45M), and is suitable for evolving but still limited image datasets prevalent in agriculture. Because of its small size, AgRegNet could be employed in light computation devices such as cell phones. Other compared approaches have a large number of training parameters (e.g. CSRNet - 16.26M by [13], and DRN - 27.32M by [31]). Furthermore, the inference time of the proposed approach is substantially lower (14.2 milliseconds) compared to Mask R-CNN (238 milliseconds) employed in the same system for flower cluster detection and segmentation [11], which improves practical adoption of the proposed model to real-time field operations.\nIt is noted that the images used in this work were single front view shots which might not display all the flowers/fruits present in the canopy due to the occlusion and limited field of view of the camera. Therefore, the robustness of the proposed approach could be further improved by leveraging multi-view images including the images from both sides of the canopy, using illumination invariant active lighting system [42], and eliminating unnecessary background objects via depth filtering. We also expect that increasing the number of annotated images will benefit the model for density map estimation and counting in varying orchard conditions and structures."}, {"title": "VI. CONCLUSION", "content": "In this article, a segmentation-assisted regression-based approach for flower and fruit density estimation, counting, and localization in an unstructured orchard environment was presented. Experiments were conducted in the apple flower and fruit datasets to evaluate the network accuracy. Through this study, the following conclusions were drawn.\n\u2022 Object density estimation, localization, and counting in agricultural fields can be simplified using a regression-based deep learning approach with point annotations in RGB images without explicit detection. The proposed approach correctly localized the majority of the flower (mAP=0.81, mAR=0.79) and fruit (mAP=0.93, mAR=0.89) centroids without precise object detection based on bounding box or polygon annotation.\n\u2022 Proposed AgRegNet showed superior performance in generating well-localized density maps compared to CSRNet, SFCN, and SCAR with high SSIM for flower (SSIM=0.94) and fruit (SSIM=0.91) datasets. For apple flower counting, the proposed approach surpassed CSRNet, SFCN, SCAR and DRN with the lowest MAE of 18.1 (percentage Mean Absolute Error=13.7%) and RMSE of 23.8. While for the fruit dataset, the proposed approach showed competitive performance with MAE of 3.1 (percentage Mean Absolute Error=5.6%) and RMSE of 4.0 against DRN, performing best with MAE of 2.4 and RMSE of 3.2.\n\u2022 Incorporation of the segmentation branch and attention mechanism was instrumental in improving the robustness to discard background objects and improving the accuracy. With the addition of a segmentation branch the MAE and RMSE of Mod. ConvNeXt - T reduced respectively by 27.1% and 21.2% while improving the SSIM of the density map by 10.7%. Even though the addition of CBAM did not enhance the outcome of the Mod. ConvNeXt - T compared to the same with the segmentation branch, the performance improvement was beneficial.\nThe results showed that the proposed technique is promising with direct practical applications for growers and is expected to be an essential step towards automating and streamlining flower thinning, crop-load estimation, and yield prediction in the specialty crop production industry. We will continue to work on improving the generalizability of the proposed method by further improving it algorithmically and by including images from multiple canopy views and wider varieties of fruit and vegetables."}]}