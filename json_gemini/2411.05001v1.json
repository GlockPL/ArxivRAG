{"title": "ANALYZING THE LANGUAGE OF VISUAL TOKENS", "authors": ["David M. Chan", "Rodolfo Corona", "Joonyong Park", "Cheol Jun Cho", "Yutong Bai", "Trevor Darrell"], "abstract": "With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages\u2014whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformer-based models have not just advanced, but fundamentally reshaped how we approach both vision and language processing, merging these domains in shared sequential representation spaces. Indeed, most recent multi-modal models including DALL-E (Ramesh et al., 2022), LLaVA (Liu et al., 2024) and Chameleon (Team, 2024) operate over joint tokenized representations of images and language, where models decompose images into \"visual languages\": linearized discrete patches or tokens analogous to words in a sentence. This process, shown in Figure 1, enables seamless integration of images into transformer architectures and allows models to solve multimodal tasks, ranging from image generation and image captioning to visual question answering and translation.\nDespite the success of such shared-structure models, current research lacks an in-depth understanding of whether the internal structure of visual tokens mirrors the principles governing natural languages. Specifically, the question arises: do languages formed of visual tokens follow the same statistical patterns, such as frequency distributions, grammatical rules, or semantic dependencies, that human languages exhibit? Investigating such statistical behavior of discrete visual tokens extends beyond theoretical curiosity; it has broad implications for practical machine learning applications. While in linguistic theory, phenomena like Zipf's law and entropy shape natural languages' structure and shape the design of machine learning algorithms, no such rules exist for visual languages. Such rules, if they exist, have the potential to motivate creating modality-specific models and procedures to capture the unique statistical properties of the underlying visual data.\nIn pursuit of such rules, in this paper we inspect the equivalence of visual and natural languages through an empirical analysis of token distributions, segmentation granularity, and syntactic and semantic structures. We start by investigating the frequency statistics of visual words and compare them to natural languages. Our analysis reveals that although visual languages can follow power-law (Zipfian) distributions, they use more tokens more uniformly. This leads to languages with greater per-token entropy and lower compression ratios, and implies that vision models may require more attention heads, larger embeddings, and longer training times with more diverse data compared to natural language models. Noting in these experiments that visual languages have coarser granularity than patches, we demonstrate through correlation analysis"}, {"title": "2 DO VISUAL TOKENS ACT LIKE WORDS?", "content": "The first, question that we examine is: Do visual tokens themselves (i.e. the patches of an image) act like words? While we often treat these tokens as either a word (or subword), as each token forms a single input sequence element in a transformer, it seems unintuitive that there would be a one to one statistical correlation between the two concepts. In this section, we look at several statistical properties of individual tokens, comparing those observed in natural language to those in visual systems."}, {"title": "2.1 PRELIMINARIES", "content": "What, explicitly, is a visual language? In this work, we consider a visual language to be a language induced over \"visual tokens\" by first converting images in a dataset to a discrete set of symbols using a visual tokenizer (often a VQ-VAE), and then linearizing those tokens into one-dimensional sequences (See Figure 1). Such a definition parallels efforts in both text-to-image diffusion and large vision and language models which have both explored using discrete visual tokens for vision-language model alignment (Team, 2024; Ramesh et al., 2022; Gu et al., 2022; Razavi et al., 2019), as well as in uni-modal models such as LVM (Bai et al., 2024) and LLamaGen (Sun et al., 2024).\nWe primarily focus on common tokenizers used for recent vision and language models, and our selection of tokenizers is overviewed in Table 1. These tokenizers are all VQ-VAE-based, trained on varying datasets, and with various methods. While some recent models such as Transfusion (Zhou et al., 2024) and LLaVA Liu et al. (2024) leverage continuous-valued tokens instead of discrete vocabularies, there is still considerable uncertainty about whether discrete or continuous-valued tokens are more effective (Mao et al., 2021). While many of our methods in this paper could apply to continuous tokens through a discrete quantization of those tokens, we leave such continuous extensions to future work. For more details on the tokenizers, see Appendix A.\nWe ground our empirical experiments in several common multi-modal datasets, including Conceptual Captions (12M) (Sharma et al., 2018), MS-COCO (Lin et al., 2014), ILSVRC (ImageNet) (Russakovsky et al., 2015) and XM-3600 (Thapliyal et al., 2022). Each of these datasets has a set of images, and (except ILSVRC) paired text in one or more languages. For more information on the datasets, see Appendix B.\nAn example visual sentence from MS-COCO (Image ID: 399655) is given in Figure 1. In all of the experiments in this paper, we linearize the tokens using a row-wise scan order (for a detailed discussion"}, {"title": "2.2 TOKEN FREQUENCY AND ZIPF'S LAW", "content": "The statistics of natural language token distributions have long been studied, beginning with Dewey (1921), who first plotted the frequency of English words. A key principle that emerged from this research is Zipf's Law (Kingsley Zipf, 1932), which describes a power-law relationship between the frequency of words and their rank in a language where a small number of high-frequency words dominate natural language, while the majority of words occur infrequently. Formally, Zipf's law states that:\n$f(r) \\propto r^{-\\alpha/\\sigma}$        (1)\nwhere f(r) is the frequency of the element with rank r and \u03b1/\u03c3 parameterize a learned Gaussian distribution (close to 1/0 in many natural languages).\nZipf's law has been observed across many languages (Gelbukh & Sidorov, 2001; Yu et al., 2018) and non-human communication systems (such as dolphins (McCowan et al., 1999)). As Mandelbrot pointed out, adherence to Zipf-like distributions ensures that communication systems-whether natural or artificial operate efficiently (Mandelbrot, 1953). Language models, especially large language models (LLMs), have been shown to follow this same pattern, with token distributions that obey Zipf's law (Patwary et al., 2019). This statistical regularity in language extends beyond word frequency - Zipf's law has also been observed in images themselves: Ruderman (1997) showed that the distribution of object sizes and spatial frequencies in natural scenes follows power-law distributions, and Crosier & Griffin (2007) showed that there was Zipfian behavior in image coding schemes such as JPEG.\nThus, we first ask the question - Do \u201cvisual languages\u201d follow Zipf's law? To do this, we tokenize the image datasets according to subsection 2.1 and compute the empirical token-rank frequency distributions on each of the datasets (See Appendix D for details). We show the empirical distributions in Figure 2. If the plots were Zipfian, we would expect them to be linear in the log-log space; while this is the case for natural languages, visual languages do not seem to generally conform to a linear curve, instead, for one and two grams, the plots follow a lognormal distribution, and for higher level N-grams are more convex in nature.\nFor one/two-grams, this indicates that token utilization is fairly uniform, with most tokens occurring in equal proportion, and the heavier tails of the distribution indicate that \u201crare\u201d are, in practice, not so rare, occurring with much higher frequency than expected under a power-law distribution. Whereas natural languages are often structured with a clear core vocabulary and then more specialized words, it seems like visual features seem to be more evenly distributed, with many features or combinations being equally likely. At higher n-grams, for visual languages there is more convex behavior, suggesting that there are very few common n-grams, instead, n-grams are often unique, and composed in ways that appear very infrequently within the datasets. Such an implication implies that visual languages are highly context-dependent (which is sensible, as visual scenes are quite complex).\nTo confirm these details, we fit a Zipf's distribution to each of the models, with the results of the fit shown in Table 2. Interestingly, the a values have opposite behaviors for visual and natural languages in the light of increasing N. In natural languages, the fact that & increases with N means that higher-order N-grams follow steeper power-law distributions, and the distribution of N-gram frequencies becomes more concentrated around a few common combinations, while the frequency of rare combinations decreases rapidly. In visual languages, on the other hand, the decrease in a with increasing N suggests that higher-order combinations of visual features follow flatter distributions: as visual N-grams increase in complexity, there is more diversity in the combinations of features and patterns, leading to richer and more distributed sets of higher-order feature combinations.\nThese phenomena together suggest that VQ-VAEs are \"spreading\" information between the independent tokens, rather than building compressive and compositional structures, which we explore further in"}, {"title": "2.3 \u03a4\u039f\u039a\u0395N INNOVATION", "content": "One thing that stands out from the experiments in subsection 2.2 is that single visual tokens appear more uniformly than single words, inspiring the question: do new images consist of mostly new tokens, or do new images re-combine existing tokens in novel ways? In natural language, this has generally been codified by Heaps'/Herdan's law (Herdan, 1964; Heaps, 1978), which says that vocabularies' sizes are concave increasing power laws of texts' sizes (See Appendix E for details).\nTo explore this effect, Figure 3 plots the number of unique tokens seen against the number of images seen for the XM-3600 dataset for several visual tokenizers and natural languages. The natural languages follow the expected distribution, with unique tokens increasing sub-linearly with respect to the number of images. The visual tokens, on the other hand, appear much more rapidly. For single tokens, almost all of the tokens in the vocabulary appear within the first 100 images, suggesting that the rate of token innovation is significantly higher than that of natural languages. For 2-grams and 4-grams, the relationship trends linear, but never approaches the sub-linear behavior that is expected of generative systems which follow Heaps' law. Additional experiments on MS-COCO are given in Appendix E.\nWe further fit a Yule-Simon distribution (Simon, 1955) to both the natural and visual languages. The Yule-Simon process is a stochastic model for generating sequences of words or tokens, where the probability of introducing a new token decreases as more tokens are added, leading to a power-law distribution; mathematically, this process is governed by a probability proportional to the current token frequency, combined with a parameter that controls the rate of new token introduction (see Appendix F for more details). The results, given in Figure 4 and Appendix F, demonstrate that the generative process for new tokens largely does not fit with that described by the Yule-Simon process in the visual case, however, fit quite well for many text languages."}, {"title": "2.4 NATURALITY", "content": "Benford's Law describes the frequency distribution of leading digits in naturally occurring datasets, where smaller digits like 1 and 2 appear disproportionately more often than larger digits like 8 and 9 (Benford, 1938). Originally observed in domains such as physics (Sambridge et al., 2010), economics (T\u00f6dter, 2009), and demographics (Miller, 2015), recently, there has been growing interest in extending this statistical principle to linguistic data (Golbeck, 2023; Meli\u00e1n et al., 2017; Hong, 2010). One of the primary applications of Benford's law is the detection of anomalies in data: datasets that do not follow Benford's law are likely to be unnatural in nature - here, we ask the question, do visual language token frequencies naturally follow Benford's law? We follow a similar tokenization process to subsection 2.2, and plot the occurrence of leading digits in the token frequency distribution (See Appendix G for more details).\nOur results are shown in Figure 5 for the MS-COCO dataset, and in Appendix G on other datasets. Interestingly, for single tokens, the distribution is unique-token-heavy, with the remaining tokens having a Gaussian distribution around six. Two-grams are the most natural, with Chameleon following Benford's law almost exactly, with three-grams significantly dominated by low/unique frequency tokens. Interestingly,"}, {"title": "2.5 ENTROPY AND REDUNDANCY", "content": "Building on the foundational work of Shannon (1951), entropy and redundancy have long been understood as key characteristics of natural language, providing insight into its inherent predictability and compressibility. While natural languages, like English, exhibit high redundancy that enables efficient encoding, it is unclear if visual languages might have similar coding behaviors. To evaluate the efficiency of encoding, we use a similar setup to subsection 2.2 and extract token streams for each of the target datasets. We then compute the entropy of the token streams, as well as compute a simple Huffman code/compression (Huffman, 1952) for each of the resulting streams. Such a hierarchical compression code allows us to estimate the overall \u201ccompressibility\u201d of the stream (See Appendix H for background/details).\nThe results are summarized in Table 3. We can see that in general, the average code length, entropy, and bits of information/sample are higher for visual languages. This suggests that visual languages have more variability and are inherently more complex to predict and encode than natural language. This is unsurprising, given the complexity and richness of the visual world, compared to the sparsity of natural language, however, it is somewhat surprising that the entropy is not massively different from natural languages, suggesting that visual tokenizers are capable of reducing the richness of natural language to suitably sparse representations for reasoning. Notably different is the \u201ccompressibility\" of the token streams. While natural language tokens are highly compressible using Huffman encoding, visual languages are almost incompressible, suggesting that information is highly distributed amongst the tokens and that there is very little structural reuse between the different images. While we explore grammars further in section 3, this experiment indicates that it is unlikely that models have non-trivial grammars of tokens, instead, these tokens are more local, and particularly high-variance.\nThese experiments have several potential implications for model design. First, since visual tokens have significantly higher entropy and lower compressibility, it may be necessary to use more attention heads, deeper models, and more dense embeddings, in visual-based models in order to capture a sufficient number of relationships and higher-level representations of visual information. Models like LLaVA (Liu et al., 2024) with simple projection layers between the visual token and text token spaces may not perform"}, {"title": "2.6 TOKEN SEGMENTATION GRANULARITY", "content": "One common question for many vision researchers is: \u201cdo visual tokens represent objects?\" Indeed, while visual tokens are spatially fixed to patches in the image, because of the VQ-VAE training process, it is un-clear if they take on additional non-spatial semantic meaning. Recently, Hsu et al. (2021) demonstrated that in audio domains, HuBERT tokens (audio-tokens) have relatively high mutual information with phoneme representations of audio, suggesting that self-supervised models are capable of learning natural structures despite being segmented to fixed-width patches. Can we answer this question for visual languages as well?\nRecently, Myers-Dean et al. (2024) introduced the SPIN dataset, a new labeled dataset of hierarchically segmented objects, where the objects are labeled at the whole, part, and sub-part levels. This gives us per-image annotations of the existence of wholes, parts, and sub-parts. From this, we compute several measures of natural correlation between these part-annotations and the visual token languages, inspired by Hsu et al. (2021) (For more details, see Appendix I): Part Purity, a metric that measures the average accuracy of assigning a visual-token to its most likely part label, reflecting image-level part consistency within a particular visual token, Visual-Token Purity, a metric that assesses how well images containing the same part label are consistently assigned to the same visual-tokens and Part-Normalized Mutual Information, an information-theoretic metric which measures the percentage of uncertainty about the part-label eliminated after observing a particular visual token.\nThe results are summarized in Table 4. In general, tokenizers appear to be most effective at capturing part-level representations, as evidenced by consistently higher Part Purity (PP) values for parts compared to wholes or sub-parts across all models. This suggests that tokenizers are better aligned with mid-level structures (parts), rather than whole objects or fine-grained sub-parts. However, Visual-Token Purity (VTP) remains low across all models and levels of granularity, indicating that images containing the same part-label are not consistently assigned to the same visual tokens, reflecting fragmentation in the clustering. PNMI values are generally higher for sub-parts than for parts or wholes, particularly in models like llamagen-vq-ds16-c2i, which shows the highest PNMI across all levels. This implies that tokenizers can capture more fine-grained information at the sub-part level, though the corresponding decrease in part purity for sub-parts suggests that while they can reduce uncertainty about part labels, their actual clustering of sub-parts is inconsistent."}, {"title": "3 ARE VISUAL LANGUAGES STRUCTURED LIKE NATURAL LANGUAGES?", "content": "In subsection 2.5 we showed that visual languages are not very compressible using Huffman encodings, suggesting that visual languages may not have hierarchical structures similar to those of natural languages. To inquire further into this question, we test whether Context-free Grammars (Chomsky & Sch\u00fctzenberger, 1959) can approximate the structure of visual languages as well as they can natural languages by fitting grammars to each modality using unsupervised grammar induction techniques.\nParticularly, we use Compound Probabilistic Context-Free Grammars (C-PCFG) (Kim et al., 2019) as the grammar formalism for our experiments. C-PCFGs are a type of neural PCFG, where grammar production rules are modeled as compound probability distributions (Robbins, 1956) \u2013 every production depends"}, {"title": "3.1 TOPOLOGICAL SIMILARITY", "content": "To expand our discussion on structural similarity, we further investigate how similar the topological structures of visual and textual tokens are, and whether these similarities can reveal meaningful insights about the underlying representations, i.e. can we observe strong structural alignment points between the natural and visual latent spaces, or are there notable deviations?\nWe begin by training GloVe embeddings (Pennington et al., 2014) on co-occurrence matrices derived from visual tokens and textual tokens present in the captions (details in Appendix J). This gives us a continuous topology of similar dimension within which we can explore potential alignment. We then explore two pairwise distance matrices between the two GloVe vector spaces: Procrustes alignment (Gower, 1975) and directed Haussdorf distance (Bowen, 1979).\nFigure J.2 gives the Procrustes similarity and Figure J.3 gives the directed Haussdorf distance between the models, with some key aggregates summarized in Table 5. While there are few clear trends, a key finding is that vision models are largely more aligned with natural language models than they are with each other, with Chameleon being slightly more central than other models (perhaps due to its training process). Overall, the lack of strong alignment trends between different vision models highlights that their latent spaces are more fragmented, suggesting that visual token representations are often model-specific or task-dependent, rather than universally structured. Notably, however, some languages align much better with visual models than others (such as Korean to the Chameleon tokenizers, or Hungarian/Polish in general), suggesting that some tokenizers may be significantly stronger when aligning to specific languages. Another interesting observation is that the directed Hausdorff distance shows that the natural language to vision model alignment is significantly further than the vision model to natural language alignment. This results implies that generation of images from text is much harder than the generation of text from images - something often observed in practice.\nGiven the overall distances between these structural representations, our experiments suggest that future model architectures should focus on reducing this asymmetry. Specialized models that effectively encode multimodal information - and perhaps aligned tokenization methods (such as CLIP), represent promising future directions for research."}, {"title": "4 CONCLUSION", "content": "This paper takes a first look at visual languages from the angle of empirical statistics. While there are similarities between how we currently treat visual and natural languages/sentences - the experiments in this paper show that, at least statistically, visual tokens and natural languages are far from trivially aligned. Such poor statistical alignments motivate both unique model architectures and training procedures for visual transformers - and we hope that this work inspires further research into novel architectures, designs, and hyper-parameters for vision-token based models. Indeed, while some of the hypotheses that we outlined in"}, {"title": "APPENDIX", "content": "The appendix consists of the following further discussion:\n\u2022 Appendix A discusses the tokenizers used when constructing the visual languages, with detailed descriptions of Chameleon, Stable Diffusion, and LlamaGen tokenizers.\n\u2022 Appendix B describes the datasets utilized in this work, including Conceptual Captions (CC12M), MS-COCO, ImageNet (ILSVRC), XM-3600, and SPIN.\n\u2022 Appendix C describes potential limitations and opportunities for future work.\n\u2022 Appendix D describes the Zipf experiments in subsection 2.2, and gives additional experimental details.\n\u2022 Appendix E describes Heaps' law, and gives additional experimental results to complement subsection 2.3.\n\u2022 Appendix F explains the Yule-Simon distribution, the methodology used to fit this distribution to observed token frequencies, and the experimental results from token frequency analysis.\n\u2022 Appendix G discusses the process used for analyzing visual tokens according to Benford's law in subsection 2.2, including n-gram extraction and first-digit distribution analysis across datasets.\n\u2022 Appendix H explains the Huffman encoding experiments, measuring entropy and compression efficiency of tokenized visual data.\n\u2022 Appendix I explores segmentation granularity and how visual tokens correspond to parts and sub-parts in images, using co-occurrence metrics like Part Purity and Visual Token Purity.\n\u2022 Appendix J discusses CPFCGs, the process for extracting GloVe embeddings from both vision and language tokenizers, and the topological analysis used in subsection 3.1."}, {"title": "A TOKENIZERS", "content": "In this work, we explore three families of VQ-VAE (Van Den Oord et al., 2017) based tokenizers for images. While the general details are given in Table 1, we expand on the details for the tokenizers here.\nChameleon (Team, 2024): Chameleon is a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text. The image tokenizer, chameleon-512, is based on Gafni et al. (2022), which is a modified VQGAN Esser et al. (2021) model which adds perceptual losses to specific image regions such as faces and salient objects (in an attempt to improve the fidelty of generated images). The chameleon tokenizer is trained from scratch on a closed-source set of licensed images, and encodes images at a resolution of 512\u00d7512 into a discrete token codebook size of 8192 and dimension 256. Notably, when training the tokenizer the model up-samples the percentage of images with faces by two times to improve performance on human face generation (which may somewhat skew the performance of the tokenizer on non-face based images).\nStable Diffusion (Compvis) (Rombach et al., 2022): Stable Diffusion is a latent text-to-image diffusion model, which learns a joint distribution over image and text representations in a discretized latent space. Similar to the chameleon tokenizer, these tokenizers are trained in an adversarial manner following Esser et al. (2021) on OpenImages Kuznetsova et al. (2020), such that a patch-based discriminator can differentiate original images from reconstructions. The stable diffusion tokenizers (compvis-vq-f8-64 and compvis-vq-f8-256) have an image resolution of 384\u00d7384 with a crop-size of 256, and use a codebook dimension of size 4, with a very high VQ quantization dimension of 16384. While these models were trained at a crop size of 256, for grammatical analysis, many of the generated sequences are much too long to solve using traditional methods. Thus, we additionally consider a model, compvis-vq-f8-64 which uses a 64\u00d764 crop of the image, which produces linearized sequences of a more manageable length of 32, used in section 3. The tokenizer compvis-vq-imagenet-f16-1024-256 (originally trained by Esser et al. (2021)) uses the same training procedure as those in Rombach et al. (2022), but was trained on the ImageNet dataset, with a codebook of dimension 256, and size 1024.\nLlamaGen (Sun et al., 2024): LlamaGen is a family of image-generation models that apply next-token pre-diction to perform iamge synthesis. The LlamaGen tokenizer, 1lamagen-vq-ds16-c2i takes images of resolution 256 \u00d7 256, and uses a codebook of size 16384 and dimension 8. 11amagen-vq-ds16-c2i is trained on the ImageNet training dataset."}, {"title": "B DATASETS", "content": "In this work, we explore the effects of tokenization across several datasets:\nConceptual Captions (12M) (Sharma et al., 2018): Conceptual captions (12M, CC12M) is a dataset with approximately 12 million image-text pairs soruce from web alt-text, traditionally used for vision-language pre-training.\nMS-COCO (Lin et al., 2014): The MS-COCO dataset is a dataset for image description containing 328K images, each with 5 ground truth descriptions in English. In addition to the standard annotations, we also leverage translated annotations from Thapliyal et al. (2022), which provide machine translations into 36 languages for each of the MS-COCO images.\nImageNet (ILSVRC) (Deng et al., 2009): ImageNet contains approximately 1.2M images which are manually annotated to indicate the objects present in each image. These annotations are linked to the WordNet hierarchy, providing a rich set of object categories. The dataset covers 1,000 object classes for the classification task, including common objects like animals, vehicles, and household items.\nXM-3600 (Thapliyal et al., 2022): The Crossmodal-3600/XM3600 dataset is a multilingual multimodal evaluation dataset designed to support image captioning tasks across 36 languages. It consists of 3600 geographically diverse images, each annotated with human-generated captions that are consistent across languages but not derived from direct translations, ensuring linguistic naturalness and cultural relevance. The images were selected from regions where these languages are spoken, drawn from the Open Images Dataset using a careful algorithm to ensure regional diversity.\nSPIN (Myers-Dean et al., 2024): The SPIN (SubPartImageNet) dataset is a hierarchical semantic segmen-tation dataset designed to provide detailed annotations for natural images at multiple levels of granularity, specifically focusing on objects, parts, and subparts. SPIN builds on the PartImageNet dataset, expanding its scope by introducing over 106,000 subpart annotations across 203 subpart categories, covering 34 part categories from diverse objects such as animals, vehicles, and human figures. The dataset contains 10,387 images divided across 11 supercategories, including rigid objects like cars and non-rigid entities like animals."}, {"title": "C LIMITATIONS", "content": "While this paper does have significant empirical results, we want to recognize the several potential limitations/opportunities for future work:\nTokenizer Selection: While the paper does focus on a fairly wide range of common (and modern) visual tokenizers, there is a fairly large potential selection of additional tokenizers that could be compared. Indeed, a key limiting factor is that all of the tokenizers explored in this work are VQ-VAE based. As discussed in subsection 2.1, a detailed analysis of continuous tokenizers (such as auto-encoders which are KL-regularized, CLIP-style encoders, or BERT-style encoders) would provide significant additional information. Directly applying natural language statistics to these continuous embeddings, however, is non-trival, as to understand ideas of \u201ctoken frequency\u201d or \u201cgrammar\u201d, such analyses would have to either (a) be extended to the continuous domain, or (b) the tokens themselves would have to be quantized to discrete representations. We believe that such extensions are highly interesting, but are worthy of detailed analysis and discussion which is outside the scope of this initial work.\nDataset Coverage: Another limiting factor of this research is the dataset coverage. While it is impossible to analyze all data, visual information is highly diverse, and domains such as medical imaging, geospatial imaging, or autonomous driving may have entirely different statistics. In general, however, we found that across the datasets that we did use (which represent a fairly general slice of traditional training data), the statistical representations were similar. For example, it is fairly challenging to distinguish any dataset-level"}, {"title": "D ZIPF'S LAW", "content": "As discussed in subsection 2.2, Zipf's Law (Kingsley Zipf, 1932), describes a power-law relationship between the frequency of words and their rank in a language where a small number of high-frequency words dominate natural language, while the majority of words occur infrequently. Formally, Zipf's law states that:\n$f(r) \\propto r^{-\\alpha/\\sigma}$\n(D.1)\nwhere f(r) is the frequency of the element with rank r and \u03b1/\u03c3 parameterize a learned Gaussian distribution (close to 1/0 in many natural languages).\nFor each dataset and tokenizer, to compute the power law fit, we leverage the method/code in Alstott et al. (2014). When fitting the power laws, because of computational limits, we limit the number of processed N-grams to 5M, and on CC12M and ILSVRC, unless otherwise noted, we compute the n-grams on only a subset of the full dataset consisting of a randomly sub-sampled 200K image set). Results broken down by N-gram are shown in Figure 2, while results broken down by model/dataset are given in Figure D.1"}, {"title": "E HEAPS'/HERDAN'S LAW", "content": "Heaps' law (also referred to as Herdan's law) is an empirical rule that describes the relationship between the size of a corpus and the number of unique word in the corpus (Heaps, 1978; Herdan, 1964). Specifically, it predicts that as the size of a text grows, the number of unique words increases, but at a decreasing rate. Mathematically, the law is described by:\n$V(N) = kN^\\beta$\n(\u0395.1)\nwhere V(N) is the number of distinct words (the vocabulary size), N is the total number of words, and k and \u03b2 are parameters, 0<\u03b2<1. Heaps' law reflects the fact that even as new text is added to a corpus, the frequency of newly introduced words diminishes, meaning a large corpus doesn't proportionally expand its vocabulary.\nPlots for unique tokens vs. images seen on XM-3600 are given in Figure 3, with those for MS-COCO given in Figure E.1."}, {"title": "F YULE-SIMON DISTRIBUTION", "content": "The Yule-Simon distribution (Willis & Yule, 1922) is a model often used to describe processes where new elements (in this case, tokens) are introduced over time with a probability that decreases as the existing set of elements grows. Specifically, for a sequence of tokens, the Yule-Simon distribution describes the probability of the k-th token occurring m times as:"}, {"title": "G BENEFORDS LAW", "content": "Benford's Law (Benford, 1938) describes the distribution of leading digits in many naturally occurring datasets, where smaller digits are more likely to appear as the first digit. Specifically, the probability P(d) of a digit d (where d is between 1 and 9) being the leading digit is given by:\n$P(d) = log_{10}(1+\\frac{1}{d})$        (G.1)\nAccording to this law, the number 1 appears as the first digit around 30% of the time, while larger digits like 9 appear less frequently, around 5% of the time.\nFor each dataset and tokenization configuration, we extract n-grams (with n = 1, 2, and 3) from tokenized text and image data. We aggregate the token frequencies by computing the distribution of the first digits of these counts. Specifically, the first digits of each token frequency are extracted, and their occurrences are counted to form a first-digit distribution. In cases where natural language data is available, we also compute aggregate distributions across multiple locales for text-based tokenizations. The aggregated text distributions include the mean, standard deviation, minimum, and maximum values for each first-digit count across different locales."}, {"title": "H HUFFMAN ENCODING / ENTROPY", "content": "Huffman encoding is a widely-used algorithm for lossless data compression", "log2(n)": "bits", "formula": ""}]}