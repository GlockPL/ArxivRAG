{"title": "Examining Independence in Ensemble Sentiment Analysis: A Study on the Limits of Large Language Models Using the Condorcet Jury Theorem", "authors": ["Baptiste Lefort Lefort", "Eric Benhamou Benhamou", "Jean-Jacques Ohana", "Beatrice Gueza", "David Saltiel", "Thomas Jacquot"], "abstract": "This paper explores the application of the Condorcet Jury theorem to the domain of sentiment analysis, specifically examining the performance of various large language models (LLMs) compared to simpler natural language processing (NLP) models. The theorem posits that a majority vote classifier should enhance predictive accuracy, provided that individual classifiers' decisions are independent. Our empirical study tests this theoretical framework by implementing a majority vote mechanism across different models, including advanced LLMs such as ChatGPT 4. Contrary to expectations, the results reveal only marginal improvements in performance when incorporating larger models, suggesting a lack of independence among them. This finding aligns with the hypothesis that despite their complexity, LLMs do not significantly outperform simpler models in reasoning tasks within sentiment analysis, showing the practical limits of model independence in the context of advanced NLP tasks.", "sections": [{"title": "Introduction", "content": "The integration of Natural Language Processing (NLP) into the financial sector has been pivotal in providing insights from textual data, evolving significantly with the release of Large Language models and generative models [4, 12]. In particular, NLP has allowed to do sentiment analysis. The terminology sentiments analysis emphasizes that the method aims at extracting market sentiments or tones and then at offering predictive insights thanks to its analysis [42, 38]. Compared to other NLP applications, financial sentiment analysis faces unique challenges [43]. Financial narratives often involve complex, domain-specific terminologies and exhibit a multiplicity of sentiments tied to different entities, which can render general sentiment analysis tools ineffective [28, 34, 39]. The complexity is further exacerbated by the nuances of financial news, which may be reactive rather than predictive, and the difficulty of integrating sentiment scores into practical investment strategies [46, 15].\nHistorically, sentiments analysis relied on simple machine learning techniques and predefined word lists [30]. Later on, with the introduction of more sophisticated NLP techniques, and in particular the deployment of models such as BERT and FinBERT, sentiment analysis has significantly refined its accuracy [11, 26]. With the emergence of Large Language Models (LLMs) like GPT, and particularly its variants tailored for conversational applications such as ChatGPT, a new paradigm in AI has been set forth [9, 35, 33]. These models offer enhanced capabilities in understanding and generating human-like text, presenting new opportunities to improve sentiment analysis within financial contexts.\nHowever, it is not clear how these new models differ from previous ones, motivating for a detailed and theoretically sound comparison with previous ones. In this paper, we focus on examining the independence of ensemble sentiment analysis conducted through LLMs, utilizing the Condorcet Theorem to investigate the limits and capabilities of these models in financial applications. The goal is to determine the extent to which these models can operate independently and to what degree their analysis can be considered reliable and unique in the context of complex sentiment tasks. Understanding this independence is critical, as it directly impacts the efficacy of models in real-world financial decision-making scenarios.\nThe paper is structured as follows: Section 2 reviews related works. Section 3 outlines the primary contributions of our work. Section 4 presents the theoretical framework for the Condorcet Jury theorem for bagging on multiple class classifications. Section 5 presents a bagging experiment for doing sentiment analysis on various LLMs and highlights that there is no real improvement when using larger models. Section 6 discusses why the Condorcet Jury theorem does not hold in this setting, proving a lack of independence among these models and suggesting that more recent models do not perform better in reasoning tasks within sentiment analysis. Section 7 provides concluding remarks and future research directions."}, {"title": "Related works", "content": "The exploration of ensemble methods in natural language processing, specifically through the lens of the Condorcet theorem, has garnered considerable attention in recent literature. This section discusses works closely related to our research and explains our contribution or similarities.\n1. Ensemble Methods in Sentiment Analysis: A significant portion of research in sentiment analysis has advocated for the use of"}, {"title": "Contributions", "content": "This paper seeks to enrich the field of natural language processing (NLP) by leveraging the Condorcet Jury Theorem within the specific context of sentiment analysis. Sentiment analysis requires sophisticated multi-class classification approaches. The key contributions can be summarized as follows:\n1. Theoretical Contribution: We expand the Condorcet Jury Theorem, which to our knowledge has only been proven for binary decisions, to address the more complex scenario of multi-class classification. This extension relies on the new introduced concept of IWTUB set. Doing multi-class is crucial for sentiment analysis as sentiments typically involves categorizing texts into multiple classes beyond simple binary labels, such as positive, negative, and neutral or even potentially more granular sentiments.\n2. Empirical Validation of Model Non Independence: Using a majority classifier with multiple NLP models, including fine-tuned FinBERT, DistilRoBERTa, GPT-3.5, and GPT-4, we found that majority voting did not improve performance. This suggests a lack of independence among the models, as indicated by the IWTUB set's last condition. Unlike traditional statistical tests like Pearson's chi-squared, Spearman's rank correlation, and mutual information, which assess dependencies between dataset variables, the Condorcet Jury Theorem uses voter competence and independence to predict the accuracy of group decisions, focusing on decision-making processes rather than dataset characteristics.\n3. Insights on LLMs: Our results suggest a significant overlap in the decision-making processes for financial sentiment analysis of both compact and advanced models and a potential lack of reasoning for generative LLMs like GPT.\nThe paper's intuition is detailed in figure 1 for enhanced clarity on the overall methodological approach."}, {"title": "Extension of Condorcet jury theorem to multi-class", "content": "The Condorcet Jury theorem [10], was initially proven for a jury that should vote for a yes or no question. It states that if there is a majority preference for one option, then that option should be chosen as the overall winner in a collective decision-making process, provided that individual jury members are better at deciding than a blind guess and that there are independent [25, 32]. Mathematically, if we change individual jury members into machine learning classifiers and assume that each classifier follows the same law, an ensemble majority voting bagging classifier should perform better than any individual classifiers. In this section, we will see how to extend these results to a multi class classification. We will need to assume that classifiers are somehow similar. This is the core assumption that will be challenged in our LLMs experiment.\nDefinition 1. IWTUB Set: We say that a set of $n$ classifiers ${C_1, C_2,..., C_n}$ is an IWTUB (Independent, Well-Trained and Uniformly Biased towards the correct Alternative) Set if all the classifiers satisfy the following four conditions:\n1. Identical Distribution: Classifiers have the same distribution.\n2. Better Than Random: Each classifier $C_i$ has an accuracy better than random guessing over the $k$ possible predicted labels, i.e., for each $C_i$ and for the true label $t$, the probability $P(C_i(x) = t)$ is more than $\\frac{1}{k}$. For binary classification scenarios, this threshold stands at 0.5. In our specific context, where there are three possible classes (positive, indecisive or negative), the threshold adjusts to one-third.\n3. Uniform Distribution Among Incorrect Alternatives: Each classifier $C_i$ has the same probability which is lower than random guessing for choosing the incorrect alternatives, i.e., for each $C_i$ and for any incorrect label $y \\neq t$, the probability $P(C_i (x) = y)$ is less than $\\frac{1}{k}$."}, {"title": "Condorcet Jury Theorem for Classifiers", "content": "Among all the assumptions, the last one (independence of models) is usually quite problematic as there are often correlation in the way models handle data. Under the assumption of IWTUB Set of classifiers, we can extend the Condorcet theorem to multi class classifications problems as follows.\nTheorem 1. Condorcet Jury Theorem for Classifiers: For an IWTUB Set of classifiers $C_1, C_2,..., C_n$, the majority vote classifier $C_{bag}$ exhibits higher accuracy than any individual classifier within the ensemble. When the number of classifiers tends to infinity, the majority classifier converges in probability to the true label. Conversely, if base classifiers perform worse than random guessing, as the number of classifiers tends to infinity, majority classifier will diverge in probability from the true label and hence be worse than the random classifier.\nProof. The proof is an adaptation of the traditional two possible predicted labels presented for instance in [36]. It relies on two arguments. The Khintchine weak law of large numbers is used to establish the convergence in probability of the majority classifier applied to the set of our classifiers to the correct solution. Recall that a majority vote classifier is transitive [22, 21] meaning that changing the order of classifiers does not change the outcome. Recall also that a majority vote classifier is a fair function [16], meaning any true label plays same role and that applying a random permutation on the false labels does not change the result under the assumption of uniform distribution among incorrect alternative.\nA classifier $C_i$ takes some inputs $X$ (in our NLP task some words) and provides an answer $y$ (in our case a numerical sentiment) indicating which label is the true label among possible answers that are integers ranging from 1 to $k$. Because of fairness, there is a symmetry in true labels. Proving the result for one specific label will ensure that the result holds for any other true label. So without loss of generality, given some inputs $X$, we can assume that the true label is the largest label $k$ to make our computation simpler.\nIn this specific case, a classifier would perform better than random guessing if its probability to spot the true label is higher than random guessing. Because of the symmetry of the problem, random guessing probability is $\\frac{1}{k}$. Hence, if we define $a$ the constant, that we also refer in the paper as the advantage of our classifier $C_i$ over randomness, the probability to claim the true label is:\n$P(C_i = k) = \\frac{1}{k} + a$\nwith $a > 0$. Because our classifiers have equal probability of choosing the incorrect alternatives, the alternative probabilities for choosing any incorrect label $l \\neq k$ is given by\n$P(C_i = l) = \\frac{P(C_i \\neq k)}{k-1} = \\frac{\\frac{1}{k}- a}{k-1}$"}, {"title": "Non Uniform Classifiers", "content": "Up to this point, our analysis has assumed that all classifiers are uniformly distributed and equally accurate. We will now relax these assumptions to consider classifiers that vary in performance but are still more effective than random guessing. To explore how these non-uniform classifiers compare to random chance, we refine the concept of Classifier Advantage, which measures the extent to which a classifier outperforms random guessing.\nDefinition 2. Classifier Advantage: Let $t$ be the true label. For a Uniformly Biased Towards the Correct Alternative (UBTCA) classifier $C_i$, we define its positive advantage $a_i$ as its advantage or edge over random guessing:\n$a_i = P(C_i = t) - \\frac{1}{k}$"}, {"title": "Bagging experiment", "content": "In this section, we introduce the results of the experiments made for comparing the state-of-the-art LLMs in financial sentiment classification: FinBERT [3], DistilRoBERTa [37] and GPT-4. We detail the framework and the individual abilities of each models. This experimental section enables us to validate the non-applicability of the Condorcet's theorem in this context and subsequently to question the assumption of models independence.\nWe use a proprietary dataset of headlines generated from Bloomberg news called market wraps that span from 2010 to 2024 and is known to be of high quality as it is scrutinized by market professionals [1]. These headlines are a consolidated summary of daily financial news, done by human journalists specialized in finance, highly regarded and extensively followed by professionals within the financial sector. The database has about 65,000 rows representing on average 15 major headlines per day over 15 years. For each headlines, the database comes with the corresponding next day returns of major equities markets, hence providing a way to systematically provides a news sentiment that is predictive of future impact by definition. We can therefore validate if a model is able to predict effectively a predictive news sentiment. This dataset is containing high-quality data and is closely followed by the market players [14]. The labels are balanced as detailed in table 1. All these arguments make this dataset valuable for fine-tuning and accurately assessing the performance of the considered LLMs on the financial classification task.\nThe dataset we present, as detailed in Table 2, is specifically designed to provide insights into economic events through news headlines, each linked to precise dates and associated sentiment classifications. This arrangement is crucial for analyzing how sentiments expressed in financial headlines correlate with market events and conditions, offering a robust foundation for temporal trend analysis over an extensive period from 2010 to 2024. This allows for longitudinal studies examining the evolution of sentiment in response to global economic, political, and social changes.\nEach row of the dataset contains a sentiment label-Positive, Negative, or Indecisive-systematically assigned by an algorithm, enhancing its utility for financial forecasting. The robustness of this dataset, especially compared to public datasets such as Financial PhraseBank [29] and StockNet Dataset [45], stems from its focus on actionable investment decisions without the noise often introduced by social media content or human annotation, which can be less predictive or non systematic and subjective. In particular, the direct linkage of each sentiment classification to subsequent market performance in this private dataset ensures that the labels reflect predictive market events, as discussed in the literature [40, 5, 44].\nThe primary objective of classifying sentiment in financial headlines is to generate predictive signals for the market. In our analysis, if a Large Language Model (LLM) can accurately classify the sentiment of all stocks mentioned in the headlines, it would yield a valuable trading signal. This capability underscores the dataset's effectiveness in enhancing market outcome predictions."}, {"title": "Models assessment", "content": "At this step, we examine the individual performances of the state-of-the-art models. They have both different architecture and different number of parameters. However they are considered as comparable for this classification task [3, 33].\nTable 3 illustrates that generative models such as those in the GPT series do not outperform encoder-only models based on the BERT architecture, despite GPT's substantially larger number of parameters. This finding challenges the assumed predictive superiority of GPT models over more compact architectures like BERT. However, it is evident that all models significantly surpass the random benchmark, indicating their capacity to learn from the labeled data provided.\nThe extensive Bloomberg dataset utilized in this study facilitates precise fine-tuning of these models. We divided the dataset into two segments: one for training the models and the other for evaluating their performance, as shown in Table 4, which reports the models' results on the test set. It is noteworthy that at the time of writing this paper, GPT-4 could not be fine-tuned, but we were able to fine-tune the latest version of GPT-3.5. Given the similar performance of the non-fine-tuned models, we anticipate comparable results when GPT-4 becomes available for fine-tuning.\nThe fine-tuned models demonstrate no significant differences in performance among themselves; all exhibit uniform improvements. This indicates that the fine-tuning process equally enhances each model, without any single model demonstrating distinct superiority. Thus, while individual models are equally enhanced, none emerges as distinctly superior.\nBased on these performances, we apply an ensemble strategy. Since all the models have comparable performances we could expect"}, {"title": "Discussion", "content": "Our experiments have confirmed that three of the four critical hypotheses necessary for applying Condorcet's theorem hold, namely.\n1. Identical Distribution The confusion matrices in Figure 2 demonstrate that the classification performance distributions across the models are consistent. Both fine-tuned and non-fine-tuned models exhibit comparable results across different classes, with the probability distributions of each LLM being markedly similar.\n2. Better than random As evidenced by the confusion matrices in Figure 2 and performance data in Tables 3 and 4, all LLMs in our study perform significantly better than a random classifier.\n3. Uniform distribution among incorrect alternatives The confusion matrices in Figure 2 indicate that the error distribution among incorrect choices is uniform across models. The similarity in error distribution and the high correlation in the instances where models fail further support this hypothesis.\nSince the three hypothesis hold and are confirmed by the experiments, the last condition of IWTUB set can not hold, namely the independence hypothesis of the LLMs models used in our bagging experiment. In particular, this indicates that GPT models cannot be considered as independent or different models from simpler models like FinBERT and DistilROBERTa. In particular, this suggests that the reasoning capacity of GPT models may not be very different form the smaller models and that they lack the capacity to reason on complex financial tasks like financial sentiment analysis."}, {"title": "Conclusion", "content": "This paper extends the Condorcet Jury Theorem, previously limited to binary decisions, to multi-class classification scenarios. This advancement incorporates the new concept of IWTUB set, introduced to facilitate the extension of the theorem to multi-class classification systems. Our empirical investigation of the applicability of a majority vote classifier revealed that despite the advanced capabilities of LLMs, such as GPT-3.5 or 4, there are only marginal enhancements in predictive accuracy when combined with smaller models. This outcome suggests a significant overlap in the decision-making processes of these models, underscoring their limited independence, which undermines the effectiveness of the majority voting mechanism from the Condorcet Jury Theorem and highlights the lack of reasoning abilities of LLMs for complex NLP task like sentiment analysis. The use of the Condorcet Jury theorem is very complementary to traditional statistical tests for independence, such as Pearson's chi-squared test, Spearman's rank correlation, and mutual information, that only focus on establishing the absence of a relationship between two variables in a dataset as the Condorcet Jury theorem leverages the assumption of voter competence and independence to"}]}