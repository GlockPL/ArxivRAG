{"title": "Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering", "authors": ["Shuzheng Si", "Haozhe Zhao", "Gang Chen", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Kaikai An", "Kangyang Luo", "Chen Qian", "Fanchao Qi", "Baobao Chang", "Maosong Sun"], "abstract": "Training LLMs on data containing unfamiliar knowledge during the instruction tuning stage can encourage hallucinations. To address this challenge, we introduce NOVA, a novel framework designed to identify high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, to ensure the quality of selected samples, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less. Extensive experiments and analysis show that NOVA significantly reduces hallucinations and allows LLMs to maintain a strong ability to follow instructions.", "sections": [{"title": "1 Introduction", "content": "Alignment is a critical procedure to ensure large language models (LLMs) follow user instructions (OpenAI, 2023a; Yang et al., 2024). Despite significant progress in LLM alignment and instruction tuning (Ouyang et al., 2022; Anthropic, 2022), state-of-the-art aligned LLMs still generate statements that appear credible but are actually incorrect, referred to as hallucinations (Ji et al., 2023; Huang et al., 2024). Such hallucinations can undermine the trustworthiness of LLMs in real-world applications (Si et al., 2023b; Min et al., 2023; Rawte et al., 2023; Wei et al., 2024a).\nPrevious studies (Kang et al., 2024; Gekhman et al., 2024; Lin et al., 2024b) indicate that tuning LLMs on instruction data that contains new or unfamiliar knowledge can encourage models to be overconfident and promote hallucinations. In other words, once the knowledge in the instruction data has not been learned during the pre-training stage of LLMs, the fine-tuned LLMs tend to produce more errors when generating responses. Therefore, there is a dilemma in instruction tuning: On the one hand, the LLMs need to learn to follow user instructions during this stage, which is crucial for user interaction in real-world applications (Wang et al., 2023b; Chen et al., 2024b); On the other hand, using high-quality data (whether manually labeled or generated by other advanced LLMs) for instruction tuning can introduce unfamiliar knowledge to LLMs, thereby encouraging hallucinations (Kang et al., 2024; Lin et al., 2024b). Thus, a critical question arises: How can we align LLMs to follow instructions and hallucinate less during the instruction tuning stage?"}, {"title": "2 Related Work", "content": "Hallucinations in LLMs. Hallucinations occur when the generated content from LLMs seems believable but does not match factual or contextual knowledge (Ji et al., 2023; Rawte et al., 2023; Huang et al., 2024). Recent studies (Lin et al., 2024b; Kang et al., 2024; Gekhman et al., 2024) attempt to analyze the causes of hallucinations in LLMs and find that tuning LLMs on data containing unseen knowledge can encourage models to be overconfident, leading to hallucinations. Therefore, recent studies (Lin et al., 2024b; Zhang et al., 2024b; Tian et al., 2024) attempt to apply RL-based methods to teach LLMs to hallucinate less after the instruction tuning stage. However, these methods are inefficient because they require additional corpus and API costs for advanced LLMs. Even worse, such RL-based methods can weaken the instruction-following ability of LLMs (Lin et al., 2024b). In this paper, instead of introducing the inefficient RL stage, we attempt to directly filter out the unfamiliar data during the instruction tuning stage, aligning LLMs to follow instructions and hallucinate less.\nData Filtering for Instruction Tuning. Data are crucial for training neural networks. (Van Engelen and Hoos, 2020; Song et al., 2022; Si et al., 2022, 2023a; Zhao et al., 2024; An et al., 2024; Si et al., 2024a; Cai et al., 2024). According to Zhou et al. (2023), data quality is more important than data quantity in instruction tuning. Therefore,"}, {"title": "3 Methodology", "content": "In this section, we will detail our proposed framework NOVA as shown in Figure 2. Previous studies (Lin et al., 2024b; Kang et al., 2024; Gekhman et al., 2024) find that tuning LLMs on data containing new or unfamiliar knowledge can encourage models to be overconfident and further lead to hallucinations. Inspired by this finding, NOVA aims to filter out the unfamiliar instruction data for the instruction tuning, thereby aligning the LLM to follow instructions and hallucinate less."}, {"title": "3.1 Internal Consistency Probing", "content": "To comprehensively measure the LLM's familiarity with instruction data, the first challenge is to evaluate how well the LLM understands the knowledge within the instructions. Prompting LLMs to generate multiple responses to the same instruction and measuring how consistent those responses are has been proven to be an effective way (Wang et al., 2023a; Chen et al., 2024a). This is because if LLMs understand the question and are confident in their answers, they will produce similar responses. A practical way to measure the consistency of free-form responses is to utilize lexical metrics (e.g., Rouge-L) (Lin et al., 2024c) or sentence-level confidence scores (e.g., perplexity) (Ren et al., 2023). However, these straightforward strategies neglect highly concentrated semantic information within the internal states of LLMs, and thus fail to capture the fine-grained differences between responses.\nHence, we propose Internal Consistency Probing (ICP) to measure the semantic consistency in the dense embedding space. For an instruction data $s = (q, r)$, $q$ denotes the instruction, and $r$ denotes the target response. For instruction $q$, we first sample $K$ responses $[r'_1, ..., r'_K]$ from a base LLM and apply few-shot demonstrations (Lin et al., 2024a) to ensure the coherence of generated responses. For $K$ generated responses, we use the internal states of the last token of each response in the last layer as the final sentence embeddings $E = [e_1, e_2, ..., e_K]$, as it effectively captures the sentence semantics (Azaria and Mitchell, 2023). We further utilize differential entropy (DE) to assess the semantic consistency in continuous embedding space, which is the extension of discrete Shannon entropy:\n$DE(X) = - \\int_{X} f(x) \\log(f(x))dx.$ (1)"}, {"title": "", "content": "We process and treat sentence embeddings $E$ as a multivariate Gaussian Distribution $E \\sim N(\\mu, \\Sigma)$. Then, the differential entropy can be expressed as:\n$DE(E) = \\frac{1}{2} \\log((2\\pi e)^d det(\\Sigma)),$ (2)\nwhere $det(\\Sigma)$ represents the determinant of the covariance matrix $\\Sigma$, $d$ is the dimension of the sentence embedding, and $e$ is the natural constant. $\\Sigma$ denotes the covariance matrix that captures the relationship between $K$ different sentence embeddings, which takes the form:\n$\\Sigma = \\frac{1}{K-1} \\sum_{i=1}^{K} (e_i - \\mu)(e_i - \\mu)^T.$ (3)\nFinally, we measure semantic consistency using $DE(E)$, term as $F_{ins}(q)$ for a given instruction $q$ in data $s$. Also, $DE(E)$ in Eq.(2) simplifies to:\n$F_{ins}(q) = \\frac{1}{2} \\log det(2) + \\frac{d}{2} (log2\\pi + 1) + G,$ (4)\nwhere $\\lambda_i$ denotes the i-th eigenvalue of the covariance matrix $\\Sigma$, which can be easily calculated by singular value decomposition. G is a constant.\nIf the LLM is familiar with the given instruction, the sentence embeddings of generated responses will be highly correlated and the value of $F_{ins}(q)$ will be close to G. On the contrary, when the LLM is indecisive, the model will generate multiple responses with different meanings leading to a significant value of $F_{ins}(q)$. In this way, we can exploit the dense semantic information to effectively measure the LLM's familiarity with the instruction."}, {"title": "3.2 Semantic Equivalence Identification", "content": "Another challenge is to estimate the knowledge in the target response and measure the LLM's familiarity with it, since the target response can contain expert-level and unfamiliar knowledge for the LLM. Training LLMs on such data can encourage hallucinations. Therefore, we propose Semantic Equivalence Identification (SEI) to measure the LLM's familiarity with the target response by calculating how many generated responses are semantically equivalent to the target response. If the target response and more generated responses convey the same meaning, it indicates that the LLM is more familiar with it, thereby training the LLM on this target response will reduce hallucinations.\nAs the target response is manually labeled or derived from advanced LLMs (e.g., GPT-4) instead of generated by the LLM itself, the internal states of the LLM cannot effectively represent the target response. Thus, unlike utilizing internal states as the proposed ICP, we calculate LLM's familiarity with target responses using the proposed semantic clustering strategy. In detail, we first cluster the generated responses that convey the same thing into a semantic cluster. This is because these responses are often free-form, and multiple generated responses can have the same meaning in different ways. Therefore, we employ an off-the-shelf natural language inference (NLI) model to cluster these responses. NLI models are trained to infer the logical entailment between an arbitrary pair of sentences. Thus, NLI models are well-suited to identify semantic equivalence, as two generated responses mean the same thing if you can entail (i.e. logically imply) each from the other (Kuhn et al., 2023; Jung et al., 2024). In this way, we can use an NLI model to consider two responses that can be entailed from each other as semantically equivalent responses. Specifically, we test each pair $(r_i, r'_j)$ of i-th and j-th generated responses as:\n$F_{equivalent}(r_i, r'_j) = I{L_{NLI}(r'_j \\Rightarrow r_i) = L_{entailment} \\wedge L_{NLI}(r_i; \\Rightarrow r'_j) = L_{entailment}},$ (5)\nwhere $L_{NLI}$ represents the predictions of the NLI model, $L_{entailment}$ means the label of entailment relation. $II$ is the indicator function.\nIn this way, we can identify the semantic equivalence of each pair of generated responses and then cluster these generated responses $[r'_1, ..., r'_K]$ into $M$ different semantic clusters $[C_1, ..., C_M]$, where m-th semantic cluster $C_m$ contains $k_m$ generated responses. Each semantic cluster $c$ is a set of generated responses that convey the same thing. We further apply the NLI model to determine which semantic cluster the target response $r$ fits in. Specifically, we use the model to test the target response $r$ and each generated response $r'_i \\in [r'_1, ..., r'_K]$:\n$F_{equivalent}(r, r'_i) = I{L_{NLI}(r \\Rightarrow r'_i) = L_{entailment} \\wedge L_{NLI}(r'_i \\Rightarrow r) = L_{entailment}}.$ (6)\nUsing this method, we can determine how many generated responses in a semantic cluster are semantically equivalent to the target response $r$. For semantic clusters $[C_1, ..., C_M]$, the counts of such generated responses are $[k'_1, k'_2, ..., k'_M]$. We use the votes in each semantic cluster to decide which"}, {"title": "", "content": "cluster the target response belongs to:\n$Index(C_{target}) = arg \\max([\\frac{k'_1}{k_1}, \\frac{k'_2}{k_2}, ..., \\frac{k'_M}{k_M}]).$ (7)\nWe calculate the ratio of the number of responses $k_{target}$ in the target cluster $C_{target}$ to the total number of generated responses as $F_{res}(r)$:\n$F_{res}(r) = \\frac{k_{target}}{\\sum_{m=1}^{M} k_m}$ (8)\nAccording to Eq.(8), when the LLM is familiar with the knowledge within the target response $r$, most of the generated responses will have the same meaning as target response $r$, thus the value of $F_{res}(r)$ will be close to 1. On the contrary, if the target response contains unseen knowledge, i.e., none of the generated responses have the same meaning as it, the value of $F_{res}(r)$ will be close to 0. To this end, we can effectively measure the LLM's familiarity with the target response."}, {"title": "3.3 Ranking, Selecting, and Training", "content": "To comprehensively estimate the knowledge and consider both the LLM's familiarity with the instruction and the target response, we calculate the ratio between $F_{ins}(q)$ and $F_{res}(r)$ for an instruction data $(q, r)$ as the final score:\n$F_{familiarity}(q, r) = \\frac{F_{res}(q)}{F_{ins}(r)}$ (9)\nThis score effectively measures how well the LLM understands the knowledge in instruction data. High $F_{familiarity}$ values indicate that the knowledge in the data aligns well with the LLM, as they show that the generated responses are very consistent for a given instruction (i.e., low $F_{ins}(q)$ values) and the generated responses are very semantically similar to the target response (i.e., high $F_{res}(r)$ values). Based on the principle of filtering unfamiliar instruction data, the data with high $F_{familiarity}$ should be selected to train the LLM.\nHowever, our early experiments observed that selecting instruction data solely based on the LLM's familiarity $F_{familiarity}$ significantly reduces hallucinations but hinders the model's ability to follow instructions. This is because considering only familiarity ignores other important characteristics of instruction data, e.g., the complexity of the instruction and the fluency of the response. Therefore,"}, {"title": "", "content": "we further introduce an expert-aligned quality reward model to measure the data quality. We use an expert-labeled preference dataset (Liu et al., 2024b) which contains 3,751 instruction data to train a reward model (more details are shown in Appendix B). To take both familiarity $F_{familiarity}(q, r)$ and quality $F_{quality}(q, r)$ into consideration, we define the mixed rank $R^{(i)}_{final}$ for i-th data as the average of the two ranks corresponding to the two metrics:\n$R^{(i)}_{final} = \\frac{1}{2} (R^{(i)}_{familiarity} + R^{(i)}_{quality}),$ (10)\nwhere $R^{(i)}_{familiarity}$ and $R^{(i)}_{quality}$ refer to the ranks of the i-th data point in the degree of familiarity and quality. In this way, we can effectively consider data quality and avoid unfamiliar data.\nFinally, we rank all the instruction data with their corresponding mixed rank $R^{final}$ to select the top-ranked data, e.g., selecting the top 5% data to apply the supervised finetuning on the LLM. Based on the proposed NOVA, we can use the suitable data to effectively align LLMs to follow instructions and hallucinate less during the instruction tuning stage."}, {"title": "4 Experiment", "content": "In this section, we conduct experiments and provide analyses to justify the effectiveness of NOVA."}, {"title": "4.1 Setup", "content": "Instruction Dataset. We conduct instruction tuning with two different instruction datasets. Alpaca (Taori et al., 2023) contains 52,002 samples that are created by employing Text-Davinci-003 model (Ouyang et al., 2022) and Self-instruct framework (Wang et al., 2023c). Alpaca-GPT4 (Peng et al., 2023) further employs more powerful GPT-4 (OpenAI, 2023b) to get high-quality instruction data.\nEvaluation. To evaluate our method comprehensively, we select widely adopted benchmarks for the targeted abilities. (1) Factuality hallucination benchmark: BioGEN (Min et al., 2023) and LongFact (Wei et al., 2024b); (2) Faithfulness hallucination benchmark: FollowRAG-Faithfulness (Dong et al., 2024), including 4 different QA datasets; (3) Instruction-following benchmark: MT-Bench (Zheng et al., 2023) and FollowRAG-Instruction. Comprehensive descriptions of tasks, datasets, and evaluation metrics are detailed in Appendix A.\nBaselines. We compare several strong baselines, including (1) Vanilla Instruction Tuning: Vanilla"}, {"title": "4.2 Main Results", "content": "NOVA Significantly Reduces Hallucinations. As shown in Table 1, NOVA shows consistent and significant improvements on three hallucination benchmarks measuring factuality and faithfulness. Compared to indiscriminately using the whole instruction dataset (i.e., Vanilla - 100%), using samples selected by NOVA to train LLMs can improve 3.5-8.6% on BioGEN, 2.0-5.1% on LongFact, and 4.9-8.5% on FollowRAG-Faithfulness. This is because NOVA effectively filters out the unfamiliar instruction data and avoids training LLMs on these data thereby reducing the hallucinations. Compared to instruction data filtering methods that focus on data quality, like IFD, our method consistently improves the performance across different selected sample ratios (5-15%) on three benchmarks. Meanwhile, these data selected by quality-focused methods may present unfamiliar knowledge to the LLM and encourage hallucinations on LongFact. On the contrary, NOVA aims to identify the samples that align well with LLM's knowledge, helping the LLM to hallucinate less. NOVA also achieves better performance than RL-based methods without introducing additional preference data. These findings underline the effectiveness of our method in aligning LLMs to hallucinate less.\nNOVA Maintains a Good Balance between Following Instructions and Reducing Hallucinations. As shown in Table 2, NOVA achieves a better instruction-following ability compared to vanilla tuning methods, especially when the LLM is trained on Alpaca. It shows that NOVA can effectively align LLMs to follow instructions. In some cases, our method surpasses data filtering methods that enhance instruction-following ability, demonstrating its effectiveness in identifying suitable data for LLMs. Unlike RL-based methods that weaken the model's instruction-following ability, our method shows superior instruction-following ability while greatly reducing hallucinations.\nNOVA Mitigates Overconfidence Phenomenon. We select 15 samples with the lowest scores for each model from LongFact-Objects and calculate"}, {"title": "4.3 Analysis", "content": "Ablation Study. We conduct the ablation study in Table 3. We can find that the proposed ICP and SEI can both help LLMs to reduce hallucinations. Also, considering only familiarity (i.e., -w/o. Quality RM) ignores other important characteristics of instruction data and limits the instruction-following ability of LLMs. Thus, even if considering familiarity alone would greatly reduce hallucinations, it is still necessary to introduce a quality reward model to maintain a good balance between following instructions and reducing hallucinations.\nScalability Study. We implement our method on the LLaMA-3-70B in Table 3 to explore whether NOVA can fit in larger LLMs. Results indicate that NOVA scales effectively to larger-scale models.\nCase Study. We conduct a case study in Table 4 to visually show the advantages of NOVA. Compared to using the whole training data, our method ensures the statements are correct and comprehensive, and the generated text is fluent and natural."}, {"title": "5 Conclusion", "content": "In this paper, we introduce NOVA, a novel framework designed to identify high-quality data that aligns well with the LLM's learned knowledge to reduce hallucination. NOVA includes Internal Consistency Probing and Semantic Equivalence Identification, which are designed to separately measure the LLM's familiarity with the given instruction and target response, then prevent the model from being trained on unfamiliar data, thereby reducing hallucinations. Lastly, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity to enhance data quality. By considering data quality and avoiding unfamiliar data, we can use the selected data to effectively align LLMs to follow instructions and hallucinate less in the instruction tuning stage. Experiments and analysis show the effectiveness of NOVA."}, {"title": "Limitations", "content": "Although empirical experiments have confirmed the effectiveness of the proposed NOVA, two major limitations remain. Firstly, our proposed method requires LLMs to generate multiple responses for the given instruction, which introduces additional execution time. However, it is worth noting that this additional execution time is used to perform offline data filtering, our proposed method does not introduce additional time overhead in the inference phase. Additionally, NOVA is primarily used for single-turn instruction data filtering, thus exploring its application in multi-turn scenarios presents an attractive direction for future research."}, {"title": "Appendix", "content": "In this section, we will detail the benchmarks and evaluation metrics."}, {"title": "A Evaluation", "content": "BioGEN. (Factuality) This benchmark requires generating short biographies for particular people entities, with a total of 500 samples. The task of generating people biographies is effective, because generations consist of verifiable statements rather than debatable or subjective ones, and the scope is broad (i.e., covering diverse nationalities, professions, and levels of rarity). To evaluate each generated response, we follow the FactScore procedure to extract the number of correct and incorrect facts. Following Min et al. (2023), we first employ GPT-3.5-Turbo-0125 to break a generation into a series of atomic facts and utilize GPT-3.5-Turbo-0125 to compute the percentage of atomic facts supported by a reliable knowledge source. The percentage of the correct statements (% FactScore), the number of generated statements (# Facts), and the ratio of generations that do not abstain from responding (% Respond) are adopted as the evaluation metrics.\nLongFact. (Factuality) LongFact requests detailed descriptions for a queried entity and expects a document-level response that is typically very long, often exceeding a thousand tokens. Specifically, LongFact consists of two subtasks: LongFact-Concepts and LongFact-Objects, separated based on whether the questions ask about concepts or objects. Following Cheng et al. (2024), we use 120 samples of each task for evaluation. The evaluation process is similar to BioGEN. We employ GPT-3.5-Turbo-0125 and report the FactScore of LongFact-Concepts and LongFact-Objects, termed as % Concepts and % Objects.\nFollowRAG. (Faithfulness and Instruction Following) FollowRAG aims to assess the model's ability to follow user instructions in complex multi-document contexts, covering 22 fine-grained atomic instructions across 6 categories. The queries in FollowRAG are sourced from 4 QA datasets across NaturalQA (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and WebQSP (tau Yih et al., 2016). It collects and verifies definitions and examples of atomic instructions using rules (e.g., code), excluding those irrelevant to retrieval-augmented generation (RAG) scenarios. FollowRAG identifies 22 types of instruction constraints, encompassing language, length, structure, and keywords. Thus, it is suitable to use FollowRAG to evaluate the model's ability to follow user instructions. Utilizing the verifiable nature of designed atomic instructions, FollowRAG automates the verification of the model's adherence to each instruction through code validation. We calculate the average pass rate for each atomic instruction across all samples to determine the instruction-following score and name this task as FollowRAG-Intruction. Also, FollowRAG provides retrieved passages as contextual information to evaluate the model's faithfulness. We name this task as FollowRAG-Faithfulness. Under new instruction constraints, the model's target output differs from the gold answers in the original QA dataset, rendering traditional metrics like EM ineffective. Following Dong et al. (2024), we use the original gold answers as a reference and utilize GPT-40-2024-05-13 to evaluate whether the model's outputs address the questions. The scoring criteria are as follows: Completely correct (1 point), Partially correct (0.5 points), Completely incorrect (0 points). The average score of all samples is taken as the final score for FollowRAG-Faithfulness.\nMT-Bench. (Instruction Following) MT-Bench is a benchmark consisting of 80 questions, designed to test instruction-following ability, covering common use cases and challenging questions. It is also carefully constructed to differentiate chatbots based on their core capabilities, including writing, role-play, extraction, reasoning, math, coding, STEM knowledge, and social science. For evaluation, MT-Bench prompts GPT-4 to act as judges and assess the quality of the models' responses. For each turn, GPT-4 will give a score on a scale of 10. Notably, since we only fine-tune on single-turn instruction data (e.g., Alpaca and Alpaca-GPT4), the evaluation is restricted to Turn 1 of MTBench, similar to previous studies (Li et al., 2024b)."}, {"title": "B Implementation Details", "content": "Hyperparameters and Devices. We use Adam optimizer (Kingma and Ba, 2017) to train our model, with a 2 \u00d7 10-5 learning rate and a batch size of 16, steers the training across three epochs. We set the maximum input length for the models to 1024. To get the generated initial responses for knowledge estimation, we set the temperature as 0.7 and set hyperparameter K as 10 to generate 10 responses for the given instruction q. We conduct"}, {"title": "C Parameter Study", "content": "We explore the effects of two important hyperparameters in our method: the number of generated responses K and the temperature T during the response generation. As shown in Figure 5, increasing the number of generated responses improves"}, {"title": "D Transferability Study", "content": "To verify the transferability of the NOVA method, we conducted experiments on different foundation models using the Alpaca instruction dataset shown in Table 7 and Table 8. We select LLaMA (Touvron et al., 2023) and Qwen-2 (Yang et al., 2024) at the 7B size as the new base models. We aim to gain deeper insights into the applicability of the NOVA method across different models, providing a reference for further research and applications. We find that the NOVA method is also applicable to other models, showing strong transferability and robustness to other models and further research. Compared to other baselines, NOVA significantly reduces hallucinations and keeps a strong ability to follow instructions."}, {"title": "E Design Exploration", "content": "The Design of NLI Model We further explore the effects of the NLI model on the final performance of NOVA. We first attempt to analyze the"}, {"title": "F Human Evaluation", "content": "During the human evaluation, the participants follow the principles in Figure 7 to make the decision. For each comparison, three options are given (Ours Wins, Tie, and Vanilla Fine-tuning Wins) and the majority voting determines the final result. We invite three Ph.D. students to compare the responses"}, {"title": "G Case Study for Selected Samples", "content": "To evaluate our proposed NOVA qualitatively, we also select some instruction samples from the Alpaca dataset for case studies as shown in Figure 8. Firstly, we can find that simply using $R_{familiarity}$ in Eq. (10) can effectively identify the simple and straightforward instruction samples that align well with LLM's knowledge. On the contrary, the sample ranked last according to $R_{familiarity}$ contains the open-ended instruction and the very subjective target response. Meanwhile, further using $R_{final}$ in Eq. (10) that considers characteristics beyond just familiarity $R_{familiarity}$ by introducing the quality reward model further enhances data quality, e.g., the complexity of instructions. We can also observe that the sample ranked last according to $R_{final}$ also contains the open-ended instruction and the detailed target response, which may introduce familiar knowledge to LLMs and further promote hallucinations."}]}