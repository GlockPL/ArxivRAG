{"title": "A Comparative Analysis of DNN-based White-Box Explainable AI Methods in Network Security", "authors": ["Osvaldo Arreche", "Mustafa Abdallah"], "abstract": "New research focuses on creating artificial intelligence (AI) solutions for network intrusion detection systems (NIDS), drawing its inspiration from the ever-growing number of intrusions on networked systems, increasing its complexity and intelligibility. Hence, the use of explainable AI (XAI) techniques in real-world intrusion detection systems comes from the requirement to comprehend and elucidate black-box AI models to security analysts. In an effort to meet such requirements, this paper focuses on applying and evaluating White-Box XAI techniques (particularly LRP, IG, and DeepLift) for NIDS via an end-to-end framework for neural network models, using three widely used network intrusion datasets (NSL-KDD, CICIDS-2017, and RoEduNet-SIMARGL2021), assessing its global and local scopes, and examining six distinct assessment measures (descriptive accuracy, sparsity, stability, robustness, efficiency, and completeness). We also compare the performance of white-box XAI methods with black-box XAI methods. The results show that using White-box XAI techniques scores high in robustness and completeness, which are crucial metrics for IDS. Moreover, the source codes for the programs developed for our XAI evaluation framework are available to be improved and used by the research community.", "sections": [{"title": "1 Introduction", "content": "The development of AI brought the advancements of automation to intrusion detection systems (IDS) [1, 2]. A few examples of such endeavor are decision trees (DT) [3, 4], random forests (RF), support vector machines (SVM), and naive bayes (NB) [5, 6]. Except for decision trees for basic IDS, the security analyst in charge often lacks explanations about the Al's decisions [7, 8]. More specifically, these earlier studies using AI focused more on the accuracy of categorization of various AI systems without providing insight into their reasoning or behavior. This limitation raises the acute need to take advantage of the new field of explainable AI (XAI) to enhance the comprehensibility of AI decisions in IDS [9]. Several recent studies (e.g., [10-14]) have begun investigating the use of XAI for IDS. For instance, a human-in-the-loop method to intelligent IDS was offered in an abstract design by the work [13]. In another example, the NSL-KDD benchmark dataset [15] was the only tool utilized by the work [10] to extract decision rules, using simply a straightforward decision tree algorithm. Differently, Local interpretable model-agnostic explanations (LIME) [16] were included by the work [11] to only give local explanations for a single AI model (SVM) for the utilized IDS. In a distinct framing, The CIA principle (\u201cC\u201d for confidentiality of information, \"I\" for the integrity of data, and \"A\" for availability) was applied in the work [12] to improve the explainability of the AI model on the CICIDS-2017 benchmark intrusion dataset. In essence, these efforts lack a comprehensive metric evaluation of XAI techniques in a diverse set of intrusion datasets.\nThe main challenge of implementing XAI tools into network intrusion detection is validating such tools, testing their quality, and evaluating the pertinent security metrics. This step is paramount to build confidence in applying them in practical settings, considering that they will eventually be widely spread in network security to help security analysts be more effective and accurate in their tasks.\nDiving deeper into XAI, a concept adopted in this work presented on [17] that divides X\u0391\u0399 into black-box and white-box methods. The first refers to the methods that do not have access to the internal parameters and architecture of the AI models. Therefore, black-box XAI methods rely solely on approximating the model's behavior by observing the relationship between input and output. These characteristics make the black-box \u03a7\u0391\u0399 methods versatile and agnostic, which applies in several AI models (e.g., LIME [18] and SHAP [19]. In contrast, white-box \u03a7\u0391\u0399 methods (i.e., the focus of this current study) are deemed transparent because of their access to the model's parameters and architecture. The white-box methods use the knowledge and access to the model to generate explanations, which is advantageous because it is not an approximation but the model itself. This difference makes them more reliable and accurate, providing more insights when reasoning their decisions, with the drawback of being model-specific. The white-box methods analyzed in this article are used in tandem with neural networks, including Integrated Gradients (IG) [20], Layer-wise Relevance Propagation (LRP) [21], and DeepLift [22-24].\nThis research intends to evaluate white-box XAI techniques for network intrusion detection systems by proposing a new XAI framework based on the previous works [17, 25]. The metrics proposed by [17] are modified for network intrusion detection systems and used to assess \u03a7\u0391\u0399 algorithms. The measures in question include completeness, efficiency, stability, robustness, descriptive accuracy, and sparsity. Here is a definition of these different metrics (or measurements).\n\u2022 Descriptive Accuracy: This metric disregards the top features when training and testing the AI model to evaluate if the disregarded feature affected the model's accuracy. For example, if a feature that detects intrusions is significant, removing that feature should reduce the accuracy of the AI model's intrusion prediction. Hence, a significant accuracy loss indicates that such features have a high XAI explainability power.\n\u2022 Sparsity: The sparsity gauges the relationship between the features' significance scores, which is quantified by comparing the number of features that fall below a threshold. For instance, if eight out of 10 features fall below a small threshold (around zero), it indicates that two features have a strong impact on the model's judgment according to the scores assigned by the \u03a7\u0391\u0399, indicating a high level of sparsity. This could help security analysts to narrow their search when monitoring network traffic.\n\u2022 Efficiency: The time it takes for an \u03a7\u0391\u0399 method to generate an explanation is used to measure its efficiency. Since it is more practical to have explanations created rapidly rather than slowly, this statistic is significant since it assesses how well the XAI technique applies to real-world systems. Since assisting security analysts is the ultimate objective, the system should be able to produce precise XAI explanations quickly.\n\u2022 Stability: The XAI method's consistency in producing explanations is gauged by the stability metric, which is quantified by examining the number of overlapped top features between several running experiments conducted under identical circumstances (i.e., to check if the produced explanation by the XAI method is the same for the same sample). An XAI approach with higher stability can be trusted more by the security analyst.\n\u2022 Robustness: The capacity of an XAI approach to producing the same explanations even when the incursion traits are perturbed is known as its robustness, even when an adversarial assault or computational faults may be the cause of this disturbance. In this experiment, the adversarial model from the work [26] is modified to work with the IDS datasets considered in our current work. In short, to create this adversarial model, one highly biased model is trained by being heavily biased in one feature for prediction, and another model with all the features plus a new feature designed to perturb (i.e., with random values) the XAI explanation technique is trained. The XAI approach under test will produce explanations based on these two trained models. The attack consists of displaying the adversarial explanation to the security analyst. For example, this might compromise the framework's integrity and pass off an attack as regular network traffic by severing the explanation from the underlying behavior.\n\u2022 Completeness: The capacity of an \u03a7\u0391\u0399 approach to correctly explain every potential network traffic sample, including corner instances, is referred to as its completeness feature. An incomplete XAI approach makes it easier for hackers to take advantage of and fool the system into producing corrupted output. Another important distinction about the nature of Completeness when comparing white-box and black-box \u03a7\u0391\u0399 methods is the inherent completeness of white-box methods [17] due to its access to the model's innards. Its access allows the white-box XAI to fulfill the requirement of valid explanations without testing all the samples individually (i.e., black-box \u03a7\u0391\u0399 methods need to do this to check their completeness) since the model is known and it is taken into consideration. Hence, for a white-box method, a wrong explanation would imply something inappropriate with the model, while the same case cannot be determined for a black-box XAI case (i.e., it could be the model or the XAI approximation method).\nWe stress that an XAI approach automatically becomes more resilient if it is complete (i.e., it can determine whether the explanation is valid, increasing the end user's trust in it). In summary, the robustness metric in this study measures how resilient an XAI system is to an adversarial assault, while the completeness metric verifies that each sample has a legitimate explanation.\nWe investigate these aforementioned six assessment measures on three widely used white-box XAI techniques, which are IG [20], DeepLift [22-24], and LRP [21].\nLRP [21] works by assigning relevance scores to each input feature or a neuron which indicates its contribution of the predictions of deep neural networks. The goal of LRP is to assign relevance scores to each input feature or neuron in the network by recursively using back-propagation, indicating its contribution to the output prediction. Such contribution obeys propagation rules guaranteeing the sum of the scores are conserved [21]. Similarly, DeepLift follows the same conservation principle of LRP, but it enhances the method by adding a new axiom, which dictates how to distribute the relevance scores (i.e., feature importance scores) [22-24].\nIntegrated Gradients (IG) works by obeying two principles. Sensitivity and Implementation Invariance. The first takes into consideration the contribution of each feature to the outcome, in other words, if such a feature contributes to the outcome it should have a non-zero contribution. The second principle, Implementation Invariance, premises the attribution method should not take into account models' particularities, meaning that two different network models that yield the same result considering the same input should have the same contributions. Consequentially, it achieves both principles by taking into consideration the model's gradients considering each input feature and multiplying them by the difference between the resulting output and a baseline [20].\nFurthermore, we detail the sequential steps taken to construct our white-box \u03a7\u0391\u0399 assessment measures and their outcomes for each of our six evaluation metrics (i.e., descriptive accuracy, sparsity, efficiency, stability, completeness, and robustness), utilizing three distinct network intrusion datasets. The first dataset was gathered from the SIMARGL project (funded by the European Union) and is the most recent RoEduNet-SIMARGL2021 dataset [27]. The dataset is invaluable for network intrusion detection systems since it includes characteristics computed from live traffic in addition to realistic network traffic. The second dataset, CICIDS-2017 [28], is a benchmark intrusion detection dataset with varying attack patterns that was developed in 2017 by the University of Brunswick's Canadian Institute for Cybersecurity. The NSL-KDD dataset [15] is the last dataset and is well-known for network intrusion detection systems widely applied as a benchmark.\nWe assess the three white-box XAI techniques (LRP, IG, and DeepLift) for the DNN model for each dataset, and we discuss the findings of the six assessment metrics produced for each \u03a7\u0391\u0399 technique, by using assessment and comparison metrics to validate these XAI approaches, our study improves in the use of white-box XAI methods for network intrusion detection systems. These metrics cover AI model qualities such as descriptive accuracy, sparsity, and explainability, as well as network security needs like stability, robustness, and efficiency. As a result, our framework enables the integration of XAI into network intrusion detection systems, opening the door to increased development in this critical field of study.\nCompendium of Contributions: Below is a summary of our principal contributions.\n\u2022 We propose a comprehensive approach for assessing local and global white-box XAI methods in the context of network intrusion detection.\n\u2022 We examine six distinct assessment measures for DeepLift, IG, and LRP, three well-known white-box \u03a7\u0391\u0399 approaches.\n\u2022 We test our \u03a7\u0391\u0399 assessment framework on Tensorflow-based Deep Neural Network AI models and three network intrusion datasets.\n\u2022 We make our source codes available so the community may use them for XAI assessment frameworks for network intrusion detection and expand upon them with additional datasets and models."}, {"title": "2 Related Work", "content": "Existing Efforts in Leveraging \u03a7\u0391\u0399 for IDS: The survey [35] provides an overview of current attempts to create a framework for developing Explainable AI-based Intrusion Detection Systems (XAI-IDS) and proposes a standard taxonomy. It discusses the differences between opaque black-box and transparent white-box models, examining the trade-offs between performance and explainability. In AI, black-box models (e.g., Neural Networks, and Random Forest) are difficult to interpret, while white-box AI models (e.g., Logistic Regression) are easier to explain but often perform poorly. Additionally, there are black-box and white-box XAI techniques [17]. Black-box XAI methods, like SHAP and LIME, do not access the model's parameters or architecture, whereas white-box methods, such as LRP, IG, and DeepLift, do.\nFurthermore, surveys [36-38] gather scattered data and improvements across different proposals for explaining black-box AI models. It can be divided into two themes of general challenges and future directions, for instance taking into consideration the user experience, formalism, or model assumptions, and research directions/challenges of XAI based on AI life cycle's phases (e.g., specific situations that might occur into the conception, developing or deploying phase). Also, this work analyzes its application in the medical area and proposes a further investigation into other areas such as IoT. In the same direction, [39] analyzes \u03a7\u0391I to make them more understandable and transparent, proposing a taxonomy of transparent models and post-hoc techniques for deep learning models. Moreover, it discusses the need for standardized metrics, addressing issues related to robustness, confidentiality, accountability, privacy, and its evolving regulatory demands. Plus, the DARPA XAI program [40] focuses on evaluating new techniques and exploring modified deep neural networks, causal models, and induction techniques with the goal of creating interpretable AI models and navigating the trade-off between explainability and performance. Lastly, the work [41] focuses on XAI medical application, it explores an in-depth explanation considering different works of machine learning models in the field considering accountability and ethical considerations. It also explores that the explanations can be tampered leading to unreliable explanations.\nSome recently published papers initiated the application and analysis for XAI in the field of IDS [10-13]. In general, the survey [35] proposes a new XAI framework for IDS and gathers most of the studies related to the field. It is divided into phases (i.e., pre-processing, modeling, and explainability). The first converts the input data into better quality, the second generates the explanations, and the later customizes the dashboard according to the target user. Other examples include [12], which proposed an explainable AI model using CICIDS-2017 in tandem with the CIA principles, [10] used DT to understand the IDS rules applied to the NSL-KDD dataset, and the work [11] which provided explanations locally using LIME combined with SVM in IDS.\nPlus, the works [33, 34] use a DNN model in the context of anomalous traffic (i.e, normal or suspicious traffic) in tandem with NSL-KDD and UNSW-NB15 datasets to propose an XAI framework focused on IoT. It leverages the explanations from RuleFit, SHAP, and LIME, to achieve trust while generating the reason behind black-box AI decisions in order to aid security users. In a different light, the research in this paper proposes an XAI framework that uses DNN models, for the multiclassification case (i.e., different attacks). Such framework employs IG, LRP, and DeepLift to explain black-box AI to security analysts, and it evaluates the explanation quality in the light of six novel metrics [17].\nThere are also other significant works in the field. [14] developed a framework called Open World Anomaly Detection (OWAD) for dealing with drifting in deep learning models. Meanwhile, DeepAID [29] tackled some shortcomings in unsupervised Deep Learning binary detection systems. Kitsune [30] developed an anomaly detection tool for camera network systems. In summary, these mentioned works are in Table 1, which compares them with this current work."}, {"title": "3 The Problem Statement", "content": "This section lays the foundation necessary for this research, before discussing its impacts. The topics are the shortcomings of black-box AI, hence the reason for the need for XAI, and lastly the evaluation of such methods applied to network IDS together with its challenges."}, {"title": "3.1 Network Intrusion Types", "content": "Network intrusion attacks come in several types. This study uses the MITRE ATT&CK framework [42], dividing the intrusions in:\n[MITRE ATT&CK ID: T1595] Network Scanning / Probe Attack: This class of assaults comprehends probing and gathering information for a network in order to find vulnerable spots [43]. However, not all probe attacks are considered port scan attacks. For instance, ping sweeps, and DNS zone transfers are other methods in this category [44, 45].\n[MITRE ATT&CK ID: TA0001, T1110, T1078] Remote to Local Attack (R2L): This intrusion comprehend initial access. In other words, an attacker without permission on a network acquires access to a machine or network. From there, the agent can acquire more privileges to expand its access over the network or systems, in other words, U2R.\n[MITRE ATT&CK ID: TA0004, T1078] User to Root Attack (U2R): This consists of an agent exploiting the machine accesses until it achieves control of the root. Such an attack jeopardizes the entire system since the attacker would have administrator privileges. Nonetheless, the agent would still need to perform R2L to gain its initial access.\n[MITRE ATT&CK ID: T1046] Network Service Discovery / PortScan (PS): This category is about an agent surveying spots that can be vulnerable to attacks in the targeted network. Its modus operandi is connecting to various ports without ever finalizing the communication channel. Then, it uses its acquired knowledge to create a mapping of the network to potential vectors of attack [46].\n[MITRE ATT&CK_ID: T1498] Denial of Service (DoS) / Network Denial of Service: In this case the agent overwhelms the victim with requests to connect with the aim of making the network unavailable. Such attacks usually consume the victim's memory rendering it temporarily unavailable [47, 48].\n[MITRE ATT&CK_ID: T1110] Brute Force: This assault is a direct approach to guessing the password of the target. It usually uses a list of commonly used passwords [47].\n[MITRE ATT&CK ID: TA0001, T1659, T1189] Web Attack / Initial Access: The category embodies gaining access to a network or container by exploiting web weak spots. For instance, a malicious agent could use a bug/glitch or an overlooked configuration to take advantage of a public application. Another example includes Drive-by Compromise [49], which, usually, does not serve as a remote server access entry point [42].\n[MITRE ATT&CK ID: TA0001] Initial Access / Infiltration: It includes the use of techniques (e.g., spear-phishing or web-server weakness exploitation) to obtain access to a network. It can vary from a password swap to the capture of a valid account or other services.\n[MITRE ATT&CK ID: T1584.005, T1059, T1036, T1070] Botnet / Compromise Infrastructure : It comprises, often, remote attacks that are executed by compromised devices using bots or scripts that could mimic the behavior of a human. It is a powerful, common, and scale-able technique that can aim for different attack vectors at the same time [50].\nRegular traffic: It comprises all other traffic that is not an attack, therefore regular or benign network traffic."}, {"title": "3.2 Intrusion Detection Systems", "content": "The malicious agents' arms race against cybersecurity sprouted evolving threats to invaluable infrastructure [51, 52]. As a response, IDS became increasingly critical to containing and avoiding these assaults (e.g., internal and external) against networks [53]. In a traditional sense, IDS assumes a signature from attacks, and if such a pattern matches it flags it as suspicious traffic to be investigated [54]. In addition, with the evolving AI over the last years, it became the norm to incorporate AI into the IDS to enhance its capabilities [55]."}, {"title": "3.3 Black-box AI Models and its caveats", "content": "The aforementioned arms race increased the complexity of the competition, and now complex, yet efficient, black-box AI models are employed to defend and flag suspicious traffic. Nonetheless, a side-effect of black-box AI models is to humanly understand it and to extract how it concluded because of the complicated relations it can abstract from features and scenarios. This challenge is a common black-box issue across many models (e.g., DNN, RF, and others) and it is aggravated when they predict an outcome that is not correct. This becomes a sensitive issue since network security is heavily based on trust in the system. Hence, it is paramount to provide valuable explanations behind the AI rationale for IDS [56]."}, {"title": "3.4 Main Categories of Explainable \u0391\u0399 (\u03a7\u0391\u0399)", "content": "XAI is a new field of research created to tackle the explainability issue behind complex AI models. It embodies analyzing the features and other attributes to understand the contribution of a sample responsible for an outcome, either a regression or classification. Usually, XAI is categorized into the following: (1) \u03a7\u0391\u0399 methods able to explain local instances (i.e., one sample), global instances (i.e., many samples), or both. (2) model-agnostic techniques (that can explain any \u0391\u0399 model) or model-specific techniques (tailored for specific AI models). According to [17], black-box methods infer AI model behavior from the input x and output f(x) without knowledge of model parameters or architecture. This method is useful for auditing or experimenting with systems without model access, but it lacks valuable model information. Conversely, white-box methods have access to model architecture and parameters, avoiding the drawbacks of black-box methods but requiring prior model access and being tied to specific models (e.g., IG and LRP for Neural Networks). SHAP, however, is model-agnostic. (3) XAI methods can also be divided by Post-hoc and Ante-hoc (i.e., intrinsic), according to [57]. Ante-hoc methods provide explanations directly from the model without additional steps after training. For example, small Decision Trees are easy to understand and explain without any further techniques besides analyzing the tree and its rules. In contrast, post-hoc XAI methods, which are the focus of this article, are applied after training to extract further understanding and explainability from black-box AI models [57]."}, {"title": "3.5 Benefits of \u03a7\u0391\u0399 for Network IDS", "content": "The lack of interpretability scenario is a challenge for human analysts, as usually, they might analyze a long number of logs to explain suspicious behaviors. Besides, a common scenario is the inability to exactly where the AI model can be improved. Plus, the situation is aggravated in the realm of NIDS, since both aspects are invaluable (i.e., clear explanation and high accuracy) to build trust in the system's capabilities. Therefore, \u03a7\u0391\u0399 systems or frameworks contribute with a crucial aspect to enable the clarification of the AI rationale, which facilitates the investigations, making the analysts more efficient."}, {"title": "3.6 Challenges of XAI for IDS and Need for Evaluating \u03a7\u0391\u0399", "content": "There are challenges to applying XAI to NIDS. Accordingly, to the metrics cited in [17, 25], the white-box and black-box XAI techniques do not fulfill all the requirements to be employed in an IDS system, they often lack performance in some key areas. Nonetheless, Section 6 analyses the aforementioned metrics, and it demonstrates that white-box XAI methods are complete by default and more robust than black-box methods. Next is a summary of the challenges:\n\u2022 Not Transparent: Often, the IDS have black-box AI models, which brings the challenge of understanding and explaining them. Consequentially, it impairs the work of security analysts when they proceed with auditory actions and even guard their system against threats.\n\u2022 \u03a7\u0391\u0399 Application in Network Security is still Limited: Network intrusion detection poses unique challenges for \u03a7\u0391\u0399 [58], which entails a need to improve their interpretation techniques for the field. Differently from other areas (e.g., Computer vision and text analysis), XAI is not yet fully consolidated in NIDS.\n\u2022 Comparison and Evaluation: Since XAI in NIDS is not yet consolidated, it brings a need for criteria and metrics to evaluate them [59]. These need to embody the requirements of securing networks, including efficiency, robustness, stability, and reliability while keeping the requirements for AI properties, which are transparency, accuracy, and explainability.\n\u2022 Robustness and Accuracy: XAI methods for NIDS need to display valid and robust explanations, while being efficient and accurate at the same time. These conditions produce the reliability and trust of XAI methods, which ensures their application in NIDS.\nHaving defined prior related works and main challenges that motivate the need of evaluating XAI, we next detail our proposed evaluation framework of white-box \u03a7\u0391\u0399 methods."}, {"title": "4 Framework", "content": "This paper aims to evaluate a newly white-box XAI framework applied to NIDS using the proposed XAI metrics. Besides, this work contributes to advancing XAI into NIDS, helping security users obtain in-depth abstractions of network traffic by extracting the white-box XAI evaluation and enhancing their trust through the evaluated metrics. Consequentially, it would augment their capability to avoid different network attacks and suspicious scenarios. The framework can be divided as follows."}, {"title": "4.1 Overview of the \u03a7\u0391\u0399 Evaluation Framework", "content": "The proposed framework starts by preparing the invaluable NIDS datasets (i.e., CICIDS-2017, NSL-KDD, RoEduNet-SIMARGL2021). Next, after the data is properly loaded and pre-processed, it is forwarded to the respective DNN model to train and test over several intrusions (i.e., Multi-class classification problem). The classes include Port Scanning, Brute Force, R2L, and others. Later, the neural network itself and predictions are moved towards the white-box \u03a7\u0391\u0399 methods, namely LRP, IG, and DeepLift to generate the desired explanations. The last part includes evaluating them in the optics of the evaluation metrics (i.e., Robustness, Completeness, Stability, Sparsity, Descriptive Accuracy, and Efficiency)."}, {"title": "4.2 In-Depth \u03a7\u0391\u0399 Evaluation Pipeline Components", "content": "We now detail the low-level components of our XAI pipeline, as shown in Figure 1.\nLoad the Network Intrusion Datasets: The first step is to load the network intrusion datasets, namely NSL-KDD [15], CICIDS-2017 [28], and RoEduNet-SIMARGL2021 [27].\nBlack-box AI Models: After preprocessing the dataset, we train black-box AI models, splitting the data into 70% for training and 30% for testing. We use deep neural networks (DNN) and choose specific parameters for each model to ensure optimal performance (see Section 5).\nAI Models Selection Criteria: The DNN models used in this work were chosen because they are widely used in similar IDS studies (e.g., [15, 47, 48]), and to examine the impact of XAI-based feature selection on AI model performance in IDS. Using white-box methods like IG, LRP, and DeepLift necessitates working with neural networks to maintain consistency with IDS literature when comparing different models paired with XAI techniques.\nXAI Methods: After constructing the black-box DNN model, we explain these models using three popular white-box XAI methods: LRP [21], IG [20], and DeepLift [22]. These methods evaluate black-box AI models both globally and locally and are post-hoc, meaning they are applied after the AI model is trained and its predictions are generated. These model-dependent methods are well-known for evaluating black-box neural network models [17].\nRationale for Choosing IG, LRP, and DeepLift: We selected these three white-box \u03a7\u0391\u0399 methods for their popularity, characteristics, and ease of implementation [17, 60]. Given the rising demand for neural network models to tackle complex problems, explaining such results is crucial, making IG, LRP, and DeepLift compelling choices since they are model-specific. Moreover, these approaches tend to be more robust, which is essential for IDS applications. Section 5 demonstrates that these techniques generally outperform black-box XAI methods. Additionally, these methods are accessible due to their available libraries for experimentation [60].\nThe proposed pipeline assesses the three X\u0391\u0399 techniques utilizing the following metrics: robustness, sparsity, efficiency, completeness, descriptive accuracy, and stability. Descriptive accuracy checks if the accuracy of an AI model drops after the most significant features, according to the respective XAI technique, are not present. Sparsity checks if the explanation is distributed among many features or concentrated on a few. Stability assesses the consistency of an XAI explanation across multiple trials under the same conditions."}, {"title": "4.3 Step-by-step Algorithms for Generating \u03a7\u0391\u0399 Evaluation Metrics", "content": "We detail the step-by-step process for generating our six XAI evaluation metrics: descriptive accuracy, sparsity, efficiency, stability, robustness, and completeness.\nC.1. Descriptive Accuracy Step-by-Step\nPrerequisite: Feature importance list from IG, LRP, DeepLift.\nFor each XAI method for DNN and each of the three datasets (NSL-KDD, CICIDS-2017, and RoEduNet-SIMARGL2021):\n1. Define k = (0, 5, 10, 25, 50, 70), the number of features to be removed in importance order.\n2. For each k value, obtain the AI model's accuracy. For k = 0, the model uses all features. For k = 5, the model uses all features except the top 5.\n3. Plot k on the X-axis and the AI model's accuracy on the Y-axis.\n4. Plot and analyze the Descriptive Accuracy graph using these axes.\nC.2. Sparsity Step-by-Step\nPrerequisite: Feature Scores obtained with DeepLift, IG, or LRP. Feature scores are normalized to the range [0,1].\nFor each of the three datasets (RoEduNet-SIMARGL2021, CICIDS-2017, and NSL-KDD), the Sparsity metric is applied as hereby:\n1. Create an X-Axis with values [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] referring to the threshold compared to the feature scores.\n2. For each X-axis value (the threshold), apply the formula in Equation (1) to determine the sparsity score.\n$Sparsity = \\frac{No\\_Features\\ s.t.\\ {Feature\\_Score < T}}{Total\\_Number\\_Features}$ (1)\n3. Each result produced shapes the Y-axis, referring to the Sparsity score for each X-axis value. The Y-axis is normalized by dividing its values by the total number of features.\n4. Use the Y-axis and X-axis to plot the Sparsity graph.\nC.3. Efficiency Step-by-Step\nPrerequisite: None.\nFor each of the three datasets ( NSL-KDD, CICIDS-2017, and RoEduNet-SIMARGL2021), the Efficiency metric is applied using IG, LRP, or DeepLift XAI methods as follows:\n1. After training the DNN model, apply IG, LRP, or DeepLift for N samples. We use 1, 100, 500, 2500, and 10000 samples.\n2. Track the time required to complete the XAI evaluation method for each run.\nC.4. Stability Step-by-Step\nPrerequisites: None\nFor each of the datasets (CICIDS-2017, NSL-KDD, and RoEduNet-SIMARGL2021), the Stability metric is applied using IG, DeepLift, and LRP \u03a7\u0391\u0399 methods at this moment:\n1. For each dataset, select the top-k features to compare in each run, depending on the total number of features. For example, CICIDS-2017 has over 70 features, so comparing the top k = 20 features is representative.\n2. Check stability using the formula:\n$Stability = \\frac{\\sum_{i=1}^{N} X_i}{k}$,\nwhere N is the number of runs, and k is the number of top features. We evaluate how many times the top-k features are common in different N runs. If all the X sets are the same, the stability score is 1; if none are the same, the score is 0.\n3. Repeat the process for N = 3 (three runs or trials) for each AI model and intrusion detection dataset.\nC.5. Robustness Step-by-Step\nPrerequisites: Algorithm from paper [26].\nFor each intrusion dataset (RoEduNet-SIMARGL2021, CICIDS-2017, and NSL-KDD), run the Robustness experiment hereby:"}, {"title": "C.6. Completeness Step-by-Step", "content": "We specify below the step-by-step process for local completeness (on a single instance) and global completeness (on various instances) for the completeness XAI evaluation metric used in this paper.\nC.6.1 Local Completeness (Single instance) Step-by-Step:\nPrerequisites: None\nFor each of the datasets (NSL-KDD", "follows": "n1. Divide the dataset by class type to assess the results by label.\n2. Produce an explanation with IG", "hereby": "n1. Split the batches by class.\n2. Produce the explanation for the original instance.\n3. Perturb the top-1 feature and check if the class changes. Repeat the process for subsequent features up to the top-k feature. The perturbation changes the feature value from 0 to 1 in steps of 0.1.\n4. If the current perturbed feature fails to change the prediction class, assign its value as originalvalue \u2013 1| to maintain relevant perturbation values for previously perturbed features.\n5. If the class changes, stop the experiment for that sample and register the change.\n6. Count how many samples changed class when perturbed and divide this number by the batch size.\n7. Calculate the percentage of samples with valid explanations"}]}