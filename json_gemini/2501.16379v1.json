{"title": "FedAGHN: Personalized Federated Learning with Attentive Graph HyperNetworks", "authors": ["Jiarui Song", "Yunheng Shen", "Chengbin Hou", "Pengyu Wang", "Jinbao Wang", "Ke Tang", "Hairong Lv"], "abstract": "Personalized Federated Learning (PFL) aims to address the statistical heterogeneity of data across clients by learning the personalized model for each client. Among various PFL approaches, the personalized aggregation-based approach conducts parameter aggregation in the server-side aggregation phase to generate personalized models, and focuses on learning appropriate collaborative relationships among clients for aggregation. However, the collaborative relationships vary in different scenarios and even at different stages of the FL process. To this end, we propose Personalized Federated Learning with Attentive Graph HyperNetworks (FedAGHN), which employs Attentive Graph HyperNetworks (AGHNs) to dynamically capture fine-grained collaborative relationships and generate client-specific personalized initial models. Specifically, AGHNs empower graphs to explicitly model the client-specific collaborative relationships, construct collaboration graphs, and introduce tunable attentive mechanism to derive the collaboration weights, so that the personalized initial models can be obtained by aggregating parameters over the collaboration graphs. Extensive experiments can demonstrate the superiority of FedAGHN. Moreover, a series of visualizations are presented to explore the effectiveness of collaboration graphs learned by FedAGHN.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) has provided a new paradigm for training the global model through client-side collaboration in distributed systems without sharing clients' private data. However, the data distributions among different clients exhibit statistical heterogeneity in real-world scenarios, posing challenges for traditional FL approaches which rely on a single global model to adapt for the diverse local data of different clients [Mills et al., 2021].\nPersonalized Federated Learning (PFL) offers a novel perspective to address statistical heterogeneity. The main concept is to learn a personalized model for each client rather than sharing a single global model. Among various PFL approaches, the personalized aggregation-based approach naturally fits the need of personalized adaptation for local data in different clients. In this work, we further investigate this approach with the focus on the collaboration mechanism.\nThe personalized aggregation-based approach performs personalized aggregation in the server-side aggregation phase instead of the global average aggregation. It mainly aims to obtain appropriate collaborative relationships for parameter aggregation, so that each client can receive additional gains without sharing local data. To achieve the goal, three key points may need to be considered:\n1. Collaboration weights and weight differences. The collaboration weights offer a pattern for aggregating clients' parameters and producing the personalized aggregated model. The weight differences of different FL scenarios should be adapted to the specific FL scenario considering its data distributions or other characteristics over clients.\n2. Self-participation strength in collaboration. The self-participation strength refers to the self-weighting of client's personalized local model during the personalized aggregating phase. Each client should evaluate its preference for retaining its learned information.\n3. Layer-wise collaboration differences. For deep learning models with multiple layers, personalized aggregation should consider the layer-wise personalization. Previous works [LeCun et al., 2015; Ma et al., 2022; Lee et al., 2023] have suggested that different layers have different behaviours, e.g., shallow layers capture generic information, while deep layers acquire specific information.\nRegarding the three points, most previous methods focus on certain aspect(s). For example, as shown in Figure 1, CFL [Sattler et al., 2020] only considers collaboration with clustered intra-group collaborating clients; H-FedAMP [Huang et al., 2021] considers the differences among collaborating clients, but the collaboration weight of the central client remains fixed. pFedLA [Ma et al., 2022] and FedALA [Zhang et al., 2023a] optimize trainable parameters to implicitly capture relationships, but encounter challenges in effectively capturing appropriate collaborative relationships."}, {"title": "2 Related Work", "content": "Specifically, we propose a novel personalized aggregation-based FL method, Personalized Federated Learning with Attentive Graph HyperNetworks (FedAGHN), which employs the proposed Attentive Graph HyperNetworks (AGHNs) to generate client-specific personalized initial models. At each communication round, AGHNs maintain a set of collaboration graphs for each client and each layer. The collaboration graph is initialized by taking both personalized local model parameters and its updates as node features. The tunable attentive mechanism is introduced to calculate collaboration weights. It leverages prior knowledge from the updates of models and two trainable parameters optimized towards the client's objective. One trainable parameter is employed to explicitly tune the weight differences of collaborating clients (Point 1), and another one is used to adjust the self-participation strength in collaboration (Point 2). After that, the personalized initial model of a client at each layer (Point 3) can be obtained by weighted aggregation over the corresponding learned graph.\nThe main contributions of this work are as follows:\n\u2022 We propose a novel personalized aggregation-based method called FedAGHN, which naturally integrates graph learning into the FL process. FedAGHN considers weight differences, self-participation strength, and layer-wise personalization to better capture the fine-grained collaboration weights for personalized aggregation.\n\u2022 We introduce two sets of trainable parameters in FedAGHN, which is beneficial to capture the dynamic evolution of collaborative relationships. These trainable parameters are optimized dynamically in the FL process to align with the objective of each client for learning the client-specific personalized model.\n\u2022 We conduct extensive experiments and discover several meaningful insights by visualizing and analyzing the collaboration graphs from different points of view, such as overall patterns, layer-wise patterns, and the evolution of patterns during FL process."}, {"title": "2.1 Personalized Federated Learning", "content": "Statistical heterogeneity has become a crucial challenge in federated learning settings [Meng et al., 2024; Tang et al., 2024]. Personalized federated learning aims to learn models that better fit each client's local data, enabling them to effectively tackle the statistical heterogeneity in FL [Dinh et al., 2022; Li et al., 2021a]. Recently, the development of PFL has led to the emergence of various approaches [Tan et al., 2023; Zhang et al., 2023c], such as local adaptation, meta-learning-based [Jiang et al., 2019; Fallah et al., 2020], multi-task-learning-based [Marfoq et al., 2021], regularization-based [T Dinh et al., 2020; Li et al., 2021b], parameter decoupling [Collins et al., 2021; Chen and Chao, 2022; Zhang et al., 2023b], knowledge distillation-based [Wu et al., 2022; Xu et al., 2023] and personalized aggregation-based approaches [Li et al., 2021c; Zhang et al., 2023a]. Details of various PFL approaches are provided in Appendix A."}, {"title": "2.2 Graph Learning", "content": "Graph learning has attracted much attention in recent years [Wang et al., 2022]. The attention mechanism is introduced over the relational weights to be optimized with certain objectives [Veli\u010dkovi\u0107 et al., 2018; Zhang et al., 2024], such that the optimized weights better reflect the relations between nodes in terms of the objectives. In PFL, each client could have different specific relations to other clients due to inherent different data distributions, while the relations between clients are often unknown and complicated. It thus becomes critical to learn the relations between clients."}, {"title": "2.3 Hypernetworks", "content": "Hypernetwork is a technique that utilizes neural network to generate parameters (such as weights) for the target network, with the objective of enhancing its performance in target tasks. The property of hypernetworks, enabling the generation of improved client model parameters in the target task, makes them highly suitable for PFL [Tang and Yao, 2024]. pFedHN [Shamsian et al., 2021] maintains a unique embedding vector for each client, serving as input to the hypernetwork for generating personalized model parameters. pFedLA [Ma et al., 2022] utilizes hypernetworks to generate layer-wise weighted aggregation parameters."}, {"title": "3 The Proposed Method", "content": "In this section, we first formally formulate the problem of PFL. After that, we present the overall framework and algorithm of the proposed FedAGHN. The technical details of AGHNs are finally elaborated step by step."}, {"title": "3.1 Problem Formulation", "content": "The objective of PFL is to learn a personalized local model for each client without sharing their private data. Assume that there are N clients, and \\( \\Theta = {\\Theta_1,...,\\Theta_N} \\) represents the parameters of these personalized local models, trained on their local datasets \\( \\mathcal{D} = {\\mathcal{D}_1,...,\\mathcal{D}_N} \\). The datasets \\( \\mathcal{D} \\) are sampled from Non-IID settings. The objective function of PFL can be formulated as\n\n\\Theta^* = \\underset{\\Theta}{arg\\, min} \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{(x,y)\\sim \\mathcal{D}_i} \\mathcal{L}_i (\\Theta_i; x, y) \\tag{1}\n\nwhere \\( \\mathcal{L}_i \\) is the empirical loss of client i on dataset \\( \\mathcal{D}_i \\).\nThe personalized aggregation-based methods mainly focus on the aggregation step to aggregate personalized models for each client, i.e.,\n\n\\Theta_i^{(t+1)} = Agg_i(\\lbrace \\Theta_j^{(t)} \\rbrace) \\tag{2}\n\nwhere \\( \\Theta_i^{(t+1)} \\) denotes the personalized initial model of client i in the next round t + 1 and \\( Agg_i \\) represents the aggregation function of different personalized aggregation-based methods, which is specified as the AGHN in our method shown in Eq. (3)."}, {"title": "3.2 FedAGHN Algorithm", "content": "In view of the fact that capturing the better fine-grained collaborative relationships among clients is the most essential and bottom-level requirement in the heterogeneous scenario of PFL, we propose a novel aggregation-based PFL method, namely Personalized Federated Learning with Attentive Graph HyperNetworks (FedAGHN), which utilizes our proposed Attentive Graph HyperNetworks (AGHNs) to generate client-specific personalized initial models.\nThe proposed FedAGHN aims to provide the collaboration graphs (i.e., collaboration weights) for the model aggregation among N clients. FedAGHN also integrates prior knowledge to explicitly model the collaboration among clients, and it can adaptively adjust collaborative relationships through the optimization of hypernetworks' parameters with respect to the clients' objectives in an end-to-end fashion.\nSpecifically, we maintain a dedicated AGHN at the server side for each client, which means that there are AGHNs = {AGHN1, AGHN2, ..., AGHNN } on the server. AGHNs are employed to capture the collaboration among clients, generating collaboration graphs \\( \\mathcal{G} = {G_1, G_2, ...,G_N} \\) for each client. Each graph \\( G_i \\) also contains multiple subgraphs at the layer-wise level. Based on these layer-wise collaboration graphs, AGHNs then produce personalized initial models \\( \\Theta = [\\Theta_1, \\Theta_2, ..., \\Theta_N] \\) which are dedicated to each client, rather than aggregating a single global model for all clients. In the server aggregation phase of each round t, the personalized initial model \\( \\Theta_i^{(t+1)} \\) in the next round t + 1 for client i can be obtained by\n\n\\Theta_i^{(t+1)} = AGHN_i(\\Theta^{(t)}, \\Delta\\Theta^{(t)}; p_i; q_i) \\tag{3}\n\nwhere the input of AGHN has two parts: the collection of personalized local model parameters denoted as \\( \\Theta^{(t)} \\), and the set of the updates of personalized local model parameters returned by the clients denoted as \\( \\Delta\\Theta^{(t)} \\). Note that, \\( p_i \\) and \\( q_i \\) are the parameters of AGHN, which are vital for AGHNs to explicitly model and tune the collaborative relationships.\nThe overall framework of FedAGHN is illustrated in Figure 2, and its pseudocode is provided in Appendix B."}, {"title": "3.3 Attentive Graph Hyper-Networks", "content": "In this section, we take the attentive graph hypernetwork for client i (i.e., AGHN) as an example to elaborate the technical details. For simplicity, we now focus on one communication round and remove the superscript reflecting the round. The illustration is shown in Figure 3.\nThe key of AGHNs is to learn the collaboration graphs (i.e., collaboration weights) such that each client can aggregate its beneficial model parameters from all clients based on the learned collaboration graphs. As the layers of deep learning models may have different functionalities, we also consider different collaboration patterns for different layers respectively, so that AGHN for client i maintains R collaboration graphs \\( G_i = {G_i^1, G_i^2, ..., G_i^R} \\) where R is the number of layers in the model at client i. Note that, there is no collaboration among different layers.\nFor convenience, we follow the graph learning terminology to present the attentive graph hyper-network at client i and layer r, which corresponds to the collaboration graph \\( G_i^r \\).\nInitialize collaboration graphs\nAs for the initialization of graph topology, the collaboration graph \\( G_i^r \\) is established for the central node (client) i, such that there are totally N nodes with edges connected to node i from all nodes j (including self-connection from i).\nRegarding the initialization of node features (h';a'), we take the personalized local model parameters \\( \\Theta^r \\) from clients at layer r to set the node features h' as \\( h^r = {h_1^r, h_2^r, ..., h_N^r} = {\\Theta_1^r, \\Theta_2^r, ...., \\Theta_N^r} \\) and we take the updates of personalized local model parameters \\( \\Delta\\Theta^r \\) from clients at layer r to set the node features a' as \\( a^r = {a_1^r, a_2^r, ..., a_N^r} = {\\Delta\\Theta_1^r, \\Delta\\Theta_2^r, ...., \\Delta\\Theta_N^r} \\) where the node features h' is employed for aggregation over graphs, and the node features a' is utilized to calculate attentive weights. The initial node features at the first round is randomly parameterized, whereas the initial node features at other rounds come from their previous round respectively.\nCalculate attentive weights\nGiven an initialized collaboration graph \\( G_i^r \\), the node features a', containing the update of personalized local model parameters \\( \\Delta\\Theta^r \\) from the previous round, can be regarded as prior knowledge.\nSpecifically, we calculate the collaboration weights between node i and each node \\( j \\in {1,2, ..., N} \\) via a newly proposed tunable attentive mechanism:\n\n\\mathcal{A}_{ij}^r = \\begin{cases}\n\\frac{e^{p\\cdot cos(a_i^r, a_j^r)}}{\\sum_{j\\neq i} e^{q\\cdot cos(a_i^r, a_j^r)}}, & j \\neq i \\\\\n p, & j = i\n\\end{cases} \\tag{4}\n\nwhere \\( \\mathcal{A}_{ij}^r \\) denotes the attentive weight between node i and node j at layer r, and \\( cos(a_i^r, a_j^r) \\) gives the prior knowledge based on cosine similarity of the two clients' previous update of personalized local model parameters.\nIn order to optimize the attentive weights with respect to the client i's objective, we introduce two trainable parameters to further tune the attentive weights. Concretely, \\( q^i \\) is adopted to tune the attentive weights between central node i and other nodes, while \\( p_i \\) is used to tune the attentive weight of self-connection of central node i. Note that, the two trainable parameters \\( q^i \\) and \\( p_i \\) are vital for AGHN, since they can explicitly capture and tune the collaborative relationships according to the client's objective in the end-to-end paradigm.\nTo accelerate convergence and avoid exploding gradients, the normalized attentive weights \\( \\alpha^r = {\\alpha_1^r, \\alpha_2^r, ..., \\alpha_N^r} \\) for collaboration graph \\( G_i^r \\) can be calculated via\n\n\\alpha_j^r = \\frac{\\mathcal{A}_{ij}^r}{\\sum_{j=1}^N \\mathcal{A}_{ij}^r} \\, \\forall j \\in {1, 2, ..., N}. \\tag{5}\n\nAggregate parameters over graphs\nThe collaboration graph \\( G_i^r \\) includes the attentive weights \\( {\\alpha_1^r, \\alpha_2^r, ..., \\alpha_N^r} \\) and the node features h' from previous round. The personalized initial model \\( \\Theta_i^r \\) for client i at layer r can be obtained by weighted aggregating over \\( G_i^r \\) as follows:\n\n\\Theta_i^r = \\sum_{j\\in{1,2,...,N}} \\alpha_j^r h_j^r. \\tag{6}\n\nThe update of AGHN\nFollowing the previous works [Shamsian et al., 2021; Ma et al., 2022], we accordingly adopt a more general way to update \\( p_i \\) and \\( q_i \\) based on gradient descent, the details can be found in Appendix B."}, {"title": "4 Experimental Settings", "content": "This section briefly describes the experimental settings. More detailed experimental settings are presented in Appendix C.\nDatasets and Models: To evaluate the performance of FedAGHN, we employ CIFAR10, CIFAR100 [Krizhevsky et al., 2009], and Tiny-ImageNet [Chrabaszcz et al., 2017] as benchmark datasets. For CIFAR10 and CIFAR100, we use the 4-layer CNN architecture with Batch Normalization (BN) layers similar to previous work [Ma et al., 2022]. To assess the performance on larger backbone networks and datasets, we utilize ResNet-18 as the backbone model for CIFAR100 and Tiny-Imagenet. In the following sections, CIFAR100* denotes using ResNet-18 on CIFAR100, and TINY indicates using ResNet-18 on Tiny-ImageNet.\nData Heterogeneity: The data heterogeneity is considered through two Non-IID scenarios: the pathological setting and the practical setting.\nBaselines and Evaluation: The proposed FedAGHN is compared against 15 baselines, including the local training method, 2 traditional federated learning methods, and 12 personalized federated learning methods. For each method, we save the best model that achieves the best performance on the local validation set during the training process, and evaluate it on the local test set of each client. The average of test accuracy across all clients is calculated.\nImplementation Details: In the specific experimental implementation, we use the SGD optimizer and set the local batch size to 64. The number of local training epochs is set to 5, and the local learning rate for models is set to 0.01. We consider 20 clients participating with the join rate of 1."}, {"title": "5 Results and Discussions", "content": "To demonstrate the effectiveness of FedAGHN and its components, we investigate the following research questions."}, {"title": "5.1 Main Experiment", "content": "\u2022 RQ1: How does FedAGHN perform against the baseline methods in different scenarios? (Section 5.1) (Table 1)\n\u2022 RQ2: Is FedAGHN still effective across various degrees of statistical heterogeneity? (Section 5.2) (Table 2)\nTo verify the superiority of the proposed FedAGHN against the 15 baseline methods, we fairly test them over the aforementioned datasets and two different Non-IID settings. The best performances and second best ones are bolded and underlined respectively in Table 1.\nOverall, FedAGHN outperforms almost all baselines and achieves the state-of-the-art regarding all cases across the two Non-IID scenarios. This might be because that FedAGHN benefits from full consideration of personalization at the layer-wise level and adaptively tuned client collaborative relationships. The only case where FedAGHN doesn't achieve the first place is in the pathological setting of CIFAR10, where it takes the second place, narrowly trailing behind Fed-PAC. This may be due to FedPAC makes it easier to achieve feature alignment when both the number of model parameters and the variety of data sample labels are small.\nThe traditional FL methods FedAvg and FedProx perform worse than local training. While the local adaptation methods FedAvg-FT and FedProx-FT, remain competitive among various PFL approaches.\nFor personalized aggregation-based methods, CFL performs competitively in the practical setting of TINY but performs worse in other settings. This might be because that CFL collaborates with a few clients whose updates of personalized local model parameters are most similar, ignoring the general information from other clients, making it less adaptable to various scenarios. H-FedAMP achieves high performance in the pathological setting of TINY and it outperforms FedAMP in most scenarios, possibly because H-FedAMP calculates the relative collaboration weights for each collaborating client, and it has a more flexible calculation of collaboration weights than FedAMP.\nThis study finds that finer-grained aggregation may result in improved performance. pFedLA performs well in most cases, possibly due to its fine-grained personalized aggregation. However, pFedLA is not adaptable to different scenarios, possibly because its weight generation relies entirely on the optimization effects of hypernetworks' model parameters and clients' embeddings. FedALA performs worse than pFedLA in most cases, since it integrates information from both local and global models at a finer granularity but overlooks the selection of collaborative relationships with other clients. FedAGHN outperforms all the above personalized aggregation-based methods, possibly because it can explicitly model collaborative relationships and adaptively tune collaboration weights while performing finer-grained aggregation.\nTo further explore the superior performance mentioned above, we also conducted an ablation study detailed in Appendix D to verify the effectiveness of its components."}, {"title": "5.2 Different Degrees of Heterogeneity", "content": "To investigate the performance of FedAGHN under various degrees of data heterogeneity, we manipulate the parameter B in Dir(3) to generate heterogeneous data with different distributions on the CIFAR10 dataset. The experimental results of FedAGHN and baseline methods under various degrees of data heterogeneity are presented in Table 2. FedAGHN outperforms all baselines across various degrees of data heterogeneity, which might be owing to the consideration of fine-grained aggregation and personalization at layer-wise."}, {"title": "6 Analysis of Collaboration in FedAGHN", "content": "In addition to the above experiments, we investigate the following research questions to further explore the collaborative relationship learned by FedAGHN.\n\u2022 RQ3: Can the collaboration graphs in FedAGHN effectively capture the collaborative relationships among clients? (Section E.2) (Figure 4, Table 3)\n\u2022 RQ4: Can FedAGHN meet the personalized requirements for layers and show differences among different layers? (Section 6.2) (Figure 5)\n\u2022 RQ5: Can FedAGHN dynamically tune collaborative relationships during FL process? (Section 6.3) (Table 4)"}, {"title": "6.1 Case Study Setting", "content": "In this section, we establish a case study setting to investigate the collaborative relationship learned by FedAGHN. The Non-IID scenario is introduced following [Xu et al., 2023]. Therefore, clients within each group follow the same data distribution, while clients across groups have different dominant classes. In this context, there are obvious differences between clients within the same group and clients in different groups, so we can intuitively estimate whether the collaborative relationships among clients are appropriately captured.\nTable 3 has shown the performance of these methods in case study setting. FedAGHN performs best on these datasets in this setting, which could be explained by the appropriate collaboration graphs analyzed in Section 6.2 and 6.3."}, {"title": "6.2 Analysis of Collaboration Graphs", "content": "Comparison with other methods\nTo evaluate the efficacy of personalized aggregation methods in capturing collaborative relationships, the visualization of collaboration graphs in CFL, H-FedAMP and FedAGHN are shown in Figure 4-a, 4-b and 4-c respectively.\nAs illustrated in Figure 4, FedAGHN can effectively capture collaborative relationships among clients, with small self-weighting value. Overall, the collaboration weights of FedAGHN generally align with the similarity of clients' data distributions in the case study setting. And FedAGHN has clear weight differences both intra-group and inter-group. For the clients within a same group, there are significant weight differences among clients, which implies that it assigns higher weights to more beneficial clients. Besides, it is shown in Figure 4-a that all clients within a group are assigned the same collaboration weights in CFL, while clients outside the group do not participate in collaboration. Furthermore, H-FedAMP introduces a fixed self-weighting value, while assigning slightly higher weights to clients within the same group and lower weights to others.\nLayer-wise collaboration graphs\nTo further investigate the layer-wise personalization in FedAGHN, we visualize the collaboration graphs corresponding to different model layers on CIFAR100. Figure 5 reveals significant differences in the collaborative relationships captured by the shallow and deep layers of FedAGHN. In the shallow layers (Figure 5-a, 5-b), the model tends to select more general information, with no significant differences between groups. FedAGHN introduces the tunable attentive mechanism to capture the fine-grained collaborative relationships among clients, which enables it to focus more on valuable information in the shallow layers, without excluding clients with different data distributions. In the deep layers (Figure 5-c, 5-d), the model tends to select more specific information and prefers collaboration among clients within the same group. Significant differences can be observed between the groups, especially in the last model layer."}, {"title": "6.3 Analysis of Dynamic Collaboration", "content": "To demonstrate that the proposed FedAGHN can dynamically tune the client collaboration during the training process, Table 4 presents the changes in collaboration weights during the training process. Overall, the weight differences among collaborating clients continue to change during the training process, possibly indicating that the adaptive tuning of collaborative relationships may enhance FedAGHN's performance. The self-weighting value gradually decreases during the FL process, indicating that clients prefer to collaborate with others. FedAGHN's adaptive tuning of collaborative relationships can better meet the collaboration requirements at different stages. The detailed analysis is presented in Appendix E."}, {"title": "7 Conclusion", "content": "This work proposed FedAGHN, a personalized aggregation-based method, to continuously capture appropriate collaborative relationships for fine-grained personalized aggregation. At each communication round, FedAGHN maintains a set of collaboration graphs for each client and each layer, which explicitly model the client-specific and layer-wise collaboration weights. The collaboration weights are obtained by the newly proposed tunable attentive mechanism. Finally, the personalized initial models are obtained by aggregating parameters over the learned collaboration graphs. Extensive experiments have demonstrated the superiority of FedAGHN. We also performed experiments to analyze how FedAGHN uniquely captures collaborative relationships across different model layers, various stages of the FL process, and self-participation weight. For future work, we suggest further understanding the evolution of collaborative relationships during the FL process, not only for FedAGHN but also for other PFL methods."}, {"title": "Appendix", "content": "A Details of Related Work"}, {"title": "A.1 Various PFL Approaches", "content": "The vanilla PFL methods, such as FedAvg-FT and FedProx-FT, employ local training of the global model to obtain personalized models, which can be classified as local adaptation methods. Meta-learning-based methods [Jiang et al., 2019; Fallah et al., 2020] and multi-task-learning-based methods [Marfoq et al., 2021] apply the principles of meta-learning and multi-task learning to PFL, aiming to enhance the model adaptation.\nRegularization-based methods, such as pFedMe [T Dinh et al., 2020] and Ditto [Li et al., 2021b], have new ideas for using global model in PFL. These methods learn personalized models for each client and employ L2 regularization to constrain the distance between the personalized models and the global model. Moreover, knowledge distillation is also a feasible way to absorb model information. FedPAC [Xu et al., 2023] is one of knowledge distillation-based methods, introducing global feature centroids for explicit local-global feature representation alignment.\nParameter decoupling methods are commonly employed to achieve better personalized or shared models. FedRep [Collins et al., 2021] first trains personalized classification layers and then trains a feature extractor. FedCP [Zhang et al., 2023b] handles global and personalized information in the features separately using a global head and a personalized head, employing conditional policies to differentiate between global and personalized information.\nPrevious personalized aggregation-based methods mainly focus on certain aspects of collaboration among clients. Our work uses collaboration graphs to model the fine-grained collaborative relationships, and the collaboration weights on graphs can be adaptively optimized with clients' objectives during FL process."}, {"title": "A.2 Graph Learning", "content": "The collaboration graph can be used to represent the relationship between clients in FL. However, natural graph structures do not exist in many real-world scenarios, which requires graph structure learning on the server side to establish collaboration graphs among clients [Fu et al., 2022]. A common approach is to construct collaboration graphs based on the model parameters of different clients. SFL [Chen et al., 2022] constructs a similarity matrix using the similarity of personalized model parameters, then it forms a graph with trainable parameters as edge weights, and optimizes the graph with regularization during local training at client side; pFedGraph [Ye et al., 2023] designs optimization goals on the server side and proceeds to optimize the collaboration graph by solving a quadratic program.\nPrevious approaches treat the model as a whole entity when constructing and utilizing the collaboration graph, disregarding the differences between different model layers. However, it is crucial to learn specific collaboration graphs for each model layer. Our work learns collaboration graphs for different model layers and the collaboration graphs can be optimized via tunable attentive mechanism during FL process."}, {"title": "A.3 Hypernetworks", "content": "By optimizing both the embedding vectors and hypernetwork parameters, pFedHN [Shamsian et al., 2021] facilitates the generation of improved personalized model parameters. pFedLA [Ma et al., 2022] validates that the aggregation parameters generated by hypernetworks can capture fine-grained relationships among clients, thereby enhancing the performance of personalized models. Additionally, hypernetworks are on the server side, which mitigates the introduction of additional communication overhead.\nThe hypernetwork architecture in pFedLA employs Soft-Max layers to generate the weights with differences for different model layers. The majority of hypernetwork parameters are shared by all clients in pFedHN. The optimization process involves a larger number of parameters, which may potentially result in smaller differences between clients or between layers.\nRegarding our work, we reduce the number of trainable parameters in the hypernetwork and retain only the vital training parameters which can tune the collaboration graphs, reducing the optimization burden while increasing the differences among model layers."}, {"title": "B Details of the Proposed FedAGHN", "content": "B.1 Pseudocode of the FedAGHN Algorithm\nThe pseudocode of the proposed FedAGHN algorithm is presented in Algorithm 1."}, {"title": "B.2 Calculate Attentive Weights", "content": "Note that, the two trainable parameters \\( q_i \\) and \\( p_i \\) are vital for AGHN. An increased value of \\( q_i \\) denotes that client i prefers greater difference among collaboration weights at layer r, while an increased value of \\( p_i \\) strengthen the self-weight for client i and thus reduce the inclination to cooperate with other clients."}, {"title": "B.3 Aggregate Parameters over Graphs", "content": "It might be worth noting that attentive weights are usually calculated using node features h when aggregating h in traditional graph learning, which is different from utilizing another node features a in our above implementation. In fact, this is exactly the unique change we make when applying the graph learning paradigm to federated learning. This is because the update of model parameters represents the updated direction of the model on local data in PFL, it can better reflect the similarity of data distributions among different clients than the model parameters, thereby better modeling collaborative relationships among clients."}, {"title": "B.4 The Update of AGHN", "content": "The collaboration weights among clients should adapt to the changes over communication rounds in federated learning. For brevity, considering each of R layers at client i, the two sets of trainable parameters are denoted as \\( q_i = {q_i^1, q_i^2,..., q_i^R} \\) and \\( p_i = {p_i^1, p_i^2, ..., p_i^R} \\), where each layer at client i maintains a pair of scalars \\( p_i^r \\) and \\( q_i^r \\).\nFollowing the previous works [Shamsian et al., 2021; Ma et al., 2022], we accordingly adopt a more general way to update \\( p_i \\) and \\( q_i \\) based on gradient descent, their updating rules are as follows\n\nqi := qi - \\eta_{hn}\\Delta q_i = q_i - \\eta_{hn}(\\nabla_{qi}\\mathcal{L}_i)^T\\Delta\\Theta_i \\\\ pi := max(0, p_i - \\eta_{hn}\\Delta P_i) \\\\\n= max(0, p_i - \\eta_{hn} (\\nabla_{pi}\\mathcal{L}_i)\\Delta\\Theta_i) \\tag{1}\n\nwhere \\( \\eta_{hn} \\) is the learning rate for hyper-networks; \\( \\Delta q_i \\) denotes the gradient vector with R dimensions for client i's updating; \\( \\nabla_{qi}\\mathcal{L}_i \\) is partial derivative with respect to \\( q_i \\); the operator T indicates for transpose; \\( \\Theta_i \\) represents the personalized initial model parameters after aggregation; \\( \\Delta\\Theta_i \\) denotes the update of personalized local model parameters. It might be worth mentioning that there is no additional communication cost while calculating the trainable parameters \\( p_i \\) and \\( q_i \\)."}, {"title": "C Details of Experimental Settings", "content": "C.1 Data Heterogeneity\nIn the pathological setting, we adopt the strategy similar to FedAvg [McMahan et al., 2017"}]}