{"title": "Disentangled Structural and Featural Representation for Task-Agnostic Graph Valuation", "authors": ["Ali Falahati", "Mohammad Mohammadi Amiri"], "abstract": "With the emergence of data marketplaces, the demand for methods to assess the value of data has increased significantly. While numerous techniques have been proposed for this purpose, none have specifically addressed graphs as the main data modality. Graphs are widely used across various fields, ranging from chemical molecules to social networks. In this study, we break down graphs into two main components: structural and featural, and we focus on evaluating data without relying on specific task-related metrics, making it applicable in practical scenarios where validation requirements may be lacking. We introduce a novel framework called blind message passing, which aligns seller's and buyer's graphs using a shared node permutation based on graph matching. This allows us to utilize the graph Wasserstein distance to quantify the differences in the structural distribution of graph datasets, called the structural disparities. We then consider featural aspects of buyer's and seller's graphs for data valuation and capture their statistical similarities and differences, referred to as relevance and diversity, respectively. Our approach ensures that buyers and sellers remain unaware of each other's datasets. Our experiments on real datasets demonstrate the effectiveness of our approach in capturing the relevance, diversity, and structural disparities of seller data for buyers, particularly in graph-based data valuation scenarios.", "sections": [{"title": "Introduction", "content": "With the advent of foundation models (Bommasani et al. 2021), the demand for large and diverse datasets has increased significantly. Data marketplaces (Agarwal, Dahleh, and Sarkar 2019; Stahl, Schomm, and Vossen 2014) have emerged as transformative platforms for exchanging data. These marketplaces allow data owners to sell their datasets and enable buyers to acquire essential data for their analytical and research needs. A notable development in this domain is the emergence of generative models as potential sellers in data marketplaces. Generative models, like generative adversarial networks (Goodfellow et al. 2020) and variational autoencoders (Kingma et al. 2013), can create synthetic datasets that mimic real-world data. These models generate high-quality, anonymized data that retains the statistical properties of the original datasets, making them particularly useful in situations where data privacy and scarcity are significant concerns. A critical aspect of these marketplaces is the valuation of data, which determines the quality and desirability of datasets. A data marketplace primarily consists of three entities: data sellers, a broker, and data buyers. Data sellers possess the data and provide it to the broker in exchange for compensation. Data buyers seek to obtain this data, with the broker facilitating the transactions. Given the value of data as a resource, it is crucial to develop a systematic approach to assess the value of the data for both sellers and buyers. This process, known as data valuation, is fundamental to ensuring a fair marketplace for all parties involved. Data valuation is a complex process that involves assessing the quality, relevance, and potential utility of data for specific applications concerning the buyer's demands. It is particularly valuable in fields such as finance, healthcare, marketing, and scientific research, where data-driven insights are crucial.\nData valuation can be performed based on either \"intrinsic\" or \"extrinsic\" factors. Intrinsic data valuation is data-driven and focuses on the quality of the dataset itself (Niu et al. 2018; Raskar et al. 2019). In contrast, extrinsic data valuation takes into account demand-supply dynamics and game-theoretic mechanisms (Luong et al. 2016; Zhang et al. 2020). Intrinsic data valuation is often paired with a utility metric for validation (Ghorbani et al. 2019; Jia et al. 2019), or with a specific machine learning (ML) task (Agarwal, Dahleh, and Sarkar 2019; Chen et al. 2019). In particular, for ML applications, data valuation frequently relies on the presence of a validation set, with validation accuracy serving as the metric (Wang et al. 2020; Yan et al. 2021). Additionally, the value of training data is often estimated by evaluating ML models trained on a specific target task (Pei 2020; Liu et al. 2021). In contrast, extrinsic data valuation techniques take into account external factors like competition and market demand (Agarwal et al. 2021; Bimpikis et al. 2019). This approach involves assessing customer demand for products and analyzing competitors' pricing strategies to determine the appropriate price for a product (Toni et al. 2017; Cong et al. 2022). In this paper, we concentrate on intrinsic data valuation for practical applications. However, tightly linking intrinsic data valuation to the existence of a validation set can be impractical. A universally accepted validation set may not be available, and a specific validation set might not adequately reflect the data distribution for a given learning task (Xu et al. 2021a). Moreover, possessing a validation set can enable malicious sellers to alter their datasets to overfit the"}, {"title": "Preliminary", "content": "Graph representation: Let a graph G be defined as G = (V,E) where V = {1, ..., v_v } is the set of nodes, with a cardinality |V| = N, and E = {e_1,...,e_m} is the set of edges, with a cardinality |E| = M. G can be represented by an adjacency matrix A \u2208 {0,1}^{N\u00d7N}, with A_{ij} = 1 if nodes v_i and v_j are connected and A_{ij} = 0 otherwise.\nL1-Wasserstein distance: The Wasserstein distance is a measure of dissimilarity between probability distributions defined on a specific metric space. Let's denote two such distributions as p\u2081 and p\u2082, operating on a metric space H (Villani et al. 2009). The L1-Wasserstein distance with Euclidean distance as the ground distance is given by:\nW_1 (P_1,P_2) = \\inf_{\\Upsilon\\in\\Gamma(P_1,P_2)} \\int_{H\\times H} ||x - y||_2d\\gamma(x, y). \\tag{1}\nHere, \u0393(P_1, P_2) denotes the set of all possible couplings (or joint distributions) \u03b3 whose marginals are p\u2081 and p\u2082, respectively. The term ||x - y||_2 represents the Euclidean distance between points x and y in the metric space H. The integral"}, {"title": "Structural attributes", "content": "In this section, we present a method to measure the structural disparity between two graphs using their positional and structural embeddings (Dwivedi et al. 2021; Cant\u00fcrk et al. 2023). To measure the structural disparity between graphs, it is essential to obtain embeddings for graphs that extract rich structural and positional features. We propose using the two common methods to generate these embeddings using positional and structural encodings to embed each graph, irrespective of its feature attributes.\nRandom walk structural embedding (RWSE) (Dwivedi et al. 2021): RWSEs are defined for k steps of random walk for node i of the graph:\nz_{i}^{RW} = [RW_{ii}^{1} RW_{ii}^{2} ... RW_{ii}^{k}]\\in R^k, \\tag{6}\nwhere $RW_{ii}^{j}$ is the probability of getting back to node i after j steps when we start walking from node i. The random walk operator is defined as RW = AB^{-1}, where A \u2208 R^{N\u00d7N} is the adjacency matrix and B \u2208 R^{N\u00d7N} is the degree matrix and $RW_{ii}$ is the i-th diagonal entry of RW.\nLaplacian eigenvenctor positional embedding (LapPE) (Dwivedi and Bresson 2020): LapPE provides accurate embeddings of graphs into Euclidean spaces. It is constructed by factorizing the graph Laplacian, defined as \u0394 = I_N - B^{-1/2}AB^{-1/2} = U\u039bU^T, where I_N is the N \u00d7 N identity matrix, and the matrices \u039b and U represent the eigenvalues and eigenvectors, respectively. The absolute value of l\u2082-norm"}, {"title": "Blind message passing", "content": "In this section, we build on the concepts introduced earlier to compare the structural attributes of buyer and seller graphs within a data marketplace. This requires addressing two key challenges. First, both the buyer and seller possess multiple graphs, necessitating an extension of the structural disparity measure\u2014originally designed for pairs of graphs\u2014to accommodate comparisons between two sets of graphs. Second, it is crucial to ensure that the graphs remain local and are not shared. We further demonstrate how the disentanglement works in practice through the blind message passing (BMP) framework for the case both parties having multiple graphs.\nOne of the most important aspects of this framework lies in the blind exchange of data between both parties\u2014neither the seller nor the buyer has access to the counterpart's data, preventing any party from tampering with its own data to gain an advantage. As depicted in Fig. 1, a trustworthy broker who initially generates a simple proxy graph from some distribution, denoted by Gkey which remains unknown to both parties. To preserve privacy and distribute the computational cost among parties, Gkey is shared with both the seller and the buyer. Both parties then find their respective optimal permutations with regard to the proxy graph Gkey using the graph matching technique in Eq. (10). Next, we define \u03b5-Conformity, which will be used to calculate the error in finding the optimal permutation using the proxy graph.\nDefinition (\u03b5-Conformity): Two graphs G1 and G2 with permutations \u03a0\u2081 and \u03a0\u2082 are defined as \u025b-conform if:\n||\u03a0_1 \u03a0_{1}^{T} - \u03a0_2 \u03a0_{2}^{T}||_F \\leq \\epsilon. \\tag{15}\nCorollary 1 (Transitivity): Two graphs G\u2081 and G\u2082 that are both matched with Gkey are \u00ea-conform with respect to \u041f\u2081 \u2252 P*(Gkey, G\u2081) and \u041f\u2082 \u2252 P*(Gkey, G\u2082) for\n\\hat{\\epsilon} = \\frac{||L_{key} - \u03a0_{1}^{T}L_1 \u03a0_{1}||_F + ||L_{key} - \u03a0_{2}^{T}L_2 \u03a0_{2}||_F}{2} , \\tag{16}\nwhere Lkey, L1, and L2 are the normalized Laplacian for Gkey, G1, and G2, respectively. Proof provided in the supplementary materials.\nRemark: The transitivity corollary ensures that the upper bound on the error when calculating the optimal permutation of G1 and G2 indirectly using a proxy graph like Gkey is \u0109. We performed an experiment to demonstrate that incorporating graph matching yields competitive results compared to approaches that do not utilize a proxy graph, thereby validating the practicality of the theoretical upper bound. Details are provided in the supplementary materials.\nThe process of matching the graphs in both parties with Gkey aligns the graphs of both parties to a near-distance-compatible node permutation, enabling them to compare structural differences. After computing node embeddings"}, {"title": "Disentanglement in Practice", "content": "We now turn to the practical implementation of structural and featural disentanglement for graph data valuation. Each of the three metrics diversity (D), relevance (R), and structural disparity (S), as defined in (4), (5), and (19), respectively, offers unique insights into the dataset and can be used independently based on user preferences. These metrics are versatile and can be integrated into various utility functions tailored to specific contexts or datasets. To ensure generalizability, we propose using an average ranking approach. Specifically, consider a scenario where multiple sellers each possess a set of graphs, and there is a single buyer with their own graph set. We compute (D), (R), and (S) for each seller's graph set relative to the buyer's graph set. Each seller's set is then ranked according to these three metrics. The final ranking and valuation of the sellers' graph sets are obtained by averaging their rankings across all three metrics. As demonstrated in our experiments, this average ranking approach is effective in practice and suitable for real-world applications.\nComputational Complexity: Our framework consists of three primary computationally intensive algorithms. The first algorithm focuses on computing GWD. The naive approach to computing the Wasserstein distance has a complexity of O(n\u00b3 log(n)), where n denotes the number of node embeddings or the number of nodes in the graphs. To mitigate this computational burden, we employ several efficient acceleration techniques. Notably, approximations based on Sinkhorn regularization have been proposed (Cuturi 2013), which can significantly reduce the complexity to near-linear time.\nThe second algorithm addresses the graph matching problem via the linear assignment problem. To optimize computational efficiency and minimize the load on the broker, we use a proxy graph Gkey. Instead of transmitting both datasets directly to the broker for the computation of graph matching, we delegate this task to the buyer and seller. By offloading the graph matching computation to the buyer and seller, the broker's responsibility is reduced to solely computing GWD. For two graphs G\u2081 = (V1, E1) and G\u2082 = (V2, E2) with |V1| = N1 and |V2| = N2 and N = max{N1, N2}, the paper (Liu, Scaglione, and Wai 2024) shows that the solution to the graph matching problem can be approximated by the following linear assignment problem and can be efficiently solved by the Hungarian method (Munkres 1957):\nP^{**} = \\arg \\max_{\u03a0 \\in \\Pi} tr(\u03a0^T \\overline{U}_1 (\\overline{U}_2)^T). \\tag{20}\nHere U\u00bf \u2208 R^{N\u00d7N} is the orthogonal matrix corresponding to eigenvectors from the eigendecomposition of L\u00bf = U\u00bf\u039bU^T and U\u00bf is the matrix containing the absolute value of the entries of U\u00bf, for i \u2208 {1, 2}. The complexity of the Hungarian algorithm is O(n\u00b3), where n is the number of nodes or, equivalently, the dimensions of the permutation matrix.\nThe third algorithm involves the eigendecomposition of the covariance matrix. The eigendecomposition of an n x n matrix using standard numerical methods, such as the QR algorithm, typically has a computational complexity of O(n\u00b3). Considering these components, the overall complexity of our framework is O(n\u00b3) and n represents the number of nodes."}, {"title": "Experiments", "content": "We evaluate the BMP framework on four tasks: (i) dataset scoring using structural disparity on node level prediction then evaluating various graph neural network (GNN) models on these datasets, (ii) dataset scoring using relevance, diversity, and structural disparity and then evaluating graph convolutional network (GCN) (Kipf and Welling 2016) model on these datasets, (iii) assessing if the structural disparity metric can distinguish between different contexts, and (iv) testing the practical performance of the featural metrics. We used datasets from Table 1 for the experiments. Details about experiments (iii), (iv), and the experimental setup can be found in the supplementary materials.\nIn the first task, we explore whether structural disparity can effectively identify the most suitable graph among three candidates offered by sellers, given a baseline graph from the buyer. To quantify each graph's value, we augment the buyer's graph with the seller's graph and evaluate performance on a test set. We start by embedding a large graph using positional embedders, then partition it into three sets: buyer, seller, and test, with the test set containing 20% of the nodes and the buyer set 10%. Structural disparity serves as the sole metric for this task. We first train a simple GCN on the seller nodes to generate unsupervised node embeddings. These embeddings are then clustered using K-Means (Lloyd 1982), resulting in three distinct candidate sets. We use the BMP framework to assess the structural disparity between each candidate set and the buyer's set. The candidate sets are ranked by their proximity to the buyer's set, with the candidate set 1 denoted by subgraph 1 in Fig. 2 being the most similar. We then evaluate the performance using three different GNN models: GCN (distinct from the one used for clustering), GraphSAGE (Hamilton, Ying, and Leskovec 2017) with a mean aggregator, and GAT (Veli\u010dkovi\u0107 et al. 2017) with an attention mechanism. For each GNN, we train three models, each using a different candidate set combined with the baseline set. The test mask is used for evaluating node classification accuracy. As summarized in Fig. 2, our results demonstrate a clear trend: lower structural disparity between a candidate set and the baseline correlates with higher node classification accuracy. Clustering was employed to ensure distinction between candidates.\nIn the second task, we evaluate our approach for capturing both structural and featural attributes between graphs using three metrics: (D), (R), and (S). The dataset is divided into three disjoint sets: baseline, training, and test, with the test set comprising 20% of the data and the baseline 10%. Recognizing that relying on a single aggregating function from the three metrics could introduce bias dependent on the dataset, we adopt a more general approach. We first rank the dataset samples according to each metric independently, then calculate an average ranking across all metrics, setting \u03b1 = 0.5 for this purpose. To assess the relative ranking of each set relative to the baseline, we compute the scores between the baseline and each individual sample in the training set, ranking them from highest to lowest. These ranked samples are then divided into five sets based on their descending rankings (i.e., Set 1 Set 2, etc.), and each set is extended with the baseline. For this task, we utilized a GCN to perform the graph classification task, training the model on each of these five sets and evaluating its accuracy on the test set. The results are summarized in Table 2. The consistently superior performance of the top two sets supports the validity of our three metrics\u2014relevance, diversity, and structural disparity\u2014in evaluating and scoring graph datasets."}, {"title": "Conclusion", "content": "In this study, we introduced a novel framework for task-agnostic graph data valuation, leveraging both structural and featural representations. Our approach uses blind message passing (BMP) and graph Wasserstein distance (GWD) for effective alignment and comparison of graph structures, ensuring privacy and efficiency in data marketplaces. We demonstrated that our metrics\u2014structural disparity, diversity, and relevance\u2014are effective in capturing the essential characteristics of graph data that are crucial for valuation. Experimental results on real-world datasets validated our method, showing that higher-scored sets lead to improved performance in various applications. This work has significant implications for data marketplaces, enabling accurate data valuation. Future work will extend this framework to more complex graph structures and additional domains."}, {"title": "Algorithms", "content": "Here we outline algorithms for obtaining GWD, structural disparity, diversity, and relevance.\nWe note that the buyer's set of graphs is Gb = {G_1^b,...,G_n^b}, and the seller's set of graphs is Gs = {G_1^s,...,G_n^s} with X^b \u2208 R^{N^b\u00d7r} and X^s \u2208 R^{N^s\u00d7r} as the features of the graphs' nodes for the buyer and seller, respectively. First, we outline the algorithm for obtaining GWD between the buyer's and seller's graphs:\nW_1 (f(\\Phi^b), f(\\Phi^s)) = \\sum_{i=1}^{|V'|} W_1(\\overline{f}^b_i, \\overline{f}^s_i)\nDw(f(\u03a6\u266d), f(\u03a6*)) = \u03a3\u03bd=1 W1(f, f)"}, {"title": "Proofs", "content": "Corollary 1 (Transitivity): Two graphs G1 and G2 that are both matched with Gkey are \u00ea-conform with respect to \u03a0\u2081 \u2252 P*(Gkey, G\u2081) and \u041f\u2082 \u2252 P*(Gkey, G\u2082) for\n\\hat{\\epsilon} = \\frac{||L_{key} - \u03a0_{1}^{T}L_1 \u03a0_{1}||_F + ||L_{key} - \u03a0_{2}^{T}L_2 \u03a0_{2}||_F}{2} . \\tag{21}\nwhere Lkey, L1, and L2 are the normalized Laplacian for Gkey, G1, and G2, respectively.\nProof. Assume that graphs G1 and G2 match with Gkey under permutations \u03a0\u2081 and \u03a0\u2082, respectively. We would like to show that G1 and G2 are \u00ea-conform; that is:\n||\u03a0_1 L_1 \u03a0_{1}^{T} - \u03a0_2 L_2 \u03a0_{2}^{T}||_F \\leq \\hat{\\epsilon} .\\tag{21}\nFor the left side of the above inequality we have\n||\u03a0_1^{T} L_1 \u03a0_{1} - \u03a0_2^{T} L_2 \u03a0_{2}||_F \\\\\n= ||\u03a0_1^{T} L_1 \u03a0_{1} - L_{key} + L_{key} - \u03a0_2^{T} L_2 \u03a0_{2}||_F \\\\\n\\leq || L_{key} - \u03a0_1^{T} L_1 \u03a0_{1}||_F + ||L_{key} \u2013 \u041f_2^{T} L_2 \u041f_{2}||_F \\\\\n= \\hat{\\epsilon}. \\tag{22}\nwhere (a) results from the triangle inequality. This completes the proof."}, {"title": "Experimental details", "content": "Datasets\nWe test 26 commonly used benchmark datasets in our experiments. Except for the first three and last two, all datasets are selected from TUDataset (Kersting et al. 2016). These datasets include PubMed, Citeseer, Cora, BZR, COX2, DHFR, MUTAG, ENZYMES, KKI, Peking_1, PROTEINS, OHSU, MSRC_21, COIL-DEL, Letter-high, Letter-low, IMDB-BINARY, IMDB-MULTI, twitch-egos, COLORS-3, SYNTHETIC, FRANKENSTEIN, DD, AIDS, MNIST, and CIFAR10. The first three datasets are selected from the Planetoid dataset (Yang, Cohen, and Salakhudinov 2016), and the last two datasets were selected from GNNBenchmark (Dwivedi et al. 2023). The statistics of the datasets that we used in the appendix are summarized in Table 3.\nImplementation details\nFor the first task\u2014dataset scoring using structural disparity on node level prediction\u2014we employed a two-layer graph convolutional network (GCN) (Kipf and Welling 2016) with a hidden dimension of 16. Additionally, we used a two-layer GraphSAGE (Hamilton, Ying, and Leskovec 2017), also with a hidden dimension of 16, and a two-layer graph attention network (GAT) (Veli\u010dkovi\u0107 et al. 2017) with a hidden dimension of 16 and 8 attention heads. For clustering, we used the same GCN model with different parameters and applied KMeans (Lloyd 1982) with 5 clusters. This task was evaluated across four different random seeds, and we report the average results.\nFor the second task\u2014dataset scoring using relevance, diversity, and structural disparity\u2014we utilized a three-layer GCN, where each hidden layer has a dimension of 64. We applied global mean pooling to aggregate node features into graph embeddings. The final layer is a multi-layer perceptron (MLP) with a dropout rate of p = 0.5. The network was trained end-to-end using the Adam optimizer (Kingma and Ba 2014), with early stopping implemented to halt training if the validation loss did not improve for 25 consecutive epochs. The initial learning rate was set to 10-2, and training was capped at 1000 epochs. A batch size of 32 was used for all datasets. Cross-entropy loss served as the loss function, and each dataset was evaluated using four random seeds. Experiments were conducted on a Windows machine equipped with an AMD Ryzen\u2122\u2122 7 4800HS processor (8-core/16-thread, 12MB Cache, 4.2 GHz max boost), an NVIDIA\u00ae GeForce RTXTM 2060 with Max-Q Design (6GB GDDR6), and 64GB of RAM.\nAdditional experiments\n(iii) Featural attributes in practice\nTo evaluate the metrics diversity and relevance for capturing the featural attributes of the graphs, we conduct experiments on two datasets under various configurations. The results are summarized in Fig. 3. The first dataset is the MNIST graph data from the GNNBenchmark dataset (Dwivedi et al. 2023). We create six distinct sets all with the same size sampled from the MNIST dataset. We examine a scenario where the buyers' graphs consist of only classes 0 to 4 from the MNIST dataset. There are five sellers, each offering graphs from MNIST but with different class ranges: classes 0 to 4 (matching the buyer), 1 to 5, 0 to 9, 3 to 9, and 5 to 9. It is evident that the diversity and relevance of the data should increase and decrease progressively from seller 1 to seller 5, a trend that our proposed estimates clearly capture. Notably, the seller providing data spanning all classes from 0 to 9 offers a diversity-relevance pair approximating the point (0.5, 0.5). This balanced position indicates that while the data"}, {"title": "(iv) Structural context-awarenes", "content": "Here we evaluate our proposed method to capture the distance between the structural attributes of graphs. It is important to determine whether our proposed framework can effectively distinguish between graphs originating from different fields. To achieve this, we first create equal-sized sets sampled randomly from each of the datasets. We then embed each graph using the positional and structural embedders. Following this, we calculate the GWD between each pair of datasets. The datasets are categorized into five main groups: Molecules, Bioinformatics, Vision, Social, and Synthetic. The results of these calculations are summarized in Fig. 4.\nDatasets in the Molecules category, such as BZR, COX2, DHFR, and MUTAG, show very high positive scores within their field and low scores with datasets from other fields. This indicates that graphs within the Molecules category have similar structural properties. Similarly, datasets from the Bioinformatics category, including KKI, Peking, PROTEINS, and OHSU, display high correlations within their group. However, they tend to have a high similarity with some of the social media datasets (IMDB-BINARY and IMDB-MULTI). The varied scores demonstrate that our framework can effectively differentiate between graphs from different fields. High scores within categories confirm the framework's ability to recognize similar graph structures, while low scores between categories highlight its capacity to distinguish disparate graph structures. Nonetheless, there are some limitations in distinguishing between the Vision datasets and the social media datasets, as indicated by their higher-than-expected scores. This can be attributed to the relatively low average number of nodes in some of the datasets within the social media and vision groups (IMDB-BINARY: 19.77, IMDB-MULTI: 13.00, Letter-high: 4.67, Letter-low: 4.68). Given that our embeddings, particularly LapPE, are sensitive to the number of nodes, this sensitivity could be a limiting factor affecting performance on datasets with a low number of nodes.\n(v) Effects of using a proxy graph\nTo evaluate the impact of using a proxy graph like Gkey on the performance of the BMP framework, it is essential to examine whether both approaches-employing the proxy graph to compute the optimal permutation versus directly finding the optimal permutation\u2014select the sets of graphs for the buyer in the same order. This analysis is conducted using the same graph datasets as in the initial task of our experiment, which involved scoring datasets based on structural disparity in node level prediction. We begin by creating 4 and 6 subgraphs through node shuffling. We then assess the structural disparity between the first subgraph which we call baseline and the remaining subgraphs, ranking them based on proximity. For instance, if subgraph 2 has the highest structural similarity to the baseline, then subgraph 2 would be ranked as Rank 1 (R1) in Table 4. Our objective is to compare the structural disparity calculated for each candidate set using the two approaches: with and without the proxy graph. In the proxy-based approach, we first determine the optimal permutation between the baseline and the proxy graph, as well as between each of the subgraphs and the proxy graph. Subsequently, we compute the GWD between them. However, in the without proxy approach, we compute the GWD directly between the baseline and each of the subgraphs without utilizing the proxy graph. The results are summarized in Table 4. As observed, the use of proxy graphs consistently yields the same rankings as the approach without a proxy. This consistency supports our hypothesis that employing a proxy graph does not directly impact the performance of the BMP framework."}, {"title": "Discussions", "content": "Here we discuss various aspects of the proposed approach, ranging from its properties to possible extensions.\n1. Scalability: While our method is effective for moderate-sized graphs, its scalability to extremely large graphs remains untested. The computational complexity of GWD, which is O(N\u00b3), may pose challenges for very large datasets. Additionally, the matrix padding in computing \u0393 can result in very sparse matrices, potentially leading to inefficiencies. Future research should explore techniques such as graph coarsening, parallel computing, and approximate algorithms to enhance scalability.\n2. Feature incorporation: Our current approach only incorporates either featural attributes related to nodes or edges. In scenarios where both nodes and edges have features, our method falls short. Further investigation is needed to extend our approach to heterogeneous graphs and hypergraphs, where both node and edge features can be simultaneously considered. This would make the method more versatile and applicable to a wider range of datasets.\n3. Dynamic graphs: Our current approach is designed for static graphs. Many real-world applications involve dynamic graphs that evolve over time, such as social networks, financial transaction networks, and communication networks. Extending our methodology to handle dynamic graphs would involve developing methods to track and adapt to changes in graph structure and features over time, significantly enhancing its applicability and robustness in real-time environments.\n4. Privacy concerns: While we emphasize privacy through the BPM framework, the approach still requires sharing some structural information, which could potentially lead to sensitive information leakage. Future work should explore incorporating more advanced privacy-preserving techniques, such as secure multi-party computation, differential privacy, and homomorphic encryption, into the BMP framework. These techniques can ensure that graph valuations can be performed without compromising sensitive information, thereby making the approach more secure.\n5. Incorporating additional features: Introducing more metrics beyond diversity, relevance, and structural disparity could enhance the data valuation process. This would provide a more comprehensive assessment of the data's value.\n6. Incorporating utility functions: In this paper, we did not introduce specific utility functions to maintain generalizability. Future work could explore incorporating these metrics into a utility function tailored to the specific context or task."}]}