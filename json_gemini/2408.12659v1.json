{"title": "Disentangled Structural and Featural Representation for Task-Agnostic Graph Valuation", "authors": ["Ali Falahati", "Mohammad Mohammadi Amiri"], "abstract": "With the emergence of data marketplaces, the demand for methods to assess the value of data has increased significantly. While numerous techniques have been proposed for this purpose, none have specifically addressed graphs as the main data modality. Graphs are widely used across various fields, ranging from chemical molecules to social networks. In this study, we break down graphs into two main components: structural and featural, and we focus on evaluating data without relying on specific task-related metrics, making it applicable in practical scenarios where validation requirements may be lacking. We introduce a novel framework called blind message passing, which aligns seller's and buyer's graphs using a shared node permutation based on graph matching. This allows us to utilize the graph Wasserstein distance to quantify the differences in the structural distribution of graph datasets, called the structural disparities. We then consider featural aspects of buyer's and seller's graphs for data valuation and capture their statistical similarities and differences, referred to as relevance and diversity, respectively. Our approach ensures that buyers and sellers remain unaware of each other's datasets. Our experiments on real datasets demonstrate the effectiveness of our approach in capturing the relevance, diversity, and structural disparities of seller data for buyers, particularly in graph-based data valuation scenarios.", "sections": [{"title": "Introduction", "content": "With the advent of foundation models (Bommasani et al. 2021), the demand for large and diverse datasets has increased significantly. Data marketplaces (Agarwal, Dahleh, and Sarkar 2019; Stahl, Schomm, and Vossen 2014) have emerged as transformative platforms for exchanging data. These marketplaces allow data owners to sell their datasets and enable buyers to acquire essential data for their analytical and research needs. A notable development in this domain is the emergence of generative models as potential sellers in data marketplaces. Generative models, like generative adversarial networks (Goodfellow et al. 2020) and variational autoencoders (Kingma et al. 2013), can create synthetic datasets that mimic real-world data. These models generate high-quality, anonymized data that retains the statistical properties of the original datasets, making them particularly useful in situations where data privacy and scarcity are significant concerns. A critical aspect of these marketplaces is the valuation of data, which determines the quality and desirability of datasets. A data marketplace primarily consists of three entities: data sellers, a broker, and data buyers. Data sellers possess the data and provide it to the broker in exchange for compensation. Data buyers seek to obtain this data, with the broker facilitating the transactions. Given the value of data as a resource, it is crucial to develop a systematic approach to assess the value of the data for both sellers and buyers. This process, known as data valuation, is fundamental to ensuring a fair marketplace for all parties involved. Data valuation is a complex process that involves assessing the quality, relevance, and potential utility of data for specific applications concerning the buyer's demands. It is particularly valuable in fields such as finance, healthcare, marketing, and scientific research, where data-driven insights are crucial.\nData valuation can be performed based on either \"intrinsic\" or \"extrinsic\" factors. Intrinsic data valuation is data-driven and focuses on the quality of the dataset itself (Niu et al. 2018; Raskar et al. 2019). In contrast, extrinsic data valuation takes into account demand-supply dynamics and game-theoretic mechanisms (Luong et al. 2016; Zhang et al. 2020). Intrinsic data valuation is often paired with a utility metric for validation (Ghorbani et al. 2019; Jia et al. 2019), or with a specific machine learning (ML) task (Agarwal, Dahleh, and Sarkar 2019; Chen et al. 2019). In particular, for ML applications, data valuation frequently relies on the presence of a validation set, with validation accuracy serving as the metric (Wang et al. 2020; Yan et al. 2021). Additionally, the value of training data is often estimated by evaluating ML models trained on a specific target task (Pei 2020; Liu et al. 2021). In contrast, extrinsic data valuation techniques take into account external factors like competition and market demand (Agarwal et al. 2021; Bimpikis et al. 2019). This approach involves assessing customer demand for products and analyzing competitors' pricing strategies to determine the appropriate price for a product (Toni et al. 2017; Cong et al. 2022). In this paper, we concentrate on intrinsic data valuation for practical applications. However, tightly linking intrinsic data valuation to the existence of a validation set can be impractical. A universally accepted validation set may not be available, and a specific validation set might not adequately reflect the data distribution for a given learning task (Xu et al. 2021a). Moreover, possessing a validation set can enable malicious sellers to alter their datasets to overfit the"}, {"title": "", "content": "validation set. Additionally, focusing on a specific ML model or task for data valuation may not align with the interests of all stakeholders. Therefore, we adopt an intrinsic data valuation approach that does not rely on validation requirements and is performed prior to any tasks such as training an ML model (Amiri et al. 2023).\nAs one of the applications, in the rapidly evolving field of personalized medicine, particularly for cancer treatment, oncology researchers aim to identify the most effective therapies tailored to individual patients' genetic profiles. This involves leveraging data marketplaces (Agarwal et al. 2019) to find potential biomarkers and therapeutic targets that match patient-specific datasets. Researchers prioritize biomarkers with proteomic profiles similar to those found in their datasets. These genetic and proteomic interactions are often represented as graphs, where nodes represent genes or proteins, and edges represent their interactions (Gonz\u00e1lez-D\u00edaz et al. 2008; Bessarabova et al. 2012; Cannataro, Guzzi, and Veltri 2010). Structural similarity in genetic interactions can indicate similar responses to specific therapies, which is crucial for identifying effective treatments or repurposing existing drugs (Silva, Faria, and Pesquita 2022). Conversely, identifying dissimilar genetic structures can be essential to avoid adverse reactions and resistance, particularly when considering off-target effects or treatment for different cancer subtypes (Zhang et al. 2014; Chandak, Huang, and Zitnik 2023). Similarly, in drug discovery, structural similarity in molecules can suggest similar biological activity, which is important for repurposing drugs or optimizing lead compounds to improve efficacy and reduce side effects (Zitnik et al. 2018). Conversely, dissimilar structures might be preferred to avoid cross-reactivity and adverse effects, especially when dealing with off-target interactions or developing drugs for different disease subtypes (Chang et al. 2010). Traditional approaches (Amiri, Berdoz, and Raskar 2023; Fleckenstein, Obaidi, and Tryfona 2023; Xu et al. 2021b) to graph dataset comparison often rely on feature-based metrics that do not fully capture the intricate structural similarities and differences in the subjected graphs. A significant challenge arises due to the lack of visibility into sellers' graph data. Sending subgraphs is impractical and irrelevant. This lack of direct access makes it challenging to accurately assess the value and relevance of external datasets. Recent work by (Chi et al. 2024) addresses the problem of data valuation for graphs using Shapley valuation (Ghorbani et al. 2019). However, their approach is not task-agnostic and requires a validation set to compute the utility of the data valuation, which may not be practical. Additionally, a significant drawback of using Shapley valuation is the computational infeasibility; the computational cost grows exponentially with the number of samples, necessitating approximation methods that can compromise performance.\nInspired by the mentioned challenges for valuing graph datasets, our paper introduces three metrics for evaluating a seller's graph dataset for a buyer in a task-agnostic manner, focusing on structural and featural attributes of graphs. We assume a buyer and a seller, each with their own graph datasets, where the goal is to value the seller's graph data for the buyer. We break down each graph into its structural and featural aspects, analyze them separately, and integrate"}, {"title": "", "content": "the analysis to understand their unique characteristics. In particular, we use structural attributes to capture the distance between graphs' structures and use their featural attributes to measure the similarities and differences in their statistical properties. We enable this by using a blind message passing framework with two unique characteristics.\nDouble-Blindness: It ensures double-blindness to each party's dataset, meaning that the buyer does not have access to part or all of the seller's dataset, and vice versa. This prevents any party from tampering with or gaining advantage from the data.\nTask-Agnostic: It is task-agnostic, meaning it is not dependent on a specific learning algorithm or utility function. Instead, it takes the output of a learning algorithm (machine learning model) and/or a dataset as input and outputs a real-value score. This makes the framework generalizable to any context that uses graphs as its modality.\nOverall, our contributions can be summarised as follows:\nWe introduce a novel metric, termed structural disparity, specifically designed for graph datasets. This metric can be utilized independently or in conjunction with the featural attributes of data to offer a comprehensive valuation of graph datasets.\nWe introduce a framework called blind message passing for task-agnostic graph dataset exchange. Our proposed framework is adaptable to datasets with varying graph structures, node/edge types, and sizes. It ensures double-blindness, meaning neither the buyer nor the seller has access to the counterpart's data. This prevents data manipulation and ensures fair valuation.\nTo the best of our knowledge, this is the first paper to investigate data valuation for graphs in a task-agnostic manner, eliminating the need for validation sets. We hope our work will inspire further research in this area, given its broad potential applications across various fields."}, {"title": "Preliminary", "content": "Graph representation: Let a graph $G$ be defined as $G = (V,E)$ where $V = \\{1, ..., v_v \\}$ is the set of nodes, with a cardinality $|V| = N$, and $E = \\{e_1,...,e_m\\}$ is the set of edges, with a cardinality $|E| = M$. $G$ can be represented by an adjacency matrix $A \\in \\{0,1\\}^{N \\times N}$, with $A_{ij} = 1$ if nodes $v_i$ and $v_j$ are connected and $A_{ij} = 0$ otherwise.\nL1-Wasserstein distance: The Wasserstein distance is a measure of dissimilarity between probability distributions defined on a specific metric space. Let's denote two such distributions as $p_1$ and $p_2$, operating on a metric space $H$ (Villani et al. 2009). The L1-Wasserstein distance with Euclidean distance as the ground distance is given by:\n$W_1(p_1,p_2) = \\inf_{\\Upsilon \\in \\Gamma(p_1,p_2)} \\int_{H \\times H} ||x - y||_2d\\gamma(x, y)$.\nHere, $\\Gamma(p_1, p_2)$ denotes the set of all possible couplings (or joint distributions) $\\gamma$ whose marginals are $p_1$ and $p_2$, respectively. The term $||x - y||_2$ represents the Euclidean distance between points $x$ and $y$ in the metric space $H$. The integral"}, {"title": "", "content": "$\\int_{H \\times H} ||x - y||_2d\\gamma(x, y)$ computes the expected value of the Euclidean distance between the points under the coupling $\\gamma$.\nData marketplace: In data marketplace we assume that there exists multiple sellers and multiple buyers each with their own graph datasets. The objective is to find the relative value of the sellers' datasets with respect to the datasets that that the buyers already have. For the sake of simplicity, we assume a single buyer and a single seller scenario. We denote the set of graphs in the buyer and seller by $G_b = \\{G_1^b,...,G_{n_b}^b\\}$ and $G_s = \\{G_1^s,..., G_{n_s}^s\\}$, respectively, where $G_l = (V_l,E_l)$ with $|V_l| = N_l$ and $|E_l| = M_l$. Each graph $G_l$ has the adjacency matrix $A \\in \\{0,1\\}^{N_l \\times N_l}$, for $l\\in \\{b, s\\}$. Furthermore, the nodes' features of the graph $G_i^s$ is $X_i^s \\in \\mathbb{R}^{N_i \\times r}$, where $r$ is the number of features for each graph's node. We define $X^l$ as the vertical concatenation of $X_i^l$, i.e., $X^l := [(X_1^l)^T ... (X_{n_l}^l)^T]^T \\in \\mathbb{R}^{N_l \\times r}$, where we define $N^l := \\sum_{i=1}^{n_l} N_i$.\nDiversity and relevance: Following the work (Amiri, Berdoz, and Raskar 2023), we argue that the featural attributes of the graphs can be effectively represented by two metrics: diversity and relevance. We employ second moment summary statistics, specifically the empirical covariance matrix, to capture the statistical properties of the features. Next, we present the approach in (Amiri, Berdoz, and Raskar 2023) in estimating diversity and relevance between the features of nodes\u00b9 in buyer and seller. To estimate diversity and relevance, first, the buyer performs eigendecomposition on the covariance matrix $(X^b)^TX^b$; i.e.,\n$\\frac{1}{N_b} (X^b)^T X^b = U \\text{Diag}(\\lambda_1, ..., \\lambda_r) U^T$,\nwhere $\\lambda_i$ is the $i$-th largest eigenvalue of $\\frac{1}{N_b} (X^b)^T X^b$, and $U = [u_1 ... u_r]$ with $u_i \\in \\mathbb{R}^r$ denoting the eigenvector corresponding to the eigenvalue $\\lambda_i$. We note that $\\lambda_i \\geq 0$ since $(X^b)^T X^b$ is a positive semi-definite matrix. We further note that $u_1,...,u_r$ represent the principal directions containing the most significant information in the covariance matrix $(X^b)^T X^b$. The buyer shares the eigenvectors $u_1,...,u_r$ with the seller, while the eigenvalues $\\lambda_1,...,\\lambda_r$ stay local at the buyer. The seller then estimates the variance of its covariance matrix $(X^s)^T X^s$ along $u_1, ..., u_r$, the principal directions important to the buyer. This is carried out as:\n$\\hat{\\lambda_i} = \\frac{1}{N_s} ||(X^s)^T X^s u_i||_2, \\quad i = 1, ..., r$,\nwhere the covariance matrix $\\frac{1}{N_s}(X^s)^T X^s$ is first projected into $u_i$ and then the $l_2$-norm of the resultant vector provides the estimate of the variance (the data matrices are zero-centered). We note that if $u_i$ is an eigenvector of $\\frac{1}{N_s} (X^s)^T X^s$, then $\\hat{\\lambda_i}$ is its corresponding eigenvalue. We also note that, intuitively, $\\lambda_i$ and $\\hat{\\lambda_i}$ capture the significance of information with, respectively, the buyer's and seller's data"}, {"title": "", "content": "along a principal direction of the buyer's data. Buyer and seller share $\\lambda_i$ and $\\hat{\\lambda_i}$, for $i = 1, ..., r$, respectively, with the broker, which uses this information to estimate the diversity and relevance of the seller's data for the buyer. We estimate diversity and relevance based on the volume of the space specified by the coordinates corresponding to the principal components of the covariance matrix of the buyer's data:\n$D = \\left(\\prod_{i=1}^{r} \\frac{\\vert \\lambda_i - \\hat{\\lambda_i} \\vert}{\\max\\{\\lambda_i, \\hat{\\lambda_i}\\}} \\right)^{1/r}$,\n$R = \\left(\\prod_{i=1}^{r} \\frac{\\min\\{\\lambda_i, \\hat{\\lambda_i}\\}}{\\max\\{\\lambda_i, \\hat{\\lambda_i}\\}} \\right)^{1/r}$.\nThe diversity is correlated with the volume of the difference between the variance of the buyer's and seller's data in each coordinate; that is, $\\prod_{i=1}^{r} |\\lambda_i - \\hat{\\lambda_i}|$. On the other hand, the relevance is correlated with the volume occupied by both buyer's and seller's data in these coordinates; that is, $\\prod_{i=1}^{r} \\min{\\{\\lambda_i, \\hat{\\lambda_i}\\}}$. Furthermore, we normalize these estimates by dividing them by the entire volume, i.e., $\\prod_{i=1}^{r} \\max{\\{\\lambda_i, \\hat{\\lambda_i}\\}}$. Finally, we use geometric mean to keep these metrics within a reasonable range, particularly in the interval $[0, 1]$. It is easy to verify that $0 < D + R < 1$. Given the two metrics presented--diversity and relevance for comparing graphs based on their featural attributes--we will introduce methods for comparing graphs based on their structural attributes in the following section."}, {"title": "Structural attributes", "content": "In this section, we present a method to measure the structural disparity between two graphs using their positional and structural embeddings (Dwivedi et al. 2021; Cant\u00fcrk et al. 2023). To measure the structural disparity between graphs, it is essential to obtain embeddings for graphs that extract rich structural and positional features. We propose using the two common methods to generate these embeddings using positional and structural encodings to embed each graph, irrespective of its feature attributes.\nRandom walk structural embedding (RWSE) (Dwivedi et al. 2021): RWSEs are defined for $k$ steps of random walk for node $i$ of the graph:\n$z^{RW}_i = [RW_{ii}^1\\quad RW_{ii}^2 \\quad ... \\quad RW_{ii}^k] \\in \\mathbb{R}^k$,\nwhere $RW_{ii}^j$ is the probability of getting back to node $i$ after $j$ steps when we start walking from node $i$. The random walk operator is defined as $RW = AB^{-1}$, where $A \\in \\mathbb{R}^{N \\times N}$ is the adjacency matrix and $B \\in \\mathbb{R}^{N \\times N}$ is the degree matrix and $RW_{ii}$ is the $i$-th diagonal entry of $RW$.\nLaplacian eigenvenctor positional embedding (LapPE) (Dwivedi and Bresson 2020): LapPE provides accurate embeddings of graphs into Euclidean spaces. It is constructed by factorizing the graph Laplacian, defined as $\\Delta = I_N - B^{-1/2}AB^{-1/2} = U \\Lambda U^T$, where $I_N$ is the $N \\times N$ identity matrix, and the matrices $\\Lambda$ and $U$ represent the eigenvalues and eigenvectors, respectively. The absolute value of $l_2$-norm"}, {"title": "", "content": "of the first non-trivial $k'$ eigenvectors for node $i$ is denoted as $LP_i$. Hence, we define LapPE as:\n$z^P_i = [LP_{i1} \\quad LP_{i2} \\quad ... \\quad LP_{ik'}]^T \\in \\mathbb{R}^{k'}$.\nTo create an expressive embedder, we concatenate the RWSE and LapPE embeddings for each node:\n$z_i^{PE} = \\text{concatenate}(z^{RW}_i, z^{LP}_i) \\in \\mathbb{R}^{k+k'}$.\nFinally, the overall embedding for the graph is constructed by concatenating the positional and structural encodings of all nodes:\n$Z = [z_1^{PE} \\quad z_2^{PE} \\quad ... \\quad z_N^{PE}] \\in \\mathbb{R}^{(k+k') \\times N}$.\nAccordingly, we define function $\\text{Emb}(\\cdot) : G \\rightarrow \\mathbb{R}^{(k+k') \\times N}$, which takes a graph and outputs the embedding $Z$.\nOur main objective is to develop a method for comparing graphs based on their structural properties. We propose using the graph Wasserstein distance (GWD), inspired by (Togninalli et al. 2019). For simplicity, consider two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$; our goal is to compute GWD between them. To this end, we first need to align the two graphs; that is, we need to find a consistent permutation between the underlying graphs. For this purpose, in the following, we define graph matching on matrices.\nDefinition (Graph Matching2): Given two graphs $G_i = (V_i, E_i)$ and their normalized Laplacian matrices $L_i$ for $i \\in \\{1, 2\\}$, their matching can be represented by a permutation matrix $P \\in \\Pi$ that optimally aligns the graph structures."}, {"title": "", "content": "Formally, the optimal permutation $P^*$ is obtained as :\n$P^* (G_1, G_2) = \\arg \\min_{P \\in \\Pi} || L_1 - P^T L_2 P||_F$.\nDefinition (Distance-Compatible): Distance-compatible permutation of two graphs $G_1 = (V_i, E_i)$ is a permutation set $(\\Pi, \\hat{\\Pi})$ such that:\n$P^*(G_1, G_2) = \\hat{\\Pi}, \\quad P^*(G_2, G_1) = \\Pi$.\nDefinition (Graph Wasserstein Distance): Given two distance-compatible graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ and $|V| = \\max\\{|V_1|, |V_2|\\}$ with respective permutation set $(\\Pi, \\hat{\\Pi})$ and $\\text{Emb}(G_l) : G_l \\rightarrow \\mathbb{R}^{(k+k') \\times |V_l|}$ as the embedder we define GWD as\u00b3:\n$Z_l = \\Pi \\times \\text{Emb}(G_l), \\quad l\\in \\{1,2\\}$,\n$D_W(G_1, G_2) := \\sum_{i=1}^{\\nu} W_1 (Z_1[:, i], Z_2[:, i])$,\nwhere $Z_l[:, i]$ is the $i$-column of $Z_l$.\nIn the above approach, we embed the graphs, focusing solely on their structure. Prior works (Keriven and Vaiter 2024), (Srinivasan and Ribeiro 2020) have demonstrated that"}, {"title": "Blind message passing", "content": "In this section, we build on the concepts introduced earlier to compare the structural attributes of buyer and seller graphs within a data marketplace. This requires addressing two key challenges. First, both the buyer and seller possess multiple graphs, necessitating an extension of the structural disparity measure\u2014originally designed for pairs of graphs\u2014to accommodate comparisons between two sets of graphs. Second, it is crucial to ensure that the graphs remain local and are not shared. We further demonstrate how the disentanglement works in practice through the blind message passing (BMP) framework for the case both parties having multiple graphs.\nOne of the most important aspects of this framework lies in the blind exchange of data between both parties\u2014neither the seller nor the buyer has access to the counterpart's data, preventing any party from tampering with its own data to gain an advantage. As depicted in Fig. 1, a trustworthy broker who initially generates a simple proxy graph from some distribution, denoted by $G_{key}$ which remains unknown to both parties. To preserve privacy and distribute the computational cost among parties, $G_{key}$ is shared with both the seller and the buyer. Both parties then find their respective optimal permutations with regard to the proxy graph $G_{key}$ using the graph matching technique in Eq. (10). Next, we define $\\varepsilon$-Conformity, which will be used to calculate the error in finding the optimal permutation using the proxy graph.\nDefinition ($\\varepsilon$-Conformity): Two graphs $G_1$ and $G_2$ with permutations $\\Pi_1$ and $\\Pi_2$ are defined as $\\varepsilon$-conform if:\n$||\\hat{\\Pi}_1 \\hat{\\Pi}_1^T - \\hat{\\Pi}_2 \\hat{\\Pi}_2^T||_F \\leq \\varepsilon$.\nCorollary 1 (Transitivity): Two graphs $G_1$ and $G_2$ that are both matched with $G_{key}$ are $\\hat{\\varepsilon}$-conform with respect to $\\hat{\\Pi}_1 \\eqslantapprox P^*(G_{key}, G_1)$ and $\\hat{\\Pi}_2 \\eqslantapprox P^*(G_{key}, G_2)$ for\n$\\hat{\\varepsilon} = \\frac{|| L_{key} - \\hat{\\Pi}_1^T L_1 \\hat{\\Pi}_1 ||_F + || L_{key} - \\hat{\\Pi}_2^T L_2 \\hat{\\Pi}_2 ||_F}{2}$,\nwhere $L_{key}, L_1$, and $L_2$ are the normalized Laplacian for $G_{key}, G_1$, and $G_2$, respectively.\nRemark: The transitivity corollary ensures that the upper bound on the error when calculating the optimal permutation of $G_1$ and $G_2$ indirectly using a proxy graph like $G_{key}$ is $\\hat{\\varepsilon}$. We performed an experiment to demonstrate that incorporating graph matching yields competitive results compared to approaches that do not utilize a proxy graph, thereby validating the practicality of the theoretical upper bound.\nThe process of matching the graphs in both parties with $G_{key}$ aligns the graphs of both parties to a near-distance-compatible node permutation, enabling them to compare structural differences. After computing node embeddings"}, {"title": "Disentanglement in Practice", "content": "We now turn to the practical implementation of structural and featural disentanglement for graph data valuation. Each of the three metrics\u2014diversity (D), relevance (R), and structural disparity (S), as defined in (4), (5), and (19), respectively, offers unique insights into the dataset and can be used independently based on user preferences. These metrics are versatile and can be integrated into various utility functions tailored to specific contexts or datasets. To ensure generalizability, we propose using an average ranking approach. Specifically, consider a scenario where multiple sellers each possess a set of graphs, and there is a single buyer with their own graph set. We compute (D), (R), and (S) for each seller's graph set relative to the buyer's graph set. Each seller's set is then ranked according to these three metrics. The final ranking and valuation of the sellers' graph sets are obtained by averaging their rankings across all three metrics. As demonstrated in our experiments, this average ranking approach is effective in practice and suitable for real-world applications.\nComputational Complexity: Our framework consists of three primary computationally intensive algorithms. The first algorithm focuses on computing GWD. The naive approach to computing the Wasserstein distance has a complexity of O(n\u00b3 log(n)), where n denotes the number of node embeddings or the number of nodes in the graphs. To mitigate this computational burden, we employ several efficient acceleration techniques. Notably, approximations based on Sinkhorn regularization have been proposed (Cuturi 2013), which can significantly reduce the complexity to near-linear time.\nThe second algorithm addresses the graph matching problem via the linear assignment problem. To optimize computational efficiency and minimize the load on the broker, we use a proxy graph $G_{key}$. Instead of transmitting both datasets directly to the broker for the computation of graph matching, we delegate this task to the buyer and seller. By offloading the graph matching computation to the buyer and seller, the broker's responsibility is reduced to solely computing GWD. For two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ with $|V_1| = N_1$ and $|V_2| = N_2$ and $N = \\max\\{N_1, N_2\\}$, the paper (Liu, Scaglione, and Wai 2024) shows that the solution to the graph matching problem can be approximated by the following linear assignment problem and can be efficiently solved by the Hungarian method (Munkres 1957):\n$P^{**} = \\arg \\max_{P \\in \\Pi} \\text{tr}(P^T \\overline{U_1} (\\overline{U_2})^T)$.\nHere $\\overline{U_i} \\in \\mathbb{R}^{N \\times N}$ is the orthogonal matrix corresponding to eigenvectors from the eigendecomposition of $L_i = U_i \\Lambda_i U_i^T$ and $\\overline{U_i}$ is the matrix containing the absolute value of the entries of $U_i$, for $i \\in \\{1, 2\\}$. The complexity of the Hungarian algorithm is $O(n^3)$, where n is the number of nodes or, equivalently, the dimensions of the permutation matrix."}, {"title": "", "content": "The third algorithm involves the eigendecomposition of the covariance matrix. The eigendecomposition of an n x n matrix using standard numerical methods, such as the QR algorithm, typically has a computational complexity of O(n\u00b3). Considering these components, the overall complexity of our framework is O(n\u00b3) and n represents the number of nodes."}, {"title": "Experiments", "content": "We evaluate the BMP framework on four tasks: (i) dataset scoring using structural disparity on node level prediction then evaluating various graph neural network (GNN) models on these datasets, (ii) dataset scoring using relevance, diversity, and structural disparity and then evaluating graph convolutional network (GCN) (Kipf and Welling 2016) model on these datasets, (iii) assessing if the structural disparity metric can distinguish between different contexts, and (iv) testing the practical performance of the featural metrics. We used datasets from Table 1 for the experiments. Details about experiments (iii), (iv), and the experimental setup can be found in the supplementary materials.\nIn the first task, we explore whether structural disparity can effectively identify the most suitable graph among three candidates offered by sellers, given a baseline graph from the buyer. To quantify each graph's value, we augment the buyer's graph with the seller's graph and evaluate performance on a test set. We start by embedding a large graph using positional embedders, then partition it into three sets: buyer, seller, and test, with the test set containing 20% of the nodes and the buyer set 10%. Structural disparity serves as the sole metric for this task. We first train a simple GCN on the seller nodes to generate unsupervised node embeddings. These embeddings are then clustered using K-Means (Lloyd 1982), resulting in three distinct candidate sets. We use the BMP framework to assess the structural disparity between each candidate set and the buyer's set. The candidate sets are ranked by their proximity to the buyer's set, with the candidate set 1 denoted by subgraph 1 in Fig. 2 being the most similar. We then evaluate the performance using three different GNN models: GCN (distinct from the one used for clustering), GraphSAGE (Hamilton, Ying, and Leskovec 2017) with a mean aggregator, and GAT (Veli\u010dkovi\u0107 et al. 2017) with an attention mechanism. For each GNN, we train three models, each using a different candidate set combined with the baseline set. The test mask is used for evaluating node classification accuracy. As summarized in Fig. 2, our results demonstrate a clear trend: lower structural disparity between a candidate set and the baseline correlates with higher node classification accuracy. Clustering was employed to ensure distinction between candidates.\nIn the second task, we evaluate our approach for capturing both structural and featural attributes between graphs using three metrics: (D), (R), and (S). The dataset is divided into three disjoint sets: baseline, training, and test, with the test set comprising 20% of the data and the baseline 10%. Recognizing that relying on a single aggregating function from the three metrics could introduce bias dependent on the dataset, we adopt a more general approach. We first rank the dataset samples according to each metric independently, then calculate an average ranking across all metrics, setting $\\alpha = 0.5$ for this purpose. To assess the relative ranking of each set relative to the baseline, we compute the scores between the baseline and each individual sample in the training set, ranking them from highest to lowest. These ranked samples are then divided into five sets based on their descending rankings (i.e., Set 1, Set 2, etc.), and each set is extended with the baseline. For this task, we utilized a GCN to perform the graph classification task, training the model on each of these five sets and evaluating its accuracy on the test set. The results are summarized in Table 2. The consistently superior performance of the top two sets supports the validity of our three metrics\u2014relevance, diversity, and structural disparity\u2014in evaluating and scoring graph datasets."}, {"title": "Conclusion", "content": "In this study, we introduced a novel framework for task-agnostic graph data valuation, leveraging both structural and featural representations. Our approach uses blind message passing (BMP) and graph Wasserstein distance (GWD) for effective alignment and comparison of graph structures, ensuring privacy and efficiency in data marketplaces. We demonstrated that our metrics\u2014structural disparity, diversity, and relevance\u2014are effective in capturing the essential characteristics of graph data that are crucial for valuation. Experimental results on real-world datasets validated our method, showing that higher-scored sets lead to improved performance in various applications. This work has significant implications for data marketplaces, enabling accurate data valuation. Future work will extend this framework to more complex graph structures and additional domains."}, {"title": "Appendix / Supplementary Materials", "content": "Here we outline algorithms for obtaining GWD", "graphs": "nAlgorithm 1: GWD\nInput: $f(\\Phi^b) = [f_1^b \\quad f_2^b \\quad ... \\quad f_{|V^b|}^b"}]}