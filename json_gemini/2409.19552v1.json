{"title": "A Universal Deep Learning Framework for Materials X-ray Absorption Spectra", "authors": ["Shubha R. Kharel", "Fanchen Meng", "Xiaohui Qu", "Matthew R. Carbone", "Deyu Lu"], "abstract": "X-ray absorption spectroscopy (XAS) is a powerful characterization technique for probing the local chemical environment of absorbing atoms. However, analyzing XAS data presents with significant challenges, often requiring extensive, computationally intensive simulations, as well as significant domain expertise. These limitations hinder the development of fast, robust XAS analysis pipelines that are essential in high-throughput studies and for autonomous experimentation. We address these challenges with a suite of transfer learning approaches for XAS prediction, each uniquely contributing to improved accuracy and efficiency, as demonstrated on K-edge spectra database covering eight 3d transition metals (Ti-Cu). Our framework is built upon three distinct strategies. First, we use M3GNet to derive latent representations of the local chemical environment of absorption sites as input for XAS prediction, achieving up to order-of-magnitude improvements over conventional featurization techniques. Second, we employ a hierarchical transfer learning strategy, training a universal multi-task model across elements before fine-tuning for element-specific predictions. This cascaded approach after element-wise fine-tuning yields models that outperform element-specific models by up to 31%. Third, we implement cross-fidelity transfer learning, adapting a universal model to predict spectra generated by simulation of a different fidelity with a much higher computational cost. This approach improves prediction accuracy by up to 24% over models trained on the target fidelity alone. Our approach is extendable to XAS prediction for a broader range of elements and offers a generalizable transfer learning framework to enhance other deep-learning models in materials science.", "sections": [{"title": "I. INTRODUCTION", "content": "X-ray absorption spectroscopy (XAS) is a widely used materials characterization technique in a broad range of scientific research fields [1-3], such as condensed matter physics, materials science, chemistry and biology. XAS is element-specific and its near-edge region, known as X-ray absorption near-edge structure (XANES), contains rich information of the local chemical environment of the absorbing site (e.g., oxidation state, coordination number and local symmetry). Therefore, XANES measurements provide important insights into the structural and electronic properties of the sample, which are needed for mechanistic understanding of the relevant physical and chemical processes.\nHowever, XANES analysis is non-trivial, as the spectral function is a complex representation of the underlying atomic structure and electronic structure. Traditionally, extracting the information from XANES spectral features heavily relies on empirical fingerprints [4] from well-established experimental standards and/or first-principles simulations. These traditional XANES analysis approaches require strong domain expertise and, when combined with first-principles simulations, can be computationally expensive, which creates a practical barrier for many researchers.\nOn the other hand, rapid advances in synchrotron X-ray facilities enables XAS measurements with high time and energy resolution. For example, using a quick XAS scanning method, a spectrum can be measured with 10 ms time resolution [5]; the energy resolution in high-energy-resolution fluorescence-detected (HERFD) XAS can reach below the core-hole lifetime broadening [6]. Such experimental instrumental development creates the demand of XANES analysis of large scale temporal data. In addition, the deployment of autonomous experimentation pipelines [7-9] makes a strong case for real-time data analysis. To address these emerging challenges, a robust data-driven XANES analysis approach is needed to lower the barrier of entry for non-experts, reduce computational cost and accelerate throughput. In practice, a data-driven XANES pipeline requires multiple key building blocks, including (a) workflow software for high-throughput XANES simulations, (b) large and diverse simulated XANES spectra databases, and (c) tailored machine learning (ML) models that capture the structure-spectrum relationship.\nIn the past several years, significant progress has been made in this field. Systematic multi-code XANES benchmarks [10] were carried out to quantify the effects of key approximations and implementations in simulation, as well as to determine the converged parameters for spectral simulations. Workflow software [11, 12] has also been developed to standardize the spectral input file generation and ensure data reproducibility. Concurrently, the corpus of open-access, publicly available simulated XANES spectra databases [13-19] continue to grow, thus facilitating the development of ML models for XANES analysis.\nML models trained to predict spectra from atomistic structure, i.e. spectroscopy surrogate models, are of par-"}, {"title": "II. METHOD", "content": "In this work, we demonstrate how a variety of deep learning models can be used to accurately predict the XANES spectrum of a large class of materials. Fig. 1 highlights our workflow, which includes data acquisition and curation, as well as ML model development, training and evaluation.\nTo create a database of paired structures and the corresponding XANES spectra, as shown in Fig. 1a we pulled structural data from the Materials Project [26], generated the FEFF [28] and the Vienna Ab initio Simulation Package (VASP) [29] spectral simulation input files using Lightshow [12], and performed spectral simulations, post-processing and curation (Sec. II A). This module ultimately produces the structure-spectrum pairs which are used as the training, validation and testing data in our ML models.\nNext, multiple transfer learning ML models are developed for site-spectrum prediction of each element, where the featurization of the local structure of the absorbing site is taken from M3GNet [27] via a feature transfer process (Sec. II B- II C and Fig. 1b). We underscore that a universal model can be developed using the data of all eight elements and fine-tuned on specific element to further improve the model performance (Sec. IIB and Fig. 1c). Details of the model architectures are explained in Sec. II D. Training and evaluation of these models, such as splitting the data into training, validation and testing splits, are described in Sec. II E."}, {"title": "A. Data Acquisition and Curation", "content": "Before ML models can be trained, structure and spectral data need to be sourced, curated, and standardized. We used the Lightshow Python package [10, 12] to pull all available Materials Project structural data of pure compounds, and primary, secondary and ternary oxides via Pymatgen [30] for Ti, V, Cr, Mn, Fe, Co, Ni and Cu. FEFF9 [28] and VASP spectral input files were also written using Lightshow."}, {"title": "1. Input File Generation", "content": "FEFF calculations were performed in the real space, and the random phase approximation was used for core-hole screening. Self-consistent field and full multiple scattering lengths were set to 7 and 9 \u00c5, respectively. In VASP spectral simulations, we used the GW-type pseudopotential, in order to obtain a good description of high-energy scattering states [10]. The k-point mesh used in the Brillouin zone sampling is an important convergence parameter of the VASP spectral calculation, which is determined using the effective crystal size method [10] with the cutoff parameter set at 24 \u00c5 in Lightshow. The total number of bands (no) included in the spectral calculation depends on system size and the chosen energy range. Here we estimated no based on the uniform electron gas model [10] with the e_range parameter set to 30 eV (20 eV) for Ti (Cu) in Lightshow. The computational cost grows linearly with . To make the high-throughput calculations tractable, we only consider materials with no less than 2200 in VASP simulations."}, {"title": "2. Removal of Unphysical Spectra and Outliers", "content": "In the first data standardization step, site-specific spectra that failed sanity checks were discarded. This includes when the FEFF calculations did not converge with the default input or when the output FEFF spectra are not physical (e.g., have negative absorption coefficients). Next we adopt a data approach to remove outliers. For any spectrum i, consider the value of the absorption coefficient at energy grid point j to be \u03bc(i). The average absorption coefficient at energy grid point j of a total N spectra is given by\n$\\mu_j = \\frac{1}{N} \\sum_{i=1} \\mu_j^{(i)},$ (1)\nand the corresponding standard deviation (without bias correction, i.e., using N instead of N \u2212 1 in the normalization [31, 32]) is given by\n$\\sigma_j = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (\\mu_j^{(i)} - \\bar{\\mu}_j)^2}$ (2)\nAny spectrum \u03bc(i) = [\u03bc(i), \u03bc(i), ...] with a dimension M, in which |\u03bc(i)\u2212 \u03bcj | > c\u03c3j for any j = 1, ..., M, is removed from the database. In other words, spectra where the absorption coefficient lies outside of the envelope set by a chosen multiple of the standard deviation are removed. For FEFF and VASP spectra, we used standard deviation thresholds of c = 2.5 and 5, respectively. These threshold choices were determined empirically to remove the significant outliers observed in spectra heat maps while retaining most of the data."}, {"title": "3. Rescaling and Edge Alignment", "content": "The raw output of the FEFF spectra was re-scaled to obtain the absolute cross section with default broadening. The output of the VASP spectra is the imaginary part of the macroscopic dielectric constant. To compare results between FEFF and VASP, we converted the imaginary dielectric constant to cross section [10]. Raw VASP spectra were calculated purposely with a small broadening of 0.05 eV. This allows one to further broaden the spectra in post-processing as needed. We applied a 0.89 (3.46) eV core-hole lifetime broadening to Ti (Cu) VASP spectra, such that the spectral feature resolution in FEFF and VASP are comparable.\nRelative edge alignment was applied to VASP site-specific spectra using the ASCF method [10, 33]. After that, a constant empirical shift of 5108.59 (9492.80) eV is applied to VASP Ti (Cu) spectra to align with refer-"}, {"title": "B. XAS Prediction Hypotheses", "content": "We propose two key hypotheses for XAS prediction that guide the rest of the methodologies for ML model design:\n\u2022 Feature Transfer Hypothesis: The structural features in ML models trained to predict ground state properties of materials, such as energies and forces, provide sufficient transferable knowledge for XAS prediction models.\n\u2022 Universal XAS Model Hypothesis: A universal XAS model, trained on diverse elements, can extract core knowledge of the structure-spectrum relationship that is also transferable to improve element-specific predictive models.\nThese two hypotheses lead us to adopt a cascaded transfer learning approach when developing our ML models."}, {"title": "C. Transfer Learning", "content": "Transfer learning draws inspiration from human cognition, where knowledge acquired from one task can be applied to another [36]. This knowledge transfer approach is adopted in machine learning by leveraging a pre-trained source model to improve the training efficiency and performance of a distinct target model.\nLike in human learning, transfer learning is effective when the learning tasks of source and target models are related, if not identical. This method has found success in many fields [37], such as computer vision [38-40], natural language processing [41-43], and speech recognition [44], offering benefits like reduced training time, improved performance, and better generalization. Due to the high computational cost to generate XAS spectra using first-principles band-structure codes, constructing large XAS datasets with different levels of theory remains a bottleneck. Effective transfer learning provides a practical route to leverage data from different fidelities and significantly enhance the performance of XAS ML models.\nTransfer learning is categorized based on approaches used, level of similarity of source and target tasks as well as that of input (domain) data [45]. In this paper, we focus on two of such transfer learning categories: inductive-transfer-learning and domain-adaptation, for which we use the feature-transfer and fine-tuning approach, respectively.\nBased on the categorization proposed by Pan and Yang [45], transfer learning is classified according to the approaches used, the level of similarity between source and target tasks, and the similarity of input (domain) data. In this paper, we focus on two of the categories they define: inductive-transfer-learning and domain-adaptation. For these categories, we employ the feature-transfer and fine-tuning approaches, respectively."}, {"title": "1. Inductive Transfer Learning via Feature-Transfer", "content": "Our methods to investigate the \"Feature Transfer Hypothesis\" falls under the category of inductive transfer learning [45] in ML terminology, as the tasks between source and target models, while distinct, share similarities in predicting physical properties of materials based on their graph representations. To harness this potential, we selected a pre-trained interatomic potential model called M3GNet [27] as our transfer source, and hypothesize that it contains valuable, transferable knowledge for XAS prediction."}, {"title": "2. Domain Adaptation via Fine-tuning", "content": "To further investigate the \"Universal XAS Model Hypothesis,\" we complement our inductive transfer learning approach with domain adaptation. This method is particularly effective when the task remains consistent but the input data distribution shifts a common scenario when predicting XAS across diverse elements. Our domain adaptation strategy is implemented in two stages: a) the development of a universal XAS model, and b) fine tuning universal models with element-specific XAS data\nWe begin by developing a universal XAS prediction model designed to operate across eight 3d-transition metals. This model is trained on an extensive dataset generated from FEFF simulations, encompassing a wide array of structural and site configurations. The fundamental premise underlying this approach is rooted in the intuition that while XAS spectra exhibit element-specific characteristics, the core physical principles governing X-ray absorption remain sufficiently similar across elements, which lead to general spectral trends beyond a single element.\nFor example, since 1920 it has been established that the edge position in the XANES spectra strongly correlates with the oxidation state of the absorbing atom [46-49]. The absorbing atom can exhibit a positive (negative) edge shift depending on whether it is oxidized (reduced), which can be understood as the shielding effects of valence electrons on the core-electron energy levels. Another important trend in 3d transition metal K-edge XANES is the relation between the pre-edge features and the cation coordination number [4, 50]. This relation is governed by the dipole-forbidden 1s \u2192 3d transition in the pre-edge of the 3d transition metal. The pre-edge intensity vanishes in a perfect octahedron, but becomes more evident, when the inversion symmetry is broken that allows the mixing of p and d orbitals. Overall, the pre-edge feature is pronounced in early 3d transition metals (e.g., Ti) [4], but becomes less obvious in late 3d transition metals [51], as filled 3d orbitals in the latter block the 1s\u2192 3d transitions. Liang et al. constructed a rank-constrained adversarial autoencoder to disentangle individual structure-spectrum relationship in the same material space as this study [52]. In addition to oxida- tion state and coordination number, the authors identified distinct spectral features associated with a set of less well-known local structure descriptors, such as oxygen coordination number, standard deviation of nearest neighbor (NN) bond angles, and minimum O-O distance on the edges of the NN polyhedron [52]. The distinct spectral features are not limited to edge position and pre-edge intensity but also include more subtle ones based on the terminology of Guda et al. [16], such as main edge position and intensity, post-edge position and intensity, and curvature of the pit.\nGiven the domain knowledge on spectral trends beyond single element, we believe that ML models can learn to capture not just element-specific features, but also essential and transferable knowledge for XANES prediction, creating a robust foundation for element-specific adaptations. Following the training of the universal XAS model, we perform targeted fine-tuning for specific elements of interest. This process allows the model to adapt its learned representations to the unique spectral signatures of individual elements while preserving the broader understanding gained from the diverse training set.\nBased on the description above, we introduce the following model names used throughout the rest of the text as shown in Fig. 1 as well. We also collectively refer them as XASModels for rest of the text.\n\u2022 ExpertXAS- These models are trained to predict site-specific spectra for a particular element using only the corresponding subset of data from either FEFF or VASP simulations.\n\u2022 UniversalXAS - This model is trained to predict spectra for all eight 3d-transition metals using the complete set of data from FEFF simulations.\n\u2022 Tuned-UniversalXAS- These models are created by fine-tuning the UniversalXAS model using only the corresponding subset of data from either FEFF or VASP simulations."}, {"title": "D. XASModels", "content": "In this section, we introduce our model for site-specific XAS prediction. As outlined in Sec. II C1, we employ a feature-based transfer learning approach, leveraging the pretrained M3GNet model. We first describe the core components of M3GNet. Then, we explain how XAS-Models are constructed by combining two key elements: frozen feature extraction layers from M3GNet and new trainable neural network blocks designed specifically for XAS prediction, which we call XAS-blocks."}, {"title": "1. M3GNet-block", "content": "M3GNet is a graph deep learning model designed to predict the inter-atomic potential, forces, and stresses from material graph representation [27]. It was trained on a comprehensive dataset from the Materials Project, comprising over 17 million entries and encompassing 89 elements from the periodic table. The efficiency M3GNet was demonstrated by the study that predicted almost 2 million stable materials from a total of approximately 32 million candidate structures [27].\nInspired by the significance of three-body interactions in interatomic potentials, M3GNet distinguishes itself in its design from other graph deep learning models by embedding three-body interaction computations within it. Specialized \"many-body to bond\" neural blocks are used to update latent features, incorporating three-body interactions among atoms based on their precise atomic co-ordinates. These \"many-body to bond\" blocks are sandwiched between graph convolution layers. A pair of these \"many-body to bond\" and graph convolution layers is collectively referred to as an M3GNet-block.\nAs information flows through M3GNet, it passes through three of these M3GNet-blocks before final decoding using the readout block. We use the latent space representation from the final M3GNet block to encode structural information for XAS prediction."}, {"title": "2. Transfer-Features", "content": "The M3GNet architecture is designed to predict both global and local material properties, with its latent space at each layer containing information relevant to both. In contrast, XASmodels focus solely on local properties. This fundamental difference needs to be taken into account when attempting to leverage M3GNet's capabilities for XAS prediction through transfer learning. The key lies in identifying an appropriate subset of M3GNet's latent space that strikes a delicate balance. This subset must be localized, capturing the specific site of interest for XAS prediction to ensure specificity of results. Simultaneously, it needs to be comprehensive enough not to discard potentially valuable information (e.g., intermediate range structural information beyond the first near-neighbor shell) that could enhance XAS predictions. A significant complexity in this process stems from M3GNet's \"black box\" nature a common issue in machine learning where advanced models have internal mechanisms that are not easily interpretable [53, 54]. This lack of transparency makes it difficult to identify which aspects of the model's latent space are most crucial for accurate XAS prediction. To address these issues, we adopt the following approach for latent space selection: we extract node features from the graph after final graph convolution layer and just before M3GNet's readout block, focusing on the site where the absorption element information was initially encoded. Our reasoning is that M3GNet's training on both local and global property prediction may have naturally produced a latent space that can be segmented into clusters containing rich local information surrounding the sites where atomic data was initially encoded, while still preserving some global context relevant to XAS prediction. While intuitively appealing, it is important to note that the effectiveness of this approach needs to be validated, a task we undertake in Sections IIIB and IV B of this manuscript.\nThe M3GNet block can be represented as a function that takes in the material structure's graph representation S and generates features in latent space i.e.\n$H_o = f(S) \\in R^{n \\times 64}$ (3)\nHere, f : S \u2192 [Rn\u00d764 represents the M3GNet function that transforms an input structure S containing n atoms into a matrix Ho. Each row of Ho encapsulates the features around the location where an individual atom i was initially encoded in the graph.\nOur focus is site-specific, so we extract the feature vector for the i-th absorbing atom as:\n$h_i^{(o)} \\in R^{64}$ (4)\nThis vector, corresponding to the i-th row of Ho, serves as the input to the subsequent MLP layers and is henceforth referred to as transfer-features."}, {"title": "3. XAS-block", "content": "The XAS-block module harnesses the power of Multi-Layer Perceptrons (MLPs) to transform M3GNet-extracted transfer-features into accurate XAS spectra. At their core, MLPs are composed of interconnected neuron layers that sequentially process and refine input data. As transfer-features flow through the XAS-block, each layer applies complex nonlinear transformations, enabling the network to capture intricate relationships between transfer-features and resulting XAS spectra. The universal approximation theorem of MLPs, a fundamental principle in machine learning, mathematically establishes that a properly constructed XAS-block can arbitrarily closely approximate any continuous function mapping transfer-features to XAS spectra, provided sufficient network depth and optimal parameter configuration.\nEach layer of the MLP applies a carefully chosen sequence of operations: linear transformation, batch normalization, activation, and dropout. This sequence maintains the network's generalization capabilities and enhances learning. For hidden layers (l = 1, 2, ..., L \u2212 1), the computation is defined as:\nh\u2081 = Dropout\u0105(SiLU(BatchNorm\u0105 (Linear (h\u012b\u22121)))) \u2200l\u2208 {1,2,..., L \u2212 1} (5)\nThis formulation encapsulates the layer-wise transformation of features, with each operation serving a specific purpose in the learning process.\nThe final output layer differs slightly to others to ensure physically valid spectra by using Softplus. This layer guarantees non-negative intensity values, which is a physical constraint for XAS spectra,\n$\\hat{\\mu} = h_L = Softplus(Linear(h_{L-1}))$ (6)\nHere, \u00fb \u2208 R141 represents the predicted XAS spectrum across 141 energy grid points.\nEach component of the MLP serves a specific purpose in the learning process, contributing to the model's overall effectiveness:\n\u2022 Linear layers form the backbone of the MLP, enabling the network to learn complex mappings between input and output spaces:\nLinear (h) = W\u2081h + bi (7)\nHere, the weight matrix, Wi, and bias vector, bi, are learnable parameters that the network adjusts during training to capture the underlying patterns in the data.\n\u2022 Batch normalization plays a crucial role in stabilizing the training process. By standardizing the inputs to each layer, it helps mitigate the internal covariate shift problem, allowing for faster and more stable training [55].\n\u2022 The SiLU activation function introduces nonlinearity into the network, which is essential for capturing complex relationships. SiLU has been shown to outperform ReLU in many scenarios [56]:\nSiLU(x) = x \u03c3(x) (8)\nwhere \u03c3(x) = (1 + e\u00af*)\u22121 is the element-wise sigmoid function and is the element-wise product. The smooth nature of SiLU helps in gradient propagation during training.\n\u2022 Dropout serves as a powerful regularization technique, preventing overfitting by randomly deactivating a fraction of nodes during training. This encourages the network to learn more robust features and reduces its reliance on any single node [57]. We set dropout rate of 0.5 for all Dropout layers in XAS-block.\n\u2022 The Softplus activation in the final layer ensures that the output respects the physical constraints of XAS spectra:\nSoftplus(x) = log(1 + ex), (9)\nwhere all operations are applied element-wise. This function guarantees positive outputs, which is crucial as XAS intensities cannot be negative.\nThe flexibility of MLP architectures allows for extensive optimization of both structural and training parameters. Key structural variables include network depth (number of layers) and width (neurons per layer), which directly impact the model's capacity to learn complex patterns. Crucial training hyperparameters include batch size, influence optimization stochasticity and generalization ability.\nTo efficiently navigate this vast hyperparameter space, we employ Bayesian optimization techniques [58]. This approach systematically explores potential configurations, using validation set performance to guide the search for optimal combinations of architectural and training parameters. The exploration of the search space and the resulting optimized configurations for all XAS-Model instances are detailed in Supplementary Table S10."}, {"title": "E. XASModel Training", "content": "In this section, we describe our methodology for training XASModels. We introduce our data partitioning scheme, which gives more consistent split sizes across data sets of different sizes and material distributions. This also ensures that there is no material information leakage between them. We then detail the training steps taken to ensure robust model performance."}, {"title": "1. Balanced Material Splitting", "content": "For training, we first divide the data into training, validation, and test sets, used respectively for model fitting, hyperparameter tuning, and final performance evaluation. The goal of our XAS prediction models is to be generalizable to unseen materials, not just unseen absorbing sites. To achieve this, we employ materials splitting, where all XAS spectra from a single material's absorption sites are grouped together, ensuring they always fall into the same split. This ensures no data from any single material appears across multiple splits (e.g., a material with two atoms, in which one atom's data is in the training set and the other atom in the testing set), preventing data leakage and overly optimistic performance estimates. We call this approach materials splitting, which provides a more stringent assessment of a model's ability to generalize to novel compounds.\nMaterials splitting raise an issue in maintaining consistent proportions across training, validation, and testing sets. While we target specific ratios (e.g., 80/10/10), the uneven distribution of XAS data across materials complicates achieving these exact proportions. Some materials in our dataset have more XAS spectra for their unique absorption sites than others, creating an inherent imbalance. Consequently, when splitting by material, the actual data proportions in each set may slightly deviate from the intended ratios. This discrepancy is especially noticeable in datasets with a wide variance in XAS spectra per material or in smaller datasets. Such imbalances can potentially impact the consistency of model training and evaluation, necessitating careful consideration during the data preparation phase.\nWe designed a heuristic algorithm to address the issue of attaining desired proportions in the splits while maintaining materials splitting. The algorithm takes greedy approach when assigning unique materials to groups. It begins by sorting the unique materials by their frequency (number of sites) in descending order, then iteratively assigns them to groups. This sorting ensures that materials with the largest number of sites are handled first, allowing for more effective balancing of the splits later on. For each unique material, the algorithm calculates how adding it would affect each group's proximity to its target size. It then assigns the material to the group where this addition would most effectively reduce the gap between current and target sizes. This process continues until all unique materials are allocated."}, {"title": "2. Training", "content": "We trained the XASModels using the Adam optimizer [59] using the mean squared error (MSE) loss function,\n$J = \\frac{1}{N} \\sum_{i=1}^N J^{(i)},$ (10)\nwhere\n$J^{(i)} = \\frac{1}{M} \\sum_{j=1}^M (\\mu_j^{(i)} - \\hat{\\mu}_j^{(i)})^2.$ (11)\nN is the number of spectra in the training set and M is the number of points in the energy grid. \u03bc(i) and  represent the ground truth and the predicted absorption coefficient, respectively, at the j-th energy grid point of the i-th target spectrum.\nBefore the start of each training run, we used an automated search [60] to determine a good initial learning rate. To prevent overfitting of models, we terminated training whenever the minimum validation loss did not improve for 50 consecutive epochs. The maximum number of epochs was set to 1000, which was verified to be sufficient to allow for convergence in training of all model variants across all element datasets."}, {"title": "III. PRELIMINARY ANALYSIS AND DATA EXPLORATION", "content": "Data exploration serves as an important indicator of the overall data quality of the dataset and helps uncover potential connections between features and targets. This is a necessary step before training ML models.\nThe ML-data features and targets frequently exist in high-dimensional spaces, rendering data exploration challenging. Our work, for instance, employs 64-dimensional feature-transfer vectors and 141-dimensional target vectors for each data point. Each element contains at least 3,500 data points with a maximum of 14,834 for Mn in the FEFF database. To address the challenge of high data dimension, we use two approaches for preliminary data analysis and exploration: a) direct visualization of the spectral data using heatmaps and b) dimensionality reduction-assisted analysis of both feature and target data.\nWe employ Uniform Manifold Approximation and Projection (UMAP) to visualize our feature and target data [61]. UMAP is a powerful dimensionality reduction technique that effectively preserves both local and global data structures. It first constructs a high-dimensional representation using a fuzzy topological framework, specifically a weighted k-neighbor graph where edge weights represent the probabilities of connections between points. Then a low-dimensional representation is constructed with the same method, which is optimized by minimizing the cross-entropy between the high- and low-dimensional fuzzy representations through a stochastic gradient method. Due to its stochastic nature, UMAP projections are inherently non-deterministic and can vary based on parameters like the number of neighbors and minimum distance. While these variations can lead to different clustering outcomes, even a single projection that reveals clear clustering provides strong evidence for the existence of clusters in the original high-dimensional space. UMAP often uncovers intricate patterns and cluster structures that may be obscured in the original data [61]."}, {"title": "A. Spectra Visual Inspection", "content": "We proceed by direct visual inspection of the spectra database with focus first on the high-density region (dark red) of the entire FEFF database in Fig. 3. Ti and V XANES exhibit clearly visible pre-edge peaks. The pre-edge peak intensity decays when moving towards late 3d transition metals, consistent with previous observations [51]. Mn and Fe XANES show a much wider range in edge location between 10 to 20 eV than others, consistent with multiple commonly observed oxidation states. Mn XANES shows at least three edge positions as families of dark red curves and Fe XANES shows at least two with smaller spacing than Mn. Correspondingly, Mn and Fe spectra exhibit a broader spectral variation or data complexity than others. In contrast, Ni and Cu XANES show a relatively simple spectral shape with one dominant peak. Regarding Ti and Cu VASP database in Fig. 4, overall the spectral shape in VASP are similar to that in FEFF. However, the low density region (dark blue) in VASP has larger variations than FEFF in both elements."}, {"title": "B. Transfer-Features Clustering", "content": "We select the node features of the absorbing site within the M3GNet latent space for transfer-feature extraction, as detailed in Sec. IID 2. While this choice is intuitively appealing, it necessitate careful examination due to the feature smoothing phenomenon. Feature smoothing occurs when node features gradually become similar with each other in deeper layers of the model [62, 63]. This happens because the deeper convolutional layers increasingly expand the effective receptive field from which the features are being aggregated. In the case of M3GNet, excessive feature-smoothing in the M3GNet block would significantly dilute local structure information around the absorbing element site, thereby hindering the site-specificity of predicted XAS and model performance.\nWe provide empirical evidence that any feature smoothing in the M3GNet block is sufficiently mild. We demonstrate this by visualizing the transfer-feature in low dimensions with UMAP and identifying similarity clusters that also correspond to the local-structure labels and physical descriptors. Specifically we use the oxidation state (OS) of the absorbing cation, the number of nearest neighbors of the absorbing cation atom, also known as the coordination number (CN), and the average number of nearest neighbors of oxygen atoms in the first shell of the absorbing cation, also known as the oxygen coordination number (OCN)\u00b9 as the labels.\nWe plot two-dimensional UMAP of all transfer features in the FEFF ML dataset in Fig. 5a-d, colored by element type and local physical descriptors: OS, CN, and OCN. In doing so, we make two key observations as summarized below that provide compelling evidence that the latent-subspace-based transfer features can be used for XAS prediction without concern for potential feature smoothing inside the M3GNet block.\n\u2022 Element Clustering - The transfer-feature UMAP in Fig. 5a shows distinct similarity clusters based on the absorbing element type. The figure clearly\n1 For example, consider an absorbing Ti atom with 2 nearest neighbor Zn and two nearest neighbor O atoms. If one of the O atoms has 2 nearest neighbors, and the other O atom has 4 nearest neighbors, then the OCN value for the absorbing Ti atom is 3."}, {"title": "C. Spectra Clustering", "content": "Now we turn to the UMAP clustering of the FEFF spectra as shown in Fig. 5e-h. In prior studies [21, 22, 51], spectral analysis through dimensionality reduction, such as principal component analysis (PCA), is typically performed on datasets of a single absorbing element. This is because XANES is an element-specific technique, and one can trivially distinguish the K-edge XANES of 3d transition metals from their absolute edge position energy. In this study, we apply UMAP to the complete FEFF database (Ti-Cu) defined on a relative energy scale, thereby removing the information regarding absolute edge positions. This approach allows the UMAP analysis to focus exclusively on spectral shapes. By visualizing the entire FEFF dataset in a single UMAP plot, we can identify relationships and trends that transcend individual elements, highlighting how spectral features correlate with broader material properties.\nAs shown in Fig. 5e, distinct clustering patterns form for each element. In addition, elements next to each other in the periodic table stay close together: early 3d transition metals (Ti, V, Cr and Mn) and late 3d transition metals (Fe, Co, Ni and Cu). However, there is not a clear separation between early and late 3d transition metals. The spectral similarity between different elements is supported by the observation in the raw spectra in Fig. 3, e.g., the pronounced pre-edge in Ti, V and Cr, and the single peak feature in Ni and Cu.\nFurther spectra clustering analysis was performed by labeling the UMAP pattern with physical descriptors (OS, CN and OCN) as shown in Fig. 5f-h. Within each element, one can see sub-level clustering with respect to the physical descriptors. For example, there is a dominant Ti4+ cluster (yellow) accompanied by a much smaller Ti2+ cluster (purple), while the Ti3+ cluster (red) is less significant and more scattered. This is consistent with the fact that the Ti database is dominated by Ti4+ cations. Furthermore, there is a clear distinction in Mn and Fe clusters with respect to OS (2+, 3+ and 4+),"}, {"title": "IV. RESULTS", "content": "When evaluating XASModels, it is imperative to fairly evaluate and compare the performance of numerous models which have been trained on different subsets of the spectra dataset of varying scales associated with the level of theory (the multiple scattering method in FEFF and the core-hole pseudopotential method"}]}