{"title": "Understanding Help-Seeking Behavior of Students Using LLMs vs. Web Search for Writing SQL Queries", "authors": ["Harsh Kumar", "Mohi Reza", "Jeb Mitchell", "Ilya Musabirov", "Lisa Zhang", "Michael Liut"], "abstract": "Growth in the use of large language models (LLMs) in programming\neducation is altering how students write SQL queries. Traditionally,\nstudents relied heavily on web search for coding assistance, but\nthis has shifted with the adoption of LLMs like ChatGPT. However,\nthe comparative process and outcomes of using web search versus\nLLMs for coding help remain underexplored. To address this, we\nconducted a randomized interview study in a database classroom\nto compare web search and LLMs, including a publicly available\nLLM (ChatGPT) and an instructor-tuned LLM, for writing SQL\nqueries. Our findings indicate that using an instructor-tuned LLM\nrequired significantly more interactions than both ChatGPT and\nweb search, but resulted in a similar number of edits to the final\nSQL query. No significant differences were found in the quality\nof the final SQL queries between conditions, although the LLM\nconditions directionally showed higher query quality. Furthermore,\nstudents using instructor-tuned LLM reported a lower mental de-\nmand. These results have implications for learning and productivity\nin programming education.", "sections": [{"title": "INTRODUCTION", "content": "A recent survey [8] of over three thousand programmers revealed\nthat 84% are using AI tools, with ChatGPT being the most popu-\nlar-74.9% of developers use it on a weekly basis. The most popu-\nlar use-case (over 80%) is using these tools as a search engine for\nnew topics, highlighting the potential for LLM-based chatbots to\nsupplement the programming learning process alongside conven-\ntional search methods. For structured tasks such as formulating\nSQL queries, LLM chatbots offer the unique ability to generate\ntailored responses coupled with explanations. This contrasts with\ntraditional search methods, where learners must hunt for and adapt\ncode snippets from sites like Stack Overflow.\nDespite the growing popularity of LLM chatbots for search\nengine-like tasks, the comparative impact of traditional web search\nversus LLM chatbots in real-world programming classrooms, par-\nticularly in supplementing student engagement and learning of\nlanguages like SQL, is not well-understood. Empirical insights into\nhow students engage with these tools can inform the design of bet-\nter learning support tools, especially in flipped classroom settings\nwhere self-regulated learning through out-of-class practice plays\nan important role. Furthermore, exploring whether tuning off-the\nshelf chatbots like ChatGPT using cost-effective and easy methods,\nsuch as adding system prompts, can enhance their usefulness for\nstudents is an open question that is of interest to many instructors\nwho are navigating the integration of AI tools into their teaching,\nwhile trying to mitigate many of the common concerns that off-\nthe-shelf LLM chatbots pose, like their tendency to generate direct\nanswers, or their lack of awareness of course specific content and\ncontext.\nTo address these issues, we conducted a mixed-methods ran-\ndomized interview study with 39 students where we compared\ntraditional search (e.g., Google, Bing) with both standard ChatGPT\n(3.5 model, which was the frontier at the time of the experiment)\nand an instructor-tuned version of the chatbot with added guard\nrails (e.g. being told not to give out direct answers) and course\ncontext (e.g. a description of the learning goals and content covered\nby the course). We asked two research questions:\n\u2022 RQ1: How does traditional web search (e.g., Google, Bing)\ncompare to LLM-based chatbots like ChatGPT in terms of\nstudent engagement and learning outcomes in programming\neducation, particularly for languages like SQL?\n\u2022 RQ2 Can low-cost tuning methods, such as adding system\nprompts, be employed by instructors to enhance the effec-\ntiveness of LLM-based chatbots like ChatGPT for educational\npurposes, and if so, how does this tuning influence student\nengagement and learning outcomes?\nWe found that students interacted with the instructor-tuned\nLLM more than twice as much compared to both standard ChatGPT\n(p = 0.01) and web search (p < 0.0001). Despite this increased\nengagement, there were no significant differences in the correct-\nness of the final SQL queries across conditions, although the LLM\nconditions showed higher query quality directionally. These re-\nsults suggest the potential value of domain experts tuning LLMs\nusing inexpensive methods like system prompts to enhance learner\nengagement.\nThe main contributions of this work are:"}, {"title": "2 RELATED WORK", "content": "The advent of large language models (LLMs) has prompted compar-\native research with traditional web search methods for information\nretrieval and problem solving. Recent studies have compared web\nsearch and LLM querying for general information-seeking tasks.\nWazzan et al. found that web search outperformed LLMs in accuracy\nin a geolocation task, with LLM users struggling to formulate effec-\ntive queries [29]. Xu observed that LLM users spent less time on\ntheir tasks and demonstrated more consistent performance across\neducation levels, but accuracy suffered in fact-checking and com-\nplex tasks compared to participants using web search [32]. Spathar-\nioti et al. reported that LLM users completed product comparison\ntasks more quickly by using fewer, more complex queries [26].\nWhile LLM users generally reported higher satisfaction and per-\nceived response quality [26, 32], their accuracy was dependent on\nthe reliability of LLM-provided information and effective prompting\n[26, 32].\nDespite widespread adoption of LLMs in programming tasks\n[9, 10, 12, 15, 21], fewer comparative studies have been conducted\nbetween web search and LLM querying in computer science and\ncomputer science education. Research shows that professionals\nselect between web search and querying LLMs through search\nstrategies that utilize self-reflection on their knowledge of the prob-\nlem and domain [33]. Yen et al. found that web search is preferred\nwhen professionals are unfamiliar with the domain, or the problem\nis poorly-defined, because it returns greater diversity of results than\nLLM queries. Using an LLM is preferred when the user believes\nthe problem is discussed frequently enough that the model has\nawareness of it, and they possess sufficient knowledge to identify\nincorrect responses [33]. In contrast, research into students' use of\nweb-based resources showed that students often exhibit shallow,\ntrial-and-error approaches without clear strategy or self-reflection\n[30]. While some students use web search for general reference\n[24], other students tend to seek quick answers and exact code\nmatches to specific problems [30]. They tend to exhibit a produc-\ntion bias where they focus on solving the immediate task by quick\nweb searches rather than engaging with foundational concepts in\ncourse resources [30]. LLMs provide a potential solution to these\nstrategy and orientation problems. Students view LLMs as provid-\ning more specific, easily understandable responses [24] and utilize\nthem for various programming education tasks like generating\npractice exercises, clarifying error messages, or providing tips on\nsyntax [5].\nHowever, for programming novices, both approaches have po-\ntential pitfalls. Web searches can lead to the faulty integration\nof poorly-understood code, resulting in compounding errors [25]."}, {"title": "3 METHODS", "content": "We conducted a mixed-methods randomized interview study with\n39 students where we compared traditional web search with both\nstandard ChatGPT (3.5 model, which was the frontier at the time of\nthe experiment) and an instructor-tuned version of the chatbot with\nadded guard rails (e.g. being told not to give out direct answers) and\ncourse context (e.g. a description of the learning goals and content\ncovered by the course). Students were provided Entity-Relationship\ndiagrams and then asked to solve two SQL-writing problems, one\nafter the other, with one of the randomly selected source of help\nfor each question."}, {"title": "3.1 Context of Deployment", "content": "This research study occurred in a 12-week introductory database\nsystems course at a large, publicly funded, research-intensive uni-\nversity in North America, and received ethics board approval (Ethics\nProtocol #123456\u00b9). This course targeted third-year undergraduate\nstudents in a four-year honors computing program, with 226 stu-\ndents enrolled in the course. The course used a flipped classroom\napproach, which allowed students to engage with video content\nand exercises on a custom Learning Management System (LMS)\nbefore attending synchronous in-person lecture sessions. Given the\nflipped approach, students often sought external resources such\nas web search and LLM-based chatbots to supplement their self-\nregulated learning, making this an ideal context for deploying the\nstudy.\nThe study was advertised as an optional activity to test the ef-\nfectiveness of an LLM-based chatbot tutor. 39 students volunteered\nto participate and were then enrolled in the interview study. The\ninterviews were conducted over Zoom and lasted almost 30 minutes\n(each round was time-boxed to 15 minutes). During the interview\nstudy, each student was randomly assigned to interact with two\nsources of help, one after the other, to solve two different types\nof SQL writing problems (randomized in order). 21 were assigned"}, {"title": "3.2 Experimental Conditions", "content": "This study considered three distinct sources of help in writing code."}, {"title": "3.2.1 Web Search.", "content": "Students in this round could freely use any web\nsearch engine to find resources to help write the SQL query. All\nstudents used Google search and then navigated to coding blogs\nand forums such as GeeksForGeeks, StackOverflow, etc. Figure 1A\nshows an example query issued by one of the students in the study."}, {"title": "3.2.2 ChatGPT.", "content": "In this round, students were directly given access\nto ChatGPT (3.5). No additional tweaks were made to the model.\nThis acted as a proxy for publicly available LLMs being used by\nstudents to solve assignment problems on their own. Figure 1B\nshows a student's interaction with ChatGPT during the study."}, {"title": "3.2.3 Instructor-tuned LLM.", "content": "Students in this round were given\naccess to a chatbot using GPT-3.5 (the frontier model at the time\nof the study). The model was configured with a system prompt to\nbe particularly helpful for writing the SQL queries and provided\nthe context of the questions used in the study. This acted as a\nproxy for instructor-provided LLM chatbots used in classrooms\n(e.g., KhanMigo, CS50 bot [19], etc.) Figure 1C shows the chatbot\ninterface used in the study."}, {"title": "3.3 Outcome Measures", "content": "Each interview study session was recorded on video, and the follow-\ning outcome measures were manually extracted from the videos."}, {"title": "3.3.1 Number of Interactions with the Source of Help.", "content": "We measure\nthis by calculating the number of queries sent to the assigned source\nof help. Each query constitutes an interaction."}, {"title": "3.3.2 Number of Edits Made to the Final SQL Query.", "content": "This was\nmeasured by calculating the number of changes made to the final\nSQL query submitted by the student, during each round of the\nstudy."}, {"title": "3.3.3 Quality of the SQL Query.", "content": "We utilized a grading rubric spec-\nified by the course instructor for the two types of questions. Based\non this rubric, one of the authors assigned a score (0-100) to each\nof the final SQL queries of the students."}, {"title": "3.3.4 Self-reported Mental Demand.", "content": "We used the Mental Demand\nsubscale from the NASA-TLX questionnaire [22] to compare the\nmental demands of completing the SQL-writing task with their\nassigned source of help. On a scale of 1 (very low) to 10 (very high),\nstudents were asked to rate how mentally demanding the task was."}, {"title": "4 RESULTS", "content": ""}, {"title": "4.1 Effect on Number of Interactions", "content": "Figure 2 left-facet shows the average number of interactions with\nthe assigned source of help by condition. We performed a Kruskal-\nWallis H test to compare the number of interactions across the\nthree conditions and the results indicated a statistically significant\ndifference between the groups ($\\chi^2(2) = 20.5$, df = 2, p < 0.0001)."}, {"title": "4.2 Effect on Number of SQL Query Edits/Changes", "content": "Figure 2 right-facet shows the average number of changes made to\nthe final SQL query provided by the student. The Kruskal-Wallis H\ntest for changes between conditions was not significant ($\\chi^2(2) =$\n1.55, df = 2, p = 0.46). In all conditions, students needed just under\n2 changes to arrive at their final query in the assigned time. How-\never, there were students in the Instructor-tuned LLM condition\nwho made over 5 changes in their final SQL query (Figure 2)."}, {"title": "4.3 Effect on the Quality of the Final SQL Query", "content": "Figure 3 right-facet shows the average correctness of the final SQL\nqueries by condition. The Kruskal-Wallis H test for the correctness\nbetween conditions was not significant ($\\chi^2(2) = 3.85$, df = 2,\np = 0.14). However, correctness was directionally higher for both\nLLM conditions than for the web search condition."}, {"title": "4.4 Self-Reported Mental Demand", "content": "The Kruskal-Wallis H test for mental demands was not significant\n($\\chi^2(2) = 2.05$, df = 2, p = 0.36). However, there is suggestive\nevidence that students in the Instructor-tuned LLM conditions re-\nported that the task was less mentally demanding compared to\nthe other conditions (see Figure 3 left-facet). When comparing\nthe instructor-tuned LLM with ChatGPT, one of the students com-\nmented \"[instructor-tuned LLM] seem more accurate or specific than\nchatGPT.\" This may indicate lesser mental demand on the student."}, {"title": "5 DISCUSSION", "content": "Key Findings. We found that students needed to interact more\nwith the Instructor-tuned LLM compared to ChatGPT and Web\nSearch. This increased interaction could be attributed to the system\nprompt for the Instructor-tuned LLM, which included sentences\nsuch as \"The instructor does not provide the exact answer to the given\nproblem...\", as well as other guardrails to prevent cheating with LLMs\n[6, 14]. One might hypothesize that this would lead to lower grades\nfor students using the Instructor-tuned LLM versus ChatGPT and\nWeb Search, which can readily provide direct answers. However,\nour results did not show significant differences in the quality of"}, {"title": "6 CONCLUSION", "content": "We conducted a randomized interview study to compare students'\nuse of LLMs (out-of-the-box and instructor-tuned) with conven-\ntional web search (status quo) for writing SQL queries. Our findings\nsuggest that an instructor-tuned LLM might lead to higher en-\ngagement, on average, compared to other sources of help while\nmaintaining the quality of downstream performance on the given\ntask. Preliminary evidence also points to a reduction in students'\nself-reported mental demand for writing SQL queries while utilizing\nthe instructor-tuned LLM. These findings highlight the potential of\ndesigning LLM-based instructional resources with the participation\nof teachers and have implications for the field of productivity and\nthe future of work, in addition to education technology."}]}