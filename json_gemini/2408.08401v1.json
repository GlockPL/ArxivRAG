{"title": "Understanding Help-Seeking Behavior of Students Using LLMs vs. Web Search for Writing SQL Queries", "authors": ["Harsh Kumar", "Mohi Reza", "Jeb Mitchell", "Ilya Musabirov", "Lisa Zhang", "Michael Liut"], "abstract": "Growth in the use of large language models (LLMs) in programming education is altering how students write SQL queries. Traditionally, students relied heavily on web search for coding assistance, but this has shifted with the adoption of LLMs like ChatGPT. However, the comparative process and outcomes of using web search versus LLMs for coding help remain underexplored. To address this, we conducted a randomized interview study in a database classroom to compare web search and LLMs, including a publicly available LLM (ChatGPT) and an instructor-tuned LLM, for writing SQL queries. Our findings indicate that using an instructor-tuned LLM required significantly more interactions than both ChatGPT and web search, but resulted in a similar number of edits to the final SQL query. No significant differences were found in the quality of the final SQL queries between conditions, although the LLM conditions directionally showed higher query quality. Furthermore, students using instructor-tuned LLM reported a lower mental demand. These results have implications for learning and productivity in programming education.", "sections": [{"title": "1 INTRODUCTION", "content": "A recent survey [8] of over three thousand programmers revealed that 84% are using AI tools, with ChatGPT being the most popular-74.9% of developers use it on a weekly basis. The most popular use-case (over 80%) is using these tools as a search engine for new topics, highlighting the potential for LLM-based chatbots to supplement the programming learning process alongside conventional search methods. For structured tasks such as formulating SQL queries, LLM chatbots offer the unique ability to generate tailored responses coupled with explanations. This contrasts with traditional search methods, where learners must hunt for and adapt code snippets from sites like Stack Overflow.\nDespite the growing popularity of LLM chatbots for search-engine-like tasks, the comparative impact of traditional web search versus LLM chatbots in real-world programming classrooms, particularly in supplementing student engagement and learning of languages like SQL, is not well-understood. Empirical insights into how students engage with these tools can inform the design of better learning support tools, especially in flipped classroom settings where self-regulated learning through out-of-class practice plays an important role. Furthermore, exploring whether tuning off-the-shelf chatbots like ChatGPT using cost-effective and easy methods, such as adding system prompts, can enhance their usefulness for students is an open question that is of interest to many instructors who are navigating the integration of AI tools into their teaching, while trying to mitigate many of the common concerns that off-the-shelf LLM chatbots pose, like their tendency to generate direct answers, or their lack of awareness of course specific content and context.\nTo address these issues, we conducted a mixed-methods randomized interview study with 39 students where we compared traditional search (e.g., Google, Bing) with both standard ChatGPT (3.5 model, which was the frontier at the time of the experiment) and an instructor-tuned version of the chatbot with added guard rails (e.g. being told not to give out direct answers) and course context (e.g. a description of the learning goals and content covered by the course). We asked two research questions:\n\u2022 RQ1: How does traditional web search (e.g., Google, Bing) compare to LLM-based chatbots like ChatGPT in terms of student engagement and learning outcomes in programming education, particularly for languages like SQL?\n\u2022 RQ2 Can low-cost tuning methods, such as adding system prompts, be employed by instructors to enhance the effectiveness of LLM-based chatbots like ChatGPT for educational purposes, and if so, how does this tuning influence student engagement and learning outcomes?\nWe found that students interacted with the instructor-tuned LLM more than twice as much compared to both standard ChatGPT (p = 0.01) and web search (p < 0.0001). Despite this increased engagement, there were no significant differences in the correctness of the final SQL queries across conditions, although the LLM conditions showed higher query quality directionally. These results suggest the potential value of domain experts tuning LLMs using inexpensive methods like system prompts to enhance learner engagement.\nThe main contributions of this work are:\n\u2022 Findings from a randomized experiment in a real-world SQL classroom comparing web search with plain ChatGPT and an instructor-tuned LLM, demonstrating how instructor-tuning can significantly impact engagement.\n\u2022 A useful snapshot of GPT-3.5 usage for SQL education, serving as a benchmark for future studies with newer models.\n\u2022 Empirical insights into web-search and LLM chatbot usage that can inform the design of learning and productivity tools for programming education using AI."}, {"title": "2 RELATED WORK", "content": "The advent of large language models (LLMs) has prompted comparative research with traditional web search methods for information retrieval and problem solving. Recent studies have compared web search and LLM querying for general information-seeking tasks. Wazzan et al. found that web search outperformed LLMs in accuracy in a geolocation task, with LLM users struggling to formulate effective queries [29]. Xu observed that LLM users spent less time on their tasks and demonstrated more consistent performance across education levels, but accuracy suffered in fact-checking and complex tasks compared to participants using web search [32]. Spatharioti et al. reported that LLM users completed product comparison tasks more quickly by using fewer, more complex queries [26]. While LLM users generally reported higher satisfaction and perceived response quality [26, 32], their accuracy was dependent on the reliability of LLM-provided information and effective prompting [26, 32].\nDespite widespread adoption of LLMs in programming tasks [9, 10, 12, 15, 21], fewer comparative studies have been conducted between web search and LLM querying in computer science and computer science education. Research shows that professionals select between web search and querying LLMs through search strategies that utilize self-reflection on their knowledge of the problem and domain [33]. Yen et al. found that web search is preferred when professionals are unfamiliar with the domain, or the problem is poorly-defined, because it returns greater diversity of results than LLM queries. Using an LLM is preferred when the user believes the problem is discussed frequently enough that the model has awareness of it, and they possess sufficient knowledge to identify incorrect responses [33]. In contrast, research into students' use of web-based resources showed that students often exhibit shallow, trial-and-error approaches without clear strategy or self-reflection [30]. While some students use web search for general reference [24], other students tend to seek quick answers and exact code matches to specific problems [30]. They tend to exhibit a production bias where they focus on solving the immediate task by quick web searches rather than engaging with foundational concepts in course resources [30]. LLMs provide a potential solution to these strategy and orientation problems. Students view LLMs as providing more specific, easily understandable responses [24] and utilize them for various programming education tasks like generating practice exercises, clarifying error messages, or providing tips on syntax [5].\nHowever, for programming novices, both approaches have potential pitfalls. Web searches can lead to the faulty integration of poorly-understood code, resulting in compounding errors [25]."}, {"title": "3 METHODS", "content": "We conducted a mixed-methods randomized interview study with 39 students where we compared traditional web search with both standard ChatGPT (3.5 model, which was the frontier at the time of the experiment) and an instructor-tuned version of the chatbot with added guard rails (e.g. being told not to give out direct answers) and course context (e.g. a description of the learning goals and content covered by the course). Students were provided Entity-Relationship diagrams and then asked to solve two SQL-writing problems, one after the other, with one of the randomly selected source of help for each question."}, {"title": "3.1 Context of Deployment", "content": "This research study occurred in a 12-week introductory database systems course at a large, publicly funded, research-intensive university in North America, and received ethics board approval (Ethics Protocol #123456\u00b9 This course targeted third-year undergraduate students in a four-year honors computing program, with 226 students enrolled in the course. The course used a flipped classroom approach, which allowed students to engage with video content and exercises on a custom Learning Management System (LMS) before attending synchronous in-person lecture sessions. Given the flipped approach, students often sought external resources such as web search and LLM-based chatbots to supplement their self-regulated learning, making this an ideal context for deploying the study.\nThe study was advertised as an optional activity to test the effectiveness of an LLM-based chatbot tutor. 39 students volunteered to participate and were then enrolled in the interview study. The interviews were conducted over Zoom and lasted almost 30 minutes (each round was time-boxed to 15 minutes). During the interview study, each student was randomly assigned to interact with two sources of help, one after the other, to solve two different types of SQL writing problems (randomized in order). 21 were assigned"}, {"title": "3.2 Experimental Conditions", "content": "This study considered three distinct sources of help in writing code."}, {"title": "3.2.1 Web Search", "content": "Students in this round could freely use any web search engine to find resources to help write the SQL query. All students used Google search and then navigated to coding blogs and forums such as GeeksForGeeks, StackOverflow, etc. Figure 1A shows an example query issued by one of the students in the study."}, {"title": "3.2.2 ChatGPT", "content": "In this round, students were directly given access to ChatGPT (3.5). No additional tweaks were made to the model. This acted as a proxy for publicly available LLMs being used by students to solve assignment problems on their own. Figure 1B shows a student's interaction with ChatGPT during the study."}, {"title": "3.2.3 Instructor-tuned LLM", "content": "Students in this round were given access to a chatbot using GPT-3.5 (the frontier model at the time of the study). The model was configured with a system prompt to be particularly helpful for writing the SQL queries and provided the context of the questions used in the study. This acted as a proxy for instructor-provided LLM chatbots used in classrooms (e.g., KhanMigo, CS50 bot [19], etc.) Figure 1C shows the chatbot interface used in the study."}, {"title": "3.3 Outcome Measures", "content": "Each interview study session was recorded on video, and the following outcome measures were manually extracted from the videos."}, {"title": "3.3.1 Number of Interactions with the Source of Help", "content": "We measure this by calculating the number of queries sent to the assigned source of help. Each query constitutes an interaction."}, {"title": "3.3.2 Number of Edits Made to the Final SQL Query", "content": "This was measured by calculating the number of changes made to the final SQL query submitted by the student, during each round of the study."}, {"title": "3.3.3 Quality of the SQL Query", "content": "We utilized a grading rubric specified by the course instructor for the two types of questions. Based on this rubric, one of the authors assigned a score (0-100) to each of the final SQL queries of the students."}, {"title": "3.3.4 Self-reported Mental Demand", "content": "We used the Mental Demand subscale from the NASA-TLX questionnaire [22] to compare the mental demands of completing the SQL-writing task with their assigned source of help. On a scale of 1 (very low) to 10 (very high), students were asked to rate how mentally demanding the task was."}, {"title": "4 RESULTS", "content": ""}, {"title": "4.1 Effect on Number of Interactions", "content": "Figure 2 left-facet shows the average number of interactions with the assigned source of help by condition. We performed a Kruskal-Wallis H test to compare the number of interactions across the three conditions and the results indicated a statistically significant difference between the groups ($\\chi^2(2) = 20.5$, df = 2, p < 0.0001)."}, {"title": "4.2 Effect on Number of SQL Query Edits/Changes", "content": "Figure 2 right-facet shows the average number of changes made to the final SQL query provided by the student. The Kruskal-Wallis H test for changes between conditions was not significant ($\\chi^2(2) = 1.55$, df = 2, p = 0.46). In all conditions, students needed just under 2 changes to arrive at their final query in the assigned time. However, there were students in the Instructor-tuned LLM condition who made over 5 changes in their final SQL query (Figure 2)."}, {"title": "4.3 Effect on the Quality of the Final SQL Query", "content": "Figure 3 right-facet shows the average correctness of the final SQL queries by condition. The Kruskal-Wallis H test for the correctness between conditions was not significant ($\\chi^2(2) = 3.85$, df = 2, p = 0.14). However, correctness was directionally higher for both LLM conditions than for the web search condition."}, {"title": "4.4 Self-Reported Mental Demand", "content": "The Kruskal-Wallis H test for mental demands was not significant ($\\chi^2(2) = 2.05$, df = 2, p = 0.36). However, there is suggestive evidence that students in the Instructor-tuned LLM conditions reported that the task was less mentally demanding compared to the other conditions (see Figure 3 left-facet). When comparing the instructor-tuned LLM with ChatGPT, one of the students commented \"[instructor-tuned LLM] seem more accurate or specific than chatGPT.\" This may indicate lesser mental demand on the student."}, {"title": "5 DISCUSSION", "content": "Key Findings. We found that students needed to interact more with the Instructor-tuned LLM compared to ChatGPT and Web Search. This increased interaction could be attributed to the system prompt for the Instructor-tuned LLM, which included sentences such as \"The instructor does not provide the exact answer to the given problem...\", as well as other guardrails to prevent cheating with LLMs [6, 14]. One might hypothesize that this would lead to lower grades for students using the Instructor-tuned LLM versus ChatGPT and Web Search, which can readily provide direct answers. However, our results did not show significant differences in the quality of the final SQL queries between conditions. This is promising, as greater engagement could potentially lead to longer-term learning [3, 16]. Students expressed interest in using instructor-tuned LLM over ChatGPT and said \"Would rather use [instructor-tuned LLM] over ChatGPT given the prior knowledge of the tables and helps with practical examples of how to join two tables.\u201d This supports the idea that scaffolded learning, in which students are guided but not given direct answers, can be as effective as direct instruction [13, 20]. Interestingly, the higher number of interactions with the Instructor-tuned LLM did not result in higher reported mental demand. In fact, students who used Instructor-tuned LLM reported levels of mental demand that were equal to or lower than those using ChatGPT and Web Search. Lowering the cognitive load can make programming more approachable, potentially reducing dropout rates in CS and encouraging more students to pursue and persist in the field [23, 28]. Additionally, there were no differences in the number of changes made to the final SQL query between the different conditions. This suggests that while the Instructor-tuned LLM may require more interaction, it does not necessarily increase the mental burden on students and maintains the same level of code refinement as other methods [4, 11]. In summary, our findings highlight the potential of using LLMs as facilitators of learning rather than just sources of information and contribute to the growing literature of designing pedagogically informed LLM-tutors [14, 18].\nBroader Implications. Studies such as this show the value of low-cost instructor tuning through system prompting for increasing student engagement. Instructor-tuned chatbots can serve as \"levers\" for instructors wishing to amplify student engagement in their courses outside of the classroom by offering personalized support to students through chatbots that have been \"tuned\" with course-specific context and content. The provision of these chatbots may also reduce over-dependency on general-purpose chat agents like ChatGPT [2, 7, 17]. At the same time, the effectiveness of these levers will depend on both the accuracy of the models themselves (something we can expect to improve with future frontier models) and the ability of students to ask the right questions (through prompts, which is harder than it may appear to users [16, 34]). Without the latter, increased engagement may not translate into increased learning, as we saw with the ratings of the quality of the final query in our study. Therefore, providing better support for students by asking the right questions is a useful direction for"}, {"title": "6 CONCLUSION", "content": "We conducted a randomized interview study to compare students' use of LLMs (out-of-the-box and instructor-tuned) with conventional web search (status quo) for writing SQL queries. Our findings suggest that an instructor-tuned LLM might lead to higher engagement, on average, compared to other sources of help while maintaining the quality of downstream performance on the given task. Preliminary evidence also points to a reduction in students' self-reported mental demand for writing SQL queries while utilizing the instructor-tuned LLM. These findings highlight the potential of designing LLM-based instructional resources with the participation of teachers and have implications for the field of productivity and the future of work, in addition to education technology."}]}