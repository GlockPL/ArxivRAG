{"title": "Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAl Co-Creation Workflows", "authors": ["Frederic Gmeiner", "Nicolai Marquardt", "Michael Bentley", "Hugo Romat", "Michel Pahud", "David Brown", "Asta Roseway", "Nikolas Martelaro", "Kenneth Holstein", "Ken Hinckley", "Nathalie Riche"], "abstract": "Despite Generative AI (GenAI) systems' potential for enhancing content creation, users often struggle to effectively integrate GenAI into their creative workflows. Core challenges include misalignment of AI-generated content with user intentions (intent elicitation and alignment), user uncertainty around how to best communicate their intents to the Al system (prompt formulation), and insufficient flexibility of Al systems to support diverse creative workflows (workflow flexibility). Motivated by these challenges, we created IntentTagger: a system for slide creation based on the notion of Intent Tags-small, atomic conceptual units that encapsulate user intent-for exploring granular and non-linear micro-prompting interactions for Human-GenAI co-creation workflows. Our user study with 12 participants provides insights into the value of flexibly expressing intent across varying levels of ambiguity, meta-intent elicitation, and the benefits and challenges of intent tag-driven workflows. We conclude by discussing the broader implications of our findings and design considerations for GenAI-supported content creation workflows.", "sections": [{"title": "Introduction", "content": "Generative AI (GenAI) models have become increasingly powerful in content creation tasks, and are rapidly being integrated into a range of professional applications, including coding IDEs [16, 32], text processors [13, 34], image and video editors [1, 35, 73], and office suites [58]. These developments offer tremendous promise for AI as a co-creation tool steered by people, but significant challenges remain:\n(1) It is hard to align Al-generated content with user intentions (intent elicitation and alignment) [16, 88], often due to the difficulty in balancing the flexibility of free-form text prompts with guided interfaces;\n(2) Users often struggle with formulating effective prompts and understanding an AI system's capabilities, leading to trial-and-error interactions (prompt formulation) [49, 84, 100];\n(3) The iterative nature of content creation, which requires continuous reflection and refinement, further complicates intent elicitation and prompt formulation since users' needs and intents aren't often completely clear upfront [14, 79, 82, 89];\n(4) Many GenAI systems impose workflows that force users to adapt their commonly used creative processes or generate content with each refinement, further complicating human-AI collaboration (workflow flexibility) [75, 87].\nIn this work, we explore potential solutions to these challenges around intent elicitation and alignment, prompt formulation, and workflow flexibility through the lens of GenAI-supported rich content creation tasks, such as slide deck creation. Slide deck creation involves complex decisions around content structure, visual style, media content, slide sequencing, and narrative flow, all influenced by implicit factors like audience or presentation duration. Often, slide decks reference external documents, evolve over time, and may be duplicated and changed for different audiences and purposes. Technically, GenAI multi-modal foundation models offer powerful capabilities to support such complex content creation tasks, for example, by transforming documents or synthesizing various content sources into a single slide deck [21, 30, 95]. However, most human-GenAl interaction challenges are amplified in the context of such complex content creation scenarios-the diverse and nonlinear workflows in slide deck creation, coupled with the difficulty of expressing requirements upfront, make it challenging for users to effectively integrate GenAI into their specific creation process.\nTo address these challenges and empower users in rich AIassisted content creation tasks, we propose intent tags as a granular and flexible technique for GenAI-supported workflows via graphical micro-prompting. Intent tags represent atomic conceptual units that encapsulate a single aspect of a user's intent. Intent tags help users steer content generation in non-linear workflows-the user instantiates multiple intent tags and edits each individually through adaptive UI elements that allow for granular control of content generation and transparent intent elicitation. To support a creative design process where users rapidly explore multiple alternatives, iterate, and reflect on the outcomes, our system proactively surfaces Al-generated suggestions through intent tags. These suggestions appear in an unobtrusive manner, allowing people to see the parameter spectrum [89] of possible outcomes that they might not have considered or been aware of and use them to iteratively refine the system's outputs as desired.\nTo explore further possibilities of intent tagging for human-GenAI co-creation workflows for rich content creation, we created IntentTagger: a GenAI-driven system allowing users to iteratively create and modify slide deck presentations using intent tags on a 2D canvas interface. Through intent tags, users steer the generation of single slides or the entire deck by defining keywords (concept tags) or including content from other documents or images (reference tags). Under the hood, IntentTagger utilizes an LLM for slide generation. IntentTagger also dynamically generates adaptive UI elements such as context-related tag suggestions, dynamic drop-down lists with slide preview tooltips, and interactive slider widgets to help users fine-tune tag expressions.\nWe explore the benefits and limitations of intent tagging through a lab user study with 12 participants. The study comprised comparative closed-ended and semi-open-ended slide-deck-related tasks that participants completed using our prototype system and, in some comparative tasks, using GenAI features from an existing commercial slide authoring tool. Our findings indicate that users felt more in control and satisfied with intent tag-based interactions than with existing chat-based and design gallery-based generative Al systems for slide deck creation. Participants especially appreciated the support of non-linear and iterative workflows, the ability to express their intent in flexible ways with varying levels of ambiguity, and the integrated system suggestions as a valuable and non-distractive aid for helping them think through the task and figure out what they want while working on the task.\nBuilding upon these study findings, we illustrate how intent tagging could be generally utilized for facilitating human-GenAI"}, {"title": "Related Work", "content": "In the field of Human-Computer Interaction (HCI), the concept of Human-AI interaction [2] and AI as co-creators [22, 62] highlight both the opportunities and challenges presented by GenAI models. Opportunities lie in automating repetitive and tedious tasks, such as data transformation or formatting, and augmenting creative processes, such as code, text, or image creation [13, 16, 69, 73], while challenges include aligning Al outputs with user intent and users' uncertainties about effective prompts and AI system's capabilities [16, 100].\nA core challenge in designing effective interactive GenAI-driven systems is ensuring that AI-generated content matches user intentions. This involves two main aspects: the user communicating their intent to the system and the system interpreting the user's intent or guiding them in gradually disclosing it (intent elicitation). Traditionally, wizard interfaces have emerged to guide users through predefined option GUI dialogues to elicit their intent step-by-step for complex tasks [20]. However, natural language processing (NLP) advancements have introduced text-promptingbased GenAI models, such as large-language foundation models (LLMs), that allow users to input their intent as free-form text prompts [45] or guide users through a dialogue to elicit their intents [15, 64, 67, 74].\nHowever, all such approaches present opposing trade-offs (see Table 1): GUI-based menus or wizards can intuitively guide user decisions and reduce ambiguity while allowing for continuous representation of options through graphical manipulatable elements, but they only offer a finite set of options that reflect pre-anticipated use case scenarios by software makers. Free-form text prompting allows users to communicate their intent more flexibly using natural language, but the lack of scaffolding and guidance requires users to know what they want (or need) and how to formulate it as an effective prompt. While easy for simple tasks, communicating intents can be challenging in complex content creation tasks when users might not be aware of all necessary options upfront and are required to iteratively figure these out (intent exploration). As a compromise, natural language chat dialogues can provide systemguided option exploration and scaffolding. However, such systems can only probe on a limited set of options in a reasonable time frame or number of conversational turns, and they often lack continuous option representation and graphical manipulation.\nOn the other hand, effectively aligning AI-generated content with user intentions also requires awareness of one's own intentions and understanding of the possible parameter spectrum [89]. Recent work in Al-mediated intent elicitation and sensemaking has explored various strategies and GenAI-driven tools to assist users in more open-ended and iterative ways to elicit and discover their (creative) intent. For example, systems like Luminate [85], Selenite [44], Sensecape [86] and Graphologue [36] utilize dynamic LLM-driven GUIs to support users in sensemaking and intent elicitation (e.g., exploring a design space, available options, or topics) by generating and visually clustering semantically related concepts or criteria to help them explore relevant dimensions and parameters matching their tacit intent and needs. Similarly, SymbolFinder [65] offers different LLM-driven GUIs to suggest and group related words and images to guide designers in gradually exploring and"}, {"title": "Systems for Supporting Prompting and Steering GenAI", "content": "Generative AI foundation models, such as large language or diffusion models [9, 72], represent a paradigm shift in artificial intelligence, offering task-agnostic pre-training on large-scale data for various downstream applications [77]. Although extremely versatile, research suggests three properties that make interacting with GenAI challenging: input flexibility (in handling free-form language, images, code, etc.), generality (applicability to a wide range of tasks), and originality (ability to generate novel content) [76].\nPrior HCI research has documented interaction challenges in prompting and steering GenAI systems across domains such as coding, illustration design, or engineering [33, 43, 46]. For example, users frequently struggle to craft text-based input prompts that will achieve desired outcomes, and face difficulties interpreting and repairing erroneous outputs [100]. Research has begun to explore mechanisms and interfaces to better support users in working with text prompt-based GenAI models. For example, various works have proposed mechanisms to support users in prompt engineering [45] through \"prompt augmentation,\" which automatically modifies and extends a users' input prompt to improve the model's generated output [7, 11, 81]. While such techniques can improve model output"}, {"title": "Non-Linear Content Creation and Iterative Design Workflows", "content": "Creating rich content documents, such as blog posts or slide presentations, involves crafting and integrating text, visuals, and multimedia to convey complex ideas while maintaining coherence and audience engagement. Guidelines for creating slide presentations often suggest standardized workflows such as crafting outlines before creating slides [3, 71, 101]. However, other literature emphasizes that slide-creation workflows are less rigid and are mostly shaped by cultural factors and organizational norms [97]. For example, in organizations, people often start or continue presentations from different starting points, such as from existing documents, other slide decks, or templates.\nRich content creation processes are also iterative and diverse in nature, typically progressing through cyclically occurring stages [5, 23], where creators alternate between 'exploration' and 'exploitation' in a sense-making process to reach a final outcome [66]. Content creators continuously develop, reflect on, and act on their ideas and plans [38, 78]. The interaction between users' perception of the material and the material itself enables creators to develop, reflect on, and question their understanding, leading to new ideas and improved plans [37]. Research has underscored the role of reflection in content creation as a necessary moment for creators to situate their ideas and plans in the appropriate context [24, 61, 80]. However, existing challenges around prompt formulation engage users in cognitively demanding trial-and-error processes and hinder"}, {"title": "Systems for Supporting Slide Deck Creation", "content": "Creating slide presentations is a multifaceted task that involves various sub-tasks, and previous research has introduced a range of approaches to support different stages of this process.\nBesides popular slide-by-slide authoring tools like PowerPoint or Keynote, research has suggested interfaces to support users in iterative slide creation workflows by managing multiple presentation versions [25], prototyping slide decks using markup language [26] or authoring slide presentations on a 2D canvas interface [41] for supporting non-sequential workflows.\nBesides manual authoring systems, numerous works have explored ways to automatically generate slide presentations and rich content documents for single topics [95], or scientific [30], technical [48], educational [21], or semantically annotated source documents [52]. Other systems focus on automatically generating presentation-specific aspects, such as visual-textual layouts [96]. While these systems allow for automatic content transformation and slide creation, they lack methods for users to influence or control the generation process.\nSeveral works have, therefore, proposed mechanisms to enable users to generate presentations in more controllable and workflow-integrated ways, such as allowing data scientists to steer the generation of slide presentations from Jupyter notebooks by interactively selecting code cells and slide layouts [93] or through a user-controllable text outline [92]. Similarly, KnowledgeDecks [17] allows users to generate slides documenting data science knowledge-discovery processes from automatically collected user behavior events from visual analytic tools.\nPrevious work has explored various authoring interfaces, automatic slide generation from source documents, and usercontrollable slide generation in domain-specific applications like data science. Expanding on these approaches, we aim to explore interaction principles for GenAI-supported slide deck creation across diverse content tasks and workflows. Our approach combines GenAI-driven slide generation from multiple input types (e.g., prompts, text documents, images) with non-predefined and nonlinear authoring workflows in mind that let users seamlessly switch between generation and editing modes across outline, individual slide, or entire deck level."}, {"title": "Exploring Design Principles for GenAI-supported Rich Content Creation", "content": "In this section, we lay out the challenges of rich content document creation in the context of slide creation and how these informed the design principles that led us to intent tagging as an interaction paradigm for enabling novel human-GenAI co-creation workflows.\nSlide deck creation is a complex task that requires balancing multiple facets, such as narrative coherence, visual style consistency, and the integration of diverse content sources [3, 71, 97]. These elements must not only align individually but also work together harmoniously to deliver an impactful presentation. The challenges arise from the need to manage and sequence multimedia content while simultaneously crafting a narrative that flows logically across slides. Achieving this requires careful consideration of storytelling, visual design, and the inclusion of relevant data or references, often making the task overwhelming, especially under time constraints or when dealing with open-ended creative briefs.\nOne of the core difficulties in slide deck creation is the need to manage these distinct yet interdependent elements-narrative, visual style, and content sources-without losing sight of the overall presentation goals.\nTraditional tools often compartmentalize these tasks, forcing users to switch between different modes or tabs, which can disrupt the creative process. Linear workflows or predefined templates, while useful in some cases, often fail to accommodate the nuanced adjustments needed to refine a slide deck to a professional standard. Moreover, as discussed in the previous section, the integration of Generative AI (GenAI) into slide deck creation introduces both opportunities and challenges. Current GenAI approaches tend to offer either (semi-)automated solutions with little ability for users to steer the content generation, design galleries with limited predefined option sets, or chat-based interactions that can feel too linear, unstructured, and limited for exploring alternatives."}, {"title": "Design Principles", "content": "Based on these requirements, we developed the following interaction design principles for GenAI-supported slide creation:\nDP1 Enabling flexible, non-linear, and iterative workflows\nTo cater to users' diverse slide creation needs and individual working styles, users should be empowered to start and refine their presentations in a variety of ways. Whether beginning with an outline, building on an existing deck, crafting a single slide, or incorporating content from other documents, users should have the flexibility to approach their tasks from any angle. Additionally, the interface should allow seamless transitions between different content views-such as slide, deck, or outline-enabling GenAI-assisted iterative enhancements at any stage of the creation process.\nDP2 Accommodating diverse steering input types\nSlide creation often requires the integration of multiple sources of information, ranging from explicit, well-defined data (e.g., an annual sales report) to more abstract, nuanced considerations (e.g., audience-specific messaging). To ensure that GenAI-generated outputs align closely with user intentions, the interface should facilitate the input of a wide range of content types, including natural language descriptions, existing documents, images, and other relevant media. By enabling users to provide rich, varied inputs, the system can better interpret and fulfill the specific needs of each presentation.\nDP3 Supporting intent expression across varying levels of abstraction\nIn many slide creation scenarios, users may have clear, specific ideas about certain aspects of their presentation, while other elements remain less defined or harder to articulate. To accommodate this variability, a GenAI interface should allow users to express their intent at different levels of abstraction-ranging from detailed, precise instructions to more general, high-level directives. This flexibility not only helps users articulate their ideas more effectively but also supports iterative exploration and refinement of their presentation content.\nDP4 Leveraging AI for contextual content and terminology suggestions\nTo assist users in overcoming challenges like the \"blank page syndrome\" or finding terminology to describe their intent, the interface should harness the associative power of LLMs to provide contextual suggestions and inspiration. By offering alternative ideas, vocabulary, and creative prompts, the system can stimulate users' thinking, helping them to refine their content and explore new directions in an iterative process.\nDP5 Providing pre-computed real-time previews\nTo allow users to better and quicker anticipate how specific choices will impact generated slides, the tool should provide pre-computed real-time previews to, for example, let users rapidly explore how different fonts or colors would look on a given slide."}, {"title": "IntentTagger: An Intent Tagging-based Slide Creation System", "content": "Driven by the requirements of rich content creation tasks and the design principles described in the previous section, we explored novel input strategies through graphical micro-prompting interactions-an interaction notion we coined intent tagging. In addition to our design principles, a central inspiration for conceptualizing creative intent elicitation through granular and flexible micro prompts that represent users' intention was the \"mood board\" technique, in which creatives iteratively compile visual and textual elements into a collage to convey an overarching theme or creative direction [31]. Similar to how mood boards enable capturing and expressing designers' intent through compiling different media with varying levels of abstractions-such as words and pictures ranging from abstract concepts, stylistic references, or specific material detailsintent tagging allows users to express their intent openly through collections of intent tags.\nTo explore further the idea of intent tagging, we created IntentTagger, an LLM-driven system allowing users to iteratively create and modify slide deck presentations using intent tags on a 2D canvas interface (see Figure 2). In this section, we demonstrate the utility of IntentTagger by first illustrating its functionalities in a use-case scenario. We then follow with a detailed description of the proposed interface mechanisms and their implementation."}, {"title": "Example Use Case Scenario", "content": "Lucy is a sustainability manager at a company and wants to prepare a short slide presentation for new employees to inform them about sustainability initiatives at her company.\nLucy starts IntentTagger and clicks on the \"Create a new presentation from prompt...\" button. A modal popup appears, and she types \"a presentation for introducing sustainability initiatives for new employees\" in the prompt field and presses enter. The modal disappears, and the three circular steering groups on the steering canvas pulse, then three concept tags appear in the \"Narrative\" circle with the parameters \"Topic: Sustainability Initiatives,\" \"Audience: New Employees\" and \"Purpose: Introduction.\"\nTag Suggestions: To get inspiration for further presentation talking points, she clicks \"Suggest More Tags,\" and shortly after, new tags appear outside of the circular tag groups. Lucy finds some of the suggested tags useful and drags these into the \"Narrative\" group to include them in the generation, such as \"Focus: Eco-friendly Practices,\" \"Highlight: Company Goals,\" \"Section: Key Initiatives,\" and \"Objective: Corporate Responsibility.\"\nIncluding External Content: Lucy also wants to include sections from an existing Word document detailing the company's policies. With the mouse cursor, she drags the document from her file browser onto the IntentTagger app, and a new reference tag appears in the \"Content Sources\" group. She clicks on the tag's \"Select Sections\" button, which opens up the tag's Text Selection Widget showing the document's content. In the widget, she highlights two document sections, clicks \"Include Section in Presentation\" from the widget's context menu, and closes the widget.\nGenerating an Outline: Next, she wants to generate a first draft of the presentation's outline and presses \"update outline from intent tags\" in the outline view panel. The tag groups start pulsing, and then the new outline appears in the outline text editor.\nManually Crafting Concept Tags: Lucy is happy with the structure overall but finds it too long (20 sections). Instead of manually editing the outline, she adds a new concept tag to the \"Narrative\" group and enters \"Number of slides\" into the tag's upper text field and \"10\" into the lower and clicks the \"update outline from intent tags\" button again. The newly generated outline is now shorter and only comprises ten sections.\nGenerating Slides: Next, Lucy wants to generate a first draft of the slide deck and clicks the \"Update slides from intent tags\" button. Shortly after, the generated slides appear in the slide preview side panel. After evaluating the slides, Lucy realizes that adding some images and adjusting the visual style could enhance the presentation's appeal.\nIncluding Suggested Images: She clicks the \"suggest more images\" button on the tag steering board, and shortly after, new Reference Image Tags appear outside the \"Content Sources\" tag group with eco-friendly-themed stock photos. Lucy drags some of the image tags into the \"Content Sources\" group to add them to the presentation.\nAdjusting the Visual Style through Tags' Alternatives Drop-down: To make visual adjustments to the slides, she drags in some of the previously suggested tags, such as \"Typography: Sans-Serif,\" \"ColorPalette: Green and Blue,\" and \"Theme: Nature\" onto the \"Visual Style\" tag group. To get a better sense of typography alternatives, she clicks on the tags' drop-down, and a list of alternative tag values appears, such as \"Serif,\" \"Monospace,\" and \"Handwritten.\" While hovering over each option with the mouse, Lucy sees a preview thumbnail of that font style applied to the current slide. Lucy chooses \"Monospace.\" Now, Lucy clicks \"Update slides from intent tags\" again, and shortly after, she sees the revised slides reflecting the intended new visual style, including the images.\nAdjusting a Single Slide with Tag Steering Overlay: Lucy decides to refine the slide deck further. She clicks on the third slide in the Slide Preview panel to open the slide edit view and review its content in detail. The slide contains a text paragraph and an image, but she wants to show a list of bullet points instead. To modify the slide, she clicks on the intent tag icon next to the slide, and the three steering tag groups appear as an overlay containing tags representing the slide's Narrative, Visual Style, and Content Sources. Lucy selects the \"Text Format\" tag inside the Narrative group and clicks on its drop-down widget. A list of alternative tag values appears, including \"Bullet Points,\" \"List,\" and \"Table.\" Lucy chooses \"Bullet Points.\" To adjust the background color, Lucy creates a new tag in the Visual Style group and types in \"Background\" and \"pastel color.\" Now, she clicks on the \"Explore slide variations\" button, and the system generates several alternative versions of the slide, each with slightly different text variations and pastel background colors. Lucy clicks through these variations and selects one that better aligns with her presentation's tone.\nApplying a Slide's Style to All Slides: To maintain consistency across the entire presentation, Lucy uses the same visual style and selects \"Apply to all slides.\"\nSatisfied with the changes, Lucy saves the presentation, ready to deliver it in the upcoming meeting with her colleagues.\nAdjusting the Presentation to New Requirements: After a couple of months, Lucy needs to adapt the presentation for a different audience-existing employees rather than new hires. She clicks on the \"Audience\" tag in the Narrative group and changes it from \"New Employees\" to \"Existing Employees.\" The system instantly updates the content and tone of the slides to better fit this new audience. For example, introductory sections are replaced with more in-depth discussions of ongoing sustainability projects and their impact."}, {"title": "Interface Features", "content": "Here, we provide a more detailed explanation of IntentTagger's features:"}, {"title": "Intent Tags: Concept Tags, Reference Tags and Groups (DP2)", "content": "Intent tags are collections of single keywords, phrases, or media artifacts describing users' intended outcomes. Tags serve as granular and flexible micro prompts on a zoomable 2D canvas. Users can steer the generative slide creation system through two types of tags:\n(1) Concept Tags: Concept Tags are two-part text micro prompts of the format [label: value], such as \"Topic: Product launch\" or \"Color scheme: Corporate.\" Tags can be active or inactive to toggle their influence on generated content.\n(2) Reference Tags: Reference Tags allow users to provide external documents such as Word documents, images, or other slide decks as references for slide generation. Users can add these files by drag-and-drop from their file browser.\nTag Groups: All active tags are visually clustered into three circular groups on the interface, each representing different slide creation aspects: Narrative, Visual Style, and Content Sources. Users can drag tags in and out of groups to activate and deactivate them."}, {"title": "Deck Steering Canvas and Slide Steering Overlay (DP1, DP3)", "content": "Users can generate and modify single slides or entire slide decks using Intent Tags. The Deck Steering Canvas allows users to steer the (re)generation of entire slide decks. For example, specifying the number of total slides or changing the overall tone of the deck's text from brief to more verbose. Alternatively, the Slide Steering Overlay allows users to steer the (re)generation and explore alternatives of single slides."}, {"title": "Outline Editor (DP1)", "content": "Parallel to slides, the system also generates a presentation outline, which can be optionally edited using the outline editor. This editor also allows participants to start a new presentation by first drafting an outline or letting the system generate an outline from provided intent tags."}, {"title": "Editing Intent Tags, Tag Widgets, and System Suggestions (DP3, DP4, DP5)", "content": "Intent tags allow users to formulate their intent in several ways, such as manually creating and editing tags or choosing from options generated through LLM-driven dynamic adaptive UI mechanisms:\nManually Crafting Tags Users can manually create and modify tags by freely typing words or phrases into its two text fields, such as \"Font: Modern\" or \"Tonality: Engaging.\"\nDrop-Down List Users can also explore alternative properties for each concept tag by browsing its dynamically generated drop-down list, which also provides pre-generated visual previews in a hover tooltip."}, {"title": "Steering Slide Generation", "content": "To summarize the different ways of steering slide generation with Intent Tags, including a tag suggestion modifies the content of a slide, using the drop-down list changes a slide's mood, and using the opposite slider widget blends between a text- and image-heavy layout."}, {"title": "Previews of Slides and Slider Values (DP5)", "content": "IntentTagger integrates several preview features, allowing users to explore alternative tag values without committing to long waiting times: The tag's Drop-down list shows pre-generated previews as tooltips of how the current slide would look like with that tag value applied. The Opposite Slider widget shows generated explanations describing the resulting effect if applied to the generation. All tag previews and explanations get asynchronously pre-generated in the background, allowing users to explore these options in real time."}, {"title": "Referencing External Documents (DP2)", "content": "Besides Concept Tags, users can also include external files for the generative slide creation as Reference Tags.\nIncluding Content From Word Documents Users may drag in Word documents as content sources for the generated presentation. Optionally, users can select sections through a text selection pop-up to include only specific parts in the generated presentation."}, {"title": "Tag Grounding Acts (DP4)", "content": "While Intent Tagging presents granular and flexible mechanisms for users to communicate their intentions to the generative Al slide creation system, this principle also works backward in so-called Tag Grounding Acts, where the system creates intent tags from user inputs such as longer text prompts or slides.\nGrounding from Text Prompt This feature allows users to start a new presentation by providing instructions via a longer conventional text prompt, which the system will then automatically decompose into individual Intent Tags.\nGrounding from Slide Each time a user invokes the Slide Steering Overlay to adjust single slides, the system first analyzes the slide's content sources, narrative, and visual style and then pre-populates the interface with intent tags representing the slide's current state. This provides an easy starting point for users to make adjustments through Intent Tags."}, {"title": "Implementation Details", "content": "IntentTagger is implemented in TypeScript using ReactJS [70] with ReactFlow [28] for the tag canvas interface and Blocknote [8] for the text editor features. Mommoth.js [94] is used for importing Word documents, and slide rendering is based on a modified fork of spectacle.js [29]. For generating slides and adaptive UI, we use the official OpenAI API [63] to execute prompts using \"gpt-40\" and the Bing search API [56] for image suggestions. To suggest new tags, we prompt GPT to generate a list of [label:value] pairs for each tag group to augment the existing set of active intent tags. For outlines, GPT is instructed to generate a presentation outline in markdown format from a list of a user's active concept and reference tags (including images if present). To generate slides, we then prompt GPT to return a template-based JSON slide deck representation by providing the outline, active intent tags, and deck references (if specified). For generating the drop-down values, slide previews, and slider explanations, the system asynchronously requests these from GPT in the background for each new or modified tag on the canvas."}, {"title": "User Study", "content": "To better understand the possible benefits and limitations of intent tagging-based interactions, we conducted a lab user study aimed at providing insights into these research questions:\nRQ1 What are the key differences between chat-based and intent tag-based interactions with GenAI?\nRQ2 How do people work with intent tags?\nRQ3 What are users' perceived benefits and challenges for GenAI-driven slide creation with intent tags?\nFor RQ1, we decided to compare intent tag-based interactions with chat-based and design gallery-based approaches (text prompting and choosing from a set of options) since these represent the currently most common forms of interacting with GenAI for content creation in commercial systems, such as in OpenAI's ChatGPT/DALL-E [7], Adobe's Firefly [1] or Microsoft's PowerPoint Copilot [58] and Designer [59] features."}, {"title": "Participants", "content": "We recruited 12 participants (8 self-identified as females, 3 males, and 1 non-binary, age M=31.6 years (SD = 7.65)) with professional slide presentation creation experience via email lists at a large software company. The participant pool consisted of individuals with diverse job titles, and the majority of selected participants used PowerPoint at least multiple times per month in their jobs (see Table 3 in the Appendix). All participants had previous experience using the Designer and Copilot features in PowerPoint, ensuring existing familiarity with generative AI functionalities for slide deck creation. Participants signed an IRB-approved consent form and were compensated with a $50 gift card after study completion."}, {"title": "Procedure and Tasks", "content": "The lab study was structured into four phases:\n1) On-Boarding (20 min): At the beginning of the session, after a general study introduction, participants watched a video tutorial demonstrating IntentTagger's core functionalities with a step-by-step example. Following the video, participants were asked to complete two five-minute guided hands-on structured tasks (editing an existing slide and making adjustments to an entire deck) to familiarize themselves with the tools' interface and operation.\n2) Comparative Tasks (2 x 10 min): After the onboarding phase, participants completed two comparative tasks, each lasting 10 minutes, to evaluate their ability to create presentations using IntentTagger and a baseline system. In both tasks, participants were asked to create a 6-slide presentation aimed at educating teenagers about the inventions and scientific discoveries of a historical figure (\"The Discoveries of Marie Curie\u201d or \u201cThe Inventions of Nikola Tesla\u201d). Participants alternated between using IntentTagger and Microsoft PowerPoint [57] across the two tasks in randomized order. For PowerPoint, participants were restricted to using only the integrated Copilot feature [58] (chatbot interface with optional document upload) and the Designer feature [59] (design gallery for slide layouts) for slide generation and modification. As a starting point, participants were provided with a Word document containing the relevant Wikipedia article in both tasks. Participants were required to think aloud during the tasks, and after completing each task, they filled out a survey with attitudinal 6-point Likert scale questions. To mitigate order effects, the order of systems and presentation topics was randomized across participants.\n3) Semi-structured Task (10 min): In the third phase, participants were tasked with creating a 7-slide presentation from scratch using IntentTagger. They chose a topic related to a hobby they enjoy, aiming to convince others of its value and explain how to get started. Participants had ten minutes to complete the task, using only the prototype system without directly editing text or images on the canvas. They were required to think aloud as they worked, and before starting, they briefly described their topic and intentions for the presentation. We created this task to encourage participants to engage more freely with IntentTagger's features, focusing on content creation and presentation design related to a topic they are knowledgeable about and emotionally connected to. Since IntentTagger's deck generation time increases per slide, we deliberately limited the number of slides to six and seven per task in phases 2 and 3 to keep the processing time for each cycle within 15 seconds.\n4) Exit Interview (20 min): In the final phase, participants participated in a semi-structured interview to provide feedback on their experience with IntentTagger, focusing on its overall utility, comparison with tools like PowerPoint's Copilot, and the effectiveness of intent tagging interactions for slide creation. They shared insights on the tool's strengths, areas for improvement, and potential integration into their professional workflows."}, {"title": "Collected Data, Measures, and Analysis", "content": "Across the study", "data": "n\u2022 Video, screen, and audio recordings and machine-generated transcripts of the task think-aloud sessions\n\u2022 Audio recordings and machine-generated transcripts of the post-task interviews\n\u2022 Post-task survey data\n\u2022 Participant-created presentations and related IntentTagger project files\nTo compare chat-based and intent tag-based interactions , we analyzed the post-task surveys from phase 2 that probed on participants' perceived ease of use, efficiency, and control over the slide generation process on a 6-point Likert scale. We applied the Wilcoxon signed-rank test to assess statistical significance and calculated 95% confidence intervals for mean differences via bootstrapping with 10,000 replications using R [68", "102": ".", "6": "of the video recordings collected in the semi-structured task (study phase 3). We manually coded participants' interactions with IntentTagger's features in Atlas.ti [4", "12": "of the interview transcripts. We followed an iterative inductive coding process (using Marvin ["}]}