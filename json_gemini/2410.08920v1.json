{"title": "Efficient Hyperparameter Importance Assessment for CNNs", "authors": ["Ruinan Wang", "Ian Nabney", "Mohammad Golbabaee"], "abstract": "Hyperparameter selection is an essential aspect of the machine learning pipeline, profoundly impacting models' robustness, stability, and generalization capabilities. Given the complex hyperparameter spaces associated with Neural Networks and the constraints of computational resources and time, optimizing all hyperparameters becomes impractical. In this context, leveraging hyperparameter importance assessment (HIA) can provide valuable guidance by narrowing down the search space. This enables machine learning practitioners to focus their optimization efforts on the hyperparameters with the most significant impact on model performance while conserving time and resources. This paper aims to quantify the importance weights of some hyperparameters in Convolutional Neural Networks (CNNs) with an algorithm called N-RReliefF, laying the groundwork for applying HIA methodologies in the Deep Learning field. We conduct an extensive study by training over ten thousand CNN models across ten popular image classification datasets, thereby acquiring a comprehensive dataset containing hyperparameter configuration instances and their corresponding performance metrics. It is demonstrated that among the investigated hyperparameters, the top five important hyperparameters of the CNN model are the number of convolutional layers, learning rate, dropout rate, optimizer and epoch.", "sections": [{"title": "Introduction", "content": "With the growing prominence of Deep Learning and Automated Machine Learning frameworks, Hyperparameter Optimization (HPO) techniques have evolved from manual, empirical tuning to automated methods such as Random Search [2], Bayesian Optimization [12], and Evolutionary Algorithms [9]. However, optimizing all hyperparameters in large search spaces is often impractical due to limited computational resources and time. Furthermore, regardless of the HPO algorithm, we must manually define the hyperparameter search space [4], often relying on rules of thumb that may lack rigour. Hyperparameter Importance Assessment (HIA) [5] can guide users by focusing on the most impactful hyperparameters. However, its use in Deep Learning remains underexplored due to"}, {"title": "Related Works", "content": "When mentioning how to quantify the importance of hyperparameters, another similar field that has many practical approaches probably comes to mind, i.e., Feature Selection [8]. Feature Selection is the process of reducing the dimensionality of input features when developing predictive models to save the computational cost of modelling and, in some cases, improve model performance [17]. It can use statistical measures to score the correlations between each input feature and the model performance for selecting the most relevant features [17], which is very similar to the aim of HIA. It was found that prior research has applied feature selection methodologies to the HIA of traditional machine learning models."}, {"title": "Algorithm Derivation", "content": ""}, {"title": "Notation", "content": "\u2022 For a machine learning model f, [\u0398] := {\u0398\u2081, \u0398\u2082,..., \u0398k} represents the hyperparameter configuration space where \u0398\u2081 stands for the first hyperpa-"}, {"title": "Estimation of W [] in the Probabilistic Framework", "content": "The key idea of the Relief family of algorithms is to estimate the quality of an attribute (i.e., the influence of hyperparameters on the model performance metric) by assessing how well the attribute values (i.e., hyperparameters) distinguish the outputs of the nearest neighbour instances [10]. Relief's estimate of W[\u0398k] can be written as the approximation of the difference between these two probabilities [7]:\nW[\u0398k] := P(diff.\u0398k | nearest diff.class) \u2013 P(diff.\u0398k | nearest same class) (1)\nIn Eq.1, the first term quantifies the degree of difference in the hyperparameter \u0398k values when comparing an instance with its nearest neighbour from a different class. Conversely, the second term measures the degree of difference in \u0398k values for that instance and its nearest neighbour from the same class.\nHowever, Relief was designed under the assumption that the model outputs are discrete categories. In reality, the performance metric P is a continuous variable so the notion of \"the same class\" and \"the different class\" does not apply in HIA. To address this challenge in regression problems, a variant known as RReliefF was proposed [11]. Unlike its predecessor, RReliefF doesn't rely on exact knowledge of whether two instances belong to the same class. Instead, it adopts a probabilistic approach to quantify the differences in model outputs, leading to a need for reformulating W[\u0398k] for this context. The following will derive the revised formulation of W[\u0398k] in the RReliefF framework. Based on Eq.1, we can rewrite W[\u0398k] to form Eq.2 for regression problems.\nW[\u0398k] := P(diff. \u0398k | nearest diff. p) \u2013 P(diff. \u0398k | nearest same p) (2)\nGiven that the model outputs are continuous variables, we can assess the variability in the output P for a given instance relative to its nearest neighbours within a specified range. This variability is quantified by Eq.3, which represents observing the difference degree in the model output values among neighbouring instances.\nPdiff(p) := P(diff. p | nearest instances) (3)"}, {"title": "", "content": "Meanwhile, we also can get Eq.4, the probability of the difference degree in the values of the hyperparameter \u0398k when comparing one instance with all its nearest neighbours within a specific range. This probability quantifies the degree of variation in \u0398k across neighbouring instances.\nPdiff(\u0398k) := P(diff. \u0398k | nearest instances) (4)\nFurthermore, we can define another important conditional probability with Eq.5. This quantifies the probability of a change in P, conditional upon differences in the hyperparameter \u0398k within the nearest instances. It specifically shows how variability in the hyperparameter is associated with variability in the model output.\nPdiff(p)|diff(\u0398k) := P(diff. p | diff. \u0398k, nearest instances) (5)\nBased on Bayes' Theorem, we can get the first term of Eq.2:\nP(diff. Ok | nearest diff. p) =  (6)\nWithin the probabilistic framework, we can acknowledge:\nP(same p | nearest instances) + Pdiff(p) = 1\nP(same p | diff. \u0398k, nearest) + Pdiff(p)|diff(\u0398k) = 1 (7)\nEq.7 can lead us to derive the second term of Eq.2:\nP(diff. Ok | nearest same p) =  (8)\nConsidering that diff(p) and diff(ek) are not independent events:\nPdiff(p) and diff(k) = Pdiff(p)|diff(k)\u00b7 Pdiff(\u0398\u03b5) (9)\nBy combining these derived probabilities, the final representation of W[\u0398k] in the RReliefF framework W[k] would be:\nW[Ok] = (10)\nAfter performing the above process on all hyperparameters, the importance weights of all hyperparameters, W[], can be obtained. In addition, to compute the importance weights for combinations of hyperparameters, we applied an enhanced normalization formula. This approach is designed to scale the weights in"}, {"title": "", "content": "a manner that considers the exponential of the sum of individual hyperparameter weights, thus facilitating comparative analysis of their combined influence [13]. The improved normalization formula is expressed as follows:\nW[\u0398m&On] =  (11)"}, {"title": "Approximating Key Terms in N-RReliefF", "content": "After completing the derivation of the N-RReliefF formula, it is found that to estimate W[] in Eq.10, we only need to approximate three terms: Eq.3, Eq.4, and Eq.9. Three weights, Ndiff(p), Ndiff(ek), and Ndiff(p) and diff(er) are defined as the approximation values of these three terms.\nNdiff(p) indicates the cumulative difference situation between the randomly sampled instance's performance metric, pm, and each neighbouring instance's performance metric pnn, where the number of neighbouring instances is J.\nNdiff(p) =  (12)\nNdiff() indicates the accumulation of differences on the specific hyperparameter Ok between the randomly sampled instance, hm, and its every neighbouring instance, hNN,\u00b7\nNdiff(k) = \u2211 diff(0mk, ONNjp)\u00b7d(hm, hNN;) (13)\nNdiff(p) and diff(r) simultaneously accounts for the cumulative differences in both the performance metric p and a specific hyperparameter Ok between a randomly sampled instance hm and each of its neighbouring instances hNN;\u00b7\nNdiff(p) and diff(k) = \u2211 diff(pm, PNN,) diff(0mk, ONNjp).d(hm, hNN;) (14)\nThe implementation process of N-RReliefF is outlined in Algorithm 1."}, {"title": "Experimental Setup", "content": ""}, {"title": "Implementation Procedures", "content": "The experimental procedure is shown in Figure 1, along with detailed explanations of how each step is performed. All experiments were conducted on a machine equipped with an NVIDIA GeForce RTX 3070Ti GPU, a 12th Gen Intel(R) Core(TM) i7-12700KF processor (3.60 GHz), and 32 GB of RAM."}, {"title": "Hyperparameter Configuration Space and Network Structure of CNNs", "content": "The experiment examined the individual and joint importance of 11 hyperparameters, noting that some have dependent relationships, such as the number of convolutional layers and the kernels per layer. Dependent hyperparameters can't be analyzed individually alongside those that influence them. Thus, we propose fixing \"parent\" hyperparameters when studying \"child\" hyperparameters. For example, to compare kernel counts across layers, we first fix the number of layers. Table 1 lists the hyperparameters, their data types, configuration spaces, and default values."}, {"title": "Network Structure", "content": "The structure of the CNN model changes dynamically during the data generation phase, but several details are fixed: (1) Each convolutional layer is followed by a ReLU activation function and a pooling layer. When the structural hyperparameter, the number of convolutional layers, is greater than one, additional"}, {"title": "Selected Datasets", "content": "Given that the model to be evaluated is a CNN, which is most commonly used for image classification, we selected ten classic and widely used benchmark image classification datasets from publicly available sources to generate the hyperparameter configurations and the corresponding performance data. These datasets are chosen to represent a variety of scenarios. To assess the impact of the number of input channels on the CNN model's performance, the datasets include"}, {"title": "Evaluation and Results", "content": ""}, {"title": "Initial Data Exploration", "content": "In the initial phase of data exploration, we commenced with an examination of the volume of data generated from various image classification datasets. Figure 2 illustrates that for each dataset, the quantity of hyperparameter configuration and associated performance data successfully surpassed the threshold of 1,000 instances.\nWith further exploration into the data generated across all ten image classification datasets, the overall distribution of the data is illustrated. As depicted in Figure 3, the distribution exhibits a bimodal tendency, skewing toward the extremes of performance, while the data volume within intermediate performance brackets remains comparatively sparse."}, {"title": "Verifying the Reliability via ICC", "content": "To ensure a balanced data distribution, especially to account for the typically smaller volume of data in the medium performance intervals, a strategy of repetitive random subsampling was adopted for input into the HIA algorithm (N-RReliefF), with each performance interval limited to a maximum of 600 data points, which was considered based on the volume of data generated for each performance interval. We conducted this subsampling ten times, resulting in ten distinct subsets. Upon feeding these subsets into the HIA method, we obtained"}, {"title": "Importance Wights of Investigated Hyperparameters", "content": "Finally, We executed N-RReliefF on the full dataset, setting K to 30 to ensure stable importance estimates by considering a broad set of neighbours and minimizing sensitivity to outliers. The results, shown in Table 3, indicate that the number of convolutional layers, learning rate, and dropout rate are the top three most important hyperparameters in CNN models, with convolutional layers having the highest importance weight. This confirms the established view of network depth as a key factor in model performance, while learning rate and dropout rate also play significant roles in generalization and overfitting prevention. In contrast, hyperparameters such as the number of filters in fully connected layers and batch size have minimal impact on performance.\nThe FANOVA comparative analysis (Table 4) corroborated these findings, revealing the same ranking of hyperparameter importance. Although the numerical weights differ, both methods highlight the critical influence of the number of convolutional layers, learning rate, and dropout rate on CNN performance."}, {"title": "Joint Importance of Hyperparameter Pairs", "content": "Due to the large number of hyperparameter combinations involved in joint importance, the ranking results are shown only for the top ten. Table 5 further proves the pivotal role that the architecture's depth plays in determining the performance of Convolutional Neural Network (CNN) models."}, {"title": "Importance of Filter Counts Across Convolutional Layers", "content": "There are often dependencies between hyperparameters affecting network structures. For example, when the number of convolutional layers is 3, the number of filters in the 3rd convolutional layer and the number of filters in the 2nd layer have to be set. But if the number of convolutional layers is 1, the above two hyperparameters do not exist. Thus, this section will explore the importance of ranking between the hyperparameters named \"the number of filters\" of different convolutional layers.\nIn the scenario where the CNN comprises two convolutional layers (Table 6), the importance weights allocated to the number of filters in the first layer"}, {"title": "Conclusions and Future Work", "content": "In this study, we investigated whether N-RReliefF, as an HIA method, can be effectively applied in the domain of deep learning. Our analysis spanned the training of over 10,000 CNN models across a diverse spectrum of 10 image classification datasets, generating an extensive collection of hyperparameter configurations and their impact on model performance. To ensure the reliability of N-RReliefF, we computed the Intraclass Correlation Coefficient (ICC) across 10 distinct subsets from our dataset. We also undertook a comparative analysis using FANOVA. Although there were numerical variations in the importance weights, the ranking order of the hyperparameters remained consistent, which confirmed the robustness of our findings.\nOur analysis revealed that the number of convolutional layers, learning rate, and dropout rate emerge as the most influential hyperparameters, in line with the established best practices observed by machine learning practitioners. This not only validates the commonly used rules of thumb in the field but also provides a quantitative basis for them, enhancing their reliability and applicability in optimizing CNN models. Additionally, our findings regarding the relative importance of filters in convolutional layers illustrate a clear trend: hyperparameters associated with layers closer to the input are more influential, supporting the principle that early layers in a network play a more critical role in performance outcomes.\nWhile this study offers valuable insights into hyperparameter importance in CNNs, there are areas for further improvement. Future work will focus on applying HIA to renowned CNN architectures such as LeNet, AlexNet, GoogleNet, and ResNet, broadening the scope of the investigation to encompass a wider range of deep learning models and providing a more comprehensive understanding of HIA's applicability and effectiveness."}]}