{"title": "DeepHQ: Learned Hierarchical Quantizer for Progressive Deep Image Coding", "authors": ["Jooyoung Lee", "Se Yoon Jeong", "Munchurl Kim"], "abstract": "Unlike fixed- or variable-rate image coding, progressive image coding (PIC) aims to compress various qualities of images into a single bitstream, increasing the versatility of bitstream utilization and providing high compression efficiency compared to simulcast compression. Research on neural network (NN)-based PIC is in its early stages, mainly focusing on applying varying quantization step sizes to the transformed latent representations in a hierarchical manner. These approaches are designed to compress only the progressively added information as the quality improves, considering that a wider quantization interval for lower-quality compression includes multiple narrower sub-intervals for higher-quality compression. However, the existing methods are based on handcrafted quantization hierarchies, resulting in suboptimal compression efficiency. In this paper, we propose an NN-based progressive coding method that firstly utilizes learned quantization step sizes via learning for each quantization layer. We also incorporate selective compression with which only the essential representation components are compressed for each quantization layer. We demonstrate that our method achieves significantly higher coding efficiency than the existing approaches with decreased decoding time and reduced model size. The source code is publicly available at https://github.com/[Tobeuploaded]", "sections": [{"title": "I. INTRODUCTION", "content": "RECENTLY, neural network (NN)-based image compression methods [1]\u2013[19] has rapidly advanced and demonstrated performance surpassing traditional codecs, such as BPG [20] and JPEG2000 [21]. Currently, various research efforts are underway not only to improve performance but also to enhance usability (or functionality) from a practical perspective. One notable research area is NN-based progressive image coding (PIC), which aims to enable the versatile utilization of a single bitstream to accommodate various transmission and consumption environments. A progressive compression model compresses an input image into various qualities in the form of a single bitstream, as depicted in Fig. 1-(c). Therefore, the progressive compression model offers high compression efficiency in an overall sense compared to the simulcast compression case of fixed-rate image compression models (Fig. 1-(a)) or variable-rate image compression models (Fig. 1-(b)) where an image is encoded into multiple separate bitstreams, each corresponding to a single quality level.\nIn the early stage of NN-based PIC, as shown in Fig. 2-(a), some methods [1]\u2013[3] stacked multiple en/decoding stages, each of which compresses the residual signal between the input and the reconstruction of the previous compression stage. The compressed bitstream is progressively accumulated and its corresponding reconstruction quality gets enhanced as the number of stacked en/decoding stages increases. However, this iterative residual coding increases complexity due to its repeated recurrent processes. On the other hand, a few recent approaches [22]\u2013[25] adopt hierarchical quantization for a single transformed lat ent representation (a feature map) with progressively decreasing quantization step sizes, as shown in Figs. 2-(b) and 3. Specifically, for each component value in the transformed latent representation, the hierarchical quantization allows a wider quantization interval for lower-quality compression and its nested narrower (finer) quantization sub-intervals for higher-quality compression, as shown in Fig. 3. By doing so, only the information required for finer quantization is progressively added to the bitstream as the compression quality gets enhanced.\nAlthough the hierarchical quantization-based PIC scheme in latent space (Fig. 2-(b)) improves coding efficiency and reduces overall complexity compared with the previous recurrent residual PIC schemes [22]\u2013[25] in pixel domain (Fig. 2-(a)), the existing PIC schemes still have the following two drawbacks: i) They use the handcrafted quantization hierarchies for all representation components, where a quantization interval is divided into three sub-intervals for finer quantization in its next quantization layer. Such a fixed quantization structure disregarding the characteristics of individual representation components may lead to sub-optimal rate-distortion (R-D) performance (See Sec. III for further details); ii) They encode all representation components at all quantization layers into the bitstream, which implies that, as reported in the field of variable-rate image coding [26], encoding all representation components regardless of the target compression quality can lead to sub-optimal performance in both compression efficiency and complexity.\nOn this basis, we introduce a novel learned hierarchical quantizer, called DeepHQ, which exploits learned quantization step sizes for each quantization layer. In addition, our DeepHQ further improves the compression efficiency by compressing only the essential representation elements for each quantization layer. Our DeepHQ achieves significantly superior coding efficiency compared with most NN-based PIC methods [2], [3], [22]\u2013[24] as well as the conventional codecs [21], [27], and shows comparable coding efficiency to the current state-of-the-art (SOTA) model [25] while utilizing a significantly smaller model size (7.88%) and consuming much shorter decoding time (11.39%). It should also be noted that Our DeepHQ model utilizes only a single trained model for progressive coding across all bit-rate ranges, making it both a progressive coding model and a variable-rate model. In contrast, the competing SOTA method [25] employs individually trained multiple refinement sub-networks, each dedicated to one of the predefined bit rate ranges, leading to a much larger whole model size. Our contributions are summarized as follows:\n\u2022 We firstly propose a learned hierarchical quantizer with the learned quantization step sizes via learning, called DeepHQ, for NN-based PIC, resultantly offering superior compression efficiency.\n\u2022 We further improve the progressive coding efficiency and significantly reduce the decoding time by incorporating the learned hierarchical quantization and the selective coding of latent representations into progressive neural image coding.\n\u2022 Our DeepHQ (w/ a single base compression model) achieves high coding efficiency comparable to the best state-of-the-art progressive coding method (w/ multiple subnetworks with different target bitrate ranges), only with 7.88% of its model size and 11.39% of the decoding time, in average."}, {"title": "II. RELATED WORK", "content": "NN-based non-progressive image coding. In NN-based image compression, research has mainly focused on non-progressive image coding. The field of NN-based non-progressive image coding is broadly divided into i) dimension reduction-based approaches [1]\u2013[3] aiming to pack as much information as possible into a small-sized representation, and ii) entropy-minimization approaches [4], [5], [7]\u2013[19] aiming to minimize (cross) entropy of latent representations while also minimizing distortions of reconstructions. In the early stage, research was actively conducted in both areas, but currently, most approaches are based on entropy minimization due to its performance advantages.\nBalle et al. [4] and Theis et al. [5] concurrently proposed the first entropy minimization-based image compression methods. They utilized entropy models (distribution approximation models) for latent representations to calculate the rate term (cross-entropy of latent representations), and they performed joint optimization in an end-to-end manner to simultaneously minimize the calculated rate and distortion. For the distribution approximation models, Balle et al. [4] adopted linear spline models while Theis et al. [5] used Gaussian scale mixture models. In contrast to the first two models [4], [5] that directly optimize the approximation model parameters, Balle et al. [7] proposed an extended model where model parameters are not directly learned but are instead predicted adaptively based on the input, and the necessary information for this model parameter prediction process was compressed through the hyper-encoder network.\nSubsequently, Minnen et al. [8] and Lee et al. [9] regarded the spatial correlation existing within the latent representation as redundancy from a compression perspective and proposed autoregressive models to mitigate it. Specifically, they utilized the previously reconstructed neighboring latent representation components in a raster scanning order environment to predict the distribution parameters of the current element, ensuring a higher distribution approximation accuracy. To further improve coding efficiency, Cheng et al. [12] and Lee et al. [13] utilized Gaussian mixture models instead of single Gaussian models and deeper en/decoder networks. In these autoregressive schemes, Chen et al. [11] first exploited the non-local attention blocks for en/decoder networks and hyper en/decoder networks and Li et al. [28] introduced a special non-local operation for context modeling by employing the global similarity. To address the high decoding time complexity of the sequentially scanning autoregressive models et al. [8], [9], [11]\u2013[13] while keeping advantages from autoregression as much as possible, a few approaches [14], [15], [29] utilized unique forms of autoregression methods. Li et al. [29] introduced a 3-D zigzag scanning order and a 3-D code dividing technique that enables better parallel entropy decoding. Minnen et al. [14] divided the latent representation into a few slices along the channel direction and performed autoregression among those slices. He et al. [15] divided the latent representation into two subsets in a spatial checkerboard pattern and predicted model parameters of one subset based on the other subset. More recently, some studies [16], [17], [19], [30], [31] have proposed replacing the traditionally dominant CNN-based architectures with Transformer [32]. Zhu et al. [16] replaced all convolutions in en/decoder networks with Swin Transformer [33] blocks and Qian et al. [17] utilizes a self-attention stack to replace the hyper en/decoder networks. Kim et al. [30] proposed an entropy model called Information Transformer that exploits both global and local dependencies to replace the hyper-encoder and -decoder. Liu et al. [19] utilized a parallel Transformer-CNN Mixture (TCM) block to incorporate the advantages of CNN and transformers. Koyuncu et al. [31] introduced a computationally efficient transformer-based autoregressive context model, called eContextformer.\nNN-based progressive image coding. Although various research efforts are ongoing to improve the practicality of NN-based image codecs, NN-based PIC is yet relatively under-explored. Initially, a few methods [1]\u2013[3] repeatedly compress and reconstruct the residual between the lower-quality reconstruction and the original input, thus progressively enhancing the compression quality as the number of iterations increases. Recently, Lu et al. [22] and Lee et al. [23] adopted hierarchical quantization, in which they perform an encoding transformation only once and apply progressively decreasing quantization step sizes to the transformed latent representations as the compression quality get improved. Both approaches utilized a handcrafted quantization hierarchy with fixed reduction ratios of quantization step sizes between the quantization layers. In addition, both methods adopt fined-grained component-wise progressive coding where representation elements are sequentially compressed. Similarly, Li et al. [24] introduced a learned progressive coding model based on a handcrafted quantization hierarchy using dead-zone quantizers. More recently, Jeon et al. [25] proposed an extended method called context-based trit-plane coding (CTC) [25] that improves the coding efficiency of DPICT [23] by adding two types of separate network modules, the context-based rate reduction (CRR) and context-based distortion reduction (CDR), that refine the estimated distribution parameters and reconstructed latent representations, respectively. However, the architectures of the CRR and CDR modules are highly complex. Furthermore, these two modules utilize a total of six models, each of which is dedicated to one of the three predefined bit-rate ranges, thus causing an extremely high number (~400 million) of model parameters.\nPartial compression of latent representations. Meanwhile, some NN-based image coding approaches [26], [34], [35] adopted partial coding of latent representations to improve coding efficiency and to reduce computational complexity at the same time. Li et al. [34] and Mentzer et al. [35] adopted 2-D importance maps to represent the spatial importance of representations, which allows for spatially different bit allocations in different regions. According to the 2-D importance map, they determine the number of channels to be involved in the compressed bit-streams. Whereas, Lee et al. [26] introduced a more generalized 3-D importance map that represents component-wise inherent importance of representations for variable-rate image coding. According to the target quality, the 3-D importance map is adjusted with a learned and dedicated adjustment vector to determine essential representation elements."}, {"title": "III. BACKGROUND AND MOTIVATIONS FOR LEARNED QUANTIZATION", "content": "In the recent NN-based PIC approaches [22]\u2013[25], an encoder network transforms input image $x$ into latent representation $y$, and $y$ is hierarchically quantized with different quantization step sizes in a coarse-to-fine manner as quantization layers recursively get deeper (higher) from the first layer (Layer 1 in Fig. 3.) with the coarsest quantization step sizes. Fig. 3 illustrates an example of the existing handcrafted hierarchical quantization process of dividing a quantization interval into three parts. In Fig. 3, the quantization step size of the $l$-th quantization layer become one-third of that of the ($l-1$)-th quantization layer such as in DPICT [23], and an original continuous value of a representation component is marked as purple stars in the three quantization layers. The index of the coarsest quantization interval that contains the original value is entropy-coded into the bitstream in the first quantization layer (Layer 1). Subsequently, in the second quantization layer (Layer 2), the quantization interval is divided into three sub-intervals, and the index of the first sub-interval to which the original component value (purple star) belongs is entropy-coded into the additional bitstream. This nested quantization process is repeatedly conducted into higher quantization layers, generating the resulting additional bitstreams. In each quantization layer, the tasks such as quantization and entropy coding for all representation components are performed first, followed by the same tasks of the subsequent upper quantization layers. The decoding process also proceeds in the order of lower-to-higher quantization layers, but the tasks of entropy-decoding and dequantization in each quantization layer are performed in the reverse order of the encoding process where the resulting quantization interval indices are converted to their representative values (e.g., the midpoints of the intervals), denoted as green, blue and orange stars in Layer 1, Layer 2 and Layer 3, respectively, in Fig. 3.\nThe probability estimations for quantization intervals in each quantization layer are necessarily required for entropy-coding. For this, its brief explanation is provided in a self-contained manner for better understanding in the following. The probability of each quantization interval is determined based on the distribution parameters (e.g., $\\mu$ and $\\sigma$ of Gaussian models) of an entropy model $p(y)$. The entropy model $p(y)$ is a learnable approximation model for the distribution of $y$, where the distribution parameters can be estimated via a neural network [7]\u2013[9], [12] or be directly learned [4], [5] in the end-to-end neural image compression fields. For the entropy model $p(y)$, the existing methods [22]\u2013[25] adopt the hyper en/decoder model [7] where $y$ is transformed (and compressed) into side information $z$ via the hyper-encoder network. From $z$, the estimated Gaussian distribution parameters $\\mu$ and $\\sigma$ of $p(y)$ are reconstructed via the hyper-decoder network. In the first quantization layer (Layer 1), the probability that $y_i$ belongs to the $k$-th interval $I_k$ is determined as $P(y_i \\in I_k) = \\Phi(\\text{max}(I_k), \\mu_i, \\sigma_i) - \\Phi(\\text{min}(I_k), \\mu_i, \\sigma_i)$, where $y_i, \\mu_i$, and $\\sigma_i$ denote $i$-th elements of $y, \\mu$, and $\\sigma$, respectively, and $\\Phi(\\cdot)$ represents a Gaussian cumulative density function. In the second and third quantization layers (Layers 2 and 3), each containing three sub-intervals, $P(y_i \\in I_k | I_k \\in \\{I_1, I_2, I_3\\})$ is determined as follows:\n$P(y_i \\in I_k) = \\frac{p(y_i \\in I_k)}{p(y_i \\in I^1) + p(y_i \\in I^2) + p(y_i \\in I^3)}$  (1)\nwhere $p(y_i \\in I_k) = \\Phi(\\text{max}(I_k), \\mu_i, \\sigma_i) - \\Phi(\\text{min}(I_k), \\mu_i, \\sigma_i)$. That is, $P(y_i \\in I_k)$ denotes the final sub-interval probability. Note that $\\sum_{k=-1}^{1} P(y_i \\in I_k)=1$.\nAs aforementioned, the existing hierarchical quantization-based PIC methods [22]\u2013[25] adopt the approach to dividing each quantization interval into three sub-intervals for all components of the latent representation $y$. However, such a fixed number and structure of quantization sub-intervals, which are independent of the target reconstruction qualities and the signal characteristics of representations, may lead to sub-optimal performance. To further enhance the overall R-D performance of NN-based PIC, the utilization of optimally learned quantization step sizes is necessitated. For example, finer quantization would be required at relatively lower quantization layers for some representation components containing global and structural information. Whereas, it would be more beneficial at relatively higher quantization layers for other components such as fine texture details while maintaining the quantization step sizes relatively larger in lower quantization layers in the R-D perspective. This has motivated us to develop the DeepHQ that can accommodate various learned quantization step sizes for each representation component in different quantization layers. In the following sections, we present technical details of our DeepHQ (Sec. IV) and show its effectiveness with experimental results (Sec. V)."}, {"title": "IV. PROPOSED METHOD", "content": "As shown in Figs. 4 and 5, our DeepHQ has two distinctive features: i) our DeepHQ utilizes learned quantization step sizes for each quantization layer and representation component and ii) our DeepHQ encodes only essential representation components in each layer. We introduce these two key methods in Secs. IV-A and IV-B, respectively. Note that we describe the hierarchical quantization first because the representation selection is an extended feature of our method. Subsequently, in Sec. IV-C, we introduce the component-wise progressive coding method that allows further fine-grained progressive coding between two discrete quantization layers. Then, we describe the training process of our DeepHQ in Sec. IV-D.\nA. Hierarchical quantization with learned step sizes\nOur DeepHQ model consists of L quantization layers, each of which uses a dedicated quantization step size vector, denoted as $\\Delta_l$. Each $\\Delta_l$ vector contains the channel-wise quantization step sizes for the $l$-th quantization layer and therefore consists of the same number of elements as the number of channels in $y$. Generally, via the optimization described in Sec. IV-D, larger $\\Delta_l$ values are learned for the lower quantization layer (lower-quality compression). The hierarchical quantization and dequantization processes at the $l$-th quantization layer is represented as follows:\n$\\tilde{y} = \\text{DQ}(k, \\Delta_l, I_{l-1})$,  (2)\nwith $k = \\text{Q}(y^*, \\Delta_l, I_{l-1})$ and $y^* = y - \\mu$,\nwhere $\\tilde{y}$ denotes the dequantized reconstruction for $y^*$, and $y^*$ is an unbiased representation obtained by shifting the representation $y$ by the estimated $\\mu$. $\\text{Q}(\\cdot)$ and $\\text{DQ}(\\cdot)$ are key elements of our work that progressively quantize and dequantize $y^*$ more finely as the quantization layer increases, recursively utilizing its lower-layer intervals $I_{l-1}$ that the values in $y^*$ belong to, along with the learned quantization step size vector $\\Delta_l$, as shown in Figs. 4 and 5. The resulting sub-interval indexes in $k$ are entropy-coded into the bitstream in a lossless manner. It should be noted that the process in Eq. 2 omits the selective compression process for briefness, while the full process including selective compression is described in Eq. 10. We determine the final representation $\\breve{y}_{final}$ that is fed into the decoder network as follows:\n$x' = \\text{De}(\\breve{y}_{final}), \\text{ with } \\breve{y}_{final} = (\\tilde{y}'_l + \\mu) / \\Delta_l * \\Delta_l^{inv},$ (3)\nwhere $x'$ is a reconstruction image of the $l$-th quantization layer, $\\text{De}(\\cdot)$ is the decoding transform function (via the decoder network), and $\\Delta_l^{inv}$ is the inverse scaling factor for the $l$-th quantization layer, which allows the asymmetry between quantization and inverse quantization that can slightly improve the coding efficiency compared with the symmetric case where $\\Delta_l = \\Delta_l^{inv}$. We adopt this asymmetric scheme inspired by variable-rate compression model [36]. However, It should be noted that we focus in this work on the progressive quantization $\\text{Q}(\\cdot)$ and dequantization $\\text{DQ}(\\cdot)$ in Eq. 2 which are based on only $\\Delta_l$. From now on, we represent the quantization processes component-wise for simplicity and better understanding, but in practice, we entirely use array operations. Figs. 4-(b) and 5-(b) show the hierarchical quantization and dequantization procedures, respectively. The quantization process on the encoder side is represented as follows:\n$\\text{Q}(y^*_i, \\Delta_{l,i}, I_{l-1,i}) = k \\text{ if } y^*_i \\in I_i^{k}$ (4)\nwith $I_i^{k} = [b_i^k, b_i^{k+1}),$\n$\\text{DQ}(k, \\Delta_{l,i}, I_{l-1,i}) = (b_i^k + b_i^{k+1})/2,$  (5)\nwhere the sub-interval boundaries are determined in the same manner as in the encoder. That is, the dequantized where $y^*_i$ is the $i$-th component of $y^*$ and $k$ represents the index of the quantization sub-interval $I_k$ for $y_i^*$ in the $l$-th$I$. We further describe how we determine the boundaries\nli"}, {"title": "1) Hierarchical boundary calculation", "content": "To determine the interval boundary vector $b_{l,i}$ for $y_i^*$ at the $l$-th quantization layer, we utilize the leanred step size value $\\Delta_{l,i}$ as follows:\n$b_{l,i}^{k} = \\begin{cases}\nLB_{l,i}, & \\text{if } c_i^{k} < LB_{l,i}, \\\\\nUB_{l,i}, & \\text{else if } c_i^{k} > UB_{l,i}, \\\\\nc_i^{k}, & \\text{otherwise}.\n\\end{cases}$ (6)\nwith $c_i^k = (k - 0.5) \\times \\Delta_{l,i} + \\breve{y}_{l-1,i},$\n$LB_{1,i} = \\text{min}(I_{l-1,i}), UB_{1,i} = \\text{max}(I_{l-1,i}),$\nwhere $b_{l,i}^{k}$ is primarily determined ascii, and the distances between adjacent $c_i^{k}$ values are equal to $\\Delta_{l,i}$. In case $c_i^{k}$ falls outside the valid range between the lower bound $LB_{1,i}$ and the upper bound $UB_{1,i}$, which represents two boundaries of the lower layer interval $I_{l-1,i}$ that $y_i^*$ belongs to, a clipping process is performed to remove redundancy between quantization layers from the compression perspective as shown in Fig. 6. In our work, $k$ is an integer ranging from $\\text{\u2013}(K - 1)/2$ to $(K - 1)/21$ and $K$ represents the total (odd) number of quantization sub-intervals. However, some intervals among the total $K$ intervals can have zero width due to the clipping operation in Eq. 6. Considering sub-intervals with widths greater than zero to be valid sub-intervals, the number of the valid sub-intervals $N_{b_{l,i}}$ is less than or equal to $K$. We empirically set $K$ to $\\text{max}(y^*/\\Delta_1) \\times 2 + 1$ that can cover all valid sub-intervals. For the first ($l$=1) quantization layer, we set $LB_{1,i}, UB_{1,i}$, and its virtual lower-layer reconstruction $\\tilde{y}_{0,i}$ values to \u2013$\\Delta_{1,i} \\times (K/2), \\Delta_{1,i} \\times (K/2)$, and 0, respectively. Note that the lower-layer reconstruction $\\tilde{y}_{l-1,i}$ becomes the center between two adjacent $c_i^{k}$ and $c_i^{k}$ values (thus also between the $b_i^k$ and $b_i^k$). We empirically found out that the boundaries centering around the lower-layer reconstruction $\\tilde{y}_{l-1,i}$ show much better coding efficiency than the boundaries derived from the mid-tread (zero-centered) $c_i^{k}$ values (See Fig. 18 for the results)."}, {"title": "2) Boundary adjustment", "content": "Some sub-interval fragments, the first and last valid sub-intervals with clipped step sizes, often\nIn practice, we use an offset to make $k$ a zero-based index tend to be significantly narrower than the learned normal quantization step sizes $\\Delta_{l,i}$. Although these narrow sub-intervals can reduce quantization error to some extent as shown in Fig. 7-(a), it comes at the cost of a higher bit rate. Thus, it decreases the compression efficiency in an overall R-D perspective. To mitigate it, as shown in Fig. 7-(b), our DeppHQ adaptively performs the boundary adjustment that can avoid severely narrowed sub-intervals. Specifically, when the ratio between the step size of the first (last) sub-interval and the normal step size $\\Delta_{l,i}$ is below a certain threshold $T$, we adaptively exploit the expanded boundaries $\\tilde{b}_{l,i}$ as follows:\n$\\breve{b}_{l,i} = \\begin{cases}\n\\tilde{b}_{l,i}, & \\text{if } r_{i,i} < T \\\\\nb_{l,i}, & \\text{otherwise}.\n\\end{cases}$ (7)\nwhere $b_{l,i}$ is selectively determined depending on $r_{i,i}$, the ratio of the first (last) valid sub-interval width in $b_{l,i}$ compared to $\\Delta_{l,i}$. If $r_{i,i}$ is smaller than the threshold $T$, the expanded boundary vector $\\breve{b}_{l,i}$ is used instead of $b_{l,i}$. We set $T$ to 0.3 from the experiments with various $T$ values shown in Fig. 8. The expanded boundaries $b_{l,i}$ are determined as:"}, {"title": null, "content": "$\\tilde{b}_{l,i} = (k - 0.5) \\times \\bar{\\Delta}_{l,i} + \\breve{y}_{l-1.i}$  (8)\nwith $\\bar{\\Delta}_{l,i} = \\frac{UB_{1,i} - LB_{1,i}}{N_{b_{l,i}} - 2},$\nwhere $\\bar{\\Delta}_{l,i}$ represents the expanded step size and $N_{b_{l,i}}$ denotes the number of valid sub-intervals in the original boundary vector $b_{l,i}$ (e.g. $N_{b_{l,i}}$=5 in Fig. 7-(a)). In Eq. 8, the denominator indicates the adjusted number of quantization sub-intervals inside the range from $LB_{1,i}$ to $UB_{1,i}$ (e.g. $N_{b_{l,i}}-2$=3 in Fig. 7-(b)). Accordingly, the range between $LB_{li}$ and $UB_{1,i}$ has an integer number of $\\bar{\\Delta}_{l,i}$-sized sub-intervals. This adaptive boundary adjustment significantly improves coding efficiency, as seen in Sec. V-C (See Fig. 18)."}, {"title": "3) PMF calculation for entropy coding and decoding", "content": "For the entropy coding and decoding of the sub-interval index $k$, the PMF-approximate for $y_i^*$ in the quantization layer $l$ is determined as follows:\n$P(y_i^* \\in I_i^{k} | y_i^* \\in I_{l-1,i}) = \\frac{\\Phi(b_{i}^{k+1}) - \\Phi(b_{i}^{k})}{\\Phi(UB_{1,i}) - \\Phi(LB_{1,i})},$  (9)\nwhere $P(\\cdot)$ represents the conditional probability that $y_i^*$ falls into the sub-interval $I_k$ at the $l$-th quantization layer when $y_i^*$ is in the interval $I_{l-1,i}$ at the lower quantization layer, and $\\Phi(\\cdot)$ represents the cumulative distribution function (CDF) determined based on the distribution parameters estimated by the hyper-decoder network. In this work, zero-mean Gaussian based on $\\sigma_i$ is used because $y_i^*=y_i - \\mu_i$ is an unbiased representation. Note that Eq. 9 is the generalized form of Eq. 1, where the denominator denotes the probability over all possible ranges where $y_i^*$ can be located in the current quantization layer, and the numerator represents the probability for each sub-interval. Fig. 6 illustrates an example of quantization intervals of two adjacent quantization layers where the denominator in Eq. 9 is highlighted with the light blue area. For example, $P(y_i \\in I_2 | y_i \\in I_{1=1,i})$ is calculated as $\\{\\Phi(8.0) - \\Phi(5.0)\\} / \\{\\Phi(15.0) - \\Phi(5.0)\\}$.\nFig. 9 shows the variations of the learned quantization step sizes $\\Delta_l$ in different quantization layers for different target qualities. As shown, our learned quantization hierarchy (DeepHQ) shows significantly diverse variations in channel-wise quantization step sizes for the representation $y$ of 320 channels in 8 different quantization layers, compared to the handcrafted quantization hierarchy with a layer-wise fixed size and number of quantization intervals. These results demonstrate that our DeepHQ actively leverages various learned channel- and quantization layer-wise quantization step sizes that benefit overall coding efficiency."}, {"title": "B. Selective encoding of representation components", "content": "Compressing all representation components regardless of a quantization layer can lead to suboptimal compression efficiency in a progressive coding model. In particular", "26": "in the variable-rate compression field", "follows": "n$\\tilde{y"}, "_l = \\text{Re}(\\text{DQ}(k,\\Delta_l, I_{l-1}), m(z,l)),$  (10)\nwith $k = \\text{Q}((y^*)_l, \\Delta_l, I_{l-1}),$\n$(y^*)_l = M(y^*, m(z,l)),$\nwhere $m(z, l)$ is the 3-D binary mask generated from the quantized hyperprior representation $z$ for the $l$-th quantization layer, indicating which representation elements of $y^*$ are selected for compression, $M(\\cdot)$ is the representation selection operator that extracts only the representation components indicated by $m(z,l)$, $(y^*)_l$ is the set of the selected components of $y^*$ for $l$-th quantization layer, and $\\text{Re}(\\cdot)$ is an operator that reshapes $(\\tilde{y}", "l$, which is in 1-D shape, back into the original 3-D shape using the same mask $m(z, l)$. Note that unselected components in $\\tilde{y}'$ are filled with zeros by $\\text{Re}(\\cdot)$ in Eq. 10. The hierarchical quantization and dequantization processes $\\text{Q}(\\cdot)$ and $\\text{DQ}(\\cdot)$ are basically the same as those described in Sec. IV, except that $LB_{1,i}=LB_{1,i}$ and $UB_{1,i}=UB_{1,i}$ are used for the representation elements which are first included at the $l$-th quantization layer.\nIn addition, because the fully generalized mask generation of the original SCR [26"], "follows": "i) A single 1x1 convolution layer is applied to the output of the penultimate convolutional layer (after the activation) in the hyper-decoder network, generating a 3-D importance map $im(z)$ of the same size as the $y$ representation. This 3-D importance map $im(z)$ represents the canonical (representative for all the compression quality levels, independently of the compression quality levels.) importance of the representation components of $y$ in a component-wise manner with the values between 0 and 1. ii) To derive the target quality"}