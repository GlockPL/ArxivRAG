{"title": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Anqi Zhao", "Jianyuan Liang", "Zhipeng Gui", "Xuefeng Guan", "Rui Li", "Huayi Wu"], "abstract": "The increasing demand for spatiotemporal data and modeling tasks in geosciences has made geospatial code generation technology a critical factor in enhancing productivity. Although large language models (LLMs) have demonstrated potential in code generation tasks, they often encounter issues such as refusal to code or hallucination in geospatial code generation due to a lack of domain-specific knowledge and code corpora. To address these challenges, this paper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along with the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and LoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first LLM focused on geospatial code generation, fine-tuned from Code Llama-7B. Furthermore, we establish a comprehensive geospatial code evaluation framework, incorporating option matching, expert validation, and prompt engineering scoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the GeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms other models in multiple-choice accuracy by 9.1% to 32.1%, in code summarization ability by 1.7% to 25.4%, and in code generation capability by 1.2% to 25.1%. This paper provides a solution and empirical validation for enhancing LLMs' performance in geospatial code generation, extends the boundaries of domain-specific model applications, and offers valuable insights into unlocking their potential in geospatial code generation.", "sections": [{"title": "1. Introduction", "content": "Code generation technology translates natural language into source code (NL2Code), which can pertain to general-purpose programming languages such as Python, C, or JavaScript, or domain-specific languages for specialized platforms like MATLAB, Bioconductor, or Google Earth Engine(Dehaerne et al., 2022; Rakhmetov, 2023). Geospatial code is specifically designed for geospatial analysis platforms (e.g., Google Earth Engine, ArcGIS) or modeling tasks(Tamiminia et al., 2020), typically implemented through wrappers around general-purpose languages or by invoking specialized geocomputational tools and libraries, such as GDAL and ArcPy in Python, or the Raster and Terra packages in R(Hou et al., 2024). With the surge in spatiotemporal data, the growing demand for geospatial modeling tasks has made geospatial code generation technology a critical factor in enhancing productivity(Al-Yadumi et al., 2021; Bill et al., 2022; Liang et al., 2024).\nCode generation is fundamentally a subset of natural language processing (NLP)(Shin and Nam, 2021). Early research in this area primarily relied on heuristic rules(Chae et al., 2017; Milligan et al., 2003; Nymeyer and Katoen, 1997) and expert systems(Depradine, 2003; Imam et al., 2014), such as probabilistic grammar-based frameworks(Cekan and Kotasek, 2017) and small-scale pretrained models(Hammad et al., 2020). These methods depended on annotated data, had limited applicability, and were difficult to scale. With the advent of large language models (LLMs) based on self-attention mechanisms and the Transformer architecture, models trained on vast corpora have demonstrated \u201cemergent\u201d abilities, such as task execution, in-context learning, and reasoning(Hadi et al., 2023). General-purpose LLMs specifically designed for code generation, such as Code LLaMA(Roziere et al., 2023), DeepSeek Coder(Guo et al., 2024), and WizardCoder(Luo et al., 2023), are reshaping the research paradigm in this field(Sun et al., 2024b). These models' instruction-based code generation capabilities allow even programming novices to produce functional code, driving the democratization of programming and significantly boosting productivity, enabling users to focus on higher-level logic and planning.\nGeneral-purpose code generation LLMs are predominantly trained on code related to standard programming tasks(Jiang et al., 2024). In contrast, geospatial code typically handles complex spatiotemporal data, characterized by specific data formats (e.g., geographic coordinates, multi-dimensional rasters, multi-band spectra) and massive datasets (e.g., global remote sensing collections). Since geospatial code is often executed on specialized platforms with unique function syntax and data flow control logic, it significantly differs from general-purpose code. Furthermore, these platforms' built-in datasets frequently use proprietary internal indexing and naming conventions, which general LLMs are usually unfamiliar with. As a result, when generating geospatial code, models may fail to encode correctly, or produce code with incorrect operator choices, non-compliant syntax, unreasonable parameters, incomplete logic, or mismatched inputs and outputs. These issues, illustrated in Fig. 1, are referred to as \u201crefusal to code\u201d or \u201ccoding hallucinations.\u201d(Hou et al., 2024)"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1 Challenges in Domain Specialization for LLMs", "content": "LLMs, based on the Transformer architecture and self-attention mechanisms, exhibit remarkable NLP capabilities due to their large-scale parameters and diverse training corpora(Zhao et al., 2023). However, they face two major challenges in domain adaptation: first, retraining requires significant computational resources, making it difficult to rapidly update domain-specific knowledge, which can lead to knowledge gaps(Feng et al., 2024). Second, the limited availability of specialized data in general-purpose corpora may result in \"knowledge hallucinations(Martino et al., 2023),\" where the model generates content that appears correct but is factually inaccurate in highly specialized tasks. Thus, advancing the domain specialization of LLMs has become a key research focus, with the goal of enhancing model performance by integrating domain-specific knowledge. While there have been successful applications in fields such as law(Cui et al., 2023; Izzidien et al., 2024; Sun et al., 2024a), finance(Wu et al., 2023), biomedicine(Wang et al., 2024a), and earth sciences(Feng et al., 2023; Foroumandi et al., 2023), research on domain-specific LLMs for geospatial code generation remains unexplored."}, {"title": "2.2 Instruction Data Generation and Fine-Tuning Strategies for Code Generation in LLMS", "content": "The application of LLMs in code generation relies on their capabilities in instruction execution, contextual learning, and reasoning to transform natural language descriptions into source code. High-quality instruction datasets are crucial for enhancing a model's ability to follow instructions(Zhang et al., 2023). Research has shown that LIMA-65B, fine-tuned with just 1,000 carefully designed prompts and responses, achieved comparable performance to GPT-4 in 43% of evaluation cases(Zhou et al., 2024). However, obtaining high-quality data is challenging due to issues of scarcity, privacy, and cost(Long et al., 2024). The Self-Instruct framework addresses these limitations by enabling models to generate synthetic data autonomously, eliminating the need for manual annotation(Wang et al., 2022). This approach has been successfully applied in models like Alpaca(Maeng et al., 2017) and Phi(Li et al., 2023), demonstrating the effectiveness of synthetic data in training(Jeong, 2024). Models are first pretrained on large-scale unannotated code to grasp structural and semantic patterns, followed by supervised fine-tuning through instruction tuning. While full fine-tuning (FFT) yields significant results(Christophe et al., 2024), it is highly resource-intensive. PEFT methods, such as LoRA(Hu et al., 2021), reduce the computational burden by minimizing parameter updates, and QLoRA(Dettmers et al., 2024) further enhances fine-tuning efficiency in resource-constrained settings through int4 quantization.Although LLMs have made notable progress in the field of code generation, particularly with open-source models like Meta Al's Code LLaMA(Roziere et al., 2023), DeepSeek Coder(Guo et al., 2024), Code Qwen(Bai et al., 2023), and WizardCoder(Luo et al., 2023), there remains a lack of specialized pretrained and instruction datasets for geospatial code generation. Moreover, fine-tuning strategies tailored for this domain have yet to be applied, and no dedicated LLM product for geospatial code generation has emerged."}, {"title": "2.3 Evaluation of Code Generation Quality in LLMs", "content": "In the task of code generation for LLMs, traditional token-matching metrics such as Exact Match(Blackwell et al., 2009), BLEU(Papineni et al., 2002), and ROUGE(Lin, 2004) struggle to accurately assess code executability, syntactic correctness, and functionality. To address this, execution-based metrics are increasingly adopted, such as pass@k, which evaluates the probability that at least one of the k generated code samples passes all unit tests(Chen et al., 2021), offering a more reliable reflection of the actual performance of the generated code. However, geospatial code often involves complex image processing and relies on platform-specific operations, making automated evaluation more challenging.Moreover, code evaluation must also consider factors such as readability, conciseness, and the accuracy of data usage(Scalabrino et al., 2018), further limiting the comprehensiveness of automated assessments. While human evaluation remains important for validating code quality, it is hindered by subjectivity and inconsistent standards(Hu et al., 2022). In recent years, methods leveraging LLMs for automatic evaluation have gained traction(Ren et al., 2020). By designing specific prompts, LLMs can act as \u201cjudges\u201d (LLM-as-a-Judge), as demonstrated in projects like AlpacaEval(Dubois et al., 2024) and MT-bench(Zheng et al., 2023). In these cases, LLMs not only provide scores but also explain the rationale behind their evaluations, improving interpretability(Zheng et al., 2023). However, the application of this approach to code generation is still in its early stages. The ICE-Score, which assesses code functionality and human preference through LLM evaluations, has shown promising correlation with human judgments and does not require additional benchmarks or references(Zhuo, 2023). This provides a novel evaluation approach, but it has yet to be applied to the evaluation of geospatial code generation."}, {"title": "3. Dataset Construction", "content": "This section provides a detailed description of the construction methods and processes for the pretraining corpus (GeoCode-PT), the supervised fine-tuning corpus (GeoCode-SFT), and the evaluation corpus (GeoCode-Eval) used in training the GeoCode-GPT model."}, {"title": "3.1 GeoCode-PT", "content": "In the pretraining phase, an unsupervised learning approach is employed, where the model autonomously learns code structures and semantic relationships by processing large-scale unannotated data, without relying on explicit instructions or labeled data. The pretraining corpus, GeoCode-PT, is designed to provide the model with broad foundational knowledge in the domain. A detailed breakdown of the GeoCode-PT dataset is shown in Table 1, comprising four main components: code documentation, operator knowledge documents, dataset knowledge documents, and encyclopedic documents."}, {"title": "3.2 GeoCode-SFT", "content": "The supervised fine-tuning corpus consists of high-quality, domain-specific instructions following an \u201cInstruct-Input-Output\" structure(Wang et al., 2024b), where the \"Instruct\" prompts the task, the \"Input\" provides the input data, and the \"Output\u201d generates the correct answer. These instructions are designed to enhance the model's ability to generate geospatial code, ensuring that GeoCode-GPT can accurately interpret human instructions and produce natural language outputs. GeoCode-SFT includes both geospatial code-related instructions and natural language-based instructions. The data breakdown is presented in Table 3. The construction of GeoCode-SFT involves the acquisition of open-source data, structured extraction, and Self-Instruct generation based on LLMs."}, {"title": "3.2.1 Structured Extraction", "content": "Structured information extraction is primarily achieved through a rule-based traversal algorithm to generate instruction sets, as illustrated in Fig. 3. The first method, rule slicing, is applicable to structured data formats such as JSON and CSV. By setting rules to partition the data, instruction sets containing \u201cinstruct,\u201d \u201cinput,\" and \"output\" are generated. For instance, in CSV data, the horizontal axis represents subjects, and the vertical axis represents attributes. By matching these, the attribute values for the subject are extracted, with \u201cinstruct\u201d as the task description, \"input\" as the subject name, and \u201coutput\u201d as the attribute value. This algorithm can quickly generate large volumes of instruction data, as shown in Fig. 3-(a). In JSON data, the key represents the attribute, and the value represents the data, which, when matched, forms the \"input\" and \"output\" fields, as shown in Fig. 3-(b).The second method, rule masking, is used for completing text or code snippets. For example, a code snippet is divided into three parts-prefix, middle, and suffix-based on sentence count. Through masking, \"instruct\" describes the completion task, \"input\" is the incomplete code snippet, and \"output\" is the completion portion, forming the instruction data, as shown in Fig. 3-(c)."}, {"title": "3.2.2 Self-Instruct generation", "content": "The instruction tuning data from structured extraction primarily organizes the textual content of the pretraining data in sequence, functioning as a form of \u201cpre-disclosure\u201d for downstream tasks. This method emphasizes surface-level information organization but lacks deeper semantic connections. In contrast, the Self-Instruct framework generates synthetic instruction sets through the model's reasoning abilities, uncovering the underlying semantic relationships and logical patterns. This approach not only systematically organizes knowledge but also reveals deeper logical relationships, aiming to significantly enhance the model's understanding and application capabilities in geospatial code generation tasks. The construction process is illustrated in Fig. 4."}, {"title": "3.2.3 Natural Language Style Instruction Tuning Data", "content": "To prevent pure code-related instruction data from potentially diminishing the general language understanding and coding capabilities of the model during fine-tuning, we collected the multilingual code understanding and generation instruction dataset, Alpaca-GPT-4, from the Hugging Face platform. We filtered the dataset to include multilingual data such as Chinese, English, Japanese, Arabic, and French. These diverse natural language instructions help enhance GeoCode-GPT's multilingual adaptability and improve the readability of its outputs."}, {"title": "3.3 GeoCode-Eval", "content": "The evaluation corpus is designed to test the model's performance in geospatial code generation tasks. Based on Benjamin Bloom's taxonomy of cognitive objectives(Forehand and technology, 2010), we developed the GeoCode-Eval dataset, which covers three key dimensions: \"Cognition and Memory,\u201d \u201cComprehension and Interpretation,\" and \"Innovation and Creation,\" as illustrated in Fig. 5."}, {"title": "4. Model Training", "content": "A high-quality base model is crucial for effective fine-tuning(Radiya-Dixit and Wang, 2020). Compared to general-purpose LLMs, general-purpose code generation models are already optimized for code generation tasks but lack domain-specific knowledge of geospatial code. Therefore, this study focuses on fine-tuning a general code generation model. QLoRA reduces GPU memory usage by approximately 75%, enabling larger batch sizes and longer sequences, making it suitable for large-scale code processing in resource-constrained environments. LORA, on the other hand, accelerates convergence during fine-tuning by about 66%, making it ideal for high-precision instruction tuning. As a result, we adopt QLoRA for pretraining and LORA for fine-tuning to achieve an optimal balance between computational resources and training efficiency. This section introduces the base model, Code Llama-7B, and the two training stages of GeoCode-GPT."}, {"title": "4.1 Base Model", "content": "In this study, we selected Code Llama as the base model. LLaMA-2 is a commonly used foundation for open-source code generation models, and Code Llama builds on this foundation with specialized training. It has demonstrated excellent performance in benchmark tests such as HumanEval and MBPP, particularly in tasks like code generation, code completion, and handling long contexts, even surpassing the larger LLaMA 2-70B model in some areas. This makes it highly suitable for optimizing geospatial code generation tasks. Additionally, Code Llama offers three parameter configurations: 7B, 13B, and 34B. Compared to the larger models, Code Llama-7B strikes an ideal balance between performance and lower resource consumption, while also supporting efficient LoRA fine-tuning. Considering performance, flexibility, and training costs, Code Llama-7B is the optimal choice for this study."}, {"title": "4.2 Pretraining", "content": "During the pretraining phase, we optimized the Code Llama-7B model using the GeoCode-PT corpus with QLoRA, combining quantization and low-rank adaptation to reduce memory usage and support the longest possible input sequences, thereby improving the model's learning efficiency. The specific hyperparameter configuration is as follows: a global batch size of 64, int4 quantization precision, and one training epoch. The initial learning rate was set at 0.0002, with a cosine decay scheduler and linear warmup over the first 5% of steps. A weight decay factor of 0.1 was applied to prevent overfitting. Gradient accumulation steps were set to 4, and the maximum input sequence length was 4096 tokens, ensuring smooth operation on two NVIDIA A100 40GB GPUs.For the QLoRA configuration, we set the rank value to 64, used NF4 quantization, and a scaling factor of 128. The main modules optimized included the q_proj, v_proj, k_proj, o_proj, and MLP layers. A dropout probability of 0.05 was applied to mitigate the risk of overfitting."}, {"title": "4.3 Fine-Tuning", "content": "After QLORA pretraining, the model acquired foundational knowledge in geospatial code generation. To further enhance the model's controllability and generalization to new tasks, we applied LoRA for lightweight fine-tuning, enabling more precise parameter adjustments. The fine-tuning settings were as follows: an initial learning rate of 0.0001 was chosen to ensure stability and gradual convergence during the fine-tuning process. The global batch size was set to 32, with gradient accumulation steps of 4, resulting in an effective batch size of 128. The input sequence length remained at 4096 tokens to handle longer contextual information. The LoRA configuration included a rank value of 64, targeting the key modules q_proj, v_proj, k_proj, o_proj, and the MLP. A dropout rate of 0.05 was maintained to prevent overfitting. The fine-tuning process was conducted on two NVIDIA A100 40GB GPUs, ensuring efficient utilization of computational resources and improving the model's performance in following complex instructions."}, {"title": "5. Evaluation", "content": "The evaluation was conducted using the GeoCode-Eval dataset. This section presents the evaluation methodology and results for GeoCode-GPT-7B, and compares its performance with leading LLMs across various domains. Commercial models included GPT-4, GPT-3.5, and ERNIE 4.0. For general open-source LLMs, we selected LLaMA 2-7B, LLaMA 3-8B, as well as general-purpose code generation models with similar parameter sizes: CodeGemma-7B, StarCoder 2-7B, and CodeGeeX 2-6B. The baseline model was Code Llama-7B, with the higher-parameter version, Code Llama-13B, included for reference."}, {"title": "5.1 Multiple-Choice Questions", "content": "Each multiple-choice question provides four options, with one correct answer, covering six dimensions: Operator Knowledge (OK), Datasets Knowledge (DK), Platform or Toolkits Knowledge (PTK), Platform or Toolkits Recognition (PTR), Programming Language Recognition (PLR), and Entity Recognition (ER). The model's output was then compared with the correct answer, and the corresponding accuracy was calculated. The evaluation results are presented in Table 5."}, {"title": "5.2 Subjective Questions", "content": "Geospatial code often involves the analysis and processing of spatiotemporal imagery, which is difficult for standard compilers to directly handle or visualize. As a result, evaluation typically relies on experts, which is costly and subjective. Similar challenges arise when assessing the readability, entity accuracy, and completeness of code or summaries. Since the semantic expression of the same entity can vary, string-matching scores lack flexibility. To address this, we designed specific prompts for GPT-4 to act as an \u201cevaluator\" for automatic assessment. Given the token cost of LLMs, the prompt template was carefully crafted to emphasize conciseness in scoring, aiming to improve expression efficiency and avoid overly lengthy responses."}, {"title": "5.2.1 Code Summarization", "content": "The evaluation of code summarization is based on three metrics: Completeness, Accuracy, and Readability. Completeness requires the summary to cover six key dimensions: code overview, datasets used, spatial scope, temporal scope, input/output data types, and the functional implementation process. Accuracy demands that, despite potential differences in expression, the semantic content must be correct. Readability focuses on clear logic, smooth phrasing, and concise expression. A prompt was designed to guide GPT-4 to reference the standard answer and assign a score, ranging from 1 to 10, which is then converted to a 0-1 scale for comprehensive evaluation. The prompt design is illustrated in Fig. 6."}, {"title": "5.2.2 Code Generation", "content": "The code generation evaluation is based on three core metrics: Accuracy, Readability, and Executability. The scoring methodology is illustrated in the figure. Accuracy refers to the proportion of key entities (such as data sources, time, space, input/output data) in the generated code that match the requirements of the task. This is calculated by using GPT-4 with a specific prompt template to extract and compare the generated code with the target entities, assessing consistency as shown in Fig. 7. Executability is determined by experts who run the generated code on different platforms; the percentage of successfully executed code represents the executability score. Readability evaluates whether the code structure is clear, comments are appropriate, and whether variable naming and function lengths follow standard conventions. Experts rank the generated code from different models through a blind selection process. The readability score is based on the average ranking of each model and is calculated using the formula $(11 - n) / 11$, where n is the average ranking of the model."}, {"title": "6. Conclusion", "content": "In this study, we developed the GeoCode-PT and GeoCode-SFT corpora, along with the GeoCode-Eval evaluation set, and introduced GeoCode-GPT-7B, the first large language model dedicated to geospatial code generation. By employing QLoRA and LoRA for pretraining and fine-tuning, we achieved significant improvements in the model's performance. We also established a comprehensive evaluation framework, incorporating option matching, expert validation, and LLM scoring, and conducted a systematic assessment using GeoCode-Eval. The results showed that GeoCode-GPT-7B demonstrated significant improvements over the base model, Code Llama-7B. Although it did not surpass GPT-4 in certain metrics, GeoCode-GPT-7B exhibited clear advantages over models with similar parameter sizes and outperformed larger models such as Code Llama-13B and ERNIE 4.0 in all metrics, as well as GPT-3.5 in most metrics. These results validate the effectiveness of the corpus construction and fine-tuning strategies, as well as GeoCode-GPT-7B's performance superiority in the GeoCode-Eval evaluation."}, {"title": "6.1 Limitations", "content": "This study provides insights into optimizing the performance boundaries of LLMs in geospatial code generation tasks. To address the gap with GPT-4 in certain metrics, future work will focus on expanding the scale of instruction data, improving data quality, and exploring more efficient methods for structured knowledge representation. Although GeoCode-GPT-7B has shown improvements in code executability, there is still room for enhancement. Future efforts will involve increasing both the scale and precision of high-quality code training corpora, along with leveraging more powerful hardware and additional training epochs to further improve the model's executability in code generation tasks."}, {"title": "6.2 Outlook", "content": "This study provides a preliminary exploration into the application of LLMs in geospatial modeling tasks. Future research can further unlock their potential by developing models with larger parameter scales and creating customized geospatial code generation models. By integrating multi-platform interfaces, automated code testing could be achieved, reducing reliance on expert knowledge and prompt engineering. Additionally, exploring cross-platform code translation capabilities could lower the syntactic barriers between platforms, enhancing user experience.Further research could focus on models tailored for repository-level geospatial code generation, incorporating indexing, contextual information, and memory modules to handle complex tasks. Lastly, the establishment of multi-agent collaboration frameworks could automate code generation, testing, and optimization, thereby improving both the quality and efficiency of outputs. We anticipate more significant advancements in this field in the near future."}]}