{"title": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation", "authors": ["Aniket Deroy", "Subhankar Maity", "Sudeshna Sarkar"], "abstract": "Automatic question generation is a critical task that involves evaluating question quality by considering factors such as engagement, pedagogical value, and the ability to stimulate critical thinking. These aspects require human-like understanding and judgment, which automated systems currently lack. However, human evaluations are costly and impractical for large-scale samples of generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM Iterative Review and Response for Optimized Rating), which leverages large language models (LLMs) to automate the evaluation process for questions generated by automated question generation systems. We experimented with several state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We observed that the scores of human evaluation metrics, namely relevance, appropriateness, novelty, complexity, and grammaticality, improved when using the feedback-based approach called MIRROR, tending to be closer to the human baseline scores. Furthermore, we observed that Pearson's correlation coefficient between GPT-4 and human experts improved when using our proposed feedback-based approach, MIRROR, compared to direct prompting for evaluation. Error analysis shows that our proposed approach, MIRROR, significantly helps to improve relevance and appropriateness.", "sections": [{"title": "1 Introduction", "content": "Automated question generation (AQG) is crucial in education because it enhances critical thinking, promotes active learning, and provides personalized learning experiences. Currently, metrics such as BLEU, METEOR, and ROUGE are used for the purpose of evaluation of open ended questions [Zhang et al., 2021, Al Faraby et al., 2023, Deroy et al., 2023a, 2024c]. However, traditional automated evaluation metrics such as BLEU, ROUGE, or METEOR, which are often used for tasks like machine translation or summarization, may not be well-suited for evaluating the quality of generated questions [Alva-Manchego et al., 2021, Bhandari et al., 2020, Reiter, 2018, Rao et al., 2022, Deroy et al., 2021]. These metrics primarily measure surface-level similarity to reference questions, rather than deeper aspects of question quality such as relevance, clarity, and engagement [Nema and Khapra, 2018, Maity et al., 2024e]. Some aspects of question quality, such as engagement, pedagogical value, and the potential to stimulate critical thinking, require a human-like understanding and judgment that automated systems cannot yet replicate. However human judgements are costly and cannot be replicated over large samples of generated questions. So we propose an LLM feedback-based system"}, {"title": "2 Related Work", "content": "Evaluating open-ended question generation using automated metrics (e.g., BLEU, ROUGE, METEOR, etc.) presents significant challenges [Nema and Khapra, 2018, Gong et al., 2022, Ushio et al., 2022, Maity et al., 2024a, 2023, 2024f,g, Deroy and Maity, 2023b, Nigam et al., 2023]. These metrics often fail to capture higher-order cognitive skills and overlook deeper educational values such as stimulating critical thinking [Al Faraby et al., 2023, Mulla and Gharpure, 2023, Deroy et al., 2024b, Nigam and Deroy, 2023, Deroy et al., 2023b, Maity et al., 2024c, Deroy and Maity, 2023a, Deroy et al., 2024a]. Automated evaluations may not fully account for nuances such as context relevance and cognitive complexity, making it difficult to assess whether questions promote skills such as analysis, synthesis, and evaluation [Mulla and Gharpure, 2023, Deroy and Maity, 2024a,b,c,d, Ghosh et al., 2023, Maity and Deroy, 2023]. Furthermore, scalability issues arise because human evaluation for large datasets is impractical [Al Faraby et al., 2023, Deroy, 2019, Maity et al., 2024d]. Using LLMs to evaluate human-like criteria such as grammaticality, relevance, appropriateness, novelty, and complexity has been a prominent research area [Chiang and Lee, 2023]. Aligning machine-generated content with human judgment is crucial for practical applicability. Incorporating feedback loops within LLMs to refine their outputs is an emerging field [Madaan et al., 2023, Chaudhari et al., 2024, Ouyang et al., 2022, Stiennon et al., 2020]. Techniques like Reinforcement Learning from Human Feedback (RLHF) fine-tune LLMs based on human preferences, enhancing the quality and contextual relevance of responses [Yuan et al., 2023, Chaudhari et al., 2024, Wu et al., 2023]. This work proposes a novel method using LLM feedback to evaluate open-ended questions based on human-like metrics: grammaticality [Ushio et al., 2022, Maity et al., 2024b,a], relevance [Maity et al., 2024b], appropriateness [Maity et al., 2024b], complexity [Gong et al., 2022, Maity et al., 2024b], and novelty [Maity et al., 2024b]. This approach aims to bridge the gap between automated metrics and human evaluation, enhancing the quality and applicability of generated questions in educational settings."}, {"title": "3 Dataset", "content": "We use 1000 samples from the EduProbe dataset [Maity et al., 2024b] for our experiments. The dataset consists of  pairs from subjects such as History, Geography, Economics, Environmental Studies, and Science. For EduProbe, we already have the gold-standard questions corresponding to the context. We also use 500 samples from the SciQ dataset [Welbl et al., 2017] for our experiments. The dataset consists of  pairs from subjects such as Physics, Chemistry, Biology, and Earth Science. For SciQ dataset we create a question corresponding to the context by using an educator. The contexts from both these datasets are considered for our experiments to generate the questions using the LLMs and then to evaluate them automatically via LLMs. Together EduProbe and SciQ datasets covers a wide domain of subjects necessary for generating open-ended questions and showing the wider applicability of our work."}, {"title": "4 Methodology", "content": "In this section, we discuss the generation of questions via prompting GPT-3.5 Turbo (Section 4.1), the direct approach to evaluating the quality of the generated questions (Section 4.2), the feedback-based approach for evaluating generated question quality (Section 4.3), and the correlation between the best-performing LLM and human experts (Section 4.4)."}, {"title": "4.1 Generating Questions via Prompting", "content": "Figure 1 provides  pair from the EduProbe and SciQ dataset respectively. Figure 2 provides the prompt for generating questions from a context. We prompt GPT-3.5 Turbo to generate question from a context corresponding to EduProbe and SciQ datasets. We evaluate the generated questions based on five metrics: grammaticality, appropriateness, relevance, novelty, and complexity."}, {"title": "4.2 Direct Approach for Evaluating Question Quality", "content": "The algorithm 1 provides the set of steps required to produce the direct approach. The overview diagram for our direct approach is shown in Figure 3. The prompt provided to the LLMs for the direct prompting approach is shown in Figure 4."}, {"title": "4.3 Feedback-based Approach for Evaluating Question Quality", "content": "The input to our proposed MIRROR approach includes human evaluation metric definitions, the initial strengths set (So), the initial flaws set (Fo), a generated question to be evaluated, and its corresponding context. The output from our approach is the final human evaluation metric scores. The goal of our algorithm is to perform repeated feedback between two LLMs (i.e., LLM\u2081, LLM2) so that the metric scores converge. The strengths and flaws generated by the LLMs are extracted from the entire output using a rule-based pattern-matching algorithm. The algorithm 2 provides the set of steps required to produce the MIRROR approach.\nInitially, the process starts by defining a set of human evaluation metrics which are the criteria used to assess the quality of the question. These metrics include grammaticality, appropriateness, relevance, complexity, and novelty which contribute to the effectiveness of the question. At the outset, two sets are initialized: one for strengths (So) and another for flaws (Fo). Both sets start empty. The next step involves computing initial scores for the question based on the predefined evaluation metrics. From this assessment by LLM\u2081, the first sets of strengths and flaws are identified, resulting in S1 and F\u2081. The process then enters an iterative loop designed to refine these initial assessments. In each iteration, the identified strengths and flaws (S1, F1) are provided as feedback to the second LLM, LLM2. Along with this feedback, the human evaluation metrics, the question itself, and its context are also provided. LLM2 then generates new scores for the evaluation metrics and updates the sets of strengths and flaws to S2 and F2. The updated strengths and flaws from LLM2 (i.e., S2, F2) are then fed back to LLM1. LLM\u2081 uses this information to re-evaluate the question, updating its scores and further refining the sets of strengths and flaws to S3 and F3. This process continues iteratively, with each model using the feedback from the other to refine its evaluation, until a convergence criterion is met. The convergence is typically defined as the point at which the scores from LLM1 and LLM2 become identical for two consecutive iterations. Once convergence is achieved, the loop terminates, and the final, converged scores and associated strengths and flaws are considered the accurate evaluation of the question. This iterative process ensures that the evaluation is thorough and benefits from the complementary perspectives of two different LLMs, ultimately leading to a more reliable and nuanced assessment of the question.\nThe overview diagram for our MIRROR approach is shown in Figure 5. The prompt provided to the LLMs for producing the MIRROR approach is shown in Figure 6. The method described is a process for refining the evaluation of a question using a feedback loop between two LLMs. The goal is to achieve a consensus on the evaluation scores and identify key strengths and flaws in the question, ensuring that the evaluation is as accurate and comprehensive as possible."}, {"title": "4.4 Human Baseline Scores and Pearson\u2019s Correlation Coefficient", "content": "We recruit three educators and ask them to evaluate the questions generated by prompting GPT-3.5 Turbo in terms of grammaticality, appropriateness, relevance, novelty, and complexity to generate the human baseline scores corresponding to EduProbe and SciQ datasets. The three educators were asked to provide scores for each metric on a scale of 1 to 5. The scores given by the educators for each metric were then averaged. The inter-annotator agreement is calculated in terms of Fleiss\u2019s Kappa. The inter-annotator agreement between the three experts is 0.67, 0.64, 0.66, 0.45, and 0.59 for grammaticality, appropriateness, relevance, novelty, and complexity, respectively. Later we also calculate the Pearson\u2019s correlation coefficient [Freedman et al., 2020] between the best-performing model (i.e., GPT-4) and human experts."}, {"title": "5 Results", "content": "Human Evaluation Results and Correlation Analysis. Table 1 shows the human evaluation metric scores for the EduProbe dataset. Table 2 shows the human evaluation metric scores for the SciQ dataset. Table 3 shows the Pearson\u2019s correlation coefficient scores for the EduProbe and SciQ datasets between GPT-4 and human experts. We observe that the human baseline scores the highest in grammaticality, relevance, and appropriateness, but lower in novelty and complexity for both the EduProbe and SciQ datasets. GPT-4 performs closest to the human baseline, especially in the feedback-based approach, with high scores in grammaticality and appropriateness. Gemini consistently scores slightly lower than GPT-4 across all metrics and evaluation methods. Llama2-70b has the lowest scores among the LLMs explored but still performs relatively well, with its highest score in grammaticality. Overall, LLMs tend to score higher in the feedback-based approach compared to the direct approach, suggesting that feedback helps improve perceived performance. In summary, GPT-4 outperforms Gemini and Llama2-70b in both evaluation methods, with scores closest to human performance, especially in grammaticality and appropriateness. Gemini performs moderately well, while Llama2-70b shows relatively lower scores across the metrics. For the EduProbe and SciQ datasets, GPT-4 shows higher correlation scores with human experts when using the feedback-based approach compared to the direct approach across all metrics. The highest correlation score is for grammaticality with a feedback-based approach. For the feedback-based approach, appropriateness and relevance show moderate correlations, suggesting a reasonable alignment with human expert evaluations in these areas. We observe lower correlations in novelty and complexity because both approaches (i.e., feedback-based and direct) show lower correlations in these areas, with the lowest being complexity in the direct approach. The direct approach generally yields lower correlation scores across all metrics, highlighting the importance of feedback in improving GPT-4\u2019s alignment with human expert evaluations. In summary, the correlation scores suggest that GPT-4\u2019s evaluations are more aligned with human experts when using the feedback-based approach, particularly in grammaticality. The alignment is moderate in appropriateness and relevance, while it is weaker in novelty and complexity. Direct approach results in overall lower correlation scores across all metrics.\nFor the SciQ and EduProbe datasets, the feedback-based approach achieves a higher correlation with human evaluations in terms of grammaticality, appropriateness, relevance, novelty, and complexity, suggesting that iteratively refining the questions using LLM feedback better aligns with human judgments on grammatical correctness (See Table 3). For appropriateness, the feedback-based approach shows a modest improvement over the direct approach. This indicates that feedback helps in evaluating whether the questions are suitable and contextually appropriate more closely to how humans would rate them. The feedback-based approach significantly outperforms direct prompting in relevance, suggesting that iterative LLM feedback effectively enhances the model\u2019s ability to judge how relevant the questions are to the given context. The feedback-based method also achieves a higher correlation in assessing novelty. This indicates that feedback helps the model better understand and evaluate the uniqueness and originality of the questions. While the improvement in complexity evaluation is smaller, the feedback-based approach still surpasses direct prompting. This suggests that feedback helps the model slightly better assess the intricacy and difficulty of the questions. Overall, the higher correlation scores for the feedback-based approach across all metrics suggest that iteratively using LLM feedback improves the alignment of GPT-4\u2019s evaluations with human expert judgments, making it a more effective method for assessing the quality of generated questions."}, {"title": "Error Analysis", "content": "We conducted a human study using questions from the EduProbe and SciQ datasets, which were evaluated by each LLM using both feedback-based and direct approaches. We observed"}, {"title": "6 Examples of Outputs Generated by LLMs Using the Feedback-based Approach", "content": "The feedback approach consists of feedback occurring between two LLMs, namely LLM\u2081 and LLM2. Figure 7 shows the output provided by LLM\u2081 on a generated question based on Economics framed from the EduProbe dataset. Figure 8 shows the output provided by LLM2 on a generated question based on Economics framed from the EduProbe dataset. We observe that the scores provided by LLM\u2081 and LLM2 for different human evaluation metrics have become the same after completion of the Feedback Approach."}, {"title": "7 Conclusion", "content": "In this work, we propose a novel system, MIRROR (Multi-LLM Iterative Review and Response for Optimized Rating), which utilizes LLMs to automate the evaluation process for open-ended questions generated by AQG systems. Our experiments with state-of-the-art LLMs, including GPT-4, Gemini, and Llama2-70b, show that the scores for the human evaluation metrics namely relevance, appropriateness, novelty, complexity, and grammaticality improve when using the feedback-based approach called MIRROR and tend to be closer to the human baseline scores. We also observe that Pearson's correlation coefficient between GPT-4 and human experts improves when using MIRROR compared to using direct apparoach for evaluation. Our error analysis shows that relevance and appropriateness are two metrics where our proposed approach, MIRROR, significantly improves. The results demonstrate that LLMs have the potential to provide a scalable and effective alternative to human evaluation, offering a promising solution for assessing question quality in AQG systems. So far, we have focused on short and medium-sized contexts, whereas future work will focus on applying MIRROR to longer contexts."}, {"title": "A Appendix", "content": "A.1 Results\nTable 4 shows the human evaluation metric scores for the EduProbe dataset. Table 5 shows the human evaluation metric scores for the SciQ dataset. Table 6 shows the Pearson's correlation coefficient scores for the EduProbe and SciQ datasets between GPT-4 and human experts. We observe that the human baseline scores the highest in grammaticality, relevance, and appropriateness, but lower in novelty and complexity for both the EduProbe and SciQ datasets. GPT-4 performs closest to the human baseline, especially in the feedback-based approach, with high scores in grammaticality and appropriateness. Gemini consistently scores slightly lower than GPT-4 across all metrics and evaluation methods. Llama2-70b has the lowest scores among the LLMs explored but still performs relatively well, with its highest score in grammaticality. Overall, LLMs tend to score higher in the feedback-based approach compared to the direct approach, suggesting that feedback helps improve perceived performance. In summary, GPT-4 outperforms Gemini and Llama2-70b in both evaluation methods, with scores closest to human performance, especially in grammaticality and appropriateness. Gemini performs moderately well, while Llama2-70b shows relatively lower scores across the metrics. For the EduProbe and SciQ datasets, GPT-4 shows higher correlation scores with human experts when using the feedback-based approach compared to the direct approach across all metrics. The highest correlation score is for grammaticality with a feedback-based approach. For the feedback-based approach, appropriateness and relevance show moderate correlations, suggesting a reasonable alignment with human expert evaluations in these areas. We observe lower correlations in novelty and complexity because both approaches (i.e., feedback-based and direct) show lower correlations in these areas, with the lowest being complexity in the direct approach. The direct approach generally yields lower correlation scores across all metrics, highlighting the importance of feedback in improving GPT-4's alignment with human expert evaluations. In summary, the correlation scores suggest that GPT-4's evaluations are more aligned with human experts when using the feedback-based approach, particularly in grammaticality. The alignment is moderate in appropriateness and relevance, while it is weaker in novelty and complexity. Direct approach results in overall lower correlation scores across all metrics.\nFor the SciQ and EduProbe datasets, the feedback-based approach achieves a higher correlation with human evaluations in terms of grammaticality, appropriateness, relevance, novelty, and complexity, suggesting that iteratively refining the questions using LLM feedback better aligns with human judgments on grammatical correctness. For appropriateness, the feedback-based approach shows a modest improvement over the direct approach. This indicates that feedback helps in evaluating whether the questions are suitable and contextually appropriate more closely to how humans would rate them. The feedback-based approach significantly outperforms direct prompting in relevance, suggesting that iterative LLM feedback effectively enhances the model's ability to judge how relevant the questions are to the given context. The feedback-based method also achieves a higher correlation in assessing novelty. This indicates that feedback helps the model better understand and evaluate the uniqueness and originality of the questions. While the improvement in complexity evaluation is smaller, the feedback-based approach still surpasses direct prompting. This suggests that feedback helps the model slightly better assess the intricacy and difficulty of the questions. Overall, the higher correlation scores for the feedback-based approach across all metrics suggest that iteratively using LLM feedback improves the alignment of GPT-4's evaluations with human expert judgments, making it a more effective method for assessing the quality of generated questions."}, {"title": "A.2 Pearson's Correlation Coefficient between LLMs and Human Experts", "content": "Table 7 and Table 8 show the results of the Pearson correlation coefficient between LLMs and human experts for the EduProbe and SciQ datasets, respectively. For the EduProbe dataset, all three models show improved correlation scores across all metrics when the feedback-based approach is used compared to the direct approach. GPT-4 shows the most significant improvement in correlation scores with the feedback-based approach, indicating that it benefits the most from receiving feedback. GPT-4 consistently outperforms Gemini and Llama2-70b across all metrics and both approaches (i.e., direct and feedback-based). Gemini performs better than Llama2-70b in all metrics, both with the direct approach and feedback-based approach in terms of correlation scores. GPT-4 with the feedback-based method achieves the highest correlation scores across all metrics, making it the best-performing model in this comparison. The significant increase in scores with the feedback-based approach in all"}, {"title": "A.3 Error Analysis", "content": "Table 9 shows the percentage of questions with exact matches in scores for different human evaluation metrics using both the direct approach and the feedback-based approach. We conducted a human study based on 100 questions from the EduProbe and SciQ datasets, respectively evaluated by each LLM using both approaches. We observed that the scores provided by human experts matched with the direct approach in 54%, 39%, 36%, 42%, and 46% of cases for grammaticality, appropriateness, relevance, novelty, and complexity, respectively, for GPT-4. In contrast, the scores provided by human"}, {"title": "A.4 Examples of Outputs Generated by LLMs Using the Feedback-based Approach", "content": "The feedback approach consists of feedback occurring between two LLMs, namely LLM\u2081 and LLM2. Figure 9 shows the output provided by LLM\u2081 on a generated question based on Economics framed from the EduProbe dataset. Figure 10 shows the output provided by LLM2 on a generated question based on Economics framed from the EduProbe dataset. We observe that the scores provided by LLM\u2081 and LLM2 for different human evaluation metrics have become the same after completion of the Feedback Approach. The context involved in the question generation process is: \"Purchasing power parity (PPP) is an economic indicator that signifies the purchasing power of the currencies of various nations of the world against each other. It helps in comparing living standards between different countries and estimating economic productivity.\". The question generated by GPT-3.5 Turbo is: \"What does purchasing power parity do?\""}, {"title": "A.5 Implementation Details", "content": "For GPT-3.5 Turbo we use a temperature value of 0.7 and max_tokens size of 2048. For GPT-4 we use a temperature value of 0.7 and max_tokens size of 2048. For Llama2-70b we use a temperature value of 0.8 and max_tokens size of 2048. For Gemini we use a temperature value of 0.7 and max_tokens size of 2048."}]}