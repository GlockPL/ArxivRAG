{"title": "GVDepth: Zero-Shot Monocular Depth Estimation for Ground Vehicles based on Probabilistic Cue Fusion", "authors": ["Karlo Koledi\u0107", "Luka Petrovi\u0107", "Ivan Markovi\u0107", "Ivan Petrovi\u0107"], "abstract": "Generalizing metric monocular depth estimation presents a significant challenge due to its ill-posed nature, while the entanglement between camera parameters and depth amplifies issues further, hindering multi-dataset training and zero-shot accuracy. This challenge is particularly evident in autonomous vehicles and mobile robotics, where data is collected with fixed camera setups, limiting the geometric diversity. Yet, this context also presents an opportunity: the fixed relationship between the camera and the ground plane imposes additional perspective geometry constraints, enabling depth regression via vertical image positions of objects. However, this cue is highly susceptible to overfitting, thus we propose a novel canonical representation that maintains consistency across varied camera setups, effectively disentangling depth from specific parameters and enhancing generalization across datasets. We also propose a novel architecture that adaptively and probabilistically fuses depths estimated via object size and vertical image position cues. A comprehensive evaluation demonstrates the effectiveness of the proposed approach on five autonomous driving datasets, achieving accurate metric depth estimation for varying resolutions, aspect ratios and camera setups. Notably, we achieve comparable accuracy to existing zero-shot methods, despite training on a single dataset with a single-camera setup. Project website: https://gvdepth.github.io/", "sections": [{"title": "1. Introduction", "content": "Accurate 3D reconstruction is one of the fundamental computer vision challenges, with far-reaching applications across various fields. Unlike multi-view approaches that rely on complex dense matching and pose estimation, learning-based Monocular Depth Estimation (MDE) exploits semantic and geometric cues to infer pixel-wise depth maps from a single image. However, these cues are often specific to a narrow distribution of the training data, resulting in poor generalization to out-of-distribution scenarios.\nIn addition to the environmental domain gap, MDE is particularly sensitive to the domain gap induced by the perspective geometry parameters of the camera system. This issue is present in most supervised [1, 11, 24, 34, 51] and self-supervised methods [14, 15, 42, 52-55] that are trained on a dataset collected with a single camera system. Models tend to overfit to this perspective geometry bias, leading to significant performance degradation when inferring depth with different cameras. Unfortunately, training a MDE model on data with a wide distribution of perspective geometries is far from trivial. Due to the inherent ambiguity of the MDE problem and entanglement of depth and camera parameters [35], many methods resort to relative depth estimation with unknown shift and scale [8, 21, 36, 43, 46, 48, 49]. Recently, zero-shot metric depth estimation has been achieved with various approaches, including embedding [16], estimating [3, 35] or disentangling the camera parameters [50]. While these methods present impressive results, they require training on data acquired from multiple camera systems, which is not always avaialable or cost-effective.\nOne specific area with numerous applications across varying fields is MDE for ground-based vehicles and robots. In this context, an additional perspective geometry constraint arises due to the fixed position of the camera relative to the ground plane. This constraint induces an additional depth cue: the vertical image position of the ground-contact point, which has been shown to be a dominant cue in autonomous driving scenarios [10]. The vertical image position cue is closely related to the depth of the ground plane that has recently been leveraged to deliver State-of-the-Art (SotA) depth estimation accuracy in both supervised [47] and self-supervised [4, 42] settings. However, these methods demonstrate limited generalization results, as they are not designed with intention to enable zero-shot estimation.\nThese findings motivate us to utilize the vertical image position cue and the inherent planarity of the scene in order to enable accurate depth estimation and improve generalization for arbitrary intrinsic and extrinsic camera parameters. Contrary to the object size cue, widely used by the aforementioned zero-shot methods [3, 16, 35, 50], we argue that the vertical image position presents a cue that is more fundamentally rooted in the perspective geometry configuration of the scene. However, without proper modeling this cue is highly susceptible to overfitting to the specific camera system present in the training data [23, 33].\nTo leverage this cue, while enabling zero-shot generalization, in this paper we propose a method to ensure its consistency for arbitrary camera setups. Specifically, we introduce a novel intermediate depth representation termed Vertical Canonical Representation. Rather than directly regressing depth values, our model predicts vertical image positions of the orthogonal projections of points onto the ground plane, as visualized in Fig. 1. By utilizing the known depth of the ground plane, this approach allows the mapping between the canonical representation and depth space to adapt with varying camera parameters, effectively disentangling the vertical position cue from specific perspective geometry. To further address potential errors in certain regions, we introduce an architecture that adaptively fuses depth predictions from both vertical position and object size cues, guided by dynamically estimated uncertainties.\nWe conduct comprehensive evaluation of zero-shot MDE in autonomous driving scenarios, thoroughly analyzing the robustness and accuracy of depth estimation with different cues. We demonstrate that our approach is the first to achieve accurate out-of-distribution metric depth estimation when trained solely on a single dataset captured by a single vehicle-camera system. This marks a significant advancement through enabling generalizable metric MDE in data-scarce and specialized scenarios, such as mobile robotics."}, {"title": "2. Related Work", "content": "Domain-dependent Monocular Depth Estimation. In a seminal work [11], learning-based MDE has been established as a supervised dense regression task via ground-truth data obtained by reprojection of LiDAR scans. Subsequent works improve upon the original approach by introducing various architectural novelties, such as convolutional models with skip connections between encoder and decoder [24], integration of conditional random fields [25, 51], adversarial training [20, 28] or attention-based architectures [1, 34]. Unfortunately, these methods are specifically fine-tuned for accuracy on well established indoor [39] and outdoor [13] benchmarks, experiencing significant accuracy degradation under distribution shifts [23].\nDomain-agnostic Monocular Depth Estimation. The inherent ambiguity and interdependence between camera parameters and depth present challenges for training a robust MDE model that generalizes across a wide range of environments. As datasets are usually collected with a single camera system, models tend to act discriminatively and associate perspective geometry parameters with the pertaining environment. Relative depth estimation methods [8, 21, 36, 43, 46, 48, 49] bypass camera parameter dependency by estimating depth with an unknown scale. While these approaches achieve visually impressive results, their inability to estimate metric depth limits their practicality in real-world applications, often necessitating integration with additional sensors.\nZoeDepth [2] is the first work to show satisfactory metric accuracy across different domains by fine-tuning a pre-trained relative depth estimation model on a single dataset with metric scale. ZeroDepth [16] chooses a different approach by providing the model with embedding of camera parameters, effectively disambiguating the features from the specifics of the camera system. Metric3D [50] addresses the focal length-depth ambiguity by estimating depth in a canonical space defined through focal length denormalization. UniDepth [35] introduces a universal approach, simultaneously predicting both the depth and camera parameters, enabling metric depth estimation for images acquired with unknown sensors. Although these methods demonstrate impressive results, they rely on abundance of training data and computational resources. Furthermore, they are not truly zero-shot, as they tend to overfit to the typically high training resolution, hindering real-time performance on resource-constrained devices that require low-resolution estimation.\nPlanar Constraints for Depth Estimation. Real-world scenes are often highly regular, featuring numerous planar structures that can serve as additional cues to guide depth"}, {"title": "3. Proposed Method", "content": "Preliminaries. In this work, we examine an extremely common application domain of MDE: ground vehicles or a robot with a fixed camera configuration. Assuming an ego coordinate system defined at the ground level, we consider a camera system parameterized by intrinsic matrix K and extrinsic matrix Rit, which transforms points from camera to ego coordinate frame:\n$K = \\begin{bmatrix}f_x & 0 & C_x\\\\ 0 & f_y & C_y\\\\ 0 & 0 & 1\\end{bmatrix}, [R|t] = \\begin{bmatrix}1 & 0 & 0\\\\ 0 & \\cos \\theta & \\sin \\theta\\\\ 0 & -\\sin \\theta & \\cos \\theta\\end{bmatrix} \\begin{bmatrix}0\\\\ 0\\\\ h\\end{bmatrix},$\nwith 0, h representing camera pitch and height, respectively. We disregard the camera yaw and roll in our parameterization; yaw has no effect on our canonical transformations, while roll is typically negligible in real-world setups.\nProblem Statement. We aim to learn a dense mapping f : (I, C) \u2192 D, with I \u2208 RH\u00d7W\u00d73, D \u2208 RH\u00d7W being RGB image and a correspondending depth map, and C representing camera parameterization in Eq. (1). Most importantly, the learned mapping should be able to generalize to images acquired with arbitrary camera system. In contrast to existing zero-shot methods [3, 16, 35, 50], which rely on multiple datasets with varying camera parameters, we aim to learn this mapping using data acquired with a single camera system CTrain."}, {"title": "3.1. Depth Cues", "content": "Monocular depth estimation relies on semantic and geometric cues in the training data to solve the depth ambiguity of monocular perspective geometry. As illustrated in Fig. 2, one of the possible cues to regress depth d is the relation between object's real-world size S and imaging size s:\n$d = f_y \\frac{S}{s}$\nHowever, this cue leads to entanglement between focal length and depth, leading to generalization issues when camera parameters change. Recent zero-shot methods address this issue by embedding [16], estimating [3, 35] or disentangling the focal length [50]. A major drawback of this cue is its reliance on the object's real world size S, which is highly dependent on the particular environmental domain.\nFor ground vehicles, a fixed position of the camera system relative to the ground introduces an additional perspective geometry constraint \u2013 known depth of the ground plane, which is constant for all images acquired by the same camera setup. For each pixel (u, v), depth d is calculated from the intersection of the camera rays dK\u22121[u, v, 1]T and the ground plane RTn \u2013 h = 0, with n = (0, -1, 0) being the upward ground normal. This constraint enables depth regression via object's ground contact point:\n$d = \\frac{f_yh}{(H \u2013 C_y - y) \\cos(\\theta) \u2013 f_y \\sin(\\theta)},$\nwhere y = H - v represents vertical image position of the ground projection, as visualized in Fig. 2. Compared to the object size cue, which is entirely semantic, this cue implicitly encodes the known perspective geometry constraints of the scene, thereby inducing a valuable prior to the ill-posed MDE problem. However, it is highly reliant on accurate and consistent camera calibration."}, {"title": "3.2. Canonical Transforms", "content": "Metric3D [50] proposed a canonical transform disentangling the predicted depth and camera parameters, enabling the use of object size cue for arbitrary camera systems. To achieve this, depth is predicted in canonical space CF with a canonical focal length fc, and post-processed to metric depth DF by applying Focal Canonical Transform FCTc (\u00b7), where DF = FCTC (CF) = fc CF, and C = {fy}.\nBesides the object size cue, in this paper we propose to leverage the vertical image position cue, effectively encoding the configuration of the scene's perspective geometry. Unfortunately, this cue also induces entanglement between camera parameters and depth. To enhance cross-dataset generalization, we introduce an additional canonical transform, which enables consistent exploitation of the ground plane prior. Under this formulation, the model estimates a dense map Cy representing vertical image position of the ground projection point. Metric depth Dy is then obtained via Vertical Canonical Transform VCTc(\u00b7), where VCTC(CY) corresponds to mapping y\u2192 d obtainable from Eq. (3). Therefore, this transformation requires knowledge of camera parameters C = {fy, Cy, h, 0}. Crucially, the proposed canonical space remains consistent under variations of camera parameters, facilitating generalization to diverse scenarios.\nThe proposed VCT is related to the depth of the ground plane, frequently used to enhance MDE accuracy in autonomous driving scenarios [4, 42, 47]. However, these methods exploit the ground-plane constraint exclusively for road pixels, resulting in limited generalization capability."}, {"title": "3.3. Model Architecture", "content": "Both cues have their advantages and drawbacks. For example, vertical position cue is highly consistent across environments due to the reduced reliance on semantic information. However, it is highly sensitive to calibration errors, and performs worse as depth increases. An ideal model should leverage both cues, taking into account the confidence for respective image regions. To that end, as visualized in Fig. 3, we design our model to predict depth maps with both cues separately, and then fuse them into a final depth map based on the estimated aleatoric uncertainty.\nWe use a standard encoder-decoder model with skip connections to process and upsample image features, gaining F\u2208 Rh\u00d7w\u00d7C, where (h, w) = (, W). We forward these features to two very small UNet networks [37], producing our canonical representations (CF, CY) and accompanying uncertainty estimates (\u03a3F, \u03a3\u03b3). Canonical representations are forwarded to FCT(\u00b7) and VCT(\u00b7), obtaining (DF, DY), which are then fused into a final depth map D \u2208 Rhxw.\n$D = \\frac{\\Sigma_Y D_F + \\Sigma_F D_Y}{\\Sigma_Y + \\Sigma_F}$"}, {"title": "3.4. Geometric Augmentation", "content": "Contrary to zero-shot methods trained on multiple datasets [3, 16, 35, 50], we aim to achieve similar generalization capabilities by training on a single camera system. However, without geometric diversity induced by varying camera parameters, model can simply disregard our proposed canonical transformations. Therefore, we apply geometric augmentations, namely random cropping and resizing, to simulate different camera perspectives. Crucially, compared to [3, 35, 50], we train our model with multiple resolutions, enabling generalization for arbitrary image sizes. A comparable augmentation strategy is suggested in [18, 42], aiming to enhance generalization for arbitrary cameras. However, as we will demonstrate, without the use of canonical transformations, this approach does not effectively transfer across different datasets."}, {"title": "3.5. Optimization", "content": "Our main training objective is to guide the fused depth map D towards the ground-truth depth D*. Given the distance in the log domain E = log(D) \u2013 log(D*), the SIlog loss [11] is defined as:\n$L_{SIlog} = \\alpha \\sqrt{[E]^2} + \\Lambda E^2[E],$"}, {"title": "4. Experiments", "content": "Datasets. We use a diverse set of autonomous driving datasets, each collected with a different camera setup. Specifically, we utilize an aggregation of KITTI [13], DDAD [15], DrivingStereo [45], Waymo [40] and Argoverse Stereo [6] datasets. We split the data into training and validation sets as in the official data splits, utilizing images only from frontal cameras. Due to the missing and sometimes inaccurate information, we calibrate extrinsic parameters, i.e., camera height and pitch, which are required for our canonical representation. Further details are provided in the supplementary materials.\nImplementation details. We use a standard encoder-decoder with skip connections [14]. In all experiments we use a ConvNext-Large [27] encoder initialized with ImageNet weights. To provide the model with additional contextual information, we fuse ground plane embedding in the decoder layers [23, 47]. In our depth cue fusion model, secondary U-Nets are designed with a lightweight structure, containing only three convolutional layers for both downsampling and upsampling.\nOur models are trained in PyTorch [31] with an AdamW optimizer [29]. Learning rate is progressively reduced with Cosine Annealing from 1e-4 to 1e-5. For all datasets, we use random cropping and resizing to \u0397 \u2208 [200, 500], with W chosen to preserve the aspect ratio after cropping. To increase resolution diversity in a single batch update, we perform gradient accumulation for 2 training steps. All models are trained for 150000 optimizer steps with an effective batch size of 16, on a single NVIDIA RTX A6000 GPU. Compared to concurrent zero-shot methods [16, 35, 50], this constitutes a fairly simple training setup, both in computational and data resources, demonstrating the efficiency of our approach.\nEvaluation details. We use established depth metrics: absolute relative error (A.Rel), root mean squared error (RMS), root mean squared log error (RMS10g), square relative error (Sq.Rel) and accuracy \u03b4\u03b5 under a threshold 1.252. Depth is evaluated in the metric range of [0 m, 80 m].\nWe evaluate GVDepth at an image resolution of H \u00d7 640, where H varies according to the dataset to preserve the original aspect ratio. For a fair comparison, domain-specific models are trained and tested using the same image dimensions, which depend on specific dataset. All models are evaluated using a ConvNext-L backbone if available; otherwise, we select the backbone with the most similar complexity. For zero-shot methods, we use the provided checkpoints. For metric zero-shot models which are optimized for a specific training resolution, we report results at both their original training resolution and our testing resolution for a comprehensive evaluation."}, {"title": "4.1. Evaluation of Zero-shot Accuracy", "content": "Comparison with State-of-the-Art. We perform extensive evaluation, comparing GVDepth with SotA domain-dependent and domain-agnostic methods. Results for KITTI and DDAD datasets are shown in Tab. 1. Both supervised [34, 51] and self-supervised [14, 54] domain-dependent methods perform well on datasets matching their training domain but overfit to environmental and geometric biases, leading to reduced accuracy in KITTI \u2192 DDAD transfer. Relative depth estimation models exhibit greater adaptability across domains; however, they require ground-truth scale and shift alignment, limiting their practical applicability. Recent zero-shot metric methods, such as ZeroDepth, UniDepth, and Metric3D, demonstrate strong generalization capabilities but rely on extensive, diverse training datasets, which are not always feasible to obtain. Furthermore, UniDepth and Metric3D suffer accuracy degradation when inferring on resolutions lower than their high-resolution training data. While resizing and padding can address this, these adjustments increase computational demands, limiting their suitability for resource-constrained systems. GVDepth delivers comparable results while being adaptable to arbitrary camera setups, aspect ratios, and image resolutions. Depending on the train-test dataset combination, our method ranks first or second across most met-"}, {"title": "4.2. Ablation studies", "content": "Evaluation of canonical representations. In Tab. 3 we report extensive evaluation results, spanning different representations and train-test dataset combinations. Moreover, in Fig. 5 we report qualitative results accompanying the table. By carefully examining the results, we can draw the following conclusions: i) The baseline model struggles to transfer effectively to out-of-distribution data. Although baseline models are trained with diverse perspectives through geometric augmentations, they still rely heavily on environmental cues specific to the training domain; (ii) Vertical image position cues generalize better than object size cues. Models with our novel VCTC(\u00b7) representation generally outperform FCTC(\u00b7) models, except in rare cases, potentially stemming from calibration errors. This is particularly evident with models trained on the Waymo dataset, which encounter difficulties when relying solely on object size cues. The probable cause is that Waymo's cameras are positioned significantly higher, causing a variation of apparent object sizes due to the change of perspective. In contrast, vertical image position cues are more robust, implicitly encoding camera height within the representation; (iii) Probabilistic fusion improves generalization by integrating both cues. Our fusion strategy achieves the most accurate predictions by adaptively weighting the more reliable cue. For example, when object size cues are more accurate (e.g. DDAD \u2192 KITTI), the model adjusts accordingly, and the same applies for vertical position cues (e.g. Waymo \u2192 Argoverse).\nAblation of fusion strategy. In Tab. 4 we examine the efficacy of different fusion strategies. Zero-shot transfer results demonstrate that our adaptive fusion weighted by aleatoric uncertainties leads to superior generalization performance.\nGeneralization in specific regions. The strong quantitative generalization results of vertical position cues do not fully capture their performance. For instance, a model with VCTC(\u00b7) might only perform better on road pixels due to its implicitly encoded planarity. However, as shown in Fig. 6,"}, {"title": "5. Conclusion and Future Work", "content": "In this work, we present GVDepth, a novel MDE model capable of accurate zero-shot transfer across varying environmental domains and camera setups. Our approach is tailored for autonomous vehicles and mobile robotics, where a fixed relation between the camera system and the ground plane introduces useful perspective geometry constraints. To leverage these constraints for zero-shot generalization, we proposed a Vertical Canonical Representation that uses known depth of the ground plane to enforce invariance to specific perspective geometry, resulting in enhanced learning and generalization. Additionally, we introduced a novel architecture that adaptively fuses depths obtained with object size and vertical image position cues, guided by dynamically estimated uncertanties.\nSystematic ablation studies across five autonomous driving datasets demonstrated that our representation significantly improves generalization compared to object size cues, commonly used in existing zero-shot methods. Overall, our approach achieves generalization accuracy comparable to or exceeding state-of-the-art zero-shot models, all while training on a single dataset and requiring significantly fewer computational and data resources. In future work, we plan to extend this approach to multi-dataset and large-scale learning, moving toward a comprehensive foundation model for MDE in autonomous driving and mobile robotics."}, {"title": "A. Model architecture", "content": "In this work, we employ a fully convolutional encoder-decoder architecture with skip connections. While vision transformer (ViT)-based encoders, such as DINOv2 [30], often achieve superior accuracy, their advantages typically rely on large-scale training. A similar fully convolutional design is adopted in Metric3D [50], demonstrating the effectiveness of such models for generalizable monocular depth estimation (MDE). Given our focus on single-dataset training, we prioritize convolutional backbones to reduce computational complexity while maintaining robust gener-alization performance.\nDecoder architecture details. Our decoder, illustrated in Fig. 5, consists of four blocks that progressively upsam-ple and fuse encoder features from a resolution of ( , ) to (, ), while integrating 16-dimensional ground plane embeddings, similarly as in [23, 47]. Decoder channels di-mensions are {756, 512, 256, 128}. Our fusion module pro-cesses (, ) resolution feature maps F with two compactU-Net-like networks [37], each containing 3 downsamplingand upsampling blocks, producing two final feature repre-sentations. These feature representations are processed withtwo convolutional blocks which predict our canonical rep-resentations (CF, CY) and accompanying uncertainty es-timates (\u03a3F, \u03a3\u03b3). After uncertainty-based fusion, final depth map is bilinearly upsampled to (H, W) resolution."}, {"title": "B. Model Complexity", "content": "In Tab. 6 we present the architectural details of the mod-els used in this work. While most results align with expec-tations, there are a few notable outliers worth discussing.First, ZeroDepth exhibits the highest inference times, de-spite utilizing the least complex backbone among all meth-ods. Essentially, ZeroDepth shifts complexity from the pre-trained backbone to its proprietary self-attention layers inthe decoder. This trade-off limits its ability to fully leveragethe benefits of large-scale pretraining, which may explainits subpar accuracy compared to UniDepth, Metric3D, andGVDepth, even though it uses significantly more data dur-ing training.\nOn the other hand, Metric3D achieves remarkably lowlatency in depth prediction despite employing a relativelycomplex backbone. This efficiency likely stems from pre-dicting depths at a reduced (, ) resolution, which is"}, {"title": "C. Dataset details", "content": "In this work, we use KITTI [13], DDAD [15], Driving-Stereo [45], Waymo [40] and Argoverse Stereo [6] datasets,both for training and evaluation. For KITTI dataset, weevaluate all models on commonly used Eigen split [11] withGarg crop [12], resulting in 23158 training images and 652testing images. On DDAD dataset we use the official train-ing and validation split, with 12650 and 3950 images, re-spectively. Since Waymo, DrivingStereo, and ArgoverseStereo are not widely used for MDE evaluation, we simplifythe process by creating custom dataset splits. The resultingtraining splits consist of {156K, 168K, 5K} samples, whilethe corresponding testing splits contain {5K, 5K, 500} sam-ples, respectively."}, {"title": "D. Camera setup calibration", "content": "In this section, we provide additional details about our cam-era calibration procedure. Our proposed Vertical Canoni-cal Transform VCTC (\u00b7), as indicated in Eq. (9), requires theknowledge of camera parameters C = {fy, Cy, h, \u03b8}. Here,for all datasets, fy and cy are usually known up to the rea-sonable error induced by the calibration procedure. How-ever, for certain datasets, camera height h and camera pitch\u03b8 are either unknown, or not properly calibrated. To remainconsistent throughout this work, we recalibrate the extrin-sic parameters for each dataset, with details provided in Al-gorithm 1. For semantic segmentation of the road plane weuse the DeepLabv3 model [7]."}]}