{"title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity", "authors": ["Weixin Liang", "Junhong Shen", "Genghan Zhang", "Ning Dong", "Luke Zettlemoyer", "Lili Yu"], "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modality-aware sparsity through modality-specific parameterization of the Mamba block. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996; 2024), we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency. We evaluate Mixture-of-Mamba across three multi-modal pretraining settings: Transfusion (interleaved text and continuous image tokens with diffusion loss), Chameleon (interleaved text and discrete image tokens), and an extended three-modality framework incorporating speech. Mixture-of-Mamba consistently reaches the same loss values at earlier training steps with significantly reduced computational costs. In the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting, Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at the 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the three-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the 1.4B scale. Our ablation study highlights the synergistic effects of decoupling projection components, where joint decoupling yields greater gains than individual modifications. These results establish modality-aware sparsity as a versatile and effective design principle, extending its impact from Transformers to SSMs and setting new benchmarks in multi-modal pretraining.", "sections": [{"title": "1. Introduction", "content": "State Space Models (SSMs) (Gu et al., 2021; Gu & Dao, 2023) have emerged as efficient alternatives to Transformers for sequential modeling, offering linear scaling in sequence length and strong performance in single-modality tasks. Mamba, a recent SSM variant, has demonstrated exceptional efficiency and scalability across diverse tasks by leveraging advanced gating mechanisms and selective state-space scanning (Gu & Dao, 2023). Despite these advantages, SSMs, including Mamba, remain inherently dense, applying the same set of parameters across all input tokens, regardless of modality. This uniform parameterization limits their ability to capture modality-specific features, leading to suboptimal performance in multi-modal pretraining.\nRecent efforts have extended SSMs to multi-modal tasks. Works like VLMamba (Qiao et al., 2024) and Cobra (Zhao et al., 2024) augment Mamba for vision-language modeling by adding LLaVA-style projection modules that map image features into the token space of Mamba. In the vision domain, Vision Mamba (Zhu et al., 2024) and VMamba (Liu et al., 2024c) incorporate bidirectional scanning schemes and selective 2D scanning paths for image patch modeling. Similarly, Mamba has been explored for diffusion-based image and video generation, as seen in DiffuSSM (Yan et al., 2024) and Zigma (Hu et al., 2024), which employ unique state-space scanning patterns. While these approaches demonstrate the adaptability of Mamba, they are orthogonal to our focus, which introduces modality-aware sparsity directly into the Mamba block itself.\nA promising approach to address such limitations is model sparsity, exemplified by Mixture-of-Experts (MoE) (Jacobs et al., 1991; Eigen et al., 2013; Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Jiang et al., 2024; Sukhbaatar et al., 2024). MoE reduces computational load by activating only a subset of model components for each input token, allowing experts to specialize in specific aspects of the data. Despite its potential, MoE-based architectures face challenges such as imbalanced expert utilization, bi-level optimization instability, and inefficient load balancing (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Shen & Yang, 2021; Xu et al., 2024). These issues motivate the need for alternative sparse architectures that are computationally efficient and easier to optimize.\nIn multi-modal contexts, prior work (Bao et al., 2022b; Wang et al., 2022; Shen et al., 2023b; Lin et al., 2024) has introduced modality-aware sparsity in Transformer-based MoE architectures. These approaches activate specific experts or parameters based on modality, enabling models to specialize in handling diverse data types. Other methods fine-tune modality-specific modules atop dense LLM backbones (Wang et al., 2023; He et al., 2024; Shen et al., 2023a; 2024b). Such methods show that simple rule-based modality routing often outperforms learned routing, likely due to improved training stability and reduced optimization challenges.\nThe closest work to our approach is MoE-Mamba (Pi\u00f3ro et al., 2024) and the related Blackmamba architecture (Anthony et al., 2024), which interleave Mamba blocks with MoE-augmented MLP layers. While effective, these hybrid designs apply sparsity only to the MLP layers, leaving the dense Mamba blocks unmodified. In contrast, we present Mixture-of-Mamba, a novel architecture that directly introduces modality-aware sparsity into the Mamba block itself. Inspired by Mixture-of-Transformers (Liang et al., 2024), our approach dynamically selects modality-specific weights in every input processing component of Mamba, enabling stable and efficient multi-modal pretraining. Furthermore, prior work (Liang et al., 2024) shows that MoE techniques can complement sparse architectures like Mixture-of-Transformers, suggesting that Mixture-of-Mamba and MoE-based MLP sparsification can be combined to achieve further gains.\nTo rigorously evaluate Mixture-of-Mamba, we conduct experiments across three multi-modal pretraining settings:\n\u2022 Transfusion: Interleaved text and continuous image tokens with distinct autoregressive and diffusion-based objectives. Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale.\n\u2022 Chameleon: Interleaved text and discrete image tokens. Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs and similar text loss with only 65.40% of the FLOPs at the 1.4B scale.\n\u2022 Three-Modality: Extension of the Chameleon setting to include speech. Mixture-of-Mamba matches speech loss using only 24.80% of the FLOPs at the 1.4B scale, while maintaining strong performance across image and text modalities.\nAdditionally, we perform an ablation study to analyze the contribution of modality-specific parameterization. Our findings reveal a synergistic effect: jointly decoupling all components yields greater gains than individual modifications, underscoring the importance of modality-aware sparsity as a holistic design principle.\nIn summary, Mixture-of-Mamba establishes a versatile and efficient architecture for SSMs by extending modality-aware sparsity into the Mamba block. This approach delivers robust performance gains and substantial computational savings across diverse multi-modal settings, setting new benchmarks in scalable multi-modal pretraining."}, {"title": "2. Method", "content": "2.1. The Mixture-of-Mamba Block\nOur hypothesis is that explicitly parametrizing the selection in SSMs with the modality can improve the data efficiency of multi-modality training (Liang et al., 2024).\nFollowing the setting of other SSMs (Gu et al., 2021), Mixture-of-Mamba is composed of homogeneous Mixture-of-Mamba blocks (line 1-13 of Algorithm 1).\nIn Mixture-of-Mamba, modality-specific parameterization is applied to all projections that explicitly process input features belonging to a single modality, including input projection ( Win-proj), intermediate projections (Wx-proj and Wdt-proj), and output projection ( Wout-proj). Conv1D and state transitions A remain shared because they operate across multiple features or on aggregated RNN-like states, where the notion of modality is not well-defined. After parametrized by modality M, the linear transformation XW + b becomes M(X, W, b; M). M applies the weight of modality m (Wm) to tokens of modality m (Xm) in parallel based on the modality mask. The output shape of M is the same as the corresponding linear transformation.\nThe shape of Win-proj is [f,(d,d)] where f is the feature dimension of input Fin and d is the expanded feature dimension. These two projections are fused together for efficiency and Wx-proj uses the same technique. Line 1, 12 and 13 can be viewed as a SwiGLU (Shazeer, 2020) around the conv+SSM (Line 2-12). x is passed to conv+SSM and z will be transformed to the gate in SwiGLU.\nThe Conv1D in Line 2 can help collect local information across time as observed in (Sun et al., 2024). Similarly, Conv1D can also gather local information across modalities and we keep the weight-sharing property of convolution without separating the convolution kernel into different modalities.\nLine 3-12 is multi-modality selective SSM. It is composed of parameter preparation (line 3-6), RNN update (line 7-11), and residual connection (line 12).\nA is the discretization time step. It is derived from u through a low-rank approximation u \u2192 \u03b4 \u2192 \u2206 followed by a softplus as shown in Line 3 and 4. A is of shape [d,n] and A is of shape [b,l,d] where b is batch size, l is sequence length, and n is the state dimension. Line 5 is a broadcast element-wise multiplication where A is unsqueezed to [b,l,d,1] and repeated to [b,l,d,n]. Line 6 first applies a batched outer product between u [b,l,d] and B [b,l,n] whose result is element-wise multiplied with A. Line 5 and 6 apply the selection to A,B and get A, B, respectively. B can be viewed as a gated input u and A can be viewed as a selection gate on the state h.\nLine 7-10 is a typical RNN operator with state h and output Yi. The yi's are concatenated together as output y. The gate application on input u is fused with gate parameter preparation at line 6 for efficiency.\nLine 12 first adds the input u to the output y as residual, which is the final output of SSM. Then, Line 12 applies the gate of \"SwiGLU\" to the output of SSM. Finally, line 13 projects o back to the feature dimension."}, {"title": "2.2. Multi-objective Training with Diffusion", "content": "Following Transfusion (Zhou et al., 2024), Mixture-of-Mamba is trained on interleaved multi-modal sequences of discrete text tokens and continuous image tokens using a combined objective that incorporates both language modeling and diffusion-based image generation. Each image is encoded as a sequence of latent patches using a Variational Autoencoder (VAE), where each patch is represented as a continuous vector. The patches are sequenced left-to-right, top-to-bottom, and inserted into the discrete text sequence.\nThe diffusion process follows the Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020), where Gaussian noise is progressively added to the latent image patches during the forward process. Given a clean latent patch xo, a noised version xt at timestep t is created as:\n$xt = \\sqrt{at}xo + \\sqrt{1 \u2212 \u0101t}\u20ac, \u2208 ~ N(0, 1),$\nwhere \u0101t is determined by a cosine noise schedule (Nichol & Dhariwal, 2021), approximated as $\\sqrt{\u0101\u012b} \u2248 cos(\ub274\u318d\ud50c)$ with adjustments. During training, noise is added to the latent patches at a randomly selected timestep t, and the model is optimized to predict the noise \u20ac.\nThe overall training objective combines the autoregressive language modeling loss LLM, applied to the discrete text tokens, with the diffusion loss LDDPM, applied to the latent image patches:\n$L = LLM + 1. LDDPM,$\nwhere A balances the contributions of the two losses.\nImportantly, the conditioning for image generation is naturally embedded within the interleaved sequence. When denoising image patches, the preceding tokens including both text describing the image and prior images serve as context for conditional generation. This unified approach enables Mixture-of-Mamba to leverage the modality-aware sparsity to efficiently model both local intra-image dependencies and long-range inter-modal relationships across the sequence."}, {"title": "2.3. Training with Uniform Representations", "content": "As an alternative to the multi-objective training paradigm, we explore a unified representation strategy in which both text and image modalities are represented as discrete tokens. Following the Chameleon framework (Chameleon Team, 2024), we treat the image data as sequences of discrete tokens obtained through a pre-trained VQ-VAE model (Gafni et al., 2022). Specifically, each image is encoded into a fixed number of tokens (e.g., 1,024) by quantizing its latent features into a learned codebook. These tokens are then arranged sequentially, similar to the processing of text tokens, resulting in a uniform discrete representation across both modalities.\nDuring training, both text and image tokens are processed using the same autoregressive objective, where the model learns to predict the next token in the sequence given all previous tokens. Formally, the training objective is:\n$Luniform = Ex1:7 [\u2013 log P(xt | X1:t\u22121)],$\nwhere x1:T represents the interleaved sequence of text and image tokens. This objective allows the model to treat text and image data equivalently, unifying the training process across modalities while relying solely on an autoregressive loss. The use of discrete tokens for images simplifies the training procedure by removing the need for separate loss formulations, as in the diffusion-based approach. It also aligns with the inherent sequence-to-sequence nature of Mixture-of-Mamba, where the same modality-aware sparsity design can be applied seamlessly across the discrete text and image tokens.\nMotivation and Robustness Testing. We include this alternative strategy to evaluate the robustness of our Mixture-of-Mamba architecture under different choices of training objectives and data representations. By experimenting with uniform discrete representations, we demonstrate that Mixture-of-Mamba consistently outperforms Mamba Dense models across various settings, including both continuous (multi-objective) and discrete (uniform) representations. This highlights the versatility of Mixture-of-Mamba and its ability to deliver performance gains regardless of the underlying choice of modality representations or training objectives."}, {"title": "3. Results", "content": "3.1. Results in Multi-objective Training (Transfusion)\nWe evaluate Mixture-of-Mamba (MoM) against Mamba Dense and Flex-Attention Transformer in the Transfusion setting, where pretraining is performed on interleaved text and image data across three model scales: 163M, 760M, and 1.4B. For clarity, performance gain is quantified as:\n$Performance Gain (%) = \\frac{LossDense - LOSSMixture}{LossDense} \u00d7 100,$\nwhere LossDense and Loss Mixture are the final losses of Mamba Dense and Mixture-of-Mamba, respectively. Relative training FLOPs reflect the computational cost required for MoM to match the training dynamics (similar loss) of Mamba Dense. Flex-Attention Transformer (i.e., Transfusion (Zhou et al., 2024)) combines both attention patterns by applying causal attention to every element in the sequence and bidirectional attention within the elements of each individual image. This makes Flex- Attention Transformer an overestimated baseline for transformers because both Mamba and Mixture-of-Mamba are strictly causal across all elements, while Flex-Attention Transformer benefits from bidirectional attention within images.\nImage Modality. Mixture-of-Mamba (MoM) consistently demonstrates superior performance in image modality training loss across all model scales. At the 1.4B scale, MoM achieves a training loss of 0.2138, outperforming Mamba Dense by 2.20% while requiring only 34.76% of the training FLOPs. Similar trends are observed at smaller scales: at the 760M scale, MoM achieves a training loss of 0.2172, a 2.37% improvement over Mamba Dense, while reducing training FLOPs to 37.76%.\nThe validation loss curves on the CC12M dataset ((Table 4, Appendix Figure 5) further illustrate these trends. Mixture-of-Mamba consistently achieves lower image validation loss compared to Mamba Dense and Flex-Attention Transformer, with the improvements becoming more pronounced as model size increases. Additionally, loss matching curves demonstrate that MoM reaches equivalent loss values at earlier training steps, highlighting its improved training efficiency.\nText Modality. In the text modality, Mixture-of-Mamba consistently outperforms Mamba Dense across both training and validation metrics. At the 1.4B scale, MoM achieves lower validation losses on both the C4 (2.2695) and Wikipedia (1.7164) datasets compared to Mamba Dense, despite their similar training losses. This indicates better generalization to unseen text data. Importantly, MoM also performs comparably to or better than Flex-Attention Transformer, particularly on validation losses, as shown in Appendix Figure 4. Similar trends are observed at smaller scales (760M and 163M), where MoM reduces validation losses while maintaining high training efficiency.\nLoss matching results in Appendix Figure 4 (b, f, j) confirm that Mixture-of-Mamba aligns closely with or surpasses Mamba Dense, reaching comparable loss values earlier during training. These improvements highlight MoM's strong performance in text tasks while maintaining its computational efficiency."}, {"title": "3.2. Results in Training with Uniform Representations (Chameleon)", "content": "We evaluate Mixture-of-Mamba (MoM) in the Chameleon setting, where both image and text modalities are represented as discrete tokens. See our training configuration in Appendix Table 6. Results are summarized in Table 1, with full results across all five scales (37M, 94M, 443M, 880M, and 1.5B) provided in Appendix Table 7. Training dynamics and validation loss trends are visualized in Appendix Figures 7, 8, and 9.\nImage Modality. Mixture-of-Mamba (MoM) consistently demonstrates better performance in image modality training loss across all model scales, achieving substantial efficiency gains over Mamba Dense. At the 443M scale, MoM achieves a training loss of 5.1703, a 3.46% improvement over Mamba Dense, while requiring only 33.40% of the training FLOPs. Similar trends are observed at other scales: at the largest 1.5B scale, MoM achieves a training loss of 5.0591, a 2.51% improvement, with only 42.50% of the training FLOPs.\nText Modality. Mixture-of-Mamba (MoM) demonstrates consistent improvements in text modality training loss across all model scales. At the largest 1.5B scale, MoM reduces training loss to 2.1614, a 3.01% improvement over Mamba Dense, while requiring only 65.40% of the training FLOPs. Validation loss on Obelisc and a proprietary version of the Shutterstock datasets (SSTK) exhibits similar trends, with MoM achieving notable improvements in loss values while maintaining significant efficiency gains. These results further highlight MoM's ability to deliver strong text performance with improved convergence efficiency. These results highlight Mixture-of-Mamba's robust and efficient improvements in the Chameleon setting across both image and text modalities, with substantial computational savings."}, {"title": "3.3. Results in Training with Three Modalities (Chameleon+Speech)", "content": "To evaluate the robustness and scalability of Mixture-of-Mamba (MoM), we extend the Chameleon framework to include a third modality: speech, alongside image and text, with all modalities represented as discrete tokens. Speech data is tokenized using an in-house tokenizer, a variant of DinoSR (Liu et al., 2024a), which extracts semantic tokens with a vocabulary size of 500, where each token corresponds to 40ms of audio content. Results are summarized in Table 2, with additional training dynamics and evaluation loss trends visualized in Appendix Figures 11, 12, 13, and 14.\nSpeech Modality. Mixture-of-Mamba (MoM) achieves substantial improvements in speech modality training loss across all model scales. At the 443M scale, MoM improves speech training loss by 7.14% compared to Mamba Dense. To match the training loss achieved by Mamba Dense, MoM requires only 19.20% of the training FLOPs, demonstrating significant efficiency gains. Similar trends hold at the largest 1.5B scale, where MoM achieves a 5.75% improvement in speech training loss and matches Mamba Dense's loss with just 24.80% of the training FLOPs."}, {"title": "3.4. Ablation Study on Decoupling Components", "content": "To better understand the design choices underpinning Mixture-of-Mamba, we conduct an ablation study on the Chameleon + Speech setting at the 443M scale. We evaluate the impact of decoupling four key components-Win-proj (0), Wx-proj (2), Wdt-proj (3), and Wout-proj (0) individually and in various combinations. This analysis enables us to test both individual and combined contributions to the model's overall performance.\nThe results show that decoupling components individually yields varying degrees of improvement, with performance gains ranging from 0.63% (Wout-proj) to 1.22% (Win-proj). Interestingly, some components (Wx-proj and Wdt-proj) exhibit minimal or even slightly negative impact when decoupled alone. However, decoupling multiple components in combination leads to significantly larger gains. For example, decoupling Win-proj and Wout-proj (0+) achieves a 2.20% improvement, while decoupling three components (0+0+0) further increases the gain to 3.11%.\nMost importantly, decoupling all four components simultaneously (0+0+0+0, Mixture-of-Mamba) achieves the largest improvement, with a performance gain of 3.80% over the Mamba baseline. This result highlights a key observation: the gain from decoupling all components together exceeds the sum of individual gains, demonstrating a synergistic effect. The combination of all decoupled projections enables better parameter allocation across modalities, leading to more efficient and effective learning. In summary, the ablation study confirms that the design of Mixture-of-Mamba is both effective and interdependent. Decoupling all key components simultaneously is important to achieving the observed substantial performance gains."}, {"title": "4. Related Work", "content": "4.1. State-Space Models and Multi-Modal Extensions\nState-space models (SSMs) (Gu et al., 2021; Gu & Dao, 2023) have recently gained traction as computationally efficient alternatives to Transformers for sequential modeling. Mamba (Gu & Dao, 2023), in particular, demonstrates strong performance on single-modality tasks by leveraging linear time complexity and advanced gating mechanisms. Extending Mamba to multi-modal tasks remains an active research area.\nIn vision-language modeling, VLMamba (Qiao et al., 2024) and Cobra (Zhao et al., 2024) augment Mamba by incorporating LLaVA-style projection modules, enabling image features to be mapped into the token space of the Mamba model for sequence modeling. In the vision domain, Vision Mamba (Zhu et al., 2024) introduces bidirectional scanning by chaining forward and backward SSM blocks, while VMamba (Liu et al., 2024c) further enhances image patch processing with a 2D Selective Scan (SS2D) module that traverses patches across multiple scanning paths.\nFor diffusion-based models, works such as DiffuSSM (Yan et al., 2024) and Zigma (Hu et al., 2024) replace attention mechanisms with SSMs for image and video generation. Zigma introduces a zigzag scanning scheme to improve efficiency for sequential diffusion tasks, while other approaches (Mo & Tian, 2024; Fei et al., 2024) explore bidirectional SSM architectures. While these works highlight the flexibility of Mamba in generative tasks, they focus primarily on architectural modifications for specific domains rather than general multi-modal pretraining."}, {"title": "4.2. Sparse Architectures for Multi-Modal Pretraining", "content": "Model sparsity, particularly Mixture-of-Experts (MoE), has been extensively explored in Transformers to reduce computational cost (Jacobs et al., 1991; Eigen et al., 2013; Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Jiang et al., 2024). MoE selectively activates subsets of parameters for each input token, allowing the model to specialize in different aspects of the data. However, challenges such as expert imbalance, bi-level optimization, and load balancing remain prevalent (Shazeer et al., 2017; Lepikhin et al., 2020; Tu et al., 2022).\nIn multi-modal tasks, modality-aware sparsity has emerged as an effective strategy. Works such as VLMo (Shen et al., 2023b), MoMA (Lin et al., 2024), and related approaches (Wang et al., 2022; Shen et al., 2022; Bao et al., 2022a; Long et al., 2023; Shen et al., 2025) assign modality-specific experts to handle the unique statistical properties of text, images, and other data types. This improves specialization while avoiding the complexities of learned routing mechanisms (Liang et al., 2022).\nTransformer-based architectures have further extended sparsity into attention mechanisms (Wang et al., 2023; Shen et al., 2024c;d; Liu et al., 2024b; Shen et al., 2024a). CogVLM (Wang et al., 2023) applies sparse techniques on top of a pre-trained Vicuna-7B model but remains limited to generating text outputs. Concurrently, Playground v3 (PGv3) (Liu et al., 2024b) integrates DiT-style image transformers with a frozen LLaMA-3 backbone to achieve state-of-the-art performance in text-to-image generation.\nOur work differs fundamentally in two key aspects. First, Mixture-of-Mamba introduces modality-aware sparsity into the Mamba block itself, generalizing sparse architectures beyond Transformers to SSMs. Unlike prior works that sparsify only the MLP or attention components, we decouple projection components of the Mamba block, enabling efficient and specialized computations across modalities. Second, Mixture-of-Mamba is trained from scratch for multi-modal generation tasks, unlike approaches like CogVLM and PGv3 that fine-tune pre-trained backbones.\nFurthermore, our design is complementary to existing MoE techniques. Prior work (Liang et al., 2024) has demonstrated that MoE-based sparsification can be combined with sparse architectures like Mixture-of-Transformers to achieve additional gains. Similarly, Mixture-of-Mamba can serve as a versatile and computationally efficient solution, offering new pathways for scalable multi-modal pretraining."}, {"title": "5. Conclusion", "content": "In this work, we introduced Mixture-of-Mamba, a novel extension of state-space models (SSMs) that incorporates modality-aware sparsity through modality-specific parameterization. By enabling modality-specific specialization while preserving the computational efficiency of SSMs, Mixture-of-Mamba consistently outperforms dense baselines across three multi-modal settings: Transfusion (interleaved text and continuous image tokens), Chameleon (interleaved text and discrete image tokens), and an extended Chameleon+Speech framework. Our results demonstrate substantial improvements in loss reduction, with training efficiency gains reaching more than double the computational efficiency compared to dense SSMs. Ablation studies further reveal a synergistic effect from jointly decoupling key projection components, highlighting the effectiveness of modality-aware sparsity. These findings establish Mixture-of-Mamba as a scalable and efficient architecture for multi-modal pretraining, paving the way for future exploration in dynamic sparsity and broader multi-modal applications."}, {"title": "Impact Statement", "content": "This work introduces efficiency improvements in multi-modal machine learning systems through modality-aware sparsity techniques. The primary impact is computational efficiency - Mixture-of-Mamba reduces computational costs by up to 65% while maintaining or improving performance. This has positive environmental implications through reduced energy consumption and democratizes access to multi-modal AI systems by lowering computational resource requirements. While these advances could enable beneficial applications in education, accessibility, and human-computer interaction, we acknowledge they could also facilitate potentially concerning applications. We encourage the research community to consider appropriate guidelines for responsible deployment of such technologies."}]}