{"title": "Enhanced Event-Based Video Reconstruction with Motion Compensation", "authors": ["Siying Liu", "Pier Luigi Dragotti"], "abstract": "Deep neural networks for event-based video reconstruction often suffer from a lack of interpretability and have high memory demands. A lightweight network called CISTA-LSTC has recently been introduced showing that high-quality reconstruction can be achieved through the systematic design of its architecture. However, its modelling assumption that input signals and output reconstructed frame share the same sparse representation neglects the displacement caused by motion. To address this, we propose warping the input intensity frames and sparse codes to enhance reconstruction quality. A CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC for motion compensation. The system relies solely on events, in which predicted flow aids in reconstruction and then reconstructed frames are used to facilitate flow estimation. We also introduce an iterative training framework for this combined system. Results demonstrate that our approach achieves state-of-the-art reconstruction accuracy and simultaneously provides reliable dense flow estimation. Furthermore, our model exhibits flexibility in that it can integrate different flow networks, suggesting its potential for further performance enhancement.", "sections": [{"title": "1 Introduction", "content": "Event cameras, closely resembling human eyes and offering enhanced energy efficiency, have drawn significant interest in the field of computer vision [10,34]. These cameras sense changes in brightness and asynchronously output pixel-wise event streams. This working principle enables the generation of event data characterized by low latency, high temporal resolution and high dynamic range. Due to the unique way in which information is encoded, events-to-video (E2V) reconstruction becomes a fundamental research topic.\nMany studies have explored various approaches to reconstruct intensity videos with high frame rates, high dynamic range, and reduced motion blur. Traditional model-based methods [2, 3, 26, 27] leverage the relationship between events and intensity as priors in their models, but their performance is limited"}, {"title": "2 Methodology", "content": "Consider an event stream {ei, i = 1,2,\u2026\u2026, N}, occurring over a duration of T seconds. Each event ei can be mathematically represented as a function\ns(x,y,t) = p_i\\delta(x - x_i, y - y_i, t - t_i). Here, \\delta(\u00b7) represents the Dirac function, (xi, Yi) denotes the spatial coordinates of the event, ti is its timestamp, and pi = \u00b11 indicates the polarity of its brightness change. In E2V reconstruction, aggregating a group of events is essential to gather sufficient information for each reconstruction. Typically, a 3D event voxel grid E \u2208 \\mathbb{R}^{B\u00d7H\u00d7W} [42] is created by accumulating event polarities into B temporal bins, where H and W represent the height and width of the image, respectively. Each polarity is distributed linearly into the two closest temporal voxels.\nIn this study, our objective is to reconstruct the frame \u00cet \u2208 \\mathbb{R}^{H\u00d7W} at time instance t by utilizing the previously reconstructed frame \u00cet-1 \u2208 \\mathbb{R}^{H\u00d7W} and the corresponding event voxel grid E_{t-1} \u2208 \\mathbb{R}^{B\u00d7H\u00d7W}. In Sec. 2.1, we revisit CISTA-LSTC network for E2V reconstruction and analyse its limitations. We then introduce a CISTA-Flow network in Sec. 2.2, which integrates the CISTA-LSTC network with a flow estimation network to address motion compensation for video reconstruction. Finally, Sec. 2.3 presents an iterative training framework for the hybrid system."}, {"title": "2.1 Overview of CISTA-LSTC Network", "content": "Overview of CISTA-LSTC The CISTA-LSTC network, as shown in Fig. 1(a), is designed based on sparse representation paradigm [1,7]. Input events Et_1 and intensity frame \u00cet\u22121 are fused as a stack of feature maps Xt. It is assumed that the input Xt shares the same sparse representation with the output frame \u00cet, i.e. X_t = D_xZ^*, \u00ce_t = D_\\hat{I}Z^*, where Dx, D\u012b are corresponding dictionaries and Z* represents sparse feature maps which are in common. The sparse coding problem can be solved by the iterative shrinkage thresholding algorithm (ISTA) [5]. The CISTA network is then developed using algorithm unfolding strategy [13,20] by unfolding the iteration in the ISTA. In the CISTA-LSTC network, the input Xt is processed by a long short-term temporal consistency (LSTC) unit to initialise sparse codes Z0. Following that, the K unfolded ISTA blocks are employed to optimise sparse codes Zk at each iteration. Finally, a long short-term"}, {"title": "Motion compensation for input frame and sparse codes", "content": "The assumption of shared sparse codes between the input Xt and the output \u00cet holds because their structures are similar. However, there exists a motion displacement between the previously reconstructed image \u00cet\u22121 and the output image \u00cet. Furthermore, the similarity between the input Xt and sparse codes ZK\u22121 t are utilized to pre- serve temporal consistency in sparse codes. This consistency is also affected by the spatial displacement introduced by motion. Although the additional informa- tion from events is expected to compensate for the displacement, we hypothesize that compensating for the motion from instant t-1 to t could enhance perfor- mance. If the motion is known, warping input frame \u00cet\u22121 and the previous sparse codes ZK\u22121 t to serve as the inputs may lead to improved reconstruction accuracy. Specifically, the inputs \u00cet\u22121 and ZK\u22121 t are warped using the known optical flow Ft-1\u2192t, as described by the following equations:\n\n\u00cet-1 = Wf(\u00ce_{t-1}, F_{t-1\u2192t}),\nZ_{t}^{K-1} = W_F(Z_{t-1}^{K-1}, F_{t-1\u2192t}),\n\nwhere the Wf (I, F) denotes the forward warping function to warp frame I using flow F\u2208 \\mathbb{R}^{2\u00d7H\u00d7W}. Each element in the flow map represents vertical and"}, {"title": "2.2 Events-to-Video Reconstruction with Motion Compensation", "content": "In this section, we introduce a CISTA-Flow network designed to enhance E2V reconstruction with motion compensation. This network integrates both a flow estimation network and a CISTA-LSTC network for E2V reconstruction. The overall recursive architecture is shown in Fig. 1(b). The flow network takes the event voxel grid Et_1 and the reconstructed frame \u00cet\u22121 as inputs to estimate the forward optical flow Ft-1\u2192t from instant t \u2212 1 to t. Subsequently, this estimated flow is utilized to warp both \u00cet\u22121 and ZK\u22121 t as the inputs for the CISTA-LSTC network. Finally, the reconstructed frame \u00cet is obtained. The initial input frame Io is initialised with 0. Through the iterative prediction, the CISTA-Flow net- work demonstrates the ability to accurately predict flow and reconstruct high- quality intensity frames using events only. The two-stage estimation process is as follows.\nFlow estimation using reconstructed images and events Various archi- tectures of the flow network can be employed, and we typically utilize the DCEI- Flow network [37] to estimate reliable dense flow. The DCEIFlow network, based on a RAFT architecture [35] initially developed for two-frame-based flow estima- tion, utilizes a single intensity frame and event streams as inputs. It employs the fused feature embedding as pseudo feature maps for I2, along with the feature maps for I1, to generate continuous and dense flow via their correlation volume and an iterative updater. In our system, ground truth (GT) intensity image is"}, {"title": "Video reconstruction using warped frames and sparse codes", "content": "With the predicted flow Ft-1\u2192t, we can warp the reconstructed image \u00cet-1 and sparse codes ZK\u22121 t to serve as inputs along with Et-1 for the subsequent reconstruction. This process can be expressed as follows:\n\n\u00ce^{w}_{t-1} = W_f(\u00ce_{t-1}, F_{t-1\u2192t}),\nZ_{t}^{K-1} = W_F(Z_{t-1}^{K-1}, F_{t-1\u2192t}),\n[\u00ce_t, Z_t^{K}, a_t, c_t] = CISTA-LSTC(E_{t-1}, \u00ce_{t-1}^w, Z_{t}^{K-1^w}, a_{t-1}, c_{t-1}),"}, {"title": "2.3 Iterative Training Framework", "content": "Simultaneously training flow and reconstruction networks poses challenges as their accuracy is interdependent. The quality of flow affects reconstruction and vice versa. To address this problem, we propose an iterative training framework based on pretrained CISTA-LSTC and DCEIFlow networks.\nThe training process can be divided into three stages. First, as shown in Fig. 3(a)(b), DCEIFlow and CISTA-LSTC undergo independent training for 50 and 60 epochs respectively. Specifically, the original DCEIFlow with the GT intensity frame as input is trained based on the pretrained model provided by authors using our dataset, denoted as DCEIFlow (GT I). The CISTA-LSTC network is trained from scratch using warped inputs generated by GT flow, referred to as CISTA (GT Flow).\nSubsequently, we conduct additional training for 20 epochs on the DCEIFlow to make it adaptable to the reconstructed frames, denoted as DCEIFlow (Rec I), as illustrated in Fig. 3(c). At this stage, the reconstructed frame is obtained from the pretrained CISTA (GT Flow) model.\nFinally, after separate training of the reconstruction and flow networks, we iteratively fine-tune both networks to obtain the CISTA-Flow network depicted"}, {"title": "Reconstruction loss", "content": "The loss function for the tth reconstructed frame is a combination of l1, structural similarity (SSIM) and perceptual loss (LPIPS) [40], as follows:\n\nL'_{rec} = ||\u00ce_t - I_t||_1 + (1 \u2013 SSIM(\u00ce_t, I_t)) + LPIPS(\u00ce_t, I_t),\n\nwhere || \u00b7 ||1 represent the l1 norm and the VGG [30] is used in the LPIPS loss. In addition, a temporal consistency loss [25] is introduced to preserve the temporal consistency of intensity using flow, which is given by:\n\nL^{tc}_{t} = ||M_t (W_f (\u00ce_{t-1}, F_{t-1\u2192t}) \u2013 \u00ce_t) ||_1\n\nAt the end of each sequence, the reconstruction loss LR, which aggregates Lrect and Ltec across the entire sequence, is given by:\n\nL_R = \\sum_{t=1}^{L_0}\\frac{1}{L} L^{rec}_{t} + \\sum_{t=L_0}^{L} \\frac{\\lambda_{tc}}{L} L^{tc}_{t},\n\nwhere \u03bbtc = 5, and Lo = 3 to skip the first few frames to allow convergence."}, {"title": "Flow loss", "content": "For the flow estimation, DCEIFlow has R iterations. We calculate the loss for the flow map at each iteration, utilizing a weighted sum of the L1 loss between the estimated flow and the GT flow map, as follows:\n\nL^{f}_t = \\sum_{i=1}^{R} w_i ||M_t (F^i_{t-1\u2192t} - \\hat{F}^i_{t-1\u2192t}) ||_1,\n\nwhere Fti\u22121\u2192t represents the bilinear downsampled flow map for the ith iteration, and Mt is downsampled accordingly. Moreover, the weight for the ith iteration is wi = \u03c6R\u2212i\u22121, where \u03c6 = 0.8 [37].\nFurthermore, a photometric loss for warped GT frames is included:\n\nL_{photo} = \\sum_{i=1}^{R} w_i ||W_f (I_{t-1}, F_{t-1\u2192t}) \u2013 I_t ||_1."}, {"title": "Finally, the flow loss", "content": "Finally, the flow loss LF is calculated over the whole sequence and we set \u03bbp = 1:\n\nL_F = \\sum_{t=1}^{L}(L^{f}_t + \\lambda_p L_{photo})."}, {"title": "3 Numerical Results", "content": ""}, {"title": "3.1 Experimental Settings and Training Details", "content": "Datasets For the training dataset, we utilize the v2e simulator [14] to gen- erate events and corresponding flow from intensity frames. Using images from MS-COCO datasets [17], we simulate videos with various motions and 2D affine transform at adaptive frame rate. It is important to note that the GT flow is ac- quired using the FlowNet from the pretrained Super-Slomo model in v2e, which estimates flow based on two intensity frames. Although exact motion parame- ters are known during data generation, the real flow introduces more occlusions in warped frames. The estimated flow can reduce intensity error between the warped frame IW and the target frame It. Therefore, we employ the estimated flow as GT flow during training, acknowledging that potential inaccuracies in flow estimation. The image size is set to 240 \u00d7 180. The dataset contains 1538 training sequences and 180 testing sequences. Each sequence has length L = 15, with approximately NE = 15, 000 events between consecutive frames. For further details, please refer to the supplementary material.\nIn addition to the simulated dataset, we evaluate reconstruction performance using real datasets, including the Event Camera Dataset (ECD) [21] and the High-Quality Frames (HQF) [32]. These were captured by DAVIS240C cameras (240 x 180). The ECD comprises 7 sequences, while the HQF contains 14 se- quences with a wider range of motions and scene types. HQF generally presents higher-quality intensity images with reduced motion blur compared to ECD.\nWe also evaluated our model on challenging datasets such as the MVSEC [43] and the HS-ERGB [36] datasets. The MVSEC dataset contains stereo event data captured from various vehicles, using a pair of DAVIS 346B cameras (346 \u00d7 260). We utilized data from the left camera only. The sequences include indoor and outdoor scenarios, as well as low-light conditions like night driving. The HS-ERGB dataset comprises sequences with high resolution and rapid motion, captured by the Prophesee Gen4 (1280 \u00d7 720). Due to the absence of reliable HDR reference images in these datasets, we only present qualitative results.\nTraining details The Adam optimizer and a batch size of 2 are used during training. For separate training of CISTA (GT Flow) and DCEIFlow (GT I), and also other reconstruction networks, the learning rate starts at 0.0001, decaying by 10% every 10 epochs. For DCEIFlow (Rec I), a constant learning rate of 0.0001 is applied. During the iterative training of the two components, a learning rate of 3e-5 is maintained."}, {"title": "Evaluation", "content": "We assess our model using the EVREAL [8] evaluation framework for E2V reconstruction, which incorporates popular datasets and mainstream networks, allowing for consistent evaluation across different methods. We adapt it to suit our settings. During inference, the constraint on the number of events Ne remains consistent with the training phase. However, because GT intensity images are only available at a fixed frame rate, we either divide events between two consecutive frames into multiple groups or combine events across several frames into a single group for the reconstruction and flow estimation.\nFor reconstruction evaluation, we utilize mean square error (MSE), SSIM and LPIPS computed using AlexNet [15]. For the ECD dataset, considering the reference frames are relatively dark, we normalize the images into the range of [0, 1] to enable comparison with the reconstructed images. No post-processing is applied to the reconstructed frames.\nFor optical flow estimation, we use the average endpoint error (EPE) to evaluate the error between predicted and GT flow vector for simulated data sequences. EPE is defined as the Euclidean distance between the GT flow vector and predicted flow vector. We also define the percentage of outliers when the EPE is greater than 3 pixels and 5% of the magnitude of the flow vector. We only evaluate the flow estimation on our simulated datasets for reference. Please refer to the supplementary material for more results."}, {"title": "3.2 Results of Video Reconstruction", "content": "Comparison against other reconstruction networks We compare the re- construction performance of the CISTA-Flow network with other advanced re- construction networks, including Unet-based networks such as E2VID [24, 25], FireNet [28], SPADE-E2VID [4], and HyperE2VID [9], as well as the transformer- based ET-Net [39] and the original CISTA-LSTC network [19]. We also assess enhanced versions, E2VID+ and FireNet+, provided by [32]. SSL-E2VID [22] is another version of E2VID. We retrain E2VID, FireNet, and SPADE-E2VID using our training data for the same epochs and also evaluate the pretrained models provided by the authors.\nIn Tab. 1, the average comparison results on the ECD, HQF, and the sim- ulated dataset (SIM) are presented. It is evident that CISTA-Flow performs the best or the second best on most metrics, particularly on SSIM. Comparing CISTA-Flow with the original CISTA-LSTC, we observe a significant improve- ment in accuracy, indicating that the reconstructed image structure is closer to the ground truth when utilizing predicted flow.\nFig. 4 show some examples of reconstructed images from different networks and predicted flow from CISTA-Flow. The images produced by CISTA-Flow are sharper, with some details recovered thanks to motion compensation. ETNet and HyperE2VID effectively restores fine details and mitigates motion blur in many instances, yet the resulting reconstructed frames often appear underexposed and contain artifacts. For example, in scenes like poster_text1 and poster_text2, where texts are difficult to identify or ghosting occurs using other methods, our CISTA-Flow network enhances text visibility and reduces motion blur. Scenes"}, {"title": "Multi-objects scenes", "content": "Apart from Fig. 4, we further present reconstruction results and related flow maps for scenes with multiple objects in Fig. 5. Different object positions result in diverse depths, leading to varying motion patterns. Flow maps demonstrate the capability of CISTA-Flow to reliably estimate dense flow in most scenarios. Comparing CISTA-Flow to the original CISTA-LSTC, accurate flow estimation leads to sharper edges and finer details in the resulting reconstructed frames. The green boxes highlight these enhancements.\nHowever, the accuracy of both reconstruction and flow estimation is interde- pendent. When reconstruction fails due to a lack of events or excessive noise, it affects the accuracy of optical flow, consequently impacting subsequent frames. In Fig. 5, areas with inaccuracies in CISTA-Flow reconstruction are highlighted by red boxes. In the desk scene, where the bottle is transparent and events are sparse around the edges, the estimated flow is inaccurate around this area and therefore affect reconstruction. In addition, the bleeding effect tends to last longer in CISTA-Flow. In scenes like slider_depth and desk_fast, the estimated flow around the black area is close to 0, resulting in a longer smearing effect. Similar issues are observed in hand_only, where inaccurate optical flow for the hand area affects reconstruction."}, {"title": "Fast motion and low light condition", "content": "To assess the robustness of our method, we tested CISTA-Flow on datasets with high resolution or challenging scenarios, such as scenes with fast motion or low light conditions. The reconstructed images are presented in Fig. 6. The comparison between CISTA-LSTC and CISTA-Flow images reveals motion compensation in the latter, despite some inaccuracies and inconsistencies in flow prediction."}, {"title": "3.3 Ablation Study of Different Warped Inputs", "content": "According to the analysis in Sec. 2.1, we assume warping the previously recon- structed frame \u00cet\u22121 and sparse codes ZK\u22121 t can enhance reconstruction quality. To validate this, we compare the effects of warping different inputs. First, we train CISTA-LSTC with different inputs warped using GT flow, and then further obtain the CISTA-Flow networks with predicted flow based on these CISTA (GT Flow) networks. As shown in Tab. 2, \"No warp\" is the standard CISTA-LSTC without warping. \"Warp I\" applies warping to the input frame, \"warp I+Z\" to both the input frame and sparse codes, and \"warp all\" extends warping to all inputs, including recurrent states at-1 and ct-1.\nFrom Tab. 2, it is evident that models with warped inputs outperform the plain CISTA-LSTC, with \"warp I+Z\" showing the best performance. For CISTA (GT Flow), warping only the intensity frame offers marginal improvements in reconstruction quality compared to \"warp I+Z\". This demonstrates the impor- tance of warping sparse codes to enhance temporal consistency. However, for CISTA-Flow, the efficacy of warping I and Z is notably diminished due to the reduced quality of estimated flow, nearly reaching the level of \"warp I\". Addi- tionally, further warping of the states is not beneficial and may even degrade reconstruction quality, as observed in \"warp all\" in Tab. 2. Since the states pre- serve the memory from previous reconstructions, and the gates regulate the for- getting rate, warping them is unnecessary. For the flow estimation, CISTA-Flow with \"warp I+Z\" performs the best, exhibiting lower EPE and fewer outliers."}, {"title": "3.4 Incorporating Different Flow Networks", "content": "The CISTA-Flow framework supports integration with different event-based flow networks. To assess the efficacy of alternative flow networks, we incorporated"}, {"title": "4 Conclusion", "content": "In this study, we introduced a model-based event reconstruction network that incorporates motion compensation. The original CISTA-LSTC model, based on the assumption of shared sparse representation between input and output signals, is effective but overlooked motion displacement. To address this, we construct a CISTA-Flow network by incorporating a flow network, specifically DCEIFlow. In this system, the reconstructed frame and events are used for flow estimation, and then the estimated flow is used to warp intensity frame and sparse codes for subsequent reconstruction for better temporal consistency. An iterative training framework is also introduced for the integrated system.\nResults show that our CISTA-Flow network achieves state-of-the-art recon- struction quality over other advanced networks, and produces reliable dense flow. With the assistance of motion compensation, the reconstructed frames exhibit sharper edges and finer details. However, its limitation lies in the interdependent quality of reconstruction and flow estimation, leading to extended impacts from inaccurate predictions. This issue can be alleviated by integrating event-only flow networks like ERAFT. The adaptability of CISTA-Flow to different flow networks demonstrate its potential for further improvements."}]}