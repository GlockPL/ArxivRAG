{"title": "Large-image Object Detection for Fine-grained Recognition of Punches Patterns in Medieval Panel Painting", "authors": ["Josh Bruegger", "Diana Ioana C\u0103tan\u0103", "Vanja Macovaz", "Matias Valdenegro-Toro", "Matthia Sabatelli", "Marco Zullich"], "abstract": "The attribution of the author of an art piece is typically a laborious manual process, usually relying on subjective evaluations of expert figures. However, there are some situations in which quantitative features of the artwork can support these evaluations. The extraction of these features can sometimes be automated, for instance, with the use of Machine Learning (ML) techniques. An example of these features is represented by repeated, mechanically impressed patterns, called punches, present chiefly in 13th and 14th-century panel paintings from Tuscany. Previous research in art history showcased a strong connection between the shapes of punches and specific artists or workshops, suggesting the possibility of using these quantitative cues to support the attribution. In the present work, we first collect a dataset of large-scale images of these panel paintings. Then, using YOLOv10, a recent and popular object detection model, we train a ML pipeline to perform object detection on the punches contained in the images. Due to the large size of the images, the detection procedure is split across multiple frames by adopting a sliding-window approach with overlaps, after which the predictions are combined for the whole image using a custom non-maximal suppression routine. Our results indicate how art historians working in the field can reliably use our method for the identification and extraction of punches.", "sections": [{"title": "1 Introduction", "content": "The process of attributing the author or authors of a work of art is topical within art history. It is usually conducted by means of meticulous qualitative investigations aimed at assessing aspects such as style, perceptive visual features, and"}, {"title": "1.1 Related Work", "content": "Object Detection OD is one of the fundamental tasks of computer vision. It consists of recognizing and localizing instances of known objects in images. The literature distinguishes between two main categories of OD methods: (a) two-shot methods, which first identify image patches containing known objects, then perform classification on the patches and (b) one-shot methods, which jointly perform localization and recognition at the same time. Famous two-shot methods include the Region-based CNN methods [10,24], while notable one-shot methods include the CNN-based YOLO [23] and its subsequent variants RetinaNet [26] and the attention-based DETR [4]. A classical paradigm was seeing one-shot methods as faster but more inaccurate and two-shot methods as slower but more accurate [1,5]. However, recent advances caused the accuracy gap between the two to close, while one-shot methods still prove to be more efficient [25]. The adoption of YOLOv10 [36], a one-stage object detector based on YOLO, is advantageous considering the good trade-off between accuracy and time efficiency in a situation like ours, whereas the OD model has to be run on multiple frames of very large images.\nMachine Learning for analyzing artworks ML has been applied to analyze artworks since the late 1990s, with works from Hachimura [12] and Corridoni et al. [7]. They used classical computer vision techniques to extract features useful for information retrieval systems. For what concerns ML-assisted artwork attribution, Kr\u00f6ner and Lattner [14], and Melzer et al. [18] concentrating on the topic of authorship attribution. These initial attempts were making use of basic feature engineering and shallow feed-forward neural networks. Later approaches include a mixture of unsupervised and supervised approaches\u2014such as Hidden Markov Models, Support Vector Machines, and Clustering\u2014for artist classification [13], and image descriptors to establish stylistic similarities [31]. The last decade has seen an increase in the usage of DL applied to art: David and Netanyahu [8] used features derived from a deep autoencoder to build an ML pipeline for author classification, while [6] solved the same task using feature extracted from a pre-trained Convolutional Neural Network (CNN). Other works, such as the one by [2], use DL for style classification. Other approaches targeting artwork classification are reviewed by Santos et al. in their survey [29].\nMore related to the present work are approaches aimed at identifying specific instances of known objects in paintings. Despite the appeal that an end-to-end automated author classification may pose, the opacity in the decision rules operated by the model may represent a hurdle for explaining a prediction to an expert in the field, such as an art historian. Models that instead target the presence"}, {"title": "2 Materials and Methods", "content": "The dataset used in the present work is composed of 8 high-resolution pictures of panel paintings from Museo Nazionale in Pisa (Italy). These artworks are listed in Table 1 and depicted in Figure 1. As the goal was to create a dataset with a heterogeneous set of punchmarks, but with certain instances of punches appearing in multiple art pieces, we operated the selection of paintings in collaboration with experts in the field. We conducted the process of collecting and digitizing the paintings following the procedure indicated by Zullich et al. [39], thus allowing us to get high-quality pictures where (a) the punchmarks are clearly visible in a good enough detail, and (b) the size of the punchmarks is approximately the same for each instance, thus relieving the object detector of the task"}, {"title": "2.2 Object Detection with YOLOv10", "content": "As introduced in section 1, OD operates a recognition of the single instances of objects of known categories within images. YOLO [23] is a one-shot object detector, i.e., it simultaneously predicts object categories and their location within an image. It conceptually divides an input image into a S \u00d7 S grid and outputs a fixed number of candidate predictions for each of the elements in the grid. The candidates are produced even in areas of the model where there may not be any instance of known objects. Each prediction contains information about the coordinates of the bouding boxes, a confidence score encoding the likelihood that"}, {"title": "Custom NMS YOLO", "content": "NMS algorithm works by identifying potentially duplicated predictions within the same area using Intersection-over-Union (IoU) and then removing the least confident predictions within this area. Given two bounding boxes B1, B2, IoU is defined as $(B_1 \\cap B_2)/(B_1 \\cup B_2)$.\nIn our case, after merging the predictions for multiple images, we are left with many cases of nested predictions, as exemplified in Figure 7. Applying YOLO NMS may cause a large number of small, high-confidence predictions to coalesce over larger ones. This would lead to a lower localization accuracy of the model due to a low IoU between ground truth boxes and predictions.\nWe modify the NMS algorithm first by adding a phase in which we remove predictions with confidence lower than a threshold c*, then by replacing IoU with the Intersection-over-Minimum (IoM) [35]:\n$IOM(B_1, B_2) = \\frac{B_1 \\cap B_2}{min\\{B_1, B_2\\}}$\nWe then calculate the pairwise IoM between the boxes. We coalesce same-class predictions with IoM above a certain threshold by removing all predictions"}, {"title": "2.3 Evaluation metrics", "content": "In order to assess the task-level performance of our model, we make use of the following popular metrics for OD tasks:\nPrecision is computed as the ratio between True Positives (TPs) and all of the model predictions. In OD, a TP is defined as a prediction whose bounding box intersects a corresponding ground truth bounding box belonging to the same category. We consider the two boxes to match when IoU is larger than a threshold \u03c4\u2208 [0,1]. We report Precision with \u03c4 = 0.5 (P\u00a9.5). Precision highlights the model's capability of correctly classifying punches, but it ignores False Negatives-i.e., ground truth bounding boxes with no matching prediction.\nRecall is calculated as the ratio between TP and all false negatives\u2014i.e., ground truth objects with no matching prediction. We report Recall at the IoU threshold \u03c4 of 0.5 (R0.5). It highlights the model's capability of exhaustively identifying punches within an image, but it ignores incorrect classifications. Precision and Recall can be combined via harmonic mean to provide the F1-Score. This metric considers both the capability of the model to output correct predictions and reduce false negatives. We report F1-Score at the IoU threshold of 0.5 F10.5. For validation purposes only, we consider the mean Average Precision (mAP) metric, that summarizes the ability of the model output correct predictions at different confidence levels and IoU thresholds. We do not make use of mAP in the test-set evaluation since the NMS procedure already removes predictions below a confidence threshold, thus rendering useless the necessity for calculating AP at different confidence levels. In addition, metrics such as Precision, Recall, and F1-Score are easier to communicate to model stakeholders (e.g., art historians) who may not be expert in machine learning or statistics."}, {"title": "2.4 Experimental settings", "content": "For training YOLOv10 on our dataset of punches, we make use of the following three variants: YOLOv10n, YOLOv10s, and YOLOv101. The only difference between these three models is the number of parameters, which we present in Table 2. As is common practice when dealing with datasets that are far in terms of"}, {"title": "3 Results", "content": "The results attained by the models on the validation split are presented in Table 3, while Table 4 showcases a per-class overview of the results on the held-out picture. The model YOLOv10n is the one with the best results in terms of F1-Score, which hints at the possibility that the task may not need an extreme level of overparameterization to be tackled. While the Precision of all three models is around 94%, YOLOv10n has a much better Recall (89.81%, compared to 86.47% of YOLOv10s and 78.49% of YOLOv101), showcasing how larger models struggle more with false negatives. It needs to be noticed that the high Precision translates also to having very few instances of predictions whose category is not present in the specific picture (2 predictions out of 1690 for YOLOv10n, 1 out of 1625 for YOLOv10s, and none for YOLOv101). This is important from an art historian perspective, since he/she prefers a lower Recall to having the predictions polluted with false positives, which may point to different punches being used in the painting, and hence hint at different authors than the expectation. Additionally, we can see how the custom NMS procedure boosts the Precision and F1-Score of YOLOv10n and YOLOv10s at the expense of Recall (albeit at a smaller magnitude than Precision). This behavior does not instead occur in YOLOv10l, which apparently struggles more with outputting high-confidence accurate predictions. The Precision boost observed is expected since the criterion for selecting the best configuration of parameters for NMS is based on mAP: a high confidence threshold will increase true positives, but introduce more false negatives, hence the behavior observed in the table. The drop in Recall is particularly noticeable for punch #333, whose Recall in YOLOv10s reached a low of 48.78% and even as low as 15.45% in YOLOv10l, meaning that the model missed the majority of its instances in the picture."}, {"title": "4 Discussion and Conclusions", "content": "In the current work, we presented a YOLOv10-based pipeline for operating predictions on punchmarks in images of panel paintings in the Florence area in the late Middle Ages. We first obtained, following a thorough photographic setting, a dataset of a few large-scale images of 8 paintings, which we proceeded to manually label, identifying 3745 occurrences of punchmarks across 27 categories. We then extracted frames from these pictures by means of random subwindows of size 1088 \u00d7 1088. Due to the very large class imbalance, we rebalanced by subsampling majority classes. Finally, we split the frames into training and validation, carefully avoiding leakage. We then proceeded to train three variants of YOLOv10. In order to combine the predictions operated on small frames onto the bigger pictures in our dataset, we resorted to a sliding window approach, overlapping each window to avoid splitting punches between different windows. When combining the predictions, we needed to take into consideration possible multiple duplicate high-confidence predictions coming from different windows. We tackled this issue with a custom non-maximal suppression strategy making use of the Intersection-over-Minimum metric. We showed, on a large-scale image held out in our dataset, how YOLOv10 is capable of producing highly precise predictions. In addition, we showed how our custom NMS strategy is capable of increasing the accuracy of the predictions output by two out of three of our YOLOv10 models in terms of Precision and F1 score, limiting the decrease in Recall.\nDespite these results, our study still has various limitations. First of all, we must notice our dataset is still composed of a low number of paintings, which hinders especially the evaluation phase. A test dataset including more picture and covering more punch classes especially those underrepresented in the training dataset-would provide with the possibility of evaluating the models outside of the single 4 punches categories from Table 4. However, given the expensive labor of the pictures shooting and the manual labeling procedures, this is an arduous task to carry out. An additional goal could be to use data (both for training and evaluation) coming from badly preserved paintings: this would allow test more difficult cases, and would also support the evaluation of the model on out-of-distribution data, similarly to what done previously by Zullich et al. [39]. Finally, we made use of some YOLOv10 variants: despite their good trade-off between detection speed and accuracy, two-stage models could provide more"}]}