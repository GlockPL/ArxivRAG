{"title": "A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method for Legal Charge Prediction", "authors": ["Jingyun Sun", "Chi Wei", "Yang Li"], "abstract": "Legal charge prediction, an essential task in legal AI, seeks to assign accurate charge labels to case descriptions, attracting significant recent interest. Existing methods primarily employ diverse neural network structures for modeling case descriptions directly, failing to effectively leverage multi-source external knowledge. We propose a prompt learning framework-based method that simultaneously leverages multi-source heterogeneous external knowledge from a legal knowledge base, a conversational LLM, and related legal articles. Specifically, we match knowledge snippets in case descriptions via the legal knowledge base and encapsulate them into the input through a hard prompt template. Additionally, we retrieve legal articles related to a given case description through contrastive learning, and then obtain factual elements within the case description through a conversational LLM. We fuse the embedding vectors of soft prompt tokens with the encoding vector of factual elements to achieve knowledge-enhanced model forward inference. Experimental results show that our method achieved state-of-the-art results on CAIL-2018, the largest legal charge prediction dataset, and our method has lower data dependency. Case studies also demonstrate our method's strong interpretability.", "sections": [{"title": "1. Introduction", "content": "Legal charge prediction is a crucial task in legal artificial intelligence, aimed at utilizing advanced technologies such as machine learning, deep learning, and natural language processing to analyze given case descriptions and thereby predict corresponding charge labels. Legal charge prediction not only helps legal professionals handle cases efficiently and accurately, reducing human errors and increasing the consistency and fairness of judgments, but also supports the enhancement of legal education and public legal awareness, by spreading legal knowledge and strengthening society's understanding and compliance with legal regulations.\nLegal charge prediction is often regarded as a classification problem, hence researchers typically adopt methods similar to those used for general text classification tasks to address it. For instance, Wang et al. proposed a convolutional neural networks-based approach [1], Yang et al. introduced a method based on bidirectional long short-term memory network [2], and Chen et al. developed a gated reccurent units-based method [3]. However, legal charge prediction differs from general text classification tasks in several ways. Firstly, legal texts, which contain a plethora of legal terminologies and keywords, are distinct from general texts, presenting challenges to universal models in understanding the content. Secondly, legal charge prediction focuses more on the factual information within texts, whereas general text classification tasks are concerned with the topics described by texts. Therefore, many researchers have utilized language models pretrained in the legal domain as the backbone to enhance the model's comprehension of legal texts [4]. Such models are better at capturing the domain-specific terminologies and keywords within legal texts. Nevertheless, the pretrained models they employed only allow for the input of 512 tokens, which is insufficient for modeling legal texts that often exceed this length. Moreover, to capture factual information in legal texts, Sukanya & Priyadarshini et al. proposed a model based on attention mechanism [5], and Wang et al. further introduced a hierarchical attention mechanism to capture factual information at different levels [6]. However, they only utilized the content of legal texts themselves to obtain factual information, without leveraging external structured knowledge.\nIn contrast, the method we propose not only enhances the model's comprehension of the textual content and task objectives but also fully leverages heterogeneous external legal knowledge from multiple sources. Firstly, we employ the newly introduced pre-trained language model, Lawformer, which is trained on a large-scale legal corpus and can accommodate text inputs exceeding 4,000 tokens, as our inference model. Lawformer aids in accurately comprehending the semantics of case descriptions and capturing the meanings expressed by legal terms and keywords. Subsequently, we utilize a legal knowledge base to match knowledge snippets from case descriptions, while employing a conversational LLM and relevant articles to extract factual elements from descriptions. This process introduces external components to assist the model in acquiring legal knowledge, thereby further enhancing the model's understanding of case descriptions. Finally, we propose using soft prompt tokens and hard prompt templates to encapsulate heterogeneous legal knowledge from multiple sources. Overall, the method presented in this paper leverages the paradigm of prompt learning to integrate heterogeneous legal knowledge from multiple sources into the model's forward reasoning, thus improving the model's predictive accuracy regarding legal charges.\nWe conducted extensive experiments on CAIL-2018, the largest legal charge prediction dataset to date [7]. The results demonstrate that our proposed method achieved results surpassing the baselines, with a macro F1 score of 0.84. Moreover, the experiments indicate that our method has the lowest dependency on training data. The performance of other baselines significantly diminishes as the scale of training data decreases, whereas our method still maintains a high F1 score. We also analyzed the contribution of each module within our method through ablation studies. Finally, we validated that our approach possesses strong interpretability, which is crucial for artificial intelligence tasks in the legal domain.\nOur primary contributions can be summarized as follows:\n1) We propose a legal charge prediction model that integrates multi-source heterogeneous legal knowledge.\n2) We introduce a method to encapsulate heterogeneous legal knowledge via the prompt-based learning framework.\n3) We propose the use of a conversational LLM and relevant legal articles to extract factual elements from case descriptions.\n4) We employ a specialized legal knowledge base to match knowledge snippets from case descriptions.\n5) We demonstrate the effectiveness of our method through extensive empirical validation."}, {"title": "2. Related work", "content": "This section presents the works related to our study. Firstly, the latest progress in legal charge prediction is introduced. Subsequently, the basic concepts and applications of prompt learning are introduced."}, {"title": "2.1 Legal charge prediction", "content": "Legal charge prediction is a crucial task in the field of legal artificial intelligence, aimed at predicting the legal charges corresponding to given case descriptions. Due to the scarcity of legal resources, individuals lacking legal knowledge often find it challenging to promptly seek legal advice from attorneys or legal professionals. Therefore, the automation of legal charge prediction can, to a certain extent, alleviate the issue of legal resource scarcity. Furthermore, legal charge prediction can also offer decision support for lawyers or judges, thereby enhancing their work efficiency.\nEarly legal charge predictions primarily relied on rule-based methods or mathematical models [8], [9], [10]. These methods have the advantage of transparent and intuitive reasoning processes, and once the inference rules are triggered, their outcomes are fixed. However, such methods exhibit poor generalization and struggle to effectively address language phenomena such as synonyms and polysemy in case descriptions. With the introduction of the Word2Vec concept by Mikolov [11], subsequent legal charge prediction methods have predominantly been based on semantic embeddings. These methods embed words in case descriptions into semantic vectors, which are then used as features inputted into a machine learning model [4], [12]. An exemplary contribution is Law2Vec proposed by Chalkidis et al [13]. Law2Vec is a specialized word embedding for the legal domain, trained on 123,066 legal documents containing 492M words. Building upon Law2Vec, Chalkidis et al. introduced a logistic regression-based method [14]. This method is simple yet effective, demonstrating an F1 score over 40% higher than rule-based methods on an EU legislative dataset. Additionally, methods combining semantic embeddings with support vector machines and k-nearest neighbors have been proposed [15].\nWith the popularity of deep learning technologies, researchers have shifted from merely combining semantic embeddings with shallow machine learning models to integrating them with neural network models [4], [16], [17], [18]. Compared to shallow machine learning models, neural network models exhibit stronger data fitting and feature learning capabilities, leading to superior performance. For instance, Wang et al. proposed a method based on convolutional neural networks [1]. This method leverages the modeling capability of convolutional networks for local key information, enabling the model to identify crucial phrases, terms, jargon, and keywords in case descriptions, thereby enhancing the performance for legal charge prediction. On the other hand, Yang et al. focused on modeling the global semantic correlations within case descriptions and introduced a method based on long short-term memory networks [2]. To reduce the computational complexity of globally modeling semantic relationships in case descriptions, Chen et al. presented a method based on gated recurrent units [3]. This method employs computationally less intensive gated recurrent units to model the global semantic correlations of case descriptions, significantly reducing computational complexity. Additionally, Sukanya & Priyadarshini proposed a model based on attention, which can attend to salient information in different aspects of a case description [5]. Building upon this, Wang et al. introduced a hierarchical attention model capable of attending to salient information at different levels within a case description [6]. This approach achieved state-of-the-art performance in various benchmarks. Despite the generally excellent results achieved by neural network-based models, they are still constrained by the effectiveness of word embeddings and rely heavily on large amounts of high-quality annotated data.\nIn recent years, an increasing number of methods for predicting legal charges have been based on pre-trained language models [4]. Unlike neural network models that take static semantic embeddings as input, pre-trained language models are pre-trained on large-scale textual data and therefore have better context understanding capabilities [19], [20], [21]. Moreover, pre-trained models can capture relationships and contexts between different words in a case description, thereby better comprehending the textual meaning of legal narratives and contributing to improved accuracy in charge prediction [22]. The core of our method lies in a pre-trained legal language model, ensuring its contextual understanding of legal texts. Diverging from existing methods, we integrate heterogeneous legal knowledge from multiple sources into the reasoning process of the language model, enabling it to acquire a more comprehensive understanding of legal knowledge specific to a given case description. Furthermore, compared to traditional neural network methods, our approach exhibits lower data dependency."}, {"title": "2.2 prompt learning", "content": "Prompt learning has recently garnered significant attention from researchers due to its ability to stimulate language models to better recall the semantic knowledge learned during pre-training [23], [24]. Unlike the standard downstream task fine-tuning paradigm, the prompt learning paradigm aligns downstream tasks with the pre-training tasks of language models. To this end, methods based on prompt learning should first convert different downstream tasks into a language modeling task [25], [26]. For instance, a traditional classification task is designed to fit the probability distribution $Y = (X; \\theta)$. Given a piece of text x = [This pizza is so delicious], the model might output the prediction y = 0 \u2208 {0,1,2} once $\\theta$ is learned. Where 0 denotes a positive sentiment label, 1 denotes a negative sentiment label, and 2 denotes a neutral sentiment label. However, the model aims to fit the function\\_Y = P(MASK = Vy|template(X);$\\theta$) by converting the task into a language modeling task. Here, template(x) is a new text transformed from the original text by inserting specific prompt words and Vy is the set of label words. For example, the original text x = [This pizza is so delicious] could be transformed into template(x)=[This pizza is so delicious. It feels [MASK]], and the model is tasked with predicting the word at the [MASK] position based on $\\theta$, thereby inferring the sentiment label. The model might generate words such as \u201camazing\u201d, \u201cbad\", or \"okay\u201d, which can then be mapped to the specific sentiment labels 0, 1, or 2.\nThe three core components in prompt learning are prompt templates, inference models, and label mappings. A prompt template is employed to encapsulate a original text into a new format featuring prompts and masks, as exemplified in the work of [27], [28], [29]. They integrate external knowledge at the prompt template stage to maximize the language model's understanding of the task. Our method also incorporates external knowledge during the prompt template stage. However, we propose the integration of multi-source heterogeneous knowledge into the prompt template unlike existing methods, thereby significantly enhancing the model's inference capabilities. Inference model is a core component of prompt learning, utilized for predicting the tokens at the mask positions based on the encapsulated text. Commonly used inference models include BERT [30], RoBERTa [31], the GPT series [32], and the T5 series [33]. Furthermore, some studies employ language models specialized in specific domains to cater to particular tasks. For instance, Zhu et al. proposed using CliniBERT [34] for prompt learning methods in the medical field, achieving state-of-the-art results. We utilize Lawformer, a pre-trained language model in the legal domain, as the inference model, making our method more suitable for legal charge prediction tasks."}, {"title": "3. Task formalization", "content": "The task of legal charge prediction aims to accomplish the following process: given a case description X containing L tokens, the model predict a legal charge label \u0177 based on the content of X. This process can be denoted as Equation 1.\n$\\hat{y} = Model(X; \\theta)$ (1)\nWhere $\\theta$ represents the learnable parameters within the model.\nIn this study, we additionally utilize a set A of legal articles, a conversational large language model LLM(\u00b7), and a legal knowledge base K as aids, thereby resulting in the Equation 2.\n$\\hat{y} = Model(X, A, LLM(\\cdot), \\Kappa; \\theta)$ (2)"}, {"title": "4. Methodology", "content": "Figure 2 illustrates our method, comprising four modules. The first module, as depicted in the lower-left corner of the figure, focuses on acquiring and encoding factual elements from the given case description X. In this module, case description X search for the most relevant N legal articles via a joint semantic space, subsequently consulting a conversational Large Language Model (LLM) for factual elements in the case description based on these legal articles. The factual elements aquired are encoded into a semantic vector $\\vec{u}$ by a BiGRU encoder, and $\\vec{u}$ is then injected into the forward computation of the soft prompt tokens to enhance the model's reasoning capabilities for the task. The second module involves knowledge matching based on a legal knowledge base, wherein knowledge from the case description X is matched with a knowledge base K. The knowledge snippets matched are then concatenated and added to the original input as prompt to further strengthen the model's reasoning. The third module is the cornerstone of our method consisting of a legal language model, as illustrated in the central part of the figure. The input to this legal language model includes five components: 1) two soft prompt tokens S\u2081 and S2; two manually constructed template texts T1 and T2; 3) the masked tokens M; 4) the case description X; and 5) the knowledge snipptes K. The objective of the legal language model is to predict the tokens at the masked positions. The fourth module aims to map the predictions of the language model at the masked positions onto a legal charge category \u0177, serving as the final process of our method.\nIn the following sections, we detail each module: Section 4.1 covers the first module, Section 4.2 the second, Section 4.3 the third, and Section 4.4 the fourth."}, {"title": "4.1 Factual elements acquisition and encoding", "content": "This section introduces the first module of our method, which involves acquiring and encoding factual elements from the given case description. Factual elements in case descriptions are crucial for legal judgments, as they influence the overall understanding of the cases and the final verdicts [35]. These elements typically include the time and place of the event, the individuals involved, and the specific process of the event."}, {"title": "4.1.1 Jointly semantic space learning", "content": "We manage to utilize relevant legal articles to extract noteworthy factual elements from case descriptions, as these articles explicitly define which factual elements are pertinent to specific legal charges. For instance, the legal article pertinent to the crime of copyright infringement is: \u201cActs such as copying and distributing literary, audio-visual, and computer software works for profit without the permission of the copyright holder, publishing books that are subject to another's exclusive publishing rights without consent, duplicating audio-visual products created by others without their permission, producing and exhibiting art works falsely attributed to another, where the amount of illegal gains is substantial or other serious circumstances are present\". From this, we can infer that elements such as whether the intent was for profit, whether copyright permission was obtained, and the amount of illegal gains, are factual aspects worthy of attention.\nTo match relevant legal articles with case descriptions, we propose the construction of a joint semantic space of both case descriptions and legal articles. We engage in contrastive training of the language model RoBERTa [31] to facilitate its learning of this joint semantic space. Compared to rule-based methods [36], [37], the language model can model deeper semantic connections between case descriptions and legal articles, thereby achieving superior matching outcomes. Furthermore, contrastive training places greater emphasis on the relative relationship between positive and negative samples compared to traditional neural network-based semantic matching methods [38]. Consequently, contrastive training aids RoBERTa in learning more distinct and discriminative features which are crucial in determining the relevance between case descriptions and legal articles. Next, we introduce the specific steps of using RoBERTa to learn the joint semantic space."}, {"title": "Step 1: Construct positive and negative pairs", "content": "Each case description in CAIL-2018, the largest dataset of legal charge prediction, has been labeled its relevant legal articles. Therefore, we can easily construct contrastive positive and negative pairs from the entire training set automatically. Given the training set containing the pairs of case descriptions and relevant legal articles $D_{Train} = {(X_1, R_1), (X_2, R_2), (X_3, R_3), ... }$, we use Algorithm 1 to automatically construct a set P of positive and negative pairs. Where, $X_i$ represents the $i_{th}$ case description in the training set, and $R_i$ represents the legal articles related to it."}, {"title": "Step 2: Obtain representations of case descriptions and legal articles", "content": "We have obtained set P of positive and negative pairs in the first step. Each pair in the set is either a related pair of case description and legal article or is irrelevant. In this step, we employ RoBERTa to acquire semantic vectors for the case description and legal article in each pair. This process is illustrated in Equation (3).\n$P = RoBERTa(P)$ (3)\nHerein, P represents a matrix, where the odd-numbered columns of P denote the semantic vectors of case descriptions, and the even-numbered columns represent the semantic vectors of legal articles. Now that we have obtained the semantic vectors for all samples in positive and negative pairs, we proceed to train RoBERTa using a contrastive loss."}, {"title": "Step 3: Train the RoBERTa via contrastive loss", "content": "During the training, RoBERTa learns to decrease the semantic distance between samples in positive pairs while increasing the distance between those in negative pairs. This objective is achieved through a contrastive loss function, which quantifies the similarity between semantic vectors in a pair. The calculation of the loss for the $i_{th}$ case description is as shown in Equation (4).\n$l_i = -log\\frac{e^{sim(\\vec{p_i}, \\vec{p_i^+})/\\tau}}{\\sum_{c=1}^C(e^{sim(\\vec{p_i}, \\vec{p_i^+})/\\tau}+e^{sim(\\vec{p_i}, \\vec{p_i^c})/\\tau})}$ (4)\nWhere, $\\vec{p_i}$ represents the semantic vector of the $i_{th}$ case description, while $\\vec{p_i^+}$ and $\\vec{p_i^-}$ respectively denote the semantic vectors of the legal articles in the $c_{th}$ positive and negative pairs. Besides, $sim(\\vec{p_1}, \\vec{p_2})$ denotes the cosine similarity between vectors $\\vec{p_1}$ and $\\vec{p_2}$, and $\\tau$ is a temperature hyperparameter.\nThe trained RoBERTa model can encode case descriptions and legal articles into a joint semantic space, where the representation of a case description and its corresponding legal articles exhibit a closer semantic distance within this space."}, {"title": "4.1.2 Relevant legal articles searching", "content": "Having obtained a joint semantic space for case descriptions and legal articles through prior operations, this section utilizes this joint semantic space to search N legal articles relevant to a given case description. Given the case description X and a set of candidate legal articles A, we first encode them into the joint semantic space using the trained RoBERTa model, obtaining their respective semantic vectors. This process is illustrated in Equation (5).\n$\\vec{x}, [\\vec{a_1}, \\vec{a_2}, ..., \\vec{a_{|A|}}\\] = RoBERTa(X, A)$ (5)\nWhere, $\\vec{x}$ represents the semantic vector of case description X, and $\\vec{a_i}$ denotes the semantic vector of the $i_{th}$ legal article in set A.\nSubsequently, we compute the relevance between the case description and each candidate legal article through vector inner product, as shown in Equation (6), thereby selecting the N legal articles with the highest relevance.\n$Relevance(\\vec{x}; \\vec{a_i}) = \\vec{x} \\cdot \\vec{a_i}$ (6)\nIn the next section, we will utilize the N legal articles searched, along with case description X, to consult a conversational large language model. This is done to acquiring noteworthy factual elements within X."}, {"title": "4.1.3 Conversational large language model consultation", "content": "Conversational LLMs, with their vast parameter count, possess robust contextual reasoning capabilities and have learned a wealth of generic world knowledge during their pre-training. Furthermore, Conversational LLMs often exhibit strong zero-shot reasoning capabilities, thus enabling their direct use as ready-made tools without the need for additional fine-tuning. Based on these reasons, we use a Conversational LLM to assist us in acquiring factual elements from X. We construct the following question template.\nWe utilize this template to conduct inquiries with the conversational LLM, resulting in a list of factual elements denoted as F = [f1, f2, ..., f|F|]. These factual elements are subsequently be encoded as semantic vectors in Section 4.1.4. When the dataset is in Chinese, we utilize GLM-130B, a Chinese conversational LLM developed by ZhiTu HuaZhang Technology Co., Ltd., in this process. When the dataset is in English, we directly invoke the API of ChatGPT to achieve this process."}, {"title": "4.1.4 Factual elements encoding", "content": "Given the list of factual elements, F = [f1, f2, ..., f|F|], obtained from the previous process, we concatenate them and input the combined sequence into a BiGRU encoder. A BiGRU consists of two GRU layers that process the data in opposite directions: one forward GRU and one backward GRU. The forward GRU processes the sequence from f\u2081 to f|F|, and the backward GRU procsses it from f|F| to f\u2081. Each GRU updates its hidden state at each step in the sequence.\nLet $\\vec{h_t}$ be the hidden state of the forward GRU at time step t, and $\\vec{h_t}$ be the hidden state of the backward GRU at time step t. They are computed as Equations (7-8).\n$\\overrightarrow{h_t} = GRU(f_t, \\overrightarrow{h_{t-1}})$ (7)\n$\\overleftarrow{h_t} = GRU(f_t, \\overleftarrow{h_{t+1}})$ (8)\nThe final semantic vector u is typically obtained by concatenating the last hidden state of the forward GRU and the first hidden state of the backward GRU, as shown in Equation (9).\n$\\vec{u} = \\[\\overrightarrow{h_{|F|}}; \\overleftarrow{h_1}\\]$ (9)\nThe semantic vector, obtained through this process, encapsulates the information of all factual elements derived from the case description. This vector will subsequently be integrated into the inference model to enhance the model's prediction capabilities regarding legal charges."}, {"title": "4.2 Knowledge matching", "content": "This section introduces the second module of our method. This module matches case description X with a given knowledge base K. The knowledge snippets matched serve as prompts to enhance the reasoning model's prediction of legal charges.\nWe use THUOCL_Law as the knowledge base. THUOCL_Law is a subbase of the Tsinghua University Open Chinese Lexicon (THUOCL), a high-quality Chinese lexicon compiled and launched by the Natural Language Processing and Social Humanities Computing Laboratory of Tsinghua University, in which all subbases have undergone multiple rounds of manual screening to ensure the accuracy."}, {"title": "4.3 Legal language model reasoning", "content": "This section introduces the third module of our method. In this module, we employ a legal language model to reason the legal charge associated with the given case description X. Traditionally, the task of predicting legal charges is viewed as a classification problem, where the model's output is directly a probability distribution, and the index of the highest probability is the predicted label. However, we transform the task of legal charge prediction into a language modeling (cloze test) task, prompting the model to predict masked tokens. Then, we map the predictions at these masked positions onto the final category labels.\nTo implement the language modeling task, we construct hard prompt templates T\u2081 and T2, as follows:\nT\u2081 = \u201cHe will be charged with criminal responsibility for\"\nT2 = \u201cKeywords in the case description are as follows:\"\nThese hard prompts serve as part of the input for the inference model, guiding the model to predict masked tokens. In addition to hard prompts, we also incorporate two soft prompts S\u2081 and S2 into the input. Semantic vector u obtained in Section 4.1 will be merged with these soft prompts, injecting the knowledge about factual elements into the model's inference. Moreover, the masked tokens M = [m1, m2, ..., m|M|] and case description X = [X1, X2, ..., XL] are also essential components of the input. Lastly, the concatenation of knowledge snippets K, acquired in Section 4.2, is also included as a part of the input.\nIn summary, the input for the inference model is composed of the case description X, hard prompt texts T\u2081 and T2, soft prompts S\u2081 and S2, the masked sequence M = [m1, m2, ..., m|M|], and the concatenation of knowledge snippets K, as illustrated in Equation (10).\nX' = \\[S_1, t_{1,1}, t_{1,2}, ..., t_{1,|T_1|}, m_1, m_2, ..., m_{|M|}, X_1, X_2, ... X_L, t_{2,1}, t_{2,2}, ..., t_{2,|T_2|}, k_1, k_2, ..., k_{|K|}, S_2\\] (10)\nWhere, $t_{1,i}$ represents the $i_{th}$ token in T1, and $t_{2,i}$ denotes the $i_{th}$ token in T2. Besides, mi stands for the $i_{th}$ mask, xi refers to the $i_{th}$ token in X, and $k_i$ indicates the $i_{th}$ token in K.\nTo facilitate understanding, we illustrate X' more intuitively through the example provided in Figure 3. In the diagram, two purple tokens represent the soft prompts, the green section is T\u2081 and the blue section is T2. The red tokens indicate the masked tokens, the black text is the original case description X, and the yellow section is the concatenation of keywords.\nNextly, we input X' into the inference model, which is a pre-trained legal language model. The inference model consists of embedding and encoding layers, as shown in Figure 1. During the embedding layer stage, all tokens in X' except the soft prompts are embedded by the embedding layer of the inference model. At the same time, the soft prompts in X' are embedded by an additional trainable embedding matrix. This process is shown in Equation (11).\n$\\vec{e_i} = \\begin{cases}\n    S\\[i],\n    \\text{if } i \\in \\text{softidx} \\\\\n    Embedding(\\text{token}\\_i),    \\text{otherwise}\n  \\end{cases}$ (11)"}, {"title": "4.4 Legal charge category mapping", "content": "We need to map the outcomes of the language modeling (cloze test) task back to the classification task. To this end, this section constructs a mapping from predicted tokens to legal charge categories. We calculate the Jaccard similarities between the predicted tokens and the texts of the legal charge labels. For instance, if the predicted tokens has the highest Jaccard similarity with \u201cManufacture, sell, and disseminate obscene materials\", then the legal charge predicted by the model for the current case description is \u201cManufacture, sell, and disseminate obscene materials\".\nTherefore, the final prediction of the model is shown in Equation (15).\n$\\hat{y} = argmax(Jaccard(m_{1:|M|}, V_y))$ (15)\nWhere \u0177 is the final label predicted by the model, and $m_{1:|M|}$ denotes the predicted tokens at the masked positions, and $V_y$ denotes the text of category y."}, {"title": "5. Experimental settings", "content": "This section details the experimental setup, including the datasets used, baselines, and implementation specifics of our method."}, {"title": "5.1 Datasets", "content": "We utilized CAIL-2018 [7], the largest Chinese legal charge prediction dataset, as our experimental dataset. The authors acquired 2.6 million public criminal cases from the Supreme People's Courts of China and subjected them to preprocessing, ultimately obtaining 2,676,075 case description texts accompanied by 196 unique legal charge labels. Each case description corresponds to only one legal charge label, so the task is a single-label classification problem. In addition, 2/3 of the total are used as the training set and the remaining 1/3 as the test set."}, {"title": "5.2 Baselines", "content": "We compare the proposed method against the following baselines to verify its advancement.\n(1) CNN [1]: The method proposed by Wang et al., which uses convolutional neural networks to model the terminologies and keywords within case descriptions.\n(2) BILSTM [2]: The method proposed by Yang et al., which models the global semantics of case descriptions via bidirectional long and short-term memory networks.\n(3) BIGRU [3]: The method proposed by Chen et al. Compared to bidirectional long and short-term memory networks, bidirectional gated networks have lower computational complexity for modeling the global semantics of case descriptions.\n(4) Attention [5]: The attention mechanism-based method proposed by Sukanya and Priyadarshini. Attention mechanisms can assign different weights to different factual information in a case description. This method achieved state-of-the-art results across multiple benchmarks.\n(5) BERT [35]: The method of fine-tuning a BERT on the legal charge prediction task. This is a powerful baseline.\n(6) HMN [6]: This is a hierarchical matching network for crime classification proposed by Wang et al. This method is a novel and strong baseline on the CAIL2018 dataset.\n(7) ChatGLM [39]: We directly engage in dialogue with the Chinese conversational LLM, ChatGLM, to obtain the legal charge label corresponding to each case description."}, {"title": "5.3 Implementation specifics", "content": "All our experiments were conducted on a 40G A100 GPU. During the training phase, the model employed a learning rate of le-5, a batch size of 8. We employ Lawformer [22], a recently proposed legal pre-trained language model, as our inference model, which can accept text inputs up to a maximum length of 4,000. The number of masked tokens to be predicted is set to 20. Besides, we set a maximum training epoch of 50 with an early stopping mechanism."}, {"title": "6. Results and discussion", "content": "This section discusses the experimental results. Section 6.1 compares our method with baselines, while Section 6.2 validates the effectiveness of each module within our method through ablation experiments. Moreover, Section 6.3 analyzes the impact of the training data size on the performance of our method. Section 6.4 analyzes the hyperparameter settings, and Section 6.5 validates the interpretability of our model through a specific case study."}, {"title": "6.1 Comparison with baselines", "content": "displays the performance of our method compared to the baselines. From the table, it can be observed that the performances of CNN, BILSTM, and BiGRU are relatively similar, with macro F1 scores ranging between 0.66 and 0.71. CNN performs the best, which may be attributed to the fact that case descriptions are often lengthy, and thus modeling the sequence structure is less effective than modeling key information. Among the traditional neural network models presented in the first four rows, Attention achieves the best performance. Attention mechanisms are capable of focusing on local key information within case descriptions as well as the correlations between different pieces of information, hence achieving performance significantly beyond that of other traditional neural network baselines. Based on this, we can also infer that factual elements and knowledge snippets within case descriptions can enhance the effectiveness of legal charge prediction.\nFrom the fifth row of the table, it is evident that the usually strong baseline BERT performs worse than traditional neural network models, achieving only a macro F1 score of 0.61. This is due to BERT's input length limitation of 512, which prevents it from fully modeling case description texts that average over 1,000 in length. In contrast, traditional neural network models do not have an input length restriction, enabling them to model case descriptions more completely and thereby achieve better results than BERT. Our method employs Lawformer as the inference model, which can accept case description inputs of over 4000 in length, thereby enabling more complete modeling of case descriptions.\nFrom the sixth row of the table, it is apparent that the performance of HMN surpasses other baselines due to its integration of hierarchical matching, which not only allows for the complete modeling of the entire case description but also enables hierarchical modeling of key information. Furthermore, from the seventh row of the table, we can observe that the performance of ChatGLM is mediocre. Despite ChatGLM having learned a vast amount of general semantic knowledge and possessing good zero-shot reasoning capabilities, it still exhibits hallucination issues in specialized fields such as law and medicine.\nFinally, from the last row of the table, we can see that our method achieved the best results, with a macro F1 score reaching 0.84. This demonstrates that our method is feasible and effective. Furthermore, it suggests that factual elements obtained via relevant articles and the conversational LLM, as well as knowledge snippets acquired from the knowledge base, have enhanced the model's ability to predict legal charges. To further validate the contribution of each module in our method, we conducted ablation experiments in Section 4.2."}, {"title": "6.2 Ablation experiments", "content": "This section analyzes the contribution of each module in our method through ablation experiments, with the experimental results shown in Table 5. The first row of the table represents the performance of our original method on the CAIL-2018 dataset.\nFirstly, we removed the knowledge snippets module, with the experimental results shown in the second row of the table. It can be observed that the macro F1 score of the model decreased by 0.02, indicating that focusing on factual elements within case descriptions indeed enhances the prediction effectiveness of legal charges.\nNextly, we retained the knowledge snippets module and removed the factual element acquisition and encoding module. It can be seen from the third row of the table that this resulted in a decrease of 0.03 in the model's macro F1 score. This indicates that factual elements within case descriptions also contribute beneficially to the legal charge prediction task.\nFinally, we removed both modules simultaneously, with the experimental results shown in the fourth row of the table. It can be observed that the performance of the model significantly decreased by"}]}