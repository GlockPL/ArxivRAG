{"title": "A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method for Legal Charge Prediction", "authors": ["Jingyun Sun", "Chi Wei", "Yang Li"], "abstract": "Legal charge prediction, an essential task in legal AI, seeks to assign accurate charge labels to case descriptions, attracting significant recent interest. Existing methods primarily employ diverse neural network structures for modeling case descriptions directly, failing to effectively leverage multi-source external knowledge. We propose a prompt learning framework-based method that simultaneously leverages multi-source heterogeneous external knowledge from a legal knowledge base, a conversational LLM, and related legal articles. Specifically, we match knowledge snippets in case descriptions via the legal knowledge base and encapsulate them into the input through a hard prompt template. Additionally, we retrieve legal articles related to a given case description through contrastive learning, and then obtain factual elements within the case description through a conversational LLM. We fuse the embedding vectors of soft prompt tokens with the encoding vector of factual elements to achieve knowledge-enhanced model forward inference. Experimental results show that our method achieved state-of-the-art results on CAIL-2018, the largest legal charge prediction dataset, and our method has lower data dependency. Case studies also demonstrate our method's strong interpretability.", "sections": [{"title": "1. Introduction", "content": "Legal charge prediction is a crucial task in legal artificial intelligence, aimed at utilizing advanced technologies such as machine learning, deep learning, and natural language processing to analyze given case descriptions and thereby predict corresponding charge labels. Legal charge prediction not only helps legal professionals handle cases efficiently and accurately, reducing human errors and increasing the consistency and fairness of judgments, but also supports the enhancement of legal education and public legal awareness, by spreading legal knowledge and strengthening society's understanding and compliance with legal regulations.\nLegal charge prediction is often regarded as a classification problem, hence researchers typically adopt methods similar to those used for general text classification tasks to address it. For instance, Wang et al. proposed a convolutional neural networks-based approach [1], Yang et al. introduced a method based on bidirectional long short-term memory network [2], and Chen et al. developed a gated reccurent units-based method [3]. However, legal charge prediction differs from general text classification tasks in several ways. Firstly, legal texts, which contain a plethora of legal terminologies and keywords, are distinct from general texts, presenting challenges to universal models in understanding the content. Secondly, legal charge prediction focuses more on the factual information within texts, whereas general text classification tasks are concerned with the topics described by texts. Therefore, many researchers have utilized language models pretrained in the legal domain as the backbone to enhance the model's comprehension of legal texts [4]. Such models are better at capturing the domain-specific terminologies and keywords within legal texts. Nevertheless, the pretrained models they employed only allow for the input of 512 tokens, which is insufficient for modeling legal texts that often exceed this length. Moreover, to capture factual information in legal texts, Sukanya & Priyadarshini et al. proposed a model based on attention mechanism [5], and Wang et al. further introduced a hierarchical attention mechanism to capture factual information at different levels [6]. However, they only utilized the content of legal texts themselves to obtain factual information, without leveraging external structured knowledge.\nIn contrast, the method we propose not only enhances the model's comprehension of the textual content and task objectives but also fully leverages heterogeneous external legal knowledge from multiple sources. Firstly, we employ the newly introduced pre-trained language model, Lawformer, which is trained on a large-scale legal corpus and can accommodate text inputs exceeding 4,000 tokens, as our inference model. Lawformer aids in accurately comprehending the semantics of case descriptions and capturing the meanings expressed by legal terms and keywords. Subsequently, we utilize a legal knowledge base to match knowledge snippets from case descriptions, while employing a conversational LLM and relevant articles to extract factual elements from descriptions. This process introduces external components to assist the model in acquiring legal knowledge, thereby further enhancing the model's understanding of case descriptions. Finally, we propose using soft prompt tokens and hard prompt templates to encapsulate heterogeneous legal knowledge from multiple sources. Overall, the method presented in this paper leverages the paradigm of prompt learning to integrate heterogeneous legal knowledge from multiple sources into the model's forward reasoning, thus improving the model's predictive accuracy regarding legal charges.\nWe conducted extensive experiments on CAIL-2018, the largest legal charge prediction dataset to date [7]. The results demonstrate that our proposed method achieved results surpassing the baselines, with a macro F1 score of 0.84. Moreover, the experiments indicate that our method has the lowest dependency on training data. The performance of other baselines significantly diminishes as the scale of training data decreases, whereas our method still maintains a high F1 score. We also analyzed the contribution of each module within our method through ablation studies. Finally, we validated that our approach possesses strong interpretability, which is crucial for artificial intelligence tasks in the legal domain.\nOur primary contributions can be summarized as follows:\n1) We propose a legal charge prediction model that integrates multi-source heterogeneous legal knowledge.\n2) We introduce a method to encapsulate heterogeneous legal knowledge via the prompt-based learning framework.\n3) We propose the use of a conversational LLM and relevant legal articles to extract factual elements from case descriptions.\n4) We employ a specialized legal knowledge base to match knowledge snippets from case descriptions.\n5) We demonstrate the effectiveness of our method through extensive empirical validation."}, {"title": "2. Related work", "content": "This section presents the works related to our study. Firstly, the latest progress in legal charge prediction is introduced. Subsequently, the basic concepts and applications of prompt learning are introduced."}, {"title": "2.1 Legal charge prediction", "content": "Legal charge prediction is a crucial task in the field of legal artificial intelligence, aimed at predicting the legal charges corresponding to given case descriptions. Due to the scarcity of legal resources, individuals lacking legal knowledge often find it challenging to promptly seek legal advice from attorneys or legal professionals. Therefore, the automation of legal charge prediction can, to a certain extent, alleviate the issue of legal resource scarcity. Furthermore, legal charge prediction can also offer decision support for lawyers or judges, thereby enhancing their work efficiency.\nEarly legal charge predictions primarily relied on rule-based methods or mathematical models [8], [9], [10]. These methods have the advantage of transparent and intuitive reasoning processes, and once the inference rules are triggered, their outcomes are fixed. However, such methods exhibit poor generalization and struggle to effectively address language phenomena such as synonyms and polysemy in case descriptions. With the introduction of the Word2Vec concept by Mikolov [11], subsequent legal charge prediction methods have predominantly been based on semantic embeddings."}, {"title": "2.2 prompt learning", "content": "Prompt learning has recently garnered significant attention from researchers due to its ability to stimulate language models to better recall the semantic knowledge learned during pre-training [23], [24]. Unlike the standard downstream task fine-tuning paradigm, the prompt learning paradigm aligns downstream tasks with the pre-training tasks of language models. To this end, methods based on prompt learning should first convert different downstream tasks into a language modeling task [25], [26]. For instance, a traditional classification task is designed to fit the probability distribution $\\mathcal{Y} = (\\mathcal{X}; \\theta)$. Given a piece of text $x$ = [This pizza is so delicious], the model might output the prediction $y = 0 \\in \\{0,1,2\\}$ once $\\theta$ is learned. Where 0 denotes a positive sentiment label, 1 denotes a negative sentiment label, and 2 denotes a neutral sentiment label. However, the model aims to fit the function $\\mathcal{Y} = P(MASK = V_y|template(X);\\theta)$ by converting the task into a language modeling task. Here, $template(x)$ is a new text transformed from the original text by inserting specific prompt words and $V_y$ is the set of label words. For example, the original text $x$ = [This pizza is so delicious] could be transformed into $template(x)$=[This pizza is so delicious. It feels [MASK]], and the model is tasked with predicting the word at the [MASK] position based on $\\theta$, thereby inferring the sentiment label. The model might generate words such as \u201camazing\u201d, \u201cbad\u201d, or \"okay\u201d, which can then be mapped to the specific sentiment labels 0, 1, or 2.\nThe three core components in prompt learning are prompt templates, inference models, and label mappings. A prompt template is employed to encapsulate a original text into a new format featuring prompts and masks, as exemplified in the work of [27], [28], [29]. They integrate external knowledge at the prompt template stage to maximize the language model's understanding of the task. Our method also incorporates external knowledge during the prompt template stage. However, we propose the integration of multi-source heterogeneous knowledge into the prompt template unlike existing methods, thereby significantly enhancing the model's inference capabilities. Inference model is a core component of prompt learning, utilized for predicting the tokens at the mask positions based on the encapsulated text. Commonly used inference models include BERT [30], RoBERTa [31], the GPT series [32], and the T5 series [33]. Furthermore, some studies employ language models specialized in specific domains to cater to particular tasks. For instance, Zhu et al. proposed using CliniBERT [34] for prompt learning methods in the medical field, achieving state-of-the-art results. We utilize Lawformer, a pre-trained language model in the legal domain, as the inference model, making our method more suitable for legal charge prediction tasks."}, {"title": "3. Task formalization", "content": "The task of legal charge prediction aims to accomplish the following process: given a case description $X$ containing L tokens, the model predict a legal charge label $\\hat{y}$ based on the content of $X$. This process can be denoted as Equation 1.\n$\\hat{y} = Model(X; \\theta)$ (1)\nWhere $\\theta$ represents the learnable parameters within the model.\nIn this study, we additionally utilize a set A of legal articles, a conversational large language model LLM(\u00b7), and a legal knowledge base Kas aids, thereby resulting in the Equation 2.\n$\\hat{y} = Model(X, A, LLM(\\cdot), \\mathcal{K}; \\theta)$ (2)"}, {"title": "4. Methodology", "content": "Figure 2 illustrates our method, comprising four modules. The first module, as depicted in the lower-left corner of the figure, focuses on acquiring and encoding factual elements from the given case description $X$. In this module, case description $X$ search for the most relevant N legal articles via a joint semantic space, subsequently consulting a conversational Large Language Model (LLM) for factual elements in the case description based on these legal articles. The factual elements aquired are encoded into a semantic vector $\\vec{u}$ by a BiGRU encoder, and $\\vec{u}$ is then injected into the forward computation of the soft prompt tokens to enhance the model's reasoning capabilities for the task. The second module involves knowledge matching based on a legal knowledge base, wherein knowledge from the case description $X$ is matched with a knowledge base K. The knowledge snippets matched are then concatenated and added to the original input as prompt to further strengthen the model's reasoning. The third module is the cornerstone of our method consisting of a legal language model, as illustrated in the central part of the figure. The input to this legal language model includes five components: 1) two soft prompt tokens $S_1$ and $S_2$; two manually constructed template texts $T_1$ and $T_2$; 3) the masked tokens M; 4) the case description $X$; and 5) the knowledge snipptes K. The objective of the legal language model is to predict the tokens at the masked positions. The fourth module aims to map the predictions of the language model at the masked positions onto a legal charge category $\\hat{y}$, serving as the final process of our method."}, {"title": "4.1 Factual elements acquisition and encoding", "content": "This section introduces the first module of our method, which involves acquiring and encoding factual elements from the given case description. Factual elements in case descriptions are crucial for legal judgments, as they influence the overall understanding of the cases and the final verdicts [35]. These elements typically include the time and place of the event, the individuals involved, and the specific process of the event. In our method, the acquisition and encoding of factual elements involve the following four processes. Firstly, we utilize the case descriptions and legal articles from the entire training set to learn a joint semantic space. Then, when a case description $X$ is given, this joint semantic space is employed to find the N legal articles most relevant to it. Subsequently, these N legal articles are used to consult a conversational LLM about the most noteworthy factual elements in the case. Finally, the obtained factual elements are encoded into a semantic vector $\\vec{u}$. The following sections will elaborate on these processes."}, {"title": "4.1.1 Jointly semantic space learning", "content": "We manage to utilize relevant legal articles to extract noteworthy factual elements from case descriptions, as these articles explicitly define which factual elements are pertinent to specific legal charges. For instance, the legal article pertinent to the crime of copyright infringement is: \u201cActs such as copying and distributing literary, audio-visual, and computer software works for profit without the permission of the copyright holder, publishing books that are subject to another's exclusive publishing rights without consent, duplicating audio-visual products created by others without their permission, producing and exhibiting art works falsely attributed to another, where the amount of illegal gains is substantial or other serious circumstances are present\". From this, we can infer that elements such as whether the intent was for profit, whether copyright permission was obtained, and the amount of illegal gains, are factual aspects worthy of attention.\nTo match relevant legal articles with case descriptions, we propose the construction of a joint semantic space of both case descriptions and legal articles. We engage in contrastive training of the language model RoBERTa [31] to facilitate its learning of this joint semantic space. Compared to rule-based methods [36], [37], the language model can model deeper semantic connections between case descriptions and legal articles, thereby achieving superior matching outcomes. Furthermore, contrastive training places greater emphasis on the relative relationship between positive and negative samples compared to traditional neural network-based semantic matching methods [38]. Consequently, contrastive training aids RoBERTa in learning more distinct and discriminative features which are crucial in determining the relevance between case descriptions and legal articles. Next, we introduce the specific steps of using RoBERTa to learn the joint semantic space."}, {"title": "Step 1: Construct positive and negative pairs", "content": "Each case description in CAIL-2018, the largest dataset of legal charge prediction, has been labeled its relevant legal articles. Therefore, we can easily construct contrastive positive and negative pairs from the entire training set automatically. Given the training set containing the pairs of case descriptions and relevant legal articles $D_{Train} = \\{(X_1, R_1), (X_2, R_2), (X_3, R_3), ... \\}$, we use Algorithm 1 to automatically construct a set $\\mathcal{P}$ of positive and negative pairs. Where, $X_i$ represents the ith case description in the training set, and $R_i$ represents the legal articles related to it."}, {"title": "Step 2: Obtain representations of case descriptions and legal articles", "content": "We have obtained set $\\mathcal{P}$ of positive and negative pairs in the first step. Each pair in the set is either a related pair of case description and legal article or is irrelevant. In this step, we employ RoBERTa to acquire semantic vectors for the case description and legal article in each pair. This process is illustrated in Equation (3).\n$P = RoBERTa(\\mathcal{P})$ (3)\nHerein, $P$ represents a matrix, where the odd-numbered columns of $P$ denote the semantic vectors of case descriptions, and the even-numbered columns represent the semantic vectors of legal articles. Now that we have obtained the semantic vectors for all samples in positive and negative pairs, we proceed to train RoBERTa using a contrastive loss."}, {"title": "Step 3: Train the RoBERTa via contrastive loss", "content": "During the training, RoBERTa learns to decrease the semantic distance between samples in positive pairs while increasing the distance between those in negative pairs. This objective is achieved through a contrastive loss function, which quantifies the similarity between semantic vectors in a pair. The calculation of the loss for the ith case description is as shown in Equation (4).\n$l_i = -log\\frac{e^{sim(p_i, p_i^+)/\\tau}}{\\sum_{c=1}^C (e^{sim(p_i, p_i^+)/\\tau}+e^{sim(p_i, p_i^c)/\\tau})}$ (4)\nWhere, $p_i$ represents the semantic vector of the ith case description, while $\\vec{p_i^+}$ and $\\vec{p_i^c}$ respectively denote the semantic vectors of the legal articles in the cth positive and negative pairs. Besides, $sim(p_1, p_2)$ denotes the cosine similarity between vectors $p_1$ and $p_2$, and $\\tau$ is a temperature hyperparameter.\nThe trained RoBERTa model can encode case descriptions and legal articles into a joint semantic space, where the representation of a case description and its corresponding legal articles exhibit a closer semantic distance within this space."}, {"title": "4.1.2 Relevant legal articles searching", "content": "Having obtained a joint semantic space for case descriptions and legal articles through prior operations, this section utilizes this joint semantic space to search N legal articles relevant to a given case description. Given the case description X and a set of candidate legal articles A, we first encode them into the joint semantic space using the trained RoBERTa model, obtaining their respective semantic vectors. This process is illustrated in Equation (5).\n$\\vec{x}, [\\vec{a_1}, \\vec{a_2}, ..., \\vec{a_{|A|}}] = RoBERTa(X, A)$ (5)\nWhere, $\\vec{x}$ represents the semantic vector of case description X, and $\\vec{a_i}$ denotes the semantic vector of the ith legal article in set A.\nSubsequently, we compute the relevance between the case description and each candidate legal article through vector inner product, as shown in Equation (6), thereby selecting the N legal articles with the highest relevance.\n$Relevance(\\vec{x}; \\vec{a_i}) = \\vec{x} \\cdot \\vec{a_i}$ (6)\nIn the next section, we will utilize the N legal articles searched, along with case description X, to consult a conversational large language model. This is done to acquiring noteworthy factual elements within X."}, {"title": "4.1.3 Conversational large language model consultation", "content": "Conversational LLMs, with their vast parameter count, possess robust contextual reasoning capabilities and have learned a wealth of generic world knowledge during their pre-training. Furthermore, Conversational LLMs often exhibit strong zero-shot reasoning capabilities, thus enabling their direct use as ready-made tools without the need for additional fine-tuning. Based on these reasons, we use a Conversational LLM to assist us in acquiring factual elements from X.\nWe construct the following question template.\nWe utilize this template to conduct inquiries with the conversational LLM, resulting in a list of factual elements denoted as $\\mathcal{F} = [f_1, f_2, ..., f_{|F|}]$. These factual elements are subsequently be encoded as semantic vectors in Section 4.1.4. When the dataset is in Chinese, we utilize GLM-130B, a Chinese conversational LLM developed by ZhiTu HuaZhang Technology Co., Ltd., in this process. When the dataset is in English, we directly invoke the API of ChatGPT to achieve this process."}, {"title": "4.1.4 Factual elements encoding", "content": "Given the list of factual elements, $\\mathcal{F} = [f_1, f_2, ..., f_{|F|}]$, obtained from the previous process, we concatenate them and input the combined sequence into a BiGRU encoder. A BiGRU consists of two GRU layers that process the data in opposite directions: one forward GRU and one backward GRU. The forward GRU processes the sequence from $f_1$ to $f_{|F|}$, and the backward GRU procsses it from $f_{|F|}$ to $f_1$. Each GRU updates its hidden state at each step in the sequence.\nLet $\\overrightarrow{h_t}$ be the hidden state of the forward GRU at time step t, and $\\overleftarrow{h_t}$ be the hidden state of the backward GRU at time step t. They are computed as Equations (7-8).\n$\\overrightarrow{h_t} = GRU(f_t,\\overrightarrow{h_{t-1}})$ (7)\n$\\overleftarrow{h_t} = GRU(f_t,\\overleftarrow{h_{t+1}})$ (8)\nThe final semantic vector $\\vec{u}$ is typically obtained by concatenating the last hidden state of the forward GRU and the first hidden state of the backward GRU, as shown in Equation (9).\n$\\vec{u} = [\\overrightarrow{h_{|F|}}; \\overleftarrow{h_1}]$ (9)\nThe semantic vector , obtained through this process, encapsulates the information of all factual elements derived from the case description. This vector will subsequently be integrated into the inference model to enhance the model's prediction capabilities regarding legal charges."}, {"title": "4.2 Knowledge matching", "content": "This section introduces the second module of our method. This module matches case description $X$ with a given knowledge base K. The knowledge snippets matched serve as prompts to enhance the reasoning model's prediction of legal charges.\nWe use THUOCL_Law as the knowledge base. THUOCL_Law is a subbase of the Tsinghua University Open Chinese Lexicon (THUOCL), a high-quality Chinese lexicon compiled and launched by the Natural Language Processing and Social Humanities Computing Laboratory of Tsinghua University, in which all subbases have undergone multiple rounds of manual screening to ensure the accuracy. As presented in the table, a knowledge snippet is essentially a keyword. As [35] discussed, keywords in case descriptions are crucial for predictions of legal charges. We simply utilize regular expressions to match these keywords from the case description $X$, thereby obtaining a list of keywords K = [$k_1, k_2, ..., k_{|K|}$], also referred to as the list of knowledge snippets. The concatenation of the knowledge snippets in K will serve as a prompt, and in conjunction with other components, act as the input for the inference model."}, {"title": "4.3 Legal language model reasoning", "content": "This section introduces the third module of our method. In this module, we employ a legal language model to reason the legal charge associated with the given case description X. Traditionally, the task of predicting legal charges is viewed as a classification problem, where the model's output is directly a probability distribution, and the index of the highest probability is the predicted label. However, we transform the task of legal charge prediction into a language modeling (cloze test) task, prompting the model to predict masked tokens. Then, we map the predictions at these masked positions onto the final category labels.\nTo implement the language modeling task, we construct hard prompt templates $T_1$ and $T_2$, as follows:\n$T_1$ = \u201cHe will be charged with criminal responsibility for\"\n$T_2$ = \u201cKeywords in the case description are as follows:\"\nThese hard prompts serve as part of the input for the inference model, guiding the model to predict masked tokens. In addition to hard prompts, we also incorporate two soft prompts $S_1$ and $S_2$ into the input. Semantic vector $\\vec{u}$ obtained in Section 4.1 will be merged with these soft prompts, injecting the knowledge about factual elements into the model's inference. Moreover, the masked tokens M = [$m_1, m_2, ..., m_{|M|}$] and case description $X$ = [$x_1, x_2, ..., x_L$] are also essential components of the input. Lastly, the concatenation of knowledge snippets K, acquired in Section 4.2, is also included as a part of the input.\nIn summary, the input for the inference model is composed of the case description X, hard prompt texts $T_1$ and $T_2$, soft prompts $S_1$ and $S_2$, the masked sequence M = [$m_1, m_2, ..., m_{|M|}$], and the concatenation of knowledge snippets K, as illustrated in Equation (10).\n$X' = [S_1, T_1, t_{1,1}, t_{1,2}, ..., t_{1,|T_1|}, m_1, m_2, ..., m_{|M|}, x_1, x_2, ... x_L, T_2, t_{2,1}, t_{2,2}, ..., t_{2,|T_2|}, k_1, k_2, ..., k_{|K|}, S_2]$ (10)\nWhere, $t_{1,i}$ represents the ith token in $T_1$, and $t_{2,i}$ denotes the ith token in $T_2$. Besides, $m_i$ stands for the ith mask, $x_i$ refers to the ith token in X, and $k_i$ indicates the ith token in K.\nTo facilitate understanding, we illustrate X' more intuitively through the example provided in Figure 3. In the diagram, two purple tokens represent the soft prompts, the green section is $T_1$ and the blue section is $T_2$. The red tokens indicate the masked tokens, the black text is the original case description X, and the yellow section is the concatenation of keywords.\nNextly, we input X' into the inference model, which is a pre-trained legal language model. The inference model consists of embedding and encoding layers, as shown in Figure 1. During the embedding layer stage, all tokens in X' except the soft prompts are embedded by the embedding layer of the inference model. At the same time, the soft prompts in X' are embedded by an additional trainable embedding matrix. This process is shown in Equation (11).\n$\\vec{e_i} = \\begin{cases} S[i], & \\text{if } i \\in softidx \\\\ Embedding(token_i), & \\text{otherwise} \\end{cases}$ (11)\nWhere $S \\in R^{|softidx| \\times d_h}$ is a trainable embedding matrix, and $soft_{idx}$ is the index of the soft prompt token. $d_h$ is embedding dimension of the model. Therefore, an embedding vector sequence E of all the tokens (including soft prompt tokens) in X' can be obtained by the equation.\n$\\mathcal{E} = [\\vec{e_{S_1}}, \\vec{e_{t_{1,1}}}, \\vec{e_{t_{1,2}}}, ..., \\vec{e_{t_{1,|T_1|}}}, \\vec{e_{m_1}}, \\vec{e_{m_2}}, ..., \\vec{e_{m_{|M|}}}, \\vec{e_{x_1}}, \\vec{e_{x_2}}, ..., \\vec{e_{x_L}}, \\vec{e_{t_{2,1}}}, \\vec{e_{t_{2,2}}}, ..., \\vec{e_{t_{2,|T_2|}}}, \\vec{e_{k_1}}, \\vec{e_{k_2}}, ..., \\vec{e_{k_{|K|}}}, \\vec{e_{S_2}}]$\nWhere $\\vec{e_{t_{1,i}}}, \\vec{e_{t_{2,i}}} \\vec{e_{m_i}}, \\vec{e_{x_i}}$ and $\\vec{e_{k_i}}$ denote the embedding vectors of the ith tokens in $T_1, T_2, M, X,$ and K respectively. Besides, $\\vec{e_{S_1}}$ and $\\vec{e_{S_2}}$ denote the embedding vectors of the soft prompt tokens $S_1$ and $S_2$.\nTo inject the inference model with factual element information during its forward computation, we add the semantic vector $\\vec{u}$ obtained in Section 4.1 to the vectors $\\vec{e_{S_1}}$ and $\\vec{e_{S_2}}$ respectively. This results in prompt vectors enriched with factual element information. This process is demonstrated in Equations (12) and (13).\n$\\vec{e_{S_1}}^* = \\vec{e_{S_1}} + \\vec{u}$ (12)\n$\\vec{e_{S_2}}^* = \\vec{e_{S_2}} + \\vec{u}$ (13)\nSubsequently, we replace $\\vec{e_{S_1}}$ and $\\vec{e_{S_2}}$ in $\\mathcal{E}$ using $\\vec{e_{S_1}}^*$ and $\\vec{e_{S_2}}^*$ to obtain:\n$\\mathcal{E}' = [\\vec{e_{S_1}^*}, \\vec{e_{t_{1,1}}}, \\vec{e_{t_{1,2}}}, ..., \\vec{e_{t_{1,|T_1|}}}, \\vec{e_{m_1}}, \\vec{e_{m_2}}, ..., \\vec{e_{m_{|M|}}}, \\vec{e_{x_1}}, \\vec{e_{x_2}}, ..., \\vec{e_{x_L}}, \\vec{e_{t_{2,1}}}, \\vec{e_{t_{2,2}}}, ..., \\vec{e_{t_{2,|T_2|}}}, \\vec{e_{k_1}}, \\vec{e_{k_2}}, ..., \\vec{e_{k_{|K|}}}, \\vec{e_{S_2}}^*]$\nFinally, we input $\\mathcal{E}'$ into the encoding layer of the inference model and obtain the hidden layer outputs of the model, that is, the contextual representations for each token. This process is shown as Equation (14).\n$R = [\\vec{r_{S_1}^*}, \\vec{r_{t_{1,1}}}, \\vec{r_{t_{1,2}}}, ..., \\vec{r_{t_{1,|T_1|}}}, \\vec{r_{m_1}}, \\vec{r_{m_2}}, ..., \\vec{r_{m_{|M|}}}, \\vec{r_{x_1}}, \\vec{r_{x_2}}, ..., \\vec{r_{x_L}}, \\vec{r_{t_{2,1}}}, \\vec{r_{t_{2,2}}}, ..., \\vec{r_{t_{2,|T_2|}}}, \\vec{r_{k_1}}, \\vec{r_{k_2}}, ..., \\vec{r_{k_{|K|}}}, \\vec{r_{S_2}}^*] = Encoding(\\mathcal{E}')$ (14)\nThe model's objective is to predict the tokens at the masked position. Therefore, by projecting $\\vec{r_{m_1}}, \\vec{r_{m_2}}, ..., \\vec{r_{m_{|M|}}}$ into the vocabulary space, we can obtain the probability distributions of the predicted tokens. Subsequently, by selecting the indices with the highest probabilities, the model can determine the tokens at the masked positions. We denote the ith predicted token at the masked positions as $\\hat{m_i}$.\nThe next section describe the process of mapping predicted tokens $\\hat{m_1}, \\hat{m_2}, ..., \\hat{m_{|M|}}$ to a charge category label."}, {"title": "4.4 Legal charge category mapping", "content": "We need to map the outcomes of the language modeling (cloze test) task back to the classification task. To this end, this section constructs a mapping from predicted tokens to legal charge categories. We calculate the Jaccard similarities between the predicted tokens and the texts of the legal charge labels. For instance, if the predicted tokens has the highest Jaccard similarity with \u201cManufacture, sell, and disseminate obscene materials\", then the legal charge predicted by the model for the current case description is \u201cManufacture, sell, and disseminate obscene materials\". Therefore, the final prediction of the model is shown in Equation (15).\n$\\hat{y} = argmax(Jaccard(\\hat{m_{1:|M|}}, V_y))$ (15)\nWhere $\\hat{y}$ is the final label predicted by the model, and $\\hat{m_{1:|M|}}$ denotes the predicted tokens at the masked positions, and $V_y$ denotes the text of category y."}, {"title": "5. Experimental settings", "content": "This section details the experimental setup, including the datasets used, baselines, and implementation specifics of our method."}, {"title": "5.1 Datasets", "content": "We utilized CAIL-2018 [7], the largest Chinese legal charge prediction dataset, as our experimental dataset. The authors acquired 2.6 million public criminal cases from the Supreme People's Courts of China and subjected them to preprocessing, ultimately obtaining 2,676,075 case description texts accompanied by 196 unique legal charge labels. Each case description corresponds to only one legal charge label, so the task is a single-label classification problem. In addition, 2/3 of the total are used as the training set and the remaining 1/3 as the test set."}, {"title": "5.2 Baselines", "content": "We compare the proposed method against the following baselines to verify its advancement.\n(1) CNN [1]: The method proposed by Wang et al., which uses convolutional neural networks to model the terminologies and keywords within case descriptions.\n(2) BILSTM [2]: The method proposed by Yang et al., which models the global semantics of case descriptions via bidirectional long and short-term memory networks.\n(3) BIGRU [3]: The method proposed by Chen et al. Compared to bidirectional long and short-term memory networks, bidirectional gated networks have lower computational complexity for modeling the global semantics of case descriptions.\n(4) Attention [5]: The attention mechanism-based method proposed by Sukanya and Priyadarshini. Attention mechanisms can assign different weights to different factual information in a case description. This method achieved state-of-the-art results across multiple benchmarks.\n(5) BERT [35]: The method of fine-tuning a BERT on the legal charge prediction task. This is a powerful baseline.\n(6) HMN [6]: This is a hierarchical matching network for crime classification proposed by Wang et al. This method is a novel and strong baseline on the CAIL2018 dataset.\n(7) ChatGLM [39]: We directly engage in dialogue with the Chinese conversational LLM, ChatGLM, to obtain the legal charge label corresponding to each case description."}, {"title": "5.3 Implementation specifics", "content": "All our experiments were conducted on a 40G A100 GPU. During the training phase, the model employed a learning rate of 1e-5, a batch size of 8. We employ Lawformer [22], a recently proposed legal pre-trained language model, as our inference model, which can accept text inputs up to a maximum length of 4,000. The number of masked tokens to be predicted is set to 20. Besides, we set a maximum training epoch of 50 with an early stopping mechanism."}, {"title": "6. Results and discussion", "content": "This section discusses the experimental results. Section 6.1 compares our method with baselines, while Section 6.2 validates the effectiveness of each module within our method through ablation experiments. Moreover, Section 6.3 analyzes the impact of the training data size on the performance of our method. Section 6.4 analyzes the hyperparameter settings, and Section 6.5 validates the interpretability of our model through a"}]}