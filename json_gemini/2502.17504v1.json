{"title": "Protein Large Language Models: A Comprehensive Survey", "authors": ["Yijia Xiao", "Wanjia Zhao", "Junkai Zhang", "Yiqiao Jin", "Han Zhang", "Zhicheng Ren", "Renliang Sun", "Haixin Wang", "Guancheng Wan", "Pan Lu", "Xiao Luo", "Yu Zhang", "James Zou", "Yizhou Sun", "Wei Wang"], "abstract": "Protein-specific large language models (Protein LLMs) are revolutionizing protein science by enabling more efficient protein structure prediction, function annotation, and design. While existing surveys focus on specific aspects or applications, this work provides the first comprehensive overview of Protein LLMs, covering their architectures, training datasets, evaluation metrics, and diverse applications. Through a systematic analysis of over 100 articles, we propose a structured taxonomy of state-of-the-art Protein LLMs, analyze how they leverage large-scale protein sequence data for improved accuracy, and explore their potential in advancing protein engineering and biomedical research. Additionally, we discuss key challenges and future directions, positioning Protein LLMs as essential tools for scientific discovery in protein science.", "sections": [{"title": "Introduction", "content": "Proteins are essential biological molecules, driving functions such as catalyzing biochemical reactions, maintaining cell structure, and enabling cellular communication. Understanding their sequence-structure-function relationships is central to biological research. However, traditional experimental methods, including X-ray crystallography, NMR spectroscopy, and cryo-electron microscopy, are time-consuming and labor-intensive, posing bottlenecks for large-scale applications.\nRecent advancements in language modeling have revolutionized computational biology, offering powerful tools for protein analysis. Protein large language models (Protein LLMs) share several foundational similarities with LLMs: 1) Training objectives and learning paradigms, both LLMs and Protein LLMs are trained in a self-supervised manner on large-scale datasets using objectives such as masked language modeling, auto-regressive modeling, or sentence permutation, learning to predict missing or next elements in sequences from the vocabulary. While LLMs predict missing words or phrases within textual data, Protein LLMs predict amino acids or subsequences within protein sequences. 2) Pretraining data. Protein LLMs adopt a data-driven paradigm to learn directly from large-scale protein datasets. The datasets for training Protein LLMs consist of vast collections of protein sequences, analogous to the textual corpora used for LLMs. This eliminates the need for explicit feature engineering, allowing Protein LLMs to learn intricate patterns, such as structural motifs, evolutionary relationships, and functional insights, similar to how LLMs capture semantic and syntactic structures in language.\nThis paradigm shift has led to the emergence of highly effective models that can predict protein folding, annotate biological functions, and even design novel proteins with desired characteristics. Beyond their predictive capabilities, Protein LLMs also provide interactive interfaces that allow users to upload protein sequences or structural files (e.g., PDB format), pose questions, and interact with the model in a conversational manner, proving deeper insights into protein structure, function, and design.\nWe present the first dedicated survey of Protein LLMs, analyzing their unique architectures, training methodologies, and practical applications in protein research. While previous studies have explored the applications of various computational methods for protein research or discussed the role of language models in general scientific domains such as biomedicine and chemistry, this survey focuses specifically on Protein LLMs\u2013a rapidly evolving area at the intersection of computational biology and NLP.\nThe key contributions are as follows:\n\u2022 Architectural Overview. A structured taxonomy of state-of-the-art Protein LLMs detailing their unique architectures for protein understanding (\u00a72) and generation (\u00a73), highlighting how these models surpass traditional experimental methods in both efficiency and accuracy (Appendix \u00a7A).\n\u2022 Data Insights. A comprehensive summary of datasets for pretraining, fine-tuning, and benchmarking Protein LLMs, providing critical insights into data curation strategies and their impact on model performance (\u00a74).\n\u2022 Evaluation Protocols. A thorough discussion of methodologies for assessing the performance and impact of Protein LLMs, including comprehensive new benchmarking strategies (\u00a75 and Appendix \u00a7B).\n\u2022 Applications. A detailed exploration of practical applications in protein prediction, annotation, and design, remarkably highlighting recent innovative advancements and showcasing the transformative potential of Protein LLMs in advancing biomedical research."}, {"title": "LLM Methods for Protein Understanding and Prediction", "content": "2.1 Problem Definition\nA protein, composed of amino acids (residues), can be represented as a sequence [x1,...,xL] in the residue token space P, where L denotes its length. According to Anfinsen's dogma, a protein's primary sequence determines its structure and function. General problems in protein understanding and prediction are as follows:\nI. Sequence-to-Property Prediction: $f_\\theta : P \\rightarrow \\mathbb{R}^+$ mapping sequences to numerical properties, such as stability or fluorescence intensity.\nII. Sequence-to-Label Prediction: $f_\\theta :P \\rightarrow \\mathcal{L}$ mapping sequences to categorical labels, including secondary structure types, contact maps, or functional annotations.\nIII. Sequence-to-Structure Prediction $f_\\theta : P \\rightarrow \\mathcal{S}$ mapping sequences to the 3D folding structures (i.e. tertiary structures).\nIV. Sequence-to-Text Understanding: $f_e : P \\rightarrow \\mathcal{T}$, where $\\mathcal{T}$ represents generated textual descriptions of protein sequences.\n2.2 Protein Sequence Models\nIndividual Protein Sequences Models. Protein language models process amino acid sequences into meaningful representations for downstream tasks including structure and function prediction. Like NLP models, they are usually first pretrained on large sequence datasets with masked language modeling (MLM) objective; and then the protein sequences' embeddings are adapted for downstream tasks. Initially, researchers leveraged long short-term memory (LSTM) architectures to learn representation of proteins. Following the breakthrough of transformer architectures in NLP, transformer-based protein language models emerged as the new paradigm. Large-scale transformer models, scaling up to billions of parameters and trained on millions of protein sequences, have demonstrated remarkable effectiveness for protein understanding and prediction tasks, and 3D structure folding. The interpretability of these Protein LLMs has also been explored, with analyzing learned representations through the lens of attention. Beyond general-purpose protein language models, several works have focused on domain-specific applications. For instance, applied BiLSTM to model viral escape patterns; TCR-BERT specialized in T-cell receptor (TCR) analysis for improved TCR-antigen binding prediction; PeptideBERT focused on predicting key properties of peptides; adapted ESM-1b for enzymatic function prediction.\nMultiple Sequence Alignments (MSA) Models. MSA aligns homologous proteins within sequence space by mapping their residues to the coordinate framework of a designated seed sequence. MSA reveals evolutionary relationships between proteins and thus serves as a cornerstone of computational biology, particularly for mutation effects prediction. The MSA Transformer"}, {"title": "Structure-Integrated and Knowledge-Enhanced Models", "content": "Beyond residue sequences, many models integrate additional information, such as structure data or external knowledge, to enhance protein understanding and prediction ability.\nStructure-Integrated Models: Structural information plays an important role in protein understanding, as a protein's functions are determined by its structures. Therefore, many works have incorporated structural information to enhance protein modeling ability. Some works utilized structure information as additional inputs. For instance, fused global structure information captured by structure encoder (GVP, GearNet, or CDConv) into representations of ESM-2; SaProt incorporated local structural information for each amino acid, derived from Foldseek, to generate structure-aware tokens. Alternatively, other works injected the structure information only in the training stage by either additional training tasks or contrastive learning. Some studies have also leveraged pretrained protein language models to improve structure models.\nKnowledge-Enhanced Models: Beyond large protein sequence datasets, information in other formats can further enhance a model's understanding of proteins in the training stage. OntoProtein and KeAP incorporated knowledge graphs data during training by additional MLM objectives and/or contrastive learning to inject factual biological knowledge into the pre-trained Protein LLMs. ProteinBERT performed dual-task learning during pretraining to learn both protein sequence modeling and Gene Ontology (GO) annotation prediction. It utilized a specialized BERT architecture with parallel input pathways for sequences and annotations. To leverage the rich information in textual descriptions or other modalities, ProteinCLIP and MolBind applied contrastive learning between protein sequences and textual descriptions and/or molecular to learn improved embeddings.\n2.4 Protein Description and Annotation Models\nThe previously mentioned models have primarily focused on learning protein representations and utilizing them for classification, regression, or 3D structure folding tasks. To enhance expressiveness and understanding, more recent models have been trained on both protein sequences and textual data, allowing them to integrate NLP capabilities with protein representation learning. proposed ProTranslator, a bilingual translation framework between protein sequences and GO functions with textual descriptions. ProTranslator encoded and aligned the textual definitions of GO functions and protein sequences within the same low-dimensional space, facilitating the annotation of novel GO functions and the generation of textual descriptions for proteins. BioTranslator further improved ProTranslator by extending the bilingual framework to a multilingual translation framework, embedding text and multiple biomedical modalities into a shared space. ProtST was a framework designed to jointly learn from protein sequences and their associated biomedical text descriptions. It integrated protein language models (e.g., ESM or ProtBERT) with biomedical language models (e.g., PubMedBERT) to fuse sequence and text information through pre-training tasks. Prot2Text combined ESM-2 with a structure encoder (RGCN) and extended function prediction from categorical classification to free-text descriptions. BioT5 and BioT5+ further unified molecular information within a more comprehensive training framework.\nThere have also been several interactive LLMs for protein understanding. These models enhanced pretrained LLMs with protein comprehension by integrating a protein processing module. For instance, ProteinChat allowed users to input protein structures and query them using texts. Prote-"}, {"title": "LLM Methods for Protein Engineering, Generation and Translation", "content": "inGPT extended this capability by supporting both protein sequences and structures as inputs. In these models, protein data were processed through Protein LLMs to generate embeddings, which were then projected to the natural language embedding space. The backbone LLMs integrated these adapted embeddings with user's queries to produce meaningful answers.\n3 LLM Methods for Protein Engineering, Generation and Translation\nProtein engineering and generation aims to design protein sequences with desired attributes (e.g. structures and properties). Given the desired attributes $\\mathcal{T}$ and reference protein sequence $\\mathcal{S}$ (optional), the model is expected to output a protein sequence $\\mathcal{S}'$ with desired attributes. Key tasks include:\nI. Protein Engineering: $f_\\theta : (\\mathcal{S},\\mathcal{T}) \\rightarrow \\mathcal{S}'$ modifies protein $\\mathcal{S}$ toward the desired attributes $\\mathcal{T}$, yielding the engineered protein $\\mathcal{S}'$.\nII. Protein Generation: $f_\\theta : (\\mathcal{T},\\mathcal{R}) \\rightarrow \\mathcal{P}$ generates proteins with attributes $\\mathcal{T}$ by sampling from the protein space using random seeds $\\mathcal{R}$.\nIII. Protein Translation: $f_e: (\\mathcal{P},\\mathcal{T}) \\rightarrow \\mathcal{P}'$ translates a protein $\\mathcal{P}$ into an alternative representation $\\mathcal{P}'$ based on the target translation parameters $\\mathcal{T}$.\n3.1 Protein Engineering Models\nProteinDT is a multimodal protein design framework that robustly integrates textual protein knowledge with sequence-based generative modeling. ProteinDT employs contrastive alignment and a facilitator module, enabling zero-shot text-to-protein generation and editing. Meanwhile, PLMEAE is a closed-loop protein engineering framework that integrates protein language models with an automated biofoundry within a Design-Build-Test-Learn cycle. Furthermore, Toursynbio introduces an agent that is capable of facilitating the modification and engineering of wet lab proteins.\n3.2 Protein Generation Models\nProtein generation models are designed to create novel protein sequences for specific engineering applications, often leveraging large-scale datasets of existing proteins with known amino acid sequences and properties. These models typically employ decoder-based architectures to generate functional protein sequences conditioned on various biological annotations. For example, ProGen is a GPT-based generative protein engineering model that treats protein engineering as an unsupervised sequence generation process, and generates functional protein sequences conditioned on annotations like molecular function or taxonomy. The model is trained on diverse, non-redundant protein sequences from databases such as UniProt and Pfam, utilizing associated tags for conditional generation. ProtGPT2 is another model that generates de novo protein sequences with natural amino acid compositions using autoregressive modeling. In particular, they noticed that the generated sequences could explore a few uncharted areas of the protein sequence space. ProGen2 is an extended version of ProGen, featuring a larger model size and a more extensive training dataset to enhance sequence diversity. Notably, ProGen2 can predict protein fitness without requiring additional fine-tuning. Recently, ProLLaMA proposed a multi-task protein language model to handle both protein sequence generation and protein understanding tasks. Built on LLaMA2, ProLLaMA introduces a two-stage training framework: (1) continued pre-training on protein sequences, and (2) instruction tuning with a 13-million-sample dataset for multitasking capabilities.\nBeyond conventional decoder-based approaches, Ankh employs an encoder-decoder architecture that optimizes efficiency by reducing parameters while maintaining high-quality protein generation. PAAG is another encoder-decoder architecture which focuses on the alignment between textual annotations and protein sequences at multiple levels before generating new sequences. Pinal does not directly generate protein sequences from text. Instead, it first constrains the protein design space by generating structure tokens, then predicts sequences based on those constraints to improve foldability and function alignment.\nWhile many of these models are designed for general protein generation, some focus on specialized applications such as antibody design. IgLM employs autoregressive sequence generation conditioned on an antibody's sequence chain type and species of origin. As a further step, PALM-H3 specifically targets SARS-CoV-2 antibody generation, highlighting how protein generation language models can be tailored for highly specific protein design tasks."}, {"title": "Protein Translation Models", "content": "Protein translation models are specifically developed to handle tasks that require translating between different protein representations, which could be helpful in protein design.\nProstT5 addresses the task of simultaneously modeling the dual nature of proteins\u2014their linear one-dimensional (1D) sequences and three-dimensional (3D) structures\u2014using a bilingual language model based on T5 and ProtT5. It extracts features and patterns from both the sequence and the structure data Fold2Seq is another model that learns structure-sequence relationships of proteins. The model could guide designs of protein sequences conditioned on desired structural folds. Recently, ProtAgents, a multiagent framework, has been proposed to handle 1D sequence generation and 3D fold generation simultaneously. LM-DESIGN is a method for reprogramming protein language models (pLMs) to design protein sequences for given structural folds."}, {"title": "Datasets", "content": "Datasets are crucial for training and evaluating Protein LLMs. They are categorized into pre-"}, {"title": "Evaluation Metrics", "content": "Comprehensive evaluation is essential for applying Protein LLMs, which are assessed on tasks like structure prediction, function prediction, and sequence generation. Appendix 5 provides detailed descriptions of structure and function prediction metrics, as well as sequence generation metrics for generative Protein LLMs.\n5.1 Structure Prediction Metrics\nRoot Mean Square Deviation (RMSD) measures the distance between predicted and actual"}, {"title": "Conclusion and Future Work", "content": "This survey provides a comprehensive overview of Protein Large Language Models, highlighting their architectures, datasets, evaluation, and applications. These works represent significant advancements in protein science and offer innovative approaches to protein analysis and design. In addition to these advancements, several challenges remain to be solved in the future.\nProtein Dynamics. AlphaFold has been shown to provide accurate static 3D structures. However, proteins are naturally dynamic molecules with various conformations. Although several works incorporate 3D structures into LLMs, the conformational dynamics of proteins have not yet been considered. Since conformational dynamics are highly related to the transporter functions of proteins, it would benefit the model to include protein dynamics.\nCombination with Single-cell Data. Recently, single-cell proteomics sequencing technology has attracted extensive attention in the field of biology, which can help us understand the pathways in specific cells. Since LLMs have shown effectiveness in understanding both proteins and single-cell data, they can be extended to learn from single-cell proteomics data in the future.\nTowards Biological Applications. Although several biological applications have been studied in recent works, a range of detailed and complex problems remain unsolved, including protein-ligand interaction learning, cryptic pocket identification, and rational ligand generation. These applications require extensive and diverse domain knowledge of proteins and their related fields. We believe LLMs have the potential to incorporate and utilize more domain knowledge to solve these problems.\nInterpretability. In addition to effectiveness, interpretability is also of strong significance for trustworthy models. Previous language models for proteins have provided extensive case studies, such as key residue analysis, which could be challenging for large-scale and closed-source models. To improve interpretability, InterPLM employs sparse autoencoders to extract biologically meaningful features from Protein LLMs, revealing their alignment with known biological concepts. Inspired by this, we should design prompts to enhance the interpretability of Protein LLMs for reliable outputs."}, {"title": "Limitations", "content": "This survey primarily focuses on Protein LLMs. We acknowledge that the study of protein interactions with other molecules (e.g., DNA, RNA) in the inter-molecular domain is a broad and valuable field worth reviewing. Given its vast scope, we do not extensively cover it in this survey, and instead focus on Protein LLMs centered on proteins themselves. In the future, we may either expand our review to include these areas or write a separate survey specifically dedicated to this domain, providing more comprehensive coverage for researchers."}]}